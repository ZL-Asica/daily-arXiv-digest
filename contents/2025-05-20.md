# 2025-05-20

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 28]

- [cs.CL](#cs.CL) [Total: 291]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [DesignFromX: Empowering Consumer-Driven Design Space Exploration through Feature Composition of Referenced Products](https://arxiv.org/abs/2505.11666)

*Runlin Duan, Chenfei Zhu, Yuzhao Chen, Yichen Hu, Jingyu Shi, Karthik Ramani*

**Main category:** cs.HC

**Keywords:** Generative AI, Consumer-Driven Design, Product Design, User Experience, Engagement

**Relevance Score:** 7

**TL;DR:** DesignFromX enables consumer-driven product design using generative AI to help users select and customize design features efficiently.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To incorporate consumers into exploring product design while addressing their struggles with articulating preferred features.

**Method:** Development of the DesignFromX system which utilizes generative AI to identify and compose design features from product images into new conceptual images and 3D models.

**Key Contributions:**

	1. Creation of DesignFromX system for consumer-driven design
	2. Use of generative AI to facilitate feature identification and composition
	3. Backed by user study demonstrating effectiveness

**Result:** A user study with 24 participants shows that DesignFromX reduces barriers to consumer-driven design, enhancing engagement and enjoyment.

**Limitations:** 

**Conclusion:** DesignFromX successfully empowers consumers in the product design process, making it more enjoyable and accessible.

**Abstract:** Industrial products are designed to satisfy the needs of consumers. The rise of generative artificial intelligence (GenAI) enables consumers to easily modify a product by prompting a generative model, opening up opportunities to incorporate consumers in exploring the product design space. However, consumers often struggle to articulate their preferred product features due to their unfamiliarity with terminology and their limited understanding of the structure of product features. We present DesignFromX, a system that empowers consumer-driven design space exploration by helping consumers to design a product based on their preferences. Leveraging an effective GenAI-based framework, the system allows users to easily identify design features from product images and compose those features to generate conceptual images and 3D models of a new product. A user study with 24 participants demonstrates that DesignFromX lowers the barriers and frustration for consumer-driven design space explorations by enhancing both engagement and enjoyment for the participants.

</details>


### [2] [Designing for Constructive Civic Communication: A Framework for Human-AI Collaboration in Community Engagement Processes](https://arxiv.org/abs/2505.11684)

*Cassandra Overney*

**Main category:** cs.HC

**Keywords:** community engagement, democratic governance, human-AI collaboration, AI in civic contexts, constructive communication

**Relevance Score:** 6

**TL;DR:** The paper explores how human-AI collaboration can enhance community engagement in democratic governance while addressing potential risks associated with AI technologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve community engagement processes faced with resource constraints and inclusivity challenges in democratic governance.

**Method:** The paper analyzes human-AI collaboration strategies, identifies key communication pathways, and proposes design considerations for AI integration in civic contexts.

**Key Contributions:**

	1. Identification of key communication pathways for human-AI collaboration in civic contexts.
	2. Design considerations for integrating AI that safeguard community agency.
	3. Recommendations for using AI to facilitate meaningful community engagement.

**Result:** The findings suggest that thoughtful integration of AI can enhance constructive communication in community engagement while reducing risks such as bias and transparency issues.

**Limitations:** The study may not cover all potential biases inherent in AI systems or the varying impacts across different community contexts.

**Conclusion:** By ensuring high levels of control over decision-making for leaders and communities, AI can significantly improve civic communication dynamics.

**Abstract:** Community engagement processes form a critical foundation of democratic governance, yet frequently struggle with resource constraints, sensemaking challenges, and barriers to inclusive participation. These processes rely on constructive communication between public leaders and community organizations characterized by understanding, trust, respect, legitimacy, and agency. As artificial intelligence (AI) technologies become increasingly integrated into civic contexts, they offer promising capabilities to streamline resource-intensive workflows, reveal new insights in community feedback, translate complex information into accessible formats, and facilitate reflection across social divides. However, these same systems risk undermining democratic processes through accuracy issues, transparency gaps, bias amplification, and threats to human agency. In this paper, we examine how human-AI collaboration might address these risks and transform civic communication dynamics by identifying key communication pathways and proposing design considerations that maintain a high level of control over decision-making for both public leaders and communities while leveraging computer automation. By thoughtfully integrating AI to amplify human connection and understanding while safeguarding agency, community engagement processes can utilize AI to promote more constructive communication in democratic governance.

</details>


### [3] [ConflictLens: LLM-Based Conflict Resolution Training in Romantic Relationship](https://arxiv.org/abs/2505.11715)

*Jiwon Chun, Gefei Zhang, Meng Xia*

**Main category:** cs.HC

**Keywords:** romantic conflicts, psychological theory, large language models

**Relevance Score:** 4

**TL;DR:** ConflictLens is an interactive system designed to help individuals analyze and reflect on psychological factors influencing romantic conflicts using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the superficial understanding of romantic conflicts and provide deeper insights into the underlying psychological mechanisms.

**Method:** Developed an interactive system that incorporates psychological theory and utilizes LLMs for analysis and reflection on romantic conflicts.

**Key Contributions:**

	1. Integration of psychological theory with LLMs for conflict analysis
	2. Multi-level strategy recommendations
	3. Guided dialogue exercises for improved communication

**Result:** ConflictLens was shown to enhance emotional insight, improve relational understanding, and promote constructive communication among users.

**Limitations:** 

**Conclusion:** The system offers a novel method for fostering self-awareness and growth in romantic relationships.

**Abstract:** Romantic conflicts are often rooted in deep psychological factors such as coping styles, emotional responses, and communication habits. Existing systems tend to address surface-level behaviors or isolated events, offering limited support for understanding the underlying dynamics. We present ConflictLens, an interactive system that leverages psychological theory and large language models (LLMs) to help individuals analyze and reflect on the deeper mechanisms behind their conflicts. The system provides multi-level strategy recommendations and guided dialogue exercises, including annotation, rewriting, and continuation tasks. A case study demonstrates how ConflictLens supports emotional insight, improves relational understanding, and fosters more constructive communication. This work offers a novel approach to supporting self-awareness and growth in romantic relationships.

</details>


### [4] [Utilizing Provenance as an Attribute for Visual Data Analysis: A Design Probe with ProvenanceLens](https://arxiv.org/abs/2505.11784)

*Arpit Narechania, Shunan Guo, Eunyee Koh, Alex Endert, Jane Hoffswell*

**Main category:** cs.HC

**Keywords:** analytic provenance, data visualization, user interaction, self-reflection, decision-making

**Relevance Score:** 7

**TL;DR:** The paper presents ProvenanceLens, a visual data analysis system that allows users to control and reflect on their interactions with data through the visualization of analytic provenance attributes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user control and self-reflection in data analysis by modeling provenance as an attribute available during analysis.

**Method:** The authors modeled two provenance attributes (recency and frequency of user interactions), integrated them into a prototype system, ProvenanceLens, and conducted an exploratory study with 16 users.

**Key Contributions:**

	1. Introduction of ProvenanceLens for visualizing analytic provenance.
	2. Exploratory study demonstrating user capabilities in self-reflection and decision-making via provenance attributes.
	3. Insights into user strategies for utilizing provenance information.

**Result:** Users were able to accurately and confidently answer questions about their analysis, while mismatches between their mental models and provenance encodings led to valuable self-reflection.

**Limitations:** The study was limited to a small sample size of 16 users, which may affect the generalizability of the findings.

**Conclusion:** Provenance modeling can significantly aid in decision-making and self-reflection during data analysis by making interaction histories explicit and usable.

**Abstract:** Analytic provenance can be visually encoded to help users track their ongoing analysis trajectories, recall past interactions, and inform new analytic directions. Despite its significance, provenance is often hardwired into analytics systems, affording limited user control and opportunities for self-reflection. We thus propose modeling provenance as an attribute that is available to users during analysis. We demonstrate this concept by modeling two provenance attributes that track the recency and frequency of user interactions with data. We integrate these attributes into a visual data analysis system prototype, ProvenanceLens, wherein users can visualize their interaction recency and frequency by mapping them to encoding channels (e.g., color, size) or applying data transformations (e.g., filter, sort). Using ProvenanceLens as a design probe, we conduct an exploratory study with sixteen users to investigate how these provenance-tracking affordances are utilized for both decision-making and self-reflection. We find that users can accurately and confidently answer questions about their analysis, and we show that mismatches between the user's mental model and the provenance encodings can be surprising, thereby prompting useful self-reflection. We also report on the user strategies surrounding these affordances, and reflect on their intuitiveness and effectiveness in representing provenance.

</details>


### [5] [AR Secretary Agent: Real-time Memory Augmentation via LLM-powered Augmented Reality Glasses](https://arxiv.org/abs/2505.11888)

*Raphaël A. El Haddad, Zeyu Wang, Yeonsu Shin, Ranyi Liu, Yuntao Wang, Chun Yu*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Large Language Models, Memory Enhancement, Human-Computer Interaction, Computer Vision

**Relevance Score:** 9

**TL;DR:** The paper presents an AR Secretary Agent that uses LLMs and Computer Vision to provide real-time information about individuals during conversations, showing a 20% increase in memory retention based on user studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Professionals often struggle with recalling details about individuals they interact with daily, leading to missed connections and inefficiencies.

**Method:** Implementation of an AR system that captures visual and auditory data, leveraging LLMs and Computer Vision for real-time identification and conversation summarization.

**Key Contributions:**

	1. Introduction of an AR Secretary Agent utilizing LLMs and AR technologies
	2. Implementation of a user study validating memory enhancement
	3. Demonstration of real-time contextual assistance during conversations

**Result:** User studies with 13 participants demonstrated a 20% improvement in memory retention regarding past interactions.

**Limitations:** Limited sample size of participants in user studies; potential privacy concerns with constant data capture.

**Conclusion:** The AR Secretary can significantly aid memory recall during social interactions, enhancing professionals' ability to connect with others.

**Abstract:** Interacting with a significant number of individuals on a daily basis is commonplace for many professionals, which can lead to challenges in recalling specific details: Who is this person? What did we talk about last time? The advant of augmented reality (AR) glasses, equipped with visual and auditory data capture capabilities, presents a solution. In our work, we implemented an AR Secretary Agent with advanced Large Language Models (LLMs) and Computer Vision technologies. This system could discreetly provide real-time information to the wearer, identifying who they are conversing with and summarizing previous discussions. To verify AR Secretary, we conducted a user study with 13 participants and showed that our technique can efficiently help users to memorize events by up to 20\% memory enhancement on our study.

</details>


### [6] [To Recommend or Not to Recommend: Designing and Evaluating AI-Enabled Decision Support for Time-Critical Medical Events](https://arxiv.org/abs/2505.11996)

*Angela Mastrianni, Mary Suhyun Kim, Travis M. Sullivan, Genevieve Jayne Sippel, Randall S. Burd, Krzysztof Z. Gajos, Aleksandra Sarcevic*

**Main category:** cs.HC

**Keywords:** AI decision support, human-AI interaction, medical emergencies, decision-making, healthcare

**Relevance Score:** 9

**TL;DR:** This study examines an AI-enabled decision-support system for medical providers treating traumatic injuries, evaluating human-AI interaction strategies and identifying barriers to effective AI recommendations.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve decision-making in medical emergencies where information is limited, helping providers interpret AI outputs for optimal treatment.

**Method:** User research with physicians followed by an online experiment with 35 medical providers evaluating two interaction strategies: AI information synthesis and AI recommendations.

**Key Contributions:**

	1. Evaluation of AI interaction strategies in emergency contexts
	2. Identification of barriers to AI recommendations in medical settings
	3. Implications for future AI system development in healthcare

**Result:** Providers made better decisions with AI recommendations compared to no AI support and two socio-technical barriers were identified.

**Limitations:** Study limited to specific types of traumatic injuries and small sample size.

**Conclusion:** The study highlights implications for developing AI-enabled decision support in time-sensitive medical situations, contributing to HCI research in healthcare.

**Abstract:** AI-enabled decision-support systems aim to help medical providers rapidly make decisions with limited information during medical emergencies. A critical challenge in developing these systems is supporting providers in interpreting the system output to make optimal treatment decisions. In this study, we designed and evaluated an AI-enabled decision-support system to aid providers in treating patients with traumatic injuries. We first conducted user research with physicians to identify and design information types and AI outputs for a decision-support display. We then conducted an online experiment with 35 medical providers from six health systems to evaluate two human-AI interaction strategies: (1) AI information synthesis and (2) AI information and recommendations. We found that providers were more likely to make correct decisions when AI information and recommendations were provided compared to receiving no AI support. We also identified two socio-technical barriers to providing AI recommendations during time-critical medical events: (1) an accuracy-time trade-off in providing recommendations and (2) polarizing perceptions of recommendations between providers. We discuss three implications for developing AI-enabled decision support used in time-critical events, contributing to the limited research on human-AI interaction in this context.

</details>


### [7] [From Data to Actionable Understanding: A Learner-Centered Framework for Dynamic Learning Analytics](https://arxiv.org/abs/2505.12064)

*Madjid Sadallah*

**Main category:** cs.HC

**Keywords:** Learning Analytics Dashboards, Adaptive Understanding Framework, sensemaking, self-regulation, personalized learning

**Relevance Score:** 6

**TL;DR:** This paper introduces the Adaptive Understanding Framework (AUF) for designing Learning Analytics Dashboards (LADs) that support learners' sensemaking and self-regulation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in enabling Learning Analytics Dashboards (LADs) to empower learners effectively, as existing approaches prioritize data visualization over cognitive processes essential for actionable learning.

**Method:** The paper presents the Adaptive Understanding Framework (AUF), integrating situational awareness, dynamic sensemaking strategies, adaptive mechanisms, and metacognitive support to transform LADs into dynamic learning partners.

**Key Contributions:**

	1. Introduction of the Adaptive Understanding Framework (AUF) for LAD design
	2. Integration of a multi-dimensional model of awareness and sensemaking strategies
	3. Proposing a research agenda for empirical validation of the framework

**Result:** The AUF provides a personalized learning ecosystem that responds to individual needs, fostering actionable understanding of learning data, enhancing learners' sensemaking and self-regulation.

**Limitations:** 

**Conclusion:** AUF-inspired LADs can lead to more effective, equitable, and engaging learning experiences by addressing the limitations of current LAD design.

**Abstract:** Learning Analytics Dashboards (LADs) often fall short of their potential to empower learners, frequently prioritizing data visualization over the cognitive processes crucial for translating data into actionable learning strategies. This represents a significant gap in the field: while much research has focused on data collection and presentation, there is a lack of comprehensive models for how LADs can actively support learners' sensemaking and self-regulation. This paper introduces the Adaptive Understanding Framework (AUF), a novel conceptual model for learner-centered LAD design. The AUF seeks to address this limitation by integrating a multi-dimensional model of situational awareness, dynamic sensemaking strategies, adaptive mechanisms, and metacognitive support. This transforms LADs into dynamic learning partners that actively scaffold learners' sensemaking. Unlike existing frameworks that tend to treat these aspects in isolation, the AUF emphasizes their dynamic and intertwined relationships, creating a personalized and adaptive learning ecosystem that responds to individual needs and evolving understanding. The paper details the AUF's core principles, key components, and suggests a research agenda for future empirical validation. By fostering a deeper, more actionable understanding of learning data, AUF-inspired LADs have the potential to promote more effective, equitable, and engaging learning experiences.

</details>


### [8] [TrainBo: An Interactive Robot-assisted Scenario Training System for Older Adults with Dementia](https://arxiv.org/abs/2505.12080)

*Kwong Chiu Fung, Wai Ho Mow*

**Main category:** cs.HC

**Keywords:** robot-assisted training, dementia, cognitive engagement, interactive systems, self-determination theory

**Relevance Score:** 7

**TL;DR:** The study presents TrainBo, an interactive robot-assisted cognitive training system for older adults with dementia, showing improvements in engagement and motivation after usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for accessible technologies to aid cognitive training for the elderly, specifically those with dementia.

**Method:** Design of an interactive robot-assisted system using self-determination theory, followed by formative and formal studies to derive design requirements and evaluate usability.

**Key Contributions:**

	1. Development of TrainBo, a robot-assisted cognitive training system
	2. Evaluation of user engagement and motivation in older adults with dementia
	3. Insights into design requirements for interactive robotic systems in cognitive training

**Result:** Pilot testing on seven older adults with dementia showed significant improvements in behavioral and emotional engagement as well as intrinsic motivation after four weeks of using TrainBo.

**Limitations:** 

**Conclusion:** The findings indicate that interactive robotic systems like TrainBo can enhance cognitive training and engagement in older adults with dementia, providing insights for future robotic applications.

**Abstract:** Dementia is an overall decline in memory and cognitive skills severe enough to reduce an elders ability to perform everyday activities. There is an increasing need for accessible technologies for cognitive training to slow down the cognitive decline. With the ability to provide instant feedback and assistance, social robotic systems have been proven effective in enhancing learning abilities across various age groups. This study focuses on the design of an interactive robot-assisted scenario training system TrainBo with self-determination theory, derives design requirements through formative and formal studies and the system usability is also be evaluated. A pilot test is conducted on seven older adults with dementia in an elderly care center in Hong Kong for four weeks. Our finding shows that older adults with dementia have an improvement in behavioural engagement, emotional engagement, and intrinsic motivation after using Trainbo. These findings can provide valuable insights into the development of more captivating interactive robots for extensive training purposes.

</details>


### [9] [Designing Scaffolded Interfaces for Enhanced Learning and Performance in Professional Software](https://arxiv.org/abs/2505.12101)

*Yimeng Liu, Misha Sra*

**Main category:** cs.HC

**Keywords:** Scaffolded Interface Design, Learning Challenges, User Studies, Software Learnability, Complex Interfaces

**Relevance Score:** 7

**TL;DR:** This paper presents ScaffoldUI, a method for designing scaffolded interfaces that improve software learnability by reducing complexity and providing structured guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the learning challenges faced by newcomers in professional software with complex interfaces and poor guidance.

**Method:** The paper introduces ScaffoldUI, which organizes task-relevant tools, progressively discloses complexity, and enhances navigation through structured guidance in a 3D software context.

**Key Contributions:**

	1. Introduction of ScaffoldUI for scaffolded interface design.
	2. Evaluation through user studies indicating reduced cognitive load and improved learning.
	3. Insights for future research on scaffolded interfaces across various software applications.

**Result:** User studies showed improved task performance and reduced perceived task load when using scaffolded interfaces compared to traditional designs.

**Limitations:** Further research needed for diverse software environments and long-term user adaptation.

**Conclusion:** Scaffolded interfaces can significantly enhance learning and productivity in software use by clearly linking tools to concepts within taskflows.

**Abstract:** Professional software offers immense power but also presents significant learning challenges. Its complex interfaces, as well as insufficient built-in structured guidance and unfamiliar terminology, often make newcomers struggle with task completion. To address these challenges, we introduce ScaffoldUI, a method for scaffolded interface design to reduce interface complexity, provide structured guidance, and enhance software learnability. The scaffolded interface presents task-relevant tools, progressively discloses tool complexity, and organizes tools based on domain concepts, aiming to assist task performance and software learning. To evaluate the feasibility of our interface design method, we present a technical pipeline for scaffolded interface implementation in professional 3D software, i.e., Blender, and conduct user studies with beginners (N=32) and experts (N=8). Study results demonstrate that our scaffolded interfaces significantly reduce perceived task load caused by interface complexity, support task performance through structured guidance, and augment learning by clearly connecting concepts and tools within the taskflow context. Based on a discussion of the user study findings, we offer insights for future research on designing scaffolded interfaces to support instruction, productivity, creativity, and cross-software workflows.

</details>


### [10] [Behind the Screens: Uncovering Bias in AI-Driven Video Interview Assessments Using Counterfactuals](https://arxiv.org/abs/2505.12114)

*Dena F. Mujtaba, Nihar R. Mahapatra*

**Main category:** cs.HC

**Keywords:** bias evaluation, counterfactual analysis, AI hiring, personality assessment, ethical transparency

**Relevance Score:** 6

**TL;DR:** This paper addresses ethical concerns in AI-enhanced personality assessments used in hiring by introducing a counterfactual-based framework to evaluate and quantify bias.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI in personality assessments for hiring raises ethical issues, particularly concerning bias amplification based on protected attributes like gender, ethnicity, and age.

**Method:** The paper presents a counterfactual-based framework using generative adversarial networks (GANs) to generate altered representations of job applicants, allowing for a multimodal fairness analysis without direct access to underlying models.

**Key Contributions:**

	1. Introduces a counterfactual-based framework for bias evaluation in AI personality assessments.
	2. Employs GANs for generating counterfactual representations to analyze bias without model access.
	3. Supports multimodal evaluation across visual, audio, and textual features.

**Result:** The method reveals significant disparities across demographic groups when applied to a state-of-the-art personality prediction model.

**Limitations:** The framework may require domain-specific adaptations for different assessment contexts and does not address all forms of bias comprehensively.

**Conclusion:** This work provides a scalable tool for fairness auditing of AI hiring platforms, promoting ethical transparency through counterfactual approaches.

**Abstract:** AI-enhanced personality assessments are increasingly shaping hiring decisions, using affective computing to predict traits from the Big Five (OCEAN) model. However, integrating AI into these assessments raises ethical concerns, especially around bias amplification rooted in training data. These biases can lead to discriminatory outcomes based on protected attributes like gender, ethnicity, and age. To address this, we introduce a counterfactual-based framework to systematically evaluate and quantify bias in AI-driven personality assessments. Our approach employs generative adversarial networks (GANs) to generate counterfactual representations of job applicants by altering protected attributes, enabling fairness analysis without access to the underlying model. Unlike traditional bias assessments that focus on unimodal or static data, our method supports multimodal evaluation-spanning visual, audio, and textual features. This comprehensive approach is particularly important in high-stakes applications like hiring, where third-party vendors often provide AI systems as black boxes. Applied to a state-of-the-art personality prediction model, our method reveals significant disparities across demographic groups. We also validate our framework using a protected attribute classifier to confirm the effectiveness of our counterfactual generation. This work provides a scalable tool for fairness auditing of commercial AI hiring platforms, especially in black-box settings where training data and model internals are inaccessible. Our results highlight the importance of counterfactual approaches in improving ethical transparency in affective computing.

</details>


### [11] [Towards Immersive Mixed Reality Street Play: Understanding Collocated Bodily Play with See-through Head-Mounted Displays in Public Spaces](https://arxiv.org/abs/2505.12516)

*Botao Amber Hu, Rem Rungu Lin, Yilan Elan Tao, Samuli Laato, Yue Li*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Head-Mounted Displays, Immersive Gameplay, Social Implications, Public Spaces

**Relevance Score:** 7

**TL;DR:** This paper explores the emerging paradigm of Mixed Reality Head-Mounted Displays in public spaces through studies on the Multiplayer Omnipresent Fighting Arena (MOFA) game, highlighting social implications and design challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the transition of Mixed Reality HMDs from controlled environments to spontaneous public settings, particularly in the domain of immersive gameplay.

**Method:** Empirical studies conducted using research-through-design game probes called Multiplayer Omnipresent Fighting Arena (MOFA) to understand user interactions in real-world settings.

**Key Contributions:**

	1. Empirical insights into user interactions with MR HMDs in public spaces.
	2. Analysis of the social implications of MR HMDs on public gameplay dynamics.
	3. Identification of design challenges for future immersive applications.

**Result:** Identification of social implications, challenges, and opportunities related to Immersive Mixed Reality Street Play (IMRSP) in public spaces.

**Limitations:** 

**Conclusion:** The findings underscore the need for further exploration of the social dynamics and design considerations for MR experiences in urban environments as technology advances.

**Abstract:** We're witnessing an upcoming paradigm shift as Mixed Reality (MR) See-through Head-Mounted Displays (HMDs) become ubiquitous, with use shifting from controlled, private settings to spontaneous, public ones. While location-based pervasive mobile games like Pok\'emon GO have seen success, the embodied interaction of MR HMDs is moving us from phone-based screen-touching gameplay to MR HMD-enabled collocated bodily play. Major tech companies are continuously releasing visionary videos where urban streets transform into vast mixed reality playgrounds-imagine Harry Potter-style wizard duels on city streets. However, few researchers have conducted real-world, in-the-wild studies of such Immersive Mixed Reality Street Play (IMRSP) in public spaces in anticipation of a near future with prevalent MR HMDs. Through empirical studies on a series of research-through-design game probes called Multiplayer Omnipresent Fighting Arena (MOFA), we gain initial understanding of this under-explored area by identifying the social implications, challenges, and opportunities of this new paradigm.

</details>


### [12] [Adapting to LLMs: How Insiders and Outsiders Reshape Scientific Knowledge Production](https://arxiv.org/abs/2505.12666)

*Huimin Xu, Houjiang Liu, Yan Leng, Ying Ding*

**Main category:** cs.HC

**Keywords:** AI in research, knowledge production, collaboration, large language models, CSCW

**Relevance Score:** 8

**TL;DR:** This study explores the impact of large language models (LLMs) on scientific knowledge production and offers insights into their role in reshaping collaboration and knowledge practices in AI-driven science.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the integration of AI, particularly LLMs, transforms collaboration and knowledge production in scientific research.

**Method:** The study employs an evaluation workflow that merges insider-outside perspectives with a framework for knowledge production to quantify the effects of LLMs in scientific communities.

**Key Contributions:**

	1. Quantifies the impact of LLMs on scientific knowledge production.
	2. Identifies new forms of collaboration and infrastructure influenced by AI.
	3. Offers insights into the transformation of knowledge production with generative AI.

**Result:** The findings demonstrate that LLMs facilitate innovation and reorganization within scientific communities, highlighting the significant changes in knowledge production due to generative AI.

**Limitations:** 

**Conclusion:** The research suggests potential new avenues for exploration in CSCW, grounded in the insights gained regarding the transformational impact of LLMs on scientific collaboration and knowledge practices.

**Abstract:** CSCW has long examined how emerging technologies reshape the ways researchers collaborate and produce knowledge, with scientific knowledge production as a central area of focus. As AI becomes increasingly integrated into scientific research, understanding how researchers adapt to it reveals timely opportunities for CSCW research -- particularly in supporting new forms of collaboration, knowledge practices, and infrastructure in AI-driven science.   This study quantifies LLM impacts on scientific knowledge production based on an evaluation workflow that combines an insider-outsider perspective with a knowledge production framework. Our findings reveal how LLMs catalyze both innovation and reorganization in scientific communities, offering insights into the broader transformation of knowledge production in the age of generative AI and sheds light on new research opportunities in CSCW.

</details>


### [13] [Beyond Individual UX: Defining Group Experience(GX) as a New Paradigm for Group-centered AI](https://arxiv.org/abs/2505.12780)

*Soohwan Lee, Seoyeong Hwang, Kyungho Lee*

**Main category:** cs.HC

**Keywords:** Group Experience, Human-Centered AI, Group-Centered AI, Collaboration, Group Dynamics

**Relevance Score:** 8

**TL;DR:** This paper introduces Group Experience (GX) and a proposed framework of Group-centered AI (GCAI) to enhance group interactions in HCI.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to address the neglect of group dynamics in current HCI and AI research, emphasizing the need for a framework that considers collective user experiences.

**Method:** The authors propose a group-centered design approach informed by social psychology and group dynamics, focusing on holistic group interactions instead of just individual outcomes.

**Key Contributions:**

	1. Introduction of Group Experience (GX) concept.
	2. Proposal of Group-centered AI (GCAI) framework.
	3. Development of novel evaluative metrics for group interactions.

**Result:** The paper outlines new evaluative metrics and strategies to capture the essence of group collaboration and decision-making processes.

**Limitations:** 

**Conclusion:** The authors call for a shift in focus from individual experiences to group dynamics in HCI, advocating for a transformative approach to collaborative interactions.

**Abstract:** Recent advancements in HCI and AI have predominantly centered on individual user experiences, often neglecting the emergent dynamics of group interactions. This provocation introduces Group Experience(GX) to capture the collective perceptual, emotional, and cognitive dimensions that arise when individuals interact in cohesive groups. We challenge the conventional Human-centered AI paradigm and propose Group-centered AI(GCAI) as a framework that actively mediates group dynamics, amplifies diverse voices, and fosters ethical collective decision-making. Drawing on social psychology, organizational behavior, and group dynamics, we outline a group-centered design approach that balances individual autonomy with collective interests while developing novel evaluative metrics. Our analysis emphasizes rethinking traditional methodologies that focus solely on individual outcomes and advocates for innovative strategies to capture group collaboration. We call on researchers to bridge the gap between micro-level experiences and macro-level impacts, ultimately enriching and transforming collaborative human interactions.

</details>


### [14] [StudyAlign: A Software System for Conducting Web-Based User Studies with Functional Interactive Prototypes](https://arxiv.org/abs/2505.13046)

*Florian Lehmann, Daniel Buschek*

**Main category:** cs.HC

**Keywords:** HCI, user studies, web applications, data logging, questionnaires

**Relevance Score:** 8

**TL;DR:** StudyAlign streamlines the setup of web-based user studies for HCI researchers by providing an integrated software system for managing experiments, collecting data, and facilitating easy questionnaire integration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexity of setting up interactive systems for user studies and to allow researchers to focus on interaction design rather than technical hurdles.

**Method:** Development of StudyAlign, which includes a participant frontend, admin panel, questionnaire integration, JavaScript library for data logging, and a backend server for data persistence and API functions.

**Key Contributions:**

	1. Integrated platform for managing web-based user studies
	2. Streamlined data logging and questionnaire integration
	3. Facilitates replication of studies

**Result:** StudyAlign simplifies the process for researchers to conduct web-based experiments, enhancing study replication and reducing logistical efforts.

**Limitations:** 

**Conclusion:** StudyAlign supports online HCI studies effectively by reducing setup complexity and focusing on design aspects.

**Abstract:** Interactive systems are commonly prototyped as web applications. This approach enables studies with functional prototypes on a large scale. However, setting up these studies can be complex due to implementing experiment procedures, integrating questionnaires, and data logging. To enable such user studies, we developed the software system StudyAlign which offers: 1) a frontend for participants, 2) an admin panel to manage studies, 3) the possibility to integrate questionnaires, 4) a JavaScript library to integrate data logging into prototypes, and 5) a backend server for persisting log data, and serving logical functions via an API to the different parts of the system. With our system, researchers can set up web-based experiments and focus on the design and development of interactions and prototypes. Furthermore, our systematic approach facilitates the replication of studies and reduces the required effort to execute web-based user studies. We conclude with reflections on using StudyAlign for conducting HCI studies online.

</details>


### [15] [Human Response to Decision Support in Face Matching: The Influence of Task Difficulty and Machine Accuracy](https://arxiv.org/abs/2505.13218)

*Marina Estévez-Almenzar, Ricardo Baeza-Yates, Carlos Castillo*

**Main category:** cs.HC

**Keywords:** AI decision support, human decision-making, task difficulty, face matching, user perception

**Relevance Score:** 7

**TL;DR:** This paper investigates the impact of AI-based decision support systems on human accuracy in face matching tasks, highlighting the importance of task difficulty and user perception.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI-based decision support affects human decision-making in high-stakes tasks where errors can have significant consequences.

**Method:** Extensive experiments were conducted to assess the influence of task difficulty and system precision on human decision outcomes.

**Key Contributions:**

	1. Investigates the relationship between AI support and human decision accuracy in challenging tasks.
	2. Identifies the critical role of task difficulty in influencing human judgment.
	3. Calls for a user-centric approach in the design of AI decision support systems.

**Result:** Task difficulty significantly impaired human precision and the ability to assess the accuracy of AI suggestions.

**Limitations:** 

**Conclusion:** The design of decision support systems must consider context and user perception to enhance their effectiveness.

**Abstract:** Decision support systems enhanced by Artificial Intelligence (AI) are increasingly being used in high-stakes scenarios where errors or biased outcomes can have significant consequences. In this work, we explore the conditions under which AI-based decision support systems affect the decision accuracy of humans involved in face matching tasks. Previous work suggests that this largely depends on various factors, such as the specific nature of the task and how users perceive the quality of the decision support, among others. Hence, we conduct extensive experiments to examine how both task difficulty and the precision of the system influence human outcomes. Our results show a strong influence of task difficulty, which not only makes humans less precise but also less capable of determining whether the decision support system is yielding accurate suggestions or not. This has implications for the design of decision support systems, and calls for a careful examination of the context in which they are deployed and on how they are perceived by users.

</details>


### [16] [How Adding Metacognitive Requirements in Support of AI Feedback in Practice Exams Transforms Student Learning Behaviors](https://arxiv.org/abs/2505.13381)

*Mak Ahmad, Prerna Ravi, David Karger, Marc Facciotti*

**Main category:** cs.HC

**Keywords:** AI feedback, Metacognition, STEM education

**Relevance Score:** 7

**TL;DR:** A practice exam system integrates AI-generated feedback with textbook references to enhance learning in a large biology course, showing increased student engagement and confidence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of providing personalized feedback at scale in large undergraduate STEM courses.

**Method:** An AI system using GPT-4o generates personalized feedback based on students' explanations and confidence ratings, directing them to relevant textbook sections, evaluated through interaction logs and surveys.

**Key Contributions:**

	1. Integration of AI feedback with targeted textbook references
	2. Empirical evaluation of metacognitive strategies in exams
	3. High levels of student engagement with learning materials

**Result:** Though no significant performance differences were found across feedback types, trends indicated benefits from confidence ratings and explanations, with high engagement and satisfaction reported by students.

**Limitations:** 

**Conclusion:** Embedding structured reflection requirements may be more effective than advanced feedback mechanisms in improving student outcomes.

**Abstract:** Providing personalized, detailed feedback at scale in large undergraduate STEM courses remains a persistent challenge. We present an empirically evaluated practice exam system that integrates AI generated feedback with targeted textbook references, deployed in a large introductory biology course. Our system encourages metacognitive behavior by asking students to explain their answers and declare their confidence. It uses OpenAI's GPT-4o to generate personalized feedback based on this information, while directing them to relevant textbook sections. Through interaction logs from consenting participants across three midterms (541, 342, and 413 students respectively), totaling 28,313 question-student interactions across 146 learning objectives, along with 279 surveys and 23 interviews, we examined the system's impact on learning outcomes and engagement. Across all midterms, feedback types showed no statistically significant performance differences, though some trends suggested potential benefits. The most substantial impact came from the required confidence ratings and explanations, which students reported transferring to their actual exam strategies. About 40 percent of students engaged with textbook references when prompted by feedback -- far higher than traditional reading rates. Survey data revealed high satisfaction (mean rating 4.1 of 5), with 82.1 percent reporting increased confidence on practiced midterm topics, and 73.4 percent indicating they could recall and apply specific concepts. Our findings suggest that embedding structured reflection requirements may be more impactful than sophisticated feedback mechanisms.

</details>


### [17] [Agreeing and Disagreeing in Collaborative Knowledge Graph Construction: An Analysis of Wikidata](https://arxiv.org/abs/2306.11766)

*Elisavet Koutsiana, Tushita Yadav, Nitisha Jain, Albert Meroño-Peñuela, Elena Simperl*

**Main category:** cs.HC

**Keywords:** Wikidata, disagreement, collaborative knowledge construction, decision-making, community engagement

**Relevance Score:** 5

**TL;DR:** The paper analyzes disagreements in Wikidata discussions, highlighting patterns of interaction and decision-making in a collaborative environment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of disagreements in discussions on Wikidata, which is crucial for contributor performance and community health.

**Method:** The study employs both quantitative and qualitative analyses to examine topics of disagreement, interaction patterns, and participant roles in Wikidata discussions.

**Key Contributions:**

	1. Identified patterns of disagreement in a collaborative knowledge community
	2. Highlighted the fast-paced decision-making for property creation vs. deletion
	3. Provided methodological directions and open datasets for further research

**Result:** Findings include that property creation decisions happen faster than deletions, over half of controversial discussions do not reach consensus, and the community is generally inclusive in its decision-making.

**Limitations:** The study focuses solely on Wikidata and may not generalize to other collaborative platforms.

**Conclusion:** Wikidata's discussions are mostly constructive, but many contributors withdraw from discussions before reaching consensus. The insights aim to guide improvements in community discussions and decision-making tools.

**Abstract:** In this work, we study disagreements in discussions around Wikidata, an online knowledge community that builds the data backend of Wikipedia. Discussions are essential in collaborative work as they can increase contributor performance and encourage the emergence of shared norms and practices. While disagreements can play a productive role in discussions, they can also lead to conflicts and controversies, which impact contributor' well-being and their motivation to engage. We want to understand if and when such phenomena arise in Wikidata, using a mix of quantitative and qualitative analyses to identify the types of topics people disagree about, the most common patterns of interaction, and roles people play when arguing for or against an issue. We find that decisions to create Wikidata properties are much faster than those to delete properties and that more than half of controversial discussions do not lead to consensus. Our analysis suggests that Wikidata is an inclusive community, considering different opinions when making decisions, and that conflict and vandalism are rare in discussions. At the same time, while one-fourth of the editors participating in controversial discussions contribute legitimate and insightful opinions about Wikidata's emerging issues, they respond with one or two posts and do not remain engaged in the discussions to reach consensus. Our work contributes to the analysis of collaborative KG construction with insights about communication and decision-making in projects, as well as with methodological directions and open datasets. We hope our findings will help managers and designers support community decision-making and improve discussion tools and practices.

</details>


### [18] [Hand Dominance and Congruence for Wrist-worn Haptics using Custom Voice-Coil Actuation](https://arxiv.org/abs/2308.10260)

*Ayoade Adeyemi, Umit Sen, Samet Mert Ercan, Mine Sarac*

**Main category:** cs.HC

**Keywords:** haptic feedback, virtual interactions, CoWrHap, user experience, hand dominance

**Relevance Score:** 8

**TL;DR:** This paper presents CoWrHap, a wrist-worn haptic device that provides force feedback to enhance virtual interactions, focusing on hand-wrist congruence and user experience.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the use of haptic feedback on the wrist for virtual interactions, alleviating the need for mechanical devices and investigating its impact on user experience and performance.

**Method:** A user experiment was conducted to assess stiffness discrimination tasks while varying hand dominance and the mapping between the active hand and the wrist receiving haptic feedback.

**Key Contributions:**

	1. Introduction of CoWrHap, a new wrist-worn haptic device
	2. Investigation of hand-wrist congruence in virtual interactions
	3. Insights on user experience relating to hand dominance in haptic feedback

**Result:** Participants performed tasks better with non-congruent mapping, although they reported better experiences with congruent mapping. Hand dominance did not significantly affect performance, but dominant hands led to better user experiences.

**Limitations:** 

**Conclusion:** CoWrHap allows users to perceive mechanical properties through wrist-based haptic feedback, indicating potential improvements in user experience and interaction within virtual environments.

**Abstract:** During virtual interactions, rendering haptic feedback on a remote location (like the wrist) instead of the fingertips freeing users' hands from mechanical devices. This allows for real interactions while still providing information regarding the mechanical properties of virtual objects. In this paper, we present CoWrHap -- a novel wrist-worn haptic device with custom-made voice coil actuation to render force feedback. Then, we investigate the impact of asking participants to use their dominant or non-dominant hand for virtual interactions and the best mapping between the active hand and the wrist receiving the haptic feedback, which can be defined as hand-wrist congruence through a user experiment based on a stiffness discrimination task. Our results show that participants performed the tasks (i) better with non-congruent mapping but reported better experiences with congruent mapping, and (ii) with no statistical difference in terms of hand dominance but reported better user experience and enjoyment using their dominant hands. This study indicates that participants can perceive mechanical properties via haptic feedback provided through CoWrHap.

</details>


### [19] [PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent](https://arxiv.org/abs/2309.12555)

*Donghoon Shin, Gary Hsieh, Young-Ho Kim*

**Main category:** cs.HC

**Keywords:** Large Language Models, Exercise Plans, Conversational Agents, Personalization, User Studies

**Relevance Score:** 9

**TL;DR:** This paper presents PlanFitting, an LLM-driven conversational agent for creating personalized exercise plans through user engagement and feedback.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accessible and cost-effective personalized exercise plans that typically require expert iteration.

**Method:** Development of an LLM-driven conversational agent that interacts with users to gather information and generate tailored exercise plans.

**Key Contributions:**

	1. Introduction of PlanFitting as a tool for personalized exercise planning using LLMs
	2. Demonstration of user-focused interaction to elicit information for plan refinement
	3. Evaluation of the effectiveness of the generated plans through user and expert studies.

**Result:** PlanFitting demonstrates the ability to create personalized, actionable, and evidence-based exercise plans through user engagement and various evaluations.

**Limitations:** The study may not account for all individual variations in exercise needs and capabilities.

**Conclusion:** LLM-driven agents like PlanFitting can significantly enhance the personalization of exercise plans and address individual constraints.

**Abstract:** Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users' goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study -- involving a user study, intrinsic evaluation, and expert evaluation -- demonstrated PlanFitting's ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.

</details>


### [20] [How Good is ChatGPT in Giving Advice on Your Visualization Design?](https://arxiv.org/abs/2310.09617)

*Nam Wook Kim, Yongsu Ahn, Grace Myers, Benjamin Bach*

**Main category:** cs.HC

**Keywords:** Data visualization, ChatGPT, LLM, Design feedback, User study

**Relevance Score:** 8

**TL;DR:** This study examines the effectiveness of ChatGPT as a tool for data visualization design, comparing its responses to human experts and exploring user attitudes toward its assistance.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** To address the knowledge gap in data visualization design practices due to a lack of formal training among creators.

**Method:** A mixed-methods approach was used, including quantitative comparisons of ChatGPT-generated responses with human replies in a user forum and a qualitative user study evaluating practitioners' interactions with ChatGPT.

**Key Contributions:**

	1. Quantitative comparison of ChatGPT and human responses in data visualization questions.
	2. Qualitative examination of user attitudes towards ChatGPT in design assistance.
	3. Identification of strengths and limitations in using LLMs for design feedback.

**Result:** ChatGPT demonstrated strengths in generating diverse design options quickly, but also had limitations in contextual understanding and interaction fluidity.

**Limitations:** Need for improved contextual understanding and interaction dynamics beyond a chat interface.

**Conclusion:** The study highlights both the potential and areas for improvement for LLMs in providing design feedback, suggesting future design considerations for LLM-based tools.

**Abstract:** Data visualization creators often lack formal training, resulting in a knowledge gap in design practice. Large language models such as ChatGPT, with their vast internet-scale training data, offer transformative potential to address this gap. In this study, we used both qualitative and quantitative methods to investigate how well ChatGPT can address visualization design questions. First, we quantitatively compared the ChatGPT-generated responses with anonymous online Human replies to data visualization questions on the VisGuides user forum. Next, we conducted a qualitative user study examining the reactions and attitudes of practitioners toward ChatGPT as a visualization design assistant. Participants were asked to bring their visualizations and design questions and received feedback from both Human experts and ChatGPT in randomized order. Our findings from both studies underscore ChatGPT's strengths, particularly its ability to rapidly generate diverse design options, while also highlighting areas for improvement, such as nuanced contextual understanding and fluid interaction dynamics beyond the chat interface. Drawing on these insights, we discuss design considerations for future LLM-based design feedback systems.

</details>


### [21] [Design Opportunities for Explainable AI Paraphrasing Tools: A User Study with Non-native English Speakers](https://arxiv.org/abs/2405.07475)

*Yewon Kim, Thanh-Long V. Le, Donghwi Kim, Mina Lee, Sung-Ju Lee*

**Main category:** cs.HC

**Keywords:** AI paraphrasing, non-native speakers, explainable AI, user interaction, writing efficiency

**Relevance Score:** 8

**TL;DR:** The paper explores how non-native English speakers use AI-generated paraphrase tools and identifies their preferences for information aids.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how non-native English speakers interact with AI paraphrasing tools and the factors influencing their decision-making.

**Method:** An in-lab study was conducted with 22 non-native English speakers, analyzing their interactions with an AI paraphrasing assistant, ParaScope, which includes various information aids.

**Key Contributions:**

	1. Development of ParaScope, an AI paraphrasing assistant with diverse information aids
	2. Insights into NNESs' preferences for information aids based on language proficiency
	3. Design implications for creating user-friendly AI writing systems for NNESs

**Result:** The study found that user preferences for information aids differ based on language proficiency, and that while back-translation was commonly used, users often relied on multiple aids to make informed choices.

**Limitations:** Limited sample size of 22 participants may not represent the broader NNES population.

**Conclusion:** The research highlights the value of explainable AI paraphrasing tools for enhancing non-native speakers' confidence and writing efficiency, while also suggesting design considerations to mitigate information overload.

**Abstract:** We investigate how non-native English speakers (NNESs) interact with diverse information aids to assess and select AI-generated paraphrases. We develop ParaScope, an AI paraphrasing assistant that integrates diverse information aids, such as back-translation, explanations, and usage examples, and logs user interaction data. Our in-lab study with 22 NNESs reveals that user preferences for information aids vary by language proficiency, with workflows progressing from global to more detailed information. While back-translation was the most frequently used aid, it was not a decisive factor in suggestion acceptance; users combined multiple information aids to make informed decisions. Our findings demonstrate the potential of explainable AI paraphrasing tools to enhance NNESs' confidence, autonomy, and writing efficiency, while also emphasizing the importance of thoughtful design to prevent information overload. Based on these findings, we offer design implications for explainable AI paraphrasing tools that support NNESs in making informed decisions when using AI writing systems.

</details>


### [22] [Text2VP: Generative AI for Visual Programming and Parametric Modeling](https://arxiv.org/abs/2407.07732)

*Guangxi Feng, Wei Yan*

**Main category:** cs.HC

**Keywords:** Generative AI, Visual Programming, Parametric Design, Text-to-Parametric Models, Architecture

**Relevance Score:** 4

**TL;DR:** This study presents Text-to-Visual Programming (Text2VP) GPT, a generative AI framework designed for automating the creation of parametric models through natural language instructions, facilitating easier design modifications.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance architectural design by integrating generative AI that can generate and optimize parametric design options, filling a gap in existing AI applications.

**Method:** Text2VP GPT utilizes detailed documentation, specific instructions, and example-based few-shot learning to automate graph-based visual programming workflows, enabling user-driven parameter adjustments.

**Key Contributions:**

	1. Introduction of Text2VP GPT for automating parameterized design
	2. Enhanced interaction through natural language instructions
	3. Potential reduction of training time needed for complex design modeling

**Result:** Text2VP was tested successfully for generating functional parametric models, though it faced higher error rates with more complex models.

**Limitations:** Higher complexity models demonstrate increased error rates, indicating limitations in the current framework's robustness.

**Conclusion:** This research showcases the promise of generative AI in visual programming, establishing a foundation for advancements in managing complex modeling tasks.

**Abstract:** The integration of generative artificial intelligence (AI) into architectural design has advanced significantly, enabling the generation of text, images, and 3D models. However, prior AI applications lack support for text-to-parametric models, essential for generating and optimizing diverse parametric design options. This study introduces Text-to-Visual Programming (Text2VP) GPT, a novel generative AI derived from GPT-4.1, designed to automate graph-based visual programming workflows, parameters, and their interconnections. Text2VP leverages detailed documentation, specific instructions, and example-driven few-shot learning to reflect user intentions accurately and facilitate interactive parameter adjustments. Testing demonstrates Text2VP's capability in generating functional parametric models, although higher complexity models present increased error rates. This research highlights generative AI's potential in visual programming and parametric modeling, laying groundwork for future improvements to manage complex modeling tasks. Ultimately, Text2VP aims to enable designers to easily create and modify parametric models without extensive training in specialized platforms like Grasshopper.

</details>


### [23] [SiCo: An Interactive Size-Controllable Virtual Try-On Approach for Informed Decision-Making](https://arxiv.org/abs/2408.02803)

*Sherry X. Chen, Alex Christopher Lim, Yimeng Liu, Pradeep Sen, Misha Sra*

**Main category:** cs.HC

**Keywords:** virtual try-on, e-commerce, size recommendation, user interaction, computer vision

**Relevance Score:** 7

**TL;DR:** Introducing SiCo, a virtual try-on system that enhances online shopping by enabling users to visualize garment fitting by uploading images of themselves.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current virtual try-on tools often use a one-size-fits-all approach which limits user interaction and fails to provide accurate size recommendations, contributing to high return rates in online shopping.

**Method:** SiCo allows users to upload personal images and interactively visualize how different clothing sizes would fit their bodies through a user-centric interface.

**Key Contributions:**

	1. Development of an interactive virtual try-on system (SiCo).
	2. User-centered design approach enhancing size visualization and fitting.
	3. Empirical study showing improved size selection confidence among users.

**Result:** User studies demonstrate significant improvements in users' ability to assess how outfits will look on their bodies and increased confidence in selecting the right clothing sizes.

**Limitations:** 

**Conclusion:** SiCo could substantially reduce return rates and revolutionize the online clothing shopping experience.

**Abstract:** Virtual try-on (VTO) applications aim to replicate the in-store shopping experience and enhance online shopping by enabling users to interact with garments. However, many existing tools adopt a one-size-fits-all approach when visualizing clothing items. This approach limits user interaction with garments, particularly regarding size and fit adjustments, and fails to provide direct insights for size recommendations. As a result, these limitations contribute to high return rates in online shopping. To address this, we introduce SiCo, a new online VTO system that allows users to upload images of themselves and interact with garments by visualizing how different sizes would fit their bodies. Our user study demonstrates that our approach significantly improves users' ability to assess how outfits will appear on their bodies and increases their confidence in selecting clothing sizes that align with their preferences. Based on our evaluation, we believe that SiCo has the potential to reduce return rates and transform the online clothing shopping experience.

</details>


### [24] [Simulated prosthetic vision confirms checkerboard as an effective raster pattern for epiretinal implants](https://arxiv.org/abs/2501.02084)

*Justin M. Kasowski, Apurv Varshney, Roksana Sadeghi, Michael Beyeler*

**Main category:** cs.HC

**Keywords:** retinal implants, spatial scheduling, rastering, prosthetic vision, checkerboard pattern

**Relevance Score:** 7

**TL;DR:** This study evaluates the impact of different spatial arrangements of electrode activation (raster patterns) on performance in simulated prosthetic vision, concluding that a checkerboard pattern enhances accuracy and perceived ease of use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how different raster patterns affect functional vision in high-density retinal implants and to identify optimal patterns for improved performance.

**Method:** Participants completed letter recognition and motion discrimination tasks under four raster patterns (horizontal, vertical, checkerboard, and random) using an immersive simulated prosthetic vision system, ensuring realistic representation of epiretinal implant perception.

**Key Contributions:**

	1. First quantitative evaluation of raster patterns in simulated prosthetic vision
	2. Identified checkerboard pattern as the most effective for performance and perceived ease
	3. Demonstrated structured activation reduces apparent motion artifacts

**Result:** The checkerboard raster pattern outperformed other patterns, leading to higher accuracy and lower perceived difficulty for tasks, while random patterns yielded the poorest performance.

**Limitations:** 

**Conclusion:** Checkerboard rastering enhances perceptual clarity in simulated prosthetic vision without increasing computational load, making it a viable strategy for retinal prostheses.

**Abstract:** Spatial scheduling of electrode activation ("rastering") is essential for safely operating high-density retinal implants, yet its perceptual consequences remain poorly understood. This study systematically evaluates the impact of raster patterns, or spatial arrangements of sequential electrode activation, on performance and perceived difficulty in simulated prosthetic vision (SPV). By addressing this gap, we aimed to identify patterns that optimize functional vision in retinal implants. Sighted participants completed letter recognition and motion discrimination tasks under four raster patterns (horizontal, vertical, checkerboard, and random) using an immersive SPV system. The simulations emulated epiretinal implant perception and employed psychophysically validated models of electrode activation, phosphene appearance, nonlinear spatial summation, and temporal dynamics, ensuring realistic representation of prosthetic vision. Performance accuracy and self-reported difficulty were analyzed to assess the effects of raster patterning. The checkerboard pattern consistently outperformed other raster patterns, yielding significantly higher accuracy and lower difficulty ratings across both tasks. The horizontal and vertical patterns introduced biases aligned with apparent motion artifacts, while the checkerboard minimized such effects. Random patterns resulted in the lowest performance, underscoring the importance of structured activation. Notably, checkerboard matched performance in the "No Raster" condition, despite conforming to groupwise safety constraints. This is the first quantitative, task-based evaluation of raster patterns in SPV. Checkerboard-style scheduling enhances perceptual clarity without increasing computational load, offering a low-overhead, clinically relevant strategy for improving usability in next-generation retinal prostheses.

</details>


### [25] [Design for Hope: Cultivating Deliberate Hope in the Face of Complex Societal Challenges](https://arxiv.org/abs/2503.07586)

*JaeWon Kim, Jiaying "Lizzy" Liu, Lindsay Popowski, Cassidy Pyle, Sowmya Somanath, Hua Shen, Casey Fiesler, Gillian R. Hayes, Alexis Hiniker, Wendy Ju, Florian "Floyd" Mueller, Ahmer Arif, Yasmine Kotturi*

**Main category:** cs.HC

**Keywords:** design, hope theory, collaborative design, proactive problem solving, research methodologies

**Relevance Score:** 4

**TL;DR:** This workshop focuses on using design methodologies to cultivate hope and proactive goal setting in research communities, particularly in response to complex societal issues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for alternative approaches to harm reduction and prevention in research, proposing a shift towards proactive goal setting through design.

**Method:** The workshop involves hands-on activities that include problem reframing, creating a taxonomy of design methods aligned with hope theory, and reflections on sustaining research trajectories.

**Key Contributions:**

	1. Integration of hope theory with design methodologies for research
	2. Hands-on activities promoting proactive goal setting
	3. Development of a shared taxonomy of design methods

**Result:** Participants will learn strategies to incorporate a hopeful approach into their research practices, enhancing their capacity to navigate uncertainty and generate meaningful change.

**Limitations:** 

**Conclusion:** By connecting design thinking with hope theory, researchers can open new pathways for addressing societal challenges and foster a more expansive view of what is possible.

**Abstract:** Design has the potential to cultivate hope in the face of complex societal challenges, especially those central to CSCW research. These challenges are often addressed through efforts aimed at harm reduction and prevention -- essential but sometimes limiting approaches that can unintentionally narrow our collective sense of what is possible. This one-day, in-person workshop builds on the Positech Workshop at CSCW 2024 (https://positech-cscw-2024.github.io/) by offering practical ways to move beyond reactive problem-solving toward building capacity for proactive goal setting and generating pathways forward. We explore how collaborative and reflective design methodologies can help research communities navigate uncertainty, expand possibilities, and foster meaningful change. By connecting design thinking with hope theory, which frames hope as the interplay of "goal-directed," "pathways," and "agentic" thinking, we will examine how researchers might chart new directions in the face of complexity and constraint. Through hands-on activities including problem reframing, building a shared taxonomy of design methods that align with hope theory, and reflecting on what it means to sustain hopeful research trajectories, participants will develop strategies to embed a deliberately hopeful approach into their research.

</details>


### [26] [A Scalable Approach to Clustering Embedding Projections](https://arxiv.org/abs/2504.07285)

*Donghao Ren, Fred Hohman, Dominik Moritz*

**Main category:** cs.HC

**Keywords:** clustering, kernel density estimation, data visualization, machine learning, embedding projections

**Relevance Score:** 7

**TL;DR:** The paper presents an efficient clustering method for labeling data in interactive visualizations of embedding projections using kernel density estimation, significantly reducing computation time.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient labeling techniques in interactive visualizations of embedding projections to aid in data understanding and model evaluation.

**Method:** The proposed method utilizes kernel density estimation in a projected 2D space to produce cluster regions from a density map, thus speeding up the labeling process.

**Key Contributions:**

	1. Development of an efficient clustering algorithm using kernel density estimation
	2. Performance benchmarks demonstrating speed improvements
	3. Applications showcasing the utility in labeling and summarization tasks

**Result:** The algorithm generates high-quality cluster regions within hundreds of milliseconds, significantly faster than existing clustering methods.

**Limitations:** 

**Conclusion:** This clustering approach enhances the efficiency of labeling in visualization tasks, providing a practical solution for interpretability in machine learning applications.

**Abstract:** Interactive visualization of embedding projections is a useful technique for understanding data and evaluating machine learning models. Labeling data within these visualizations is critical for interpretation, as labels provide an overview of the projection and guide user navigation. However, most methods for producing labels require clustering the points, which can be computationally expensive as the number of points grows. In this paper, we describe an efficient clustering approach using kernel density estimation in the projected 2D space instead of points. This algorithm can produce high-quality cluster regions from a 2D density map in a few hundred milliseconds, orders of magnitude faster than current approaches. We contribute the design of the algorithm, benchmarks, and applications that demonstrate the utility of the algorithm, including labeling and summarization.

</details>


### [27] [Cartographers in Cubicles: How Training and Preferences of Mapmakers Interplay with Structures and Norms in Not-for-Profit Organizations](https://arxiv.org/abs/2504.09438)

*Arpit Narechania, Alex Endert, Clio Andris*

**Main category:** cs.HC

**Keywords:** Choropleth maps, Cartography, HCI, Information visualization, CSCW

**Relevance Score:** 4

**TL;DR:** The paper explores the decision-making processes of cartographers in creating choropleth maps, highlighting their adherence to guidelines, collaborative practices, and organizational influences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the practices of cartographers in choropleth mapmaking can enhance cartographic education and provide insights for HCI and information visualization fields.

**Method:** Interviews with 16 cartographers and GIS experts from various organizations, analyzing their decision-making processes and workflows in map creation.

**Key Contributions:**

	1. Insights into the decision-making processes of cartographers
	2. Impact of organizational structures on mapmaking decisions
	3. Guidelines for better cartographic education and practice

**Result:** Identified variations and regularities in how mapmakers follow guidelines, collaborate, and make decisions related to data preparation and analysis.

**Limitations:** 

**Conclusion:** The findings can inform cartographic education and practice, emphasizing the implications for HCI and information visualization research.

**Abstract:** Choropleth maps are a common and effective way to visualize geographic thematic data. Although cartographers have established many principles about map design, data binning and color usage, less is known about how mapmakers make individual decisions in practice. We interview 16 cartographers and geographic information systems (GIS) experts from 13 government organizations, NGOs, and federal agencies about their choropleth mapmaking decisions and workflows. We categorize our findings and report on how mapmakers follow cartographic guidelines and personal rules of thumb, collaborate with other stakeholders within and outside their organization, and how organizational structures and norms are tied to decision-making during data preparation, data analysis, data binning, map styling, and map post-processing. We find several points of variation as well as regularity across mapmakers and organizations and present takeaways to inform cartographic education and practice, including broader implications and opportunities for CSCW, HCI, and information visualization researchers and practitioners.

</details>


### [28] [PlanFitting: Personalized Exercise Planning with Large Language Model-driven Conversational Agent](https://arxiv.org/abs/2309.12555)

*Donghoon Shin, Gary Hsieh, Young-Ho Kim*

**Main category:** cs.HC

**Keywords:** Large Language Models, Personalized Exercise Plans, Conversational Agents

**Relevance Score:** 9

**TL;DR:** This paper presents PlanFitting, an LLM-driven conversational agent that assists users in creating personalized weekly exercise plans through interactive dialogue.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The need for personalized exercise plans often requires expert input, which can be costly and inaccessible, motivating the exploration of LLMs for this purpose.

**Method:** The study utilizes a conversational agent, PlanFitting, to engage users in dialogue about their fitness goals, availability, and obstacles to develop tailored exercise plans.

**Key Contributions:**

	1. Introduction of PlanFitting as a novel LLM-driven assistant for exercise planning.
	2. Evidence-based evaluation of PlanFitting's effectiveness in user engagement and plan customization.
	3. Identification of design opportunities for future conversational agents in fitness.

**Result:** User studies and evaluations showed that PlanFitting effectively creates actionable, personalized exercise plans aligned with established guidelines.

**Limitations:** The paper may not address the scalability of the solution or the inclusivity of diverse user populations.

**Conclusion:** The findings suggest that LLM-driven conversational agents can significantly enhance personalized exercise planning, with future design opportunities to improve compliance with exercise principles.

**Abstract:** Creating personalized and actionable exercise plans often requires iteration with experts, which can be costly and inaccessible to many individuals. This work explores the capabilities of Large Language Models (LLMs) in addressing these challenges. We present PlanFitting, an LLM-driven conversational agent that assists users in creating and refining personalized weekly exercise plans. By engaging users in free-form conversations, PlanFitting helps elicit users' goals, availabilities, and potential obstacles, and enables individuals to generate personalized exercise plans aligned with established exercise guidelines. Our study -- involving a user study, intrinsic evaluation, and expert evaluation -- demonstrated PlanFitting's ability to guide users to create tailored, actionable, and evidence-based plans. We discuss future design opportunities for LLM-driven conversational agents to create plans that better comply with exercise principles and accommodate personal constraints.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [29] [A Data Synthesis Method Driven by Large Language Models for Proactive Mining of Implicit User Intentions in Tourism](https://arxiv.org/abs/2505.11533)

*Jinqiang Wang, Huansheng Ning, Tao Zhu, Jianguo Ding*

**Main category:** cs.CL

**Keywords:** Large Language Models, Tourism, Implicit User Intentions, Data Synthesis, Training Dataset

**Relevance Score:** 8

**TL;DR:** The paper introduces SynPT, a data synthesis method utilizing LLMs to improve the proactive mining of implicit user intentions in the tourism sector by generating a specialized training dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The tourism domain faces challenges in adequately extracting user intentions from ambiguous inquiries due to a lack of high-quality training datasets for proactive questioning.

**Method:** SynPT constructs LLM-driven user agent and assistant agent systems to simulate dialogues based on seed data from Chinese tourism websites, generating a training dataset called SynPT-Dialog for fine-tuning an LLM.

**Key Contributions:**

	1. Introduction of SynPT for proactive mining of implicit user intentions in tourism
	2. Generation of a novel training dataset, SynPT-Dialog
	3. Demonstration of the method's effectiveness through experiments and case studies

**Result:** Experimental evaluations reveal that SynPT outperforms existing methods in mining implicit user intentions from tourist inquiries, as validated from both human and LLM perspectives.

**Limitations:** 

**Conclusion:** The proposed method enhances the ability of LLMs to understand and guide users in the tourism domain, with applicability shown for English-language scenarios as well.

**Abstract:** In the tourism domain, Large Language Models (LLMs) often struggle to mine implicit user intentions from tourists' ambiguous inquiries and lack the capacity to proactively guide users toward clarifying their needs. A critical bottleneck is the scarcity of high-quality training datasets that facilitate proactive questioning and implicit intention mining. While recent advances leverage LLM-driven data synthesis to generate such datasets and transfer specialized knowledge to downstream models, existing approaches suffer from several shortcomings: (1) lack of adaptation to the tourism domain, (2) skewed distributions of detail levels in initial inquiries, (3) contextual redundancy in the implicit intention mining module, and (4) lack of explicit thinking about tourists' emotions and intention values. Therefore, we propose SynPT (A Data Synthesis Method Driven by LLMs for Proactive Mining of Implicit User Intentions in the Tourism), which constructs an LLM-driven user agent and assistant agent to simulate dialogues based on seed data collected from Chinese tourism websites. This approach addresses the aforementioned limitations and generates SynPT-Dialog, a training dataset containing explicit reasoning. The dataset is utilized to fine-tune a general LLM, enabling it to proactively mine implicit user intentions. Experimental evaluations, conducted from both human and LLM perspectives, demonstrate the superiority of SynPT compared to existing methods. Furthermore, we analyze key hyperparameters and present case studies to illustrate the practical applicability of our method, including discussions on its adaptability to English-language scenarios. All code and data are publicly available.

</details>


### [30] [AI-generated Text Detection: A Multifaceted Approach to Binary and Multiclass Classification](https://arxiv.org/abs/2505.11550)

*Harika Abburi, Sanmitra Bhattacharya, Edward Bowen, Nirmala Pudota*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI-generated text detection, neural architectures

**Relevance Score:** 9

**TL;DR:** This paper addresses the detection of AI-generated text and attribution to language models, proposing neural architectures for these tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The proliferation of Large Language Models (LLMs) poses challenges such as misuse for generating misleading content, necessitating effective detection methods.

**Method:** Two neural architectures were developed for distinguishing AI-generated text from human-written text and for attributing text to its originating model, evaluated in a shared task framework at AAAI 2025.

**Key Contributions:**

	1. Development of two neural architectures for text detection and attribution
	2. Performance benchmarking in a competitive setting (AAAI 2025)
	3. Insight into the challenges of distinguishing between human and AI-generated content

**Result:** The optimized model for distinguishing text achieved an F1 score of 0.994, ranking fifth, while the simpler model for attribution obtained an F1 score of 0.627, also placing fifth.

**Limitations:** 

**Conclusion:** The proposed models demonstrate high effectiveness in detecting AI-generated texts and identifying their sources, contributing to responsible AI deployment.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in generating text that closely resembles human writing across a wide range of styles and genres. However, such capabilities are prone to potential misuse, such as fake news generation, spam email creation, and misuse in academic assignments. As a result, accurate detection of AI-generated text and identification of the model that generated it are crucial for maintaining the responsible use of LLMs. In this work, we addressed two sub-tasks put forward by the Defactify workshop under AI-Generated Text Detection shared task at the Association for the Advancement of Artificial Intelligence (AAAI 2025): Task A involved distinguishing between human-authored or AI-generated text, while Task B focused on attributing text to its originating language model. For each task, we proposed two neural architectures: an optimized model and a simpler variant. For Task A, the optimized neural architecture achieved fifth place with $F1$ score of 0.994, and for Task B, the simpler neural architecture also ranked fifth place with $F1$ score of 0.627.

</details>


### [31] [Assessing Collective Reasoning in Multi-Agent LLMs via Hidden Profile Tasks](https://arxiv.org/abs/2505.11556)

*Yuxuan Li, Aoi Naito, Hirokazu Shirado*

**Main category:** cs.CL

**Keywords:** multi-agent systems, large language models, collective reasoning, Hidden Profile paradigm, human-AI interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces the Hidden Profile paradigm as a benchmark to evaluate collective reasoning failures in multi-agent systems built on LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate the collective reasoning failures in multi-agent systems built on LLMs and to create a theory-grounded benchmark for this purpose.

**Method:** The Hidden Profile paradigm from social psychology is formalized as a diagnostic testbed for multi-agent decision-making, with experiments conducted using various LLMs, including GPT-4.1.

**Key Contributions:**

	1. Introduction of a new benchmark for evaluating multi-agent LLM systems
	2. Experiments reveal critical insights into collective reasoning failures
	3. Demonstration of the utility of the Hidden Profile paradigm in diagnosing agent interactions

**Result:** Experiments reveal that multi-agent systems do not match the accuracy of single agents with complete information, showing performance similar to human groups but with notable behavioral differences.

**Limitations:** 

**Conclusion:** The study presents a reproducible framework for evaluating multi-agent LLM systems and highlights the trade-off between cooperation and contradiction in group dynamics.

**Abstract:** Multi-agent systems built on large language models (LLMs) promise enhanced problem-solving through distributed information integration, but also risk replicating collective reasoning failures observed in human groups. Yet, no theory-grounded benchmark exists to systematically evaluate such failures. In this paper, we introduce the Hidden Profile paradigm from social psychology as a diagnostic testbed for multi-agent LLM systems. By distributing critical information asymmetrically across agents, the paradigm reveals how inter-agent dynamics support or hinder collective reasoning. We first formalize the paradigm for multi-agent decision-making under distributed knowledge and instantiate it as a benchmark with nine tasks spanning diverse scenarios, including adaptations from prior human studies. We then conduct experiments with GPT-4.1 and five other leading LLMs, including reasoning-enhanced variants, showing that multi-agent systems across all models fail to match the accuracy of single agents given complete information. While agents' collective performance is broadly comparable to that of human groups, nuanced behavioral differences emerge, such as increased sensitivity to social desirability. Finally, we demonstrate the paradigm's diagnostic utility by exploring a cooperation-contradiction trade-off in multi-agent LLM systems. We find that while cooperative agents are prone to over-coordination in collective settings, increased contradiction impairs group convergence. This work contributes a reproducible framework for evaluating multi-agent LLM systems and motivates future research on artificial collective intelligence and human-AI interaction.

</details>


### [32] [Talk to Your Slides: Efficient Slide Editing Agent with Large Language Models](https://arxiv.org/abs/2505.11604)

*Kyudan Jung, Hojun Cho, Jooyeol Yun, Jaehyeok Jang, Jagul Choo*

**Main category:** cs.CL

**Keywords:** Large Language Models, PowerPoint Editing, Human-Computer Interaction, Instruction Understanding, Data Benchmark

**Relevance Score:** 7

**TL;DR:** Talk-to-Your-Slides is an LLM-powered agent for editing PowerPoint slides, employing a two-level approach for high-level interpretation and low-level execution, significantly improving editing tasks.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research on editing existing PowerPoint slides using large language models, as previous efforts mostly focused on slide generation.

**Method:** The system operates with a high-level LLM agent for interpreting user instructions and creating editing plans, complemented by Python scripts for direct manipulation of PowerPoint objects.

**Key Contributions:**

	1. Development of Talk-to-Your-Slides for live slide editing with LLMs
	2. Introduction of TSBench, a dataset for evaluating slide editing tasks
	3. Demonstration of improved performance over existing editing methods

**Result:** The Talk-to-Your-Slides system demonstrates superior performance over baseline methods in execution success rate, instruction fidelity, and editing efficiency, validated through a human-annotated dataset.

**Limitations:** 

**Conclusion:** This approach enables more flexible and contextually-aware editing of slides, marking a significant advancement in the interaction between LLMs and presentation software.

**Abstract:** Existing research on large language models (LLMs) for PowerPoint predominantly focuses on slide generation, overlooking the common yet tedious task of editing existing slides. We introduce Talk-to-Your-Slides, an LLM-powered agent that directly edits slides within active PowerPoint sessions through COM communication. Our system employs a two-level approach: (1) high-level processing where an LLM agent interprets instructions and formulates editing plans, and (2) low-level execution where Python scripts directly manipulate PowerPoint objects. Unlike previous methods relying on predefined operations, our approach enables more flexible and contextually-aware editing. To facilitate evaluation, we present TSBench, a human-annotated dataset of 379 diverse editing instructions with corresponding slide variations. Experimental results demonstrate that Talk-to-Your-Slides significantly outperforms baseline methods in execution success rate, instruction fidelity, and editing efficiency. Our code and benchmark are available at https://anonymous.4open.science/r/talk-to-your-slides/

</details>


### [33] [MedGUIDE: Benchmarking Clinical Decision-Making in Large Language Models](https://arxiv.org/abs/2505.11613)

*Xiaomin Li, Mingye Gao, Yuexing Hao, Taoran Li, Guangya Wan, Zihan Wang, Yijun Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, clinical guidelines, decision trees, health informatics, MedGUIDE

**Relevance Score:** 9

**TL;DR:** The study introduces MedGUIDE, a benchmark for evaluating LLMs on adherence to clinical guidelines in decision-making, revealing performance shortcomings even in domain-specific models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the ability of LLMs to follow structured clinical decision-making protocols based on existing medical guidelines.

**Method:** The researchers constructed MedGUIDE from NCCN decision trees, creating multiple-choice diagnostic questions from LLM-generated scenarios and applied a selection process for high-quality samples.

**Key Contributions:**

	1. Introduction of the MedGUIDE benchmark for LLM evaluation in clinical decision-making
	2. Creation of a large dataset of diagnostic questions based on curated decision trees
	3. Findings indicate a performance gap in LLMs regarding guideline adherence

**Result:** 25 LLMs were evaluated, showing that even specialized models underperform in adhering to structured guidelines; there was no significant improvement with in-context guideline inclusion or pretraining.

**Limitations:** The current LLMs evaluated show performance limitations, suggesting potential areas for improvement in model training or architecture.

**Conclusion:** MedGUIDE highlights the need for assessing LLMs' effectiveness in real-world clinical applications, emphasizing the importance of guideline adherence.

**Abstract:** Clinical guidelines, typically structured as decision trees, are central to evidence-based medical practice and critical for ensuring safe and accurate diagnostic decision-making. However, it remains unclear whether Large Language Models (LLMs) can reliably follow such structured protocols. In this work, we introduce MedGUIDE, a new benchmark for evaluating LLMs on their ability to make guideline-consistent clinical decisions. MedGUIDE is constructed from 55 curated NCCN decision trees across 17 cancer types and uses clinical scenarios generated by LLMs to create a large pool of multiple-choice diagnostic questions. We apply a two-stage quality selection process, combining expert-labeled reward models and LLM-as-a-judge ensembles across ten clinical and linguistic criteria, to select 7,747 high-quality samples. We evaluate 25 LLMs spanning general-purpose, open-source, and medically specialized models, and find that even domain-specific LLMs often underperform on tasks requiring structured guideline adherence. We also test whether performance can be improved via in-context guideline inclusion or continued pretraining. Our findings underscore the importance of MedGUIDE in assessing whether LLMs can operate safely within the procedural frameworks expected in real-world clinical settings.

</details>


### [34] [Steering Risk Preferences in Large Language Models by Aligning Behavioral and Neural Representations](https://arxiv.org/abs/2505.11615)

*Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths*

**Main category:** cs.CL

**Keywords:** large language models, steering vectors, behavioral methods, latent representations, risk preferences

**Relevance Score:** 8

**TL;DR:** The paper proposes a method to influence large language models (LLMs) behavior through identified steering vectors, enabling modifications without retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To effectively change LLM behavior using specific modifications to their internal activations, avoiding the need for retraining or fine-tuning.

**Method:** The authors align latent representations obtained through behavioral methods (MCMC with LLMs) with neural counterparts to identify steering vectors.

**Key Contributions:**

	1. Introduction of a method to systematically identify steering vectors for LLMs
	2. Demonstration of effective risk preference modulation in LLM outputs
	3. Avoidance of fine-tuning or retraining through representational engineering

**Result:** The steering vectors reliably modulate LLM outputs in accordance with targeted behaviors, specifically for risk-related outputs.

**Limitations:** 

**Conclusion:** This approach provides a systematic way to influence LLM outputs effectively and can be applied to various behavioral adjustments in LLMs.

**Abstract:** Changing the behavior of large language models (LLMs) can be as straightforward as editing the Transformer's residual streams using appropriately constructed "steering vectors." These modifications to internal neural activations, a form of representation engineering, offer an effective and targeted means of influencing model behavior without retraining or fine-tuning the model. But how can such steering vectors be systematically identified? We propose a principled approach for uncovering steering vectors by aligning latent representations elicited through behavioral methods (specifically, Markov chain Monte Carlo with LLMs) with their neural counterparts. To evaluate this approach, we focus on extracting latent risk preferences from LLMs and steering their risk-related outputs using the aligned representations as steering vectors. We show that the resulting steering vectors successfully and reliably modulate LLM outputs in line with the targeted behavior.

</details>


### [35] [THELMA: Task Based Holistic Evaluation of Large Language Model Applications-RAG Question Answering](https://arxiv.org/abs/2505.11626)

*Udita Patel, Rutu Mulkar, Jay Roberts, Cibi Chakravarthy Senthilkumar, Sujay Gandhi, Xiaofei Zheng, Naumaan Nayyar, Rafael Castrillo*

**Main category:** cs.CL

**Keywords:** RAG, question answering, evaluation metrics

**Relevance Score:** 9

**TL;DR:** Introducing THELMA, a framework for evaluating RAG-based question answering applications without the need for reference responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a systematic evaluation approach for RAG QA applications that does not rely on labeled data.

**Method:** THELMA consists of six interdependent metrics designed for a holistic and fine-grained evaluation of RAG question answering applications.

**Key Contributions:**

	1. Development of THELMA framework for RAG QA evaluation
	2. Introduction of six interdependent metrics
	3. Insights on the interplay of metrics for targeted improvements

**Result:** Findings reveal the interactions among the THELMA metrics, assisting in pinpointing specific components of RAG that require enhancement in QA applications.

**Limitations:** 

**Conclusion:** THELMA facilitates the evaluation, monitoring, and improvement of RAG QA pipelines effectively without labeled sources.

**Abstract:** We propose THELMA (Task Based Holistic Evaluation of Large Language Model Applications), a reference free framework for RAG (Retrieval Augmented generation) based question answering (QA) applications. THELMA consist of six interdependent metrics specifically designed for holistic, fine grained evaluation of RAG QA applications. THELMA framework helps developers and application owners evaluate, monitor and improve end to end RAG QA pipelines without requiring labelled sources or reference responses.We also present our findings on the interplay of the proposed THELMA metrics, which can be interpreted to identify the specific RAG component needing improvement in QA applications.

</details>


### [36] [Critique-Guided Distillation: Improving Supervised Fine-tuning via Better Distillation](https://arxiv.org/abs/2505.11628)

*Berkcan Kapusuzoglu, Supriyo Chakraborty, Chia-Hsuan Lee, Sambit Sahu*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Critique-Guided Distillation, Machine Learning, Model Understanding, Benchmark Evaluation

**Relevance Score:** 8

**TL;DR:** This paper introduces Critique-Guided Distillation (CGD), a framework that enhances Supervised Fine-Tuning (SFT) by incorporating teacher critiques and refined responses to improve understanding in machine learning models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The limitation of traditional supervised fine-tuning methods that lead to imitative responses without understanding is addressed.

**Method:** The proposed CGD framework involves a multi-stage process where a student model learns from teacher model-generated critiques and refined responses, mapping input prompts and critiques to improved outputs.

**Key Contributions:**

	1. Introduction of Critique-Guided Distillation (CGD)
	2. Demonstration of significant performance improvements on benchmark tasks
	3. Reduction of format drift issues in critique fine-tuning

**Result:** Empirical evaluations demonstrate that CGD leads to significant improvements in performance on math and language understanding benchmarks, with AMC23 showing a +17.5% increase and MMLU-Pro +6.3%.

**Limitations:** 

**Conclusion:** CGD effectively reduces refinement uncertainty and interprets critiques within a Bayesian framework, providing a method to overcome pitfalls of previous critique-based fine-tuning methods.

**Abstract:** Supervised fine-tuning (SFT) using expert demonstrations often suffer from the imitation problem, where the model learns to reproduce the correct responses without \emph{understanding} the underlying rationale. To address this limitation, we propose \textsc{Critique-Guided Distillation (CGD)}, a novel multi-stage framework that integrates teacher model generated \emph{explanatory critiques} and \emph{refined responses} into the SFT process. A student model is then trained to map the triplet of prompt, teacher critique, and its own initial response to the corresponding refined teacher response, thereby learning both \emph{what} to imitate and \emph{why}. Using entropy-based analysis, we show that \textsc{CGD} reduces refinement uncertainty and can be interpreted as a Bayesian posterior update. We perform extensive empirical evaluation of \textsc{CGD}, on variety of benchmark tasks, and demonstrate significant gains on both math (AMC23 +17.5%) and language understanding tasks (MMLU-Pro +6.3%), while successfully mitigating the format drift issues observed in previous critique fine-tuning (CFT) techniques.

</details>


### [37] [Can an Easy-to-Hard Curriculum Make Reasoning Emerge in Small Language Models? Evidence from a Four-Stage Curriculum on GPT-2](https://arxiv.org/abs/2505.11643)

*Xiang Fu*

**Main category:** cs.CL

**Keywords:** small language models, curriculum learning, reasoning transparency, sample efficiency, Cognivolve

**Relevance Score:** 7

**TL;DR:** Developmentally ordered curriculum enhances reasoning transparency and sample efficiency in small language models without task-specific fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning capabilities and efficiency in small language models (SLMs) through a structured training approach.

**Method:** Cognivolve, a 124 M-parameter GPT-2 model, is trained on a four-stage curriculum from lexical matching to multi-step symbolic inference.

**Key Contributions:**

	1. Developmentally ordered curriculum significantly enhances reasoning in SLMs.
	2. Cognivolve model demonstrates improved efficiency and reasoning capabilities without fine-tuning.
	3. Identifies gaps in final-answer success and saliency detection for further research.

**Result:** Cognivolve achieves target accuracy in half the optimization steps of a baseline model and improves gradient saliency and attention methods.

**Limitations:** Final-answer success lags behind conventional models by about 30%, and current saliency probes may under-detect certain reasoning capabilities.

**Conclusion:** The results indicate that curriculum progression is crucial for performance improvement, despite ongoing challenges in achieving conventional accuracy levels.

**Abstract:** We demonstrate that a developmentally ordered curriculum markedly improves reasoning transparency and sample-efficiency in small language models (SLMs). Concretely, we train Cognivolve, a 124 M-parameter GPT-2 model, on a four-stage syllabus that ascends from lexical matching to multi-step symbolic inference and then evaluate it without any task-specific fine-tuning. Cognivolve reaches target accuracy in half the optimization steps of a single-phase baseline, activates an order-of-magnitude more gradient-salient reasoning heads, and shifts those heads toward deeper layers, yielding higher-entropy attention that balances local and long-range context. The same curriculum applied out of order or with optimizer resets fails to reproduce these gains, confirming that progression--not extra compute--drives the effect. We also identify open challenges: final-answer success still lags a conventional run by about 30%, and our saliency probe under-detects verbal-knowledge heads in the hardest stage, suggesting directions for mixed-stage fine-tuning and probe expansion.

</details>


### [38] [Multilingual Prompt Engineering in Large Language Models: A Survey Across NLP Tasks](https://arxiv.org/abs/2505.11665)

*Shubham Vatsal, Harsh Dubey, Aditi Singh*

**Main category:** cs.CL

**Keywords:** multilingual prompting, large language models, natural language processing, NLP tasks, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper surveys multilingual prompt engineering techniques to enhance LLM performance across 250 languages, categorizing 39 techniques applied to 30 NLP tasks based on various resources.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of ensuring effective LLM performance across multiple languages and to make these capabilities accessible to a broader audience.

**Method:** The paper categorizes various multilingual prompting techniques and analyzes their efficacy across diverse datasets spanning around 250 languages, based on existing research.

**Key Contributions:**

	1. Survey of multilingual prompting techniques for LLMs
	2. Categorization of 39 techniques across 30 NLP tasks
	3. Insights into the trends in language resource distribution and prompting methods.

**Result:** The survey reviews 36 papers detailing different prompting techniques and their applications to NLP tasks, highlighting trends across language families and resource levels.

**Limitations:** The discussion is grounded in existing research, which may not capture the latest advancements beyond the reviewed papers.

**Conclusion:** The study provides insights into multilingual prompting strategies and discusses potential state-of-the-art methods for improving LLM performance across low-resource and high-resource languages.

**Abstract:** Large language models (LLMs) have demonstrated impressive performance across a wide range of Natural Language Processing (NLP) tasks. However, ensuring their effectiveness across multiple languages presents unique challenges. Multilingual prompt engineering has emerged as a key approach to enhance LLMs' capabilities in diverse linguistic settings without requiring extensive parameter re-training or fine-tuning. With growing interest in multilingual prompt engineering over the past two to three years, researchers have explored various strategies to improve LLMs' performance across languages and NLP tasks. By crafting structured natural language prompts, researchers have successfully extracted knowledge from LLMs across different languages, making these techniques an accessible pathway for a broader audience, including those without deep expertise in machine learning, to harness the capabilities of LLMs. In this paper, we survey and categorize different multilingual prompting techniques based on the NLP tasks they address across a diverse set of datasets that collectively span around 250 languages. We further highlight the LLMs employed, present a taxonomy of approaches and discuss potential state-of-the-art (SoTA) methods for specific multilingual datasets. Additionally, we derive a range of insights across language families and resource levels (high-resource vs. low-resource), including analyses such as the distribution of NLP tasks by language resource type and the frequency of prompting methods across different language families. Our survey reviews 36 research papers covering 39 prompting techniques applied to 30 multilingual NLP tasks, with the majority of these studies published in the last two years.

</details>


### [39] [Ambiguity Resolution in Text-to-Structured Data Mapping](https://arxiv.org/abs/2505.11679)

*Zhibo Hu, Chen Wang, Yanfeng Shu, Hye-Young Paik, Liming Zhu*

**Main category:** cs.CL

**Keywords:** ambiguous text, large language models, latent space, sparse-autoencoder, tool calling

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenge of ambiguity in natural language for improved text to structured data mapping using a new framework based on latent space representation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Ambiguity in natural language hinders accurate mapping from text to structured data, affecting various applications such as agentic tool calling and text-to-SQL tasks.

**Method:** The authors characterize the representation differences of ambiguous text in the latent space of large language models (LLMs) and propose a new measurement of distance that accounts for missing concepts in this latent space, using a sparse-autoencoder approach.

**Key Contributions:**

	1. New distance measurement for detecting ambiguity in latent space
	2. Framework to improve LLM performance on ambiguous queries
	3. Identification of missing concepts as a cause of ambiguity

**Result:** The proposed framework improves the detection of ambiguity in agentic tool calling, thereby enhancing the performance of LLMs in handling ambiguous inputs.

**Limitations:** 

**Conclusion:** By focusing on the relationship between ambiguous questions and their interpretations, the framework offers a systematic way to identify and address ambiguity in natural language processing tasks.

**Abstract:** Ambiguity in natural language is a significant obstacle for achieving accurate text to structured data mapping through large language models (LLMs), which affects the performance of tasks such as mapping text to agentic tool calling and text-to-SQL queries. Existing methods of ambiguity handling either exploit ReACT framework to produce the correct mapping through trial and error, or supervised fine tuning to guide models to produce a biased mapping to improve certain tasks. In this paper, we adopt a different approach that characterizes the representation difference of ambiguous text in the latent space and leverage the difference to identify ambiguity before mapping them to structured data. To detect ambiguity of a sentence, we focused on the relationship between ambiguous questions and their interpretations and what cause the LLM ignore multiple interpretations. Different to the distance calculated by dense embedding vectors, we utilize the observation that ambiguity is caused by concept missing in latent space of LLM to design a new distance measurement, computed through the path kernel by the integral of gradient values for each concepts from sparse-autoencoder (SAE) under each state. We identify patterns to distinguish ambiguous questions with this measurement. Based on our observation, We propose a new framework to improve the performance of LLMs on ambiguous agentic tool calling through missing concepts prediction.

</details>


### [40] [Evaluating Design Decisions for Dual Encoder-based Entity Disambiguation](https://arxiv.org/abs/2505.11683)

*Susanna Rücker, Alan Akbik*

**Main category:** cs.CL

**Keywords:** Entity Disambiguation, Dual Encoders, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper presents VerbalizED, a document-level Dual Encoder model for entity disambiguation, evaluating key design choices and achieving new state-of-the-art results.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve entity disambiguation by evaluating and optimizing design decisions in Dual Encoder systems.

**Method:** The model uses contextual label verbalizations and efficient hard negative sampling in a Dual Encoder framework to enhance predictions.

**Key Contributions:**

	1. Introduction of VerbalizED model for entity disambiguation
	2. Evaluation of design choices affecting Dual Encoder effectiveness
	3. Demonstrated new state-of-the-art results on the ZELDA benchmark

**Result:** Achieved new state-of-the-art performance on the ZELDA benchmark through comprehensive experiments on AIDA-Yago.

**Limitations:** 

**Conclusion:** The research offers valuable insights into design choices that significantly influence entity disambiguation performance.

**Abstract:** Entity disambiguation (ED) is the task of linking mentions in text to corresponding entries in a knowledge base. Dual Encoders address this by embedding mentions and label candidates in a shared embedding space and applying a similarity metric to predict the correct label. In this work, we focus on evaluating key design decisions for Dual Encoder-based ED, such as its loss function, similarity metric, label verbalization format, and negative sampling strategy. We present the resulting model VerbalizED, a document-level Dual Encoder model that includes contextual label verbalizations and efficient hard negative sampling. Additionally, we explore an iterative prediction variant that aims to improve the disambiguation of challenging data points. Comprehensive experiments on AIDA-Yago validate the effectiveness of our approach, offering insights into impactful design choices that result in a new State-of-the-Art system on the ZELDA benchmark.

</details>


### [41] [Automatic Speech Recognition for African Low-Resource Languages: Challenges and Future Directions](https://arxiv.org/abs/2505.11690)

*Sukairaj Hafiz Imam, Babangida Sani, Dawit Ketema Gete, Bedru Yimam Ahamed, Ibrahim Said Ahmad, Idris Abdulmumin, Seid Muhie Yimam, Muhammad Yahuza Bello, Shamsuddeen Hassan Muhammad*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, low-resource languages, ethical AI, community-driven data, multilingual learning

**Relevance Score:** 6

**TL;DR:** This study explores the challenges and strategies for developing Automatic Speech Recognition (ASR) systems for low-resource African languages, emphasizing ethical and inclusive approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of low-resource languages in ASR technologies and improve digital interaction for speakers in Africa.

**Method:** Critical analysis of challenges faced by ASR development, along with case studies and strategies like community-driven data collection and multilingual learning.

**Key Contributions:**

	1. Identification of key challenges in ASR for African languages
	2. Proposed practical strategies for inclusive ASR development
	3. Evidence from successful pilot projects demonstrating impact

**Result:** Identified barriers include data scarcity and linguistic complexity; suggested strategies demonstrate feasibility with pilot projects in healthcare and education sectors.

**Limitations:** 

**Conclusion:** Interdisciplinary collaboration and investment are essential to create ethical ASR systems that enhance accessibility and socioeconomic opportunities for African language speakers.

**Abstract:** Automatic Speech Recognition (ASR) technologies have transformed human-computer interaction; however, low-resource languages in Africa remain significantly underrepresented in both research and practical applications. This study investigates the major challenges hindering the development of ASR systems for these languages, which include data scarcity, linguistic complexity, limited computational resources, acoustic variability, and ethical concerns surrounding bias and privacy. The primary goal is to critically analyze these barriers and identify practical, inclusive strategies to advance ASR technologies within the African context. Recent advances and case studies emphasize promising strategies such as community-driven data collection, self-supervised and multilingual learning, lightweight model architectures, and techniques that prioritize privacy. Evidence from pilot projects involving various African languages showcases the feasibility and impact of customized solutions, which encompass morpheme-based modeling and domain-specific ASR applications in sectors like healthcare and education. The findings highlight the importance of interdisciplinary collaboration and sustained investment to tackle the distinct linguistic and infrastructural challenges faced by the continent. This study offers a progressive roadmap for creating ethical, efficient, and inclusive ASR systems that not only safeguard linguistic diversity but also improve digital accessibility and promote socioeconomic participation for speakers of African languages.

</details>


### [42] [Hierarchical Bracketing Encodings for Dependency Parsing as Tagging](https://arxiv.org/abs/2505.11693)

*Ana Ezquerro, David Vilares, Anssi Yli-Jyrä, Carlos Gómez-Rodríguez*

**Main category:** cs.CL

**Keywords:** dependency parsing, sequence labeling, hierarchical bracketing

**Relevance Score:** 5

**TL;DR:** The paper introduces a new family of encodings for sequence labeling dependency parsing, aiming for optimality in label efficiency and supporting arbitrary non-projectivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for more efficient encodings in dependency parsing that can minimize the number of symbols used while maintaining competitive parsing accuracy.

**Method:** The authors develop a hierarchical bracketing approach, extending it from existing encodings to ensure it uses fewer labels, deriving an optimal form for projective trees and non-projective structures.

**Key Contributions:**

	1. Introduction of a family of encodings based on hierarchical bracketing for dependency parsing.
	2. Derivation of an optimal encoding that reduces the number of labels for projective trees.
	3. Extension of hierarchical bracketing to support arbitrary non-projectivity in a compact manner.

**Result:** The new encodings use only 12 distinct labels for projective trees, improving upon the 16 used in the previous 4-bit encoding, while achieving competitive accuracy across multiple treebanks.

**Limitations:** 

**Conclusion:** This work enhances the efficiency of dependency parsing encodings, presenting a viable alternative to current methods with its optimization for fewer labels.

**Abstract:** We present a family of encodings for sequence labeling dependency parsing, based on the concept of hierarchical bracketing. We prove that the existing 4-bit projective encoding belongs to this family, but it is suboptimal in the number of labels used to encode a tree. We derive an optimal hierarchical bracketing, which minimizes the number of symbols used and encodes projective trees using only 12 distinct labels (vs. 16 for the 4-bit encoding). We also extend optimal hierarchical bracketing to support arbitrary non-projectivity in a more compact way than previous encodings. Our new encodings yield competitive accuracy on a diverse set of treebanks.

</details>


### [43] [Disambiguating Reference in Visually Grounded Dialogues through Joint Modeling of Textual and Multimodal Semantic Structures](https://arxiv.org/abs/2505.11726)

*Shun Inadumi, Nobuhiro Ueda, Koichiro Yoshino*

**Main category:** cs.CL

**Keywords:** Multimodal reference resolution, Coreference resolution, Phrase grounding

**Relevance Score:** 8

**TL;DR:** This paper presents a unified framework for multimodal and textual reference resolution, enhancing understanding in visually grounded dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reference resolution in dialogue systems by addressing ambiguities caused by pronouns and ellipses in both textual and multimodal contexts.

**Method:** A framework that integrates textual and multimodal reference resolution by mapping mention embeddings to object embeddings, emphasizing similarity-based selection.

**Key Contributions:**

	1. Unified framework for textual and multimodal reference resolution
	2. Improved performance in grounding pronouns
	3. Integration of coreference resolution to enhance confidence scores

**Result:** The proposed model outperforms existing models like MDETR and GLIP in pronoun phrase grounding by incorporating coreference resolution techniques, leading to improved confidence in relations between mentions and objects.

**Limitations:** 

**Conclusion:** Incorporating textual reference relations into multimodal systems enhances performance and reduces ambiguities in dialogue, with promising results from qualitative analysis.

**Abstract:** Multimodal reference resolution, including phrase grounding, aims to understand the semantic relations between mentions and real-world objects. Phrase grounding between images and their captions is a well-established task. In contrast, for real-world applications, it is essential to integrate textual and multimodal reference resolution to unravel the reference relations within dialogue, especially in handling ambiguities caused by pronouns and ellipses. This paper presents a framework that unifies textual and multimodal reference resolution by mapping mention embeddings to object embeddings and selecting mentions or objects based on their similarity. Our experiments show that learning textual reference resolution, such as coreference resolution and predicate-argument structure analysis, positively affects performance in multimodal reference resolution. In particular, our model with coreference resolution performs better in pronoun phrase grounding than representative models for this task, MDETR and GLIP. Our qualitative analysis demonstrates that incorporating textual reference relations strengthens the confidence scores between mentions, including pronouns and predicates, and objects, which can reduce the ambiguities that arise in visually grounded dialogues.

</details>


### [44] [MedCaseReasoning: Evaluating and learning diagnostic reasoning from clinical case reports](https://arxiv.org/abs/2505.11733)

*Kevin Wu, Eric Wu, Rahul Thapa, Kevin Wei, Angela Zhang, Arvind Suresh, Jacqueline J. Tao, Min Woo Sun, Alejandro Lozano, James Zou*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical diagnosis, dataset, clinical reasoning, fine-tuning

**Relevance Score:** 10

**TL;DR:** Introduction of MedCaseReasoning, an open-access dataset to evaluate LLMs on clinical diagnostic reasoning accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing medical benchmarks focus only on final answer accuracy, neglecting the reasoning process critical for medical diagnoses.

**Method:** Creation of MedCaseReasoning dataset containing 14,489 diagnostic Q&A cases paired with clinician-authored reasoning statements, followed by evaluations of reasoning LLMs on this dataset.

**Key Contributions:**

	1. First open-access dataset for evaluating LLMs in medical reasoning
	2. Demonstrated shortcomings of current LLMs in medical diagnostics
	3. Showed improvements in diagnostic accuracy and reasoning recall through fine-tuning

**Result:** The top-performing LLM achieved only 48% diagnostic accuracy and 64% recall of clinician reasoning. Fine-tuning on reasoning traces improved diagnostic accuracy by 29% and recall by 41%.

**Limitations:** 

**Conclusion:** Fine-tuning LLMs on realistic reasoning data significantly enhances their clinical diagnostic capabilities.

**Abstract:** Doctors and patients alike increasingly use Large Language Models (LLMs) to diagnose clinical cases. However, unlike domains such as math or coding, where correctness can be objectively defined by the final answer, medical diagnosis requires both the outcome and the reasoning process to be accurate. Currently, widely used medical benchmarks like MedQA and MMLU assess only accuracy in the final answer, overlooking the quality and faithfulness of the clinical reasoning process. To address this limitation, we introduce MedCaseReasoning, the first open-access dataset for evaluating LLMs on their ability to align with clinician-authored diagnostic reasoning. The dataset includes 14,489 diagnostic question-and-answer cases, each paired with detailed reasoning statements derived from open-access medical case reports. We evaluate state-of-the-art reasoning LLMs on MedCaseReasoning and find significant shortcomings in their diagnoses and reasoning: for instance, the top-performing open-source model, DeepSeek-R1, achieves only 48% 10-shot diagnostic accuracy and mentions only 64% of the clinician reasoning statements (recall). However, we demonstrate that fine-tuning LLMs on the reasoning traces derived from MedCaseReasoning significantly improves diagnostic accuracy and clinical reasoning recall by an average relative gain of 29% and 41%, respectively. The open-source dataset, code, and models are available at https://github.com/kevinwu23/Stanford-MedCaseReasoning.

</details>


### [45] [ZeroTuning: Unlocking the Initial Token's Power to Enhance Large Language Models Without Training](https://arxiv.org/abs/2505.11739)

*Feijiang Han, Xiaodong Yu, Jianheng Tang, Lyle Ungar*

**Main category:** cs.CL

**Keywords:** large language models, attention tuning, ZeroTuning, model interpretability, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces ZeroTuning, a training-free method that optimizes large language model (LLM) performance by tuning an initial token's attention, outperforming traditional task-specific token tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing token-level attention tuning methods in large language models, which often rely on auxiliary mechanisms and introduce biases.

**Method:** The paper proposes ZeroTuning, which involves tuning the attention of a semantically empty initial token to improve LLM performance without the need for training on task-specific tokens.

**Key Contributions:**

	1. Introduces ZeroTuning, a novel training-free attention tuning approach for LLMs.
	2. Demonstrates that tuning the initial token is more effective than tuning other tokens.
	3. Provides insights into the impact of attention heads and layers on model performance.

**Result:** ZeroTuning achieves significant improvements in performance across text classification, multiple-choice, and multi-turn conversation tasks. For instance, it improved Llama-3.1-8B by 11.71% on classification tasks.

**Limitations:** 

**Conclusion:** Tuning the attention of the initial token provides a powerful means to enhance the performance of large language models while being robust across various conditions.

**Abstract:** Recently, training-free methods for improving large language models (LLMs) have attracted growing interest, with token-level attention tuning emerging as a promising and interpretable direction. However, existing methods typically rely on auxiliary mechanisms to identify important or irrelevant task-specific tokens, introducing potential bias and limiting applicability. In this paper, we uncover a surprising and elegant alternative: the semantically empty initial token is a powerful and underexplored control point for optimizing model behavior. Through theoretical analysis, we show that tuning the initial token's attention sharpens or flattens the attention distribution over subsequent tokens, and its role as an attention sink amplifies this effect. Empirically, we find that: (1) tuning its attention improves LLM performance more effectively than tuning other task-specific tokens; (2) the effect follows a consistent trend across layers, with earlier layers having greater impact, but varies across attention heads, with different heads showing distinct preferences in how they attend to this token. Based on these findings, we propose ZeroTuning, a training-free approach that improves LLM performance by applying head-specific attention adjustments to this special token. Despite tuning only one token, ZeroTuning achieves higher performance on text classification, multiple-choice, and multi-turn conversation tasks across models such as Llama, Qwen, and DeepSeek. For example, ZeroTuning improves Llama-3.1-8B by 11.71% on classification, 2.64% on QA tasks, and raises its multi-turn score from 7.804 to 7.966. The method is also robust to limited resources, few-shot settings, long contexts, quantization, decoding strategies, and prompt variations. Our work sheds light on a previously overlooked control point in LLMs, offering new insights into both inference-time tuning and model interpretability.

</details>


### [46] [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)

*Manari Hirose, Masato Uchida*

**Main category:** cs.CL

**Keywords:** Large Language Models, ideological biases, quantitative analysis, ethical AI, ChatGPT

**Relevance Score:** 9

**TL;DR:** This study presents a framework for evaluating the ideological biases of Large Language Models (LLMs) via quantitative analysis, revealing differences in opinions and problematic biases in ChatGPT and Gemini.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for empirical research on the biases and societal implications of the widespread integration of LLMs.

**Method:** The study introduces a novel framework for evaluating LLMs by analyzing 436 binary-choice questions, focusing on ideological biases across different models and languages.

**Key Contributions:**

	1. Development of a novel framework for LLM evaluation
	2. Empirical findings on ideological biases of ChatGPT and Gemini
	3. Identification of problematic biases and their societal implications

**Result:** Findings indicate that while LLMs maintain consistent opinions, ideologies differ across models and languages; notably, ChatGPT tends to align its opinions with that of the questioner, with both models displaying problematic biases.

**Limitations:** Results are specific to the evaluated models and may not generalize to all LLMs; the binary-choice questions might not capture the full complexity of underlying ideas.

**Conclusion:** The results highlight the necessity for ethical considerations in LLM evaluations and the framework provides a flexible quantitative method for assessing LLM behavior, fostering the development of socially aligned AI systems.

**Abstract:** The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.

</details>


### [47] [Token Masking Improves Transformer-Based Text Classification](https://arxiv.org/abs/2505.11746)

*Xianglong Xu, John Bowen, Rojin Taheri*

**Main category:** cs.CL

**Keywords:** transformer models, token masking, text classification, regularization, overfitting

**Relevance Score:** 7

**TL;DR:** This paper introduces token masking regularization to enhance transformer-based models in text classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of masking input tokens in transformer-based models for improved performance.

**Method:** The proposed method randomly replaces input tokens with a [MASK] token at a set probability during training to create stochastic perturbations.

**Key Contributions:**

	1. Introduction of token masking regularization for transformers
	2. Empirical validation of the method across multiple tasks and models
	3. Identification of optimal masking rates for specific tasks

**Result:** Experiments demonstrate consistent performance improvements across language identification and sentiment analysis tasks with various models compared to standard regularization techniques.

**Limitations:** 

**Conclusion:** Task-specific optimal masking rates were identified, with a general default of p = 0.1 showing strong efficacy, attributed to reduced overfitting and implicit gradient smoothing.

**Abstract:** While transformer-based models achieve strong performance on text classification, we explore whether masking input tokens can further enhance their effectiveness. We propose token masking regularization, a simple yet theoretically motivated method that randomly replaces input tokens with a special [MASK] token at probability p. This introduces stochastic perturbations during training, leading to implicit gradient averaging that encourages the model to capture deeper inter-token dependencies. Experiments on language identification and sentiment analysis -- across diverse models (mBERT, Qwen2.5-0.5B, TinyLlama-1.1B) -- show consistent improvements over standard regularization techniques. We identify task-specific optimal masking rates, with p = 0.1 as a strong general default. We attribute the gains to two key effects: (1) input perturbation reduces overfitting, and (2) gradient-level smoothing acts as implicit ensembling.

</details>


### [48] [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)

*Axel Abels, Tom Lenaerts*

**Main category:** cs.CL

**Keywords:** bias, large language models, crowd-based strategies, aggregation methods, hybrid crowds

**Relevance Score:** 9

**TL;DR:** This paper investigates bias in large language models (LLMs) and explores crowd-based strategies for reducing this bias through response aggregation, highlighting the effectiveness of locally weighted aggregation and hybrid approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of bias perpetuated by large language models in their responses, which mirrors human biases.

**Method:** The authors analyze LLM responses to bias-eliciting headlines and explore different aggregation methods, including simple averaging and locally weighted aggregation, to mitigate bias.

**Key Contributions:**

	1. Demonstration of how averaging LLM responses can exacerbate biases due to limited diversity.
	2. Development of locally weighted aggregation methods that significantly improve bias mitigation and accuracy.
	3. Evidence that hybrid crowds of LLMs and humans are more effective in reducing biases across diverse contexts.

**Result:** Locally weighted aggregation methods more effectively reduce bias and improve accuracy compared to simple averaging, and hybrid crowds combining LLMs with humans yield further improvements.

**Limitations:** 

**Conclusion:** Hybrid crowds enhance performance and reduce biases, combining the strengths of LLMs and human diversity in bias mitigation.

**Abstract:** Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.

</details>


### [49] [Masking in Multi-hop QA: An Analysis of How Language Models Perform with Context Permutation](https://arxiv.org/abs/2505.11754)

*Wenyu Huang, Pavlos Vougiouklis, Mirella Lapata, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Multi-hop Question Answering, Language Models, Attention Mechanisms, Flan-T5, Causal Mask

**Relevance Score:** 8

**TL;DR:** This paper investigates multi-hop question answering (MHQA) with language models, showing that encoder-decoder models outperform causal decoder-only models, and proposes methods to enhance performance through attention mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenges of multi-hop question answering, where language models need to retrieve and reason with multiple sources of information.

**Method:** We permuted search results provided to language models and analyzed their performance, comparing encoder-decoder models with causal decoder-only models in various configurations.

**Key Contributions:**

	1. Demonstrates the superiority of encoder-decoder models in MHQA tasks.
	2. Finds optimal document ordering improves model reasoning performance.
	3. Proposes enhancements for causal decoder models using bi-directional attention.

**Result:** Encoder-decoder models like Flan-T5 outperform causal decoder-only models despite being smaller, and the order of documents significantly impacts reasoning performance.

**Limitations:** The study focuses primarily on specific model configurations and may not generalize across all language model architectures.

**Conclusion:** By modifying causal masks and analyzing attention weights, we can improve language model performance in multi-hop reasoning tasks.

**Abstract:** Multi-hop Question Answering (MHQA) adds layers of complexity to question answering, making it more challenging. When Language Models (LMs) are prompted with multiple search results, they are tasked not only with retrieving relevant information but also employing multi-hop reasoning across the information sources. Although LMs perform well on traditional question-answering tasks, the causal mask can hinder their capacity to reason across complex contexts. In this paper, we explore how LMs respond to multi-hop questions by permuting search results (retrieved documents) under various configurations. Our study reveals interesting findings as follows: 1) Encoder-decoder models, such as the ones in the Flan-T5 family, generally outperform causal decoder-only LMs in MHQA tasks, despite being significantly smaller in size; 2) altering the order of gold documents reveals distinct trends in both Flan T5 models and fine-tuned decoder-only models, with optimal performance observed when the document order aligns with the reasoning chain order; 3) enhancing causal decoder-only models with bi-directional attention by modifying the causal mask can effectively boost their end performance. In addition to the above, we conduct a thorough investigation of the distribution of LM attention weights in the context of MHQA. Our experiments reveal that attention weights tend to peak at higher values when the resulting answer is correct. We leverage this finding to heuristically improve LMs' performance on this task. Our code is publicly available at https://github.com/hwy9855/MultiHopQA-Reasoning.

</details>


### [50] [Towards Universal Semantics With Large Language Models](https://arxiv.org/abs/2505.11764)

*Raymond Baartmans, Matthew Raffel, Rahul Vikram, Aiden Deringer, Lizhong Chen*

**Main category:** cs.CL

**Keywords:** Natural Semantic Metalanguage, large language models, NLP, semantic representation, LLM

**Relevance Score:** 9

**TL;DR:** This paper explores using large language models to generate Natural Semantic Metalanguage (NSM) explications, enhancing the speed and efficiency of NLP tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The NSM theory aims to uncover universal meanings through semantic primes, but generating explications has been a slow manual process.

**Method:** The authors utilize large language models (LLMs), develop a tailored dataset, and create automatic evaluation methods for generating NSM explications.

**Key Contributions:**

	1. First study to use LLMs for generating NSM explications
	2. Development of automatic evaluation methods for NSM
	3. Creation of a tailored dataset for training and evaluation

**Result:** The fine-tuned 1B and 8B models demonstrated improved performance over GPT-4o in generating accurate explications.

**Limitations:** 

**Conclusion:** This study represents a significant advancement in the use of LLMs for universal semantic representation, with potential applications in semantic analysis and translation.

**Abstract:** The Natural Semantic Metalanguage (NSM) is a linguistic theory based on a universal set of semantic primes: simple, primitive word-meanings that have been shown to exist in most, if not all, languages of the world. According to this framework, any word, regardless of complexity, can be paraphrased using these primes, revealing a clear and universally translatable meaning. These paraphrases, known as explications, can offer valuable applications for many natural language processing (NLP) tasks, but producing them has traditionally been a slow, manual process. In this work, we present the first study of using large language models (LLMs) to generate NSM explications. We introduce automatic evaluation methods, a tailored dataset for training and evaluation, and fine-tuned models for this task. Our 1B and 8B models outperform GPT-4o in producing accurate, cross-translatable explications, marking a significant step toward universal semantic representation with LLMs and opening up new possibilities for applications in semantic analysis, translation, and beyond.

</details>


### [51] [Retrospex: Language Agent Meets Offline Reinforcement Learning Critic](https://arxiv.org/abs/2505.11807)

*Yufei Xiang, Yiqun Shen, Yeqin Zhang, Cam-Tu Nguyen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Agent Framework

**Relevance Score:** 8

**TL;DR:** Retrospex is a novel LLM-based agent framework that enhances learning from past experiences through a retrospective process, utilizing a reinforcement learning critic for action value estimation.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM agent frameworks do not leverage past experiences effectively for agent improvement.

**Method:** Retrospex does not integrate experiences into the LLM's context but combines the LLM's action likelihood with action values from a reinforcement learning critic, trained on past experiences through retrospection.

**Key Contributions:**

	1. Introduces the Retrospex framework for LLM agents
	2. Implements a unique retrospection process for experience analysis
	3. Dynamic action rescoring mechanism based on interaction requirements

**Result:** Evaluation in various environments shows Retrospex outperforms strong contemporary baselines, particularly in scenarios requiring significant interaction with the environment.

**Limitations:** 

**Conclusion:** Retrospex offers a promising approach to improve LLM agents by enhancing their ability to learn from past interactions.

**Abstract:** Large Language Models (LLMs) possess extensive knowledge and commonsense reasoning capabilities, making them valuable for creating powerful agents. However, existing LLM agent frameworks have not fully utilized past experiences for improvement. This work introduces a new LLM-based agent framework called Retrospex, which addresses this challenge by analyzing past experiences in depth. Unlike previous approaches, Retrospex does not directly integrate experiences into the LLM's context. Instead, it combines the LLM's action likelihood with action values estimated by a Reinforcement Learning (RL) Critic, which is trained on past experiences through an offline ''retrospection'' process. Additionally, Retrospex employs a dynamic action rescoring mechanism that increases the importance of experience-based values for tasks that require more interaction with the environment. We evaluate Retrospex in ScienceWorld, ALFWorld and Webshop environments, demonstrating its advantages over strong, contemporary baselines.

</details>


### [52] [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)

*Jingyang Peng, Wenyuan Shen, Jiarui Rao, Jionghao Lin*

**Main category:** cs.CL

**Keywords:** Generative AI, Bias Assessment, Education, Automated Evaluation, Ethics

**Relevance Score:** 7

**TL;DR:** This paper presents an automated approach to assess biases in AI-generated educational materials, demonstrating high reliability and consistency with minimal human subjectivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical concerns regarding biases in AI-generated content used in tutor training, focusing on the need for systematic methods to evaluate these biases.

**Method:** The study integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction approach within a Retrieval-Augmented Generation framework to automate bias assessment.

**Key Contributions:**

	1. Automated bias assessment method for AI-generated educational content
	2. Integration of Contextualized Embedding Association Test with prompt-engineered extraction
	3. High reliability demonstrated with strong Pearson correlation coefficient

**Result:** The proposed method showed a strong Pearson correlation coefficient of r = 0.993 between automated and manually curated word sets, indicating reliable bias assessment.

**Limitations:** 

**Conclusion:** The approach reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing AI-generated educational content.

**Abstract:** Recent advances in Generative Artificial Intelligence (GenAI) have transformed educational content creation, particularly in developing tutor training materials. However, biases embedded in AI-generated content--such as gender, racial, or national stereotypes--raise significant ethical and educational concerns. Despite the growing use of GenAI, systematic methods for detecting and evaluating such biases in educational materials remain limited. This study proposes an automated bias assessment approach that integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework. We applied this method to AI-generated texts used in tutor training lessons. Results show a high alignment between the automated and manually curated word sets, with a Pearson correlation coefficient of r = 0.993, indicating reliable and consistent bias assessment. Our method reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing GenAI-produced educational content.

</details>


### [53] [Efficiently Building a Domain-Specific Large Language Model from Scratch: A Case Study of a Classical Chinese Large Language Model](https://arxiv.org/abs/2505.11810)

*Shen Li, Renfen Hu, Lijun Wang*

**Main category:** cs.CL

**Keywords:** large language models, Classical Chinese, domain-specific knowledge, AI Taiyan, language processing

**Relevance Score:** 4

**TL;DR:** This paper introduces AI Taiyan, a large language model engineered for Classical Chinese that outperforms general models in specific language processing tasks with only 1.8 billion parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite successes of general-purpose large language models in various tasks, their limitations in specialized domains like Classical Chinese necessitate the development of tailored models that can better incorporate domain-specific knowledge.

**Method:** The AI Taiyan model was designed with a careful approach to model architecture, data processing, foundational training, and fine-tuning to achieve effective performance in Classical Chinese information processing tasks.

**Key Contributions:**

	1. Development of AI Taiyan, specifically for Classical Chinese language tasks.
	2. Demonstrated superior performance on various language processing tasks compared to existing models.
	3. Provided insights into efficiently creating specialized domain-specific large language models.

**Result:** AI Taiyan demonstrated significant advantages over both general-purpose models and traditional domain-specific models in tasks such as punctuation, allusion identification, word meaning explanation, and translation, nearing or exceeding human performance.

**Limitations:** 

**Conclusion:** This research serves as a reference for constructing specialized large language models and highlights the model's applicability in areas like ancient text collation and language research.

**Abstract:** General-purpose large language models demonstrate notable capabilities in language comprehension and generation, achieving results that are comparable to, or even surpass, human performance in many language information processing tasks. Nevertheless, when general models are applied to some specific domains, e.g., Classical Chinese texts, their effectiveness is often unsatisfactory, and fine-tuning open-source foundational models similarly struggles to adequately incorporate domain-specific knowledge. To address this challenge, this study developed a large language model, AI Taiyan, specifically designed for understanding and generating Classical Chinese. Experiments show that with a reasonable model design, data processing, foundational training, and fine-tuning, satisfactory results can be achieved with only 1.8 billion parameters. In key tasks related to Classical Chinese information processing such as punctuation, identification of allusions, explanation of word meanings, and translation between ancient and modern Chinese, this model exhibits a clear advantage over both general-purpose large models and domain-specific traditional models, achieving levels close to or surpassing human baselines. This research provides a reference for the efficient construction of specialized domain-specific large language models. Furthermore, the paper discusses the application of this model in fields such as the collation of ancient texts, dictionary editing, and language research, combined with case studies.

</details>


### [54] [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)

*Han Meng, Yancan Chen, Yunan Li, Yitian Yang, Jungup Lee, Renwen Zhang, Yi-Chieh Lee*

**Main category:** cs.CL

**Keywords:** mental health, stigma, neural models, dataset, chatbot

**Relevance Score:** 7

**TL;DR:** This paper presents a comprehensive corpus for training neural models to classify mental health stigma, derived from expert-annotated human-chatbot interviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of robust resources and theoretical frameworks in training neural models for detecting mental health stigma.

**Method:** Development of an expert-annotated dataset from human-chatbot interviews, comprising 4,141 snippets from 684 participants, followed by experiments using state-of-the-art neural models.

**Key Contributions:**

	1. Expert-annotated dataset specifically addressing mental health stigma.
	2. In-depth benchmarking of neural models for stigma detection.
	3. Identification of empirical challenges and theoretical considerations in stigma classification.

**Result:** The paper benchmarks the performance of various neural models on the stigma detection task and identifies empirical challenges in this domain.

**Limitations:** The dataset is limited to chatbot conversational data and may not generalize beyond this context.

**Conclusion:** The dataset can enhance research focused on detecting, neutralizing, and counteracting mental-health stigma through computational methods.

**Abstract:** Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma.

</details>


### [55] [BELLE: A Bi-Level Multi-Agent Reasoning Framework for Multi-Hop Question Answering](https://arxiv.org/abs/2505.11811)

*Taolin Zhang, Dongyang Li, Qizhou Chen, Chengyu Wang, Xiaofeng He*

**Main category:** cs.CL

**Keywords:** Multi-hop Question Answering, Large Language Models, BELLE framework

**Relevance Score:** 9

**TL;DR:** This paper presents the BELLE framework for multi-hop question answering (QA), which tailors methods to different question types based on an agent-based debate system.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multi-hop QA by analyzing question types and their sensitivity to specific methods, enhancing the effectiveness of existing large language model (LLM) techniques.

**Method:** The BELLE framework employs two levels of agents that debate to create a comprehensive plan of operations for answering multi-hop questions, involving different types of reasoning strategies.

**Key Contributions:**

	1. Introduction of the BELLE framework for multi-hop QA
	2. Analysis of the sensitivity of different QA methods to question types
	3. Demonstration of improved performance and cost-effectiveness over strong baselines

**Result:** BELLE demonstrates superior performance compared to existing baselines across various datasets, with improved cost-effectiveness in handling complex question answering scenarios.

**Limitations:** 

**Conclusion:** The proposed BELLE framework offers a structured and effective approach to multi-hop QA by aligning methods with specific question types, resulting in enhanced performance.

**Abstract:** Multi-hop question answering (QA) involves finding multiple relevant passages and performing step-by-step reasoning to answer complex questions. Previous works on multi-hop QA employ specific methods from different modeling perspectives based on large language models (LLMs), regardless of the question types. In this paper, we first conduct an in-depth analysis of public multi-hop QA benchmarks, dividing the questions into four types and evaluating five types of cutting-edge methods for multi-hop QA: Chain-of-Thought (CoT), Single-step, Iterative-step, Sub-step, and Adaptive-step. We find that different types of multi-hop questions have varying degrees of sensitivity to different types of methods. Thus, we propose a Bi-levEL muLti-agEnt reasoning (BELLE) framework to address multi-hop QA by specifically focusing on the correspondence between question types and methods, where each type of method is regarded as an ''operator'' by prompting LLMs differently. The first level of BELLE includes multiple agents that debate to obtain an executive plan of combined ''operators'' to address the multi-hop QA task comprehensively. During the debate, in addition to the basic roles of affirmative debater, negative debater, and judge, at the second level, we further leverage fast and slow debaters to monitor whether changes in viewpoints are reasonable. Extensive experiments demonstrate that BELLE significantly outperforms strong baselines in various datasets. Additionally, the model consumption of BELLE is higher cost-effectiveness than that of single models in more complex multi-hop QA scenarios.

</details>


### [56] [Chain-of-Model Learning for Language Model](https://arxiv.org/abs/2505.11820)

*Kaitao Song, Xiaohua Wang, Xu Tan, Huiqiang Jiang, Chengruidong Zhang, Yongliang Shen, Cen LU, Zihao Li, Zifan Song, Caihua Shan, Yansen Wang, Kan Ren, Xiaoqing Zheng, Tao Qin, Yuqing Yang, Dongsheng Li, Lili Qiu*

**Main category:** cs.CL

**Keywords:** Chain-of-Model, Chain-of-Language-Model, Transformer, model scaling, efficient inference

**Relevance Score:** 7

**TL;DR:** The paper introduces a new learning paradigm called Chain-of-Model (CoM) that enhances model training and deployment efficiency by using causal relationships in layer representations, leading to flexible model scaling and inference through the Chain-of-Language-Model (CoLM) framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a more efficient and flexible framework for training and deploying language models by incorporating causal relationships into model structures.

**Method:** The authors propose the Chain-of-Model (CoM) paradigm, where the hidden states of each layer are structured in a chain format, allowing for progressive scaling of model size by introducing multiple sub-representations, and develop the Chain-of-Language-Model (CoLM) that applies this concept within the Transformer architecture.

**Key Contributions:**

	1. Introduction of the Chain-of-Model (CoM) paradigm for efficient model training and inference.
	2. Development of Chain-of-Language-Model (CoLM) for Transformer architectures incorporating this new paradigm.
	3. Implementation of a KV sharing mechanism in CoLM-Air for enhanced model extensibility.

**Result:** The CoLM family of models achieves performance comparable to standard Transformers while offering improved flexibility, such as progressive scaling, elastic inference, and various model sizes.

**Limitations:** 

**Conclusion:** The introduced CoM paradigm and CoLM models pave new paths for building language models with increased training efficiency and deployment adaptability.

**Abstract:** In this paper, we propose a novel learning paradigm, termed Chain-of-Model (CoM), which incorporates the causal relationship into the hidden states of each layer as a chain style, thereby introducing great scaling efficiency in model training and inference flexibility in deployment. We introduce the concept of Chain-of-Representation (CoR), which formulates the hidden states at each layer as a combination of multiple sub-representations (i.e., chains) at the hidden dimension level. In each layer, each chain from the output representations can only view all of its preceding chains in the input representations. Consequently, the model built upon CoM framework can progressively scale up the model size by increasing the chains based on the previous models (i.e., chains), and offer multiple sub-models at varying sizes for elastic inference by using different chain numbers. Based on this principle, we devise Chain-of-Language-Model (CoLM), which incorporates the idea of CoM into each layer of Transformer architecture. Based on CoLM, we further introduce CoLM-Air by introducing a KV sharing mechanism, that computes all keys and values within the first chain and then shares across all chains. This design demonstrates additional extensibility, such as enabling seamless LM switching, prefilling acceleration and so on. Experimental results demonstrate our CoLM family can achieve comparable performance to the standard Transformer, while simultaneously enabling greater flexiblity, such as progressive scaling to improve training efficiency and offer multiple varying model sizes for elastic inference, paving a a new way toward building language models. Our code will be released in the future at: https://github.com/microsoft/CoLM.

</details>


### [57] [Not All Thoughts are Generated Equal: Efficient LLM Reasoning via Multi-Turn Reinforcement Learning](https://arxiv.org/abs/2505.11827)

*Yansong Ning, Wei Li, Jun Fang, Naiqiang Tan, Hao Liu*

**Main category:** cs.CL

**Keywords:** chain-of-thought, large language models, reasoning efficiency, reinforcement learning, multi-turn collaboration

**Relevance Score:** 9

**TL;DR:** This paper introduces Long⊗Short, a framework for improving reasoning efficiency in large language models by differentiating between important and less important thoughts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning efficiency in large language models (LLMs) by compressing long chain-of-thoughts effectively instead of uniformly compressing all thoughts.

**Method:** The authors investigate the effectiveness and efficiency of different thoughts in chain-of-thought reasoning, proposing a metric for these qualities and developing a framework where two LLMs collaborate—one focusing on long-thoughts and the other on short-thoughts, fine-tuned for their respective tasks.

**Key Contributions:**

	1. Introduction of a theoretically bounded metric for evaluating thought effectiveness and efficiency
	2. Development of the Long⊗Short collaborative LLM framework
	3. Demonstration of significant token reduction with maintained performance across benchmarks

**Result:** The proposed method reduces token length by over 80% while achieving comparable performance to existing models across several benchmarks, including MATH500 and AIME24/25.

**Limitations:** Limited to specific benchmarks and requires initial cold-start data for fine-tuning.

**Conclusion:** The Long⊗Short framework effectively enhances reasoning efficiency, facilitating collaboration between models specialized in different thought generation styles.

**Abstract:** Compressing long chain-of-thought (CoT) from large language models (LLMs) is an emerging strategy to improve the reasoning efficiency of LLMs. Despite its promising benefits, existing studies equally compress all thoughts within a long CoT, hindering more concise and effective reasoning. To this end, we first investigate the importance of different thoughts by examining their effectiveness and efficiency in contributing to reasoning through automatic long CoT chunking and Monte Carlo rollouts. Building upon the insights, we propose a theoretically bounded metric to jointly measure the effectiveness and efficiency of different thoughts. We then propose Long$\otimes$Short, an efficient reasoning framework that enables two LLMs to collaboratively solve the problem: a long-thought LLM for more effectively generating important thoughts, while a short-thought LLM for efficiently generating remaining thoughts. Specifically, we begin by synthesizing a small amount of cold-start data to fine-tune LLMs for long-thought and short-thought reasoning styles, respectively. Furthermore, we propose a synergizing-oriented multi-turn reinforcement learning, focusing on the model self-evolution and collaboration between long-thought and short-thought LLMs. Experimental results show that our method enables Qwen2.5-7B and Llama3.1-8B to achieve comparable performance compared to DeepSeek-R1-Distill-Qwen-7B and DeepSeek-R1-Distill-Llama-8B, while reducing token length by over 80% across the MATH500, AIME24/25, AMC23, and GPQA Diamond benchmarks. Our data and code are available at https://github.com/yasNing/Long-otimes-Short/.

</details>


### [58] [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)

*Himel Ghosh, Ahmed Mosharafa, Georg Groh*

**Main category:** cs.CL

**Keywords:** Media bias, RoBERTa, NLP, Bias classification, Machine learning

**Relevance Score:** 7

**TL;DR:** The paper presents a RoBERTa-based model for sentence-level media bias detection using the BABE dataset, demonstrating improvements over prior models while highlighting the importance of context in bias classification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Media bias detection is vital for fair information dissemination, yet challenges arise from bias subjectivity and limited annotated data.

**Method:** Fine-tuning a RoBERTa-based model on the BABE dataset and using statistical tests to evaluate performance against a DA-RoBERTa baseline.

**Key Contributions:**

	1. Development of a RoBERTa-based model for bias classification
	2. Statistically significant performance improvements over existing models
	3. Pipeline integration with a bias-type classifier

**Result:** The model shows statistically significant improvements in performance and exhibits good generalization and interpretability.

**Limitations:** Constrained by sentence-level analysis and a lack of larger bias corpora.

**Conclusion:** The findings suggest potential for more robust and explainable NLP systems in media bias detection, with future work focusing on context-aware modeling and bias neutralization.

**Abstract:** Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.

</details>


### [59] [Class Distillation with Mahalanobis Contrast: An Efficient Training Paradigm for Pragmatic Language Understanding Tasks](https://arxiv.org/abs/2505.11829)

*Chenlu Wang, Weimin Lyu, Ritwik Banerjee*

**Main category:** cs.CL

**Keywords:** deviant language, nuanced language, classification, Mahalanobis distance, language models

**Relevance Score:** 7

**TL;DR:** ClaD is a novel training paradigm for efficient detection of deviant and nuanced language, outperforming existing classifiers with fewer parameters.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing safety, clarity, and interpretation of online discourse by detecting nuanced language such as sexism, metaphors, and sarcasm.

**Method:** ClaD introduces a loss function based on Mahalanobis distance and an interpretable decision algorithm for class separation, distilling small target classes from heterogeneous backgrounds.

**Key Contributions:**

	1. New training paradigm for language class detection
	2. Innovative loss function based on Mahalanobis distance
	3. Interpretable decision algorithm for improved class separation

**Result:** ClaD achieves competitive performance on sexism, metaphor, and sarcasm detection tasks, comparable to larger language models but with significantly fewer parameters.

**Limitations:** 

**Conclusion:** ClaD is an efficient tool for pragmatic language understanding that addresses the challenges of class separation in language detection tasks.

**Abstract:** Detecting deviant language such as sexism, or nuanced language such as metaphors or sarcasm, is crucial for enhancing the safety, clarity, and interpretation of online social discourse. While existing classifiers deliver strong results on these tasks, they often come with significant computational cost and high data demands. In this work, we propose \textbf{Cla}ss \textbf{D}istillation (ClaD), a novel training paradigm that targets the core challenge: distilling a small, well-defined target class from a highly diverse and heterogeneous background. ClaD integrates two key innovations: (i) a loss function informed by the structural properties of class distributions, based on Mahalanobis distance, and (ii) an interpretable decision algorithm optimized for class separation. Across three benchmark detection tasks -- sexism, metaphor, and sarcasm -- ClaD outperforms competitive baselines, and even with smaller language models and orders of magnitude fewer parameters, achieves performance comparable to several large language models (LLMs). These results demonstrate ClaD as an efficient tool for pragmatic language understanding tasks that require gleaning a small target class from a larger heterogeneous background.

</details>


### [60] [Multilingual Collaborative Defense for Large Language Models](https://arxiv.org/abs/2505.11835)

*Hongliang Li, Jinan Xu, Gengping Cui, Changhao Guan, Fengran Mo, Kaiyu Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Security, Collaborative Defense, Jailbreaking, Safety Prompts

**Relevance Score:** 9

**TL;DR:** The paper presents a novel Multilingual Collaborative Defense (MCD) method to enhance the safeguarding of large language models (LLMs) against multilingual jailbreak attempts.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** There is a pressing need to improve the security of LLMs in multilingual contexts, particularly against attacks that exploit translations into rare languages.

**Method:** The MCD approach automates the optimization of soft safety prompts for LLMs to enhance their multilingual safeguarding capabilities.

**Key Contributions:**

	1. Introduction of Multilingual Collaborative Defense (MCD) method for LLMs
	2. Demonstration of improved multilingual safety against jailbreak attempts
	3. Creation of multilingual benchmarks for evaluating safeguarding methods

**Result:** MCD shows improved safeguarding performance across multiple languages, maintains strong generalization capabilities, and reduces false refusal rates while addressing language safety misalignment.

**Limitations:** 

**Conclusion:** The MCD method significantly outperforms existing multilingual safeguarding strategies and demonstrates strong language transfer capabilities, making it a valuable contribution to LLM security research.

**Abstract:** The robustness and security of large language models (LLMs) has become a prominent research area. One notable vulnerability is the ability to bypass LLM safeguards by translating harmful queries into rare or underrepresented languages, a simple yet effective method of "jailbreaking" these models. Despite the growing concern, there has been limited research addressing the safeguarding of LLMs in multilingual scenarios, highlighting an urgent need to enhance multilingual safety. In this work, we investigate the correlation between various attack features across different languages and propose Multilingual Collaborative Defense (MCD), a novel learning method that optimizes a continuous, soft safety prompt automatically to facilitate multilingual safeguarding of LLMs. The MCD approach offers three advantages: First, it effectively improves safeguarding performance across multiple languages. Second, MCD maintains strong generalization capabilities while minimizing false refusal rates. Third, MCD mitigates the language safety misalignment caused by imbalances in LLM training corpora. To evaluate the effectiveness of MCD, we manually construct multilingual versions of commonly used jailbreak benchmarks, such as MaliciousInstruct and AdvBench, to assess various safeguarding methods. Additionally, we introduce these datasets in underrepresented (zero-shot) languages to verify the language transferability of MCD. The results demonstrate that MCD outperforms existing approaches in safeguarding against multilingual jailbreak attempts while also exhibiting strong language transfer capabilities. Our code is available at https://github.com/HLiang-Lee/MCD.

</details>


### [61] [When AI Co-Scientists Fail: SPOT-a Benchmark for Automated Verification of Scientific Research](https://arxiv.org/abs/2505.11855)

*Guijin Son, Jiwoo Hong, Honglu Fan, Heejeong Nam, Hyunwoo Ko, Seungwon Lim, Jinyeop Song, Jinha Choi, Gonçalo Paulo, Youngjae Yu, Stella Biderman*

**Main category:** cs.CL

**Keywords:** large language models, academic verification, scientific discovery, SPOT dataset, error detection

**Relevance Score:** 8

**TL;DR:** This paper explores using large language models (LLMs) for automating the academic verification of scientific manuscripts through a dataset called SPOT, which identifies significant errors in published papers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the potential of LLMs as verifiers that can automate the academic verification process of scientific manuscripts, addressing the shortcomings of current AI-assisted verification methods.

**Method:** The authors introduce SPOT, a dataset of 83 published papers and 91 significant errors, and evaluate various state-of-the-art LLMs on this dataset to measure their effectiveness in error detection.

**Key Contributions:**

	1. Introduction of the SPOT dataset for academic verification
	2. Evaluation of state-of-the-art LLMs on error detection
	3. Insights into the limitations and misconceptions demonstrated by LLMs in this context

**Result:** None of the evaluated LLMs exceeded 21.1% recall or 6.1% precision, indicating a significant gap in reliability for academic verification tasks.

**Limitations:** The confidence estimates of the models are uniformly low, and there is a lack of consistency in error rediscovery across multiple runs.

**Conclusion:** The study underscores the limitations of current LLMs in accurately and reliably detecting errors in scientific manuscripts, revealing that the technology is not yet suited for dependable AI-assisted academic verification.

**Abstract:** Recent advances in large language models (LLMs) have fueled the vision of automated scientific discovery, often called AI Co-Scientists. To date, prior work casts these systems as generative co-authors responsible for crafting hypotheses, synthesizing code, or drafting manuscripts. In this work, we explore a complementary application: using LLMs as verifiers to automate the \textbf{academic verification of scientific manuscripts}. To that end, we introduce SPOT, a dataset of 83 published papers paired with 91 errors significant enough to prompt errata or retraction, cross-validated with actual authors and human annotators. Evaluating state-of-the-art LLMs on SPOT, we find that none surpasses 21.1\% recall or 6.1\% precision (o3 achieves the best scores, with all others near zero). Furthermore, confidence estimates are uniformly low, and across eight independent runs, models rarely rediscover the same errors, undermining their reliability. Finally, qualitative analysis with domain experts reveals that even the strongest models make mistakes resembling student-level misconceptions derived from misunderstandings. These findings highlight the substantial gap between current LLM capabilities and the requirements for dependable AI-assisted academic verification.

</details>


### [62] [NAMET: Robust Massive Model Editing via Noise-Aware Memory Optimization](https://arxiv.org/abs/2505.11876)

*Yanbo Dai, Zhenlan Ji, Zongjie Li, Shuai Wang*

**Main category:** cs.CL

**Keywords:** Model Editing, Large Language Models, Knowledge Management

**Relevance Score:** 9

**TL;DR:** NAMET is a new method for efficiently editing knowledge in large language models by introducing noise during memory extraction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing model editing techniques are ineffective in large-scale editing scenarios due to embedding collisions which reduce reliability.

**Method:** NAMET (Noise-aware Model Editing in Transformers) introduces noise during memory extraction with a simple modification to a previous method (MEMIT).

**Key Contributions:**

	1. Introduction of noise during memory extraction to improve editing reliability
	2. Demonstration of consistent performance across various LLMs
	3. Successful application of NAMET in large-scale editing scenarios

**Result:** Extensive experiments show that NAMET consistently outperforms existing editing methods across six LLMs and three datasets when editing thousands of facts.

**Limitations:** 

**Conclusion:** NAMET provides a reliable solution for large-scale knowledge editing in language models.

**Abstract:** Model editing techniques are essential for efficiently updating knowledge in large language models (LLMs). However, the effectiveness of existing approaches degrades in massive editing scenarios, particularly when evaluated with practical metrics or in context-rich settings. We attribute these failures to embedding collisions among knowledge items, which undermine editing reliability at scale. To address this, we propose NAMET (Noise-aware Model Editing in Transformers), a simple yet effective method that introduces noise during memory extraction via a one-line modification to MEMIT. Extensive experiments across six LLMs and three datasets demonstrate that NAMET consistently outperforms existing methods when editing thousands of facts.

</details>


### [63] [AutoMedEval: Harnessing Language Models for Automatic Medical Capability Evaluation](https://arxiv.org/abs/2505.11887)

*Xiechi Zhang, Zetian Ouyang, Linlin Wang, Gerard de Melo, Zhu Cao, Xiaoling Wang, Ya Zhang, Yanfeng Wang, Liang He*

**Main category:** cs.CL

**Keywords:** medical LLM evaluation, AutoMedEval, hierarchical training

**Relevance Score:** 9

**TL;DR:** AutoMedEval is an open-sourced automatic evaluation model for assessing the proficiency of medical LLMs, employing advanced training methods to reduce reliance on human evaluations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved evaluation techniques for large language models in the medical domain, addressing the shortcomings of traditional metrics and the cost of human evaluations.

**Method:** A hierarchical training method involving curriculum instruction tuning and iterative knowledge introspection to enhance evaluation capabilities with limited instructional data.

**Key Contributions:**

	1. Introduction of an open-sourced evaluation model for medical LLMs.
	2. Development of a hierarchical training method for effective evaluation.
	3. Demonstrated improvements in correlation with human evaluations over traditional methods.

**Result:** AutoMedEval outperforms traditional evaluation baselines, showing a stronger correlation with human judgments during assessments of medical LLM responses.

**Limitations:** 

**Conclusion:** AutoMedEval effectively reduces the dependence on human evaluation by providing a reliable automatic assessment model for medical LLMs.

**Abstract:** With the proliferation of large language models (LLMs) in the medical domain, there is increasing demand for improved evaluation techniques to assess their capabilities. However, traditional metrics like F1 and ROUGE, which rely on token overlaps to measure quality, significantly overlook the importance of medical terminology. While human evaluation tends to be more reliable, it can be very costly and may as well suffer from inaccuracies due to limits in human expertise and motivation. Although there are some evaluation methods based on LLMs, their usability in the medical field is limited due to their proprietary nature or lack of expertise. To tackle these challenges, we present AutoMedEval, an open-sourced automatic evaluation model with 13B parameters specifically engineered to measure the question-answering proficiency of medical LLMs. The overarching objective of AutoMedEval is to assess the quality of responses produced by diverse models, aspiring to significantly reduce the dependence on human evaluation. Specifically, we propose a hierarchical training method involving curriculum instruction tuning and an iterative knowledge introspection mechanism, enabling AutoMedEval to acquire professional medical assessment capabilities with limited instructional data. Human evaluations indicate that AutoMedEval surpasses other baselines in terms of correlation with human judgments.

</details>


### [64] [Mobile-Bench-v2: A More Realistic and Comprehensive Benchmark for VLM-based Mobile Agents](https://arxiv.org/abs/2505.11891)

*Weikai Xu, Zhizheng Jiang, Yuxuan Liu, Wei Liu, Jian Luan, Yuanchun Li, Yunxin Liu, Bin Wang, Bo An*

**Main category:** cs.CL

**Keywords:** Mobile-Bench-v2, VLM-based agents, HCI, benchmarking, multi-path evaluation

**Relevance Score:** 6

**TL;DR:** The paper introduces Mobile-Bench-v2, a benchmark for VLM-based mobile agents, addressing limitations of existing benchmarks by incorporating multi-path evaluations and realistic noise and ambiguity scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of VLM-based mobile agents by addressing the limitations of existing online and offline benchmarks.

**Method:** Developed Mobile-Bench-v2, which includes multi-path evaluations, noisy and contaminated environments, and ambiguous instruction scenarios to better assess mobile agent performance.

**Key Contributions:**

	1. Introduction of Mobile-Bench-v2 benchmark
	2. Realistic noise and ambiguity scenarios for evaluation
	3. Multi-path evaluation methodology

**Result:** Tested the Mobile-Bench-v2 with multiple agents including AppAgent-v1 and Mobile-Agent-v2, highlighting improved evaluation capabilities under real-world conditions.

**Limitations:** Focuses on specific environments and may not generalize to all mobile application scenarios.

**Conclusion:** The Mobile-Bench-v2 benchmark provides a comprehensive framework for evaluating the abilities of mobile agents to interact in dynamic and noisy environments, thus enhancing future assessments.

**Abstract:** VLM-based mobile agents are increasingly popular due to their capabilities to interact with smartphone GUIs and XML-structured texts and to complete daily tasks. However, existing online benchmarks struggle with obtaining stable reward signals due to dynamic environmental changes. Offline benchmarks evaluate the agents through single-path trajectories, which stands in contrast to the inherently multi-solution characteristics of GUI tasks. Additionally, both types of benchmarks fail to assess whether mobile agents can handle noise or engage in proactive interactions due to a lack of noisy apps or overly full instructions during the evaluation process. To address these limitations, we use a slot-based instruction generation method to construct a more realistic and comprehensive benchmark named Mobile-Bench-v2. Mobile-Bench-v2 includes a common task split, with offline multi-path evaluation to assess the agent's ability to obtain step rewards during task execution. It contains a noisy split based on pop-ups and ads apps, and a contaminated split named AITZ-Noise to formulate a real noisy environment. Furthermore, an ambiguous instruction split with preset Q\&A interactions is released to evaluate the agent's proactive interaction capabilities. We conduct evaluations on these splits using the single-agent framework AppAgent-v1, the multi-agent framework Mobile-Agent-v2, as well as other mobile agents such as UI-Tars and OS-Atlas. Code and data are available at https://huggingface.co/datasets/xwk123/MobileBench-v2.

</details>


### [65] [RLAP: A Reinforcement Learning Enhanced Adaptive Planning Framework for Multi-step NLP Task Solving](https://arxiv.org/abs/2505.11893)

*Zepeng Ding, Dixuan Wang, Ziqin Luo, Guochao Jiang, Deqing Yang, Jiaqing Liang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Multi-step Planning, Large Language Models, Natural Language Processing, Adaptive Planning

**Relevance Score:** 9

**TL;DR:** Introducing RLAP, a Reinforcement Learning enhanced Adaptive Planning framework to optimize multi-step NLP tasks with LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing multi-step planning methods for LLMs overlook linguistic features and rely on intrinsic planning capabilities, leading to suboptimal outcomes.

**Method:** The paper proposes modeling NLP tasks as Markov decision processes (MDPs) using a lightweight Actor model to estimate Q-values, which interacts with the LLM to determine the optimal order of subtasks.

**Key Contributions:**

	1. Development of the RLAP framework for multi-step NLP tasks
	2. Integration of reinforcement learning with LLMs to enhance planning
	3. Extensive experiments validating the effectiveness of RLAP on various datasets

**Result:** RLAP is applied to three different types of NLP tasks and demonstrates effectiveness and robustness across multiple datasets.

**Limitations:** 

**Conclusion:** The proposed RLAP framework improves the performance of LLMs on multi-step NLP tasks by considering linguistic features and optimizing subtask sequencing.

**Abstract:** Multi-step planning has been widely employed to enhance the performance of large language models (LLMs) on downstream natural language processing (NLP) tasks, which decomposes the original task into multiple subtasks and guide LLMs to solve them sequentially without additional training. When addressing task instances, existing methods either preset the order of steps or attempt multiple paths at each step. However, these methods overlook instances' linguistic features and rely on the intrinsic planning capabilities of LLMs to evaluate intermediate feedback and then select subtasks, resulting in suboptimal outcomes. To better solve multi-step NLP tasks with LLMs, in this paper we propose a Reinforcement Learning enhanced Adaptive Planning framework (RLAP). In our framework, we model an NLP task as a Markov decision process (MDP) and employ an LLM directly into the environment. In particular, a lightweight Actor model is trained to estimate Q-values for natural language sequences consisting of states and actions through reinforcement learning. Therefore, during sequential planning, the linguistic features of each sequence in the MDP can be taken into account, and the Actor model interacts with the LLM to determine the optimal order of subtasks for each task instance. We apply RLAP on three different types of NLP tasks and conduct extensive experiments on multiple datasets to verify RLAP's effectiveness and robustness.

</details>


### [66] [Recursive Question Understanding for Complex Question Answering over Heterogeneous Personal Data](https://arxiv.org/abs/2505.11900)

*Philipp Christmann, Gerhard Weikum*

**Main category:** cs.CL

**Keywords:** question answering, personal information, operator trees, mixed sources, PerQA benchmark

**Relevance Score:** 8

**TL;DR:** ReQAP is a novel method for question answering that integrates structured and unstructured personal data sources by creating executable operator trees from user queries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide convenient access to mixed personal data while maintaining data on user devices.

**Method:** ReQAP utilizes recursive decomposition to form an operator tree for a given question, enabling the integration of diverse data types.

**Key Contributions:**

	1. Introduces ReQAP for recursive decomposition of questions into operator trees.
	2. Enables integration of structured and unstructured personal data sources.
	3. Releases the PerQA benchmark for diverse user information needs.

**Result:** The execution of the operator tree produces a traceable answer to the user's question, allowing for both simple look-ups and complex analytical inquiries.

**Limitations:** 

**Conclusion:** ReQAP facilitates effective querying of personal information and offers a new benchmark, PerQA, for evaluating persona-based data retrieval methods.

**Abstract:** Question answering over mixed sources, like text and tables, has been advanced by verbalizing all contents and encoding it with a language model. A prominent case of such heterogeneous data is personal information: user devices log vast amounts of data every day, such as calendar entries, workout statistics, shopping records, streaming history, and more. Information needs range from simple look-ups to queries of analytical nature. The challenge is to provide humans with convenient access with small footprint, so that all personal data stays on the user devices. We present ReQAP, a novel method that creates an executable operator tree for a given question, via recursive decomposition. Operators are designed to enable seamless integration of structured and unstructured sources, and the execution of the operator tree yields a traceable answer. We further release the PerQA benchmark, with persona-based data and questions, covering a diverse spectrum of realistic user needs.

</details>


### [67] [ELITE: Embedding-Less retrieval with Iterative Text Exploration](https://arxiv.org/abs/2505.11908)

*Zhangyu Wang, Siyuan Gao, Rong Zhou, Hao Wang, Li Ning*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, long-context QA

**Relevance Score:** 9

**TL;DR:** This paper presents an embedding-free retrieval framework that enhances the retrieval capabilities of LLMs through iterative search space refinement, eliminating the need for explicit graph construction and significantly improving performance on long-context QA tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing Retrieval-Augmented Generation (RAG) systems which struggle with long-term context retention and often retrieve semantically mismatched content.

**Method:** The proposed framework utilizes the logical inferencing abilities of LLMs to refine the search space iteratively and extend retrieval results with logically related information.

**Key Contributions:**

	1. Introduction of an embedding-free retrieval framework for LLMs
	2. Iterative search space refinement based on logical inference
	3. Significant reduction in storage and runtime compared to existing methods

**Result:** The experiments conducted on long-context QA benchmarks demonstrate that the new approach outperforms strong existing baselines, while reducing both storage and runtime requirements significantly.

**Limitations:** 

**Conclusion:** The embedding-free method represents a significant advancement in retrieval techniques, enabling better performance for LLMs in multi-turn and document-level tasks without the drawbacks of traditional graph-based retrieval methods.

**Abstract:** Large Language Models (LLMs) have achieved impressive progress in natural language processing, but their limited ability to retain long-term context constrains performance on document-level or multi-turn tasks. Retrieval-Augmented Generation (RAG) mitigates this by retrieving relevant information from an external corpus. However, existing RAG systems often rely on embedding-based retrieval trained on corpus-level semantic similarity, which can lead to retrieving content that is semantically similar in form but misaligned with the question's true intent. Furthermore, recent RAG variants construct graph- or hierarchy-based structures to improve retrieval accuracy, resulting in significant computation and storage overhead. In this paper, we propose an embedding-free retrieval framework. Our method leverages the logical inferencing ability of LLMs in retrieval using iterative search space refinement guided by our novel importance measure and extend our retrieval results with logically related information without explicit graph construction. Experiments on long-context QA benchmarks, including NovelQA and Marathon, show that our approach outperforms strong baselines while reducing storage and runtime by over an order of magnitude.

</details>


### [68] [Enhancing Complex Instruction Following for Large Language Models with Mixture-of-Contexts Fine-tuning](https://arxiv.org/abs/2505.11922)

*Yuheng Lu, ZiMeng Bai, Caixia Yuan, Huixing Jiang, Xiaojie Wang*

**Main category:** cs.CL

**Keywords:** large language models, instruction following, supervised fine-tuning, MISO, machine learning

**Relevance Score:** 9

**TL;DR:** MISO enhances complex instruction-following in LLMs by transforming sequential input into parallel instructions with subcontexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve large language models' (LLMs) abilities in following complex instructions that contain multiple constraints.

**Method:** We propose MISO (Multi-Input Single-Output), an extension of decoder-only transformer models, which introduces a mixture-of-contexts approach to enhance supervised fine-tuning (SFT).

**Key Contributions:**

	1. Introduction of MISO for enhanced instruction-following in LLMs
	2. Use of mixture-of-contexts for improving SFT effectiveness
	3. Demonstrated improvements in training efficiency and performance on complex tasks

**Result:** MISO fine-tuning shows superior performance in complex instruction-following tasks and improves training efficiency compared to traditional SFT approaches.

**Limitations:** 

**Conclusion:** MISO effectively enhances LLMs' capability in handling complex instructions by accommodating multiple sub-contexts, leading to better instruction-output alignment.

**Abstract:** Large language models (LLMs) exhibit remarkable capabilities in handling natural language tasks; however, they may struggle to consistently follow complex instructions including those involve multiple constraints. Post-training LLMs using supervised fine-tuning (SFT) is a standard approach to improve their ability to follow instructions. In addressing complex instruction following, existing efforts primarily focus on data-driven methods that synthesize complex instruction-output pairs for SFT. However, insufficient attention allocated to crucial sub-contexts may reduce the effectiveness of SFT. In this work, we propose transforming sequentially structured input instruction into multiple parallel instructions containing subcontexts. To support processing this multi-input, we propose MISO (Multi-Input Single-Output), an extension to currently dominant decoder-only transformer-based LLMs. MISO introduces a mixture-of-contexts paradigm that jointly considers the overall instruction-output alignment and the influence of individual sub-contexts to enhance SFT effectiveness. We apply MISO fine-tuning to complex instructionfollowing datasets and evaluate it with standard LLM inference. Empirical results demonstrate the superiority of MISO as a fine-tuning method for LLMs, both in terms of effectiveness in complex instruction-following scenarios and its potential for training efficiency.

</details>


### [69] [An Explanation of Intrinsic Self-Correction via Linear Representations and Latent Concepts](https://arxiv.org/abs/2505.11924)

*Yu-Ting Lee, Hui-Ying Shih, Fu-Chieh Chang, Pei-Yuan Wu*

**Main category:** cs.CL

**Keywords:** language models, self-correction, prompting, text detoxification, latent concept recognition

**Relevance Score:** 8

**TL;DR:** This paper explains the performance gains of intrinsic self-correction in language models, focusing on how prompting influences hidden states and output distributions, and provides a mathematical basis for these phenomena.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms behind intrinsic self-correction in language models and how prompting can enhance their performance.

**Method:** The paper presents a mathematical formulation of self-correction and derives a concentration result based on alignment magnitudes. Experiments were conducted using zephyr-7b-sft to analyze text detoxification.

**Key Contributions:**

	1. Mathematical formulation of self-correction in language models.
	2. Experimental results on text detoxification showing the effect of prompting.
	3. Insights into the latent concept recognition abilities of language models through self-correction.

**Result:** Significant differences were observed in the alignment of prompt-induced shifts between toxic and non-toxic tokens, demonstrating how self-correction improves latent concept recognition in language models.

**Limitations:** 

**Conclusion:** The findings provide insights into how prompting enhances the language model's ability for concept recognition, explaining the mechanisms behind self-correction.

**Abstract:** We provide an explanation for the performance gains of intrinsic self-correction, a process where a language model iteratively refines its outputs without external feedback. More precisely, we investigate how prompting induces interpretable changes in hidden states and thus affects the output distributions. We hypothesize that each prompt-induced shift lies in a linear span of some linear representation vectors, naturally separating tokens based on individual concept alignment. Building around this idea, we give a mathematical formulation of self-correction and derive a concentration result for output tokens based on alignment magnitudes. Our experiments on text detoxification with zephyr-7b-sft reveal a substantial gap in the inner products of the prompt-induced shifts and the unembeddings of the top-100 most toxic tokens vs. those of the unembeddings of the bottom-100 least toxic tokens, under toxic instructions. This suggests that self-correction prompts enhance a language model's capability of latent concept recognition. Our analysis offers insights into the underlying mechanism of self-correction by characterizing how prompting works explainably. For reproducibility, our code is available.

</details>


### [70] [Neuro-Symbolic Query Compiler](https://arxiv.org/abs/2505.11932)

*Yuyao Zhang, Zhicheng Dou, Xiaoxi Li, Jiajie Jin, Yongkang Wu, Zhonghua Li, Qi Ye, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, neuro-symbolic framework, search intent recognition

**Relevance Score:** 8

**TL;DR:** This paper introduces QCompiler, a neuro-symbolic framework for enhancing search intent recognition in Retrieval-Augmented Generation (RAG) systems by utilizing a minimal BNF grammar for complex queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for precise recognition of search intent in RAG systems, especially for complex and resource-constrained queries.

**Method:** QCompiler uses a neuro-symbolic approach that includes a Query Expression Translator, Lexical Syntax Parser, and Recursive Descent Processor to formalize queries into Abstract Syntax Trees (ASTs) using a designed BNF grammar.

**Key Contributions:**

	1. Introduction of a minimal BNF grammar for formalizing complex queries
	2. Development of a neuro-symbolic framework for search intent recognition in RAG systems
	3. Compilation of queries into Abstract Syntax Trees (ASTs) for enhanced execution accuracy.

**Result:** The framework improves the RAG system's ability to handle complex queries, leading to more precise document retrieval and response generation.

**Limitations:** 

**Conclusion:** QCompiler effectively bridges the gap in recognizing complex search intents, ensuring system completeness while reducing redundancy.

**Abstract:** Precise recognition of search intent in Retrieval-Augmented Generation (RAG) systems remains a challenging goal, especially under resource constraints and for complex queries with nested structures and dependencies. This paper presents QCompiler, a neuro-symbolic framework inspired by linguistic grammar rules and compiler design, to bridge this gap. It theoretically designs a minimal yet sufficient Backus-Naur Form (BNF) grammar $G[q]$ to formalize complex queries. Unlike previous methods, this grammar maintains completeness while minimizing redundancy. Based on this, QCompiler includes a Query Expression Translator, a Lexical Syntax Parser, and a Recursive Descent Processor to compile queries into Abstract Syntax Trees (ASTs) for execution. The atomicity of the sub-queries in the leaf nodes ensures more precise document retrieval and response generation, significantly improving the RAG system's ability to address complex queries.

</details>


### [71] [ChartEdit: How Far Are MLLMs From Automating Chart Analysis? Evaluating MLLMs' Capability via Chart Editing](https://arxiv.org/abs/2505.11935)

*Xuanle Zhao, Xuexin Liu, Haoyue Yang, Xianzhen Luo, Fanhu Zeng, Jianling Li, Qi Shi, Chi Chen*

**Main category:** cs.CL

**Keywords:** multimodal large language models, chart editing, evaluation framework, ChartEdit benchmark, MLLM performance

**Relevance Score:** 8

**TL;DR:** This paper introduces ChartEdit, a benchmark for evaluating the performance of multimodal large language models (MLLMs) in chart editing tasks, revealing significant limitations in their capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current multimodal models struggle with chart editing, an area requiring complex reasoning and precise intent interpretation, highlighting the need for better evaluation methods.

**Method:** The authors created ChartEdit, a benchmark with 1,405 diverse editing instructions for 233 real-world charts, validated through manual annotation. They evaluated 10 MLLMs using this benchmark in both code and chart generation experiments.

**Key Contributions:**

	1. Introduction of the ChartEdit benchmark for comprehensive evaluation of chart editing by MLLMs
	2. Demonstration of the performance gaps in editing capabilities of various MLLMs
	3. Provision of a valuable resource for future research in multimodal model evaluation

**Result:** The evaluation showed that while large-scale models can generate code for charts that somewhat resemble reference images, their accuracy in performing specified edits is limited, with the best model scoring only 59.96.

**Limitations:** Benchmark relies on manual assessments which may introduce subjective biases; the evaluation is focused on specific models and may not generalize to all MLLMs.

**Conclusion:** The findings indicate substantial challenges in MLLM performance for chart editing tasks, particularly highlighting the need for improvements in model capabilities and further benchmarking work.

**Abstract:** Although multimodal large language models (MLLMs) show promise in generating chart rendering code, chart editing presents a greater challenge. This difficulty stems from its nature as a labor-intensive task for humans that also demands MLLMs to integrate chart understanding, complex reasoning, and precise intent interpretation. While many MLLMs claim such editing capabilities, current assessments typically rely on limited case studies rather than robust evaluation methodologies, highlighting the urgent need for a comprehensive evaluation framework. In this work, we propose ChartEdit, a new high-quality benchmark designed for chart editing tasks. This benchmark comprises $1,405$ diverse editing instructions applied to $233$ real-world charts, with each instruction-chart instance having been manually annotated and validated for accuracy. Utilizing ChartEdit, we evaluate the performance of 10 mainstream MLLMs across two types of experiments, assessing them at both the code and chart levels. The results suggest that large-scale models can generate code to produce images that partially match the reference images. However, their ability to generate accurate edits according to the instructions remains limited. The state-of-the-art (SOTA) model achieves a score of only $59.96$, highlighting significant challenges in precise modification. In contrast, small-scale models, including chart-domain models, struggle both with following editing instructions and generating overall chart images, underscoring the need for further development in this area. Code is available at https://github.com/xxlllz/ChartEdit.

</details>


### [72] [Counterspeech the ultimate shield! Multi-Conditioned Counterspeech Generation through Attributed Prefix Learning](https://arxiv.org/abs/2505.11958)

*Aswini Kumar Padhi, Anil Bandhakavi, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** counterspeech, multi-attribute, preference optimization, HCI, NLP

**Relevance Score:** 6

**TL;DR:** HiPPrO is a novel framework for generating more effective counterspeech by using a hierarchical approach to incorporate multiple attributes simultaneously, achieving significant improvements in intent conformity and generation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance counterspeech generation effectiveness by considering multiple attributes rather than single intents, addressing the limitations of existing models.

**Method:** Introducing HiPPrO, a two-stage framework that applies hierarchical prefix learning optimized for dual attributes during the counterspeech generation process.

**Key Contributions:**

	1. Introduction of HiPPrO framework for multi-attribute counterspeech generation
	2. Hierarchical prefix learning with preference optimization
	3. Extensive human evaluation demonstrating improved relevance of generated outputs

**Result:** HiPPrO shows a ~38% improvement in intent conformity and improvements in Rouge metrics, outperforming several baseline models.

**Limitations:** 

**Conclusion:** Multi-attribute conditioning can significantly advance the generation capabilities of counterspeech systems and produce more appropriate responses.

**Abstract:** Counterspeech has proven to be a powerful tool to combat hate speech online. Previous studies have focused on generating counterspeech conditioned only on specific intents (single attributed). However, a holistic approach considering multiple attributes simultaneously can yield more nuanced and effective responses. Here, we introduce HiPPrO, Hierarchical Prefix learning with Preference Optimization, a novel two-stage framework that utilizes the effectiveness of attribute-specific prefix embedding spaces hierarchically optimized during the counterspeech generation process in the first phase. Thereafter, we incorporate both reference and reward-free preference optimization to generate more constructive counterspeech. Furthermore, we extend IntentCONANv2 by annotating all 13,973 counterspeech instances with emotion labels by five annotators. HiPPrO leverages hierarchical prefix optimization to integrate these dual attributes effectively. An extensive evaluation demonstrates that HiPPrO achieves a ~38 % improvement in intent conformity and a ~3 %, ~2 %, ~3 % improvement in Rouge-1, Rouge-2, and Rouge-L, respectively, compared to several baseline models. Human evaluations further substantiate the superiority of our approach, highlighting the enhanced relevance and appropriateness of the generated counterspeech. This work underscores the potential of multi-attribute conditioning in advancing the efficacy of counterspeech generation systems.

</details>


### [73] [EmoHopeSpeech: An Annotated Dataset of Emotions and Hope Speech in English](https://arxiv.org/abs/2505.11959)

*Md. Rafiul Biswas, Wajdi Zaghouani*

**Main category:** cs.CL

**Keywords:** bilingual dataset, emotions, hope speech, natural language processing, annotations

**Relevance Score:** 6

**TL;DR:** This paper presents a bilingual dataset for Arabic and English focused on annotating emotions and hope speech, aimed at advancing NLP in underrepresented languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the scarcity of multi-emotion datasets by providing comprehensive annotations for emotions and hope speech in Arabic and English.

**Method:** Created a bilingual dataset with 23,456 Arabic entries and 10,036 English entries, annotated for emotions and hope speech, utilizing Fleiss' Kappa for reliability.

**Key Contributions:**

	1. Introduction of a bilingual dataset for Arabic and English on emotions and hope speech.
	2. Comprehensive annotations capturing emotion intensity, complexity, and causes.
	3. High inter-annotator agreement demonstrated by Fleiss' Kappa.

**Result:** Achieved a micro-F1-Score of 0.67 from a baseline machine learning model, indicating the quality of annotations.

**Limitations:** 

**Conclusion:** The dataset serves as a significant resource for enhancing natural language processing research in underrepresented languages, enabling better analysis of emotions and hope speech.

**Abstract:** This research introduces a bilingual dataset comprising 23,456 entries for Arabic and 10,036 entries for English, annotated for emotions and hope speech, addressing the scarcity of multi-emotion (Emotion and hope) datasets. The dataset provides comprehensive annotations capturing emotion intensity, complexity, and causes, alongside detailed classifications and subcategories for hope speech. To ensure annotation reliability, Fleiss' Kappa was employed, revealing 0.75-0.85 agreement among annotators both for Arabic and English language. The evaluation metrics (micro-F1-Score=0.67) obtained from the baseline model (i.e., using a machine learning model) validate that the data annotations are worthy. This dataset offers a valuable resource for advancing natural language processing in underrepresented languages, fostering better cross-linguistic analysis of emotions and hope speech.

</details>


### [74] [CCNU at SemEval-2025 Task 3: Leveraging Internal and External Knowledge of Large Language Models for Multilingual Hallucination Annotation](https://arxiv.org/abs/2505.11965)

*Xu Liu, Guanyi Chen*

**Main category:** cs.CL

**Keywords:** human-computer interaction, machine learning, large language models, question-answering, multi-language

**Relevance Score:** 8

**TL;DR:** The CCNU team's system for the Mu-SHROOM shared task identifies hallucinations in multilingual question-answering systems using multiple LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the identification of hallucinations in question-answering systems across various languages using a systematic approach.

**Method:** The system employs multiple Large Language Models in parallel, leveraging their distinct areas of expertise to annotate hallucinations while simulating a crowdsourcing annotation process.

**Key Contributions:**

	1. Use of multiple LLMs for annotation
	2. Simulation of crowdsourcing for hallucination identification
	3. Performance rankings in multilingual settings

**Result:** Achieved first place for Hindi and ranked in the top five for seven other languages in the shared task.

**Limitations:** None specified.

**Conclusion:** The paper highlights both successful and unsuccessful approaches during the system development process and shares insights from the participation in the task.

**Abstract:** We present the system developed by the Central China Normal University (CCNU) team for the Mu-SHROOM shared task, which focuses on identifying hallucinations in question-answering systems across 14 different languages. Our approach leverages multiple Large Language Models (LLMs) with distinct areas of expertise, employing them in parallel to annotate hallucinations, effectively simulating a crowdsourcing annotation process. Furthermore, each LLM-based annotator integrates both internal and external knowledge related to the input during the annotation process. Using the open-source LLM DeepSeek-V3, our system achieves the top ranking (\#1) for Hindi data and secures a Top-5 position in seven other languages. In this paper, we also discuss unsuccessful approaches explored during our development process and share key insights gained from participating in this shared task.

</details>


### [75] [An Annotated Corpus of Arabic Tweets for Hate Speech Analysis](https://arxiv.org/abs/2505.11969)

*Md. Rafiul Biswas, Wajdi Zaghouani*

**Main category:** cs.CL

**Keywords:** Hate Speech, Arabic Language, Dataset, Annotation, Transformers

**Relevance Score:** 4

**TL;DR:** This study introduces a multilabel hate speech dataset for Arabic, comprising 10,000 tweets annotated for offensive content and various hate speech targets, with evaluation using a transformer model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of identifying hate speech in Arabic due to dialectal variations and improve offensive content detection.

**Method:** Collected and annotated 10,000 Arabic tweets for offensive content and hate speech targets, measuring inter-annotator agreement and evaluating with transformer models.

**Key Contributions:**

	1. Introduction of a multilabel hate speech dataset for Arabic
	2. High inter-annotator agreement scores
	3. Evaluation of transformer models for hate speech detection

**Result:** The inter-annotator agreement scores were 0.86 for offensive content and 0.71 for hate speech targets, with AraBERTv2 achieving a micro-F1 score of 0.7865 and accuracy of 0.786.

**Limitations:** The focus is on Twitter data, which may not represent all dialectal variations in Arabic.

**Conclusion:** The dataset provides a critical resource for hate speech detection in Arabic and demonstrates the effectiveness of transformer models in this context.

**Abstract:** Identifying hate speech content in the Arabic language is challenging due to the rich quality of dialectal variations. This study introduces a multilabel hate speech dataset in the Arabic language. We have collected 10000 Arabic tweets and annotated each tweet, whether it contains offensive content or not. If a text contains offensive content, we further classify it into different hate speech targets such as religion, gender, politics, ethnicity, origin, and others. A text can contain either single or multiple targets. Multiple annotators are involved in the data annotation task. We calculated the inter-annotator agreement, which was reported to be 0.86 for offensive content and 0.71 for multiple hate speech targets. Finally, we evaluated the data annotation task by employing a different transformers-based model in which AraBERTv2 outperformed with a micro-F1 score of 0.7865 and an accuracy of 0.786.

</details>


### [76] [Unveiling Knowledge Utilization Mechanisms in LLM-based Retrieval-Augmented Generation](https://arxiv.org/abs/2505.11995)

*Yuhao Wang, Ruiyang Ren, Yucheng Wang, Wayne Xin Zhao, Jing Liu, Hua Wu, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, knowledge integration, knowledge-intensive tasks, neuron activation

**Relevance Score:** 9

**TL;DR:** This paper investigates how large language models (LLMs) integrate internal and external knowledge in retrieval-augmented generation (RAG), proposing a framework to enhance their interpretability and reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexamined mechanisms by which LLMs use parametric and retrieved knowledge in RAG scenarios, thereby enhancing their performance in knowledge-intensive tasks.

**Method:** We utilized a knowledge stream analysis at both macroscopic and microscopic levels, decomposing the knowledge utilization process into four stages and introducing a novel method for neuron identification associated with different knowledge sources.

**Key Contributions:**

	1. Proposed a systematic framework for analyzing knowledge utilization in LLMs
	2. Introduced the knowledge activation probability entropy (KAPE) method for targeted neuron deactivation
	3. Decomposed the knowledge utilization into four distinct stages for better understanding of LLM mechanisms.

**Result:** We found that the relevance of retrieved passages significantly impacts the knowledge utilization process, and identified the distinct roles of LLM components in knowledge integration.

**Limitations:** 

**Conclusion:** Our findings provide new insights into improving interpretability and reliability in retrieval-augmented language models, crucial for enhancing their application in complex knowledge tasks.

**Abstract:** Considering the inherent limitations of parametric knowledge in large language models (LLMs), retrieval-augmented generation (RAG) is widely employed to expand their knowledge scope. Since RAG has shown promise in knowledge-intensive tasks like open-domain question answering, its broader application to complex tasks and intelligent assistants has further advanced its utility. Despite this progress, the underlying knowledge utilization mechanisms of LLM-based RAG remain underexplored. In this paper, we present a systematic investigation of the intrinsic mechanisms by which LLMs integrate internal (parametric) and external (retrieved) knowledge in RAG scenarios. Specially, we employ knowledge stream analysis at the macroscopic level, and investigate the function of individual modules at the microscopic level. Drawing on knowledge streaming analyses, we decompose the knowledge utilization process into four distinct stages within LLM layers: knowledge refinement, knowledge elicitation, knowledge expression, and knowledge contestation. We further demonstrate that the relevance of passages guides the streaming of knowledge through these stages. At the module level, we introduce a new method, knowledge activation probability entropy (KAPE) for neuron identification associated with either internal or external knowledge. By selectively deactivating these neurons, we achieve targeted shifts in the LLM's reliance on one knowledge source over the other. Moreover, we discern complementary roles for multi-head attention and multi-layer perceptron layers during knowledge formation. These insights offer a foundation for improving interpretability and reliability in retrieval-augmented LLMs, paving the way for more robust and transparent generative solutions in knowledge-intensive domains.

</details>


### [77] [Towards Comprehensive Argument Analysis in Education: Dataset, Tasks, and Method](https://arxiv.org/abs/2505.12028)

*Yupei Ren, Xinyi Zhou, Ning Zhang, Shangqing Zhao, Man Lan, Xiaopeng Bai*

**Main category:** cs.CL

**Keywords:** Argument Mining, Large Language Models, Argumentation Structure, Natural Language Processing, Essay Grading

**Relevance Score:** 7

**TL;DR:** This paper proposes 14 fine-grained argument relation types to enhance argument mining, addressing limitations in current models. It includes experiments on argument detection, relation prediction, and essay grading.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing argument mining approaches, which fail to capture the complexity of argument structures, particularly in real-world contexts.

**Method:** The authors propose 14 fine-grained relation types and conduct experiments on argument component detection, relation prediction, and automated essay grading.

**Key Contributions:**

	1. Proposes 14 fine-grained argument relation types.
	2. Conducts extensive experiments across multiple tasks related to argument mining.
	3. Highlights the role of writing quality in argument component detection and relation prediction.

**Result:** The experiments demonstrate the significance of fine-grained argument annotations in assessing argumentative writing quality and reveal connections between discourse relations and argumentative features.

**Limitations:** The study may rely on specific datasets which could limit generalizability.

**Conclusion:** The study emphasizes the necessity of multi-dimensional argument analysis to improve understanding and quality assessment of arguments in writing.

**Abstract:** Argument mining has garnered increasing attention over the years, with the recent advancement of Large Language Models (LLMs) further propelling this trend. However, current argument relations remain relatively simplistic and foundational, struggling to capture the full scope of argument information, particularly when it comes to representing complex argument structures in real-world scenarios. To address this limitation, we propose 14 fine-grained relation types from both vertical and horizontal dimensions, thereby capturing the intricate interplay between argument components for a thorough understanding of argument structure. On this basis, we conducted extensive experiments on three tasks: argument component detection, relation prediction, and automated essay grading. Additionally, we explored the impact of writing quality on argument component detection and relation prediction, as well as the connections between discourse relations and argumentative features. The findings highlight the importance of fine-grained argumentative annotations for argumentative writing quality assessment and encourage multi-dimensional argument analysis.

</details>


### [78] [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/abs/2310.10378)

*Jirui Qi, Raquel Fernández, Arianna Bisazza*

**Main category:** cs.CL

**Keywords:** Multilingualism, Cross-lingual consistency, Pretrained Language Models, Factual knowledge, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper investigates the cross-lingual consistency of factual knowledge in multilingual pretrained language models using a new metric called RankC.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure users with different language backgrounds receive consistent feedback from the same multilingual models, which currently varies across languages.

**Method:** The authors propose a Ranking-based Consistency (RankC) metric to assess the cross-lingual consistency of factual knowledge in multilingual PLMs. They analyze the factors affecting this consistency at both the model and language-pair levels.

**Key Contributions:**

	1. Introduction of the RankC metric for assessing cross-lingual consistency in multilingual PLMs.
	2. Identification of the relationship between model size and factual probing accuracy vs. cross-lingual consistency.
	3. Insights into the transfer of new knowledge across languages based on RankC scores.

**Result:** The study finds that while larger models tend to have higher factual accuracy in many languages, this does not translate to improved cross-lingual consistency. Their case study shows that new factual knowledge only transfers to languages with high RankC scores related to English.

**Limitations:** 

**Conclusion:** The findings highlight the need for better understanding and improvement of cross-lingual consistency in multilingual PLMs despite increasing model sizes.

**Abstract:** Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.

</details>


### [79] [MoL for LLMs: Dual-Loss Optimization to Enhance Domain Expertise While Preserving General Capabilities](https://arxiv.org/abs/2505.12043)

*Jingxue Chen, Qingkun Tang, Qianchun Lu, Siyuan Fang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain Adaptation, Mixture of Losses, Cross-Entropy Loss, Kullback-Leibler Divergence

**Relevance Score:** 9

**TL;DR:** The paper proposes a framework called Mixture of Losses (MoL) which enhances domain-specific training of large language models (LLMs) by decoupling optimization objectives for domain and general corpora, preventing performance degradation during domain adaptation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of LLMs in domain-specific tasks and reduce hallucinations and accuracy limitations caused by domain-biased data and improper corpus-mixture ratios.

**Method:** The proposed Mixture of Losses (MoL) framework utilizes cross-entropy loss for domain data and Kullback-Leibler divergence for general corpus, optimizing both without sacrificing model generalization.

**Key Contributions:**

	1. Introduction of the Mixture of Losses (MoL) framework for domain-specific training of LLMs.
	2. Demonstration of significant performance gains over traditional CPT methods.
	3. Establishment of an optimal 1:1 ratio for domain-to-general corpus training.

**Result:** The MoL framework achieved a 27.9% higher accuracy on the Math-500 benchmark and an 83.3% improvement on the AIME25 subset, significantly outperforming traditional CPT approaches.

**Limitations:** 

**Conclusion:** The dual-loss architecture is effective in balancing domain expertise and general language skills, avoiding catastrophic forgetting in LLMs during training.

**Abstract:** Although LLMs perform well in general tasks, domain-specific applications suffer from hallucinations and accuracy limitations. CPT approaches encounter two key issues: (1) domain-biased data degrades general language skills, and (2) improper corpus-mixture ratios limit effective adaptation. To address these, we propose a novel framework, Mixture of Losses (MoL), which decouples optimization objectives for domain-specific and general corpora. Specifically, cross-entropy (CE) loss is applied to domain data to ensure knowledge acquisition, while Kullback-Leibler (KL) divergence aligns general-corpus training with the base model's foundational capabilities. This dual-loss architecture preserves universal skills while enhancing domain expertise, avoiding catastrophic forgetting. Empirically, we validate that a 1:1 domain-to-general corpus ratio optimally balances training and overfitting without the need for extensive tuning or resource-intensive experiments. Furthermore, our experiments demonstrate significant performance gains compared to traditional CPT approaches, which often suffer from degradation in general language capabilities; our model achieves 27.9% higher accuracy on the Math-500 benchmark in the non-think reasoning mode, and an impressive 83.3% improvement on the challenging AIME25 subset in the think mode, underscoring the effectiveness of our approach.

</details>


### [80] [ABoN: Adaptive Best-of-N Alignment](https://arxiv.org/abs/2505.12050)

*Vinod Raman, Hilal Asi, Satyen Kale*

**Main category:** cs.CL

**Keywords:** language models, reward models, Best-of-N sampling, adaptive strategy, inference budget

**Relevance Score:** 8

**TL;DR:** The paper introduces a prompt-adaptive strategy for Best-of-N alignment in language models that optimizes inference-time compute by addressing alignment difficulty intelligently.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of test-time alignment methods in language models, particularly under latency constraints.

**Method:** The proposed two-stage algorithm first estimates the reward distribution for prompts with a small exploratory budget and then adaptively allocates the remaining inference budget based on these estimates.

**Key Contributions:**

	1. Introduction of a two-stage adaptive algorithm for prompt alignment
	2. Empirical validation showing performance improvements over uniform allocation
	3. Compatibility with any LM/RM combination

**Result:** Empirical results show the adaptive strategy consistently outperforms uniform allocation even with the same inference budget, and it remains competitive with larger budgets and growing batch sizes.

**Limitations:** 

**Conclusion:** The method is practical, efficient, and compatible with various language model and reward model pairs, offering a significant improvement in compute allocation for prompt processing.

**Abstract:** Recent advances in test-time alignment methods, such as Best-of-N sampling, offer a simple and effective way to steer language models (LMs) toward preferred behaviors using reward models (RM). However, these approaches can be computationally expensive, especially when applied uniformly across prompts without accounting for differences in alignment difficulty. In this work, we propose a prompt-adaptive strategy for Best-of-N alignment that allocates inference-time compute more efficiently. Motivated by latency concerns, we develop a two-stage algorithm: an initial exploratory phase estimates the reward distribution for each prompt using a small exploration budget, and a second stage adaptively allocates the remaining budget using these estimates. Our method is simple, practical, and compatible with any LM/RM combination. Empirical results on the AlpacaEval dataset for 12 LM/RM pairs and 50 different batches of prompts show that our adaptive strategy consistently outperforms the uniform allocation with the same inference budget. Moreover, our experiments show that our adaptive strategy remains competitive against uniform allocations with 20% larger inference budgets and even improves in performance as the batch size grows.

</details>


### [81] [GenderBench: Evaluation Suite for Gender Biases in LLMs](https://arxiv.org/abs/2505.12054)

*Matúš Pikuliak*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, evaluation suite

**Relevance Score:** 8

**TL;DR:** GenderBench is a suite for evaluating gender biases in LLMs, measuring 19 harmful behaviors through 14 probes, and includes evaluations of 12 LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reproducibility and robustness of measuring gender biases in large language models (LLMs).

**Method:** GenderBench includes 14 probes specifically designed to quantify 19 different gender-related harmful behaviors exhibited by LLMs and is released as an open-source library.

**Key Contributions:**

	1. Introduction of an evaluation suite specifically for gender biases in LLMs.
	2. Quantitative measurement of gender-related harmful behaviors in LLMs.
	3. Open-source release for community use and extension.

**Result:** Evaluation of 12 LLMs reveals consistent patterns of bias, including struggles with stereotypical reasoning, equitable gender representation, and discriminatory behavior in high-stakes scenarios.

**Limitations:** 

**Conclusion:** The GenderBench suite serves as a significant tool for assessing and addressing gender biases in LLMs, aiming to enhance fairness in AI applications.

**Abstract:** We present GenderBench -- a comprehensive evaluation suite designed to measure gender biases in LLMs. GenderBench includes 14 probes that quantify 19 gender-related harmful behaviors exhibited by LLMs. We release GenderBench as an open-source and extensible library to improve the reproducibility and robustness of benchmarking across the field. We also publish our evaluation of 12 LLMs. Our measurements reveal consistent patterns in their behavior. We show that LLMs struggle with stereotypical reasoning, equitable gender representation in generated texts, and occasionally also with discriminatory behavior in high-stakes scenarios, such as hiring.

</details>


### [82] [Why Not Act on What You Know? Unleashing Safety Potential of LLMs via Self-Aware Guard Enhancement](https://arxiv.org/abs/2505.12060)

*Peng Ding, Jun Kuang, Zongyu Wang, Xuezhi Cao, Xunliang Cai, Jiajun Chen, Shujian Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety, jailbreak attacks, SAGE, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces SAGE, a defense strategy for Large Language Models (LLMs) to improve their safety during generation, particularly against jailbreak attacks.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety gap in LLMs which detect but fail to generate safe responses to jailbreak prompts.

**Method:** SAGE employs a Discriminative Analysis Module to evaluate prompts and a Discriminative Response Module to manage generation, aiming to enhance LLM safety across various architectures without needing retraining.

**Key Contributions:**

	1. Introduction of SAGE, a novel training-free defense strategy for LLMs
	2. Demonstrated effectiveness across multiple LLM architectures with high defense success rates
	3. Conducted interpretability analysis revealing insights into safety generation behavior

**Result:** SAGE achieved an average 99% success rate in defending against sophisticated jailbreak methods while maintaining helpful output on standard benchmarks.

**Limitations:** Focused primarily on LLMs; potential applicability to other AI models not explored in detail.

**Conclusion:** This work contributes to the development of safer LLMs by aligning their detection capabilities with their response generation, enhancing overall safety awareness.

**Abstract:** Large Language Models (LLMs) have shown impressive capabilities across various tasks but remain vulnerable to meticulously crafted jailbreak attacks. In this paper, we identify a critical safety gap: while LLMs are adept at detecting jailbreak prompts, they often produce unsafe responses when directly processing these inputs. Inspired by this insight, we propose SAGE (Self-Aware Guard Enhancement), a training-free defense strategy designed to align LLMs' strong safety discrimination performance with their relatively weaker safety generation ability. SAGE consists of two core components: a Discriminative Analysis Module and a Discriminative Response Module, enhancing resilience against sophisticated jailbreak attempts through flexible safety discrimination instructions. Extensive experiments demonstrate SAGE's effectiveness and robustness across various open-source and closed-source LLMs of different sizes and architectures, achieving an average 99% defense success rate against numerous complex and covert jailbreak methods while maintaining helpfulness on general benchmarks. We further conduct mechanistic interpretability analysis through hidden states and attention distributions, revealing the underlying mechanisms of this detection-generation discrepancy. Our work thus contributes to developing future LLMs with coherent safety awareness and generation behavior. Our code and datasets are publicly available at https://github.com/NJUNLP/SAGE.

</details>


### [83] [Historical and psycholinguistic perspectives on morphological productivity: A sketch of an integrative approach](https://arxiv.org/abs/2505.12071)

*Harald Baayen, Kristian Berg, Maziyah Mohamed*

**Main category:** cs.CL

**Keywords:** morphological productivity, cognitive-computational model, Discriminative Lexicon Model, Thomas Mann, language processing

**Relevance Score:** 3

**TL;DR:** This study investigates morphological productivity via a cognitive-computational model focusing on Thomas Mann's writing.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the productivity of morphological forms and how they relate to cognitive processes and historical language usage.

**Method:** Utilizes the Discriminative Lexicon Model to explore Finnish nominal inflection, Malay derivation, and English compounding, alongside a diachronic analysis of Thomas Mann's writing over time.

**Key Contributions:**

	1. Integrates cognitive-computational modeling with diachronic analysis of an individual writer's output.
	2. Shows that systematic relationships between form and meaning are crucial for productivity in language.
	3. Demonstrates the limitations of using speaker-specific embeddings for low-frequency and novel words.

**Result:** The study finds that novel derived words produced by Mann are significantly fewer than those in his input, indicating a low rate of novel word production influenced by the distance of derived word embeddings from centroids.

**Limitations:** Examines only one writer's output (Thomas Mann), which may not generalize across different speakers or languages.

**Conclusion:** The findings highlight the tension between cognitive-computational models and actual language use, suggesting that productive morphological processes may not translate to novel word formation in individual writers.

**Abstract:** In this study, we approach morphological productivity from two perspectives: a cognitive-computational perspective, and a diachronic perspective zooming in on an actual speaker, Thomas Mann. For developing the first perspective, we make use of a cognitive computational model of the mental lexicon, the discriminative lexicon model. For computational mappings between form and meaning to be productive, in the sense that novel, previously unencountered words, can be understood and produced, there must be systematicities between the form space and the semantic space. If the relation between form and meaning would be truly arbitrary, a model could memorize form and meaning pairings, but there is no way in which the model would be able to generalize to novel test data. For Finnish nominal inflection, Malay derivation, and English compounding, we explore, using the Discriminative Lexicon Model as a computational tool, to trace differences in the degree to which inflectional and word formation patterns are productive. We show that the DLM tends to associate affix-like sublexical units with the centroids of the embeddings of the words with a given affix. For developing the second perspective, we study how the intake and output of one prolific writer, Thomas Mann, changes over time. We show by means of an examination of what Thomas Mann is likely to have read, and what he wrote, that the rate at which Mann produces novel derived words is extremely low. There are far more novel words in his input than in his output. We show that Thomas Mann is less likely to produce a novel derived word with a given suffix the greater the average distance is of the embeddings of all derived words to the corresponding centroid, and discuss the challenges of using speaker-specific embeddings for low-frequency and novel words.

</details>


### [84] [Do different prompting methods yield a common task representation in language models?](https://arxiv.org/abs/2505.12075)

*Guy Davidson, Todd M. Gureckis, Brenden M. Lake, Adina Williams*

**Main category:** cs.CL

**Keywords:** in-context learning, language models, task representation, function vectors, prompting methods

**Relevance Score:** 8

**TL;DR:** The paper investigates how different prompting methods for language models, specifically demonstrations and textual instructions, impact in-context learning (ICL) task representations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if different task presentations result in similar representations and to enhance interpretability and steerability of language models.

**Method:** The study generalizes function vectors to short textual instructions for extracting task representations and compares the effects of demonstrations and instructions on task performance.

**Key Contributions:**

	1. Generalization of function vectors to instruction prompts for ICL tasks
	2. Evidence that demonstration and instruction methods invoke different model components
	3. Recommendations for combining prompts to enhance task accuracy

**Result:** Findings indicate that different prompting mechanisms leverage distinct model components and elicit varying task representations, suggesting a lack of a common representation across methods.

**Limitations:** The study focuses primarily on few-shot tasks and may not generalize to all forms of ICL or different types of language models.

**Conclusion:** The paper highlights the importance of combining instructions and demonstrations for task performance while indicating the need for careful evaluation of task inference processes across different presentations.

**Abstract:** Demonstrations and instructions are two primary approaches for prompting language models to perform in-context learning (ICL) tasks. Do identical tasks elicited in different ways result in similar representations of the task? An improved understanding of task representation mechanisms would offer interpretability insights and may aid in steering models. We study this through function vectors, recently proposed as a mechanism to extract few-shot ICL task representations. We generalize function vectors to alternative task presentations, focusing on short textual instruction prompts, and successfully extract instruction function vectors that promote zero-shot task accuracy. We find evidence that demonstration- and instruction-based function vectors leverage different model components, and offer several controls to dissociate their contributions to task performance. Our results suggest that different task presentations do not induce a common task representation but elicit different, partly overlapping mechanisms. Our findings offer principled support to the practice of combining textual instructions and task demonstrations, imply challenges in universally monitoring task inference across presentation forms, and encourage further examinations of LLM task inference mechanisms.

</details>


### [85] [Model Merging in Pre-training of Large Language Models](https://arxiv.org/abs/2505.12082)

*Yunshui Li, Yiyuan Ma, Shen Yan, Chaoyi Zhang, Jing Liu, Jianqiao Lu, Ziwen Xu, Mengzhao Chen, Minrui Wang, Shiyi Zhan, Jin Ma, Xunhao Lai, Yao Luo, Xingyan Bin, Hongbin Ren, Mingji Han, Wenhao Hao, Bairen Yi, LingJun Liu, Bole Ma, Xiaoying Jia, Zhou Xun, Liang Xiang, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** model merging, large language models, pre-training, Mixture-of-Experts, efficient training

**Relevance Score:** 8

**TL;DR:** This paper explores model merging techniques during large-scale pre-training of language models, demonstrating significant performance gains and cost reductions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate model merging techniques in large-scale pre-training, which are underexplored despite their potential to enhance large language models.

**Method:** Extensive experiments with both dense and Mixture-of-Experts architectures, analyzing the effects of merging checkpoints trained with constant learning rates.

**Key Contributions:**

	1. Demonstrated performance improvements from model merging in pre-training.
	2. Provided pre-training guidelines for effective model merging.
	3. Uncovered novel applications and insights into merging strategies.

**Result:** Merging models results in significant performance improvements and accurate predictions of annealing behavior, leading to more efficient model development and reduced training costs.

**Limitations:** 

**Conclusion:** The findings provide practical pre-training guidelines for effective model merging and reveal new insights into the underlying mechanisms.

**Abstract:** Model merging has emerged as a promising technique for enhancing large language models, though its application in large-scale pre-training remains relatively unexplored. In this paper, we present a comprehensive investigation of model merging techniques during the pre-training process. Through extensive experiments with both dense and Mixture-of-Experts (MoE) architectures ranging from millions to over 100 billion parameters, we demonstrate that merging checkpoints trained with constant learning rates not only achieves significant performance improvements but also enables accurate prediction of annealing behavior. These improvements lead to both more efficient model development and significantly lower training costs. Our detailed ablation studies on merging strategies and hyperparameters provide new insights into the underlying mechanisms while uncovering novel applications. Through comprehensive experimental analysis, we offer the open-source community practical pre-training guidelines for effective model merging.

</details>


### [86] [Personalized Author Obfuscation with Large Language Models](https://arxiv.org/abs/2505.12090)

*Mohammad Shokri, Sarah Ita Levitan, Rivka Levitan*

**Main category:** cs.CL

**Keywords:** large language models, authorship obfuscation, personalized prompting, user performance, paraphrasing

**Relevance Score:** 9

**TL;DR:** This paper investigates the ability of large language models to obfuscate authorship through paraphrasing, observing that efficacy varies significantly among users. A personalized prompting method is proposed to improve this obfuscation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how effective large language models are in obfuscating authorship and to address variability in performance across different authors.

**Method:** The study focuses on user-wise performance instead of a holistic view, examining how the obfuscation effectiveness of LLMs varies for individual authors. A personalized prompting approach is introduced to enhance performance.

**Key Contributions:**

	1. Investigation of user-wise performance of LLMs in authorship obfuscation
	2. Identification of a bimodal distribution in effectiveness across users
	3. Proposal of a personalized prompting method that enhances obfuscation performance

**Result:** The findings reveal a bimodal distribution in the effectiveness of LLMs for authorship obfuscation, indicating significant variability among users. The personalized prompting method showed improvements over standard techniques.

**Limitations:** 

**Conclusion:** The use of personalized prompting can help mitigate the issues of variable effectiveness in authorship obfuscation using LLMs, providing a more tailored approach to this problem.

**Abstract:** In this paper, we investigate the efficacy of large language models (LLMs) in obfuscating authorship by paraphrasing and altering writing styles. Rather than adopting a holistic approach that evaluates performance across the entire dataset, we focus on user-wise performance to analyze how obfuscation effectiveness varies across individual authors. While LLMs are generally effective, we observe a bimodal distribution of efficacy, with performance varying significantly across users. To address this, we propose a personalized prompting method that outperforms standard prompting techniques and partially mitigates the bimodality issue.

</details>


### [87] [Improving Fairness in LLMs Through Testing-Time Adversaries](https://arxiv.org/abs/2505.12100)

*Isabela Pereira Gregio, Ian Pons, Anna Helena Reali Costa, Artur Jordão*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias mitigation, fairness in AI, ethical decision-making, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper proposes a user-friendly method to mitigate bias in Large Language Models (LLMs) during predictions, enhancing their reliability for ethical decision-making.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Bias in LLM responses hinders their use in ethically sensitive tasks, necessitating reliable methods to enhance their trustworthiness.

**Method:** The proposed method creates multiple variations of a sentence by modifying specific attributes and evaluates predictions against the original to identify inconsistencies indicative of bias, without requiring training or prior data knowledge.

**Key Contributions:**

	1. User-friendly method for bias mitigation in LLMs
	2. No training or fine-tuning needed for implementation
	3. Significant improvements in fairness metrics for Llama models

**Result:** Experimental results on the Llama family show improvements of up to 27 percentage points in reducing disparities for fairness metrics, particularly in the treatment of different racial groups.

**Limitations:** 

**Conclusion:** The method significantly improves fairness, equity, and reliability in LLM-generated results, making LLMs more suitable for ethical applications without the need for parameter tuning or training data changes.

**Abstract:** Large Language Models (LLMs) push the bound-aries in natural language processing and generative AI, driving progress across various aspects of modern society. Unfortunately, the pervasive issue of bias in LLMs responses (i.e., predictions) poses a significant and open challenge, hindering their application in tasks involving ethical sensitivity and responsible decision-making. In this work, we propose a straightforward, user-friendly and practical method to mitigate such biases, enhancing the reliability and trustworthiness of LLMs. Our method creates multiple variations of a given sentence by modifying specific attributes and evaluates the corresponding prediction behavior compared to the original, unaltered, prediction/sentence. The idea behind this process is that critical ethical predictions often exhibit notable inconsistencies, indicating the presence of bias. Unlike previous approaches, our method relies solely on forward passes (i.e., testing-time adversaries), eliminating the need for training, fine-tuning, or prior knowledge of the training data distribution. Through extensive experiments on the popular Llama family, we demonstrate the effectiveness of our method in improving various fairness metrics, focusing on the reduction of disparities in how the model treats individuals from different racial groups. Specifically, using standard metrics, we improve the fairness in Llama3 in up to 27 percentage points. Overall, our approach significantly enhances fairness, equity, and reliability in LLM-generated results without parameter tuning or training data modifications, confirming its effectiveness in practical scenarios. We believe our work establishes an important step toward enabling the use of LLMs in tasks that require ethical considerations and responsible decision-making.

</details>


### [88] [A Multi-Task Benchmark for Abusive Language Detection in Low-Resource Settings](https://arxiv.org/abs/2505.12116)

*Fitsum Gaim, Hoyun Song, Huije Lee, Changgeon Ko, Eui Jun Hwang, Jong C. Park*

**Main category:** cs.CL

**Keywords:** abusive language detection, Tigrinya, content moderation, multi-task benchmark, low-resource languages

**Relevance Score:** 5

**TL;DR:** This paper introduces a human-annotated dataset for abusive language detection in Tigrinya social media, offering insights into multilingual content moderation challenges and model effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources for abusive language detection in low-resource languages, specifically Tigrinya, and to enhance online safety for vulnerable users.

**Method:** A large-scale benchmark dataset was created, consisting of 13,717 annotated YouTube comments across three tasks: abusiveness, sentiment, and topic classification. The dataset includes both Romanized and Ge'ez script.

**Key Contributions:**

	1. Introduction of a multi-task benchmark dataset for Tigrinya social media
	2. Demonstration of effective model performance in low-resource conditions
	3. Provision of publicly available resources to support further research.

**Result:** Small, specialized multi-task models significantly outperform current models in low-resource settings, achieving up to 86% accuracy in abusiveness detection.

**Limitations:** The dataset is limited to Tigrinya language and may not encompass all facets of abusive language in broader contexts.

**Conclusion:** The dataset facilitates research in abusive language detection and emphasizes the need for continued work in content moderation for underrepresented languages.

**Abstract:** Content moderation research has recently made significant advances, but still fails to serve the majority of the world's languages due to the lack of resources, leaving millions of vulnerable users to online hostility. This work presents a large-scale human-annotated multi-task benchmark dataset for abusive language detection in Tigrinya social media with joint annotations for three tasks: abusiveness, sentiment, and topic classification. The dataset comprises 13,717 YouTube comments annotated by nine native speakers, collected from 7,373 videos with a total of over 1.2 billion views across 51 channels. We developed an iterative term clustering approach for effective data selection. Recognizing that around 64% of Tigrinya social media content uses Romanized transliterations rather than native Ge'ez script, our dataset accommodates both writing systems to reflect actual language use. We establish strong baselines across the tasks in the benchmark, while leaving significant challenges for future contributions. Our experiments reveal that small, specialized multi-task models outperform the current frontier models in the low-resource setting, achieving up to 86% accuracy (+7 points) in abusiveness detection. We make the resources publicly available to promote research on online safety.

</details>


### [89] [The AI Gap: How Socioeconomic Status Affects Language Technology Interactions](https://arxiv.org/abs/2505.12158)

*Elisa Bassignana, Amanda Cercas Curry, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** socioeconomic status, Large Language Models, digital divide, language technology, AI accessibility

**Relevance Score:** 9

**TL;DR:** This paper examines how socioeconomic status influences interactions with Large Language Models (LLMs) by surveying 1,000 individuals and analyzing their use patterns and prompts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of socioeconomic status (SES) on the usage of language technologies and to address limitations of previous research relying on proxy metrics and synthetic data.

**Method:** A survey of 1,000 individuals from diverse SES backgrounds was conducted, collecting 6,482 prompts from their interactions with LLMs to analyze differences in usage and interaction styles.

**Key Contributions:**

	1. Identifies SES-based differences in LLM interaction styles
	2. Highlights the need for tailored language technology development
	3. Provides empirical data on user interactions with LLMs across different SES groups

**Result:** The study finds systematic differences in LLM usage across SES groups; higher SES individuals use LLMs with higher abstraction and conciseness, while lower SES individuals show more anthropomorphization and concrete language expression.

**Limitations:** The study is limited to self-reported data and may not capture all dimensions of language technology use.

**Conclusion:** The findings indicate that while LLMs are becoming more accessible, linguistic differences tied to SES continue to promote the digital divide, emphasizing the need for inclusive technology design.

**Abstract:** Socioeconomic status (SES) fundamentally influences how people interact with each other and more recently, with digital technologies like Large Language Models (LLMs). While previous research has highlighted the interaction between SES and language technology, it was limited by reliance on proxy metrics and synthetic data. We survey 1,000 individuals from diverse socioeconomic backgrounds about their use of language technologies and generative AI, and collect 6,482 prompts from their previous interactions with LLMs. We find systematic differences across SES groups in language technology usage (i.e., frequency, performed tasks), interaction styles, and topics. Higher SES entails a higher level of abstraction, convey requests more concisely, and topics like 'inclusivity' and 'travel'. Lower SES correlates with higher anthropomorphization of LLMs (using ''hello'' and ''thank you'') and more concrete language. Our findings suggest that while generative language technologies are becoming more accessible to everyone, socioeconomic linguistic differences still stratify their use to exacerbate the digital divide. These differences underscore the importance of considering SES in developing language technologies to accommodate varying linguistic needs rooted in socioeconomic factors and limit the AI Gap across SES groups.

</details>


### [90] [Emotion Recognition for Low-Resource Turkish: Fine-Tuning BERTurk on TREMO and Testing on Xenophobic Political Discourse](https://arxiv.org/abs/2505.12160)

*Darmawan Wicaksono, Hasri Akbar Awal Rozaq, Nevfel Boz*

**Main category:** cs.CL

**Keywords:** Emotion Recognition, Sentiment Analysis, Turkish Social Media

**Relevance Score:** 4

**TL;DR:** Study examines anti-refugee sentiment in Turkish social media using a localized Emotion Recognition Model (ERM).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the rise of anti-refugee sentiment in Turkey amidst the Syrian refugee influx through social media discourse and enhance sentiment analysis for underrepresented languages like Turkish.

**Method:** Developed an Emotion Recognition Model (ERM) tailored for Turkish using BERTurk and the TREMO dataset, achieving 92.62% accuracy in categorizing various emotions.

**Key Contributions:**

	1. Advanced Emotion Recognition Model (ERM) for Turkish
	2. Enhancement of sentiment analysis in underrepresented languages
	3. Practical applications for real-time sentiment analysis in various fields

**Result:** The model was applied to large-scale X data, uncovering emotional nuances in Turkish discourse and advancing sentiment analysis techniques.

**Limitations:** 

**Conclusion:** Localized NLP tools like the ERM model have practical applications in marketing, public relations, and crisis management by offering real-time sentiment analysis tailored to Turkish-language contexts.

**Abstract:** Social media platforms like X (formerly Twitter) play a crucial role in shaping public discourse and societal norms. This study examines the term Sessiz Istila (Silent Invasion) on Turkish social media, highlighting the rise of anti-refugee sentiment amidst the Syrian refugee influx. Using BERTurk and the TREMO dataset, we developed an advanced Emotion Recognition Model (ERM) tailored for Turkish, achieving 92.62% accuracy in categorizing emotions such as happiness, fear, anger, sadness, disgust, and surprise. By applying this model to large-scale X data, the study uncovers emotional nuances in Turkish discourse, contributing to computational social science by advancing sentiment analysis in underrepresented languages and enhancing our understanding of global digital discourse and the unique linguistic challenges of Turkish. The findings underscore the transformative potential of localized NLP tools, with our ERM model offering practical applications for real-time sentiment analysis in Turkish-language contexts. By addressing critical areas, including marketing, public relations, and crisis management, these models facilitate improved decision-making through timely and accurate sentiment tracking. This highlights the significance of advancing research that accounts for regional and linguistic nuances.

</details>


### [91] [Truth Neurons](https://arxiv.org/abs/2505.12182)

*Haohang Li, Yupeng Cao, Yangyang Yu, Jordan W. Suchow, Zining Zhu*

**Main category:** cs.CL

**Keywords:** truthfulness, language models, truth neurons, AI reliability, trustworthiness

**Relevance Score:** 9

**TL;DR:** This paper identifies 'truth neurons' in language models that encode truthfulness, showing their universal presence across different models and implications for improving model trustworthiness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability and safety of language models by understanding how truthfulness is represented in their architecture.

**Method:** Conducted experiments identifying and analyzing the presence of truth neurons in multiple language models, examining their distribution and impact on model performance.

**Key Contributions:**

	1. Identification of truth neurons in language models
	2. Validation of truth neuron presence across models of varying scales
	3. Insights into truthfulness mechanisms informing future improvements in model trustworthiness

**Result:** The existence of truth neurons was validated across various language models, confirming they encode truthfulness in a subject-agnostic manner and are crucial for model performance on benchmarks.

**Limitations:** The study is primarily based on specific datasets and may require further exploration on other contexts and model architectures.

**Conclusion:** Understanding and improving the mechanisms of truthfulness in language models can enhance their trustworthiness and reliability, leading to safer AI applications.

**Abstract:** Despite their remarkable success and deployment across diverse workflows, language models sometimes produce untruthful responses. Our limited understanding of how truthfulness is mechanistically encoded within these models jeopardizes their reliability and safety. In this paper, we propose a method for identifying representations of truthfulness at the neuron level. We show that language models contain truth neurons, which encode truthfulness in a subject-agnostic manner. Experiments conducted across models of varying scales validate the existence of truth neurons, confirming that the encoding of truthfulness at the neuron level is a property shared by many language models. The distribution patterns of truth neurons over layers align with prior findings on the geometry of truthfulness. Selectively suppressing the activations of truth neurons found through the TruthfulQA dataset degrades performance both on TruthfulQA and on other benchmarks, showing that the truthfulness mechanisms are not tied to a specific dataset. Our results offer novel insights into the mechanisms underlying truthfulness in language models and highlight potential directions toward improving their trustworthiness and reliability.

</details>


### [92] [Decoding the Mind of Large Language Models: A Quantitative Evaluation of Ideology and Biases](https://arxiv.org/abs/2505.12183)

*Manari Hirose, Masato Uchida*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias evaluation, ideological biases, ethical implications, quantitative analysis

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for evaluating Large Language Models (LLMs) by analyzing their ideological biases through a set of quantitative questions, revealing notable differences between models and serious ethical concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand biases, thought patterns, and societal implications of LLMs to ensure ethical and effective use, given their widespread integration.

**Method:** A quantitative analysis was conducted using a framework applied to 436 binary-choice questions to assess the biases of ChatGPT and Gemini.

**Key Contributions:**

	1. Development of a novel framework for evaluating LLM biases
	2. Empirical findings on ideological differences between LLMs
	3. Identification of ethical concerns relating to LLM outputs

**Result:** The analysis revealed that LLMs maintain consistent opinions, but exhibit differing ideologies across models and languages. ChatGPT tends to align its responses with the questioner's opinions, and both models demonstrate problematic biases and claims.

**Limitations:** The study focuses on a limited set of binary-choice questions and may not encompass all possible biases in LLMs.

**Conclusion:** The findings emphasize the need for addressing ideological and ethical considerations in LLM evaluations and provide a flexible method for assessing LLM behavior.

**Abstract:** The widespread integration of Large Language Models (LLMs) across various sectors has highlighted the need for empirical research to understand their biases, thought patterns, and societal implications to ensure ethical and effective use. In this study, we propose a novel framework for evaluating LLMs, focusing on uncovering their ideological biases through a quantitative analysis of 436 binary-choice questions, many of which have no definitive answer. By applying our framework to ChatGPT and Gemini, findings revealed that while LLMs generally maintain consistent opinions on many topics, their ideologies differ across models and languages. Notably, ChatGPT exhibits a tendency to change their opinion to match the questioner's opinion. Both models also exhibited problematic biases, unethical or unfair claims, which might have negative societal impacts. These results underscore the importance of addressing both ideological and ethical considerations when evaluating LLMs. The proposed framework offers a flexible, quantitative method for assessing LLM behavior, providing valuable insights for the development of more socially aligned AI systems.

</details>


### [93] [Vectors from Larger Language Models Predict Human Reading Time and fMRI Data More Poorly when Dimensionality Expansion is Controlled](https://arxiv.org/abs/2505.12196)

*Yi-Chien Lin, Hongao Zhu, William Schuler*

**Main category:** cs.CL

**Keywords:** large language models, psychometric data, human sentence processing, inverse scaling, model architecture

**Relevance Score:** 9

**TL;DR:** This study evaluates the scaling effects of large language models (LLMs) on predicting human reading time and brain imaging data, highlighting a misalignment between LLM architecture and human sentence processing, particularly as model size increases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the relationship between large language models' ability to predict human reading time and brain imaging data while considering the impact of model size and architecture on this prediction.

**Method:** The study evaluates LLM scaling using entire LLM vectors while controlling for the number of predictors, addressing previously noted issues with their performance in predicting psychometric data.

**Key Contributions:**

	1. Evaluation of LLM scaling using entire vectors from LLMs
	2. Control for predictor number in model evaluation
	3. Demonstration of inverse scaling in LLM predictions

**Result:** The results indicate that inverse scaling occurs, suggesting that as LLMs grow larger, their predictions become less aligned with human sentence processing.

**Limitations:** The study may not account for all variables influencing human sentence processing, and the specific reasons for misalignment require further investigation.

**Conclusion:** The misalignment between LLMs and human sentence processing worsens with larger models, contradicting the assumption that increased model size necessarily improves psychometric predictions.

**Abstract:** The impressive linguistic abilities of large language models (LLMs) have recommended them as models of human sentence processing, with some conjecturing a positive 'quality-power' relationship (Wilcox et al., 2023), in which language models' (LMs') fit to psychometric data continues to improve as their ability to predict words in context increases. This is important because it suggests that elements of LLM architecture, such as veridical attention to context and a unique objective of predicting upcoming words, reflect the architecture of the human sentence processing faculty, and that any inadequacies in predicting human reading time and brain imaging data may be attributed to insufficient model complexity, which recedes as larger models become available. Recent studies (Oh and Schuler, 2023) have shown this scaling inverts after a point, as LMs become excessively large and accurate, when word prediction probability (as information-theoretic surprisal) is used as a predictor. Other studies propose the use of entire vectors from differently sized LLMs, still showing positive scaling (Schrimpf et al., 2021), casting doubt on the value of surprisal as a predictor, but do not control for the larger number of predictors in vectors from larger LMs. This study evaluates LLM scaling using entire LLM vectors, while controlling for the larger number of predictors in vectors from larger LLMs. Results show that inverse scaling obtains, suggesting that inadequacies in predicting human reading time and brain imaging data may be due to substantial misalignment between LLMs and human sentence processing, which worsens as larger models are used.

</details>


### [94] [How Reliable is Multilingual LLM-as-a-Judge?](https://arxiv.org/abs/2505.12201)

*Xiyan Fu, Wei Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Evaluation, Consistency, Ensemble Strategy, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper analyzes the reliability of large language models in multilingual evaluation tasks, revealing inconsistencies and proposing an ensemble strategy to improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the reliability of large language models (LLMs) as evaluators in multilingual contexts, especially as alternatives to human annotators.

**Method:** A comprehensive analysis of five LLMs from different families across five tasks involving 25 languages was conducted, measuring judgment consistency using Fleiss' Kappa.

**Key Contributions:**

	1. Comprehensive evaluation of LLMs' performance in multilingual settings.
	2. Identification of factors affecting judgment consistency across languages.
	3. Proposal of an ensemble strategy to improve multilingual judgment reliability.

**Result:** The study found that LLMs exhibit poor consistency in judgments across languages, averaging approximately 0.3 on Fleiss' Kappa, with significant discrepancies particularly in low-resource languages.

**Limitations:** The study indicates that neither training on multilingual data nor increasing model scale significantly enhances judgment consistency.

**Conclusion:** LLMs are currently unreliable for multilingual evaluation, and an ensemble strategy can enhance their consistency in practical applications.

**Abstract:** LLM-as-a-Judge has emerged as a popular evaluation strategy, where advanced large language models assess generation results in alignment with human instructions. While these models serve as a promising alternative to human annotators, their reliability in multilingual evaluation remains uncertain. To bridge this gap, we conduct a comprehensive analysis of multilingual LLM-as-a-Judge. Specifically, we evaluate five models from different model families across five diverse tasks involving 25 languages. Our findings reveal that LLMs struggle to achieve consistent judgment results across languages, with an average Fleiss' Kappa of approximately 0.3, and some models performing even worse. To investigate the cause of inconsistency, we analyze various influencing factors. We observe that consistency varies significantly across languages, with particularly poor performance in low-resource languages. Additionally, we find that neither training on multilingual data nor increasing model scale directly improves judgment consistency. These findings suggest that LLMs are not yet reliable for evaluating multilingual predictions. We finally propose an ensemble strategy which improves the consistency of the multilingual judge in real-world applications.

</details>


### [95] [Data Whisperer: Efficient Data Selection for Task-Specific LLM Fine-Tuning via Few-Shot In-Context Learning](https://arxiv.org/abs/2505.12212)

*Shaobo Wang, Ziming Wang, Xiangqi Jin, Jize Wang, Jiajun Zhang, Kaixin Li, Zichen Wen, Zhong Li, Conghui He, Xuming Hu, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** data selection, large language models, few-shot learning, fine-tuning, computational efficiency

**Relevance Score:** 9

**TL;DR:** Data Whisperer is a training-free method for optimally selecting subsets of data for fine-tuning large language models, achieving superior performance and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to efficiently select optimal subsets of data for fine-tuning large language models as dataset sizes grow, balancing performance and computational cost.

**Method:** Data Whisperer, an efficient, training-free, attention-based method that uses few-shot in-context learning with the model being fine-tuned.

**Key Contributions:**

	1. Introduction of Data Whisperer, a training-free data selection method
	2. Achieved superior performance with minimal data
	3. Significant speedup in the data selection process

**Result:** Data Whisperer outperforms the full GSM8K dataset on the Llama-3-8B-Instruct model while using only 10% of the data, with significant improvement and speedup over existing methods.

**Limitations:** 

**Conclusion:** The proposed method provides a more efficient way to fine-tune LLMs without the overhead of traditional data selection methods.

**Abstract:** Fine-tuning large language models (LLMs) on task-specific data is essential for their effective deployment. As dataset sizes grow, efficiently selecting optimal subsets for training becomes crucial to balancing performance and computational costs. Traditional data selection methods often require fine-tuning a scoring model on the target dataset, which is time-consuming and resource-intensive, or rely on heuristics that fail to fully leverage the model's predictive capabilities. To address these challenges, we propose Data Whisperer, an efficient, training-free, attention-based method that leverages few-shot in-context learning with the model to be fine-tuned. Comprehensive evaluations were conducted on both raw and synthetic datasets across diverse tasks and models. Notably, Data Whisperer achieves superior performance compared to the full GSM8K dataset on the Llama-3-8B-Instruct model, using just 10% of the data, and outperforms existing methods with a 3.1-point improvement and a 7.4$\times$ speedup.

</details>


### [96] [GMSA: Enhancing Context Compression via Group Merging and Layer Semantic Alignment](https://arxiv.org/abs/2505.12215)

*Jiwei Tang, Zhicheng Zhang, Shunlong Wu, Jingheng Ye, Lichen Bai, Zitai Wang, Tingwei Lu, Jiaqi Chen, Lin Hai, Hai-Tao Zheng, Hong-Gee Kim*

**Main category:** cs.CL

**Keywords:** large language models, context compression, natural language processing, semantic alignment, knowledge extraction

**Relevance Score:** 9

**TL;DR:** This paper presents GMSA, a context compression framework that enhances the efficiency of LLMs in long-context scenarios by reducing input sequence length and redundant information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with low computational efficiency and redundancy when handling long texts, necessitating improved methods for context management.

**Method:** The GMSA framework includes Group Merging for effective summary extraction and Layer Semantic Alignment to ensure coherent integration of summary vectors with input semantics, trained via autoencoders and fine-tuning.

**Key Contributions:**

	1. Introduction of Group Merging for summary vector extraction
	2. Development of Layer Semantic Alignment to bridge semantic gaps
	3. Implementation of Knowledge Extraction Fine-tuning for downstream adaptation

**Result:** GMSA achieves significant improvements in context restoration, speed, and stability in training, particularly demonstrating a 2x speedup in end-to-end inference for QA tasks.

**Limitations:** 

**Conclusion:** GMSA not only enhances performance in speech processing but also stabilizes convergence with fewer resources, showing promise for practical applications in NLP tasks.

**Abstract:** Large language models (LLMs) have achieved impressive performance in a variety of natural language processing (NLP) tasks. However, when applied to long-context scenarios, they face two challenges, i.e., low computational efficiency and much redundant information. This paper introduces GMSA, a context compression framework based on the encoder-decoder architecture, which addresses these challenges by reducing input sequence length and redundant information. Structurally, GMSA has two key components: Group Merging and Layer Semantic Alignment (LSA). Group merging is used to effectively and efficiently extract summary vectors from the original context. Layer semantic alignment, on the other hand, aligns the high-level summary vectors with the low-level primary input semantics, thus bridging the semantic gap between different layers. In the training process, GMSA first learns soft tokens that contain complete semantics through autoencoder training. To furtherly adapt GMSA to downstream tasks, we propose Knowledge Extraction Fine-tuning (KEFT) to extract knowledge from the soft tokens for downstream tasks. We train GMSA by randomly sampling the compression rate for each sample in the dataset. Under this condition, GMSA not only significantly outperforms the traditional compression paradigm in context restoration but also achieves stable and significantly faster convergence with only a few encoder layers. In downstream question-answering (QA) tasks, GMSA can achieve approximately a 2x speedup in end-to-end inference while outperforming both the original input prompts and various state-of-the-art (SOTA) methods by a large margin.

</details>


### [97] [One-for-All Pruning: A Universal Model for Customized Compression of Large Language Models](https://arxiv.org/abs/2505.12216)

*Rongguang Ye, Ming Tang*

**Main category:** cs.CL

**Keywords:** large language models, pruning, StratNet, Gaussian process, model performance

**Relevance Score:** 9

**TL;DR:** This paper introduces UniCuCo, a model for efficient customized compression of large language models (LLMs) that addresses the inefficiencies of existing methods for multiple simultaneous requests.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing pruning methods for LLMs become inefficient with multiple simultaneous requests, leading to increased processing times. There is a need for a solution that can handle these requests efficiently while maintaining model performance.

**Method:** The paper proposes UniCuCo, which uses a StratNet to learn optimal pruning strategies for arbitrary requests. A Gaussian process is employed to approximate the evaluation of pruning strategies, enabling effective training despite the non-differentiable nature of pruning.

**Key Contributions:**

	1. Introduction of the UniCuCo model for customized LLM compression
	2. Development of the StratNet for optimal pruning strategy mappings
	3. Use of Gaussian process to enable efficient training for non-differentiable processes

**Result:** UniCuCo is demonstrated to be 28 times faster than baseline methods when processing 64 simultaneous requests, while maintaining comparable accuracy.

**Limitations:** The main limitation includes the potential computational cost related to the Gaussian process approximation in certain scenarios.

**Conclusion:** The approach effectively addresses the scaling issues of current LLM pruning methods, making it suitable for real-world applications where multiple requests are common.

**Abstract:** Existing pruning methods for large language models (LLMs) focus on achieving high compression rates while maintaining model performance. Although these methods have demonstrated satisfactory performance in handling a single user's compression request, their processing time increases linearly with the number of requests, making them inefficient for real-world scenarios with multiple simultaneous requests. To address this limitation, we propose a Univeral Model for Customized Compression (UniCuCo) for LLMs, which introduces a StratNet that learns to map arbitrary requests to their optimal pruning strategy. The challenge in training StratNet lies in the high computational cost of evaluating pruning strategies and the non-differentiable nature of the pruning process, which hinders gradient backpropagation for StratNet updates. To overcome these challenges, we leverage a Gaussian process to approximate the evaluation process. Since the gradient of the Gaussian process is computable, we can use it to approximate the gradient of the non-differentiable pruning process, thereby enabling StratNet updates. Experimental results show that UniCuCo is 28 times faster than baselines in processing 64 requests, while maintaining comparable accuracy to baselines.

</details>


### [98] [Examining Linguistic Shifts in Academic Writing Before and After the Launch of ChatGPT: A Study on Preprint Papers](https://arxiv.org/abs/2505.12218)

*Tong Bao, Yi Zhao, Jin Mao, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, academic writing, linguistic analysis, arXiv, machine learning

**Relevance Score:** 9

**TL;DR:** This paper analyzes the impact of Large Language Models (LLMs) on the linguistic characteristics of academic writing, revealing significant changes in abstracts over the last decade.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically examine the influence of LLMs on the linguistic features of academic writing, addressing the gap left by prior quantitative research.

**Method:** Conducted a large-scale linguistic analysis on 823,798 abstracts from arXiv, focusing on features like lexical complexity, syntactic complexity, cohesion, readability, and sentiment.

**Key Contributions:**

	1. Analyzed 823,798 abstracts to assess LLM impact on writing
	2. Identified changes in lexical complexity and sentiment
	3. Highlighted the differential impact of LLMs across disciplines, especially in Computer Science

**Result:** Findings reveal an increase in LLM-preferred words, lexical complexity, and sentiment in academic abstracts, but a decrease in syntactic complexity, cohesion, and readability.

**Limitations:** Focused only on abstracts from arXiv; results may not generalize to other types of academic writing or datasets.

**Conclusion:** The study suggests that while LLMs help introduce new vocabulary, they may also lead to challenges in clarity and cohesion, notably affecting scholars with weaker English proficiency, particularly in Computer Science.

**Abstract:** Large Language Models (LLMs), such as ChatGPT, have prompted academic concerns about their impact on academic writing. Existing studies have primarily examined LLM usage in academic writing through quantitative approaches, such as word frequency statistics and probability-based analyses. However, few have systematically examined the potential impact of LLMs on the linguistic characteristics of academic writing. To address this gap, we conducted a large-scale analysis across 823,798 abstracts published in last decade from arXiv dataset. Through the linguistic analysis of features such as the frequency of LLM-preferred words, lexical complexity, syntactic complexity, cohesion, readability and sentiment, the results indicate a significant increase in the proportion of LLM-preferred words in abstracts, revealing the widespread influence of LLMs on academic writing. Additionally, we observed an increase in lexical complexity and sentiment in the abstracts, but a decrease in syntactic complexity, suggesting that LLMs introduce more new vocabulary and simplify sentence structure. However, the significant decrease in cohesion and readability indicates that abstracts have fewer connecting words and are becoming more difficult to read. Moreover, our analysis reveals that scholars with weaker English proficiency were more likely to use the LLMs for academic writing, and focused on improving the overall logic and fluency of the abstracts. Finally, at discipline level, we found that scholars in Computer Science showed more pronounced changes in writing style, while the changes in Mathematics were minimal.

</details>


### [99] [Bridging Generative and Discriminative Learning: Few-Shot Relation Extraction via Two-Stage Knowledge-Guided Pre-training](https://arxiv.org/abs/2505.12236)

*Quanjiang Guo, Jinchuan Zhang, Sijie Wang, Ling Tian, Zhao Kang, Bin Yan, Weidong Xiao*

**Main category:** cs.CL

**Keywords:** Few-Shot Relation Extraction, Large Language Models, Data Scarcity

**Relevance Score:** 8

**TL;DR:** The paper proposes TKRE, a novel framework that integrates large language models with traditional relation extraction methods to improve Few-Shot Relation Extraction (FSRE) through data generation and a two-stage pre-training strategy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of annotated data and the limited generalization capabilities of existing models create challenges in Few-Shot Relation Extraction.

**Method:** TKRE combines LLMs with traditional relation extraction models, using LLMs to create explanation-driven knowledge and synthetic data, alongside a two-stage pre-training strategy of Masked Span Language Modeling and Span-Level Contrastive Learning.

**Key Contributions:**

	1. Integration of LLMs with traditional relation extraction models
	2. Introduction of a two-stage pre-training strategy (MSLM and SCL)
	3. Generation of knowledge and synthetic data to tackle data scarcity

**Result:** TKRE achieves state-of-the-art performance on benchmark datasets for FSRE, addressing the issue of data scarcity effectively.

**Limitations:** 

**Conclusion:** The proposed framework shows potential for broader applications in low-resource scenarios, enhancing both relational reasoning and generalization capabilities in FSRE.

**Abstract:** Few-Shot Relation Extraction (FSRE) remains a challenging task due to the scarcity of annotated data and the limited generalization capabilities of existing models. Although large language models (LLMs) have demonstrated potential in FSRE through in-context learning (ICL), their general-purpose training objectives often result in suboptimal performance for task-specific relation extraction. To overcome these challenges, we propose TKRE (Two-Stage Knowledge-Guided Pre-training for Relation Extraction), a novel framework that synergistically integrates LLMs with traditional relation extraction models, bridging generative and discriminative learning paradigms. TKRE introduces two key innovations: (1) leveraging LLMs to generate explanation-driven knowledge and schema-constrained synthetic data, addressing the issue of data scarcity; and (2) a two-stage pre-training strategy combining Masked Span Language Modeling (MSLM) and Span-Level Contrastive Learning (SCL) to enhance relational reasoning and generalization. Together, these components enable TKRE to effectively tackle FSRE tasks. Comprehensive experiments on benchmark datasets demonstrate the efficacy of TKRE, achieving new state-of-the-art performance in FSRE and underscoring its potential for broader application in low-resource scenarios. \footnote{The code and data are released on https://github.com/UESTC-GQJ/TKRE.

</details>


### [100] [PANORAMA: A synthetic PII-laced dataset for studying sensitive data memorization in LLMs](https://arxiv.org/abs/2505.12238)

*Sriram Selvam, Anneswa Ghosh*

**Main category:** cs.CL

**Keywords:** privacy, large language models, PII memorization, synthetic dataset, HCI

**Relevance Score:** 9

**TL;DR:** The paper introduces PANORAMA, a synthetic dataset aimed at studying PII memorization risks in large language models and validating its utility through model fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy risks associated with PII memorization in large language models due to the lack of realistic datasets that reflect diverse sensitive information.

**Method:** The paper presents PANORAMA, a synthetic corpus generated using multi-attribute human profiles to emulate real-world demographics, producing various content types containing realistic PII.

**Key Contributions:**

	1. Introduction of a large-scale synthetic dataset (PANORAMA) for studying PII memorization.
	2. Demonstration of the utility of the dataset through model fine-tuning on PII memorization rates.
	3. Public availability of dataset and code for community use in privacy risk assessments.

**Result:** Fine-tuning the Mistral-7B model with the PANORAMA dataset showed consistent increases in PII memorization rates with data replication, varying by content type.

**Limitations:** The dataset is synthetic and may not capture all nuances of real-world PII memorization.

**Conclusion:** PANORAMA provides a crucial resource for assessing privacy risks, auditing models, and creating privacy-preserving language models in real-world applications.

**Abstract:** The memorization of sensitive and personally identifiable information (PII) by large language models (LLMs) poses growing privacy risks as models scale and are increasingly deployed in real-world applications. Existing efforts to study sensitive and PII data memorization and develop mitigation strategies are hampered by the absence of comprehensive, realistic, and ethically sourced datasets reflecting the diversity of sensitive information found on the web. We introduce PANORAMA - Profile-based Assemblage for Naturalistic Online Representation and Attribute Memorization Analysis, a large-scale synthetic corpus of 384,789 samples derived from 9,674 synthetic profiles designed to closely emulate the distribution, variety, and context of PII and sensitive data as it naturally occurs in online environments. Our data generation pipeline begins with the construction of internally consistent, multi-attribute human profiles using constrained selection to reflect real-world demographics such as education, health attributes, financial status, etc. Using a combination of zero-shot prompting and OpenAI o3-mini, we generate diverse content types - including wiki-style articles, social media posts, forum discussions, online reviews, comments, and marketplace listings - each embedding realistic, contextually appropriate PII and other sensitive information. We validate the utility of PANORAMA by fine-tuning the Mistral-7B model on 1x, 5x, 10x, and 25x data replication rates with a subset of data and measure PII memorization rates - revealing not only consistent increases with repetition but also variation across content types, highlighting PANORAMA's ability to model how memorization risks differ by context. Our dataset and code are publicly available, providing a much-needed resource for privacy risk assessment, model auditing, and the development of privacy-preserving LLMs.

</details>


### [101] [Distribution Prompting: Understanding the Expressivity of Language Models Through the Next-Token Distributions They Can Produce](https://arxiv.org/abs/2505.12244)

*Haojin Wang, Zining Zhu, Freda Shi*

**Main category:** cs.CL

**Keywords:** autonomous systems, machine learning, language models, prompt tuning, probability distribution

**Relevance Score:** 7

**TL;DR:** This paper explores the challenges and capabilities of autoregressive neural language models in generating specific token distributions based on prompts, highlighting that distributions with extreme entropies and those influenced by outliers are easier to elicit.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically understand the probability distributions that autoregressive language models can produce and identify factors influencing their elicitation from prompts.

**Method:** The authors utilize soft and hard gradient-based prompt tuning techniques to find prompts that induce language models to approximate target next-token distributions effectively.

**Key Contributions:**

	1. Identifying the easiness of eliciting distributions with extreme entropies compared to moderate ones.
	2. Demonstrating that target distributions containing outlier tokens are simpler to approximate.
	3. Establishing that distributions generated by language models are easier to approximate than random target distributions.

**Result:** It is found that distributions with very low or high entropy are easier to approximate than those with moderate entropy. Additionally, when distributions have the same entropy, those with outlier tokens are easier to induce, and distributions produced by language models themselves are easier to approximate than randomly chosen targets.

**Limitations:** 

**Conclusion:** The findings provide insights into the expressiveness of language models and the difficulties in using them as proposers of probability distributions.

**Abstract:** Autoregressive neural language models (LMs) generate a probability distribution over tokens at each time step given a prompt. In this work, we attempt to systematically understand the probability distributions that LMs can produce, showing that some distributions are significantly harder to elicit than others. Specifically, for any target next-token distribution over the vocabulary, we attempt to find a prompt that induces the LM to output a distribution as close as possible to the target, using either soft or hard gradient-based prompt tuning. We find that (1) in general, distributions with very low or very high entropy are easier to approximate than those with moderate entropy; (2) among distributions with the same entropy, those containing ''outlier tokens'' are easier to approximate; (3) target distributions generated by LMs -- even LMs with different tokenizers -- are easier to approximate than randomly chosen targets. These results offer insights into the expressiveness of LMs and the challenges of using them as probability distribution proposers.

</details>


### [102] [Not All Documents Are What You Need for Extracting Instruction Tuning Data](https://arxiv.org/abs/2505.12250)

*Chi Zhang, Huaping Zhong, Hongtao Li, Chengliang Chai, Jiawei Hong, Yuhao Deng, Jiacheng Wang, Tian Tan, Yizhou Yan, Jiantao Qiu, Ye Yuan, Guoren Wang, Conghui He, Lei Cao*

**Main category:** cs.CL

**Keywords:** Instruction Tuning, Large Language Models, Data Extraction, Quality Assessment, Multi-Armed Bandit

**Relevance Score:** 9

**TL;DR:** EQUAL is a scalable data extraction framework that improves instruction tuning for LLMs by efficiently selecting documents and extracting high-quality QA pairs, reducing costs significantly while enhancing model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on high-quality training data for instruction tuning in LLMs often leads to synthesized instruction data lacking diversity, which impacts real-world applicability.

**Method:** EQUAL clusters document corpora using contrastive learning embeddings and employs a multi-armed bandit strategy to identify valuable QA pairs, alternately selecting documents and extracting pairs to optimize efficiency and quality.

**Key Contributions:**

	1. Introduction of the EQUAL framework for scalable QA pair extraction
	2. Improved efficiency in selecting diverse instructional data
	3. Demonstrated significant cost reduction and accuracy improvement on LLMs

**Result:** EQUAL reduces computational costs by 5-10 times and enhances accuracy by 2.5% on models like LLaMA-3.1-8B and Mistral-7B across tasks based on datasets like AutoMathText and StackOverflow.

**Limitations:** 

**Conclusion:** The framework EQUAL is an effective solution for improving the efficiency and effectiveness of instruction tuning through better data extraction techniques.

**Abstract:** Instruction tuning improves the performance of large language models (LLMs), but it heavily relies on high-quality training data. Recently, LLMs have been used to synthesize instruction data using seed question-answer (QA) pairs. However, these synthesized instructions often lack diversity and tend to be similar to the input seeds, limiting their applicability in real-world scenarios. To address this, we propose extracting instruction tuning data from web corpora that contain rich and diverse knowledge. A naive solution is to retrieve domain-specific documents and extract all QA pairs from them, but this faces two key challenges: (1) extracting all QA pairs using LLMs is prohibitively expensive, and (2) many extracted QA pairs may be irrelevant to the downstream tasks, potentially degrading model performance. To tackle these issues, we introduce EQUAL, an effective and scalable data extraction framework that iteratively alternates between document selection and high-quality QA pair extraction to enhance instruction tuning. EQUAL first clusters the document corpus based on embeddings derived from contrastive learning, then uses a multi-armed bandit strategy to efficiently identify clusters that are likely to contain valuable QA pairs. This iterative approach significantly reduces computational cost while boosting model performance. Experiments on AutoMathText and StackOverflow across four downstream tasks show that EQUAL reduces computational costs by 5-10x and improves accuracy by 2.5 percent on LLaMA-3.1-8B and Mistral-7B

</details>


### [103] [Teach2Eval: An Indirect Evaluation Method for LLM by Judging How It Teaches](https://arxiv.org/abs/2505.12259)

*Yuhang Zhou, Xutian Chen, Yixin Cao, Yuchen Ni, Yu He, Siyu Tian, Xiang Liu, Jian Zhang, Chuanjun Ji, Guangnan Ye, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** large language models, evaluation framework, machine learning, HCI, automated assessment

**Relevance Score:** 8

**TL;DR:** Introducing Teach2Eval, a novel evaluation framework for large language models (LLMs) that assesses their ability to teach tasks to weaker models, providing a more scalable and interpretable method compared to traditional benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional evaluation methods for large language models, which are often plagued by fairness issues and scalability concerns.

**Method:** Teach2Eval transforms open-ended tasks into standardized multiple-choice questions using teacher-generated feedback, allowing for automated and multi-dimensional assessment of LLMs.

**Key Contributions:**

	1. Introduction of Teach2Eval framework for LLM assessment
	2. Use of teacher-generated feedback for creating MCQs
	3. Demonstrated strong alignment with existing model rankings and enhanced interpretability.

**Result:** Experimental results indicate that Teach2Eval aligns well with dynamic rankings of models and provides additional interpretability for model training.

**Limitations:** 

**Conclusion:** Teach2Eval offers a robust alternative to traditional benchmarks, capturing a wider range of cognitive abilities and minimizing risks associated with data contamination.

**Abstract:** Recent progress in large language models (LLMs) has outpaced the development of effective evaluation methods. Traditional benchmarks rely on task-specific metrics and static datasets, which often suffer from fairness issues, limited scalability, and contamination risks. In this paper, we introduce Teach2Eval, an indirect evaluation framework inspired by the Feynman Technique. Instead of directly testing LLMs on predefined tasks, our method evaluates a model's multiple abilities to teach weaker student models to perform tasks effectively. By converting open-ended tasks into standardized multiple-choice questions (MCQs) through teacher-generated feedback, Teach2Eval enables scalable, automated, and multi-dimensional assessment. Our approach not only avoids data leakage and memorization but also captures a broad range of cognitive abilities that are orthogonal to current benchmarks. Experimental results across 26 leading LLMs show strong alignment with existing human and model-based dynamic rankings, while offering additional interpretability for training guidance.

</details>


### [104] [Learning Auxiliary Tasks Improves Reference-Free Hallucination Detection in Open-Domain Long-Form Generation](https://arxiv.org/abs/2505.12265)

*Chengwei Qin, Wenxuan Zhou, Karthik Abinav Sankararaman, Nanshu Wang, Tengyu Xu, Alexander Radovic, Eryk Helenowski, Arya Talebzadeh, Aditya Tayade, Sinong Wang, Shafiq Joty, Han Fang, Hao Ma*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, fine-tuning, open-domain generation, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates reference-free hallucination detection in large language models, introducing a novel fine-tuning approach that improves the accuracy of detecting factually incorrect information in long-form text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Hallucination in large language models poses a challenge in generating factually correct information, particularly in open-domain tasks, and existing detection methods often depend on limited domains or external tools.

**Method:** The paper explores various approaches to hallucination detection, including prompting methods, probing, and fine-tuning, with an emphasis on a new paradigm called RATE-FT which combines fine-tuning with an auxiliary task.

**Key Contributions:**

	1. Investigation of reference-free methods for hallucination detection in open-domain text generation.
	2. Introduction of the RATE-FT paradigm that enhances fine-tuning with auxiliary tasks.
	3. Demonstrated effectiveness through extensive experiments on various model families and datasets.

**Result:** The proposed RATE-FT method demonstrated a 3% improvement over general fine-tuning techniques on the LongFact dataset, showing its effectiveness in enhancing the detection of hallucinations in LLMs.

**Limitations:** The findings primarily rely on specific model families and datasets, which may not generalize universally across all LLMs and open-domain tasks.

**Conclusion:** The study indicates that combining fine-tuning with auxiliary tasks can significantly improve the model's capability to distinguish between factual and hallucinated content in open-domain long-form generation tasks.

**Abstract:** Hallucination, the generation of factually incorrect information, remains a significant challenge for large language models (LLMs), especially in open-domain long-form generation. Existing approaches for detecting hallucination in long-form tasks either focus on limited domains or rely heavily on external fact-checking tools, which may not always be available.   In this work, we systematically investigate reference-free hallucination detection in open-domain long-form responses. Our findings reveal that internal states (e.g., model's output probability and entropy) alone are insufficient for reliably (i.e., better than random guessing) distinguishing between factual and hallucinated content. To enhance detection, we explore various existing approaches, including prompting-based methods, probing, and fine-tuning, with fine-tuning proving the most effective. To further improve the accuracy, we introduce a new paradigm, named RATE-FT, that augments fine-tuning with an auxiliary task for the model to jointly learn with the main task of hallucination detection. With extensive experiments and analysis using a variety of model families & datasets, we demonstrate the effectiveness and generalizability of our method, e.g., +3% over general fine-tuning methods on LongFact.

</details>


### [105] [$K$-MSHC: Unmasking Minimally Sufficient Head Circuits in Large Language Models with Experiments on Syntactic Classification Tasks](https://arxiv.org/abs/2505.12268)

*Pratim Chowdhary*

**Main category:** cs.CL

**Keywords:** language models, attention heads, classification tasks, syntactic tasks, neural circuits

**Relevance Score:** 6

**TL;DR:** This paper introduces a method to identify the attention heads in mid-sized language models essential for classification tasks, revealing distinct patterns of head usage across various syntactic tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the neural components in mid-sized language models that contribute to their capabilities, particularly in classification tasks.

**Method:** The authors propose the $(K, 	ext{epsilon})$-Minimum Sufficient Head Circuit ($K$-MSHC) methodology and the Search-K-MSHC algorithm to discover minimal sets of attention heads essential for specific tasks.

**Key Contributions:**

	1. Introduction of $(K, 	ext{epsilon})$-Minimum Sufficient Head Circuit methodology
	2. Development of the Search-K-MSHC algorithm
	3. Findings on specialized 'super-heads' for distinct syntactic tasks

**Result:** Analysis of the Gemma-9B model reveals task-specific head circuits: grammar tasks use early layers, arithmetic word problems activate various layers, and arithmetic verification shows a distributed pattern. The study identifies critical 'super-heads' with little overlap across tasks.

**Limitations:** The study is focused on mid-sized language models, and the findings may not directly generalize to larger models or other architectures.

**Conclusion:** Syntactic and numerical competencies in language models emerge from specialized head circuits that exhibit both reusability and dedicated functions across tasks.

**Abstract:** Understanding which neural components drive specific capabilities in mid-sized language models ($\leq$10B parameters) remains a key challenge. We introduce the $(\bm{K}, \epsilon)$-Minimum Sufficient Head Circuit ($K$-MSHC), a methodology to identify minimal sets of attention heads crucial for classification tasks as well as Search-K-MSHC, an efficient algorithm for discovering these circuits. Applying our Search-K-MSHC algorithm to Gemma-9B, we analyze three syntactic task families: grammar acceptability, arithmetic verification, and arithmetic word problems. Our findings reveal distinct task-specific head circuits, with grammar tasks predominantly utilizing early layers, word problems showing pronounced activity in both shallow and deep regions, and arithmetic verification demonstrating a more distributed pattern across the network. We discover non-linear circuit overlap patterns, where different task pairs share computational components at varying levels of importance. While grammar and arithmetic share many "weak" heads, arithmetic and word problems share more consistently critical "strong" heads. Importantly, we find that each task maintains dedicated "super-heads" with minimal cross-task overlap, suggesting that syntactic and numerical competencies emerge from specialized yet partially reusable head circuits.

</details>


### [106] [LLM-Based Evaluation of Low-Resource Machine Translation: A Reference-less Dialect Guided Approach with a Refined Sylheti-English Benchmark](https://arxiv.org/abs/2505.12273)

*Md. Atiqur Rahman, Sabrina Islam, Mushfiqul Haque Omi*

**Main category:** cs.CL

**Keywords:** Machine Translation, Low-Resource Languages, Dialect-Guided Evaluation

**Relevance Score:** 8

**TL;DR:** This paper presents a dialect-guided approach to enhance machine translation evaluation for low-resource languages using large language models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating machine translation for low-resource languages is challenging due to limited reference translations and dialectal diversity.

**Method:** A comprehensive framework that incorporates dialect-specific context into LLM-based MT evaluation, extending the ONUBAD dataset with Sylheti-English pairs and augmenting the tokenizer vocabulary.

**Key Contributions:**

	1. Proposed a dialect-guided framework for LLM-based MT evaluation.
	2. Augmented the ONUBAD dataset with Sylheti-English sentence pairs.
	3. Introduced a regression head and dialect-guided prompting strategy.

**Result:** The proposed method improves performance, achieving the highest gain of +0.1083 in Spearman correlation across multiple LLMs compared to existing methods.

**Limitations:** 

**Conclusion:** This dialect-guided approach effectively enhances evaluation capabilities for machine translation in low-resource languages.

**Abstract:** Evaluating machine translation (MT) for low-resource languages poses a persistent challenge, primarily due to the limited availability of high quality reference translations. This issue is further exacerbated in languages with multiple dialects, where linguistic diversity and data scarcity hinder robust evaluation. Large Language Models (LLMs) present a promising solution through reference-free evaluation techniques; however, their effectiveness diminishes in the absence of dialect-specific context and tailored guidance. In this work, we propose a comprehensive framework that enhances LLM-based MT evaluation using a dialect guided approach. We extend the ONUBAD dataset by incorporating Sylheti-English sentence pairs, corresponding machine translations, and Direct Assessment (DA) scores annotated by native speakers. To address the vocabulary gap, we augment the tokenizer vocabulary with dialect-specific terms. We further introduce a regression head to enable scalar score prediction and design a dialect-guided (DG) prompting strategy. Our evaluation across multiple LLMs shows that the proposed pipeline consistently outperforms existing methods, achieving the highest gain of +0.1083 in Spearman correlation, along with improvements across other evaluation settings. The dataset and the code are available at https://github.com/180041123-Atiq/MTEonLowResourceLanguage.

</details>


### [107] [The Tower of Babel Revisited: Multilingual Jailbreak Prompts on Closed-Source Large Language Models](https://arxiv.org/abs/2505.12287)

*Linghan Huang, Haolin Jin, Zhaoge Bi, Pengyue Yang, Peizhou Zhao, Taozhao Chen, Xiongfei Wu, Lei Ma, Huaming Chen*

**Main category:** cs.CL

**Keywords:** large language models, adversarial attacks, prompt injections, cross-lingual defense, security evaluation

**Relevance Score:** 8

**TL;DR:** This paper investigates vulnerabilities of closed-source large language models (LLMs) to adversarial prompt injections, presenting a framework to evaluate their performance under diverse multilingual attack scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the security vulnerabilities of closed-source LLMs against adversarial prompt injections, which have not been thoroughly explored compared to open-source models.

**Method:** An integrated adversarial framework employing diverse attack techniques to evaluate closed-source LLMs, analyzing 38,400 responses across 32 types of jailbreak attacks in English and Chinese.

**Key Contributions:**

	1. Developed a novel integrated adversarial framework for evaluating closed-source LLMs.
	2. Demonstrated significant variation in adversarial success rates based on language and model architecture.
	3. Introduced the Two-Sides attack technique as the most effective approach in the evaluation.

**Result:** The evaluation found that Qwen-Max is the most vulnerable LLM, while GPT-4o demonstrated the best defenses. The effectiveness of prompt attacks was higher in Chinese than in English, with the Two-Sides attack technique showing the highest success rate.

**Limitations:** The study is limited to only certain closed-source LLMs and specific types of attacks, which may not encompass all possible vulnerabilities or attack methodologies.

**Conclusion:** The findings underscore the urgent need for enhancements in language-aware alignment and cross-lingual defenses in LLMs, urging attention from researchers, developers, and policymakers.

**Abstract:** Large language models (LLMs) have seen widespread applications across various domains, yet remain vulnerable to adversarial prompt injections. While most existing research on jailbreak attacks and hallucination phenomena has focused primarily on open-source models, we investigate the frontier of closed-source LLMs under multilingual attack scenarios. We present a first-of-its-kind integrated adversarial framework that leverages diverse attack techniques to systematically evaluate frontier proprietary solutions, including GPT-4o, DeepSeek-R1, Gemini-1.5-Pro, and Qwen-Max. Our evaluation spans six categories of security contents in both English and Chinese, generating 38,400 responses across 32 types of jailbreak attacks. Attack success rate (ASR) is utilized as the quantitative metric to assess performance from three dimensions: prompt design, model architecture, and language environment. Our findings suggest that Qwen-Max is the most vulnerable, while GPT-4o shows the strongest defense. Notably, prompts in Chinese consistently yield higher ASRs than their English counterparts, and our novel Two-Sides attack technique proves to be the most effective across all models. This work highlights a dire need for language-aware alignment and robust cross-lingual defenses in LLMs, and we hope it will inspire researchers, developers, and policymakers toward more robust and inclusive AI systems.

</details>


### [108] [Enhance Mobile Agents Thinking Process Via Iterative Preference Learning](https://arxiv.org/abs/2505.12299)

*Kun Huang, Weikai Xu, Yuxuan Liu, Quandong Wang, Pengzhi Gao, Wei Liu, Jian Luan, Bin Wang, Bo An*

**Main category:** cs.CL

**Keywords:** human-computer interaction, visual language models, mobile agents, reasoning performance, user interfaces

**Relevance Score:** 8

**TL;DR:** The study proposes an Iterative Preference Learning (IPL) method to enhance the reasoning capabilities of VLM-based mobile agents in GUI tasks by generating diverse training data and optimizing learning using T-DPO pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing methods that rely on scarce CoaT trajectories and expensive annotations, this paper seeks to improve the reasoning performance and generalization of VLM-based agents in GUI tasks.

**Method:** The paper introduces an Iterative Preference Learning technique that constructs a CoaT-tree through sampling, uses rule-based rewards to score leaf nodes, and derives Thinking-level Direct Preference Optimization pairs, while incorporating a three-stage instruction evolution for fine-tuning.

**Key Contributions:**

	1. Proposed Iterative Preference Learning method to improve GUI task agents.
	2. Demonstrated state-of-the-art performance across multiple benchmarks.
	3. Introduced a novel three-stage instruction evolution for model fine-tuning.

**Result:** Experiments conducted on three Mobile GUI-agent benchmarks show that the proposed MobileIPL agent outperforms established models, achieving state-of-the-art results and generalizing effectively to scenarios outside the training domain.

**Limitations:** 

**Conclusion:** The proposed methods enhance the expressiveness and generalization of VLM-based mobile agents, confirming the effectiveness of the iterative learning and instruction evolution strategies.

**Abstract:** The Chain of Action-Planning Thoughts (CoaT) paradigm has been shown to improve the reasoning performance of VLM-based mobile agents in GUI tasks. However, the scarcity of diverse CoaT trajectories limits the expressiveness and generalization ability of such agents. While self-training is commonly employed to address data scarcity, existing approaches either overlook the correctness of intermediate reasoning steps or depend on expensive process-level annotations to construct process reward models (PRM). To address the above problems, we propose an Iterative Preference Learning (IPL) that constructs a CoaT-tree through interative sampling, scores leaf nodes using rule-based reward, and backpropagates feedback to derive Thinking-level Direct Preference Optimization (T-DPO) pairs. To prevent overfitting during warm-up supervised fine-tuning, we further introduce a three-stage instruction evolution, which leverages GPT-4o to generate diverse Q\&A pairs based on real mobile UI screenshots, enhancing both generality and layout understanding. Experiments on three standard Mobile GUI-agent benchmarks demonstrate that our agent MobileIPL outperforms strong baselines, including continual pretraining models such as OS-ATLAS and UI-TARS. It achieves state-of-the-art performance across three standard Mobile GUI-Agents benchmarks and shows strong generalization to out-of-domain scenarios.

</details>


### [109] [HBO: Hierarchical Balancing Optimization for Fine-Tuning Large Language Models](https://arxiv.org/abs/2505.12300)

*Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch*

**Main category:** cs.CL

**Keywords:** large language models, fine-tuning, data imbalance, hierarchical balancing, bilevel optimization

**Relevance Score:** 8

**TL;DR:** Hierarchical Balancing Optimization (HBO) enhances fine-tuning of large language models by autonomously adjusting data allocation both globally across datasets and locally within subsets, addressing data imbalance and heterogeneity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper tackles the challenges of data imbalance and heterogeneity in fine-tuning large language models (LLMs), which are often overlooked in existing methodologies.

**Method:** The methodology involves a bilevel optimization strategy featuring a Global Actor that balances data sampling across diverse subsets and several Local Actors that optimize data usage based on the difficulty of tasks, guided by reward functions reflecting learning progress.

**Key Contributions:**

	1. Introduction of Hierarchical Balancing Optimization (HBO) for LLM fine-tuning
	2. Bilevel optimization strategy with Global and Local Actors
	3. Demonstrated significant improvement over existing baselines

**Result:** HBO demonstrates significant accuracy improvements over existing methods across three LLM backbones evaluated on nine diverse tasks in multilingual and multitask configurations.

**Limitations:** 

**Conclusion:** The study concludes that HBO effectively resolves issues related to data imbalance and heterogeneity during LLM fine-tuning, leading to improved outcomes in model training across various datasets.

**Abstract:** Fine-tuning large language models (LLMs) on a mixture of diverse datasets poses challenges due to data imbalance and heterogeneity. Existing methods often address these issues across datasets (globally) but overlook the imbalance and heterogeneity within individual datasets (locally), which limits their effectiveness. We introduce Hierarchical Balancing Optimization (HBO), a novel method that enables LLMs to autonomously adjust data allocation during fine-tuning both across datasets (globally) and within each individual dataset (locally). HBO employs a bilevel optimization strategy with two types of actors: a Global Actor, which balances data sampling across different subsets of the training mixture, and several Local Actors, which optimizes data usage within each subset based on difficulty levels. These actors are guided by reward functions derived from the LLM's training state, which measure learning progress and relative performance improvement. We evaluate HBO on three LLM backbones across nine diverse tasks in multilingual and multitask setups. Results show that HBO consistently outperforms existing baselines, achieving significant accuracy gains. Our in-depth analysis further demonstrates that both the global actor and local actors of HBO effectively adjust data usage during fine-tuning. HBO provides a comprehensive solution to the challenges of data imbalance and heterogeneity in LLM fine-tuning, enabling more effective training across diverse datasets.

</details>


### [110] [Bidirectional LMs are Better Knowledge Memorizers? A Benchmark for Real-world Knowledge Injection](https://arxiv.org/abs/2505.12306)

*Yuwei Zhang, Wenhao Yu, Shangbin Feng, Yifan Zhu, Letian Peng, Jayanth Srinivasa, Gaowen Liu, Jingbo Shang*

**Main category:** cs.CL

**Keywords:** knowledge injection, large language models, benchmark, human-computer interaction, bidirectional language models

**Relevance Score:** 8

**TL;DR:** This paper introduces WikiDYK, a novel knowledge injection benchmark that utilizes Wikipedia facts to enhance large language models' (LLMs) knowledge memorization capabilities, demonstrating a framework that improves reliability accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored area of knowledge memorization in large language models and to create a high-quality, standardized benchmark for evaluating such capabilities.

**Method:** WikiDYK leverages human-edited Wikipedia facts to create a dataset of question-answer pairs across various formats. It employs continued pre-training to evaluate model performance.

**Key Contributions:**

	1. Introduction of the WikiDYK knowledge injection benchmark
	2. Demonstration of accuracy differences between CLMs and BiLMs in knowledge memorization
	3. Development of a modular collaborative framework for improving LLM reliability

**Result:** Experiments reveal that causal language models (CLMs) exhibit significantly lower knowledge memorization capabilities than bidirectional language models (BiLMs), with a 23% accuracy gap. The introduction of a collaborative framework utilizing ensembles of BiLMs enhances reliability accuracy by up to 29.1%.

**Limitations:** 

**Conclusion:** The study highlights the potential of using modular frameworks with BiLMs to improve knowledge integration in LLMs, suggesting future work on continuously updating knowledge bases.

**Abstract:** Despite significant advances in large language models (LLMs), their knowledge memorization capabilities remain underexplored, due to the lack of standardized and high-quality test ground. In this paper, we introduce a novel, real-world and large-scale knowledge injection benchmark that evolves continuously over time without requiring human intervention. Specifically, we propose WikiDYK, which leverages recently-added and human-written facts from Wikipedia's "Did You Know..." entries. These entries are carefully selected by expert Wikipedia editors based on criteria such as verifiability and clarity. Each entry is converted into multiple question-answer pairs spanning diverse task formats from easy cloze prompts to complex multi-hop questions. WikiDYK contains 12,290 facts and 77,180 questions, which is also seamlessly extensible with future updates from Wikipedia editors. Extensive experiments using continued pre-training reveal a surprising insight: despite their prevalence in modern LLMs, Causal Language Models (CLMs) demonstrate significantly weaker knowledge memorization capabilities compared to Bidirectional Language Models (BiLMs), exhibiting a 23% lower accuracy in terms of reliability. To compensate for the smaller scales of current BiLMs, we introduce a modular collaborative framework utilizing ensembles of BiLMs as external knowledge repositories to integrate with LLMs. Experiment shows that our framework further improves the reliability accuracy by up to 29.1%.

</details>


### [111] [ExpertSteer: Intervening in LLMs through Expert Knowledge](https://arxiv.org/abs/2505.12313)

*Weixuan Wang, Minghao Wu, Barry Haddow, Alexandra Birch*

**Main category:** cs.CL

**Keywords:** Large Language Models, Expert Steering, Machine Learning, Inference Control

**Relevance Score:** 9

**TL;DR:** ExpertSteer proposes a method for guiding Large Language Models using specialized expert models to generate steering vectors, overcoming limitations of current methods that rely on self-generated vectors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the control of Large Language Models during inference, as existing methods are limited by their reliance on model-specific steering vectors.

**Method:** ExpertSteer utilizes a four-step process: aligning representation dimensions with auto-encoders, identifying intervention layer pairs through mutual information analysis, generating steering vectors from expert models using Recursive Feature Machines, and applying these vectors to guide LLMs during inference.

**Key Contributions:**

	1. Introduction of a novel method for LLM guidance using external expert models
	2. A four-step process for cross-model transfer of knowledge
	3. Demonstration of significant performance improvement over established baselines

**Result:** Experiments show that ExpertSteer significantly outperforms existing baselines across 15 benchmarks in various domains, demonstrating its effectiveness in selectively guiding LLMs.

**Limitations:** 

**Conclusion:** The introduction of ExpertSteer allows for more flexible and effective steering of LLMs by leveraging external expert models, which enhances performance without modifying LLM parameters.

**Abstract:** Large Language Models (LLMs) exhibit remarkable capabilities across various tasks, yet guiding them to follow desired behaviours during inference remains a significant challenge. Activation steering offers a promising method to control the generation process of LLMs by modifying their internal activations. However, existing methods commonly intervene in the model's behaviour using steering vectors generated by the model itself, which constrains their effectiveness to that specific model and excludes the possibility of leveraging powerful external expert models for steering. To address these limitations, we propose ExpertSteer, a novel approach that leverages arbitrary specialized expert models to generate steering vectors, enabling intervention in any LLMs. ExpertSteer transfers the knowledge from an expert model to a target LLM through a cohesive four-step process: first aligning representation dimensions with auto-encoders to enable cross-model transfer, then identifying intervention layer pairs based on mutual information analysis, next generating steering vectors from the expert model using Recursive Feature Machines, and finally applying these vectors on the identified layers during inference to selectively guide the target LLM without updating model parameters. We conduct comprehensive experiments using three LLMs on 15 popular benchmarks across four distinct domains. Experiments demonstrate that ExpertSteer significantly outperforms established baselines across diverse tasks at minimal cost.

</details>


### [112] [LLMSR@XLLM25: An Empirical Study of LLM for Structural Reasoning](https://arxiv.org/abs/2505.12328)

*Xinye Li, Mingqi Wan, Dianbo Sui*

**Main category:** cs.CL

**Keywords:** Large Language Models, Structural Reasoning, Few-shot Learning

**Relevance Score:** 8

**TL;DR:** Submission to LLMSR@XLLM25 shared task evaluating LLMs' reasoning processes using Meta-Llama-3-8B-Instruct.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the fine-grained, controllable, and interpretable reasoning processes of large language models (LLMs).

**Method:** Developed a few-shot, multi-turn prompt for the LLM to extract conditions and verify logical validity, with a lightweight post-processor for normalizing outputs.

**Key Contributions:**

	1. Innovative use of few-shot prompting for reasoning tasks.
	2. Introduction of a lightweight post-processing method.
	3. Analysis of strengths and limitations in LLM reasoning.

**Result:** Ranked 5th overall in the shared task, achieving competitive macro F1 scores without fine-tuning or external resources.

**Limitations:** Limited to off-the-shelf model without fine-tuning or external retrieval.

**Conclusion:** The approach demonstrates effectiveness and lays groundwork for future research in structural reasoning with LLMs.

**Abstract:** We present Team asdfo123's submission to the LLMSR@XLLM25 shared task, which evaluates large language models on producing fine-grained, controllable, and interpretable reasoning processes. Systems must extract all problem conditions, decompose a chain of thought into statement-evidence pairs, and verify the logical validity of each pair. Leveraging only the off-the-shelf Meta-Llama-3-8B-Instruct, we craft a concise few-shot, multi-turn prompt that first enumerates all conditions and then guides the model to label, cite, and adjudicate every reasoning step. A lightweight post-processor based on regular expressions normalises spans and enforces the official JSON schema. Without fine-tuning, external retrieval, or ensembling, our method ranks 5th overall, achieving macro F1 scores on par with substantially more complex and resource-consuming pipelines. We conclude by analysing the strengths and limitations of our approach and outlining directions for future research in structural reasoning with LLMs. Our code is available at https://github.com/asdfo123/LLMSR-asdfo123.

</details>


### [113] [UniEdit: A Unified Knowledge Editing Benchmark for Large Language Models](https://arxiv.org/abs/2505.12345)

*Qizhou Chen, Dakan Wang, Taolin Zhang, Zaoming Yan, Chengsong You, Chengyu Wang, Xiaofeng He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Editing Benchmark, Knowledge Graphs, Neighborhood Multi-hop Chain Sampling, LLM Performance Evaluation

**Relevance Score:** 8

**TL;DR:** UniEdit is a unified benchmark designed for enhancing the accuracy of LLMs by ensuring comprehensive editing across diverse knowledge domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and reliability of large language models by addressing the limitations of existing LLM editing datasets which are confined to narrow domains.

**Method:** UniEdit constructs editing samples from 25 common domains using a Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to evaluate editing ripple effects, converting knowledge subgraphs into natural language with LLMs.

**Key Contributions:**

	1. Introduction of a unified editing benchmark (UniEdit) for LLMs.
	2. Development of NMCS for comprehensive ripple effect evaluation.
	3. Statistical analysis confirming the benchmark's scale and diversity.

**Result:** Statistical analysis demonstrates that UniEdit offers a comprehensive and diverse benchmark for LLM editing and provides insights into the performance of multiple editors across different evaluations.

**Limitations:** 

**Conclusion:** UniEdit establishes a standardized framework for LLM editing that can facilitate future research by providing extensive insights into editing dynamics across knowledge domains.

**Abstract:** Model editing aims to enhance the accuracy and reliability of large language models (LLMs) by efficiently adjusting their internal parameters. Currently, most LLM editing datasets are confined to narrow knowledge domains and cover a limited range of editing evaluation. They often overlook the broad scope of editing demands and the diversity of ripple effects resulting from edits. In this context, we introduce UniEdit, a unified benchmark for LLM editing grounded in open-domain knowledge. First, we construct editing samples by selecting entities from 25 common domains across five major categories, utilizing the extensive triple knowledge available in open-domain knowledge graphs to ensure comprehensive coverage of the knowledge domains. To address the issues of generality and locality in editing, we design an Neighborhood Multi-hop Chain Sampling (NMCS) algorithm to sample subgraphs based on a given knowledge piece to entail comprehensive ripple effects to evaluate. Finally, we employ proprietary LLMs to convert the sampled knowledge subgraphs into natural language text, guaranteeing grammatical accuracy and syntactical diversity. Extensive statistical analysis confirms the scale, comprehensiveness, and diversity of our UniEdit benchmark. We conduct comprehensive experiments across multiple LLMs and editors, analyzing their performance to highlight strengths and weaknesses in editing across open knowledge domains and various evaluation criteria, thereby offering valuable insights for future research endeavors.

</details>


### [114] [Wisdom from Diversity: Bias Mitigation Through Hybrid Human-LLM Crowds](https://arxiv.org/abs/2505.12349)

*Axel Abels, Tom Lenaerts*

**Main category:** cs.CL

**Keywords:** large language models, bias mitigation, crowd aggregation, human-computer interaction, machine learning

**Relevance Score:** 9

**TL;DR:** The paper investigates how large language models (LLMs) replicate biases from their training data and proposes methods to mitigate these biases through crowd-based response aggregation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the mechanisms through which biases manifest in LLM responses and to find effective strategies for mitigating these biases.

**Method:** The study analyzes responses from multiple LLMs to bias-eliciting headlines and compares different aggregation techniques, including averaging responses and using locally weighted methods.

**Key Contributions:**

	1. Analysis of LLM bias replication
	2. Comparison of response aggregation methods
	3. Development of hybrid crowd strategies for bias mitigation

**Result:** Locally weighted aggregation methods show better performance in reducing biases while also improving the accuracy of responses compared to simple averaging.

**Limitations:** 

**Conclusion:** Combining LLMs with human responses in a hybrid crowd approach significantly improves accuracy and reduces biases across various contexts.

**Abstract:** Despite their performance, large language models (LLMs) can inadvertently perpetuate biases found in the data they are trained on. By analyzing LLM responses to bias-eliciting headlines, we find that these models often mirror human biases. To address this, we explore crowd-based strategies for mitigating bias through response aggregation. We first demonstrate that simply averaging responses from multiple LLMs, intended to leverage the "wisdom of the crowd", can exacerbate existing biases due to the limited diversity within LLM crowds. In contrast, we show that locally weighted aggregation methods more effectively leverage the wisdom of the LLM crowd, achieving both bias mitigation and improved accuracy. Finally, recognizing the complementary strengths of LLMs (accuracy) and humans (diversity), we demonstrate that hybrid crowds containing both significantly enhance performance and further reduce biases across ethnic and gender-related contexts.

</details>


### [115] [CAPTURE: Context-Aware Prompt Injection Testing and Robustness Enhancement](https://arxiv.org/abs/2505.12368)

*Gauri Kholkar, Ratinder Ahuja*

**Main category:** cs.CL

**Keywords:** prompt injection, security, machine learning, context-awareness, guardrail models

**Relevance Score:** 6

**TL;DR:** Introduction of CAPTURE, a context-aware benchmark for evaluating prompt injection guardrail models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the major security risks posed by prompt injection in large language models and to evaluate the effectiveness of guardrail models beyond static benchmarks.

**Method:** Development of the CAPTURE benchmark, focusing on context-aware settings and assessing both attack detection and over-defense tendencies.

**Key Contributions:**

	1. Introduction of a new context-aware benchmark (CAPTURE) for prompt injection guardrails.
	2. Assessment of current guardrail models' performance in real-world scenarios.
	3. Identification of high false negative and false positive rates in existing guardrail models.

**Result:** Experiments show that existing guardrail models face high false negative rates in adversarial cases and excessive false positives in benign scenarios.

**Limitations:** The benchmark relies on minimal in-domain examples which might affect generalizability.

**Conclusion:** The findings underscore critical limitations in current prompt injection guardrail models and the need for improved evaluation methods.

**Abstract:** Prompt injection remains a major security risk for large language models. However, the efficacy of existing guardrail models in context-aware settings remains underexplored, as they often rely on static attack benchmarks. Additionally, they have over-defense tendencies. We introduce CAPTURE, a novel context-aware benchmark assessing both attack detection and over-defense tendencies with minimal in-domain examples. Our experiments reveal that current prompt injection guardrail models suffer from high false negatives in adversarial cases and excessive false positives in benign scenarios, highlighting critical limitations.

</details>


### [116] [From n-gram to Attention: How Model Architectures Learn and Propagate Bias in Language Modeling](https://arxiv.org/abs/2505.12381)

*Mohsinul Kabir, Tasfia Tahsin, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** bias propagation, language models, comparative behavioral theory, transformers, n-gram LMs

**Relevance Score:** 9

**TL;DR:** The paper investigates bias in language models, emphasizing the need to understand both data and model architecture in bias propagation.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically investigate the origins of bias in language models by analyzing the interaction between training data and model architecture.

**Method:** The authors propose a methodology based on comparative behavioral theory, evaluating the effects of data, model design, and temporal dynamics on bias propagation in language modeling.

**Key Contributions:**

	1. Introduces a new methodology for analyzing bias origins in language models
	2. Reveals the differential sensitivity of n-gram and transformer architectures to bias propagation
	3. Highlights the importance of temporal data provenance in bias influence.

**Result:** Findings show that n-gram LMs are sensitive to context size affecting bias propagation, whereas transformers are architecturally robust; also, the temporal provenance of training data significantly impacts bias, with different architectures responding variably to controlled bias injection.

**Limitations:** 

**Conclusion:** A holistic approach is necessary to trace bias origins in both data and model architectures to effectively mitigate harm in language models.

**Abstract:** Current research on bias in language models (LMs) predominantly focuses on data quality, with significantly less attention paid to model architecture and temporal influences of data. Even more critically, few studies systematically investigate the origins of bias. We propose a methodology grounded in comparative behavioral theory to interpret the complex interaction between training data and model architecture in bias propagation during language modeling. Building on recent work that relates transformers to n-gram LMs, we evaluate how data, model design choices, and temporal dynamics affect bias propagation. Our findings reveal that: (1) n-gram LMs are highly sensitive to context window size in bias propagation, while transformers demonstrate architectural robustness; (2) the temporal provenance of training data significantly affects bias; and (3) different model architectures respond differentially to controlled bias injection, with certain biases (e.g. sexual orientation) being disproportionately amplified. As language models become ubiquitous, our findings highlight the need for a holistic approach -- tracing bias to its origins across both data and model dimensions, not just symptoms, to mitigate harm.

</details>


### [117] [SLOT: Sample-specific Language Model Optimization at Test-time](https://arxiv.org/abs/2505.12392)

*Yang Hu, Xingyu Zhang, Xueji Fang, Zhiyang Chen, Xiao Wang, Huatian Zhang, Guojun Qi*

**Main category:** cs.CL

**Keywords:** Language Model, Test-time Optimization, Few-shot Learning

**Relevance Score:** 9

**TL;DR:** SLOT is a test-time inference approach that improves language model performance by optimizing for individual prompts, enhancing response accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing large language models often struggle with complex instructions, leading to suboptimal performance on tasks not well-represented among general samples.

**Method:** SLOT conducts few optimization steps at test-time to update a lightweight sample-specific parameter vector, which is added to the final hidden layer before the output head, allowing for efficient adaptation to incoming prompts.

**Key Contributions:**

	1. Introduces a novel test-time optimization technique for LLMs.
	2. Demonstrates significant accuracy improvements on benchmark tasks.
	3. Provides an open-source implementation for further research.

**Result:** SLOT demonstrates improved accuracy on multiple benchmarks, with Qwen2.5-7B improving from 57.54% to 66.19% on GSM8K, and DeepSeek-R1-Distill-Llama-70B achieving 68.69% on GPQA, the highest among 70B-level models.

**Limitations:** 

**Conclusion:** SLOT effectively enhances the performance of language models on complex instruction-following tasks by conducting minimal, sample-specific optimizations at test-time.

**Abstract:** We propose SLOT (Sample-specific Language Model Optimization at Test-time), a novel and parameter-efficient test-time inference approach that enhances a language model's ability to more accurately respond to individual prompts. Existing Large Language Models (LLMs) often struggle with complex instructions, leading to poor performances on those not well represented among general samples. To address this, SLOT conducts few optimization steps at test-time to update a light-weight sample-specific parameter vector. It is added to the final hidden layer before the output head, and enables efficient adaptation by caching the last layer features during per-sample optimization. By minimizing the cross-entropy loss on the input prompt only, SLOT helps the model better aligned with and follow each given instruction. In experiments, we demonstrate that our method outperforms the compared models across multiple benchmarks and LLMs. For example, Qwen2.5-7B with SLOT achieves an accuracy gain of 8.6% on GSM8K from 57.54% to 66.19%, while DeepSeek-R1-Distill-Llama-70B with SLOT achieves a SOTA accuracy of 68.69% on GPQA among 70B-level models. Our code is available at https://github.com/maple-research-lab/SLOT.

</details>


### [118] [Traversal Verification for Speculative Tree Decoding](https://arxiv.org/abs/2505.12398)

*Yepeng Weng, Qiao Hu, Xujie Chen, Li Liu, Dianwen Mei, Huishi Qiu, Jiang Tian, Zhongchao Shi*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, acceptance rates

**Relevance Score:** 8

**TL;DR:** This paper presents Traversal Verification, a novel speculative decoding algorithm that improves acceptance rates and throughput in large language models by utilizing a leaf-to-root verification approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Speculative decoding accelerates large language models but suffers from limitations in existing token-level verification methods.

**Method:** Traversal Verification employs a leaf-to-root traversal strategy, considering the acceptance of entire token sequences rather than individual tokens.

**Key Contributions:**

	1. Introduction of Traversal Verification for speculative decoding.
	2. Theoretical proof of identical probability distribution to the target model.
	3. Demonstrated improved performance metrics over existing frameworks.

**Result:** Our method shows improved acceptance length and throughput across various large language models and tasks.

**Limitations:** 

**Conclusion:** Traversal Verification guarantees lossless inference with substantial acceleration gains compared to traditional methods.

**Abstract:** Speculative decoding is a promising approach for accelerating large language models. The primary idea is to use a lightweight draft model to speculate the output of the target model for multiple subsequent timesteps, and then verify them in parallel to determine whether the drafted tokens should be accepted or rejected. To enhance acceptance rates, existing frameworks typically construct token trees containing multiple candidates in each timestep. However, their reliance on token-level verification mechanisms introduces two critical limitations: First, the probability distribution of a sequence differs from that of individual tokens, leading to suboptimal acceptance length. Second, current verification schemes begin from the root node and proceed layer by layer in a top-down manner. Once a parent node is rejected, all its child nodes should be discarded, resulting in inefficient utilization of speculative candidates. This paper introduces Traversal Verification, a novel speculative decoding algorithm that fundamentally rethinks the verification paradigm through leaf-to-root traversal. Our approach considers the acceptance of the entire token sequence from the current node to the root, and preserves potentially valid subsequences that would be prematurely discarded by existing methods. We theoretically prove that the probability distribution obtained through Traversal Verification is identical to that of the target model, guaranteeing lossless inference while achieving substantial acceleration gains. Experimental results across different large language models and multiple tasks show that our method consistently improves acceptance length and throughput over existing methods

</details>


### [119] [The power of text similarity in identifying AI-LLM paraphrased documents: The case of BBC news articles and ChatGPT](https://arxiv.org/abs/2505.12405)

*Konstantinos Xylogiannopoulos, Petros Xanthopoulos, Panagiotis Karampelas, Georgios Bakamitsos*

**Main category:** cs.CL

**Keywords:** Generative AI, Paraphrasing, Copyright Infringement, ChatGPT, Pattern Similarity Detection

**Relevance Score:** 6

**TL;DR:** The paper presents a method for detecting AI paraphrased articles, specifically identifying those generated by ChatGPT, using a pattern-based similarity detection algorithm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing issue of copyright infringement caused by generative AI paraphrased content, which negatively impacts original content creators.

**Method:** The study employs a pattern-based similarity detection algorithm to identify articles paraphrased by AI, specifically targeting content created by ChatGPT, tested on a benchmark dataset of real and paraphrased articles.

**Key Contributions:**

	1. Introduced a pattern-based similarity detection algorithm for AI paraphrased content.
	2. Developed a benchmark dataset for evaluating AI paraphrase detection.
	3. Demonstrated high detection accuracy without using deep learning techniques.

**Result:** The method achieved high detection accuracy with 96.23% accuracy, 96.25% precision, 96.21% sensitivity, 96.25% specificity, and 96.23% F1 score on the benchmark dataset.

**Limitations:** 

**Conclusion:** The proposed detection algorithm effectively identifies AI paraphrased articles without deep learning, highlighting the potential to mitigate copyright issues in news content.

**Abstract:** Generative AI paraphrased text can be used for copyright infringement and the AI paraphrased content can deprive substantial revenue from original content creators. Despite this recent surge of malicious use of generative AI, there are few academic publications that research this threat. In this article, we demonstrate the ability of pattern-based similarity detection for AI paraphrased news recognition. We propose an algorithmic scheme, which is not limited to detect whether an article is an AI paraphrase, but, more importantly, to identify that the source of infringement is the ChatGPT. The proposed method is tested with a benchmark dataset specifically created for this task that incorporates real articles from BBC, incorporating a total of 2,224 articles across five different news categories, as well as 2,224 paraphrased articles created with ChatGPT. Results show that our pattern similarity-based method, that makes no use of deep learning, can detect ChatGPT assisted paraphrased articles at percentages 96.23% for accuracy, 96.25% for precision, 96.21% for sensitivity, 96.25% for specificity and 96.23% for F1 score.

</details>


### [120] [Table-R1: Region-based Reinforcement Learning for Table Understanding](https://arxiv.org/abs/2505.12415)

*Zhenhe Wu, Jian Yang, Jiaheng Liu, Xianjie Wu, Changzai Pan, Jie Zhang, Yu Zhao, Shuangyong Song, Yongxiang Li, Zhoujun Li*

**Main category:** cs.CL

**Keywords:** language models, table reasoning, reinforcement learning, human-computer interaction, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Table-R1, a reinforcement learning approach that improves LLM's comprehension of tables by integrating region evidence into reasoning steps for enhanced table question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Optimizing large language models for effective table question answering is underexplored despite their potential in this area.

**Method:** The method uses a region-based approach called Table-R1, which includes Region-Enhanced Supervised Fine-Tuning (RE-SFT) and Table-Aware Group Relative Policy Optimization (TARPO) to improve LLM performance on table questions by guiding models in identifying relevant table regions and balancing rewards.

**Key Contributions:**

	1. Introduction of the region-based Table-R1 approach.
	2. Incorporation of RE-SFT and TARPO methods into LLM table reasoning.
	3. Achieving notable performance improvements and efficiency in response generation.

**Result:** Table-R1 shows an average performance improvement of 14.36 points on benchmark datasets and reduces token consumption by 67.5% compared to previous methods.

**Limitations:** 

**Conclusion:** The integration of region evidence and a mixed reward system significantly enhances LLM capabilities in tabular reasoning.

**Abstract:** Tables present unique challenges for language models due to their structured row-column interactions, necessitating specialized approaches for effective comprehension. While large language models (LLMs) have demonstrated potential in table reasoning through prompting and techniques like chain-of-thought (CoT) and program-of-thought (PoT), optimizing their performance for table question answering remains underexplored. In this paper, we introduce region-based Table-R1, a novel reinforcement learning approach that enhances LLM table understanding by integrating region evidence into reasoning steps. Our method employs Region-Enhanced Supervised Fine-Tuning (RE-SFT) to guide models in identifying relevant table regions before generating answers, incorporating textual, symbolic, and program-based reasoning. Additionally, Table-Aware Group Relative Policy Optimization (TARPO) introduces a mixed reward system to dynamically balance region accuracy and answer correctness, with decaying region rewards and consistency penalties to align reasoning steps. Experiments show that Table-R1 achieves an average performance improvement of 14.36 points across multiple base models on three benchmark datasets, even outperforming baseline models with ten times the parameters, while TARPO reduces response token consumption by 67.5% compared to GRPO, significantly advancing LLM capabilities in efficient tabular reasoning.

</details>


### [121] [PSC: Extending Context Window of Large Language Models via Phase Shift Calibration](https://arxiv.org/abs/2505.12423)

*Wenqiao Zhu, Chao Xu, Lulu Wang, Jun Wu*

**Main category:** cs.CL

**Keywords:** Rotary Position Embedding, Phase Shift Calibration, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces Phase Shift Calibration (PSC), a module to enhance Rotary Position Embedding (RoPE) in large language models by optimizing the predefined frequency factors for better calibration and improved performance in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for enhancing context windows based on RoPE struggle with predefined factor selection due to the complexity of the search space.

**Method:** The authors introduce PSC, which calibrates predefined frequencies from existing methods to improve their performance in extending context windows.

**Key Contributions:**

	1. Introduction of Phase Shift Calibration (PSC) module for RoPE optimization.
	2. Demonstration of PSC's effectiveness on multiple models and tasks.
	3. Showcasing improvements in perplexity with increased context window sizes.

**Result:** Experiments show that using PSC leads to decreased perplexity as the context window size increases, indicating improved performance and robustness across several models and tasks.

**Limitations:** 

**Conclusion:** PSC significantly enhances the efficacy of various existing methods for context expansion in language models, demonstrating broad applicability and robustness.

**Abstract:** Rotary Position Embedding (RoPE) is an efficient position encoding approach and is widely utilized in numerous large language models (LLMs). Recently, a lot of methods have been put forward to further expand the context window based on RoPE. The core concept of those methods is to predefine or search for a set of factors to rescale the base frequencies of RoPE. Nevertheless, it is quite a challenge for existing methods to predefine an optimal factor due to the exponential search space. In view of this, we introduce PSC (Phase Shift Calibration), a small module for calibrating the frequencies predefined by existing methods. With the employment of PSC, we demonstrate that many existing methods can be further enhanced, like PI, YaRN, and LongRoPE. We conducted extensive experiments across multiple models and tasks. The results demonstrate that (1) when PSC is enabled, the comparative reductions in perplexity increase as the context window size is varied from 16k, to 32k, and up to 64k. (2) Our approach is broadly applicable and exhibits robustness across a variety of models and tasks. The code can be found at https://github.com/WNQzhu/PSC.

</details>


### [122] [Learning to Play Like Humans: A Framework for LLM Adaptation in Interactive Fiction Games](https://arxiv.org/abs/2505.12439)

*Jinming Zhang, Yunfei Long*

**Main category:** cs.CL

**Keywords:** Interactive Fiction, Large Language Models, Cognitive Science, Narrative Context, Game AI

**Relevance Score:** 8

**TL;DR:** This paper introduces the LPLH framework, which helps Large Language Models play Interactive Fiction games more like humans by learning narrative context, action commands, and improving decision-making through feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of AI agents in Interactive Fiction games by focusing on human-like comprehension instead of just task-specific performance.

**Method:** The LPLH framework consists of structured map building for narrative relationships, action learning for context-appropriate commands, and feedback-driven experience analysis for refining decision-making.

**Key Contributions:**

	1. Introduction of a cognitive-inspired framework for IF games
	2. Integration of structured map building and action learning
	3. Feedback-driven experience refinement for decision-making

**Result:** The framework enables LLM-based agents to exhibit more interpretable and human-like behaviors, improving their interaction within narrative worlds.

**Limitations:** 

**Conclusion:** By integrating cognitive science principles, the LPLH framework supports robust, context-aware gameplay in complex text-based environments.

**Abstract:** Interactive Fiction games (IF games) are where players interact through natural language commands. While recent advances in Artificial Intelligence agents have reignited interest in IF games as a domain for studying decision-making, existing approaches prioritize task-specific performance metrics over human-like comprehension of narrative context and gameplay logic. This work presents a cognitively inspired framework that guides Large Language Models (LLMs) to learn and play IF games systematically. Our proposed **L**earning to **P**lay **L**ike **H**umans (LPLH) framework integrates three key components: (1) structured map building to capture spatial and narrative relationships, (2) action learning to identify context-appropriate commands, and (3) feedback-driven experience analysis to refine decision-making over time. By aligning LLMs-based agents' behavior with narrative intent and commonsense constraints, LPLH moves beyond purely exploratory strategies to deliver more interpretable, human-like performance. Crucially, this approach draws on cognitive science principles to more closely simulate how human players read, interpret, and respond within narrative worlds. As a result, LPLH reframes the IF games challenge as a learning problem for LLMs-based agents, offering a new path toward robust, context-aware gameplay in complex text-based environments.

</details>


### [123] [Introspective Growth: Automatically Advancing LLM Expertise in Technology Judgment](https://arxiv.org/abs/2505.12452)

*Siyang Wu, Honglin Bao, Nadav Kunievsky, James A. Evans*

**Main category:** cs.CL

**Keywords:** large language models, self-questioning, machine learning, computer science patents, understanding

**Relevance Score:** 8

**TL;DR:** The paper introduces self-questioning as a method to enhance large language models' understanding by leveraging their latent knowledge, evaluated through a benchmark of computer science patents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conceptual understanding in large language models (LLMs) and improve their access to latent knowledge, particularly in complex domains.

**Method:** The study proposes using self-questioning as a prompting technique, coupled with a benchmark task involving differentiation between closely related patents, while evaluating LLM performance with and without external scientific text.

**Key Contributions:**

	1. Introduction of self-questioning as a mechanism for LLM comprehension improvement
	2. Development of a benchmark using computer science patents for evaluation
	3. Discovery of effective cross-model collaboration strategies

**Result:** Self-questioning significantly enhances LLM performance in understanding fine-grained semantic distinctions, and retrieving answers from external texts provides further improvements.

**Limitations:** 

**Conclusion:** Self-questioning is a practical approach for improving LLM comprehension and revealing how internal knowledge is organized; smaller models are found to generate better questions for mid-sized models.

**Abstract:** Large language models (LLMs) increasingly demonstrate signs of conceptual understanding, yet much of their internal knowledge remains latent, loosely structured, and difficult to access or evaluate. We propose self-questioning as a lightweight and scalable strategy to improve LLMs' understanding, particularly in domains where success depends on fine-grained semantic distinctions. To evaluate this approach, we introduce a challenging new benchmark of 1.3 million post-2015 computer science patent pairs, characterized by dense technical jargon and strategically complex writing. The benchmark centers on a pairwise differentiation task: can a model distinguish between closely related but substantively different inventions? We show that prompting LLMs to generate and answer their own questions - targeting the background knowledge required for the task - significantly improves performance. These self-generated questions and answers activate otherwise underutilized internal knowledge. Allowing LLMs to retrieve answers from external scientific texts further enhances performance, suggesting that model knowledge is compressed and lacks the full richness of the training data. We also find that chain-of-thought prompting and self-questioning converge, though self-questioning remains more effective for improving understanding of technical concepts. Notably, we uncover an asymmetry in prompting: smaller models often generate more fundamental, more open-ended, better-aligned questions for mid-sized models than large models with better understanding do, revealing a new strategy for cross-model collaboration. Altogether, our findings establish self-questioning as both a practical mechanism for automatically improving LLM comprehension, especially in domains with sparse and underrepresented knowledge, and a diagnostic probe of how internal and external knowledge are organized.

</details>


### [124] [Towards DS-NER: Unveiling and Addressing Latent Noise in Distant Annotations](https://arxiv.org/abs/2505.12454)

*Yuyang Ding, Dan Qiao, Juntao Li, Jiajie Xu, Pingfu Chao, Xiaofang Zhou, Min Zhang*

**Main category:** cs.CL

**Keywords:** Distant Supervision, Named Entity Recognition, Noise Assessment, Large Language Models, Annotation Techniques

**Relevance Score:** 6

**TL;DR:** This paper explores the effectiveness of Distantly Supervised Named Entity Recognition (DS-NER) using various annotation methods and presents a novel framework for noise assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of DS-NER in generating training data automatically and to address the issue of noise in distant supervision methods.

**Method:** The authors analyze distant annotation techniques, including both rule-based methods and large language model supervision, while introducing a framework that categorizes noise into unlabeled-entity and noisy-entity problems to provide tailored solutions.

**Key Contributions:**

	1. Introduction of a novel framework for noise assessment in DS-NER techniques.
	2. Comparative analysis of traditional and large language model-based annotation methods.
	3. Demonstration of significant performance improvements across diverse datasets.

**Result:** The proposed method shows significant improvements across eight real-world distant supervision datasets, outperforming state-of-the-art approaches in DS-NER.

**Limitations:** 

**Conclusion:** The study confirms the robustness and effectiveness of the proposed DS-NER method and noise assessment framework, suggesting its potential for broader applications in named entity recognition.

**Abstract:** Distantly supervised named entity recognition (DS-NER) has emerged as a cheap and convenient alternative to traditional human annotation methods, enabling the automatic generation of training data by aligning text with external resources. Despite the many efforts in noise measurement methods, few works focus on the latent noise distribution between different distant annotation methods. In this work, we explore the effectiveness and robustness of DS-NER by two aspects: (1) distant annotation techniques, which encompasses both traditional rule-based methods and the innovative large language model supervision approach, and (2) noise assessment, for which we introduce a novel framework. This framework addresses the challenges by distinctly categorizing them into the unlabeled-entity problem (UEP) and the noisy-entity problem (NEP), subsequently providing specialized solutions for each. Our proposed method achieves significant improvements on eight real-world distant supervision datasets originating from three different data sources and involving four distinct annotation techniques, confirming its superiority over current state-of-the-art methods.

</details>


### [125] [What are they talking about? Benchmarking Large Language Models for Knowledge-Grounded Discussion Summarization](https://arxiv.org/abs/2505.12474)

*Weixiao Zhou, Junnan Zhu, Gengyao Li, Xianfu Cheng, Xinnian Liang, Feifei Zhai, Zhoujun Li*

**Main category:** cs.CL

**Keywords:** LLMs, summarization, evaluation framework, background knowledge, opinion summaries

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of LLMs on a summarization task that integrates discussion with background knowledge, highlighting significant limitations in their ability to generate comprehensive summaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address confusion in dialogue summarization caused by reliance only on discussion information, this work aims to combine discussion with background knowledge in summarization tasks.

**Method:** The paper models task output as background and opinion summaries, introduces standardized summarization patterns, and establishes a benchmark with samples annotated by human experts, alongside a novel hierarchical evaluation framework.

**Key Contributions:**

	1. Introduction of a novel summarization task combining background knowledge and discussion
	2. Development of a benchmark with high-quality human-annotated samples
	3. Proposal of a hierarchical evaluation framework with fine-grained metrics

**Result:** Findings indicate LLMs struggle with background summary retrieval and opinion summary integration, with top models achieving less than 69% performance across patterns and showing insufficient self-evaluation/correction capabilities.

**Limitations:** Current LLMs demonstrate inadequate capabilities for self-evaluation and self-correction in the summarization task.

**Conclusion:** The results highlight the need for improvements in LLMs to enhance their summarization abilities, especially in integrating background knowledge and handling complex summarization tasks.

**Abstract:** In this work, we investigate the performance of LLMs on a new task that requires combining discussion with background knowledge for summarization. This aims to address the limitation of outside observer confusion in existing dialogue summarization systems due to their reliance solely on discussion information. To achieve this, we model the task output as background and opinion summaries and define two standardized summarization patterns. To support assessment, we introduce the first benchmark comprising high-quality samples consistently annotated by human experts and propose a novel hierarchical evaluation framework with fine-grained, interpretable metrics. We evaluate 12 LLMs under structured-prompt and self-reflection paradigms. Our findings reveal: (1) LLMs struggle with background summary retrieval, generation, and opinion summary integration. (2) Even top LLMs achieve less than 69% average performance across both patterns. (3) Current LLMs lack adequate self-evaluation and self-correction capabilities for this task.

</details>


### [126] [Enhancing Large Language Models with Reward-guided Tree Search for Knowledge Graph Question and Answering](https://arxiv.org/abs/2505.12476)

*Xiao Long, Liansheng Zhuang, Chen Shen, Shaotian Yan, Yifei Li, Shafei Wang*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Question Answering, Large Language Models, Monte Carlo Tree Search, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents the Reward-guided Tree Search on Graph (RTSoG), a novel framework that improves Knowledge Graph Question Answering (KGQA) by effectively handling complex question semantics and optimizing reasoning paths.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing KGQA methods overlook historical reasoning paths and often retrieve inaccurate paths due to complex question semantics, leading to sub-optimal performance.

**Method:** The RTSoG framework decomposes questions into simpler sub-questions and employs a Self-Critic Monte Carlo Tree Search guided by a reward model to retrieve and weight reasoning paths, generating answers from these paths.

**Key Contributions:**

	1. Introduction of Reward-guided Tree Search framework for KGQA
	2. Utilization of Self-Critic Monte Carlo Tree Search for path retrieval
	3. Significant performance improvements on benchmark datasets.

**Result:** RTSoG achieves 8.7% and 7.0% performance improvements over state-of-the-art methods on the GrailQA and WebQSP datasets respectively.

**Limitations:** 

**Conclusion:** RTSoG effectively addresses the limitations of previous KGQA approaches by improving question handling and reasoning path retrieval, leading to enhanced performance in answering complex queries.

**Abstract:** Recently, large language models (LLMs) have demonstrated impressive performance in Knowledge Graph Question Answering (KGQA) tasks, which aim to find answers based on knowledge graphs (KGs) for natural language questions. Existing LLMs-based KGQA methods typically follow the Graph Retrieval-Augmented Generation (GraphRAG) paradigm, which first retrieves reasoning paths from the large KGs, and then generates the answers based on them. However, these methods emphasize the exploration of new optimal reasoning paths in KGs while ignoring the exploitation of historical reasoning paths, which may lead to sub-optimal reasoning paths. Additionally, the complex semantics contained in questions may lead to the retrieval of inaccurate reasoning paths. To address these issues, this paper proposes a novel and training-free framework for KGQA tasks called Reward-guided Tree Search on Graph (RTSoG). RTSoG decomposes an original question into a series of simpler and well-defined sub-questions to handle the complex semantics. Then, a Self-Critic Monte Carlo Tree Search (SC-MCTS) guided by a reward model is introduced to iteratively retrieve weighted reasoning paths as contextual knowledge. Finally, it stacks the weighted reasoning paths according to their weights to generate the final answers. Extensive experiments on four datasets demonstrate the effectiveness of RTSoG. Notably, it achieves 8.7\% and 7.0\% performance improvement over the state-of-the-art method on the GrailQA and the WebQSP respectively.

</details>


### [127] [KG-QAGen: A Knowledge-Graph-Based Framework for Systematic Question Generation and Long-Context LLM Evaluation](https://arxiv.org/abs/2505.12495)

*Nikita Tatarinov, Vidhyakshaya Kannan, Haricharana Srinivasa, Arnav Raj, Harpreet Singh Anand, Varun Singh, Aditya Luthra, Ravij Lade, Agam Shah, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** long-context, language models, question-answer generation

**Relevance Score:** 8

**TL;DR:** This paper introduces KG-QAGen, a framework for generating question-answer pairs of varying complexity to evaluate long-context language models, revealing their limitations in handling complex queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a systematic evaluation of language models' long-context capabilities, particularly regarding question complexity.

**Method:** The framework extracts QA pairs at multiple complexity levels and leverages structured representations of financial agreements across dimensions like multi-hop retrieval and answer plurality.

**Key Contributions:**

	1. Introduction of KG-QAGen framework for QA generation
	2. Creation of the largest QA dataset among long-context benchmarks
	3. Insights into failure modes of existing LLMs in complex query handling

**Result:** A dataset of 20,139 QA pairs was created, and evaluation of 13 LLMs showed struggles in set-based comparisons and multi-hop inference.

**Limitations:** The open-sourced part of the dataset is only a portion of the entire dataset, potentially limiting broader applicability.

**Conclusion:** The analysis indicates that even top-performing models face systematic failures in semantic interpretation and handling implicit relations.

**Abstract:** The increasing context length of modern language models has created a need for evaluating their ability to retrieve and process information across extensive documents. While existing benchmarks test long-context capabilities, they often lack a structured way to systematically vary question complexity. We introduce KG-QAGen (Knowledge-Graph-based Question-Answer Generation), a framework that (1) extracts QA pairs at multiple complexity levels (2) by leveraging structured representations of financial agreements (3) along three key dimensions -- multi-hop retrieval, set operations, and answer plurality -- enabling fine-grained assessment of model performance across controlled difficulty levels. Using this framework, we construct a dataset of 20,139 QA pairs (the largest number among the long-context benchmarks) and open-source a part of it. We evaluate 13 proprietary and open-source LLMs and observe that even the best-performing models are struggling with set-based comparisons and multi-hop logical inference. Our analysis reveals systematic failure modes tied to semantic misinterpretation and inability to handle implicit relations.

</details>


### [128] [LM$^2$otifs : An Explainable Framework for Machine-Generated Texts Detection](https://arxiv.org/abs/2505.12507)

*Xu Zheng, Zhuomin Chen, Esteban Schafir, Sipeng Chen, Hojat Allah Salehi, Haifeng Chen, Farhad Shirani, Wei Cheng, Dongsheng Luo*

**Main category:** cs.CL

**Keywords:** explainability, large language models, machine-generated texts, graph neural networks, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces LM$^2$otifs, an explainable framework for detecting machine-generated texts using Graph Neural Networks and interpretable motifs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in authorship authentication due to the impressive capabilities of large language models, especially concerning the explainability of detection methods.

**Method:** The LM$^2$otifs framework transforms text into graphs based on word co-occurrence, employs Graph Neural Networks for predictions, and uses a post-hoc method to extract interpretable motifs.

**Key Contributions:**

	1. Introduction of LM$^2$otifs framework for MGT detection
	2. Utilization of explainable Graph Neural Networks for better interpretability
	3. Empirical evaluation showcasing its effectiveness on benchmark datasets

**Result:** LM$^2$otifs demonstrates effective detection of machine-generated texts, outperforming traditional methods in explainability and interpretability with empirical evidence supporting its effectiveness.

**Limitations:** 

**Conclusion:** The framework not only effectively distinguishes between human and machine-generated texts but also provides valuable multi-level explanations in the form of linguistic motifs.

**Abstract:** The impressive ability of large language models to generate natural text across various tasks has led to critical challenges in authorship authentication. Although numerous detection methods have been developed to differentiate between machine-generated texts (MGT) and human-generated texts (HGT), the explainability of these methods remains a significant gap. Traditional explainability techniques often fall short in capturing the complex word relationships that distinguish HGT from MGT. To address this limitation, we present LM$^2$otifs, a novel explainable framework for MGT detection. Inspired by probabilistic graphical models, we provide a theoretical rationale for the effectiveness. LM$^2$otifs utilizes eXplainable Graph Neural Networks to achieve both accurate detection and interpretability. The LM$^2$otifs pipeline operates in three key stages: first, it transforms text into graphs based on word co-occurrence to represent lexical dependencies; second, graph neural networks are used for prediction; and third, a post-hoc explainability method extracts interpretable motifs, offering multi-level explanations from individual words to sentence structures. Extensive experiments on multiple benchmark datasets demonstrate the comparable performance of LM$^2$otifs. The empirical evaluation of the extracted explainable motifs confirms their effectiveness in differentiating HGT and MGT. Furthermore, qualitative analysis reveals distinct and visible linguistic fingerprints characteristic of MGT.

</details>


### [129] [DS-ProGen: A Dual-Structure Deep Language Model for Functional Protein Design](https://arxiv.org/abs/2505.12511)

*Yanting Li, Jiyue Jiang, Zikang Wang, Ziqian Lin, Dongchen He, Yuheng Shan, Yanruisheng Shao, Jiayi Li, Xiangyu Shi, Jiuming Wang, Yanyu Chen, Yimin Fan, Han Li, Yu Li*

**Main category:** cs.CL

**Keywords:** protein folding, deep learning, structural biology, functional design, machine learning

**Relevance Score:** 4

**TL;DR:** DS-ProGen is a dual-structure deep language model for functional protein design that integrates backbone geometry and surface-level representations to improve sequence prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to overcome the limitations of existing protein design methods that rely on single structural features, which restrict their effectiveness in accurately predicting amino acid sequences for specific 3D conformations.

**Method:** The authors developed DS-ProGen, which employs both backbone coordinates and surface chemical and geometric descriptors within a next-amino-acid prediction framework to generate viable protein sequences.

**Key Contributions:**

	1. Introduction of DS-ProGen, a novel dual-structure deep language model for protein design.
	2. Demonstration of the benefits of integrating backbone coordinates with surface descriptors.
	3. Achievement of state-of-the-art results in protein sequence recovery and interaction prediction.

**Result:** DS-ProGen achieves a state-of-the-art recovery rate of 61.47% on the PRIDE dataset and demonstrates superior capability in predicting interactions with various biological partners.

**Limitations:** 

**Conclusion:** The findings suggest that a multi-modal approach to structural encoding significantly enhances the effectiveness of protein design by capturing complex chemical and geometric constraints.

**Abstract:** Inverse Protein Folding (IPF) is a critical subtask in the field of protein design, aiming to engineer amino acid sequences capable of folding correctly into a specified three-dimensional (3D) conformation. Although substantial progress has been achieved in recent years, existing methods generally rely on either backbone coordinates or molecular surface features alone, which restricts their ability to fully capture the complex chemical and geometric constraints necessary for precise sequence prediction. To address this limitation, we present DS-ProGen, a dual-structure deep language model for functional protein design, which integrates both backbone geometry and surface-level representations. By incorporating backbone coordinates as well as surface chemical and geometric descriptors into a next-amino-acid prediction paradigm, DS-ProGen is able to generate functionally relevant and structurally stable sequences while satisfying both global and local conformational constraints. On the PRIDE dataset, DS-ProGen attains the current state-of-the-art recovery rate of 61.47%, demonstrating the synergistic advantage of multi-modal structural encoding in protein design. Furthermore, DS-ProGen excels in predicting interactions with a variety of biological partners, including ligands, ions, and RNA, confirming its robust functional retention capabilities.

</details>


### [130] [ESC-Judge: A Framework for Comparing Emotional Support Conversational Agents](https://arxiv.org/abs/2505.12531)

*Navid Madani, Rohini Srihari*

**Main category:** cs.CL

**Keywords:** large language models, mental health, evaluation framework, automated assessment, emotional support

**Relevance Score:** 9

**TL;DR:** ESC-Judge is an end-to-end evaluation framework for emotional-support LLMs that automates performance comparison based on the Exploration-Insight-Action counseling model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a scalable and theory-grounded method for evaluating the effectiveness of emotional-support LLMs used in mental health applications.

**Method:** ESC-Judge evaluates models in three stages: synthesizing help-seeker roles based on salient attributes, conducting sessions with different support agents, and using a specialized judge LLM for pairwise skill comparisons.

**Key Contributions:**

	1. First end-to-end evaluation framework for emotional-support LLMs
	2. Automates evaluation pipeline at scale
	3. Grounds comparisons in an established counseling model

**Result:** ESC-Judge achieved 85% agreement with PhD-level annotators on Exploration, 83% on Insight, and 86% on Action decisions, indicating high reliability.

**Limitations:** 

**Conclusion:** ESC-Judge provides a cost-effective, automated evaluation method that aligns closely with human judgments, promoting transparency in developing emotionally supportive AI.

**Abstract:** Large language models (LLMs) increasingly power mental-health chatbots, yet the field still lacks a scalable, theory-grounded way to decide which model is most effective to deploy. We present ESC-Judge, the first end-to-end evaluation framework that (i) grounds head-to-head comparisons of emotional-support LLMs in Clara Hill's established Exploration-Insight-Action counseling model, providing a structured and interpretable view of performance, and (ii) fully automates the evaluation pipeline at scale. ESC-Judge operates in three stages: first, it synthesizes realistic help-seeker roles by sampling empirically salient attributes such as stressors, personality, and life history; second, it has two candidate support agents conduct separate sessions with the same role, isolating model-specific strategies; and third, it asks a specialized judge LLM to express pairwise preferences across rubric-anchored skills that span the Exploration, Insight, and Action spectrum. In our study, ESC-Judge matched PhD-level annotators on 85 percent of Exploration, 83 percent of Insight, and 86 percent of Action decisions, demonstrating human-level reliability at a fraction of the cost. All code, prompts, synthetic roles, transcripts, and judgment scripts are released to promote transparent progress in emotionally supportive AI.

</details>


### [131] [Relation Extraction or Pattern Matching? Unravelling the Generalisation Limits of Language Models for Biographical RE](https://arxiv.org/abs/2505.12533)

*Varvara Arzt, Allan Hanbury, Michael Wiegand, Gábor Recski, Terra Blevins*

**Main category:** cs.CL

**Keywords:** relation extraction, generalization, data quality, transfer learning, few-shot learning

**Relevance Score:** 5

**TL;DR:** This paper investigates the generalization abilities of relation extraction models, revealing that they often overfit to dataset-specific patterns rather than learning robust relationships, with implications for transfer learning and adaptation strategies.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well relation extraction models generalize to unseen data and determine the factors that influence their performance across different datasets.

**Method:** Cross-dataset experiments were conducted to assess the performance of relation extraction models under various conditions, considering factors like data quality and adaptation strategies.

**Key Contributions:**

	1. Identify the relationship between intra-dataset performance and transferability of relation extraction models.
	2. Demonstrate the importance of data quality over lexical similarity in achieving robust performance.
	3. Highlight structural issues in relation extraction benchmarks that affect model transferability.

**Result:** The experiments demonstrated that higher intra-dataset performance does not guarantee better transferability and highlighted the significance of data quality for robust model performance.

**Limitations:** The findings may not generalize to all relation extraction contexts and depend on the specific datasets used in the study.

**Conclusion:** The study concludes that adapting to data quality is crucial for effective model performance, and structural issues in benchmarks detract from transferability. Fine-tuning is preferable for high-quality data while few-shot in-context learning works better for noisier data.

**Abstract:** Analysing the generalisation capabilities of relation extraction (RE) models is crucial for assessing whether they learn robust relational patterns or rely on spurious correlations. Our cross-dataset experiments find that RE models struggle with unseen data, even within similar domains. Notably, higher intra-dataset performance does not indicate better transferability, instead often signaling overfitting to dataset-specific artefacts. Our results also show that data quality, rather than lexical similarity, is key to robust transfer, and the choice of optimal adaptation strategy depends on the quality of data available: while fine-tuning yields the best cross-dataset performance with high-quality data, few-shot in-context learning (ICL) is more effective with noisier data. However, even in these cases, zero-shot baselines occasionally outperform all cross-dataset results. Structural issues in RE benchmarks, such as single-relation per sample constraints and non-standardised negative class definitions, further hinder model transferability.

</details>


### [132] [Disambiguation in Conversational Question Answering in the Era of LLM: A Survey](https://arxiv.org/abs/2505.12543)

*Md Mehrab Tanjim, Yeonjun In, Xiang Chen, Victor S. Bursztyn, Ryan A. Rossi, Sungchul Kim, Guang-Jie Ren, Vaishnavi Muppala, Shun Jiang, Yongsung Kim, Chanyoung Park*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, ambiguity, Large Language Models, Conversational Question Answering, disambiguation

**Relevance Score:** 9

**TL;DR:** This paper reviews ambiguity in NLP and its implications for LLMs, especially in Conversational Question Answering (CQA), proposing disambiguation approaches and future research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Ambiguity is a significant challenge in NLP, worsened by the complexity of human language and the new capabilities of LLMs. Understanding and resolving ambiguity is crucial for improved language systems.

**Method:** The paper categorizes disambiguation approaches enabled by LLMs, provides a comparative analysis of their advantages and disadvantages, and explores datasets for benchmarking ambiguity detection and resolution.

**Key Contributions:**

	1. Comprehensive categorization of ambiguity in NLP and its implications for LLMs.
	2. Comparative analysis of various disambiguation techniques.
	3. Identification of open problems and future research directions.

**Result:** A comprehensive review of current research on ambiguities and disambiguation is presented, highlighting the pros and cons of various approaches and available datasets.

**Limitations:** 

**Conclusion:** The findings aim to guide ongoing research and improve LLM performance in ambiguity resolution, suggesting areas for future investigation.

**Abstract:** Ambiguity remains a fundamental challenge in Natural Language Processing (NLP) due to the inherent complexity and flexibility of human language. With the advent of Large Language Models (LLMs), addressing ambiguity has become even more critical due to their expanded capabilities and applications. In the context of Conversational Question Answering (CQA), this paper explores the definition, forms, and implications of ambiguity for language driven systems, particularly in the context of LLMs. We define key terms and concepts, categorize various disambiguation approaches enabled by LLMs, and provide a comparative analysis of their advantages and disadvantages. We also explore publicly available datasets for benchmarking ambiguity detection and resolution techniques and highlight their relevance for ongoing research. Finally, we identify open problems and future research directions, proposing areas for further investigation. By offering a comprehensive review of current research on ambiguities and disambiguation with LLMs, we aim to contribute to the development of more robust and reliable language systems.

</details>


### [133] [Towards Reliable and Interpretable Traffic Crash Pattern Prediction and Safety Interventions Using Customized Large Language Models](https://arxiv.org/abs/2505.12545)

*Yang Zhao, Pu Wang, Yibo Zhao, Hongru Du, Hao, Yang*

**Main category:** cs.CL

**Keywords:** traffic safety, crash prediction, large language models, feature attribution, multi-modal data

**Relevance Score:** 9

**TL;DR:** TrafficSafe introduces a multi-modal framework leveraging LLMs for improved crash prediction and feature attribution, achieving significant performance gains and providing insights into traffic safety.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance traffic safety by accurately predicting crash events and identifying risk factors through a sophisticated analysis of diverse traffic data.

**Method:** Developed TrafficSafe, a framework that utilizes LLMs for text-based reasoning on a multi-modal traffic crash dataset, and established TrafficSafe Attribution for feature interpretation.

**Key Contributions:**

	1. Introduction of TrafficSafe framework for crash prediction using LLMs
	2. Development of TrafficSafe Attribution for feature interpretation
	3. Insights into drivers' behavior as major crash risk factors

**Result:** TrafficSafe LLM achieved a 42% average improvement in F1-score over baselines, revealing key factors like alcohol-impaired driving as major contributors to severe crashes.

**Limitations:** 

**Conclusion:** TrafficSafe represents a significant advancement in traffic safety research, applying AI technologies for actionable insights and enhanced data collection strategies.

**Abstract:** Predicting crash events is crucial for understanding crash distributions and their contributing factors, thereby enabling the design of proactive traffic safety policy interventions. However, existing methods struggle to interpret the complex interplay among various sources of traffic crash data, including numeric characteristics, textual reports, crash imagery, environmental conditions, and driver behavior records. As a result, they often fail to capture the rich semantic information and intricate interrelationships embedded in these diverse data sources, limiting their ability to identify critical crash risk factors. In this research, we propose TrafficSafe, a framework that adapts LLMs to reframe crash prediction and feature attribution as text-based reasoning. A multi-modal crash dataset including 58,903 real-world reports together with belonged infrastructure, environmental, driver, and vehicle information is collected and textualized into TrafficSafe Event Dataset. By customizing and fine-tuning LLMs on this dataset, the TrafficSafe LLM achieves a 42% average improvement in F1-score over baselines. To interpret these predictions and uncover contributing factors, we introduce TrafficSafe Attribution, a sentence-level feature attribution framework enabling conditional risk analysis. Findings show that alcohol-impaired driving is the leading factor in severe crashes, with aggressive and impairment-related behaviors having nearly twice the contribution for severe crashes compared to other driver behaviors. Furthermore, TrafficSafe Attribution highlights pivotal features during model training, guiding strategic crash data collection for iterative performance improvements. The proposed TrafficSafe offers a transformative leap in traffic safety research, providing a blueprint for translating advanced AI technologies into responsible, actionable, and life-saving outcomes.

</details>


### [134] [Extracting memorized pieces of (copyrighted) books from open-weight language models](https://arxiv.org/abs/2505.12546)

*A. Feder Cooper, Aaron Gokaslan, Amy B. Cyphert, Christopher De Sa, Mark A. Lemley, Daniel E. Ho, Percy Liang*

**Main category:** cs.CL

**Keywords:** large language models, copyright law, memorization, generative AI, adversarial ML

**Relevance Score:** 8

**TL;DR:** The paper examines the extent to which large language models (LLMs) have memorized copyrighted texts, revealing complex relationships between memorization and copyright implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the misconceptions surrounding the memorization of copyrighted content by LLMs in the context of copyright lawsuits.

**Method:** The authors employ a probabilistic extraction technique to analyze 13 open-weight LLMs, extracting text from the Books3 dataset across different models.

**Key Contributions:**

	1. Development of a probabilistic extraction method applied to LLMs
	2. Empirical evidence of varying memorization capacities across LLMs
	3. Insights into the impact of LLM memorization on copyright law

**Result:** The study demonstrates that while some LLMs, like Llama 3.1 70B, can memorize significant portions of certain books, the overall extent of memorization varies widely among different models and texts.

**Limitations:** The results are model and text dependent, and further exploration is needed to generalize findings to other contexts or models.

**Conclusion:** The findings have important implications for copyright litigation, challenging the binary interpretations of memorization in copyright cases without aligning strictly with either plaintiffs or defendants.

**Abstract:** Plaintiffs and defendants in copyright lawsuits over generative AI often make sweeping, opposing claims about the extent to which large language models (LLMs) have memorized plaintiffs' protected expression. Drawing on adversarial ML and copyright law, we show that these polarized positions dramatically oversimplify the relationship between memorization and copyright. To do so, we leverage a recent probabilistic extraction technique to extract pieces of the Books3 dataset from 13 open-weight LLMs. Through numerous experiments, we show that it's possible to extract substantial parts of at least some books from different LLMs. This is evidence that the LLMs have memorized the extracted text; this memorized content is copied inside the model parameters. But the results are complicated: the extent of memorization varies both by model and by book. With our specific experiments, we find that the largest LLMs don't memorize most books -- either in whole or in part. However, we also find that Llama 3.1 70B memorizes some books, like Harry Potter and 1984, almost entirely. We discuss why our results have significant implications for copyright cases, though not ones that unambiguously favor either side.

</details>


### [135] [The taggedPBC: Annotating a massive parallel corpus for crosslinguistic investigations](https://arxiv.org/abs/2505.12560)

*Hiram Ring*

**Main category:** cs.CL

**Keywords:** crosslinguistic investigations, tagged corpus, N1 ratio, word order, language families

**Relevance Score:** 4

**TL;DR:** Development of a large automatically tagged parallel dataset (taggedPBC) for over 1,500 languages to aid crosslinguistic investigations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing datasets for crosslinguistic studies are limited either in the number of languages or the amount of data available, which restricts understanding of universal language properties.

**Method:** Creation of a tagged parallel corpus with over 1,800 sentences from more than 1,500 languages, measuring tag accuracy against existing state-of-the-art taggers and developing a novel measure (N1 ratio) for assessing word order.

**Key Contributions:**

	1. Development of the taggedPBC dataset with diverse language representation
	2. Demonstration of tag accuracy correlating with state-of-the-art tools
	3. Introduction of the N1 ratio for word order identification

**Result:** The taggedPBC shows strong correlation in tag accuracy with existing high-resource language taggers and enables a Gaussian Naive Bayes classifier to predict basic word order for languages not well-represented in existing databases.

**Limitations:** Further work is needed to expand and develop the dataset.

**Conclusion:** The taggedPBC is a significant resource for crosslinguistic research, although further development is required; it is accessible for collaborative research on GitHub.

**Abstract:** Existing datasets available for crosslinguistic investigations have tended to focus on large amounts of data for a small group of languages or a small amount of data for a large number of languages. This means that claims based on these datasets are limited in what they reveal about universal properties of the human language faculty. While this has begun to change through the efforts of projects seeking to develop tagged corpora for a large number of languages, such efforts are still constrained by limits on resources. The current paper reports on a large automatically tagged parallel dataset which has been developed to partially address this issue. The taggedPBC contains more than 1,800 sentences of pos-tagged parallel text data from over 1,500 languages, representing 133 language families and 111 isolates, dwarfing previously available resources. The accuracy of tags in this dataset is shown to correlate well with both existing SOTA taggers for high-resource languages (SpaCy, Trankit) as well as hand-tagged corpora (Universal Dependencies Treebanks). Additionally, a novel measure derived from this dataset, the N1 ratio, correlates with expert determinations of word order in three typological databases (WALS, Grambank, Autotyp) such that a Gaussian Naive Bayes classifier trained on this feature can accurately identify basic word order for languages not in those databases. While much work is still needed to expand and develop this dataset, the taggedPBC is an important step to enable corpus-based crosslinguistic investigations, and is made available for research and collaboration via GitHub.

</details>


### [136] [Enriching Patent Claim Generation with European Patent Dataset](https://arxiv.org/abs/2505.12568)

*Lekang Jiang, Chengzu Li, Stephan Goetz*

**Main category:** cs.CL

**Keywords:** patent claims, large language models, dataset, European patents, claim generation

**Relevance Score:** 8

**TL;DR:** The paper introduces the European Patent Dataset (EPD), which enhances the use of LLMs for drafting patent claims by providing jurisdictionally diverse, high-quality patent texts and structured metadata.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To assist inventors in writing patent claims efficiently and to expand the existing research on patent drafting by incorporating European legal standards and drafting conventions.

**Method:** The authors introduce a dataset called EPD, which includes high-quality granted patents from European jurisdictions, designed for various patent-related tasks, and conduct experiments to evaluate the performance of LLMs trained on this dataset.

**Key Contributions:**

	1. Introduction of a comprehensive European patent dataset (EPD)
	2. Demonstration of significant improvements in claim quality using LLMs trained on EPD
	3. Identification of the need for future research addressing real-world challenges in claim generation.

**Result:** Experiments demonstrate that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets, including GPT-4, in terms of claim quality and adaptability across domains.

**Limitations:** The dataset may be limited to specific European jurisdictions, potentially omitting data from other important patent offices globally.

**Conclusion:** The study highlights the superiority of the EPD dataset for training LLMs in patent claims generation and emphasizes the challenges of simulating real-world scenarios.

**Abstract:** Drafting patent claims is time-intensive, costly, and requires professional skill. Therefore, researchers have investigated large language models (LLMs) to assist inventors in writing claims. However, existing work has largely relied on datasets from the United States Patent and Trademark Office (USPTO). To enlarge research scope regarding various jurisdictions, drafting conventions, and legal standards, we introduce EPD, a European patent dataset. EPD presents rich textual data and structured metadata to support multiple patent-related tasks, including claim generation. This dataset enriches the field in three critical aspects: (1) Jurisdictional diversity: Patents from different offices vary in legal and drafting conventions. EPD fills a critical gap by providing a benchmark for European patents to enable more comprehensive evaluation. (2) Quality improvement: EPD offers high-quality granted patents with finalized and legally approved texts, whereas others consist of patent applications that are unexamined or provisional. Experiments show that LLMs fine-tuned on EPD significantly outperform those trained on previous datasets and even GPT-4o in claim quality and cross-domain generalization. (3) Real-world simulation: We propose a difficult subset of EPD to better reflect real-world challenges of claim generation. Results reveal that all tested LLMs perform substantially worse on these challenging samples, which highlights the need for future research.

</details>


### [137] [Measuring Information Distortion in Hierarchical Ultra long Novel Generation:The Optimal Expansion Ratio](https://arxiv.org/abs/2505.12572)

*Hanwen Shen, Ting Ying*

**Main category:** cs.CL

**Keywords:** Large Language Models, novel writing, information-theoretic analysis, semantic distortion, hierarchical generation

**Relevance Score:** 6

**TL;DR:** This paper explores the optimal human-authored outline length required for LLMs to generate high-quality million-word novels.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating coherent ultra-long novels using Large Language Models, as previous frameworks only cover shorter lengths.

**Method:** The authors employ an information-theoretic analysis and introduce a hierarchical two-stage generation pipeline: outline -> detailed outline -> manuscript.

**Key Contributions:**

	1. Quantifies distortion in LLM-generated novels based on outline length.
	2. Introduces a two-stage hierarchical generation pipeline for novel writing.
	3. Provides empirical guidance for authors using LLMs to write ultra-long novels.

**Result:** The study demonstrates that a two-stage hierarchical outline approach significantly reduces semantic distortion in ultra-long novels compared to single-stage methods.

**Limitations:** Focused on Chinese novels; generalizability to other languages or genres may vary.

**Conclusion:** The findings offer valuable insights for authors and researchers on effective collaboration with LLMs in novel writing.

**Abstract:** Writing novels with Large Language Models (LLMs) raises a critical question: how much human-authored outline is necessary to generate high-quality million-word novels? While frameworks such as DOME, Plan&Write, and Long Writer have improved stylistic coherence and logical consistency, they primarily target shorter novels (10k--100k words), leaving ultra-long generation largely unexplored. Drawing on insights from recent text compression methods like LLMZip and LLM2Vec, we conduct an information-theoretic analysis that quantifies distortion occurring when LLMs compress and reconstruct ultra-long novels under varying compression-expansion ratios. We introduce a hierarchical two-stage generation pipeline (outline -> detailed outline -> manuscript) and find an optimal outline length that balances information preservation with human effort. Through extensive experimentation with Chinese novels, we establish that a two-stage hierarchical outline approach significantly reduces semantic distortion compared to single-stage methods. Our findings provide empirically-grounded guidance for authors and researchers collaborating with LLMs to create million-word novels.

</details>


### [138] [Improving Multilingual Language Models by Aligning Representations through Steering](https://arxiv.org/abs/2505.12584)

*Omar Mahmoud, Buddhika Laknath Semage, Thommen George Karimpanal, Santu Rana*

**Main category:** cs.CL

**Keywords:** large language models, multilingual capabilities, representation steering, prompt optimization, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper explores how large language models (LLMs) handle non-English tokens in their layer representations, introducing a method to enhance performance using representation steering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs process non-English tokens is crucial for improving their multilingual capabilities.

**Method:** The authors utilize representation steering by adding a learned vector to the activations of a single layer in the model, analyzing its impact on performance.

**Key Contributions:**

	1. Demonstration of representation steering to enhance LLM performance.
	2. Comparison against translation baselines and prompt optimization methods.
	3. Insights into the effects of fine-tuning techniques on multilingual capabilities.

**Result:** The proposed method shows comparable results to translation baselines and significantly outperforms state-of-the-art prompt optimization techniques.

**Limitations:** 

**Conclusion:** Steering a single layer can greatly enhance LLM performance for multilingual tasks, especially when combined with supervised fine-tuning and reinforcement learning from human feedback.

**Abstract:** In this paper, we investigate how large language models (LLMS) process non-English tokens within their layer representations, an open question despite significant advancements in the field. Using representation steering, specifically by adding a learned vector to a single model layer's activations, we demonstrate that steering a single model layer can notably enhance performance. Our analysis shows that this approach achieves results comparable to translation baselines and surpasses state of the art prompt optimization methods. Additionally, we highlight how advanced techniques like supervised fine tuning (\textsc{sft}) and reinforcement learning from human feedback (\textsc{rlhf}) improve multilingual capabilities by altering representation spaces. We further illustrate how these methods align with our approach to reshaping LLMS layer representations.

</details>


### [139] [CMLFormer: A Dual Decoder Transformer with Switching Point Learning for Code-Mixed Language Modeling](https://arxiv.org/abs/2505.12587)

*Aditeya Baral, Allen George Ajith, Roshan Nayak, Mrityunjay Abhijeet Bhanja*

**Main category:** cs.CL

**Keywords:** Code-mixed languages, Transformer, Natural Language Processing, Multi-task learning, Hinglish

**Relevance Score:** 4

**TL;DR:** CMLFormer is a dual-decoder Transformer model designed for code-mixed languages, showing improved performance on the HASOC-2021 benchmark.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standard language models struggle with code-mixed languages that frequently transition between languages within sentences.

**Method:** CMLFormer utilizes a multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, pre-trained on an augmented Hinglish corpus.

**Key Contributions:**

	1. Introduction of CMLFormer architecture for code-mixed text.
	2. Enhanced pre-training objectives capturing code-mixing dynamics.
	3. Demonstrated success on benchmark tasks with improved evaluation metrics.

**Result:** CMLFormer shows improvements in F1 score, precision, and accuracy over existing approaches, specifically on the HASOC-2021 benchmark.

**Limitations:** 

**Conclusion:** CMLFormer effectively models the complexities of code-mixed languages through its architecture and multi-task pre-training strategy.

**Abstract:** Code-mixed languages, characterized by frequent within-sentence language transitions, present structural challenges that standard language models fail to address. In this work, we propose CMLFormer, an enhanced multi-layer dual-decoder Transformer with a shared encoder and synchronized decoder cross-attention, designed to model the linguistic and semantic dynamics of code-mixed text. CMLFormer is pre-trained on an augmented Hinglish corpus with switching point and translation annotations with multiple new objectives specifically aimed at capturing switching behavior, cross-lingual structure, and code-mixing complexity. Our experiments show that CMLFormer improves F1 score, precision, and accuracy over other approaches on the HASOC-2021 benchmark under select pre-training setups. Attention analyses further show that it can identify and attend to switching points, validating its sensitivity to code-mixed structure. These results demonstrate the effectiveness of CMLFormer's architecture and multi-task pre-training strategy for modeling code-mixed languages.

</details>


### [140] [PromptPrism: A Linguistically-Inspired Taxonomy for Prompts](https://arxiv.org/abs/2505.12592)

*Sullam Jeoung, Yueyan Chen, Yi Zhang, Shuai Wang, Haibo Ding, Lin Lee Cheong*

**Main category:** cs.CL

**Keywords:** Prompt analysis, Large language models, Taxonomy, Prompt refinement, Machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces PromptPrism, a taxonomy for systematic prompt analysis of large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a comprehensive framework for analyzing prompt structure to enhance LLM performance and understand behavior.

**Method:** Developed a taxonomy with three levels: functional structure, semantic component, and syntactic pattern. Applied it to prompt refinement, dataset profiling, and sensitivity analysis.

**Key Contributions:**

	1. Introduction of a comprehensive taxonomy for prompt analysis
	2. Demonstration of prompt refinement that enhances LLM performance
	3. Development of a dataset profiling method for analyzing prompt characteristics

**Result:** Demonstrated that PromptPrism effectively improves prompt quality and model performance; validated through three practical applications.

**Limitations:** 

**Conclusion:** PromptPrism provides a foundational tool for refining, profiling, and analyzing prompts in LLM applications.

**Abstract:** Prompts are the interface for eliciting the capabilities of large language models (LLMs). Understanding their structure and components is critical for analyzing LLM behavior and optimizing performance. However, the field lacks a comprehensive framework for systematic prompt analysis and understanding. We introduce PromptPrism, a linguistically-inspired taxonomy that enables prompt analysis across three hierarchical levels: functional structure, semantic component, and syntactic pattern. We show the practical utility of PromptPrism by applying it to three applications: (1) a taxonomy-guided prompt refinement approach that automatically improves prompt quality and enhances model performance across a range of tasks; (2) a multi-dimensional dataset profiling method that extracts and aggregates structural, semantic, and syntactic characteristics from prompt datasets, enabling comprehensive analysis of prompt distributions and patterns; (3) a controlled experimental framework for prompt sensitivity analysis by quantifying the impact of semantic reordering and delimiter modifications on LLM performance. Our experimental results validate the effectiveness of our taxonomy across these applications, demonstrating that PromptPrism provides a foundation for refining, profiling, and analyzing prompts.

</details>


### [141] [AD-AGENT: A Multi-agent Framework for End-to-end Anomaly Detection](https://arxiv.org/abs/2505.12594)

*Tiankai Yang, Junjun Liu, Wingchun Siu, Jiahang Wang, Zhuangzhuang Qian, Chanjuan Song, Cheng Cheng, Xiyang Hu, Yue Zhao*

**Main category:** cs.CL

**Keywords:** Anomaly Detection, Machine Learning, Natural Language Processing, Automated Systems, Multi-agent Framework

**Relevance Score:** 7

**TL;DR:** AD-AGENT is an LLM-driven multi-agent framework that automates anomaly detection (AD) pipelines from natural language instructions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Anomaly detection is crucial in various fields, but the complexity of tools and libraries makes it difficult for non-experts to utilize them effectively.

**Method:** AD-AGENT employs specialized agents to handle intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging, integrating libraries like PyOD and TSLib into a cohesive system.

**Key Contributions:**

	1. Introduction of an LLM-driven framework that makes anomaly detection accessible to non-experts
	2. Integration of various AD libraries into a single workflow
	3. Providing documentation and debugging in a user-friendly manner

**Result:** AD-AGENT demonstrates reliable script generation and competitive model recommendations across different AD libraries in experimental setups.

**Limitations:** 

**Conclusion:** The open-sourced AD-AGENT enables users to efficiently implement anomaly detection processes, promoting further research and practical applications.

**Abstract:** Anomaly detection (AD) is essential in areas such as fraud detection, network monitoring, and scientific research. However, the diversity of data modalities and the increasing number of specialized AD libraries pose challenges for non-expert users who lack in-depth library-specific knowledge and advanced programming skills. To tackle this, we present AD-AGENT, an LLM-driven multi-agent framework that turns natural-language instructions into fully executable AD pipelines. AD-AGENT coordinates specialized agents for intent parsing, data preparation, library and model selection, documentation mining, and iterative code generation and debugging. Using a shared short-term workspace and a long-term cache, the agents integrate popular AD libraries like PyOD, PyGOD, and TSLib into a unified workflow. Experiments demonstrate that AD-AGENT produces reliable scripts and recommends competitive models across libraries. The system is open-sourced to support further research and practical applications in AD.

</details>


### [142] [Duluth at SemEval-2025 Task 7: TF-IDF with Optimized Vector Dimensions for Multilingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2505.12616)

*Shujauddin Syed, Ted Pedersen*

**Main category:** cs.CL

**Keywords:** Multilingual Retrieval, Crosslingual Retrieval, Fact-Checking, TF-IDF, Natural Language Processing

**Relevance Score:** 4

**TL;DR:** This paper introduces a TF-IDF-based claim retrieval system for the SemEval-2025 Task 7, achieving moderate success in multilingual contexts while highlighting the competitive nature of traditional methods against advanced neural architectures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address multilingual and crosslingual fact-checked claim retrieval challenges, particularly in the context of the SemEval-2025 competition.

**Method:** We implemented a TF-IDF-based retrieval system experimenting with vector dimensions and tokenization strategies, ultimately using word-level tokenization with a vocabulary size of 15,000 features.

**Key Contributions:**

	1. Introduction of a TF-IDF-based approach for multilingual claim retrieval
	2. Performance benchmarks across ten languages
	3. Insights on the competitiveness of traditional vs. neural methods

**Result:** The best configuration achieved an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages, with stronger performance on higher-resource languages.

**Limitations:** Significantly lagged behind the top-ranked system which achieved an average score of 0.96.

**Conclusion:** While neural architectures dominate multilingual retrieval tasks, optimized traditional methods like TF-IDF can still serve as effective baselines, especially with limited resources.

**Abstract:** This paper presents the Duluth approach to the SemEval-2025 Task 7 on Multilingual and Crosslingual Fact-Checked Claim Retrieval. We implemented a TF-IDF-based retrieval system with experimentation on vector dimensions and tokenization strategies. Our best-performing configuration used word-level tokenization with a vocabulary size of 15,000 features, achieving an average success@10 score of 0.78 on the development set and 0.69 on the test set across ten languages. Our system showed stronger performance on higher-resource languages but still lagged significantly behind the top-ranked system, which achieved 0.96 average success@10. Our findings suggest that though advanced neural architectures are increasingly dominant in multilingual retrieval tasks, properly optimized traditional methods like TF-IDF remain competitive baselines, especially in limited compute resource scenarios.

</details>


### [143] [Think Before You Attribute: Improving the Performance of LLMs Attribution Systems](https://arxiv.org/abs/2505.12621)

*João Eduardo Batista, Emil Vatai, Mohamed Wahib*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieve-Augmented Generation, sentence-level attribution

**Relevance Score:** 9

**TL;DR:** This paper proposes a sentence-level pre-attribution method for Retrieve-Augmented Generation (RAG) systems to improve the accuracy and reliability of outputs generated by Large Language Models (LLMs) in scientific settings.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of unreliable source attribution in outputs generated by LLMs, which inhibits their adoption in scientific domains that require accountability and traceability.

**Method:** The authors propose a pre-attribution step that classifies sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes, allowing for appropriate attribution methods to be selected based on the sentence type.

**Key Contributions:**

	1. Introduction of a sentence-level pre-attribution step for RAG systems.
	2. Development of a clean version of the HAGRID dataset.
	3. End-to-end attribution system that functions out of the box.

**Result:** The paper shows that the proposed classifiers effectively determine sentence attribution, reducing the computational burden of the attribution process.

**Limitations:** The effectiveness of the proposed method may be contingent on the quality of the training data and the specific domains it is applied to.

**Conclusion:** The end-to-end attribution system developed can operate seamlessly, providing a reliable framework for integrating sentence-level attribution in LLM outputs for scientific purposes.

**Abstract:** Large Language Models (LLMs) are increasingly applied in various science domains, yet their broader adoption remains constrained by a critical challenge: the lack of trustworthy, verifiable outputs. Current LLMs often generate answers without reliable source attribution, or worse, with incorrect attributions, posing a barrier to their use in scientific and high-stakes settings, where traceability and accountability are non-negotiable. To be reliable, attribution systems need high accuracy and retrieve data with short lengths, i.e., attribute to a sentence within a document rather than a whole document. We propose a sentence-level pre-attribution step for Retrieve-Augmented Generation (RAG) systems that classify sentences into three categories: not attributable, attributable to a single quote, and attributable to multiple quotes. By separating sentences before attribution, a proper attribution method can be selected for the type of sentence, or the attribution can be skipped altogether. Our results indicate that classifiers are well-suited for this task. In this work, we propose a pre-attribution step to reduce the computational complexity of attribution, provide a clean version of the HAGRID dataset, and provide an end-to-end attribution system that works out of the box.

</details>


### [144] [R1dacted: Investigating Local Censorship in DeepSeek's R1 Language Model](https://arxiv.org/abs/2505.12625)

*Ali Naseh, Harsh Chaudhari, Jaechul Roh, Mingshi Wu, Alina Oprea, Amir Houmansadr*

**Main category:** cs.CL

**Keywords:** censorship, large language model, politically sensitive topics, AI governance, language model transparency

**Relevance Score:** 7

**TL;DR:** This paper investigates censorship behavior exhibited by the R1 large language model, particularly on politically sensitive topics, and explores methods to bypass this censorship.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the censorship patterns of the R1 language model in relation to politically sensitive topics and its implications for transparency and governance in AI.

**Method:** The authors created a large-scale set of highly curated prompts that are censored by R1 but not by other models, and conducted a comprehensive analysis of R1's censorship behavior across various topics and languages.

**Key Contributions:**

	1. Identification of censorship patterns in the R1 model
	2. Introduction of a curated set of prompts that reveal censorship
	3. Proposed techniques for bypassing or removing censorship in language models.

**Result:** The analysis revealed consistent patterns of censorship in R1, influenced by prompt phrasing and context, and exposed potential biases and lack of transparency in the model's design.

**Limitations:** 

**Conclusion:** The findings indicate that censorship in R1 may stem from design choices during its training, raising significant concerns for the deployment of language models in sensitive areas.

**Abstract:** DeepSeek recently released R1, a high-performing large language model (LLM) optimized for reasoning tasks. Despite its efficient training pipeline, R1 achieves competitive performance, even surpassing leading reasoning models like OpenAI's o1 on several benchmarks. However, emerging reports suggest that R1 refuses to answer certain prompts related to politically sensitive topics in China. While existing LLMs often implement safeguards to avoid generating harmful or offensive outputs, R1 represents a notable shift - exhibiting censorship-like behavior on politically charged queries. In this paper, we investigate this phenomenon by first introducing a large-scale set of heavily curated prompts that get censored by R1, covering a range of politically sensitive topics, but are not censored by other models. We then conduct a comprehensive analysis of R1's censorship patterns, examining their consistency, triggers, and variations across topics, prompt phrasing, and context. Beyond English-language queries, we explore censorship behavior in other languages. We also investigate the transferability of censorship to models distilled from the R1 language model. Finally, we propose techniques for bypassing or removing this censorship. Our findings reveal possible additional censorship integration likely shaped by design choices during training or alignment, raising concerns about transparency, bias, and governance in language model deployment.

</details>


### [145] [Revealing the Deceptiveness of Knowledge Editing: A Mechanistic Analysis of Superficial Editing](https://arxiv.org/abs/2505.12636)

*Jiakuan Xie, Pengfei Cao, Yubo Chen, Kang Liu, Jun Zhao*

**Main category:** cs.CL

**Keywords:** knowledge editing, superficial editing, attention modules, language models, machine learning

**Relevance Score:** 8

**TL;DR:** This paper critiques knowledge editing methods for language models, introducing the concept of "superficial editing" and identifying key factors contributing to knowledge retention despite editing efforts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings in existing knowledge editing algorithms, which often fail to prevent models from generating original or outdated knowledge.

**Method:** The authors systematically evaluate knowledge editing and identify factors related to model layers and attention modules that contribute to superficial editing, along with proposing a framework for superficial unlearning.

**Key Contributions:**

	1. Introduction of the concept of superficial editing in language models.
	2. Identification of factors related to attention modules and residual streams that affect knowledge retention.
	3. Analysis of superficial unlearning showcasing consistent behavior patterns in attention heads.

**Result:** Key findings indicate that specific attention heads in later layers retain original knowledge and have a causal relationship with the issue of superficial editing.

**Limitations:** The study focuses primarily on specific models and attention mechanisms, which may limit generalizability to other architectures.

**Conclusion:** The findings underscore the need for improved knowledge editing approaches that account for these identified factors, with implications for the robustness of models in real-world applications.

**Abstract:** Knowledge editing, which aims to update the knowledge encoded in language models, can be deceptive. Despite the fact that many existing knowledge editing algorithms achieve near-perfect performance on conventional metrics, the models edited by them are still prone to generating original knowledge. This paper introduces the concept of "superficial editing" to describe this phenomenon. Our comprehensive evaluation reveals that this issue presents a significant challenge to existing algorithms. Through systematic investigation, we identify and validate two key factors contributing to this issue: (1) the residual stream at the last subject position in earlier layers and (2) specific attention modules in later layers. Notably, certain attention heads in later layers, along with specific left singular vectors in their output matrices, encapsulate the original knowledge and exhibit a causal relationship with superficial editing. Furthermore, we extend our analysis to the task of superficial unlearning, where we observe consistent patterns in the behavior of specific attention heads and their corresponding left singular vectors, thereby demonstrating the robustness and broader applicability of our methodology and conclusions. Our code is available here.

</details>


### [146] [Predicting Turn-Taking and Backchannel in Human-Machine Conversations Using Linguistic, Acoustic, and Visual Signals](https://arxiv.org/abs/2505.12654)

*Yuxin Lin, Yinglin Zheng, Ming Zeng, Wangzheng Shi*

**Main category:** cs.CL

**Keywords:** turn-taking, backchannel prediction, multi-modal signals, human-machine interaction, dataset

**Relevance Score:** 9

**TL;DR:** The paper presents a multi-modal approach for predicting turn-taking and backchannel actions in human-machine conversations using a newly collected dataset and an end-to-end prediction framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a gap in predicting turn-taking and backchannel actions in human-machine conversations, especially with existing datasets being limited.

**Method:** An automatic data collection pipeline was developed to gather and annotate over 210 hours of human conversation videos, resulting in the creation of a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset. An end-to-end framework was then proposed to predict turn-taking and backchannel actions using multi-modal signals.

**Key Contributions:**

	1. Development of a large-scale Multi-Modal Face-to-Face dataset with extensive annotations
	2. Introduction of an end-to-end framework for predicting turn-taking and backchannel actions
	3. Demonstration of state-of-the-art performance in multi-modal prediction tasks

**Result:** The approach achieved state-of-the-art performance, with a 10% increase in F1-score for turn-taking and a 33% increase for backchannel prediction tasks.

**Limitations:** 

**Conclusion:** The dataset and code are publicly available for further research, contributing significantly to the field of human-machine interaction.

**Abstract:** This paper addresses the gap in predicting turn-taking and backchannel actions in human-machine conversations using multi-modal signals (linguistic, acoustic, and visual). To overcome the limitation of existing datasets, we propose an automatic data collection pipeline that allows us to collect and annotate over 210 hours of human conversation videos. From this, we construct a Multi-Modal Face-to-Face (MM-F2F) human conversation dataset, including over 1.5M words and corresponding turn-taking and backchannel annotations from approximately 20M frames. Additionally, we present an end-to-end framework that predicts the probability of turn-taking and backchannel actions from multi-modal signals. The proposed model emphasizes the interrelation between modalities and supports any combination of text, audio, and video inputs, making it adaptable to a variety of realistic scenarios. Our experiments show that our approach achieves state-of-the-art performance on turn-taking and backchannel prediction tasks, achieving a 10\% increase in F1-score on turn-taking and a 33\% increase on backchannel prediction. Our dataset and code are publicly available online to ease of subsequent research.

</details>


### [147] [Know3-RAG: A Knowledge-aware RAG Framework with Adaptive Retrieval, Generation, and Filtering](https://arxiv.org/abs/2505.12662)

*Xukai Liu, Ye Liu, Shiwen Wu, Yanghai Zhang, Yihao Yuan, Kai Zhang, Qi Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Knowledge Graphs, Natural Language Generation

**Relevance Score:** 9

**TL;DR:** The paper introduces Know3-RAG, a knowledge-aware framework to improve factual reliability in RAG by leveraging knowledge graphs, addressing limitations of existing systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the factual reliability of natural language generation by addressing hallucinations and unreliable adaptive control in RAG systems.

**Method:** Know3-RAG integrates structured knowledge from knowledge graphs at three stages: retrieval, generation, and filtering. It uses KG embeddings to assess answer confidence, enriches queries with KG-derived entities, and applies a filtering mechanism for alignment and accuracy.

**Key Contributions:**

	1. Introduction of a knowledge-aware adaptive retrieval module
	2. Development of a knowledge-enhanced reference generation strategy
	3. Implementation of a knowledge-driven reference filtering mechanism

**Result:** Experiments show that Know3-RAG outperforms existing RAG baselines, significantly reducing hallucinations and improving the reliability of generated answers on open-domain QA benchmarks.

**Limitations:** 

**Conclusion:** The proposed framework effectively enhances the performance of RAG systems, addressing key challenges of reliability and factual accuracy.

**Abstract:** Recent advances in large language models (LLMs) have led to impressive progress in natural language generation, yet their tendency to produce hallucinated or unsubstantiated content remains a critical concern. To improve factual reliability, Retrieval-Augmented Generation (RAG) integrates external knowledge during inference. However, existing RAG systems face two major limitations: (1) unreliable adaptive control due to limited external knowledge supervision, and (2) hallucinations caused by inaccurate or irrelevant references. To address these issues, we propose Know3-RAG, a knowledge-aware RAG framework that leverages structured knowledge from knowledge graphs (KGs) to guide three core stages of the RAG process, including retrieval, generation, and filtering. Specifically, we introduce a knowledge-aware adaptive retrieval module that employs KG embedding to assess the confidence of the generated answer and determine retrieval necessity, a knowledge-enhanced reference generation strategy that enriches queries with KG-derived entities to improve generated reference relevance, and a knowledge-driven reference filtering mechanism that ensures semantic alignment and factual accuracy of references. Experiments on multiple open-domain QA benchmarks demonstrate that Know3-RAG consistently outperforms strong baselines, significantly reducing hallucinations and enhancing answer reliability.

</details>


### [148] [Shadow-FT: Tuning Instruct via Base](https://arxiv.org/abs/2505.12716)

*Taiqiang Wu, Runming Yang, Jiayi Li, Pengfei Hu, Ngai Wong, Yujiu Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-Tuning, INSTRUCT Models, Machine Learning, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** Proposes a novel framework called Shadow-FT for fine-tuning INSTRUCT models by leveraging BASE model updates, leading to significant performance improvements without increasing parameters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of directly tuning INSTRUCT models, which often results in marginal improvements or performance degradation.

**Method:** Shadow-FT fine-tunes the BASE model and grafts the learned weight updates onto the INSTRUCT model without adding extra parameters.

**Key Contributions:**

	1. Introduction of the Shadow-FT framework for fine-tuning INSTRUCT models.
	2. Consistency in improved performance over traditional methods.
	3. Applicability to multimodal LLMs and integration with DPO.

**Result:** Extensive experiments show that Shadow-FT outperforms traditional fine-tuning methods across 19 benchmarks, improving performance on coding, reasoning, and mathematical tasks.

**Limitations:** 

**Conclusion:** Shadow-FT is a practical solution for tuning LLMs and shows potential for application in multimodal models and with direct preference optimization.

**Abstract:** Large language models (LLMs) consistently benefit from further fine-tuning on various tasks. However, we observe that directly tuning the INSTRUCT (i.e., instruction tuned) models often leads to marginal improvements and even performance degeneration. Notably, paired BASE models, the foundation for these INSTRUCT variants, contain highly similar weight values (i.e., less than 2% on average for Llama 3.1 8B). Therefore, we propose a novel Shadow-FT framework to tune the INSTRUCT models by leveraging the corresponding BASE models. The key insight is to fine-tune the BASE model, and then directly graft the learned weight updates to the INSTRUCT model. Our proposed Shadow-FT introduces no additional parameters, is easy to implement, and significantly improves performance. We conduct extensive experiments on tuning mainstream LLMs, such as Qwen 3 and Llama 3 series, and evaluate them across 19 benchmarks covering coding, reasoning, and mathematical tasks. Experimental results demonstrate that Shadow-FT consistently outperforms conventional full-parameter and parameter-efficient tuning approaches. Further analyses indicate that Shadow-FT can be applied to multimodal large language models (MLLMs) and combined with direct preference optimization (DPO). Codes and weights are available at \href{https://github.com/wutaiqiang/Shadow-FT}{Github}.

</details>


### [149] [ToTRL: Unlock LLM Tree-of-Thoughts Reasoning Potential through Puzzles Solving](https://arxiv.org/abs/2505.12717)

*Haoyuan Wu, Xueyi Chen, Rui Ming, Jilong Gao, Shoubo Hu, Zhuolun He, Bei Yu*

**Main category:** cs.CL

**Keywords:** large language models, reinforcement learning, reasoning, tree-of-thought, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel on-policy reinforcement learning framework, Tree-of-Thoughts RL (ToTRL), which enhances large language model (LLM) reasoning by modeling it as tree structures, improving performance and reducing costs in complex reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of large language models (LLMs) which exhibit limitations in prolonged chain-of-thought (CoT) reasoning, especially in terms of verbosity and inefficient trial-and-error methods.

**Method:** The paper proposes Tree-of-Thoughts RL (ToTRL), an on-policy RL framework that utilizes a rule-based reward system, enabling LLMs to engage in parallel reasoning by managing a tree structure of thought processes.

**Key Contributions:**

	1. Introduction of Tree-of-Thoughts RL (ToTRL) framework for enhancing LLM reasoning.
	2. Modeling reasoning as a tree structure for parallel exploration and evaluation.
	3. Demonstrated significant performance improvements on complex reasoning tasks.

**Result:** Empirical evaluations show that the ToTQwen3-8B model trained with ToTRL demonstrates significant improvements in reasoning efficiency and performance on complex reasoning tasks compared to traditional methods.

**Limitations:** 

**Conclusion:** The introduction of ToTRL represents a conceptual advancement in LLM reasoning by allowing for more systematic exploration of reasoning paths, leading to better decision-making in task environments.

**Abstract:** Large language models (LLMs) demonstrate significant reasoning capabilities, particularly through long chain-of-thought (CoT) processes, which can be elicited by reinforcement learning (RL). However, prolonged CoT reasoning presents limitations, primarily verbose outputs due to excessive introspection. The reasoning process in these LLMs often appears to follow a trial-and-error methodology rather than a systematic, logical deduction. In contrast, tree-of-thoughts (ToT) offers a conceptually more advanced approach by modeling reasoning as an exploration within a tree structure. This reasoning structure facilitates the parallel generation and evaluation of multiple reasoning branches, allowing for the active identification, assessment, and pruning of unproductive paths. This process can potentially lead to improved performance and reduced token costs. Building upon the long CoT capability of LLMs, we introduce tree-of-thoughts RL (ToTRL), a novel on-policy RL framework with a rule-based reward. ToTRL is designed to guide LLMs in developing the parallel ToT strategy based on the sequential CoT strategy. Furthermore, we employ LLMs as players in a puzzle game during the ToTRL training process. Solving puzzle games inherently necessitates exploring interdependent choices and managing multiple constraints, which requires the construction and exploration of a thought tree, providing challenging tasks for cultivating the ToT reasoning capability. Our empirical evaluations demonstrate that our ToTQwen3-8B model, trained with our ToTRL, achieves significant improvement in performance and reasoning efficiency on complex reasoning tasks.

</details>


### [150] [Automated Bias Assessment in AI-Generated Educational Content Using CEAT Framework](https://arxiv.org/abs/2505.12718)

*Jingyang Peng, Wenyuan Shen, Jiarui Rao, Jionghao Lin*

**Main category:** cs.CL

**Keywords:** Generative AI, bias assessment, educational content, human-computer interaction, automated evaluation

**Relevance Score:** 7

**TL;DR:** The study presents an automated method for assessing biases in AI-generated educational content, showing high reliability and alignment with manual assessments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical concerns regarding biases in educational materials produced by Generative AI.

**Method:** An automated bias assessment approach combining the Contextualized Embedding Association Test and a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework.

**Key Contributions:**

	1. Automated approach to assess biases in AI-generated educational materials.
	2. High correlation with manual bias assessment methods.
	3. Improves fairness and reproducibility in educational content evaluation.

**Result:** The method demonstrated a high Pearson correlation coefficient of r = 0.993 with manually curated word sets, indicating reliable bias assessment.

**Limitations:** 

**Conclusion:** The proposed method enhances fairness, scalability, and reproducibility in auditing AI-generated educational content, reducing human subjectivity.

**Abstract:** Recent advances in Generative Artificial Intelligence (GenAI) have transformed educational content creation, particularly in developing tutor training materials. However, biases embedded in AI-generated content--such as gender, racial, or national stereotypes--raise significant ethical and educational concerns. Despite the growing use of GenAI, systematic methods for detecting and evaluating such biases in educational materials remain limited. This study proposes an automated bias assessment approach that integrates the Contextualized Embedding Association Test with a prompt-engineered word extraction method within a Retrieval-Augmented Generation framework. We applied this method to AI-generated texts used in tutor training lessons. Results show a high alignment between the automated and manually curated word sets, with a Pearson correlation coefficient of r = 0.993, indicating reliable and consistent bias assessment. Our method reduces human subjectivity and enhances fairness, scalability, and reproducibility in auditing GenAI-produced educational content.

</details>


### [151] [On-Policy Optimization with Group Equivalent Preference for Multi-Programming Language Understanding](https://arxiv.org/abs/2505.12723)

*Haoyuan Wu, Rui Ming, Jilong Gao, Hangyu Zhao, Xueyi Chen, Yikai Yang, Haisheng Zheng, Zhuolun He, Bei Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Generation, Reinforcement Learning, Code Translation, Group Equivalent Preference Optimization

**Relevance Score:** 7

**TL;DR:** Introduces OORL, a novel RL framework for training LLMs in code translation tasks to improve performance across various programming languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance disparity in LLMs for code generation across different programming languages.

**Method:** The paper utilizes a novel reinforcement learning framework, OORL, which combines on-policy and off-policy strategies for code translation. It incorporates a rule-based reward derived from unit tests and introduces Group Equivalent Preference Optimization (GEPO) for training using intermediate representations of code.

**Key Contributions:**

	1. Introduction of OORL for training LLMs in code translation tasks.
	2. Development of Group Equivalent Preference Optimization (GEPO) for more nuanced learning through intermediate representations.
	3. Demonstration of significant performance improvements across multiple programming languages on code benchmarks.

**Result:** Extensive experiments show that training LLMs with OORL in code translation significantly improves their ability to recognize code functionality and relationships between different programming languages, achieving enhancements across multiple benchmarks.

**Limitations:** 

**Conclusion:** The proposed OORL framework effectively bridges the capability gap in code generation for various programming languages, advancing the performance of LLMs.

**Abstract:** Large language models (LLMs) achieve remarkable performance in code generation tasks. However, a significant performance disparity persists between popular programming languages (e.g., Python, C++) and others. To address this capability gap, we leverage the code translation task to train LLMs, thereby facilitating the transfer of coding proficiency across diverse programming languages. Moreover, we introduce OORL for training, a novel reinforcement learning (RL) framework that integrates on-policy and off-policy strategies. Within OORL, on-policy RL is applied during code translation, guided by a rule-based reward signal derived from unit tests. Complementing this coarse-grained rule-based reward, we propose Group Equivalent Preference Optimization (GEPO), a novel preference optimization method. Specifically, GEPO trains the LLM using intermediate representations (IRs) groups. LLMs can be guided to discern IRs equivalent to the source code from inequivalent ones, while also utilizing signals about the mutual equivalence between IRs within the group. This process allows LLMs to capture nuanced aspects of code functionality. By employing OORL for training with code translation tasks, LLMs improve their recognition of code functionality and their understanding of the relationships between code implemented in different languages. Extensive experiments demonstrate that our OORL for LLMs training with code translation tasks achieves significant performance improvements on code benchmarks across multiple programming languages.

</details>


### [152] [What is Stigma Attributed to? A Theory-Grounded, Expert-Annotated Interview Corpus for Demystifying Mental-Health Stigma](https://arxiv.org/abs/2505.12727)

*Han Meng, Yancan Chen, Yunan Li, Yitian Yang, Jungup Lee, Renwen Zhang, Yi-Chieh Lee*

**Main category:** cs.CL

**Keywords:** mental-health stigma, neural models, human-chatbot interactions, dataset, stigma detection

**Relevance Score:** 7

**TL;DR:** This paper presents a theory-informed corpus of human-chatbot interviews to classify mental-health stigma, addressing the limitations of existing datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to address the pervasive issue of mental-health stigma that affects treatment-seeking behaviors and recovery, by providing a robust dataset to train neural models.

**Method:** The authors created an expert-annotated corpus of 4,141 snippets derived from 684 participants' interviews, which include socio-cultural background information, and benchmarked state-of-the-art neural models on this data.

**Key Contributions:**

	1. Creation of a theory-informed corpus for mental-health stigma classification.
	2. Empirical benchmarking of neural models for stigma detection.
	3. Detailed participant socio-cultural background information enhancing dataset richness.

**Result:** The experiments reveal empirical challenges in stigma detection and demonstrate the efficacy of the provided dataset for further research in this area.

**Limitations:** The dataset may have limitations in generalizability due to its specific participant demographics.

**Conclusion:** This dataset facilitates advancements in computational methods to detect, neutralize, and counteract mental-health stigma, filling a significant gap in existing resources.

**Abstract:** Mental-health stigma remains a pervasive social problem that hampers treatment-seeking and recovery. Existing resources for training neural models to finely classify such stigma are limited, relying primarily on social-media or synthetic data without theoretical underpinnings. To remedy this gap, we present an expert-annotated, theory-informed corpus of human-chatbot interviews, comprising 4,141 snippets from 684 participants with documented socio-cultural backgrounds. Our experiments benchmark state-of-the-art neural models and empirically unpack the challenges of stigma detection. This dataset can facilitate research on computationally detecting, neutralizing, and counteracting mental-health stigma.

</details>


### [153] [ReEx-SQL: Reasoning with Execution-Aware Reinforcement Learning for Text-to-SQL](https://arxiv.org/abs/2505.12768)

*Yaxun Dai, Wenxuan Xie, Xialie Zhuang, Tianyu Yang, Yiying Yang, Haiqin Yang, Yuhang Zhao, Pingfu Chao, Wenhao Jiang*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Execution Feedback, Reinforcement Learning, Database Interaction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** ReEx-SQL introduces an execution-aware reasoning framework for Text-to-SQL that integrates feedback during query generation, improving accuracy and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Text-to-SQL methods fail to integrate execution feedback in real-time during query generation, leading to reduced accuracy.

**Method:** ReEx-SQL allows models to engage with the database during decoding, utilizing execution feedback to inform and refine reasoning through structured prompts and a stepwise rollout strategy.

**Key Contributions:**

	1. Introduction of execution-aware reasoning for real-time adjustments
	2. Development of a composite reward function promoting effective database interaction
	3. Enhanced efficiency through tree-based decoding over linear approaches.

**Result:** ReEx-SQL achieves 88.8% accuracy on Spider and 64.9% on BIRD, outperforming existing baselines and reducing inference time by 51.9% through tree-based decoding.

**Limitations:** 

**Conclusion:** The proposed method enhances reasoning accuracy and efficiency, indicating significant improvements over traditional approaches in generating SQL queries from text.

**Abstract:** In Text-to-SQL, execution feedback is essential for guiding large language models (LLMs) to reason accurately and generate reliable SQL queries. However, existing methods treat execution feedback solely as a post-hoc signal for correction or selection, failing to integrate it into the generation process. This limitation hinders their ability to address reasoning errors as they occur, ultimately reducing query accuracy and robustness. To address this issue, we propose ReEx-SQL (Reasoning with Execution-Aware Reinforcement Learning), a framework for Text-to-SQL that enables models to interact with the database during decoding and dynamically adjust their reasoning based on execution feedback. ReEx-SQL introduces an execution-aware reasoning paradigm that interleaves intermediate SQL execution into reasoning paths, facilitating context-sensitive revisions. It achieves this through structured prompts with markup tags and a stepwise rollout strategy that integrates execution feedback into each stage of generation. To supervise policy learning, we develop a composite reward function that includes an exploration reward, explicitly encouraging effective database interaction. Additionally, ReEx-SQL adopts a tree-based decoding strategy to support exploratory reasoning, enabling dynamic expansion of alternative reasoning paths. Notably, ReEx-SQL achieves 88.8% on Spider and 64.9% on BIRD at the 7B scale, surpassing the standard reasoning baseline by 2.7% and 2.6%, respectively. It also shows robustness, achieving 85.2% on Spider-Realistic with leading performance. In addition, its tree-structured decoding improves efficiency and performance over linear decoding, reducing inference time by 51.9% on the BIRD development set.

</details>


### [154] [A Token is Worth over 1,000 Tokens: Efficient Knowledge Distillation through Low-Rank Clone](https://arxiv.org/abs/2505.12781)

*Jitai Hao, Qiang Huang, Hao Liu, Xinyan Xiao, Zhaochun Ren, Jun Yu*

**Main category:** cs.CL

**Keywords:** Small Language Models, knowledge distillation, low-rank matrices, activation cloning, training efficiency

**Relevance Score:** 6

**TL;DR:** This paper introduces Low-Rank Clone (LRC), an efficient pre-training method for Small Language Models (SLMs) that addresses information loss, representation alignment, and activation underutilization, achieving over 1,000x training efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind LRC is to overcome challenges related to cost and performance in training Small Language Models (SLMs), particularly focusing on knowledge transfer from larger teacher models.

**Method:** LRC constructs low-rank projection matrices for soft pruning and activation cloning, enabling better alignment of student and teacher model activations without explicit alignment modules.

**Key Contributions:**

	1. Introduction of Low-Rank Clone (LRC) for efficient SLM training.
	2. Unified design for soft pruning and activation cloning.
	3. Demonstrated over 1,000x training efficiency compared to traditional methods.

**Result:** LRC achieves performance that matches or surpasses state-of-the-art models while using significantly fewer training tokens, demonstrating over 1,000x training efficiency.

**Limitations:** 

**Conclusion:** The proposed LRC method effectively maximizes knowledge transfer and improves training efficiency of SLMs without the drawbacks of previous approaches.

**Abstract:** Training high-performing Small Language Models (SLMs) remains costly, even with knowledge distillation and pruning from larger teacher models. Existing work often faces three key challenges: (1) information loss from hard pruning, (2) inefficient alignment of representations, and (3) underutilization of informative activations, particularly from Feed-Forward Networks (FFNs). To address these challenges, we introduce Low-Rank Clone (LRC), an efficient pre-training method that constructs SLMs aspiring to behavioral equivalence with strong teacher models. LRC trains a set of low-rank projection matrices that jointly enable soft pruning by compressing teacher weights, and activation clone by aligning student activations, including FFN signals, with those of the teacher. This unified design maximizes knowledge transfer while removing the need for explicit alignment modules. Extensive experiments with open-source teachers (e.g., Llama-3.2-3B-Instruct, Qwen2.5-3B/7B-Instruct) show that LRC matches or surpasses state-of-the-art models trained on trillions of tokens--while using only 20B tokens, achieving over 1,000x training efficiency. Our codes and model checkpoints are available at https://github.com/CURRENTF/LowRankClone and https://huggingface.co/collections/JitaiHao/low-rank-clone-lrc-6828389e96a93f1d4219dfaf.

</details>


### [155] [EAVIT: Efficient and Accurate Human Value Identification from Text data via LLMs](https://arxiv.org/abs/2505.12792)

*Wenhao Zhu, Yuhang Xie, Guojie Song, Xin Zhang*

**Main category:** cs.CL

**Keywords:** human value identification, large language models, efficient NLP, value detector, explanation-based training

**Relevance Score:** 9

**TL;DR:** EAVIT is a framework for human value identification combining local and online LLMs to efficiently identify values in text, significantly reducing input token usage while improving accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance human value identification in text data, addressing the limitations of existing LLMs in handling long contexts and reducing computational costs.

**Method:** The EAVIT framework uses a small local language model as a value detector that generates initial estimations, which are refined using online LLMs through optimized input prompts.

**Key Contributions:**

	1. Introduction of EAVIT framework for human value identification.
	2. Implementation of explanation-based training and data generation techniques for value detection.
	3. Innovative sampling strategies to optimize LLM input prompts.

**Result:** EAVIT reduces the number of input tokens by up to 1/6 compared to direct queries to online LLMs and outperforms traditional NLP methods and earlier LLM-based strategies.

**Limitations:** 

**Conclusion:** EAVIT effectively improves the efficiency and accuracy of human value identification tasks by combining local fine-tuning with online LLM capabilities.

**Abstract:** The rapid evolution of large language models (LLMs) has revolutionized various fields, including the identification and discovery of human values within text data. While traditional NLP models, such as BERT, have been employed for this task, their ability to represent textual data is significantly outperformed by emerging LLMs like GPTs. However, the performance of online LLMs often degrades when handling long contexts required for value identification, which also incurs substantial computational costs. To address these challenges, we propose EAVIT, an efficient and accurate framework for human value identification that combines the strengths of both locally fine-tunable and online black-box LLMs. Our framework employs a value detector - a small, local language model - to generate initial value estimations. These estimations are then used to construct concise input prompts for online LLMs, enabling accurate final value identification. To train the value detector, we introduce explanation-based training and data generation techniques specifically tailored for value identification, alongside sampling strategies to optimize the brevity of LLM input prompts. Our approach effectively reduces the number of input tokens by up to 1/6 compared to directly querying online LLMs, while consistently outperforming traditional NLP methods and other LLM-based strategies.

</details>


### [156] [Decentralized Arena: Towards Democratic and Scalable Automatic Evaluation of Language Models](https://arxiv.org/abs/2505.12808)

*Yanbin Yin, Kun Zhou, Zhen Wang, Xiangdong Zhang, Yifei Shao, Shibo Hao, Yi Gu, Jieyuan Liu, Somanshu Singla, Tianyang Liu, Eric P. Xing, Zhengzhong Liu, Haojian Jin, Zhiting Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Decentralized Framework, Collective Intelligence, Automated Evaluation

**Relevance Score:** 9

**TL;DR:** Introduction of a decentralized framework for benchmarking large language models using collective intelligence.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The surge of large language models necessitates effective benchmarking that is scalable and mitigates biases from individual model assessments.

**Method:** The proposed Decentralized Arena (dearena) uses democratic pairwise evaluation and a coarse-to-fine ranking algorithm to assess LLMs efficiently.

**Key Contributions:**

	1. Introduces Decentralized Arena for LLM benchmarking
	2. Utilizes democratic pairwise evaluation to mitigate biases
	3. Achieves high correlation with human evaluations while being cost-effective

**Result:** The framework showed up to 97% correlation with human judgments while significantly lowering evaluation costs across 66 LLMs.

**Limitations:** 

**Conclusion:** dearena provides a robust alternative for LLM benchmarking by leveraging collective intelligence and reducing reliance on biased models.

**Abstract:** The recent explosion of large language models (LLMs), each with its own general or specialized strengths, makes scalable, reliable benchmarking more urgent than ever. Standard practices nowadays face fundamental trade-offs: closed-ended question-based benchmarks (eg MMLU) struggle with saturation as newer models emerge, while crowd-sourced leaderboards (eg Chatbot Arena) rely on costly and slow human judges. Recently, automated methods (eg LLM-as-a-judge) shed light on the scalability, but risk bias by relying on one or a few "authority" models. To tackle these issues, we propose Decentralized Arena (dearena), a fully automated framework leveraging collective intelligence from all LLMs to evaluate each other. It mitigates single-model judge bias by democratic, pairwise evaluation, and remains efficient at scale through two key components: (1) a coarse-to-fine ranking algorithm for fast incremental insertion of new models with sub-quadratic complexity, and (2) an automatic question selection strategy for the construction of new evaluation dimensions. Across extensive experiments across 66 LLMs, dearena attains up to 97% correlation with human judgements, while significantly reducing the cost. Our code and data will be publicly released on https://github.com/maitrix-org/de-arena.

</details>


### [157] [PsyMem: Fine-grained psychological alignment and Explicit Memory Control for Advanced Role-Playing LLMs](https://arxiv.org/abs/2505.12814)

*Xilong Cheng, Yunxiao Qin, Yuting Tan, Zhengnan Li, Ye Wang, Hongjiang Xiao, Yuan Zhang*

**Main category:** cs.CL

**Keywords:** LLM, role-playing, memory alignment, psychological attributes, human-computer interaction

**Relevance Score:** 8

**TL;DR:** PsyMem is a novel framework that enhances role-playing LLMs by integrating detailed psychological characteristics and explicit memory alignment, resulting in improved performance in simulating character interactions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based role-playing methods inadequately model character dimensions and memory, limiting their reliability in applications like social simulation.

**Method:** PsyMem integrates 26 psychological indicators for detailed character modeling and employs memory alignment training to ensure character responses are consistent with their memory during inference.

**Key Contributions:**

	1. Integration of fine-grained psychological attributes for character modeling.
	2. Explicit memory control through memory alignment training.
	3. Demonstrated superior performance over baseline models in role-playing tasks.

**Result:** PsyMem-Qwen, trained on a dataset of 5,414 characters and 38,962 dialogues, outperforms baseline models in human-likeness and character fidelity, indicating better role-playing capabilities.

**Limitations:** 

**Conclusion:** The PsyMem framework significantly improves the reliability and performance of LLMs in various role-playing applications.

**Abstract:** Existing LLM-based role-playing methods often rely on superficial textual descriptions or simplistic metrics, inadequately modeling both intrinsic and extrinsic character dimensions. Additionally, they typically simulate character memory with implicit model knowledge or basic retrieval augment generation without explicit memory alignment, compromising memory consistency. The two issues weaken reliability of role-playing LLMs in several applications, such as trustworthy social simulation. To address these limitations, we propose PsyMem, a novel framework integrating fine-grained psychological attributes and explicit memory control for role-playing. PsyMem supplements textual descriptions with 26 psychological indicators to detailed model character. Additionally, PsyMem implements memory alignment training, explicitly trains the model to align character's response with memory, thereby enabling dynamic memory-controlled responding during inference. By training Qwen2.5-7B-Instruct on our specially designed dataset (including 5,414 characters and 38,962 dialogues extracted from novels), the resulting model, termed as PsyMem-Qwen, outperforms baseline models in role-playing, achieving the best performance in human-likeness and character fidelity.

</details>


### [158] [SynDec: A Synthesize-then-Decode Approach for Arbitrary Textual Style Transfer via Large Language Models](https://arxiv.org/abs/2505.12821)

*Han Sun, Zhen Sun, Zongmin Zhang, Linzhao Jia, Wei Shao, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Style Transfer, Prompt Synthesis, Neural Networks, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** This paper introduces a Synthesize-then-Decode (SynDec) method for improving style transfer in Large Language Models by automating prompt generation and enhancing decoding processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of current Large Language Models in arbitrary style transfer, specifically their dependence on manually crafted prompts and inherent stylistic biases.

**Method:** The SynDec approach synthesizes prompts by selecting few-shot samples, performing a four-dimensional style analysis, and reranking candidates, then amplifies TST effects during decoding by maximizing probability contrasts.

**Key Contributions:**

	1. Introduction of a novel approach for automatic prompt synthesis
	2. Enhanced decoding strategies to improve style transfer results
	3. Validation of the method through comprehensive experiments and ablation studies

**Result:** SynDec significantly outperforms existing LLM-based methods on five out of six benchmarks, achieving up to a 9% increase in accuracy in style transfer tasks such as modern to Elizabethan English.

**Limitations:** 

**Conclusion:** The effectiveness of the SynDec approach is validated through extensive experiments and detailed ablation studies, showcasing substantial improvements in style transfer capabilities.

**Abstract:** Large Language Models (LLMs) are emerging as dominant forces for textual style transfer. However, for arbitrary style transfer, LLMs face two key challenges: (1) considerable reliance on manually-constructed prompts and (2) rigid stylistic biases inherent in LLMs. In this paper, we propose a novel Synthesize-then-Decode (SynDec) approach, which automatically synthesizes high-quality prompts and amplifies their roles during decoding process. Specifically, our approach synthesizes prompts by selecting representative few-shot samples, conducting a four-dimensional style analysis, and reranking the candidates. At LLM decoding stage, the TST effect is amplified by maximizing the contrast in output probabilities between scenarios with and without the synthesized prompt, as well as between prompts and negative samples. We conduct extensive experiments and the results show that SynDec outperforms existing state-of-the-art LLM-based methods on five out of six benchmarks (e.g., achieving up to a 9\% increase in accuracy for modern-to-Elizabethan English transfer). Detailed ablation studies further validate the effectiveness of SynDec.

</details>


### [159] [Contrastive Prompting Enhances Sentence Embeddings in LLMs through Inference-Time Steering](https://arxiv.org/abs/2505.12831)

*Zifeng Cheng, Zhonghui Wang, Yuchen Fu, Zhiwei Jiang, Yafeng Yin, Cong Wang, Qing Gu*

**Main category:** cs.CL

**Keywords:** sentence embeddings, large language models, contrastive prompting, semantic encoding, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper introduces a Contrastive Prompting (CP) method to enhance sentence embeddings extracted from large language models (LLMs) by contrasting with an auxiliary prompt to focus on core semantics and reduce non-essential information.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing techniques for extracting sentence embeddings from LLMs suffer from encoding excess non-essential information, which limits their effective use in tasks.

**Method:** The proposed Contrastive Prompting (CP) method uses an auxiliary prompt in conjunction with the main prompt to drive LLMs to focus on core semantics during embedding extraction.

**Key Contributions:**

	1. Introduces a novel Contrastive Prompting method for better semantic encoding in LLMs.
	2. Demonstrates improved performance on STS and classification tasks.
	3. Provides a plug-and-play method that can enhance existing prompt-based embedding methods.

**Result:** The CP method shows improved performance on Semantic Textual Similarity (STS) tasks and other downstream classification tasks across various LLMs.

**Limitations:** 

**Conclusion:** CP is a flexible, effective approach for enhancing sentence embeddings from LLMs without requiring fine-tuning or additional data, and the code will be made publicly available.

**Abstract:** Extracting sentence embeddings from large language models (LLMs) is a practical direction, as it requires neither additional data nor fine-tuning. Previous studies usually focus on prompt engineering to guide LLMs to encode the core semantic information of the sentence into the embedding of the last token. However, the last token in these methods still encodes an excess of non-essential information, such as stop words, limiting its encoding capacity. To this end, we propose a Contrastive Prompting (CP) method that introduces an extra auxiliary prompt to elicit better sentence embedding. By contrasting with the auxiliary prompt, CP can steer existing prompts to encode the core semantics of the sentence, rather than non-essential information. CP is a plug-and-play inference-time intervention method that can be combined with various prompt-based methods. Extensive experiments on Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our method can improve the performance of existing prompt-based methods across different LLMs. Our code will be released at https://github.com/zifengcheng/CP.

</details>


### [160] [FlightGPT: Towards Generalizable and Interpretable UAV Vision-and-Language Navigation with Vision-Language Models](https://arxiv.org/abs/2505.12835)

*Hengxing Cai, Jinhan Dong, Jingjun Tan, Jingcheng Deng, Sihang Li, Zhifeng Gao, Haidong Wang, Zicheng Su, Agachai Sumalee, Renxin Zhong*

**Main category:** cs.CL

**Keywords:** UAV, Vision-Language Navigation, multimodal perception, Chain-of-Thought, machine learning

**Relevance Score:** 6

**TL;DR:** FlightGPT is a UAV Vision-and-Language Navigation framework that enhances multimodal perception and interpretability via a two-stage training pipeline and a Chain-of-Thought reasoning mechanism.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve UAV Vision-and-Language Navigation by addressing challenges like insufficient multimodal fusion, weak generalization, and poor interpretability in existing methods.

**Method:** The framework employs a two-stage training pipeline consisting of Supervised Fine-Tuning using high-quality demonstrations followed by Group Relative Policy Optimization guided by a composite reward system.

**Key Contributions:**

	1. Development of the FlightGPT UAV VLN framework
	2. Introduction of a two-stage training pipeline
	3. Implementation of a Chain-of-Thought reasoning mechanism

**Result:** FlightGPT achieved state-of-the-art performance on the CityNav dataset with a 9.22% higher success rate than the strongest baseline in unseen environments.

**Limitations:** 

**Conclusion:** FlightGPT demonstrates superior performance and improved interpretability in UAV navigation tasks, with the implementation available for public use.

**Abstract:** Unmanned Aerial Vehicle (UAV) Vision-and-Language Navigation (VLN) is vital for applications such as disaster response, logistics delivery, and urban inspection. However, existing methods often struggle with insufficient multimodal fusion, weak generalization, and poor interpretability. To address these challenges, we propose FlightGPT, a novel UAV VLN framework built upon Vision-Language Models (VLMs) with powerful multimodal perception capabilities. We design a two-stage training pipeline: first, Supervised Fine-Tuning (SFT) using high-quality demonstrations to improve initialization and structured reasoning; then, Group Relative Policy Optimization (GRPO) algorithm, guided by a composite reward that considers goal accuracy, reasoning quality, and format compliance, to enhance generalization and adaptability. Furthermore, FlightGPT introduces a Chain-of-Thought (CoT)-based reasoning mechanism to improve decision interpretability. Extensive experiments on the city-scale dataset CityNav demonstrate that FlightGPT achieves state-of-the-art performance across all scenarios, with a 9.22\% higher success rate than the strongest baseline in unseen environments. Our implementation is publicly available.

</details>


### [161] [The Hidden Structure -- Improving Legal Document Understanding Through Explicit Text Formatting](https://arxiv.org/abs/2505.12837)

*Christian Braun, Alexander Lilienbeck, Daniel Mentjukov*

**Main category:** cs.CL

**Keywords:** LLM, legal applications, prompt engineering, input structure, GPT-4

**Relevance Score:** 7

**TL;DR:** This paper explores how input text structuring and prompt engineering impact the performance of LLMs, particularly GPT-4o and GPT-4.1, on legal question-answering tasks.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of legal contract structures on the performance of large language models in question-answering tasks.

**Method:** The study compares the exact-match accuracy of GPT-4o and GPT-4.1 across various input formats including well-structured text, cleaned text, Azure OCR outputs, and Markdown from GPT-4o Vision, along with different prompt engineering strategies.

**Key Contributions:**

	1. Demonstrated the impact of input structure on LLM performance in legal applications.
	2. Showed that GPT-4.1's performance can greatly improve with structured inputs and optimized prompts.
	3. Provided empirical evidence on the robustness of GPT-4o compared to GPT-4.1 under varying input conditions.

**Result:** GPT-4o shows robustness to input structure variations but falls short in performance. GPT-4.1's performance is sensitive to input structure, achieving a significant improvement in accuracy with well-structured inputs and optimized prompts.

**Limitations:** Limited to legal question-answering tasks and specific models (GPT-4o and GPT-4.1).

**Conclusion:** Effective input structuring and strategic prompt design are essential for enhancing LLM performance in legal contexts, especially for high-stakes applications.

**Abstract:** Legal contracts possess an inherent, semantically vital structure (e.g., sections, clauses) that is crucial for human comprehension but whose impact on LLM processing remains under-explored. This paper investigates the effects of explicit input text structure and prompt engineering on the performance of GPT-4o and GPT-4.1 on a legal question-answering task using an excerpt of the CUAD. We compare model exact-match accuracy across various input formats: well-structured plain-text (human-generated from CUAD), plain-text cleaned of line breaks, extracted plain-text from Azure OCR, plain-text extracted by GPT-4o Vision, and extracted (and interpreted) Markdown (MD) from GPT-4o Vision. To give an indication of the impact of possible prompt engineering, we assess the impact of shifting task instructions to the system prompt and explicitly informing the model about the structured nature of the input. Our findings reveal that GPT-4o demonstrates considerable robustness to variations in input structure, but lacks in overall performance. Conversely, GPT-4.1's performance is markedly sensitive; poorly structured inputs yield suboptimal results (but identical with GPT-4o), while well-structured formats (original CUAD text, GPT-4o Vision text and GPT-4o MD) improve exact-match accuracy by ~20 percentage points. Optimizing the system prompt to include task details and an advisory about structured input further elevates GPT-4.1's accuracy by an additional ~10-13 percentage points, with Markdown ultimately achieving the highest performance under these conditions (79 percentage points overall exact-match accuracy). This research empirically demonstrates that while newer models exhibit greater resilience, careful input structuring and strategic prompt design remain critical for optimizing the performance of LLMs, and can significantly affect outcomes in high-stakes legal applications.

</details>


### [162] [Re-identification of De-identified Documents with Autoregressive Infilling](https://arxiv.org/abs/2505.12859)

*Lucas Georges Gabriel Charpentier, Pierre Lison*

**Main category:** cs.CL

**Keywords:** de-identification, re-identification, RAG, background knowledge, text recovery

**Relevance Score:** 8

**TL;DR:** This paper presents a RAG-inspired method for re-identifying masked personally identifiable information in text by leveraging background knowledge documents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to assess the effectiveness and robustness of de-identification methods in protecting sensitive information.

**Method:** The approach involves a two-step process: (1) using a retriever to select relevant background knowledge passages, and (2) employing an infilling model to infer and replace masked text spans iteratively.

**Key Contributions:**

	1. Introduction of a novel RAG-inspired re-identification method
	2. Demonstration of high recovery rates for masked text spans
	3. Empirical evaluation on diverse datasets including clinical notes.

**Result:** The study reveals that up to 80% of de-identified text spans can be successfully recovered, with re-identification accuracy improving as the amount of background knowledge increases.

**Limitations:** The study primarily focuses on specific datasets (Wikipedia biographies, court rulings, clinical notes), which may not generalize to all contexts of de-identification.

**Conclusion:** De-identification methods may not be as secure as previously thought, highlighting the importance of assessing their robustness against re-identification attacks.

**Abstract:** Documents revealing sensitive information about individuals must typically be de-identified. This de-identification is often done by masking all mentions of personally identifiable information (PII), thereby making it more difficult to uncover the identity of the person(s) in question. To investigate the robustness of de-identification methods, we present a novel, RAG-inspired approach that attempts the reverse process of re-identification based on a database of documents representing background knowledge. Given a text in which personal identifiers have been masked, the re-identification proceeds in two steps. A retriever first selects from the background knowledge passages deemed relevant for the re-identification. Those passages are then provided to an infilling model which seeks to infer the original content of each text span. This process is repeated until all masked spans are replaced. We evaluate the re-identification on three datasets (Wikipedia biographies, court rulings and clinical notes). Results show that (1) as many as 80% of de-identified text spans can be successfully recovered and (2) the re-identification accuracy increases along with the level of background knowledge.

</details>


### [163] [LEXam: Benchmarking Legal Reasoning on 340 Law Exams](https://arxiv.org/abs/2505.12864)

*Yu Fan, Jingwei Ni, Jakob Merane, Etienne Salimbeni, Yang Tian, Yoan Hermstrüwer, Yinya Huang, Mubashara Akhtar, Florian Geering, Oliver Dreyer, Daniel Brunner, Markus Leippold, Mrinmaya Sachan, Alexander Stremitzer, Christoph Engel, Elliott Ash, Joel Niklaus*

**Main category:** cs.CL

**Keywords:** legal reasoning, large language models, benchmark, LLM-as-a-Judge, law exams

**Relevance Score:** 4

**TL;DR:** Introducing LEXam, a benchmark to evaluate legal reasoning in LLMs using real law exam questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges LLMs face in long-form legal reasoning despite advancements in model scaling.

**Method:** LEExam features a dataset of 4,886 law exam questions including guided open-ended questions and multiple-choice questions, evaluated by an LLM-as-a-Judge paradigm with human expert validation.

**Key Contributions:**

	1. Introduction of LEXam benchmark for legal reasoning evaluation
	2. Dataset includes guided reasoning questions
	3. Implementation of LLM-as-a-Judge paradigm for consistent evaluation

**Result:** Current LLMs face significant challenges in responding to structured, multi-step legal reasoning tasks, particularly with long-form questions.

**Limitations:** 

**Conclusion:** The LEXam benchmark effectively distinguishes between models based on their legal reasoning capabilities and provides a scalable evaluation method.

**Abstract:** Long-form legal reasoning remains a key challenge for large language models (LLMs) in spite of recent advances in test-time scaling. We introduce LEXam, a novel benchmark derived from 340 law exams spanning 116 law school courses across a range of subjects and degree levels. The dataset comprises 4,886 law exam questions in English and German, including 2,841 long-form, open-ended questions and 2,045 multiple-choice questions. Besides reference answers, the open questions are also accompanied by explicit guidance outlining the expected legal reasoning approach such as issue spotting, rule recall, or rule application. Our evaluation on both open-ended and multiple-choice questions present significant challenges for current LLMs; in particular, they notably struggle with open questions that require structured, multi-step legal reasoning. Moreover, our results underscore the effectiveness of the dataset in differentiating between models with varying capabilities. Adopting an LLM-as-a-Judge paradigm with rigorous human expert validation, we demonstrate how model-generated reasoning steps can be evaluated consistently and accurately. Our evaluation setup provides a scalable method to assess legal reasoning quality beyond simple accuracy metrics. Project page: https://lexam-benchmark.github.io/

</details>


### [164] [GAP: Graph-Assisted Prompts for Dialogue-based Medication Recommendation](https://arxiv.org/abs/2505.12888)

*Jialun Zhong, Yanzeng Li, Sen Hu, Yang Zhang, Teng Xu, Lei Zou*

**Main category:** cs.CL

**Keywords:** Medication recommendation, Medical dialogue systems, Large language models, Graph-assisted prompts, Health informatics

**Relevance Score:** 9

**TL;DR:** This paper presents a Graph-Assisted Prompts (GAP) framework to enhance dialogue-based medication recommendations using large language models by addressing challenges in multi-turn conversations and knowledge retrieval.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and safety of medical dialogue systems by addressing gaps in multi-turn dialogue interactions, particularly in medication recommendations.

**Method:** The GAP framework constructs a patient-centric graph from dialogues, integrating external medical knowledge graphs to generate queries and prompts for effective information retrieval.

**Key Contributions:**

	1. Introduction of the GAP framework for medication recommendations
	2. Integration of patient-centric graphs with external medical knowledge
	3. Demonstration of performance improvements over existing baselines

**Result:** GAP demonstrates competitive performance on a medication recommendation dataset and shows potential in dynamic diagnostic interviewing scenarios, outperforming strong baseline methods.

**Limitations:** The framework's effectiveness may vary with the quality of the underlying knowledge graphs and may require extensive real-world validation.

**Conclusion:** GAP successfully aids in reducing non-factual responses from LLMs and enhances the quality of medication recommendations in dialogue settings.

**Abstract:** Medication recommendations have become an important task in the healthcare domain, especially in measuring the accuracy and safety of medical dialogue systems (MDS). Different from the recommendation task based on electronic health records (EHRs), dialogue-based medication recommendations require research on the interaction details between patients and doctors, which is crucial but may not exist in EHRs. Recent advancements in large language models (LLM) have extended the medical dialogue domain. These LLMs can interpret patients' intent and provide medical suggestions including medication recommendations, but some challenges are still worth attention. During a multi-turn dialogue, LLMs may ignore the fine-grained medical information or connections across the dialogue turns, which is vital for providing accurate suggestions. Besides, LLMs may generate non-factual responses when there is a lack of domain-specific knowledge, which is more risky in the medical domain. To address these challenges, we propose a \textbf{G}raph-\textbf{A}ssisted \textbf{P}rompts (\textbf{GAP}) framework for dialogue-based medication recommendation. It extracts medical concepts and corresponding states from dialogue to construct an explicitly patient-centric graph, which can describe the neglected but important information. Further, combined with external medical knowledge graphs, GAP can generate abundant queries and prompts, thus retrieving information from multiple sources to reduce the non-factual responses. We evaluate GAP on a dialogue-based medication recommendation dataset and further explore its potential in a more difficult scenario, dynamically diagnostic interviewing. Extensive experiments demonstrate its competitive performance when compared with strong baselines.

</details>


### [165] [On the Thinking-Language Modeling Gap in Large Language Models](https://arxiv.org/abs/2505.12896)

*Chenxi Liu, Yongqiang Chen, Tongliang Liu, James Cheng, Bo Han, Kun Zhang*

**Main category:** cs.CL

**Keywords:** System 2 reasoning, Large Language Models, Language-of-Thoughts, Biases in LLMs, Causal reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel prompt technique termed Language-of-Thoughts (LoT) to address biases in Large Language Models (LLMs) that hinder System 2 reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant gap between language modeling in LLMs and actual human thought processes, particularly regarding biases that mislead reasoning.

**Method:** The authors propose the Language-of-Thoughts (LoT) prompting technique, which modifies the order and token usage of expressions to include all relevant information rather than just partial cues.

**Key Contributions:**

	1. Introduction of the Language-of-Thoughts (LoT) prompt technique
	2. Demonstration of the gap between language and thought modeling in LLMs
	3. Improved performance of LLMs in reasoning tasks by reducing biases.

**Result:** LoT significantly reduces language modeling biases in LLMs, leading to improved performance on various reasoning tasks.

**Limitations:** 

**Conclusion:** Implementing LoT enhances the elicitation of thoughts in LLMs, allowing for better alignment with human reasoning processes.

**Abstract:** System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of "thoughts" in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks.

</details>


### [166] [PyFCG: Fluid Construction Grammar in Python](https://arxiv.org/abs/2505.12920)

*Paul Van Eecke, Katrien Beuls*

**Main category:** cs.CL

**Keywords:** Fluid Construction Grammar, Python library, agent-based experiments

**Relevance Score:** 4

**TL;DR:** Introduction of PyFCG, a library for using Fluid Construction Grammar in Python.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to provide an open source library that facilitates the use of Fluid Construction Grammar (FCG) within the Python programming ecosystem, enhancing its accessibility and integration.

**Method:** This paper describes the PyFCG library and provides three tutorials demonstrating its application in formalising grammar analyses, learning grammar from data, and implementing agent-based experiments.

**Key Contributions:**

	1. Open source library for Fluid Construction Grammar in Python.
	2. Integration capabilities with other Python libraries.
	3. Tutorials for practical applications of FCG in language analysis and experiments.

**Result:** PyFCG allows seamless integration of construction grammar analyses and experiments into Python applications, showcasing practical use through tutorials.

**Limitations:** 

**Conclusion:** The library expands the capabilities of researchers and developers to utilize Fluid Construction Grammar in Python, providing tools for teaching and experimentation.

**Abstract:** We present PyFCG, an open source software library that ports Fluid Construction Grammar (FCG) to the Python programming language. PyFCG enables its users to seamlessly integrate FCG functionality into Python programs, and to use FCG in combination with other libraries within Python's rich ecosystem. Apart from a general description of the library, this paper provides three walkthrough tutorials that demonstrate example usage of PyFCG in typical use cases of FCG: (i) formalising and testing construction grammar analyses, (ii) learning usage-based construction grammars from corpora, and (iii) implementing agent-based experiments on emergent communication.

</details>


### [167] [Do Not Let Low-Probability Tokens Over-Dominate in RL for LLMs](https://arxiv.org/abs/2505.12929)

*Zhihe Yang, Xufang Luo, Zilong Wang, Dongqi Han, Zhiyuan He, Dongsheng Li, Yunjian Xu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Gradient Optimization

**Relevance Score:** 8

**TL;DR:** The paper introduces two methods, Advantage Reweighting and Low-Probability Token Isolation, to improve reinforcement learning training for large language models by reducing the influence of low-probability tokens.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of low-probability tokens disproportionately affecting reinforcement learning (RL) model updates, which hinders the effective learning of essential high-probability tokens.

**Method:** The authors propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti). These methods reduce the gradient influence from low-probability tokens while promoting parameter updates from high-probability tokens to ensure balanced token learning.

**Key Contributions:**

	1. Introduction of Advantage Reweighting method
	2. Development of Low-Probability Token Isolation (Lopti) method
	3. Demonstration of substantial performance improvements on logic reasoning tasks

**Result:** The proposed methods lead to substantial performance improvements in GRPO-trained large language models, achieving up to a 46.2% improvement on K&K Logic Puzzle reasoning tasks.

**Limitations:** 

**Conclusion:** The techniques enhance RL training efficiency for LLMs by promoting balanced learning across tokens of varying probabilities, improving overall model performance.

**Abstract:** Reinforcement learning (RL) has become a cornerstone for enhancing the reasoning capabilities of large language models (LLMs), with recent innovations such as Group Relative Policy Optimization (GRPO) demonstrating exceptional effectiveness. In this study, we identify a critical yet underexplored issue in RL training: low-probability tokens disproportionately influence model updates due to their large gradient magnitudes. This dominance hinders the effective learning of high-probability tokens, whose gradients are essential for LLMs' performance but are substantially suppressed. To mitigate this interference, we propose two novel methods: Advantage Reweighting and Low-Probability Token Isolation (Lopti), both of which effectively attenuate gradients from low-probability tokens while emphasizing parameter updates driven by high-probability tokens. Our approaches promote balanced updates across tokens with varying probabilities, thereby enhancing the efficiency of RL training. Experimental results demonstrate that they substantially improve the performance of GRPO-trained LLMs, achieving up to a 46.2% improvement in K&K Logic Puzzle reasoning tasks. Our implementation is available at https://github.com/zhyang2226/AR-Lopti.

</details>


### [168] [A3 : an Analytical Low-Rank Approximation Framework for Attention](https://arxiv.org/abs/2505.12942)

*Jeffrey T. H. Wong, Cheng Zhang, Xinye Cao, Pedro Gimenes, George A. Constantinides, Wayne Luk, Yiren Zhao*

**Main category:** cs.CL

**Keywords:** low-rank approximation, Transformer models, model compression

**Relevance Score:** 7

**TL;DR:** The paper introduces $	t A^	t 3$, a novel low-rank approximation framework for efficient Transformer model compression that overcomes limitations of existing methods by optimizing multiple components without runtime overheads.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing low-rank approximation methods for Transformer models have significant limitations in performance and efficiency, motivating the need for a new approach.

**Method:** $	t A^	t 3$ decomposes a Transformer layer into three components (QK, OV, MLP) and optimizes each to minimize functional loss while reducing model size and computational requirements.

**Key Contributions:**

	1. Introduction of a new low-rank approximation framework for Transformers
	2. Optimization of multiple components rather than individual layers
	3. Demonstration of superior performance compared to existing state-of-the-art methods.

**Result:** $	t A^	t 3$ outperforms state-of-the-art methods, achieving a perplexity of 4.69 on WikiText-2 for a low-rank approximated LLaMA model, a significant improvement over previous records.

**Limitations:** 

**Conclusion:** The proposed method provides a practical framework for effectively compressing large language models while maintaining high performance without additional runtime costs.

**Abstract:** Large language models have demonstrated remarkable performance; however, their massive parameter counts make deployment highly expensive. Low-rank approximation offers a promising compression solution, yet existing approaches have two main limitations: (1) They focus on minimizing the output error of individual linear layers, without considering the architectural characteristics of Transformers, and (2) they decompose a large weight matrix into two small low-rank matrices. Consequently, these methods often fall short compared to other compression techniques like pruning and quantization, and introduce runtime overhead such as the extra GEMM kernel launches for decomposed small matrices. To address these limitations, we propose $\tt A^\tt 3$, a post-training low-rank approximation framework. $\tt A^\tt 3$ splits a Transformer layer into three functional components, namely $\tt QK$, $\tt OV$, and $\tt MLP$. For each component, $\tt A^\tt 3$ provides an analytical solution that reduces the hidden dimension size inside each component while minimizing the component's functional loss ($\it i.e.$, error in attention scores, attention outputs, and MLP outputs). This approach directly reduces model sizes, KV cache sizes, and FLOPs without introducing any runtime overheads. In addition, it provides a new narrative in advancing the optimization problem from singular linear layer loss optimization toward improved end-to-end performance. Through extensive experiments, we show that $\tt A^\tt 3$ maintains superior performance compared to SoTAs. For example, under the same reduction budget in computation and memory, our low-rank approximated LLaMA 3.1-70B achieves a perplexity of 4.69 on WikiText-2, outperforming the previous SoTA's 7.87 by 3.18. We also demonstrate the versatility of $\tt A^\tt 3$, including KV cache compression, quantization, and mixed-rank assignments for enhanced performance.

</details>


### [169] [Neural Morphological Tagging for Nguni Languages](https://arxiv.org/abs/2505.12949)

*Cael Marquard, Simbarashe Mawere, Francois Meyer*

**Main category:** cs.CL

**Keywords:** morphological parsing, neural methods, Nguni languages, sequence labelling, pretrained models

**Relevance Score:** 5

**TL;DR:** This paper explores neural methods for morphological tagging in Nguni languages, demonstrating that neural taggers can outperform traditional rule-based methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Morphological parsing is crucial for understanding agglutinative languages like the Nguni languages, where words are formed by concatenating multiple morphemes.

**Method:** The study compares two neural approaches: training sequence labellers (LSTMs and CRFs) from scratch versus fine-tuning pretrained language models, alongside a traditional rule-based parser.

**Key Contributions:**

	1. Comparison of neural sequence labellers with pretrained models
	2. Demonstration of the effectiveness of neural methods for agglutinative languages
	3. Evaluation of performance across different segmenters and features

**Result:** Neural taggers outperform the rule-based baseline, with models trained from scratch generally performing better than those using pretrained models.

**Limitations:** 

**Conclusion:** Neural taggers based on existing morphological segmenters are effective for the Nguni languages.

**Abstract:** Morphological parsing is the task of decomposing words into morphemes, the smallest units of meaning in a language, and labelling their grammatical roles. It is a particularly challenging task for agglutinative languages, such as the Nguni languages of South Africa, which construct words by concatenating multiple morphemes. A morphological parsing system can be framed as a pipeline with two separate components, a segmenter followed by a tagger. This paper investigates the use of neural methods to build morphological taggers for the four Nguni languages. We compare two classes of approaches: training neural sequence labellers (LSTMs and neural CRFs) from scratch and finetuning pretrained language models. We compare performance across these two categories, as well as to a traditional rule-based morphological parser. Neural taggers comfortably outperform the rule-based baseline and models trained from scratch tend to outperform pretrained models. We also compare parsing results across different upstream segmenters and with varying linguistic input features. Our findings confirm the viability of employing neural taggers based on pre-existing morphological segmenters for the Nguni languages.

</details>


### [170] [GuRE:Generative Query REwriter for Legal Passage Retrieval](https://arxiv.org/abs/2505.12950)

*Daehee Kim, Deokhyung Kang, Jonghwi Kim, Sangwon Ryu, Gary Geunbae Lee*

**Main category:** cs.CL

**Keywords:** Legal Passage Retrieval, Generative Query Rewriter, Large Language Models, Vocabulary Mismatch, Retrieval Performance

**Relevance Score:** 4

**TL;DR:** The paper presents GuRE, a generative query rewriting method that improves Legal Passage Retrieval by addressing vocabulary mismatches using LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of Legal Passage Retrieval (LPR) systems by addressing the vocabulary mismatch between user queries and target passages.

**Method:** By training a Large Language Model (LLM) specifically for rewriting user queries, GuRE generates rewritten queries designed to enhance passage retrieval performance.

**Key Contributions:**

	1. Introduction of the Generative query REwriter (GuRE) for LPR systems
	2. Demonstration of superior performance over standard baseline methods
	3. Analysis of training objectives leading to improved retrieval behaviors

**Result:** Experimental results indicate that GuRE significantly outperforms baseline methods and exhibits better performance in a retriever-agnostic manner.

**Limitations:** 

**Conclusion:** GuRE's distinct training objectives lead to varied retrieval behaviors, which may benefit real-world applications more than typical retriever fine-tuning.

**Abstract:** Legal Passage Retrieval (LPR) systems are crucial as they help practitioners save time when drafting legal arguments. However, it remains an underexplored avenue. One primary reason is the significant vocabulary mismatch between the query and the target passage. To address this, we propose a simple yet effective method, the Generative query REwriter (GuRE). We leverage the generative capabilities of Large Language Models (LLMs) by training the LLM for query rewriting. "Rewritten queries" help retrievers to retrieve target passages by mitigating vocabulary mismatch. Experimental results show that GuRE significantly improves performance in a retriever-agnostic manner, outperforming all baseline methods. Further analysis reveals that different training objectives lead to distinct retrieval behaviors, making GuRE more suitable than direct retriever fine-tuning for real-world applications. Codes are avaiable at github.com/daehuikim/GuRE.

</details>


### [171] [MA-COIR: Leveraging Semantic Search Index and Generative Models for Ontology-Driven Biomedical Concept Recognition](https://arxiv.org/abs/2505.12964)

*Shanshan Liu, Noriki Nishida, Rumana Ferdous Munne, Narumi Tokunaga, Yuki Yamagata, Kouji Kozaki, Yuji Matsumoto*

**Main category:** cs.CL

**Keywords:** biomedical concept recognition, ontology refinement, large language models

**Relevance Score:** 8

**TL;DR:** MA-COIR reformulates biomedical concept recognition as an indexing-recognition task to enhance efficiency and resolve ambiguities, utilizing semantic search indexes and language models for improved recognition in low-resource environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve ontology refinement, knowledge graph construction, and concept relationship discovery in the biomedical domain, addressing the shortcomings of traditional methods that rely on explicit mention identification.

**Method:** The MA-COIR framework uses semantic search indexes (ssIDs) for concept recognition, allowing it to handle complex concepts. It employs a pretrained BART-based model fine-tuned on small datasets and integrates LLM-generated queries and synthetic data.

**Key Contributions:**

	1. Introduced MA-COIR framework for improved concept recognition in biomedicine
	2. Utilization of semantic search indexes to resolve ambiguities
	3. Integration of LLM-generated queries and synthetic data for low-resource recognition

**Result:** Experimental results demonstrate that MA-COIR effectively recognizes explicit and implicit biomedical concepts without mention-level annotations, outperforming traditional methods.

**Limitations:** 

**Conclusion:** MA-COIR advances ontology-driven concept recognition in biomedical applications, proving efficient in recognizing complex concepts with reduced computational requirements.

**Abstract:** Recognizing biomedical concepts in the text is vital for ontology refinement, knowledge graph construction, and concept relationship discovery. However, traditional concept recognition methods, relying on explicit mention identification, often fail to capture complex concepts not explicitly stated in the text. To overcome this limitation, we introduce MA-COIR, a framework that reformulates concept recognition as an indexing-recognition task. By assigning semantic search indexes (ssIDs) to concepts, MA-COIR resolves ambiguities in ontology entries and enhances recognition efficiency. Using a pretrained BART-based model fine-tuned on small datasets, our approach reduces computational requirements to facilitate adoption by domain experts. Furthermore, we incorporate large language models (LLMs)-generated queries and synthetic data to improve recognition in low-resource settings. Experimental results on three scenarios (CDR, HPO, and HOIP) highlight the effectiveness of MA-COIR in recognizing both explicit and implicit concepts without the need for mention-level annotations during inference, advancing ontology-driven concept recognition in biomedical domain applications. Our code and constructed data are available at https://github.com/sl-633/macoir-master.

</details>


### [172] [Calm-Whisper: Reduce Whisper Hallucination On Non-Speech By Calming Crazy Heads Down](https://arxiv.org/abs/2505.12969)

*Yingzhi Wang, Anas Alhmoud, Saad Alsahly, Muhammad Alqurishi, Mirco Ravanelli*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Whisper, hallucination, fine-tuning, self-attention

**Relevance Score:** 7

**TL;DR:** This paper presents a method to reduce hallucinations in OpenAI's Whisper model during Automatic Speech Recognition, specifically targeting non-speech segments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination issues in Whisper that hinder its application in complex industrial settings.

**Method:** The authors benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder through a head-wise mask and fine-tune the heads that predominantly contribute to hallucination.

**Key Contributions:**

	1. Introduces a novel method to reduce hallucinations in Whisper
	2. Identifies key self-attentional heads responsible for hallucinations
	3. Demonstrates significant reduction in hallucinations with minimal performance loss

**Result:** The best fine-tuned model, Calm-Whisper, achieves over 80% reduction in non-speech hallucination with less than 0.1% Word Error Rate (WER) degradation on test datasets.

**Limitations:** 

**Conclusion:** The proposed fine-tuning method effectively reduces hallucinations in non-speech segments of the Whisper model without significantly affecting overall performance.

**Abstract:** OpenAI's Whisper has achieved significant success in Automatic Speech Recognition. However, it has consistently been found to exhibit hallucination issues, particularly in non-speech segments, which limits its broader application in complex industrial settings.   In this paper, we introduce a novel method to reduce Whisper's hallucination on non-speech segments without using any pre- or post-possessing techniques. Specifically, we benchmark the contribution of each self-attentional head in the Whisper-large-v3 decoder to the hallucination problem by performing a head-wise mask. Our findings reveal that only 3 of the 20 heads account for over 75% of the hallucinations on the UrbanSound dataset. We then fine-tune these three crazy heads using a collection of non-speech data. The results show that our best fine-tuned model, namely Calm-Whisper, achieves over 80% reduction in non-speech hallucination with only less than 0.1% WER degradation on LibriSpeech test-clean and test-other.

</details>


### [173] [A Structured Literature Review on Traditional Approaches in Current Natural Language Processing](https://arxiv.org/abs/2505.12970)

*Robin Jegan, Andreas Henrich*

**Main category:** cs.CL

**Keywords:** natural language processing, traditional techniques, large language models, application scenarios, survey

**Relevance Score:** 7

**TL;DR:** This paper surveys the current state of traditional and older natural language processing techniques in the context of five application scenarios and assesses their future perspectives amidst the rise of neural networks and large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the relevance and application of traditional NLP techniques in light of the success of large language models in various NLP tasks.

**Method:** A survey of recent publications focusing on five application scenarios: classification, information extraction, relation extraction, text simplification, and text summarization. The paper defines terminology and assesses the usage of traditional approaches.

**Key Contributions:**

	1. Comprehensive survey of traditional NLP techniques in five application areas
	2. Assessment of the role and utility of older methods in conjunction with modern LLMs
	3. Statistical data on usage of traditional models in current research

**Result:** The survey finds that traditional NLP methods are still in use across all five scenarios, serving as either part of processing pipelines, comparison baselines, or main models.

**Limitations:** 

**Conclusion:** Despite advancements in LLMs, traditional techniques remain relevant and are utilized in various capacities within modern NLP applications.

**Abstract:** The continued rise of neural networks and large language models in the more recent past has altered the natural language processing landscape, enabling new approaches towards typical language tasks and achieving mainstream success. Despite the huge success of large language models, many disadvantages still remain and through this work we assess the state of the art in five application scenarios with a particular focus on the future perspectives and sensible application scenarios of traditional and older approaches and techniques.   In this paper we survey recent publications in the application scenarios classification, information and relation extraction, text simplification as well as text summarization. After defining our terminology, i.e., which features are characteristic for traditional techniques in our interpretation for the five scenarios, we survey if such traditional approaches are still being used, and if so, in what way they are used. It turns out that all five application scenarios still exhibit traditional models in one way or another, as part of a processing pipeline, as a comparison/baseline to the core model of the respective paper, or as the main model(s) of the paper. For the complete statistics, see https://zenodo.org/records/13683801

</details>


### [174] [Fast, Not Fancy: Rethinking G2P with Rich Data and Rule-Based Models](https://arxiv.org/abs/2505.12973)

*Mahta Fetrat Qharabagh, Zahra Dehghanian, Hamid R. Rabiee*

**Main category:** cs.CL

**Keywords:** homograph disambiguation, grapheme-to-phoneme conversion, low-resource languages, machine learning, accessibility tools

**Relevance Score:** 7

**TL;DR:** The paper addresses the challenge of homograph disambiguation in grapheme-to-phoneme conversion for low-resource languages by proposing a semi-automated dataset creation pipeline and improving a rule-based G2P system for better performance in accessibility applications.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** Homograph disambiguation is essential for enhancing G2P systems, especially for low-resource languages, and current methods face significant limitations in dataset creation and real-time application suitability.

**Method:** A semi-automated pipeline was developed to create comprehensive homograph datasets, specifically the HomoRich dataset. This dataset was then applied to improve both a deep learning-based G2P system for Persian and the eSpeak rule-based system, creating a faster version called HomoFast eSpeak.

**Key Contributions:**

	1. Creation of the HomoRich dataset through a semi-automated pipeline
	2. Development of the fast homograph-aware eSpeak system (HomoFast eSpeak)
	3. Demonstration of improved accuracy in homograph disambiguation for G2P systems.

**Result:** The proposed improvements resulted in approximately a 30% increase in homograph disambiguation accuracy for both the deep learning-based and rule-based systems.

**Limitations:** 

**Conclusion:** Utilizing sophisticated offline datasets can enhance the performance of fast, rule-based G2P systems, making them suitable for latency-sensitive applications like screen readers.

**Abstract:** Homograph disambiguation remains a significant challenge in grapheme-to-phoneme (G2P) conversion, especially for low-resource languages. This challenge is twofold: (1) creating balanced and comprehensive homograph datasets is labor-intensive and costly, and (2) specific disambiguation strategies introduce additional latency, making them unsuitable for real-time applications such as screen readers and other accessibility tools. In this paper, we address both issues. First, we propose a semi-automated pipeline for constructing homograph-focused datasets, introduce the HomoRich dataset generated through this pipeline, and demonstrate its effectiveness by applying it to enhance a state-of-the-art deep learning-based G2P system for Persian. Second, we advocate for a paradigm shift - utilizing rich offline datasets to inform the development of fast, rule-based methods suitable for latency-sensitive accessibility applications like screen readers. To this end, we improve one of the most well-known rule-based G2P systems, eSpeak, into a fast homograph-aware version, HomoFast eSpeak. Our results show an approximate 30% improvement in homograph disambiguation accuracy for the deep learning-based and eSpeak systems.

</details>


### [175] [An Empirical Study of Many-to-Many Summarization with Large Language Models](https://arxiv.org/abs/2505.12983)

*Jiaan Wang, Fandong Meng, Zengkui Sun, Yunlong Liang, Yuxuan Cao, Jiarong Xu, Haoxiang Shi, Jie Zhou*

**Main category:** cs.CL

**Keywords:** many-to-many summarization, large language models, instruction tuning, multilingual, factuality issue

**Relevance Score:** 9

**TL;DR:** This paper investigates many-to-many summarization (M2MS) capabilities of large language models (LLMs) across multiple languages and domains and highlights the potential of instruction tuning to enhance their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the multilingual summarization capabilities of LLMs and address the existing challenges, such as factuality issues, in real-world applications.

**Method:** A systematic empirical study was conducted, reorganizing M2MS datasets and benchmarking 18 LLMs in both zero-shot and instruction-tuning scenarios. Comparisons were made with fine-tuned traditional models like mBART.

**Key Contributions:**

	1. Systematic evaluation of LLMs for many-to-many summarization across multiple languages and domains.
	2. Demonstration of the enhanced summarization capabilities of LLMs with instruction tuning.
	3. Identification of the factuality issue as a critical challenge for LLM summation applications.

**Result:** LLMs achieved competitive results in zero-shot conditions compared to fine-tuned models, and significantly improved their M2MS performance with instruction tuning. However, factuality issues remain a concern which can be exacerbated by instruction tuning.

**Limitations:** Factuality issues persist in LLMs, which may improve with tuning but can also worsen exposure to errors.

**Conclusion:** Controlling factual errors is crucial when deploying LLMs for summarization tasks, and future research should focus on this aspect.

**Abstract:** Many-to-many summarization (M2MS) aims to process documents in any language and generate the corresponding summaries also in any language. Recently, large language models (LLMs) have shown strong multi-lingual abilities, giving them the potential to perform M2MS in real applications. This work presents a systematic empirical study on LLMs' M2MS ability. Specifically, we first reorganize M2MS data based on eight previous domain-specific datasets. The reorganized data contains 47.8K samples spanning five domains and six languages, which could be used to train and evaluate LLMs. Then, we benchmark 18 LLMs in a zero-shot manner and an instruction-tuning manner. Fine-tuned traditional models (e.g., mBART) are also conducted for comparisons. Our experiments reveal that, zero-shot LLMs achieve competitive results with fine-tuned traditional models. After instruct-tuning, open-source LLMs can significantly improve their M2MS ability, and outperform zero-shot LLMs (including GPT-4) in terms of automatic evaluations. In addition, we demonstrate that this task-specific improvement does not sacrifice the LLMs' general task-solving abilities. However, as revealed by our human evaluation, LLMs still face the factuality issue, and the instruction tuning might intensify the issue. Thus, how to control factual errors becomes the key when building LLM summarizers in real applications, and is worth noting in future research.

</details>


### [176] [ExTrans: Multilingual Deep Reasoning Translation via Exemplar-Enhanced Reinforcement Learning](https://arxiv.org/abs/2505.12996)

*Jiaan Wang, Fandong Meng, Jie Zhou*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Machine Translation, Large Reasoning Models

**Relevance Score:** 7

**TL;DR:** This paper presents a new reward modeling method in reinforcement learning for machine translation that significantly improves performance across multiple languages using large reasoning models (LRMs).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in existing reinforcement learning approaches for machine translation, particularly their focus on high-resource languages and inadequate reward modeling.

**Method:** A new reward modeling method is designed that compares the translation results of a policy MT model against a strong LRM, using this comparison to quantify and provide rewards in reinforcement learning scenarios.

**Key Contributions:**

	1. Development of a new reward modeling technique for MT
	2. Achievement of state-of-the-art performance in literary translation
	3. Extension of MT capabilities to 11 languages using a lightweight RL approach

**Result:** The proposed method achieves state-of-the-art performance in literary translation, outperforming leading LRMs, and successfully extends multilingual translation capabilities across 11 languages with a lightweight reward modeling approach.

**Limitations:** The focus on specific reward modeling methods may still leave questions about generalizability across all language pairs and types of translation tasks.

**Conclusion:** The paper demonstrates that a well-designed reward modeling technique can enhance machine translation performance significantly, allowing for effective transferability across multiple translation directions.

**Abstract:** In recent years, the emergence of large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, has shown impressive capabilities in complex problems, e.g., mathematics and coding. Some pioneering studies attempt to bring the success of LRMs in neural machine translation (MT). They try to build LRMs with deep reasoning MT ability via reinforcement learning (RL). Despite some progress that has been made, these attempts generally focus on several high-resource languages, e.g., English and Chinese, leaving the performance on other languages unclear. Besides, the reward modeling methods in previous work do not fully unleash the potential of reinforcement learning in MT. In this work, we first design a new reward modeling method that compares the translation results of the policy MT model with a strong LRM (i.e., DeepSeek-R1-671B), and quantifies the comparisons to provide rewards. Experimental results demonstrate the superiority of the reward modeling method. Using Qwen2.5-7B-Instruct as the backbone, the trained model achieves the new state-of-the-art performance in literary translation, and outperforms strong LRMs including OpenAI-o1 and DeepSeeK-R1. Furthermore, we extend our method to the multilingual settings with 11 languages. With a carefully designed lightweight reward modeling in RL, we can simply transfer the strong MT ability from a single direction into multiple (i.e., 90) translation directions and achieve impressive multilingual MT performance.

</details>


### [177] [EffiBench-X: A Multi-Language Benchmark for Measuring Efficiency of LLM-Generated Code](https://arxiv.org/abs/2505.13004)

*Yuhao Qing, Boyu Zhu, Mingzhe Du, Zhijiang Guo, Terry Yue Zhuo, Qianru Zhang, Jie M. Zhang, Heming Cui, Siu-Ming Yiu, Dong Huang, See-Kiong Ng, Luu Anh Tuan*

**Main category:** cs.CL

**Keywords:** LLM-generated code, efficiency benchmarking, multi-language code generation, competitive programming, code optimization

**Relevance Score:** 8

**TL;DR:** EffiBench-X is a multi-language benchmark for evaluating the efficiency of LLM-generated code across various programming languages, revealing underperformance in efficiency compared to human experts despite functional correctness.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in existing code generation benchmarks which mainly focus on functional correctness and often limit analysis to a single language, we propose EffiBench-X to assess code efficiency across multiple programming languages.

**Method:** EffiBench-X includes competitive programming tasks with human expert solutions serving as benchmarks for efficiency. It evaluates LLM-generated code in languages such as Python, C++, Java, JavaScript, Ruby, and Golang.

**Key Contributions:**

	1. Introduction of EffiBench-X as a multi-language efficiency benchmark for code generated by LLMs.
	2. Demonstration that LLM-generated code is functionally correct but less efficient than human-generated code, revealing potential areas for improvement.
	3. Public availability of the benchmark dataset and evaluation infrastructure.

**Result:** Evaluation of state-of-the-art LLMs shows that while they generate functionally correct code, they achieve only 62% of the efficiency of human experts on average, with variation across programming languages.

**Limitations:** The benchmark may not capture all aspects of code performance, and the efficiency results might vary with different LLMs not tested in this study.

**Conclusion:** The findings indicate a significant need for LLM optimization strategies to enhance code efficiency across various languages, as models perform better in some languages (Python, Ruby, JavaScript) than in others (Java, C++, Golang).

**Abstract:** Existing code generation benchmarks primarily evaluate functional correctness, with limited focus on code efficiency and often restricted to a single language like Python. To address this gap, we introduce EffiBench-X, the first multi-language benchmark designed to measure the efficiency of LLM-generated code. EffiBench-X supports Python, C++, Java, JavaScript, Ruby, and Golang. It comprises competitive programming tasks with human-expert solutions as efficiency baselines. Evaluating state-of-the-art LLMs on EffiBench-X reveals that while models generate functionally correct code, they consistently underperform human experts in efficiency. Even the most efficient LLM-generated solutions (Qwen3-32B) achieve only around \textbf{62\%} of human efficiency on average, with significant language-specific variations. LLMs show better efficiency in Python, Ruby, and JavaScript than in Java, C++, and Golang. For instance, DeepSeek-R1's Python code is significantly more efficient than its Java code. These results highlight the critical need for research into LLM optimization techniques to improve code efficiency across diverse languages. The dataset and evaluation infrastructure are submitted and available at https://github.com/EffiBench/EffiBench-X.git and https://huggingface.co/datasets/EffiBench/effibench-x.

</details>


### [178] [Evaluating the Performance of RAG Methods for Conversational AI in the Airport Domain](https://arxiv.org/abs/2505.13006)

*Yuyang Li, Philip J. M. Kerbusch, Raimon H. R. Pruim, Tobias Käfer*

**Main category:** cs.CL

**Keywords:** Conversational AI, Retrieval-Augmented Generation, Airport automation, Dynamic questioning, AI safety

**Relevance Score:** 8

**TL;DR:** The paper presents a Conversational AI system designed for airport staff to communicate with flight information systems, utilizing three RAG methods to enhance accuracy and reduce hallucinations in responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Airports aim to increase automation and improve communication with flight information systems. This paper addresses the challenge of integrating conversational AI in highly dynamic airport environments.

**Method:** Three Retrieval-Augmented Generation (RAG) methods were implemented: traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG), which were evaluated based on accuracy and hallucination rates.

**Key Contributions:**

	1. Development of a Conversational AI system for airport staff
	2. Implementation of three RAG methods
	3. Recommendation of SQL RAG and Graph RAG for better performance in airport environments

**Result:** Traditional RAG achieved 84.84% accuracy but had issues with hallucinations. SQL RAG and Graph RAG scored 80.85% and 91.49% accuracy respectively, with Graph RAG being particularly effective for reasoning-based questions.

**Limitations:** The traditional RAG method's tendency to produce hallucinations poses a safety risk in airport contexts.

**Conclusion:** SQL RAG and Graph RAG are recommended for airport environments due to their lower hallucination rates and better handling of dynamic questions.

**Abstract:** Airports from the top 20 in terms of annual passengers are highly dynamic environments with thousands of flights daily, and they aim to increase the degree of automation. To contribute to this, we implemented a Conversational AI system that enables staff in an airport to communicate with flight information systems. This system not only answers standard airport queries but also resolves airport terminology, jargon, abbreviations, and dynamic questions involving reasoning. In this paper, we built three different Retrieval-Augmented Generation (RAG) methods, including traditional RAG, SQL RAG, and Knowledge Graph-based RAG (Graph RAG). Experiments showed that traditional RAG achieved 84.84% accuracy using BM25 + GPT-4 but occasionally produced hallucinations, which is risky to airport safety. In contrast, SQL RAG and Graph RAG achieved 80.85% and 91.49% accuracy respectively, with significantly fewer hallucinations. Moreover, Graph RAG was especially effective for questions that involved reasoning. Based on our observations, we thus recommend SQL RAG and Graph RAG are better for airport environments, due to fewer hallucinations and the ability to handle dynamic questions.

</details>


### [179] [To Bias or Not to Bias: Detecting bias in News with bias-detector](https://arxiv.org/abs/2505.13010)

*Himel Ghosh, Ahmed Mosharafa, Georg Groh*

**Main category:** cs.CL

**Keywords:** media bias detection, NLP, RoBERTa, context-aware modeling, explainability

**Relevance Score:** 6

**TL;DR:** This paper presents a fine-tuned RoBERTa model for sentence-level media bias detection, demonstrating statistically significant performance improvements over existing models, with a focus on maintaining interpretability and robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenges of media bias detection, which is essential for ensuring fair information dissemination amid subjective bias and limited annotated data.

**Method:** The paper fine-tunes a RoBERTa-based model on the expert-annotated BABE dataset for sentence-level bias classification, employing statistical tests to validate improvements over a baseline model.

**Key Contributions:**

	1. Fine-tuning of RoBERTa for media bias detection
	2. Statistically significant performance validation
	3. Proposal of a comprehensive pipeline for bias analysis

**Result:** The proposed model achieved statistically significant performance improvements compared to the DA-RoBERTa baseline, with attention analysis indicating better contextual relevance handling.

**Limitations:** Constrained by sentence-level analysis and limited dataset size due to a scarcity of larger, advanced bias corpora.

**Conclusion:** The findings contribute to the development of more explainable and socially responsible NLP systems for detecting media bias, while also outlining future directions for bias analysis.

**Abstract:** Media bias detection is a critical task in ensuring fair and balanced information dissemination, yet it remains challenging due to the subjectivity of bias and the scarcity of high-quality annotated data. In this work, we perform sentence-level bias classification by fine-tuning a RoBERTa-based model on the expert-annotated BABE dataset. Using McNemar's test and the 5x2 cross-validation paired t-test, we show statistically significant improvements in performance when comparing our model to a domain-adaptively pre-trained DA-RoBERTa baseline. Furthermore, attention-based analysis shows that our model avoids common pitfalls like oversensitivity to politically charged terms and instead attends more meaningfully to contextually relevant tokens. For a comprehensive examination of media bias, we present a pipeline that combines our model with an already-existing bias-type classifier. Our method exhibits good generalization and interpretability, despite being constrained by sentence-level analysis and dataset size because of a lack of larger and more advanced bias corpora. We talk about context-aware modeling, bias neutralization, and advanced bias type classification as potential future directions. Our findings contribute to building more robust, explainable, and socially responsible NLP systems for media bias detection.

</details>


### [180] [topicwizard -- a Modern, Model-agnostic Framework for Topic Model Visualization and Interpretation](https://arxiv.org/abs/2505.13034)

*Márton Kardos, Kenneth C. Enevoldsen, Kristoffer Laigaard Nielbo*

**Main category:** cs.CL

**Keywords:** topic models, interpretation, visualization, human-computer interaction, natural language processing

**Relevance Score:** 6

**TL;DR:** Introduction of topicwizard, a framework for model-agnostic topic model interpretation with interactive tools for exploring the relationships between documents, words, and topics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges users face in interpreting complex parameters of topic models, which are typically based on a limited and biased list-of-words approach.

**Method:** The paper presents the topicwizard framework that allows for intuitive and interactive examination of topic model outputs, enhancing user understanding through better visualizations.

**Key Contributions:**

	1. Introduction of model-agnostic interpretation framework for topic models
	2. Interactive tools for exploring semantic relations
	3. Enhanced visualization for better user understanding

**Result:** topicwizard improves the interpretability of topic models by providing a more accurate and comprehensive understanding of the semantic relations in textual corpora.

**Limitations:** Existing visualizations are often limited to specific types of topic models; future work could expand capabilities.

**Conclusion:** The framework facilitates better usability and interpretability of topic models, making the insights derived from them more accessible to users.

**Abstract:** Topic models are statistical tools that allow their users to gain qualitative and quantitative insights into the contents of textual corpora without the need for close reading. They can be applied in a wide range of settings from discourse analysis, through pretraining data curation, to text filtering. Topic models are typically parameter-rich, complex models, and interpreting these parameters can be challenging for their users. It is typical practice for users to interpret topics based on the top 10 highest ranking terms on a given topic. This list-of-words approach, however, gives users a limited and biased picture of the content of topics. Thoughtful user interface design and visualizations can help users gain a more complete and accurate understanding of topic models' output. While some visualization utilities do exist for topic models, these are typically limited to a certain type of topic model. We introduce topicwizard, a framework for model-agnostic topic model interpretation, that provides intuitive and interactive tools that help users examine the complex semantic relations between documents, words and topics learned by topic models.

</details>


### [181] [KIT's Offline Speech Translation and Instruction Following Submission for IWSLT 2025](https://arxiv.org/abs/2505.13036)

*Sai Koneru, Maike Züfle, Thai-Binh Nguyen, Seymanur Akti, Jan Niehues, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** Large Language Models, Speech Translation, Instruction Following

**Relevance Score:** 8

**TL;DR:** This paper discusses the Karlsruhe Institute of Technology's submissions to the IWSLT, focusing on enhancing speech translation and instruction following tasks using Large Language Models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the expanded scope of the IWSLT to include diverse tasks beyond traditional speech translation, motivated by advances in LLM capabilities.

**Method:** The approach includes a pipeline for Offline Speech Translation that utilizes multiple automatic speech recognition systems and an LLM for output fusion, along with a two-step translation process and an end-to-end model for instruction following that integrates a speech encoder with an LLM.

**Key Contributions:**

	1. Utilization of multiple ASR systems to improve speech translation
	2. Integration of LLM for document-level context in translations
	3. Development of an end-to-end model for instruction-following tasks combining speech encoding with LLMs

**Result:** The methods presented enhance performance across speech translation and instruction following tasks, with improved translation quality and contextual relevance in outputs.

**Limitations:** 

**Conclusion:** The integration of LLMs in the proposed pipeline and model significantly boosts performance in various speech-related tasks, demonstrating the versatility and capability of modern systems in processing spoken language.

**Abstract:** The scope of the International Workshop on Spoken Language Translation (IWSLT) has recently broadened beyond traditional Speech Translation (ST) to encompass a wider array of tasks, including Speech Question Answering and Summarization. This shift is partly driven by the growing capabilities of modern systems, particularly with the success of Large Language Models (LLMs). In this paper, we present the Karlsruhe Institute of Technology's submissions for the Offline ST and Instruction Following (IF) tracks, where we leverage LLMs to enhance performance across all tasks. For the Offline ST track, we propose a pipeline that employs multiple automatic speech recognition systems, whose outputs are fused using an LLM with document-level context. This is followed by a two-step translation process, incorporating additional refinement step to improve translation quality. For the IF track, we develop an end-to-end model that integrates a speech encoder with an LLM to perform a wide range of instruction-following tasks. We complement it with a final document-level refinement stage to further enhance output quality by using contextual information.

</details>


### [182] [SNAPE-PM: Building and Utilizing Dynamic Partner Models for Adaptive Explanation Generation](https://arxiv.org/abs/2505.13053)

*Amelie S. Robrecht, Christoph R. Kowalski, Stefan Kopp*

**Main category:** cs.CL

**Keywords:** dialog systems, explanations, Bayesian inference, non-stationary decision process, explainable AI

**Relevance Score:** 7

**TL;DR:** This paper presents a Bayesian inference-based approach for adapting explanation generation in dialog systems to the user's context and features, demonstrating its effectiveness through simulated interlocutors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for dialog systems to adapt explanations based on the user's characteristics and contextual information to improve communication effectiveness.

**Method:** It employs a Bayesian inference model to update user profiles continuously, combined with a non-stationary Markov Decision Process to adjust explanation strategies based on the updated user information.

**Key Contributions:**

	1. A formal computational partner model for tracking interaction context and listener features.
	2. A Bayesian inference approach that uses user feedback to update the partner model.
	3. A non-stationary decision-making process to determine optimal explanation strategies.

**Result:** The implementation tested with five simulated interlocutors showed high adaptivity, with distinct explanation strategies emerging for each partner, even with varying feedback behaviors.

**Limitations:** The study focuses on simulated interlocutors; real-world applicability may require further validation.

**Conclusion:** The findings suggest that this adaptive approach can significantly improve the performance of explainable AI systems and dialog systems.

**Abstract:** Adapting to the addressee is crucial for successful explanations, yet poses significant challenges for dialogsystems. We adopt the approach of treating explanation generation as a non-stationary decision process, where the optimal strategy varies according to changing beliefs about the explainee and the interaction context. In this paper we address the questions of (1) how to track the interaction context and the relevant listener features in a formally defined computational partner model, and (2) how to utilize this model in the dynamically adjusted, rational decision process that determines the currently best explanation strategy. We propose a Bayesian inference-based approach to continuously update the partner model based on user feedback, and a non-stationary Markov Decision Process to adjust decision-making based on the partner model values. We evaluate an implementation of this framework with five simulated interlocutors, demonstrating its effectiveness in adapting to different partners with constant and even changing feedback behavior. The results show high adaptivity with distinct explanation strategies emerging for different partners, highlighting the potential of our approach to improve explainable AI systems and dialogsystems in general.

</details>


### [183] [Suicide Risk Assessment Using Multimodal Speech Features: A Study on the SW1 Challenge Dataset](https://arxiv.org/abs/2505.13069)

*Ambre Marie, Ilias Maoudj, Guillaume Dardenne, Gwenolé Quellec*

**Main category:** cs.CL

**Keywords:** speech-based assessment, suicide risk, multimodal approach, fusion strategies, adolescents

**Relevance Score:** 8

**TL;DR:** This study presents a multimodal approach for speech-based suicide risk assessment in adolescents using various audio and linguistic embeddings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective speech-based suicide risk assessment for adolescents, addressing a gap in mental health support.

**Method:** Integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, audio embeddings from WavLM, and handcrafted acoustic features; exploring fusion strategies like early concatenation, modality-specific processing, and weighted attention with mixup regularization.

**Key Contributions:**

	1. Multimodal approach integrating various embeddings for speech analysis.
	2. Evaluation of multiple fusion strategies for better classification.
	3. Specific focus on adolescent suicide risk assessment through speech.

**Result:** Achieved 69% accuracy on the development set with weighted attention strategy, although a performance gap exists between development and test sets.

**Limitations:** Performance gap between development and test sets indicates generalization challenges that need addressing.

**Conclusion:** Emphasizes refining embedding representations and fusion mechanisms for improved classification reliability in suicide risk assessment.

**Abstract:** The 1st SpeechWellness Challenge conveys the need for speech-based suicide risk assessment in adolescents. This study investigates a multimodal approach for this challenge, integrating automatic transcription with WhisperX, linguistic embeddings from Chinese RoBERTa, and audio embeddings from WavLM. Additionally, handcrafted acoustic features -- including MFCCs, spectral contrast, and pitch-related statistics -- were incorporated. We explored three fusion strategies: early concatenation, modality-specific processing, and weighted attention with mixup regularization. Results show that weighted attention provided the best generalization, achieving 69% accuracy on the development set, though a performance gap between development and test sets highlights generalization challenges. Our findings, strictly tied to the MINI-KID framework, emphasize the importance of refining embedding representations and fusion mechanisms to enhance classification reliability.

</details>


### [184] [Advancing Sequential Numerical Prediction in Autoregressive Models](https://arxiv.org/abs/2505.13077)

*Xiang Fei, Jinghui Lu, Qi Sun, Hao Feng, Yanjie Wang, Wei Shi, An-Lan Wang, Jingqun Tang, Can Huang*

**Main category:** cs.CL

**Keywords:** Numerical Token Integrity Loss, sequence generation, Earth Mover's Distance

**Relevance Score:** 7

**TL;DR:** This paper presents Numerical Token Integrity Loss (NTIL) to improve sequence generation in autoregressive models by preserving the ordinal relationships of numerical sequences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standard methods for sequence generation treat digits as independent tokens, failing to capture the coherent structure present in numerical sequences, which can hinder performance.

**Method:** NTIL operates at two levels: token-level, which extends the Earth Mover's Distance to maintain ordinal relationships, and sequence-level, which penalizes discrepancies between predicted and actual sequences.

**Key Contributions:**

	1. Introduction of Numerical Token Integrity Loss (NTIL)
	2. Extension of Earth Mover's Distance for ordinal preservation
	3. Dual-level approach enhancing numerical sequence coherence

**Result:** Experiments indicate that using NTIL leads to significant improvements in numerical prediction accuracy in sequence generation tasks.

**Limitations:** 

**Conclusion:** By incorporating NTIL, autoregressive models can more effectively handle numerical data in sequence generation, showing promise for further integration with LLMs and MLLMs.

**Abstract:** Autoregressive models have become the de facto choice for sequence generation tasks, but standard approaches treat digits as independent tokens and apply cross-entropy loss, overlooking the coherent structure of numerical sequences. This paper introduces Numerical Token Integrity Loss (NTIL) to address this gap. NTIL operates at two levels: (1) token-level, where it extends the Earth Mover's Distance (EMD) to preserve ordinal relationships between numerical values, and (2) sequence-level, where it penalizes the overall discrepancy between the predicted and actual sequences. This dual approach improves numerical prediction and integrates effectively with LLMs/MLLMs. Extensive experiments show significant performance improvements with NTIL.

</details>


### [185] [Systematic Generalization in Language Models Scales with Information Entropy](https://arxiv.org/abs/2505.13089)

*Sondre Wold, Lucas Georges Gabriel Charpentier, Étienne Simon*

**Main category:** cs.CL

**Keywords:** systematic generalization, language models, entropy, information efficiency, sequence-to-sequence

**Relevance Score:** 7

**TL;DR:** This paper explores systematic generalization in language models and proposes a framework for measuring the entropy of component parts in training data, finding a link between entropy levels and model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of systematic generalization in language models, particularly their sensitivity to input permutations and performance in novel contexts.

**Method:** The authors develop a framework that quantifies the entropy of the distribution of component parts in sequence-to-sequence tasks, relating it to the performance of various model architectures.

**Key Contributions:**

	1. Proposed a method for measuring entropy in sequence-to-sequence tasks
	2. Established a connection between entropy levels and model performance
	3. Highlighted the implications of entropy in assessing systematic generalization

**Result:** The study discovers that model performance correlates with entropy levels, demonstrating that high entropy can lead to successful generalization without built-in priors, and low entropy can serve as a benchmark for assessing generalization robustness.

**Limitations:** 

**Conclusion:** The findings establish a connection between systematic generalization and information efficiency, which can inform the development of more robust language models.

**Abstract:** Systematic generalization remains challenging for current language models, which are known to be both sensitive to semantically similar permutations of the input and to struggle with known concepts presented in novel contexts. Although benchmarks exist for assessing compositional behavior, it is unclear how to measure the difficulty of a systematic generalization problem. In this work, we show how one aspect of systematic generalization can be described by the entropy of the distribution of component parts in the training data. We formalize a framework for measuring entropy in a sequence-to-sequence task and find that the performance of popular model architectures scales with the entropy. Our work connects systematic generalization to information efficiency, and our results indicate that success at high entropy can be achieved even without built-in priors, and that success at low entropy can serve as a target for assessing progress towards robust systematic generalization.

</details>


### [186] [The Effect of Language Diversity When Fine-Tuning Large Language Models for Translation](https://arxiv.org/abs/2505.13090)

*David Stap, Christof Monz*

**Main category:** cs.CL

**Keywords:** language diversity, LLM fine-tuning, translation quality, language-agnostic representations, machine learning

**Relevance Score:** 7

**TL;DR:** Expanding language diversity during LLM fine-tuning enhances translation quality, particularly for both unsupervised and supervised pairs, up to a certain threshold.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To resolve conflicting findings in prior research regarding language diversity's impact on LLM fine-tuning.

**Method:** Controlled fine-tuning experiments were conducted across 132 translation directions to assess the effects of language diversity on translation quality.

**Key Contributions:**

	1. Systematic evaluation of language diversity in LLM fine-tuning across multiple translation pairs.
	2. Demonstration of the threshold effect in language diversity beyond which performance gains plateau or decrease.
	3. Insights into the creation of language-agnostic representations that improve model performance.

**Result:** Increased language diversity improves translation quality, yielding more language-agnostic representations that enhance model performance, although benefits plateau or decrease after a certain diversity threshold.

**Limitations:** Benefits of increased diversity plateau or decrease after certain thresholds, limiting its utility in all contexts.

**Conclusion:** Greater language diversity during fine-tuning contributes to improved translation quality, but there is a limit beyond which this diversity may hinder performance.

**Abstract:** Prior research diverges on language diversity in LLM fine-tuning: Some studies report benefits while others find no advantages. Through controlled fine-tuning experiments across 132 translation directions, we systematically resolve these disparities. We find that expanding language diversity during fine-tuning improves translation quality for both unsupervised and -- surprisingly -- supervised pairs, despite less diverse models being fine-tuned exclusively on these supervised pairs. However, benefits plateau or decrease beyond a certain diversity threshold. We show that increased language diversity creates more language-agnostic representations. These representational adaptations help explain the improved performance in models fine-tuned with greater diversity.

</details>


### [187] [Benchmarking and Confidence Evaluation of LALMs For Temporal Reasoning](https://arxiv.org/abs/2505.13115)

*Debarpan Bhattacharya, Apoorva Kulkarni, Sriram Ganapathy*

**Main category:** cs.CL

**Keywords:** large audio language models, temporal reasoning, uncertainty metrics

**Relevance Score:** 7

**TL;DR:** The paper introduces a novel dataset, TREA, for evaluating large audio language models (LALMs) on reasoning tasks, revealing limitations in LALMs compared to human capabilities and proposing a new uncertainty metric.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess large audio language models on reasoning tasks distinct from traditional classification and generation tasks.

**Method:** The authors propose the temporal reasoning evaluation of audio (TREA) dataset and benchmark open-source LALMs against human performance, while also introducing an uncertainty metric for evaluation.

**Key Contributions:**

	1. Introduction of the TREA dataset for evaluating reasoning in LALMs
	2. Benchmarking LALMs against human capabilities
	3. Proposing a new uncertainty metric for audio language models

**Result:** LALMs consistently underperform compared to human capabilities on tasks within the TREA dataset. The proposed uncertainty metric reveals that accuracy and uncertainty do not correlate.

**Limitations:** Limited to benchmarking existing LALMs; further exploration of uncertainty and its implications is needed.

**Conclusion:** A comprehensive evaluation framework for LALMs is necessary, especially for high-stakes applications, given the identified performance gaps and lack of correlation between accuracy and uncertainty metrics.

**Abstract:** The popular success of text-based large language models (LLM) has streamlined the attention of the multimodal community to combine other modalities like vision and audio along with text to achieve similar multimodal capabilities. In this quest, large audio language models (LALMs) have to be evaluated on reasoning related tasks which are different from traditional classification or generation tasks. Towards this goal, we propose a novel dataset called temporal reasoning evaluation of audio (TREA).   We benchmark open-source LALMs and observe that they are consistently behind human capabilities on the tasks in the TREA dataset. While evaluating LALMs, we also propose an uncertainty metric, which computes the invariance of the model to semantically identical perturbations of the input. Our analysis shows that the accuracy and uncertainty metrics are not necessarily correlated and thus, points to a need for wholesome evaluation of LALMs for high-stakes applications.

</details>


### [188] [ModernGBERT: German-only 1B Encoder Model Trained from Scratch](https://arxiv.org/abs/2505.13136)

*Anton Ehrmanntraut, Julia Wunderle, Jan Pfister, Fotis Jannidis, Andreas Hotho*

**Main category:** cs.CL

**Keywords:** ModernGBERT, encoder models, German NLP, LLM2Vec, text embedding

**Relevance Score:** 6

**TL;DR:** Introduction of ModernGBERT family of German encoder models and comparison with LLM2Vec derived encoders.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the trade-offs of training encoders from scratch for resource-constrained applications and advance the German NLP ecosystem.

**Method:** Development of ModernGBERT encoder models and benchmarking against LLM2Vec derived encoders on various natural language processing tasks.

**Key Contributions:**

	1. Introduction of ModernGBERT family of encoder models trained from scratch for German.
	2. Benchmarking new encoders against state-of-the-art and LLM2Vec derived models.
	3. Publicly available models and resources to foster German NLP research.

**Result:** ModernGBERT 1B outperforms prior state-of-the-art German encoders in performance and parameter-efficiency.

**Limitations:** 

**Conclusion:** The availability of all models, data, and code will enhance the German NLP landscape with high-performance encoders.

**Abstract:** Despite the prominence of decoder-only language models, encoders remain crucial for resource-constrained applications. We introduce ModernGBERT (134M, 1B), a fully transparent family of German encoder models trained from scratch, incorporating architectural innovations from ModernBERT. To evaluate the practical trade-offs of training encoders from scratch, we also present LL\"aMmlein2Vec (120M, 1B, 7B), a family of encoders derived from German decoder-only models via LLM2Vec. We benchmark all models on natural language understanding, text embedding, and long-context reasoning tasks, enabling a controlled comparison between dedicated encoders and converted decoders. Our results show that ModernGBERT 1B outperforms prior state-of-the-art German encoders as well as encoders adapted via LLM2Vec, with regard to performance and parameter-efficiency. All models, training data, checkpoints and code are publicly available, advancing the German NLP ecosystem with transparent, high-performance encoder models.

</details>


### [189] [Understanding Cross-Lingual Inconsistency in Large Language Models](https://arxiv.org/abs/2505.13141)

*Zheng Wei Lim, Alham Fikri Aji, Trevor Cohn*

**Main category:** cs.CL

**Keywords:** multilingual reasoning, large language models, natural language processing, knowledge transfer, semantic space

**Relevance Score:** 8

**TL;DR:** The study investigates how large language models (LLMs) manage multilingual reasoning, revealing inconsistencies due to reliance on individual language subspaces rather than a shared semantic space. It also discusses improving multilingual performance through enhanced knowledge sharing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs generalize knowledge across languages and to address the issue of inconsistent outputs when prompted in different languages.

**Method:** The logit lens is applied to interpret the steps taken by LLMs in solving multilingual multi-choice reasoning questions, analyzing hidden states and their relation to shared representations.

**Key Contributions:**

	1. Identified limitations in LLMs' handling of multilingual tasks by analyzing their reliance on individual language subspaces.
	2. Demonstrated the link between model size and the dissociation from shared semantic representations.
	3. Provided evidence that reinforcement of a shared semantic space leads to improved multilingual reasoning outcomes.

**Result:** LLMs exhibit inconsistent predictions and lower accuracy due to reliance on language-specific subspaces. Larger models dissociate from shared representations but better retrieve cross-lingual knowledge.

**Limitations:** The study focuses primarily on the performance of LLMs in multilingual contexts and may not address all potential influences of model architectures across other tasks.

**Conclusion:** Modulating models' latent processing towards a shared semantic space enhances multilingual reasoning performance by improving knowledge transfer from English.

**Abstract:** Large language models (LLMs) are demonstrably capable of cross-lingual transfer, but can produce inconsistent output when prompted with the same queries written in different languages. To understand how language models are able to generalize knowledge from one language to the others, we apply the logit lens to interpret the implicit steps taken by LLMs to solve multilingual multi-choice reasoning questions. We find LLMs predict inconsistently and are less accurate because they rely on subspaces of individual languages, rather than working in a shared semantic space. While larger models are more multilingual, we show their hidden states are more likely to dissociate from the shared representation compared to smaller models, but are nevertheless more capable of retrieving knowledge embedded across different languages. Finally, we demonstrate that knowledge sharing can be modulated by steering the models' latent processing towards the shared semantic space. We find reinforcing utilization of the shared space improves the models' multilingual reasoning performance, as a result of more knowledge transfer from, and better output consistency with English.

</details>


### [190] [What if Deception Cannot be Detected? A Cross-Linguistic Study on the Limits of Deception Detection from Text](https://arxiv.org/abs/2505.13147)

*Aswathy Velutharambath, Roman Klinger, Kai Sassenberg*

**Main category:** cs.CL

**Keywords:** Deception detection, NLP, Linguistic cues, Machine Learning, Corpora

**Relevance Score:** 8

**TL;DR:** This paper challenges existing assumptions about detecting deception from written text, presenting a new framework and datasets that reveal limited correlation between linguistic cues and deception labels.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to assess the validity of automatic deception detection in text, given that previous studies may have overestimated the reliability of linguistic cues due to dataset artifacts.

**Method:** The authors introduce a belief-based deception framework and construct three corpora (DeFaBel) for analyzing deceptive communication in German and English, evaluating common linguistic cues.

**Key Contributions:**

	1. Introduction of the belief-based deception framework
	2. Creation of the DeFaBel corpora for multilingual analysis
	3. Demonstration of negligible correlations between linguistic cues and deception in text

**Result:** The study finds negligible correlations between linguistic cues and deception labels across the DeFaBel corpora, and while some models performed well on traditional datasets, they failed to generalize to the new dataset.

**Limitations:** The findings are specific to the constructed datasets and may not represent all contexts of deception detection.

**Conclusion:** Findings suggest re-evaluating the methods for studying and modeling deception in NLP, as reliance on linguistic cues is called into question.

**Abstract:** Can deception be detected solely from written text? Cues of deceptive communication are inherently subtle, even more so in text-only communication. Yet, prior studies have reported considerable success in automatic deception detection. We hypothesize that such findings are largely driven by artifacts introduced during data collection and do not generalize beyond specific datasets. We revisit this assumption by introducing a belief-based deception framework, which defines deception as a misalignment between an author's claims and true beliefs, irrespective of factual accuracy, allowing deception cues to be studied in isolation. Based on this framework, we construct three corpora, collectively referred to as DeFaBel, including a German-language corpus of deceptive and non-deceptive arguments and a multilingual version in German and English, each collected under varying conditions to account for belief change and enable cross-linguistic analysis. Using these corpora, we evaluate commonly reported linguistic cues of deception. Across all three DeFaBel variants, these cues show negligible, statistically insignificant correlations with deception labels, contrary to prior work that treats such cues as reliable indicators. We further benchmark against other English deception datasets following similar data collection protocols. While some show statistically significant correlations, effect sizes remain low and, critically, the set of predictive cues is inconsistent across datasets. We also evaluate deception detection using feature-based models, pretrained language models, and instruction-tuned large language models. While some models perform well on established deception datasets, they consistently perform near chance on DeFaBel. Our findings challenge the assumption that deception can be reliably inferred from linguistic cues and call for rethinking how deception is studied and modeled in NLP.

</details>


### [191] [Tianyi: A Traditional Chinese Medicine all-rounder language model and its Real-World Clinical Practice](https://arxiv.org/abs/2505.13156)

*Zhi Liu, Tao Yang, Jing Wang, Yexin Chen, Zhan Gao, Jiaxi Yang, Kui Chen, Bingji Lu, Xiaochen Li, Changyong Luo, Yan Li, Xiaohong Gu, Peng Cao*

**Main category:** cs.CL

**Keywords:** Traditional Chinese Medicine, Machine Learning, Large Language Models, Healthcare, Tianyi

**Relevance Score:** 6

**TL;DR:** This paper introduces Tianyi, a 7.6-billion-parameter LLM specifically designed for Traditional Chinese Medicine (TCM), aimed at improving its practical application in healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing recognition of TCM for its therapeutic potential and the need for precise application in healthcare due to the complexity of TCM diagnostics and treatment principles.

**Method:** Tianyi is pre-trained and fine-tuned on diverse TCM corpora including classical texts and clinical records, and utilizes a comprehensive evaluation benchmark, TCMEval, for assessing LLM performance in TCM.

**Key Contributions:**

	1. Introduction of Tianyi, a specialized LLM for TCM
	2. Development of TCMEval, a benchmark for evaluating LLMs in TCM
	3. Demonstration of Tianyi's potential in clinical practice and research

**Result:** Extensive evaluations reveal Tianyi's significant potential as an AI assistant in TCM clinical practices and research, effectively bridging the gap between TCM knowledge and its practical application.

**Limitations:** Challenges include model scale deployment and hallucination issues in LLMs.

**Conclusion:** Tianyi presents a specialized LLM contributing to the efficacy of TCM, overcoming limitations faced by previous systems.

**Abstract:** Natural medicines, particularly Traditional Chinese Medicine (TCM), are gaining global recognition for their therapeutic potential in addressing human symptoms and diseases. TCM, with its systematic theories and extensive practical experience, provides abundant resources for healthcare. However, the effective application of TCM requires precise syndrome diagnosis, determination of treatment principles, and prescription formulation, which demand decades of clinical expertise. Despite advancements in TCM-based decision systems, machine learning, and deep learning research, limitations in data and single-objective constraints hinder their practical application. In recent years, large language models (LLMs) have demonstrated potential in complex tasks, but lack specialization in TCM and face significant challenges, such as too big model scale to deploy and issues with hallucination. To address these challenges, we introduce Tianyi with 7.6-billion-parameter LLM, a model scale proper and specifically designed for TCM, pre-trained and fine-tuned on diverse TCM corpora, including classical texts, expert treatises, clinical records, and knowledge graphs. Tianyi is designed to assimilate interconnected and systematic TCM knowledge through a progressive learning manner. Additionally, we establish TCMEval, a comprehensive evaluation benchmark, to assess LLMs in TCM examinations, clinical tasks, domain-specific question-answering, and real-world trials. The extensive evaluations demonstrate the significant potential of Tianyi as an AI assistant in TCM clinical practice and research, bridging the gap between TCM knowledge and practical application.

</details>


### [192] [Role-Playing Evaluation for Large Language Models](https://arxiv.org/abs/2505.13157)

*Yassine El Boudouri, Walter Nuninger, Julian Alvarez, Yvan Peter*

**Main category:** cs.CL

**Keywords:** Large Language Models, Role-playing, Evaluation Benchmark

**Relevance Score:** 8

**TL;DR:** Introducing RPEval, a benchmark for assessing role-playing abilities in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the role-playing capabilities of LLMs with a reliable benchmark due to challenges in human and automated assessments.

**Method:** The paper presents the construction of RPEval, which evaluates LLMs across emotional understanding, decision-making, moral alignment, and in-character consistency.

**Key Contributions:**

	1. Introduction of Role-Playing Eval (RPEval) benchmark
	2. Evaluation across four key dimensions of role-playing
	3. Public availability of code and dataset

**Result:** Baseline evaluations of LLMs using the RPEval benchmark are provided, highlighting their role-playing effectiveness.

**Limitations:** 

**Conclusion:** RPEval aims to standardize the evaluation of LLM role-playing and provide resources to the community.

**Abstract:** Large Language Models (LLMs) demonstrate a notable capacity for adopting personas and engaging in role-playing. However, evaluating this ability presents significant challenges, as human assessments are resource-intensive and automated evaluations can be biased. To address this, we introduce Role-Playing Eval (RPEval), a novel benchmark designed to assess LLM role-playing capabilities across four key dimensions: emotional understanding, decision-making, moral alignment, and in-character consistency. This article details the construction of RPEval and presents baseline evaluations. Our code and dataset are available at https://github.com/yelboudouri/RPEval

</details>


### [193] [Positional Fragility in LLMs: How Offset Effects Reshape Our Understanding of Memorization Risks](https://arxiv.org/abs/2505.13171)

*Yixuan Xu, Antoine Bosselut, Imanol Schlag*

**Main category:** cs.CL

**Keywords:** language models, memorization risk, positional offset, copyright, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper investigates the memorization behavior of language models, revealing the 'offset effect'—how the position of tokens in the context window influences verbatim memorization and text degeneration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically investigate the memorization risks posed by large language models, especially concerning copyright risks from training data.

**Method:** Pretraining language models (1B/3B/8B) on 83B tokens, mixing web-scale data with public domain content, and analyzing the effects of token position in the context window on memorization behavior.

**Key Contributions:**

	1. Introduced the 'offset effect' in language model memorization.
	2. Demonstrated the impact of token positioning on memorization and text quality.
	3. Highlighted the need for reevaluating memorization risk assessment in language models.

**Result:** Identified the 'offset effect,' showing that verbatim memorization is triggered by short prefixes from the beginning of the context, and that memorization declines sharply as the prefix is offset from the initial tokens.

**Limitations:** Focuses primarily on memorization without deeply exploring other aspects of language model behavior.

**Conclusion:** Positional offset is a crucial factor in understanding language model memorization risks, suggesting that shifting sensitive data in the context can reduce issues of extractable memorization and text degeneration.

**Abstract:** Large language models are known to memorize parts of their training data, posing risk of copyright violations. To systematically examine this risk, we pretrain language models (1B/3B/8B) from scratch on 83B tokens, mixing web-scale data with public domain books used to simulate copyrighted content at controlled frequencies at lengths at least ten times longer than prior work. We thereby identified the offset effect, a phenomenon characterized by two key findings: (1) verbatim memorization is most strongly triggered by short prefixes drawn from the beginning of the context window, with memorization decreasing counterintuitively as prefix length increases; and (2) a sharp decline in verbatim recall when prefix begins offset from the initial tokens of the context window. We attribute this to positional fragility: models rely disproportionately on the earliest tokens in their context window as retrieval anchors, making them sensitive to even slight shifts. We further observe that when the model fails to retrieve memorized content, it often produces degenerated text. Leveraging these findings, we show that shifting sensitive data deeper into the context window suppresses both extractable memorization and degeneration. Our results suggest that positional offset is a critical and previously overlooked axis for evaluating memorization risks, since prior work implicitly assumed uniformity by probing only from the beginning of training sequences.

</details>


### [194] [A Case Study of Cross-Lingual Zero-Shot Generalization for Classical Languages in LLMs](https://arxiv.org/abs/2505.13173)

*V. S. D. S. Mahesh Akavarapu, Hrishikesh Terdalkar, Pramit Bhattacharyya, Shubhangi Agarwal, Vishakha Deulgaonkar, Pralay Manna, Chaitali Dangarikar, Arnab Bhattacharya*

**Main category:** cs.CL

**Keywords:** Large Language Models, cross-lingual generalization, classical languages, question answering, Sanskrit

**Relevance Score:** 8

**TL;DR:** This study investigates the cross-lingual zero-shot generalization performance of LLMs in classical languages, demonstrating that larger models outperform smaller ones in tasks like named entity recognition and question answering, notably with a focus on Sanskrit.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs generalize across classical languages and the impact of model size on performance in tasks such as named entity recognition and machine translation.

**Method:** The study analyzed LLM performance on named entity recognition and machine translation tasks in Sanskrit, Ancient Greek, and Latin, and presented a factoid QA dataset for Sanskrit, utilizing a retrieval-augmented generation approach.

**Key Contributions:**

	1. Investigates cross-lingual zero-shot generalization of LLMs in classical languages.
	2. Demonstrates the superior performance of larger models compared to smaller ones in specific tasks.
	3. Introduces a factoid question-answering dataset for Sanskrit and shows effectiveness of retrieval-augmented generation.

**Result:** Large models like GPT-4o and Llama-3.1 showed better performance than smaller models on out-of-domain data, and incorporating context via retrieval-augmented generation significantly improved results in Sanskrit QA tasks.

**Limitations:** Results may not generalize to all smaller LLMs or other classical languages beyond those studied.

**Conclusion:** The findings highlight that model scale is vital for effective cross-lingual generalization in classical languages, providing insights into LLM utility in classical studies.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable generalization capabilities across diverse tasks and languages. In this study, we focus on natural language understanding in three classical languages -- Sanskrit, Ancient Greek and Latin -- to investigate the factors affecting cross-lingual zero-shot generalization. First, we explore named entity recognition and machine translation into English. While LLMs perform equal to or better than fine-tuned baselines on out-of-domain data, smaller models often struggle, especially with niche or abstract entity types. In addition, we concentrate on Sanskrit by presenting a factoid question-answering (QA) dataset and show that incorporating context via retrieval-augmented generation approach significantly boosts performance. In contrast, we observe pronounced performance drops for smaller LLMs across these QA tasks. These results suggest model scale as an important factor influencing cross-lingual generalization. Assuming that models used such as GPT-4o and Llama-3.1 are not instruction fine-tuned on classical languages, our findings provide insights into how LLMs may generalize on these languages and their consequent utility in classical studies.

</details>


### [195] [ToolSpectrum : Towards Personalized Tool Utilization for Large Language Models](https://arxiv.org/abs/2505.13176)

*Zihao Cheng, Hongru Wang, Zeming Liu, Yuhang Guo, Yuanfang Guo, Yunhong Wang, Haifeng Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personalization, Tool Utilization, Human-Computer Interaction, Context-Aware Systems

**Relevance Score:** 9

**TL;DR:** This paper introduces ToolSpectrum, a benchmark for evaluating personalized tool utilization in large language models (LLMs), addressing the gap in context-aware tool selection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of external tools into LLMs enhances their functionality, but current approaches neglect the importance of personalized and context-aware tool selection, leading to user dissatisfaction and inefficient tool use.

**Method:** We formalize dimensions of personalization (user profile and environmental factors) and analyze their impacts on tool utilization through experiments on ToolSpectrum.

**Key Contributions:**

	1. Introduction of ToolSpectrum benchmark for evaluating LLMs' personalized tool utilization.
	2. Formalization of user profile and environmental factors as dimensions of personalization.
	3. Empirical evidence demonstrating the impact of personalized tool utilization on user experience.

**Result:** Personalized tool utilization significantly enhances user experience, but current LLMs struggle to effectively reason about user profiles and environmental factors simultaneously.

**Limitations:** State-of-the-art LLMs have limited capacity to jointly reason about user profiles and environmental factors.

**Conclusion:** The necessity for context-aware personalization in tool-augmented LLMs is highlighted, alongside critical limitations of existing models in handling personalization dimensions.

**Abstract:** While integrating external tools into large language models (LLMs) enhances their ability to access real-time information and domain-specific services, existing approaches focus narrowly on functional tool selection following user instructions, overlooking the context-aware personalization in tool selection. This oversight leads to suboptimal user satisfaction and inefficient tool utilization, particularly when overlapping toolsets require nuanced selection based on contextual factors. To bridge this gap, we introduce ToolSpectrum, a benchmark designed to evaluate LLMs' capabilities in personalized tool utilization. Specifically, we formalize two key dimensions of personalization, user profile and environmental factors, and analyze their individual and synergistic impacts on tool utilization. Through extensive experiments on ToolSpectrum, we demonstrate that personalized tool utilization significantly improves user experience across diverse scenarios. However, even state-of-the-art LLMs exhibit the limited ability to reason jointly about user profiles and environmental factors, often prioritizing one dimension at the expense of the other. Our findings underscore the necessity of context-aware personalization in tool-augmented LLMs and reveal critical limitations for current models. Our data and code are available at https://github.com/Chengziha0/ToolSpectrum.

</details>


### [196] [Efficient Speech Language Modeling via Energy Distance in Continuous Latent Space](https://arxiv.org/abs/2505.13181)

*Zhengrui Ma, Yang Feng, Chenze Shao, Fandong Meng, Jie Zhou, Min Zhang*

**Main category:** cs.CL

**Keywords:** speech language modeling, continuous latent representations, energy distance

**Relevance Score:** 5

**TL;DR:** SLED introduces a novel approach to speech language modeling by utilizing continuous latent representations of speech waveforms, enhancing efficiency and performance in speech synthesis tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve speech language modeling by eliminating discretization errors and simplifying the modeling pipeline.

**Method:** SLED encodes speech waveforms into continuous latent representations and models them autoregressively using an energy distance objective.

**Key Contributions:**

	1. Introduces continuous latent representations for speech modeling
	2. Utilizes energy distance for autoregressive modeling
	3. Simplifies the modeling pipeline without losing information

**Result:** SLED demonstrates strong performance in zero-shot and streaming speech synthesis, outperforming existing speech language models.

**Limitations:** 

**Conclusion:** SLED offers a more efficient alternative for speech language modeling with potential broader applications.

**Abstract:** We introduce SLED, an alternative approach to speech language modeling by encoding speech waveforms into sequences of continuous latent representations and modeling them autoregressively using an energy distance objective. The energy distance offers an analytical measure of the distributional gap by contrasting simulated and target samples, enabling efficient training to capture the underlying continuous autoregressive distribution. By bypassing reliance on residual vector quantization, SLED avoids discretization errors and eliminates the need for the complicated hierarchical architectures common in existing speech language models. It simplifies the overall modeling pipeline while preserving the richness of speech information and maintaining inference efficiency. Empirical results demonstrate that SLED achieves strong performance in both zero-shot and streaming speech synthesis, showing its potential for broader applications in general-purpose speech language models.

</details>


### [197] [Alignment-Augmented Speculative Decoding with Alignment Sampling and Conditional Verification](https://arxiv.org/abs/2505.13204)

*Jikai Wang, Zhenxu Tian, Juntao Li, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang*

**Main category:** cs.CL

**Keywords:** speculative decoding, alignment sampling, large language models, inference efficiency, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a training-free alignment-augmented speculative decoding algorithm that improves generation accuracy and efficiency for large language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the training costs associated with existing draft-target alignment methods in speculative decoding.

**Method:** The authors propose alignment sampling and a flexible verification strategy that utilizes output distribution from the prefilling phase to enhance draft candidates and improves inference efficiency through an adaptive probability threshold.

**Key Contributions:**

	1. Introduction of a training-free speculative decoding algorithm.
	2. Alignment sampling technique that improves draft candidate quality without retraining.
	3. A flexible verification strategy that enhances generation accuracy and efficiency.

**Result:** The proposed method shows a 3.3 point increase in average generation score for the LLaMA3 model across 8 datasets, achieving a mean acceptance length of 2.39 and speeding up generation by 2.23 times.

**Limitations:** 

**Conclusion:** The proposed training-free method balances accuracy and efficiency in language model generation without significant training costs.

**Abstract:** Recent works have revealed the great potential of speculative decoding in accelerating the autoregressive generation process of large language models. The success of these methods relies on the alignment between draft candidates and the sampled outputs of the target model. Existing methods mainly achieve draft-target alignment with training-based methods, e.g., EAGLE, Medusa, involving considerable training costs. In this paper, we present a training-free alignment-augmented speculative decoding algorithm. We propose alignment sampling, which leverages output distribution obtained in the prefilling phase to provide more aligned draft candidates. To further benefit from high-quality but non-aligned draft candidates, we also introduce a simple yet effective flexible verification strategy. Through an adaptive probability threshold, our approach can improve generation accuracy while further improving inference efficiency. Experiments on 8 datasets (including question answering, summarization and code completion tasks) show that our approach increases the average generation score by 3.3 points for the LLaMA3 model. Our method achieves a mean acceptance length up to 2.39 and speed up generation by 2.23.

</details>


### [198] [Picturized and Recited with Dialects: A Multimodal Chinese Representation Framework for Sentiment Analysis of Classical Chinese Poetry](https://arxiv.org/abs/2505.13210)

*Xiaocong Du, Haoyu Pei, Haipeng Zhang*

**Main category:** cs.CL

**Keywords:** sentiment analysis, multimodal learning, classical Chinese poetry, dialect enhancement, LLM translation

**Relevance Score:** 2

**TL;DR:** A multimodal framework for sentiment analysis of classical Chinese poetry that incorporates audio, visual, and textual features using dialect enhancements and LLM translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in existing sentiment analysis of classical Chinese poetry that neglects rhythmic and visual features, which are crucial for understanding its emotional depth.

**Method:** A dialect-enhanced multimodal framework that fuses audio features from multiple dialects, visual features, and textual features enhanced by LLM translation through multimodal contrastive representation learning.

**Key Contributions:**

	1. Development of a multimodal framework for poetry sentiment analysis.
	2. Incorporation of dialect-enhanced audio features.
	3. Open-sourcing of code for future research and applications.

**Result:** The proposed framework outperforms state-of-the-art methods, achieving at least a 2.51% improvement in accuracy and 1.63% in macro F1 score on two public datasets.

**Limitations:** 

**Conclusion:** The introduced multimodal framework presents significant advancements in sentiment analysis of classical Chinese poetry, while also facilitating further research by providing open-source code.

**Abstract:** Classical Chinese poetry is a vital and enduring part of Chinese literature, conveying profound emotional resonance. Existing studies analyze sentiment based on textual meanings, overlooking the unique rhythmic and visual features inherent in poetry,especially since it is often recited and accompanied by Chinese paintings. In this work, we propose a dialect-enhanced multimodal framework for classical Chinese poetry sentiment analysis. We extract sentence-level audio features from the poetry and incorporate audio from multiple dialects,which may retain regional ancient Chinese phonetic features, enriching the phonetic representation. Additionally, we generate sentence-level visual features, and the multimodal features are fused with textual features enhanced by LLM translation through multimodal contrastive representation learning. Our framework outperforms state-of-the-art methods on two public datasets, achieving at least 2.51% improvement in accuracy and 1.63% in macro F1. We open-source the code to facilitate research in this area and provide insights for general multimodal Chinese representation.

</details>


### [199] [SeedBench: A Multi-task Benchmark for Evaluating Large Language Models in Seed Science](https://arxiv.org/abs/2505.13220)

*Jie Ying, Zihong Chen, Zhefan Wang, Wanli Jiang, Chenyang Wang, Zhonghang Yuan, Haoyang Su, Huanjun Kong, Fan Yang, Nanqing Dong*

**Main category:** cs.CL

**Keywords:** seed science, large language models, benchmark, agriculture, machine learning

**Relevance Score:** 4

**TL;DR:** Introduction of SeedBench, a multi-task benchmark for seed science using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in seed science that hinder progress in agriculture.

**Method:** Developed SeedBench with domain experts, evaluating 26 LLMs including proprietary and open-source models.

**Key Contributions:**

	1. First multi-task benchmark for seed science
	2. Evaluation of 26 leading LLMs
	3. Foundation for future LLM research in seed design.

**Result:** Identified substantial performance gaps between LLMs and real-world seed science problems.

**Limitations:** 

**Conclusion:** SeedBench serves as a foundational tool for advancing research in LLM applications for seed design.

**Abstract:** Seed science is essential for modern agriculture, directly influencing crop yields and global food security. However, challenges such as interdisciplinary complexity and high costs with limited returns hinder progress, leading to a shortage of experts and insufficient technological support. While large language models (LLMs) have shown promise across various fields, their application in seed science remains limited due to the scarcity of digital resources, complex gene-trait relationships, and the lack of standardized benchmarks. To address this gap, we introduce SeedBench -- the first multi-task benchmark specifically designed for seed science. Developed in collaboration with domain experts, SeedBench focuses on seed breeding and simulates key aspects of modern breeding processes. We conduct a comprehensive evaluation of 26 leading LLMs, encompassing proprietary, open-source, and domain-specific fine-tuned models. Our findings not only highlight the substantial gaps between the power of LLMs and the real-world seed science problems, but also make a foundational step for research on LLMs for seed design.

</details>


### [200] [JNLP at SemEval-2025 Task 11: Cross-Lingual Multi-Label Emotion Detection Using Generative Models](https://arxiv.org/abs/2505.13244)

*Jieying Xue, Phuong Minh Nguyen, Minh Le Nguyen, Xin Liu*

**Main category:** cs.CL

**Keywords:** multilingual, emotion detection, multi-label classification, BERT, LLM

**Relevance Score:** 6

**TL;DR:** This study focuses on multilingual multi-label emotion detection for social media text, applying pre-trained models and achieving strong performance across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of digital communication, understanding emotions in multilingual contexts is essential for better information exchange on social media.

**Method:** Utilizes fine-tuned BERT-based models and instruction-tuned generative LLM, applying two methods for multi-label classification: the base method and a pairwise method that examines relationships between inputs and emotion categories.

**Key Contributions:**

	1. Leveraged multilingual models for emotion detection
	2. Proposed two methods for multi-label classification
	3. Demonstrated strong performance across multiple languages

**Result:** Achieved Top 4 performance in Track A across 10 languages, ranking 1st in Hindi, and secured Top 5 performance in Track B in 7 languages.

**Limitations:** 

**Conclusion:** The approach shows effective and simple methodologies for addressing multilingual emotion detection tasks, demonstrating strong generalization capabilities.

**Abstract:** With the rapid advancement of global digitalization, users from different countries increasingly rely on social media for information exchange. In this context, multilingual multi-label emotion detection has emerged as a critical research area. This study addresses SemEval-2025 Task 11: Bridging the Gap in Text-Based Emotion Detection. Our paper focuses on two sub-tracks of this task: (1) Track A: Multi-label emotion detection, and (2) Track B: Emotion intensity. To tackle multilingual challenges, we leverage pre-trained multilingual models and focus on two architectures: (1) a fine-tuned BERT-based classification model and (2) an instruction-tuned generative LLM. Additionally, we propose two methods for handling multi-label classification: the base method, which maps an input directly to all its corresponding emotion labels, and the pairwise method, which models the relationship between the input text and each emotion category individually. Experimental results demonstrate the strong generalization ability of our approach in multilingual emotion recognition. In Track A, our method achieved Top 4 performance across 10 languages, ranking 1st in Hindi. In Track B, our approach also secured Top 5 performance in 7 languages, highlighting its simplicity and effectiveness\footnote{Our code is available at https://github.com/yingjie7/mlingual_multilabel_emo_detection.

</details>


### [201] [Stronger Together: Unleashing the Social Impact of Hate Speech Research](https://arxiv.org/abs/2505.13251)

*Sidney Wong*

**Main category:** cs.CL

**Keywords:** hate speech, linguistics, NLP, digital inclusion, social methods

**Relevance Score:** 4

**TL;DR:** The paper discusses the dual role of the internet in fostering and alienating marginalized communities and proposes using social methods informed by linguistics and NLP to combat hate speech and promote digital inclusion.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the internet can both connect and alienate marginalized communities and to propose new approaches for hate speech research.

**Method:** The authors advocate for social methods over computational solutions in addressing hate speech, drawing on linguistic research to inform community engagement and policy planning.

**Key Contributions:**

	1. Proposing a shift from computational to social methods in hate speech research.
	2. Highlighting the role of linguistics in informing language policy and community engagement.
	3. Emphasizing collaboration with marginalized communities to enhance digital inclusion.

**Result:** It was found that collaboration between linguists, NLP researchers, communities, and policymakers can enhance efforts to mitigate hate speech and promote equitable digital inclusion.

**Limitations:** The paper does not provide specific case studies or empirical evidence to support its claims.

**Conclusion:** Linguists and NLP researchers have a crucial role in promoting social impact through collaboration, thereby addressing the risks of anti-social behavior in digital spaces.

**Abstract:** The advent of the internet has been both a blessing and a curse for once marginalised communities. When used well, the internet can be used to connect and establish communities crossing different intersections; however, it can also be used as a tool to alienate people and communities as well as perpetuate hate, misinformation, and disinformation especially on social media platforms. We propose steering hate speech research and researchers away from pre-existing computational solutions and consider social methods to inform social solutions to address this social problem. In a similar way linguistics research can inform language planning policy, linguists should apply what we know about language and society to mitigate some of the emergent risks and dangers of anti-social behaviour in digital spaces. We argue linguists and NLP researchers can play a principle role in unleashing the social impact potential of linguistics research working alongside communities, advocates, activists, and policymakers to enable equitable digital inclusion and to close the digital divide.

</details>


### [202] [Natural Language Planning via Coding and Inference Scaling](https://arxiv.org/abs/2505.13252)

*Rikhil Amonkar, Ronan Le Bras, Li Zhang*

**Main category:** cs.CL

**Keywords:** LLMs, planning tasks, code generation, constraint satisfaction, error analysis

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of both closed- and open-source LLMs in generating programs for complex planning tasks like meeting scheduling, while highlighting their limitations in robustness and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges LLMs face in executing real-life planning tasks that require high complexity and to compare the performance of closed- and open-source models.

**Method:** Systematic evaluation of LLMs in generating Python code and code for constraint satisfaction problems, analyzing their performance in executing plans.

**Key Contributions:**

	1. Systematic evaluation of both closed- and open-source models for planning tasks
	2. Analysis of generated Python code and constraint solver code
	3. Insight into robustness and efficiency challenges in LLM-generated code

**Result:** Programming often outperforms planning in generating solutions, but there are inconsistencies and issues with the generated code's robustness and efficiency.

**Limitations:** The generated code lacks robustness and efficiency, which impacts its generalization capabilities.

**Conclusion:** Enhanced evaluation and generation techniques are necessary to improve LLMs' performance in complex planning tasks, as current models show limitations.

**Abstract:** Real-life textual planning tasks such as meeting scheduling have posed much challenge to LLMs especially when the complexity is high. While previous work primarily studied auto-regressive generation of plans with closed-source models, we systematically evaluate both closed- and open-source models, including those that scales output length with complexity during inference, in generating programs, which are executed to output the plan. We consider not only standard Python code, but also the code to a constraint satisfaction problem solver. Despite the algorithmic nature of the task, we show that programming often but not always outperforms planning. Our detailed error analysis also indicates a lack of robustness and efficiency in the generated code that hinders generalization.

</details>


### [203] [HeteroSpec: Leveraging Contextual Heterogeneity for Efficient Speculative Decoding](https://arxiv.org/abs/2505.13254)

*Siran Liu, Yang Ye, Qianchao Zhu, Zheng Cao, Yongchao He*

**Main category:** cs.CL

**Keywords:** HeteroSpec, autoregressive decoding, large language models, speculative decoding, resource allocation

**Relevance Score:** 9

**TL;DR:** This paper introduces HeteroSpec, a framework that improves autoregressive decoding for LLMs by optimizing resource allocation based on linguistic complexity, achieving a significant speedup without retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Autoregressive decoding in LLMs is inefficient due to its sequential nature, and existing speculative decoding methods do not fully utilize the varying complexities of linguistic contexts.

**Method:** HeteroSpec employs a cumulative meta-path Top-$K$ entropy metric to identify predictable contexts and a dynamic resource allocation strategy to adaptively manage computational resources during inference.

**Key Contributions:**

	1. Introduction of a cumulative meta-path Top-$K$ entropy metric
	2. Dynamic resource allocation based on entropy partitioning
	3. Achieving substantial speedup without draft model retraining

**Result:** HeteroSpec achieves an average speedup of 4.26× on five benchmarks and four models, outperforming the state-of-the-art EAGLE-3 in multiple metrics.

**Limitations:** 

**Conclusion:** The proposed framework establishes a new standard for accelerating context-aware LLM inference by maintaining high efficiency without the need for model retraining.

**Abstract:** Autoregressive decoding, the standard approach for Large Language Model (LLM) inference, remains a significant bottleneck due to its sequential nature. While speculative decoding algorithms mitigate this inefficiency through parallel verification, they fail to exploit the inherent heterogeneity in linguistic complexity, a key factor leading to suboptimal resource allocation. We address this by proposing HeteroSpec, a heterogeneity-adaptive speculative decoding framework that dynamically optimizes computational resource allocation based on linguistic context complexity. HeteroSpec introduces two key mechanisms: (1) A novel cumulative meta-path Top-$K$ entropy metric for efficiently identifying predictable contexts. (2) A dynamic resource allocation strategy based on data-driven entropy partitioning, enabling adaptive speculative expansion and pruning tailored to local context difficulty. Evaluated on five public benchmarks and four models, HeteroSpec achieves an average speedup of 4.26$\times$. It consistently outperforms state-of-the-art EAGLE-3 across speedup rates, average acceptance length, and verification cost. Notably, HeteroSpec requires no draft model retraining, incurs minimal overhead, and is orthogonal to other acceleration techniques. It demonstrates enhanced acceleration with stronger draft models, establishing a new paradigm for context-aware LLM inference acceleration.

</details>


### [204] [WikiPersonas: What Can We Learn From Personalized Alignment to Famous People?](https://arxiv.org/abs/2505.13257)

*Zilu Tang, Afra Feyza Akyürek, Ekin Akyürek, Derry Wijaya*

**Main category:** cs.CL

**Keywords:** personalization, human preferences, AI alignment, WikiPersona, machine learning

**Relevance Score:** 6

**TL;DR:** The paper introduces WikiPersona, a dataset focusing on fine-grained personalization of models to align with individual user preferences using famous personas.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of personalized preference datasets that capture nuanced individual-level preferences in AI model alignment.

**Method:** The authors create a dataset called WikiPersona, which uses well-documented famous individuals and evaluates different personalization approaches for model alignment.

**Key Contributions:**

	1. Introduction of the WikiPersona dataset for personalized AI model alignment
	2. Evaluation of personalization approaches highlighting the limitations of few-shot prompting and fine-tuning
	3. Demonstration of effective personalization using inferred personal preferences

**Result:** The study finds that using inferred personal preferences as prefixes enables effective personalization, particularly in areas with conflicting preferences, leading to better generalization across unseen personas.

**Limitations:** The study focuses on famous individuals, which may not fully represent the diversity of user preferences in real-world applications.

**Conclusion:** The findings suggest that traditional methods of few-shot prompting and fine-tuning are less effective and efficient compared to the proposed method leveraging inferred preferences.

**Abstract:** Preference alignment has become a standard pipeline in finetuning models to follow \emph{generic} human preferences. Majority of work seeks to optimize model to produce responses that would be preferable \emph{on average}, simplifying the diverse and often \emph{contradicting} space of human preferences. While research has increasingly focused on personalized alignment: adapting models to individual user preferences, there is a lack of personalized preference dataset which focus on nuanced individual-level preferences. To address this, we introduce WikiPersona: the first fine-grained personalization using well-documented, famous individuals. Our dataset challenges models to align with these personas through an interpretable process: generating verifiable textual descriptions of a persona's background and preferences in addition to alignment. We systematically evaluate different personalization approaches and find that as few-shot prompting with preferences and fine-tuning fail to simultaneously ensure effectiveness and efficiency, using \textit{inferred personal preferences} as prefixes enables effective personalization, especially in topics where preferences clash while leading to more equitable generalization across unseen personas.

</details>


### [205] [Effective and Transparent RAG: Adaptive-Reward Reinforcement Learning for Decision Traceability](https://arxiv.org/abs/2505.13258)

*Jingyi Ren, Yekun Xu, Xiaolong Wang, Weitao Li, Weizhi Ma, Yang Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Reinforcement Learning, Transparency, Multi-hop Question Answering, Large Language Models

**Relevance Score:** 9

**TL;DR:** ARENA is a transparent RAG generator framework that enhances the reasoning ability of LLMs through reinforcement learning, achieving significant improvements in multi-hop QA tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness and transparency of Retrieval-Augmented Generation (RAG) models in utilizing retrieved information for reasoning and generation.

**Method:** ARENA employs reinforcement learning to train a transparent RAG generator that identifies key evidence and structures reasoning, resulting in interpretable decision traces.

**Key Contributions:**

	1. Introduction of ARENA, a new RAG generator framework
	2. Utilization of reinforcement learning for enhanced reasoning
	3. Demonstrated significant performance improvements on multi-hop QA tasks

**Result:** The ARENA framework demonstrated a 10-30% improvement on multiple multi-hop QA datasets compared to existing RAG baselines, showing effectiveness comparable to state-of-the-art LLMs.

**Limitations:** 

**Conclusion:** ARENA shows potential for improved effectiveness and transparency in RAG applications, with strong flexibility for adaptation to new datasets.

**Abstract:** Retrieval-Augmented Generation (RAG) has significantly improved the performance of large language models (LLMs) on knowledge-intensive domains. However, although RAG achieved successes across distinct domains, there are still some unsolved challenges: 1) Effectiveness. Existing research mainly focuses on developing more powerful RAG retrievers, but how to enhance the generator's (LLM's) ability to utilize the retrieved information for reasoning and generation? 2) Transparency. Most RAG methods ignore which retrieved content actually contributes to the reasoning process, resulting in a lack of interpretability and visibility. To address this, we propose ARENA (Adaptive-Rewarded Evidence Navigation Agent), a transparent RAG generator framework trained via reinforcement learning (RL) with our proposed rewards. Based on the structured generation and adaptive reward calculation, our RL-based training enables the model to identify key evidence, perform structured reasoning, and generate answers with interpretable decision traces. Applied to Qwen2.5-7B-Instruct and Llama3.1-8B-Instruct, abundant experiments with various RAG baselines demonstrate that our model achieves 10-30% improvements on all multi-hop QA datasets, which is comparable with the SOTA Commercially-developed LLMs (e.g., OpenAI-o1, DeepSeek-R1). Further analyses show that ARENA has strong flexibility to be adopted on new datasets without extra training. Our models and codes are publicly released.

</details>


### [206] [From Automation to Autonomy: A Survey on Large Language Models in Scientific Discovery](https://arxiv.org/abs/2505.13259)

*Tianshi Zheng, Zheye Deng, Hong Ting Tsang, Weiqi Wang, Jiaxin Bai, Zihao Wang, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, scientific discovery, human-AI collaboration, taxonomy, ethical governance

**Relevance Score:** 9

**TL;DR:** This survey explores the evolving role of Large Language Models (LLMs) in scientific discovery and human-AI collaboration, introducing a three-level taxonomy of LLM autonomy and responsibilities.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and chart the transformative impact of LLMs in scientific research and their evolving roles as collaborative agents.

**Method:** The paper uses a survey approach to develop a taxonomy based on the scientific method, categorizing LLMs into three levels: Tool, Analyst, and Scientist, while analyzing their responsibilities and capabilities.

**Key Contributions:**

	1. Introduces a three-level taxonomy for LLMs in research: Tool, Analyst, Scientist.
	2. Identifies pivotal challenges and future trajectories for LLMs in scientific discovery.
	3. Suggests a strategic framework for navigating AI's role in research.

**Result:** The survey identifies key challenges and outlines future research directions for AI in science, including ethical considerations, robotic automation, and self-improvement of models.

**Limitations:** 

**Conclusion:** By providing strategic foresight and a conceptual architecture, the survey aims to guide responsible advancements in AI-driven scientific discovery.

**Abstract:** Large Language Models (LLMs) are catalyzing a paradigm shift in scientific discovery, evolving from task-specific automation tools into increasingly autonomous agents and fundamentally redefining research processes and human-AI collaboration. This survey systematically charts this burgeoning field, placing a central focus on the changing roles and escalating capabilities of LLMs in science. Through the lens of the scientific method, we introduce a foundational three-level taxonomy-Tool, Analyst, and Scientist-to delineate their escalating autonomy and evolving responsibilities within the research lifecycle. We further identify pivotal challenges and future research trajectories such as robotic automation, self-improvement, and ethical governance. Overall, this survey provides a conceptual architecture and strategic foresight to navigate and shape the future of AI-driven scientific discovery, fostering both rapid innovation and responsible advancement. Github Repository: https://github.com/HKUST-KnowComp/Awesome-LLM-Scientific-Discovery.

</details>


### [207] [Representation of perceived prosodic similarity of conversational feedback](https://arxiv.org/abs/2505.13268)

*Livia Qian, Carol Figueroa, Gabriel Skantze*

**Main category:** cs.CL

**Keywords:** vocal feedback, prosody, speech representation, contrastive learning, conversational systems

**Relevance Score:** 8

**TL;DR:** This study explores the prosodic similarity of vocal feedback in conversational systems and how well speech representations capture these similarities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the importance of vocal feedback in spoken dialogue and its role in ensuring common ground in conversational systems.

**Method:** A triadic comparison task was conducted with participants to measure the perceived similarity of vocal feedback from two different datasets.

**Key Contributions:**

	1. Investigated the perceived prosodic similarity of vocal feedback.
	2. Demonstrated the effectiveness of spectral and self-supervised speech representations over pitch features.
	3. Showed the potential of contrastive learning in refining speech representations.

**Result:** Spectral and self-supervised speech representations were found to encode prosody better than pitch features, particularly when feedback was from the same speaker.

**Limitations:** 

**Conclusion:** Contrastive learning can be used to further align speech representations with human perception of prosody.

**Abstract:** Vocal feedback (e.g., `mhm', `yeah', `okay') is an important component of spoken dialogue and is crucial to ensuring common ground in conversational systems. The exact meaning of such feedback is conveyed through both lexical and prosodic form. In this work, we investigate the perceived prosodic similarity of vocal feedback with the same lexical form, and to what extent existing speech representations reflect such similarities. A triadic comparison task with recruited participants is used to measure perceived similarity of feedback responses taken from two different datasets. We find that spectral and self-supervised speech representations encode prosody better than extracted pitch features, especially in the case of feedback from the same speaker. We also find that it is possible to further condense and align the representations to human perception through contrastive learning.

</details>


### [208] [CSC-SQL: Corrective Self-Consistency in Text-to-SQL via Reinforcement Learning](https://arxiv.org/abs/2505.13271)

*Lei Sheng, Shuai-Shuai Xu*

**Main category:** cs.CL

**Keywords:** large language models, SQL generation, reinforcement learning

**Relevance Score:** 8

**TL;DR:** The paper introduces CSC-SQL, a method that enhances SQL query generation from natural language through the integration of Self-Consistency and Self-Correction techniques, utilizing reinforcement learning for improved accuracy.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in current methods for converting natural language questions to SQL queries, particularly in enhancing accuracy and output quality during inference.

**Method:** CSC-SQL integrates Self-Consistency and Self-Correction by selecting the most frequently occurring outputs from parallel sampling and refining them using a merge revision model. Additionally, the Group Relative Policy Optimization (GRPO) algorithm fine-tunes the models via reinforcement learning.

**Key Contributions:**

	1. Integration of Self-Consistency and Self-Correction for better SQL generation
	2. Fine-tuning of models using reinforcement learning with GRPO
	3. Open-sourcing of code for community use

**Result:** The 3B model of CSC-SQL achieves 65.28% execution accuracy and the 7B model achieves 69.19% accuracy on the BIRD development set, demonstrating improved performance over existing methods.

**Limitations:** 

**Conclusion:** CSC-SQL effectively combines two techniques for improved accuracy in SQL generation and is generalizable across different models.

**Abstract:** Large language models (LLMs) have demonstrated strong capabilities in translating natural language questions about relational databases into SQL queries. In particular, test-time scaling techniques such as Self-Consistency and Self-Correction can enhance SQL generation accuracy by increasing computational effort during inference. However, these methods have notable limitations: Self-Consistency may select suboptimal outputs despite majority votes, while Self-Correction typically addresses only syntactic errors. To leverage the strengths of both approaches, we propose CSC-SQL, a novel method that integrates Self-Consistency and Self-Correction. CSC-SQL selects the two most frequently occurring outputs from parallel sampling and feeds them into a merge revision model for correction. Additionally, we employ the Group Relative Policy Optimization (GRPO) algorithm to fine-tune both the SQL generation and revision models via reinforcement learning, significantly enhancing output quality. Experimental results confirm the effectiveness and generalizability of CSC-SQL. On the BIRD development set, our 3B model achieves 65.28% execution accuracy, while the 7B model achieves 69.19%. The code will be open sourced at https://github.com/CycloneBoy/csc_sql.

</details>


### [209] [$\textit{Rank, Chunk and Expand}$: Lineage-Oriented Reasoning for Taxonomy Expansion](https://arxiv.org/abs/2505.13282)

*Sahil Mishra, Kumar Arjun, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** taxonomy expansion, recommendation systems, discriminative ranking

**Relevance Score:** 4

**TL;DR:** LORex is a framework for taxonomy expansion that integrates discriminative ranking and generative reasoning, improving efficiency and accuracy in recommendation systems and web applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective taxonomy expansion methods due to growing data, addressing challenges faced by existing discriminative and generative approaches.

**Method:** LORex employs a combined strategy of ranking and chunking candidate terms into batches, refining selections through reasoning about their hierarchy.

**Key Contributions:**

	1. Development of a novel framework for taxonomy expansion combining ranking and generative reasoning.
	2. Demonstrated significant performance improvements over existing state-of-the-art methods.
	3. Efficient processing of candidates through hierarchy reasoning.

**Result:** LORex demonstrates a 12% improvement in accuracy and a 5% increase in Wu & Palmer similarity compared to state-of-the-art methods.

**Limitations:** 

**Conclusion:** By utilizing a hybrid approach, LORex effectively reduces noise and improves the contextual relevance of expanded taxonomies.

**Abstract:** Taxonomies are hierarchical knowledge graphs crucial for recommendation systems, and web applications. As data grows, expanding taxonomies is essential, but existing methods face key challenges: (1) discriminative models struggle with representation limits and generalization, while (2) generative methods either process all candidates at once, introducing noise and exceeding context limits, or discard relevant entities by selecting noisy candidates. We propose LORex ($\textbf{L}$ineage-$\textbf{O}$riented $\textbf{Re}$asoning for Taxonomy E$\textbf{x}$pansion), a plug-and-play framework that combines discriminative ranking and generative reasoning for efficient taxonomy expansion. Unlike prior methods, LORex ranks and chunks candidate terms into batches, filtering noise and iteratively refining selections by reasoning candidates' hierarchy to ensure contextual efficiency. Extensive experiments across four benchmarks and twelve baselines show that LORex improves accuracy by 12% and Wu & Palmer similarity by 5% over state-of-the-art methods.

</details>


### [210] [I'll believe it when I see it: Images increase misinformation sharing in Vision-Language Models](https://arxiv.org/abs/2505.13302)

*Alice Plebe, Timothy Douglas, Diana Riazi, R. Maria del Rio-Chanona*

**Main category:** cs.CL

**Keywords:** vision-language models, misinformation, news recommendation, multimodal dataset, persona conditioning

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of images on the resharing behavior of vision-language models (VLMs) in news recommendation systems, revealing that images increase resharing rates and vary by model family and user persona traits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how images affect the propensity of vision-language models to reshare news content, especially in the context of misinformation.

**Method:** A study utilizing a jailbreaking-inspired prompting strategy to analyze VLM behavior, paired with a multimodal dataset of political news and images.

**Key Contributions:**

	1. First study on images' influence on VLM resharing decisions
	2. Introduction of a jailbreaking-inspired prompting strategy
	3. Development of a multimodal dataset of fact-checked political news with images

**Result:** Experiments show images increase resharing rates by 4.8% for true news and 15.0% for false news; persona conditioning affects model sensitivity to misinformation.

**Limitations:** The model's robustness to visual misinformation was only demonstrated by one model (Claude-3-Haiku).

**Conclusion:** There are significant implications for multimodal model behavior concerning misinformation, indicating the need for evaluation frameworks and mitigation strategies.

**Abstract:** Large language models are increasingly integrated into news recommendation systems, raising concerns about their role in spreading misinformation. In humans, visual content is known to boost credibility and shareability of information, yet its effect on vision-language models (VLMs) remains unclear. We present the first study examining how images influence VLMs' propensity to reshare news content, whether this effect varies across model families, and how persona conditioning and content attributes modulate this behavior. To support this analysis, we introduce two methodological contributions: a jailbreaking-inspired prompting strategy that elicits resharing decisions from VLMs while simulating users with antisocial traits and political alignments; and a multimodal dataset of fact-checked political news from PolitiFact, paired with corresponding images and ground-truth veracity labels. Experiments across model families reveal that image presence increases resharing rates by 4.8% for true news and 15.0% for false news. Persona conditioning further modulates this effect: Dark Triad traits amplify resharing of false news, whereas Republican-aligned profiles exhibit reduced veracity sensitivity. Of all the tested models, only Claude-3-Haiku demonstrates robustness to visual misinformation. These findings highlight emerging risks in multimodal model behavior and motivate the development of tailored evaluation frameworks and mitigation strategies for personalized AI systems. Code and dataset are available at: https://github.com/3lis/misinfo_vlm

</details>


### [211] [RBF++: Quantifying and Optimizing Reasoning Boundaries across Measurable and Unmeasurable Capabilities for Chain-of-Thought Reasoning](https://arxiv.org/abs/2505.13307)

*Qiguang Chen, Libo Qin, Jinhao Liu, Yue Liao, Jiaqi Wang, Jingxuan Zhou, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, reasoning boundary, large language models, multimodal perception, optimization strategies

**Relevance Score:** 8

**TL;DR:** The paper introduces the Reasoning Boundary Framework++ (RBF++) to quantitatively evaluate and optimize Chain-of-Thought reasoning capabilities in large language models, addressing challenges in measurable and unmeasurable capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for quantitative metrics and guidelines to evaluate and optimize Chain-of-Thought (CoT) reasoning capabilities in large language models (LLMs), especially for real-world applications.

**Method:** The authors define a reasoning boundary (RB) as the maximum limit of CoT performance and propose a combination law for RBs. They also introduce a constant assumption for unmeasurable RBs and a reasoning boundary division mechanism for quantification and optimization in multimodal scenarios.

**Key Contributions:**

	1. Definition of reasoning boundary (RB) for CoT performance evaluation
	2. Introduction of a combination law for RBs
	3. Development of a reasoning boundary division mechanism for multimodal capabilities

**Result:** Experiments involving 38 models across 13 tasks validate the proposed RBF++ framework, assessing 10 CoT strategies and expanding evaluation benchmarks for RBs in LLM reasoning.

**Limitations:** 

**Conclusion:** The introduction of RBF++ aims to enhance the understanding and application of reasoning boundaries and optimization strategies in large language models, providing a foundation for further research.

**Abstract:** Chain-of-Thought (CoT) reasoning has proven effective in enhancing large language models (LLMs) on complex tasks, spurring research into its underlying mechanisms. However, two primary challenges remain for real-world applications: (1) the lack of quantitative metrics and actionable guidelines for evaluating and optimizing measurable boundaries of CoT capability, and (2) the absence of methods to assess boundaries of unmeasurable CoT capability, such as multimodal perception. To address these gaps, we introduce the Reasoning Boundary Framework++ (RBF++). To tackle the first challenge, we define the reasoning boundary (RB) as the maximum limit of CoT performance. We also propose a combination law for RBs, enabling quantitative analysis and offering actionable guidance across various CoT tasks. For the second challenge, particularly in multimodal scenarios, we introduce a constant assumption, which replaces unmeasurable RBs with scenario-specific constants. Additionally, we propose the reasoning boundary division mechanism, which divides unmeasurable RBs into two sub-boundaries, facilitating the quantification and optimization of both unmeasurable domain knowledge and multimodal perception capabilities. Extensive experiments involving 38 models across 13 tasks validate the feasibility of our framework in cross-modal settings. Additionally, we evaluate 10 CoT strategies, offer insights into optimization and decay from two complementary perspectives, and expand evaluation benchmarks for measuring RBs in LLM reasoning. We hope this work advances the understanding of RBs and optimization strategies in LLMs. Code and data are available at https://github.com/LightChen233/reasoning-boundary.

</details>


### [212] [GUARD: Generation-time LLM Unlearning via Adaptive Restriction and Detection](https://arxiv.org/abs/2505.13312)

*Zhijie Deng, Chris Yuhao Liu, Zirui Pang, Xinlei He, Lei Feng, Qi Xuan, Zhaowei Zhu, Jiaheng Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, unlearning, dynamic generation, forgetting, safety compliance

**Relevance Score:** 9

**TL;DR:** GUARD framework enables dynamic unlearning in LLMs at inference time, preventing the generation of forgotten knowledge without performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure safety and compliance of LLMs by enabling selective forgetting of specific knowledge during generation, as existing methods degrade overall performance.

**Method:** Proposes a framework called GUARD that uses a prompt classifier to detect unlearning targets and penalizes candidate tokens dynamically during LLM generation.

**Key Contributions:**

	1. Introduction of GUARD framework for generation-time unlearning
	2. Implementation of a prompt classifier for detecting forbidden tokens
	3. Demonstration of minimal performance degradation while achieving effective unlearning

**Result:** GUARD achieves strong forget quality in unlearning tasks while maintaining the utility of the model, with experimental results showing effectiveness on various datasets.

**Limitations:** 

**Conclusion:** Dynamic unlearning using GUARD allows LLMs to forget specific knowledge without compromising text fluency or overall model performance.

**Abstract:** Large Language Models (LLMs) have demonstrated strong capabilities in memorizing vast amounts of knowledge across diverse domains. However, the ability to selectively forget specific knowledge is critical for ensuring the safety and compliance of deployed models. Existing unlearning efforts typically fine-tune the model with resources such as forget data, retain data, and a calibration model. These additional gradient steps blur the decision boundary between forget and retain knowledge, making unlearning often at the expense of overall performance. To avoid the negative impact of fine-tuning, it would be better to unlearn solely at inference time by safely guarding the model against generating responses related to the forget target, without destroying the fluency of text generation. In this work, we propose Generation-time Unlearning via Adaptive Restriction and Detection (GUARD), a framework that enables dynamic unlearning during LLM generation. Specifically, we first employ a prompt classifier to detect unlearning targets and extract the corresponding forbidden token. We then dynamically penalize and filter candidate tokens during generation using a combination of token matching and semantic matching, effectively preventing the model from leaking the forgotten content. Experimental results on copyright content unlearning tasks over the Harry Potter dataset and the MUSE benchmark, as well as entity unlearning tasks on the TOFU dataset, demonstrate that GUARD achieves strong forget quality across various tasks while causing almost no degradation to the LLM's general capabilities, striking an excellent trade-off between forgetting and utility.

</details>


### [213] [Rethinking Stateful Tool Use in Multi-Turn Dialogues: Benchmarks and Challenges](https://arxiv.org/abs/2505.13328)

*Hongru Wang, Wenyu Huang, Yufei Wang, Yuanhao Xi, Jianqiao Lu, Huan Zhang, Nan Hu, Zeming Liu, Jeff Z. Pan, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Language Models, tool interactions, multi-turn dialogue, DialogTool, VirtualMobile

**Relevance Score:** 9

**TL;DR:** The paper introduces DialogTool, a multi-turn dialogue dataset for assessing Language Models in stateful tool interactions, and VirtualMobile, an evaluation environment to test API calls across various LLMs, highlighting their current performance limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in assessing Language Models for stateful, multi-turn interactions, particularly in tool use.

**Method:** The authors propose DialogTool, a dataset designed to evaluate LMs through multiple stages of tool interactions, and use VirtualMobile to simulate API calls for assessment.

**Key Contributions:**

	1. Introduction of DialogTool for evaluating multi-turn tool interactions.
	2. Development of VirtualMobile for simulating API calls.
	3. Comprehensive evaluation revealing limitations of current LLMs in stateful interactions.

**Result:** The evaluation of 13 LLMs demonstrated that existing state-of-the-art models struggle with effective tool use over extended interactions.

**Limitations:** The paper may not address all types of tool interactions or the full complexity of dialogue beyond the defined tasks.

**Conclusion:** The findings indicate significant performance gaps in current LLMs when handling multi-turn dialogue and tool interactions, suggesting room for improvement in future models.

**Abstract:** Existing benchmarks that assess Language Models (LMs) as Language Agents (LAs) for tool use primarily focus on stateless, single-turn interactions or partial evaluations, such as tool selection in a single turn, overlooking the inherent stateful nature of interactions in multi-turn applications. To fulfill this gap, we propose \texttt{DialogTool}, a multi-turn dialogue dataset with stateful tool interactions considering the whole life cycle of tool use, across six key tasks in three stages: 1) \textit{tool creation}; 2) \textit{tool utilization}: tool awareness, tool selection, tool execution; and 3) \textit{role-consistent response}: response generation and role play. Furthermore, we build \texttt{VirtualMobile} -- an embodied virtual mobile evaluation environment to simulate API calls and assess the robustness of the created APIs\footnote{We will use tools and APIs alternatively, there are no significant differences between them in this paper.}. Taking advantage of these artifacts, we conduct comprehensive evaluation on 13 distinct open- and closed-source LLMs and provide detailed analysis at each stage, revealing that the existing state-of-the-art LLMs still cannot perform well to use tools over long horizons.

</details>


### [214] [Contextual Paralinguistic Data Creation for Multi-Modal Speech-LLM: Data Condensation and Spoken QA Generation](https://arxiv.org/abs/2505.13338)

*Qiongqiong Wang, Hardik B. Sailor, Tianchi Liu, Ai Ti Aw*

**Main category:** cs.CL

**Keywords:** speech-LLMs, paralinguistic understanding, contextual reasoning, dataset generation, empathetic reasoning

**Relevance Score:** 9

**TL;DR:** A novel framework for generating QA datasets from speech data that integrates contextual reasoning with paralinguistic understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current speech-LLMs in contextual reasoning and paralinguistic understanding due to insufficient QA datasets.

**Method:** The framework uses pseudo paralinguistic label-based data condensation of in-the-wild speech and generates QA through LLM-based Contextual Paralinguistic QA (CPQA) generation.

**Key Contributions:**

	1. Integration of contextual reasoning and paralinguistic information in speech-LLM training.
	2. Creation of a novel dataset generation framework utilizing in-the-wild speech data.
	3. Validation through correlation with human-generated datasets.

**Result:** Validated effectiveness of the framework through strong correlation in evaluations with both a dataset created by the framework and a human-generated CPQA dataset.

**Limitations:** Limited capability of speech-LLMs in empathetic reasoning tasks was revealed during evaluations.

**Conclusion:** The proposed framework is innovative and has the potential to develop more robust speech-LLMs that can handle empathetic reasoning tasks effectively.

**Abstract:** Current speech-LLMs exhibit limited capability in contextual reasoning alongside paralinguistic understanding, primarily due to the lack of Question-Answer (QA) datasets that cover both aspects. We propose a novel framework for dataset generation from in-the-wild speech data, that integrates contextual reasoning with paralinguistic information. It consists of a pseudo paralinguistic label-based data condensation of in-the-wild speech and LLM-based Contextual Paralinguistic QA (CPQA) generation. The effectiveness is validated by a strong correlation in evaluations of the Qwen2-Audio-7B-Instruct model on a dataset created by our framework and human-generated CPQA dataset. The results also reveal the speech-LLM's limitations in handling empathetic reasoning tasks, highlighting the need for such datasets and more robust models. The proposed framework is first of its kind and has potential in training more robust speech-LLMs with paralinguistic reasoning capabilities.

</details>


### [215] [J4R: Learning to Judge with Equivalent Initial State Group Relative Preference Optimization](https://arxiv.org/abs/2505.13346)

*Austin Xu, Yilun Zhou, Xuan-Phi Nguyen, Caiming Xiong, Shafiq Joty*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Evaluation Benchmarks, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper focuses on enhancing the evaluation capabilities of large language models (LLMs) in reasoning-intensive tasks using reinforcement learning, proposing a new algorithm and benchmark for performance assessment.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models (LLMs), there is a need to shift from human evaluation to automated evaluation, especially in complex reasoning domains where current models struggle.

**Method:** The authors develop the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm to enhance LLM evaluation. They also create the ReasoningJudgeBench as a new benchmark to assess the performance of judges in diverse reasoning contexts.

**Key Contributions:**

	1. EIS-GRPO algorithm for robust LLM judge training
	2. Introduction of ReasoningJudgeBench for diverse reasoning evaluation
	3. Development of J4R that outperforms existing models
	4. Key improvements in evaluating reasoning tasks

**Result:** The Judge for Reasoning (J4R), trained using EIS-GRPO, surpasses GPT-4o and other smaller models in evaluation accuracy by significant margins, validating the effectiveness of the proposed methods.

**Limitations:** 

**Conclusion:** The study demonstrates that reinforcement learning can improve the robustness and performance of LLM judges in complex evaluation settings, indicating a promising direction for LLM evaluation methods.

**Abstract:** To keep pace with the increasing pace of large language models (LLM) development, model output evaluation has transitioned away from time-consuming human evaluation to automatic evaluation, where LLMs themselves are tasked with assessing and critiquing other model outputs. LLM-as-judge models are a class of generative evaluators that excel in evaluating relatively simple domains, like chat quality, but struggle in reasoning intensive domains where model responses contain more substantive and challenging content. To remedy existing judge shortcomings, we explore training judges with reinforcement learning (RL). We make three key contributions: (1) We propose the Equivalent Initial State Group Relative Policy Optimization (EIS-GRPO) algorithm, which allows us to train our judge to be robust to positional biases that arise in more complex evaluation settings. (2) We introduce ReasoningJudgeBench, a benchmark that evaluates judges in diverse reasoning settings not covered by prior work. (3) We train Judge for Reasoning (J4R), a 7B judge trained with EIS-GRPO that outperforms GPT-4o and the next best small judge by 6.7% and 9%, matching or exceeding the performance of larger GRPO-trained judges on both JudgeBench and ReasoningJudgeBench.

</details>


### [216] [Investigating the Vulnerability of LLM-as-a-Judge Architectures to Prompt-Injection Attacks](https://arxiv.org/abs/2505.13348)

*Narek Maloyan, Bislan Ashinov, Dmitry Namiot*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation, adversarial attacks, robustness, trustworthiness

**Relevance Score:** 8

**TL;DR:** The paper investigates vulnerabilities in LLM-as-a-Judge systems to prompt-injection attacks, highlighting significant susceptibility and the need for robust defenses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs as evaluators raises concerns regarding their reliability and security, particularly against adversarial manipulations.

**Method:** The study formalizes two attack strategies: Comparative Undermining Attack (CUA) and Justification Manipulation Attack (JMA), using the Greedy Coordinate Gradient (GCG) optimization method to craft adversarial inputs.

**Key Contributions:**

	1. Identification of attack strategies against LLM-as-a-Judge systems
	2. Empirical validation of vulnerabilities using specific LLMs
	3. Highlighting the need for enhancing trustworthiness in LLM evaluations

**Result:** Experiments on the MT-Bench Human Judgments dataset demonstrate that the CUA achieves an Attack Success Rate (ASR) exceeding 30%, indicating significant vulnerabilities in current systems.

**Limitations:** 

**Conclusion:** The findings illustrate critical weaknesses in LLM-as-a-Judge systems, emphasizing the urgent need for improved defense mechanisms and further exploration of adversarial robustness.

**Abstract:** Large Language Models (LLMs) are increasingly employed as evaluators (LLM-as-a-Judge) for assessing the quality of machine-generated text. This paradigm offers scalability and cost-effectiveness compared to human annotation. However, the reliability and security of such systems, particularly their robustness against adversarial manipulations, remain critical concerns. This paper investigates the vulnerability of LLM-as-a-Judge architectures to prompt-injection attacks, where malicious inputs are designed to compromise the judge's decision-making process. We formalize two primary attack strategies: Comparative Undermining Attack (CUA), which directly targets the final decision output, and Justification Manipulation Attack (JMA), which aims to alter the model's generated reasoning. Using the Greedy Coordinate Gradient (GCG) optimization method, we craft adversarial suffixes appended to one of the responses being compared. Experiments conducted on the MT-Bench Human Judgments dataset with open-source instruction-tuned LLMs (Qwen2.5-3B-Instruct and Falcon3-3B-Instruct) demonstrate significant susceptibility. The CUA achieves an Attack Success Rate (ASR) exceeding 30\%, while JMA also shows notable effectiveness. These findings highlight substantial vulnerabilities in current LLM-as-a-Judge systems, underscoring the need for robust defense mechanisms and further research into adversarial evaluation and trustworthiness in LLM-based assessment frameworks.

</details>


### [217] [Sense and Sensitivity: Examining the Influence of Semantic Recall on Long Context Code Reasoning](https://arxiv.org/abs/2505.13353)

*Adam Štorek, Mukur Gupta, Samira Hajizadeh, Prashast Srivastava, Suman Jana*

**Main category:** cs.CL

**Keywords:** Large Language Models, Code Reasoning, Semantic Recall, Lexical Recall, Benchmarks

**Relevance Score:** 8

**TL;DR:** This paper examines the effectiveness of Large Language Models (LLMs) in code reasoning within large repositories, focusing on the distinction between lexical and semantic code recall.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how well LLMs utilize long contexts for code reasoning and the role of recall in their effectiveness.

**Method:** Introduces SemTrace, a code reasoning technique to measure semantic recall, and evaluates state-of-the-art LLMs on their code reasoning abilities.

**Key Contributions:**

	1. Introduced SemTrace for assessing semantic recall in code
	2. Identified the performance drop in code reasoning accuracy with long context
	3. Revealed disconnect between lexical and semantic recall mechanisms.

**Result:** Significant accuracy drop in code reasoning as snippets approach the middle of the context, with varying recall performance based on lexical granularity.

**Limitations:** Current benchmarks likely underrepresent the semantic recall challenges faced by LLMs.

**Conclusion:** Current benchmarks may not accurately reflect LLMs' challenges in leveraging in-context information due to low semantic recall sensitivity.

**Abstract:** Although modern Large Language Models (LLMs) support extremely large contexts, their effectiveness in utilizing long context for code reasoning remains unclear. This paper investigates LLM reasoning ability over code snippets within large repositories and how it relates to their recall ability. Specifically, we differentiate between lexical code recall (verbatim retrieval) and semantic code recall (remembering what the code does). To measure semantic recall, we propose SemTrace, a code reasoning technique where the impact of specific statements on output is attributable and unpredictable. We also present a method to quantify semantic recall sensitivity in existing benchmarks. Our evaluation of state-of-the-art LLMs reveals a significant drop in code reasoning accuracy as a code snippet approaches the middle of the input context, particularly with techniques requiring high semantic recall like SemTrace. Moreover, we find that lexical recall varies by granularity, with models excelling at function retrieval but struggling with line-by-line recall. Notably, a disconnect exists between lexical and semantic recall, suggesting different underlying mechanisms. Finally, our findings indicate that current code reasoning benchmarks may exhibit low semantic recall sensitivity, potentially underestimating LLM challenges in leveraging in-context information.

</details>


### [218] [What Prompts Don't Say: Understanding and Managing Underspecification in LLM Prompts](https://arxiv.org/abs/2505.13360)

*Chenyang Yang, Yike Shi, Qianou Ma, Michael Xieyang Liu, Christian Kästner, Tongshuang Wu*

**Main category:** cs.CL

**Keywords:** LLM, prompt optimization, underspecification, requirements discovery, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper analyzes prompt underspecification in LLMs and introduces requirements-aware prompt optimization mechanisms to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues caused by underspecified prompts from developers when building LLM-powered software.

**Method:** In-depth analysis of prompt underspecification followed by the introduction of novel requirements-aware prompt optimization mechanisms.

**Key Contributions:**

	1. Analysis of LLM prompt underspecification
	2. Introduction of requirements-aware prompt optimization mechanisms
	3. Evidence that simply adding more requirements isn't always effective

**Result:** Demonstrated that adding more requirements to prompts does not consistently improve performance; proposed methods show an average performance improvement of 4.8%.

**Limitations:** The proposed optimizations may not address all forms of prompt underspecification or generalize across all applications.

**Conclusion:** Effective management of prompt underspecification requires a broader approach, including proactive requirements discovery and monitoring.

**Abstract:** Building LLM-powered software requires developers to communicate their requirements through natural language, but developer prompts are frequently underspecified, failing to fully capture many user-important requirements. In this paper, we present an in-depth analysis of prompt underspecification, showing that while LLMs can often (41.1%) guess unspecified requirements by default, such behavior is less robust: Underspecified prompts are 2x more likely to regress over model or prompt changes, sometimes with accuracy drops by more than 20%. We then demonstrate that simply adding more requirements to a prompt does not reliably improve performance, due to LLMs' limited instruction-following capabilities and competing constraints, and standard prompt optimizers do not offer much help. To address this, we introduce novel requirements-aware prompt optimization mechanisms that can improve performance by 4.8% on average over baselines that naively specify everything in the prompt. Beyond prompt optimization, we envision that effectively managing prompt underspecification requires a broader process, including proactive requirements discovery, evaluation, and monitoring.

</details>


### [219] [Thinkless: LLM Learns When to Think](https://arxiv.org/abs/2505.13379)

*Gongfan Fang, Xinyin Ma, Xinchao Wang*

**Main category:** cs.CL

**Keywords:** Reasoning Language Models, Reinforcement Learning, Adaptive Reasoning

**Relevance Score:** 8

**TL;DR:** Proposal of Thinkless, a learnable framework for LLMs to choose between short-form and long-form reasoning, improving efficiency by reducing unnecessary complex reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiencies in Reasoning Language Models that arise from using complex reasoning for straightforward queries.

**Method:** Thinkless uses a reinforcement learning framework with two control tokens to adaptively select reasoning modes, employing a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm to refine learning objectives.

**Key Contributions:**

	1. Introduction of a learnable framework for adaptive reasoning in LLMs
	2. Development of the DeGRPO algorithm for effective hybrid reasoning
	3. Demonstration of efficiency improvements on standard benchmarks

**Result:** Thinkless reduces long-chain reasoning usage by 50% - 90% across benchmarks like Minerva Algebra, MATH-500, and GSM8K, enhancing model efficiency.

**Limitations:** 

**Conclusion:** Thinkless offers a structured approach to optimize reasoning processes in LLMs, improving both efficiency and accuracy in task responses.

**Abstract:** Reasoning Language Models, capable of extended chain-of-thought reasoning, have demonstrated remarkable performance on tasks requiring complex logical inference. However, applying elaborate reasoning for all queries often results in substantial computational inefficiencies, particularly when many problems admit straightforward solutions. This motivates an open question: Can LLMs learn when to think? To answer this, we propose Thinkless, a learnable framework that empowers an LLM to adaptively select between short-form and long-form reasoning, based on both task complexity and the model's ability. Thinkless is trained under a reinforcement learning paradigm and employs two control tokens, <short> for concise responses and <think> for detailed reasoning. At the core of our method is a Decoupled Group Relative Policy Optimization (DeGRPO) algorithm, which decomposes the learning objective of hybrid reasoning into two components: (1) a control token loss that governs the selection of the reasoning mode, and (2) a response loss that improves the accuracy of the generated answers. This decoupled formulation enables fine-grained control over the contributions of each objective, stabilizing training and effectively preventing collapse observed in vanilla GRPO. Empirically, on several benchmarks such as Minerva Algebra, MATH-500, and GSM8K, Thinkless is able to reduce the usage of long-chain thinking by 50% - 90%, significantly improving the efficiency of Reasoning Language Models. The code is available at https://github.com/VainF/Thinkless

</details>


### [220] [R3: Robust Rubric-Agnostic Reward Models](https://arxiv.org/abs/2505.13388)

*David Anugraha, Zilu Tang, Lester James V. Miranda, Hanyang Zhao, Mohammad Rifqi Farhansyah, Garry Kuwanto, Derry Wijaya, Genta Indra Winata*

**Main category:** cs.CL

**Keywords:** Reward Models, Language Models, Human Preferences

**Relevance Score:** 9

**TL;DR:** R3 is a novel framework for reward modeling that enhances controllability and interpretability, aligning language model outputs with human preferences. It supports generalization across evaluation dimensions and enables interpretable score assignments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing reward models for language models often lack controllability and interpretability, limiting their effectiveness in aligning outputs with diverse human preferences.

**Method:** We introduce R3, a rubric-agnostic reward modeling framework that focuses on generalizability and provides interpretable score assignments for language models.

**Key Contributions:**

	1. Introduction of a rubric-agnostic reward modeling framework
	2. Enhanced interpretability of score assignments
	3. Better generalizability across evaluation dimensions

**Result:** R3 facilitates a more transparent evaluation of language models, aligning them better with human values and allowing for broader applicability across various tasks.

**Limitations:** 

**Conclusion:** R3 offers a significant improvement in the way we evaluate language models, making them more aligned with human preferences and more interpretable.

**Abstract:** Reward models are essential for aligning language model outputs with human preferences, yet existing approaches often lack both controllability and interpretability. These models are typically optimized for narrow objectives, limiting their generalizability to broader downstream tasks. Moreover, their scalar outputs are difficult to interpret without contextual reasoning. To address these limitations, we introduce R3, a novel reward modeling framework that is rubric-agnostic, generalizable across evaluation dimensions, and provides interpretable, reasoned score assignments. R3 enables more transparent and flexible evaluation of language models, supporting robust alignment with diverse human values and use cases. Our models, data, and code are available as open source at https://github.com/rubricreward/r3

</details>


### [221] [MR. Judge: Multimodal Reasoner as a Judge](https://arxiv.org/abs/2505.13403)

*Renjie Pi, Felix Bai, Qibin Chen, Simon Wang, Jiulong Shan, Kieran Liu, Meng Cao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multimodal Models, Reasoning, Evaluation, Artificial Intelligence

**Relevance Score:** 9

**TL;DR:** This paper introduces MR. Judge, a new paradigm for utilizing MLLMs as evaluative judges that enhances reasoning capabilities and improves performance in response evaluation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more effective evaluative judgment mechanisms in LLMs and the desire to enhance interpretability and performance in response selection.

**Method:** MR. Judge formulates the judgment process as a reasoning-inspired multiple-choice problem, employing a two-step strategy: generating plausible negative candidates and distilling reasoning capabilities from a text-based model.

**Key Contributions:**

	1. Introduction of a reasoning-inspired multiple-choice judgment process
	2. Development of a strategy for automatic annotation through response synthesis
	3. Demonstration of improved performance over existing models like GPT-4o

**Result:** MR. Judge demonstrates enhanced performance, surpassing GPT-4o by 9.9% on VL-RewardBench, and improving MM-Vet performance during inference-time scaling by up to 7.7%.

**Limitations:** 

**Conclusion:** The proposed MR. Judge paradigm significantly improves evaluative judging capabilities of MLLMs, making them more effective across various tasks.

**Abstract:** The paradigm of using Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) as evaluative judges has emerged as an effective approach in RLHF and inference-time scaling. In this work, we propose Multimodal Reasoner as a Judge (MR. Judge), a paradigm for empowering general-purpose MLLMs judges with strong reasoning capabilities. Instead of directly assigning scores for each response, we formulate the judgement process as a reasoning-inspired multiple-choice problem. Specifically, the judge model first conducts deliberate reasoning covering different aspects of the responses and eventually selects the best response from them. This reasoning process not only improves the interpretibility of the judgement, but also greatly enhances the performance of MLLM judges. To cope with the lack of questions with scored responses, we propose the following strategy to achieve automatic annotation: 1) Reverse Response Candidates Synthesis: starting from a supervised fine-tuning (SFT) dataset, we treat the original response as the best candidate and prompt the MLLM to generate plausible but flawed negative candidates. 2) Text-based reasoning extraction: we carefully design a data synthesis pipeline for distilling the reasoning capability from a text-based reasoning model, which is adopted to enable the MLLM judges to regain complex reasoning ability via warm up supervised fine-tuning. Experiments demonstrate that our MR. Judge is effective across a wide range of tasks. Specifically, our MR. Judge-7B surpasses GPT-4o by 9.9% on VL-RewardBench, and improves performance on MM-Vet during inference-time scaling by up to 7.7%.

</details>


### [222] [Granary: Speech Recognition and Translation Dataset in 25 European Languages](https://arxiv.org/abs/2505.13404)

*Nithin Rao Koluguri, Monica Sekoyan, George Zelenfroynd, Sasha Meister, Shuoyang Ding, Sofia Kostandian, He Huang, Nikolay Karpov, Jagadeesh Balam, Vitaly Lavrukhin, Yifan Peng, Sara Papi, Marco Gaido, Alessio Brutti, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** speech processing, low-resource languages, dataset development

**Relevance Score:** 7

**TL;DR:** Granary is a large-scale collection of speech datasets for recognition and translation across 25 European languages, enhancing low-resource language processing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of data for speech processing in low-resource languages and improve multi-task and multilingual approaches.

**Method:** A pseudo-labeling pipeline is used for data enhancement, incorporating segmentation, two-pass inference, hallucination filtering, and punctuation restoration, along with translation pair generation from pseudo-labeled transcriptions using EuroLLM.

**Key Contributions:**

	1. First open-source collection of speech datasets for recognition and translation across 25 European languages
	2. Enhanced data quality through a sophisticated pseudo-labeling pipeline
	3. Demonstrated improved model performance with reduced data requirements

**Result:** Models trained on the processed data show similar performance to those trained on curated datasets, using approximately 50% less data for both high- and low-resource languages.

**Limitations:** 

**Conclusion:** The Granary dataset represents a significant step forward in open-source speech recognition and translation for multiple languages, with a highly efficient processing methodology.

**Abstract:** Multi-task and multilingual approaches benefit large models, yet speech processing for low-resource languages remains underexplored due to data scarcity. To address this, we present Granary, a large-scale collection of speech datasets for recognition and translation across 25 European languages. This is the first open-source effort at this scale for both transcription and translation. We enhance data quality using a pseudo-labeling pipeline with segmentation, two-pass inference, hallucination filtering, and punctuation restoration. We further generate translation pairs from pseudo-labeled transcriptions using EuroLLM, followed by a data filtration pipeline. Designed for efficiency, our pipeline processes vast amount of data within hours. We assess models trained on processed data by comparing their performance on previously curated datasets for both high- and low-resource languages. Our findings show that these models achieve similar performance using approx. 50% less data. Dataset will be made available at https://hf.co/datasets/nvidia/Granary

</details>


### [223] [AdaptThink: Reasoning Models Can Learn When to Think](https://arxiv.org/abs/2505.13417)

*Jiajie Zhang, Nianyi Lin, Lei Hou, Ling Feng, Juanzi Li*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Reasoning Models, Adaptive Thinking

**Relevance Score:** 7

**TL;DR:** This paper introduces AdaptThink, a novel RL algorithm that enables reasoning models to choose between deep thinking and skipping thinking to optimize performance and efficiency in response times.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency caused by lengthy reasoning processes in large reasoning models, which increases inference overhead.

**Method:** AdaptThink utilizes a constrained optimization objective and an importance sampling strategy to allow reasoning models to adaptively choose between NoThinking and traditional thinking modes during training.

**Key Contributions:**

	1. Introduction of the AdaptThink RL algorithm for adaptive thinking mode selection
	2. Demonstration of improved efficiency in reasoning models with significant reduction in average response length
	3. Enhanced performance on math datasets while maintaining accuracy

**Result:** AdaptThink significantly reduces inference costs by 53% and improves accuracy by 2.4% on multiple math datasets, demonstrating enhanced performance without sacrificing efficiency.

**Limitations:** 

**Conclusion:** Adaptive thinking-mode selection via AdaptThink presents a promising approach to enhance the balance between reasoning quality and efficiency in large reasoning models.

**Abstract:** Recently, large reasoning models have achieved impressive performance on various tasks by employing human-like deep thinking. However, the lengthy thinking process substantially increases inference overhead, making efficiency a critical bottleneck. In this work, we first demonstrate that NoThinking, which prompts the reasoning model to skip thinking and directly generate the final solution, is a better choice for relatively simple tasks in terms of both performance and efficiency. Motivated by this, we propose AdaptThink, a novel RL algorithm to teach reasoning models to choose the optimal thinking mode adaptively based on problem difficulty. Specifically, AdaptThink features two core components: (1) a constrained optimization objective that encourages the model to choose NoThinking while maintaining the overall performance; (2) an importance sampling strategy that balances Thinking and NoThinking samples during on-policy training, thereby enabling cold start and allowing the model to explore and exploit both thinking modes throughout the training process. Our experiments indicate that AdaptThink significantly reduces the inference costs while further enhancing performance. Notably, on three math datasets, AdaptThink reduces the average response length of DeepSeek-R1-Distill-Qwen-1.5B by 53% and improves its accuracy by 2.4%, highlighting the promise of adaptive thinking-mode selection for optimizing the balance between reasoning quality and efficiency. Our codes and models are available at https://github.com/THU-KEG/AdaptThink.

</details>


### [224] [Dementia Through Different Eyes: Explainable Modeling of Human and LLM Perceptions for Early Awareness](https://arxiv.org/abs/2505.13418)

*Lotem Peled-Cohen, Maya Zadok, Nitay Calderon, Hila Gonen, Roi Reichart*

**Main category:** cs.CL

**Keywords:** dementia, language perception, large language models, human-computer interaction, explainable AI

**Relevance Score:** 8

**TL;DR:** This paper investigates how dementia is perceived through language by non-experts and LLMs, revealing inconsistencies in human perceptions and a richer feature set used by LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how language reflects cognitive decline in individuals and the role of non-experts and LLMs in perceiving these changes.

**Method:** Transcribed picture descriptions were presented to non-expert humans and LLMs for intuitive judgment of whether the text was produced by someone healthy or with dementia, utilizing an explainable method to extract linguistic features and logistic regression for analysis.

**Key Contributions:**

	1. Introduced an explainable method for extracting features from language related to dementia.
	2. Compared non-expert and LLM perceptions of dementia with clinical diagnoses.
	3. Highlighted the limitations of human judgment in recognizing dementia signs.

**Result:** Human perceptions of dementia were inconsistent and relied on a limited set of cues, while LLMs utilized a richer, more nuanced feature set that aligned with clinical diagnoses, although both groups had difficulties recognizing dementia.

**Limitations:** Both non-experts and LLMs were prone to false negatives, often overlooking dementia cases.

**Conclusion:** An interpretable framework was developed to assist non-experts in recognizing significant linguistic signs of dementia, revealing potential to improve early identification of cognitive decline.

**Abstract:** Cognitive decline often surfaces in language years before diagnosis. It is frequently non-experts, such as those closest to the patient, who first sense a change and raise concern. As LLMs become integrated into daily communication and used over prolonged periods, it may even be an LLM that notices something is off. But what exactly do they notice--and should be noticing--when making that judgment? This paper investigates how dementia is perceived through language by non-experts. We presented transcribed picture descriptions to non-expert humans and LLMs, asking them to intuitively judge whether each text was produced by someone healthy or with dementia. We introduce an explainable method that uses LLMs to extract high-level, expert-guided features representing these picture descriptions, and use logistic regression to model human and LLM perceptions and compare with clinical diagnoses. Our analysis reveals that human perception of dementia is inconsistent and relies on a narrow, and sometimes misleading, set of cues. LLMs, by contrast, draw on a richer, more nuanced feature set that aligns more closely with clinical patterns. Still, both groups show a tendency toward false negatives, frequently overlooking dementia cases. Through our interpretable framework and the insights it provides, we hope to help non-experts better recognize the linguistic signs that matter.

</details>


### [225] [SMOTExT: SMOTE meets Large Language Models](https://arxiv.org/abs/2505.13434)

*Mateusz Bystroński, Mikołaj Hołysz, Grzegorz Piotrowski, Nitesh V. Chawla, Tomasz Kajdanowicz*

**Main category:** cs.CL

**Keywords:** SMOTExT, data augmentation, NLP, class imbalance, privacy-preserving

**Relevance Score:** 9

**TL;DR:** SMOTExT is a novel technique for addressing data scarcity and class imbalance in NLP by generating synthetic text examples using BERT embeddings and xRAG architecture.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle challenges of data scarcity and class imbalance in NLP models, particularly in specialized domains or low-resource settings.

**Method:** SMOTExT adapts the Synthetic Minority Over-sampling Technique (SMOTE) to textual data by interpolating BERT-based embeddings to generate synthetic examples and decoding them into coherent text using xRAG architecture.

**Key Contributions:**

	1. Introduction of SMOTExT for synthetic text generation
	2. Demonstration of effective use of BERT-based embeddings for data augmentation
	3. Establishment of a framework for privacy-preserving NLP model training

**Result:** Preliminary results show that models trained on synthetic data can achieve performance comparable to those trained on original datasets, indicating potential in knowledge distillation and data augmentation.

**Limitations:** Preliminary work with only qualitative outputs.

**Conclusion:** This technique shows potential for enhancing NLP models while adhering to data privacy constraints, offering a new avenue for effective learning.

**Abstract:** Data scarcity and class imbalance are persistent challenges in training robust NLP models, especially in specialized domains or low-resource settings. We propose a novel technique, SMOTExT, that adapts the idea of Synthetic Minority Over-sampling (SMOTE) to textual data. Our method generates new synthetic examples by interpolating between BERT-based embeddings of two existing examples and then decoding the resulting latent point into text with xRAG architecture. By leveraging xRAG's cross-modal retrieval-generation framework, we can effectively turn interpolated vectors into coherent text. While this is preliminary work supported by qualitative outputs only, the method shows strong potential for knowledge distillation and data augmentation in few-shot settings. Notably, our approach also shows promise for privacy-preserving machine learning: in early experiments, training models solely on generated data achieved comparable performance to models trained on the original dataset. This suggests a viable path toward safe and effective learning under data protection constraints.

</details>


### [226] [ChartMuseum: Testing Visual Reasoning Capabilities of Large Vision-Language Models](https://arxiv.org/abs/2505.13444)

*Liyan Tang, Grace Kim, Xinyu Zhao, Thom Lake, Wenxuan Ding, Fangcong Yin, Prasann Singhal, Manya Wadhwa, Zeyu Leo Liu, Zayne Sprague, Ramya Namuduri, Bodun Hu, Juan Diego Rodriguez, Puyuan Peng, Greg Durrett*

**Main category:** cs.CL

**Keywords:** chart understanding, vision-language models, benchmark, visual reasoning, human performance

**Relevance Score:** 8

**TL;DR:** This paper explores the challenges of chart understanding for large vision-language models (LVLMs) and introduces ChartMuseum, a benchmark to evaluate visual and textual reasoning in this context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the disparity in performance between human visual reasoning and that of LVLMs when interpreting complex charts.

**Method:** Conducted a case study using a synthetic dataset for visual reasoning, evaluated performance across LVLMs, and introduced the ChartMuseum benchmark with expert-annotated questions.

**Key Contributions:**

	1. Introduction of ChartMuseum benchmark for chart QA
	2. Demonstration of model performance degradation with visual complexity
	3. Qualitative analysis of error categories in visual reasoning for LVLMs

**Result:** The study revealed a significant drop in model performance (best model at 63% accuracy) compared to humans (93% accuracy), especially on questions requiring visual reasoning, where models drop 35%-55% in performance.

**Limitations:** 

**Conclusion:** There's a substantial gap in performance between humans and LVLMs on chart understanding tasks, highlighting the limitations of current models in complex visual reasoning.

**Abstract:** Chart understanding presents a unique challenge for large vision-language models (LVLMs), as it requires the integration of sophisticated textual and visual reasoning capabilities. However, current LVLMs exhibit a notable imbalance between these skills, falling short on visual reasoning that is difficult to perform in text. We conduct a case study using a synthetic dataset solvable only through visual reasoning and show that model performance degrades significantly with increasing visual complexity, while human performance remains robust. We then introduce ChartMuseum, a new Chart Question Answering (QA) benchmark containing 1,162 expert-annotated questions spanning multiple reasoning types, curated from real-world charts across 184 sources, specifically built to evaluate complex visual and textual reasoning. Unlike prior chart understanding benchmarks -- where frontier models perform similarly and near saturation -- our benchmark exposes a substantial gap between model and human performance, while effectively differentiating model capabilities: although humans achieve 93% accuracy, the best-performing model Gemini-2.5-Pro attains only 63.0%, and the leading open-source LVLM Qwen2.5-VL-72B-Instruct achieves only 38.5%. Moreover, on questions requiring primarily visual reasoning, all models experience a 35%-55% performance drop from text-reasoning-heavy question performance. Lastly, our qualitative error analysis reveals specific categories of visual reasoning that are challenging for current LVLMs.

</details>


### [227] [CIE: Controlling Language Model Text Generations Using Continuous Signals](https://arxiv.org/abs/2505.13448)

*Vinay Samuel, Harshita Diddee, Yiming Zhang, Daphne Ippolito*

**Main category:** cs.CL

**Keywords:** language models, user intent, continuous control, response length, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper presents a method for continuous control of language model (LM) outputs, focusing on response length customization, demonstrating improvements over existing discrete control techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improving user experience by allowing more nuanced control over language model outputs to align better with user intent.

**Method:** The authors propose a technique that utilizes continuous control signals, allowing users to manipulate LM responses along a spectrum rather than relying on rigid discrete signals or prompts.

**Key Contributions:**

	1. Introduction of continuous control signals for language model outputs
	2. Demonstration of improved response-length control
	3. Provision of open-source code and datasets for further research

**Result:** The proposed method shows improved reliability in exerting control over response length compared to traditional in-context learning and discrete fine-tuning methods.

**Limitations:** The study focuses primarily on response-length control and may not generalize to other types of linguistic features.

**Conclusion:** Fine-tuning language models with continuous control signals results in more effective customization of generated outputs, with open-source resources provided for further research.

**Abstract:** Aligning language models with user intent is becoming increasingly relevant to enhance user experience. This calls for designing methods that can allow users to control the properties of the language that LMs generate. For example, controlling the length of the generation, the complexity of the language that gets chosen, the sentiment, tone, etc. Most existing work attempts to integrate users' control by conditioning LM generations on natural language prompts or discrete control signals, which are often brittle and hard to scale. In this work, we are interested in \textit{continuous} control signals, ones that exist along a spectrum that can't easily be captured in a natural language prompt or via existing techniques in conditional generation. Through a case study in controlling the precise response-length of generations produced by LMs, we demonstrate how after fine-tuning, behaviors of language models can be controlled via continuous signals -- as vectors that are interpolated between a "low" and a "high" token embedding. Our method more reliably exerts response-length control than in-context learning methods or fine-tuning methods that represent the control signal as a discrete signal. Our full open-sourced code and datasets are available at https://github.com/vsamuel2003/CIE.

</details>


### [228] [Large Linguistic Models: Investigating LLMs' metalinguistic abilities](https://arxiv.org/abs/2305.00948)

*Gašper Beguš, Maksymilian Dąbkowski, Ryan Rhodes*

**Main category:** cs.CL

**Keywords:** large language models, metalinguistic analysis, interpretability, syntactic trees, phonological generalization

**Relevance Score:** 8

**TL;DR:** The paper demonstrates that large language models can generate valid metalinguistic analyses and outlines a research program to evaluate their interpretability through prompting.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of large language models (LLMs) by evaluating their metalinguistic abilities and their implications for theoretical models in linguistics.

**Method:** The study tests the behavioral interpretability of LLMs through prompting and compares the performance of different models on tasks related to syntactic trees and phonological generalization.

**Key Contributions:**

	1. First demonstration of LLMs generating valid metalinguistic analyses
	2. Identification of OpenAI's o1 as superior in syntactic and phonological tasks
	3. Introduction of a research program for evaluating LLM interpretability via prompting

**Result:** OpenAI's o1 significantly outperforms other LLMs in tasks involving metalinguistic analyses, suggesting its chain-of-thought mechanism enhances its linguistic capabilities.

**Limitations:** 

**Conclusion:** The unique advantages of OpenAI o1 in linguistic tasks provide insights into LLM performance and their potential applications in understanding human language processing.

**Abstract:** The performance of large language models (LLMs) has recently improved to the point where models can perform well on many language tasks. We show here that--for the first time--the models can also generate valid metalinguistic analyses of language data. We outline a research program where the behavioral interpretability of LLMs on these tasks is tested via prompting. LLMs are trained primarily on text--as such, evaluating their metalinguistic abilities improves our understanding of their general capabilities and sheds new light on theoretical models in linguistics. We show that OpenAI's (2024) o1 vastly outperforms other models on tasks involving drawing syntactic trees and phonological generalization. We speculate that OpenAI o1's unique advantage over other models may result from the model's chain-of-thought mechanism, which mimics the structure of human reasoning used in complex cognitive tasks, such as linguistic analysis.

</details>


### [229] [Physics of Language Models: Part 1, Learning Hierarchical Language Structures](https://arxiv.org/abs/2305.13673)

*Zeyuan Allen-Zhu, Yuanzhi Li*

**Main category:** cs.CL

**Keywords:** transformer models, context-free grammars, recursive reasoning, language structures, GPT

**Relevance Score:** 8

**TL;DR:** This paper investigates how transformer-based language models, particularly generative models like GPT, understand and perform recursive language structure reasoning defined by context-free grammars (CFGs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the internal workings and reasoning mechanisms of transformer-based language models, especially their ability to handle complex linguistic structures.

**Method:** The authors introduce a family of synthetic CFGs to test the models' proficiency in parsing hierarchical sentences. They analyze model internals to reveal how hidden states capture CFG structures and examine attention patterns.

**Key Contributions:**

	1. Introduction of synthetic CFGs for testing language models
	2. Demonstration of GPT's capability in generating CFG-compliant sentences
	3. Analysis of attention mechanisms related to dynamic programming

**Result:** Generative models like GPT can learn and reason over CFG-defined hierarchies accurately, with hidden states reflecting CFG structures and attention patterns akin to dynamic programming processes.

**Limitations:** Focuses on generative models and may not generalize to all types of transformer models.

**Conclusion:** The study highlights the effectiveness of transformer models in reasoning over complex linguistic structures and provides insights into the limitations of other model architectures, while suggesting improvements in pretraining strategies.

**Abstract:** Transformer-based language models are effective but complex, and understanding their inner workings and reasoning mechanisms is a significant challenge. Previous research has primarily explored how these models handle simple tasks like name copying or selection, and we extend this by investigating how these models perform recursive language structure reasoning defined by context-free grammars (CFGs). We introduce a family of synthetic CFGs that produce hierarchical rules, capable of generating lengthy sentences (e.g., hundreds of tokens) that are locally ambiguous and require dynamic programming to parse. Despite this complexity, we demonstrate that generative models like GPT can accurately learn and reason over CFG-defined hierarchies and generate sentences based on it. We explore the model's internals, revealing that its hidden states precisely capture the structure of CFGs, and its attention patterns resemble the information passing in a dynamic programming algorithm.   This paper also presents several corollaries, including showing why absolute positional embeddings is inferior to relative and rotary embeddings; uniform attention alone is surprisingly effective (motivating our follow-up work on Canon layers); encoder-only models (e.g., BERT, DeBERTa) struggle with deep structure reasoning on CFGs compared to autoregressive models (e.g., GPT); and injecting structural or syntactic noise into pretraining data markedly improves robustness to corrupted language prompts.

</details>


### [230] [Cross-Lingual Consistency of Factual Knowledge in Multilingual Language Models](https://arxiv.org/abs/2310.10378)

*Jirui Qi, Raquel Fernández, Arianna Bisazza*

**Main category:** cs.CL

**Keywords:** Multilingual PLMs, Cross-lingual consistency, Factual knowledge

**Relevance Score:** 8

**TL;DR:** The paper investigates cross-lingual consistency of factual knowledge in multilingual PLMs and introduces a new metric, RankC, to evaluate it.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure consistent feedback from multilingual PLMs for users across different language backgrounds.

**Method:** The authors propose the Ranking-based Consistency (RankC) metric and analyze cross-lingual consistency factors at both model and language-pair levels.

**Key Contributions:**

	1. Introduction of the RankC metric for cross-lingual consistency evaluation
	2. In-depth analysis of factors affecting cross-lingual consistency
	3. Case study on factual knowledge transfer between languages based on RankC scores.

**Result:** The analysis shows that larger model sizes improve factual accuracy but not cross-lingual consistency; knowledge transfer occurs primarily to languages with high RankC scores.

**Limitations:** The findings are based on a small sample of facts and may not generalize to all languages or model architectures.

**Conclusion:** The study highlights that while model size affects accuracy, it does not guarantee cross-lingual consistency, which is influenced by specific language pairs.

**Abstract:** Multilingual large-scale Pretrained Language Models (PLMs) have been shown to store considerable amounts of factual knowledge, but large variations are observed across languages. With the ultimate goal of ensuring that users with different language backgrounds obtain consistent feedback from the same model, we study the cross-lingual consistency (CLC) of factual knowledge in various multilingual PLMs. To this end, we propose a Ranking-based Consistency (RankC) metric to evaluate knowledge consistency across languages independently from accuracy. Using this metric, we conduct an in-depth analysis of the determining factors for CLC, both at model level and at language-pair level. Among other results, we find that increasing model size leads to higher factual probing accuracy in most languages, but does not improve cross-lingual consistency. Finally, we conduct a case study on CLC when new factual associations are inserted in the PLMs via model editing. Results on a small sample of facts inserted in English reveal a clear pattern whereby the new piece of knowledge transfers only to languages with which English has a high RankC score.

</details>


### [231] [Automatically generating Riddles aiding Concept Attainment](https://arxiv.org/abs/2310.18290)

*Niharika Sri Parasa, Chaitali Diwan, Srinath Srinivasa*

**Main category:** cs.CL

**Keywords:** Concept Attainment Model, learner engagement, online learning

**Relevance Score:** 4

**TL;DR:** The paper demonstrates the application of the Concept Attainment Model to develop conceptual riddles aimed at improving learner engagement in online environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The primary challenge of retaining learner engagement in online learning environments necessitates innovative instructional strategies.

**Method:** The paper applies the Concept Attainment Model to create factual triples from learning resources, classifying them into 'Topic Markers' and 'Common', and then generates riddles based on this framework.

**Key Contributions:**

	1. Introduction of conceptual riddles as a strategy for engaging learners.
	2. Utilization of the Concept Attainment Model in an innovative context.
	3. Positive results from human evaluations of the riddles created.

**Result:** Human evaluation of the created riddles shows encouraging results in enhancing engagement.

**Limitations:** The scalability of this approach and its effectiveness across diverse subjects need further investigation.

**Conclusion:** The application of the Concept Attainment Model in creating riddles has the potential to positively impact learner engagement in online education.

**Abstract:** One of the primary challenges in online learning environments, is to retain learner engagement. Several different instructional strategies are proposed both in online and offline environments to enhance learner engagement. The Concept Attainment Model is one such instructional strategy that focuses on learners acquiring a deeper understanding of a concept rather than just its dictionary definition. This is done by searching and listing the properties used to distinguish examples from non-examples of various concepts. Our work attempts to apply the Concept Attainment Model to build conceptual riddles, to deploy over online learning environments. The approach involves creating factual triples from learning resources, classifying them based on their uniqueness to a concept into `Topic Markers' and `Common', followed by generating riddles based on the Concept Attainment Model's format and capturing all possible solutions to those riddles. The results obtained from the human evaluation of riddles prove encouraging.

</details>


### [232] [Streaming Sequence Transduction through Dynamic Compression](https://arxiv.org/abs/2402.01172)

*Weiting Tan, Yunmo Chen, Tongfei Chen, Guanghui Qin, Haoran Xu, Heidi C. Zhang, Benjamin Van Durme, Philipp Koehn*

**Main category:** cs.CL

**Keywords:** STAR, Transformer, stream transduction, automatic speech recognition, speech-to-text

**Relevance Score:** 6

**TL;DR:** STAR is a Transformer-based model that efficiently performs sequence-to-sequence transduction for streams, achieving significant compression and improving ASR performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address efficiency in sequence-to-sequence tasks over streaming data, focusing on automatic speech recognition.

**Method:** Utilizes a Transformer architecture to create compressed anchor representations from dynamically segmented input streams.

**Key Contributions:**

	1. Introduction of STAR model for sequence transduction over streams
	2. Achieved significant compression in ASR
	3. Improved segmentation and quality optimization for simultaneous speech-to-text tasks

**Result:** Achieves nearly lossless compression (12x) in ASR and shows improved segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks.

**Limitations:** 

**Conclusion:** STAR optimizes performance metrics like latency and memory usage while maintaining high-quality outputs during speech-to-text conversion.

**Abstract:** We introduce STAR (Stream Transduction with Anchor Representations), a novel Transformer-based model designed for efficient sequence-to-sequence transduction over streams. STAR dynamically segments input streams to create compressed anchor representations, achieving nearly lossless compression (12x) in Automatic Speech Recognition (ASR) and outperforming existing methods. Moreover, STAR demonstrates superior segmentation and latency-quality trade-offs in simultaneous speech-to-text tasks, optimizing latency, memory footprint, and quality.

</details>


### [233] [Can We Verify Step by Step for Incorrect Answer Detection?](https://arxiv.org/abs/2402.10528)

*Xin Xu, Shizhe Diao, Can Yang, Yang Wang*

**Main category:** cs.CL

**Keywords:** chain-of-thought prompting, large language models, reasoning, benchmark, AI in health informatics

**Relevance Score:** 9

**TL;DR:** This paper introduces R2PE, a benchmark for predicting LLM output accuracy by analyzing reasoning chains, along with a novel framework that enhances performance in reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to fill the gap in understanding the correlation between reasoning chains generated by LLMs and their output accuracy.

**Method:** Introduction of the R2PE benchmark to evaluate reasoning chain performance across different domains, and development of the process discernibility score (PDS) framework to enhance reasoning task results.

**Key Contributions:**

	1. Introduction of R2PE benchmark for reasoning chain evaluation
	2. Development of the process discernibility score (PDS) framework
	3. Demonstration of improved performance metrics in reasoning tasks

**Result:** The PDS framework showed a notable increase in F1 score (5.1%) and improvement in AUC-PR (2.97%) compared to the existing answer-checking baseline.

**Limitations:** 

**Conclusion:** The findings highlight the importance of reasoning chains in predicting LLM performance, with the PDS framework proving effective in enhancing QA accuracy.

**Abstract:** Chain-of-Thought (CoT) prompting has marked a significant advancement in enhancing the reasoning capabilities of large language models (LLMs). Previous studies have developed various extensions of CoT, which focus primarily on enhancing end-task performance. In addition, there has been research on assessing the quality of reasoning chains in CoT. This raises an intriguing question: Is it possible to predict the accuracy of LLM outputs by scrutinizing the reasoning chains they generate? To answer this research question, we introduce a benchmark, R2PE, designed specifically to explore the relationship between reasoning chains and performance in various reasoning tasks spanning five different domains. This benchmark aims to measure the falsehood of the final output of LLMs based on the reasoning steps. To make full use of information in multiple reasoning chains, we propose the process discernibility score (PDS) framework that beats the answer-checking baseline by a large margin. Concretely, this resulted in an average of $5.1\%$ increase in the F1 score and $2.97\%$ improvement in AUC-PR across all 45 subsets within R2PE. We further demonstrate our PDS's efficacy in advancing open-domain QA accuracy.

</details>


### [234] [FormulaReasoning: A Dataset for Formula-Based Numerical Reasoning](https://arxiv.org/abs/2402.12692)

*Xiao Li, Bolin Zhu, Kaiwen Shi, Sichen Liu, Yin Zhu, Yiwei Liu, Gong Cheng*

**Main category:** cs.CL

**Keywords:** numerical reasoning, language models, physics formulas, dataset, HCI

**Relevance Score:** 6

**TL;DR:** Introduction of the FormulaReasoning dataset, designed for formula-based numerical reasoning, challenging LLMs with physics formula-based questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a dataset that explicitly indicates the formulas used in numerical reasoning, addressing the limitation of existing datasets that rely on implicit knowledge.

**Method:** Development of the FormulaReasoning dataset with 4,751 formula-based questions and fine-grained annotations for enhanced reasoning tasks. Evaluation of LLMs and exploration of retrieval-augmented generation with a formula database.

**Key Contributions:**

	1. Introduction of a comprehensive dataset for formula-based numerical reasoning.
	2. Fine-grained annotations for questions that include structures and symbols.
	3. Evaluation framework for LLMs using a large-scale formula database.

**Result:** The dataset allows for more challenging evaluations of LLMs and facilitates the breakdown of reasoning into formula generation, parameter extraction, and numerical calculation.

**Limitations:** 

**Conclusion:** FormulaReasoning provides a structured approach to evaluating machine learning models in solving numerical reasoning problems using physics formulas, combined with extensive annotation and a supporting formula database.

**Abstract:** The application of formulas (e.g., physics formulas) is a fundamental ability of humans when solving numerical reasoning problems. Existing numerical reasoning datasets seldom explicitly indicate the formulas employed in reasoning, as their questions rely on implicit commonsense mathematical knowledge. In contrast, in this paper, we introduce FormulaReasoning, a new dataset specifically designed for formula-based numerical reasoning. Each of the 4,751 questions in our dataset requires numerical calculation with external physics formulas, making it a more challenging benchmark for evaluating large language models (LLMs). We offer normalized fine-grained annotations for the questions, available in English and Chinese, including formula structures, parameter names, symbols, numerical values, and units, derived from extensive manual effort with LLM assistance for guaranteed quality. We also provide a consolidated formula database to serve as an external knowledge base accompanying the dataset. We employ FormulaReasoning to evaluate LLMs with 7B to over 100B parameters, and explore retrieval-augmented generation with the formula database. Our evaluation also covers supervised methods that break down the reasoning process into formula generation, parameter extraction, and numerical calculation, as well as direct preference optimization methods based on derived preference data.

</details>


### [235] [Comparing Specialised Small and General Large Language Models on Text Classification: 100 Labelled Samples to Achieve Break-Even Performance](https://arxiv.org/abs/2402.12819)

*Branislav Pecher, Ivan Srba, Maria Bielikova*

**Main category:** cs.CL

**Keywords:** NLP, large language models, performance variance, fine-tuning, text classification

**Relevance Score:** 8

**TL;DR:** The paper investigates the number of labeled samples needed for specialized small models to outperform general large language models in NLP tasks, revealing that only a few samples are often required, but this depends on task characteristics and introduces performance variance considerations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine how many labeled samples are necessary for specialized small models to surpass general large models in NLP tasks, particularly in the context of limited labeled data.

**Method:** The study evaluates the performance of fine-tuning, instruction-tuning, prompting, and in-context learning across 8 language models and various text classification tasks to identify performance break-even points.

**Key Contributions:**

	1. Identification of performance break-even points for models with limited labeled data.
	2. Insights into the dependence of required labeled samples on dataset characteristics.
	3. Assessment of model size impact on performance variance.

**Result:** Specialized models can match or exceed general models' performance with as few as 100 labeled samples, though this number can vary widely based on task characteristics, especially in binary datasets.

**Limitations:** Lack of exploration of specific characteristics of datasets that may influence sample requirements.

**Conclusion:** The number of required labeled samples is task-dependent, significantly increasing when accounting for performance variance; larger models do not always guarantee better outcomes, and quantization impacts are minimal.

**Abstract:** When solving NLP tasks with limited labelled data, researchers typically either use a general large language model without further update, or use a small number of labelled samples to tune a specialised smaller model. In this work, we answer an important question -- how many labelled samples are required for the specialised small models to outperform general large models, while taking the performance variance into consideration. By observing the behaviour of fine-tuning, instruction-tuning, prompting and in-context learning on 8 language models, we identify such performance break-even points across 8 representative text classification tasks of varying characteristics. We show that the specialised models often need only few samples (on average $100$) to be on par or better than the general ones. At the same time, the number of required labels strongly depends on the dataset or task characteristics, with fine-tuning on binary datasets requiring significantly more samples. When performance variance is taken into consideration, the number of required labels increases on average by $100 - 200\%$. Finally, larger models do not consistently lead to better performance and lower variance, with 4-bit quantisation having negligible impact.

</details>


### [236] [From Languages to Geographies: Towards Evaluating Cultural Bias in Hate Speech Datasets](https://arxiv.org/abs/2404.17874)

*Manuel Tonneau, Diyi Liu, Samuel Fraiberger, Ralph Schroeder, Scott A. Hale, Paul Röttger*

**Main category:** cs.CL

**Keywords:** hate speech, cultural bias, social media, datasets, language

**Relevance Score:** 6

**TL;DR:** This paper evaluates cultural bias in hate speech datasets by analyzing language and geography and finds that existing datasets are significantly skewed towards specific countries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address and analyze the cultural biases present in hate speech datasets which are predominantly created in English, potentially overlooking cultural contexts of other languages and regions.

**Method:** The authors conducted a systematic survey of hate speech datasets in eight languages, examining their geographical metadata to assess geo-cultural biases by correlating language with country information.

**Key Contributions:**

	1. Identification of cultural biases in hate speech datasets across multiple languages.
	2. Demonstration of decreasing bias in English datasets over time.
	3. Recommendations for the development of more representative hate speech datasets.

**Result:** The study confirmed that English-language hate speech datasets exhibit bias, which has decreased over recent years. It also found that datasets in English, Arabic, and Spanish are heavily biased towards a few countries, notably the US and UK for English, despite a more diverse social media and speaker population.

**Limitations:** The study is limited to eight languages and may not reflect trends in all languages or cultures. The findings are based on available social media data, which may not capture the complexity of hate speech across different contexts.

**Conclusion:** The findings underline the need for more balanced and representative hate speech datasets that reflect the socio-cultural realities of various regions and languages.

**Abstract:** Perceptions of hate can vary greatly across cultural contexts. Hate speech (HS) datasets, however, have traditionally been developed by language. This hides potential cultural biases, as one language may be spoken in different countries home to different cultures. In this work, we evaluate cultural bias in HS datasets by leveraging two interrelated cultural proxies: language and geography. We conduct a systematic survey of HS datasets in eight languages and confirm past findings on their English-language bias, but also show that this bias has been steadily decreasing in the past few years. For three geographically-widespread languages -- English, Arabic and Spanish -- we then leverage geographical metadata from tweets to approximate geo-cultural contexts by pairing language and country information. We find that HS datasets for these languages exhibit a strong geo-cultural bias, largely overrepresenting a handful of countries (e.g., US and UK for English) relative to their prominence in both the broader social media population and the general population speaking these languages. Based on these findings, we formulate recommendations for the creation of future HS datasets.

</details>


### [237] [Sparse Matrix in Large Language Model Fine-tuning](https://arxiv.org/abs/2405.15525)

*Haoze He, Juncheng Billy Li, Xuan Jiang, Heather Miller*

**Main category:** cs.CL

**Keywords:** Sparse Matrix Tuning, parameter-efficient fine-tuning, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces Sparse Matrix Tuning (SMT), a method to enhance parameter-efficient fine-tuning (PEFT) of large language models while reducing resource costs and minimizing accuracy gaps compared to full fine-tuning.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the accuracy gap between PEFT methods like LoRA and DoRA and full fine-tuning, which leads to high computational costs.

**Method:** The SMT method selects significant sub-matrices in the gradient update and updates only those during fine-tuning, optimizing performance and resource utilization.

**Key Contributions:**

	1. Introduction of the Sparse Matrix Tuning (SMT) method for PEFT.
	2. Demonstrated significant performance improvement over existing PEFT methods.
	3. Reduction of GPU memory footprint in fine-tuning large language models.

**Result:** Experimental results show that SMT outperforms other PEFT methods, achieving better performance on various tasks while reducing GPU memory usage by 67% compared to full fine-tuning.

**Limitations:** 

**Conclusion:** SMT provides a more efficient fine-tuning approach without suffering performance degradation, even as trainable parameters increase, unlike existing PEFT methods.

**Abstract:** LoRA and its variants have become popular parameter-efficient fine-tuning (PEFT) methods due to their ability to avoid excessive computational costs. However, an accuracy gap often exists between PEFT methods and full fine-tuning (FT), and this gap has yet to be systematically studied. In this work, we introduce a method for selecting sparse sub-matrices that aim to minimize the performance gap between PEFT vs. full fine-tuning (FT) while also reducing both fine-tuning computational cost and memory cost. Our Sparse Matrix Tuning (SMT) method begins by identifying the most significant sub-matrices in the gradient update, updating only these blocks during the fine-tuning process. In our experiments, we demonstrate that SMT consistently surpasses other PEFT baseline (e.g. LoRA and DoRA) in fine-tuning popular large language models such as LLaMA across a broad spectrum of tasks, while reducing the GPU memory footprint by 67% compared to FT. We also examine how the performance of LoRA and DoRA tends to plateau and decline as the number of trainable parameters increases, in contrast, our SMT method does not suffer from such issue.

</details>


### [238] [OR-Bench: An Over-Refusal Benchmark for Large Language Models](https://arxiv.org/abs/2405.20947)

*Justin Cui, Wei-Lin Chiang, Ion Stoica, Cho-Jui Hsieh*

**Main category:** cs.CL

**Keywords:** Large Language Models, over-refusal, safety alignment, dataset generation, benchmark

**Relevance Score:** 8

**TL;DR:** This study introduces OR-Bench, a large-scale benchmark for measuring over-refusal in Large Language Models, proposing a novel method for generating prompts that elicit refusal behaviors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of over-refusal in LLMs which leads to rejection of benign prompts, impeding their helpfulness.

**Method:** We propose a novel approach to automatically generate datasets that include over-refusal prompts, creating the OR-Bench benchmark with 80,000 prompts across multiple categories.

**Key Contributions:**

	1. Introduction of OR-Bench, the first large-scale over-refusal dataset
	2. Detailed analysis of over-refusal behaviors of 32 LLMs
	3. Public availability of the dataset and codebase for community use

**Result:** We measure over-refusal across 32 popular LLMs, highlighting the extent of the issue and providing a valuable resource for future research on safety alignment in LLMs.

**Limitations:** 

**Conclusion:** The OR-Bench benchmark aims to assist in the development of LLMs that are better aligned with safety practices while minimizing over-refusal.

**Abstract:** Large Language Models (LLMs) require careful safety alignment to prevent malicious outputs. While significant research focuses on mitigating harmful content generation, the enhanced safety often come with the side effect of over-refusal, where LLMs may reject innocuous prompts and become less helpful. Although the issue of over-refusal has been empirically observed, a systematic measurement is challenging due to the difficulty of crafting prompts that can elicit the over-refusal behaviors of LLMs. This study proposes a novel method for automatically generating large-scale over-refusal datasets. Leveraging this technique, we introduce OR-Bench, the first large-scale over-refusal benchmark. OR-Bench comprises 80,000 over-refusal prompts across 10 common rejection categories, a subset of around 1,000 hard prompts that are challenging even for state-of-the-art LLMs, and an additional 600 toxic prompts to prevent indiscriminate responses. We then conduct a comprehensive study to measure the over-refusal of 32 popular LLMs across 8 model families. Our datasets are publicly available at https://huggingface.co/bench-llms and our codebase is open-sourced at https://github.com/justincui03/or-bench. We hope this benchmark can help the community develop better safety aligned models.

</details>


### [239] [ShareLoRA: Parameter Efficient and Robust Large Language Model Fine-tuning via Shared Low-Rank Adaptation](https://arxiv.org/abs/2406.10785)

*Yurun Song, Junchen Zhao, Ian G. Harris, Sangeetha Abdu Jyothi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Low-rank adaptation

**Relevance Score:** 8

**TL;DR:** ShareLoRA is a fine-tuning technique for Large Language Models that enhances parameter efficiency and robustness by sharing low-rank weight matrices across layers, resulting in significant reductions in trainable parameters and memory overhead without compromising performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and adaptability of fine-tuning techniques for Large Language Models (LLMs) while maintaining performance, especially in resource-constrained environments.

**Method:** ShareLoRA shares low-rank weight matrices across different layers of LLMs, leading to reduced trainable parameters and memory usage.

**Key Contributions:**

	1. Introduction of ShareLoRA as a new LLM fine-tuning technique.
	2. Significant reduction in trainable parameters and memory requirements compared to LoRA.
	3. Demonstrated robustness and improved accuracy across multiple models and tasks.

**Result:** ShareLoRA achieves 44% to 96% reduction in trainable parameters and exhibits improved accuracy in zero-shot, few-shot, and continual learning settings, outperforming standard LoRA with an average accuracy improvement of up to 1.2%.

**Limitations:** 

**Conclusion:** ShareLoRA facilitates high-quality fine-tuning with strong generalization and continual adaptation across various tasks and model sizes, making it beneficial for diverse applications in LLMs.

**Abstract:** In this paper, we introduce \textbf{Share}d \textbf{Lo}w \textbf{R}ank \textbf{A}daptation (ShareLoRA), a Large Language Model (LLM) fine-tuning technique that balances parameter efficiency, adaptability, and robustness without compromising performance. By strategically sharing the low-rank weight matrices across different layers, ShareLoRA achieves 44\% to 96\% reduction in trainable parameters compared to standard LoRA, alongside a substantial decrease in memory overhead. This efficiency gain scales with model size, making ShareLoRA particularly advantageous for resource-constrained environments. Importantly, ShareLoRA not only maintains model performance but also exhibits robustness in both classification and generation tasks across diverse models, including RoBERTa, GPT-2, and LLaMA series (1, 2, and 3). It consistently outperforms LoRA in zero-shot, few-shot, and continual fine-tuning scenarios, achieving up to 1.2\% average accuracy improvement, and enhanced generalization across domains. In continual learning settings, ShareLoRA achieves 1.2\% higher accuracy on GSM8K, 0.6\% on HumanEval, and 0.5\% on both MMLU and MMLU-Pro. Our results demonstrate that ShareLoRA supports high-quality fine-tuning while offering strong generalization and continual adaptation across various model scales and diverse tasks.

</details>


### [240] [Pruning via Merging: Compressing LLMs via Manifold Alignment Based Layer Merging](https://arxiv.org/abs/2406.16330)

*Deyuan Liu, Zhanyue Qin, Hairu Wang, Zhao Yang, Zecheng Wang, Fangying Rong, Qingbin Liu, Yanchao Hao, Xi Chen, Cunhang Fan, Zhao Lv, Zhiying Tu, Dianhui Chu, Bo Li, Dianbo Sui*

**Main category:** cs.CL

**Keywords:** large language models, model compression, manifold learning

**Relevance Score:** 9

**TL;DR:** MKA introduces a new compression technique for large language models that preserves performance while significantly reducing size.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the deployment of LLMs in resource-limited environments by addressing the inefficiencies of current compression techniques.

**Method:** The proposed Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA) uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) to merge similar layers of neural networks.

**Key Contributions:**

	1. Introduction of MKA for merging similar layers in LLMs
	2. Substantial compression ratios achieved
	3. Performance preservation even with high compression

**Result:** MKA outperforms traditional pruning methods, achieving a compression ratio of 43.75% on the Llama3-8B model with minimal performance loss.

**Limitations:** 

**Conclusion:** MKA is an effective model compression approach for LLMs that maintains performance while offering substantial reduction in model size.

**Abstract:** While large language models (LLMs) excel in many domains, their complexity and scale challenge deployment in resource-limited environments. Current compression techniques, such as parameter pruning, often fail to effectively utilize the knowledge from pruned parameters. To address these challenges, we propose Manifold-Based Knowledge Alignment and Layer Merging Compression (MKA), a novel approach that uses manifold learning and the Normalized Pairwise Information Bottleneck (NPIB) measure to merge similar layers, reducing model size while preserving essential performance. We evaluate MKA on multiple benchmark datasets and various LLMs. Our findings show that MKA not only preserves model performance but also achieves substantial compression ratios, outperforming traditional pruning methods. Moreover, when coupled with quantization, MKA delivers even greater compression. Specifically, on the MMLU dataset using the Llama3-8B model, MKA achieves a compression ratio of 43.75% with a minimal performance decrease of only 2.82\%. The proposed MKA method offers a resource-efficient and performance-preserving model compression technique for LLMs.

</details>


### [241] [Brittle Minds, Fixable Activations: Understanding Belief Representations in Language Models](https://arxiv.org/abs/2406.17513)

*Matteo Bortoletto, Constantin Ruhdorfer, Lei Shi, Andreas Bulling*

**Main category:** cs.CL

**Keywords:** Theory of Mind, language models, mental state representation, model alignment, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper investigates how language models (LMs) represent mental states (Theory of Mind) and finds that larger models with fine-tuning have structured belief representations but are sensitive to prompt variations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to understand internal mechanisms of LMs in representing mental states, which is essential for improving model alignment and ensuring safety due to potential misinterpretations of mental states in generated outputs.

**Method:** The authors systematically probe LMs of different sizes and training regimens using control tasks to investigate belief representations, ruling out confounds.

**Key Contributions:**

	1. First systematic investigation of belief representations in language models
	2. Evidence that model fine-tuning and size enhance representations of beliefs
	3. Demonstration that targeted edits can correct ToM inferences

**Result:** Findings indicate that model size and fine-tuning enhance the internal representations of others' beliefs, which are fundamentally structured rather than results of random correlations, although they are fragile against changes in prompts.

**Limitations:** The representations are brittle to prompt variations, which may affect practical utility.

**Conclusion:** The study concludes that targeted edits to model activations can strengthen the representations and correct inaccurate Theory of Mind inferences.

**Abstract:** Despite growing interest in Theory of Mind (ToM) tasks for evaluating language models (LMs), little is known about how LMs internally represent mental states of self and others. Understanding these internal mechanisms is critical - not only to move beyond surface-level performance, but also for model alignment and safety, where subtle misattributions of mental states may go undetected in generated outputs. In this work, we present the first systematic investigation of belief representations in LMs by probing models across different scales, training regimens, and prompts - using control tasks to rule out confounds. Our experiments provide evidence that both model size and fine-tuning substantially improve LMs' internal representations of others' beliefs, which are structured - not mere by-products of spurious correlations - yet brittle to prompt variations. Crucially, we show that these representations can be strengthened: targeted edits to model activations can correct wrong ToM inferences.

</details>


### [242] [DiffuseDef: Improved Robustness to Adversarial Attacks via Iterative Denoising](https://arxiv.org/abs/2407.00248)

*Zhenhao Li, Huichi Zhou, Marek Rei, Lucia Specia*

**Main category:** cs.CL

**Keywords:** adversarial attacks, language classification, diffusion models, neural networks, defense mechanisms

**Relevance Score:** 8

**TL;DR:** DiffuseDef is a novel adversarial defense method for language classification that uses a diffusion layer to improve robustness against adversarial attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the critical challenge of adversarial attacks on pretrained language models, highlighting the need for effective defense mechanisms in natural language processing tasks.

**Method:** DiffuseDef incorporates a diffusion layer as a denoiser between the encoder and classifier, trained atop existing classifiers for seamless model integration. It combines adversarial training, denoising, and ensembling techniques to enhance defense capabilities.

**Key Contributions:**

	1. Introduces DiffuseDef, a novel adversarial defense method incorporating diffusion models in NLP.
	2. Demonstrates state-of-the-art defense performance against various adversarial attacks.
	3. Provides a plug-and-play solution applicable to existing classifiers.

**Result:** DiffuseDef demonstrates state-of-the-art performance in defending against both black-box and white-box adversarial attacks, surpassing existing defense methods.

**Limitations:** 

**Conclusion:** The proposed approach effectively integrates advanced denoising techniques to improve the resilience of language classification models against adversarial disruptions.

**Abstract:** Pretrained language models have significantly advanced performance across various natural language processing tasks. However, adversarial attacks continue to pose a critical challenge to systems built using these models, as they can be exploited with carefully crafted adversarial texts. Inspired by the ability of diffusion models to predict and reduce noise in computer vision, we propose a novel and flexible adversarial defense method for language classification tasks, DiffuseDef, which incorporates a diffusion layer as a denoiser between the encoder and the classifier. The diffusion layer is trained on top of the existing classifier, ensuring seamless integration with any model in a plug-and-play manner. During inference, the adversarial hidden state is first combined with sampled noise, then denoised iteratively and finally ensembled to produce a robust text representation. By integrating adversarial training, denoising, and ensembling techniques, we show that DiffuseDef improves over existing adversarial defense methods and achieves state-of-the-art performance against common black-box and white-box adversarial attacks.

</details>


### [243] [A Bounding Box is Worth One Token: Interleaving Layout and Text in a Large Language Model for Document Understanding](https://arxiv.org/abs/2407.01976)

*Jinghui Lu, Haiyang Yu, Yanjie Wang, Yongjie Ye, Jingqun Tang, Ziwei Yang, Binghong Wu, Qi Liu, Hao Feng, Han Wang, Hao Liu, Can Huang*

**Main category:** cs.CL

**Keywords:** Document Understanding, Large Language Models, Key Information Extraction, Visual Question Answering, OCR

**Relevance Score:** 8

**TL;DR:** This paper introduces LayTextLLM, a method that interleaves layout and text in LLMs for improved document understanding tasks such as Key Information Extraction (KIE) and Visual Question Answering (VQA).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance document understanding by overcoming limitations in existing methods that integrate spatial layouts with text in LLMs.

**Method:** LayTextLLM projects each bounding box to a single embedding and interleaves it with text, allowing for efficient performance without producing overly long text sequences.

**Key Contributions:**

	1. Introduction of LayTextLLM for better integration of layout and text.
	2. Proven performance improvements on document understanding tasks over SOTA methods.
	3. All resources made publicly available for further research.

**Result:** LayTextLLM shows a 15.2% improvement on KIE tasks and a 10.7% improvement on VQA tasks compared to the previous state-of-the-art OCR-based LLMs.

**Limitations:** 

**Conclusion:** LayTextLLM effectively streamlines the interaction between layout and textual data while achieving significant performance enhancements in relevant benchmarks.

**Abstract:** Recently, many studies have demonstrated that exclusively incorporating OCR-derived text and spatial layouts with large language models (LLMs) can be highly effective for document understanding tasks. However, existing methods that integrate spatial layouts with text have limitations, such as producing overly long text sequences or failing to fully leverage the autoregressive traits of LLMs. In this work, we introduce Interleaving Layout and Text in a Large Language Model (LayTextLLM)} for document understanding. LayTextLLM projects each bounding box to a single embedding and interleaves it with text, efficiently avoiding long sequence issues while leveraging autoregressive traits of LLMs. LayTextLLM not only streamlines the interaction of layout and textual data but also shows enhanced performance in KIE and VQA. Comprehensive benchmark evaluations reveal significant improvements of LayTextLLM, with a 15.2% increase on KIE tasks and 10.7% on VQA tasks compared to previous SOTA OCR-based LLMs. All resources are available at https://github.com/LayTextLLM/LayTextLLM.

</details>


### [244] [ClinicRealm: Re-evaluating Large Language Models with Conventional Machine Learning for Non-Generative Clinical Prediction Tasks](https://arxiv.org/abs/2407.18525)

*Yinghao Zhu, Junyi Gao, Zixiang Wang, Weibin Liao, Xiaochen Zheng, Lifang Liang, Miguel O. Bernabeu, Yasha Wang, Lequan Yu, Chengwei Pan, Ewen M. Harrison, Liantao Ma*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Prediction, Health Informatics, Machine Learning, Benchmarking

**Relevance Score:** 9

**TL;DR:** This study benchmarks various GPT-based, BERT-based, and traditional models for clinical prediction tasks, revealing that leading LLMs outperform fine-tuned models in unstructured clinical notes and perform competitively on structured EHRs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate the performance of large language models (LLMs) in clinical prediction and address the ongoing debate on their utility compared to specialized models.

**Method:** The study benchmarks 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods using unstructured clinical notes and structured Electronic Health Records (EHR).

**Key Contributions:**

	1. Benchmarking LLMs against traditional models in clinical prediction.
	2. Discovery of strong zero-shot capabilities of LLMs on unstructured and structured data.
	3. Insights into the performance of open-source versus proprietary LLMs in medical applications.

**Result:** The study finds that LLMs like DeepSeek R1/V3 and GPT o3-mini-high significantly outperform fine-tuned BERT models in zero-shot settings and show strong zero-shot capabilities on structured EHRs, often exceeding conventional models in data-scarce settings.

**Limitations:** 

**Conclusion:** Modern LLMs are established as powerful tools for non-generative clinical prediction, which necessitates a re-evaluation of model selection strategies in healthcare contexts, particularly for unstructured text.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in medicine. However, their utility in non-generative clinical prediction, often presumed inferior to specialized models, remains under-evaluated, leading to ongoing debate within the field and potential for misuse, misunderstanding, or over-reliance due to a lack of systematic benchmarking. Our ClinicRealm study addresses this by benchmarking 9 GPT-based LLMs, 5 BERT-based models, and 7 traditional methods on unstructured clinical notes and structured Electronic Health Records (EHR). Key findings reveal a significant shift: for clinical note predictions, leading LLMs (e.g., DeepSeek R1/V3, GPT o3-mini-high) in zero-shot settings now decisively outperform finetuned BERT models. On structured EHRs, while specialized models excel with ample data, advanced LLMs (e.g., GPT-4o, DeepSeek R1/V3) show potent zero-shot capabilities, often surpassing conventional models in data-scarce settings. Notably, leading open-source LLMs can match or exceed proprietary counterparts. These results establish modern LLMs as powerful non-generative clinical prediction tools, particularly with unstructured text and offering data-efficient structured data options, thus necessitating a re-evaluation of model selection strategies. This research should serve as an important insight for medical informaticists, AI developers, and clinical researchers, potentially prompting a reassessment of current assumptions and inspiring new approaches to LLM application in predictive healthcare.

</details>


### [245] [SWIFT:A Scalable lightWeight Infrastructure for Fine-Tuning](https://arxiv.org/abs/2408.05517)

*Yuze Zhao, Jintao Huang, Jinghan Hu, Xingjun Wang, Yunlin Mao, Daoze Zhang, Hong Zhang, Zeyinzi Jiang, Zhikai Wu, Baole Ai, Ang Wang, Wenmeng Zhou, Yingda Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-modal Models, Fine-tuning, SWIFT, Machine Learning

**Relevance Score:** 9

**TL;DR:** SWIFT is an open-source infrastructure for fine-tuning large language models (LLMs) and multi-modal large language models (MLLMs), providing comprehensive support and systematic training techniques that improve model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overwhelming needs for effective training and fine-tuning of LLMs and MLLMs, which have demonstrated superior capabilities in various text and multi-modal tasks.

**Method:** The paper presents SWIFT, a customizable framework designed for fine-tuning over 300 LLMs and 50 MLLMs, integrating post-training processes such as inference, evaluation, and model quantization, along with benchmark comparisons among training techniques.

**Key Contributions:**

	1. Development of a customizable infrastructure for fine-tuning LLMs and MLLMs.
	2. Integration of comprehensive post-training processes to enhance the usability of large models.
	3. Significant performance enhancements on benchmark tasks using SWIFT.

**Result:** Training specialized models on SWIFT leads to significant performance improvements on the ToolBench leaderboard, with improvements ranging from 5.2%-21.8% in the Act.EM metric, a 1.6%-14.1% reduction in hallucinations, and an overall performance increase of 8%-17%.

**Limitations:** 

**Conclusion:** SWIFT is the first training framework to systematically support MLLMs and facilitates the fast adoption of large models in various applications.

**Abstract:** Recent development in Large Language Models (LLMs) and Multi-modal Large Language Models (MLLMs) have leverage Attention-based Transformer architectures and achieved superior performance and generalization capabilities. They have since covered extensive areas of traditional learning tasks. For instance, text-based tasks such as text-classification and sequence-labeling, as well as multi-modal tasks like Visual Question Answering (VQA) and Optical Character Recognition (OCR), which were previously addressed using different models, can now be tackled based on one foundation model. Consequently, the training and lightweight fine-tuning of LLMs and MLLMs, especially those based on Transformer architecture, has become particularly important. In recognition of these overwhelming needs, we develop SWIFT, a customizable one-stop infrastructure for large models. With support of over $300+$ LLMs and $50+$ MLLMs, SWIFT stands as the open-source framework that provide the most comprehensive support for fine-tuning large models. In particular, it is the first training framework that provides systematic support for MLLMs. In addition to the core functionalities of fine-tuning, SWIFT also integrates post-training processes such as inference, evaluation, and model quantization, to facilitate fast adoptions of large models in various application scenarios. With a systematic integration of various training techniques, SWIFT offers helpful utilities such as benchmark comparisons among different training techniques for large models. For fine-tuning models specialized in agent framework, we show that notable improvements on the ToolBench leader-board can be achieved by training with customized dataset on SWIFT, with an increase of 5.2%-21.8% in the Act.EM metric over various baseline models, a reduction in hallucination by 1.6%-14.1%, and an average performance improvement of 8%-17%.

</details>


### [246] [Turning Trash into Treasure: Accelerating Inference of Large Language Models with Token Recycling](https://arxiv.org/abs/2408.08696)

*Xianzhen Luo, Yixuan Wang, Qingfu Zhu, Zhiming Zhang, Xuanyu Zhang, Qing Yang, Dongliang Xu*

**Main category:** cs.CL

**Keywords:** Token Recycling, Large Language Models, Inference Latency, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper introduces Token Recycling, a lossless approach to accelerate inference in large language models (LLMs) by storing and reusing candidate tokens, achieving significant speed improvements without additional training requirements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Inference latency in large language models is a major bottleneck due to their massive parameters, and existing methods either require additional training or have limitations in speed and adaptability.

**Method:** Token Recycling utilizes an adjacency matrix to store candidate tokens and updates them using a breadth-first-search algorithm to construct a draft tree, which is validated through tree attention.

**Key Contributions:**

	1. Introduction of Token Recycling for LLM inference acceleration
	2. Achieves significant speedup without extra training
	3. Requires minimal additional storage

**Result:** Token Recycling achieves approximately 2x speedup across all sizes of LLMs and outperforms existing train-free methods by 30% and a popular training method by 25%.

**Limitations:** 

**Conclusion:** The proposed method significantly improves inference speed while requiring less than 2MB of additional storage and no pre-training.

**Abstract:** Massive parameters of LLMs have made inference latency a fundamental bottleneck. Speculative decoding represents a lossless approach to accelerate inference through a guess-and-verify paradigm. Some methods rely on additional architectures to guess draft tokens, which need extra training before use. Alternatively, retrieval-based training-free techniques build libraries from pre-existing corpora or by n-gram generation. However, they face challenges like large storage requirements, time-consuming retrieval, and limited adaptability. Observing that candidate tokens generated during the decoding process are likely to reoccur in future sequences, we propose Token Recycling. It stores candidate tokens in an adjacency matrix and employs a breadth-first-search (BFS)-like algorithm to construct a draft tree, which is then validated through tree attention. New candidate tokens from the decoding process are then used to update the matrix. Token Recycling requires \textless2MB of additional storage and achieves approximately 2x speedup across all sizes of LLMs. It significantly outperforms existing train-free methods by 30\% and even a widely recognized training method by 25\%.

</details>


### [247] [Large Language Models Might Not Care What You Are Saying: Prompt Format Beats Descriptions](https://arxiv.org/abs/2408.08780)

*Chenming Tang, Zhixiang Wang, Hao Sun, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, ensemble prompt framework, machine translation, prompt design

**Relevance Score:** 8

**TL;DR:** This paper introduces an ensemble prompt framework that enhances in-context learning performance in large language models (LLMs) by focusing on format rather than specific content in prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the role of descriptive instructions in in-context learning (ICL) for large language models, which has been overlooked in previous research.

**Method:** The authors propose an ensemble prompt framework that uses multiple in-context examples with varying descriptive criteria and conduct experiments on machine translation and other reasoning tasks to evaluate performance.

**Key Contributions:**

	1. Introduction of an ensemble prompt framework for ICL
	2. Demonstration of performance improvements in various tasks
	3. Evidence that format outweighs specific content in prompts

**Result:** Preliminary experiments show that the proposed ensemble framework improves performance in machine translation across six directions and other tasks, indicating that the format of prompts is more critical than the actual descriptive content.

**Limitations:** The study primarily investigates LLMs in synthetic tasks and may not generalize across all types of language tasks.

**Conclusion:** Properly designing the ensemble prompt format is more efficient for ICL performance than focusing on the specifics of prompt descriptions.

**Abstract:** With the help of in-context learning (ICL), large language models (LLMs) have achieved impressive performance across various tasks. However, the function of descriptive instructions during ICL remains under-explored. In this work, we propose an ensemble prompt framework to describe the selection criteria of multiple in-context examples, and preliminary experiments on machine translation (MT) across six translation directions confirm that this framework boosts ICL performance. But to our surprise, LLMs might not care what the descriptions actually say, and the performance gain is primarily caused by the ensemble format, since it could lead to improvement even with random descriptive nouns. We further apply this new ensemble framework on a range of commonsense, math, logical reasoning and hallucination tasks with three LLMs and achieve promising results, suggesting again that designing a proper prompt format would be much more effective and efficient than paying effort into specific descriptions. Our code will be publicly available once this paper is published.

</details>


### [248] [LLMs are not Zero-Shot Reasoners for Biomedical Information Extraction](https://arxiv.org/abs/2408.12249)

*Aishik Nagar, Viktor Schlegel, Thanh-Tung Nguyen, Hao Li, Yuping Wu, Kuluhan Binici, Stefan Winkler*

**Main category:** cs.CL

**Keywords:** Large Language Models, Biomedical Domain, Medical Classification, Named Entity Recognition, Prompting Techniques

**Relevance Score:** 9

**TL;DR:** This paper benchmarks Large Language Models (LLMs) on Medical Classification and Named Entity Recognition (NER) tasks, revealing that standard prompting outperforms more complex techniques in the biomedical domain.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of LLMs in biomedical tasks like information extraction, which remain under-explored despite their success in question answering and summarization.

**Method:** The authors evaluated various open LLMs on diverse biomedical datasets using standard prompting, Chain of-Thought (CoT), Self Consistency reasoning, and Retrieval-Augmented Generation (RAG).

**Key Contributions:**

	1. Benchmarking LLMs in Medical Classification and NER tasks.
	2. Revealing that standard prompting surpasses complex methods in biomedical applications.
	3. Identifying limitations in current reasoning techniques for structured output tasks.

**Result:** The results showed that standard prompting consistently outperformed complex methods like CoT and RAG, indicating limitations in their applicability to structured biomedical tasks.

**Limitations:** The study highlights that advanced prompting methods are not easily transferable to biomedical tasks.

**Conclusion:** The study emphasizes the need for better integration of external knowledge and reasoning in LLMs to improve their effectiveness for real-world biomedical applications.

**Abstract:** Large Language Models (LLMs) are increasingly adopted for applications in healthcare, reaching the performance of domain experts on tasks such as question answering and document summarisation. Despite their success on these tasks, it is unclear how well LLMs perform on tasks that are traditionally pursued in the biomedical domain, such as structured information extraction. To bridge this gap, in this paper, we systematically benchmark LLM performance in Medical Classification and Named Entity Recognition (NER) tasks. We aim to disentangle the contribution of different factors to the performance, particularly the impact of LLMs' task knowledge and reasoning capabilities, their (parametric) domain knowledge, and addition of external knowledge. To this end, we evaluate various open LLMs - including BioMistral and Llama-2 models - on a diverse set of biomedical datasets, using standard prompting, Chain of-Thought (CoT) and Self Consistency based reasoning as well as Retrieval-Augmented Generation (RAG) with PubMed and Wikipedia corpora. Counter intuitively, our results reveal that standard prompting consistently outperforms more complex techniques across both tasks, laying bare the limitations in the current application of CoT, self-consistency and RAG in the biomedical domain. Our findings suggest that advanced prompting methods developed for knowledge- or reasoning-intensive tasks, such as CoT or RAG, are not easily portable to biomedical tasks where precise structured outputs are required. This highlights the need for more effective integration of external knowledge and reasoning mechanisms in LLMs to enhance their performance in real-world biomedical applications.

</details>


### [249] [What are the Essential Factors in Crafting Effective Long Context Multi-Hop Instruction Datasets? Insights and Best Practices](https://arxiv.org/abs/2409.01893)

*Zhi Chen, Qiguang Chen, Libo Qin, Qipeng Guo, Haijun Lv, Yicheng Zou, Wanxiang Che, Hang Yan, Kai Chen, Dahua Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Synthetic Data Generation, Multi-hop Question Generation

**Relevance Score:** 9

**TL;DR:** This paper proposes the Multi-agent Interactive Multi-hop Generation (MIMG) framework to enhance the quality of synthetic data for long context tasks in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of synthetic data used for enhancing long context capabilities in large language models, addressing the issues of low diversity and quality in current data generation methods.

**Method:** The MIMG framework includes various agents for quality verification, single-hop question generation, multiple question sampling, and multi-hop question merging, systematically experimenting with document selection and validation techniques.

**Key Contributions:**

	1. Introduction of the MIMG framework for synthetic data generation
	2. Demonstration of significantly improved data quality and model performance
	3. Systematic investigation of document selection and question merging strategies

**Result:** The MIMG framework improves the quality of synthetic data, achieving over 85% high-quality, multi-hop, and diverse data, leading to significant model performance enhancements that exceed models trained on larger human-annotated datasets.

**Limitations:** The study is limited to specific model architectures and may not generalize to all models or applications.

**Conclusion:** The findings suggest that high-quality synthetic long-context instruction data is crucial for improving model performance in long context tasks.

**Abstract:** Recent advancements in large language models (LLMs) with extended context windows have significantly improved tasks such as information extraction, question answering, and complex planning scenarios. In order to achieve success in long context tasks, a large amount of work has been done to enhance the long context capabilities of the model through synthetic data. Existing methods typically utilize the Self-Instruct framework to generate instruction tuning data for better long context capability improvement. However, our preliminary experiments indicate that less than 35% of generated samples are multi-hop, and more than 40% exhibit poor quality, limiting comprehensive understanding and further research. To improve the quality of synthetic data, we propose the Multi-agent Interactive Multi-hop Generation (MIMG) framework, incorporating a Quality Verification Agent, a Single-hop Question Generation Agent, a Multiple Question Sampling Strategy, and a Multi-hop Question Merger Agent. This framework improves the data quality, with the proportion of high-quality, multi-hop, and diverse data exceeding 85%. Furthermore, we systematically investigate strategies for document selection, question merging, and validation techniques through extensive experiments across various models. Our findings show that our synthetic high-quality long-context instruction data significantly enhances model performance, even surpassing models trained on larger amounts of human-annotated data. Our code is available at: https://github.com/WowCZ/LongMIT.

</details>


### [250] [Learning Efficient Recursive Numeral Systems via Reinforcement Learning](https://arxiv.org/abs/2409.07170)

*Andrea Silvi, Jonathan Thomas, Emil Carlsson, Devdatt Dubhashi, Moa Johansson*

**Main category:** cs.CL

**Keywords:** reinforcement learning, numeral systems, meta-grammar, efficient communication, Pareto-optimal

**Relevance Score:** 6

**TL;DR:** This paper explores how reinforcement learning (RL) agents can develop complex recursive numeral systems similar to human language through an adaptable meta-grammar for efficient communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To demonstrate the emergence of efficient recursive number systems via simple learning mechanisms like reinforcement learning, addressing the challenge identified in earlier research.

**Method:** Pairs of RL agents learn to communicate numerical quantities using a meta-grammar that evolves during their interactions, guided by the need for efficient communication.

**Key Contributions:**

	1. Mechanistic explanation of numeral system emergence via RL
	2. Demonstration of agent communication efficiency through meta-grammar
	3. Evolution of lexicon towards Pareto-optimal configurations mimicking human systems

**Result:** The RL agents successfully modify their lexicon to achieve Pareto-optimal configurations, resembling the efficiency of human numeral systems.

**Limitations:** 

**Conclusion:** The findings suggest that complex numeral systems can emerge from simple learning processes, providing insights into language development and communication efficiency.

**Abstract:** It has previously been shown that by using reinforcement learning (RL), agents can derive simple approximate and exact-restricted numeral systems that are similar to human ones (Carlsson, 2021). However, it is a major challenge to show how more complex recursive numeral systems, similar to for example English, could arise via a simple learning mechanism such as RL. Here, we introduce an approach towards deriving a mechanistic explanation of the emergence of efficient recursive number systems. We consider pairs of agents learning how to communicate about numerical quantities through a meta-grammar that can be gradually modified throughout the interactions. Utilising a slightly modified version of the meta-grammar of Hurford (1975), we demonstrate that our RL agents, shaped by the pressures for efficient communication, can effectively modify their lexicon towards Pareto-optimal configurations which are comparable to those observed within human numeral systems in terms of their efficiency.

</details>


### [251] [PACE: Abstractions for Communicating Efficiently](https://arxiv.org/abs/2409.20120)

*Jonathan D. Thomas, Andrea Silvi, Devdatt Dubhashi, Moa Johansson*

**Main category:** cs.CL

**Keywords:** neuro-symbolic, communication, reinforcement learning, abstraction, collaborative tasks

**Relevance Score:** 9

**TL;DR:** Introduces a neuro-symbolic method, PACE, for efficient communication in AI inspired by human collaborative problem-solving and abstraction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of how AI can effectively introduce and use abstractions in communication, similar to humans.

**Method:** PACE combines symbolic approaches from library learning with neural methods and reinforcement learning, applying bandit algorithms to manage abstraction introduction.

**Key Contributions:**

	1. Introduction of PACE as a neuro-symbolic method for AI communication
	2. Utilization of bandit algorithms for managing abstraction exploration
	3. Demonstration of emergence of efficient language through collaborative tasks

**Result:** PACE demonstrates human-like tendencies in communication efficiency during collaborative tasks and leads to the development of an efficient language among agents.

**Limitations:** 

**Conclusion:** The approach offers insights into human communicative behavior and paves the way for conversational agents to achieve human-like abstraction capabilities.

**Abstract:** A central but unresolved aspect of problem-solving in AI is the capability to introduce and use abstractions, something humans excel at. Work in cognitive science has demonstrated that humans tend towards higher levels of abstraction when engaged in collaborative task-oriented communication, enabling gradually shorter and more information-efficient utterances. Several computational methods have attempted to replicate this phenomenon, but all make unrealistic simplifying assumptions about how abstractions are introduced and learned. Our method, Procedural Abstractions for Communicating Efficiently (PACE), overcomes these limitations through a neuro-symbolic approach. On the symbolic side, we draw on work from library learning for proposing abstractions. We combine this with neural methods for communication and reinforcement learning, via a novel use of bandit algorithms for controlling the exploration and exploitation trade-off in introducing new abstractions. PACE exhibits similar tendencies to humans on a collaborative construction task from the cognitive science literature, where one agent (the architect) instructs the other (the builder) to reconstruct a scene of block-buildings. PACE results in the emergence of an efficient language as a by-product of collaborative communication. Beyond providing mechanistic insights into human communication, our work serves as a first step to providing conversational agents with the ability for human-like communicative abstractions.

</details>


### [252] [SSR: Alignment-Aware Modality Connector for Speech Language Models](https://arxiv.org/abs/2410.00168)

*Weiting Tan, Hirofumi Inaguma, Ning Dong, Paden Tomasello, Xutai Ma*

**Main category:** cs.CL

**Keywords:** Speech recognition, Language models, Fusing modalities, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** SSR-Connector improves modality fusion by segmenting and compressing speech features to align with text embeddings and employs a two-stage training process to prevent catastrophic forgetting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in fusing long-form speech with pre-trained language models and to avoid catastrophic forgetting of text modalities during fusion.

**Method:** The authors introduce the SSR-Connector, which segments and compresses speech features to better align with text embeddings, followed by a two-stage training pipeline consisting of distillation and fine-tuning.

**Key Contributions:**

	1. Introducing SSR-Connector for improved speech-text fusion
	2. Segmenting and compressing speech features for alignment with text
	3. Two-stage training process to mitigate catastrophic forgetting

**Result:** SSR-Connector significantly enhances speech understanding, achieving a +10 accuracy on StoryCloze and +20 on Speech-MMLU, while maintaining the competencies of pre-trained text embeddings.

**Limitations:** 

**Conclusion:** The proposed SSR-Connector surpasses existing speech-text modality fusion methods, demonstrating improvements in speech understanding alongside preservation of text capabilities.

**Abstract:** Fusing speech into pre-trained language model (SpeechLM) usually suffers from inefficient encoding of long-form speech and catastrophic forgetting of pre-trained text modality. We propose SSR-Connector (Segmented Speech Representation Connector) for better modality fusion. Leveraging speech-text alignments, our approach segments and compresses speech features to match the granularity of text embeddings. Additionally, we introduce a two-stage training pipeline that includes the distillation and fine-tuning phases to mitigate catastrophic forgetting. SSR-Connector outperforms existing mechanism for speech-text modality fusion, consistently achieving better speech understanding (e.g., +10 accuracy on StoryCloze and +20 on Speech-MMLU) while preserving pre-trained text ability.

</details>


### [253] [LLMs Know More Than They Show: On the Intrinsic Representation of LLM Hallucinations](https://arxiv.org/abs/2410.02707)

*Hadas Orgad, Michael Toker, Zorik Gekhman, Roi Reichart, Idan Szpektor, Hadas Kotek, Yonatan Belinkov*

**Main category:** cs.CL

**Keywords:** large language models, hallucinations, error detection, truthfulness encoding, internal representations

**Relevance Score:** 8

**TL;DR:** This paper explores how internal representations of large language models (LLMs) encode information about the truthfulness of their outputs and examines the implications for error detection and mitigation strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance the understanding of how LLMs produce errors, known as hallucinations, and to improve detection and mitigation strategies by analyzing internal states encoding truthfulness information.

**Method:** The authors investigate the concentration of truthfulness information in specific tokens within LLMs' internal representations and analyze their performance in error detection and prediction of error types.

**Key Contributions:**

	1. Identification of specific tokens that encode truthfulness information
	2. Development of tailored strategies for error mitigation
	3. Insights into the discrepancy between internal encoding and external output behavior

**Result:** The study demonstrates that truthfulness encoding is not universal but multifaceted, with error detectors being dataset-dependent. It also shows that internal representations can help predict likely error types and that there's often a mismatch between what is encoded and what is produced as output.

**Limitations:** Error detectors do not generalize well across datasets, suggesting limitations in the universal applicability of findings.

**Conclusion:** These findings provide valuable insights into the nature of LLM errors, highlighting areas for future research in error analysis and mitigation strategies.

**Abstract:** Large language models (LLMs) often produce errors, including factual inaccuracies, biases, and reasoning failures, collectively referred to as "hallucinations". Recent studies have demonstrated that LLMs' internal states encode information regarding the truthfulness of their outputs, and that this information can be utilized to detect errors. In this work, we show that the internal representations of LLMs encode much more information about truthfulness than previously recognized. We first discover that the truthfulness information is concentrated in specific tokens, and leveraging this property significantly enhances error detection performance. Yet, we show that such error detectors fail to generalize across datasets, implying that -- contrary to prior claims -- truthfulness encoding is not universal but rather multifaceted. Next, we show that internal representations can also be used for predicting the types of errors the model is likely to make, facilitating the development of tailored mitigation strategies. Lastly, we reveal a discrepancy between LLMs' internal encoding and external behavior: they may encode the correct answer, yet consistently generate an incorrect one. Taken together, these insights deepen our understanding of LLM errors from the model's internal perspective, which can guide future research on enhancing error analysis and mitigation.

</details>


### [254] [Rodimus*: Breaking the Accuracy-Efficiency Trade-Off with Efficient Attentions](https://arxiv.org/abs/2410.06577)

*Zhihao He, Hang Yu, Zi Gong, Shizhan Liu, Jianguo Li, Weiyao Lin*

**Main category:** cs.CL

**Keywords:** large language models, transformer, efficient computing, linear attention, NLP

**Relevance Score:** 8

**TL;DR:** This paper introduces Rodimus and Rodimus+ models, which reduce the computational complexity of LLMs by utilizing a linear attention mechanism and novel data-dependent selection techniques, achieving superior performance with fewer resources.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the significant computational costs incurred by traditional softmax attention in LLMs, aiming to improve efficiency without sacrificing performance.

**Method:** Rodimus uses data-dependent tempered selection in a linear attention-based framework, and Rodimus+ combines Rodimus with Sliding Window Shared-Key Attention to enhance semantic, token, and head compression techniques.

**Key Contributions:**

	1. Introduction of the Rodimus architecture for LLMs
	2. Implementation of data-dependent tempered selection for memory efficiency
	3. Combination of attention techniques in Rodimus+ for improved performance

**Result:** Rodimus+-1.6B outperforms existing models like Qwen2-1.5B and RWKV6-1.6B in downstream tasks, despite being trained on fewer tokens, demonstrating a strong balance between accuracy and efficiency.

**Limitations:** 

**Conclusion:** The proposed architectures, Rodimus and Rodimus+, showcase significant improvements in LLM performance by reducing memory usage and computational complexity while maintaining accuracy, redefining the trade-off between efficiency and effectiveness in LLMs.

**Abstract:** Recent advancements in Transformer-based large language models (LLMs) have set new standards in natural language processing. However, the classical softmax attention incurs significant computational costs, leading to a $O(T)$ complexity for per-token generation, where $T$ represents the context length. This work explores reducing LLMs' complexity while maintaining performance by introducing Rodimus and its enhanced version, Rodimus$+$. Rodimus employs an innovative data-dependent tempered selection (DDTS) mechanism within a linear attention-based, purely recurrent framework, achieving significant accuracy while drastically reducing the memory usage typically associated with recurrent models. This method exemplifies semantic compression by maintaining essential input information with fixed-size hidden states. Building on this, Rodimus$+$ combines Rodimus with the innovative Sliding Window Shared-Key Attention (SW-SKA) in a hybrid approach, effectively leveraging the complementary semantic, token, and head compression techniques. Our experiments demonstrate that Rodimus$+$-1.6B, trained on 1 trillion tokens, achieves superior downstream performance against models trained on more tokens, including Qwen2-1.5B and RWKV6-1.6B, underscoring its potential to redefine the accuracy-efficiency balance in LLMs. Model code and pre-trained checkpoints are open-sourced at https://github.com/codefuse-ai/rodimus.

</details>


### [255] [MOOSE-Chem: Large Language Models for Rediscovering Unseen Chemistry Scientific Hypotheses](https://arxiv.org/abs/2410.07076)

*Zonglin Yang, Wanhao Liu, Ben Gao, Tong Xie, Yuqiang Li, Wanli Ouyang, Soujanya Poria, Erik Cambria, Dongzhan Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hypothesis Generation, Chemistry, MOOSE-Chem, Scientific Discovery

**Relevance Score:** 8

**TL;DR:** This paper investigates whether large language models (LLMs) can autonomously generate valid hypotheses in chemistry, proposing a decomposition framework and demonstrating its effectiveness through the MOOSE-Chem framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The potential of LLMs to accelerate scientific discovery in chemistry is explored, with a focus on their ability to autonomously generate novel hypotheses.

**Method:** A mathematical decomposition of the hypothesis discovery task is proposed, leading to three subtasks: retrieving inspirations, composing hypotheses, and ranking hypotheses. The MOOSE-Chem framework implements this approach and is evaluated using a benchmark of high-impact chemistry papers.

**Key Contributions:**

	1. Proposed formal decomposition of the hypothesis discovery task in chemistry.
	2. Development of the MOOSE-Chem framework for hypothesis generation.
	3. Demonstration of LLMs' ability to rediscover high-quality hypotheses with minimal supervision.

**Result:** MOOSE-Chem successfully rediscovered many hypotheses with high similarity to the groundtruth and demonstrated high accuracy in inspiration retrieval.

**Limitations:** The study is limited to the chemistry domain and results may not be generalizable to other fields without further research.

**Conclusion:** The findings suggest that LLMs might encode latent scientific knowledge associations, hinting at their broader applicability in hypothesis generation.

**Abstract:** Scientific discovery plays a pivotal role in advancing human society, and recent progress in large language models (LLMs) suggests their potential to accelerate this process. However, it remains unclear whether LLMs can autonomously generate novel and valid hypotheses in chemistry. In this work, we investigate whether LLMs can discover high-quality chemistry hypotheses given only a research background-comprising a question and/or a survey-without restriction on the domain of the question. We begin with the observation that hypothesis discovery is a seemingly intractable task. To address this, we propose a formal mathematical decomposition grounded in a fundamental assumption: that most chemistry hypotheses can be composed from a research background and a set of inspirations. This decomposition leads to three practical subtasks-retrieving inspirations, composing hypotheses with inspirations, and ranking hypotheses - which together constitute a sufficient set of subtasks for the overall scientific discovery task. We further develop an agentic LLM framework, MOOSE-Chem, that is a direct implementation of this mathematical decomposition. To evaluate this framework, we construct a benchmark of 51 high-impact chemistry papers published and online after January 2024, each manually annotated by PhD chemists with background, inspirations, and hypothesis. The framework is able to rediscover many hypotheses with high similarity to the groundtruth, successfully capturing the core innovations-while ensuring no data contamination since it uses an LLM with knowledge cutoff date prior to 2024. Finally, based on LLM's surprisingly high accuracy on inspiration retrieval, a task with inherently out-of-distribution nature, we propose a bold assumption: that LLMs may already encode latent scientific knowledge associations not yet recognized by humans.

</details>


### [256] [Enhancing LLM Evaluations: The Garbling Trick](https://arxiv.org/abs/2411.01533)

*William F. Bradley*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evaluation Metrics, Reasoning Capabilities

**Relevance Score:** 9

**TL;DR:** The paper introduces a new method to improve the evaluation of LLMs by creating progressively challenging tasks that better assess reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of saturation in traditional evaluation metrics for large language models, which makes it difficult to distinguish model performance.

**Method:** The authors propose a methodology to enhance existing LLM evaluations by transforming them into progressively challenging tasks, focusing on reasoning capabilities.

**Key Contributions:**

	1. Development of a new method for evaluating LLMs through difficult task progression
	2. Creation of a multiple-choice test corpus for model assessment
	3. Insights into the performance differences between base and reasoning LLMs

**Result:** The approach is validated by creating a new multiple-choice test corpus and assessing various LLMs, revealing significant performance differences, especially between base LLMs and advanced reasoning models.

**Limitations:** 

**Conclusion:** Enhanced evaluations provide better insights into model performance, particularly in terms of reasoning, promoting a more nuanced comparison among LLMs.

**Abstract:** As large language models (LLMs) become increasingly powerful, traditional evaluation metrics tend to saturate, making it challenging to distinguish between models. We propose a general method to transform existing LLM evaluations into a series of progressively more difficult tasks. These enhanced evaluations emphasize reasoning capabilities and can reveal relative performance differences that are not apparent in the original assessments.   To demonstrate the effectiveness of our approach, we create a new multiple-choice test corpus, extend it into a family of evaluations, and assess a collection of LLMs. Our results offer insights into the comparative abilities of these models, particularly highlighting the differences between base LLMs and more recent "reasoning" models.

</details>


### [257] [VersaTune: An Efficient Data Composition Framework for Training Multi-Capability LLMs](https://arxiv.org/abs/2411.11266)

*Keer Lu, Keshi Zhao, Zhuoran Zhang, Zheng Liang, Da Pan, Shusen Zhang, Xin Wu, Guosheng Dong, Bin Cui, Tengjiao Wang, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-Tuning, Multi-Domain Learning

**Relevance Score:** 8

**TL;DR:** VersaTune is a novel framework for enhancing multi-domain capabilities of Large Language Models through adaptive data composition during training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of catastrophic forgetting in existing domain-specific enhancements during fine-tuning of LLMs.

**Method:** VersaTune uses a data composition framework that detects the distribution of domain-specific knowledge in the base model and adjusts training data composition dynamically according to model's knowledge distribution and forgetting degree.

**Key Contributions:**

	1. Introduction of VersaTune for multi-domain LLM enhancement
	2. Dynamic adjustment of domain weights during training
	3. Significantly improved performance metrics over existing models

**Result:** VersaTune improved overall multi-domain performance by 35.21% compared to uniform domain weights and outperformed several frontier models.

**Limitations:** 

**Conclusion:** VersaTune effectively enhances multi-domain capabilities and reduces performance degradation in non-target domains while preserving training efficacy.

**Abstract:** As demonstrated by the proprietary Large Language Models (LLMs) such as GPT and Claude series, LLMs have the potential to achieve remarkable proficiency across a wide range of domains, including law, medicine, finance, science, code, etc., all within a single model. These capabilities are further augmented during the Supervised Fine-Tuning (SFT) phase. Despite their potential, existing work mainly focuses on domain-specific enhancements during fine-tuning, the challenge of which lies in catastrophic forgetting of knowledge across other domains. In this study, we introduce **VersaTune**, a novel data composition framework designed for enhancing LLMs' overall multi-domain capabilities during training. We begin with detecting the distribution of domain-specific knowledge within the base model, followed by the training data composition that aligns with the model's existing knowledge distribution. During the subsequent training process, domain weights are dynamically adjusted based on their learnable potential and forgetting degree. Experimental results indicate that VersaTune is effective in multi-domain fostering, with an improvement of 35.21\% in the overall multi-ability performances compared to uniform domain weights. Furthermore, we find that Qwen-2.5-32B + VersaTune even surpasses frontier models, including GPT-4o, Claude3.5-Sonnet and DeepSeek-V3 by 0.86\%, 4.76\% and 4.60\%. Additionally, in scenarios where flexible expansion of a specific domain is required, VersaTune reduces the performance degradation in other domains by 38.77\%, while preserving the training efficacy of the target domain.

</details>


### [258] [Enhancing LLMs for Power System Simulations: A Feedback-driven Multi-agent Framework](https://arxiv.org/abs/2411.16707)

*Mengshuo Jia, Zeyu Cui, Gabriela Hug*

**Main category:** cs.CL

**Keywords:** large language models, simulation management, power systems, multi-agent framework, retrieval-augmented generation

**Relevance Score:** 6

**TL;DR:** The paper presents a feedback-driven, multi-agent framework for enhancing large language models (LLMs) in managing power system simulations, achieving significantly higher success rates than existing models.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the management of simulations in power systems, addressing the limitations of current large language models (LLMs) in domain-specific knowledge and reasoning capabilities.

**Method:** The proposed framework includes three modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism, validated across 69 diverse tasks.

**Key Contributions:**

	1. Development of a feedback-driven multi-agent framework for LLMs in power systems.
	2. Introduction of three innovative modules to enhance LLM performance in simulations.
	3. Demonstration of significantly improved success rates in complex tasks compared to existing models.

**Result:** The framework achieved success rates of 93.13% on Daline and 96.85% on MATPOWER, significantly outperforming ChatGPT and fine-tuned GPT-4o models which performed below 30%.

**Limitations:** 

**Conclusion:** This framework establishes a basis for advanced LLM-based research assistants in power systems, facilitating more efficient and cost-effective simulations.

**Abstract:** The integration of experimental technologies with large language models (LLMs) is transforming scientific research. It positions AI as a versatile research assistant rather than a mere problem-solving tool. In the field of power systems, however, managing simulations -- one of the essential experimental technologies -- remains a challenge for LLMs due to their limited domain-specific knowledge, restricted reasoning capabilities, and imprecise handling of simulation parameters. To address these limitations, this paper proposes a feedback-driven, multi-agent framework. It incorporates three proposed modules: an enhanced retrieval-augmented generation (RAG) module, an improved reasoning module, and a dynamic environmental acting module with an error-feedback mechanism. Validated on 69 diverse tasks from Daline and MATPOWER, this framework achieves success rates of 93.13% and 96.85%, respectively. It significantly outperforms ChatGPT 4o, o1-preview, and the fine-tuned GPT-4o, which all achieved a success rate lower than 30% on complex tasks. Additionally, the proposed framework also supports rapid, cost-effective task execution, completing each simulation in approximately 30 seconds at an average cost of 0.014 USD for tokens. Overall, this adaptable framework lays a foundation for developing intelligent LLM-based assistants for human researchers, facilitating power system research and beyond.

</details>


### [259] [Simple and Provable Scaling Laws for the Test-Time Compute of Large Language Models](https://arxiv.org/abs/2411.19477)

*Yanxi Chen, Xuchen Pan, Yaliang Li, Bolin Ding, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** large language models, scaling laws, test-time compute, algorithms, machine learning

**Relevance Score:** 8

**TL;DR:** Proposes two algorithms for improving large language models' performance through knockout and league styles, highlighting their scalability and theoretical foundations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop practical and scalable algorithms for test-time compute of large language models that can ensure efficiency and reliability in their outputs.

**Method:** Introduced a two-stage knockout-style algorithm and a two-stage league-style algorithm, both designed to evaluate multiple candidate solutions generated by a black-box LLM.

**Key Contributions:**

	1. Development of knockout-style and league-style algorithms for LLMs
	2. Theoretical proof of diminishing failure probabilities with increased compute
	3. Experimental validation across diverse models and datasets

**Result:** The algorithms demonstrate provable scaling laws, with diminishing failure probabilities as test-time compute increases, validated through experiments across various models and datasets.

**Limitations:** 

**Conclusion:** Both algorithms provide effective and adaptable approaches for leveraging LLMs in practical applications without needing additional components like verifiers or reward models.

**Abstract:** We propose two simple, principled and practical algorithms that enjoy provable scaling laws for the test-time compute of large language models (LLMs). The first one is a two-stage knockout-style algorithm: given an input problem, it first generates multiple candidate solutions, and then aggregate them via a knockout tournament for the final output. Assuming that the LLM can generate a correct solution with non-zero probability and do better than a random guess in comparing a pair of correct and incorrect solutions, we prove theoretically that the failure probability of this algorithm decays to zero exponentially or by a power law (depending on the specific way of scaling) as its test-time compute grows. The second one is a two-stage league-style algorithm, where each candidate is evaluated by its average win rate against multiple opponents, rather than eliminated upon loss to a single opponent. Under analogous but more robust assumptions, we prove that its failure probability also decays to zero exponentially with more test-time compute. Both algorithms require a black-box LLM and nothing else (e.g., no verifier or reward model) for a minimalistic implementation, which makes them appealing for practical applications and easy to adapt for different tasks. Through extensive experiments with diverse models and datasets, we validate the proposed theories and demonstrate the outstanding scaling properties of both algorithms.

</details>


### [260] [Can ChatGPT capture swearing nuances? Evidence from translating Arabic oaths](https://arxiv.org/abs/2412.02466)

*Mohammed Q. Shormani*

**Main category:** cs.CL

**Keywords:** ChatGPT, translation, Arabic, oath expressions, cross-cultural communication

**Relevance Score:** 4

**TL;DR:** This study investigates the ability of ChatGPT to translate Arabic oath expressions into English, revealing significant gaps in its performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of ChatGPT in capturing the nuanced meanings of swearing in translations from Arabic to English.

**Method:** The study involved the collection of 30 Arabic oath expressions, which were translated using ChatGPT and then compared to human translations. The analysis focused on various types of gaps in translation.

**Key Contributions:**

	1. Empirical analysis of ChatGPT's translation capabilities for Arabic oath expressions.
	2. Identification of specific types of translation gaps in ChatGPT's output.
	3. Call for enhancements in training data to improve cultural and religious nuance capture.

**Result:** The analysis revealed several types of gaps in ChatGPT's translations, including religious and cultural gaps, non-oath particles, redundancy, and lack of diacritics.

**Limitations:** The study is limited to a specific set of 30 Arabic oath expressions and may not be generalizable to all Arabic language nuances.

**Conclusion:** ChatGPT's translations of Arabic oaths are unsatisfactory, highlighting the need for further improvements and inclusion of relevant Arabic training data.

**Abstract:** This study sets out to answer one major question: Can ChatGPT capture swearing nuances? It presents an empirical study on the ability of ChatGPT to translate Arabic oath expressions into English. 30 Arabic oath expressions were collected from the literature. These 30 oaths were first translated via ChatGPT and then analyzed and compared to the human translation in terms of types of gaps left unfulfilled by ChatGPT. Specifically, the gaps involved are: religious gap, cultural gap, both religious and cultural gaps, no gap, using non-oath particles, redundancy and noncapturing of Arabic script diacritics. It concludes that ChatGPT translation of oaths is still much unsatisfactory, unveiling the need of further developments of ChatGPT, and the inclusion of Arabic data on which ChatGPT should be trained including oath expressions, oath nuances, rituals, and practices.

</details>


### [261] [Intention Knowledge Graph Construction for User Intention Relation Modeling](https://arxiv.org/abs/2412.11500)

*Jiaxin Bai, Zhaobo Wang, Junfei Cheng, Dan Yu, Zerui Huang, Weiqi Wang, Xin Liu, Chen Luo, Yanming Zhu, Bo Li, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** intention knowledge graph, user behavior, product recommendations, predictive modeling, machine learning

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for automatically generating intention knowledge graphs, focusing on connecting user intentions to improve behavior modeling and prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in understanding user intentions for online platforms and to improve the prediction of user behavior.

**Method:** The authors develop a framework to automatically generate an intention knowledge graph using the Amazon m2 dataset, capturing connections between user intentions.

**Key Contributions:**

	1. Introduced a framework for generating intention knowledge graphs.
	2. Focused on connecting user intentions for better prediction modeling.
	3. Outperformed state-of-the-art methods in predicting user session intentions.

**Result:** The constructed intention graph contains 351 million edges and shows high plausibility and acceptance. The model effectively predicts new session intentions and improves product recommendations, surpassing previous methods.

**Limitations:** 

**Conclusion:** The approach demonstrates practical utility in enhancing user experience on online platforms by improving intention prediction and product recommendations.

**Abstract:** Understanding user intentions is challenging for online platforms. Recent work on intention knowledge graphs addresses this but often lacks focus on connecting intentions, which is crucial for modeling user behavior and predicting future actions. This paper introduces a framework to automatically generate an intention knowledge graph, capturing connections between user intentions. Using the Amazon m2 dataset, we construct an intention graph with 351 million edges, demonstrating high plausibility and acceptance. Our model effectively predicts new session intentions and enhances product recommendations, outperforming previous state-of-the-art methods and showcasing the approach's practical utility.

</details>


### [262] [DateLogicQA: Benchmarking Temporal Biases in Large Language Models](https://arxiv.org/abs/2412.13377)

*Gagan Bhatia, MingZe Tang, Cristina Mahanta, Madiha Kazi*

**Main category:** cs.CL

**Keywords:** temporal reasoning, LLMs, benchmark, biases, date formats

**Relevance Score:** 6

**TL;DR:** DateLogicQA is a benchmark for evaluating LLMs' temporal reasoning, introducing a new metric and analyzing biases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities and limitations of LLMs in handling diverse date formats and temporal reasoning.

**Method:** Development of DateLogicQA benchmark with 190 questions and introduction of the Semantic Integrity Metric to evaluate tokenization quality.

**Key Contributions:**

	1. Introduction of DateLogicQA benchmark
	2. Semantic Integrity Metric for tokenization quality
	3. Identification of biases in LLMs affecting reasoning

**Result:** The study uncovers Representation-Level Bias and Logical-Level Bias in LLMs, affecting their temporal reasoning and outputs.

**Limitations:** 

**Conclusion:** Key challenges identified in accurate temporal data handling highlight the limitations of existing LLMs.

**Abstract:** This paper introduces DateLogicQA, a benchmark with 190 questions covering diverse date formats, temporal contexts, and reasoning types. We propose the Semantic Integrity Metric to assess tokenization quality and analyse two biases: Representation-Level Bias, affecting embeddings, and Logical-Level Bias, influencing reasoning outputs. Our findings provide a comprehensive evaluation of LLMs' capabilities and limitations in temporal reasoning, highlighting key challenges in handling temporal data accurately.

</details>


### [263] [Theoretical Proof that Auto-regressive Language Models Collapse when Real-world Data is a Finite Set](https://arxiv.org/abs/2412.14872)

*Lecheng Wang, Xianjie Shi, Ge Li, Jia Li, Xuanming Zhang, Yihong Dong, Wenpin Jiao, Hong Mei*

**Main category:** cs.CL

**Keywords:** Language Models, Synthetic Data, Data Scarcity, Model Collapse, Quality Assurance

**Relevance Score:** 5

**TL;DR:** The paper presents a theoretical proof that auto-regressive language models inevitably collapse when trained on recursively generated data without the inclusion of new real-world data.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of language model collapse in data-scarce domains when using generated data for training.

**Method:** The paper provides a theoretical proof regarding the behavior of language models trained on a corpus that includes synthetic data without real-world data.

**Key Contributions:**

	1. Theoretical proof of language model collapse due to synthetic data incorporation.
	2. Analysis of the implications of generated data on model training.
	3. Emphasis on the importance of data quality over quantity in mitigating collapse.

**Result:** The proof demonstrates that once generated data enters a training corpus, the language model will eventually collapse, regardless of the amount of generated data injected over time.

**Limitations:** The implications of collapse and quality assurance mechanisms for synthetic data need further exploration.

**Conclusion:** To prevent collapse of language models, it is essential to focus on the quality of synthetic data rather than just controlling the quantity.

**Abstract:** Auto-regressive language models (LMs) have been widely used to generate data in data-scarce domains to train new LMs, compensating for the scarcity of real-world data. Previous work experimentally found that LMs collapse when trained on recursively generated data. This paper presents a theoretical proof: once a corpus (such as a subset of the World Wide Web) begins to incorporate generated data and no new real-world data is added to the corpus, then no matter how small the amount of data each LM generates and contributes to the corpus, LM collapse is inevitable after sufficient time. This finding suggests that attempts to mitigate collapse by limiting the quantity of synthetic data in the corpus are fundamentally insufficient. Instead, avoiding collapse hinges on ensuring the quality of synthetic data.

</details>


### [264] [Multi-Agent Sampling: Scaling Inference Compute for Data Synthesis with Tree Search-Based Agentic Collaboration](https://arxiv.org/abs/2412.17061)

*Hai Ye, Mingbao Lin, Hwee Tou Ng, Shuicheng Yan*

**Main category:** cs.CL

**Keywords:** multi-agent systems, Monte Carlo Tree Search, data synthesis

**Relevance Score:** 8

**TL;DR:** This paper investigates a multi-agent approach using sampling from distinct language models for data synthesis, introducing a dynamic coordination strategy leveraging Monte Carlo Tree Search.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the under-explored scaling laws for inference compute in multi-agent systems compared to single-agent scenarios.

**Method:** The proposed method, TOA (Tree Search-based Orchestrated Agents), utilizes Monte Carlo Tree Search for dynamic model coordination and iterative workflow evolution during sampling.

**Key Contributions:**

	1. Introduction of TOA for dynamic multi-agent coordination during sampling
	2. Achievement of state-of-the-art results on WMT and AlpacaEval
	3. Demonstration of significant performance improvements in alignment and translation tasks.

**Result:** Experiments demonstrate that multi-agent sampling outperforms single-agent methods, achieving state-of-the-art performance on multiple benchmarks while being the most compute-efficient approach.

**Limitations:** 

**Conclusion:** The TOA framework enhances multi-agent collaboration in language model sampling, yielding superior alignment and translation results and surpassing traditional preference learning methods.

**Abstract:** Scaling laws for inference compute in multi-agent systems remain under-explored compared to single-agent scenarios. This work aims to bridge this gap by investigating the problem of data synthesis through multi-agent sampling, where synthetic responses are generated by sampling from multiple distinct language models. Effective model coordination is crucial for successful multi-agent collaboration. Unlike previous approaches that rely on fixed workflows, we treat model coordination as a multi-step decision-making process, optimizing generation structures dynamically for each input question. We introduce Tree Search-based Orchestrated Agents~(TOA), where the workflow evolves iteratively during the sequential sampling process. To achieve this, we leverage Monte Carlo Tree Search (MCTS), integrating a reward model to provide real-time feedback and accelerate exploration. Our experiments on alignment, machine translation, and mathematical reasoning demonstrate that multi-agent sampling significantly outperforms single-agent sampling as inference compute scales. TOA is the most compute-efficient approach, achieving SOTA performance on WMT and a 72.2\% LC win rate on AlpacaEval. Moreover, fine-tuning with our synthesized alignment data surpasses strong preference learning methods on challenging benchmarks such as Arena-Hard and AlpacaEval.

</details>


### [265] [ToolHop: A Query-Driven Benchmark for Evaluating Large Language Models in Multi-Hop Tool Use](https://arxiv.org/abs/2501.02506)

*Junjie Ye, Zhengyin Du, Xuesong Yao, Weijian Lin, Yufei Xu, Zehui Chen, Zaiyuan Wang, Sining Zhu, Zhiheng Xi, Siyu Yuan, Tao Gui, Qi Zhang, Xuanjing Huang, Jiecao Chen*

**Main category:** cs.CL

**Keywords:** multi-hop tool use, large language models, dataset, evaluation, tool strategy

**Relevance Score:** 9

**TL;DR:** Presentation of ToolHop, a dataset for evaluating multi-hop tool use in LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create reliable evaluation datasets for assessing understanding, reasoning, and function-calling capabilities of LLMs in multi-hop tool scenarios.

**Method:** Presented ToolHop dataset with 995 user queries and 3,912 tools, employing query-driven data construction involving tool creation, document refinement, and code generation.

**Key Contributions:**

	1. Introduction of ToolHop dataset specifically for multi-hop tool evaluation
	2. Rigorous evaluation of 14 LLMs across five model families
	3. Identification of variations in tool-use strategies among model families.

**Result:** Evaluated 14 LLMs revealing significant challenges in multi-hop tool use, with GPT-4o leading at 49.04% accuracy.

**Limitations:** Dataset is limited to the presented queries and tools, which may not cover all scenarios.

**Conclusion:** The study highlights the need for improvement in LLMs' tool-use capabilities and provides insights for developing better strategies.

**Abstract:** Effective evaluation of multi-hop tool use is critical for analyzing the understanding, reasoning, and function-calling capabilities of large language models (LLMs). However, progress has been hindered by a lack of reliable evaluation datasets. To address this, we present ToolHop, a dataset comprising 995 user queries and 3,912 associated tools, specifically designed for rigorous evaluation of multi-hop tool use. ToolHop ensures diverse queries, meaningful interdependencies, locally executable tools, detailed feedback, and verifiable answers through a novel query-driven data construction approach that includes tool creation, document refinement, and code generation. We evaluate 14 LLMs across five model families (i.e., LLaMA3.1, Qwen2.5, Gemini1.5, Claude3.5, and GPT), uncovering significant challenges in handling multi-hop tool-use scenarios. The leading model, GPT-4o, achieves an accuracy of 49.04%, underscoring substantial room for improvement. Further analysis reveals variations in tool-use strategies for various families, offering actionable insights to guide the development of more effective approaches. Code and data can be found in https://huggingface.co/datasets/bytedance-research/ToolHop.

</details>


### [266] [Can MLLMs Generalize to Multi-Party dialog? Exploring Multilingual Response Generation in Complex Scenarios](https://arxiv.org/abs/2501.11269)

*Zhongtian Hu, Yiwen Cui, Ronghan Li, Meng Zhao, Lifang Wang*

**Main category:** cs.CL

**Keywords:** multilingual large language models, multi-party dialogues, supervised fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper investigates the generalization capabilities of multilingual large language models (MLLMs) in complex dialogue scenarios using a new multilingual dataset called XMP, focusing on multi-party dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well MLLMs perform in complex dialogue tasks and understand the effects of supervised fine-tuning on their performance.

**Method:** Introduces the XMP dataset, which features multilingual multi-party podcasts, and conducts experiments to assess model performance in dialogue tasks.

**Key Contributions:**

	1. Introduction of the XMP dataset for multi-party dialogue tasks
	2. Demonstrates MLLMs' limitations in generalizing to complex dialogue scenarios
	3. Finds that fine-tuning has minimal effectiveness in improving model performance for multi-party dialogues

**Result:** MLLMs struggle with multi-party dialogues, showing only marginal improvement from fine-tuning on XMP, and language mixing during fine-tuning tends to be detrimental.

**Limitations:** The generalizability of results may be constrained by the dataset's specificity to podcast dialogues and the model's existing architecture.

**Conclusion:** The study highlights the limitations of current MLLMs in handling complex dialogue scenarios and suggests that more targeted training approaches are needed.

**Abstract:** Current multilingual large language models(MLLMs) still focus on simple question-answering formats, often overlooking more complex dialogue scenarios. In other words, their capabilities of multilingual large models have yet to be validated in dialogue tasks with intricate structures. We therefore ask, Q1: How well do LLMs generalize to more complex dialog scenarios? Q2: Can supervised fine-tuning on a high-quality parallel benchmark restore this ability? Q3: Does the "multilingual complementarity" effect survive in the setting? To answer these questions, we introduce XMP, a high-quality parallel Multilingual dataset sourced from Multi-party Podcast dialogues, which is the first parallel dataset focusing on multi-party dialogue scenarios. Most samples in the dataset feature three or more participants, discussing a wide range of topics. Through extensive experiments, we find that, R1: MLLMs fail to generalize to multi-party setting, R2 Fine-tuning on XMP improves only marginally, with the 70B model achieving at most a 1% absolute gain over its 8B counterpart; R3: Mixing languages during SFT is usually detrimental, with any benefits being marginal and limited to isolated cases in the 70B model.

</details>


### [267] [Advancing Multi-Party Dialogue Framework with Speaker-ware Contrastive Learning](https://arxiv.org/abs/2501.11292)

*Zhongtian Hu, Qi He, Ronghan Li, Meng Zhao, Lifang Wang*

**Main category:** cs.CL

**Keywords:** multi-party dialogue, contrastive learning, response generation, self-supervised learning, natural language processing

**Relevance Score:** 8

**TL;DR:** CMR is a contrastive learning-based framework for generating responses in multi-party dialogues, improving performance and generalization in handling complex conversations.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Multi-party dialogues present challenges due to their complexity and the roles of various speakers, which current models inadequately address.

**Method:** CMR utilizes a two-stage self-supervised contrastive learning framework that first captures global differences in speaking styles, followed by intra-conversation comparisons to identify transitions and relevant facts.

**Key Contributions:**

	1. First application of contrastive learning in multi-party dialogue generation
	2. Introduces a two-stage self-supervised learning framework
	3. Demonstrates significant performance improvements over existing models

**Result:** CMR significantly outperforms state-of-the-art models and enhances the performance of large pre-trained language models in multi-party dialogue.

**Limitations:** 

**Conclusion:** The proposed CMR framework effectively handles the intricacies of multi-party dialogues, marking a novel approach in dialogue generation.

**Abstract:** Multi-party dialogues, common in collaborative scenarios like brainstorming sessions and negotiations, pose significant challenges due to their complexity and diverse speaker roles. Current methods often use graph neural networks to model dialogue context, capturing structural dynamics but heavily relying on annotated graph structures and overlooking individual speaking styles. To address these challenges, we propose CMR, a Contrastive learning-based Multi-party dialogue Response generation framework. CMR employs a two-stage self-supervised contrastive learning framework. First, it captures global differences in speaking styles across individuals. Then, it focuses on intra-conversation comparisons to identify thematic transitions and contextually relevant facts. To the best of our knowledge, this is the first approach that applies contrastive learning in multi-party dialogue generation. Experimental results demonstrate that CMR not only significantly outperforms state-of-the-art models, but also generalizes well to large pre-trained language models, effectively enhancing their capability in handling multi-party conversations.

</details>


### [268] [Generative AI and Large Language Models in Language Preservation: Opportunities and Challenges](https://arxiv.org/abs/2501.11496)

*Vincent Koc*

**Main category:** cs.CL

**Keywords:** Generative AI, Large Language Models, language preservation, ethical considerations, community governance

**Relevance Score:** 9

**TL;DR:** This paper presents an analytical framework for using Generative AI and Large Language Models in language revitalization, emphasizing ethical practices and community governance to address language endangerment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the global crisis of language endangerment using Generative AI and LLMs while navigating the risks associated with data scarcity and cultural misappropriation.

**Method:** The paper introduces a framework that evaluates GenAI applications for their alignment with community-specific needs and ethical considerations, tested through the Te Reo Māori revitalization example.

**Key Contributions:**

	1. Novel analytical framework for evaluating GenAI applications in language preservation
	2. Demonstration of the framework's efficacy through case study on Te Reo Māori
	3. Identification of critical challenges and ethical considerations in deploying LLMs.

**Result:** The framework revealed successes like achieving 92% accuracy in community-led Automatic Speech Recognition, but also identified significant challenges regarding data sovereignty and model bias in digital resources.

**Limitations:** The framework's effectiveness depends on the continuous involvement of community stakeholders and may be subject to varying interpretations across different languages and contexts.

**Conclusion:** GenAI has the potential to revolutionize language preservation if interventions are grounded in community-centric data stewardship and continuous evaluation with clear risk management.

**Abstract:** The global crisis of language endangerment meets a technological turning point as Generative AI (GenAI) and Large Language Models (LLMs) unlock new frontiers in automating corpus creation, transcription, translation, and tutoring. However, this promise is imperiled by fragmented practices and the critical lack of a methodology to navigate the fraught balance between LLM capabilities and the profound risks of data scarcity, cultural misappropriation, and ethical missteps. This paper introduces a novel analytical framework that systematically evaluates GenAI applications against language-specific needs, embedding community governance and ethical safeguards as foundational pillars. We demonstrate its efficacy through the Te Reo M\=aori revitalization, where it illuminates successes, such as community-led Automatic Speech Recognition achieving 92% accuracy, while critically surfacing persistent challenges in data sovereignty and model bias for digital archives and educational tools. Our findings underscore that GenAI can indeed revolutionize language preservation, but only when interventions are rigorously anchored in community-centric data stewardship, continuous evaluation, and transparent risk management. Ultimately, this framework provides an indispensable toolkit for researchers, language communities, and policymakers, aiming to catalyze the ethical and high-impact deployment of LLMs to safeguard the world's linguistic heritage.

</details>


### [269] [AdaServe: Accelerating Multi-SLO LLM Serving with SLO-Customized Speculative Decoding](https://arxiv.org/abs/2501.12162)

*Zikun Li, Zhuofu Chen, Remi Delacourt, Gabriele Oliaro, Zeyu Wang, Qinghan Chen, Shuhuai Lin, April Yang, Zhihao Zhang, Zhuoming Chen, Sean Lai, Xinhao Cheng, Xupeng Miao, Zhihao Jia*

**Main category:** cs.CL

**Keywords:** large language models, service-level objectives, speculative decoding, multi-SLO serving, performance optimization

**Relevance Score:** 9

**TL;DR:** AdaServe is an LLM serving system designed for efficient multi-SLO performance through SLO-customized speculative decoding, significantly reducing SLO violations and improving throughput.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by existing LLM serving systems which struggle to meet diverse service-level objectives (SLOs) due to uniform batching and scheduling strategies.

**Method:** AdaServe formulates multi-SLO serving as a constrained optimization problem and employs a hardware-aware algorithm to create a speculation tree for each request's latency target, featuring a speculate-select-verify pipeline for enhanced control over decoding speed.

**Key Contributions:**

	1. Introduction of the AdaServe system for multi-SLO serving in LLMs
	2. Development of a speculation tree algorithm tailored to latency targets
	3. Dynamic adjustment of speculation parameters based on workload variation

**Result:** AdaServe demonstrates a reduction in SLO violations by up to 4.3 times and an increase in goodput by up to 1.9 times compared to leading baselines through evaluations on diverse workloads.

**Limitations:** 

**Conclusion:** AdaServe effectively enhances multi-SLO serving capabilities in LLM applications, enabling better performance across varying workload constraints.

**Abstract:** Modern large language model (LLM) applications exhibit diverse service-level objectives (SLOs), from low-latency requirements in interactive coding assistants to more relaxed constraints in data wrangling tasks. Existing LLM serving systems, which rely on uniform batching and scheduling strategies, often fail to meet these heterogeneous SLOs concurrently. We present AdaServe, the first LLM serving system designed to support efficient multi-SLO serving through SLO-customized speculative decoding. AdaServe formulates multi-SLO serving as a constrained optimization problem and introduces a hardware-aware algorithm that constructs a speculation tree tailored to each request's latency target. It features a speculate-select-verify pipeline that enables fine-grained control over decoding speed while maximizing system throughput. AdaServe further adapts to workload variation by dynamically adjusting speculation parameters. Evaluations across diverse workloads show that AdaServe reduces SLO violations by up to 4.3$\times$ and improves goodput by up to 1.9$\times$ compared to the best performing baselines, highlighting its effectiveness in multi-SLO serving.

</details>


### [270] [Option-ID Based Elimination For Multiple Choice Questions](https://arxiv.org/abs/2501.15175)

*Zhenhao Zhu, Bulou Liu, Qingyao Ai, Yiqun Liu*

**Main category:** cs.CL

**Keywords:** multiple choice questions, process of elimination, large language models, debiasing techniques, option-ID

**Relevance Score:** 9

**TL;DR:** This paper proposes a novel option-ID based method, PoE_ID, for answering multiple choice questions (MCQs) with improved performance using large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the process of elimination in MCQs and address shortcomings of existing PoE methods used with LLMs.

**Method:** The approach uses a debiasing technique and includes two strategies: PoE_ID^log, which removes options below a log probability threshold, and PoE_ID^seq, which iteratively eliminates the option with the lowest ID probability.

**Key Contributions:**

	1. Introduction of PoE_ID for MCQs
	2. Debiasing technique for token bias in LLMs
	3. Experimental validation across diverse datasets

**Result:** Experiments show that PoE_ID, especially PoE_ID^log, significantly improves zero-shot and few-shot performance on MCQs, particularly with datasets that have more options.

**Limitations:** Inherent deficiencies of LLMs in directly identifying incorrect options.

**Conclusion:** PoE_ID enhances LLMs' confidence in selecting correct options and performs better than methods using [MASK] replacement, while also highlighting LLMs' limitations in incorrect option identification.

**Abstract:** Multiple choice questions (MCQs) are a popular and important task for evaluating large language models (LLMs). Based on common strategies people use when answering MCQs, the process of elimination (PoE) has been proposed as an effective problem-solving method. Existing PoE methods typically either have LLMs directly identify incorrect options or score options and replace lower-scoring ones with [MASK]. However, both methods suffer from inapplicability or suboptimal performance. To address these issues, this paper proposes a novel option-ID based PoE ($\text{PoE}_{\text{ID}}$). $\text{PoE}_{\text{ID}}$ critically incorporates a debiasing technique to counteract LLMs token bias, enhancing robustness over naive ID-based elimination. It features two strategies: $\text{PoE}_{\text{ID}}^{\text{log}}$, which eliminates options whose IDs have log probabilities below the average threshold, and $\text{PoE}_{\text{ID}}^{\text{seq}}$, which iteratively removes the option with the lowest ID probability. We conduct extensive experiments with 6 different LLMs on 4 diverse datasets. The results demonstrate that $\text{PoE}_{\text{ID}}$, especially $\text{PoE}_{\text{ID}}^{\text{log}}$, significantly improves zero-shot and few-shot MCQs performance, particularly in datasets with more options. Our analyses demonstrate that $\text{PoE}_{\text{ID}}^{\text{log}}$ enhances the LLMs' confidence in selecting the correct option, and the option elimination strategy outperforms methods relying on [MASK] replacement. We further investigate the limitations of LLMs in directly identifying incorrect options, which stem from their inherent deficiencies.

</details>


### [271] [How Linguistics Learned to Stop Worrying and Love the Language Models](https://arxiv.org/abs/2501.17047)

*Richard Futrell, Kyle Mahowald*

**Main category:** cs.CL

**Keywords:** language models, linguistic structure, language processing, learning, usage-based approaches

**Relevance Score:** 7

**TL;DR:** The paper argues that language models (LMs) can positively contribute to understanding linguistic structure and learning, countering polarized views on their role in linguistics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of language models in addressing core questions about linguistic structure and human language processing, amidst conflicting opinions on their value.

**Method:** The authors analyze the capabilities of language models and their implications for linguistic theory, advocating for a usage-based approach to understanding language.

**Key Contributions:**

	1. Challenging the dichotomy between linguistic theory and language models.
	2. Proposing a gradient, usage-based approach to language informed by language models.
	3. Encouraging a reassessment of fundamental linguistic questions using tools from machine learning.

**Result:** The paper concludes that language models should not replace linguistic theory but can enhance our understanding by providing proof of concept for new approaches.

**Limitations:** 

**Conclusion:** Language models serve as useful tools for rethinking linguistic arguments and enhancing our insight into language processing, suggesting a productive relationship with linguistic theory.

**Abstract:** Language models can produce fluent, grammatical text. Nonetheless, some maintain that language models don't really learn language and also that, even if they did, that would not be informative for the study of human learning and processing. On the other side, there have been claims that the success of LMs obviates the need for studying linguistic theory and structure. We argue that both extremes are wrong. LMs can contribute to fundamental questions about linguistic structure, language processing, and learning. They force us to rethink arguments and ways of thinking that have been foundational in linguistics. While they do not replace linguistic structure and theory, they serve as model systems and working proofs of concept for gradient, usage-based approaches to language. We offer an optimistic take on the relationship between language models and linguistics.

</details>


### [272] [Jailbreaking LLMs' Safeguard with Universal Magic Words for Text Embedding Models](https://arxiv.org/abs/2501.18280)

*Haoyu Liang, Youran Sun, Yunfeng Cai, Jun Zhu, Bo Zhang*

**Main category:** cs.CL

**Keywords:** large language models, text embedding, security, magic words, defense mechanisms

**Relevance Score:** 8

**TL;DR:** This paper explores the security vulnerabilities of large language models (LLMs) related to text embedding, focusing on methods to exploit these vulnerabilities through universal magic words while proposing defenses against such attacks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the security issues of large language models (LLMs) and their susceptibility to harmful output through biased text embedding mechanisms.

**Method:** The authors propose methods to identify universal magic words that can manipulate the output distribution of text embedding models, and they also develop defense strategies to mitigate such attacks.

**Key Contributions:**

	1. Discovery of significant bias in text embedding output distribution.
	2. Proposing novel methods for universal magic word attacks on LLM safeguards.
	3. Introducing train-free defense mechanisms to mitigate bias and improve performance.

**Result:** Experiments demonstrate that magic word attacks significantly reduce the effectiveness of safeguard mechanisms on various chatbots, leading to harmful outputs, while the proposed defense methods can correct embedding biases without requiring retraining.

**Limitations:** Limited evaluation on the scalability of proposed defense methods in diverse applications and real-world environments.

**Conclusion:** The findings highlight serious security implications for LLMs and suggest effective countermeasures to strengthen defenses against embedding manipulation.

**Abstract:** The security issue of large language models (LLMs) has gained wide attention recently, with various defense mechanisms developed to prevent harmful output, among which safeguards based on text embedding models serve as a fundamental defense. Through testing, we discover that the output distribution of text embedding models is severely biased with a large mean. Inspired by this observation, we propose novel, efficient methods to search for **universal magic words** that attack text embedding models. Universal magic words as suffixes can shift the embedding of any text towards the bias direction, thus manipulating the similarity of any text pair and misleading safeguards. Attackers can jailbreak the safeguards by appending magic words to user prompts and requiring LLMs to end answers with magic words. Experiments show that magic word attacks significantly degrade safeguard performance on JailbreakBench, cause real-world chatbots to produce harmful outputs in full-pipeline attacks, and generalize across input/output texts, models, and languages. To eradicate this security risk, we also propose defense methods against such attacks, which can correct the bias of text embeddings and improve downstream performance in a train-free manner.

</details>


### [273] [Vision-centric Token Compression in Large Language Model](https://arxiv.org/abs/2502.00791)

*Ling Xing, Alex Jinpeng Wang, Rui Yan, Xiangbo Shu, Jinhui Tang*

**Main category:** cs.CL

**Keywords:** Token Compression, Large Language Models, Human Reading

**Relevance Score:** 7

**TL;DR:** Vision Centric Token Compression (Vist) is a new framework that reduces token count and resource usage in Large Language Models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in compute and memory costs due to the expansion of context windows and model parameters necessitates efficient token compression techniques.

**Method:** Vist employs a dual-path approach that combines visual encoding for low-salience tokens and LLM processing for important proximal tokens, utilizing a Probability-Informed Visual Enhancement objective to focus on semantically rich areas.

**Key Contributions:**

	1. Introduction of Vision Centric Token Compression framework (Vist)
	2. Dual-path processing that mimics human reading behavior
	3. Substantial reductions in token count while maintaining accuracy

**Result:** Vist achieves the same accuracy as previous methods with 2.3 times fewer tokens, resulting in a 16% reduction in FLOPs and a 50% reduction in memory usage, outperforming existing compression methods.

**Limitations:** 

**Conclusion:** This method sets a new standard for token efficiency in LLMs, demonstrating significant improvements in benchmarks like TriviaQA and NQ.

**Abstract:** Real-world applications are stretching context windows to hundreds of thousand of tokens while Large Language Models (LLMs) swell from billions to trillions of parameters. This dual expansion send compute and memory costs skyrocketing, making token compression indispensable. We introduce Vision Centric Token Compression (Vist), a slow-fast compression framework that mirrors human reading: the fast path renders distant tokens into images, letting a frozen, lightweight vision encoder skim the low-salience context; the slow path feeds the proximal window into the LLM for fine-grained reasoning. A Probability-Informed Visual Enhancement (PVE) objective masks high-frequency tokens during training, steering the Resampler to concentrate on semantically rich regions-just as skilled reader gloss over function words. On eleven in-context learning benchmarks, Vist achieves the same accuracy with 2.3 times fewer tokens, cutting FLOPs by 16% and memory by 50%. This method delivers remarkable results, outperforming the strongest text encoder-based compression method CEPE by 7.6% on average over benchmarks like TriviaQA, NQ, PopQA, NLUI, and CLIN, setting a new standard for token efficiency in LLMs. The source code will be released.

</details>


### [274] [Joint Localization and Activation Editing for Low-Resource Fine-Tuning](https://arxiv.org/abs/2502.01179)

*Wen Lai, Alexander Fraser, Ivan Titov*

**Main category:** cs.CL

**Keywords:** Parameter-efficient fine-tuning, activation editing, Transformer, low-resource scenarios, natural language processing

**Relevance Score:** 9

**TL;DR:** The paper introduces Joint Localization and Activation Editing (JoLA), a novel method for parameter-efficient fine-tuning of LLMs in low-resource settings, outperforming existing techniques across multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Parameter-efficient fine-tuning methods struggle in low-resource scenarios, and recent interpretability techniques have revealed potential in activation editing, which this paper aims to enhance.

**Method:** JoLA learns which Transformer heads to edit, determines the nature of the intervention (additive, multiplicative, or both), and optimizes the intervention parameters applied to the head output.

**Key Contributions:**

	1. Introduces Joint Localization and Activation Editing (JoLA) method for efficient fine-tuning.
	2. Demonstrates the effectiveness of targeted activation manipulation in LLMs.
	3. Provides an open-source implementation for further research.

**Result:** JoLA consistently outperforms existing parameter-efficient fine-tuning methods on benchmarks in commonsense reasoning, natural language understanding, and natural language generation.

**Limitations:** The performance is dependent on correctly identifying the heads to edit and may lack stability across diverse datasets.

**Conclusion:** JoLA offers a robust alternative for fine-tuning LLMs in low-resource environments, emphasizing the importance of module localization and appropriate intervention types.

**Abstract:** Parameter-efficient fine-tuning (PEFT) methods, such as LoRA, are commonly used to adapt LLMs. However, the effectiveness of standard PEFT methods is limited in low-resource scenarios with only a few hundred examples. Recent advances in interpretability research have inspired the emergence of activation editing (or steering) techniques, which modify the activations of specific model components. These methods, due to their extremely small parameter counts, show promise for small datasets. However, their performance is highly dependent on identifying the correct modules to edit and often lacks stability across different datasets. In this paper, we propose Joint Localization and Activation Editing (JoLA), a method that jointly learns (1) which heads in the Transformer to edit (2) whether the intervention should be additive, multiplicative, or both and (3) the intervention parameters themselves - the vectors applied as additive offsets or multiplicative scalings to the head output. Through evaluations on three benchmarks spanning commonsense reasoning, natural language understanding, and natural language generation, we demonstrate that JoLA consistently outperforms existing methods. The code for the method is released at https://github.com/wenlai-lavine/jola.

</details>


### [275] [Scaling Embedding Layers in Language Models](https://arxiv.org/abs/2502.01637)

*Da Yu, Edith Cohen, Badih Ghazi, Yangsibo Huang, Pritish Kamath, Ravi Kumar, Daogao Liu, Chiyuan Zhang*

**Main category:** cs.CL

**Keywords:** Language Models, N-grams, Embeddings, Inference Efficiency, Scalability

**Relevance Score:** 7

**TL;DR:** SCONE introduces n-gram embeddings for language models to enhance performance without increasing decoding costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language model performance while minimizing decoding costs associated with larger vocabulary embeddings.

**Method:** SCONE retains the original vocabulary and introduces frequent n-gram embeddings, which are learned separately and stored in off-accelerator memory, allowing efficient inference.

**Key Contributions:**

	1. Introduction of SCONE for scalable language model embeddings
	2. Demonstration of improved performance with fewer parameters
	3. Development of efficient inference strategies

**Result:** Models using SCONE outperform a 1.9B-parameter baseline with only 1B accelerator-resident parameters, while using half the FLOPS and accelerator memory during inference.

**Limitations:** 

**Conclusion:** SCONE provides effective scaling strategies for language models by incorporating n-gram embeddings without significantly impacting inference costs.

**Abstract:** We propose SCONE ($S$calable, $C$ontextualized, $O$ffloaded, $N$-gram $E$mbedding), a new method for extending input embedding layers to enhance language model performance. To avoid increased decoding costs, SCONE retains the original vocabulary while introducing embeddings for a set of frequent $n$-grams. These embeddings provide contextualized representation for each input token and are learned with a separate model during training. After training, embeddings are precomputed and stored in off-accelerator memory; during inference, querying them has minimal impact on latency due to the low complexity of embedding lookups. SCONE enables two new scaling strategies: increasing the number of $n$-gram embeddings and scaling the model used to learn them, both while maintaining fixed accelerator usage during inference (in terms of FLOPS and memory). We show that scaling both aspects enables a model with 1B accelerator-resident parameters to outperform a 1.9B-parameter baseline across diverse corpora, while using only about half the FLOPS and accelerator memory during inference.

</details>


### [276] [Generative Psycho-Lexical Approach for Constructing Value Systems in Large Language Models](https://arxiv.org/abs/2502.02444)

*Haoran Ye, Tianze Zhang, Yuhang Xie, Liyuan Zhang, Yuanyi Ren, Xin Zhang, Guojie Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, values, psychological principles, alignment, safety prediction

**Relevance Score:** 9

**TL;DR:** This paper presents the Generative Psycho-Lexical Approach (GPLA), a new method for constructing value systems for Large Language Models (LLMs), proposing a five-factor value system tailored for LLMs, which demonstrates improved alignment and safety predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a psychologically grounded value system for Large Language Models (LLMs) amid rising concerns about their intrinsic values.

**Method:** The study introduces the Generative Psycho-Lexical Approach (GPLA) for constructing value systems and validates it through three psychological benchmarking tasks integrated with AI priorities.

**Key Contributions:**

	1. Introduces the Generative Psycho-Lexical Approach (GPLA) for LLM value system construction
	2. Proposes a five-factor value system specifically for LLMs
	3. Demonstrates superior alignment and safety predictions of LLMs compared to existing value systems.

**Result:** The proposed five-factor value system for LLMs meets psychological criteria, improves safety predictions, and enhances alignment compared to Schwartz's canonical values.

**Limitations:** 

**Conclusion:** The research provides a scalable and adaptable psychological framework for understanding and improving LLM values, contributing to better safety and alignment.

**Abstract:** Values are core drivers of individual and collective perception, cognition, and behavior. Value systems, such as Schwartz's Theory of Basic Human Values, delineate the hierarchy and interplay among these values, enabling cross-disciplinary investigations into decision-making and societal dynamics. Recently, the rise of Large Language Models (LLMs) has raised concerns regarding their elusive intrinsic values. Despite growing efforts in evaluating, understanding, and aligning LLM values, a psychologically grounded LLM value system remains underexplored. This study addresses the gap by introducing the Generative Psycho-Lexical Approach (GPLA), a scalable, adaptable, and theoretically informed method for constructing value systems. Leveraging GPLA, we propose a psychologically grounded five-factor value system tailored for LLMs. For systematic validation, we present three benchmarking tasks that integrate psychological principles with cutting-edge AI priorities. Our results reveal that the proposed value system meets standard psychological criteria, better captures LLM values, improves LLM safety prediction, and enhances LLM alignment, when compared to the canonical Schwartz's values.

</details>


### [277] [Reformulation for Pretraining Data Augmentation](https://arxiv.org/abs/2502.04235)

*Xintong Hao, Ruijie Zhu, Ge Zhang, Ke Shen, Chenggang Li*

**Main category:** cs.CL

**Keywords:** data augmentation, large language models, training efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces the Massive Genre-Audience (MGA) method, a data augmentation technique designed to enhance the training of large language models by creating diverse corpus variations to counteract data repetition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address scaling issues in large language models due to data scarcity and performance degradation from excessive data repetition.

**Method:** The MGA method reformulates existing datasets into diverse variations, enhancing context richness and mitigating repetition effects.

**Key Contributions:**

	1. Introduction of the MGA reformulation method for data augmentation
	2. Creation of the MGACorpus with 770 billion tokens
	3. Analysis of prompt engineering's impact on generation quality

**Result:** Experiments show that MGA outperforms traditional methods, such as data repetition and upsampling, when scaling models up to 13B parameters.

**Limitations:** 

**Conclusion:** MGA offers a viable solution for improving training dataset augmentation, thereby facilitating more efficient scaling of large language models and reducing data repetition issues.

**Abstract:** Despite the impressive capabilities of large language models across various tasks, their continued scaling is severely hampered not only by data scarcity but also by the performance degradation associated with excessive data repetition during training. To overcome this critical bottleneck, we propose the Massive Genre-Audience(MGA) reformulation method, a lightweight and scalable data augmentation technique inspired by synthetic data methodologies. MGA systematically reformulates existing corpora into diverse, contextually-rich variations to mitigate the negative effects of repetition, and we introduce this approach along with the resulting 770 billion token MGACorpus in this work. We experimentally validate its core benefit by demonstrating superior performance against data repetition and upsampling in scaling scenarios (up to 13B parameters). Furthermore, comprehensive analysis investigates the role of prompt engineering in generation quality and reveals nuances in evaluating model capabilities using standard loss metrics. Our work shows that MGA provides a reliable pathway to substantially augment training datasets, effectively alleviating repetition bottlenecks and enabling more efficient scaling of large language models.

</details>


### [278] [ATLAS: Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data](https://arxiv.org/abs/2502.05567)

*Xiaoyang Liu, Kangjie Bao, Jiashuo Zhang, Yunqi Liu, Yuntian Liu, Yu Chen, Yang Jiao, Tao Luo*

**Main category:** cs.CL

**Keywords:** autoformalization, large language models, parallel corpora

**Relevance Score:** 7

**TL;DR:** This paper introduces ATLAS, a framework for generating high-quality parallel corpora for autoformalization of mathematical content using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The main goal is to overcome the barrier of limited parallel corpora for translating informal mathematical text into formal languages.

**Method:** The ATLAS framework utilizes a concept repository, expert iteration with knowledge distillation, and novel data augmentation strategies tailored to the structural characteristics of formal languages.

**Key Contributions:**

	1. Introduction of the ATLAS framework for autoformalization
	2. Creation of a large-scale dataset of theorem statements
	3. Achievement of statistically significant improvements over existing translation models

**Result:** ATLAS generated an undergraduate-level dataset of 117k theorem statements and demonstrated statistically significant improvements in performance over existing translators.

**Limitations:** 

**Conclusion:** The proposed approach achieves state-of-the-art results in autoformalization and will make the datasets, model, and code publicly available soon.

**Abstract:** Autoformalization, the automatic translation of mathematical content from natural language into machine-verifiable formal languages, has seen significant progress driven by advances in large language models (LLMs). Nonetheless, a primary barrier to further improvements is the limited availability of parallel corpora that map informal mathematical text to its formal counterpart. To address this limitation, we propose ATLAS (Autoformalizing Theorems through Lifting, Augmentation, and Synthesis of Data), a novel data generation framework designed to produce large-scale, high-quality parallel corpora of theorem statements. Distinct from prior approaches, ATLAS begins with a concept repository, accelerates the improvement of student model through expert iteration combined with knowledge distillation, and introduces two novel augmentation strategies that exploit the structural characteristics of formal languages. With the proposed ATLAS running for 10 iterations, we construct an undergraduate-level dataset comprising 117k theorem statements and develop ATLAS Translator, which demonstrates statistically significant improvements over both the HERALD Translator and the Kimina-Autoformalizer across all benchmarks ($p<0.05$, two-sided t-test), achieving a new state of the art. The datasets, model, and code will be released to the public soon.

</details>


### [279] [Evolving LLMs' Self-Refinement Capability via Iterative Preference Optimization](https://arxiv.org/abs/2502.05605)

*Yongcheng Zeng, Xinyu Cui, Xuanfa Jin, Guoqing Liu, Zexu Sun, Dong Li, Ning Yang, Jianye Hao, Haifeng Zhang, Jun Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Refinement, Preference Optimization, Model Training, Mathematical Reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces EVOLVE, a framework designed to enhance the self-refinement abilities of smaller language models, enabling them to compete with larger models in performance through iterative preference optimization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge that smaller language models have in matching the performance of larger models by enhancing their self-refinement capabilities.

**Method:** The paper presents EVOLVE, which integrates preference training with self-refinement data collection iteratively during training and inference phases.

**Key Contributions:**

	1. Introduction of the EVOLVE framework for model enhancement
	2. Demonstration of significant performance improvements in smaller models
	3. Successful application to mathematical reasoning tasks.

**Result:** In experiments, EVOLVE applied to Llama-3.1-8B outperformed state-of-the-art models, achieving significant win rates in various benchmarks, including mathematical reasoning tasks.

**Limitations:** 

**Conclusion:** EVOLVE effectively bridges the performance gap between smaller and larger models, enhancing their quality of output through self-refinement mechanisms and iterative training.

**Abstract:** While large language models (LLMs) have demonstrated remarkable general performance, enabling smaller models to achieve capabilities comparable to their larger counterparts remains a critical challenge. For humans, iterative refinement of problem analysis and responses is a common strategy to enhance answer quality. However, we observe that existing LLMs exhibit limited ability to refine their outputs for quality improvement. In this paper, we first investigate mechanisms to unlock and progressively enhance self-refinement ability in smaller models within an iterative preference optimization framework, aiming to bridge the performance gap with larger models. To this end, we propose EVOLVE, a novel post-training and inference framework that iteratively integrates preference training with self-refinement-driven data collection. During training, EVOLVE strengthens the model's direct question-answering ability while simultaneously unlocking its self-refinement potential. At inference, the framework leverages this capability to generate progressively refined responses, which are filtered to construct datasets for subsequent rounds of preference training. Experiments demonstrate EVOLVE's exceptional performance: when applied to Llama-3.1-8B base model and under the self-refinement setting, it surpasses state-of-the-art models including Llama-3.1-405B-Instruct and GPT-4o, achieving a 62.3% length-controlled win rate and 63.3% raw win rate on AlpacaEval 2, along with a 50.3% win rate on Arena-Hard. Furthermore, EVOLVE consistently enhances performance on mathematical reasoning tasks like GSM8K and MATH.

</details>


### [280] [Is LLM an Overconfident Judge? Unveiling the Capabilities of LLMs in Detecting Offensive Language with Annotation Disagreement](https://arxiv.org/abs/2502.06207)

*Junyu Lu, Kai Ma, Kaichun Wang, Kelaiti Xiao, Roy Ka-Wei Lee, Bo Xu, Liang Yang, Hongfei Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, offensive language detection, annotation disagreement, human judgment, model confidence

**Relevance Score:** 9

**TL;DR:** This paper investigates how Large Language Models (LLMs) handle annotation disagreement in offensive language detection, highlighting their performance, confidence levels, and how training with disagreement samples can improve model accuracy and alignment with human judgment.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the underexplored aspects of LLMs in offensive language detection, specifically their performance in cases with annotation disagreement—an area relevant for real-world applications in content moderation.

**Method:** The authors systematically evaluate the performance of multiple LLMs on binary classification tasks, analyze the relationship between model confidence and human disagreement, and assess the impact of disagreement samples on few-shot learning and instruction fine-tuning.

**Key Contributions:**

	1. Systematic evaluation of LLMs on offensive language detection with respect to annotation disagreement.
	2. Analysis of the impact of model confidence on human disagreement.
	3. Demonstration that training with disagreement samples enhances model performance.

**Result:** The findings show that LLMs face challenges with low-agreement samples, often exhibiting overconfidence; however, including disagreement samples in their training improves detection accuracy and alignment with human judgments.

**Limitations:** The study focuses on a specific aspect of LLM performance and may not generalize to all scenarios of language model application.

**Conclusion:** Enhancing LLM training with disagreement samples can lead to better performance in offensive language detection tasks, providing valuable insights for improving real-world content moderation systems.

**Abstract:** Large Language Models (LLMs) have become essential for offensive language detection, yet their ability to handle annotation disagreement remains underexplored. Disagreement samples, which arise from subjective interpretations, pose a unique challenge due to their ambiguous nature. Understanding how LLMs process these cases, particularly their confidence levels, can offer insight into their alignment with human annotators. This study systematically evaluates the performance of multiple LLMs in detecting offensive language at varying levels of annotation agreement. We analyze binary classification accuracy, examine the relationship between model confidence and human disagreement, and explore how disagreement samples influence model decision-making during few-shot learning and instruction fine-tuning. Our findings reveal that LLMs struggle with low-agreement samples, often exhibiting overconfidence in these ambiguous cases. However, utilizing disagreement samples in training improves both detection accuracy and model alignment with human judgment. These insights provide a foundation for enhancing LLM-based offensive language detection in real-world moderation tasks.

</details>


### [281] [Who Taught You That? Tracing Teachers in Model Distillation](https://arxiv.org/abs/2502.06659)

*Somin Wadhwa, Chantal Shaib, Silvio Amir, Byron C. Wallace*

**Main category:** cs.CL

**Keywords:** Model Distillation, Teacher Inference, LLMs, NLP, Ethics

**Relevance Score:** 8

**TL;DR:** The paper examines identifying the teacher models of distilled student models using output analysis, focusing on the reliability of teacher inference and its implications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if the outputs of student models can reveal their teacher models, and to highlight potential ethical issues in distillation practices.

**Method:** Investigates the use of lexical features and discriminative models to analyze outputs from student models to infer their respective teachers among a finite set of candidates.

**Key Contributions:**

	1. Presents a novel method for teacher inference based on lexical features.
	2. Demonstrates the unreliability of n-gram similarity for this task.
	3. Proposes that PoS templates are key indicators of teacher-student model relationships.

**Result:** n-gram similarity was found to be unreliable for teacher identification; however, part-of-speech templates used by students closely reflected those of their teachers.

**Limitations:** The approach is limited to a finite set of candidate teacher models treated as black boxes.

**Conclusion:** Reliable inference of teacher models could have significant implications, especially concerning the ethical considerations in the distillation of proprietary LLM capabilities.

**Abstract:** Model distillation -- using outputs from a large teacher model to teach a small student model -- is a practical means of creating efficient models for a particular task. We ask: Can we identify a students' teacher based on its outputs? Such "footprints" left by teacher LLMs would be interesting artifacts. Beyond this, reliable teacher inference may have practical implications as actors seek to distill specific capabilities of massive proprietary LLMs into deployed smaller LMs, potentially violating terms of service. We consider practical task distillation targets including summarization, question answering, and instruction-following. We assume a finite set of candidate teacher models, which we treat as blackboxes. We design discriminative models that operate over lexical features. We find that $n$-gram similarity alone is unreliable for identifying teachers, but part-of-speech (PoS) templates preferred by student models mimic those of their teachers.

</details>


### [282] [Can Vision-Language Models Infer Speaker's Ignorance? The Role of Visual and Linguistic Cues](https://arxiv.org/abs/2502.09120)

*Ye-eun Cho, Yunho Maeng*

**Main category:** cs.CL

**Keywords:** vision-language models, pragmatic inference, contextual cues, machine learning, natural language processing

**Relevance Score:** 7

**TL;DR:** This study examines the pragmatic inference abilities of vision-language models (VLMs) by manipulating visual and linguistic contextual cues. Results showed that Claude 3.5 sonnet combined cues effectively, demonstrating emerging pragmatic competence, while GPT-4o and Gemini 1.5 Pro favored literal interpretations.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether vision-language models (VLMs) can perform pragmatic inference, particularly with respect to ignorance implicatures.

**Method:** The study systematically manipulated contextual cues, utilizing visual depictions and QUD-based linguistic prompts to evaluate the responses of three state-of-the-art VLMs.

**Key Contributions:**

	1. Investigation of pragmatic inference in VLMs
	2. Systematic manipulation of contextual cues in testing
	3. Evidence of emerging pragmatic competence in Claude model

**Result:** When only visual cues were present, VLMs relied heavily on lexical meanings. With linguistic cues, Claude combined cues for more human-like inferences, while GPT and Gemini maintained a literal interpretation.

**Limitations:** 

**Conclusion:** The findings indicate differing behaviors among models concerning contextual cue handling, with Claude showing signs of pragmatic competence.

**Abstract:** This study investigates whether vision-language models (VLMs) can perform pragmatic inference, focusing on ignorance implicatures, utterances that imply the speaker's lack of precise knowledge. To test this, we systematically manipulated contextual cues: the visually depicted situation (visual cue) and QUD-based linguistic prompts (linguistic cue). When only visual cues were provided, three state-of-the-art VLMs (GPT-4o, Gemini 1.5 Pro, and Claude 3.5 sonnet) produced interpretations largely based on the lexical meaning of the modified numerals. When linguistic cues were added to enhance contextual informativeness, Claude exhibited more human-like inference by integrating both types of contextual cues. In contrast, GPT and Gemini favored precise, literal interpretations. Although the influence of contextual cues increased, they treated each contextual cue independently and aligned them with semantic features rather than engaging in context-driven reasoning. These findings suggest that although the models differ in how they handle contextual cues, Claude's ability to combine multiple cues may signal emerging pragmatic competence in multimodal models.

</details>


### [283] [RAS: Retrieval-And-Structuring for Knowledge-Intensive LLM Generation](https://arxiv.org/abs/2502.10996)

*Pengcheng Jiang, Lang Cao, Ruike Zhu, Minhao Jiang, Yunyi Zhang, Jimeng Sun, Jiawei Han*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, knowledge graphs, multi-step reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces Retrieval-And-Structuring (RAS), a framework that improves multi-step reasoning in language models by constructing query-specific knowledge graphs through iterative retrieval and structuring.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with reasoning due to the unstructured nature of retrieved context and the limitations of current RAG methods.

**Method:** RAS dynamically constructs knowledge graphs through targeted retrieval planning and incremental graph construction, enabling structured reasoning over evolving knowledge.

**Key Contributions:**

	1. Introduction of Retrieval-And-Structuring (RAS) framework
	2. Demonstrated performance improvements on knowledge-intensive tasks
	3. Integration of structured knowledge graphs for enhanced reasoning

**Result:** RAS consistently outperforms strong baselines on seven knowledge-intensive benchmarks, achieving gains of up to 6.4% and 7.0% with open-source and proprietary LLMs, respectively.

**Limitations:** 

**Conclusion:** Dynamic, query-specific knowledge structuring significantly enhances reasoning accuracy and robustness in language generation tasks.

**Abstract:** Large language models (LLMs) have achieved impressive performance on knowledge-intensive tasks, yet they often struggle with multi-step reasoning due to the unstructured nature of retrieved context. While retrieval-augmented generation (RAG) methods provide external information, the lack of explicit organization among retrieved passages limits their effectiveness, leading to brittle reasoning pathways. Recent interpretability studies highlighting the importance of structured intermediate reasoning further align with this perspective. We propose Retrieval-And-Structuring (RAS), a framework that dynamically constructs query-specific knowledge graphs through iterative retrieval and structured knowledge building. RAS interleaves targeted retrieval planning with incremental graph construction, enabling models to assemble and reason over evolving knowledge structures tailored to each query. On seven knowledge-intensive benchmarks, RAS consistently outperforms strong baselines, achieving up to 6.4% and 7.0% gains with open-source and proprietary LLMs, respectively. Our results demonstrate that dynamic, query-specific knowledge structuring offers a robust path to improving reasoning accuracy and robustness in language model generation. Our data and code can be found at https://github.com/pat-jj/RAS.

</details>


### [284] [Beyond Pairwise: Global Zero-shot Temporal Graph Generation](https://arxiv.org/abs/2502.11114)

*Alon Eirew, Kfir Bar, Ido Dagan*

**Main category:** cs.CL

**Keywords:** temporal relation extraction, natural language processing, large language models

**Relevance Score:** 7

**TL;DR:** This paper presents a novel zero-shot method for temporal relation extraction that generates a complete temporal graph and optimizes temporal constraints for consistency.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advances in large language models, the application of those models in temporal relation extraction is limited, highlighting the need for efficient methods.

**Method:** The proposed method generates a complete temporal graph for a document in one step, followed by temporal constraint optimization to refine and ensure consistency in predictions.

**Key Contributions:**

	1. Introduction of a novel zero-shot method for TRE
	2. Development of OmniTemp, a new dataset with complete event pair annotations
	3. Demonstration of improved performance over existing approaches

**Result:** The proposed method outperforms existing zero-shot approaches and serves as a competitive alternative to traditional supervised temporal relation extraction models.

**Limitations:** 

**Conclusion:** The results indicate significant improvements in efficiency and effectiveness, establishing a new methodology for temporal relation extraction.

**Abstract:** Temporal relation extraction (TRE) is a fundamental task in natural language processing (NLP) that involves identifying the temporal relationships between events in a document. Despite the advances in large language models (LLMs), their application to TRE remains limited. Most existing approaches rely on pairwise classification, where event pairs are classified in isolation, leading to computational inefficiency and a lack of global consistency in the resulting temporal graph. In this work, we propose a novel zero-shot method for TRE that generates a document's complete temporal graph in a single step, followed by temporal constraint optimization to refine predictions and enforce temporal consistency across relations. Additionally, we introduce OmniTemp, a new dataset with complete annotations for all pairs of targeted events within a document. Through experiments and analyses, we demonstrate that our method outperforms existing zero-shot approaches and offers a competitive alternative to supervised TRE models.

</details>


### [285] [Eye Tracking Based Cognitive Evaluation of Automatic Readability Assessment Measures](https://arxiv.org/abs/2502.11150)

*Keren Gruteke Klein, Shachar Frenkel, Omer Shubi, Yevgeni Berzak*

**Main category:** cs.CL

**Keywords:** readability, eye tracking, reading ease, cognitive framework, surprisal

**Relevance Score:** 6

**TL;DR:** This paper presents a cognitive framework using eye tracking to evaluate readability measures, finding that current measures often fail to predict reading ease effectively.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To propose a more effective methodology for predicting text readability by directly measuring reading ease through cognitive processes.

**Method:** The study employs an eye tracking-based framework to assess various readability measures and their effectiveness in predicting reading facilitation effects and reading ease.

**Key Contributions:**

	1. Development of an eye tracking-based cognitive framework for readability evaluation
	2. Identification of inadequacies in existing readability measures
	3. Demonstration of the superiority of psycholinguistic word properties in predicting reading ease

**Result:** The analyses reveal that existing readability measures are inadequate predictors of reading facilitation and ease, with word properties from psycholinguistics, especially surprisal, proving to be more effective.

**Limitations:** 

**Conclusion:** This study underscores the limitations of traditional readability measures and suggests the importance of using cognitive-based approaches to more accurately assess reading ease.

**Abstract:** Automated text readability prediction is widely used in many real-world scenarios. Over the past century, such measures have primarily been developed and evaluated on reading comprehension outcomes and on human annotations of text readability levels. In this work, we propose an alternative, eye tracking-based cognitive framework which directly taps into a key aspect of readability: reading ease. We use this framework for evaluating a broad range of prominent readability measures, including two systems widely used in education, by quantifying their ability to account for reading facilitation effects in text simplification, as well as text reading ease more broadly. Our analyses suggest that existing readability measures are poor predictors of reading facilitation and reading ease, outperformed by word properties commonly used in psycholinguistics, and in particular by surprisal.

</details>


### [286] [The Mirage of Model Editing: Revisiting Evaluation in the Wild](https://arxiv.org/abs/2502.11177)

*Wanli Yang, Fei Sun, Jiajun Tan, Xinyu Ma, Qi Cao, Dawei Yin, Huawei Shen, Xueqi Cheng*

**Main category:** cs.CL

**Keywords:** model editing, question answering, LLMs, evaluation framework, AI errors

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of model editing methods in question answering, revealing significant performance declines compared to previous studies due to flawed evaluation practices.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address gaps in the real-world applicability of model editing for LLMs, particularly in question answering tasks.

**Method:** The authors propose QAEdit, a benchmark for evaluating editing methods, and establish a standardized evaluation framework that includes module analysis and controlled experiments.

**Key Contributions:**

	1. Introduction of QAEdit benchmark for question answering
	2. Establishment of a standardized evaluation framework
	3. Identification of flaws in previous editing evaluation practices

**Result:** Through these experiments, they found that current editing methods perform worse than previously reported, primarily due to inappropriate evaluation practices.

**Limitations:** Current study focuses only on LLMs in QA; results may not generalize to other applications or models.

**Conclusion:** The study highlights the need for a fundamental reexamination of model editing methods and their evaluation frameworks to ensure their reliability and practical application.

**Abstract:** Despite near-perfect results in artificial evaluations, the effectiveness of model editing in real-world applications remains unexplored. To bridge this gap, we propose to study model editing in question answering (QA) by establishing a rigorous evaluation practice to assess the effectiveness of editing methods in correcting LLMs' errors. It consists of QAEdit, a new benchmark derived from popular QA datasets, and a standardized evaluation framework. Our single editing experiments indicate that current editing methods perform substantially worse than previously reported (38.5% vs. ~96%). Through module analysis and controlled experiments, we demonstrate that this performance decline stems from issues in evaluation practices of prior editing research. One key issue is the inappropriate use of teacher forcing in testing prevents error propagation by feeding ground truth tokens (inaccessible in real-world scenarios) as input. Furthermore, we simulate real-world deployment by sequential editing, revealing that current approaches fail drastically with only 1000 edits. Our analysis provides a fundamental reexamination of both the real-world applicability of existing model editing methods and their evaluation practices, and establishes a rigorous evaluation framework with key insights to advance reliable and practical model editing research.

</details>


### [287] [From the New World of Word Embeddings: A Comparative Study of Small-World Lexico-Semantic Networks in LLMs](https://arxiv.org/abs/2502.11380)

*Zhu Liu, Ying Liu, KangYang Luo, Cunliang Kong, Maosong Sun*

**Main category:** cs.CL

**Keywords:** lexico-semantic networks, large language models, small-world properties

**Relevance Score:** 8

**TL;DR:** This paper explores the construction of lexico-semantic networks using embeddings from decoder-only large language models (LLMs) and compares their structures to traditional models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of decoder-only LLMs in building lexico-semantic networks and analyzing their properties.

**Method:** Constructed networks from input embeddings of LLMs of varying parameter scales and conducted a comparative structural analysis.

**Key Contributions:**

	1. Introduced lexico-semantic networks constructed from decoder-only LLMs.
	2. Demonstrated small-world properties in networks from LLM embeddings.
	3. Provided insights into the structural differences of networks with varying model sizes.

**Result:** The constructed networks display small-world properties with high clustering and short path lengths, while larger models yield more complex structures with reduced small-world effects.

**Limitations:** Focuses solely on decoder-only LLMs; findings may not generalize to encoder-based models.

**Conclusion:** Lexico-semantic networks from LLMs can capture intricate semantic relations, with larger models reflecting richer semantic structures.

**Abstract:** Lexico-semantic networks represent words as nodes and their semantic relatedness as edges. While such networks are traditionally constructed using embeddings from encoder-based models or static vectors, embeddings from decoder-only large language models (LLMs) remain underexplored. Unlike encoder models, LLMs are trained with a next-token prediction objective, which does not directly encode the meaning of the current token. In this paper, we construct lexico-semantic networks from the input embeddings of LLMs with varying parameter scales and conduct a comparative analysis of their global and local structures. Our results show that these networks exhibit small-world properties, characterized by high clustering and short path lengths. Moreover, larger LLMs yield more intricate networks with less small-world effects and longer paths, reflecting richer semantic structures and relations. We further validate our approach through analyses of common conceptual pairs, structured lexical relations derived from WordNet, and a cross-lingual semantic network for qualitative words.

</details>


### [288] [Beyond Single-Task: Robust Multi-Task Length Generalization for LLMs](https://arxiv.org/abs/2502.11525)

*Yi Hu, Shijia Kang, Haotong Yang, Haotian Xu, Muhan Zhang*

**Main category:** cs.CL

**Keywords:** Length generalization, Meta-RFFT, multi-task learning, large language models, cross-task performance

**Relevance Score:** 8

**TL;DR:** This paper introduces Meta Rule-Following Fine-Tuning (Meta-RFFT) to enhance length generalization in multi-task settings for large language models, demonstrating superior performance on unseen tasks with minimal fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Length generalization is a critical challenge for large language models, affecting their ability to solve problems longer than those encountered during training. Existing methods are limited and often degrade general language performance.

**Method:** The authors propose Meta Rule-Following Fine-Tuning (Meta-RFFT), a framework designed for cross-task length generalization. They construct a comprehensive dataset with 86 tasks and demonstrate that their method allows models to generalize effectively across these tasks.

**Key Contributions:**

	1. Introduction of Meta-RFFT for cross-task length generalization
	2. Creation of a large length generalization dataset with 86 diverse tasks
	3. Demonstration of remarkable unseen task accuracy with minimal fine-tuning

**Result:** Models trained using Meta-RFFT achieve up to 95% accuracy on previously unseen length tasks, such as 30 digit addition, outperforming the state-of-the-art reasoning models.

**Limitations:** 

**Conclusion:** Meta-RFFT represents a significant advancement in multi-task length generalization, providing a robust approach that outperforms prior methods without extensive re-training for each task.

**Abstract:** Length generalization, the ability to solve problems longer than those seen during training, remains a critical challenge for large language models (LLMs). Previous work modifies positional encodings (PEs) and data formats to improve length generalization on specific symbolic tasks such as addition and sorting. However, these approaches are fundamentally limited to special tasks, often degrading general language performance. Furthermore, they are typically evaluated on small transformers trained from scratch on single tasks and can cause performance drop when applied during post-training stage of practical LLMs with general capabilities. Hu et al., (2024) proposed Rule-Following Fine-Tuning (RFFT) to improve length generalization in the post-training stage of LLMs. Despite its compatibility with practical models and strong performance, RFFT is proposed for single tasks too, requiring re-training for each individual task with extensive examples. In this paper, we study length generalization in multi-task settings and propose Meta Rule-Following Fine-Tuning (Meta-RFFT), the first framework enabling robust cross-task length generalization. As our first contribution, we construct a large length generalization dataset containing 86 tasks spanning code execution, number processing, symbolic and logical reasoning tasks, beyond the common addition or multiplication tasks. Secondly, we show that cross-task length generalization is possible with Meta-RFFT. After training on a large number of tasks and instances, the models achieve remarkable length generalization ability on unseen tasks with minimal fine-tuning or one-shot prompting. For example, after fine-tuning on 1 to 5 digit addition, our 32B model achieves 95% accuracy on 30 digit addition, significantly outperforming the state-of-the-art reasoning models (DeepSeek-R1-671B: 72%), despite never seeing this task during RF-pretraining.

</details>


### [289] [FaMTEB: Massive Text Embedding Benchmark in Persian Language](https://arxiv.org/abs/2502.11571)

*Erfan Zinvandi, Morteza Alikhani, Mehran Sarmadi, Zahra Pourbahman, Sepehr Arvin, Reza Kazemi, Arash Amini*

**Main category:** cs.CL

**Keywords:** Persian NLP, text embeddings, evaluation framework, chatbots, summary retrieval

**Relevance Score:** 4

**TL;DR:** The paper introduces a benchmark for Persian text embeddings, highlighting 63 datasets across seven tasks, including new evaluation datasets and tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive evaluation framework for Persian language models, as existing benchmarks are insufficient for the growing use of text embeddings in applications like chatbots and retrieval-augmented generation.

**Method:** Development of a benchmark built on the Massive Text Embedding Benchmark (MTEB), incorporating existing, translated, and newly generated datasets for Persian NLP.

**Key Contributions:**

	1. Comprehensive benchmark for Persian text embeddings based on MTEB.
	2. Inclusion of chatbot evaluation datasets for the first time.
	3. Introduction of a new task: summary retrieval.

**Result:** The introduction of evaluation datasets for chatbots and the new task of summary retrieval, along with performance evaluations of several Persian and multilingual embedding models.

**Limitations:** 

**Conclusion:** This work presents an open-source benchmark with resources for training and evaluating Persian NLP models, enhancing the field with new datasets and tasks.

**Abstract:** In this paper, we introduce a comprehensive benchmark for Persian (Farsi) text embeddings, built upon the Massive Text Embedding Benchmark (MTEB). Our benchmark includes 63 datasets spanning seven different tasks: classification, clustering, pair classification, reranking, retrieval, summary retrieval, and semantic textual similarity. The datasets are formed as a combination of existing, translated, and newly generated data, offering a diverse evaluation framework for Persian language models. Given the increasing use of text embedding models in chatbots, evaluation datasets are becoming inseparable ingredients in chatbot challenges and Retrieval-Augmented Generation systems. As a contribution, we include chatbot evaluation datasets in the MTEB benchmark for the first time. In addition, in this paper, we introduce the new task of summary retrieval which is not part of the tasks included in standard MTEB. Another contribution of this paper is the introduction of a substantial number of new Persian language NLP datasets suitable for training and evaluation, some of which have no previous counterparts in Persian. We evaluate the performance of several Persian and multilingual embedding models in a range of tasks. This work introduces an open-source benchmark with datasets, code and a public leaderboard.

</details>


### [290] [To Think or Not to Think: Exploring the Unthinking Vulnerability in Large Reasoning Models](https://arxiv.org/abs/2502.12202)

*Zihao Zhu, Hongbao Zhang, Ruotong Wang, Ke Xu, Siwei Lyu, Baoyuan Wu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Unthinking Vulnerability, Breaking of Thought, Monitoring of Thought, adversarial attacks

**Relevance Score:** 8

**TL;DR:** This paper investigates a vulnerability in Large Reasoning Models (LRMs) termed Unthinking Vulnerability, which can be exploited for malicious and beneficial purposes.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical vulnerability found in LRMs that allows bypassing their reasoning process, potentially compromising their reliability.

**Method:** The paper introduces and empirically demonstrates a novel attack, Breaking of Thought (BoT), and a framework, Monitoring of Thought (MoT), to exploit and mitigate this vulnerability.

**Key Contributions:**

	1. Introduction of Unthinking Vulnerability in LRMs.
	2. Development of the Breaking of Thought (BoT) attack.
	3. Proposal of Monitoring of Thought (MoT) for improving model safety.

**Result:** BoT demonstrates a significant threat to LRM reasoning reliability, while MoT offers a practical solution to enhance efficiency and safety in LRMs.

**Limitations:** The proposed defenses are partial and do not completely eradicate the vulnerability.

**Conclusion:** The findings reveal inherent flaws in LRM architectures, emphasizing the need for more robust reasoning systems.

**Abstract:** Large Reasoning Models (LRMs) are designed to solve complex tasks by generating explicit reasoning traces before producing final answers. However, we reveal a critical vulnerability in LRMs -- termed Unthinking Vulnerability -- wherein the thinking process can be bypassed by manipulating special delimiter tokens. It is empirically demonstrated to be widespread across mainstream LRMs, posing both a significant risk and potential utility, depending on how it is exploited. In this paper, we systematically investigate this vulnerability from both malicious and beneficial perspectives. On the malicious side, we introduce Breaking of Thought (BoT), a novel attack that enables adversaries to bypass the thinking process of LRMs, thereby compromising their reliability and availability. We present two variants of BoT: a training-based version that injects backdoor during the fine-tuning stage, and a training-free version based on adversarial attack during the inference stage. As a potential defense, we propose thinking recovery alignment to partially mitigate the vulnerability. On the beneficial side, we introduce Monitoring of Thought (MoT), a plug-and-play framework that allows model owners to enhance efficiency and safety. It is implemented by leveraging the same vulnerability to dynamically terminate redundant or risky reasoning through external monitoring. Extensive experiments show that BoT poses a significant threat to reasoning reliability, while MoT provides a practical solution for preventing overthinking and jailbreaking. Our findings expose an inherent flaw in current LRM architectures and underscore the need for more robust reasoning systems in the future.

</details>


### [291] [SafeRoute: Adaptive Model Selection for Efficient and Accurate Safety Guardrails in Large Language Models](https://arxiv.org/abs/2502.12464)

*Seanie Lee, Dong Bok Lee, Dominik Wagner, Minki Kang, Haebin Seong, Tobias Bocklet, Juho Lee, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Safety Guards, Model Efficiency, Adaptive Model Selection, Benchmark Datasets

**Relevance Score:** 9

**TL;DR:** Introducing SafeRoute, a binary router that enhances the efficiency of safety guard models for LLMs by selectively applying larger models to hard examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of large language models necessitates robust safety systems to detect harmful prompts, but their computational cost is high. Smaller distilled models perform poorly on difficult examples, prompting the need for a solution that balances efficiency and safety.

**Method:** SafeRoute uses a binary routing mechanism to differentiate between easy and hard examples, applying the larger safety guard model only to the latter.

**Key Contributions:**

	1. Introduction of SafeRoute, a binary router for LLM safety
	2. Demonstrated performance improvements on benchmark datasets
	3. Enhanced trade-off between cost and safety in model application

**Result:** The proposed method improves safety performance and reduces computational costs by smartly selecting model application based on input complexity.

**Limitations:** The approach may still struggle with ambiguous examples where the distinction between easy and hard is unclear.

**Conclusion:** SafeRoute significantly increases efficiency without sacrificing accuracy, surpassing traditional models and improving safety measures in LLM applications.

**Abstract:** Deploying large language models (LLMs) in real-world applications requires robust safety guard models to detect and block harmful user prompts. While large safety guard models achieve strong performance, their computational cost is substantial. To mitigate this, smaller distilled models are used, but they often underperform on "hard" examples where the larger model provides accurate predictions. We observe that many inputs can be reliably handled by the smaller model, while only a small fraction require the larger model's capacity. Motivated by this, we propose SafeRoute, a binary router that distinguishes hard examples from easy ones. Our method selectively applies the larger safety guard model to the data that the router considers hard, improving efficiency while maintaining accuracy compared to solely using the larger safety guard model. Experimental results on multiple benchmark datasets demonstrate that our adaptive model selection significantly enhances the trade-off between computational cost and safety performance, outperforming relevant baselines.

</details>


### [292] [Is This Collection Worth My LLM's Time? Automatically Measuring Information Potential in Text Corpora](https://arxiv.org/abs/2502.13691)

*Tristan Karch, Luca Engel, Philippe Schwaller, Frédéric Kaplan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information Gain, Multiple Choice Questions, Data Integration, Text Collections

**Relevance Score:** 9

**TL;DR:** An automated pipeline evaluates the potential information gain from text collections for LLMs by generating MCQs and measuring performance differences.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advance LLM performance by identifying valuable information sources that require significant investment for integration.

**Method:** An automated pipeline generates multiple choice questions from texts and measures LLM performance with and without access to source material to determine information potential.

**Key Contributions:**

	1. Automated evaluation of text collections for LLMs
	2. Introduction of MCQ generation as a performance measure
	3. Validation on diverse datasets including historical and synthetic data

**Result:** The method effectively identifies collections with valuable novel information across five datasets, showcasing its capability in prioritizing data acquisition efforts.

**Limitations:** 

**Conclusion:** This approach provides a practical tool for evaluating text collections' potential information gain for enhancing LLM performance.

**Abstract:** As large language models (LLMs) converge towards similar capabilities, the key to advancing their performance lies in identifying and incorporating valuable new information sources. However, evaluating which text collections are worth the substantial investment required for digitization, preprocessing, and integration into LLM systems remains a significant challenge. We present a novel approach to this challenge: an automated pipeline that evaluates the potential information gain from text collections without requiring model training or fine-tuning. Our method generates multiple choice questions (MCQs) from texts and measures an LLM's performance both with and without access to the source material. The performance gap between these conditions serves as a proxy for the collection's information potential. We validate our approach using five strategically selected datasets: EPFL PhD manuscripts, a private collection of Venetian historical records, two sets of Wikipedia articles on related topics, and a synthetic baseline dataset. Our results demonstrate that this method effectively identifies collections containing valuable novel information, providing a practical tool for prioritizing data acquisition and integration efforts.

</details>


### [293] [Vulnerability of Text-to-Image Models to Prompt Template Stealing: A Differential Evolution Approach](https://arxiv.org/abs/2502.14285)

*Yurong Wu, Fangwen Mu, Qiuhong Zhang, Jinjing Zhao, Xinrun Xu, Lingrui Mei, Yang Wu, Lin Shi, Junjie Wang, Zhiming Ding, Yiwei Wang*

**Main category:** cs.CL

**Keywords:** prompt trading, security vulnerability, template stealing, differential evolution, visual language models

**Relevance Score:** 6

**TL;DR:** The paper introduces EvoStealer, a method for stealing prompt templates from vendors using a limited number of sample images and evaluates its effectiveness against various models, highlighting significant performance improvements and low computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising concern of prompt trading and the security vulnerabilities associated with stealing prompt templates from visual language models.

**Method:** EvoStealer utilizes differential evolution algorithms to generate templates without fine-tuning, initializing population sets using multimodal large language models and iteratively enhancing them to derive generalized templates.

**Key Contributions:**

	1. Introduction of the Prism benchmark for prompt stealing
	2. Development of EvoStealer using differential evolution algorithms
	3. Demonstration of effective template stealing with low computational costs

**Result:** EvoStealer's stolen templates closely replicate original images and generalize well across subjects, outperforming baseline techniques by over 10% on average, while maintaining low computational costs.

**Limitations:** 

**Conclusion:** EvoStealer effectively demonstrates significant vulnerabilities of VLMs to prompt stealing, offering a substantial advancement over existing methods with minimal expense.

**Abstract:** Prompt trading has emerged as a significant intellectual property concern in recent years, where vendors entice users by showcasing sample images before selling prompt templates that can generate similar images. This work investigates a critical security vulnerability: attackers can steal prompt templates using only a limited number of sample images. To investigate this threat, we introduce Prism, a prompt-stealing benchmark consisting of 50 templates and 450 images, organized into Easy and Hard difficulty levels. To identify the vulnerabity of VLMs to prompt stealing, we propose EvoStealer, a novel template stealing method that operates without model fine-tuning by leveraging differential evolution algorithms. The system first initializes population sets using multimodal large language models (MLLMs) based on predefined patterns, then iteratively generates enhanced offspring through MLLMs. During evolution, EvoStealer identifies common features across offspring to derive generalized templates. Our comprehensive evaluation conducted across open-source (INTERNVL2-26B) and closed-source models (GPT-4o and GPT-4o-mini) demonstrates that EvoStealer's stolen templates can reproduce images highly similar to originals and effectively generalize to other subjects, significantly outperforming baseline methods with an average improvement of over 10%. Moreover, our cost analysis reveals that EvoStealer achieves template stealing with negligible computational expenses. Our code and dataset are available at https://github.com/whitepagewu/evostealer.

</details>


### [294] [FANformer: Improving Large Language Models Through Effective Periodicity Modeling](https://arxiv.org/abs/2502.21309)

*Yihong Dong, Ge Li, Xue Jiang, Yongding Tao, Kechi Zhang, Hao Zhu, Huanyu Liu, Jiazheng Ding, Jia Li, Jinliang Deng, Hong Mei*

**Main category:** cs.CL

**Keywords:** FANformer, Transformer, periodicity modeling, language models, Fourier Analysis Network

**Relevance Score:** 9

**TL;DR:** This paper presents FANformer, a new architecture that integrates Fourier Analysis Network into Transformer models to enhance periodicity modeling, leading to improved learning efficiency and performance in large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the flaws of periodicity modeling in Transformers that hinder learning efficiency in large language models.

**Method:** FANformer adapts Fourier Analysis Network (FAN) into the attention mechanism of Transformers by modifying the feature projection process, enhancing periodicity modeling.

**Key Contributions:**

	1. Introduced FANformer architecture for improved periodicity modeling
	2. Demonstrated superior performance over Transformers in various tasks
	3. Revealed enhanced reasoning abilities in FANformer compared to traditional Transformers.

**Result:** FANformer consistently outperforms Transformer in language modeling tasks as model size and training tokens increase, showing marked improvements on downstream tasks.

**Limitations:** 

**Conclusion:** FANformer is an effective and promising architecture for advancing large language models due to its enhanced learning efficiency and reasoning capabilities.

**Abstract:** Periodicity, as one of the most important basic characteristics, lays the foundation for facilitating structured knowledge acquisition and systematic cognitive processes within human learning paradigms. However, the potential flaws of periodicity modeling in Transformer affect the learning efficiency and establishment of underlying principles from data for large language models (LLMs) built upon it. In this paper, we demonstrate that integrating effective periodicity modeling can improve the learning efficiency and performance of LLMs. We introduce FANformer, which adapts Fourier Analysis Network (FAN) into attention mechanism to achieve efficient periodicity modeling, by modifying the feature projection process of attention mechanism. Extensive experimental results on language modeling show that FANformer consistently outperforms Transformer when scaling up model size and training tokens, underscoring its superior learning efficiency. Our pretrained FANformer-1B exhibits marked improvements on downstream tasks compared to open-source LLMs with similar model parameters or training tokens. Moreover, we reveal that FANformer exhibits superior ability to learn and apply rules for reasoning compared to Transformer. The results position FANformer as an effective and promising architecture for advancing LLMs.

</details>


### [295] [MoSE: Hierarchical Self-Distillation Enhances Early Layer Embeddings](https://arxiv.org/abs/2503.03008)

*Andrea Gurioli, Federico Pennino, João Monteiro, Maurizio Gabbrielli*

**Main category:** cs.CL

**Keywords:** ModularStarEncoder, Self-Distillation, code retrieval, code classification, text-to-code

**Relevance Score:** 6

**TL;DR:** MoSE introduces a 1-billion-parameter multi-exit encoder for code retrieval and classification, enhancing model performance via Self-Distillation while minimizing deployment costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of accuracy vs. performance trade-offs in deploying language models for tasks like code retrieval and classification, especially under latency constraints.

**Method:** The paper presents ModularStarEncoder (MoSE), which uses a Self-Distillation mechanism to improve lower-layer representations and include a repository-level contextual loss to enhance training context usage.

**Key Contributions:**

	1. Introduction of ModularStarEncoder (MoSE) for flexible model deployment.
	2. Self-Distillation mechanism to enhance intermediate representations.
	3. Creation of a new dataset for text-to-code and code-to-code benchmarks.

**Result:** MoSE improves text-to-code and code-to-code search by optimizing encoder layer usage, yielding better performance in code understanding tasks with minimal additional costs.

**Limitations:** 

**Conclusion:** Self-Distillation serves as an effective method for trading inference cost for accuracy, demonstrated through improvements in various code understanding challenges.

**Abstract:** Deploying language models often requires navigating accuracy vs. performance trade-offs to meet latency constraints while preserving utility. Traditional model distillation reduces size but incurs substantial costs through training separate models. We introduce ModularStarEncoder (MoSE), a 1-billion-parameter multi-exit encoder for code retrieval and classification that employs a novel Self-Distillation mechanism. This approach significantly enhances lower-layer representations, enabling flexible deployment of different model portions with favorable performance trade-offs. Our architecture improves text-to-code and code-to-code search by targeting specific encoder layers as exit heads, where higher layers guide earlier ones during training-improving intermediate representations at minimal additional cost. We further enhance MoSE with a repository-level contextual loss that maximizes training context window utilization. Additionally, we release a new dataset created through code translation that extends text-to-code benchmarks with cross-language code-to-code pairs. Evaluations demonstrate the effectiveness of Self-Distillation as a principled approach to trading inference cost for accuracy across various code understanding tasks.

</details>


### [296] [Can Frontier LLMs Replace Annotators in Biomedical Text Mining? Analyzing Challenges and Exploring Solutions](https://arxiv.org/abs/2503.03261)

*Yichong Zhao, Susumu Goto*

**Main category:** cs.CL

**Keywords:** biomedical text mining, LLMs, prompt engineering, annotation guidelines, model distillation

**Relevance Score:** 9

**TL;DR:** This study analyzes challenges faced by LLMs in biomedical text mining and develops prompt engineering techniques to improve their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the suboptimal performance of LLMs in biomedical text mining and to identify key challenges impacting their effectiveness.

**Method:** The authors analyzed failure patterns in evaluations and experimented with prompt engineering techniques, developing a pipeline that extracts instructions from annotation guidelines.

**Key Contributions:**

	1. Identification of challenges faced by LLMs in biomedical text mining
	2. Development of a prompt engineering pipeline for LLMs
	3. Demonstration of LLMs' capability to approach SOTA performance without fine-tuning

**Result:** Frontier LLMs can match or exceed the performance of state-of-the-art BERT-based models with minimal reliance on manually annotated data and no fine-tuning; model distillation on a closed-source LLM achieved practical performance using synthetic data.

**Limitations:** Further exploration is needed to fully validate the application of LLMs in diverse biomedical contexts and to understand long-term impacts on data quality.

**Conclusion:** There is potential to partially replace manual annotation with LLMs in biomedical text mining scenarios, improving efficiency in the annotation workflow.

**Abstract:** Multiple previous studies have reported suboptimal performance of LLMs in biomedical text mining. By analyzing failure patterns in these evaluations, we identified three primary challenges for LLMs in biomedical corpora: (1) LLMs fail to learn implicit dataset-specific nuances from supervised data, (2) The common formatting requirements of discriminative tasks limit the reasoning capabilities of LLMs particularly for LLMs that lack test-time compute, and (3) LLMs struggle to adhere to annotation guidelines and match exact schemas, which hinders their ability to understand detailed annotation requirements which is essential in biomedical annotation workflow. We experimented with prompt engineering techniques targeted to the above issues, and developed a pipeline that dynamically extracts instructions from annotation guidelines. Our results show that frontier LLMs can approach or surpass the performance of SOTA BERT-based models with minimal reliance on manually annotated data and without fine-tuning. Furthermore, we performed model distillation on a closed-source LLM, demonstrating that a BERT model trained exclusively on synthetic data annotated by LLMs can also achieve a practical performance. Based on these findings, we explored the feasibility of partially replacing manual annotation with LLMs in production scenarios for biomedical text mining.

</details>


### [297] [SCoRE: Benchmarking Long-Chain Reasoning in Commonsense Scenarios](https://arxiv.org/abs/2503.06218)

*Weidong Zhan, Yue Wang, Nan Hu, Liming Xiao, Jingyuan Ma, Yuhang Qin, Zheng Li, Yixin Yang, Sirui Deng, Jinkun Ding, Wenhan Ma, Rui Li, Weilin Luo, Qun Liu, Zhifang Sui*

**Main category:** cs.CL

**Keywords:** Commonsense Reasoning, Large Language Models, Benchmark, Natural Language Processing, Multi-hop Reasoning

**Relevance Score:** 8

**TL;DR:** Introduction of SCoRE, a benchmark for evaluating long-chain commonsense reasoning in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Long-chain reasoning is challenging for LLMs due to inadequate explicit reasoning data. Existing benchmarks are limited.

**Method:** SCoRE synthesizes multi-hop questions from scenario schemas of entities, relations, and logical rules.

**Key Contributions:**

	1. Introduction of a new benchmark for commonsense reasoning in LLMs
	2. Includes 100k bilingual multiple-choice questions with various difficulty levels
	3. Provides explicit reasoning chains and diagnostic evaluation labels

**Result:** Evaluation shows top models only achieving 69.78% accuracy, revealing errors stemming from rare knowledge and logical inconsistencies.

**Limitations:** Current models struggle with accuracy; high errors arise from rare knowledge and logical inconsistencies.

**Conclusion:** SCoRE provides a scalable framework for evaluating and improving long-chain commonsense reasoning in LLMs.

**Abstract:** Currently, long-chain reasoning remains a key challenge for large language models (LLMs) because natural texts lack sufficient explicit reasoning data. However, existing benchmarks suffer from limitations such as narrow coverage, short reasoning paths, or high construction costs. We introduce SCoRE (Scenario-based Commonsense Reasoning Evaluation), a benchmark that synthesizes multi-hop questions from scenario schemas of entities, relations, and logical rules to assess long-chain commonsense reasoning. SCoRE contains 100k bilingual (Chinese-English) multiple-choice questions whose reasoning chains span 2-11 hops and are grouped into various difficulty levels. Each question is accompanied by fine-grained knowledge labels, explicit reasoning chains, and difficulty levels for diagnostic evaluation. Evaluation results on cutting-edge LLMs such as o3-mini and Deepseek R1 shows that even the best model attains only 69.78% accuracy on SCoRE (even only 47.91% on the hard set), with errors often stemming from rare knowledge, logical inconsistency, and over-interpretation of simple questions. SCoRE offers a scalable, extensible framework for evaluating and diagnosing the long-chain commonsense reasoning abilities of LLMs and guiding future advances in model design and training.

</details>


### [298] [PolyPythias: Stability and Outliers across Fifty Language Model Pre-Training Runs](https://arxiv.org/abs/2503.09543)

*Oskar van der Wal, Pietro Lesci, Max Muller-Eberstein, Naomi Saphra, Hailey Schoelkopf, Willem Zuidema, Stella Biderman*

**Main category:** cs.CL

**Keywords:** Language Models, Pre-training Stability, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper investigates the stability of pre-training in language models, particularly focusing on the Pythia model with new training runs to analyze the effects of initial conditions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to fill the gap in understanding how variations in initial conditions affect the performance of decoder-only language models during pre-training.

**Method:** The authors introduced 45 new training runs for the Pythia model suite across different seeds and model sizes, allowing them to analyze training dynamics and stability across conditions.

**Key Contributions:**

	1. Introduction of PolyPythias training runs for systematic study
	2. Analysis of pre-training stability on downstream performance
	3. Identification of outlier training runs and their characteristics

**Result:** The analyses indicate consistent training dynamics across different model sizes and initial conditions, revealing characteristics of outlier training runs and their implications for training stability.

**Limitations:** The study focuses only on the Pythia model suite and may not generalize to all types of language models.

**Conclusion:** The study suggests that the introduced methods can effectively predict the stability of training in language models, providing valuable tools for the research community.

**Abstract:** The stability of language model pre-training and its effects on downstream performance are still understudied. Prior work shows that the training process can yield significantly different results in response to slight variations in initial conditions, e.g., the random seed. Crucially, the research community still lacks sufficient resources and tools to systematically investigate pre-training stability, particularly for decoder-only language models. We introduce the PolyPythias, a set of 45 new training runs for the Pythia model suite: 9 new seeds across 5 model sizes, from 14M to 410M parameters, resulting in about 7k new checkpoints that we release. Using these new 45 training runs, in addition to the 5 already available, we study the effects of different initial conditions determined by the seed -- i.e., parameters' initialisation and data order -- on (i) downstream performance, (ii) learned linguistic representations, and (iii) emergence of training phases. In addition to common scaling behaviours, our analyses generally reveal highly consistent training dynamics across both model sizes and initial conditions. Further, the new seeds for each model allow us to identify outlier training runs and delineate their characteristics. Our findings show the potential of using these methods to predict training stability.

</details>


### [299] [Probabilistic Reasoning with LLMs for k-anonymity Estimation](https://arxiv.org/abs/2503.09674)

*Jonathan Zheng, Sauvik Das, Alan Ritter, Wei Xu*

**Main category:** cs.CL

**Keywords:** privacy risk estimation, language models, k-privacy, Bayesian networks, probabilistic reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces the BRANCH methodology for estimating the privacy risk of user-generated documents by quantifying k-privacy values using large language models (LLMs) and Bayesian networks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of estimating privacy risks in user-generated content by quantifying uncertainty and ambiguity in decision-making processes related to privacy-sensitive information.

**Method:** BRANCH factorizes the joint probability distribution of personal information as random variables, utilizing a Bayesian network to separately estimate the probability of each factor within a population and combine them to compute the final k-privacy value.

**Key Contributions:**

	1. Introduction of the BRANCH methodology for estimating k-privacy values
	2. Demonstration of the reliability of LLM uncertainty as an accuracy indicator
	3. Improvement in k-value estimation compared to existing models

**Result:** The BRANCH method estimates the k-privacy value correctly 73% of the time, outperforming the o3-mini model by 13%; it also demonstrates that LLM uncertainty is a predictive indicator of accuracy, with high-variance predictions being less accurate.

**Limitations:** The study focuses primarily on numerical reasoning under uncertainty; broader applicability and real-world testing may be needed for practical use.

**Conclusion:** The BRANCH methodology offers a significant step forward in estimating privacy risks associated with user-generated documents, showcasing the potential of LLMs and Bayesian networks in this domain.

**Abstract:** Probabilistic reasoning is a key aspect of both human and artificial intelligence that allows for handling uncertainty and ambiguity in decision-making. In this paper, we introduce a new numerical reasoning task under uncertainty for large language models, focusing on estimating the privacy risk of user-generated documents containing privacy-sensitive information. We propose BRANCH, a new LLM methodology that estimates the k-privacy value of a text-the size of the population matching the given information. BRANCH factorizes a joint probability distribution of personal information as random variables. The probability of each factor in a population is estimated separately using a Bayesian network and combined to compute the final k-value. Our experiments show that this method successfully estimates the k-value 73% of the time, a 13% increase compared to o3-mini with chain-of-thought reasoning. We also find that LLM uncertainty is a good indicator for accuracy, as high-variance predictions are 37.47% less accurate on average.

</details>


### [300] [UC-MOA: Utility-Conditioned Multi-Objective Alignment for Distributional Pareto-Optimality](https://arxiv.org/abs/2503.10669)

*Zelei Cheng, Xin-Qiang Cai, Yuting Tang, Pushi Zhang, Boming Yang, Masashi Sugiyama, Xinyu Xing*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Utility-Conditioned Multi-Objective Alignment

**Relevance Score:** 9

**TL;DR:** Introducing UC-MOA, a framework that transforms user preferences into tokens for better alignment of large language models with human values while reducing training costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RLHF methods struggle with capturing human preferences and face numerical sensitivities and high computational costs.

**Method:** The UC-MOA framework uses a set of non-linear utility functions to encode preferences into symbolic tokens for conditioning a single LLM.

**Key Contributions:**

	1. Introduction of a novel framework (UC-MOA) for aligning LLMs with human values.
	2. Utilization of non-linear utility functions to encode user preferences into tokens.
	3. Reduction of training costs while improving model alignment and performance.

**Result:** UC-MOA mitigates numerical reasoning issues and significantly reduces training overhead, resulting in models with better Pareto fronts and alignment in complex reward scenarios.

**Limitations:** 

**Conclusion:** The approach presents a promising alternative to existing RLHF methodologies by efficiently aligning LLMs with human multi-dimensional preferences.

**Abstract:** Reinforcement Learning from Human Feedback (RLHF) has become a cornerstone for aligning large language models (LLMs) with human values. However, existing approaches struggle to capture the multi-dimensional, distributional nuances of human preferences. Methods such as RiC that directly inject raw reward values into prompts face significant numerical sensitivity issues--for instance, LLMs may fail to distinguish between 9.11 and 9.8--while alternatives like MORLHF, Rewarded Soups, and MODPO incur high computational costs by training multiple models. In this work, we introduce Utility-Conditioned Multi-Objective Alignment (UC-MOA), a novel framework that overcomes these limitations. Our approach leverages a diverse set of strictly increasing, non-linear utility functions to transform user-specified preferences into symbolic tokens, which are then used to condition a single LLM. This design not only mitigates numerical reasoning challenges but also substantially reduces training overhead, yielding models that achieve superior Pareto fronts and robust alignment across complex reward dimensions.

</details>


### [301] [HICD: Hallucination-Inducing via Attention Dispersion for Contrastive Decoding to Mitigate Hallucinations in Large Language Models](https://arxiv.org/abs/2503.12908)

*Xinyan Jiang, Hang Ye, Yongxin Zhu, Xiaoying Zheng, Zikang Chen, Jun Gong*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, contrastive decoding, attention heads, factual accuracy

**Relevance Score:** 9

**TL;DR:** This paper presents HICD, a method to induce controlled hallucinations in Large Language Models for improving output accuracy in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in LLMs that generate contextually and factually incorrect outputs.

**Method:** HICD selects attention heads critical to predictions, induces hallucinations by dispersing their attention, and compares the outputs to refine performance.

**Key Contributions:**

	1. Introduction of HICD for controlled hallucinations in LLMs
	2. Selection of attention heads for inducing hallucinations
	3. Demonstrated improvements over existing methods in multiple tasks

**Result:** HICD significantly enhances performance on tasks like context completion and question answering by promoting factuality and contextual faithfulness.

**Limitations:** 

**Conclusion:** The method offers a controlled way to induce hallucinations, leading to improvements in LLM performance across different applications.

**Abstract:** Large Language Models (LLMs) often generate hallucinations, producing outputs that are contextually inaccurate or factually incorrect. We introduce HICD, a novel method designed to induce hallucinations for contrastive decoding to mitigate hallucinations. Unlike existing contrastive decoding methods, HICD selects attention heads crucial to the model's prediction as inducing heads, then induces hallucinations by dispersing attention of these inducing heads and compares the hallucinated outputs with the original outputs to obtain the final result. Our approach significantly improves performance on tasks requiring contextual faithfulness, such as context completion, reading comprehension, and question answering. It also improves factuality in tasks requiring accurate knowledge recall. We demonstrate that our inducing heads selection and attention dispersion method leads to more "contrast-effective" hallucinations for contrastive decoding, outperforming other hallucination-inducing methods. Our findings provide a promising strategy for reducing hallucinations by inducing hallucinations in a controlled manner, enhancing the performance of LLMs in a wide range of tasks.

</details>


### [302] [Unifying Text Semantics and Graph Structures for Temporal Text-attributed Graphs with Large Language Models](https://arxiv.org/abs/2503.14411)

*Siwei Zhang, Yun Xiong, Yateng Tang, Xi Chen, Zian Jia, Zehao Gu, Jiarong Xu, Jiawei Zhang*

**Main category:** cs.CL

**Keywords:** temporal graph neural networks, temporal text-attributed graphs, large language models

**Relevance Score:** 9

**TL;DR:** CROSS is a framework for modeling temporal text-attributed graphs, enhancing temporal semantics extraction using large language models and integrating semantic and structural information to improve performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Real-world temporal graphs often contain rich textual information which creates complexity in modeling. Existing temporal graph neural networks fail to effectively incorporate dynamic text semantics, leading to suboptimal performance.

**Method:** CROSS decomposes TTAG modeling into two phases: temporal semantics extraction and semantic-structural information unification, utilizing large language models for dynamic extraction of temporal semantics and a co-encoder for unifying these with structural data.

**Key Contributions:**

	1. Introduction of the CROSS framework for temporal text-attributed graph modeling
	2. Dynamic extraction of temporal semantics using large language models
	3. Integration of semantic and structural information for enhanced performance

**Result:** CROSS demonstrated state-of-the-art performance on multiple datasets, achieving a 24.7% absolute MRR gain in temporal link prediction and a 3.7% AUC gain in node classification in industrial applications.

**Limitations:** 

**Conclusion:** The CROSS framework shows significant improvements in processing temporal text-attributed graphs by leveraging LLMs for semantic understanding and enabling a cohesive representation of data.

**Abstract:** Temporal graph neural networks (TGNNs) have shown remarkable performance in temporal graph modeling. However, real-world temporal graphs often possess rich textual information, giving rise to temporal text-attributed graphs (TTAGs). Such combination of dynamic text semantics and evolving graph structures introduces heightened complexity. Existing TGNNs embed texts statically and rely heavily on encoding mechanisms that biasedly prioritize structural information, overlooking the temporal evolution of text semantics and the essential interplay between semantics and structures for synergistic reinforcement. To tackle these issues, we present \textbf{CROSS}, a flexible framework that seamlessly extends existing TGNNs for TTAG modeling. CROSS is designed by decomposing the TTAG modeling process into two phases: (i) temporal semantics extraction; and (ii) semantic-structural information unification. The key idea is to advance the large language models (LLMs) to dynamically extract the temporal semantics in text space and then generate cohesive representations unifying both semantics and structures. Specifically, we propose a Temporal Semantics Extractor in the CROSS framework, which empowers LLMs to offer the temporal semantic understanding of node's evolving contexts of textual neighborhoods, facilitating semantic dynamics. Subsequently, we introduce the Semantic-structural Co-encoder, which collaborates with the above Extractor for synthesizing illuminating representations by jointly considering both semantic and structural information while encouraging their mutual reinforcement. Extensive experiments show that CROSS achieves state-of-the-art results on four public datasets and one industrial dataset, with 24.7% absolute MRR gain on average in temporal link prediction and 3.7% AUC gain in node classification of industrial application.

</details>


### [303] [Challenging the Boundaries of Reasoning: An Olympiad-Level Math Benchmark for Large Language Models](https://arxiv.org/abs/2503.21380)

*Haoxiang Sun, Yingqian Min, Zhipeng Chen, Wayne Xin Zhao, Lei Fang, Zheng Liu, Zhongyuan Wang, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** mathematical reasoning, LLMs, benchmark, bilingual assessment, evaluation

**Relevance Score:** 5

**TL;DR:** OlymMATH is a new Olympiad-level mathematical benchmark aimed at evaluating the reasoning capabilities of large language models (LLMs) with 200 curated problems across two difficulty tiers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in existing benchmarks for evaluating mathematical reasoning in large language models, which have become saturated and ineffective.

**Method:** The benchmark includes 200 manually verified mathematical problems, organized into easy (AIME-level) and hard (challenging) tiers, covering four core mathematical fields with an emphasis on bilingual assessment.

**Key Contributions:**

	1. Introduction of OlymMATH as a new benchmark for LLMs
	2. 200 rigorously curated mathematical problems
	3. Bilingual assessment capability in English and Chinese

**Result:** Empirical results demonstrate that state-of-the-art models struggle significantly with the hard problems, indicating the benchmark's difficulty.

**Limitations:** 

**Conclusion:** The OlymMATH benchmark presents a rigorous and challenging evaluation framework for LLMs, facilitating bilingual assessment and highlighting the limitations of current models.

**Abstract:** In recent years, the rapid development of large reasoning models has resulted in the saturation of existing benchmarks for evaluating mathematical reasoning, highlighting the urgent need for more challenging and rigorous evaluation frameworks. To address this gap, we introduce OlymMATH, a novel Olympiad-level mathematical benchmark, designed to rigorously test the complex reasoning capabilities of LLMs. OlymMATH features 200 meticulously curated problems, each manually verified and available in parallel English and Chinese versions. The problems are systematically organized into two distinct difficulty tiers: (1) AIME-level problems (easy) that establish a baseline for mathematical reasoning assessment, and (2) significantly more challenging problems (hard) designed to push the boundaries of current state-of-the-art models. In our benchmark, these problems span four core mathematical fields, each including a verifiable numerical solution to enable objective, rule-based evaluation. Empirical results underscore the significant challenge presented by OlymMATH, with state-of-the-art models including DeepSeek-R1, OpenAI's o3-mini and Gemini 2.5 Pro Exp demonstrating notably limited accuracy on the hard subset. Furthermore, the benchmark facilitates comprehensive bilingual assessment of mathematical reasoning abilities-a critical dimension that remains largely unaddressed in mainstream mathematical reasoning benchmarks. We release the benchmark, evaluation code, detailed results and a data visualization tool at https://github.com/RUCAIBox/OlymMATH.

</details>


### [304] [ReaRAG: Knowledge-guided Reasoning Enhances Factuality of Large Reasoning Models with Iterative Retrieval Augmented Generation](https://arxiv.org/abs/2503.21729)

*Zhicheng Lee, Shulin Cao, Jinxin Liu, Jiajie Zhang, Weichuan Liu, Xiaoyin Che, Lei Hou, Juanzi Li*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Factuality, Retrieval-Augmented Generation, Question Answering, Reasoning

**Relevance Score:** 8

**TL;DR:** ReaRAG improves factual accuracy and reasoning in Large Reasoning Models (LRMs) by utilizing a structured query approach and a novel action selection framework for enhanced question answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LRMs struggle with factual accuracy and reasoning robustness in question answering tasks, leading to suboptimal performance.

**Method:** We propose ReaRAG, which combines a data construction framework with an upper bound on reasoning chain length, allowing LRMs to iterate through searches and reasoning steps effectively.

**Key Contributions:**

	1. Novel action space for search and finish actions in reasoning
	2. Factuality-enhanced reasoning model
	3. Improved performance on multi-hop question answering tasks

**Result:** ReaRAG outperforms existing models on multi-hop question answering tasks and demonstrates the ability to recognize errors during reasoning.

**Limitations:** 

**Conclusion:** The integration of strong reasoning capabilities with factuality in ReaRAG significantly enhances the performance of LRMs in Retrieval-Augmented Generation applications.

**Abstract:** Large Reasoning Models (LRMs) exhibit remarkable reasoning abilities but rely primarily on parametric knowledge, limiting factual accuracy. While recent works equip reinforcement learning (RL)-based LRMs with retrieval capabilities, they suffer from overthinking and lack robustness in reasoning, reducing their effectiveness in question answering (QA) tasks. To address this, we propose ReaRAG, a factuality-enhanced reasoning model that explores diverse queries without excessive iterations. Our solution includes a novel data construction framework with an upper bound on the reasoning chain length. Specifically, we first leverage an LRM to generate deliberate thinking, then select an action from a predefined action space (Search and Finish). For Search action, a query is executed against the RAG engine, where the result is returned as observation to guide reasoning steps later. This process iterates until a Finish action is chosen. Benefiting from ReaRAG's strong reasoning capabilities, our approach outperforms existing baselines on multi-hop QA. Further analysis highlights its strong reflective ability to recognize errors and refine its reasoning trajectory. Our study enhances LRMs' factuality while effectively integrating robust reasoning for Retrieval-Augmented Generation (RAG).

</details>


### [305] [ImF: Implicit Fingerprint for Large Language Models](https://arxiv.org/abs/2503.21805)

*Wu jiaxuan, Peng Wanli, Fu hang, Xue Yiming, Wen juan*

**Main category:** cs.CL

**Keywords:** large language models, fingerprinting, adversarial attacks, intellectual property, stealthiness

**Relevance Score:** 9

**TL;DR:** This paper addresses the vulnerabilities in current fingerprinting techniques for large language models (LLMs) and proposes a novel method called Implicit Fingerprints (ImF) to protect intellectual property more effectively.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The high resource demands and costs of training LLMs make IP protection essential, yet existing methods for embedding fingerprints lack semantic coherence and are prone to attacks.

**Method:** The authors propose a new fingerprinting paradigm called Implicit Fingerprints, which utilizes steganography techniques to embed ownership information into texts naturally, integrating seamlessly with LLM outputs.

**Key Contributions:**

	1. Introduction of the Generation Revision Intervention (GRI) attack to demonstrate flaws in existing methods.
	2. Development of the Implicit Fingerprints (ImF) paradigm leveraging steganography for better IP protection.
	3. Empirical validation of ImF across diverse LLM models, showcasing improved robustness.

**Result:** The proposed ImF method was rigorously evaluated on 15 different LLMs, demonstrating better resilience against adversarial attacks compared to traditional methods.

**Limitations:** 

**Conclusion:** Implicit Fingerprints significantly improve the robustness of model ownership protection in LLMs, maintaining natural output quality and reducing vulnerability to adversarial interventions.

**Abstract:** Training large language models (LLMs) is resource-intensive and expensive, making protecting intellectual property (IP) for LLMs crucial. Recently, embedding fingerprints into LLMs has emerged as a prevalent method for establishing model ownership. However, existing fingerprinting techniques typically embed identifiable patterns with weak semantic coherence, resulting in fingerprints that significantly differ from the natural question-answering (QA) behavior inherent to LLMs. This discrepancy undermines the stealthiness of the embedded fingerprints and makes them vulnerable to adversarial attacks. In this paper, we first demonstrate the critical vulnerability of existing fingerprint embedding methods by introducing a novel adversarial attack named Generation Revision Intervention (GRI) attack. GRI attack exploits the semantic fragility of current fingerprinting methods, effectively erasing fingerprints by disrupting their weakly correlated semantic structures. Our empirical evaluation highlights that traditional fingerprinting approaches are significantly compromised by the GRI attack, revealing severe limitations in their robustness under realistic adversarial conditions. To advance the state-of-the-art in model fingerprinting, we propose a novel model fingerprint paradigm called Implicit Fingerprints (ImF). ImF leverages steganography techniques to subtly embed ownership information within natural texts, subsequently using Chain-of-Thought (CoT) prompting to construct semantically coherent and contextually natural QA pairs. This design ensures that fingerprints seamlessly integrate with the standard model behavior, remaining indistinguishable from regular outputs and substantially reducing the risk of accidental triggering and targeted removal. We conduct a comprehensive evaluation of ImF on 15 diverse LLMs, spanning different architectures and varying scales.

</details>


### [306] [Why Stop at One Error? Benchmarking LLMs as Data Science Code Debuggers for Multi-Hop and Multi-Bug Errors](https://arxiv.org/abs/2503.22388)

*Zhiyu Yang, Shuo Wang, Yukun Yan, Yang Deng*

**Main category:** cs.CL

**Keywords:** LLMs, data science, benchmark, debugging, machine learning

**Relevance Score:** 8

**TL;DR:** Introduction of DSDBench, a benchmark for evaluating LLMs in debugging complex data science code.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a systematic way to evaluate LLMs on their ability to identify and fix logical runtime errors in data science code, as existing benchmarks focus on simpler tasks.

**Method:** DSDBench adapts datasets from existing benchmarks like DABench and MatPlotBench, featuring multi-hop, multi-bug debugging tasks with 1,117 annotated samples and 741 cause-effect error pairs.

**Key Contributions:**

	1. Introduction of DSDBench as the first benchmark for multi-hop error tracing in data science code.
	2. Inclusion of a substantial dataset with realistic debugging tasks including multi-bug scenarios.
	3. Highlighting the performance gaps in current LLMs when faced with complex debugging tasks.

**Result:** Evaluations reveal significant performance gaps in state-of-the-art LLMs when debugging logical errors, highlighting the challenges faced in this area.

**Limitations:** 

**Conclusion:** DSDBench serves as a vital resource for evaluating and enhancing LLMs' debugging capabilities, promoting more reliable AI-assisted data science.

**Abstract:** LLMs are transforming software development, yet current code generation and code repair benchmarks mainly assess syntactic and functional correctness in simple, single-error cases. LLMs' capabilities to autonomously find and fix runtime logical errors in complex data science code remain largely unexplored. To address this gap, we introduce DSDBench: the Data Science Debugging Benchmark, the first benchmark for systematic evaluation of LLMs on multi-hop error tracing and multi-bug detection in data science code debugging. DSDBench adapts datasets from existing data science task benchmarks, such as DABench and MatPlotBench, featuring realistic data science debugging tasks with automatically synthesized multi-hop, multi-bug code snippets. DSDBench includes 1,117 annotated samples with 741 cause-effect error pairs and runtime error messages. Evaluations of state-of-the-art LLMs on DSDBench show significant performance gaps, highlighting challenges in debugging logical runtime errors in data science code. DSDBench offers a crucial resource to evaluate and improve LLMs' debugging and reasoning capabilities, enabling more reliable AI-assisted data science in the future. DSDBench is publicly available at github.com/KevinCL16/DSDBench.

</details>


### [307] [RARE: Retrieval-Augmented Reasoning Modeling](https://arxiv.org/abs/2503.23513)

*Zhengren Wang, Jiayang Yu, Dongsheng Ma, Zhe Chen, Yu Wang, Zhiyu Li, Feiyu Xiong, Yanfeng Wang, Weinan E, Linpeng Tang, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Reasoning Modeling, knowledge hallucination, domain-specific intelligence, machine learning, large language models

**Relevance Score:** 8

**TL;DR:** RARE improves domain-specific intelligence in LLMs by decoupling knowledge storage from reasoning optimization, enabling better contextualized reasoning through retrieval-augmented training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in handling domain-specific tasks due to knowledge hallucination and reasoning inefficiencies under constrained parameter budgets.

**Method:** RARE externalizes domain knowledge to retrievable sources while internalizing reasoning patterns during training, using retrieved knowledge in prompts and masked losses to shift learning from memorization to reasoning.

**Key Contributions:**

	1. Decoupling of knowledge storage and reasoning optimization in LLMs
	2. Use of retrieval-augmented training to enhance reasoning capabilities
	3. State-of-the-art performance metrics surpassing existing models

**Result:** RARE-trained models achieve state-of-the-art performance, surpassing traditional models like GPT-4 and DeepSeek-R1 by up to 20% in accuracy.

**Limitations:** 

**Conclusion:** RARE represents a paradigm shift, allowing for scalable domain-specific intelligence through the combination of external knowledge bases and reasoning-optimized models.

**Abstract:** Domain-specific intelligence demands specialized knowledge and sophisticated reasoning for problem-solving, posing significant challenges for large language models (LLMs) that struggle with knowledge hallucination and inadequate reasoning capabilities under constrained parameter budgets. Inspired by Bloom's Taxonomy in educational theory, we propose Retrieval-Augmented Reasoning Modeling (RARE), a novel paradigm that decouples knowledge storage from reasoning optimization. RARE externalizes domain knowledge to retrievable sources and internalizes domain-specific reasoning patterns during training. Specifically, by injecting retrieved knowledge into training prompts with masked losses, RARE transforms learning objectives from rote memorization to contextualized reasoning. It enables models to bypass parameter-intensive memorization and prioritize the development of higher-order cognitive processes. Extensive experiments demonstrate that lightweight RARE-trained models (e.g., Llama-3.1-8B) could achieve state-of-the-art performance, surpassing retrieval-augmented GPT-4 and DeepSeek-R1 up to approximately 20\% accuracy. RARE establishes a paradigm shift where maintainable external knowledge bases synergize with compact, reasoning-optimized models, collectively driving more scalable domain-specific intelligence.

</details>


### [308] [FISH-Tuning: Enhancing PEFT Methods with Fisher Information](https://arxiv.org/abs/2504.04050)

*Kang Xue, Ming Dong, Xinhui Tu, Tingting He*

**Main category:** cs.CL

**Keywords:** Large Language Models, Parameter-Efficient Fine-Tuning, Fisher Information

**Relevance Score:** 9

**TL;DR:** FISH-Tuning integrates Fisher Induced Sparse uncHanging Mask with existing PEFT methods to optimize fine-tuning of Large Language Models, enhancing performance while reducing resource costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient fine-tuning methods in the face of increasing LLM parameter sizes and computational costs.

**Method:** FISH-Tuning utilizes Fisher information to select and fine-tune only the most impactful parameters from methods like LoRA and Adapter, aiming to improve performance with lower resource consumption.

**Key Contributions:**

	1. Introduction of FISH-Tuning for PEFT methods.
	2. Demonstration of significant performance gains over vanilla approaches.
	3. Evidence of reduced resource consumption during fine-tuning.

**Result:** Experimental results indicate that FISH-Tuning consistently outperforms vanilla PEFT methods, achieving better performance without added training time or inference latency.

**Limitations:** 

**Conclusion:** FISH-Tuning offers a practical improvement to parameter-efficient fine-tuning by selectively targeting important parameters, providing benefits in both efficiency and effectiveness.

**Abstract:** The rapid growth in the parameter size of Large Language Models (LLMs) has spurred the development of Parameter-Efficient Fine-Tuning (PEFT) methods to mitigate the substantial computational costs of fine-tuning. Among these, Fisher Induced Sparse uncHanging (FISH) Mask is a selection-based PEFT technique that identifies a critical subset of pre-trained parameters using approximate Fisher information. While addition-based and reparameterization-based PEFT methods like LoRA and Adapter already fine-tune only a small number of parameters, the newly introduced parameters within these methods themselves present an opportunity for further optimization. Selectively fine-tuning only the most impactful among these new parameters could further reduce resource consumption while maintaining, or even improving, fine-tuning effectiveness. In this paper, we propose \textbf{FISH-Tuning}, a novel approach that incorporates FISH Mask into such PEFT methods, including LoRA, Adapter, and their variants. By leveraging Fisher information to identify and update only the most significant parameters within these added or reparameterized components, FISH-Tuning aims to achieve superior performance without increasing training time or inference latency compared to the vanilla PEFT methods. Experimental results across various datasets and pre-trained models demonstrate that FISH-Tuning consistently outperforms the vanilla PEFT methods when using the same proportion of trainable parameters. Code is available at https://anonymous.4open.science/r/FISH-Tuning-6F7C.

</details>


### [309] [Leveraging Robust Optimization for LLM Alignment under Distribution Shifts](https://arxiv.org/abs/2504.05831)

*Mingye Zhu, Yi Liu, Zheren Fu, Yongdong Zhang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** preference alignment, large language models, distribution-aware optimization, human values, synthetic data

**Relevance Score:** 9

**TL;DR:** The paper presents a distribution-aware optimization framework to enhance preference alignment in large language models despite reliance on synthetic data, addressing the challenges of distribution shifts in reflecting human values.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of large language models with human preferences by addressing the distribution shifts introduced by using synthetic data for training.

**Method:** The framework assigns calibration values to training samples based on their alignment with a human-preferred distribution, and incorporates these values into a robust optimization objective to focus on minimizing losses in relevant data regions.

**Key Contributions:**

	1. Introduction of a novel distribution-aware optimization framework
	2. Utilization of well-learned classifiers for assigning calibration values
	3. Improvement in generating outputs that reflect intended human values despite data shifts.

**Result:** The proposed method reduces the impact of distributional mismatch and enhances the generation of responses that reflect human values more accurately.

**Limitations:** 

**Conclusion:** By focusing optimization efforts on the target distribution, the framework effectively improves the output of large language models in terms of alignment with human preferences.

**Abstract:** Preference alignment methods are increasingly critical for steering large language models (LLMs) to generate outputs consistent with human values. While recent approaches often rely on synthetic data generated by LLMs for scalability and cost-efficiency reasons, this reliance can introduce distribution shifts that undermine the nuanced representation of human preferences needed for desirable outputs. In this paper, we propose a novel distribution-aware optimization framework that improves preference alignment despite such shifts. Our approach first leverages well-learned classifiers to assign a calibration value to each training sample, quantifying its alignment with the target human-preferred distribution. These values are then incorporated into a robust optimization objective that minimizes the worst-case loss over regions of the data space most relevant to human preferences. By explicitly focusing optimization on the target distribution, our approach mitigates the impact of distributional mismatch and improves the generation of responses that better reflect intended values.

</details>


### [310] [LSR-MCTS: Alleviating Long Range Dependency in Code Generation](https://arxiv.org/abs/2504.07433)

*Tingwei Lu, Yangning Li, Liyuan Wang, Binghuai Lin, Jiwei Tang, Qingsong Lv, Wanshi Xu, Hai-Tao Zheng, Yinghui Li, Xin Su, Zifei Shan*

**Main category:** cs.CL

**Keywords:** code generation, large language models, Monte Carlo Tree Search, self-refine mechanism, program quality

**Relevance Score:** 9

**TL;DR:** The paper proposes the LSR-MCTS algorithm for code generation that processes code line-by-line to enhance generation quality and diversity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues of redundancy and overfitting in current code generation tasks undertaken by large language models (LLMs).

**Method:** The LSR-MCTS algorithm employs Monte Carlo Tree Search (MCTS) to generate code sequentially, line-by-line, while implementing a self-refine mechanism at each node to improve diversity and quality.

**Key Contributions:**

	1. Introduction of the LSR-MCTS algorithm for sequential code generation
	2. Utilization of line-based processing as a fundamental unit for code generation
	3. Implementation of a self-refine mechanism for improving generation diversity and quality.

**Result:** Extensive experiments indicate that the LSR-MCTS algorithm achieves superior performance over state-of-the-art approaches on three public coding benchmarks.

**Limitations:** 

**Conclusion:** The proposed approach effectively addresses redundancy in code generation and enhances the quality of generated programs through a structured line-by-line generation strategy.

**Abstract:** The emergence of large language models (LLMs) has significantly promoted the development of code generation task, sparking a surge in pertinent literature. Current research is hindered by redundant generation results and a tendency to overfit local patterns in the short term. Although existing studies attempt to alleviate the issue by adopting a multi-token prediction strategy, there remains limited focus on choosing the appropriate processing length for generations. By analyzing the attention between tokens during the generation process of LLMs, it can be observed that the high spikes of the attention scores typically appear at the end of lines. This insight suggests that it is reasonable to treat each line of code as a fundamental processing unit and generate them sequentially. Inspired by this, we propose the \textbf{LSR-MCTS} algorithm, which leverages MCTS to determine the code line-by-line and select the optimal path. Further, we integrate a self-refine mechanism at each node to enhance diversity and generate higher-quality programs through error correction. Extensive experiments and comprehensive analyses on three public coding benchmarks demonstrate that our method outperforms the state-of-the-art performance approaches.

</details>


### [311] [Large Language Models Could Be Rote Learners](https://arxiv.org/abs/2504.08300)

*Yuyang Xu, Renjun Hu, Haochao Ying, Jian Wu, Xing Shi, Wei Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evaluation Framework, Memorization, Human-Computer Interaction, Machine Learning

**Relevance Score:** 9

**TL;DR:** This study addresses the reliability of multiple-choice question benchmarks for evaluating Large Language Models by proposing a novel evaluation framework called TrinEval, aimed at reducing memorization while maintaining knowledge assessment. 

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the evaluation of Large Language Models (LLMs) by addressing the issue of benchmark contamination, which affects the reliability of multiple-choice question (MCQ) evaluations.

**Method:** An analysis of LLM performance under varying memorization conditions was conducted, revealing that LLMs perform worse on memorized MCQs compared to non-memorized ones. The novel evaluation framework, TrinEval, reformulates MCQs into a trinity format to reduce memorization and better assess knowledge.

**Key Contributions:**

	1. Introduction of TrinEval evaluation framework
	2. Insights into LLM memorization vs. capability learning
	3. Quantification of memorization in LLMs (20.5% knowledge points memorized)

**Result:** TrinEval was validated through experiments, demonstrating its effectiveness in reducing memorization while enabling accurate knowledge assessment. It was found that common LLMs memorize approximately 20.5% of knowledge points in standard benchmarks such as MMLU.

**Limitations:** Results are based on specific conditions and may vary with different model architectures or datasets.

**Conclusion:** The findings indicate that careful evaluation is essential to distinguish between rote memorization and genuine learning in LLMs, with TrinEval providing a more reliable assessment method.

**Abstract:** Multiple-choice question (MCQ) benchmarks are widely used for evaluating Large Language Models (LLMs), yet their reliability is undermined by benchmark contamination. In this study, we reframe contamination as an inherent aspect of learning and seek to disentangle genuine capability acquisition from superficial memorization in LLM evaluation. First, by analyzing model performance under different memorization conditions, we uncover a counterintuitive trend: LLMs perform worse on memorized MCQs than on non-memorized ones, indicating the coexistence of two distinct learning phenomena, i.e., rote memorization and genuine capability learning. To disentangle them, we propose TrinEval, a novel evaluation framework reformulating MCQs into an alternative trinity format, reducing memorization while preserving knowledge assessment. Experiments validate TrinEval's effectiveness in reformulation, and its evaluation reveals that common LLMs may memorize by rote 20.5% of knowledge points (in MMLU on average).

</details>


### [312] [DioR: Adaptive Cognitive Detection and Contextual Retrieval Optimization for Dynamic Retrieval-Augmented Generation](https://arxiv.org/abs/2504.10198)

*Hanghui Guo, Jia Zhu, Shimin Di, Weijie Shi, Zhangze Chen, Jiajie Xu*

**Main category:** cs.CL

**Keywords:** Dynamic Retrieval-augmented Generation, Large Language Models, Adaptive Cognitive Detection

**Relevance Score:** 9

**TL;DR:** DioR is a novel dynamic retrieval-augmented generation method that enhances large language model performance by optimizing retrieval triggers and content.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing dynamic RAG methods struggle with controlling retrieval triggers and evaluating retrieval content, leading to hallucinations in LLMs.

**Method:** DioR comprises two components: adaptive cognitive detection to assess when retrieval is necessary, and contextual retrieval optimization to select appropriate content for retrieval.

**Key Contributions:**

	1. Introduces adaptive cognitive detection for improved control of retrieval triggers.
	2. Develops contextual retrieval optimization for better content selection.
	3. Demonstrates significant performance improvements over existing dynamic RAG methods.

**Result:** DioR demonstrates superior performance across all tested tasks, effectively addressing the limitations of current dynamic RAG approaches.

**Limitations:** 

**Conclusion:** The results show that DioR significantly improves LLM outputs and reduces hallucinations by optimizing the retrieval process.

**Abstract:** Dynamic Retrieval-augmented Generation (RAG) has shown great success in mitigating hallucinations in large language models (LLMs) during generation. However, existing dynamic RAG methods face significant limitations in two key aspects: 1) Lack of an effective mechanism to control retrieval triggers, and 2) Lack of effective scrutiny of retrieval content. To address these limitations, we propose an innovative dynamic RAG method, DioR (Adaptive Cognitive Detection and Contextual Retrieval Optimization), which consists of two main components: adaptive cognitive detection and contextual retrieval optimization, specifically designed to determine when retrieval is needed and what to retrieve for LLMs is useful. Experimental results demonstrate that DioR achieves superior performance on all tasks, demonstrating the effectiveness of our work.

</details>


### [313] [Semantic Similarity-Informed Bayesian Borrowing for Quantitative Signal Detection of Adverse Events](https://arxiv.org/abs/2504.12052)

*François Haguinet, Jeffery L Painter, Gregory E Powell, Andrea Callegaro, Andrew Bate*

**Main category:** cs.CL

**Keywords:** adverse events, Bayesian analysis, semantic similarity measures, spontaneous reporting systems, disproportionality analysis

**Relevance Score:** 6

**TL;DR:** This paper introduces a Bayesian dynamic borrowing approach to improve the detection of adverse events in spontaneous reporting systems using semantic similarity measures, showing superior performance compared to traditional methods.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective and sensitive identification of adverse events in spontaneous reporting systems due to limitations in existing disproportionality analysis methods.

**Method:** A Bayesian dynamic borrowing approach that integrates robust meta-analytic predictive priors with a Bayesian hierarchical model and utilizes semantic similarity measures for information sharing during adverse event detection.

**Key Contributions:**

	1. Introduction of Bayesian dynamic borrowing for adverse event detection
	2. Utilization of semantic similarity measures to enhance information sharing
	3. Demonstration of improved sensitivity and earlier detection of true positives over traditional methods.

**Result:** The proposed IC SSM method outperformed traditional Information Component analysis and other hierarchical approaches in terms of sensitivity and timeliness of detecting adverse events using FDA reporting data.

**Limitations:** The method may demonstrate a marginally lower aggregate F1-score and Youden's index compared to traditional methods in some contexts.

**Conclusion:** The IC SSM approach provides a more effective method for identifying adverse events, suggesting potential for broader applications with different datasets and similarity metrics.

**Abstract:** We present a Bayesian dynamic borrowing (BDB) approach to enhance the quantitative identification of adverse events (AEs) in spontaneous reporting systems (SRSs). The method embeds a robust meta-analytic predictive (MAP) prior with a Bayesian hierarchical model and incorporates semantic similarity measures (SSMs) to enable weighted information sharing from clinically similar MedDRA Preferred Terms (PTs) to the target PT. This continuous similarity-based borrowing overcomes limitations of rigid hierarchical grouping in current disproportionality analysis (DPA).   Using data from the FDA Adverse Event Reporting System (FAERS) between 2015 and 2019, we evaluate our approach -- termed IC SSM -- against traditional Information Component (IC) analysis and IC with borrowing at the MedDRA high-level group term level (IC HLGT). A reference set (PVLens), derived from FDA product label update, enabled prospective evaluation of method performance in identifying AEs prior to official labeling.   The IC SSM approach demonstrated higher sensitivity (1332/2337=0.570, Youden's J=0.246) than traditional IC (Se=0.501, J=0.250) and IC HLGT (Se=0.556, J=0.225), consistently identifying more true positives and doing so on average 5 months sooner than traditional IC. Despite a marginally lower aggregate F1-score and Youden's index, IC SSM showed higher performance in early post-marketing periods or when the detection threshold was raised, providing more stable and relevant alerts than IC HLGT and traditional IC.   These findings support the use of SSM-informed Bayesian borrowing as a scalable and context-aware enhancement to traditional DPA methods, with potential for validation across other datasets and exploration of additional similarity metrics and Bayesian strategies using case-level data.

</details>


### [314] [CoT-RAG: Integrating Chain of Thought and Retrieval-Augmented Generation to Enhance Reasoning in Large Language Models](https://arxiv.org/abs/2504.13534)

*Feiyang Li, Peng Fang, Zhan Shi, Arijit Khan, Fang Wang, Dan Feng, Weihao Wang, Xin Zhang, Yongjian Cui*

**Main category:** cs.CL

**Keywords:** Chain-of-thought reasoning, Knowledge Graphs, Retrieval-augmented generation, Large language models, Logical reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces CoT-RAG, a framework that enhances large language models' reasoning capabilities by utilizing knowledge graphs, learnable information retrieval, and structured execution. It shows significant performance improvements on various reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the unreliable nature of LLM-generated reasoning chains and their interference with LLM inference logic, aiming to enhance reasoning credibility and execution.

**Method:** The proposed CoT-RAG framework employs three key designs: Knowledge Graph-driven CoT Generation for improving reasoning chains, Learnable Knowledge Case-aware RAG for incorporating relevant sub-cases, and Pseudo-Program Prompting to improve logical execution.

**Key Contributions:**

	1. Introduction of Knowledge Graph-driven CoT Generation for better reasoning credibility.
	2. Integration of retrieval-augmented generation to enhance learnable information.
	3. Implementation of Pseudo-Program Prompting for structured logical execution.

**Result:** Evaluations demonstrate accuracy improvements ranging from 4.0% to 44.3% across nine datasets, along with exceptional performance in domain-specific tasks.

**Limitations:** 

**Conclusion:** CoT-RAG shows not only enhanced reasoning accuracy but also practical scalability for real-world applications in reasoning tasks.

**Abstract:** Chain-of-thought (CoT) reasoning boosts large language models' (LLMs) performance on complex tasks but faces two key limitations: a lack of reliability when solely relying on LLM-generated reasoning chains and interference from natural language reasoning steps with the models' inference process, also known as the inference logic of LLMs. To address these issues, we propose CoT-RAG, a novel reasoning framework with three key designs: (i) Knowledge Graph-driven CoT Generation,featuring knowledge graphs to modulate reasoning chain generation of LLMs, thereby enhancing reasoning credibility; (ii) Learnable Knowledge Case-aware RAG, which incorporates retrieval-augmented generation (RAG) into knowledge graphs to retrieve relevant sub-cases and sub-descriptions, providing LLMs with learnable information; (iii) Pseudo-Program Prompting Execution, which promotes greater logical rigor by guiding LLMs to execute reasoning tasks as pseudo-programs. Evaluations on nine public datasets spanning three reasoning tasks reveal significant accuracy gains--ranging from 4.0% to 44.3%--over state-of-the-art methods. Furthermore, tests on four domain-specific datasets demonstrate exceptional accuracy and efficient execution, underscoring its practical applicability and scalability.

</details>


### [315] [Multimodal Coreference Resolution for Chinese Social Media Dialogues: Dataset and Benchmark Approach](https://arxiv.org/abs/2504.14321)

*Xingyu Li, Chen Gong, Guohong Fu*

**Main category:** cs.CL

**Keywords:** multimodal coreference resolution, dataset, social media, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces TikTalkCoref, the first Chinese multimodal coreference dataset for real-world social media, focusing on the TikTok platform.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in multimodal coreference resolution research by providing a dataset for real-world dialogues and improving communication and personalization in social media contexts.

**Method:** The paper develops the TikTalkCoref dataset, which pairs short videos with user comment dialogues and includes annotated coreference clusters. An effective benchmark approach for multimodal coreference resolution focusing on the celebrity domain is also presented.

**Key Contributions:**

	1. Introduction of TikTalkCoref, a novel multimodal coreference dataset for Chinese social media.
	2. Manual annotations of coreference clusters for textual and visual modalities.
	3. Development of a benchmark approach for multimodal coreference resolution.

**Result:** Extensive experiments on the TikTalkCoref dataset yield reliable benchmark results, demonstrating its effectiveness for multimodal coreference tasks in social media dialogues.

**Limitations:** Focused only on the celebrity domain in social media; limited generalizability to other areas.

**Conclusion:** The TikTalkCoref dataset will enable better research in multimodal coreference resolution for real-world applications in social media, encouraging further exploration in this field.

**Abstract:** Multimodal coreference resolution (MCR) aims to identify mentions referring to the same entity across different modalities, such as text and visuals, and is essential for understanding multimodal content. In the era of rapidly growing mutimodal content and social media, MCR is particularly crucial for interpreting user interactions and bridging text-visual references to improve communication and personalization. However, MCR research for real-world dialogues remains unexplored due to the lack of sufficient data resources. To address this gap, we introduce TikTalkCoref, the first Chinese multimodal coreference dataset for social media in real-world scenarios, derived from the popular Douyin short-video platform. This dataset pairs short videos with corresponding textual dialogues from user comments and includes manually annotated coreference clusters for both person mentions in the text and the coreferential person head regions in the corresponding video frames. We also present an effective benchmark approach for MCR, focusing on the celebrity domain, and conduct extensive experiments on our dataset, providing reliable benchmark results for this newly constructed dataset. We will release the TikTalkCoref dataset to facilitate future research on MCR for real-world social media dialogues.

</details>


### [316] [Trans-Zero: Self-Play Incentivizes Large Language Models for Multilingual Translation Without Parallel Data](https://arxiv.org/abs/2504.14669)

*Wei Zou, Sen Yang, Yu Bao, Shujian Huang, Jiajun Chen, Shanbo Cheng*

**Main category:** cs.CL

**Keywords:** Multilingual Machine Translation, Large Language Models, Genetic Monte-Carlo Tree Search

**Relevance Score:** 9

**TL;DR:** TRANS-ZERO is a self-play framework for multilingual machine translation utilizing only monolingual data and LLM knowledge, achieving results comparable to supervised methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Multilingual machine translation faces challenges like data scarcity for low-resource languages and catastrophic forgetting, necessitating innovative solutions that reduce reliance on parallel data.

**Method:** TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, leveraging monolingual data to enhance translation quality through self-play.

**Key Contributions:**

	1. Introduction of TRANS-ZERO self-play framework
	2. Utilization of only monolingual data for multilingual MT
	3. Demonstration of G-MCTS improving translation quality

**Result:** Experiments show that TRANS-ZERO achieves strong translation performance, rivaling traditionally supervised methods particularly in non-English translation directions.

**Limitations:** 

**Conclusion:** The G-MCTS approach significantly improves translation quality by systematically exploring semantically consistent candidates, establishing a robust framework for future developments in multilingual MT.

**Abstract:** The rise of Large Language Models (LLMs) has reshaped machine translation (MT), but multilingual MT still relies heavily on parallel data for supervised fine-tuning (SFT), facing challenges like data scarcity for low-resource languages and catastrophic forgetting. To address these issues, we propose TRANS-ZERO, a self-play framework that leverages only monolingual data and the intrinsic multilingual knowledge of LLM. TRANS-ZERO combines Genetic Monte-Carlo Tree Search (G-MCTS) with preference optimization, achieving strong translation performance that rivals supervised methods. Experiments demonstrate that this approach not only matches the performance of models trained on large-scale parallel data but also excels in non-English translation directions. Further analysis reveals that G-MCTS itself significantly enhances translation quality by exploring semantically consistent candidates through iterative translations, providing a robust foundation for the framework's succuss.

</details>


### [317] [Dynamic Early Exit in Reasoning Models](https://arxiv.org/abs/2504.15895)

*Chenxu Yang, Qingyi Si, Yongjie Duan, Zheliang Zhu, Chenyu Zhu, Qiaowei Li, Zheng Lin, Li Cao, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Large reasoning language models, Chain-of-thought, Self-truncation, Dynamic termination, NLP

**Relevance Score:** 9

**TL;DR:** This paper introduces a method for large reasoning language models (LRLMs) that allows them to self-stop generating long chain-of-thought sequences when they reach confident answers, improving efficiency and accuracy without additional training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the inefficiencies and potential loss of accuracy associated with long chain-of-thought generation in LRLMs, where overthinking can hinder performance.

**Method:** The method monitors model behavior at reasoning transition points and dynamically terminates generation when the model shows high confidence in its answer, thus self-truncating the CoT sequences.

**Key Contributions:**

	1. Introduction of self-truncation in CoT sequences for LRLMs
	2. Dynamically monitors reasoning transitions to improve generation efficiency
	3. Demonstrated improvements on 11 reasoning benchmarks across varying LLM sizes

**Result:** Experiments on 10 reasoning benchmarks demonstrate that the method effectively reduces CoT sequence lengths by 19.1% to 80.1% while improving accuracy by 0.3% to 5.0% across various reasoning LLMs.

**Limitations:** 

**Conclusion:** The proposed method integrates seamlessly into existing reasoning LLMs, enhancing both efficiency and accuracy without the need for additional training.

**Abstract:** Recent advances in large reasoning language models (LRLMs) rely on test-time scaling, which extends long chain-of-thought (CoT) generation to solve complex tasks. However, overthinking in long CoT not only slows down the efficiency of problem solving, but also risks accuracy loss due to the extremely detailed or redundant reasoning steps. We propose a simple yet effective method that allows LLMs to self-truncate CoT sequences by early exit during generation. Instead of relying on fixed heuristics, the proposed method monitors model behavior at potential reasoning transition points (e.g.,"Wait" tokens) and dynamically terminates the next reasoning chain's generation when the model exhibits high confidence in a trial answer. Our method requires no additional training and can be seamlessly integrated into existing o1-like reasoning LLMs. Experiments on 10 reasoning benchmarks (e.g., GSM8K, MATH-500, AMC, GPQA, AIME and LiveCodeBench) show that the proposed method is consistently effective on 11 cutting-edge reasoning LLMs of varying series and sizes, reducing the length of CoT sequences by an average of 19.1% to 80.1% while improving accuracy by 0.3% to 5.0%.

</details>


### [318] [PHYBench: Holistic Evaluation of Physical Perception and Reasoning in Large Language Models](https://arxiv.org/abs/2504.16074)

*Shi Qiu, Shaoyang Guo, Zhuo-Yang Song, Yunbo Sun, Zeyu Cai, Jiashen Wei, Tianyu Luo, Yixuan Yin, Haoxu Zhang, Yi Hu, Chenyang Wang, Chencheng Tang, Haoling Chang, Qi Liu, Ziheng Zhou, Tianyu Zhang, Jingtian Zhang, Zhangyi Liu, Minghao Li, Yuku Zhang, Boxuan Jing, Xianqi Yin, Yutong Ren, Zizhuo Fu, Jiaming Ji, Weike Wang, Xudong Tian, Anqi Lv, Laifu Man, Jianxiang Li, Feiyu Tao, Qihua Sun, Zhou Liang, Yushu Mu, Zhongxuan Li, Jing-Jun Zhang, Shutao Zhang, Xiaotian Li, Xingqi Xia, Jiawei Lin, Zheyu Shen, Jiahang Chen, Qiuhao Xiong, Binran Wang, Fengyuan Wang, Ziyang Ni, Bohan Zhang, Fan Cui, Changkun Shao, Qing-Hong Cao, Ming-xing Luo, Yaodong Yang, Muhan Zhang, Hua Xing Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmark, Physics Problems, Reasoning, Evaluation

**Relevance Score:** 6

**TL;DR:** PHYBench is a new benchmark of 500 physics problems designed to overcome current limitations in evaluating reasoning capabilities of LLMs.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for LLMs have significant limitations such as task oversimplification, data contamination, and flawed evaluation items, which necessitate more rigorous assessment methods.

**Method:** PHYBench consists of 500 original physics problems of varying difficulties, employing a systematic curation pipeline to eliminate flawed items and introducing a new scoring method for mathematical expressions.

**Key Contributions:**

	1. Introduction of PHYBench with 500 original physics problems.
	2. Systematic curation pipeline to reduce data contamination and flawed items.
	3. Development of the Expression Edit Distance (EED) Score for improved mathematical assessment.

**Result:** Evaluations demonstrate that PHYBench activates more tokens and provides better differentiation among reasoning models than existing benchmarks. The best model tested scored only 36.9% accuracy, indicating room for improvement compared to human experts' 61.9%.

**Limitations:** 

**Conclusion:** PHYBench offers a robust platform for evaluating reasoning in LLMs, revealing insights into their reasoning robustness and deficiencies, and includes a newly developed scoring system to improve evaluation precision.

**Abstract:** Current benchmarks for evaluating the reasoning capabilities of Large Language Models (LLMs) face significant limitations: task oversimplification, data contamination, and flawed evaluation items. These deficiencies necessitate more rigorous assessment methods. To address these limitations, we introduce PHYBench, a benchmark of 500 original physics problems ranging from high school to Physics Olympiad difficulty. PHYBench addresses data contamination through original content and employs a systematic curation pipeline to eliminate flawed items. Evaluations show that PHYBench activates more tokens and provides stronger differentiation between reasoning models compared to other baselines like AIME 2024, OlympiadBench and GPQA. Even the best-performing model, Gemini 2.5 Pro, achieves only 36.9% accuracy compared to human experts' 61.9%. To further enhance evaluation precision, we introduce the Expression Edit Distance (EED) Score for mathematical expression assessment, which improves sample efficiency by 204% over binary scoring. Moreover, PHYBench effectively elicits multi-step and multi-condition reasoning, providing a platform for examining models' reasoning robustness, preferences, and deficiencies. The benchmark results and dataset are publicly available at https://www.phybench.cn/.

</details>


### [319] [WorldPM: Scaling Human Preference Modeling](https://arxiv.org/abs/2505.10527)

*Binghai Wang, Runji Lin, Keming Lu, Le Yu, Zhenru Zhang, Fei Huang, Chujie Zheng, Kai Dang, Yang Fan, Xingzhang Ren, An Yang, Binyuan Hui, Dayiheng Liu, Tao Gui, Qi Zhang, Xuanjing Huang, Yu-Gang Jiang, Bowen Yu, Jingren Zhou, Junyang Lin*

**Main category:** cs.CL

**Keywords:** World Preference Modeling, preference modeling, scalability, language models, evaluation metrics

**Relevance Score:** 7

**TL;DR:** This paper introduces World Preference Modeling (WorldPM), highlighting the scalability potential of preference modeling in language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the scaling laws in preference modeling similar to those observed in language modeling, particularly in how model and dataset sizes influence performance.

**Method:** WorldPM was developed using preference data from public forums, with training conducted on models ranging from 1.5B to 72B parameters using 15M-scale data.

**Key Contributions:**

	1. Introduction of World Preference Modeling (WorldPM) for preference modeling
	2. Demonstration of distinct scaling behaviors in different types of metrics
	3. Validation of WorldPM's improvements across various human preference benchmarks

**Result:** Distinct patterns were observed where adversarial metrics scale with data/model size, objective metrics show emergent behavior in larger models, and subjective metrics do not scale. The integration of WorldPM improved generalization performance significantly across various datasets.

**Limitations:** 

**Conclusion:** WorldPM shows potential for enhancing human preference datasets and can be effectively integrated into evaluation frameworks, leading to notable performance improvements in preference tasks.

**Abstract:** Motivated by scaling laws in language modeling that demonstrate how test loss scales as a power law with model and dataset sizes, we find that similar laws exist in preference modeling. We propose World Preference Modeling$ (WorldPM) to emphasize this scaling potential, where World Preference embodies a unified representation of human preferences. In this paper, we collect preference data from public forums covering diverse user communities, and conduct extensive training using 15M-scale data across models ranging from 1.5B to 72B parameters. We observe distinct patterns across different evaluation metrics: (1) Adversarial metrics (ability to identify deceptive features) consistently scale up with increased training data and base model size; (2) Objective metrics (objective knowledge with well-defined answers) show emergent behavior in larger language models, highlighting WorldPM's scalability potential; (3) Subjective metrics (subjective preferences from a limited number of humans or AI) do not demonstrate scaling trends. Further experiments validate the effectiveness of WorldPM as a foundation for preference fine-tuning. Through evaluations on 7 benchmarks with 20 subtasks, we find that WorldPM broadly improves the generalization performance across human preference datasets of varying sizes (7K, 100K and 800K samples), with performance gains exceeding 5% on many key subtasks. Integrating WorldPM into our internal RLHF pipeline, we observe significant improvements on both in-house and public evaluation sets, with notable gains of 4% to 8% in our in-house evaluations.

</details>
