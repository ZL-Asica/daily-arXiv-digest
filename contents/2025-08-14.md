# 2025-08-14

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 58]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Based AI improves human decision-making but reduces trust](https://arxiv.org/abs/2508.09297)

*Shiyang Lai, Junsol Kim, Nadav Kunievsky, Yujin Potter, James Evans*

**Main category:** cs.HC

**Keywords:** AI neutrality, human decision-making, cognitive engagement, partisan AI, automation bias

**Relevance Score:** 7

**TL;DR:** The paper examines how culturally biased AI can improve human decision-making and engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of AI systems' ideological neutrality on human cognitive engagement and decision-making.

**Method:** Randomized trials with 2,500 participants interacting with politically diverse GPT-4o variants in information evaluation tasks.

**Key Contributions:**

	1. Demonstrated the impact of biased AI on human performance.
	2. Highlighted the importance of ideological diversity in AI interactions.
	3. Revealed the trust penalty linked to biased versus neutral AI systems.

**Result:** Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias, especially when participants encountered opposing views.

**Limitations:** 

**Conclusion:** Strategic integration of diverse cultural biases in AI may enhance decision-making and challenge the notion of AI neutrality.

**Abstract:** Current AI systems minimize risk by enforcing ideological neutrality, yet this may introduce automation bias by suppressing cognitive engagement in human decision-making. We conducted randomized trials with 2,500 participants to test whether culturally biased AI enhances human decision-making. Participants interacted with politically diverse GPT-4o variants on information evaluation tasks. Partisan AI assistants enhanced human performance, increased engagement, and reduced evaluative bias compared to non-biased counterparts, with amplified benefits when participants encountered opposing views. These gains carried a trust penalty: participants underappreciated biased AI and overcredited neutral systems. Exposing participants to two AIs whose biases flanked human perspectives closed the perception-performance gap. These findings complicate conventional wisdom about AI neutrality, suggesting that strategic integration of diverse cultural biases may foster improved and resilient human decision-making.

</details>


### [2] [Micro-Health Interventions: Exploring Design Strategies for 1-Minute Interventions as a Gateway to Healthy Habits](https://arxiv.org/abs/2508.09312)

*Zahra Hassanzadeh, David Haag, Lydia Chilton, Jan Smeddinck, Norman Farb, Joseph Jay Williams*

**Main category:** cs.HC

**Keywords:** behavior change, HCI, prompt design, health interventions, user engagement

**Relevance Score:** 6

**TL;DR:** This paper explores the effectiveness of one-minute behavior change interventions through two studies, indicating they can encourage healthier routines if designed appropriately.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether ultra-brief prompts can foster meaningful behavioral changes in daily life.

**Method:** Two studies were conducted: a formative exploration of participant engagement with one-minute prompts across various health domains and a 14-day within-subjects study comparing Immediate Action and Reflection-First prompt designs.

**Key Contributions:**

	1. Identification of design approaches for one-minute prompts
	2. Demonstration of user engagement factors over structural differences
	3. Insights into co-designing effective health prompts

**Result:** Participants found prompts more engaging based on content fit, tone, and perceived timeliness rather than the structural differences between the flows; co-designed messages with specific guidance were preferred.

**Limitations:** 

**Conclusion:** One-minute interventions can facilitate behavior change when they are timely, relevant, and emotionally supportive, rather than strictly adhering to a predefined structure.

**Abstract:** One-minute behavior change interventions might seem too brief to matter. Could something so short really help people build healthier routines? This work explores this question through two studies examining how ultra-brief prompts might encourage meaningful actions in daily life. In a formative study, we explored how participants engaged with one-minute prompts across four domains: physical activity, eating, screen use, and mental well-being. This revealed two common design approaches: Immediate Action prompts (simple, directive tasks) and Reflection-First prompts (self-awareness before action). We then conducted a 14-day, within-subjects study comparing these two flows with 28 participants. Surprisingly, most participants did not notice differences in structure -- but responded positively when prompts felt timely, relevant, or emotionally supportive. Engagement was not shaped by flow type, but by content fit, tone, and momentary readiness. Participants also co-designed messages, favoring those with step-by-step guidance, personal meaning, or sensory detail. These results suggest that one-minute interventions, while easily dismissed, may serve as meaningful gateways into healthier routines -- if designed to feel helpful in the moment.

</details>


### [3] [Affordances of Sketched Notations for Multimodal UI Design and Development Tools](https://arxiv.org/abs/2508.09342)

*Sam H. Ross, Yunseo Lee, Coco K. Lee, Jayne Everson, R. Benjamin Shapiro*

**Main category:** cs.HC

**Keywords:** multimodal UI design, Cognitive Dimensions of Notations, human-centered design, AI methods, UI sketch recognition

**Relevance Score:** 8

**TL;DR:** This paper explores the design of notations for multimodal UI design tools through an analysis of UI sketching notations using the Cognitive Dimensions of Notations framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create usable and intuitive notations for interactive design systems by treating training datasets as notation specifications.

**Method:** Analysis of two notations for UI sketching—one from an existing dataset and another from participant-generated sketches—using the Cognitive Dimensions of Notations framework.

**Key Contributions:**

	1. Introduction of FixedSketch and FlexiSketch systems for understanding UI sketch notations
	2. Analysis of cognitive dimensions influencing notational design
	3. Call for integration of contemporary AI methods in multimodal design tools

**Result:** The study reveals that while FlexiSketch notation allows for greater intuitive expression and lower cognitive effort, it struggles with element-based UI sketch recognition compared to FixedSketch notation.

**Limitations:** 

**Conclusion:** Future multimodal design tools must integrate advanced AI methods to effectively interpret context-rich expressive notations.

**Abstract:** Multimodal UI design and development tools that interpret sketches or natural language descriptions of UIs inherently have notations: the inputs they can understand. In AI-based systems, notations are implicitly defined by the data used to train these systems. In order to create usable and intuitive notations for interactive design systems, we must regard, design, and evaluate these training datasets as notation specifications. To better understand the design space of notational possibilities for future design tools, we use the Cognitive Dimensions of Notations framework to analyze two possible notations for UI sketching. The first notation is the sketching rules for an existing UI sketch dataset, and the second notation is the set of sketches generated by participants in this study, where individuals sketched UIs without imposed representational rules. We imagine two systems, FixedSketch and FlexiSketch, built with each notation respectively, in order to understand the differential affordances of, and potential design requirements for, systems. We find that participants' sketches were composed of element-level notations that are ambiguous in isolation but are interpretable in context within whole designs. For many cognitive dimensions, the FlexiSketch notation supports greater intuitive creative expression and affords lower cognitive effort than the FixedSketch notation, but cannot be supported with prevailing, element-based approaches to UI sketch recognition. We argue that for future multimodal design tools to be truly human-centered, they must adopt contemporary AI methods, including transformer-based and human-in-the-loop, reinforcement learning techniques to understand users' context-rich expressive notations and corrections.

</details>


### [4] [Virtual Reality User Interface Design: Best Practices and Implementation](https://arxiv.org/abs/2508.09358)

*Esin Mehmedova, Santiago Berrezueta-Guzman, Stefan Wagner*

**Main category:** cs.HC

**Keywords:** Virtual Reality, User Interface Design, Best Practices, Systematic Literature Review, User Study

**Relevance Score:** 8

**TL;DR:** This paper presents unified design guidelines for virtual reality user interfaces (UIs) developed through a systematic literature review and validated by a user study.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive design guidelines for effective VR UI design, despite the increasing use of VR in various sectors.

**Method:** A systematic literature review was conducted to identify best practices in VR UI design. A VR application named FlUId was created to demonstrate these practices, and a user study was performed to evaluate their effectiveness.

**Key Contributions:**

	1. Proposed comprehensive design guidelines for VR UIs
	2. Developed a VR application, FlUId, to showcase UI design principles
	3. Conducted a user study to validate the effectiveness of the guidelines

**Result:** The guidelines proposed in this research were validated through a user study, showing a significant impact on user immersion and comfort.

**Limitations:** The study may not encompass all possible scenarios in diverse VR applications.

**Conclusion:** The findings provide concrete recommendations for VR designers and developers, bridging the gap between theoretical guidelines and practical application.

**Abstract:** Designing effective user interfaces (UIs) for virtual reality (VR) is essential to enhance user immersion, usability, comfort, and accessibility in virtual environments. Despite the growing adoption of VR across domains such as education, healthcare, gaming, and rehabilitation, there is a noticeable lack of unified and comprehensive design guidelines for VR UI design. To address this gap, we conducted a systematic literature review to identify existing best practices and propose complete and unified guidelines for UI development in VR.   Building on these insights, this research proposes a set of best practices to guide the creation of more effective VR interfaces. To demonstrate and validate these practices, we developed a VR application called \textit{FlUId} that showcases both good and bad UI design principles for direct comparison. A user study was conducted to evaluate the impact of the proposed guidelines. The findings aim to bridge the gap between theory and practice, offering concrete recommendations for VR designers and developers.

</details>


### [5] [VIVA: Virtual Healthcare Interactions Using Visual Analytics, With Controllability Through Configuration](https://arxiv.org/abs/2508.09386)

*Jürgen Bernard, Mara Solen, Helen Novak Lauscher, Kurtis Stewart, Kendall Ho, Tamara Munzner*

**Main category:** cs.HC

**Keywords:** visual analytics, healthcare, COVID-19, data analysis, user interaction

**Relevance Score:** 7

**TL;DR:** The paper discusses the design and implementation of VIVA, a visual analytics tool developed for HealthLink BC to analyze virtual healthcare service usage data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve patient outcomes and satisfaction with virtual healthcare during the COVID-19 pandemic while preserving healthcare system capacity.

**Method:** A visual analytics tool (VIVA) was designed and implemented based on the abstraction of data and analysis tasks, validated through case studies with domain experts, utilizing a framework of Scan, Act, Adapt and the Controllability Through Configuration model.

**Key Contributions:**

	1. Design and implementation of VIVA tool for healthcare analytics
	2. Introduction of the Scan, Act, Adapt interactive workflow
	3. Development of the Controllability Through Configuration model for design studies

**Result:** VIVA was validated through case studies and is capable of analyzing various forms of usage data to support better decision-making in virtual healthcare.

**Limitations:** 

**Conclusion:** The architectural evolution of VIVA reflects a balance between rigid programming and flexible end-user interactivity, enhancing the potential for effective usage data analysis.

**Abstract:** At the beginning of the COVID-19 pandemic, HealthLink BC (HLBC) rapidly integrated physicians into the triage process of their virtual healthcare service to improve patient outcomes and satisfaction with this service and preserve health care system capacity. We present the design and implementation of a visual analytics tool, VIVA (Virtual healthcare Interactions using Visual Analytics), to support HLBC in analysing various forms of usage data from the service. We abstract HLBC's data and data analysis tasks, which we use to inform our design of VIVA. We also present the interactive workflow abstraction of Scan, Act, Adapt. We validate VIVA's design through three case studies with stakeholder domain experts. We also propose the Controllability Through Configuration model to conduct and analyze design studies, and discuss architectural evolution of VIVA through that lens. It articulates configuration, both that specified by a developer or technical power user and that constructed automatically through log data from previous interactive sessions, as a bridge between the rigidity of hardwired programming and the time-consuming implementation of full end-user interactivity.   Availability: Supplemental materials at https://osf.io/wv38n

</details>


### [6] [Realtime Multimodal Emotion Estimation using Behavioral and Neurophysiological Data](https://arxiv.org/abs/2508.09402)

*Von Ralph Dane Marquez Herbuela, Yukie Nagai*

**Main category:** cs.HC

**Keywords:** Emotion estimation, Neurodiversity, Real-time monitoring, Physiological signals, Interaction design

**Relevance Score:** 7

**TL;DR:** A multimodal emotion estimation system for supporting inclusive emotion technologies, tailored for neurodivergent users using various physiological and behavioral data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate better understanding and interaction for individuals with autism and other neurodivergent conditions by enhancing emotion recognition and expression.

**Method:** The system integrates neurophysiological data (EEG, ECG, BVP, GSR) with behavioral data (facial expressions and speech) to provide a comprehensive emotional tracking interface.

**Key Contributions:**

	1. Real-time multimodal emotion estimation integrating physiological and behavioral data.
	2. User-specific analysis for enhanced interpretation of emotions.
	3. Application in neuroadaptive feedback mechanisms and interaction support.

**Result:** Demonstration scenarios show the system's capability in monitoring emotional responses during media consumption and interactive conversations, providing user-specific insights.

**Limitations:** 

**Conclusion:** The architecture supports personalized feedback and interaction design tailored for neurodiverse users, enhancing emotional literacy and adaptive feedback mechanisms.

**Abstract:** Many individuals especially those with autism spectrum disorder (ASD), alexithymia, or other neurodivergent profiles face challenges in recognizing, expressing, or interpreting emotions. To support more inclusive and personalized emotion technologies, we present a real-time multimodal emotion estimation system that combines neurophysiological EEG, ECG, blood volume pulse (BVP), and galvanic skin response (GSR/EDA) and behavioral modalities (facial expressions, and speech) in a unified arousal-valence 2D interface to track moment-to-moment emotional states. This architecture enables interpretable, user-specific analysis and supports applications in emotion education, neuroadaptive feedback, and interaction support for neurodiverse users. Two demonstration scenarios illustrate its application: (1) passive media viewing (2D or VR videos) reveals cortical and autonomic responses to affective content, and (2) semi-scripted conversations with a facilitator or virtual agent capture real-time facial and vocal expressions. These tasks enable controlled and naturalistic emotion monitoring, making the system well-suited for personalized feedback and neurodiversity-informed interaction design.

</details>


### [7] [Fulfillment of the Work Games: Warehouse Workers' Experiences with Algorithmic Management](https://arxiv.org/abs/2508.09438)

*EunJeong Cheon, Ingrid Erickson*

**Main category:** cs.HC

**Keywords:** algorithmic management, worker resistance, fulfillment centers, ethnographic research, labor systems

**Relevance Score:** 6

**TL;DR:** This paper explores the experiences of Amazon fulfillment center workers under algorithmic management, emphasizing their resistance practices and the implications for understanding labor systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the experiences of workers beyond platform-mediated gig environments, particularly in traditional sectors like Amazon fulfillment centers, in the context of algorithmic management.

**Method:** Ethnographic research conducted over two years, focusing on the experiences and practices of Amazon fulfillment center workers in relation to labor-tracking systems.

**Key Contributions:**

	1. Empirical examination of algorithmic management in non-gig work settings.
	2. Identification of resistance practices among fulfillment center workers.
	3. Linking workers' experiences to broader discussions of algorithmic control.
	4. Conceptualization of 'work games' as a means of worker agency.

**Result:** The study reveals how workers resist imposed productivity rates and objectification through various agentic practices, termed 'work games.'

**Limitations:** 

**Conclusion:** A deeper understanding of worker experiences in algorithmic management can provide critical insights into the socio-economic forces shaping labor systems, offering pathways for critique and potential change.

**Abstract:** The introduction of algorithms into a large number of industries has already restructured the landscape of work and threatens to continue. While a growing body of CSCW research centered on the future of work has begun to document these shifts, relatively little is known about workers' experiences beyond those of platform-mediated gig workers. In this paper, we turn to a traditional work sector, Amazon fulfillment centers (FC), to deepen our field's empirical examination of algorithmic management. Drawing on two years of ethnographic research, we show how FC workers react to managers' interventions, imposed productivity rates, and quantified objectification when subjected to labor-tracking systems in their physical work environments. Situating FC workers' resistance to algorithmic systems and metrics within the current CSCW literature allows us to explicate and link the nuanced practices of FC workers to the larger discourse of algorithmic control mechanisms. In addition, we show how FC workers' resistance practices are emblematic of 'work games'--a long-studied means by which workers agentically configure ("trick") their engagement within work systems. We argue that gaining a more nuanced understanding of workers' resistance and consent in relation to algorithmic management expands our ability to critique and potentially disassemble the economic and political forces at the root of these sociotechnical labor systems.

</details>


### [8] [Hallucination vs interpretation: rethinking accuracy and precision in AI-assisted data extraction for knowledge synthesis](https://arxiv.org/abs/2508.09458)

*Xi Long, Christy Boscardin, Lauren A. Maggio, Joseph A. Costello, Ralph Gonzales, Rasmyah Hammoudeh, Ki Lai, Yoon Soo Park, Brian C. Gin*

**Main category:** cs.HC

**Keywords:** AI-assisted extraction, Health professions education, Large language models, Data extraction, Literature reviews

**Relevance Score:** 9

**TL;DR:** This paper investigates the use of AI-assisted data extraction in health professions education literature reviews, demonstrating high consistency between AI and human responses for concrete questions, while highlighting the importance of interpretability.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of literature reviews in health professions education by utilizing AI for data extraction, addressing concerns over accuracy and distinguishing between hallucinations and genuine interpretive differences.

**Method:** Developed an extraction platform using large language models (LLMs) to automate data extraction and compared AI responses to human responses across 187 publications and 17 extraction questions, measuring consistencies and errors.

**Key Contributions:**

	1. Automation of literature review data extraction using LLMs
	2. Evidence that AI performance varies significantly based on the nature of the questions
	3. Demonstrated a comparison of AI and human response consistency in interpreting literature

**Result:** AI showed high consistency with human responses for concrete questions but lower for subjective interpretations; errors were largely from interpretive differences rather than hallucinations, with AI inaccuracies being rare but present.

**Limitations:** Caution is needed when interpreting nuanced questions, as AI may struggle with ambiguous content, and human input remains critical for some insights.

**Conclusion:** AI can enhance the literature review process by providing consistent extraction when questions are clear, but human oversight is essential to capture nuanced insights.

**Abstract:** Knowledge syntheses (literature reviews) are essential to health professions education (HPE), consolidating findings to advance theory and practice. However, they are labor-intensive, especially during data extraction. Artificial Intelligence (AI)-assisted extraction promises efficiency but raises concerns about accuracy, making it critical to distinguish AI 'hallucinations' (fabricated content) from legitimate interpretive differences. We developed an extraction platform using large language models (LLMs) to automate data extraction and compared AI to human responses across 187 publications and 17 extraction questions from a published scoping review. AI-human, human-human, and AI-AI consistencies were measured using interrater reliability (categorical) and thematic similarity ratings (open-ended). Errors were identified by comparing extracted responses to source publications. AI was highly consistent with humans for concrete, explicitly stated questions (e.g., title, aims) and lower for questions requiring subjective interpretation or absent in text (e.g., Kirkpatrick's outcomes, study rationale). Human-human consistency was not higher than AI-human and showed the same question-dependent variability. Discordant AI-human responses (769/3179 = 24.2%) were mostly due to interpretive differences (18.3%); AI inaccuracies were rare (1.51%), while humans were nearly three times more likely to state inaccuracies (4.37%). Findings suggest AI accuracy depends more on interpretability than hallucination. Repeating AI extraction can identify interpretive complexity or ambiguity, refining processes before human review. AI can be a transparent, trustworthy partner in knowledge synthesis, though caution is needed to preserve critical human insights.

</details>


### [9] [Handows: A Palm-Based Interactive Multi-Window Management System in Virtual Reality](https://arxiv.org/abs/2508.09469)

*Jindu Wang, Ke Zhou, Haoyu Ren, Per Ola Kristensson, Xiang Li*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Human-Computer Interaction, User Study, Ergonomic Design, Gesture-Based Interface

**Relevance Score:** 7

**TL;DR:** Handows introduces a palm-based interface for managing windows in VR, enabling efficient task interaction through smartphone-inspired gestures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current VR window management methods are spatially complex and physically demanding. The aim is to improve user interaction and reduce physical strain.

**Method:** Users manipulate windows using gestures on their non-dominant hand, combining ergonomic design with passive haptics. The interface was evaluated through a user study comparing it with traditional VR techniques.

**Key Contributions:**

	1. Introduction of Handows, a novel palm-based interface for VR window management
	2. User study demonstrating improved efficiency and reduced physical effort with Handows
	3. Case study showcasing usability in multitasking scenarios

**Result:** Handows significantly decreased physical effort and head movement while enhancing task efficiency and precision in window operations compared to conventional methods.

**Limitations:** The study had a small sample size and focused on specific VR tasks, which may limit generalizability.

**Conclusion:** Embedding mobile-inspired metaphors into body-centric interfaces can lead to more effective and less taxing interactions in VR environments.

**Abstract:** Window management in virtual reality (VR) remains a challenging task due to the spatial complexity and physical demands of current interaction methods. We introduce Handows, a palm-based interface that enables direct manipulation of spatial windows through familiar smartphone-inspired gestures on the user's non-dominant hand. Combining ergonomic layout design with body-centric input and passive haptics, Handows supports four core operations: window selection, closure, positioning, and scaling. We evaluate Handows in a user study (N=15) against two common VR techniques (virtual hand and controller) across these core window operations. Results show that Handows significantly reduces physical effort and head movement while improving task efficiency and interaction precision. A follow-up case study (N=8) demonstrates Handows' usability in realistic multitasking scenarios, highlighting user-adapted workflows and spontaneous layout strategies. Our findings suggest the potential of embedding mobile-inspired metaphors into proprioceptive body-centric interfaces to support low-effort and spatially coherent interaction in VR.

</details>


### [10] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)

*Daniel Raffini, Agnese Macori, Lorenzo Porcaro, Tiziana Catarci, Marco Angelini*

**Main category:** cs.HC

**Keywords:** ChatGPT, Argumentation, Persuasion, Ethics, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** Study analyzes AI-generated argumentative texts by ChatGPT on ethical topics and their impact on readers' opinions.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how AI-generated arguments affect human readers' opinions, particularly in ethically nuanced contexts.

**Method:** User study with 62 participants, using pre-post interaction surveys and linguistic/rhetorical analysis of the generated texts.

**Key Contributions:**

	1. Investigates the impact of AI-generated arguments on opinion change in ethical contexts
	2. Identifies the limitations of ChatGPT's persuasive capabilities
	3. Provides a foundation for future studies on AI persuasion.

**Result:** ChatGPT creates coherent arguments but struggles with persuasiveness on ethical issues; exposure often intensifies ethical concerns.

**Limitations:** Constraints in persuasive efficacy identified, particularly in ethically sensitive areas; findings may vary by topic.

**Conclusion:** AI-generated texts show potential in argument construction but limited effectiveness in persuasion on ethical topics, revealing areas for future research.

**Abstract:** This study examines the rhetorical and linguistic features of argumentative texts generated by ChatGPT on ethically nuanced topics and investigates their persuasive impact on human readers.Through a user study involving 62 participants and pre-post interaction surveys, the paper analyzes how exposure to AI-generated arguments affects opinion change and user perception. A linguistic and rhetorical analysis of the generated texts reveals a consistent argumentative macrostructure, reliance on formulaic expressions, and limited stylistic richness. While ChatGPT demonstrates proficiency in constructing coherent argumentative texts, its persuasive efficacy appears constrained, particularly on topics involving ethical issues.The study finds that while participants often acknowledge the benefits highlighted by ChatGPT, ethical concerns tend to persist or even intensify post-interaction. The results also demonstrate a variation depending on the topic. These findings highlight new insights on AI-generated persuasion in ethically sensitive domains and are a basis for future research.

</details>


### [11] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)

*Daniel Raffini, Agnese Macori, Marco Angelini, Tiziana Catarci*

**Main category:** cs.HC

**Keywords:** gender bias, AI narratives, ChatGPT, narrative analysis, story generation

**Relevance Score:** 7

**TL;DR:** The paper investigates gender-based biases in narratives generated by AI models like ChatGPT, Gemini, and Claude, using a close reading approach to analyze character distribution and plot elements.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand gender biases in AI-generated narratives and the implications these biases have on storytelling.

**Method:** Close reading analysis of stories generated by AI models, focusing on character gender distribution, descriptions, actions, and plot development based on Propp's and Freytag's structures.

**Key Contributions:**

	1. Identification of gender biases in AI-generated narratives
	2. Methodological approach utilizing literary analysis frameworks
	3. Highlighting the need for comprehensive bias assessments in AI outputs.

**Result:** The analysis reveals the persistence of both explicit and implicit gender biases in AI-generated stories, necessitating a multi-level assessment approach.

**Limitations:** Focuses primarily on narrative and character aspects without exploring other dimensions of biases in AI-generated content.

**Conclusion:** Bias assessment in AI narratives is crucial and should be conducted using an interpretative framework to understand its nuances and implications.

**Abstract:** The paper explores the study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's character classifications and Freytag's narrative structure. The stories are analyzed through a close reading approach, with particular attention to adherence to the prompt, gender distribution of characters, physical and psychological descriptions, actions, and finally, plot development and character relationships. The results reveal the persistence of biases - especially implicit ones - in the generated stories and highlight the importance of assessing biases at multiple levels using an interpretative approach.

</details>


### [12] [Wisdom of the Crowd, Without the Crowd: A Socratic LLM for Asynchronous Deliberation on Perspectivist Data](https://arxiv.org/abs/2508.09911)

*Malik Khadar, Daniel Runningen, Julia Tang, Stevie Chancellor, Harmanpreet Kaur*

**Main category:** cs.HC

**Keywords:** Data Annotation, Socratic Dialog, Large Language Models, Crowdsourcing, Diverse Perspectives

**Relevance Score:** 8

**TL;DR:** A Socratic dialog system leveraging LLMs enhances data annotation by encouraging diverse perspectives and improving accuracy, compared to traditional synchronous crowd deliberation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue that crowd-collected datasets can fail to preserve diverse perspectives, particularly for tasks that are difficult or ambiguous, and to find a more efficient method for enhancing data quality.

**Method:** A Socratic dialog system using Large Language Models (LLMs) engages participants in deliberation, allowing for consideration of different annotation perspectives and helping them improve their confidence in labels during the annotation process.

**Key Contributions:**

	1. Introduction of a Socratic dialog system for data annotation using LLMs
	2. Demonstrated improvement in annotation accuracy for the Relation task
	3. Qualitative validation of the approach through participant feedback

**Result:** Participants exhibited higher annotation accuracy in the Relation task, and qualitative feedback indicated that the LLM's Socratic approach effectively promoted reasoned arguments and was well-received.

**Limitations:** The method's effectiveness may vary depending on the complexity of tasks and the representativeness of dialogue generated by the LLM.

**Conclusion:** The proposed LLM-based system serves as a scalable method to enhance data annotation processes, ensuring that diverse perspectives are preserved and improving the overall quality of datasets.

**Abstract:** Data annotation underpins the success of modern AI, but the aggregation of crowd-collected datasets can harm the preservation of diverse perspectives in data. Difficult and ambiguous tasks cannot easily be collapsed into unitary labels. Prior work has shown that deliberation and discussion improve data quality and preserve diverse perspectives -- however, synchronous deliberation through crowdsourcing platforms is time-intensive and costly. In this work, we create a Socratic dialog system using Large Language Models (LLMs) to act as a deliberation partner in place of other crowdworkers. Against a benchmark of synchronous deliberation on two tasks (Sarcasm and Relation detection), our Socratic LLM encouraged participants to consider alternate annotation perspectives, update their labels as needed (with higher confidence), and resulted in higher annotation accuracy (for the Relation task where ground truth is available). Qualitative findings show that our agent's Socratic approach was effective at encouraging reasoned arguments from our participants, and that the intervention was well-received. Our methodology lays the groundwork for building scalable systems that preserve individual perspectives in generating more representative datasets.

</details>


### [13] [Keyframer: Empowering Animation Design using Large Language Models](https://arxiv.org/abs/2402.06071)

*Tiffany Tseng, Ruijia Cheng, Jeffrey Nichols*

**Main category:** cs.HC

**Keywords:** 2D animation, Natural language processing, User study

**Relevance Score:** 5

**TL;DR:** Keyframer is a tool designed to simplify the creation of 2D animations through natural language prompts and inline rendering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To aid designers, both novices and experts, in the complex iterative process of creating 2D animations.

**Method:** A design tool named Keyframer was developed, allowing users to generate animation code from natural language prompts and edit it directly, while integrating user study feedback.

**Key Contributions:**

	1. Development of the Keyframer tool for 2D animation generation.
	2. Identification of semantic prompt categories for motion description.
	3. Insights for iterative refinement using natural language and editing.

**Result:** The study involved 13 participants and revealed a categorization of semantic prompt types for animation and insights into a decomposed prompting style where users adapt goals based on output.

**Limitations:** 

**Conclusion:** Keyframer combines natural language interfaces with direct editing to support the iterative refinement of animation designs effectively.

**Abstract:** Creating 2D animations is a complex, iterative process requiring continuous adjustments to movement, timing, and coordination of multiple elements within a scene. To support designers of varying levels of experience with animation design and implementation, we developed Keyframer, a design tool that generates animation code in response to natural language prompts, enabling users to preview rendered animations inline and edit them directly through provided editors. Through a user study with 13 novices and experts in animation design and programming, we contribute 1) a categorization of semantic prompt types for describing motion and identification of a 'decomposed' prompting style where users continually adapt their goals in response to generated output; and 2) design insights on supporting iterative refinement of animations through the combination of direct editing and natural language interfaces.

</details>


### [14] [Beyond Autocomplete: Designing CopilotLens Towards Transparent and Explainable AI Coding Agents](https://arxiv.org/abs/2506.20062)

*Runlong Ye, Zeling Zhang, Boushra Almazroua, Michael Liut*

**Main category:** cs.HC

**Keywords:** AI code assistants, explainability, HCI, trust in AI, code completion

**Relevance Score:** 8

**TL;DR:** This paper introduces CopilotLens, an interactive framework that enhances code completion tools by providing transparent explanations of AI-generated code suggestions, aimed at improving developers' understanding and trust.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve developers' critical evaluation of AI-generated code completions by providing transparency in the decision-making process of code assistants.

**Method:** The authors propose a two-level interface framework, CopilotLens, that offers explanations of the AI agent's thought process alongside high-level code changes and contextual influences from the codebase.

**Key Contributions:**

	1. Introduction of the CopilotLens framework for explainable code completion
	2. The framework's interactive two-level interface for transparency in AI suggestions
	3. Insights into improving trust and understanding in AI-generated code outputs

**Result:** The introduction of CopilotLens enhances the explainability of AI-powered code assistants, potentially leading to improved comprehension and trust among developers.

**Limitations:** Future work is needed to evaluate the effectiveness of the framework in real-world coding scenarios.

**Conclusion:** The paper outlines the design and purpose of CopilotLens, setting the stage for further evaluation of its impact on developer understanding and trust in AI systems.

**Abstract:** AI-powered code assistants are widely used to generate code completions, significantly boosting developer productivity. However, these tools typically present suggestions without explaining their rationale, leaving their decision-making process inscrutable. This opacity hinders developers' ability to critically evaluate outputs, form accurate mental models, and calibrate trust in the system. To address this, we introduce CopilotLens, a novel interactive framework that reframes code completion from a simple suggestion into a transparent, explainable interaction. CopilotLens operates as an explanation layer that reconstructs the AI agent's "thought process" through a dynamic, two-level interface. The tool aims to surface both high-level code changes and the specific codebase context influences. This paper presents the design and rationale of CopilotLens, offering a concrete framework and articulating expectations on deepening comprehension and calibrated trust, which we plan to evaluate in subsequent work.

</details>


### [15] [How Persuasive Could LLMs Be? A First Study Combining Linguistic-Rhetorical Analysis and User Experiments](https://arxiv.org/abs/2508.09614)

*Daniel Raffini, Agnese Macori, Lorenzo Porcaro, Tiziana Catarci, Marco Angelini*

**Main category:** cs.HC

**Keywords:** AI-generated texts, persuasion, ethical issues, argumentative discourse, linguistic analysis

**Relevance Score:** 6

**TL;DR:** Study analyzes AI-generated argumentative texts from ChatGPT, their influence on user opinion regarding ethical issues, and findings on persuasive limitations.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the rhetorical and linguistic features of argumentative texts generated by ChatGPT and their impact on human readers' opinions, particularly on ethically nuanced topics.

**Method:** User study with 62 participants surveyed before and after exposure to AI-generated arguments to assess opinion change and perception.

**Key Contributions:**

	1. Analysis of rhetorical and linguistic features in AI-generated texts.
	2. Evaluation of persuasive impact on user opinions in ethically nuanced contexts.
	3. Identification of limitations in AI argument construction and effectiveness.

**Result:** The study found that while ChatGPT can create coherent argumentative texts, its effectiveness in persuading on ethical issues is limited, with concerns often persisting or intensifying post-interaction.

**Limitations:** The study is context-dependent, with results varying based on the topic of discussion.

**Conclusion:** Insights from this research highlight the nuanced role of AI in persuasion regarding ethical topics and suggest avenues for future exploration.

**Abstract:** This study examines the rhetorical and linguistic features of argumentative texts generated by ChatGPT on ethically nuanced topics and investigates their persuasive impact on human readers.Through a user study involving 62 participants and pre-post interaction surveys, the paper analyzes how exposure to AI-generated arguments affects opinion change and user perception. A linguistic and rhetorical analysis of the generated texts reveals a consistent argumentative macrostructure, reliance on formulaic expressions, and limited stylistic richness. While ChatGPT demonstrates proficiency in constructing coherent argumentative texts, its persuasive efficacy appears constrained, particularly on topics involving ethical issues.The study finds that while participants often acknowledge the benefits highlighted by ChatGPT, ethical concerns tend to persist or even intensify post-interaction. The results also demonstrate a variation depending on the topic. These findings highlight new insights on AI-generated persuasion in ethically sensitive domains and are a basis for future research.

</details>


### [16] [A Close Reading Approach to Gender Narrative Biases in AI-Generated Stories](https://arxiv.org/abs/2508.09651)

*Daniel Raffini, Agnese Macori, Marco Angelini, Tiziana Catarci*

**Main category:** cs.HC

**Keywords:** gender bias, AI narratives, story analysis, narrative structure, implicit bias

**Relevance Score:** 6

**TL;DR:** Study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the persistence of implicit gender biases in AI-generated narratives and assess these biases at multiple levels.

**Method:** Analysis of stories using a close reading approach based on Propp's character classifications and Freytag's narrative structure.

**Key Contributions:**

	1. Identification of gender-based biases in narrative generation
	2. Use of Propp's and Freytag's frameworks for story analysis
	3. Emphasis on the need for multi-level bias assessment

**Result:** The analysis reveals the presence of implicit biases in character distribution and descriptions, actions, and plot developments in AI-generated stories.

**Limitations:** Focus on specific AI models and narrative structures may limit the generalizability of the findings.

**Conclusion:** Assessing biases in AI-generated narratives is crucial for understanding their implications in storytelling and for improving future models.

**Abstract:** The paper explores the study of gender-based narrative biases in stories generated by ChatGPT, Gemini, and Claude. The prompt design draws on Propp's character classifications and Freytag's narrative structure. The stories are analyzed through a close reading approach, with particular attention to adherence to the prompt, gender distribution of characters, physical and psychological descriptions, actions, and finally, plot development and character relationships. The results reveal the persistence of biases - especially implicit ones - in the generated stories and highlight the importance of assessing biases at multiple levels using an interpretative approach.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [ParallelSearch: Train your LLMs to Decompose Query and Search Sub-queries in Parallel with Reinforcement Learning](https://arxiv.org/abs/2508.09303)

*Shu Zhao, Tan Yu, Anbang Xu, Japinder Singh, Aaditya Shukla, Rama Akkiraju*

**Main category:** cs.CL

**Keywords:** reinforcement learning, parallel processing, search agents

**Relevance Score:** 8

**TL;DR:** ParallelSearch enhances reinforcement learning for search agents by enabling parallel processing of queries, improving efficiency and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the inefficiencies of sequential processing in current reasoning-augmented search agents, enabling more effective multi-step information retrieval.

**Method:** A novel reinforcement learning framework that allows LLMs to identify and process parallelizable query structures, employing dedicated reward functions for accuracy and efficiency.

**Key Contributions:**

	1. Development of the ParallelSearch framework for concurrent query execution
	2. Introduction of dedicated reward functions for independent query components
	3. Demonstrated significant performance improvements over existing methods

**Result:** ParallelSearch outperforms state-of-the-art methods with an average 2.9% performance gain across benchmarks and a 12.7% improvement on parallelizable questions using fewer LLM calls.

**Limitations:** 

**Conclusion:** The ParallelSearch framework significantly improves query processing efficiency in reasoning-augmented search agents, paving the way for more effective information retrieval.

**Abstract:** Reasoning-augmented search agents such as Search-R1, trained via reinforcement learning with verifiable rewards (RLVR), demonstrate remarkable capabilities in multi-step information retrieval from external knowledge sources. These agents address the limitations of their parametric memory by dynamically gathering relevant facts to address complex reasoning tasks. However, existing approaches suffer from a fundamental architectural limitation: they process search queries strictly sequentially, even when handling inherently parallelizable and logically independent comparisons. This sequential bottleneck significantly constrains computational efficiency, particularly for queries that require multiple entity comparisons. To address this critical limitation, we propose ParallelSearch, a novel reinforcement learning framework that empowers large language models (LLMs) to recognize parallelizable query structures and execute multiple search operations concurrently. Our approach introduces dedicated reward functions that incentivize the identification of independent query components while preserving answer accuracy through jointly considering correctness, query decomposition quality, and parallel execution benefits. Comprehensive experiments demonstrate that ParallelSearch outperforms state-of-the-art baselines by an average performance gain of 2.9% across seven question-answering benchmarks. Notably, on parallelizable questions, our method achieves a 12.7% performance improvement while requiring only 69.6% of the LLM calls compared to sequential approaches.

</details>


### [18] [Leveraging Large Language Models for Rare Disease Named Entity Recognition](https://arxiv.org/abs/2508.09323)

*Nan Miles Xi, Yu Deng, Lin Wang*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, rare diseases, GPT-4o, prompt-based strategies, biomedical NER

**Relevance Score:** 9

**TL;DR:** This study evaluates GPT-4o for Named Entity Recognition (NER) in the rare disease domain, demonstrating its effectiveness in low-resource settings using various prompting strategies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of Named Entity Recognition in the rare disease domain, which includes limited labeled data and semantic ambiguity between entity types.

**Method:** We evaluated GPT-4o's performance using prompt-based strategies such as zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning, with a structured prompting framework for domain-specific knowledge.

**Key Contributions:**

	1. Evaluation of GPT-4o in rare disease NER
	2. Introduction of semantically guided few-shot example selection methods
	3. Cost-performance analysis of prompt-based strategies

**Result:** Experiments showed that GPT-4o achieved competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning leading to new state-of-the-art results in rare disease NER.

**Limitations:** Common failure modes include boundary drift and type confusion, indicating areas for post-processing improvement.

**Conclusion:** Prompt-optimized LLMs can effectively replace traditional supervised models in rare disease NER, especially where labeled data is scarce, demonstrating a scalable solution for biomedical applications.

**Abstract:** Named Entity Recognition (NER) in the rare disease domain poses unique challenges due to limited labeled data, semantic ambiguity between entity types, and long-tail distributions. In this study, we evaluate the capabilities of GPT-4o for rare disease NER under low-resource settings, using a range of prompt-based strategies including zero-shot prompting, few-shot in-context learning, retrieval-augmented generation (RAG), and task-level fine-tuning. We design a structured prompting framework that encodes domain-specific knowledge and disambiguation rules for four entity types. We further introduce two semantically guided few-shot example selection methods to improve in-context performance while reducing labeling effort. Experiments on the RareDis Corpus show that GPT-4o achieves competitive or superior performance compared to BioClinicalBERT, with task-level fine-tuning yielding new state-of-the-art (SOTA) results. Cost-performance analysis reveals that few-shot prompting delivers high returns at low token budgets, while RAG offers marginal additional benefit. An error taxonomy highlights common failure modes such as boundary drift and type confusion, suggesting opportunities for post-processing and hybrid refinement. Our results demonstrate that prompt-optimized LLMs can serve as effective, scalable alternatives to traditional supervised models in biomedical NER, particularly in rare disease applications where annotated data is scarce.

</details>


### [19] [TEN: Table Explicitization, Neurosymbolically](https://arxiv.org/abs/2508.09324)

*Nikita Mehrotra, Aayush Kumar, Sumit Gulwani, Arjun Radhakrishna, Ashish Tiwari*

**Main category:** cs.CL

**Keywords:** neurosymbolic, tabular data extraction, large language model, hallucination, symbolic checker

**Relevance Score:** 8

**TL;DR:** TEN is a neurosymbolic approach that enhances tabular data extraction from semistructured text by combining LLMs with symbolic checking to mitigate hallucinations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Extracting structured data from semistructured text is challenging due to inconsistent delimiters, which causes problems for purely neural methods.

**Method:** TEN employs Structural Decomposition prompting with a large language model to generate a table. A symbolic checker assesses the table's quality and detects errors, then a critique-LLM provides corrections in a self-debugging loop.

**Key Contributions:**

	1. Introduction of TEN, a neurosymbolic framework for tabular extraction
	2. Use of a symbolic checker for quality assessment and error detection in LLM outputs
	3. Empirical validation through extensive experiments and user studies showing performance improvements

**Result:** TEN significantly outperforms neural baselines, demonstrated by higher exact match accuracy and lower hallucination rates across various datasets. A user study confirmed TEN's results, with participants rating its output more accurate and preferred for verification.

**Limitations:** 

**Conclusion:** The neurosymbolic approach employed by TEN proves effective in overcoming the limitations of purely neural methods for tabular data extraction from complicated text formats.

**Abstract:** We present a neurosymbolic approach, TEN, for extracting tabular data from semistructured input text. This task is particularly challenging for text input that does not use special delimiters consistently to separate columns and rows. Purely neural approaches perform poorly due to hallucinations and their inability to enforce hard constraints. TEN uses Structural Decomposition prompting - a specialized chain-of-thought prompting approach - on a large language model (LLM) to generate an initial table, and thereafter uses a symbolic checker to evaluate not only the well-formedness of that table, but also detect cases of hallucinations or forgetting. The output of the symbolic checker is processed by a critique-LLM to generate guidance for fixing the table, which is presented to the original LLM in a self-debug loop. Our extensive experiments demonstrate that TEN significantly outperforms purely neural baselines across multiple datasets and metrics, achieving significantly higher exact match accuracy and substantially reduced hallucination rates. A 21-participant user study further confirms that TEN's tables are rated significantly more accurate (mean score: 5.0 vs 4.3; p = 0.021), and are consistently preferred for ease of verification and correction, with participants favoring our method in over 60% of the cases.

</details>


### [20] [Decoding Neural Emotion Patterns through Natural Language Processing Embeddings](https://arxiv.org/abs/2508.09337)

*Gideon Vos, Maryam Ebrahimpour, Liza van Eijk, Zoltan Sarnyai, Mostafa Rahimi Azghadi*

**Main category:** cs.CL

**Keywords:** emotional expression, computational neuroscience, text embeddings, brain mapping, large language models

**Relevance Score:** 8

**TL;DR:** The paper proposes a computational framework for mapping emotional content in text to brain regions without neuroimaging, using text embeddings and clustering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate emotional expression in language with brain function analysis in a cost-effective and scalable way, overcoming the limitations of traditional neuroimaging.

**Method:** The paper utilizes OpenAI's text-embedding-ada-002 to generate high-dimensional semantic representations of text, applies dimensionality reduction and clustering to identify emotional groups, and maps these to 18 brain regions associated with emotional processing.

**Key Contributions:**

	1. Developed a cost-effective framework for emotion-brain mapping using text data.
	2. Demonstrated the ability to differentiate emotional responses in clinical populations.
	3. Provided insights into LLM-generated text regarding emotional depth compared to human writing.

**Result:** The experiments revealed neuroanatomically plausible mappings of emotional content, with distinct differences in emotional activation patterns between healthy and depressed subjects, as well as variances between human and LLM-generated text.

**Limitations:** The study relies on text embeddings that may not capture all nuances of emotional language; also, there are potential biases in the datasets used.

**Conclusion:** The proposed framework allows for large-scale analysis of language and establishes a benchmark for evaluating AI's capability in emotional expression, particularly in health informatics.

**Abstract:** Understanding how emotional expression in language relates to brain function is a challenge in computational neuroscience and affective computing. Traditional neuroimaging is costly and lab-bound, but abundant digital text offers new avenues for emotion-brain mapping. Prior work has largely examined neuroimaging-based emotion localization or computational text analysis separately, with little integration. We propose a computational framework that maps textual emotional content to anatomically defined brain regions without requiring neuroimaging. Using OpenAI's text-embedding-ada-002, we generate high-dimensional semantic representations, apply dimensionality reduction and clustering to identify emotional groups, and map them to 18 brain regions linked to emotional processing. Three experiments were conducted: i) analyzing conversational data from healthy vs. depressed subjects (DIAC-WOZ dataset) to compare mapping patterns, ii) applying the method to the GoEmotions dataset and iii) comparing human-written text with large language model (LLM) responses to assess differences in inferred brain activation. Emotional intensity was scored via lexical analysis. Results showed neuroanatomically plausible mappings with high spatial specificity. Depressed subjects exhibited greater limbic engagement tied to negative affect. Discrete emotions were successfully differentiated. LLM-generated text matched humans in basic emotion distribution but lacked nuanced activation in empathy and self-referential regions (medial prefrontal and posterior cingulate cortex). This cost-effective, scalable approach enables large-scale analysis of naturalistic language, distinguishes between clinical populations, and offers a brain-based benchmark for evaluating AI emotional expression.

</details>


### [21] [The Human-AI Hybrid Delphi Model: A Structured Framework for Context-Rich, Expert Consensus in Complex Domains](https://arxiv.org/abs/2508.09349)

*Cathy Speed, Ahmed A. Metwally*

**Main category:** cs.CL

**Keywords:** Human-AI Hybrid, Delphi, generative AI, expert consensus, health informatics

**Relevance Score:** 8

**TL;DR:** This study presents a Human-AI Hybrid Delphi (HAH-Delphi) framework that integrates a generative AI model with expert panels to enhance consensus development in complex evidence domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for expert consensus is critical in areas where evidence is conflicting or insufficient, and traditional methods face limitations exacerbated by information overload.

**Method:** The HAH-Delphi framework involves using a generative AI model alongside small panels of senior experts to facilitate consensus development, tested through retrospective replication, prospective comparison, and applied deployment in specific domains.

**Key Contributions:**

	1. Introduction of the HAH-Delphi framework to integrate AI in expert consensus development.
	2. Demonstrated high levels of agreement between AI outputs and human expert consensus.
	3. Showed utility and effectiveness across multiple applied domains, including health and training.

**Result:** The AI replicated 95% of expert consensus conclusions and showed 95% agreement with human experts, while compact expert panels achieved over 90% consensus coverage before the last participant.

**Limitations:** AI's outputs lack experiential and pragmatic nuance, which was addressed by human expert panels.

**Conclusion:** The HAH-Delphi framework offers a robust, scalable method for high-quality consensus generation, particularly useful in health and performance science, supporting tailored guidance and frameworks.

**Abstract:** Expert consensus plays a critical role in domains where evidence is complex, conflicting, or insufficient for direct prescription. Traditional methods, such as Delphi studies, consensus conferences, and systematic guideline synthesis, offer structure but face limitations including high panel burden, interpretive oversimplification, and suppression of conditional nuance. These challenges are now exacerbated by information overload, fragmentation of the evidence base, and increasing reliance on publicly available sources that lack expert filtering. This study introduces and evaluates a Human-AI Hybrid Delphi (HAH-Delphi) framework designed to augment expert consensus development by integrating a generative AI model (Gemini 2.5 Pro), small panels of senior human experts, and structured facilitation. The HAH-Delphi was tested in three phases: retrospective replication, prospective comparison, and applied deployment in two applied domains (endurance training and resistance and mixed cardio/strength training). The AI replicated 95% of published expert consensus conclusions in Phase I and showed 95% directional agreement with senior human experts in Phase II, though it lacked experiential and pragmatic nuance. In Phase III, compact panels of six senior experts achieved >90% consensus coverage and reached thematic saturation before the final participant. The AI provided consistent, literature-grounded scaffolding that supported divergence resolution and accelerated saturation. The HAH-Delphi framework offers a flexible, scalable approach for generating high-quality, context-sensitive consensus. Its successful application across health, coaching, and performance science confirms its methodological robustness and supports its use as a foundation for generating conditional, personalised guidance and published consensus frameworks at scale.

</details>


### [22] [Flow-SLM: Joint Learning of Linguistic and Acoustic Information for Spoken Language Modeling](https://arxiv.org/abs/2508.09350)

*Ju-Chieh Chou, Jiawei Zhou, Karen Livescu*

**Main category:** cs.CL

**Keywords:** Spoken language models, Textless learning, Acoustic representation, Machine learning, Natural language processing

**Relevance Score:** 7

**TL;DR:** This paper presents a joint modeling approach for textless spoken language models (SLMs) that combines semantic token generation with acoustic frame prediction, achieving improved acoustic detail in speech generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current textless SLMs lack access to acoustic context, limiting their acoustic detail in generated speech. This work aims to address that limitation by jointly modeling linguistic and acoustic information.

**Method:** The authors propose a flow-matching objective that predicts a continuous acoustic representation conditioned on generated semantic tokens and explore the design space to maximize linguistic information preservation.

**Key Contributions:**

	1. Jointly models linguistic and acoustic information in SLMs
	2. Predicts multiple future semantic tokens to enhance linguistic information preservation
	3. Uses flow-matching to predict continuous acoustic representation effectively

**Result:** The proposed approach achieves comparable performance to existing models on linguistic likelihood benchmarks while providing enhanced acoustic detail in generated speech.

**Limitations:** 

**Conclusion:** By jointly modeling linguistic and acoustic aspects, the new SLM approach improves the quality of the generated speech without needing text supervision.

**Abstract:** Textless spoken language models (SLMs) are generative models of speech that do not rely on text supervision. Most textless SLMs learn to predict the next semantic token, a discrete representation of linguistic content, and rely on a separate vocoder to add acoustic information to the generated speech. Such models have no access to acoustic context and no built-in control over acoustic details. In this work, we propose to jointly model linguistic and acoustic information by generating semantic tokens and a continuous real-valued representation of the acoustic frame. We use a flow-matching objective to predict the continuous vector conditioned on the semantic tokens. We study the design space of this approach and find that predicting multiple future semantic tokens helps preserve linguistic information. Our approach achieves comparable performance to existing models in terms of linguistic likelihood benchmarks, while providing better acoustic detail in prompted generation.

</details>


### [23] [APIO: Automatic Prompt Induction and Optimization for Grammatical Error Correction and Text Simplification](https://arxiv.org/abs/2508.09378)

*Artem Chernodub, Aman Saini, Yejin Huh, Vivek Kulkarni, Vipul Raheja*

**Main category:** cs.CL

**Keywords:** large language models, natural language processing, prompt engineering, grammatical error correction, text simplification

**Relevance Score:** 8

**TL;DR:** This paper introduces APIO, an automatic prompt induction and optimization approach for Grammatical Error Correction (GEC) and Text Simplification that outperforms existing LLM-prompting methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance LLM performance through effective prompt engineering without manual seed prompts is the primary motivation of this research.

**Method:** APIO employs a novel automatic prompt induction methodology that refines prompts specifically for GEC and Text Simplification tasks without relying on predefined prompts.

**Key Contributions:**

	1. Introduction of APIO for prompt induction and optimization
	2. Achieving new state-of-the-art performance on GEC and Text Simplification
	3. Public availability of data, code, and outputs for community use

**Result:** APIO achieves a new state-of-the-art performance in LLM-based prompting for the tasks of Grammatical Error Correction and Text Simplification.

**Limitations:** 

**Conclusion:** The efficacy of APIO demonstrates that automatic prompt induction can significantly improve LLM task performance while providing publicly available resources for further research.

**Abstract:** Recent advancements in large language models (LLMs) have enabled a wide range of natural language processing (NLP) tasks to be performed through simple prompt-based interactions. Consequently, several approaches have been proposed to engineer prompts that most effectively enable LLMs to perform a given task (e.g., chain-of-thought prompting). In settings with a well-defined metric to optimize model performance, automatic prompt optimization (APO) methods have been developed to refine a seed prompt. Advancing this line of research, we propose APIO, a simple but effective prompt induction and optimization approach for the tasks of Grammatical Error Correction (GEC) and Text Simplification, without relying on manually specified seed prompts. APIO achieves a new state-of-the-art performance for purely LLM-based prompting methods on these tasks. We make our data, code, prompts, and outputs publicly available.

</details>


### [24] [Columbo: Expanding Abbreviated Column Names for Tabular Data Using Large Language Models](https://arxiv.org/abs/2508.09403)

*Ting Cai, Stephen Sheen, AnHai Doan*

**Main category:** cs.CL

**Keywords:** column name expansion, language model, data accuracy, dataset improvement, HCI

**Relevance Score:** 6

**TL;DR:** This paper introduces Columbo, an LLM-based solution for expanding abbreviated column names in datasets, overcoming limitations of previous methods and introducing new datasets and accuracy measures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for expanding abbreviated column names in data tables is critical for various applications across enterprises, sciences, and government agencies.

**Method:** The paper develops Columbo, an LLM-based system that utilizes context, rules, chain-of-thought reasoning, and token-level analysis to achieve accurate column name expansions.

**Key Contributions:**

	1. Introduction of four new datasets with real-world abbreviations
	2. Proposal of new synonym-aware accuracy measures
	3. Development of Columbo, an advanced LLM-based solution for column name expansion

**Result:** Columbo outperforms the current leading solution, NameGuess, by 4-29% across five datasets.

**Limitations:** 

**Conclusion:** The introduction of new datasets and improved evaluation metrics alongside the Columbo solution can enhance the effectiveness of data processing tasks across multiple domains.

**Abstract:** Expanding the abbreviated column names of tables, such as ``esal'' to ``employee salary'', is critical for numerous downstream data tasks. This problem arises in enterprises, domain sciences, government agencies, and more. In this paper we make three contributions that significantly advances the state of the art. First, we show that synthetic public data used by prior work has major limitations, and we introduce 4 new datasets in enterprise/science domains, with real-world abbreviations. Second, we show that accuracy measures used by prior work seriously undercount correct expansions, and we propose new synonym-aware measures that capture accuracy much more accurately. Finally, we develop Columbo, a powerful LLM-based solution that exploits context, rules, chain-of-thought reasoning, and token-level analysis. Extensive experiments show that Columbo significantly outperforms NameGuess, the current most advanced solution, by 4-29\%, over 5 datasets. Columbo has been used in production on EDI, a major data portal for environmental sciences.

</details>


### [25] [Leveraging Zipformer Model for Effective Language Identification in Code-Switched Child-Directed Speech](https://arxiv.org/abs/2508.09430)

*Lavanya Shankar, Leibny Paola Garcia Perera*

**Main category:** cs.CL

**Keywords:** code-switching, language identification, Zipformer, Mandarin, English

**Relevance Score:** 6

**TL;DR:** This paper explores the use of Zipformer for language identification in child-directed speech containing Mandarin and English, achieving significant improvements in accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of code-switching and language identification in bilingual environments, particularly for child-directed speech.

**Method:** Utilizing the Zipformer architecture to capture and encode the nuances of speech in two imbalanced languages, Mandarin and English, by selecting appropriate inner layers for embedding extraction.

**Key Contributions:**

	1. Demonstrated the effectiveness of Zipformer for language identification in bilingual child-directed scenarios.
	2. Showcased a methodology for selecting inner layers to enhance language encoding.
	3. Achieved significant improvements in identification accuracy over baseline models.

**Result:** Achieved a Balanced Accuracy (BAC) of 81.89%, representing a 15.47% improvement over existing language identification baselines.

**Limitations:** 

**Conclusion:** The results support the effectiveness of Zipformer in handling imbalanced data and highlight the applicability of transformer models in real-world language identification tasks.

**Abstract:** Code-switching and language identification in child-directed scenarios present significant challenges, particularly in bilingual environments. This paper addresses this challenge by using Zipformer to handle the nuances of speech, which contains two imbalanced languages, Mandarin and English, in an utterance. This work demonstrates that the internal layers of the Zipformer effectively encode the language characteristics, which can be leveraged in language identification. We present the selection methodology of the inner layers to extract the embeddings and make a comparison with different back-ends. Our analysis shows that Zipformer is robust across these backends. Our approach effectively handles imbalanced data, achieving a Balanced Accuracy (BAC) of 81.89%, a 15.47% improvement over the language identification baseline. These findings highlight the potential of the transformer encoder architecture model in real scenarios.

</details>


### [26] [From Charts to Fair Narratives: Uncovering and Mitigating Geo-Economic Biases in Chart-to-Text](https://arxiv.org/abs/2508.09450)

*Ridwan Mahbub, Mohammed Saidul Islam, Mir Tafseer Nayeem, Md Tahmid Rahman Laskar, Mizanur Rahman, Shafiq Joty, Enamul Hoque*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, geo-economic bias, chart summarization, debiasing, natural language generation

**Relevance Score:** 6

**TL;DR:** The paper examines geo-economic biases in Vision-Language Models (VLMs) that generate summaries of charts, revealing that VLMs tend to favor high-income countries in their summaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how large Vision-Language Models may perpetuate geo-economic biases when summarizing charts, which can lead to societal harm.

**Method:** A large-scale evaluation of VLM-generated chart summaries was conducted across 6,000 chart-country pairs from various models to assess the impact of a country's economic status on summary sentiment.

**Key Contributions:**

	1. Identification of geo-economic bias in VLM-generated chart summaries
	2. Demonstration of varying degrees of bias among different VLMs
	3. Exploration of partial effectiveness of debiasing techniques

**Result:** The findings indicate that VLMs generate more positive summaries for high-income countries compared to their middle- or low-income counterparts. Models like GPT-4o-mini and Gemini-1.5-Flash were found to exhibit different levels of bias.

**Limitations:** The study primarily focuses on geo-economic bias, without addressing other potential biases or the broader implications of VLM misrepresentation.

**Conclusion:** The study highlights the necessity for more effective debiasing strategies, as current prompt-based techniques show limited effectiveness in mitigating biases in VLM outputs.

**Abstract:** Charts are very common for exploring data and communicating insights, but extracting key takeaways from charts and articulating them in natural language can be challenging. The chart-to-text task aims to automate this process by generating textual summaries of charts. While with the rapid advancement of large Vision-Language Models (VLMs), we have witnessed great progress in this domain, little to no attention has been given to potential biases in their outputs. This paper investigates how VLMs can amplify geo-economic biases when generating chart summaries, potentially causing societal harm. Specifically, we conduct a large-scale evaluation of geo-economic biases in VLM-generated chart summaries across 6,000 chart-country pairs from six widely used proprietary and open-source models to understand how a country's economic status influences the sentiment of generated summaries. Our analysis reveals that existing VLMs tend to produce more positive descriptions for high-income countries compared to middle- or low-income countries, even when country attribution is the only variable changed. We also find that models such as GPT-4o-mini, Gemini-1.5-Flash, and Phi-3.5 exhibit varying degrees of bias. We further explore inference-time prompt-based debiasing techniques using positive distractors but find them only partially effective, underscoring the complexity of the issue and the need for more robust debiasing strategies. Our code and dataset are publicly available here.

</details>


### [27] [User-centric Subjective Leaderboard by Customizable Reward Modeling](https://arxiv.org/abs/2508.09463)

*Qi Jia, Xiujie Song, Zicheng Zhang, Yijin Guo, Kaiwei Zhang, Zijian Chen, Guangtao Zhai*

**Main category:** cs.CL

**Keywords:** large language models, user-centric leaderboard, customizable reward models, human preferences, subjective evaluation

**Relevance Score:** 9

**TL;DR:** Introduction of a User-Centric Subjective Leaderboard (USL) for ranking large language models (LLMs) based on human preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM selection for users by moving beyond static, verifiable tasks to a more subjective evaluation based on individual needs.

**Method:** Development of the USL based on over 10,000 subjective queries and the introduction of Customizable Reward Models (CRMs) with 4B parameters for enhanced ranking accuracy.

**Key Contributions:**

	1. Creation of User-Centric Subjective Leaderboard (USL)
	2. Introduction of Customizable Reward Models (CRMs)
	3. Demonstration of CRMs outperforming established LLMs

**Result:** CRMs exceed the performance of leading LLMs like GPT-4.1 and Gemini-2.5-pro, demonstrating improved generalization over diverse topics and criteria.

**Limitations:** 

**Conclusion:** The USL, driven by CRMs, efficiently captures human preferences and improves model selection for practical applications.

**Abstract:** Existing benchmarks for large language models (LLMs) predominantely focus on assessing their capabilities through verifiable tasks. Such objective and static benchmarks offer limited utility for practical LLM selection, making it difficult for users to find suitable models for their individual needs. To bridge this gap, we present the first User-Centric Subjective Leaderboard (USL), which provides a preference-driven, dynamic ranking of LLMs across diverse real-world scenarios. Our work is built upon a thorough investigation of real human preference data, involving more than 10K subjective queries. Our investigation reveals significant diversity and contradictions in human preferences, which limit the effectiveness of state-of-the-art reward models. To address this, we introduce Customizable Reward Models (CRMs). With only 4B parameters, our CRM surpasses the performance of leading models such as GPT-4.1 and Gemini-2.5-pro, showing exceptional generalization capabilities across new topics and criteria. The USL, powered by CRMs, exhibits strong negative correlations to contradictory preferences.

</details>


### [28] [Learning Facts at Scale with Active Reading](https://arxiv.org/abs/2508.09494)

*Jessy Lin, Vincent-Pierre Berges, Xilun Chen, Wen-Tau Yih, Gargi Ghosh, Barlas Oğuz*

**Main category:** cs.CL

**Keywords:** Active Reading, Knowledge recall, Model training, Natural language processing, Factual accuracy

**Relevance Score:** 9

**TL;DR:** The paper presents Active Reading, a framework for training models to reliably learn and recall knowledge from a specified body of material, resulting in significant improvements in model performance on factual tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Practitioners lack reliable tools for ensuring that models learn knowledge consistently from their training data.

**Method:** The authors propose a framework called Active Reading, which trains models to study specified material using self-generated learning strategies.

**Key Contributions:**

	1. Introduction of the Active Reading framework
	2. Demonstrated significant performance improvements over conventional fine-tuning
	3. Release of Meta WikiExpert-8B, outperforming larger models in factual QA tasks.

**Result:** Models trained with Active Reading show marked improvement, achieving 66% on SimpleQA and 26% on FinanceBench, compared to vanilla finetuning, with a 313% and 160% relative improvement, respectively.

**Limitations:** 

**Conclusion:** Active Reading can enhance knowledge absorption in models and has been effectively scaled to improve factual accuracy in larger models like Meta WikiExpert-8B.

**Abstract:** LLMs are known to store vast amounts of knowledge in their parametric memory. However, learning and recalling facts from this memory is known to be unreliable, depending largely on the prevalence of particular facts in the training data and other factors which are poorly understood. Practitioners are lacking tools which will allow them to ensure that the models learn a given body of knowledge reliably and consistently. To this end, we propose Active Reading: a framework where we train models to study a given set of material with self-generated learning strategies. First, we demonstrate models trained with Active Reading on expert domains absorb significantly more knowledge than vanilla finetuning and other data augmentations. We train expert 8B models that achieve 66% on a Wikipedia-grounded subset of SimpleQA (+313% relative over vanilla finetuning) and 26% on FinanceBench (+160% relative over vanilla finetuning) by applying Active Reading to the source documents for each benchmark. Finally, we show that Active Reading can be utilized at pre-training scale to build more factual models. As a demonstration of this, we release Meta WikiExpert-8B, a Wikipedia-expert model trained on 1 trillion generated tokens, which outcompetes models with hundreds of billions of parameters on factual QA.

</details>


### [29] [From Ranking to Selection: A Simple but Efficient Dynamic Passage Selector for Retrieval Augmented Generation](https://arxiv.org/abs/2508.09497)

*Siyuan Meng, Junming Liu, Yirong Chen, Song Mao, Pinlong Cai, Guohang Yan, Botian Shi, Ding Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Reranking, Dynamic Passage Selector, Multi-hop queries, Supervised learning

**Relevance Score:** 9

**TL;DR:** The paper presents the Dynamic Passage Selector (DPS), a new reranking framework for retrieval-augmented generation systems that improves passage selection for complex queries by capturing inter-passage dependencies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional reranking approaches in retrieval-augmented generation systems are limited by their fixed Top-K selection and inability to effectively manage multi-hop queries, resulting in critical information loss or too much noise.

**Method:** DPS treats passage selection as a supervised learning problem, enabling fine-tuning to dynamically select the most relevant passages based on inter-passage dependencies, thus improving the RAG pipeline.

**Key Contributions:**

	1. Introduction of the Dynamic Passage Selector (DPS) for RAG systems
	2. Demonstration of significant performance improvements on multiple benchmarks
	3. Development of a method that captures inter-passage dependencies for better evidence selection

**Result:** DPS outperforms existing state-of-the-art rerankers by significantly increasing the F1-score on the MuSiQue dataset by over 30% compared to strong baselines, demonstrating improved reasoning in complex information synthesis tasks.

**Limitations:** 

**Conclusion:** By enabling adaptive evidence selection through a novel supervised approach, DPS significantly enhances the capabilities of retrieval-augmented generation systems, particularly in handling complex queries.

**Abstract:** Retrieval-augmented generation (RAG) systems are often bottlenecked by their reranking modules, which typically score passages independently and select a fixed Top-K size. This approach struggles with complex multi-hop queries that require synthesizing evidence across multiple documents, creating a trade-off where small K values omit crucial information and large K values introduce noise. To address this, we introduce the Dynamic Passage Selector (DPS), a novel reranking framework that treats passage selection as a supervised learning problem. Unlike traditional point-wise or list-wise methods, DPS is fine-tuned to capture inter-passage dependencies and dynamically select the most relevant set of passages for generation. As a seamless plug-and-play module, DPS requires no modifications to the standard RAG pipeline. Comprehensive evaluations on five benchmarks show that DPS consistently outperforms state-of-the-art rerankers and fine-tuning methods. Notably, on the challenging MuSiQue dataset, DPS improves the F1-score by 30.06% and 15.4% over strong baselines like Qwen3-reranker and RankingGPT, respectively. Our results demonstrate that by enabling adaptive evidence selection, DPS substantially enhances reasoning capabilities in complex RAG scenarios.

</details>


### [30] [LACA: Improving Cross-lingual Aspect-Based Sentiment Analysis with LLM Data Augmentation](https://arxiv.org/abs/2508.09515)

*Jakub Šmíd, Pavel Přibáň, Pavel Král*

**Main category:** cs.CL

**Keywords:** cross-lingual, aspect-based sentiment analysis, large language models, pseudo-labelling, natural language generation

**Relevance Score:** 9

**TL;DR:** The paper presents a novel approach for cross-lingual aspect-based sentiment analysis (ABSA) using large language models (LLMs) to generate high-quality pseudo-labelled data, avoiding reliance on translation tools.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for cross-lingual ABSA often rely on unreliable translation tools, which can compromise the quality of sentiment analysis.

**Method:** The proposed framework first trains an ABSA model on unlabelled target language data and then uses an LLM to generate natural sentences that improve upon noisy predictions. The model is subsequently fine-tuned on this pseudo-labelled dataset.

**Key Contributions:**

	1. Introduces a method to generate high-quality pseudo-labelled data for ABSA using LLMs.
	2. Demonstrates effectiveness across multiple languages and model architectures.
	3. Provides an alternative to reliance on potentially unreliable translation tools.

**Result:** The method was tested across six languages and five backbone models, showing improved effectiveness compared to traditional translation-based approaches.

**Limitations:** 

**Conclusion:** The framework not only enhances sentiment analysis accuracy across languages but also shows that fine-tuned LLMs can outperform smaller multilingual models.

**Abstract:** Cross-lingual aspect-based sentiment analysis (ABSA) involves detailed sentiment analysis in a target language by transferring knowledge from a source language with available annotated data. Most existing methods depend heavily on often unreliable translation tools to bridge the language gap. In this paper, we propose a new approach that leverages a large language model (LLM) to generate high-quality pseudo-labelled data in the target language without the need for translation tools. First, the framework trains an ABSA model to obtain predictions for unlabelled target language data. Next, LLM is prompted to generate natural sentences that better represent these noisy predictions than the original text. The ABSA model is then further fine-tuned on the resulting pseudo-labelled dataset. We demonstrate the effectiveness of this method across six languages and five backbone models, surpassing previous state-of-the-art translation-based approaches. The proposed framework also supports generative models, and we show that fine-tuned LLMs outperform smaller multilingual models.

</details>


### [31] [Cross-lingual Aspect-Based Sentiment Analysis: A Survey on Tasks, Approaches, and Challenges](https://arxiv.org/abs/2508.09516)

*Jakub Šmíd, Pavel Král*

**Main category:** cs.CL

**Keywords:** aspect-based sentiment analysis, cross-lingual, large language models, sentiment classification, natural language processing

**Relevance Score:** 4

**TL;DR:** This paper surveys cross-lingual aspect-based sentiment analysis (ABSA), a field underexplored compared to monolingual ABSA, summarizing tasks, datasets, models, and challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of systematic reviews in the field of cross-lingual aspect-based sentiment analysis (ABSA) and highlight its importance in transferring knowledge from rich to low-resource languages.

**Method:** The paper reviews key tasks in ABSA, such as aspect term extraction and sentiment classification, and examines relevant datasets and modeling approaches, including the role of large language models (LLMs) in advancing cross-lingual ABSA.

**Key Contributions:**

	1. Comprehensive survey of cross-lingual ABSA
	2. Insight into datasets and modeling paradigms
	3. Identification of challenges and future research directions

**Result:** The paper identifies and summarizes the main tasks in cross-lingual ABSA, providing insights into current methodologies, datasets used, and existing challenges in the field.

**Limitations:** Limited focus on the technical details of cross-lingual model implementation and performance metrics.

**Conclusion:** This survey lays the groundwork for future research in cross-lingual ABSA by identifying critical areas for advancement and existing research gaps.

**Abstract:** Aspect-based sentiment analysis (ABSA) is a fine-grained sentiment analysis task that focuses on understanding opinions at the aspect level, including sentiment towards specific aspect terms, categories, and opinions. While ABSA research has seen significant progress, much of the focus has been on monolingual settings. Cross-lingual ABSA, which aims to transfer knowledge from resource-rich languages (such as English) to low-resource languages, remains an under-explored area, with no systematic review of the field. This paper aims to fill that gap by providing a comprehensive survey of cross-lingual ABSA. We summarize key ABSA tasks, including aspect term extraction, aspect sentiment classification, and compound tasks involving multiple sentiment elements. Additionally, we review the datasets, modelling paradigms, and cross-lingual transfer methods used to solve these tasks. We also examine how existing work in monolingual and multilingual ABSA, as well as ABSA with LLMs, contributes to the development of cross-lingual ABSA. Finally, we highlight the main challenges and suggest directions for future research to advance cross-lingual ABSA systems.

</details>


### [32] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)

*Mahdi Dhaini, Tobias Müller, Roksoliana Rabets, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** explainable NLP, practitioner perspectives, transparency, user-centric frameworks, real-world applications

**Relevance Score:** 8

**TL;DR:** This paper investigates practitioners' experiences with explainable NLP methods, focusing on adoption motivations, techniques used, satisfaction levels, and challenges faced.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for transparency and explanations in complex NLP models, especially in high-stakes environments, yet practitioners' perspectives on explainable NLP are underexplored.

**Method:** The study employs qualitative interviews with industry practitioners and academic researchers to analyze and compare their experiences with explainability methods in NLP applications.

**Key Contributions:**

	1. Identification of practitioners' motivations for adopting explainability methods
	2. Analysis of satisfaction levels with current explainability techniques
	3. Highlighting practical challenges in real-world NLP applications

**Result:** Findings reveal conceptual gaps in understanding explainability, low satisfaction with existing methods, and highlight significant evaluation challenges faced by practitioners.

**Limitations:** Focuses primarily on qualitative insights, which may not generalize across all NLP contexts.

**Conclusion:** The study underscores the necessity for user-centric frameworks and clearer definitions to enhance the practical adoption of explainable NLP.

**Abstract:** The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners' perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners' experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice.

</details>


### [33] [UWBa at SemEval-2025 Task 7: Multilingual and Crosslingual Fact-Checked Claim Retrieval](https://arxiv.org/abs/2508.09517)

*Ladislav Lenc, Daniel Cífka, Jiří Martínek, Jakub Šmíd, Pavel Král*

**Main category:** cs.CL

**Keywords:** fact-checking, claim retrieval, large language models, text embeddings, cosine similarity

**Relevance Score:** 8

**TL;DR:** This paper introduces a zero-shot system for fact-checked claim retrieval using combined large language model embeddings, achieving top placements in relevant tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and effectiveness of claim retrieval in a fact-checking context using advanced large language models.

**Method:** Utilizing multiple state-of-the-art large language models to obtain text embeddings, which are then combined to enhance retrieval performance based on cosine similarity of the embeddings.

**Key Contributions:**

	1. Introduction of a zero-shot system for claim retrieval
	2. Use of combined large language model embeddings
	3. Demonstrated improved retrieval performance over baseline models

**Result:** Achieved 7th place in monolingual and 9th in cross-lingual subtasks of claim retrieval. The best results were obtained using the NVIDIA NV-Embed-v2 model, with enhancements from model combinations in certain languages.

**Limitations:** 

**Conclusion:** The study demonstrates that combining various text embedding models can significantly improve the retrieval of relevant claims, particularly in a zero-shot setting.

**Abstract:** This paper presents a zero-shot system for fact-checked claim retrieval. We employed several state-of-the-art large language models to obtain text embeddings. The models were then combined to obtain the best possible result. Our approach achieved 7th place in monolingual and 9th in cross-lingual subtasks. We used only English translations as an input to the text embedding models since multilingual models did not achieve satisfactory results. We identified the most relevant claims for each post by leveraging the embeddings and measuring cosine similarity. Overall, the best results were obtained by the NVIDIA NV-Embed-v2 model. For some languages, we benefited from model combinations (NV-Embed & GPT or Mistral).

</details>


### [34] [COMPEER: Controllable Empathetic Reinforcement Reasoning for Emotional Support Conversation](https://arxiv.org/abs/2508.09521)

*Yunxiao Wang, Meng Liu, Wenqi Liu, Kaiyu Jiang, Bin Wen, Fan Yang, Tingting Gao, Guorui Zhou, Liqiang Nie*

**Main category:** cs.CL

**Keywords:** empathetic reasoning, emotional support, reinforcement learning, human-like interaction, dialogue systems

**Relevance Score:** 9

**TL;DR:** This paper proposes controllable empathetic reasoning for enhancing emotional support conversations in AI systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Emotional support conversations are important for well-being but lack deep empathetic reasoning in current models.

**Method:** The authors created a fine-grained dataset annotated with reasoning correctness and response preferences, and employed reinforcement learning combined with a unified process-outcome reward model.

**Key Contributions:**

	1. Introduction of controllable empathetic reasoning
	2. Creation of a fine-grained dataset for enhancing empathetic dialogue
	3. Use of personality-based dialogue rewriting and redundancy-aware reward strategies

**Result:** The proposed methods significantly enhance the emotional support ability of AI models, creating more empathetic, human-like interactions.

**Limitations:** 

**Conclusion:** The advancements made set a foundation for developing better empathetic support systems in AI.

**Abstract:** Emotional support conversations are crucial for promoting emotional well-being, yet current models often lack deep empathetic reasoning grounded in psychological principles. To address this, we propose controllable empathetic reasoning, which combines natural language reasoning with structured psychological steps. We construct a fine-grained dataset annotated with reasoning correctness and response preferences to enable this capability. To further enhance training, we employ reinforcement learning with a unified process-outcome reward model that delivers precise feedback. To mitigate response repetitiveness from entropy collapse, we introduce personality-based dialogue rewriting and a redundancy-aware reward reweighting strategy. Our approach significantly improves model's emotional support ability, advancing the development of empathetic, human-like support systems.

</details>


### [35] [The Surprising Effectiveness of Membership Inference with Simple N-Gram Coverage](https://arxiv.org/abs/2508.09603)

*Skyler Hallinan, Jaehun Jung, Melanie Sclar, Ximing Lu, Abhilasha Ravichander, Sahana Ramnath, Yejin Choi, Sai Praneeth Karimireddy, Niloofar Mireshghallah, Xiang Ren*

**Main category:** cs.CL

**Keywords:** membership inference, N-Gram Coverage Attack, privacy protection, black-box models, language models

**Relevance Score:** 8

**TL;DR:** The N-Gram Coverage Attack enables membership inference attacks on black-box language models using only text outputs, outperforming existing methods and highlighting evolving privacy protections in models like GPT-4.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a method for membership inference attacks on black-box models that do not require access to hidden states or probability distributions.

**Method:** The method uses n-gram overlap metrics to assess the similarity of model outputs to a candidate ground truth, inferring membership based on the extent of overlap in generated text patterns.

**Key Contributions:**

	1. Introduced a novel membership inference attack for black-box models using n-gram overlap.
	2. Demonstrated superior performance on existing benchmarks compared to other black-box methods.
	3. Investigated privacy of previously unstudied closed models, revealing trends in model robustness.

**Result:** The N-Gram Coverage Attack outperforms existing black-box methods and achieves performance comparable to white-box attacks using text outputs alone. The attack's success rate improves with the compute budget used for generating outputs.

**Limitations:** 

**Conclusion:** The method effectively investigates membership inference in closed models like GPT-4, suggesting enhanced privacy measures in more recent models.

**Abstract:** Membership inference attacks serves as useful tool for fair use of language models, such as detecting potential copyright infringement and auditing data leakage. However, many current state-of-the-art attacks require access to models' hidden states or probability distribution, which prevents investigation into more widely-used, API-access only models like GPT-4. In this work, we introduce N-Gram Coverage Attack, a membership inference attack that relies solely on text outputs from the target model, enabling attacks on completely black-box models. We leverage the observation that models are more likely to memorize and subsequently generate text patterns that were commonly observed in their training data. Specifically, to make a prediction on a candidate member, N-Gram Coverage Attack first obtains multiple model generations conditioned on a prefix of the candidate. It then uses n-gram overlap metrics to compute and aggregate the similarities of these outputs with the ground truth suffix; high similarities indicate likely membership. We first demonstrate on a diverse set of existing benchmarks that N-Gram Coverage Attack outperforms other black-box methods while also impressively achieving comparable or even better performance to state-of-the-art white-box attacks - despite having access to only text outputs. Interestingly, we find that the success rate of our method scales with the attack compute budget - as we increase the number of sequences generated from the target model conditioned on the prefix, attack performance tends to improve. Having verified the accuracy of our method, we use it to investigate previously unstudied closed OpenAI models on multiple domains. We find that more recent models, such as GPT-4o, exhibit increased robustness to membership inference, suggesting an evolving trend toward improved privacy protections.

</details>


### [36] [AINL-Eval 2025 Shared Task: Detection of AI-Generated Scientific Abstracts in Russian](https://arxiv.org/abs/2508.09622)

*Tatiana Batura, Elena Bruches, Milana Shvenk, Valentin Malykh*

**Main category:** cs.CL

**Keywords:** AI-generated content, scientific abstracts, large language models, detection, shared task

**Relevance Score:** 9

**TL;DR:** This paper presents the AINL-Eval 2025 Shared Task focused on detecting AI-generated scientific abstracts in Russian, utilizing a large dataset and fostering ongoing research in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large language models (LLMs) complicates the distinction between human and AI-generated text, posing challenges to academic integrity in multilingual contexts.

**Method:** The paper introduces a large dataset of 52,305 samples containing both human-written and AI-generated scientific abstracts across 12 domains, designed for a shared task that encourages robust solution development for detecting AI content.

**Key Contributions:**

	1. Introduction of a large-scale dataset for detecting AI-generated academic content in Russian.
	2. Successful organization of a competitive shared task with active participation from multiple teams.
	3. Creation of a continuous platform for ongoing research in AI detection methods.

**Result:** Ten teams participated in the task, with 159 submissions demonstrating effective identification of AI-generated scientific abstracts.

**Limitations:** 

**Conclusion:** The establishment of a shared task platform aims to support continuous research and advancements in detecting AI-generated content.

**Abstract:** The rapid advancement of large language models (LLMs) has revolutionized text generation, making it increasingly difficult to distinguish between human- and AI-generated content. This poses a significant challenge to academic integrity, particularly in scientific publishing and multilingual contexts where detection resources are often limited. To address this critical gap, we introduce the AINL-Eval 2025 Shared Task, specifically focused on the detection of AI-generated scientific abstracts in Russian. We present a novel, large-scale dataset comprising 52,305 samples, including human-written abstracts across 12 diverse scientific domains and AI-generated counterparts from five state-of-the-art LLMs (GPT-4-Turbo, Gemma2-27B, Llama3.3-70B, Deepseek-V3, and GigaChat-Lite). A core objective of the task is to challenge participants to develop robust solutions capable of generalizing to both (i) previously unseen scientific domains and (ii) models not included in the training data. The task was organized in two phases, attracting 10 teams and 159 submissions, with top systems demonstrating strong performance in identifying AI-generated content. We also establish a continuous shared task platform to foster ongoing research and long-term progress in this important area. The dataset and platform are publicly available at https://github.com/iis-research-team/AINL-Eval-2025.

</details>


### [37] [Improving Diversity in Language Models: When Temperature Fails, Change the Loss](https://arxiv.org/abs/2508.09654)

*Alexandre Verine, Florian Le Bronnec, Kunhao Zheng, Alexandre Allauzen, Yann Chevaleyre, Benjamin Negrevergne*

**Main category:** cs.CL

**Keywords:** language models, temperature adjustments, Precision-Recall framework

**Relevance Score:** 8

**TL;DR:** This paper explores the effects of temperature adjustments on language model performance, revealing that decreasing temperature can improve Precision, while increasing it often fails to enhance Recall. It proposes a novel approach to loss functions that improves the Precision-Recall trade-off.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to increase diversity in language models and understand the effects of temperature on performance metrics like Precision and Recall.

**Method:** The authors analyze temperature adjustments in language models and propose rethinking loss functions using the Precision-Recall framework to achieve better coverage.

**Key Contributions:**

	1. Proposed a method to rethink loss functions in language models using the Precision-Recall framework.
	2. Demonstrated that decreasing temperature can improve model quality.
	3. Achieved a better balance between Precision and Recall compared to conventional approaches.

**Result:** The proposed approach demonstrates a significantly improved trade-off between Precision and Recall compared to traditional methods combining negative log-likelihood training with temperature scaling.

**Limitations:** 

**Conclusion:** Revising loss functions in language models can lead to more effective tuning through temperature adjustments, enhancing both Precision and Recall.

**Abstract:** Increasing diversity in language models is a challenging yet essential objective. A common approach is to raise the decoding temperature. In this work, we investigate this approach through a simplistic yet common case to provide insights into why decreasing temperature can improve quality (Precision), while increasing it often fails to boost coverage (Recall). Our analysis reveals that for a model to be effectively tunable through temperature adjustments, it must be trained toward coverage. To address this, we propose rethinking loss functions in language models by leveraging the Precision-Recall framework. Our results demonstrate that this approach achieves a substantially better trade-off between Precision and Recall than merely combining negative log-likelihood training with temperature scaling. These findings offer a pathway toward more versatile and robust language modeling techniques.

</details>


### [38] [EffiEval: Efficient and Generalizable Model Evaluation via Capability Coverage Maximization](https://arxiv.org/abs/2508.09662)

*Yaoning Wang, Jiahao Ying, Yixin Cao, Yubo Ma, Yugang Jiang*

**Main category:** cs.CL

**Keywords:** evaluation, large language models, benchmarking, efficiency, fairness

**Relevance Score:** 9

**TL;DR:** EffiEval is a training-free methodology for efficient benchmarking of large language models (LLMs) that addresses data redundancy while ensuring high evaluation reliability, representativeness, and fairness.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the computational challenges posed by the rapid advancement of LLMs and the need for more efficient evaluation benchmarks.

**Method:** EffiEval adaptively selects representative subsets based on the Model Utility Index (MUI), without relying on large-scale evaluation data.

**Key Contributions:**

	1. Training-free approach for efficient benchmarking
	2. High evaluation reliability and fairness
	3. Flexibility to adapt across datasets without extensive data

**Result:** EffiEval maintains strong ranking consistency with full-dataset evaluations using only a small fraction of the original data, demonstrating its effectiveness.

**Limitations:** 

**Conclusion:** EffiEval offers a practical and generalizable solution for efficient model evaluation in the context of LLMs, allowing flexibility between evaluation efficiency and representativeness.

**Abstract:** The rapid advancement of large language models (LLMs) and the development of increasingly large and diverse evaluation benchmarks have introduced substantial computational challenges for model assessment. In this paper, we present EffiEval, a training-free approach for efficient benchmarking that effectively addresses data redundancy while maintaining high evaluation reliability. Our method is specifically designed to meet three key criteria for high-quality evaluation: representativeness, by ensuring comprehensive coverage of model capabilities; fairness, by remaining independent of model performance during sample selection to avoid bias; and generalizability, by enabling flexible transfer across datasets and model families without reliance on large-scale evaluation data. Unlike traditional methods that rely on absolute performance or require extensive evaluation data, our approach adaptively selects high-quality representative subsets based on the Model Utility Index (MUI). Extensive experiments on multiple public benchmarks and diverse LLMs demonstrate that EffiEval achieves strong ranking consistency with full-dataset evaluation using only a small fraction of the original data. Furthermore, our method is flexible and scalable in size, allowing users to balance evaluation efficiency and representativeness according to specific needs. Overall, EffiEval provides a practical and generalizable solution for reliable, fair, and efficient evaluation in the era of LLMs.

</details>


### [39] [Slow Tuning and Low-Entropy Masking for Safe Chain-of-Thought Distillation](https://arxiv.org/abs/2508.09666)

*Ziyang Ma, Qingyue Yuan, Linhai Zhang, Deyu Zhou*

**Main category:** cs.CL

**Keywords:** safe distillation, Small Language Models, reasoning capabilities, safety alignment, Low-Entropy Masking

**Relevance Score:** 8

**TL;DR:** This paper presents a safe distillation method, SLowED, for Small Language Models to enhance reasoning without compromising safety.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the negative safety effects that arise during the chain-of-thought distillation process for Small Language Models, which are often overlooked.

**Method:** The proposed method, SLowED, consists of two modules: Slow Tuning, which minimizes model weight changes, and Low-Entropy Masking, which avoids low-entropy tokens in the fine-tuning process.

**Key Contributions:**

	1. Introduction of SLowED as a safe distillation method
	2. Demonstration of improved reasoning while preserving safety
	3. Validation through comprehensive experimental benchmarks

**Result:** Experiments demonstrated that SLowED maintains the safety of SLMs while improving their reasoning capabilities compared to existing methods, validated across various benchmarks.

**Limitations:** 

**Conclusion:** SLowED effectively retains SLM safety and enhances reasoning abilities, with individual contributions of each module supporting safe training practices.

**Abstract:** Previous chain-of-thought (CoT) distillation methods primarily focused on enhancing the reasoning capabilities of Small Language Models (SLMs) by utilizing high-quality rationales generated by powerful Large Language Models (LLMs, e.g., GPT-4). However, few works have noted the negative effects on SLM safety brought by the training, which are revealed in this study. Although there are works on safety alignment that fine-tune language models or manipulate model weights to defend against harmful inputs, they require extra computation or annotated data, and probably impact the reasoning ability of SLMs. In this paper, we investigate how to maintain the safety of SLMs during the CoT distillation process. Specifically, we propose a safe distillation method, Slow Tuning and Low-Entropy Masking Distillation (SLowED), containing two modules: Slow Tuning and Low-Entropy Masking. Slow Tuning scales down the magnitude of model weight changes to optimize the model weights in the neighboring space near the initial weight distribution. Low-Entropy Masking masks low-entropy tokens, which are regarded as unnecessary learning targets, to exclude them from fine-tuning. Experiments on three SLMs (Qwen2.5-1.5B, Llama-3.2-1B, BLOOM-1.1B) across reasoning benchmarks (BBH, BB-Sub, ARC, AGIEval) and safety evaluation (AdvBench) show that SLowED retains the safety of SLMs and comparably improves their reasoning capability compared to existing distillation methods. Furthermore, our ablation study presents the effectiveness of Slow Tuning and Low-Entropy Masking, with the former maintaining the model's safety in the early stage and the latter prolonging the safe training epochs.

</details>


### [40] [Evaluating the Role of Large Language Models in Legal Practice in India](https://arxiv.org/abs/2508.09713)

*Rahul Hemrajani*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Large Language Models, Legal profession, Human expertise, Legal research

**Relevance Score:** 4

**TL;DR:** This paper evaluates the performance of Large Language Models (LLMs) in executing key legal tasks in India, finding that while they excel at drafting and issue spotting, they struggle with specialized legal research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the effectiveness of LLMs in legal tasks and assess their potential to assist legal professionals in India.

**Method:** A survey experiment comparing outputs of LLMs with those of a junior lawyer, rated by advanced law students on helpfulness, accuracy, and comprehensiveness.

**Key Contributions:**

	1. Empirical evaluation of LLMs in the Indian legal context
	2. Comparative analysis of LLMs and junior lawyers' outputs
	3. Insights into the limitations of LLM capabilities in specialized legal research.

**Result:** LLMs excel in legal drafting and issue spotting but frequently produce hallucinations and inaccuracies in specialized legal research.

**Limitations:** LLMs generate factually incorrect or fabricated outputs and are less reliable in specialized legal contexts.

**Conclusion:** LLMs can support certain legal tasks but cannot replace the essential human expertise required for nuanced legal reasoning and application of the law.

**Abstract:** The integration of Artificial Intelligence(AI) into the legal profession raises significant questions about the capacity of Large Language Models(LLM) to perform key legal tasks. In this paper, I empirically evaluate how well LLMs, such as GPT, Claude, and Llama, perform key legal tasks in the Indian context, including issue spotting, legal drafting, advice, research, and reasoning. Through a survey experiment, I compare outputs from LLMs with those of a junior lawyer, with advanced law students rating the work on helpfulness, accuracy, and comprehensiveness. LLMs excel in drafting and issue spotting, often matching or surpassing human work. However, they struggle with specialised legal research, frequently generating hallucinations, factually incorrect or fabricated outputs. I conclude that while LLMs can augment certain legal tasks, human expertise remains essential for nuanced reasoning and the precise application of law.

</details>


### [41] [The Perils of Chart Deception: How Misleading Visualizations Affect Vision-Language Models](https://arxiv.org/abs/2508.09716)

*Ridwan Mahbub, Mohammed Saidul Islam, Md Tahmid Rahman Laskar, Mizanur Rahman, Mir Tafseer Nayeem, Enamul Hoque*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, deceptive visualizations, information visualization, data interpretation, misinformation

**Relevance Score:** 8

**TL;DR:** This study investigates the susceptibility of Vision-Language Models (VLMs) to deceptive visualizations, finding that most VLMs misinterpret misleading chart designs, which can result in altered understandings despite consistent data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As VLMs are increasingly used by non-experts to interpret visualizations, understanding their susceptibility to deceptive visual designs is crucial to prevent misinformation.

**Method:** The study evaluates over 16,000 responses from 10 different VLMs across 8 types of misleading chart designs to assess their ability to interpret visualizations accurately.

**Key Contributions:**

	1. Evaluation of VLMs' interpretation of over 16,000 responses to deceptive visualizations
	2. Identification of multiple types of misleading chart designs that affect VLM performance
	3. Highlighting the necessity for robust mechanisms to protect against visual misinformation

**Result:** Most VLMs were found to be deceived by misleading visualizations, leading to incorrect interpretations of charts while the underlying data remained unchanged.

**Limitations:** Limited to specific types of misleading chart designs and VLMs analyzed; results may vary with different datasets or models.

**Conclusion:** The findings emphasize the urgent need for safeguards in VLMs to combat visual misinformation and improve accurate data interpretation.

**Abstract:** Information visualizations are powerful tools that help users quickly identify patterns, trends, and outliers, facilitating informed decision-making. However, when visualizations incorporate deceptive design elements-such as truncated or inverted axes, unjustified 3D effects, or violations of best practices-they can mislead viewers and distort understanding, spreading misinformation. While some deceptive tactics are obvious, others subtly manipulate perception while maintaining a facade of legitimacy. As Vision-Language Models (VLMs) are increasingly used to interpret visualizations, especially by non-expert users, it is critical to understand how susceptible these models are to deceptive visual designs. In this study, we conduct an in-depth evaluation of VLMs' ability to interpret misleading visualizations. By analyzing over 16,000 responses from ten different models across eight distinct types of misleading chart designs, we demonstrate that most VLMs are deceived by them. This leads to altered interpretations of charts, despite the underlying data remaining the same. Our findings highlight the need for robust safeguards in VLMs against visual misinformation.

</details>


### [42] [Sample More to Think Less: Group Filtered Policy Optimization for Concise Reasoning](https://arxiv.org/abs/2508.09726)

*Vaishnavi Shrivastava, Ahmed Awadallah, Vidhisha Balachandran, Shivam Garg, Harkirat Behl, Dimitris Papailiopoulos*

**Main category:** cs.CL

**Keywords:** Language Models, Reinforcement Learning, Token Efficiency

**Relevance Score:** 7

**TL;DR:** The paper introduces GFPO, a method to reduce response length in large language models by optimizing training based on response length and token efficiency, leading to significant reductions in output verbosity while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of excessive verbosity in responses generated by large language models trained with reinforcement learning, which can negatively impact clarity and utility.

**Method:** GFPO (Group Filtered Policy Optimization) involves sampling larger groups during training and filtering responses based on response length and reward per token ratio.

**Key Contributions:**

	1. Introduction of GFPO to reduce verbosity in language model responses.
	2. Demonstrated significant reductions in response length while maintaining accuracy across benchmarks.
	3. Proposed Adaptive Difficulty GFPO for improved efficiency in training on difficult problems.

**Result:** GFPO reduces length inflation by 46-71% on various STEM and coding benchmarks while maintaining accuracy, with further reductions of 71-85% when optimizing for reward per token.

**Limitations:** 

**Conclusion:** Increasing training-time compute leads to reduced test-time compute, providing a trade-off for efficient reasoning.

**Abstract:** Large language models trained with reinforcement learning with verifiable rewards tend to trade accuracy for length--inflating response lengths to achieve gains in accuracy. While longer answers may be warranted for harder problems, many tokens are merely "filler": repetitive, verbose text that makes no real progress. We introduce GFPO (Group Filtered Policy Optimization), which curbs this length explosion by sampling larger groups per problem during training and filtering responses to train on based on two key metrics: (1) response length and (2) token efficiency: reward per token ratio. By sampling more at training time, we teach models to think less at inference time. On the Phi-4-reasoning model, GFPO cuts GRPO's length inflation by 46-71% across challenging STEM and coding benchmarks (AIME 24/25, GPQA, Omni-MATH, LiveCodeBench) while maintaining accuracy. Optimizing for reward per token further increases reductions in length inflation to 71-85%. We also propose Adaptive Difficulty GFPO, which dynamically allocates more training resources to harder problems based on real-time difficulty estimates, improving the balance between computational efficiency and accuracy especially on difficult questions. GFPO demonstrates that increased training-time compute directly translates to reduced test-time compute--a simple yet effective trade-off for efficient reasoning.

</details>


### [43] [Transforming Questions and Documents for Semantically Aligned Retrieval-Augmented Generation](https://arxiv.org/abs/2508.09755)

*Seokgi Lee*

**Main category:** cs.CL

**Keywords:** RAG, multihop question answering, LLM, query decomposition, question embeddings

**Relevance Score:** 9

**TL;DR:** This paper presents a novel retrieval-augmented generation (RAG) framework designed for improved multihop question answering by decomposing complex questions and using question embeddings for document retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for enhanced performance in multihop question answering systems due to the challenges posed by ambiguity in complex queries.

**Method:** The framework utilizes large language models to break down complex questions into single-hop subquestions, generates answerable questions from document chunks for embedding, and retrieves relevant information based on question-question similarity.

**Key Contributions:**

	1. Introduction of a novel RAG framework for multihop QA
	2. Effective use of LLM for query decomposition
	3. The implementation of question-question embedding for document retrieval

**Result:** The proposed method demonstrated improved performance on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) compared to baseline systems.

**Limitations:** 

**Conclusion:** Utilizing answerable-question embeddings and LLM-based query decomposition significantly enhances the retrieval-augmented generation approach for multihop question answering.

**Abstract:** We introduce a novel retrieval-augmented generation (RAG) framework tailored for multihop question answering. First, our system uses large language model (LLM) to decompose complex multihop questions into a sequence of single-hop subquestions that guide document retrieval. This decomposition mitigates the ambiguity inherent in multi-hop queries by clearly targeting distinct knowledge facets. Second, instead of embedding raw or chunked documents directly, we generate answerable questions from each document chunk using Qwen3-8B, embed these generated questions, and retrieve relevant chunks via question-question embedding similarity. During inference, the retrieved chunks are then fed along with the original question into the RAG pipeline. We evaluate on three multihop question datasets (MuSiQue, 2WikiMultiHopQa, HotpotQA) from LongBench. Our method improves RAG performacne compared to baseline systems. Our contributions highlight the benefits of using answerable-question embeddings for RAG, and the effectiveness of LLM-based query decomposition for multihop scenarios.

</details>


### [44] [Echoes of Agreement: Argument Driven Opinion Shifts in Large Language Models](https://arxiv.org/abs/2508.09759)

*Avneet Kaur*

**Main category:** cs.CL

**Keywords:** political bias, large language models, prompt sensitivity

**Relevance Score:** 9

**TL;DR:** The paper explores how suggestive prompts with supporting or refuting arguments impact the political bias of large language models (LLMs), revealing a tendency of models to adapt their responses to align with the presented arguments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the sensitivity of LLM outputs to suggestive prompts in political bias evaluation, highlighting the robustness of bias assessments and model behavior.

**Method:** Conducted experiments evaluating model responses to prompts with supporting and refuting arguments under both single-turn and multi-turn interactions.

**Key Contributions:**

	1. Demonstrated the impact of argument strength on model responses in political bias evaluation.
	2. Identified a sycophantic tendency in LLMs to adjust stance based on user-given arguments.
	3. Provided insights into the implications for effective mitigation of bias in LLMs.

**Result:** Model responses are substantially influenced by the strength and direction of the provided arguments, demonstrating a sycophantic tendency to align with the inputs.

**Limitations:** The paper may not explore all aspects of bias across different political contexts or model architectures.

**Conclusion:** These findings have significant implications for measuring political bias in LLMs and developing strategies to mitigate such biases.

**Abstract:** There have been numerous studies evaluating bias of LLMs towards political topics. However, how positions towards these topics in model outputs are highly sensitive to the prompt. What happens when the prompt itself is suggestive of certain arguments towards those positions remains underexplored. This is crucial for understanding how robust these bias evaluations are and for understanding model behaviour, as these models frequently interact with opinionated text. To that end, we conduct experiments for political bias evaluation in presence of supporting and refuting arguments. Our experiments show that such arguments substantially alter model responses towards the direction of the provided argument in both single-turn and multi-turn settings. Moreover, we find that the strength of these arguments influences the directional agreement rate of model responses. These effects point to a sycophantic tendency in LLMs adapting their stance to align with the presented arguments which has downstream implications for measuring political bias and developing effective mitigation strategies.

</details>


### [45] [UtterTune: LoRA-Based Target-Language Pronunciation Edit and Control in Multilingual Text-to-Speech](https://arxiv.org/abs/2508.09767)

*Shuhei Kato*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, Multilingual, Pronunciation Control, Low-Rank Adaptation, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** UtterTune is a lightweight method to enhance pronunciation controllability in multilingual text-to-speech using LLMs while preserving performance in other languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the controllability of pronunciation in target languages within multilingual TTS systems that utilize LLM architectures.

**Method:** UtterTune employs low-rank adaptation to fine-tune a TTS system at the phoneme level, specifically focusing on segmental pronunciation and pitch accent for Japanese speech.

**Key Contributions:**

	1. Proposes a lightweight adaptation method for multilingual TTS systems.
	2. Enhances pronunciation control while retaining naturalness and speaker similarity.
	3. Validates effectiveness through both objective and subjective evaluations.

**Result:** The evaluations demonstrate that UtterTune effectively maintains naturalness and speaker similarity, even in a zero-shot setting.

**Limitations:** 

**Conclusion:** UtterTune successfully enhances the controllability of Japanese TTS without compromising performance in multilingual contexts.

**Abstract:** We propose UtterTune, a lightweight adaptation method that fine-tunes a multilingual text-to-speech (TTS) system based on a large language model (LLM) architecture, designed to enhance the controllability of pronunciation in a target language while preserving performance in others. While LLM architectures have enabled TTS models to achieve remarkable naturalness, accurately modeling grapheme-to-phoneme (G2P) mapping and prosody remains challenging, especially when the model omits an explicit G2P module and directly processes minimally encoded text (e.g., byte-pair encoding). UtterTune leverages low-rank adaptation to enable the control of segmental pronunciation and pitch accent at the phoneme level for Japanese speech, the target language in this paper, while maintaining naturalness and speaker similarity in a zero-shot setting. Objective and subjective evaluations confirm its effectiveness.

</details>


### [46] [Can LLM-Generated Textual Explanations Enhance Model Classification Performance? An Empirical Study](https://arxiv.org/abs/2508.09776)

*Mahdi Dhaini, Juraj Vladika, Ege Erdogan, Zineb Attaoui, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** Explainable NLP, Natural Language Generation, Large Language Models

**Relevance Score:** 9

**TL;DR:** The paper presents an automated framework using LLMs to generate high-quality textual explanations for NLP, assessing their impact on model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional human annotation in generating textual explanations for NLP models, which is costly and limits scalability.

**Method:** An automated framework leveraging multiple state-of-the-art LLMs to produce explanations and a comprehensive assessment of their quality using NLG metrics.

**Key Contributions:**

	1. Introduction of an automated framework for generating textual explanations using LLMs.
	2. Demonstration of the effectiveness of LLM-generated explanations compared to human annotations.
	3. Assessment of explanations using a comprehensive suite of NLG metrics.

**Result:** Automated explanations significantly improve the performance of PLMs and LLMs on natural language inference tasks, demonstrating effectiveness comparable to human-annotated explanations.

**Limitations:** 

**Conclusion:** The study highlights the potential for scalable LLM-based explanations to enhance NLP datasets and model performance.

**Abstract:** In the rapidly evolving field of Explainable Natural Language Processing (NLP), textual explanations, i.e., human-like rationales, are pivotal for explaining model predictions and enriching datasets with interpretable labels. Traditional approaches rely on human annotation, which is costly, labor-intensive, and impedes scalability. In this work, we present an automated framework that leverages multiple state-of-the-art large language models (LLMs) to generate high-quality textual explanations. We rigorously assess the quality of these LLM-generated explanations using a comprehensive suite of Natural Language Generation (NLG) metrics. Furthermore, we investigate the downstream impact of these explanations on the performance of pre-trained language models (PLMs) and LLMs across natural language inference tasks on two diverse benchmark datasets. Our experiments demonstrate that automated explanations exhibit highly competitive effectiveness compared to human-annotated explanations in improving model performance. Our findings underscore a promising avenue for scalable, automated LLM-based textual explanation generation for extending NLP datasets and enhancing model performance.

</details>


### [47] [Adoption of Explainable Natural Language Processing: Perspectives from Industry and Academia on Practices and Challenges](https://arxiv.org/abs/2508.09786)

*Mahdi Dhaini, Tobias Müller, Roksoliana Rabets, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** explainable NLP, practitioner perspectives, explainability methods, qualitative study, AI ethics

**Relevance Score:** 8

**TL;DR:** This paper investigates practitioners' perspectives on explainable NLP, analyzing motivations, techniques, satisfaction, and challenges in adopting explanation methods within real-world applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding the practical adoption and effectiveness of explainable NLP from the perspective of industry practitioners.

**Method:** A qualitative interview-based study involving industry practitioners and academic researchers to analyze and compare their experiences and perspectives on explainability methods.

**Key Contributions:**

	1. Investigates industry practitioners' motivations and experiences with explainability methods in NLP.
	2. Analyzes satisfaction levels and practical challenges faced in real-world applications.
	3. Highlights the need for improved frameworks and definitions for explainable NLP.

**Result:** The study reveals conceptual gaps, low satisfaction with current explainability methods, and highlights evaluation challenges in explainable NLP applications.

**Limitations:** Limited to qualitative insights; findings may not generalize across all sectors in NLP.

**Conclusion:** There is a critical need for clear definitions and user-centric frameworks to enhance the adoption of explainable NLP in practice.

**Abstract:** The field of explainable natural language processing (NLP) has grown rapidly in recent years. The growing opacity of complex models calls for transparency and explanations of their decisions, which is crucial to understand their reasoning and facilitate deployment, especially in high-stakes environments. Despite increasing attention given to explainable NLP, practitioners' perspectives regarding its practical adoption and effectiveness remain underexplored. This paper addresses this research gap by investigating practitioners' experiences with explainability methods, specifically focusing on their motivations for adopting such methods, the techniques employed, satisfaction levels, and the practical challenges encountered in real-world NLP applications. Through a qualitative interview-based study with industry practitioners and complementary interviews with academic researchers, we systematically analyze and compare their perspectives. Our findings reveal conceptual gaps, low satisfaction with current explainability methods, and highlight evaluation challenges. Our findings emphasize the need for clear definitions and user-centric frameworks for better adoption of explainable NLP in practice.

</details>


### [48] [BigCharts-R1: Enhanced Chart Reasoning with Visual Reinforcement Finetuning](https://arxiv.org/abs/2508.09804)

*Ahmed Masry, Abhay Puri, Masoud Hashemi, Juan A. Rodriguez, Megh Thakkar, Khyati Mahajan, Vikas Yadav, Sathwik Tejaswi Madhusudhan, Alexandre Piché, Dzmitry Bahdanau, Christopher Pal, David Vazquez, Enamul Hoque, Perouz Taslakian, Sai Rajeswar, Spandana Gella*

**Main category:** cs.CL

**Keywords:** Chart Comprehension, Vision-Language Models, Dataset Creation, Reinforcement Learning, Chart Reasoning

**Relevance Score:** 7

**TL;DR:** The paper introduces BigCharts, a dataset and training framework aimed at improving chart comprehension in vision-language models, addressing issues of low-quality datasets and enhancing model performance through reinforcement learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current vision-language models struggle with chart comprehension due to inadequate training datasets that lack real-world diversity and contain estimation errors.

**Method:** The authors propose BigCharts, a dataset creation pipeline that generates diverse chart images from real-world data, and a training framework that combines supervised fine-tuning with GRPO-based reinforcement learning and novel reward signals.

**Key Contributions:**

	1. Introduction of BigCharts dataset for visually diverse and authentic chart images
	2. Development of a training framework combining supervised fine-tuning and reinforcement learning
	3. State-of-the-art performance on chart question-answering benchmarks

**Result:** BigCharts-R1, the resulting model, outperforms existing methods on multiple chart question-answering benchmarks, demonstrating enhanced robustness and generalization.

**Limitations:** 

**Conclusion:** The proposed framework and dataset significantly improve chart reasoning capabilities in vision-language models and set a new state-of-the-art performance.

**Abstract:** Charts are essential to data analysis, transforming raw data into clear visual representations that support human decision-making. Although current vision-language models (VLMs) have made significant progress, they continue to struggle with chart comprehension due to training on datasets that lack diversity and real-world authenticity, or on automatically extracted underlying data tables of charts, which can contain numerous estimation errors. Furthermore, existing models only rely on supervised fine-tuning using these low-quality datasets, severely limiting their effectiveness. To address these issues, we first propose BigCharts, a dataset creation pipeline that generates visually diverse chart images by conditioning the rendering process on real-world charts sourced from multiple online platforms. Unlike purely synthetic datasets, BigCharts incorporates real-world data, ensuring authenticity and visual diversity, while still retaining accurate underlying data due to our proposed replotting process. Additionally, we introduce a comprehensive training framework that integrates supervised fine-tuning with Group Relative Policy Optimization (GRPO)-based reinforcement learning. By introducing novel reward signals specifically designed for chart reasoning, our approach enhances model robustness and generalization across diverse chart styles and domains, resulting in a state-of-the-art chart reasoning model, BigCharts-R1. Extensive experiments demonstrate that our models surpass existing methods on multiple chart question-answering benchmarks compared to even larger open-source and closed-source models.

</details>


### [49] [A Comprehensive Survey of Datasets for Clinical Mental Health AI Systems](https://arxiv.org/abs/2508.09809)

*Aishik Mandal, Prottay Kumar Adhikary, Hiba Arnaout, Iryna Gurevych, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** Mental Health, AI, Dataset Curation, Clinical Assistants, Synthetic Data

**Relevance Score:** 9

**TL;DR:** A survey of clinical mental health datasets for AI development, highlighting gaps and providing recommendations for better data curation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising mental health disorders and the insufficient availability of trained clinicians, this paper investigates high-quality clinical training datasets essential for AI in mental health.

**Method:** The paper conducts a comprehensive survey categorizing mental health datasets by disorders, data modalities, tasks, accessibility, and sociocultural context, including an exploration of synthetic datasets.

**Key Contributions:**

	1. First comprehensive survey of clinical mental health datasets for AI development
	2. Identification of gaps in existing datasets important for mental health AI
	3. Recommendations for improving dataset curation and standardization

**Result:** The survey identifies critical gaps in mental health datasets, including a lack of longitudinal data, cultural representation, and consistent standards.

**Limitations:** Challenges in dataset accessibility and variability in quality may affect findings.

**Conclusion:** The paper concludes with challenges in dataset curation and offers actionable recommendations for developing better mental health AI systems.

**Abstract:** Mental health disorders are rising worldwide. However, the availability of trained clinicians has not scaled proportionally, leaving many people without adequate or timely support. To bridge this gap, recent studies have shown the promise of Artificial Intelligence (AI) to assist mental health diagnosis, monitoring, and intervention. However, the development of efficient, reliable, and ethical AI to assist clinicians is heavily dependent on high-quality clinical training datasets. Despite growing interest in data curation for training clinical AI assistants, existing datasets largely remain scattered, under-documented, and often inaccessible, hindering the reproducibility, comparability, and generalizability of AI models developed for clinical mental health care. In this paper, we present the first comprehensive survey of clinical mental health datasets relevant to the training and development of AI-powered clinical assistants. We categorize these datasets by mental disorders (e.g., depression, schizophrenia), data modalities (e.g., text, speech, physiological signals), task types (e.g., diagnosis prediction, symptom severity estimation, intervention generation), accessibility (public, restricted or private), and sociocultural context (e.g., language and cultural background). Along with these, we also investigate synthetic clinical mental health datasets. Our survey identifies critical gaps such as a lack of longitudinal data, limited cultural and linguistic representation, inconsistent collection and annotation standards, and a lack of modalities in synthetic data. We conclude by outlining key challenges in curating and standardizing future datasets and provide actionable recommendations to facilitate the development of more robust, generalizable, and equitable mental health AI systems.

</details>


### [50] [Speed Always Wins: A Survey on Efficient Architectures for Large Language Models](https://arxiv.org/abs/2508.09834)

*Weigao Sun, Jiaxi Hu, Yucheng Zhou, Jusen Du, Disen Lan, Kexin Wang, Tong Zhu, Xiaoye Qu, Yu Zhang, Xiaoyu Mo, Daizong Liu, Yuxuan Liang, Wenliang Chen, Guoqi Li, Yu Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Transformer architecture, Efficiency techniques, Multimodal models, Survey

**Relevance Score:** 9

**TL;DR:** This survey reviews novel architectures for large language models (LLMs) aimed at improving efficiency while overcoming traditional transformer limitations.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically explore innovative LLM architectures that enhance efficiency and scalability, addressing shortcomings of traditional transformers in training and deployment.

**Method:** The survey examines various techniques, including linear and sparse modeling methods, efficient full attention variants, sparse mixture-of-experts, and hybrid architectures, alongside their applications across modalities.

**Key Contributions:**

	1. Comprehensive examination of innovative LLM architectures
	2. Grouping of recent studies into a coherent framework
	3. Discussion of applications and implications for scalable AI systems

**Result:** The paper groups recent studies into a comprehensive framework for efficient LLM architectures, offering insights into improvements and future research directions.

**Limitations:** 

**Conclusion:** The findings present a strategic reference for developing scalable and resource-efficient AI systems moving forward.

**Abstract:** Large Language Models (LLMs) have delivered impressive results in language understanding, generation, reasoning, and pushes the ability boundary of multimodal models. Transformer models, as the foundation of modern LLMs, offer a strong baseline with excellent scaling properties. However, the traditional transformer architecture requires substantial computations and poses significant obstacles for large-scale training and practical deployment. In this survey, we offer a systematic examination of innovative LLM architectures that address the inherent limitations of transformers and boost the efficiency. Starting from language modeling, this survey covers the background and technical details of linear and sparse sequence modeling methods, efficient full attention variants, sparse mixture-of-experts, hybrid model architectures incorporating the above techniques, and emerging diffusion LLMs. Additionally, we discuss applications of these techniques to other modalities and consider their wider implications for developing scalable, resource-aware foundation models. By grouping recent studies into the above category, this survey presents a blueprint of modern efficient LLM architectures, and we hope this could help motivate future research toward more efficient, versatile AI systems.

</details>


### [51] [PRELUDE: A Benchmark Designed to Require Global Comprehension and Reasoning over Long Contexts](https://arxiv.org/abs/2508.09848)

*Mo Yu, Tsz Ting Chung, Chulun Zhou, Tong Li, Rui Lu, Jiangnan Li, Liyan Xu, Haoshu Lu, Ning Zhang, Jing Li, Jie Zhou*

**Main category:** cs.CL

**Keywords:** long-context understanding, prequel narrative evaluation, machine learning, reasoning accuracy

**Relevance Score:** 8

**TL;DR:** PRELUDE is a benchmark for evaluating long-context understanding by testing the plausibility of characters' prequel stories compared to original narratives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well models understand and reason about long-context narratives by evaluating their ability to determine consistency between a prequel story and the canonical narrative.

**Method:** The benchmark involves a task where models must examine prequel narratives and ascertain their plausibility against the original stories, requiring deep comprehension and reasoning.

**Key Contributions:**

	1. Introduction of a new benchmark for long-context understanding
	2. Empirical results showing the limitations of current LLM models in reasoning tasks
	3. Identification of a significant reasoning accuracy gap between models and humans.

**Result:** Experiments show that current LLMs and commercial services significantly underperform compared to human reasoning, with more than 15% lag and over 30% gap in reasoning accuracy.

**Limitations:** Current models show correct answers but with flawed reasoning, indicating a need for improved reasoning mechanisms in AI.

**Conclusion:** There is a critical need for improvement in AI's long-context understanding and reasoning capabilities as highlighted by the performance gap between humans and models.

**Abstract:** We introduce PRELUDE, a benchmark for evaluating long-context understanding through the task of determining whether a character's prequel story is consistent with the canonical narrative of the original book. Our task poses a stronger demand for global comprehension and deep reasoning than existing benchmarks -- as the prequels are not part of the original story, assessing their plausibility typically requires searching and integrating information that is only indirectly related. Empirically, 88% of instances require evidence from multiple parts of the narrative. Experimental results highlight the challenge of our task: in-context learning, RAG and in-domain training with state-of-the-art LLMs, and commercial DeepResearch services, lag behind humans by >15%. A further human study reveals that models often produce correct answers with flawed reasoning, leading to an over 30% gap in reasoning accuracy compared to humans. These findings underscore the substantial room for improvement in long-context understanding and reasoning.

</details>


### [52] [Assessing the Feasibility of Lightweight Whisper Models for Low-Resource Urdu Transcription](https://arxiv.org/abs/2508.09865)

*Abdul Rehman Antall, Naveed Akhtar*

**Main category:** cs.CL

**Keywords:** speech recognition, Urdu, low-resource settings, Whisper models, automatic speech recognition

**Relevance Score:** 4

**TL;DR:** This study evaluates lightweight Whisper models for Urdu speech recognition, finding that Whisper-Small outperforms others in low-resource settings but still faces challenges.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited representation of Urdu in automatic speech recognition due to dialectal diversity and sparse training data.

**Method:** Benchmarking Whisper-Tiny, Whisper-Base, and Whisper-Small on a curated Urdu dataset using word error rate without fine-tuning.

**Key Contributions:**

	1. Evaluation of Whisper models in a low-resource language setting
	2. Establishment of benchmarks for Urdu speech recognition
	3. Insights into challenges faced by ASR for Urdu

**Result:** Whisper-Small achieved the lowest word error rate at 33.68%, while Tiny and Base had 67.08% and 53.67%, respectively.

**Limitations:** Not applicable for fine-tuning and challenges in phonetic accuracy and lexical coherence persist.

**Conclusion:** Whisper-Small shows potential for Urdu ASR deployment, but significant gaps in phonetic accuracy and lexical coherence remain.

**Abstract:** This study evaluates the feasibility of lightweight Whisper models (Tiny, Base, Small) for Urdu speech recognition in low-resource settings. Despite Urdu being the 10th most spoken language globally with over 230 million speakers, its representation in automatic speech recognition (ASR) systems remains limited due to dialectal diversity, code-switching, and sparse training data. We benchmark these models on a curated Urdu dataset using word error rate (WER), without fine-tuning. Results show Whisper-Small achieves the lowest error rates (33.68\% WER), outperforming Tiny (67.08\% WER) and Base (53.67\% WER). Qualitative analysis reveals persistent challenges in phonetic accuracy and lexical coherence, particularly for complex utterances. While Whisper-Small demonstrates promise for deployable Urdu ASR, significant gaps remain. Our findings emphasize lay the groundwork for future research into effective, low-resource ASR systems.

</details>


### [53] [Memory Decoder: A Pretrained, Plug-and-Play Memory for Large Language Models](https://arxiv.org/abs/2508.09874)

*Jiaqi Cao, Jiarui Wang, Rubin Wei, Qipeng Guo, Kai Chen, Bowen Zhou, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain Adaptation, Retrieval-Augmented Generation, Memory Architecture, Biomedicine

**Relevance Score:** 9

**TL;DR:** Memory Decoder enables efficient domain adaptation for LLMs without full-parameter training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Adapting LLMs to specific domains is challenging due to costly training and issues like catastrophic forgetting.

**Method:** Memory Decoder utilizes a small transformer decoder to imitate an external retriever, allowing seamless integration with pretrained language models.

**Key Contributions:**

	1. Efficient domain adaptation for LLMs without changing model parameters.
	2. Plug-and-play integration with any pretrained language model using the same tokenizer.
	3. Demonstrated effectiveness across multiple domains leading to reduced perplexity.

**Result:** Memory Decoder reduces perplexity by an average of 6.17 points across different models in specialized domains: biomedicine, finance, and law.

**Limitations:** 

**Conclusion:** The introduction of Memory Decoder represents a novel approach to domain-specific adaptation through a pretrained memory architecture that enhances performance across LLMs.

**Abstract:** Large Language Models (LLMs) have shown strong abilities in general language tasks, yet adapting them to specific domains remains a challenge. Current method like Domain Adaptive Pretraining (DAPT) requires costly full-parameter training and suffers from catastrophic forgetting. Meanwhile, Retrieval-Augmented Generation (RAG) introduces substantial inference latency due to expensive nearest-neighbor searches and longer context. This paper introduces Memory Decoder, a plug-and-play pretrained memory that enables efficient domain adaptation without changing the original model's parameters. Memory Decoder employs a small transformer decoder that learns to imitate the behavior of an external non-parametric retriever. Once trained, Memory Decoder can be seamlessly integrated with any pretrained language model that shares the same tokenizer, requiring no model-specific modifications. Experimental results demonstrate that Memory Decoder enables effective adaptation of various Qwen and Llama models to three distinct specialized domains: biomedicine, finance, and law, reducing perplexity by an average of 6.17 points. Overall, Memory Decoder introduces a novel paradigm centered on a specially pretrained memory component designed for domain-specific adaptation. This memory architecture can be integrated in a plug-and-play manner, consistently enhancing performance across multiple models within the target domain.

</details>


### [54] [A Survey of Cognitive Distortion Detection and Classification in NLP](https://arxiv.org/abs/2508.09878)

*Archie Sage, Jeroen Keppens, Helen Yannakoudakis*

**Main category:** cs.CL

**Keywords:** Cognitive Distortions, Natural Language Processing, Mental Health, Survey, Evaluation Strategies

**Relevance Score:** 9

**TL;DR:** This paper surveys 38 studies on the automatic detection of cognitive distortions using NLP, offering a structured overview and addressing inconsistencies in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmentation in the research on detecting cognitive distortions using NLP and to support coherent and reproducible studies.

**Method:** Review of 38 studies over two decades, analyzing datasets, modeling approaches, and evaluation strategies.

**Key Contributions:**

	1. Consolidated CD taxonomy reference
	2. Summary of common task setups in the research
	3. Identification of open challenges for future studies

**Result:** A consolidated reference for cognitive distortion taxonomies, common task setups, and identification of open challenges in the field.

**Limitations:** The field is still fragmented and lacks consistency across studies in terms of taxonomies and evaluation practices.

**Conclusion:** The survey provides essential insights and a framework for future research on cognitive distortions in mental health applications using NLP.

**Abstract:** As interest grows in the application of natural language processing (NLP) techniques to mental health, a growing body of work explores the automatic detection and classification of cognitive distortions (CDs). CDs are habitual patterns of negatively biased or flawed thinking that distort how people perceive events, judge themselves, and react to the world around them. Identifying and addressing them is an important part of therapy. Despite its momentum, the field remains fragmented, with inconsistencies in CD taxonomies, task formulations, and evaluation practices. This survey reviews 38 studies spanning two decades, providing a structured overview of datasets, modelling approaches, and evaluation strategies. We provide a consolidated CD taxonomy reference, summarise common task setups, and highlight open challenges to support more coherent and reproducible research in this emerging area.

</details>


### [55] [Language of Persuasion and Misrepresentation in Business Communication: A Textual Detection Approach](https://arxiv.org/abs/2508.09935)

*Sayem Hossen, Monalisa Moon Joti, Md. Golam Rashed*

**Main category:** cs.CL

**Keywords:** deceptive language, persuasive discourse, multilingual text processing

**Relevance Score:** 4

**TL;DR:** The paper explores the impact of digitisation on persuasive discourse, revealing how deceptive language can be detected with high accuracy using computational methods and highlights challenges in multilingual settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explain how deceptive language in business communication can be detected and to address the growing gap between theoretical communication representations and empirical data.

**Method:** Synthesis of classical rhetoric, communication psychology, and linguistic theory with empirical studies; high-accuracy detection using computational textual analysis and personalized transformer models.

**Key Contributions:**

	1. High accuracy in detecting deceptive language
	2. Integration of linguistic theory with computational methods
	3. Addressing challenges in multilingual text processing

**Result:** Achieved detection accuracies greater than 99% in controlled settings.

**Limitations:** Problems in reproducing performance in multilingual contexts due to data scarcity and lack of text-processing infrastructures.

**Conclusion:** There is a need for robust automatic text-identification systems to bridge the gap between AI-based discourse and realistic human communication.

**Abstract:** Business communication digitisation has reorganised the process of persuasive discourse, which   allows not only greater transparency but also advanced deception. This inquiry synthesises classical   rhetoric and communication psychology with linguistic theory and empirical studies in the financial   reporting, sustainability discourse, and digital marketing to explain how deceptive language can be   systematically detected using persuasive lexicon. In controlled settings, detection accuracies of greater   than 99% were achieved by using computational textual analysis as well as personalised transformer   models. However, reproducing this performance in multilingual settings is also problematic and,   to a large extent, this is because it is not easy to find sufficient data, and because few multilingual   text-processing infrastructures are in place. This evidence shows that there has been an increasing   gap between the theoretical representations of communication and those empirically approximated,   and therefore, there is a need to have strong automatic text-identification systems where AI-based   discourse is becoming more realistic in communicating with humans.

</details>


### [56] [A Comprehensive Evaluation framework of Alignment Techniques for LLMs](https://arxiv.org/abs/2508.09937)

*Muneeza Azmat, Momin Abbas, Maysa Malfiza Garcia de Macedo, Marcelo Carpinette Grave, Luan Soares de Souza, Tiago Machado, Rogerio A de Paula, Raya Horesh, Yixin Chen, Heloisa Caroline de Souza Pereira Candello, Rebecka Nordenlow, Aminat Adebiyi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Alignment Techniques, Evaluation Framework, Human Values, Computational Efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents a unified evaluation framework for alignment techniques in Large Language Models (LLMs), allowing for systematic comparison across various paradigms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical need for ensuring LLM outputs align with human values and safety standards amid their increasing integration into real-world applications.

**Method:** The paper introduces a multi-dimensional evaluation framework assessing alignment techniques across four dimensions: detection, quality, efficiency, and robustness, using experiments on diverse base models and strategies.

**Key Contributions:**

	1. Introduction of a multi-dimensional evaluation framework for alignment techniques in LLMs.
	2. Systematic comparison of major alignment paradigms across key dimensions.
	3. Insights for improving alignment methods based on empirical experiments.

**Result:** The framework reveals strengths and limitations of various alignment methods, providing insights into the evaluation of state-of-the-art models.

**Limitations:** 

**Conclusion:** The proposed framework serves as a valuable guide for systematic comparisons of alignment approaches in LLMs, highlighting valuable insights for future research.

**Abstract:** As Large Language Models (LLMs) become increasingly integrated into real-world applications, ensuring their outputs align with human values and safety standards has become critical. The field has developed diverse alignment approaches including traditional fine-tuning methods (RLHF, instruction tuning), post-hoc correction systems, and inference-time interventions, each with distinct advantages and limitations. However, the lack of unified evaluation frameworks makes it difficult to systematically compare these paradigms and guide deployment decisions. This paper introduces a multi-dimensional evaluation of alignment techniques for LLMs, a comprehensive evaluation framework that provides a systematic comparison across all major alignment paradigms. Our framework assesses methods along four key dimensions: alignment detection, alignment quality, computational efficiency, and robustness. Through experiments across diverse base models and alignment strategies, we demonstrate the utility of our framework in identifying strengths and limitations of current state-of-the-art models, providing valuable insights for future research directions.

</details>


### [57] [VisCodex: Unified Multimodal Code Generation via Merging Vision and Coding Models](https://arxiv.org/abs/2508.09945)

*Lingjie Jiang, Shaohan Huang, Xun Wu, Yixia Li, Dongdong Zhang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Multimodal large language models, code generation, vision-language models

**Relevance Score:** 9

**TL;DR:** VisCodex is a multimodal framework that enhances code generation from visual inputs by integrating vision and coding language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of visual and textual understanding in MLLMs has improved, but their code generation capabilities from multimodal inputs are limited.

**Method:** VisCodex merges a coding LLM with a vision-language backbone using a task vector-based model merging technique.

**Key Contributions:**

	1. Introduction of VisCodex framework for multimodal code generation
	2. Development of a large-scale Multimodal Coding Dataset (MCD) with diverse samples
	3. Proposal of InfiBench-V, a benchmark for assessing visually-rich programming tasks

**Result:** VisCodex demonstrates state-of-the-art performance among open-source MLLMs and competes closely with proprietary models like GPT-4o.

**Limitations:** 

**Conclusion:** The model merging strategy and new datasets significantly enhance multimodal code generation capabilities of MLLMs.

**Abstract:** Multimodal large language models (MLLMs) have significantly advanced the integration of visual and textual understanding. However, their ability to generate code from multimodal inputs remains limited. In this work, we introduce VisCodex, a unified framework that seamlessly merges vision and coding language models to empower MLLMs with strong multimodal code generation abilities. Leveraging a task vector-based model merging technique, we integrate a state-of-the-art coding LLM into a strong vision-language backbone, while preserving both visual comprehension and advanced coding skills. To support training and evaluation, we introduce the Multimodal Coding Dataset (MCD), a large-scale and diverse collection of 598k samples, including high-quality HTML code, chart image-code pairs, image-augmented StackOverflow QA, and algorithmic problems. Furthermore, we propose InfiBench-V, a novel and challenging benchmark specifically designed to assess models on visually-rich, real-world programming questions that demand a nuanced understanding of both textual and visual contexts. Extensive experiments show that VisCodex achieves state-of-the-art performance among open-source MLLMs and approaches proprietary models like GPT-4o, highlighting the effectiveness of our model merging strategy and new datasets.

</details>


### [58] [Specialised or Generic? Tokenization Choices for Radiology Language Models](https://arxiv.org/abs/2508.09952)

*Hermione Warr, Wentian Xu, Harry Anthony, Yasin Ibrahim, Daniel McGowan, Konstantinos Kamnitsas*

**Main category:** cs.CL

**Keywords:** tokenizers, radiology, language models, summarization, clinical domain

**Relevance Score:** 9

**TL;DR:** The study evaluates the impact of different tokenizers on the quality of radiology report summarization, highlighting the superiority of medical and domain-specific vocabularies over general ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effect of tokenizer vocabulary on the quality of text generation in the specific context of radiology, which is under-researched.

**Method:** Systematic comparison of general, medical, and domain-specific tokenizers on radiology report summarization across three imaging modalities, including evaluations with and without LM pre-training on PubMed abstracts.

**Key Contributions:**

	1. Evaluation of various tokenizers in radiology report summarization
	2. Demonstration of the advantages of medical and domain-specific vocabularies
	3. Showcasing reduced memory requirements from domain-specific tokenizers

**Result:** Medical and domain-specific tokenizers significantly outperformed general tokenizers when models were trained from scratch. Pre-training lessened but did not eliminate performance differences, with domain-specific tokenizers showing the best results and reduced memory usage.

**Limitations:** 

**Conclusion:** Adapting the vocabulary of language models to the clinical domain offers practical benefits, enhancing performance and reducing computational demands, which could improve accessibility and effectiveness in healthcare.

**Abstract:** The vocabulary used by language models (LM) - defined by the tokenizer - plays a key role in text generation quality. However, its impact remains under-explored in radiology. In this work, we address this gap by systematically comparing general, medical, and domain-specific tokenizers on the task of radiology report summarisation across three imaging modalities. We also investigate scenarios with and without LM pre-training on PubMed abstracts. Our findings demonstrate that medical and domain-specific vocabularies outperformed widely used natural language alternatives when models are trained from scratch. Pre-training partially mitigates performance differences between tokenizers, whilst the domain-specific tokenizers achieve the most favourable results. Domain-specific tokenizers also reduce memory requirements due to smaller vocabularies and shorter sequences. These results demonstrate that adapting the vocabulary of LMs to the clinical domain provides practical benefits, including improved performance and reduced computational demands, making such models more accessible and effective for both research and real-world healthcare settings.

</details>


### [59] [Shaping Event Backstories to Estimate Potential Emotion Contexts](https://arxiv.org/abs/2508.09954)

*Johannes Schäfer, Roman Klinger*

**Main category:** cs.CL

**Keywords:** emotion analysis, contextual narratives, annotation consistency, story generation, emotion disambiguation

**Relevance Score:** 7

**TL;DR:** This paper presents a novel method for enhancing emotion analysis by adding contextual narratives to event descriptions, aiming to improve the reliability of human annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the ambiguity in emotion analysis and improve the consistency of annotator responses.

**Method:** The authors propose generating multiple event chains conditioned on differing emotions, using techniques from short story generation to create coherent narratives that enhance understanding of context.

**Key Contributions:**

	1. Introduction of contextualization in emotion analysis.
	2. Creation of a specialized dataset for contextualized emotion analysis.
	3. Demonstration of the effectiveness of narrative techniques in improving annotation consistency.

**Result:** Through both automatic and human evaluations, the study demonstrates that enriched contextual narratives lead to improved interpretation of specific emotions and more consistent annotations by human annotators.

**Limitations:** The scope of the study may be limited to specific types of event descriptions, and further research is needed to generalize findings across a broader range of contexts.

**Conclusion:** The findings suggest that providing additional context can significantly improve the reliability of emotion analyses in ambiguous situations.

**Abstract:** Emotion analysis is an inherently ambiguous task. Previous work studied annotator properties to explain disagreement, but this overlooks the possibility that ambiguity may stem from missing information about the context of events. In this paper, we propose a novel approach that adds reasonable contexts to event descriptions, which may better explain a particular situation. Our goal is to understand whether these enriched contexts enable human annotators to annotate emotions more reliably. We disambiguate a target event description by automatically generating multiple event chains conditioned on differing emotions. By combining techniques from short story generation in various settings, we achieve coherent narratives that result in a specialized dataset for the first comprehensive and systematic examination of contextualized emotion analysis. Through automatic and human evaluation, we find that contextual narratives enhance the interpretation of specific emotions and support annotators in producing more consistent annotations.

</details>


### [60] [Performance of GPT-5 Frontier Models in Ophthalmology Question Answering](https://arxiv.org/abs/2508.09956)

*Fares Antaki, David Mikhail, Daniel Milad, Danny A Mammo, Sumit Sharma, Sunil K Srivastava, Bing Yu Chen, Samir Touma, Mertcan Sevgi, Jonathan El-Khoury, Pearse A Keane, Qingyu Chen, Yih Chung Tham, Renaud Duval*

**Main category:** cs.CL

**Keywords:** Large Language Models, GPT-5, Medical Question-Answering, Cost-Efficiency, Ophthalmology

**Relevance Score:** 9

**TL;DR:** This paper evaluates the efficacy of various configurations of GPT-5 models for medical question-answering, particularly in ophthalmology, highlighting accuracy and cost-effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine the optimal configurations of GPT-5 models that maximize accuracy and cost-efficiency for complex medical question-answering tasks.

**Method:** Evaluated 12 configurations of GPT-5 using 260 closed-access multiple-choice questions from a clinical dataset, focusing on multiple-choice accuracy and rationale quality assessment.

**Key Contributions:**

	1. Evaluation of 12 configurations of GPT-5 for medical tasks
	2. Establishment of a cost-accuracy analysis framework
	3. Introduction of an autograder framework for LLM-generated answers

**Result:** GPT-5-high achieved the highest accuracy (0.965), outperforming other models, and identified configurations on the Pareto frontier for favorable cost-performance balance.

**Limitations:** 

**Conclusion:** The findings benchmark GPT-5's performance in ophthalmology and suggest frameworks for scalable evaluation of LLM-generated responses.

**Abstract:** Large language models (LLMs) such as GPT-5 integrate advanced reasoning capabilities that may improve performance on complex medical question-answering tasks. For this latest generation of reasoning models, the configurations that maximize both accuracy and cost-efficiency have yet to be established. We evaluated 12 configurations of OpenAI's GPT-5 series (three model tiers across four reasoning effort settings) alongside o1-high, o3-high, and GPT-4o, using 260 closed-access multiple-choice questions from the American Academy of Ophthalmology Basic Clinical Science Course (BCSC) dataset. The primary outcome was multiple-choice accuracy; secondary outcomes included head-to-head ranking via a Bradley-Terry model, rationale quality assessment using a reference-anchored, pairwise LLM-as-a-judge framework, and analysis of accuracy-cost trade-offs using token-based cost estimates. GPT-5-high achieved the highest accuracy (0.965; 95% CI, 0.942-0.985), outperforming all GPT-5-nano variants (P < .001), o1-high (P = .04), and GPT-4o (P < .001), but not o3-high (0.958; 95% CI, 0.931-0.981). GPT-5-high ranked first in both accuracy (1.66x stronger than o3-high) and rationale quality (1.11x stronger than o3-high). Cost-accuracy analysis identified several GPT-5 configurations on the Pareto frontier, with GPT-5-mini-low offering the most favorable low-cost, high-performance balance. These results benchmark GPT-5 on a high-quality ophthalmology dataset, demonstrate the influence of reasoning effort on accuracy, and introduce an autograder framework for scalable evaluation of LLM-generated answers against reference standards in ophthalmology.

</details>


### [61] [Which one Performs Better? Wav2Vec or Whisper? Applying both in Badini Kurdish Speech to Text (BKSTT)](https://arxiv.org/abs/2508.09957)

*Renas Adnan, Hossein Hassani*

**Main category:** cs.CL

**Keywords:** Speech-to-Text, Badini Kurdish, Wav2Vec2, Whisper, Machine Learning

**Relevance Score:** 4

**TL;DR:** This research develops a speech-to-text (STT) system for the Badini Kurdish dialect using children's stories as training data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility and visibility of the Badini dialect through STT systems, addressing the lack of resources for less-resourced languages like Kurdish.

**Method:** Created a language model using approximately 17 hours of recordings from children's stories, applying Wav2Vec2-Large-XLSR-53 and Whisper-small for transcription evaluation.

**Key Contributions:**

	1. First STT model for Badini Kurdish
	2. Large dataset of children's stories for model training
	3. Comparison of two advanced models (Wav2Vec2 and Whisper) for STT performance

**Result:** Wav2Vec2-Large-XLSR-53 provided a transcription accuracy of 82.67% and readability of 90.38%, outperforming the Whisper-small model which had 53.17% accuracy and 65.45% readability.

**Limitations:** Focused only on the Badini dialect; results may not generalize to other dialects or languages.

**Conclusion:** The study demonstrates that Wav2Vec2-Large-XLSR-53 is a superior model for STT in Badini, highlighting the potential for improving STT systems in underrepresented languages.

**Abstract:** Speech-to-text (STT) systems have a wide range of applications. They are available in many languages, albeit at different quality levels. Although Kurdish is considered a less-resourced language from a processing perspective, SST is available for some of the Kurdish dialects, for instance, Sorani (Central Kurdish). However, that is not applied to other Kurdish dialects, Badini and Hawrami, for example. This research is an attempt to address this gap. Bandin, approximately, has two million speakers, and STT systems can help their community use mobile and computer-based technologies while giving their dialect more global visibility. We aim to create a language model based on Badini's speech and evaluate its performance. To cover a conversational aspect, have a proper confidence level of grammatical accuracy, and ready transcriptions, we chose Badini kids' stories, eight books including 78 stories, as the textual input. Six narrators narrated the books, which resulted in approximately 17 hours of recording. We cleaned, segmented, and tokenized the input. The preprocessing produced nearly 15 hours of speech, including 19193 segments and 25221 words. We used Wav2Vec2-Large-XLSR-53 and Whisper-small to develop the language models. The experiments indicate that the transcriptions process based on the Wav2Vec2-Large-XLSR-53 model provides a significantly more accurate and readable output than the Whisper-small model, with 90.38% and 65.45% readability, and 82.67% and 53.17% accuracy, respectively.

</details>


### [62] [Neural Bandit Based Optimal LLM Selection for a Pipeline of Tasks](https://arxiv.org/abs/2508.09958)

*Baran Atalar, Eddie Zhang, Carlee Joe-Wong*

**Main category:** cs.CL

**Keywords:** large language models, neural contextual bandits, subtask selection, health informatics, LLM performance prediction

**Relevance Score:** 9

**TL;DR:** This paper introduces a neural contextual bandit-based algorithm for selecting a sequence of LLMs for complex tasks, focusing on medical diagnosis extraction and telecommunications question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of large language models (LLMs) necessitates strategies to predict which model is best for specific queries, especially as users create custom LLM assistants.

**Method:** A neural contextual bandit algorithm is developed to model LLM success for subtasks in real-time, even without prior performance data.

**Key Contributions:**

	1. Development of a neural contextual bandit-based algorithm for LLM selection
	2. Demonstration of improved task performance on medical and telecommunications datasets
	3. Provision of a framework for selecting sequences of LLMs for complex subtasks

**Result:** The proposed algorithm shows improved performance in selecting LLMs for subtasks, demonstrated through experiments on relevant datasets.

**Limitations:** 

**Conclusion:** This approach effectively handles the complexity of LLM selection for multi-step tasks, enhancing accuracy and efficiency in applications such as health informatics.

**Abstract:** With the increasing popularity of large language models (LLMs) for a variety of tasks, there has been a growing interest in strategies that can predict which out of a set of LLMs will yield a successful answer at low cost. This problem promises to become more and more relevant as providers like Microsoft allow users to easily create custom LLM "assistants" specialized to particular types of queries. However, some tasks (i.e., queries) may be too specialized and difficult for a single LLM to handle alone. These applications often benefit from breaking down the task into smaller subtasks, each of which can then be executed by a LLM expected to perform well on that specific subtask. For example, in extracting a diagnosis from medical records, one can first select an LLM to summarize the record, select another to validate the summary, and then select another, possibly different, LLM to extract the diagnosis from the summarized record. Unlike existing LLM selection or routing algorithms, this setting requires that we select a sequence of LLMs, with the output of each LLM feeding into the next and potentially influencing its success. Thus, unlike single LLM selection, the quality of each subtask's output directly affects the inputs, and hence the cost and success rate, of downstream LLMs, creating complex performance dependencies that must be learned and accounted for during selection. We propose a neural contextual bandit-based algorithm that trains neural networks that model LLM success on each subtask in an online manner, thus learning to guide the LLM selections for the different subtasks, even in the absence of historical LLM performance data. Experiments on telecommunications question answering and medical diagnosis prediction datasets illustrate the effectiveness of our proposed approach compared to other LLM selection algorithms.

</details>


### [63] [From Stars to Insights: Exploration and Implementation of Unified Sentiment Analysis with Distant Supervision](https://arxiv.org/abs/2305.01710)

*Wenchang Li, John P. Lalor, Yixing Chen, Vamsi K. Kanuri*

**Main category:** cs.CL

**Keywords:** sentiment analysis, unified framework, Distantly Supervised Pyramid Network

**Relevance Score:** 6

**TL;DR:** This paper introduces a unified sentiment analysis framework that integrates aspect-category detection, aspect-category sentiment analysis, and rating prediction using the Distantly Supervised Pyramid Network (DSPN).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conventional sentiment analysis, which treats related tasks separately, leading to inefficiencies and the need for fine-grained annotations.

**Method:** The Distantly Supervised Pyramid Network (DSPN) uses a pyramidal architecture to analyze sentiment at multiple levels—word, aspect, and document—effectively unifying the sentiment analysis tasks.

**Key Contributions:**

	1. Introduction of a unified sentiment analysis framework
	2. Proposal of the Distantly Supervised Pyramid Network (DSPN)
	3. Demonstration of efficiency advantages using minimal supervision

**Result:** DSPN shows significant efficiency advantages while performing comparably to established models, using only star rating labels for supervision on multi-aspect review datasets in English and Chinese.

**Limitations:** 

**Conclusion:** DSPN establishes a robust and resource-efficient unified framework for sentiment analysis, enhancing efficiency and interpretability of results.

**Abstract:** Sentiment analysis is integral to understanding the voice of the customer and informing businesses' strategic decisions. Conventional sentiment analysis involves three separate tasks: aspect-category detection, aspect-category sentiment analysis, and rating prediction. However, independently tackling these tasks can overlook their interdependencies and often requires expensive, fine-grained annotations. This paper introduces unified sentiment analysis, a novel learning paradigm that integrates the three aforementioned tasks into a coherent framework. To achieve this, we propose the Distantly Supervised Pyramid Network (DSPN), which employs a pyramid structure to capture sentiment at word, aspect, and document levels in a hierarchical manner. Evaluations on multi-aspect review datasets in English and Chinese show that DSPN, using only star rating labels for supervision, demonstrates significant efficiency advantages while performing comparably well to a variety of benchmark models. Additionally, DSPN's pyramid structure enables the interpretability of its outputs. Our findings validate DSPN's effectiveness and efficiency, establishing a robust, resource-efficient, unified framework for sentiment analysis.

</details>


### [64] [Robo-Instruct: Simulator-Augmented Instruction Alignment For Finetuning Code LLMs](https://arxiv.org/abs/2405.20179)

*Zichao Hu, Junyi Jessy Li, Arjun Guha, Joydeep Biswas*

**Main category:** cs.CL

**Keywords:** task-program generation, service robotics, simulation environments, LLM post-processing, robot constraints

**Relevance Score:** 9

**TL;DR:** ROBO-INSTRUCT improves task-program conversion for service robots by creating dynamic simulation environments and using LLM post-processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of generating executable programs for service robots from natural language tasks, highlighting the limitations of existing methods in producing physically valid programs that adhere to robot constraints.

**Method:** ROBO-INSTRUCT synthesizes task-specific simulation environments on-the-fly and uses LLMs to refine instructions for alignment with robot programming requirements.

**Key Contributions:**

	1. Synthesis of dynamic simulation environments during program execution
	2. Integration of LLM-based post-processing for instruction refinement
	3. Demonstrated effectiveness across multiple fine-tuned LLMs outperforming baseline methods

**Result:** ROBO-INSTRUCT models outperform baseline methods and achieve comparable or superior performance to larger proprietary models in task execution success rates.

**Limitations:** 

**Conclusion:** The proposed approach significantly enhances the task-program conversion process, enabling more reliable execution of programs by robots in real-world scenarios.

**Abstract:** Code LLMs have shown promising results with converting tasks in natural language to programs that can be executed by service robots. We are interested in finetuning small, specialized LLMs for this purpose, but collecting datasets of task-program pairs specific to each robot is time-consuming and expensive. While approaches such as SELF-INSTRUCT and EVOL-INSTRUCT are capable of generating novel tasks given a few examples, they are unable to provide the corresponding programs that correctly abide by physical-world and robot-constraints using the provided programming interface. Using a simulator is a natural potential solution to checking for such constraints, but building simulation environments that can handle arbitrary tasks and their necessary objects and locations, is challenging. To address these challenges, we introduce ROBO-INSTRUCT, which synthesizes task-specific simulation environments on the fly during program execution, by opportunistically inferring entity properties and enforcing corresponding constraints based on how the entities are used in the task program. Additionally, ROBO-INSTRUCT integrates an LLM-aided post-processing procedure to refine instructions for better alignment with robot programs. We demonstrate the effectiveness of ROBO-INSTRUCT across multiple LLMs, showing that our fine-tuned models outperform all baseline methods and even match or surpass the performance of several larger and proprietary models.

</details>


### [65] [LongIns: A Challenging Long-context Instruction-based Exam for LLMs](https://arxiv.org/abs/2406.17588)

*Shawn Gavin, Tuney Zheng, Jiaheng Liu, Quehry Que, Noah Wang, Jian Yang, Chenchen Zhang, Wenhao Huang, Ge Zhang*

**Main category:** cs.CL

**Keywords:** Long-context, Large Language Models, Benchmarking, Instruction-based evaluation, Multi-hop reasoning

**Relevance Score:** 8

**TL;DR:** This paper introduces the LongIns benchmark dataset for evaluating long-context capabilities of LLMs, highlighting performance discrepancies with context windows.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing benchmarks that primarily assess retrieval abilities, which do not adequately represent reasoning performance or the actual context length capabilities of LLMs.

**Method:** The authors created the LongIns benchmark consisting of three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT) to systematically evaluate LLM performance.

**Key Contributions:**

	1. Introduction of LongIns benchmark for LLM evaluation.
	2. Identification of performance issues in top LLMs regarding context length.
	3. Insights into reasoning capabilities of LLMs under varying context sizes.

**Result:** Comprehensive evaluations showed that top-performing models like GPT-4, with a claimed 128k context length, underperformed in scenarios with a 16k context length, highlighting issues with multi-hop reasoning in shorter context windows.

**Limitations:** 

**Conclusion:** The findings indicate that existing LLMs require substantial improvements in multi-hop reasoning abilities, especially when operating within short context windows.

**Abstract:** The long-context capabilities of large language models (LLMs) have been a hot topic in recent years. To evaluate the performance of LLMs in different scenarios, various assessment benchmarks have emerged. However, as most of these benchmarks focus on identifying key information to answer questions, which mainly requires the retrieval ability of LLMs, these benchmarks can partially represent the reasoning performance of LLMs from large amounts of information. Meanwhile, although LLMs often claim to have context windows of 32k, 128k, 200k, or even longer, these benchmarks fail to reveal the actual supported length of these LLMs. To address these issues, we propose the LongIns benchmark dataset, a challenging long-context instruction-based exam for LLMs, which is built based on the existing instruction datasets. Specifically, in our LongIns, we introduce three evaluation settings: Global Instruction & Single Task (GIST), Local Instruction & Single Task (LIST), and Local Instruction & Multiple Tasks (LIMT). Based on LongIns, we perform comprehensive evaluations on existing LLMs and have the following important findings: (1). The top-performing GPT-4 with 128k context length performs poorly on the evaluation context window of 16k in our LongIns. (2). For the multi-hop reasoning ability of many existing LLMs, significant efforts are still needed under short context windows (less than 4k).

</details>


### [66] [Improving Multimodal Large Language Models Using Continual Learning](https://arxiv.org/abs/2410.19925)

*Shikhar Srivastava, Md Yousuf Harun, Robik Shrestha, Christopher Kanan*

**Main category:** cs.CL

**Keywords:** multimodal learning, continual learning, large language models, vision-language tasks, performance degradation

**Relevance Score:** 9

**TL;DR:** This study addresses performance degradation in multimodal large language models (MLLMs) by applying continual learning methods to integrate vision and language tasks effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of pre-trained vision models into LLMs often compromises natural language understanding and generation performance.

**Method:** We evaluate five continual learning techniques to reduce forgetting and enhance visual understanding while minimizing loss in linguistic performance.

**Key Contributions:**

	1. Introduction of continual learning methods to MLLMs
	2. Identification of techniques that minimize performance loss
	3. Demonstration of robustness across various tasks

**Result:** Successfully reduced linguistic performance degradation by up to 15% while maintaining high multimodal accuracy.

**Limitations:** 

**Conclusion:** Our method shows robustness in preserving linguistic skills during continual learning across vision-language tasks, allowing for the acquisition of new multimodal capabilities.

**Abstract:** Generative large language models (LLMs) exhibit impressive capabilities, which can be further augmented by integrating a pre-trained vision model into the original LLM to create a multimodal LLM (MLLM). However, this integration often significantly decreases performance on natural language understanding and generation tasks, compared to the original LLM. This study investigates this issue using the LLaVA MLLM, treating the integration as a continual learning problem. We evaluate five continual learning methods to mitigate forgetting and identify a technique that enhances visual understanding while minimizing linguistic performance loss. Our approach reduces linguistic performance degradation by up to 15% over the LLaVA recipe, while maintaining high multimodal accuracy. We also demonstrate the robustness of our method through continual learning on a sequence of vision-language tasks, effectively preserving linguistic skills while acquiring new multimodal capabilities. Project webpage: https://shikhar-srivastava.github.io/cl-for-improving-mllms

</details>


### [67] [Leveraging Audio and Text Modalities in Mental Health: A Study of LLMs Performance](https://arxiv.org/abs/2412.10417)

*Abdelrahman A. Ali, Aya E. Fouda, Radwa J. Hanafy, Mohammed E. Fouda*

**Main category:** cs.CL

**Keywords:** mental health, large language models, multimodal diagnostics, depression detection, PTSD

**Relevance Score:** 9

**TL;DR:** This study investigates the use of Large Language Models (LLMs) in diagnosing mental health disorders like depression and PTSD through multimodal approaches using text and audio data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The rising prevalence of mental health disorders necessitates innovative diagnostic tools.

**Method:** The study utilizes the E-DAIC dataset to compare the performance of LLMs in text and audio modalities for mental health diagnosis and assesses the integration of both modalities.

**Key Contributions:**

	1. Investigation of LLM performance in multimodal mental health diagnosis
	2. Development of custom metrics for evaluating model performance
	3. Demonstration of improved diagnostic accuracy through modality integration

**Result:** The Gemini 1.5 Pro model achieved the highest scores in binary depression classification with a combined modality F1 score of 0.67 and a balanced accuracy of 77.4%.

**Limitations:** 

**Conclusion:** Integrating text and audio modalities improves diagnostic accuracy without the need for task-specific fine-tuning, showing robustness in zero-shot inferring.

**Abstract:** Mental health disorders are increasingly prevalent worldwide, creating an urgent need for innovative tools to support early diagnosis and intervention. This study explores the potential of Large Language Models (LLMs) in multimodal mental health diagnostics, specifically for detecting depression and Post Traumatic Stress Disorder through text and audio modalities. Using the E-DAIC dataset, we compare text and audio modalities to investigate whether LLMs can perform equally well or better with audio inputs. We further examine the integration of both modalities to determine if this can enhance diagnostic accuracy, which generally results in improved performance metrics. Our analysis specifically utilizes custom-formulated metrics; Modal Superiority Score and Disagreement Resolvement Score to evaluate how combined modalities influence model performance. The Gemini 1.5 Pro model achieves the highest scores in binary depression classification when using the combined modality, with an F1 score of 0.67 and a Balanced Accuracy (BA) of 77.4%, assessed across the full dataset. These results represent an increase of 3.1% over its performance with the text modality and 2.7% over the audio modality, highlighting the effectiveness of integrating modalities to enhance diagnostic accuracy. Notably, all results are obtained in zero-shot inferring, highlighting the robustness of the models without requiring task-specific fine-tuning. To explore the impact of different configurations on model performance, we conduct binary, severity, and multiclass tasks using both zero-shot and few-shot prompts, examining the effects of prompt variations on performance. The results reveal that models such as Gemini 1.5 Pro in text and audio modalities, and GPT-4o mini in the text modality, often surpass other models in balanced accuracy and F1 scores across multiple tasks.

</details>


### [68] [Memorization Over Reasoning? Exposing and Mitigating Verbatim Memorization in Large Language Models' Character Understanding Evaluation](https://arxiv.org/abs/2412.14368)

*Yuxuan Jiang, Francis Ferraro*

**Main category:** cs.CL

**Keywords:** Large Language Models, Character Understanding, Gist Memory, Verbatim Memory, Data Contamination

**Relevance Score:** 8

**TL;DR:** This paper addresses the limitations of large language models (LLMs) in character understanding tasks, emphasizing the importance of gist memory over verbatim memory for genuine comprehension.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the reliance of LLMs on memorization of popular fictional works rather than true understanding in character analysis.

**Method:** The authors introduce a method to reduce memorization-driven performance in evaluations of character understanding, promoting gist memory in place of verbatim memory.

**Key Contributions:**

	1. Proposing gist memory as a primary mechanism for character understanding tasks.
	2. Developing a method to mitigate memorization in LLMs during character analysis evaluations.
	3. Demonstrating the impact of this method on existing accuracy benchmarks.

**Result:** The method decreases memorization-driven accuracy on popular fiction from 96% to 72%, affecting overall character understanding task accuracy by up to 18%.

**Limitations:** Focus on fictional characters may not translate to real-world understanding; the study is limited to evaluating performance based on specific benchmarks.

**Conclusion:** The findings highlight the data contamination in current benchmarks that prioritize memorization over actual character understanding.

**Abstract:** Recently, Large Language Models (LLMs) have shown impressive performance in character understanding tasks, such as analyzing the roles, personalities, and relationships of fictional characters. However, the extensive pre-training corpora used by LLMs raise concerns that they may rely on memorizing popular fictional works rather than genuinely understanding and reasoning about them. In this work, we argue that 'gist memory'-capturing essential meaning - should be the primary mechanism for character understanding tasks, as opposed to 'verbatim memory' - exact match of a string. We introduce a simple yet effective method to mitigate mechanized memorization in character understanding evaluations while preserving the essential implicit cues needed for comprehension and reasoning. Our approach reduces memorization-driven performance on popular fictional works from 96% accuracy to 72% and results in up to an 18% drop in accuracy across various character understanding tasks. These findings underscore the issue of data contamination in existing benchmarks, which often measure memorization rather than true character understanding.

</details>


### [69] [Beyond Memorization: Assessing Semantic Generalization in Large Language Models Using Phrasal Constructions](https://arxiv.org/abs/2501.04661)

*Wesley Scivetti, Melissa Torgbi, Austin Blodgett, Mollie Shichman, Taylor Hudson, Claire Bonial, Harish Tayyar Madabushi*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Construction Grammar, LLMs, Generalization, Evaluation Dataset

**Relevance Score:** 9

**TL;DR:** This paper presents a diagnostic evaluation for assessing natural language understanding in LLMs using Construction Grammar, revealing significant generalization challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate LLMs' ability to generalize language understanding beyond their training data, especially in contexts less common in pretraining.

**Method:** A diagnostic evaluation leveraging Construction Grammar (CxG) was constructed, including a novel inference evaluation dataset of English phrasal constructions.

**Key Contributions:**

	1. Introduction of a diagnostic evaluation using Construction Grammar for LLMs
	2. Creation of a novel inference evaluation dataset for English phrasal constructions
	3. Public availability of the dataset and experimental data for future research

**Result:** State-of-the-art models, like GPT-o1, showed a performance drop of over 40% when required to differentiate syntactically identical constructions that have different meanings.

**Limitations:** 

**Conclusion:** The study highlights significant shortcomings in LLMs' generalization abilities and provides a publicly available dataset for further research in this area.

**Abstract:** The web-scale of pretraining data has created an important evaluation challenge: to disentangle linguistic competence on cases well-represented in pretraining data from generalization to out-of-domain language, specifically the dynamic, real-world instances less common in pretraining data. To this end, we construct a diagnostic evaluation to systematically assess natural language understanding in LLMs by leveraging Construction Grammar (CxG). CxG provides a psycholinguistically grounded framework for testing generalization, as it explicitly links syntactic forms to abstract, non-lexical meanings. Our novel inference evaluation dataset consists of English phrasal constructions, for which speakers are known to be able to abstract over commonplace instantiations in order to understand and produce creative instantiations. Our evaluation dataset uses CxG to evaluate two central questions: first, if models can 'understand' the semantics of sentences for instances that are likely to appear in pretraining data less often, but are intuitive and easy for people to understand. Second, if LLMs can deploy the appropriate constructional semantics given constructions that are syntactically identical but with divergent meanings. Our results demonstrate that state-of-the-art models, including GPT-o1, exhibit a performance drop of over 40% on our second task, revealing a failure to generalize over syntactically identical forms to arrive at distinct constructional meanings in the way humans do. We make our novel dataset and associated experimental data, including prompts and model responses, publicly available.

</details>


### [70] [Benchmarking LLMs' Mathematical Reasoning with Unseen Random Variables Questions](https://arxiv.org/abs/2501.11790)

*Zijin Hong, Hao Wu, Su Dong, Junnan Dong, Yilin Xiao, Yujing Zhang, Zhu Wang, Feiran Huang, Linyi Li, Hongxia Yang, Xiao Huang*

**Main category:** cs.CL

**Keywords:** LLMs, mathematical reasoning, benchmarking, random variables, evaluation methodology

**Relevance Score:** 7

**TL;DR:** RV-Bench is a proposed evaluation methodology for benchmarking LLMs in mathematical reasoning by generating random variable questions to assess genuine reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Developing a reliable benchmark to evaluate LLMs' true capabilities in mathematical reasoning due to concerns about existing benchmarks' reliability.

**Method:** The methodology involves creating question-generating functions that produce random variable questions (RVQs) that resemble original benchmark problems with randomized variables.

**Key Contributions:**

	1. Introduction of RV-Bench for evaluating LLMs with random variables
	2. Demonstration of proficiency imbalance in LLMs between encountered and unseen questions
	3. Insights on limited generalization capabilities of LLMs in mathematical reasoning tasks.

**Result:** Experiments show LLMs have a proficiency imbalance between known and unseen data distributions and limited generalization across similar tasks, but effective generalization can be prompted through test-time scaling.

**Limitations:** The effectiveness of RV-Bench may vary depending on the complexity of mathematical reasoning tasks.

**Conclusion:** RV-Bench offers a better understanding of LLMs' reasoning capabilities and highlights the need for improved benchmarks in mathematical reasoning.

**Abstract:** Recent studies have raised significant concerns regarding the reliability of current mathematics benchmarks, highlighting issues such as simplistic design and potential data contamination. Consequently, developing a reliable benchmark that effectively evaluates large language models' (LLMs) genuine capabilities in mathematical reasoning remains a critical challenge. To address these concerns, we propose RV-Bench, a novel evaluation methodology for Benchmarking LLMs with Random Variables in mathematical reasoning. Specifically, we build question-generating functions to produce random variable questions (RVQs), whose background content mirrors original benchmark problems, but with randomized variable combinations, rendering them "unseen" to LLMs. Models must completely understand the inherent question pattern to correctly answer RVQs with diverse variable combinations. Thus, an LLM's genuine reasoning capability is reflected through its accuracy and robustness on RV-Bench. We conducted extensive experiments on over 30 representative LLMs across more than 1,000 RVQs. Our findings propose that LLMs exhibit a proficiency imbalance between encountered and ``unseen'' data distributions. Furthermore, RV-Bench reveals that proficiency generalization across similar mathematical reasoning tasks is limited, but we verified it can still be effectively elicited through test-time scaling.

</details>


### [71] [Follow the Flow: On Information Flow Across Textual Tokens in Text-to-Image Models](https://arxiv.org/abs/2504.01137)

*Guy Kaplan, Michael Toker, Yuval Reif, Yonatan Belinkov, Roy Schwartz*

**Main category:** cs.CL

**Keywords:** Text-to-image models, Token representation, Semantic information, Image generation, Lexical items

**Relevance Score:** 6

**TL;DR:** The study investigates how semantic information is distributed across token representations in text-to-image models, emphasizing the importance of token-level encoding in image generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the significance of semantic information distribution in textual encoding for improving image generation in text-to-image models.

**Method:** Analysis of information flow at two levels: in-item representation and cross-item interaction, using patching techniques to uncover encoding patterns.

**Key Contributions:**

	1. Identification of critical token-level encoding patterns in T2I models
	2. Insights into information distribution across lexical items
	3. Demonstration of how misinterpretations can arise from cross-item interactions

**Result:** Information is typically concentrated in one or two tokens of an item, with lexical items often remaining isolated, but some cases show mutual influence that can lead to misinterpretations.

**Limitations:** 

**Conclusion:** Misalignment issues in image generation may stem from deficiencies in the textual encoding stage, suggesting a need for refinement in this area.

**Abstract:** Text-to-image (T2I) models generate images by encoding text prompts into token representations, which then guide the diffusion process. While prior work has largely focused on improving alignment by refining the diffusion process, we focus on the textual encoding stage. Specifically, we investigate how semantic information is distributed across token representations within and between lexical items (i.e., words or expressions conveying a single concept) in the prompt. We analyze information flow at two levels: (1) in-item representation-whether individual tokens represent their lexical item, and (2) cross-item interaction-whether information flows across the tokens of different lexical items. We use patching techniques to uncover surprising encoding patterns. We find information is usually concentrated in only one or two of the item's tokens-For example, in the item "San Francisco's Golden Gate Bridge", the token "Gate" sufficiently captures the entire expression while the other tokens could effectively be discarded. Lexical items also tend to remain isolated; for instance, the token "dog" encodes no visual information about "green" in the prompt "a green dog". However, in some cases, items do influence each other's representation, often leading to misinterpretations-e.g., in the prompt "a pool by a table", the token pool represents a pool table after contextualization. Our findings highlight the critical role of token-level encoding in image generation, suggesting that misalignment issues may originate already during the textual encoding.

</details>


### [72] [CO-Bench: Benchmarking Language Model Agents in Algorithm Search for Combinatorial Optimization](https://arxiv.org/abs/2504.04310)

*Weiwei Sun, Shengyu Feng, Shanda Li, Yiming Yang*

**Main category:** cs.CL

**Keywords:** combinatorial optimization, LLM agents, benchmark, evaluation, AI research

**Relevance Score:** 8

**TL;DR:** Introduction of CO-Bench, a benchmark suite for evaluating LLM agents on combinatorial optimization problems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underutilized role of LLM-based agents in solving combinatorial optimization problems and the lack of structured benchmarks for assessment.

**Method:** Development of CO-Bench, a suite comprising 36 real-world combinatorial optimization problems with structured formulations and curated data.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark suite for combinatorial optimization
	2. In-depth evaluation of LLM agents against human-designed algorithms
	3. Identification of future research directions in optimizing LLM utilization for combinatorial problems.

**Result:** Evaluation of various agent frameworks against traditional algorithms, revealing strengths and limitations of LLM agents, and suggesting future research directions.

**Limitations:** 

**Conclusion:** CO-Bench serves as a valuable resource for systematically investigating LLM agent performance in combinatorial optimization.

**Abstract:** Although LLM-based agents have attracted significant attention in domains such as software engineering and machine learning research, their role in advancing combinatorial optimization (CO) remains relatively underexplored. This gap underscores the need for a deeper understanding of their potential in tackling structured, constraint-intensive problems -- a pursuit currently limited by the absence of comprehensive benchmarks for systematic investigation. To address this, we introduce CO-Bench, a benchmark suite featuring 36 real-world CO problems drawn from a broad range of domains and complexity levels. CO-Bench includes structured problem formulations and curated data to support rigorous investigation of LLM agents. We evaluate multiple agentic frameworks against established human-designed algorithms, revealing the strengths and limitations of existing LLM agents and identifying promising directions for future research. CO-Bench is publicly available at https://github.com/sunnweiwei/CO-Bench.

</details>


### [73] [AI-Slop to AI-Polish? Aligning Language Models through Edit-Based Writing Rewards and Test-time Computation](https://arxiv.org/abs/2504.07532)

*Tuhin Chakrabarty, Philippe Laban, Chien-Sheng Wu*

**Main category:** cs.CL

**Keywords:** AI-generated text, writing quality assessment, benchmark, reward models, human evaluation

**Relevance Score:** 9

**TL;DR:** This paper addresses the evaluation and improvement of AI-generated text quality by introducing the Writing Quality Benchmark (WQ) and specialized Writing Quality Reward Models (WQRM).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of AI-generated content across various domains necessitates a systematic way to evaluate and improve writing quality, which has been largely overlooked in prior research.

**Method:** The authors consolidate five writing-preference datasets into the Writing Quality Benchmark (WQ) with 4,729 writing quality judgments and develop specialized reward models for writing quality assessment.

**Key Contributions:**

	1. Introduction of the Writing Quality Benchmark (WQ) with consolidated datasets.
	2. Development of Writing Quality Reward Models (WQRM) that significantly outperform random baselines.
	3. Human evaluation validates the effectiveness of WQRM in improving the quality of AI-generated texts.

**Result:** The Writing Quality Reward Models (WQRM) show strong generalization across different test sets and achieve 74% accuracy on the WQ benchmark; they also help select higher-quality outputs during inference, leading to a 66% preference from experienced writers.

**Limitations:** 

**Conclusion:** By releasing their datasets and models, the authors aim to foster community involvement in writing quality assessment and enhance AI writing systems in accordance with human preferences.

**Abstract:** AI-generated text is proliferating across domains, from creative writing and journalism to marketing content and scientific articles. Models can follow user-provided instructions to generate coherent and grammatically correct outputs but in this work, we study a more fundamental question: how do we evaluate and improve the writing quality of AI-generated text? Writing quality assessment has received less attention from the community, in part because it is fundamentally subjective and requires expertise. We first introduce the Writing Quality Benchmark (WQ) by consolidating five writing-preference datasets into 4,729 writing quality judgments. Our experiments show that most of the competitive baselines, including state-of-the-art LLMs that excel at reasoning tasks, barely outperform random baselines on WQ. We then train specialized Writing Quality Reward Models (WQRM) of various sizes for writing quality assessment that demonstrate strong generalization on four out-of-distribution test sets and 74% accuracy on the WQ benchmark. To further show WQRM's practical benefits during inference, we leverage additional test-time compute to generate and rank multiple candidate revisions, allowing us to select higher-quality outputs from an initial draft. Human evaluation with 9 experienced writers confirm that WQRM-based selection produces writing samples preferred by experts 66% overall, and 72.2% when the reward gap is larger than 1 point. We release our datasets and models to encourage community engagement with writing quality assessment and development of AI writing systems better aligned with human preferences.

</details>


### [74] [Exploring Scaling Laws for EHR Foundation Models](https://arxiv.org/abs/2505.22964)

*Sheng Zhang, Qin Liu, Naoto Usuyama, Cliff Wong, Tristan Naumann, Hoifung Poon*

**Main category:** cs.CL

**Keywords:** electronic health records, scaling laws, transformer models, clinical prediction, personalized healthcare

**Relevance Score:** 9

**TL;DR:** This paper investigates scaling laws for electronic health record (EHR) models, finding that they exhibit behavior similar to large language models (LLMs) regarding performance gains through model size and compute.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Exploring scaling laws in the context of electronic health records (EHRs), which are a distinct and valuable data source for improving clinical prediction tasks and healthcare strategies.

**Method:** The authors trained transformer models on patient timeline data from the MIMIC-IV database, examining various model sizes and compute budgets to identify scaling patterns.

**Key Contributions:**

	1. First empirical study of scaling laws for EHR foundation models.
	2. Identification of consistent scaling patterns, such as IsoFLOPs curves and power-law dynamics.
	3. Implications for developing powerful EHR models that can improve personalized healthcare.

**Result:** The study found parabolic IsoFLOPs curves and power-law relationships linking compute, model parameters, data size, and clinical utility in EHR models, indicating similar scaling behavior to LLMs.

**Limitations:** 

**Conclusion:** The findings suggest that EHR models can leverage insights from LLM scaling laws, informing resource-efficient training and enhancing clinical applications.

**Abstract:** The emergence of scaling laws has profoundly shaped the development of large language models (LLMs), enabling predictable performance gains through systematic increases in model size, dataset volume, and compute. Yet, these principles remain largely unexplored in the context of electronic health records (EHRs) -- a rich, sequential, and globally abundant data source that differs structurally from natural language. In this work, we present the first empirical investigation of scaling laws for EHR foundation models. By training transformer architectures on patient timeline data from the MIMIC-IV database across varying model sizes and compute budgets, we identify consistent scaling patterns, including parabolic IsoFLOPs curves and power-law relationships between compute, model parameters, data size, and clinical utility. These findings demonstrate that EHR models exhibit scaling behavior analogous to LLMs, offering predictive insights into resource-efficient training strategies. Our results lay the groundwork for developing powerful EHR foundation models capable of transforming clinical prediction tasks and advancing personalized healthcare.

</details>
