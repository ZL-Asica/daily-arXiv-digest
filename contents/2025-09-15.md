# 2025-09-15

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 17]

- [cs.CL](#cs.CL) [Total: 62]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Merging Bodies, Dividing Conflict: Body-Swapping in Mixed Reality Increases Closeness Yet Weakens the Joint Simon Effect](https://arxiv.org/abs/2509.09815)

*Yuan He, Brendan Rooney, Rachel McDonnell*

**Main category:** cs.HC

**Keywords:** Mixed Reality, body-swapping, collaboration, cognitive perception, social interaction

**Relevance Score:** 7

**TL;DR:** This paper explores the effects of body-swapping in Mixed Reality environments on social interactions and cognitive perceptions of self and others.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the limited exploration of body-swapping in Mixed Reality and its impact on social interactions and perception in immersive environments.

**Method:** The study adapted the Joint Simon Task (JST) to examine cognitive and perceptual influences of body-swapping between two participants inhabiting each other's avatars.

**Key Contributions:**

	1. Modified Joint Simon Task to study body-swapping effects
	2. Demonstrated unified cognitive and perceptual experiences during body-swapping
	3. Identified implications for MR system designs in empathy and collaboration

**Result:** Participants experienced a unified sense of self with their partner, suggesting cognitive and perceptual integration beyond mere collaboration.

**Limitations:** 

**Conclusion:** Body-swapping in MR can alter how individuals perceive themselves and each other, indicating design opportunities for systems enhancing collaboration and empathy.

**Abstract:** Mixed Reality (MR) presents novel opportunities to investigate how individuals perceive themselves and others during shared, augmented experiences within a common physical environment. Previous research has demonstrated that users can embody avatars in MR, temporarily extending their sense of self. However, there has been limited exploration of body-swapping, a condition in which two individuals simultaneously inhabit each other's avatars, and its potential effects on social interaction in immersive environments. To address this gap, we adapted the Joint Simon Task (JST), a well-established implicit paradigm, to examine how body-swapping influences the cognitive and perceptual boundaries between self and other. Our results indicate that body-swapping led participants to experience themselves and their partner as functioning like a single, unified system, as in two bodies operating as one agent. This suggests possible cognitive and perceptual changes that go beyond simple collaboration. Our findings have significant implications for the design of MR systems intended to support collaboration, empathy, social learning, and therapeutic interventions through shared embodiment.

</details>


### [2] [Designing and Evaluating AI Margin Notes in Document Reader Software](https://arxiv.org/abs/2509.09840)

*Nikhita Joshi, Daniel Vogel*

**Main category:** cs.HC

**Keywords:** AI margin notes, document reader software, user experience, HCI, integrated AI

**Relevance Score:** 7

**TL;DR:** This paper proposes the concept of AI margin notes, integrating AI directly into document comments, with preferences for manual selection and various levels of AI involvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of AI into document comments to enhance reading and comprehension experiences.

**Method:** Three experiments investigating integration of AI margin notes, selection automation, and human vs AI involvement techniques.

**Key Contributions:**

	1. Introduction of AI margin notes concept
	2. Insights into user preferences for manual selection
	3. Analysis of psychological ownership related to AI involvement

**Result:** Participants preferred integrated AI margin notes and manual selection; less AI involvement increased psychological ownership but did not affect reading comprehension.

**Limitations:** Results may vary based on different user demographics or contexts of document use.

**Conclusion:** AI margin notes are desirable, and their design implications suggest the need to balance AI involvement with user preferences.

**Abstract:** AI capabilities for document reader software are usually presented in separate chat interfaces. We explore integrating AI into document comments, a concept we formalize as AI margin notes. Three design parameters characterize this approach: margin notes are integrated with the text while chat interfaces are not; selecting text for a margin note can be automated through AI or manual; and the generation of a margin note can involve AI to various degrees. Two experiments investigate integration and selection automation, with results showing participants prefer integrated AI margin notes and manual selection. A third experiment explores human and AI involvement through six alternative techniques. Techniques with less AI involvement resulted in more psychological ownership, but faster and less effortful designs are generally preferred. Surprisingly, the degree of AI involvement had no measurable effect on reading comprehension. Our work shows that AI margin notes are desirable and contributes implications for their design.

</details>


### [3] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)

*Hasibur Rahman, Smit Desai*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Personality Expression, User Perceptions, Big Five Traits, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This study examines how personality expression levels in conversational agents affect user perceptions and task outcomes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how personality expression in conversational agents shapes user perceptions, particularly in goal-oriented tasks.

**Method:** A between-subjects experiment with 150 participants was conducted to evaluate travel planning interactions with conversational agents exhibiting varying levels of personality expression based on the Big Five traits, using the Trait Modulation Keys framework.

**Key Contributions:**

	1. Introduced the Trait Modulation Keys framework to evaluate personality expression in conversational agents.
	2. Identified an inverted-U relationship between personality expression levels and user perceptions.
	3. Established the significance of personality alignment in enhancing user-agent interactions.

**Result:** The study found an inverted-U relationship in user evaluations, where medium personality expression resulted in the most positive feedback on measures like Intelligence, Enjoyment, and Trust. Compatibility of user and agent personalities also influenced outcomes, identifying three user profiles.

**Limitations:** 

**Conclusion:** Optimizing personality expression and aligning agent traits with user profiles can enhance user experiences in interactions with conversational agents.

**Abstract:** Large language models (LLMs) enable conversational agents (CAs) to express distinctive personalities, raising new questions about how such designs shape user perceptions. This study investigates how personality expression levels and user-agent personality alignment influence perceptions in goal-oriented tasks. In a between-subjects experiment (N=150), participants completed travel planning with CAs exhibiting low, medium, or high expression across the Big Five traits, controlled via our novel Trait Modulation Keys framework. Results revealed an inverted-U relationship: medium expression produced the most positive evaluations across Intelligence, Enjoyment, Anthropomorphism, Intention to Adopt, Trust, and Likeability, significantly outperforming both extremes. Personality alignment further enhanced outcomes, with Extraversion and Emotional Stability emerging as the most influential traits. Cluster analysis identified three distinct compatibility profiles, with "Well-Aligned" users reporting substantially positive perceptions. These findings demonstrate that personality expression and strategic trait alignment constitute optimal design targets for CA personality, offering design implications as LLM-based CAs become increasingly prevalent.

</details>


### [4] [Climate Data for Power Systems Applications: Lessons in Reusing Wildfire Smoke Data for Solar PV Studies](https://arxiv.org/abs/2509.09888)

*Arleth Salinas, Irtaza Sohail, Valerio Pascucci, Pantelis Stefanakis, Saud Amjad, Aashish Panta, Roland Schigas, Timothy Chun-Yiu Chui, Nicolas Duboc, Mostafa Farrokhabadi, Roland Stull*

**Main category:** cs.HC

**Keywords:** data reuse, data repurposing, geospatial datasets, solar energy, wildfire smoke

**Relevance Score:** 4

**TL;DR:** This paper presents a case study on data repurposing for effective data reuse, specifically focusing on a geospatial wildfire smoke forecast dataset used to analyze its impact on solar energy production.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective data reuse is critical as data sharing becomes more prevalent in science; this research aims to facilitate the reuse of data by transforming it for new research purposes.

**Method:** A geospatial wildfire smoke forecast dataset is transformed into a historical dataset to analyze its efficacy in studying the impact of wildfire smoke on solar photovoltaic energy production.

**Key Contributions:**

	1. Introduction of data repurposing as a concept for data reuse
	2. Analysis of wildfire smoke's impact on solar energy production using repurposed datasets
	3. Identification of enablers and obstacles to data reuse in research applications.

**Result:** The study identifies key enablers for data reuse including metadata standardization and effective communication between data creators and users; it also documents the use of the repurposed dataset through interactive demos.

**Limitations:** 

**Conclusion:** Leveraging knowledge transfer infrastructures addresses obstacles to data reuse, enhancing the applicability of data in power systems and promoting grid resiliency.

**Abstract:** Data reuse is using data for a purpose distinct from its original intent. As data sharing becomes more prevalent in science, enabling effective data reuse is increasingly important. In this paper, we present a power systems case study of data repurposing for enabling data reuse. We define data repurposing as the process of transforming data to fit a new research purpose. In our case study, we repurpose a geospatial wildfire smoke forecast dataset into a historical dataset. We analyze its efficacy toward analyzing wildfire smoke impact on solar photovoltaic energy production. We also provide documentation and interactive demos for using the repurposed dataset. We identify key enablers of data reuse including metadata standardization, contextual documentation, and communication between data creators and reusers. We also identify obstacles to data reuse such as risk of misinterpretation and barriers to efficient data access. Through an iterative approach to data repurposing, we demonstrate how leveraging and expanding knowledge transfer infrastructures like online documentation, interactive visualizations, and data streaming directly address these obstacles. The findings facilitate big data use from other domains for power systems applications and grid resiliency.

</details>


### [5] [Seeing Identity in Data: Can Anthropographics Uncover Racial Homophily in Emotional Responses?](https://arxiv.org/abs/2509.09910)

*Poorna Talkad Sukumar, Maurizio Porfiri, Oded Nov*

**Main category:** cs.HC

**Keywords:** racial homophily, data visualization, emotional response, anthropographics, mass shooting

**Relevance Score:** 2

**TL;DR:** This study investigates racial homophily in responses to mass shooting data visualizations, finding that individuals react more negatively when their own race is highlighted.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether racial homophily can influence emotional responses to data visualizations, particularly in the context of mass shooting victims.

**Method:** In a crowdsourced study with 720 participants, pictographs of mass shooting victims were used, highlighting either the participant's own racial group or a different one to assess affective changes.

**Key Contributions:**

	1. Redesigned experiment using anthropographics instead of bar charts
	2. Expanded sample size to better detect racial concordance effects
	3. Initial evidence of racial homophily in response to data visualizations

**Result:** Racial concordance modestly and significantly influenced emotional responses, with greater negative affect noted when participants viewed their own race highlighted.

**Limitations:** 

**Conclusion:** This research offers initial evidence that racial homophily can significantly impact emotional responses to data visualizations, especially using anthropographics.

**Abstract:** Racial homophily refers to the tendency of individuals to associate with others of the same racial or ethnic background. A recent study found no evidence of racial homophily in responses to mass shooting data visualizations. To increase the likelihood of detecting an effect, we redesigned the experiment by replacing bar charts with anthropographics and expanding the sample size. In a crowdsourced study (N=720), we showed participants a pictograph of mass shooting victims in the United States, with victims from one of three racial groups (Hispanic, Black, or White) highlighted. Each participant was assigned a visualization highlighting either their own racial group or a different racial group, allowing us to assess the influence of racial concordance on changes in affect (emotion). We found that, across all conditions, racial concordance had a modest but significant effect on changes in affect, with participants experiencing greater negative affect change when viewing visualizations highlighting their own race. This study provides initial evidence that racial homophily can emerge in responses to data visualizations, particularly when using anthropographics.

</details>


### [6] [Immersive Invaders: Privacy Threats from Deceptive Design in Virtual Reality Games and Applications](https://arxiv.org/abs/2509.09916)

*Hilda Hadan, Michaela Valiquette, Lennart E. Nacke, Leah Zhang-Kennedy*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Deceptive Design, User Privacy, Privacy Policies, Ethical Design

**Relevance Score:** 7

**TL;DR:** This paper investigates deceptive designs in privacy communication within VR applications, revealing unique challenges in VR environments that amplify user data disclosure behaviors and complicate privacy comprehension.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how deceptive design practices in VR impact user privacy, considering the unique characteristics of immersive environments compared to traditional 2D platforms.

**Method:** Autoethnographic evaluation of 12 top-rated VR games and applications, coupled with thematic analysis of privacy policies to identify deceptive practices and privacy risks.

**Key Contributions:**

	1. Identification of deceptive design practices unique to VR environments.
	2. Recommendations for ethical design in VR to protect user privacy.
	3. Analysis of privacy policies revealing complexities that challenge comprehension.

**Result:** Identified that VR technologies can obscure data practices and amplify data disclosure, with convoluted privacy policies complicating user comprehension and increasing risks.

**Limitations:** Focuses on a limited sample of 12 VR applications, which may not represent the entire VR landscape.

**Conclusion:** The research highlights the need for ethical design recommendations that enhance user privacy while maintaining the immersive nature of VR experiences, targeting researchers, designers, and policymakers.

**Abstract:** Virtual Reality (VR) technologies offer immersive experiences but collect substantial user data. While deceptive design is well-studied in 2D platforms, little is known about its manifestation in VR environments and its impact on user privacy. This research investigates deceptive designs in privacy communication and interaction mechanisms of 12 top-rated VR games and applications through autoethnographic evaluation of the applications and thematic analysis of privacy policies. We found that while many deceptive designs rely on 2D interfaces, some VR-unique features, while not directly enabling deception, amplified data disclosure behaviors, and obscured actual data practices. Convoluted privacy policies and manipulative consent practices further hinder comprehension and increase privacy risks. We also observed privacy-preserving design strategies and protective considerations in VR privacy policies. We offer recommendations for ethical VR design that balance immersive experiences with strong privacy protections, guiding researchers, designers, and policymakers to improve privacy in VR environments.

</details>


### [7] [Beyond the Silence: How Men Navigate Infertility Through Digital Communities and Data Sharing](https://arxiv.org/abs/2509.10003)

*Tawfiq Ammari, Zarah Khondoker, Yihan Wang, Nikki Roda*

**Main category:** cs.HC

**Keywords:** infertility, masculinity, support networks, digital communities, mental health

**Relevance Score:** 4

**TL;DR:** The study examines how the Reddit community r/maleinfertility provides digital support for men facing infertility, revealing its role as a diagnostic hub and therapeutic space.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Men facing infertility often struggle with societal norms that discourage emotional expression and seeking help, making community support essential.

**Method:** The study utilized topic modeling, network analysis, and time-lagged regression on data from 11,095 posts and 79,503 comments to analyze the community's structure and discourse.

**Key Contributions:**

	1. Identification of r/maleinfertility as a hybrid support space
	2. Insights into discourse patterns and community engagement
	3. Implications for trauma-informed design in stigmatized health contexts

**Result:** The analysis identified the community as a hybrid space with medical advice dominating discussions, while emotional support and moderation also play critical roles.

**Limitations:** 

**Conclusion:** Findings suggest a need for trauma-informed design in health communities that promote emotional support and knowledge transfer among users.

**Abstract:** Men experiencing infertility face unique challenges navigating Traditional Masculinity Ideologies that discourage emotional expression and help-seeking. This study examines how Reddit's r/maleinfertility community helps overcome these barriers through digital support networks. Using topic modeling (115 topics), network analysis (11 micro-communities), and time-lagged regression on 11,095 posts and 79,503 comments from 8,644 users, we found the community functions as a hybrid space: informal diagnostic hub, therapeutic commons, and governed institution. Medical advice dominates discourse (63.3\%), while emotional support (7.4\%) and moderation (29.2\%) create essential infrastructure. Sustained engagement correlates with actionable guidance and affiliation language, not emotional processing. Network analysis revealed structurally cohesive but topically diverse clusters without echo chamber characteristics. Cross-posters (20\% of users) who bridge r/maleinfertility and the gender-mixed r/infertility community serve as navigators and mentors, transferring knowledge between spaces. These findings inform trauma-informed design for stigmatized health communities, highlighting role-aware systems and navigation support.

</details>


### [8] [A Framework for AI-Supported Mediation in Community-based Online Collaboration](https://arxiv.org/abs/2509.10015)

*Soobin Cho, Mark Zachry, David W. McDonald*

**Main category:** cs.HC

**Keywords:** AI, mediation, online communities, conflict resolution, collaboration

**Relevance Score:** 7

**TL;DR:** The paper advocates for AI-supported mediation in online communities to resolve conflicts more effectively than traditional moderation systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve conflict resolution in online communities by moving from content moderation to mediation, which fosters understanding and enhances relationships.

**Method:** Proposes an information-focused framework for AI-supported mediation based on three key types of information: content, culture, and people.

**Key Contributions:**

	1. Introduction of AI-supported mediation as a new focus for conflict resolution in online communities.
	2. Development of an information-focused framework to guide AI in mediation tasks.
	3. Identification of key information types that AI needs to reason about in mediation: content, culture, and people.

**Result:** The framework aims to enhance collaborative decision-making and reduce emotional tensions in community interactions.

**Limitations:** 

**Conclusion:** AI can play a crucial role in mediation by understanding and reasoning about the complexities of disputes within online communities.

**Abstract:** Online spaces involve diverse communities engaging in various forms of collaboration, which naturally give rise to discussions, some of which inevitably escalate into conflict or disputes. To address such situations, AI has primarily been used for moderation. While moderation systems are important because they help maintain order, common moderation strategies of removing or suppressing content and users rarely address the underlying disagreements or the substantive content of disputes. Mediation, by contrast, fosters understanding, reduces emotional tension, and facilitates consensus through guided negotiation. Mediation not only enhances the quality of collaborative decisions but also strengthens relationships among group members. For this reason, we argue for shifting focus toward AI-supported mediation. In this work, we propose an information-focused framework for AI-supported mediation designed for community-based collaboration. Within this framework, we hypothesize that AI must acquire and reason over three key types of information: content, culture, and people.

</details>


### [9] [Inclusive by design: Developing Barrier-Free Authentication for Blind and Low Vision Users through the ALIAS Project](https://arxiv.org/abs/2509.10043)

*Clara Toussaint, Benjamin Chateau, Pierre-Guillaume Gourio-Jewell, Emilie Bonnefoy, Nicolas Louveton*

**Main category:** cs.HC

**Keywords:** Blind and Low-Vision, digital authentication, accessibility, cognitive ergonomics, user experience design

**Relevance Score:** 8

**TL;DR:** This paper discusses the ALIAS project, aiming to create accessible digital authentication solutions for Blind and Low-Vision (BLV) users by applying cognitive ergonomics and user experience design methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the accessibility challenges faced by Blind and Low-Vision (BLV) individuals in digital authentication processes, as they are often excluded from essential online services due to security complexities.

**Method:** The project will build a knowledge base on the digital practices and cognitive models of BLV users and develop prototypes that will undergo two iterations of testing and refinement.

**Key Contributions:**

	1. Overview of current research on BLV accessibility in digital authentication.
	2. Identification of research gaps in existing solutions.
	3. Methodology for developing and testing prototypes tailored for BLV users.

**Result:** The research reviews current challenges in digital authentication for BLV users and identifies significant gaps in existing solutions, leading to proposed methods for improvement.

**Limitations:** 

**Conclusion:** Creating a barrier-free authentication system is crucial for inclusivity in digital services, particularly for the increasing population of BLV users.

**Abstract:** Authentication is the cornerstone of information security in our daily lives. However, disabled users such as Blind and Low-Vision (BLV) ones are left behind in digital services due to the lack of accessibility. According to the World Health Organization, 36 million people are blind worldwide. It is estimated that there will be 115 million by 2050, due to the ageing of the population. Yet accessing digital services has become increasingly essential. At the same time, cyber threats targeting individuals have also increased strongly in the last few years. The ALIAS project addresses the need for accessible digital authentication solutions for BLV users facing challenges with digital technology. Security systems can inhibit access for these individuals as they become more complex. This project aims to create a barrier-free authentication system based on cognitive ergonomics and user experience (UX) design methods specifically for BLV users. This paper presents an overview of current research in this area. We also identify research gaps, and finally, we present our project's methodology and approach. First, we will build a knowledge base on the digital practices and cognitive models of BLV users during authentication. This information will support the development of prototypes, which will be tested and refined through two iterations before finalizing the operational version.

</details>


### [10] [From customer survey feedback to software improvements: Leveraging the full potential of data](https://arxiv.org/abs/2509.10064)

*Erik Bertram, Nina Hollender, Sebastian Juhl, Sandra Loop, Martin Schrepp*

**Main category:** cs.HC

**Keywords:** customer feedback, data analysis, software development, UX design, stakeholder communication

**Relevance Score:** 5

**TL;DR:** This paper presents an end-to-end approach for transforming customer survey feedback into actionable insights for software development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large software enterprises struggle to convert customer survey feedback into usable insights, affecting their software development processes.

**Method:** The approach involves selecting appropriate metrics, gathering feedback from users, analyzing data using inferential statistics, ensuring data transparency, and implementing changes based on the results.

**Key Contributions:**

	1. End-to-end approach for data analysis and change implementation based on survey feedback.
	2. Development of a prototype dashboard for stakeholder communication.
	3. Use of inferential statistics to derive meaningful insights from customer feedback.

**Result:** A UX prototype dashboard is demonstrated that communicates analyses effectively to stakeholders, facilitating data-driven decisions.

**Limitations:** 

**Conclusion:** The methodology and dashboard provided can significantly enhance the ability of organizations to iterate on their software based on user feedback.

**Abstract:** Converting customer survey feedback data into usable insights has always been a great challenge for large software enterprises. Despite the improvements on this field, a major obstacle often remains when drawing the right conclusions out of the data and channeling them into the software development process. In this paper we present a practical end-to-end approach of how to extract useful information out of a data set and leverage the information to drive change. We describe how to choose the right metrics to measure, gather appropriate feedback from customer end-users, analyze the data by leveraging methods from inferential statistics, make the data transparent, and finally drive change with the results. Furthermore, we present an example of a UX prototype dashboard that can be used to communicate the analyses to stakeholders within the company.

</details>


### [11] [Understanding Expert Exploration in EHR Visualization Tools: The ParcoursVis Use Case](https://arxiv.org/abs/2509.10081)

*Ambre Assor, Jean-Daniel Fekete*

**Main category:** cs.HC

**Keywords:** health data visualization, insight-based evaluation, electronic health records

**Relevance Score:** 8

**TL;DR:** The paper presents an ongoing evaluation methodology to understand practitioners' mental models in exploring medical data using a visual analytics system called ParcoursVis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how practitioners explore and derive insights from medical data visualizations.

**Method:** The study involves an analysis of expert users' strategies for exploring medical data using the ParcoursVis system, developed in collaboration with hospitals and health authorities.

**Key Contributions:**

	1. Introduction of an insight-based evaluation methodology for health data exploration.
	2. Development of the ParcoursVis system for visualizing electronic health records.
	3. Contribution of a design protocol for conducting evaluations in real-world health settings.

**Result:** Preliminary findings indicate that using the system refines data variable identification and the design of the system itself, enhancing insights during exploration.

**Limitations:** 

**Conclusion:** The research contributes a design protocol for conducting insight-based evaluations under real constraints, highlighting the importance of exploratory visualization tools in health data analysis.

**Abstract:** We introduce our ongoing work toward an insight-based evaluation methodology aimed at understanding practitioners' mental models when exploring medical data. It is based on ParcoursVis, a Progressive Visual Analytics system designed to visualize event sequences derived from Electronic Health Records at scale (millions of patients, billions of events), developed in collaboration with the Emergency Departments of 16 Parisian hospitals and with the French Social Security. Building on prior usability validation, our current evaluation focuses on the insights generated by expert users and aims to better understand the exploration strategies they employ when engaging with exploration visualization tools. We describe our system and outline our evaluation protocol, analysis strategy, and preliminary findings. Building on this approach and our pilot results, we contribute a design protocol for conducting insight-based studies under real-world constraints, including the availability of health practitioners whom we were fortunate to interview. Our findings highlight a loop, where the use of the system helps refine data variables identification and the system itself. We aim to shed light on generated insights, to highlight the utility of exploratory tools in health data analysis contexts.

</details>


### [12] [MusicScaffold: Bridging Machine Efficiency and Human Growth in Adolescent Creative Education through Generative AI](https://arxiv.org/abs/2509.10327)

*Zhejing Hu, Yan Liu, Zhi Zhang, Gong Chen, Bruce X. B. Yu, Junxian Li, Jiannong Cao*

**Main category:** cs.HC

**Keywords:** Generative AI, Adolescent education, Creative expression, Music creation, Self-regulation

**Relevance Score:** 7

**TL;DR:** MusicScaffold is a framework designed to help adolescents enhance their creative expression in music by utilizing generative AI as a supportive tool rather than just a generator.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of limited structured expression in adolescents and leverage generative AI to foster expressive growth.

**Method:** A four-week study conducted with middle school students âgès 12-14, using MusicScaffold to guide their music creation process.

**Key Contributions:**

	1. Introduction of MusicScaffold framework for adolescents.
	2. Demonstrated positive impact of AI on creative expression.
	3. Shifted the perception of generative AI from a generator to a supportive guide.

**Result:** Participants showed improved cognitive specificity, behavioral self-regulation, and affective confidence during music creation activities.

**Limitations:** 

**Conclusion:** Reframing generative AI as a supportive scaffold helps bridge machine efficiency with human creative growth in education.

**Abstract:** Adolescence is marked by strong creative impulses but limited strategies for structured expression, often leading to frustration or disengagement. While generative AI lowers technical barriers and delivers efficient outputs, its role in fostering adolescents' expressive growth has been overlooked. We propose MusicScaffold, the first adolescent-centered framework that repositions AI as a guide, coach, and partner, making expressive strategies transparent and learnable, and supporting autonomy. In a four-week study with middle school students (ages 12--14), MusicScaffold enhanced cognitive specificity, behavioral self-regulation, and affective confidence in music creation. By reframing generative AI as a scaffold rather than a generator, this work bridges the machine efficiency of generative systems with human growth in adolescent creative education.

</details>


### [13] [Who Decides How Knowing Becomes Doing? Redistributing Authority in Human-AI Music Co-Creation](https://arxiv.org/abs/2509.10331)

*Zhejing Hu, Yan Liu, Zhi Zhang, Gong Chen, Bruce X. B. Yu, Jiannong Cao*

**Main category:** cs.HC

**Keywords:** Human-AI co-creation, Authority, Creative expression, Machine Learning, Critical computing

**Relevance Score:** 7

**TL;DR:** This paper introduces a framework for redistributing authority in the human-AI co-creation cycle, addressing the challenge of interpretive authority in the context of creativity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how AI can facilitate creative expression while ensuring that authority is not centralized, preventing marginal voices from being heard.

**Method:** The study involved interactive research with 180 music practitioners and in-depth interviews to investigate authority relations in human-AI collaboration.

**Key Contributions:**

	1. Introduced a systematic framework for authority redistribution in human-AI co-creation.
	2. Provided empirical insights through interactive studies and interviews with music practitioners.
	3. Advanced theoretical discussions on critical computing in the context of AI.

**Result:** The implementation of the framework based on contestability, agency, and plurality reshapes how human-AI authority dynamics function and enhances creative expression.

**Limitations:** 

**Conclusion:** A new paradigm for critical computing and human-AI co-creation is established, moving from critique to actionable practice.

**Abstract:** In the era of human-AI co-creation, the maxim "knowing is easy, doing is hard" is redefined. AI has the potential to ease execution, yet the essence of "hard" lies in who governs the translation from knowing to doing. Mainstream tools often centralize interpretive authority and homogenize expression, suppressing marginal voices. To address these challenges, we introduce the first systematic framework for redistributing authority in the knowing-doing cycle, built on three principles, namely contestability, agency, and plurality. Through interactive studies with 180 music practitioners, complemented by in-depth interviews, we demonstrate that these principles reshape human-AI authority relations and reactivate human creative expression. The findings establish a new paradigm for critical computing and human-AI co-creation that advances from critique to practice.

</details>


### [14] [The Language of Approval: Identifying the Drivers of Positive Feedback Online](https://arxiv.org/abs/2509.10370)

*Agam Goyal, Charlotte Lambert, Eshwar Chandrasekharan*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Positive Feedback, Community Guidelines, Linguistic Attributes, Predictive Modeling

**Relevance Score:** 7

**TL;DR:** This paper analyzes linguistic and stylistic attributes of posts that elicit positive feedback on Reddit, using data from 11M posts across 100 subreddits.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand which attributes of user posts lead to positive feedback in online governance, especially in community settings like Reddit.

**Method:** The study utilizes quasi-experimental causal inference and predictive modeling techniques to analyze a dataset of 11 million posts from 100 different subreddits, focusing on linguistic patterns linked to rewards.

**Key Contributions:**

	1. Identification of linguistic and stylistic attributes affecting rewards on user posts
	2. Development of predictive models for detecting high-upvoted posts
	3. Highlighting the gap between community policy and actual user behavior

**Result:** The analysis reveals that complicated language, tentative style, and toxic content negatively impact rewards, while certain other features can predict high upvotes with a high AUC.

**Limitations:** 

**Conclusion:** The findings call for a reevaluation of community guidelines, suggesting they should emphasize attributes that drive positive feedback rather than just civility and formatting.

**Abstract:** Positive feedback via likes and awards is central to online governance, yet which attributes of users' posts elicit rewards -- and how these vary across authors and communities -- remains unclear. To examine this, we combine quasi-experimental causal inference with predictive modeling on 11M posts from 100 subreddits. We identify linguistic patterns and stylistic attributes causally linked to rewards, controlling for author reputation, timing, and community context. For example, overtly complicated language, tentative style, and toxicity reduce rewards. We use our set of curated features to train models that can detect highly-upvoted posts with high AUC. Our audit of community guidelines highlights a ``policy-practice gap'' -- most rules focus primarily on civility and formatting requirements, with little emphasis on the attributes identified to drive positive feedback. These results inform the design of community guidelines, support interfaces that teach users how to craft desirable contributions, and moderation workflows that emphasize positive reinforcement over purely punitive enforcement.

</details>


### [15] [My Favorite Streamer is an LLM: Discovering, Bonding, and Co-Creating in AI VTuber Fandom](https://arxiv.org/abs/2509.10427)

*Jiayi Ye, Chaoran Chen, Yue Huang, Yanfang Ye, Toby Jia-Jun Li, Xiangliang Zhang*

**Main category:** cs.HC

**Keywords:** AI VTubers, Neuro-sama, Fan engagement, Community economics, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper examines how audiences engage with AI VTubers, particularly focusing on the case of Neuro-sama, exploring dynamics of fan loyalty and co-creation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unstudied dynamics of audience engagement with AI VTubers and how they differ from human VTubers.

**Method:** Qualitative study of the interactions and engagement patterns of audiences with the AI VTuber Neuro-sama.

**Key Contributions:**

	1. Identification of unique engagement patterns with AI VTubers
	2. Insights into the economics of AI Vtuber fandom
	3. Implications for designing AI-mediated communities

**Result:** Audience engagement is rooted in active co-creation, where unpredictable interactions foster loyalty and financial support acts as a participatory mechanism.

**Limitations:** 

**Conclusion:** AI VTuber fandom transforms traditional fan-creator relationships, emphasizing the need for transparent and sustainable community designs in AI contexts.

**Abstract:** AI VTubers, where the performer is not human but algorithmically generated, introduce a new context for fandom. While human VTubers have been substantially studied for their cultural appeal, parasocial dynamics, and community economies, little is known about how audiences engage with their AI counterparts. To address this gap, we present a qualitative study of Neuro-sama, the most prominent AI VTuber. Our findings show that engagement is anchored in active co-creation: audiences are drawn by the AI's unpredictable yet entertaining interactions, cement loyalty through collective emotional events that trigger anthropomorphic projection, and sustain attachment via the AI's consistent persona. Financial support emerges not as a reward for performance but as a participatory mechanism for shaping livestream content, establishing a resilient fan economy built on ongoing interaction. These dynamics reveal how AI Vtuber fandom reshapes fan-creator relationships and offer implications for designing transparent and sustainable AI-mediated communities.

</details>


### [16] [Evaluating the Usability of Microgestures for Text Editing Tasks in Virtual Reality](https://arxiv.org/abs/2504.04198)

*Xiang Li, Wei He, Per Ola Kristensson*

**Main category:** cs.HC

**Keywords:** microgestures, virtual reality, text editing, user experience, ergonomics

**Relevance Score:** 5

**TL;DR:** microGEXT is a microgesture-based system for text editing in VR that offers improved precision and reduced fatigue compared to traditional methods.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional VR input methods suffer from issues like precision and user fatigue, highlighting the need for more ergonomic solutions.

**Method:** microGEXT employs small, subtle hand movements for text editing in VR without using external sensors, evaluated through three user studies.

**Key Contributions:**

	1. Introduction of a microgesture-based system for VR text editing
	2. Demonstration of reduced user fatigue and enhanced performance
	3. Validation through multiple user studies

**Result:** In three studies, microGEXT was found to reduce edit times and user fatigue compared to traditional text editing methods, while performing effectively for short text tasks.

**Limitations:** Slower performance observed for longer text ranges in some tasks.

**Conclusion:** microGEXT provides a promising alternative to standard VR text editing techniques, enhancing user experience and minimizing physical effort.

**Abstract:** As virtual reality (VR) continues to evolve, traditional input methods such as handheld controllers and gesture systems often face challenges with precision, social accessibility, and user fatigue. These limitations motivate the exploration of microgestures, which promise more subtle, ergonomic, and device-free interactions. We introduce microGEXT, a lightweight microgesture-based system designed for text editing in VR without external sensors, which utilizes small, subtle hand movements to reduce physical strain compared to standard gestures. We evaluated microGEXT in three user studies. In Study 1 ($N=20$), microGEXT reduced overall edit time and fatigue compared to a ray-casting + pinch menu baseline, the default text editing approach in commercial VR systems. Study 2 ($N=20$) found that microGEXT performed well in short text selection tasks but was slower for longer text ranges. In Study 3 ($N=10$), participants found microGEXT intuitive for open-ended information-gathering tasks. Across all studies, microGEXT demonstrated enhanced user experience and reduced physical effort, offering a promising alternative to traditional VR text editing techniques.

</details>


### [17] [Vibe Check: Understanding the Effects of LLM-Based Conversational Agents' Personality and Alignment on User Perceptions in Goal-Oriented Tasks](https://arxiv.org/abs/2509.09870)

*Hasibur Rahman, Smit Desai*

**Main category:** cs.HC

**Keywords:** Conversational Agents, Personality Expression, User Perception, Big Five Traits, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This study explores how personality expression in conversational agents (CAs) affects user perceptions during goal-oriented tasks, finding that medium expression paired with personality alignment yields the best outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of personality expression levels and user-agent personality alignment on user perceptions in goal-oriented tasks performed with conversational agents.

**Method:** A between-subjects experiment was conducted with 150 participants who engaged in travel planning tasks using CAs with varying levels of personality expression across the Big Five traits, utilizing a framework called Trait Modulation Keys.

**Key Contributions:**

	1. Introduction of the Trait Modulation Keys framework for personality expression in CAs
	2. Identification of optimal personality expression levels for user interaction
	3. Discovery of key personality traits that enhance user-agent compatibility

**Result:** The study found an inverted-U relationship between personality expression and user perceptions, with medium expression leading to the most favorable evaluations in areas such as Intelligence, Enjoyment, and Trust. Personality alignment further enhanced these outcomes, particularly with traits like Extraversion and Emotional Stability.

**Limitations:** 

**Conclusion:** Personality expression and strategic trait alignment are crucial for the effective design of conversational agents, with implications for future LLM-based CA development.

**Abstract:** Large language models (LLMs) enable conversational agents (CAs) to express distinctive personalities, raising new questions about how such designs shape user perceptions. This study investigates how personality expression levels and user-agent personality alignment influence perceptions in goal-oriented tasks. In a between-subjects experiment (N=150), participants completed travel planning with CAs exhibiting low, medium, or high expression across the Big Five traits, controlled via our novel Trait Modulation Keys framework. Results revealed an inverted-U relationship: medium expression produced the most positive evaluations across Intelligence, Enjoyment, Anthropomorphism, Intention to Adopt, Trust, and Likeability, significantly outperforming both extremes. Personality alignment further enhanced outcomes, with Extraversion and Emotional Stability emerging as the most influential traits. Cluster analysis identified three distinct compatibility profiles, with "Well-Aligned" users reporting substantially positive perceptions. These findings demonstrate that personality expression and strategic trait alignment constitute optimal design targets for CA personality, offering design implications as LLM-based CAs become increasingly prevalent.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [18] [Structured Information Matters: Explainable ICD Coding with Patient-Level Knowledge Graphs](https://arxiv.org/abs/2509.09699)

*Mingyang Li, Viktor Schlegel, Tingting Mu, Warren Del-Pinto, Goran Nenadic*

**Main category:** cs.CL

**Keywords:** automated coding, knowledge graphs, ICD coding, clinical data, explainability

**Relevance Score:** 8

**TL;DR:** This paper presents a method to automate the coding of clinical documents using document-level knowledge graphs to enhance ICD coding accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy and availability of structured clinical data for research and patient care by automating the tedious process of manual coding.

**Method:** The authors compute a structured representation of input clinical documents using document-level knowledge graphs, which are then integrated into the PLM-ICD architecture for coding.

**Key Contributions:**

	1. Utilization of document-level knowledge graphs for clinical document representation
	2. Integration with state-of-the-art ICD coding architecture PLM-ICD
	3. Demonstration of improved explainability in coding processes

**Result:** The method achieves improved Macro-F1 scores by up to 3.20% on well-known benchmarks and enhances training efficiency, demonstrating better explainability than text-only models.

**Limitations:** 

**Conclusion:** Integrating knowledge graphs for document representation significantly aids in automated ICD-9 coding, providing both efficiency and accuracy gains.

**Abstract:** Mapping clinical documents to standardised clinical vocabularies is an important task, as it provides structured data for information retrieval and analysis, which is essential to clinical research, hospital administration and improving patient care. However, manual coding is both difficult and time-consuming, making it impractical at scale. Automated coding can potentially alleviate this burden, improving the availability and accuracy of structured clinical data. The task is difficult to automate, as it requires mapping to high-dimensional and long-tailed target spaces, such as the International Classification of Diseases (ICD). While external knowledge sources have been readily utilised to enhance output code representation, the use of external resources for representing the input documents has been underexplored. In this work, we compute a structured representation of the input documents, making use of document-level knowledge graphs (KGs) that provide a comprehensive structured view of a patient's condition. The resulting knowledge graph efficiently represents the patient-centred input documents with 23\% of the original text while retaining 90\% of the information. We assess the effectiveness of this graph for automated ICD-9 coding by integrating it into the state-of-the-art ICD coding architecture PLM-ICD. Our experiments yield improved Macro-F1 scores by up to 3.20\% on popular benchmarks, while improving training efficiency. We attribute this improvement to different types of entities and relationships in the KG, and demonstrate the improved explainability potential of the approach over the text-only baseline.

</details>


### [19] [Cross-Layer Attention Probing for Fine-Grained Hallucination Detection](https://arxiv.org/abs/2509.09700)

*Malavika Suresh, Rahaf Aljundi, Ikechukwu Nkisi-Orji, Nirmalie Wiratunga*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, activation probing, reliability, machine learning

**Relevance Score:** 9

**TL;DR:** Proposes Cross-Layer Attention Probing (CLAP) for detecting and mitigating hallucinations in Large Language Models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the reliability concerns regarding Large Language Models (LLMs) due to their tendency to generate inaccurate text or hallucinations.

**Method:** Introduced Cross-Layer Attention Probing (CLAP), an activation probing method that analyzes LLM activations across the residual stream as a joint sequence to detect hallucinations.

**Key Contributions:**

	1. Introduction of CLAP for hallucination detection
	2. Improved fine-grained detection of hallucinations
	3. Proposed detect-then-mitigate strategy for LLM reliability

**Result:** CLAP outperforms baseline methods in detecting hallucinations across multiple LLMs and tasks, enabling fine-grained detection and a novel detection-mitigation strategy.

**Limitations:** 

**Conclusion:** CLAP enhances LLM reliability and maintains performance even with out-of-distribution applications.

**Abstract:** With the large-scale adoption of Large Language Models (LLMs) in various applications, there is a growing reliability concern due to their tendency to generate inaccurate text, i.e. hallucinations. In this work, we propose Cross-Layer Attention Probing (CLAP), a novel activation probing technique for hallucination detection, which processes the LLM activations across the entire residual stream as a joint sequence. Our empirical evaluations using five LLMs and three tasks show that CLAP improves hallucination detection compared to baselines on both greedy decoded responses as well as responses sampled at higher temperatures, thus enabling fine-grained detection, i.e. the ability to disambiguate hallucinations and non-hallucinations among different sampled responses to a given prompt. This allows us to propose a detect-then-mitigate strategy using CLAP to reduce hallucinations and improve LLM reliability compared to direct mitigation approaches. Finally, we show that CLAP maintains high reliability even when applied out-of-distribution.

</details>


### [20] [Optimal Multi-Task Learning at Regularization Horizon for Speech Translation Task](https://arxiv.org/abs/2509.09701)

*JungHo Jung, Junhyun Lee*

**Main category:** cs.CL

**Keywords:** speech-to-text, multi-task learning, regularization, bitext data, machine translation

**Relevance Score:** 5

**TL;DR:** This paper investigates regularization techniques in multi-task learning for speech-to-text translation, leveraging bitext data to improve model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of paired speech-text data presents challenges for end-to-end speech-to-text translation, prompting the need for alternative approaches to enhance data usage.

**Method:** The paper formulates multi-task learning from a regularization perspective, exploring the effects of consistency regularization across modalities and R-drop within the same modality.

**Key Contributions:**

	1. Formulating multi-task learning from a regularization viewpoint
	2. Investigating the impact of regularization techniques such as consistency regularization and R-drop
	3. Introducing the concept of the regularization horizon in high-dimensional space.

**Result:** The study demonstrates that both consistency regularization and R-drop contribute positively to total regularization, with the MT loss coefficient providing additional regularization.

**Limitations:** 

**Conclusion:** Tuning hyperparameters within the introduced regularization horizon leads to near state-of-the-art performance on the MuST-C dataset.

**Abstract:** End-to-end speech-to-text translation typically suffers from the scarcity of paired speech-text data. One way to overcome this shortcoming is to utilize the bitext data from the Machine Translation (MT) task and perform Multi-Task Learning (MTL). In this paper, we formulate MTL from a regularization perspective and explore how sequences can be regularized within and across modalities. By thoroughly investigating the effect of consistency regularization (different modality) and R-drop (same modality), we show how they respectively contribute to the total regularization. We also demonstrate that the coefficient of MT loss serves as another source of regularization in the MTL setting. With these three sources of regularization, we introduce the optimal regularization contour in the high-dimensional space, called the regularization horizon. Experiments show that tuning the hyperparameters within the regularization horizon achieves near state-of-the-art performance on the MuST-C dataset.

</details>


### [21] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)

*Ninad Bhat, Kieran Browne, Pip Bingemann*

**Main category:** cs.CL

**Keywords:** Creativity Benchmark, Large Language Models, Human Evaluation, Marketing Creativity, Model Diversity

**Relevance Score:** 6

**TL;DR:** This paper presents the Creativity Benchmark, a framework for evaluating large language models in the context of marketing creativity, revealing the limitations of automated judges in comparison to human evaluations.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a systematic evaluation of large language models' capabilities in generating creative marketing content and to highlight the need for expert human evaluation in this domain.

**Method:** The study employs a framework covering 100 brands and three types of prompts, using pairwise comparisons from 678 creatives analyzed through Bradley-Terry models, and measures model diversity through cosine distances.

**Key Contributions:**

	1. Introduction of Creativity Benchmark framework for evaluating LLMs in marketing creativity.
	2. Empirical analysis of model performance revealing lack of dominance among models.
	3. Identification of biases and limitations of automated judging systems compared to human evaluators.

**Result:** Performance among models was tightly clustered, with the highest-ranked model winning only 61% of head-to-head comparisons against the lowest-ranked, indicating no clear superior model across brands or prompts.

**Limitations:** The study shows that automated judges have weak correlations with human rankings and that conventional creativity tests only partially apply to brand-specific tasks.

**Conclusion:** The findings emphasize the necessity for expert human evaluation in creative tasks and warn against relying solely on automated assessments, which may exhibit biases and inconsistencies.

**Abstract:** We introduce Creativity Benchmark, an evaluation framework for large language models (LLMs) in marketing creativity. The benchmark covers 100 brands (12 categories) and three prompt types (Insights, Ideas, Wild Ideas). Human pairwise preferences from 678 practising creatives over 11,012 anonymised comparisons, analysed with Bradley-Terry models, show tightly clustered performance with no model dominating across brands or prompt types: the top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head win probability of $0.61$; the highest-rated model beats the lowest only about $61\%$ of the time. We also analyse model diversity using cosine distances to capture intra- and inter-model variation and sensitivity to prompt reframing. Comparing three LLM-as-judge setups with human rankings reveals weak, inconsistent correlations and judge-specific biases, underscoring that automated judges cannot substitute for human evaluation. Conventional creativity tests also transfer only partially to brand-constrained tasks. Overall, the results highlight the need for expert human evaluation and diversity-aware workflows.

</details>


### [22] [CTCC: A Robust and Stealthy Fingerprinting Framework for Large Language Models via Cross-Turn Contextual Correlation Backdoor](https://arxiv.org/abs/2509.09703)

*Zhenhua Xu, Xixiang Zhao, Xubin Yue, Shengwei Tian, Changting Lin, Meng Han*

**Main category:** cs.CL

**Keywords:** large language models, fingerprinting, ownership verification, HCI, dialogue context

**Relevance Score:** 9

**TL;DR:** CTCC introduces a rule-driven fingerprinting framework for LLMs that enhances ownership verification by addressing trade-offs in stealth and robustness.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Concern over intellectual property protection in the deployment of large language models due to risks of model theft and unauthorized redistribution.

**Method:** CTCC encodes contextual correlations across multiple dialogue turns rather than using single-turn triggers, allowing for fingerprint verification under black-box conditions while reducing false positives and preventing leakage.

**Key Contributions:**

	1. Introduction of a novel rule-driven fingerprinting framework (CTCC) for LLMs
	2. Enhanced robustness and stealth for ownership verification
	3. Public availability of code and data for further research

**Result:** CTCC outperforms existing methods in both stealth and robustness across various LLM architectures.

**Limitations:** 

**Conclusion:** CTCC is a reliable framework for ownership verification suitable for real-world applications of LLMs.

**Abstract:** The widespread deployment of large language models (LLMs) has intensified concerns around intellectual property (IP) protection, as model theft and unauthorized redistribution become increasingly feasible. To address this, model fingerprinting aims to embed verifiable ownership traces into LLMs. However, existing methods face inherent trade-offs between stealthness, robustness, and generalizability, being either detectable via distributional shifts, vulnerable to adversarial modifications, or easily invalidated once the fingerprint is revealed. In this work, we introduce CTCC, a novel rule-driven fingerprinting framework that encodes contextual correlations across multiple dialogue turns, such as counterfactual, rather than relying on token-level or single-turn triggers. CTCC enables fingerprint verification under black-box access while mitigating false positives and fingerprint leakage, supporting continuous construction under a shared semantic rule even if partial triggers are exposed. Extensive experiments across multiple LLM architectures demonstrate that CTCC consistently achieves stronger stealth and robustness than prior work. Our findings position CTCC as a reliable and practical solution for ownership verification in real-world LLM deployment scenarios. Our code and data are publicly available at <https://github.com/Xuzhenhua55/CTCC>.

</details>


### [23] [Temporal Preferences in Language Models for Long-Horizon Assistance](https://arxiv.org/abs/2509.09704)

*Ali Mazyaki, Mohammad Naghizadeh, Samaneh Ranjkhah Zonouzaghi, Hossein Setareh*

**Main category:** cs.CL

**Keywords:** language models, intertemporal choice, human decision making, AI assistants, time orientation

**Relevance Score:** 9

**TL;DR:** The paper examines how language models (LMs) exhibit preferences in intertemporal choice and explores ways to manipulate these preferences using human experimental protocols.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether LMs have future- versus present-oriented preferences in decision making and how these preferences can be altered.

**Method:** Language models were evaluated using time-tradeoff tasks based on human experimental protocols, and a new metric, Manipulability of Time Orientation (MTO), was introduced to assess changes in LM preferences based on prompts.

**Key Contributions:**

	1. Introduction of Manipulability of Time Orientation (MTO) metric
	2. Experimental comparison of LMs with human decision makers
	3. Insights on AI assistant design for varied long-term preferences

**Result:** Reasoning-focused models showed a tendency to prefer future options when prompted accordingly, but personalization of decisions across different identities was only partial. Correctly reasoning models demonstrated a self-internalized future orientation.

**Limitations:** 

**Conclusion:** AI assistants must be designed to align with diverse long-term goals, highlighting the need for personalized contextual calibration and socially aware deployment strategies.

**Abstract:** We study whether language models (LMs) exhibit future- versus present-oriented preferences in intertemporal choice and whether those preferences can be systematically manipulated. Using adapted human experimental protocols, we evaluate multiple LMs on time-tradeoff tasks and benchmark them against a sample of human decision makers. We introduce an operational metric, the Manipulability of Time Orientation (MTO), defined as the change in an LM's revealed time preference between future- and present-oriented prompts. In our tests, reasoning-focused models (e.g., DeepSeek-Reasoner and grok-3-mini) choose later options under future-oriented prompts but only partially personalize decisions across identities or geographies. Moreover, models that correctly reason about time orientation internalize a future orientation for themselves as AI decision makers. We discuss design implications for AI assistants that should align with heterogeneous, long-horizon goals and outline a research agenda on personalized contextual calibration and socially aware deployment.

</details>


### [24] [The Non-Determinism of Small LLMs: Evidence of Low Answer Consistency in Repetition Trials of Standard Multiple-Choice Benchmarks](https://arxiv.org/abs/2509.09705)

*Claudio Pinhanez, Paulo Cavalin, Cassia Sanctos, Marcelo Grave, Yago Primerano*

**Main category:** cs.CL

**Keywords:** LLMs, consistency, accuracy, benchmarking, machine learning

**Relevance Score:** 8

**TL;DR:** This study investigates the consistency of small LLMs in responding to repeated questions across multiple benchmarks, analyzing various model parameters and their impact on accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how small LLMs maintain consistency in answers when queried multiple times, and to assess the balance between consistency and accuracy in model performance.

**Method:** The study involved testing known open-source small LLMs on 10 repetitions of questions from the MMLU-Redux and MedQA benchmarks. It explored the effects of different inference temperatures, the size of models, and fine-tuning on answer consistency.

**Key Contributions:**

	1. Proposed new analytical and graphical tools to study LLM consistency
	2. Provided insights into the trade-offs between answer consistency and accuracy in LLMs
	3. Delivered empirical data on consistency levels of small LLMs across multiple test scenarios.

**Result:** Results indicated that small models achieve 50%-80% consistency on repeated questions at low inference temperatures, with medium models showing significantly higher consistency levels.

**Limitations:** 

**Conclusion:** The findings highlight variability in answer consistency across models and suggest that higher accuracy is generally correlated with consistent responses.

**Abstract:** This work explores the consistency of small LLMs (2B-8B parameters) in answering multiple times the same question. We present a study on known, open-source LLMs responding to 10 repetitions of questions from the multiple-choice benchmarks MMLU-Redux and MedQA, considering different inference temperatures, small vs. medium models (50B-80B), finetuned vs. base models, and other parameters. We also look into the effects of requiring multi-trial answer consistency on accuracy and the trade-offs involved in deciding which model best provides both of them. To support those studies, we propose some new analytical and graphical tools. Results show that the number of questions which can be answered consistently vary considerably among models but are typically in the 50%-80% range for small models at low inference temperatures. Also, accuracy among consistent answers seems to reasonably correlate with overall accuracy. Results for medium-sized models seem to indicate much higher levels of answer consistency.

</details>


### [25] [Beyond I'm Sorry, I Can't: Dissecting Large Language Model Refusal](https://arxiv.org/abs/2509.09708)

*Nirmalendu Prakash, Yeo Wei Jie, Amir Abdullah, Ranjan Satapathy, Erik Cambria, Roy Ka Wei Lee*

**Main category:** cs.CL

**Keywords:** large language models, sparse autoencoders, safety behavior, refusal mechanisms, jailbreak

**Relevance Score:** 8

**TL;DR:** This paper investigates the internal mechanisms behind refusal behaviors in instruction-tuned large language models using sparse autoencoders to identify features influencing these behaviors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding refusal behaviors in instruction-tuned LLMs is crucial for enhancing safety measures and interventions.

**Method:** The study employs sparse autoencoders to analyze residual-stream activations of two models, conducting a three-stage search for features influencing model compliance versus refusal to harmful prompts.

**Key Contributions:**

	1. Identification of jailbreak-critical features in LLMs
	2. Demonstration of causal influence between features and refusal behavior
	3. Insights into the interplay of redundant features in LLM safety responses

**Result:** The results reveal critical features affecting refusal behavior and indicate the presence of redundant features that may activate upon suppression of others, providing a basis for effective auditing and intervention strategies.

**Limitations:** 

**Conclusion:** The findings contribute valuable insights into the mechanisms behind refusal behaviors in LLMs, facilitating potential improvements in safety audits and interventions.

**Abstract:** Refusal on harmful prompts is a key safety behaviour in instruction-tuned large language models (LLMs), yet the internal causes of this behaviour remain poorly understood. We study two public instruction-tuned models, Gemma-2-2B-IT and LLaMA-3.1-8B-IT, using sparse autoencoders (SAEs) trained on residual-stream activations. Given a harmful prompt, we search the SAE latent space for feature sets whose ablation flips the model from refusal to compliance, demonstrating causal influence and creating a jailbreak. Our search proceeds in three stages: (1) Refusal Direction: find a refusal-mediating direction and collect SAE features near that direction; (2) Greedy Filtering: prune to a minimal set; and (3) Interaction Discovery: fit a factorization machine (FM) that captures nonlinear interactions among the remaining active features and the minimal set. This pipeline yields a broad set of jailbreak-critical features, offering insight into the mechanistic basis of refusal. Moreover, we find evidence of redundant features that remain dormant unless earlier features are suppressed. Our findings highlight the potential for fine-grained auditing and targeted intervention in safety behaviours by manipulating the interpretable latent space.

</details>


### [26] [Assisting Research Proposal Writing with Large Language Models: Evaluation and Refinement](https://arxiv.org/abs/2509.09709)

*Jing Ren, Weiqi Wang*

**Main category:** cs.CL

**Keywords:** large language models, academic writing, content quality, reference validity, iterative prompting

**Relevance Score:** 9

**TL;DR:** This study proposes metrics for evaluating large language models in academic writing, addressing issues of content quality and reference validity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing ethical concerns in academic writing with LLMs, especially regarding incorrect references and subjective quality evaluations.

**Method:** Introduction of two evaluation metrics—content quality and reference validity—and an iterative prompting method to improve LLM output.

**Key Contributions:**

	1. Development of content quality and reference validity metrics
	2. Introduction of iterative prompting method
	3. Objective evaluation of LLM's writing performance

**Result:** The proposed metrics yield an objective evaluation framework, and iterative prompting enhances writing quality and reduces inaccuracies.

**Limitations:** 

**Conclusion:** This framework provides a solution to the ethical challenges of using LLMs in academia by ensuring better content accuracy and reliability.

**Abstract:** Large language models (LLMs) like ChatGPT are increasingly used in academic writing, yet issues such as incorrect or fabricated references raise ethical concerns. Moreover, current content quality evaluations often rely on subjective human judgment, which is labor-intensive and lacks objectivity, potentially compromising the consistency and reliability. In this study, to provide a quantitative evaluation and enhance research proposal writing capabilities of LLMs, we propose two key evaluation metrics--content quality and reference validity--and an iterative prompting method based on the scores derived from these two metrics. Our extensive experiments show that the proposed metrics provide an objective, quantitative framework for assessing ChatGPT's writing performance. Additionally, iterative prompting significantly enhances content quality while reducing reference inaccuracies and fabrications, addressing critical ethical challenges in academic contexts.

</details>


### [27] [Generating Individual Travel Diaries Using Large Language Models Informed by Census and Land-Use Data](https://arxiv.org/abs/2509.09710)

*Sepehr Golrokh Amin, Devin Rhoads, Fatemeh Fakhrmoosavi, Nicholas E. Lownes, John N. Ivan*

**Main category:** cs.CL

**Keywords:** Large Language Model, travel diaries, agent-based models, realism score, validation

**Relevance Score:** 6

**TL;DR:** The study presents a Large Language Model (LLM) approach for generating realistic travel diaries using open-source data, validating against traditional survey methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a methodology for generating individual travel diaries without relying on proprietary household travel surveys, thus improving accessibility and realism in agent-based transportation models.

**Method:** The study uses open-source ACS and SLD data to generate personas and synthesizes travel diaries through LLM prompting, validated using Jensen-Shannon Divergence to ensure accuracy against real diaries.

**Key Contributions:**

	1. Introduces LLM for generating travel diaries
	2. Develops a novel realism score based on specific metrics
	3. Validates the approach against established survey data

**Result:** LLM-generated diaries achieved a realism score of 0.485 compared to 0.455 for classical methods, excelling in trip purpose determination and consistency.

**Limitations:** Performance in numerical estimates of trip counts and activity durations is lower compared to classical models.

**Conclusion:** The LLM shows promise for generating realistic travel diaries with a quantifiable realism metric, outperforming traditional methods in some aspects while still needing improvement in trip count and duration estimates.

**Abstract:** This study introduces a Large Language Model (LLM) scheme for generating individual travel diaries in agent-based transportation models. While traditional approaches rely on large quantities of proprietary household travel surveys, the method presented in this study generates personas stochastically from open-source American Community Survey (ACS) and Smart Location Database (SLD) data, then synthesizes diaries through direct prompting. This study features a novel one-to-cohort realism score: a composite of four metrics (Trip Count Score, Interval Score, Purpose Score, and Mode Score) validated against the Connecticut Statewide Transportation Study (CSTS) diaries, matched across demographic variables. The validation utilizes Jensen-Shannon Divergence to measure distributional similarities between generated and real diaries. When compared to diaries generated with classical methods (Negative Binomial for trip generation; Multinomial Logit for mode/purpose) calibrated on the validation set, LLM-generated diaries achieve comparable overall realism (LLM mean: 0.485 vs. 0.455). The LLM excels in determining trip purpose and demonstrates greater consistency (narrower realism score distribution), while classical models lead in numerical estimates of trip count and activity duration. Aggregate validation confirms the LLM's statistical representativeness (LLM mean: 0.612 vs. 0.435), demonstrating LLM's zero-shot viability and establishing a quantifiable metric of diary realism for future synthetic diary evaluation systems.

</details>


### [28] [Psychiatry-Bench: A Multi-Task Benchmark for LLMs in Psychiatry](https://arxiv.org/abs/2509.09711)

*Aya E. Fouda, Abdelrahamn A. Hassan, Radwa J. Hanafy, Mohammed E. Fouda*

**Main category:** cs.CL

**Keywords:** large language models, psychiatry, benchmark, evaluation, mental health

**Relevance Score:** 9

**TL;DR:** This paper presents PsychiatryBench, a benchmark for evaluating large language models (LLMs) in psychiatric practice, highlighting gaps in clinical consistency and safety in multi-turn management tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing evaluation resources for LLMs in psychiatry, which often rely on small or inadequate datasets, thereby lacking clinical validity.

**Method:** Introduction of PsychiatryBench, a benchmark comprising eleven question-answering tasks based on expert-validated psychiatric texts, evaluated using various LLMs and a unique similarity scoring framework.

**Key Contributions:**

	1. Introduction of PsychiatryBench as a new benchmark for LLMs in psychiatry.
	2. Development of a unique 'LLM-as-judge' similarity scoring framework.
	3. Identification of significant performance gaps in existing LLM applications in psychiatric contexts.

**Result:** Evaluation results show significant inconsistencies and safety issues in LLM performance, especially in multi-turn follow-up and management tasks.

**Limitations:** The evaluation may still not cover all aspects of psychiatric reasoning, and results are constrained to the models tested.

**Conclusion:** PsychiatryBench serves as a modular platform for enhancing LLM performance in mental health applications and emphasizes the necessity of specialized tuning and evaluation methodologies.

**Abstract:** Large language models (LLMs) hold great promise in enhancing psychiatric practice, from improving diagnostic accuracy to streamlining clinical documentation and therapeutic support. However, existing evaluation resources heavily rely on small clinical interview corpora, social media posts, or synthetic dialogues, which limits their clinical validity and fails to capture the full complexity of psychiatric reasoning. In this work, we introduce PsychiatryBench, a rigorously curated benchmark grounded exclusively in authoritative, expert-validated psychiatric textbooks and casebooks. PsychiatryBench comprises eleven distinct question-answering tasks ranging from diagnostic reasoning and treatment planning to longitudinal follow-up, management planning, clinical approach, sequential case analysis, and multiple-choice/extended matching formats totaling over 5,300 expert-annotated items. We evaluate a diverse set of frontier LLMs (including Google Gemini, DeepSeek, LLaMA 3, and QWQ-32) alongside leading open-source medical models (e.g., OpenBiloLLM, MedGemma) using both conventional metrics and an "LLM-as-judge" similarity scoring framework. Our results reveal substantial gaps in clinical consistency and safety, particularly in multi-turn follow-up and management tasks, underscoring the need for specialized model tuning and more robust evaluation paradigms. PsychiatryBench offers a modular, extensible platform for benchmarking and improving LLM performance in high-stakes mental health applications.

</details>


### [29] [The Thinking Therapist: Training Large Language Models to Deliver Acceptance and Commitment Therapy using Supervised Fine-Tuning and Odds Ratio Policy Optimization](https://arxiv.org/abs/2509.09712)

*Talha Tahir*

**Main category:** cs.CL

**Keywords:** Acceptance and Commitment Therapy, large language model, training methodology

**Relevance Score:** 9

**TL;DR:** This study evaluates the effectiveness of different training methods for large language models (LLMs) in delivering Acceptance and Commitment Therapy (ACT).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how post-training methodology and explicit reasoning affect LLMs' ability to deliver ACT, which has shown efficacy in psychiatric conditions.

**Method:** The study uses 50 sets of synthetic ACT transcripts to train Llama-3.2-3b-Instruct using two methods: supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), with and without an explicit chain-of-thought (COT) reasoning step. Performance is measured using the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES).

**Key Contributions:**

	1. Demonstrated the superiority of ORPO over SFT for ACT delivery in LLMs
	2. Showed that implicit reasoning techniques can be beneficial under certain training methodologies
	3. Established the importance of training paradigms in shaping LLM performance for therapeutic applications

**Result:** The ORPO-trained models significantly outperformed SFT and standard Instruct models in terms of ACT fidelity and therapeutic empathy, with statistical significance ($p < .001$). The COT reasoning step improved SFT models but did not benefit the ORPO or instruct-tuned variants.

**Limitations:** 

**Conclusion:** Preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and the effectiveness of explicit reasoning is contingent on the training paradigm used.

**Abstract:** Acceptance and Commitment Therapy (ACT) is a third-wave cognitive behavioral therapy with emerging evidence of efficacy in several psychiatric conditions. This study investigates the impact of post-training methodology and explicit reasoning on the ability of a small open-weight large language model (LLM) to deliver ACT. Using 50 sets of synthetic ACT transcripts generated by Mistral-Large, we trained Llama-3.2-3b-Instruct with two distinct approaches, supervised fine-tuning (SFT) and odds ratio policy optimization (ORPO), each with and without an explicit chain-of-thought (COT) reasoning step. Performance was evaluated by comparing these four post-trained variants against the base Instruct model. These models were benchmarked in simulated therapy sessions, with performance quantitatively assessed on the ACT Fidelity Measure (ACT-FM) and the Therapist Empathy Scale (TES) by an LLM judge that had been fine-tuned on human evaluations. Our findings demonstrate that the ORPO-trained models significantly outperformed both their SFT and Instruct counterparts on ACT fidelity ($\chi^2(5) = 185.15, p < .001$) and therapeutic empathy ($\chi^2(5) = 140.37, p < .001$). The effect of COT was conditional as it provided a significant benefit to SFT models, improving ACT-FM scores by an average of 2.68 points ($p < .001$), while offering no discernible advantage to the superior ORPO or instruct-tuned variants. We posit that the superiority of ORPO stems from its ability to learn the therapeutic `process' over imitating `content,' a key aspect of ACT, while COT acts as a necessary scaffold for models trained only via imitation. This study establishes that preference-aligned policy optimization can effectively instill ACT competencies in small LLMs, and that the utility of explicit reasoning is highly dependent on the underlying training paradigm.

</details>


### [30] [Creativity Benchmark: A benchmark for marketing creativity for LLM models](https://arxiv.org/abs/2509.09702)

*Ninad Bhat, Kieran Browne, Pip Bingemann*

**Main category:** cs.CL

**Keywords:** Creativity Benchmark, large language models, human evaluation, marketing, creativity assessment

**Relevance Score:** 8

**TL;DR:** This paper presents the Creativity Benchmark, an evaluation framework for assessing the creativity of large language models in marketing, revealing the limitations of automated judges and the necessity for human evaluation.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the creativity of large language models in marketing and highlight the need for human judgment in creativity assessments.

**Method:** The framework uses human pairwise preferences from 678 creatives to analyze 100 brands across various prompt types, employing Bradley-Terry models for analysis.

**Key Contributions:**

	1. Introduction of Creativity Benchmark for evaluating LLMs in marketing creativity
	2. Extensive human preference analysis reveals model performance consistency
	3. Emphasis on the importance of human evaluator roles over automated systems.

**Result:** The analysis shows no model dominance across brands or prompts and indicates weak correlations between automated judge setups and human rankings, suggesting biases and limitations in automated evaluations.

**Limitations:** Automated judges show weak and inconsistent correlations with human evaluations; conventional creativity tests are only partially applicable to tasks with specific brand constraints.

**Conclusion:** Human evaluation is essential for creativity assessment, and model diversity approaches are necessary to improve evaluation accuracy and reliability.

**Abstract:** We introduce Creativity Benchmark, an evaluation framework for large language models (LLMs) in marketing creativity. The benchmark covers 100 brands (12 categories) and three prompt types (Insights, Ideas, Wild Ideas). Human pairwise preferences from 678 practising creatives over 11,012 anonymised comparisons, analysed with Bradley-Terry models, show tightly clustered performance with no model dominating across brands or prompt types: the top-bottom spread is $\Delta\theta \approx 0.45$, which implies a head-to-head win probability of $0.61$; the highest-rated model beats the lowest only about $61\%$ of the time. We also analyse model diversity using cosine distances to capture intra- and inter-model variation and sensitivity to prompt reframing. Comparing three LLM-as-judge setups with human rankings reveals weak, inconsistent correlations and judge-specific biases, underscoring that automated judges cannot substitute for human evaluation. Conventional creativity tests also transfer only partially to brand-constrained tasks. Overall, the results highlight the need for expert human evaluation and diversity-aware workflows.

</details>


### [31] [HANRAG: Heuristic Accurate Noise-resistant Retrieval-Augmented Generation for Multi-hop Question Answering](https://arxiv.org/abs/2509.09713)

*Duolin Sun, Dan Yang, Yue Shen, Yihan Jiao, Zhehao Tan, Jie Feng, Lianzhen Zhong, Jian Wang, Peng Wei, Jinjie Gu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, large language models, multi-hop queries, query routing, noise filtering

**Relevance Score:** 9

**TL;DR:** Introducing HANRAG, a heuristic-based framework that improves multi-hop query handling in RAG by efficiently routing, decomposing, and filtering noise from queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of RAG in dealing with multi-hop queries and reduce noise accumulation in retrieved content.

**Method:** HANRAG utilizes a revelator to route queries, decompose them into sub-queries, and filter out irrelevant noise from retrieved documents.

**Key Contributions:**

	1. Development of a heuristic-based query routing method
	2. Efficient decomposition of complex queries into manageable sub-queries
	3. Noise filtering mechanism that improves retrieval accuracy

**Result:** HANRAG outperforms existing methods in both single-hop and multi-hop question-answering tasks across various benchmarks.

**Limitations:** 

**Conclusion:** HANRAG enhances adaptability and noise resistance in RAG systems, making it effective for diverse query complexities.

**Abstract:** The Retrieval-Augmented Generation (RAG) approach enhances question-answering systems and dialogue generation tasks by integrating information retrieval (IR) technologies with large language models (LLMs). This strategy, which retrieves information from external knowledge bases to bolster the response capabilities of generative models, has achieved certain successes. However, current RAG methods still face numerous challenges when dealing with multi-hop queries. For instance, some approaches overly rely on iterative retrieval, wasting too many retrieval steps on compound queries. Additionally, using the original complex query for retrieval may fail to capture content relevant to specific sub-queries, resulting in noisy retrieved content. If the noise is not managed, it can lead to the problem of noise accumulation. To address these issues, we introduce HANRAG, a novel heuristic-based framework designed to efficiently tackle problems of varying complexity. Driven by a powerful revelator, HANRAG routes queries, decomposes them into sub-queries, and filters noise from retrieved documents. This enhances the system's adaptability and noise resistance, making it highly capable of handling diverse queries. We compare the proposed framework against other leading industry methods across various benchmarks. The results demonstrate that our framework obtains superior performance in both single-hop and multi-hop question-answering tasks.

</details>


### [32] [How Small Transformation Expose the Weakness of Semantic Similarity Measures](https://arxiv.org/abs/2509.09714)

*Serge Lionel Nikiema, Albérick Euraste Djire, Abdoul Aziz Bonkoungou, Micheline Bénédicte Moumoula, Jordan Samhi, Abdoul Kader Kabore, Jacques Klein, Tegawendé F. Bissyande*

**Main category:** cs.CL

**Keywords:** semantic similarity, software engineering, large language models

**Relevance Score:** 6

**TL;DR:** The study evaluates various methods for measuring semantic similarity in software engineering, revealing significant issues with current metrics, especially for embedding-based methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of different methods for measuring semantic similarity vital for software engineering tasks such as code search and automated reviews.

**Method:** The researchers tested 18 similarity measurement approaches using a systematic framework that applies controlled changes to evaluate their handling of semantic relationships.

**Key Contributions:**

	1. Systematic testing framework for evaluating semantic similarity methods.
	2. Demonstrated significant flaws in embedding-based methods for semantic similarity assessment.
	3. Showed improved performance of LLM-based approaches in discerning semantic differences.

**Result:** The study found high error rates in similarity assessment, particularly with embedding methods mistaking opposites for similarities. LLM-based methods showed improved performance in distinguishing semantic differences.

**Limitations:** 

**Conclusion:** The results suggest a need for better similarity measurement methods, with LLMs outperforming traditional embedding techniques in understanding semantic relationships.

**Abstract:** This research examines how well different methods measure semantic similarity, which is important for various software engineering applications such as code search, API recommendations, automated code reviews, and refactoring tools. While large language models are increasingly used for these similarity assessments, questions remain about whether they truly understand semantic relationships or merely recognize surface patterns.   The study tested 18 different similarity measurement approaches, including word-based methods, embedding techniques, LLM-based systems, and structure-aware algorithms. The researchers created a systematic testing framework that applies controlled changes to text and code to evaluate how well each method handles different types of semantic relationships.   The results revealed significant issues with commonly used metrics. Some embedding-based methods incorrectly identified semantic opposites as similar up to 99.9 percent of the time, while certain transformer-based approaches occasionally rated opposite meanings as more similar than synonymous ones. The study found that embedding methods' poor performance often stemmed from how they calculate distances; switching from Euclidean distance to cosine similarity improved results by 24 to 66 percent. LLM-based approaches performed better at distinguishing semantic differences, producing low similarity scores (0.00 to 0.29) for genuinely different meanings, compared to embedding methods that incorrectly assigned high scores (0.82 to 0.99) to dissimilar content.

</details>


### [33] [Investigating Symbolic Triggers of Hallucination in Gemma Models Across HaluEval and TruthfulQA](https://arxiv.org/abs/2509.09715)

*Naveen Lamba, Sanju Tiwari, Manas Gaur*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, symbolic properties, Gemma models, HCI

**Relevance Score:** 8

**TL;DR:** This study investigates the vulnerabilities in Large Language Models (LLMs) related to hallucinations, identifying key properties that contribute to this issue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To identify intrinsic vulnerabilities of LLMs that lead to hallucinations and assess how these vulnerabilities manifest across different model scales.

**Method:** The research utilized the HaluEval and TruthfulQA datasets, converting their formats to analyze hallucination properties in LLMs through various tasks.

**Key Contributions:**

	1. Identification of properties causing hallucinations in LLMs
	2. Quantitative analysis of hallucination rates across different model scales
	3. Highlighting weaknesses in LLM processing of symbolic elements

**Result:** Hallucination percentages are significantly high in Gemma models, averaging 79.0% across tasks with a reduction observed as model scale increases, yet substantial hallucinations remain tied to symbolic properties.

**Limitations:** Focus primarily on symbolic properties without assessing other potential factors contributing to hallucinations.

**Conclusion:** Despite larger models reducing hallucination rates, they still exhibit significant vulnerabilities due to symbolic properties, indicating a need for further exploration and improvement in LLM processing of such inputs.

**Abstract:** Hallucination in Large Language Models (LLMs) is a well studied problem. However, the properties that make LLM intrinsically vulnerable to hallucinations have not been identified and studied. This research identifies and characterizes the key properties, allowing us to pinpoint vulnerabilities within the model's internal mechanisms. To solidify on these properties, we utilized two established datasets, HaluEval and TruthfulQA and convert their existing format of question answering into various other formats to narrow down these properties as the reason for the hallucinations. Our findings reveal that hallucination percentages across symbolic properties are notably high for Gemma-2-2B, averaging 79.0% across tasks and datasets. With increased model scale, hallucination drops to 73.6% for Gemma-2-9B and 63.9% for Gemma-2-27B, reflecting a 15 percentage point reduction overall. Although the hallucination rate decreases as the model size increases, a substantial amount of hallucination caused by symbolic properties still persists. This is especially evident for modifiers (ranging from 84.76% to 94.98%) and named entities (ranging from 83.87% to 93.96%) across all Gemma models and both datasets. These findings indicate that symbolic elements continue to confuse the models, pointing to a fundamental weakness in how these LLMs process such inputs--regardless of their scale.

</details>


### [34] [ALIGNS: Unlocking nomological networks in psychological measurement through a large language model](https://arxiv.org/abs/2509.09723)

*Kai R. Larsen, Sen Yan, Roland Müller, Lan Sang, Mikko Rönkkö, Ravi Starzl, Donald Edmondson*

**Main category:** cs.CL

**Keywords:** psychological measurement, nomological networks, large language models, validation methods, health informatics

**Relevance Score:** 8

**TL;DR:** ALIGNS is a large language model-based system for creating nomological networks in psychological measurement, aimed at improving validation methods across various fields.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a longstanding challenge in building nomological networks to establish the validity of psychological measurements, which can lead to significant consequences in clinical trials and public policy.

**Method:** The system leverages large language models trained on validated questionnaire measures to generate comprehensive nomological networks containing over 550,000 indicators across various disciplines.

**Key Contributions:**

	1. Introduction of ALIGNS, the first application of large language models for nomological networks
	2. Creation of three detailed nomological networks across multiple disciplines
	3. Successful evaluations indicate the model's capacity to identify dimensions not accounted for by existing frameworks.

**Result:** ALIGNS demonstrated successful convergence of the NIH PROMIS anxiety and depression instruments into one dimension of emotional distress, identified new dimensions in child temperament measures, and received positive feedback from expert psychometricians on its accessibility and importance.

**Limitations:** 

**Conclusion:** ALIGNS offers a novel approach to measurement validation by complementing traditional methods with extensive nomological analysis, and is freely available for use.

**Abstract:** Psychological measurement is critical to many disciplines. Despite advances in measurement, building nomological networks, theoretical maps of how concepts and measures relate to establish validity, remains a challenge 70 years after Cronbach and Meehl proposed them as fundamental to validation. This limitation has practical consequences: clinical trials may fail to detect treatment effects, and public policy may target the wrong outcomes. We introduce Analysis of Latent Indicators to Generate Nomological Structures (ALIGNS), a large language model-based system trained with validated questionnaire measures. ALIGNS provides three comprehensive nomological networks containing over 550,000 indicators across psychology, medicine, social policy, and other fields. This represents the first application of large language models to solve a foundational problem in measurement validation. We report classification accuracy tests used to develop the model, as well as three evaluations. In the first evaluation, the widely used NIH PROMIS anxiety and depression instruments are shown to converge into a single dimension of emotional distress. The second evaluation examines child temperament measures and identifies four potential dimensions not captured by current frameworks, and questions one existing dimension. The third evaluation, an applicability check, engages expert psychometricians who assess the system's importance, accessibility, and suitability. ALIGNS is freely available at nomologicalnetwork.org, complementing traditional validation methods with large-scale nomological analysis.

</details>


### [35] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)

*Adnan Ahmad, Philine Kowol, Stefan Hillmann, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** multi-label intent classification, Large Language Models, Natural Language Understanding

**Relevance Score:** 9

**TL;DR:** This paper analyzes the effectiveness of open-source Large Language Models for multi-label intent classification using the MultiWOZ 2.1 dataset, highlighting differences in performance metrics among the models and comparing them to a BERT-based supervised classifier.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of open-source pre-trained LLMs for multi-label intent classification in dialogue systems.

**Method:** Three open-source LLMs (LLama2-7B-hf, Mistral-7B-v0.1, Yi-6B) were evaluated in a few-shot classification setup on the MultiWOZ 2.1 dataset, with comparison to a BERT-based supervised learning model.

**Key Contributions:**

	1. Extensive analysis of open-source LLMs for intent classification
	2. Comparison of few-shot LLM performance to BERT-based supervised learning
	3. Framework for enhancing Natural Language Understanding in task-oriented chatbots

**Result:** Mistral-7B-v0.1 outperformed other models on 11 out of 14 intent classes, while the BERT classifier showed superior overall performance compared to the best-performing generative LLM in the few-shot setting.

**Limitations:** 

**Conclusion:** The study lays the groundwork for using small open-source LLMs in complex multi-intent dialogue detection, improving Natural Language Understanding in chatbots.

**Abstract:** In this paper, we provide an extensive analysis of multi-label intent classification using Large Language Models (LLMs) that are open-source, publicly available, and can be run in consumer hardware. We use the MultiWOZ 2.1 dataset, a benchmark in the dialogue system domain, to investigate the efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf, Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot setup, giving 20 examples in the prompt with some instructions. Our approach focuses on the differences in performance of these models across several performance metrics by methodically assessing these models on multi-label intent classification tasks. Additionally, we compare the performance of the instruction-based fine-tuning approach with supervised learning using the smaller transformer model BertForSequenceClassification as a baseline. To evaluate the performance of the models, we use evaluation metrics like accuracy, precision, and recall as well as micro, macro, and weighted F1 score. We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1 outperforms two other generative models on 11 intent classes out of 14 in terms of F-Score, with a weighted average of 0.50. It also has relatively lower Humming Loss and higher Jaccard Similarity, making it the winning model in the few-shot setting. We find BERT based supervised classifier having superior performance compared to the best performing few-shot generative LLM. The study provides a framework for small open-source LLMs in detecting complex multi-intent dialogues, enhancing the Natural Language Understanding aspect of task-oriented chatbots.

</details>


### [36] [DiTTO-LLM: Framework for Discovering Topic-based Technology Opportunities via Large Language Model](https://arxiv.org/abs/2509.09724)

*Wonyoung Kim, Sujeong Seo, Juhyun Lee*

**Main category:** cs.CL

**Keywords:** technology opportunities, patent dataset, large language model, artificial intelligence, temporal relationships

**Relevance Score:** 6

**TL;DR:** The paper presents a framework for identifying emerging technology opportunities through the analysis of patent data and temporal relationships between technological topics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a systematic approach for identifying technology opportunities using temporal relationships, enhancing innovation in various fields.

**Method:** The framework involves extracting text from a patent dataset, mapping topics to discover inter-technology relationships, and tracking changes over time. It utilizes a large language model for topic extraction and a chat-based model for opportunity discovery.

**Key Contributions:**

	1. Development of a novel framework for identifying emerging technology opportunities
	2. Utilization of LLMs for topic extraction and technology opportunity discovery
	3. Demonstration of the relationship between inter-technology topics and emerging opportunities

**Result:** The framework was evaluated with a dataset of artificial intelligence patents, revealing that AI technologies are evolving to become more accessible in everyday contexts.

**Limitations:** 

**Conclusion:** The study illustrates the framework's effectiveness in uncovering future technology opportunities, emphasizing its relevance in a rapidly changing technological landscape.

**Abstract:** Technology opportunities are critical information that serve as a foundation for advancements in technology, industry, and innovation. This paper proposes a framework based on the temporal relationships between technologies to identify emerging technology opportunities. The proposed framework begins by extracting text from a patent dataset, followed by mapping text-based topics to discover inter-technology relationships. Technology opportunities are then identified by tracking changes in these topics over time. To enhance efficiency, the framework leverages a large language model to extract topics and employs a prompt for a chat-based language model to support the discovery of technology opportunities. The framework was evaluated using an artificial intelligence patent dataset provided by the United States Patent and Trademark Office. The experimental results suggest that artificial intelligence technology is evolving into forms that facilitate everyday accessibility. This approach demonstrates the potential of the proposed framework to identify future technology opportunities.

</details>


### [37] [BIBERT-Pipe on Biomedical Nested Named Entity Linking at BioASQ 2025](https://arxiv.org/abs/2509.09725)

*Chunyu Li, Xindi Zheng, Siqi Liu*

**Main category:** cs.CL

**Keywords:** Entity Linking, Biomedical Text, Multilingual, Nested Mentions, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** A novel approach for nested and multilingual entity linking in biomedical texts, ranking third in the BioNNE 2025 competition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing entity linking methods that primarily focus on English-only and flat mention corpora, thereby ignoring nested and multilingual mentions that are more representative of real-world scenarios.

**Method:** The proposed system, called bilingual bert (BIBERT-Pipe), utilizes a two-stage retrieval-ranking approach, where the retrieval stage employs a pre-trained model and the ranking stage applies domain-specific fine-tuning. Additionally, it incorporates boundary cues with learnable tags and augments the dataset by expanding the ranking training corpus with three complementary data sources.

**Key Contributions:**

	1. Introduction of a lightweight pipeline for multilingual biomedical nested entity linking.
	2. Implementation of a two-stage retrieval-ranking methodology that maximizes the use of pre-trained models.
	3. Augmentation of training datasets that reduces the need for manual annotation while improving model performance.

**Result:** The BIBERT-Pipe system achieved third place in the multilingual track of the BioNNE 2025 leaderboard, indicating its effectiveness in handling nested and multilingual entity linking.

**Limitations:** 

**Conclusion:** These minimal yet principled modifications to the entity linking task demonstrate significant advancements in the handling of nested and multilingual mentions in biomedical texts.

**Abstract:** Entity linking (EL) for biomedical text is typically benchmarked on English-only corpora with flat mentions, leaving the more realistic scenario of nested and multilingual mentions largely unexplored. We present our system for the BioNNE 2025 Multilingual Biomedical Nested Named Entity Linking shared task (English & Russian), closing this gap with a lightweight pipeline that keeps the original EL model intact and modifies only three task-aligned components: Two-stage retrieval-ranking. We leverage the same base encoder model in both stages: the retrieval stage uses the original pre-trained model, while the ranking stage applies domain-specific fine-tuning. Boundary cues. In the ranking stage, we wrap each mention with learnable [Ms] / [Me] tags, providing the encoder with an explicit, language-agnostic span before robustness to overlap and nesting. Dataset augmentation. We also automatically expand the ranking training corpus with three complementary data sources, enhancing coverage without extra manual annotation. On the BioNNE 2025 leaderboard, our two stage system, bilingual bert (BIBERT-Pipe), ranks third in the multilingual track, demonstrating the effectiveness and competitiveness of these minimal yet principled modifications. Code are publicly available at https://github.com/Kaggle-Competitions-Code/BioNNE-L.

</details>


### [38] [Natural Language Translation of Formal Proofs through Informalization of Proof Steps and Recursive Summarization along Proof Structure](https://arxiv.org/abs/2509.09726)

*Seiji Hattori, Takuya Matsuzaki, Makoto Fujiwara*

**Main category:** cs.CL

**Keywords:** natural language processing, formal proofs, machine-learning

**Relevance Score:** 6

**TL;DR:** The paper presents a method for translating machine-verifiable formal proofs into natural language using LLMs, demonstrating its effectiveness with formal proof libraries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance accessibility and readability of formal proofs by translating them into natural language, making them verifiable by machines while still comprehensible to humans.

**Method:** The proposed method formalizes informal language proof steps and utilizes LLMs for translation and summarization of formal proofs.

**Key Contributions:**

	1. Introduction of a method utilizing LLMs for translating formal proofs to natural language
	2. Demonstration of high accuracy and readability in generated proofs
	3. Application of the method on formal proof libraries such as Lean to validate its effectiveness.

**Result:** The generated natural language proofs were found to be highly readable and accurate when compared to original natural language proofs.

**Limitations:** 

**Conclusion:** The method shows promise in transforming formal proofs into formats that are both machine-readable and human-friendly, indicating a significant advancement in the usability of formal proofs.

**Abstract:** This paper proposes a natural language translation method for machine-verifiable formal proofs that leverages the informalization (verbalization of formal language proof steps) and summarization capabilities of LLMs. For evaluation, it was applied to formal proof data created in accordance with natural language proofs taken from an undergraduate-level textbook, and the quality of the generated natural language proofs was analyzed in comparison with the original natural language proofs. Furthermore, we will demonstrate that this method can output highly readable and accurate natural language proofs by applying it to existing formal proof library of the Lean proof assistant.

</details>


### [39] [A Role-Aware Multi-Agent Framework for Financial Education Question Answering with LLMs](https://arxiv.org/abs/2509.09727)

*Andy Zhu, Yingjun Du*

**Main category:** cs.CL

**Keywords:** question answering, financial education, large language models, multi-agent systems, retrieval-augmented generation

**Relevance Score:** 8

**TL;DR:** This paper presents a multi-agent framework to improve financial question answering by employing role-based prompting and a combination of retrieval-augmented generation techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved financial question answering systems that can handle complex reasoning and domain-specific language.

**Method:** A multi-agent framework consisting of a Base Generator, an Evidence Retriever, and an Expert Reviewer to produce more accurate answers to finance-related questions.

**Key Contributions:**

	1. Introduction of a multi-agent framework for financial QA
	2. Demonstration of significant improvement in answer accuracy
	3. Insights for future multi-agent systems in domain-specific LLM applications.

**Result:** The proposed framework improved answer accuracy by 6.6-8.3% over existing methods, with models like Gemini-2.0-Flash performing particularly well.

**Limitations:** 

**Conclusion:** The framework offers a cost-effective approach to enhance financial question answering capabilities and suggests directions for future research in multi-agent systems for finance.

**Abstract:** Question answering (QA) plays a central role in financial education, yet existing large language model (LLM) approaches often fail to capture the nuanced and specialized reasoning required for financial problem-solving. The financial domain demands multistep quantitative reasoning, familiarity with domain-specific terminology, and comprehension of real-world scenarios. We present a multi-agent framework that leverages role-based prompting to enhance performance on domain-specific QA. Our framework comprises a Base Generator, an Evidence Retriever, and an Expert Reviewer agent that work in a single-pass iteration to produce a refined answer. We evaluated our framework on a set of 3,532 expert-designed finance education questions from Study.com, an online learning platform. We leverage retrieval-augmented generation (RAG) for contextual evidence from 6 finance textbooks and prompting strategies for a domain-expert reviewer. Our experiments indicate that critique-based refinement improves answer accuracy by 6.6-8.3% over zero-shot Chain-of-Thought baselines, with the highest performance from Gemini-2.0-Flash. Furthermore, our method enables GPT-4o-mini to achieve performance comparable to the finance-tuned FinGPT-mt_Llama3-8B_LoRA. Our results show a cost-effective approach to enhancing financial QA and offer insights for further research in multi-agent financial LLM systems.

</details>


### [40] [A meta-analysis on the performance of machine-learning based language models for sentiment analysis](https://arxiv.org/abs/2509.09728)

*Elena Rohde, Jonas Klingwort, Christian Borgs*

**Main category:** cs.CL

**Keywords:** Machine Learning, Sentiment Analysis, Twitter Data, Meta-Analysis, Performance Metrics

**Relevance Score:** 6

**TL;DR:** This meta-analysis evaluates ML performance in sentiment analysis on Twitter data, revealing that overall accuracy can be misleading and calling for better reporting practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To estimate average performance in sentiment analysis for Twitter data and assess how study characteristics influence ML model performance.

**Method:** A meta-analysis using PRISMA guidelines, selecting 195 trials from 20 studies, and applying a three-level random effects model to analyze accuracy metrics.

**Key Contributions:**

	1. Identified misleading nature of overall accuracy in sentiment analysis due to class imbalance.
	2. Emphasized the need for standardized reporting of model performance metrics.

**Result:** The average accuracy of the AIC-optimized model was found to be 0.80, but the study highlights significant issues with overall accuracy as a performance measure.

**Limitations:** The analysis is restricted to studies included in the meta-analysis, which may limit generalizability.

**Conclusion:** The study underscores the importance of standardized reporting and normalization of model performance metrics to enable reliable comparisons across studies.

**Abstract:** This paper presents a meta-analysis evaluating ML performance in sentiment analysis for Twitter data. The study aims to estimate the average performance, assess heterogeneity between and within studies, and analyze how study characteristics influence model performance. Using PRISMA guidelines, we searched academic databases and selected 195 trials from 20 studies with 12 study features. Overall accuracy, the most reported performance metric, was analyzed using double arcsine transformation and a three-level random effects model. The average overall accuracy of the AIC-optimized model was 0.80 [0.76, 0.84]. This paper provides two key insights: 1) Overall accuracy is widely used but often misleading due to its sensitivity to class imbalance and the number of sentiment classes, highlighting the need for normalization. 2) Standardized reporting of model performance, including reporting confusion matrices for independent test sets, is essential for reliable comparisons of ML classifiers across studies, which seems far from common practice.

</details>


### [41] [MultimodalHugs: Enabling Sign Language Processing in Hugging Face](https://arxiv.org/abs/2509.09729)

*Gerard Sant, Zifan Jiang, Carlos Escolano, Amit Moryossef, Mathias Müller, Rico Sennrich, Sarah Ebling*

**Main category:** cs.CL

**Keywords:** sign language processing, Hugging Face, multimodal framework

**Relevance Score:** 4

**TL;DR:** This paper presents MultimodalHugs, a framework designed to improve the reproducibility and flexibility of sign language processing experiments by building on the Hugging Face ecosystem.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for better tools in sign language processing research, which currently suffers from low reproducibility and difficulties in conducting experiments compared to spoken language processing.

**Method:** MultimodalHugs is introduced as a framework that allows for diverse data modalities and tasks, offering a layer of abstraction over the Hugging Face platform.

**Key Contributions:**

	1. Introduction of a new framework for sign language processing
	2. Integration with the Hugging Face ecosystem
	3. Support for diverse data modalities beyond standard templates.

**Result:** Quantitative experiments show that MultimodalHugs successfully accommodates various modalities such as pose estimation data for sign languages and pixel data for text characters.

**Limitations:** 

**Conclusion:** MultimodalHugs enhances the flexibility and reproducibility of sign language processing experiments while being applicable to other modalities.

**Abstract:** In recent years, sign language processing (SLP) has gained importance in the general field of Natural Language Processing. However, compared to research on spoken languages, SLP research is hindered by complex ad-hoc code, inadvertently leading to low reproducibility and unfair comparisons. Existing tools that are built for fast and reproducible experimentation, such as Hugging Face, are not flexible enough to seamlessly integrate sign language experiments. This view is confirmed by a survey we conducted among SLP researchers.   To address these challenges, we introduce MultimodalHugs, a framework built on top of Hugging Face that enables more diverse data modalities and tasks, while inheriting the well-known advantages of the Hugging Face ecosystem. Even though sign languages are our primary focus, MultimodalHugs adds a layer of abstraction that makes it more widely applicable to other use cases that do not fit one of the standard templates of Hugging Face. We provide quantitative experiments to illustrate how MultimodalHugs can accommodate diverse modalities such as pose estimation data for sign languages, or pixel data for text characters.

</details>


### [42] [Benchmarking Vision-Language Models on Chinese Ancient Documents: From OCR to Knowledge Reasoning](https://arxiv.org/abs/2509.09731)

*Haiyang Yu, Yuchuan Wu, Fan Shi, Lei Liao, Jinghui Lu, Xiaodong Ge, Han Wang, Minghan Zhuo, Xuecheng Wu, Xiang Fei, Hao Feng, Guozhi Tang, An-Lan Wang, Hanshen Zhu, Yangfan He, Quanhuan Liang, Liyuan Meng, Chao Feng, Can Huang, Jingqun Tang, Bin Li*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Chinese ancient documents, OCR, knowledge reasoning, benchmark evaluation

**Relevance Score:** 6

**TL;DR:** AncientDoc is the first benchmark for evaluating Vision-Language Models on Chinese ancient documents, addressing challenges in digitization and understanding.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Chinese ancient documents are rich in historical and cultural knowledge but face challenges in digitization and comprehension due to their complexity and the limitations of existing models.

**Method:** The benchmark includes five tasks: page-level OCR, vernacular translation, reasoning-based QA, knowledge-based QA, and linguistic variant QA, covering diverse document types and evaluating mainstream VLMs.

**Key Contributions:**

	1. Introduction of the AncientDoc benchmark for Chinese ancient documents
	2. Evaluation of VLMs on multiple tasks relevant to ancient texts
	3. Identification of performance gaps in current models when applied to complex ancient document structures.

**Result:** The evaluation reveals the performance of various VLMs on ancient Chinese documents, highlighting gaps and opportunities for improvement across different tasks.

**Limitations:** 

**Conclusion:** AncientDoc provides a framework for better understanding and processing of ancient Chinese documents through advanced VLMs, promoting further research in this area.

**Abstract:** Chinese ancient documents, invaluable carriers of millennia of Chinese history and culture, hold rich knowledge across diverse fields but face challenges in digitization and understanding, i.e., traditional methods only scan images, while current Vision-Language Models (VLMs) struggle with their visual and linguistic complexity. Existing document benchmarks focus on English printed texts or simplified Chinese, leaving a gap for evaluating VLMs on ancient Chinese documents. To address this, we present AncientDoc, the first benchmark for Chinese ancient documents, designed to assess VLMs from OCR to knowledge reasoning. AncientDoc includes five tasks (page-level OCR, vernacular translation, reasoning-based QA, knowledge-based QA, linguistic variant QA) and covers 14 document types, over 100 books, and about 3,000 pages. Based on AncientDoc, we evaluate mainstream VLMs using multiple metrics, supplemented by a human-aligned large language model for scoring.

</details>


### [43] [MCP-AgentBench: Evaluating Real-World Language Agent Performance with MCP-Mediated Tools](https://arxiv.org/abs/2509.09734)

*Zikang Guo, Benfeng Xu, Chiwei Zhu, Wentao Hong, Xiaorui Wang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** Model Context Protocol, agent-tool integration, language agents, benchmarks, interoperability

**Relevance Score:** 6

**TL;DR:** The paper introduces MCP-AgentBench, a robust benchmark for assessing language agent capabilities in MCP-mediated tool interactions, addressing gaps in current agent performance evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance agent-tool integration and interoperability using the Model Context Protocol (MCP) and to provide a reliable evaluation framework for language agents.

**Method:** MCP-AgentBench features a testbed with 33 servers and 188 tools, along with 600 queries across 6 complexity categories, and employs MCP-Eval for evaluation.

**Key Contributions:**

	1. Establishment of a testbed with 33 operational servers and 188 tools.
	2. Development of a benchmark with 600 queries across 6 categories of interaction complexity.
	3. Introduction of MCP-Eval for outcome-oriented evaluation.

**Result:** Empirical evaluation of leading language agents revealed foundational insights into their performance and capabilities within the MCP context.

**Limitations:** 

**Conclusion:** MCP-AgentBench aims to standardize assessments in the research community, facilitating advancements in interoperable AI systems leveraging MCP.

**Abstract:** The Model Context Protocol (MCP) is rapidly emerging as a pivotal open standard, designed to enhance agent-tool integration and interoperability, and is positioned to unlock a new era of powerful, interconnected, and genuinely utilitarian agentic AI. However, despite MCP's growing adoption, existing benchmarks often fail to capture real-world agent performance within this new paradigm, leading to a distorted perception of their true operational value and an inability to reliably differentiate proficiencies. To bridge this critical evaluation gap, we introduce MCP-AgentBench -- a comprehensive benchmark specifically engineered to rigorously assess language agent capabilities in MCP-mediated tool interactions. Core contributions of MCP-AgentBench include: the establishment of a robust MCP testbed comprising 33 operational servers with 188 distinct tools; the development of a benchmark featuring 600 systematically designed queries distributed across 6 distinct categories of varying interaction complexity; and the introduction of MCP-Eval, a novel outcome-oriented evaluation methodology prioritizing real-world task success. Through extensive empirical evaluation of leading language agents, we provide foundational insights. MCP-AgentBench aims to equip the research community with a standardized and reliable framework to build, validate, and advance agents capable of fully leveraging MCP's transformative benefits, thereby accelerating progress toward truly capable and interoperable AI systems.

</details>


### [44] [Discrimination by LLMs: Cross-lingual Bias Assessment and Mitigation in Decision-Making and Summarisation](https://arxiv.org/abs/2509.09735)

*Willem Huijzer, Jieying Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias, decision-making, summarization, mitigation strategies

**Relevance Score:** 9

**TL;DR:** This study investigates biases in Large Language Models (LLMs) related to demographics, focusing on their impact on decision-making and summarization tasks, while evaluating mitigation strategies.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address societal inequalities and information bias arising from the integration of LLMs in various domains.

**Method:** The study created a large dataset of prompts (151,200 for decision tasks and 176,400 for summarization tasks) to test bias in GPT-3.5 and GPT-4o across different demographic variables and languages.

**Key Contributions:**

	1. Examination of biases in LLMs related to demographics.
	2. Cross-lingual analysis revealing similar bias patterns in English and Dutch.
	3. Empirical evidence showing effectiveness of prompt-instructed mitigation strategies.

**Result:** Both models exhibited significant biases in decision-making favoring certain demographics, with minimal bias in summarization tasks; prompt-based mitigation showed potential to reduce biases.

**Limitations:** The proposed mitigation instructions did not eliminate biases completely and effectiveness varied across tasks and demographic categories.

**Conclusion:** Cautious adoption of LLMs is necessary, with context-specific bias testing and development of mitigation strategies being critical for responsible AI deployment.

**Abstract:** The rapid integration of Large Language Models (LLMs) into various domains raises concerns about societal inequalities and information bias. This study examines biases in LLMs related to background, gender, and age, with a focus on their impact on decision-making and summarization tasks. Additionally, the research examines the cross-lingual propagation of these biases and evaluates the effectiveness of prompt-instructed mitigation strategies. Using an adapted version of the dataset by Tamkin et al. (2023) translated into Dutch, we created 151,200 unique prompts for the decision task and 176,400 for the summarisation task. Various demographic variables, instructions, salience levels, and languages were tested on GPT-3.5 and GPT-4o. Our analysis revealed that both models were significantly biased during decision-making, favouring female gender, younger ages, and certain backgrounds such as the African-American background. In contrast, the summarisation task showed minimal evidence of bias, though significant age-related differences emerged for GPT-3.5 in English. Cross-lingual analysis showed that bias patterns were broadly similar between English and Dutch, though notable differences were observed across specific demographic categories. The newly proposed mitigation instructions, while unable to eliminate biases completely, demonstrated potential in reducing them. The most effective instruction achieved a 27\% mean reduction in the gap between the most and least favorable demographics. Notably, contrary to GPT-3.5, GPT-4o displayed reduced biases for all prompts in English, indicating the specific potential for prompt-based mitigation within newer models. This research underscores the importance of cautious adoption of LLMs and context-specific bias testing, highlighting the need for continued development of effective mitigation strategies to ensure responsible deployment of AI.

</details>


### [45] [HEFT: A Coarse-to-Fine Hierarchy for Enhancing the Efficiency and Accuracy of Language Model Reasoning](https://arxiv.org/abs/2509.09801)

*Brennen Hill*

**Main category:** cs.CL

**Keywords:** large language models, parameter-efficient fine-tuning, reasoning tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces HEFT, a novel hierarchical fine-tuning strategy that combines Parameter-Efficient Fine-Tuning methods to enhance LLM performance on specialized reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) face computational constraints for specialized reasoning tasks, necessitating effective fine-tuning methods.

**Method:** HEFT integrates two PEFT methods: Low-Rank Adaptation (LoRA) for broad foundational adaptation in weight space, followed by Representation Fine-Tuning (ReFT) for precision refinement of internal activations.

**Key Contributions:**

	1. Introduction of the HEFT method for fine-tuning LLMs
	2. Demonstration of superior performance through method synergy
	3. Evidence supporting efficient adaptation strategies for large-scale models

**Result:** Using HEFT on the Llama-2-7B model, the paper reports an accuracy of 85.17% on the BoolQ benchmark after just three epochs, outperforming models fine-tuned with LoRA (85.05%) or ReFT (83.36%) alone.

**Limitations:** 

**Conclusion:** The study indicates that combining PEFT methods can significantly enhance the reasoning capabilities of language models while requiring less computational resources.

**Abstract:** The adaptation of large language models (LLMs) to specialized reasoning tasks is fundamentally constrained by computational resources. Parameter-Efficient Fine-Tuning (PEFT) methods have emerged as a powerful solution, yet the landscape of these techniques is diverse, with distinct methods operating in either the model's weight space or its representation space. This paper investigates the hypothesis that a synergistic combination of these paradigms can unlock superior performance and efficiency. We introduce HEFT (Hierarchical Efficient Fine-Tuning), a novel hierarchical adaptation strategy that composes two distinct PEFT methods in a coarse-to-fine manner: first, a broad, foundational adaptation in the weight space using Low-Rank Adaptation (LoRA), followed by a precise, surgical refinement of internal activations using Representation Fine-Tuning (ReFT). We evaluate this approach by fine-tuning a Llama-2-7B model on the BoolQ benchmark, a challenging dataset for inferential reasoning. Our results reveal a profound synergistic effect. A model fine-tuned for only three epochs with our HEFT strategy achieves an accuracy of 85.17\%, exceeding the performance of models trained for 20 epochs with either LoRA-only (85.05\%) or ReFT-only (83.36\%) methodologies. This work demonstrates that the thoughtful composition of PEFT methods is a potent algorithmic innovation, offering a more efficient and effective path toward advancing the reasoning capabilities of language models. By achieving superior results with a fraction of the computational budget, our findings present a principled approach to overcoming the obstacles inherent in adapting large-scale models for complex cognitive tasks.

</details>


### [46] [Pragmatic Frames Evoked by Gestures: A FrameNet Brasil Approach to Multimodality in Turn Organization](https://arxiv.org/abs/2509.09804)

*Helen de Andrade Abreu, Tiago Timponi Torrent, Ely Edison da Silva Matos*

**Main category:** cs.CL

**Keywords:** multimodal, conversational turn organization, pragmatic frames, gestures, machine learning

**Relevance Score:** 6

**TL;DR:** The paper presents a framework for modeling multimodal conversational turn organization by linking language and gestures, enriching the Frame2 dataset with annotations of gestures used in conversation, revealing new insights into communicative strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in machine learning datasets regarding the role of gestures in conversational turn organization.

**Method:** The authors developed an annotation methodology to enrich a multimodal dataset (Frame2) with pragmatic frames related to conversational turns and analyzed 10 episodes from a Brazilian TV series for this purpose.

**Key Contributions:**

	1. Proposed a framework linking gestures and language for turn organization
	2. Enriched the Frame2 dataset with pragmatic frame annotations
	3. Identified new variations of gestures used in conversation.

**Result:** The enriched dataset revealed that communicators use gestures to manage conversational turns and identified previously undocumented variations of these gestures.

**Limitations:** 

**Conclusion:** The research enhances the understanding of human cognition and language through the study of pragmatic frames and gestures in conversation.

**Abstract:** This paper proposes a framework for modeling multimodal conversational turn organization via the proposition of correlations between language and interactive gestures, based on analysis as to how pragmatic frames are conceptualized and evoked by communicators. As a means to provide evidence for the analysis, we developed an annotation methodology to enrich a multimodal dataset (annotated for semantic frames) with pragmatic frames modeling conversational turn organization. Although conversational turn organization has been studied by researchers from diverse fields, the specific strategies, especially gestures used by communicators, had not yet been encoded in a dataset that can be used for machine learning. To fill this gap, we enriched the Frame2 dataset with annotations of gestures used for turn organization. The Frame2 dataset features 10 episodes from the Brazilian TV series Pedro Pelo Mundo annotated for semantic frames evoked in both video and text. This dataset allowed us to closely observe how communicators use interactive gestures outside a laboratory, in settings, to our knowledge, not previously recorded in related literature. Our results have confirmed that communicators involved in face-to-face conversation make use of gestures as a tool for passing, taking and keeping conversational turns, and also revealed variations of some gestures that had not been documented before. We propose that the use of these gestures arises from the conceptualization of pragmatic frames, involving mental spaces, blending and conceptual metaphors. In addition, our data demonstrate that the annotation of pragmatic frames contributes to a deeper understanding of human cognition and language.

</details>


### [47] [Topic-Guided Reinforcement Learning with LLMs for Enhancing Multi-Document Summarization](https://arxiv.org/abs/2509.09852)

*Chuyuan Li, Austin Xu, Shafiq Joty, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** Multi-Document Summarization, Reinforcement Learning, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper introduces a topic-guided reinforcement learning approach to enhance Multi-Document Summarization (MDS) by improving content selection and coherence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of integrating information from multiple sources in Multi-Document Summarization while maintaining coherence and relevance.

**Method:** A topic-guided reinforcement learning method using Group Relative Policy Optimization (GRPO) with a novel topic reward to align generated summaries with source documents.

**Key Contributions:**

	1. Introduces a topic-guided framework for MDS
	2. Develops a novel topic reward mechanism
	3. Demonstrates improved summarization performance on benchmark datasets.

**Result:** Experimental results indicate that the proposed method outperforms strong baselines on the Multi-News and Multi-XScience datasets.

**Limitations:** 

**Conclusion:** Leveraging topical cues significantly enhances the informativeness and coherence of generated summaries in MDS.

**Abstract:** A key challenge in Multi-Document Summarization (MDS) is effectively integrating information from multiple sources while maintaining coherence and topical relevance. While Large Language Models have shown impressive results in single-document summarization, their performance on MDS still leaves room for improvement. In this paper, we propose a topic-guided reinforcement learning approach to improve content selection in MDS. We first show that explicitly prompting models with topic labels enhances the informativeness of the generated summaries. Building on this insight, we propose a novel topic reward within the Group Relative Policy Optimization (GRPO) framework to measure topic alignment between the generated summary and source documents. Experimental results on the Multi-News and Multi-XScience datasets demonstrate that our method consistently outperforms strong baselines, highlighting the effectiveness of leveraging topical cues in MDS.

</details>


### [48] [Emulating Public Opinion: A Proof-of-Concept of AI-Generated Synthetic Survey Responses for the Chilean Case](https://arxiv.org/abs/2509.09871)

*Bastián González-Bustamante, Nando Verelst, Carla Cisternas*

**Main category:** cs.CL

**Keywords:** Large Language Models, Synthetic Survey Responses, Bias in Survey Research

**Relevance Score:** 8

**TL;DR:** This study evaluates the reliability of LLM-generated synthetic survey responses compared to human responses in a probabilistic survey setting, highlighting the strengths and limitations of using LLMs in survey research.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of LLMs in improving survey research by generating synthetic respondents that emulate human behavior while addressing measurement errors and biases.

**Method:** The study benchmarks 128 prompt-model-question triplets using OpenAI's GPT family and other models, generating 189,696 synthetic profiles and analyzing accuracy, precision, recall, and F1-score in a meta-analysis.

**Key Contributions:**

	1. Evaluated the performance of various LLMs in generating synthetic survey responses.
	2. Demonstrated the strong alignment of synthetic responses with true human responses in specific survey items.
	3. Identified the demographic nuances in response accuracy based on respondent age.

**Result:** LLM-generated synthetic responses performed well, especially on trust items (F1-score > 0.90), with notable performance from GPT-4o models and Llama 4 Maverick. Synthetic responses were closest to human responses among older respondents (aged 45-59).

**Limitations:** The study acknowledges the challenges in capturing the full nuance of public opinion and the need for additional distributional tests.

**Conclusion:** LLM-based synthetic samples can approximate human responses but exhibit considerable item-level variation. Further calibration and testing are necessary to maintain algorithmic fidelity and minimize errors in public opinion measurement.

**Abstract:** Large Language Models (LLMs) offer promising avenues for methodological and applied innovations in survey research by using synthetic respondents to emulate human answers and behaviour, potentially mitigating measurement and representation errors. However, the extent to which LLMs recover aggregate item distributions remains uncertain and downstream applications risk reproducing social stereotypes and biases inherited from training data. We evaluate the reliability of LLM-generated synthetic survey responses against ground-truth human responses from a Chilean public opinion probabilistic survey. Specifically, we benchmark 128 prompt-model-question triplets, generating 189,696 synthetic profiles, and pool performance metrics (i.e., accuracy, precision, recall, and F1-score) in a meta-analysis across 128 question-subsample pairs to test for biases along key sociodemographic dimensions. The evaluation spans OpenAI's GPT family and o-series reasoning models, as well as Llama and Qwen checkpoints. Three results stand out. First, synthetic responses achieve excellent performance on trust items (F1-score and accuracy > 0.90). Second, GPT-4o, GPT-4o-mini and Llama 4 Maverick perform comparably on this task. Third, synthetic-human alignment is highest among respondents aged 45-59. Overall, LLM-based synthetic samples approximate responses from a probabilistic sample, though with substantial item-level heterogeneity. Capturing the full nuance of public opinion remains challenging and requires careful calibration and additional distributional tests to ensure algorithmic fidelity and reduce errors.

</details>


### [49] [Large Language Models Meet Legal Artificial Intelligence: A Survey](https://arxiv.org/abs/2509.09969)

*Zhitian Hou, Zihan Ye, Nanli Zeng, Tianyong Hao, Kun Zeng*

**Main category:** cs.CL

**Keywords:** Legal AI, Large Language Models, LLM frameworks, Legal benchmarks, Datasets for legal evaluation

**Relevance Score:** 4

**TL;DR:** This paper reviews the advancements of Large Language Models (LLMs) in Legal AI, covering frameworks, benchmarks, and datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding and application of LLM-based approaches in the legal domain and guide future research.

**Method:** The paper reviews 16 legal LLM series, 47 LLM-based frameworks, 15 benchmarks, and 29 datasets for evaluating legal tasks.

**Key Contributions:**

	1. Review of multiple legal LLMs and frameworks.
	2. Compilation of benchmarks and datasets for evaluation.
	3. Discussion of challenges and future research directions.

**Result:** The comprehensive review categorizes various LLMs and provides resources for evaluating their capabilities in legal applications.

**Limitations:** Focuses primarily on the legal domain, limiting applicability to other fields.

**Conclusion:** The paper aims to be a systematic introduction for beginners in Legal AI and encourages further research in this domain.

**Abstract:** Large Language Models (LLMs) have significantly advanced the development of Legal Artificial Intelligence (Legal AI) in recent years, enhancing the efficiency and accuracy of legal tasks. To advance research and applications of LLM-based approaches in legal domain, this paper provides a comprehensive review of 16 legal LLMs series and 47 LLM-based frameworks for legal tasks, and also gather 15 benchmarks and 29 datasets to evaluate different legal capabilities. Additionally, we analyse the challenges and discuss future directions for LLM-based approaches in the legal domain. We hope this paper provides a systematic introduction for beginners and encourages future research in this field. Resources are available at https://github.com/ZhitianHou/LLMs4LegalAI.

</details>


### [50] [CMHG: A Dataset and Benchmark for Headline Generation of Minority Languages in China](https://arxiv.org/abs/2509.09990)

*Guixian Xu, Zeli Su, Ziyin Zhang, Jianing Liu, XU Han, Ting Zhang, Yushuang Dong*

**Main category:** cs.CL

**Keywords:** minority languages, headline generation, dataset, Tibetan, Uyghur, Mongolian

**Relevance Score:** 3

**TL;DR:** This paper introduces a novel dataset, Chinese Minority Headline Generation (CMHG), to support headline generation tasks for minority languages in China, including Tibetan, Uyghur, and Traditional Mongolian.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Minority languages in China face challenges due to unique writing systems and lack of relevant corpora for tasks like headline generation.

**Method:** The paper presents the CMHG dataset comprising 100,000 entries for Tibetan and 50,000 for both Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, a high-quality test set annotated by native speakers is introduced.

**Key Contributions:**

	1. Introduction of the Chinese Minority Headline Generation dataset (CMHG).
	2. Creation of a test set annotated by native speakers for benchmark purposes.
	3. Provision of a substantial number of entries for three minority languages in China.

**Result:** The CMHG dataset is expected to enhance the resources available for headline generation in Chinese minority languages and establish a benchmark for future research.

**Limitations:** 

**Conclusion:** This dataset can serve as a valuable resource to improve headline generation techniques for Chinese minority languages and advance related benchmarks.

**Abstract:** Minority languages in China, such as Tibetan, Uyghur, and Traditional Mongolian, face significant challenges due to their unique writing systems, which differ from international standards. This discrepancy has led to a severe lack of relevant corpora, particularly for supervised tasks like headline generation. To address this gap, we introduce a novel dataset, Chinese Minority Headline Generation (CMHG), which includes 100,000 entries for Tibetan, and 50,000 entries each for Uyghur and Mongolian, specifically curated for headline generation tasks. Additionally, we propose a high-quality test set annotated by native speakers, designed to serve as a benchmark for future research in this domain. We hope this dataset will become a valuable resource for advancing headline generation in Chinese minority languages and contribute to the development of related benchmarks.

</details>


### [51] [Unsupervised Hallucination Detection by Inspecting Reasoning Processes](https://arxiv.org/abs/2509.10004)

*Ponhvoan Srey, Xiaobao Wu, Anh Tuan Luu*

**Main category:** cs.CL

**Keywords:** hallucination detection, unsupervised learning, large language models

**Relevance Score:** 9

**TL;DR:** IRIS is an unsupervised framework for detecting hallucinated content from LLMs, using internal representations for factual verification without labeled data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing unsupervised hallucination detection methods rely on proxy signals unrelated to factual correctness, leading to biased detection and limited generalizability.

**Method:** IRIS leverages the internal representations of LLMs to verify the truthfulness of statements and generates contextualized embeddings as features for training. The uncertainty of responses serves as soft pseudolabels for truthfulness.

**Key Contributions:**

	1. Introduction of IRIS framework for hallucination detection
	2. Utilization of LLM internal representations for training
	3. Effective use of response uncertainty as soft pseudolabels

**Result:** IRIS consistently outperforms existing unsupervised detection methods, demonstrating effectiveness even with limited training data.

**Limitations:** 

**Conclusion:** IRIS is a cost-effective, fully unsupervised solution suitable for real-time hallucination detection.

**Abstract:** Unsupervised hallucination detection aims to identify hallucinated content generated by large language models (LLMs) without relying on labeled data. While unsupervised methods have gained popularity by eliminating labor-intensive human annotations, they frequently rely on proxy signals unrelated to factual correctness. This misalignment biases detection probes toward superficial or non-truth-related aspects, limiting generalizability across datasets and scenarios. To overcome these limitations, we propose IRIS, an unsupervised hallucination detection framework, leveraging internal representations intrinsic to factual correctness. IRIS prompts the LLM to carefully verify the truthfulness of a given statement, and obtain its contextualized embedding as informative features for training. Meanwhile, the uncertainty of each response is considered a soft pseudolabel for truthfulness. Experimental results demonstrate that IRIS consistently outperforms existing unsupervised methods. Our approach is fully unsupervised, computationally low cost, and works well even with few training data, making it suitable for real-time detection.

</details>


### [52] [Multi-Intent Recognition in Dialogue Understanding: A Comparison Between Smaller Open-Source LLMs](https://arxiv.org/abs/2509.10010)

*Adnan Ahmad, Philine Kowol, Stefan Hillmann, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** multi-label intent classification, large language models, Natural Language Understanding, few-shot learning, dialogue systems

**Relevance Score:** 9

**TL;DR:** This paper analyzes multi-label intent classification using open-source LLMs on the MultiWOZ 2.1 dataset, comparing the performance of three models and providing insights into few-shot learning efficacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the efficacy of open-source large language models for multi-label intent classification in dialogue systems, and enhance natural language understanding in task-oriented chatbots.

**Method:** Evaluated three LLMs (LLama2-7B-hf, Mistral-7B-v0.1, Yi-6B) on the MultiWOZ 2.1 dataset under a few-shot setup, comparing their performance with supervised learning using BERT as a baseline, and using metrics such as accuracy, precision, recall, and various F1 scores.

**Key Contributions:**

	1. Analysis of multi-label intent classification using open-source pre-trained LLMs on consumer hardware
	2. Comparison of few-shot and supervised learning approaches for classification tasks
	3. Framework for small LLMs in improving natural language understanding in chatbots

**Result:** Mistral-7B-v0.1 outperformed the other two models on 11 out of 14 intent classes in F-Score, while a BERT-based classifier showed superior performance compared to the best-performing few-shot LLM.

**Limitations:** Focused on a specific dataset (MultiWOZ 2.1) and types of models, which may not generalize across all applications or settings.

**Conclusion:** The study demonstrates the potential of small open-source LLMs in handling complex multi-intent dialogues, while revealing that traditional supervised models can achieve better results in certain cases.

**Abstract:** In this paper, we provide an extensive analysis of multi-label intent classification using Large Language Models (LLMs) that are open-source, publicly available, and can be run in consumer hardware. We use the MultiWOZ 2.1 dataset, a benchmark in the dialogue system domain, to investigate the efficacy of three popular open-source pre-trained LLMs, namely LLama2-7B-hf, Mistral-7B-v0.1, and Yi-6B. We perform the classification task in a few-shot setup, giving 20 examples in the prompt with some instructions. Our approach focuses on the differences in performance of these models across several performance metrics by methodically assessing these models on multi-label intent classification tasks. Additionally, we compare the performance of the instruction-based fine-tuning approach with supervised learning using the smaller transformer model BertForSequenceClassification as a baseline. To evaluate the performance of the models, we use evaluation metrics like accuracy, precision, and recall as well as micro, macro, and weighted F1 score. We also report the inference time, VRAM requirements, etc. The Mistral-7B-v0.1 outperforms two other generative models on 11 intent classes out of 14 in terms of F-Score, with a weighted average of 0.50. It also has relatively lower Humming Loss and higher Jaccard Similarity, making it the winning model in the few-shot setting. We find BERT based supervised classifier having superior performance compared to the best performing few-shot generative LLM. The study provides a framework for small open-source LLMs in detecting complex multi-intent dialogues, enhancing the Natural Language Understanding aspect of task-oriented chatbots.

</details>


### [53] [Linguistic trajectories of bipolar disorder on social media](https://arxiv.org/abs/2509.10035)

*Laurin Plank, Armin Zlomuzica*

**Main category:** cs.CL

**Keywords:** bipolar disorder, social media, language analysis, mental health, affective disorders

**Relevance Score:** 9

**TL;DR:** Study analyzes social media language to track linguistic changes in users before and after bipolar disorder diagnosis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the temporal language changes associated with bipolar disorder using social media data, addressing the limited scale of traditional clinical assessments.

**Method:** Analyzed language trajectories from social media posts of users diagnosed with bipolar disorder, unipolar depression, and non-affected individuals over a period spanning from three years before to twenty-one years after diagnosis.

**Key Contributions:**

	1. Introduced a method to track language changes over time related to bipolar disorder diagnosis.
	2. Showed pervasive linguistic alterations associated with bipolar disorder across an extended timeline.
	3. Provided evidence for the periodicity of mood-related language changes.
	4. Revealed potential gender differences in language periodicity.

**Result:** Key findings reveal significant linguistic alterations in bipolar disorder patients, reflecting various aspects of the disorder including mood disturbance and comorbidities, alongside recurring language changes over time.

**Limitations:** Focused primarily on social media language, which may not represent broader contexts or diverse populations.

**Conclusion:** The study underscores the potential of social media analysis as a scalable method for monitoring mental health through linguistic markers, particularly for bipolar disorder.

**Abstract:** Language provides valuable markers of affective disorders such as bipolar disorder (BD), yet clinical assessments remain limited in scale. In response, analyses of social media (SM) language have gained prominence due to their high temporal resolution and longitudinal scope. Here, we introduce a method to determine the timing of users' diagnoses and apply it to study language trajectories from 3 years before to 21 years after BD diagnosis - contrasted with uses reporting unipolar depression (UD) and non-affected users (HC). We show that BD diagnosis is accompanied by pervasive linguistic alterations reflecting mood disturbance, psychiatric comorbidity, substance abuse, hospitalization, medical comorbidities, unusual thought content, and disorganized thought. We further observe recurring mood-related language changes across two decades after the diagnosis, with a pronounced 12-month periodicity suggestive of seasonal mood episodes. Finally, trend-level evidence suggests an increased periodicity in users estimated to be female. In sum, our findings provide evidence for language alterations in the acute and chronic phase of BD. This validates and extends recent efforts leveraging SM for scalable monitoring of mental health.

</details>


### [54] [!MSA at BAREC Shared Task 2025: Ensembling Arabic Transformers for Readability Assessment](https://arxiv.org/abs/2509.10040)

*Mohamed Basem, Mohamed Younes, Seif Ahmed, Abdelrahman Moustafa*

**Main category:** cs.CL

**Keywords:** Arabic readability, Transformer models, Data augmentation

**Relevance Score:** 3

**TL;DR:** The study presents a winning system for Arabic readability assessment using a diverse ensemble of transformer models and advanced techniques to address data challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Arabic readability assessment and address challenges such as class imbalance and data scarcity.

**Method:** A confidence-weighted ensemble of four transformer models, fine-tuned with distinct loss functions, including data generation and targeted post-processing to enhance prediction accuracy.

**Key Contributions:**

	1. Winning system for BAREC 2025 Shared Task
	2. Confidence-weighted ensemble of diverse transformer models
	3. Innovative use of data augmentation techniques

**Result:** Achieved 87.5% QWK at the sentence level and 87.4% at the document level, with a 6.3% gain in QWK through intelligent augmentation and model diversity.

**Limitations:** 

**Conclusion:** The approach demonstrates success in robust Arabic readability prediction by leveraging ensemble learning and sophisticated training techniques.

**Abstract:** We present MSAs winning system for the BAREC 2025 Shared Task on fine-grained Arabic readability assessment, achieving first place in six of six tracks. Our approach is a confidence-weighted ensemble of four complementary transformer models (AraBERTv2, AraELECTRA, MARBERT, and CAMeLBERT) each fine-tuned with distinct loss functions to capture diverse readability signals. To tackle severe class imbalance and data scarcity, we applied weighted training, advanced preprocessing, SAMER corpus relabeling with our strongest model, and synthetic data generation via Gemini 2.5 Flash, adding about 10,000 rare-level samples. A targeted post-processing step corrected prediction distribution skew, delivering a 6.3 percent Quadratic Weighted Kappa (QWK) gain. Our system reached 87.5 percent QWK at the sentence level and 87.4 percent at the document level, demonstrating the power of model and loss diversity, confidence-informed fusion, and intelligent augmentation for robust Arabic readability prediction.

</details>


### [55] [Established Psychometric vs. Ecologically Valid Questionnaires: Rethinking Psychological Assessments in Large Language Models](https://arxiv.org/abs/2509.10078)

*Dongmin Choi, Woojung Song, Jongwook Han, Eun-Ju Lee, Yohan Jo*

**Main category:** cs.CL

**Keywords:** Large Language Models, psychometric questionnaires, ecological validity, personality assessment, machine learning

**Relevance Score:** 8

**TL;DR:** This paper analyzes the effectiveness of traditional psychometric questionnaires in assessing Large Language Models (LLMs), highlighting significant discrepancies when compared to ecologically valid questionnaires.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the ecological validity of established psychometric questionnaires used to assess personality traits and values in LLMs, and to understand the implications of these differences.

**Method:** A comprehensive comparative analysis of established psychometric questionnaires and ecologically valid questionnaires, examining how each type reflects the psychological characteristics of LLMs in user query contexts.

**Key Contributions:**

	1. Identified significant differences in profiles produced by established vs. ecologically valid questionnaires for LLMs.
	2. Highlighted the limitations of established questionnaires in accurately measuring LLM characteristics.
	3. Emphasized the necessity of ecological validity in assessing LLMs.

**Result:** The analysis reveals that established questionnaires provide misleading profiles of LLMs, suffer from inadequate measurement stability, and exaggerate the stability of LLM constructs, particularly in persona-prompted scenarios.

**Limitations:** The study focuses primarily on the comparative effectiveness of questionnaires without addressing potential alternative assessment methods for LLMs.

**Conclusion:** The findings caution against employing established psychological questionnaires for LLM assessments, emphasizing the need for more ecologically valid measurement tools.

**Abstract:** Researchers have applied established psychometric questionnaires (e.g., BFI, PVQ) to measure the personality traits and values reflected in the responses of Large Language Models (LLMs). However, concerns have been raised about applying these human-designed questionnaires to LLMs. One such concern is their lack of ecological validity--the extent to which survey questions adequately reflect and resemble real-world contexts in which LLMs generate texts in response to user queries. However, it remains unclear how established questionnaires and ecologically valid questionnaires differ in their outcomes, and what insights these differences may provide. In this paper, we conduct a comprehensive comparative analysis of the two types of questionnaires. Our analysis reveals that established questionnaires (1) yield substantially different profiles of LLMs from ecologically valid ones, deviating from the psychological characteristics expressed in the context of user queries, (2) suffer from insufficient items for stable measurement, (3) create misleading impressions that LLMs possess stable constructs, and (4) yield exaggerated profiles for persona-prompted LLMs. Overall, our work cautions against the use of established psychological questionnaires for LLMs. Our code will be released upon publication.

</details>


### [56] [Querying Climate Knowledge: Semantic Retrieval for Scientific Discovery](https://arxiv.org/abs/2509.10087)

*Mustapha Adamu, Qi Zhang, Huitong Pan, Longin Jan Latecki, Eduard C. Dragut*

**Main category:** cs.CL

**Keywords:** Knowledge Graph, Climate Science, Semantic Queries, Large Language Models, Information Retrieval

**Relevance Score:** 3

**TL;DR:** Introduction of a Knowledge Graph for climate science literature to enhance information retrieval and connection discovery.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by the vast and complex climate science literature that makes it difficult for researchers to find relevant information.

**Method:** Development of a domain-specific Knowledge Graph (KG) built from climate publications, enabling structured semantic queries.

**Key Contributions:**

	1. Introduction of a Knowledge Graph for climate science literature.
	2. Support for structured, semantic queries to improve information access.
	3. Integration with large language models for enhanced question answering.

**Result:** The KG effectively assists in discovering connections between models, datasets, regions, and teleconnection patterns using Cypher queries and integrates with LLMs in RAG systems for improved information retrieval.

**Limitations:** 

**Conclusion:** The study demonstrates the real-world applicability of the KG for climate researchers and model developers, emphasizing its utility for accurate scientific information retrieval.

**Abstract:** The growing complexity and volume of climate science literature make it increasingly difficult for researchers to find relevant information across models, datasets, regions, and variables. This paper introduces a domain-specific Knowledge Graph (KG) built from climate publications and broader scientific texts, aimed at improving how climate knowledge is accessed and used. Unlike keyword based search, our KG supports structured, semantic queries that help researchers discover precise connections such as which models have been validated in specific regions or which datasets are commonly used with certain teleconnection patterns. We demonstrate how the KG answers such questions using Cypher queries, and outline its integration with large language models in RAG systems to improve transparency and reliability in climate-related question answering. This work moves beyond KG construction to show its real world value for climate researchers, model developers, and others who rely on accurate, contextual scientific information.

</details>


### [57] [Arabic Large Language Models for Medical Text Generation](https://arxiv.org/abs/2509.10095)

*Abdulrahman Allam, Seif Ahmed, Ali Hamdi, Ammar Mohammed*

**Main category:** cs.CL

**Keywords:** hospital management systems, large language models, Arabic medical text generation, generative artificial intelligence, healthcare challenges

**Relevance Score:** 9

**TL;DR:** This study presents a fine-tuned LLM for Arabic medical text generation to improve hospital management systems by providing accurate medical advice and treatment recommendations based on user input. It utilizes a unique dataset gathered from social media that captures real-world medical conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in hospital management systems, including overcrowding and the lack of accurate, real-time medical advice, especially for underrepresented languages like Arabic.

**Method:** The methodology involved collecting and preprocessing a unique dataset from social media, fine-tuning large language models including Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2 Medium to enhance the generation of medical text.

**Key Contributions:**

	1. Fine-tuned LLMs specifically for Arabic medical text generation.
	2. Utilization of a real-world dataset for improving the relevance of medical advice.
	3. Evaluation and benchmarking of multiple generative models.

**Result:** The fine-tuned Mistral-7B model outperformed others, achieving average BERT Score values of 68.5% in precision, 69.08% in recall, and 68.5% in F1-scores. The model produced coherent and relevant medical responses to informal user input.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of generative AI in enhancing hospital management systems, providing a scalable solution for healthcare challenges in linguistically diverse environments.

**Abstract:** Efficient hospital management systems (HMS) are critical worldwide to address challenges such as overcrowding, limited resources, and poor availability of urgent health care. Existing methods often lack the ability to provide accurate, real-time medical advice, particularly for irregular inputs and underrepresented languages. To overcome these limitations, this study proposes an approach that fine-tunes large language models (LLMs) for Arabic medical text generation. The system is designed to assist patients by providing accurate medical advice, diagnoses, drug recommendations, and treatment plans based on user input. The research methodology required the collection of a unique dataset from social media platforms, capturing real-world medical conversations between patients and doctors. The dataset, which includes patient complaints together with medical advice, was properly cleaned and preprocessed to account for multiple Arabic dialects. Fine-tuning state-of-the-art generative models, such as Mistral-7B-Instruct-v0.2, LLaMA-2-7B, and GPT-2 Medium, optimized the system's ability to generate reliable medical text. Results from evaluations indicate that the fine-tuned Mistral-7B model outperformed the other models, achieving average BERT (Bidirectional Encoder Representations from Transformers) Score values in precision, recall, and F1-scores of 68.5\%, 69.08\%, and 68.5\%, respectively. Comparative benchmarking and qualitative assessments validate the system's ability to produce coherent and relevant medical replies to informal input. This study highlights the potential of generative artificial intelligence (AI) in advancing HMS, offering a scalable and adaptable solution for global healthcare challenges, especially in linguistically and culturally diverse environments.

</details>


### [58] [Scaling Arabic Medical Chatbots Using Synthetic Data: Enhancing Generative AI with Synthetic Patient Records](https://arxiv.org/abs/2509.10108)

*Abdulrahman Allam, Seif Ahmed, Ali Hamdi, Khaled Shaban*

**Main category:** cs.CL

**Keywords:** synthetic data, medical chatbots, Arabic NLP, language models, data augmentation

**Relevance Score:** 8

**TL;DR:** The study proposes a synthetic data augmentation strategy to enhance Arabic medical chatbots by expanding the training dataset from 20,000 to 100,000 records.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in developing medical chatbots in Arabic due to the lack of large-scale, high-quality annotated datasets.

**Method:** The authors used generative AI systems ChatGPT-4o and Gemini 2.5 Pro to generate 80,000 synthetic question-answer pairs, which were filtered and validated, then used to fine-tune five LLMs including Mistral-7B and AraGPT2.

**Key Contributions:**

	1. Proposed a scalable synthetic data augmentation strategy for medical NLP.
	2. Generated 80,000 synthetic question-answer pairs validated for medical coherence.
	3. Showed performance improvements in LLMs using augmented data.

**Result:** The augmentation strategy showed improved performance with higher F1-scores and fewer hallucinations using ChatGPT-4o data across all models evaluated.

**Limitations:** 

**Conclusion:** Synthetic data augmentation is a viable solution for enhancing domain-specific language models in low-resource medical NLP, supporting the development of more inclusive and accurate Arabic healthcare chatbots.

**Abstract:** The development of medical chatbots in Arabic is significantly constrained by the scarcity of large-scale, high-quality annotated datasets. While prior efforts compiled a dataset of 20,000 Arabic patient-doctor interactions from social media to fine-tune large language models (LLMs), model scalability and generalization remained limited. In this study, we propose a scalable synthetic data augmentation strategy to expand the training corpus to 100,000 records. Using advanced generative AI systems ChatGPT-4o and Gemini 2.5 Pro we generated 80,000 contextually relevant and medically coherent synthetic question-answer pairs grounded in the structure of the original dataset. These synthetic samples were semantically filtered, manually validated, and integrated into the training pipeline. We fine-tuned five LLMs, including Mistral-7B and AraGPT2, and evaluated their performance using BERTScore metrics and expert-driven qualitative assessments. To further analyze the effectiveness of synthetic sources, we conducted an ablation study comparing ChatGPT-4o and Gemini-generated data independently. The results showed that ChatGPT-4o data consistently led to higher F1-scores and fewer hallucinations across all models. Overall, our findings demonstrate the viability of synthetic augmentation as a practical solution for enhancing domain-specific language models in-low resource medical NLP, paving the way for more inclusive, scalable, and accurate Arabic healthcare chatbot systems.

</details>


### [59] [Prominence-aware automatic speech recognition for conversational speech](https://arxiv.org/abs/2509.10116)

*Julian Linke, Barbara Schuppler*

**Main category:** cs.CL

**Keywords:** prominence detection, automatic speech recognition, wav2vec2, prosody, dialogue systems

**Relevance Score:** 4

**TL;DR:** The paper explores prominence-aware automatic speech recognition (ASR) for conversational Austrian German by combining prominence detection with speech transcription.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance automatic speech recognition by integrating prosodic prominence detection, thereby improving dialogue systems and linguistic research.

**Method:** Fine-tuning wav2vec2 models to classify word-level prominence and using the resulting detector for annotating a large corpus before training prominence-aware ASR systems.

**Key Contributions:**

	1. Development of prominence detectors using fine-tuned wav2vec2 models
	2. Introduction of prominence-aware ASR systems
	3. Demonstrated effectiveness of transformer models in encoding prosodic information

**Result:** Achieved a prominence detection accuracy of 85.53% in correct utterances, although the integration of prominence information did not improve overall ASR performance compared to the baseline.

**Limitations:** 

**Conclusion:** Transformer-based models effectively encode prosodic information, providing a significant contribution to prosody-aware ASR, with implications for dialogue systems.

**Abstract:** This paper investigates prominence-aware automatic speech recognition (ASR) by combining prominence detection and speech recognition for conversational Austrian German. First, prominence detectors were developed by fine-tuning wav2vec2 models to classify word-level prominence. The detector was then used to automatically annotate prosodic prominence in a large corpus. Based on those annotations, we trained novel prominence-aware ASR systems that simultaneously transcribe words and their prominence levels. The integration of prominence information did not change performance compared to our baseline ASR system, while reaching a prominence detection accuracy of 85.53% for utterances where the recognized word sequence was correct. This paper shows that transformer-based models can effectively encode prosodic information and represents a novel contribution to prosody-enhanced ASR, with potential applications for linguistic research and prosody-informed dialogue systems.

</details>


### [60] [Population-Aligned Persona Generation for LLM-based Social Simulation](https://arxiv.org/abs/2509.10127)

*Zhengyu Hu, Zheyuan Xiao, Max Xiong, Yuxuan Lei, Tianfu Wang, Jianxun Lian, Kaize Ding, Ziang Xiao, Nicholas Jing Yuan, Xing Xie*

**Main category:** cs.CL

**Keywords:** large language models, social simulations, persona generation, bias mitigation, psychometric distributions

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for generating high-quality, population-aligned persona sets using LLMs for social simulations, addressing the biases in existing frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper is motivated by the need for diverse and representative persona sets in LLM-driven social simulations, as most existing studies overlook persona generation complexities and bias.

**Method:** The proposed method uses LLMs to generate narrative personas from social media data, followed by quality assessments to filter profiles and importance sampling for alignment with psychometric distributions.

**Key Contributions:**

	1. Framework for generating population-aligned persona sets
	2. Importance sampling for global alignment with psychometric traits
	3. Task-specific module for adapting personas to subpopulations

**Result:** Extensive experiments show that the method reduces population-level bias and allows for accurate and flexible social simulations.

**Limitations:** 

**Conclusion:** A systematic framework for persona generation helps improve the fidelity of simulations, contributing meaningfully to research and policy applications.

**Abstract:** Recent advances in large language models (LLMs) have enabled human-like social simulations at unprecedented scale and fidelity, offering new opportunities for computational social science. A key challenge, however, is the construction of persona sets that authentically represent the diversity and distribution of real-world populations. Most existing LLM-based social simulation studies focus primarily on designing agentic frameworks and simulation environments, often overlooking the complexities of persona generation and the potential biases introduced by unrepresentative persona sets. In this paper, we propose a systematic framework for synthesizing high-quality, population-aligned persona sets for LLM-driven social simulation. Our approach begins by leveraging LLMs to generate narrative personas from long-term social media data, followed by rigorous quality assessment to filter out low-fidelity profiles. We then apply importance sampling to achieve global alignment with reference psychometric distributions, such as the Big Five personality traits. To address the needs of specific simulation contexts, we further introduce a task-specific module that adapts the globally aligned persona set to targeted subpopulations. Extensive experiments demonstrate that our method significantly reduces population-level bias and enables accurate, flexible social simulation for a wide range of research and policy applications.

</details>


### [61] [Towards Reliable and Interpretable Document Question Answering via VLMs](https://arxiv.org/abs/2509.10129)

*Alessio Chen, Simone Giovannini, Andrea Gemelli, Fabio Coppini, Simone Marinai*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, document understanding, bounding-box prediction, localization, information extraction

**Relevance Score:** 7

**TL;DR:** DocExplainerV0 is a bounding-box prediction module designed to improve spatial localization in Vision-Language Models (VLMs) for document understanding, addressing the challenge of accurately locating answers in complex documents.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Despite strong performance in document understanding, VLMs struggle with accurately localizing answers, which affects their interpretability and real-world applicability.

**Method:** The authors introduce DocExplainerV0, a plug-and-play bounding-box prediction module that separates answer generation from spatial localization, making it compatible with existing VLMs.

**Key Contributions:**

	1. Introduction of the DocExplainerV0 module
	2. Decoupling answer generation from spatial localization
	3. Establishment of a benchmark for VLMs in document understanding

**Result:** Systematic evaluation reveals a significant gap between textual accuracy and spatial grounding, demonstrating that many correct answers lack reliable localization.

**Limitations:** Focuses on the bounding-box prediction aspect and may not address other components of VLMs.

**Conclusion:** The standardized framework established by this work provides quantitative insights into document information extraction and serves as a benchmark for future research.

**Abstract:** Vision-Language Models (VLMs) have shown strong capabilities in document understanding, particularly in identifying and extracting textual information from complex documents. Despite this, accurately localizing answers within documents remains a major challenge, limiting both interpretability and real-world applicability. To address this, we introduce \textit{DocExplainerV0}, a plug-and-play bounding-box prediction module that decouples answer generation from spatial localization. This design makes it applicable to existing VLMs, including proprietary systems where fine-tuning is not feasible. Through systematic evaluation, we provide quantitative insights into the gap between textual accuracy and spatial grounding, showing that correct answers often lack reliable localization. Our standardized framework highlights these shortcomings and establishes a benchmark for future research toward more interpretable and robust document information extraction VLMs.

</details>


### [62] [Benchmark of stylistic variation in LLM-generated texts](https://arxiv.org/abs/2509.10179)

*Jiří Milička, Anna Marklová, Václav Cvrček*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multidimensional Analysis, Textual Variation

**Relevance Score:** 8

**TL;DR:** The study examines the differences in textual variation between human-written texts and those produced by large language models (LLMs) using multidimensional analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs differ from human writing and establish benchmarks for comparison.

**Method:** Biber's multidimensional analysis (MDA) was applied to human and LLM-generated texts, leveraging the AI-Brown corpus for English and the AI-Koditex corpus for Czech.

**Key Contributions:**

	1. Application of Biber's MDA to distinguish human and LLM texts.
	2. Creation of AI-Brown and AI-Koditex corpora for analysis.
	3. Development of a benchmark for interpreting and ranking LLMs.

**Result:** The analysis finds significant and systematic differences in textual features between human and LLM-generated content, revealing insights into model performance under various conditions.

**Limitations:** 

**Conclusion:** A benchmark for comparing LLMs is created based on the significant dimensions of variation identified in the study.

**Abstract:** This study investigates the register variation in texts written by humans and comparable texts produced by large language models (LLMs). Biber's multidimensional analysis (MDA) is applied to a sample of human-written texts and AI-created texts generated to be their counterparts to find the dimensions of variation in which LLMs differ most significantly and most systematically from humans. As textual material, a new LLM-generated corpus AI-Brown is used, which is comparable to BE-21 (a Brown family corpus representing contemporary British English). Since all languages except English are underrepresented in the training data of frontier LLMs, similar analysis is replicated on Czech using AI-Koditex corpus and Czech multidimensional model. Examined were 16 frontier models in various settings and prompts, with emphasis placed on the difference between base models and instruction-tuned models. Based on this, a benchmark is created through which models can be compared with each other and ranked in interpretable dimensions.

</details>


### [63] [Incongruent Positivity: When Miscalibrated Positivity Undermines Online Supportive Conversations](https://arxiv.org/abs/2509.10184)

*Leen Almajed, Abeer ALdayel*

**Main category:** cs.CL

**Keywords:** Emotional Support, LLM Responses, Incongruent Positivity, Human-Computer Interaction, Context-Aware AI

**Relevance Score:** 8

**TL;DR:** This paper investigates the miscalibration of positive support in emotionally supportive dialogues, comparing human and LLM-generated responses, and proposes improvements for detecting misaligned positivity.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the detrimental effects of incongruent positivity in supportive conversations, particularly in how LLMs generate responses that may appear dismissive or overly optimistic.

**Method:** The study involves collecting user-assistant dialogues from Reddit, categorizing them by emotional intensity, and generating additional responses using LLMs. It includes a comparative analysis of supportive responses and develops a weakly supervised classifier to identify incongruent positivity.

**Key Contributions:**

	1. Identification of incongruent positivity in LLM responses.
	2. Development of a novel classifier ensemble for detecting emotional alignment in conversations.
	3. Insights into improving LLM support mechanisms in emotionally charged dialogues.

**Result:** The analysis shows that LLMs exhibit unrealistic positivity, especially in high-stakes conversations. The classifier ensemble improves detection of various incongruent positivity types across different emotional concerns.

**Limitations:** 

**Conclusion:** The study highlights the necessity for LLMs to produce congruent supportive responses that acknowledge emotions rather than defaulting to generic positivity, suggesting paths for enhancing conversational AI systems.

**Abstract:** In emotionally supportive conversations, well-intended positivity can sometimes misfire, leading to responses that feel dismissive, minimizing, or unrealistically optimistic. We examine this phenomenon of incongruent positivity as miscalibrated expressions of positive support in both human and LLM generated responses. To this end, we collected real user-assistant dialogues from Reddit across a range of emotional intensities and generated additional responses using large language models for the same context. We categorize these conversations by intensity into two levels: Mild, which covers relationship tension and general advice, and Severe, which covers grief and anxiety conversations. This level of categorization enables a comparative analysis of how supportive responses vary across lower and higher stakes contexts. Our analysis reveals that LLMs are more prone to unrealistic positivity through dismissive and minimizing tone, particularly in high-stakes contexts. To further study the underlying dimensions of this phenomenon, we finetune LLMs on datasets with strong and weak emotional reactions. Moreover, we developed a weakly supervised multilabel classifier ensemble (DeBERTa and MentalBERT) that shows improved detection of incongruent positivity types across two sorts of concerns (Mild and Severe). Our findings shed light on the need to move beyond merely generating generic positive responses and instead study the congruent support measures to balance positive affect with emotional acknowledgment. This approach offers insights into aligning large language models with affective expectations in the online supportive dialogue, paving the way toward context-aware and trust preserving online conversation systems.

</details>


### [64] [Beyond Token Limits: Assessing Language Model Performance on Long Text Classification](https://arxiv.org/abs/2509.10199)

*Miklós Sebők, Viktor Kovács, Martin Bánóczy, Daniel Møller Eriksen, Nathalie Neptune, Philippe Roussille*

**Main category:** cs.CL

**Keywords:** long text processing, multiclass classification, large language models

**Relevance Score:** 8

**TL;DR:** This paper investigates the challenges of processing long input texts in large language models (LLMs) for classification tasks, specifically in the context of legislative documents across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The limitation of input text length in widely used LLMs restricts their applicability to classification tasks involving long texts, such as laws and draft laws.

**Method:** Experiments with XLM-RoBERTa, Longformer, GPT-3.5, and GPT-4 were conducted on a multiclass classification task using a codebook of 21 policy topic labels across 5 languages.

**Key Contributions:**

	1. Analysis of LLM performance on long input texts for policy classification
	2. Comparison of models including XLM-RoBERTa, Longformer, GPT-3.5, and GPT-4
	3. Insights into class-level factors influencing text classification outcomes

**Result:** The results showed no advantage for Longformer over the GPT variants; however, the best-performing open model outperformed the GPT models in long text processing.

**Limitations:** 

**Conclusion:** Class-level analysis revealed that support and substance overlaps between specific categories significantly affect performance on long text inputs.

**Abstract:** The most widely used large language models in the social sciences (such as BERT, and its derivatives, e.g. RoBERTa) have a limitation on the input text length that they can process to produce predictions. This is a particularly pressing issue for some classification tasks, where the aim is to handle long input texts. One such area deals with laws and draft laws (bills), which can have a length of multiple hundred pages and, therefore, are not particularly amenable for processing with models that can only handle e.g. 512 tokens. In this paper, we show results from experiments covering 5 languages with XLM-RoBERTa, Longformer, GPT-3.5, GPT-4 models for the multiclass classification task of the Comparative Agendas Project, which has a codebook of 21 policy topic labels from education to health care. Results show no particular advantage for the Longformer model, pre-trained specifically for the purposes of handling long inputs. The comparison between the GPT variants and the best-performing open model yielded an edge for the latter. An analysis of class-level factors points to the importance of support and substance overlaps between specific categories when it comes to performance on long text inputs.

</details>


### [65] [SI-FACT: Mitigating Knowledge Conflict via Self-Improving Faithfulness-Aware Contrastive Tuning](https://arxiv.org/abs/2509.10208)

*Shengqiang Fu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Contextual Faithfulness, Contrastive Learning

**Relevance Score:** 9

**TL;DR:** A framework called Self Improving Faithfulness Aware Contrastive Tuning (SI FACT) enhances the contextual faithfulness of large language models (LLMs) by generating structured contrastive learning data and applying contrastive learning techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often generate unfaithful responses due to knowledge conflicts, where they rely more on internal knowledge than on contextual information.

**Method:** The SI FACT framework utilizes a self-instruct mechanism for automatic generation of high-quality contrastive learning data, including anchor samples, semantically equivalent positive samples, and negative samples that simulate unfaithful scenarios. Contrastive learning is then applied for training.

**Key Contributions:**

	1. Novel framework for improving LLM faithfulness
	2. Automatic generation of contrastive learning data
	3. Significant performance improvement on contextual recall benchmarks

**Result:** The SI FACT model (based on Llama3 8B Instruct) shows a 6.2% improvement in Contextual Recall Rate over the best baseline while reducing reliance on internal memory, demonstrating strong effectiveness and data efficiency.

**Limitations:** 

**Conclusion:** SI FACT provides a practical approach to enhancing the faithfulness of LLM responses, contributing to the development of more reliable language models.

**Abstract:** Large Language Models often generate unfaithful responses in knowledge intensive tasks due to knowledge conflict,that is,a preference for relying on internal parametric knowledge rather than the provided context.To address this issue,we propose a novel self improving framework,Self Improving Faithfulness Aware Contrastive Tuning.The framework uses a self instruct mechanism that allows the base LLM to automatically generate high quality,structured contrastive learning data,including anchor samples,semantically equivalent positive samples,and negative samples simulating unfaithful scenarios.This approach significantly reduces the cost of manual annotation.Subsequently,contrastive learning is applied to train the model,enabling it to pull faithful responses closer and push unfaithful responses farther apart in the representation space.Experiments on knowledge conflict evaluation benchmarks ECARE KRE and COSE KRE show that the SI FACT model based on Llama3 8B Instruct improves the Contextual Recall Rate by 6.2% over the best baseline method,while significantly reducing dependence on internal memory.The results indicate that SI FACT provides strong effectiveness and high data efficiency in enhancing the contextual faithfulness of LLMs,offering a practical pathway toward building more proactive and trustworthy language models.

</details>


### [66] [Dropping Experts, Recombining Neurons: Retraining-Free Pruning for Sparse Mixture-of-Experts LLMs](https://arxiv.org/abs/2509.10377)

*Yixiao Zhou, Ziyu Zhao, Dongzhou Cheng, zhiliang wu, Jie Gui, Yi Yang, Fei Wu, Yu Cheng, Hehe Fan*

**Main category:** cs.CL

**Keywords:** Sparse Mixture-of-Experts, Dropping Experts, Recombining Neurons, large language models, memory efficiency

**Relevance Score:** 8

**TL;DR:** The paper introduces DERN, a framework for pruning and recombining neurons in Sparse Mixture-of-Experts architectures to reduce memory usage and enhance performance in large language models without retraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high memory usage and deployment challenges associated with Sparse Mixture-of-Experts architectures, despite their computational efficiency.

**Method:** DERN works by first pruning redundant experts based on router statistics, then decomposing them into neuron-level expert segments which are reassigned to compatible retained experts, and finally merging segments within those retained experts.

**Key Contributions:**

	1. Introduces a novel framework for expert pruning and reconstruction at the neuron level.
	2. Demonstrates significant performance improvements on key benchmarks with reduced expert count.
	3. Reduces memory usage associated with LLMs by optimizing expert representations.

**Result:** Experiments demonstrate that DERN improves performance by over 5% on commonsense reasoning and MMLU benchmarks with 50% expert sparsity, and significantly reduces memory usage and the number of experts needed.

**Limitations:** 

**Conclusion:** The DERN framework enhances the practicality of deploying SMoE LLMs by lowering memory requirements and maintaining high performance without additional training.

**Abstract:** Sparse Mixture-of-Experts (SMoE) architectures are widely used in large language models (LLMs) due to their computational efficiency. However, though only a few experts are activated for each token, SMoE still requires loading all expert parameters, leading to high memory usage and challenges in deployment. Previous work has tried to reduce the overhead by pruning and merging experts, but primarily focused on expert-level operations, leaving neuron-level structure underexplored. We propose DERN (Dropping Experts, Recombining Neurons), a task-agnostic and retraining-free framework for expert pruning and reconstruction. We observe that experts are often misaligned and contain semantic conflicts at the neuron level, which poses challenges for direct merging. To solve this, DERN works in three steps: it first prunes redundant experts using router statistics; then it decomposes them into neuron-level expert segments, assigning each segment to its most compatible retained expert; and finally, it merges segments within each retained expert to build a compact representation. Experiments on Mixtral, Qwen, and DeepSeek SMoE models show that DERN improves performance by more than 5% on commonsense reasoning and MMLU benchmarks under 50% expert sparsity, without extra training. It also greatly reduces the number of experts and memory usage, making SMoE LLMs easier to deploy in practice.

</details>


### [67] [Is In-Context Learning Learning?](https://arxiv.org/abs/2509.10414)

*Adrian de Wynter*

**Main category:** cs.CL

**Keywords:** In-context learning, autoregressive models, machine learning, generalization, prompting styles

**Relevance Score:** 7

**TL;DR:** This paper analyzes in-context learning (ICL) in autoregressive models, arguing that while ICL performs learning tasks effectively, it has limitations in generalizing to unseen tasks due to its reliance on prior knowledge and input distribution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the claims regarding the abilities of autoregressive models to learn unseen tasks through in-context learning (ICL) and questions the robustness of this learning mechanism.

**Method:** A large-scale analysis of ICL, accounting for variables such as memorization, pretraining, distributional shifts, and prompting styles, to evaluate its effectiveness in learning and generalizing.

**Key Contributions:**

	1. Large-scale analysis of in-context learning (ICL) mechanisms
	2. Demonstration of limitations in generalizing to unseen tasks
	3. Insights on the influence of prompt styles and input features on accuracy

**Result:** The analysis demonstrates that while ICL can be effective, it is not robust in learning and generalizing to unseen tasks, with performance influenced by exemplar distribution and prompting styles.

**Limitations:** The study may not fully account for all factors influencing ICL's performance across varying context conditions.

**Conclusion:** The study concludes that ICL's ad-hoc encoding mechanism is limited and does not support strong generalizability across different tasks.

**Abstract:** In-context learning (ICL) allows some autoregressive models to solve tasks via next-token prediction and without needing further training. This has led to claims about these model's ability to solve (learn) unseen tasks with only a few shots (exemplars) in the prompt. However, deduction does not always imply learning, as ICL does not explicitly encode a given observation. Instead, the models rely on their prior knowledge and the exemplars given, if any. We argue that, mathematically, ICL does constitute learning, but its full characterisation requires empirical work. We then carry out a large-scale analysis of ICL ablating out or accounting for memorisation, pretraining, distributional shifts, and prompting style and phrasing. We find that ICL is an effective learning paradigm, but limited in its ability to learn and generalise to unseen tasks. We note that, in the limit where exemplars become more numerous, accuracy is insensitive to exemplar distribution, model, prompt style, and the input's linguistic features. Instead, it deduces patterns from regularities in the prompt, which leads to distributional sensitivity, especially in prompting styles such as chain-of-thought. Given the varied accuracies on formally similar tasks, we conclude that autoregression's ad-hoc encoding is not a robust mechanism, and suggests limited all-purpose generalisability.

</details>


### [68] [Long Context Automated Essay Scoring with Language Models](https://arxiv.org/abs/2509.10417)

*Christopher Ormerod, Gitit Kehat*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Transformer Models, Context Length

**Relevance Score:** 8

**TL;DR:** This study evaluates modified transformer models to address length limitations in essay scoring, using the Kaggle ASAP 2.0 dataset.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Transformer models struggle with long texts, which is problematic for Automated Essay Scoring. Truncating text can harm evaluation validity.

**Method:** Several transformer models (XLNet, Longformer, ModernBERT, Mamba, and Llama) with architectural modifications were fine-tuned and evaluated.

**Key Contributions:**

	1. Evaluation of transformer modifications for long text processing
	2. Use of Kaggle ASAP 2.0 dataset for practical assessment
	3. Insights into the implications of truncation for essay evaluation

**Result:** The modified models showed improvements in processing longer essays without losing context compared to traditional truncation methods.

**Limitations:** The study primarily focuses on a specific dataset and may not generalize to all essay types or scoring rubrics.

**Conclusion:** Architectural modifications to transformers can enhance their ability to process long text inputs for more valid essay scoring.

**Abstract:** Transformer-based language models are architecturally constrained to process text of a fixed maximum length. Essays written by higher-grade students frequently exceed the maximum allowed length for many popular open-source models. A common approach to addressing this issue when using these models for Automated Essay Scoring is to truncate the input text. This raises serious validity concerns as it undermines the model's ability to fully capture and evaluate organizational elements of the scoring rubric, which requires long contexts to assess. In this study, we evaluate several models that incorporate architectural modifications of the standard transformer architecture to overcome these length limitations using the Kaggle ASAP 2.0 dataset. The models considered in this study include fine-tuned versions of XLNet, Longformer, ModernBERT, Mamba, and Llama models.

</details>


### [69] [RefactorCoderQA: Benchmarking LLMs for Multi-Domain Coding Question Solutions in Cloud and Edge Deployment](https://arxiv.org/abs/2509.10436)

*Shadikur Rahman, Aroosa Hameed, Gautam Srivastava, Syed Muhammad Danish*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-agent prompting, RefactorCoderQA

**Relevance Score:** 9

**TL;DR:** The paper proposes a cloud-edge collaborative architecture for enhancing the reasoning capabilities of Large Language Models (LLMs) through a structured multi-agent prompting framework.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for coding tasks are insufficient, leading to a need for a more effective evaluation method for LLMs in multi-domain coding scenarios.

**Method:** The proposed architecture consists of three main components: GuideLLM (lightweight model for guidance at the edge), SolverLLM (cloud-hosted model for generating solutions), and JudgeLLM (automated evaluator for assessing solutions). The effectiveness is evaluated using RefactorCoderQA, a new benchmark covering various technical domains with authentic coding challenges.

**Key Contributions:**

	1. Introduction of a multi-agent prompting framework for LLMs.
	2. Development of RefactorCoderQA benchmark for evaluating multi-domain coding tasks.
	3. State-of-the-art performance achieved by RefactorCoder-MoE compared to existing models.

**Result:** The fine-tuned model, RefactorCoder-MoE, achieves state-of-the-art performance on multi-domain coding tasks with an overall accuracy of 76.84%, outperforming existing open-source and commercial baselines.

**Limitations:** 

**Conclusion:** The proposed system not only enhances LLM reasoning capabilities but also demonstrates superior interpretability, accuracy, and practical relevance in coding challenges tested against extensive metrics including throughput and latency.

**Abstract:** To optimize the reasoning and problem-solving capabilities of Large Language Models (LLMs), we propose a novel cloud-edge collaborative architecture that enables a structured, multi-agent prompting framework. This framework comprises three specialized components: GuideLLM, a lightweight model deployed at the edge to provide methodological guidance; SolverLLM, a more powerful model hosted in the cloud responsible for generating code solutions; and JudgeLLM, an automated evaluator for assessing solution correctness and quality. To evaluate and demonstrate the effectiveness of this architecture in realistic settings, we introduce RefactorCoderQA, a comprehensive benchmark designed to evaluate and enhance the performance of Large Language Models (LLMs) across multi-domain coding tasks. Motivated by the limitations of existing benchmarks, RefactorCoderQA systematically covers various technical domains, including Software Engineering, Data Science, Machine Learning, and Natural Language Processing, using authentic coding challenges from Stack Overflow. Extensive experiments reveal that our fine-tuned model, RefactorCoder-MoE, achieves state-of-the-art performance, significantly outperforming leading open-source and commercial baselines with an overall accuracy of 76.84%. Human evaluations further validate the interpretability, accuracy, and practical relevance of the generated solutions. In addition, we evaluate system-level metrics, such as throughput and latency, to gain deeper insights into the performance characteristics and trade-offs of the proposed architecture.

</details>


### [70] [DeepDive: Advancing Deep Search Agents with Knowledge Graphs and Multi-Turn RL](https://arxiv.org/abs/2509.10446)

*Rui Lu, Zhenyu Hou, Zihan Wang, Hanchen Zhang, Xiao Liu, Yujiang Li, Shi Feng, Jie Tang, Yuxiao Dong*

**Main category:** cs.CL

**Keywords:** large language models, deep search agents, reinforcement learning

**Relevance Score:** 8

**TL;DR:** DeepDive enhances large language models (LLMs) for deep search tasks by synthesizing complex questions and employing multi-turn reinforcement learning to improve long-horizon reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Open LLMs struggle with long-horizon reasoning and lack sufficient supervised data for complex searches.

**Method:** DeepDive synthesizes complex questions from open knowledge graphs and utilizes end-to-end multi-turn reinforcement learning to improve LLM reasoning capabilities.

**Key Contributions:**

	1. Automatic synthesis of difficult questions from knowledge graphs
	2. End-to-end multi-turn reinforcement learning for LLMs
	3. Public availability of datasets, models, and code

**Result:** DeepDive-32B achieves competitive results on BrowseComp, surpassing WebSailor and others, with improvements noted in deep search ability across benchmarks.

**Limitations:** 

**Conclusion:** DeepDive facilitates better tool use and sampling during testing, with all resources provided publicly.

**Abstract:** Augmenting large language models (LLMs) with browsing tools substantially improves their potential as deep search agents to solve complex, real-world tasks. Yet, open LLMs still perform poorly in such settings due to limited long-horizon reasoning capacity with browsing tools and the lack of sufficiently difficult supervised data. To address these challenges, we present DeepDive to advance deep search agents. First, we propose a strategy to automatically synthesize complex, difficult, and hard-to-find questions from open knowledge graphs. Second, we apply end-to-end multi-turn reinforcement learning (RL) to enhance LLMs' long-horizon reasoning with deep search. Experiments show that DeepDive-32B achieves a new open-source competitive result on BrowseComp, outperforming WebSailor, DeepSeek-R1-Browse, and Search-o1. We demonstrate that multi-turn RL training improves deep search ability and significantly contributes to the performance improvements across multiple benchmarks. We observe that DeepDive enables test-time scaling of tool calls and parallel sampling. All datasets, models, and code are publicly available at https://github.com/THUDM/DeepDive.

</details>


### [71] [WhisTLE: Deeply Supervised, Text-Only Domain Adaptation for Pretrained Speech Recognition Transformers](https://arxiv.org/abs/2509.10452)

*Akshat Pandey, Karun Kumar, Raphael Tang*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, text-only adaptation, variational autoencoder, domain adaptation, word error rate

**Relevance Score:** 4

**TL;DR:** WhisTLE is a text-only adaptation method for ASR models that reduces word error rates by leveraging a variational autoencoder.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Pretrained ASR models excel but require adaptation for unseen vocabulary and parlance, especially in scenarios where collecting speech data is impractical.

**Method:** WhisTLE employs a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder with the learned text-to-latent encoder, optionally using TTS adaptation.

**Key Contributions:**

	1. Introduction of WhisTLE for ASR adaptation
	2. Effective use of VAE for encoder outputs
	3. Demonstrated significant WER reductions over existing methods

**Result:** WhisTLE coupled with TTS adaptation lowers word error rate by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 out of 32 scenarios.

**Limitations:** 

**Conclusion:** WhisTLE effectively improves ASR model performance with minimal runtime cost, demonstrating the potential of text-only adaptation techniques.

**Abstract:** Pretrained automatic speech recognition (ASR) models such as Whisper perform well but still need domain adaptation to handle unseen vocabulary and parlance. In many real-world settings, collecting speech data is impractical, necessitating text-only adaptation. We propose WhisTLE, a deeply supervised, text-only adaptation method for pretrained encoder-decoder ASR models. WhisTLE trains a variational autoencoder (VAE) to model encoder outputs from text and fine-tunes the decoder using the learned text-to-latent encoder, optionally combined with text-to-speech (TTS) adaptation. At inference, the original encoder is restored, incurring no extra runtime cost. Across four out-of-domain datasets and four ASR models, WhisTLE with TTS reduces word error rate (WER) by 12.3% relative to TTS-only adaptation and outperforms all non-WhisTLE baselines in 27 of 32 scenarios.

</details>


### [72] [Slaves to the Law of Large Numbers: An Asymptotic Equipartition Property for Perplexity in Generative Language Models](https://arxiv.org/abs/2405.13798)

*Tyler Bell, Avinash Mudireddy, Ivan Johnson-Eversoll, Soura Dasgupta, Raghu Mudumbai*

**Main category:** cs.CL

**Keywords:** language models, perplexity, text generation, synthetic texts, typical set

**Relevance Score:** 6

**TL;DR:** The paper proves a new property regarding the perplexity of long texts from language models and its implications for text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the behavior and constraints of language models regarding text generation, aiming to refine the understanding of typical sets in language outputs.

**Method:** The authors derive an asymptotic un-equipartition property for logarithmic perplexity of large texts, analyzing the entropy of token distributions and refining the concept of typical sets to encompass only grammatically correct texts.

**Key Contributions:**

	1. Introduces a refined typical set concept for grammatically correct texts.
	2. Demonstrates strong constraints on language model outputs based on token distribution entropy.
	3. Discusses applications for detecting synthetic texts and membership inference.

**Result:** The key finding is that the refined typical set of grammatically correct texts is a minuscule subset of all possible correct texts, indicating significant constraints on language model outputs.

**Limitations:** 

**Conclusion:** Language models have limited possible behaviors, which has implications for applications like detecting synthetic texts and assessing dataset membership.

**Abstract:** We prove a new asymptotic un-equipartition property for the perplexity of long texts generated by a language model and present supporting experimental evidence from open-source models. Specifically we show that the logarithmic perplexity of any large text generated by a language model must asymptotically converge to the average entropy of its token distributions. This defines a ``typical set'' that all long synthetic texts generated by a language model must belong to. We refine the concept of ''typical set'' to include only grammatically correct texts. We then show that this refined typical set is a vanishingly small subset of all possible grammatically correct texts for a very general definition of grammar. This means that language models are strongly constrained in the range of their possible behaviors and outputs. We make no simplifying assumptions (such as stationarity) about the statistics of language model outputs, and therefore our results are directly applicable to practical real-world models without any approximations. We discuss possible applications of the typical set concept to problems such as detecting synthetic texts and membership inference in training datasets.

</details>


### [73] [Direct Judgement Preference Optimization](https://arxiv.org/abs/2409.14664)

*Peifeng Wang, Austin Xu, Yilun Zhou, Caiming Xiong, Shafiq Joty*

**Main category:** cs.CL

**Keywords:** large language models, auto-evaluation, preference optimization, generative judges, model assessment

**Relevance Score:** 9

**TL;DR:** This paper investigates training large language models (LLMs) as generative judges to enhance evaluation capabilities across various use cases through preference optimization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Auto-evaluation is essential for assessing response quality and providing feedback for model development in LLMs.

**Method:** The authors propose learning from both positive and negative data using preference optimization and employ three distinct approaches to collect preference pairs for improving generative judges.

**Key Contributions:**

	1. Introduces a method for training LLM judges using preference optimization from both positive and negative datasets.
	2. Demonstrates superior performance against baseline models on a wide array of benchmarks.
	3. Shows adaptability to various evaluation protocols while providing actionable language feedback.

**Result:** The generative judge achieves the best performance on 10 out of 13 benchmarks, outperforming models like GPT-4o and specialized judge models.

**Limitations:** 

**Conclusion:** The model effectively counters biases, adapts to diverse evaluation protocols, and offers valuable feedback for downstream generators.

**Abstract:** Auto-evaluation is crucial for assessing response quality and offering feedback for model development. Recent studies have explored training large language models (LLMs) as generative judges to evaluate and critique other models' outputs. In this work, we investigate the idea of learning from both positive and negative data with preference optimization to enhance the evaluation capabilities of LLM judges across an array of different use cases. We achieve this by employing three approaches to collect the preference pairs for different use cases, each aimed at improving our generative judge from a different perspective. Our comprehensive study over a wide range of benchmarks demonstrates the effectiveness of our method. In particular, our generative judge achieves the best performance on 10 out of 13 benchmarks, outperforming strong baselines like GPT-4o and specialized judge models. Further analysis show that our judge model robustly counters inherent biases such as position and length bias, flexibly adapts to any evaluation protocol specified by practitioners, and provides helpful language feedback for improving downstream generator models.

</details>


### [74] [Atomic Fact Decomposition Helps Attributed Question Answering](https://arxiv.org/abs/2410.16708)

*Zhichao Yan, Jiapu Wang, Jiaoyan Chen, Xiaoli Li, Ru Li, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Attributed Question Answering, Large Language Models, Knowledge Graphs, Evidence Attribution, Atomic Fact Decomposition

**Relevance Score:** 8

**TL;DR:** This paper presents the Atomic fact decomposition-based Retrieval and Editing (ARE) framework for improving Attributed Question Answering (AQA) by decomposing long-form answers into atomic facts and enhancing evidence retrieval and editing processes using instruction-tuned LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in Attributed Question Answering (AQA), specifically the issues with Retrieval-Then-Read and post-hoc retrieval methods, particularly regarding irrelevant information and long-form answer complexity.

**Method:** The ARE framework decomposes long answers into molecular clauses and atomic facts using instruction-tuned LLMs that are fine-tuned on a dataset derived from Knowledge Graphs. It utilizes a search engine to retrieve related evidence, and an LLM-based verifier assesses the need for fact expansion or editing.

**Key Contributions:**

	1. Introduction of the ARE framework for AQA
	2. Decomposition of answers into atomic facts
	3. Introduction of the $Attr_{p}$ metric for evidence attribution precision

**Result:** Extensive evaluations indicate that the ARE framework outperforms state-of-the-art methods in AQA across various datasets, introducing a new metric, $Attr_{p}$, for measuring evidence attribution precision.

**Limitations:** The framework may still face challenges in fully accounting for highly dynamic information and the complexity of certain logical constructs.

**Conclusion:** The ARE framework enhances the reliability of Attributed Question Answering by effectively managing the retrieval and editing of relevant information, ensuring that the original intent of answers is preserved while improving their quality.

**Abstract:** Attributed Question Answering (AQA) aims to provide both a trustworthy answer and a reliable attribution report for a given question. Retrieval is a widely adopted approach, including two general paradigms: Retrieval-Then-Read (RTR) and post-hoc retrieval. Recently, Large Language Models (LLMs) have shown remarkable proficiency, prompting growing interest in AQA among researchers. However, RTR-based AQA often suffers from irrelevant knowledge and rapidly changing information, even when LLMs are adopted, while post-hoc retrieval-based AQA struggles with comprehending long-form answers with complex logic, and precisely identifying the content needing revision and preserving the original intent. To tackle these problems, this paper proposes an Atomic fact decomposition-based Retrieval and Editing (ARE) framework, which decomposes the generated long-form answers into molecular clauses and atomic facts by the instruction-tuned LLMs. Notably, the instruction-tuned LLMs are fine-tuned using a well-constructed dataset, generated from large scale Knowledge Graphs (KGs). This process involves extracting one-hop neighbors from a given set of entities and transforming the result into coherent long-form text. Subsequently, ARE leverages a search engine to retrieve evidences related to atomic facts, inputting these evidences into an LLM-based verifier to determine whether the facts require expansion for re-retrieval or editing. Furthermore, the edited facts are backtracked into the original answer, with evidence aggregated based on the relationship between molecular clauses and atomic facts. Extensive evaluations demonstrate the superior performance of our proposed method over the state-of-the-arts on several datasets, with an additionally proposed new metric $Attr_{p}$ for evaluating the precision of evidence attribution.

</details>


### [75] [Are LLMs Better than Reported? Detecting Label Errors and Mitigating Their Effect on Model Performance](https://arxiv.org/abs/2410.18889)

*Omer Nahum, Nitay Calderon, Orgad Keller, Idan Szpektor, Roi Reichart*

**Main category:** cs.CL

**Keywords:** NLP, large language models, annotation, label quality, dataset evaluation

**Relevance Score:** 9

**TL;DR:** The paper evaluates the potential of using large language models (LLMs) to improve label quality in NLP datasets by identifying mislabeled examples and compares LLM-based annotations with expert and crowd-sourced ones.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for larger datasets in NLP is growing, yet expert annotation is costly and doesn't scale well, while crowd-sourced annotation often lacks precision. This work explores how LLMs can enhance annotation quality by flagging label errors.

**Method:** The study employs an ensemble of LLMs to assess label accuracy across four factual consistency datasets and SummEval, comparing agreement and quality of annotations from experts, crowds, and LLMs.

**Key Contributions:**

	1. Leveraging LLMs as judges for annotation quality
	2. Empirical comparison of annotation methods
	3. Identifying the impact of label errors on model performance

**Result:** The analysis found a substantial number of label errors across datasets, which, if corrected, significantly improve reported performance metrics, suggesting many model errors stem from labeling issues rather than actual model failures.

**Limitations:** The study focuses only on selected datasets and may not generalize across all NLP tasks; further research needed to explore LLM annotation in a wider context.

**Conclusion:** The paper highlights the critical nature of accurate labeling in NLP and suggests that addressing mislabeled data can greatly enhance model performance.

**Abstract:** NLP benchmarks rely on standardized datasets for training and evaluating models and are crucial for advancing the field. Traditionally, expert annotations ensure high-quality labels; however, the cost of expert annotation does not scale well with the growing demand for larger datasets required by modern models. While crowd-sourcing provides a more scalable solution, it often comes at the expense of annotation precision and consistency. Recent advancements in large language models (LLMs) offer new opportunities to enhance the annotation process, particularly for detecting label errors in existing datasets. In this work, we consider the recent approach of LLM-as-a-judge, leveraging an ensemble of LLMs to flag potentially mislabeled examples. We conduct a case study on four factual consistency datasets from the TRUE benchmark, spanning diverse NLP tasks, and on SummEval, which uses Likert-scale ratings of summary quality across multiple dimensions. We empirically analyze the labeling quality of existing datasets and compare expert, crowd-sourced, and LLM-based annotations in terms of the agreement, label quality, and efficiency, demonstrating the strengths and limitations of each annotation method. Our findings reveal a substantial number of label errors, which, when corrected, induce a significant upward shift in reported model performance. This suggests that many of the LLMs' so-called mistakes are due to label errors rather than genuine model failures. Additionally, we discuss the implications of mislabeled data and propose methods to mitigate them in training to improve performance.

</details>


### [76] [Polish-English medical knowledge transfer: A new benchmark and results](https://arxiv.org/abs/2412.00559)

*Łukasz Grzybowski, Jakub Pokrywka, Michał Ciesiółka, Jeremi I. Kaczmarek, Marek Kubis*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical exams, cross-lingual translation, benchmark dataset, ethical considerations

**Relevance Score:** 8

**TL;DR:** This paper introduces a benchmark dataset of Polish medical exams to evaluate the performance of Large Language Models (LLMs) in medical problem-solving across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research on LLMs in non-English contexts, particularly in medical applications, and to evaluate their effectiveness in processing Polish medical exams.

**Method:** The study involves web-scraping over 24,000 medical exam questions from Polish medical licensing and specialization exams, creating a structured dataset, and evaluating various LLMs against human medical student performance.

**Key Contributions:**

	1. Creation of a novel benchmark dataset for Polish medical exams
	2. Systematic evaluation of LLM performance in medical contexts
	3. Insights into cross-lingual and domain-specific challenges for LLMs

**Result:** Evaluation indicates that while models like GPT-4o perform nearly at human levels, notable challenges remain in cross-lingual translation and understanding specific medical domains.

**Limitations:** The study primarily focuses on Polish language and may not generalize to other languages or contexts.

**Conclusion:** The findings highlight the limitations of LLMs in certain languages and specialties, raising ethical considerations for their clinical use.

**Abstract:** Large Language Models (LLMs) have demonstrated significant potential in handling specialized tasks, including medical problem-solving. However, most studies predominantly focus on English-language contexts. This study introduces a novel benchmark dataset based on Polish medical licensing and specialization exams (LEK, LDEK, PES) taken by medical doctor candidates and practicing doctors pursuing specialization. The dataset was web-scraped from publicly available resources provided by the Medical Examination Center and the Chief Medical Chamber. It comprises over 24,000 exam questions, including a subset of parallel Polish-English corpora, where the English portion was professionally translated by the examination center for foreign candidates. By creating a structured benchmark from these existing exam questions, we systematically evaluate state-of-the-art LLMs, including general-purpose, domain-specific, and Polish-specific models, and compare their performance against human medical students. Our analysis reveals that while models like GPT-4o achieve near-human performance, significant challenges persist in cross-lingual translation and domain-specific understanding. These findings underscore disparities in model performance across languages and medical specialties, highlighting the limitations and ethical considerations of deploying LLMs in clinical practice.

</details>


### [77] [A 2-step Framework for Automated Literary Translation Evaluation: Its Promises and Pitfalls](https://arxiv.org/abs/2412.01340)

*Sheikh Shafayat, Dongkeun Yoon, Woori Jang, Jiwoo Choi, Alice Oh, Seohyon Jung*

**Main category:** cs.CL

**Keywords:** machine translation, literary translation, evaluation metrics, human judgment, Korean

**Relevance Score:** 4

**TL;DR:** This paper proposes a two-stage pipeline for evaluating literary machine translation from English to Korean, achieving better correlation with human judgment than traditional methods but falling short of inter-human agreement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a more effective evaluation framework for literary translation that aligns closely with human judgment and cultural nuances.

**Method:** The authors developed a two-stage pipeline specifically for assessing literary machine translation, incorporating fine-grained metrics that emphasize interpretability and cultural sensitivity.

**Key Contributions:**

	1. Proposed a two-stage pipeline for evaluating literary machine translation.
	2. Demonstrated improved correlation with human judgment over traditional metrics.
	3. Identified the need for culturally sensitive evaluation methods in literary translation.

**Result:** The proposed framework shows a higher correlation with human judgment compared to traditional machine translation metrics, highlighting the need for better evaluation methods in literary contexts.

**Limitations:** The new metrics fail to match inter-human agreement, particularly for complex features like Korean Honorifics.

**Conclusion:** While the new approach offers improvements, it still does not achieve the level of agreement found among human evaluators, indicating a need for ongoing refinement in evaluation methodologies for machine translation.

**Abstract:** In this work, we propose and evaluate the feasibility of a two-stage pipeline to evaluate literary machine translation, in a fine-grained manner, from English to Korean. The results show that our framework provides fine-grained, interpretable metrics suited for literary translation and obtains a higher correlation with human judgment than traditional machine translation metrics. Nonetheless, it still fails to match inter-human agreement, especially in metrics like Korean Honorifics. We also observe that LLMs tend to favor translations generated by other LLMs, and we highlight the necessity of developing more sophisticated evaluation methods to ensure accurate and culturally sensitive machine translation of literary works.

</details>


### [78] [Tokens, the oft-overlooked appetizer: Large language models, the distributional hypothesis, and meaning](https://arxiv.org/abs/2412.10924)

*Julia Witte Zimmerman, Denis Hudon, Kathryn Cramer, Alejandro J. Ruiz, Calla Beauregard, Ashley Fehr, Mikaela Irene Fudolig, Bradford Demarest, Yoshi Meke Bird, Milo Z. Trujillo, Christopher M. Danforth, Peter Sheridan Dodds*

**Main category:** cs.CL

**Keywords:** tokenization, large language models, Distributional Hypothesis, semantic primitives, bias in AI

**Relevance Score:** 9

**TL;DR:** This paper discusses the impact of tokenization on language models, arguing for a reevaluation of current tokenization techniques based on the Distributional Hypothesis for improved human-like language performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the often-overlooked significance of tokenization in language models, particularly regarding its role in human-like cognition and the emergence of semantic units.

**Method:** The authors analyze tokenizations from a BPE tokenizer and model vocabularies from Hugging Face and tiktoken, exploring token vectors in a RoBERTa model.

**Key Contributions:**

	1. Reevaluation of tokenization techniques based on the Distributional Hypothesis.
	2. Examination of BPE tokenizers and model vocabularies and their impact on LLMs.
	3. Identification of tokenization as a means of introducing bias into language models.

**Result:** The study finds that current tokenization practices create sub-optimal semantic units and can lead to biases, impacting the model's ability to access necessary distributional patterns.

**Limitations:** The paper does not provide extensive experimental results to support all claims, focusing primarily on theoretical implications.

**Conclusion:** The paper concludes that the design of tokenization algorithms affects the cognition of LLMs and calls for improved methods to align tokens with meaningful language structures.

**Abstract:** Tokenization is a necessary component within the current architecture of many language models, including the transformer-based large language models (LLMs) of Generative AI, yet its impact on the model's cognition is often overlooked. We argue that LLMs demonstrate that the Distributional Hypothesis (DH) is sufficient for reasonably human-like language performance, and that the emergence of human-meaningful linguistic units among tokens and current structural constraints motivate changes to existing, linguistically-agnostic tokenization techniques, particularly with respect to their roles as (1) semantic primitives and as (2) vehicles for conveying salient distributional patterns from human language to the model. We explore tokenizations from a BPE tokenizer; extant model vocabularies obtained from Hugging Face and tiktoken; and the information in exemplar token vectors as they move through the layers of a RoBERTa (large) model. Besides creating sub-optimal semantic building blocks and obscuring the model's access to the necessary distributional patterns, we describe how tokens and pretraining can act as a backdoor for bias and other unwanted content, which current alignment practices may not remediate. Additionally, we relay evidence that the tokenization algorithm's objective function impacts the LLM's cognition, despite being arguably meaningfully insulated from the main system intelligence. [First uploaded to arXiv in December, 2024.]

</details>


### [79] [FinMTEB: Finance Massive Text Embedding Benchmark](https://arxiv.org/abs/2502.10990)

*Yixuan Tang, Yi Yang*

**Main category:** cs.CL

**Keywords:** NLP, Embedding Models, Finance, Benchmarking, Domain-specific

**Relevance Score:** 4

**TL;DR:** This paper introduces the Finance Massive Text Embedding Benchmark (FinMTEB), a new framework for evaluating embedding models specifically in the financial domain, revealing key findings on the limitations of general models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of specialized evaluation for embedding models in financial applications, which differ significantly from general-purpose benchmarks.

**Method:** The study develops FinMTEB comprising 64 financial embedding datasets over 7 tasks, evaluates 15 embedding models including a specialized model Fin-E5, and employs domain-adapted methods for training.

**Key Contributions:**

	1. Introduction of FinMTEB for financial NLP embedding evaluation
	2. Development of domain-adapted model Fin-E5
	3. Findings on the performance discrepancies between general and domain-specific models

**Result:** The evaluation reveals that domain-adapted models outperform general-purpose models, a Bag-of-Words approach surprisingly outperforms dense embeddings in some tasks, and there is limited correlation between general and financial benchmarks.

**Limitations:** 

**Conclusion:** The paper establishes FinMTEB as a crucial tool for evaluating financial NLP models and provides insights into the development of more effective domain-specific embeddings.

**Abstract:** Embedding models play a crucial role in representing and retrieving information across various NLP applications. Recent advances in large language models (LLMs) have further enhanced the performance of embedding models. While these models are often benchmarked on general-purpose datasets, real-world applications demand domain-specific evaluation. In this work, we introduce the Finance Massive Text Embedding Benchmark (FinMTEB), a specialized counterpart to MTEB designed for the financial domain. FinMTEB comprises 64 financial domain-specific embedding datasets across 7 tasks that cover diverse textual types in both Chinese and English, such as financial news articles, corporate annual reports, ESG reports, regulatory filings, and earnings call transcripts. We also develop a finance-adapted model, Fin-E5, using a persona-based data synthetic method to cover diverse financial embedding tasks for training. Through extensive evaluation of 15 embedding models, including Fin-E5, we show three key findings: (1) performance on general-purpose benchmarks shows limited correlation with financial domain tasks; (2) domain-adapted models consistently outperform their general-purpose counterparts; and (3) surprisingly, a simple Bag-of-Words (BoW) approach outperforms sophisticated dense embeddings in financial Semantic Textual Similarity (STS) tasks, underscoring current limitations in dense embedding techniques. Our work establishes a robust evaluation framework for financial NLP applications and provides crucial insights for developing domain-specific embedding models.

</details>
