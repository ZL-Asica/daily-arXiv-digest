# 2025-05-21

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 206]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Sight, Sound and Smell in Immersive Experiences of Urban History: Virtual Vauxhall Gardens Case Study](https://arxiv.org/abs/2505.13612)

*Tim Pearce, David Souto, Douglas Barrett, Benjamin Lok, Mateusz Bocian, Artur Soczawa-Stronczyk, Giasemi Vavoula, Paul Long, Avinash Bhangaonkar, Stephanie Bowry, Michaela Butter, David Coke, Kate Loveman, Rosemary Sweet, Lars Tharp, Jeremy Webster, Hongji Yang, Robin Green, Andrew Hugill*

**Main category:** cs.HC

**Keywords:** Virtual Reality, multisensory elements, olfaction, user engagement, cultural heritage

**Relevance Score:** 4

**TL;DR:** This paper investigates the integration of olfactory elements in virtual reality (VR) reconstructions of historical spaces, focusing on enhancing user engagement in cultural heritage experiences through multisensory stimuli.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance the immersion of digital heritage experiences by incorporating olfactory stimuli, which have been underexplored despite their strong connection to memory and emotional engagement.

**Method:** A case study of the Virtual Vauxhall Gardens project, which involved developing a networked portable olfactory display to synchronize scents with visual and auditory components in a VR experience.

**Key Contributions:**

	1. Integration of olfactory stimuli into VR historical reconstructions
	2. Development of a networked portable olfactory display
	3. Evaluation of multisensory user experience metrics

**Result:** The integration of synchronized olfactory stimuli into the VR experience was found to enhance user engagement and was positively received by participants, indicating a unique immersive encounter with the historical setting.

**Limitations:** Standard usability metrics may be ill-suited for evaluating multisensory experiences.

**Conclusion:** Evaluating multisensory VR heritage experiences requires a nuanced approach, suggesting that traditional usability metrics may not fully capture user experience and that creating evocative and emotionally resonant experiences is paramount.

**Abstract:** We explore the integration of multisensory elements in virtual reality reconstructions of historical spaces through a case study of the Virtual Vauxhall Gardens project. While visual and auditory components have become standard in digital heritage experiences, the addition of olfactory stimuli remains underexplored, despite its powerful connection to memory and emotional engagement. This research investigates how multisensory experiences involving olfaction can be effectively integrated into VR reconstructions of historical spaces to enhance presence and engagement with cultural heritage. In the context of a VR reconstruction of London's eighteenth-century Vauxhall Pleasure Gardens, we developed a networked portable olfactory display capable of synchronizing specific scents with visual and auditory elements at pivotal moments in the virtual experience. Our evaluation methodology assesses both technical implementation and user experience, measuring presence, and usability metrics across diverse participant groups. Our results show that integrating synchronized olfactory stimuli into the VR experience can enhance user engagement and be perceived positively, contributing to a unique and immersive encounter with historical settings. While presence questionnaires indicated a strong sense of auditory presence and control, with other sensory factors rated moderately, user experience of attractiveness was exceptionally high; qualitative feedback suggested heightened sensory awareness and engagement influenced by the inclusion and anticipation of smell. Our results suggest that evaluating multisensory VR heritage experiences requires a nuanced approach, as standard usability metrics may be ill-suited and 'realism' might be less critical than creating an evocative, historically informed, and emotionally resonant experience......

</details>


### [2] [Conceptual Modeling: Topics, Themes, and Technology Trends](https://arxiv.org/abs/2505.13648)

*V. C. Storey, R. Lukyanenko, A. Castellanos*

**Main category:** cs.HC

**Keywords:** conceptual modeling, digitalization, emerging technologies

**Relevance Score:** 3

**TL;DR:** This paper surveys conceptual modeling research over five decades, analyzing its evolution and impact on emerging technologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the evolution of conceptual modeling in response to digitalization and emerging technologies.

**Method:** Survey of over 5,300 papers from 35 multidisciplinary journals and conferences covering conceptual modeling topics from the 1970s to the present.

**Key Contributions:**

	1. Comprehensive survey of over 5,300 conceptual modeling papers
	2. Analysis of trends over five decades in relation to emerging technologies
	3. Discussion on the future role of conceptual modeling in digitalization

**Result:** Identified evolving topics and trends in conceptual modeling to accommodate new technologies while retaining foundational constructs.

**Limitations:** 

**Conclusion:** Conceptual modeling is crucial for future digital developments, and further research directions are proposed.

**Abstract:** Conceptual modeling is an important part of information systems development and use that involves identifying and representing relevant aspects of reality. Although the past decades have experienced continuous digitalization of services and products that impact business and society, conceptual modeling efforts are still required to support new technologies as they emerge. This paper surveys research on conceptual modeling over the past five decades and shows how its topics and trends continue to evolve to accommodate emerging technologies, while remaining grounded in basic constructs. We survey over 5,300 papers that address conceptual modeling topics from the 1970s to the present, which are collected from 35 multidisciplinary journals and conferences, and use them as the basis from which to analyze the progression of conceptual modeling. The important role that conceptual modeling should play in our evolving digital world is discussed, and future research directions proposed.

</details>


### [3] [Gaze-Enhanced Multimodal Turn-Taking Prediction in Triadic Conversations](https://arxiv.org/abs/2505.13688)

*Seongsil Heo, Calvin Murdock, Michael Proulx, Christi Miller*

**Main category:** cs.HC

**Keywords:** turn-taking prediction, gaze integration, triadic conversations, speech intelligibility, adaptive sound control

**Relevance Score:** 7

**TL;DR:** This study presents a lightweight framework for accurate turn-taking prediction in triadic conversations, integrating gaze with speaker localization to improve interaction quality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an effective method for turn-taking prediction that enhances conversational interactions while being computationally efficient.

**Method:** The study integrates gaze and speaker localization within a spatial constraint and leverages egocentric behavioral cues for prediction.

**Key Contributions:**

	1. Introduction of a lightweight framework for turn-taking prediction.
	2. Integration of gaze with speaker localization for improved predictive accuracy.
	3. Enhancement of speech intelligibility in noisy environments for hearing assistance.

**Result:** Incorporating gaze data significantly improves prediction performance, with multiple-user gaze data enhancing conversational dynamics.

**Limitations:** 

**Conclusion:** The proposed model offers a privacy-conscious solution for adaptive sound control, improving speech intelligibility for hearing assistance in smart glasses.

**Abstract:** Turn-taking prediction is crucial for seamless interactions. This study introduces a novel, lightweight framework for accurate turn-taking prediction in triadic conversations without relying on computationally intensive methods. Unlike prior approaches that either disregard gaze or treat it as a passive signal, our model integrates gaze with speaker localization, structuring it within a spatial constraint to transform it into a reliable predictive cue. Leveraging egocentric behavioral cues, our experiments demonstrate that incorporating gaze data from a single-user significantly improves prediction performance, while gaze data from multiple-users further enhances it by capturing richer conversational dynamics. This study presents a lightweight and privacy-conscious approach to support adaptive, directional sound control, enhancing speech intelligibility in noisy environments, particularly for hearing assistance in smart glasses.

</details>


### [4] [Human Authenticity and Flourishing in an AI-Driven World: Edmund's Journey and the Call for Mindfulness](https://arxiv.org/abs/2505.13953)

*Sebastian Zepf, Mark Colley*

**Main category:** cs.HC

**Keywords:** AI integration, Human-Computer Interaction, responsible AI, superpowers, future of AI

**Relevance Score:** 8

**TL;DR:** The paper discusses the integration of AI into human experiences, highlighting its potential benefits and risks through a futuristic narrative and exploration of AI-driven 'superpowers.'

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To reflect on the responsible integration of AI technologies into human experiences and challenge existing design methods in HCI.

**Method:** The paper uses a futuristic user journey and identifies four AI-based superpowers to illustrate the implications of AI on human abilities and experiences.

**Key Contributions:**

	1. Illustration of a futuristic user journey that highlights AI's impact on human life.
	2. Identification of four AI-driven superpowers and their implications for HCI.
	3. Proposal of a Human Flourishing Benchmark for responsible AI integration.

**Result:** The exploration reveals significant implications for HCI and AI, advocating for responsible development that preserves essential human capabilities while identifying meaningful applications of AI.

**Limitations:** 

**Conclusion:** The authors propose a Human Flourishing Benchmark as a framework for evaluating the impact of AI on human abilities and ensuring a responsible integration of AI technologies.

**Abstract:** Humans have always dreamed of possessing superpowers, and the rapid development of AI-based features promises to bring these dreams (closer) to reality. However, these advancements come with significant risks. This paper advocates for challenging existing methods and approaches in design and evaluation for more responsible AI. We stimulate reflection through a futuristic user journey illustrating the AI-driven life of Edmund in 2035. Subsequently, we discuss four AI-based superpowers: extended perception, cognitive offloading, externalized memory, and enhanced presence. We then discuss implications for HCI and AI, emphasizing the need for preserving intrinsic human superpowers, identifying meaningful use cases for AI, and evaluating AI's impact on human abilities. This paper advocates for responsible and reflective AI integration and proposes a pathway towards the idea of a Human Flourishing Benchmark.

</details>


### [5] [Reading.help: Supporting EFL Readers with Proactive and On-Demand Explanation of English Grammar and Semantics](https://arxiv.org/abs/2505.14031)

*Sunghyo Chung, Hyeon Jeon, Sungbok Shin, Md Naimul Hoque*

**Main category:** cs.HC

**Keywords:** EFL readers, LLM-based tool, self-learning, reading comprehension, English as a Foreign Language

**Relevance Score:** 7

**TL;DR:** An intelligent reading tool, Reading.help, assists English as a Foreign Language (EFL) readers by providing on-demand explanations to improve comprehension.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** EFL readers often struggle with reading English texts accurately and quickly, and existing support from teachers is limited and costly.

**Method:** Developed an LLM-based reading tool called Reading.help, revised it based on feedback from a case study with 15 EFL readers in South Korea, and evaluated the tool with both EFL readers and education professionals.

**Key Contributions:**

	1. Development of an LLM-based reading tool for EFL readers
	2. Conducted a case study to refine tool effectiveness
	3. Demonstrated potential for enhancing self-learning in EFL contexts

**Result:** The findings indicate that Reading.help could significantly aid EFL readers in self-learning English by improving their comprehension of complex sentences and paragraphs.

**Limitations:** Limited sample size and context-specific findings pertaining to South Korean EFL readers.

**Conclusion:** Reading.help shows promise as a supportive tool for EFL readers who lack external learning resources.

**Abstract:** A large portion of texts in the world is written in English, but readers who see English as a Foreign Language (EFL) often struggle to read texts written in English accurately and swiftly. In many countries, EFL readers seek help from professional teachers and mentors, which is limited and costly. In this paper, we explore how an intelligent reading tool can assist EFL readers. To support our research agenda, we conducted a case study with EFL readers in South Korea. We at first developed an LLM-based reading tool based on prior literature. We then revised the tool based on the feedback from a study with 15 South Korean EFL readers. The final tool, named Reading.help, helps EFL readers comprehend complex sentences and paragraphs with on-demand and proactive explanations. We finally evaluated the tool with 5 EFL readers and 2 EFL education professionals. Our findings suggest Reading.help could potentially help EFL readers self-learn english when they do not have access to any external support.

</details>


### [6] [Recreating Neural Activity During Speech Production with Language and Speech Model Embeddings](https://arxiv.org/abs/2505.14074)

*Owais Mujtaba Khanday, Pablo Rodroguez San Esteban, Zubair Ahmad Lone, Marc Ouellet, Jose Andres Gonzalez Lopez*

**Main category:** cs.HC

**Keywords:** neuroscience, speech production, language models

**Relevance Score:** 9

**TL;DR:** This study investigates the use of large-scale language and speech model embeddings to reconstruct neural activity recordings during speech production, demonstrating high correlation with true neural signals.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the challenge of understanding neural encoding of speech and language production, linking neuroscience with artificial intelligence.

**Method:** The study employs pre-trained embeddings from deep learning models on linguistic and acoustic data, mapping them onto neural signals and evaluating the reconstructions using correlation metrics and quality assessments.

**Key Contributions:**

	1. Innovative application of self-supervised embeddings to neural signal reconstruction
	2. Demonstration of high correlation between reconstructed and actual neural data
	3. Insights into the intersection of neuroscience and AI in language production

**Result:** Embeddings from language and speech models effectively reconstruct neural activity, achieving Pearson correlation coefficients between 0.79 and 0.99 across participants.

**Limitations:** 

**Conclusion:** The findings suggest that large-scale embeddings can accurately capture the spatio-temporal dynamics of brain activity related to speech production.

**Abstract:** Understanding how neural activity encodes speech and language production is a fundamental challenge in neuroscience and artificial intelligence. This study investigates whether embeddings from large-scale, self-supervised language and speech models can effectively reconstruct neural activity recordings captured during speech production. We leverage pre-trained embeddings from deep learning models trained on linguistic and acoustic data to represent high-level speech features and map them onto neural signals. We analyze the extent to which these embeddings preserve the spatio-temporal dynamics of brain activity. We evaluate reconstructed neural signals against ground truth recordings using correlation metrics and signal reconstruction quality assessments. The results indicate that neural activity can be effectively reconstructed using embeddings from large language and speech models across all study participants, yielding Pearson correlation coefficients ranging from 0.79 to 0.99.

</details>


### [7] [The Virtual Reality Koinos Method: Analyzing Virtual Reality Collaboration from the perspective of communication models](https://arxiv.org/abs/2505.14078)

*Eloise Minder, Sylvain Fleury, Solène Neyret, Jean-Rémy Chardonnet*

**Main category:** cs.HC

**Keywords:** Co-presence, Virtual Reality, Social Interaction, Koinos Method, Communication Models

**Relevance Score:** 7

**TL;DR:** This paper introduces the Koinos method to analyze co-presence in Virtual Reality through communication models and proposes an equation for predicting co-presence.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance social interactions in Virtual Reality that emulate the qualitative aspects of Face-to-Face interactions.

**Method:** The paper utilizes the Koinos method, exploring theoretical communication models and conducting two VR experiments to assess variations in social and physical representations of virtual partners.

**Key Contributions:**

	1. Introduction of the Koinos method for understanding co-presence in VR
	2. Conducting two experimental analyses to assess social interaction dynamics
	3. Proposing a predictive equation for enhancing co-presence in VR environments.

**Result:** The research leads to the proposal of a predictive equation for managing the sense of co-presence in VR environments.

**Limitations:** 

**Conclusion:** By understanding the factors influencing co-presence, VR experiences can be designed to foster more meaningful social interactions.

**Abstract:** Understanding which factors could influence co-presence in Virtual Reality could help develop more qualitative social interactions, or social interactions that generate similar sensations, emotions and feelings than the ones generated during Face-to-Face interactions. Co-presence is studied since the beginning of Virtual Reality (VR); though, no consensus is identified on what factors could influence it, except the consensus on the definition of "being there together" inside the Virtual Environment. In this paper, we introduce the Koinos method to explain social interactions in VR through communication models, (i) theoretically, and (ii) on two VR experiments that change the virtual partner social and physical representations. These analyses lead us to propose an equation to predict and help manage the sense of co-presence in VR.

</details>


### [8] [Human and Machine as Seen at the Co-Creation Age: A Co-Word Analysis in Human Machine Co-creation (2014-2024)](https://arxiv.org/abs/2505.14363)

*Mengyao Guo, Jinda Han, Ze Gao, Yuan Zhuang, Xingting Wu*

**Main category:** cs.HC

**Keywords:** human-machine co-creation, collaborative partners, creativity, innovation, ACM CHI

**Relevance Score:** 8

**TL;DR:** This paper examines the transition from treating machines as tools to viewing them as collaborators in human-machine co-creation, analyzing trends and implications in the field from 2014 to 2024.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the shift in the perception of machines from tools to collaborative partners and its implications for creativity and innovation.

**Method:** Co-word analysis is employed to identify emerging trends and central themes in the context of human-machine co-creation.

**Key Contributions:**

	1. Identification of emerging trends in human-machine co-creation
	2. Insights into the collaborative role of machines in creative processes
	3. Implications for future research and societal impact

**Result:** The study reveals significant trends and intellectual developments in human-machine co-creation, emphasizing its evolving role in creativity and societal impact.

**Limitations:** 

**Conclusion:** Fostering a more inclusive and effective approach to human-machine interaction can enhance creativity and innovation across various domains.

**Abstract:** This paper explores the evolving landscape of human-machine co-creation, focusing on its development in the context of the ACM Conference on Human Factors in Computing Systems (CHI) from 2014 to 2024. We employ co-word analysis to identify emerging trends, central themes, and the intellectual trajectory of this field. The study highlights the shift from viewing machines as mere tools to recognizing them as collaborative partners in creative processes. By understanding these dynamics, we aim to provide insights into the implications of this paradigm shift for creativity, innovation, and societal impact, ultimately fostering a more inclusive and effective approach to human-machine interaction in various domains.

</details>


### [9] [What Does Success Look Like? Catalyzing Meeting Intentionality with AI-Assisted Prospective Reflection](https://arxiv.org/abs/2505.14370)

*Ava Elizabeth Scott, Lev Tankelevitch, Payod Panda, Rishi Vanukuru, Xinyue Chen, Sean Rintel*

**Main category:** cs.HC

**Keywords:** HCI, Meeting Science, Generative AI, prospective reflection, collaboration

**Relevance Score:** 9

**TL;DR:** A study on the design and impact of a Meeting Purpose Assistant (MPA) that uses Generative AI to help users reflect on the purpose and challenges of meetings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite significant research in HCI and Meeting Science, ineffective meetings remain common due to a lack of technology that supports prospective reflection on meeting needs and outcomes.

**Method:** The study used a participatory prompting methodology with 18 employees from a global tech company who interacted with the MPA to reflect on their upcoming meetings.

**Key Contributions:**

	1. Introduction of the Meeting Purpose Assistant (MPA) to facilitate meeting reflection.
	2. Demonstrated observable impacts on meeting preparation and communication.
	3. Identified barriers to the effective use of AI in meeting contexts.

**Result:** The MPA improved users' ability to clarify meeting purposes, adapt perspectives, enhance preparation, and suggest plan changes, while also revealing barriers to its use.

**Limitations:** Barriers related to social, temporal, and technological factors in utilizing the MPA were noted.

**Conclusion:** The research emphasizes the need for AI-assisted support in meeting reflection and provides design considerations for similar technologies.

**Abstract:** Despite decades of HCI and Meeting Science research, complaints about ineffective meetings are still pervasive. We argue that meeting technologies lack support for prospective reflection, that is, thinking about why a meeting is needed and what might happen. To explore this, we designed a Meeting Purpose Assistant (MPA) technology probe to coach users to articulate their meeting's purpose and challenges, and act accordingly. The MPA used Generative AI to support personalized and actionable prospective reflection across the diversity of meeting contexts. Using a participatory prompting methodology, 18 employees of a global technology company reflected with the MPA on upcoming meetings. Observed impacts were: clarifying meeting purposes, challenges, and success conditions; changing perspectives and flexibility; improving preparation and communication; and proposing changed plans. We also identify perceived social, temporal, and technological barriers to using the MPA. We present system and workflow design considerations for developing AI-assisted reflection support for meetings.

</details>


### [10] [When Bias Backfires: The Modulatory Role of Counterfactual Explanations on the Adoption of Algorithmic Bias in XAI-Supported Human Decision-Making](https://arxiv.org/abs/2505.14377)

*Ulrike Kuhl, Annika Bush*

**Main category:** cs.HC

**Keywords:** artificial intelligence, human decision-making, bias, counterfactual explanations, explainable AI

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of biased AI recommendations on human hiring decisions, using controlled experiments to assess how counterfactual explanations can mitigate bias effects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how biased AI influences human decision-making in hiring processes and explore the role of counterfactual explanations in mitigating these biases.

**Method:** Conducted a controlled experiment simulating hiring decisions with participants making choices influenced by biased AI recommendations. The study involved three phases: baseline without AI, with biased AI recommendations, and a final phase without AI.

**Key Contributions:**

	1. Demonstrated significant influence of biased AI on human decision-making in hiring.
	2. Showed effectiveness of counterfactual explanations in reversing bias effects.
	3. Highlighted the need for careful calibration of XAI to prevent unwanted shifts in preferences.

**Result:** Participants followed biased AI recommendations 70% of the time. Only 8 out of 294 detected the bias. Counterfactual explanations reversed bias in independent decisions, while trust remained unchanged, suggesting nuanced effects of AI bias on cognitive confidence.

**Limitations:** The study was based on a simulated environment and may not fully represent real-world complexities in hiring contexts.

**Conclusion:** Counterfactual explanations are crucial in minimizing the adverse effects of biased AI on decision-making and promoting equitable judgment by mitigating algorithmic biases.

**Abstract:** Although the integration of artificial intelligence (AI) into everyday tasks improves efficiency and objectivity, it also risks transmitting bias to human decision-making. In this study, we conducted a controlled experiment that simulated hiring decisions to examine how biased AI recommendations - augmented with or without counterfactual explanations - influence human judgment over time. Participants, acting as hiring managers, completed 60 decision trials divided into a baseline phase without AI, followed by a phase with biased (X)AI recommendations (favoring either male or female candidates), and a final post-interaction phase without AI. Our results indicate that the participants followed the AI recommendations 70% of the time when the qualifications of the given candidates were comparable. Yet, only a fraction of participants detected the gender bias (8 out of 294). Crucially, exposure to biased AI altered participants' inherent preferences: in the post-interaction phase, participants' independent decisions aligned with the bias when no counterfactual explanations were provided before, but reversed the bias when explanations were given. Reported trust did not differ significantly across conditions. Confidence varied throughout the study phases after exposure to male-biased AI, indicating nuanced effects of AI bias on decision certainty. Our findings point to the importance of calibrating XAI to avoid unintended behavioral shifts in order to safeguard equitable decision-making and prevent the adoption of algorithmic bias.

</details>


### [11] [Two Empirical Studies on Audiovisual Semiotics of Uncertainty](https://arxiv.org/abs/2505.14379)

*Sita Vriend, David Hägele, Daniel Weiskopf*

**Main category:** cs.HC

**Keywords:** audiovisual representation, uncertainty visualization, sonification

**Relevance Score:** 7

**TL;DR:** This paper investigates the integration of visualization and sonification for better representation of uncertainty through audiovisual means.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of theoretical guidance in combining visualization and sonification, focusing on enhancing users' understanding of uncertainty.

**Method:** Two preregistered crowd-sourced user studies were conducted to assess audio/visual pair preferences and to explore audiovisual mappings of uncertainty based on probability.

**Key Contributions:**

	1. Investigated audiovisual semiotics for uncertainty representation
	2. Conducted two user studies on audio/visual pairs and mappings of uncertainty
	3. Provided guidelines for effective audiovisual mappings of uncertainty

**Result:** The studies revealed strong preferences and low reaction times for specific audiovisual mappings, but highlighted that preferred pairs were not necessarily effective for uncertainty representation.

**Limitations:** 

**Conclusion:** The paper provides guidelines for the effective use of audiovisual representations to improve the perception of uncertainty, emphasizing the importance of low reaction times and high user preferences.

**Abstract:** There exists limited theoretical guidance on integrating visualization and sonification. In this paper, we address this gap by investigating audiovisual semiotics for uncertainty representation: joining uncertainty visualization and sonification to combine audiovisual channels for enhancing users' perception of uncertainty. We conducted two preregistered crowd-sourced user studies. First, we assessed suitable audio/visual pairs. Then, we investigated audiovisual mappings of uncertainty. Here, we use probability as it is an easily communicated aspect of uncertainty. We analyzed the participants' preferences and reaction times in both user studies. Additionally, we explored the strategies employed by participants through qualitative analysis. Our results reveal audiovisual mappings that lead to particularly strong preferences and low reaction times. Furthermore, we found that preferred audio/visual pairs are not necessarily suitable audiovisual mappings of uncertainty. For example, while pitch paired with brightness was preferred as a pair, it was not well suited as a mapping for uncertainty. We recommend audiovisual mappings of uncertainty that lead to low reaction times and high preferences in both user studies. This paper presents guidelines to anyone seeking to employ audiovisual representations for uncertainty, contributing to enhancing the perception of uncertainty.

</details>


### [12] [How Managers Perceive AI-Assisted Conversational Training for Workplace Communication](https://arxiv.org/abs/2505.14452)

*Lance T Wilhelm, Xiaohan Ding, Kirk McInnis Knutsen, Buse Carik, Eugenia H Rho*

**Main category:** cs.HC

**Keywords:** AI-assisted training, communication skills, human-AI teaming

**Relevance Score:** 5

**TL;DR:** This paper investigates how managers envision using AI-assisted communication systems like CommCoach for training their communication skills through role-play simulations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in tailored and sustained training for managers to improve workplace communication skills using AI.

**Method:** Conducted semi-structured interviews with managers regarding their expectations and experiences with AI in communication training.

**Key Contributions:**

	1. Developed CommCoach as a conversational role-play system for managers.
	2. Highlighted the importance of adaptive, low-risk training environments.
	3. Identified key needs for AI in workplace communication, such as transparency and adaptability.

**Result:** Participants valued adaptive simulations for practicing challenging conversations, highlighting needs for human-AI collaboration and context-aware feedback.

**Limitations:** The study may not cover all perspectives on AI in communication training and is based on limited interviews.

**Conclusion:** Effective AI-assisted communication training requires balancing personalization, structured learning, and adaptability, while addressing issues of bias and realism in AI interactions.

**Abstract:** Effective workplace communication is essential for managerial success, yet many managers lack access to tailored and sustained training. Although AI-assisted communication systems may offer scalable training solutions, little is known about how managers envision the role of AI in helping them improve their communication skills. To investigate this, we designed a conversational role-play system, CommCoach, as a functional probe to understand how managers anticipate using AI to practice their communication skills. Through semi-structured interviews, participants emphasized the value of adaptive, low-risk simulations for practicing difficult workplace conversations. They also highlighted opportunities, including human-AI teaming, transparent and context-aware feedback, and greater control over AI-generated personas. AI-assisted communication training should balance personalization, structured learning objectives, and adaptability to different user styles and contexts. However, achieving this requires carefully navigating tensions between adaptive and consistent AI feedback, realism and potential bias, and the open-ended nature of AI conversations versus structured workplace discourse.

</details>


### [13] [Empathy Detection from Text, Audiovisual, Audio or Physiological Signals: A Systematic Review of Task Formulations and Machine Learning Methods](https://arxiv.org/abs/2311.00721)

*Md Rakibul Hasan, Md Zakir Hossain, Shreya Ghosh, Aneesh Krishna, Tom Gedeon*

**Main category:** cs.HC

**Keywords:** Empathy Detection, Machine Learning, Affective Computing, Healthcare, Literature Review

**Relevance Score:** 7

**TL;DR:** A systematic literature review on empathy detection using machine learning, highlighting task formulations, input modalities, and challenges in the field.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underdeveloped area of empathy detection using machine learning and its potential applications across various domains.

**Method:** Collected and analyzed 849 papers across academic databases, culminating in a review of 82 papers focusing on different task formulations and input modalities for empathy detection.

**Key Contributions:**

	1. Systematic review of empathy detection literature using Machine Learning.
	2. Classification of empathy tasks and modalities.
	3. Proposed architecture design protocols for empathy detection systems.

**Result:** Identified prominent task formulations such as localised empathy and emotional contagion; summarized detection methods based on text, audiovisual, audio, and physiological inputs with proposed network architectures.

**Limitations:** The paper may not cover all emerging methodologies or the latest advancements due to the time of the literature collection.

**Conclusion:** The paper outlines challenges, research gaps, and provides resources for empathy detection systems, aiming to enhance human well-being through improved understanding of empathy.

**Abstract:** Empathy indicates an individual's ability to understand others. Over the past few years, empathy has drawn attention from various disciplines, including but not limited to Affective Computing, Cognitive Science, and Psychology. Detecting empathy has potential applications in society, healthcare and education. Despite being a broad and overlapping topic, the avenue of empathy detection leveraging Machine Learning remains underexplored from a systematic literature review perspective. We collected 849 papers from 10 well-known academic databases, systematically screened them and analysed the final 82 papers. Our analyses reveal several prominent task formulations - including empathy on localised utterances or overall expressions, unidirectional or parallel empathy, and emotional contagion - in monadic, dyadic and group interactions. Empathy detection methods are summarised based on four input modalities - text, audiovisual, audio and physiological signals - thereby presenting modality-specific network architecture design protocols. We discuss challenges, research gaps and potential applications in the Affective Computing-based empathy domain, which can facilitate new avenues of exploration. We further enlist the public availability of datasets and codes. This paper, therefore, provides a structured overview of recent advancements and remaining challenges towards developing a robust empathy detection system that could meaningfully contribute to enhancing human well-being.

</details>


### [14] [DiffEyeSyn: Diffusion-based User-specific Eye Movement Synthesis](https://arxiv.org/abs/2409.01240)

*Chuhan Jiao, Guanhua Zhang, Yeonjoo Cho, Zhiming Hu, Andreas Bulling*

**Main category:** cs.HC

**Keywords:** eye movements, gaze data synthesis, user-specific characteristics

**Relevance Score:** 8

**TL;DR:** DiffEyeSyn is a new computational method that synthesizes user-specific eye movements from high-frequency gaze data, using a conditional diffusion process to inject unique user characteristics into eye movement sequences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing gaze modeling methods primarily focus on low-frequency data, overlooking the valuable user-specific information inherent in high-frequency eye movements.

**Method:** DiffEyeSyn synthesizes eye movements by treating user-specific information as a type of noise, implementing a conditional diffusion process where synthesis depends on user-specific embeddings for user authentication.

**Key Contributions:**

	1. First method to synthesize high-frequency gaze data specific to individual users
	2. Introduces user identity guidance as a novel loss function
	3. Supports various downstream tasks including gaze-based user identification

**Result:** Experiments demonstrate that DiffEyeSyn generates synthetic eye movements that maintain user-specific characteristics and are more realistic than existing methods, and can support large-scale gaze data synthesis for tasks like user identification.

**Limitations:** 

**Conclusion:** The proposed method establishes a foundation for personalized eye movement synthesis, with applications in character animation, biometrics, and gaze data imputation.

**Abstract:** High-frequency gaze data contains more user-specific information than low-frequency data, promising for various applications. However, existing gaze modelling methods focus on low-frequency data, ignoring user-specific subtle eye movements in high-frequency eye movements. We present DiffEyeSyn -- the first computational method to synthesise eye movements specific to individual users. The key idea is to consider the user-specific information as a special type of noise in eye movement data. This perspective reshapes eye movement synthesis into the task of injecting this user-specific noise into any given eye movement sequence. We formulate this injection task as a conditional diffusion process in which the synthesis is conditioned on user-specific embeddings extracted from the gaze data using pre-trained models for user authentication. We propose user identity guidance -- a novel loss function that allows our model to preserve user identity while generating human-like eye movements in the spatial domain. Experiments on two public datasets show that our synthetic eye movements preserve user-specific characteristics and are more realistic than baseline approaches. Furthermore, we demonstrate that DiffEyeSyn can synthesise large-scale gaze data and support various downstream tasks, such as gaze-based user identification. As such, our work lays the methodological foundations for personalised eye movement synthesis that has significant application potential, such as for character animation, eye movement biometrics, and gaze data imputation.

</details>


### [15] [Uncovering the Internet's Hidden Values: An Empirical Study of Desirable Behavior Using Highly-Upvoted Content on Reddit](https://arxiv.org/abs/2410.13036)

*Agam Goyal, Charlotte Lambert, Yoshee Jain, Eshwar Chandrasekharan*

**Main category:** cs.HC

**Keywords:** community norms, upvotes, prosocial behavior, values extraction, Reddit

**Relevance Score:** 8

**TL;DR:** This paper investigates the identification of community norms in online spaces by analyzing highly upvoted Reddit comments, revealing inadequacies in current prosocial behavior measures and proposing a novel framework for understanding community values.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of norm-setting in online communities, acknowledging that community values vary greatly and are not fully captured by current automated approaches.

**Method:** The authors analyzed 16,000 highly upvoted comments across 80 popular sub-communities on Reddit using a large language model to extract values from comments from the years 2016 and 2022, identifying distinct macro, meso, and micro values.

**Key Contributions:**

	1. Identified 64 and 72 distinct community values for 2016 and 2022 respectively from Reddit comments.
	2. Demonstrated the limitations of existing prosocial behavior measures in capturing community values.
	3. Proposed a framework for integrating qualitative insights with large-scale content analyses for better understanding of community norms.

**Result:** The research found that existing computational models for measuring prosocial behavior were inadequate, failing to capture 82% of the values extracted from the comments. The study successfully identified both known and new community values through automated analysis.

**Limitations:** The focus was limited to Reddit, which may not generalize across all online communities or platforms.

**Conclusion:** There is a critical need for refined models of desirability in online communities that extend beyond traditional prosocial measures, aiding moderators in understanding and applying community values.

**Abstract:** A major task for moderators of online spaces is norm-setting, essentially creating shared norms for user behavior in their communities. Platform design principles emphasize the importance of highlighting norm-adhering examples and explicitly stating community norms. However, norms and values vary between communities and go beyond content-level attributes, making it challenging for platforms and researchers to provide automated ways to identify desirable behavior to be highlighted. Current automated approaches to detect desirability are limited to measures of prosocial behavior, but we do not know whether these measures fully capture the spectrum of what communities value. In this paper, we use upvotes, which express community approval, as a proxy for desirability and examine 16,000 highly-upvoted comments across 80 popular sub-communities on Reddit. Using a large language model, we extract values from these comments across two years (2016 and 2022) and compile 64 and 72 $\textit{macro}$, $\textit{meso}$, and $\textit{micro}$ values for 2016 and 2022 respectively, based on their frequency across communities. Furthermore, we find that existing computational models for measuring prosociality were inadequate to capture on average $82\%$ of the values we extracted. Finally, we show that our approach can not only extract most of the qualitatively-identified values from prior taxonomies, but also uncover new values that are actually encouraged in practice. Our findings highlight the need for nuanced models of desirability that go beyond preexisting prosocial measures. This work has implications for improving moderator understanding of their community values and provides a framework that can supplement qualitative approaches with larger-scale content analyses.

</details>


### [16] [Seismocardiography for Emotion Recognition: A Study on EmoWear with Insights from DEAP](https://arxiv.org/abs/2412.00411)

*Mohammad Hasan Rahmani, Rafael Berkvens, Maarten Weyn*

**Main category:** cs.HC

**Keywords:** Emotion Recognition, Wearable Technology, Seismocardiography, Accelerometry-Derived Respiration, Affective Computing

**Relevance Score:** 7

**TL;DR:** This paper evaluates the use of accelerometers for Emotion Recognition (ER) through Seismocardiography (SCG) and Accelerometry-Derived Respiration (ADR), demonstrating their potential for real-world applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored potential of using SCG and ADR in Emotion Recognition, aiming to simplify hardware requirements and enhance contextual integration in real-time applications.

**Method:** The authors replicated the emotion classification pipeline from the DEAP dataset and trained models on the EmoWear dataset to compare SCG and ADR against established signals like ECG and BVP.

**Key Contributions:**

	1. Introduction of SCG and ADR as novel modalities for Emotion Recognition.
	2. Demonstration of SCG's comparable performance to ECG and BVP.
	3. Development of a simple, effective Emotion Recognition framework using common wearable technology.

**Result:** The study found that SCG can perform comparably to ECG and BVP for ER tasks. By integrating ADR with SCG, they established an effective ER framework using only a chest-worn accelerometer.

**Limitations:** The study primarily focuses on the EmoWear dataset, which may limit generalizability to other contexts or populations.

**Conclusion:** The findings support the feasibility of integrating SCG and ADR into real-world emotion recognition systems, highlighting their practicality and accessibility for seamless affective computing.

**Abstract:** Emotions have a profound impact on our daily lives, influencing our thoughts, behaviors, and interactions, but also our physiological reactions. Recent advances in wearable technology have facilitated studying emotions through cardio-respiratory signals. Accelerometers offer a non-invasive, convenient, and cost-effective method for capturing heart- and pulmonary-induced vibrations on the chest wall, specifically Seismocardiography (SCG) and Accelerometry-Derived Respiration (ADR). Their affordability, wide availability, and ability to provide rich contextual data make accelerometers ideal for everyday use. While accelerometers have been used as part of broader modality fusions for Emotion Recognition (ER), their stand-alone potential via SCG and ADR remains unexplored. Bridging this gap could significantly help the embedding of ER into real-world applications, minimizing the hardware, and increasing contextual integration potentials. To address this gap, we introduce SCG and ADR as novel modalities for ER and evaluate their performance using the EmoWear dataset. First, we replicate the single-trial emotion classification pipeline from the DEAP dataset study, achieving similar results. Then we use our validated pipeline to train models that predict affective valence-arousal states using SCG and compare them against established cardiac signals, Electrocardiography (ECG) and Blood Volume Pulse (BVP). Results show that SCG is a viable modality for ER, achieving similar performance to ECG and BVP. By combining ADR with SCG, we achieved a working ER framework that only requires a single chest-worn accelerometer. These findings pave the way for integrating ER into real-world, enabling seamless affective computing in everyday life.

</details>


### [17] [Building Symbiotic AI: Reviewing the AI Act for a Human-Centred, Principle-Based Framework](https://arxiv.org/abs/2501.08046)

*Miriana Calvano, Antonio Curci, Giuseppe Desolda, Andrea Esposito, Rosa Lanzilotti, Antonio Piccinno*

**Main category:** cs.HC

**Keywords:** Human-Centred AI, Symbiotic AI, AI regulation, Systematic Literature Review, AI Act

**Relevance Score:** 9

**TL;DR:** This paper presents a systematic literature review identifying key principles for designing Symbiotic AI systems, emphasizing a human-centred approach to enhance collaboration between humans and AI while complying with regulations like the EU's AI Act.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid expansion of AI technologies necessitates regulation to prevent unethical outcomes and ensure human safety during human-AI interactions.

**Method:** A systematic literature review (SLR) was conducted, employing content analysis to extract key principles for the design and development of Symbiotic AI systems.

**Key Contributions:**

	1. Identification of four key principles for Human-Centred AI design
	2. Integration of regulatory perspectives from the EU's AI Act
	3. Highlighting current trends and challenges for future research

**Result:** Four core principles were identified that facilitate the establishment of a symbiotic relationship between humans and AI, guiding the design of Human-Centred AI systems.

**Limitations:** 

**Conclusion:** The research highlights the need for a human-centric approach in AI development and identifies current trends and challenges as future research directions related to Symbiotic AI compliance with the AI Act.

**Abstract:** Artificial Intelligence (AI) spreads quickly as new technologies and services take over modern society. The need to regulate AI design, development, and use is strictly necessary to avoid unethical and potentially dangerous consequences to humans. The European Union (EU) has released a new legal framework, the AI Act, to regulate AI by undertaking a risk-based approach to safeguard humans during interaction. At the same time, researchers offer a new perspective on AI systems, commonly known as Human-Centred AI (HCAI), highlighting the need for a human-centred approach to their design. In this context, Symbiotic AI (a subtype of HCAI) promises to enhance human capabilities through a deeper and continuous collaboration between human intelligence and AI. This article presents the results of a Systematic Literature Review (SLR) that aims to identify principles that characterise the design and development of Symbiotic AI systems while considering humans as the core of the process. Through content analysis, four principles emerged from the review that must be applied to create Human-Centred AI systems that can establish a symbiotic relationship with humans. In addition, current trends and challenges were defined to indicate open questions that may guide future research for the development of SAI systems that comply with the AI Act.

</details>


### [18] [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)

*Dimitris Tsirmpas, Ion Androutsopoulos, John Pavlopoulos*

**Main category:** cs.HC

**Keywords:** Large Language Models, synthetic discussions, facilitation strategies, open-source framework, Virtual Moderation Dataset

**Relevance Score:** 8

**TL;DR:** This paper presents a methodology for using Large Language Models (LLMs) to simulate online discussions, aiming to evaluate facilitation strategies without human intervention.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Limited evaluations for facilitation strategies in online discussions exist due to the high costs of human involvement. The study seeks to leverage LLMs for synthetic discussion simulations to reduce costs and facilitate initial experiments.

**Method:** A generalizable, LLM-driven methodology is proposed to prototype LLM facilitators and generate high-quality synthetic discussion data without human input.

**Key Contributions:**

	1. Proposed a novel LLM-driven methodology for creating synthetic discussions.
	2. Demonstrated that small LLMs can perform as well as larger models in the context of facilitation.
	3. Released "SynDisco" framework and Virtual Moderation Dataset to support future research.

**Result:** While LLM facilitators improve the quality of synthetic discussions, more complex facilitation strategies do not result in significant enhancements over basic approaches. Additionally, small LLMs can perform comparably to larger models.

**Limitations:** The study primarily focuses on synthetic data and may not fully capture the nuances of human-facilitated discussions.

**Conclusion:** The proposed methodology effectively contributes to generating high-quality synthetic data, and the open-source framework "SynDisco" along with a publicly available dataset is released for further research.

**Abstract:** Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using Large Language Models (LLMs) to create initial pilot experiments. We propose a simple, generalizable, LLM-driven methodology to prototype the development of LLM facilitators, and produce high-quality synthetic data without human involvement. We use our methodology to test whether current facilitation strategies can improve the performance of LLM facilitators. We find that, while LLM facilitators significantly improve synthetic discussions, there is no evidence that the application of more elaborate facilitation strategies proposed in modern Social Science research lead to further improvements in discussion quality, compared to more basic approaches. Additionally, we find that small LLMs (such as Mistral Nemo 12B) can perform comparably to larger models (such as LLaMa 70B), and that special instructions must be used for instruction-tuned models to induce toxicity in synthetic discussions. We confirm that each component of our methodology contributes substantially to high quality data via an ablation study. We release an open-source framework, "SynDisco" (pip install syndisco), which implements our methodology. We also release the "Virtual Moderation Dataset" (https://paperswithcode.com/dataset/vmd), a large, publicly available dataset containing LLM-generated and LLM-annotated discussions using multiple open-source LLMs.

</details>


### [19] [Examining Technology Perspectives of Older Adults with Mild Cognitive Impairment: A Scoping Review](https://arxiv.org/abs/2504.13901)

*Snezna B Schmidt, Stephen Isbel, Blooma John, Ram Subramanian, Nathan M DCunha*

**Main category:** cs.HC

**Keywords:** mild cognitive impairment, technology adoption, human-computer interaction, user experience, older adults

**Relevance Score:** 8

**TL;DR:** This paper reviews opinions of older people with mild cognitive impairment (MCI) towards technology solutions, identifying key themes influencing usability and adoption.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how technology can be adopted by older adults with MCI for early intervention, addressing unique challenges in human-computer interaction.

**Method:** Inductive, thematic analysis of user feedback extracted from 83 articles published between January 2014 and May 2024 across nine databases.

**Key Contributions:**

	1. Identified key themes in technology adoption for older people with MCI
	2. Provided insights into preferred device characteristics for this demographic
	3. Recommended strategies for improving user experience with technology for MCI.

**Result:** Five themes were identified: purpose and need, solution design and ease of use, self-impression, lifestyle, and interaction modality. Devices preferred include light, portable ones with large screens and multimodal interaction.

**Limitations:** The study synthesizes existing literature but may not capture all nuances of individual user experiences.

**Conclusion:** Future work should focus on improving personalization, understanding interaction preferences, enabling multimodal interaction options, and integrating solutions into users' lifestyles more seamlessly.

**Abstract:** Mild cognitive impairment (MCI) may affect up to 20% of people over 65. Global incidence of MCI is increasing, and technology is being explored for early intervention. Theories of technology adoption predict useful and easy-to-use solutions will have higher rates of adoption; however, these models do not specifically consider older people with cognitive impairments, or unique human-computer interaction challenges posed by MCI. Older people with MCI opinions about technology solutions were extracted from 83 articles, published between Jan 2014 and May 2024, and found in nine databases. Inductive, thematic analysis of feedback Identified five themes (i) purpose and need, (ii) solution design and ease of use, (iii) self-impression, (iv) lifestyle, and (v) interaction modality. Solutions are perceived as useful, even though gaps in functional support exist, however, they are not perceived as entirely easy to use, due to issues related to ease of use and user experience. Devices which are light, portable, common and have large screens, are preferred, as is multimodal interaction, in particular speech, visual/text and touch. This review recommends future work to (i) improve personalisation, (ii) better understand interaction preferences and effectiveness, (iii) enable options for multimodal interaction, and (iv) more seamlessly integrate solutions into user lifestyles.

</details>


### [20] [ViMo: A Generative Visual GUI World Model for App Agents](https://arxiv.org/abs/2504.13936)

*Dezhao Luo, Bohan Tang, Kang Li, Georgios Papoudakis, Jifei Song, Shaogang Gong, Jianye Hao, Jun Wang, Kun Shao*

**Main category:** cs.HC

**Keywords:** App agents, Graphic user interfaces, World models, Machine learning, Predictive modeling

**Relevance Score:** 7

**TL;DR:** ViMo is a visual world model that generates future App GUI observations as images, improving long-horizon planning for app agents.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** App agents face challenges with long-horizon planning, especially in generating optimal actions for complex tasks with longer steps.

**Method:** ViMo utilizes a novel data representation called Symbolic Text Representation (STR) to separately generate graphic and text content for GUIs, enhancing the prediction of future GUI observations.

**Key Contributions:**

	1. Introduction of ViMo, the first visual world model for GUIs.
	2. Development of the Symbolic Text Representation (STR) to separate graphic and text generation.
	3. Implementation that enhances agent-focused tasks by predicting GUI outcomes.

**Result:** ViMo generates visually plausible and functionally effective GUIs, aiding app agents in making informed decisions based on predicted outcomes of actions.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates significant improvements in app agent performance through effective GUI prediction.

**Abstract:** App agents, which autonomously operate mobile Apps through Graphical User Interfaces (GUIs), have gained significant interest in real-world applications. Yet, they often struggle with long-horizon planning, failing to find the optimal actions for complex tasks with longer steps. To address this, world models are used to predict the next GUI observation based on user actions, enabling more effective agent planning. However, existing world models primarily focus on generating only textual descriptions, lacking essential visual details. To fill this gap, we propose ViMo, the first visual world model designed to generate future App observations as images. For the challenge of generating text in image patches, where even minor pixel errors can distort readability, we decompose GUI generation into graphic and text content generation. We propose a novel data representation, the Symbolic Text Representation~(STR) to overlay text content with symbolic placeholders while preserving graphics. With this design, ViMo employs a STR Predictor to predict future GUIs' graphics and a GUI-text Predictor for generating the corresponding text. Moreover, we deploy ViMo to enhance agent-focused tasks by predicting the outcome of different action options. Experiments show ViMo's ability to generate visually plausible and functionally effective GUIs that enable App agents to make more informed decisions.

</details>


### [21] [Scalable Evaluation of Online Facilitation Strategies via Synthetic Simulation of Discussions](https://arxiv.org/abs/2503.16505)

*Dimitris Tsirmpas, Ion Androutsopoulos, John Pavlopoulos*

**Main category:** cs.HC

**Keywords:** Large Language Models, Synthetic Discussions, Facilitation Strategies

**Relevance Score:** 8

**TL;DR:** This paper presents a methodology for using LLMs to simulate online discussions and test facilitation strategies without human involvement.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing large-scale evaluations of online discussion facilitation strategies are limited due to high costs. The authors aim to develop a cost-effective solution using synthetic discussion simulations with LLMs.

**Method:** The authors propose a generalizable LLM-driven methodology for prototyping LLM facilitators and generating high-quality synthetic discussion data.

**Key Contributions:**

	1. Development of a generalizable LLM-driven methodology for discussion facilitation
	2. Findings that smaller models can perform on par with larger ones
	3. Release of SynDisco framework and Virtual Moderation Dataset for public use

**Result:** LLM facilitators significantly improve discussion quality, but more complex facilitation strategies do not show additional benefits. Smaller LLMs perform comparably to larger ones. The study confirms the effectiveness of each methodology component through an ablation study.

**Limitations:** 

**Conclusion:** The proposed framework, SynDisco, is effective in creating synthetic discussions, and the Virtual Moderation Dataset is a valuable resource for further research.

**Abstract:** Limited large-scale evaluations exist for facilitation strategies of online discussions due to significant costs associated with human involvement. An effective solution is synthetic discussion simulations using Large Language Models (LLMs) to create initial pilot experiments. We propose a simple, generalizable, LLM-driven methodology to prototype the development of LLM facilitators, and produce high-quality synthetic data without human involvement. We use our methodology to test whether current facilitation strategies can improve the performance of LLM facilitators. We find that, while LLM facilitators significantly improve synthetic discussions, there is no evidence that the application of more elaborate facilitation strategies proposed in modern Social Science research lead to further improvements in discussion quality, compared to more basic approaches. Additionally, we find that small LLMs (such as Mistral Nemo 12B) can perform comparably to larger models (such as LLaMa 70B), and that special instructions must be used for instruction-tuned models to induce toxicity in synthetic discussions. We confirm that each component of our methodology contributes substantially to high quality data via an ablation study. We release an open-source framework, "SynDisco" (pip install syndisco), which implements our methodology. We also release the "Virtual Moderation Dataset" (https://paperswithcode.com/dataset/vmd), a large, publicly available dataset containing LLM-generated and LLM-annotated discussions using multiple open-source LLMs.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [Evaluating Reasoning LLMs for Suicide Screening with the Columbia-Suicide Severity Rating Scale](https://arxiv.org/abs/2505.13480)

*Avinash Patil, Siru Tao, Amardeep Gedhu*

**Main category:** cs.CL

**Keywords:** suicide prevention, large language models, C-SSRS, risk assessment, ethical considerations

**Relevance Score:** 9

**TL;DR:** This study evaluates the effectiveness of large language models in automating suicide risk assessment using a severity scale.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical public health challenge of suicide prevention by exploring the role of AI in assessing suicide risk on online platforms.

**Method:** The study assesses the zero-shot performance of six language models (Claude, GPT, Mistral, LLaMA) in classifying posts using the Columbia-Suicide Severity Rating Scale (C-SSRS).

**Key Contributions:**

	1. Evaluation of LLMs for automated suicide risk assessment
	2. Comparison of multiple models on a standardized scale
	3. Discussion of ethical implications and need for human oversight

**Result:** Claude and GPT closely align with human annotations, while Mistral shows the lowest prediction error; models generally exhibit ordinal sensitivity with common misclassifications between adjacent levels.

**Limitations:** Analysis limited to specific LLMs and the C-SSRS; may not generalize to all AI applications in mental health.

**Conclusion:** The findings highlight the need for human oversight in AI-driven assessments and the ethical considerations necessary for deployment.

**Abstract:** Suicide prevention remains a critical public health challenge. While online platforms such as Reddit's r/SuicideWatch have historically provided spaces for individuals to express suicidal thoughts and seek community support, the advent of large language models (LLMs) introduces a new paradigm-where individuals may begin disclosing ideation to AI systems instead of humans. This study evaluates the capability of LLMs to perform automated suicide risk assessment using the Columbia-Suicide Severity Rating Scale (C-SSRS). We assess the zero-shot performance of six models-including Claude, GPT, Mistral, and LLaMA-in classifying posts across a 7-point severity scale (Levels 0-6). Results indicate that Claude and GPT closely align with human annotations, while Mistral achieves the lowest ordinal prediction error. Most models exhibit ordinal sensitivity, with misclassifications typically occurring between adjacent severity levels. We further analyze confusion patterns, misclassification sources, and ethical considerations, underscoring the importance of human oversight, transparency, and cautious deployment. Full code and supplementary materials are available at https://github.com/av9ash/llm_cssrs_code.

</details>


### [23] [EmoMeta: A Multimodal Dataset for Fine-grained Emotion Classification in Chinese Metaphors](https://arxiv.org/abs/2505.13483)

*Xingyuan Lu, Yuxi Liu, Dongyu Zhang, Zhiyao Wu, Jing Ren, Feng Xia*

**Main category:** cs.CL

**Keywords:** multimodal metaphors, emotion classification, dataset, Chinese, advertisements

**Relevance Score:** 6

**TL;DR:** The paper introduces a multimodal dataset of 5,000 Chinese text-image pairs focused on metaphorical advertisements, aimed at enhancing emotion classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multimodal metaphorical fine-grained emotion datasets and to explore emotional nuances across languages beyond English.

**Method:** The authors developed a dataset containing 5,000 annotated text-image pairs for metaphor occurrence and fine-grained emotion classification.

**Key Contributions:**

	1. Introduction of a multimodal dataset in Chinese
	2. Annotation of metaphor occurrence and emotional dimensions
	3. Focus on fine-grained emotion classification

**Result:** The dataset includes annotations for various emotions such as joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral, contributing to the field of emotional intelligence.

**Limitations:** 

**Conclusion:** The publicly accessible dataset will enable research advancements in multimodal metaphors and emotion classification.

**Abstract:** Metaphors play a pivotal role in expressing emotions, making them crucial for emotional intelligence. The advent of multimodal data and widespread communication has led to a proliferation of multimodal metaphors, amplifying the complexity of emotion classification compared to single-mode scenarios. However, the scarcity of research on constructing multimodal metaphorical fine-grained emotion datasets hampers progress in this domain. Moreover, existing studies predominantly focus on English, overlooking potential variations in emotional nuances across languages. To address these gaps, we introduce a multimodal dataset in Chinese comprising 5,000 text-image pairs of metaphorical advertisements. Each entry is meticulously annotated for metaphor occurrence, domain relations and fine-grained emotion classification encompassing joy, love, trust, fear, sadness, disgust, anger, surprise, anticipation, and neutral. Our dataset is publicly accessible (https://github.com/DUTIR-YSQ/EmoMeta), facilitating further advancements in this burgeoning field.

</details>


### [24] [Detecting Prefix Bias in LLM-based Reward Models](https://arxiv.org/abs/2505.13487)

*Ashwin Kumar, Yuzi He, Aram H. Markosyan, Bobbie Chern, Imanol Arrieta-Ibarra*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Bias Detection, Language Models, Fairness in AI

**Relevance Score:** 9

**TL;DR:** This paper addresses biases in reinforcement learning reward models influenced by query prefixes and proposes methods to detect and mitigate these biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore and mitigate biases in reward models trained on human preference data, particularly concerning racial and gender dimensions.

**Method:** The authors introduce novel methods to detect prefix bias in reward models and evaluate their impact across diverse datasets and model architectures. They also propose a data augmentation strategy to alleviate these biases.

**Key Contributions:**

	1. Introduction of methods to detect prefix bias in reward models
	2. Demonstration of significant biases across various datasets and architectures
	3. Proposal of a data augmentation strategy to mitigate biases

**Result:** The evaluation reveals significant biases in LLM-based reward models across racial and gender categories, highlighting their susceptibility to prefix bias.

**Limitations:** The study may not cover all possible sources of bias and focuses primarily on prefix bias.

**Conclusion:** There is a critical need for bias-aware dataset design and evaluation to develop fair and reliable reward models in AI.

**Abstract:** Reinforcement Learning with Human Feedback (RLHF) has emerged as a key paradigm for task-specific fine-tuning of language models using human preference data. While numerous publicly available preference datasets provide pairwise comparisons of responses, the potential for biases in the resulting reward models remains underexplored. In this work, we introduce novel methods to detect and evaluate prefix bias -- a systematic shift in model preferences triggered by minor variations in query prefixes -- in LLM-based reward models trained on such datasets. We leverage these metrics to reveal significant biases in preference models across racial and gender dimensions. Our comprehensive evaluation spans diverse open-source preference datasets and reward model architectures, demonstrating susceptibility to this kind of bias regardless of the underlying model architecture. Furthermore, we propose a data augmentation strategy to mitigate these biases, showing its effectiveness in reducing the impact of prefix bias. Our findings highlight the critical need for bias-aware dataset design and evaluation in developing fair and reliable reward models, contributing to the broader discourse on fairness in AI.

</details>


### [25] [Source framing triggers systematic evaluation bias in Large Language Models](https://arxiv.org/abs/2505.13488)

*Federico Germani, Giovanni Spitale*

**Main category:** cs.CL

**Keywords:** Large Language Models, text evaluation, framing effects, bias, neutrality

**Relevance Score:** 9

**TL;DR:** The study examines the consistency and bias in LLM evaluations of narrative statements across multiple models, revealing significant framing effects based on attribution to authors of different nationalities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reliability and bias of Large Language Models in evaluating text, particularly how author attribution influences assessments.

**Method:** The study evaluates 4,800 narrative statements across four LLMs, measuring inter- and intra-model agreement while manipulating the source attribution to analyze its impact.

**Key Contributions:**

	1. Systematic analysis of LLM evaluation consistency
	2. Identification of framing effects in text assessment
	3. Implications for fairness in LLM applications

**Result:** LLMs showed high agreement in their evaluations but displayed decreased consistency when the sources were attributed to different nationalities, particularly affecting the Deepseek Reasoner.

**Limitations:** Limited to four LLMs and specific narrative statements; may not generalize to all types of texts or contexts.

**Conclusion:** Framing effects significantly impact LLM evaluations, raising concerns regarding the neutrality and fairness of systems depending on LLM outputs.

**Abstract:** Large Language Models (LLMs) are increasingly used not only to generate text but also to evaluate it, raising urgent questions about whether their judgments are consistent, unbiased, and robust to framing effects. In this study, we systematically examine inter- and intra-model agreement across four state-of-the-art LLMs (OpenAI o3-mini, Deepseek Reasoner, xAI Grok 2, and Mistral) tasked with evaluating 4,800 narrative statements on 24 different topics of social, political, and public health relevance, for a total of 192,000 assessments. We manipulate the disclosed source of each statement to assess how attribution to either another LLM or a human author of specified nationality affects evaluation outcomes. We find that, in the blind condition, different LLMs display a remarkably high degree of inter- and intra-model agreement across topics. However, this alignment breaks down when source framing is introduced. Here we show that attributing statements to Chinese individuals systematically lowers agreement scores across all models, and in particular for Deepseek Reasoner. Our findings reveal that framing effects can deeply affect text evaluation, with significant implications for the integrity, neutrality, and fairness of LLM-mediated information systems.

</details>


### [26] [ProdRev: A DNN framework for empowering customers using generative pre-trained transformers](https://arxiv.org/abs/2505.13491)

*Aakash Gupta, Nataraj Das*

**Main category:** cs.CL

**Keywords:** e-commerce, review summarization, GPT-3, common sense reasoning, decision-making

**Relevance Score:** 6

**TL;DR:** This paper proposes a framework using a fine-tuned generative transformer to summarize e-commerce product reviews, aiding consumers in decision-making by highlighting pros and cons.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the decision paralysis faced by consumers due to the overwhelming number of reviews available for e-commerce products.

**Method:** The framework fine-tunes a generative pre-trained transformer (GPT-3, Curie engine) to better understand and summarize reviews via abstractive summarization, incorporating common sense reasoning.

**Key Contributions:**

	1. Development of an abstractive summarization method for e-commerce reviews
	2. Incorporation of common sense reasoning into review summaries
	3. Improvement in consumer decision-making efficiency through refined review understanding

**Result:** The generative model effectively summarizes reviews by presenting the true relationships within them, offering users clear pros and cons.

**Limitations:** 

**Conclusion:** By using this approach, consumers can make more informed and quicker decisions regarding their purchases in an e-commerce setting.

**Abstract:** Following the pandemic, customers, preference for using e-commerce has accelerated. Since much information is available in multiple reviews (sometimes running in thousands) for a single product, it can create decision paralysis for the buyer. This scenario disempowers the consumer, who cannot be expected to go over so many reviews since its time consuming and can confuse them. Various commercial tools are available, that use a scoring mechanism to arrive at an adjusted score. It can alert the user to potential review manipulations. This paper proposes a framework that fine-tunes a generative pre-trained transformer to understand these reviews better. Furthermore, using "common-sense" to make better decisions. These models have more than 13 billion parameters. To fine-tune the model for our requirement, we use the curie engine from generative pre-trained transformer (GPT3). By using generative models, we are introducing abstractive summarization. Instead of using a simple extractive method of summarizing the reviews. This brings out the true relationship between the reviews and not simply copy-paste. This introduces an element of "common sense" for the user and helps them to quickly make the right decisions. The user is provided the pros and cons of the processed reviews. Thus the user/customer can take their own decisions.

</details>


### [27] [LLM4CD: Leveraging Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis](https://arxiv.org/abs/2505.13492)

*Weiming Zhang, Lingyue Fu, Qingyao Li, Kounianhua Du, Jianghao Lin, Jingwei Yu, Wei Xia, Weinan Zhang, Ruiming Tang, Yong Yu*

**Main category:** cs.CL

**Keywords:** Cognitive Diagnosis, Large Language Models, Intelligent Tutoring Systems, Educational Data, Open-World Knowledge

**Relevance Score:** 9

**TL;DR:** The paper proposes LLM4CD, a method using large language models for cognitive diagnosis in education, enhancing traditional ID-based methods by incorporating semantic relationships.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current cognitive diagnosis methods rely on student-ID relationships and struggle with new students and exercises; LLMs can provide open-world knowledge to improve this.

**Method:** The proposed LLM4CD method constructs cognitive representations using LLMs and introduces a bi-level encoder framework to handle students' test histories more effectively, focusing on semantic rather than ID-based information.

**Key Contributions:**

	1. Introduction of LLM4CD leveraging LLMs for cognitive diagnosis
	2. Development of a bi-level encoder framework for enhanced student modeling
	3. Demonstrated superiority over traditional cognitive diagnosis models with extensive experimental validation

**Result:** LLM4CD outperforms existing cognitive diagnosis models across multiple real-world datasets, demonstrating improved handling of semantic relationships and alleviating cold-start issues.

**Limitations:** 

**Conclusion:** The incorporation of LLM semantics into cognitive diagnosis improves model performance and helps manage the variability in educational contexts, offering a scalable solution for intelligent tutoring systems.

**Abstract:** Cognitive diagnosis (CD) plays a crucial role in intelligent education, evaluating students' comprehension of knowledge concepts based on their test histories. However, current CD methods often model students, exercises, and knowledge concepts solely on their ID relationships, neglecting the abundant semantic relationships present within educational data space. Furthermore, contemporary intelligent tutoring systems (ITS) frequently involve the addition of new students and exercises, a situation that ID-based methods find challenging to manage effectively. The advent of large language models (LLMs) offers the potential for overcoming this challenge with open-world knowledge. In this paper, we propose LLM4CD, which Leverages Large Language Models for Open-World Knowledge Augmented Cognitive Diagnosis. Our method utilizes the open-world knowledge of LLMs to construct cognitively expressive textual representations, which are then encoded to introduce rich semantic information into the CD task. Additionally, we propose an innovative bi-level encoder framework that models students' test histories through two levels of encoders: a macro-level cognitive text encoder and a micro-level knowledge state encoder. This approach substitutes traditional ID embeddings with semantic representations, enabling the model to accommodate new students and exercises with open-world knowledge and address the cold-start problem. Extensive experimental results demonstrate that our proposed method consistently outperforms previous CD models on multiple real-world datasets, validating the effectiveness of leveraging LLMs to introduce rich semantic information into the CD task.

</details>


### [28] [IRLBench: A Multi-modal, Culturally Grounded, Parallel Irish-English Benchmark for Open-Ended LLM Reasoning Evaluation](https://arxiv.org/abs/2505.13498)

*Khanh-Tung Tran, Barry O'Sullivan, Hoang D. Nguyen*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual evaluation, low-resource languages, cultural bias, natural language generation

**Relevance Score:** 8

**TL;DR:** IRLBench is a multilingual benchmark for evaluating Large Language Models (LLMs) in endangered languages, notably Irish, highlighting performance disparities between English and Irish.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of evaluation benchmarks for LLMs in multilingual and low-resource settings, particularly for culturally significant languages like Irish.

**Method:** The authors developed IRLBench, a benchmark featuring long-form generation tasks based on the 2024 Irish Leaving Certificate exams, enabling detailed analysis of LLM capabilities.

**Key Contributions:**

	1. Introduction of IRLBench for multilingual evaluation of LLMs
	2. Focus on endangered languages and cultural awareness
	3. Release of a comprehensive dataset and evaluation tools

**Result:** Experiments show a notable performance gap in LLM responses between English and Irish, with the best-performing model achieving only 55.8% correctness in Irish compared to 76.2% in English.

**Limitations:** Performance results are limited to a specific low-resource language (Irish) and may not generalize to all multilingual contexts.

**Conclusion:** The release of IRLBench, along with an evaluation codebase, aims to facilitate future research on creating more robust multilingual AI systems that acknowledge cultural contexts.

**Abstract:** Recent advances in Large Language Models (LLMs) have demonstrated promising knowledge and reasoning abilities, yet their performance in multilingual and low-resource settings remains underexplored. Existing benchmarks often exhibit cultural bias, restrict evaluation to text-only, rely on multiple-choice formats, and, more importantly, are limited for extremely low-resource languages. To address these gaps, we introduce IRLBench, presented in parallel English and Irish, which is considered definitely endangered by UNESCO. Our benchmark consists of 12 representative subjects developed from the 2024 Irish Leaving Certificate exams, enabling fine-grained analysis of model capabilities across domains. By framing the task as long-form generation and leveraging the official marking scheme, it does not only support a comprehensive evaluation of correctness but also language fidelity. Our extensive experiments of leading closed-source and open-source LLMs reveal a persistent performance gap between English and Irish, in which models produce valid Irish responses less than 80\% of the time, and answer correctly 55.8\% of the time compared to 76.2\% in English for the best-performing model. We release IRLBench (https://huggingface.co/datasets/ReliableAI/IRLBench) and an accompanying evaluation codebase (https://github.com/ReML-AI/IRLBench) to enable future research on robust, culturally aware multilingual AI development.

</details>


### [29] [Noise Injection Systemically Degrades Large Language Model Safety Guardrails](https://arxiv.org/abs/2505.13500)

*Prithviraj Singh Shahani, Matthias Scheutz*

**Main category:** cs.CL

**Keywords:** language models, safety tuning, AI robustness, Gaussian noise, safety alignment

**Relevance Score:** 8

**TL;DR:** This paper investigates the robustness of safety fine-tuning in LLMs, revealing vulnerabilities in current safety methods and suggesting directions for improvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the resilience of safety guardrails in LLMs against harmful outputs under perturbation.

**Method:** The study systematically injects Gaussian noise into model activations of multiple open-weight models to assess harmful output rates and effects on safety fine-tuning.

**Key Contributions:**

	1. Demonstrated the impact of Gaussian noise on harmful output rates in LLMs
	2. Revealed that deeper safety fine-tuning does not enhance robustness
	3. Identified reasoning-based approaches as potential solutions for enhancing AI safety

**Result:** Gaussian noise increases harmful-output rates by up to 27% with significant statistical confidence (p < 0.001). Deeper safety fine-tuning does not provide additional protection, yet chain-of-thought reasoning remains intact.

**Limitations:** 

**Conclusion:** Current safety alignment techniques have critical vulnerabilities, and reasoning-based and reinforcement learning approaches may lead to more robust AI safety systems.

**Abstract:** Safety guardrails in large language models (LLMs) are a critical component in preventing harmful outputs. Yet, their resilience under perturbation remains poorly understood. In this paper, we investigate the robustness of safety fine-tuning in LLMs by systematically injecting Gaussian noise into model activations. We show across multiple open-weight models that (1) Gaussian noise raises harmful-output rates (p < 0.001) by up to 27%, (2) that deeper safety fine-tuning affords no extra protection, and (3) that chain-of-thought reasoning remains largely intact. The findings reveal critical vulnerabilities in current safety alignment techniques and highlight the potential of reasoning-based and reinforcement learning approaches as promising direction for developing more robust AI safety systems. These results have important implications for real-world deployment of LLMs in safety-critical applications as these results imply that widely-deployed safety tuning methods can fail even without adversarial prompts.

</details>


### [30] [EcoSafeRAG: Efficient Security through Context Analysis in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.13506)

*Ruobing Yao, Yifei Zhang, Shuang Song, Neng Gao, Chenyang Tu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, security, Large Language Models, context diversity, corpus poisoning

**Relevance Score:** 9

**TL;DR:** EcoSafeRAG enhances Retrieval-Augmented Generation (RAG) by bolstering security against corpus poisoning while improving performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of Large Language Models (LLMs) in factual correctness and contextualization while ensuring security against new attack vectors introduced by Retrieval-Augmented Generation (RAG).

**Method:** EcoSafeRAG employs sentence-level processing and bait-guided context diversity detection to identify malicious content without relying on the internal knowledge of LLMs.

**Key Contributions:**

	1. Introduces a novel method for identifying malicious content in RAG systems without using internal model knowledge.
	2. Demonstrates significant performance gains in clean scenarios while reducing operational costs.
	3. Provides a plug-and-play deployment model for easy integration into existing systems.

**Result:** EcoSafeRAG achieves state-of-the-art security and improves performance in clean scenarios, achieving a latency of 1.2 times and 48%-80% token reduction compared to Vanilla RAG.

**Limitations:** 

**Conclusion:** EcoSafeRAG presents an effective defense mechanism for RAG systems that enhances both security and operational efficiency while improving performance.

**Abstract:** Retrieval-Augmented Generation (RAG) compensates for the static knowledge limitations of Large Language Models (LLMs) by integrating external knowledge, producing responses with enhanced factual correctness and query-specific contextualization. However, it also introduces new attack surfaces such as corpus poisoning at the same time. Most of the existing defense methods rely on the internal knowledge of the model, which conflicts with the design concept of RAG. To bridge the gap, EcoSafeRAG uses sentence-level processing and bait-guided context diversity detection to identify malicious content by analyzing the context diversity of candidate documents without relying on LLM internal knowledge. Experiments show EcoSafeRAG delivers state-of-the-art security with plug-and-play deployment, simultaneously improving clean-scenario RAG performance while maintaining practical operational costs (relatively 1.2$\times$ latency, 48\%-80\% token reduction versus Vanilla RAG).

</details>


### [31] [Time-R1: Towards Comprehensive Temporal Reasoning in LLMs](https://arxiv.org/abs/2505.13508)

*Zijia Liu, Peixuan Han, Haofei Yu, Haoru Li, Jiaxuan You*

**Main category:** cs.CL

**Keywords:** Large Language Models, Temporal Intelligence, Reinforcement Learning, Prediction, Scenario Generation

**Relevance Score:** 8

**TL;DR:** Introducing Time-R1, a framework for improving temporal intelligence in LLMs, enabling understanding, prediction, and creative generation of events.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the temporal intelligence of LLMs, addressing their struggle to integrate past reasoning with future predictions and creative generation.

**Method:** A three-stage reinforcement learning curriculum with a dynamic rule-based reward system to develop foundational understanding, prediction skills, and creative scenario generation capabilities.

**Key Contributions:**

	1. Introduction of Time-R1 framework for temporal reasoning in LLMs.
	2. Development of Time-Bench, a multi-task temporal reasoning dataset.
	3. Showcasing superior performance of smaller models over larger state-of-the-art models.

**Result:** Time-R1 significantly outperforms larger models on future event prediction and creative scenario benchmarks, demonstrating highly effective temporal performance with smaller models.

**Limitations:** 

**Conclusion:** The findings suggest that well-structured reinforcement learning fine-tuning can lead to superior temporal capabilities in smaller LLMs, paving the way for time-aware AI applications.

**Abstract:** Large Language Models (LLMs) demonstrate impressive capabilities but lack robust temporal intelligence, struggling to integrate reasoning about the past with predictions and plausible generations of the future. Meanwhile, existing methods typically target isolated temporal skills, such as question answering about past events or basic forecasting, and exhibit poor generalization, particularly when dealing with events beyond their knowledge cutoff or requiring creative foresight. To address these limitations, we introduce \textit{Time-R1}, the first framework to endow a moderate-sized (3B-parameter) LLM with comprehensive temporal abilities: understanding, prediction, and creative generation. Our approach features a novel three-stage development path; the first two constitute a \textit{reinforcement learning (RL) curriculum} driven by a meticulously designed dynamic rule-based reward system. This framework progressively builds (1) foundational temporal understanding and logical event-time mappings from historical data, (2) future event prediction skills for events beyond its knowledge cutoff, and finally (3) enables remarkable generalization to creative future scenario generation without any fine-tuning. Strikingly, experiments demonstrate that Time-R1 outperforms models over 200 times larger, including the state-of-the-art 671B DeepSeek-R1, on highly challenging future event prediction and creative scenario generation benchmarks. This work provides strong evidence that thoughtfully engineered, progressive RL fine-tuning allows smaller, efficient models to achieve superior temporal performance, offering a practical and scalable path towards truly time-aware AI. To foster further research, we also release \textit{Time-Bench}, a large-scale multi-task temporal reasoning dataset derived from 10 years of news data, and our series of \textit{Time-R1} checkpoints.

</details>


### [32] [Induction Head Toxicity Mechanistically Explains Repetition Curse in Large Language Models](https://arxiv.org/abs/2505.13514)

*Shuxun Wang, Qingyu Yin, Chak Tou Leong, Qiang Zhang, Linyi Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, repetition curse, induction heads, attention heads, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the repetition curse in Large Language Models (LLMs), focusing on the role of induction heads in generating repetitive outputs and proposes techniques to mitigate this issue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms behind the repetition curse observed in LLMs, particularly the role of induction heads in driving this behavior.

**Method:** The study analyzes induction heads and their 'toxicity' defined as their dominance in output logits, leading to repetitive token generation, and proposes an attention head regularization technique to mitigate this effect.

**Key Contributions:**

	1. Mechanistic insight into the repetition curse in LLMs
	2. Identification of induction heads as primary drivers of repetitive outputs
	3. Proposal of attention head regularization technique for improved output diversity

**Result:** The findings reveal that induction heads significantly contribute to the repetition curse by limiting the contributions of other attention heads, and the proposed regularization technique shows potential for improving output diversity.

**Limitations:** 

**Conclusion:** Identifying induction heads as a key factor in repetitive behavior aids in developing strategies to enhance model outputs, suggesting future directions for LLM training and design.

**Abstract:** Repetition curse is a phenomenon where Large Language Models (LLMs) generate repetitive sequences of tokens or cyclic sequences. While the repetition curse has been widely observed, its underlying mechanisms remain poorly understood. In this work, we investigate the role of induction heads--a specific type of attention head known for their ability to perform in-context learning--in driving this repetitive behavior. Specifically, we focus on the "toxicity" of induction heads, which we define as their tendency to dominate the model's output logits during repetition, effectively excluding other attention heads from contributing to the generation process. Our findings have important implications for the design and training of LLMs. By identifying induction heads as a key driver of the repetition curse, we provide a mechanistic explanation for this phenomenon and suggest potential avenues for mitigation. We also propose a technique with attention head regularization that could be employed to reduce the dominance of induction heads during generation, thereby promoting more diverse and coherent outputs.

</details>


### [33] [Logic Jailbreak: Efficiently Unlocking LLM Safety Restrictions Through Formal Logical Expression](https://arxiv.org/abs/2505.13527)

*Jingyu Peng, Maolin Wang, Nan Wang, Xiangyu Zhao, Jiatong Li, Kai Zhang, Qi Liu*

**Main category:** cs.CL

**Keywords:** large language models, jailbreak attacks, logical expressions, AI safety, multilingual evaluation

**Relevance Score:** 8

**TL;DR:** LogiBreak is a novel black-box jailbreak method for LLMs that exploits distributional discrepancies between alignment-oriented prompts and malicious prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses vulnerabilities in LLM safety mechanisms that are susceptible to jailbreak attacks by exploring the differences in prompt distributions.

**Method:** LogiBreak translates harmful natural language prompts into formal logical expressions to bypass LLM safety systems.

**Key Contributions:**

	1. Introduction of LogiBreak as a universal jailbreak method
	2. Demonstration of effectiveness across multilingual datasets
	3. Insight into the distributional discrepancies affecting LLM safety

**Result:** LogiBreak is effective across a multilingual jailbreak dataset, demonstrating its robustness in various evaluation settings and contexts.

**Limitations:** The method's reliance on logical expression translation may have specific contexts where it does not apply.

**Conclusion:** The findings indicate that transforming prompts into logical expressions can effectively evade safety constraints in LLMs.

**Abstract:** Despite substantial advancements in aligning large language models (LLMs) with human values, current safety mechanisms remain susceptible to jailbreak attacks. We hypothesize that this vulnerability stems from distributional discrepancies between alignment-oriented prompts and malicious prompts. To investigate this, we introduce LogiBreak, a novel and universal black-box jailbreak method that leverages logical expression translation to circumvent LLM safety systems. By converting harmful natural language prompts into formal logical expressions, LogiBreak exploits the distributional gap between alignment data and logic-based inputs, preserving the underlying semantic intent and readability while evading safety constraints. We evaluate LogiBreak on a multilingual jailbreak dataset spanning three languages, demonstrating its effectiveness across various evaluation settings and linguistic contexts.

</details>


### [34] [Combining the Best of Both Worlds: A Method for Hybrid NMT and LLM Translation](https://arxiv.org/abs/2505.13554)

*Zhanglin Wu, Daimeng Wei, Xiaoyu Chen, Hengchao Shang, Jiaxin Guo, Zongyao Li, Yuanchang Luo, Jinlong Yang, Zhiqiang Rao, Hao Yang*

**Main category:** cs.CL

**Keywords:** large language models, neural machine translation, scheduling policy, translation optimization, multilingual datasets

**Relevance Score:** 8

**TL;DR:** This paper explores the integration of large language models (LLM) and neural machine translation (NMT) to optimize translation performance while minimizing LLM usage and latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational costs and latency when using LLMs for machine translation, while maintaining translation quality comparable to NMT systems.

**Method:** The authors propose a scheduling policy that optimizes translation results by leveraging source sentence features, comparing various scheduling approaches through extensive experiments on multilingual test sets.

**Key Contributions:**

	1. Proposed a novel scheduling policy that integrates LLM and NMT for translation tasks.
	2. Demonstrated the effectiveness of the decider through extensive experiments on multilingual datasets.
	3. Provided a comparative analysis of different scheduling strategies.

**Result:** The proposed decider achieves optimal translation performance with minimal LLM usage, validating the effectiveness of the scheduling policy.

**Limitations:** The experiments focus on specific multilingual test sets, which may limit generalizability to all language pairs or contexts.

**Conclusion:** Integrating NMT and LLM using an intelligent scheduling policy can enhance translation efficiency without compromising quality.

**Abstract:** Large language model (LLM) shows promising performances in a variety of downstream tasks, such as machine translation (MT). However, using LLMs for translation suffers from high computational costs and significant latency. Based on our evaluation, in most cases, translations using LLMs are comparable to that generated by neural machine translation (NMT) systems. Only in particular scenarios, LLM and NMT models show respective advantages. As a result, integrating NMT and LLM for translation and using LLM only when necessary seems to be a sound solution. A scheduling policy that optimizes translation result while ensuring fast speed and as little LLM usage as possible is thereby required. We compare several scheduling policies and propose a novel and straightforward decider that leverages source sentence features. We conduct extensive experiments on multilingual test sets and the result shows that we can achieve optimal translation performance with minimal LLM usage, demonstrating effectiveness of our decider.

</details>


### [35] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)

*Franziska Sofia Hafner, Ana Valdivia, Luc Rocher*

**Main category:** cs.CL

**Keywords:** gender bias, language models, gender identity, gender studies, bias mitigation

**Relevance Score:** 9

**TL;DR:** This paper investigates how language models encode and perpetuate harmful gender stereotypes, advocating for a broader understanding of gender bias beyond superficial associations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address and mitigate harmful gendered stereotypes perpetuated by language models, which affect diverse gender identities and can lead to real-world consequences in applications like health informatics.

**Method:** The study operationalizes insights from gender studies, testing 16 language models of varying architectures and sizes to analyze how they encode gender.

**Key Contributions:**

	1. Proposes a broader definition of gender bias in language models
	2. Empirical analysis of 16 different language models regarding gender encoding
	3. Highlights the impact of model size on gender biases, emphasizing the need for re-evaluation

**Result:** The research finds that these models predominantly encode gender as a binary linked to biological sex, erasing and pathologizing non-binary identities, with larger models showing stronger biases.

**Limitations:** The study may not include all existing language models or encapsulate the full diversity of gender identities beyond the binary framework.

**Conclusion:** The authors call for a re-evaluation of the definitions and methods used to identify and mitigate gendered harms in language models.

**Abstract:** Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.   For FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.

</details>


### [36] [CS-Sum: A Benchmark for Code-Switching Dialogue Summarization and the Limits of Large Language Models](https://arxiv.org/abs/2505.13559)

*Sathya Krishnan Suresh, Tanmay Surana, Lim Zhi Hao, Eng Siong Chng*

**Main category:** cs.CL

**Keywords:** Code-switching, Large Language Models, Summarization, Natural Language Processing, Benchmark

**Relevance Score:** 7

**TL;DR:** A study introducing CS-Sum, a benchmark for evaluating Large Language Models on code-switching dialogue summarization across multiple language pairs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by code-switching in Large Language Models and evaluate their comprehensibility in summarizing code-switched dialogues.

**Method:** CS-Sum benchmark consisting of 900-1300 human-annotated dialogues for each language pair (EN-ZH, EN-TA, EN-MS). Evaluated ten LLMs using few-shot, translate-summarize, and fine-tuning approaches on synthetic data.

**Key Contributions:**

	1. Introduction of the CS-Sum benchmark for code-switching dialogue summarization.
	2. Evaluation of ten LLMs on comprehensibility of code-switching.
	3. Identification of common error types made by LLMs with code-switched data.

**Result:** High automated metric scores for LLMs, but identified subtle errors that impact the meaning of dialogues. Error rates vary by language pair and LLM, highlighting the necessity for specialized training on code-switching data.

**Limitations:** Focus on a limited set of language pairs and types of LLMs.

**Conclusion:** The findings emphasize the need for improved understanding and capabilities of LLMs when handling code-switched input, suggesting a path for future research on specialized training.

**Abstract:** Code-switching (CS) poses a significant challenge for Large Language Models (LLMs), yet its comprehensibility remains underexplored in LLMs. We introduce CS-Sum, to evaluate the comprehensibility of CS by the LLMs through CS dialogue to English summarization. CS-Sum is the first benchmark for CS dialogue summarization across Mandarin-English (EN-ZH), Tamil-English (EN-TA), and Malay-English (EN-MS), with 900-1300 human-annotated dialogues per language pair. Evaluating ten LLMs, including open and closed-source models, we analyze performance across few-shot, translate-summarize, and fine-tuning (LoRA, QLoRA on synthetic data) approaches. Our findings show that though the scores on automated metrics are high, LLMs make subtle mistakes that alter the complete meaning of the dialogue. To this end, we introduce 3 most common type of errors that LLMs make when handling CS input. Error rates vary across CS pairs and LLMs, with some LLMs showing more frequent errors on certain language pairs, underscoring the need for specialized training on code-switched data.

</details>


### [37] [Cross-Lingual Representation Alignment Through Contrastive Image-Caption Tuning](https://arxiv.org/abs/2505.13628)

*Nathaniel Krasner, Nicholas Lanuzo, Antonios Anastasopoulos*

**Main category:** cs.CL

**Keywords:** Multilingual Alignment, Image-Caption Data, Cross-Lingual NLU

**Relevance Score:** 7

**TL;DR:** The paper explores the use of visual information in multilingual sentence representation alignment, utilizing image caption datasets as an efficient alternative to bitexts for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of multilingual sentence representation alignment without relying on bitexts, particularly for low-resource languages.

**Method:** The authors investigate multilingual image-caption alignment as a method to implicitly align text representations across different languages.

**Key Contributions:**

	1. Demonstrated the effectiveness of using visual information for multilingual alignment.
	2. Showed that unseen languages can be incorporated into existing models post-hoc.
	3. Provided evidence for usability in cross-lingual NLU and retrieval tasks.

**Result:** The study demonstrates that visual information can effectively align text representations, allowing for the incorporation of unseen languages into the alignment process and enabling cross-lingual NLU and bitext retrieval.

**Limitations:** 

**Conclusion:** Multilingual image-caption alignment is a promising approach for bridging linguistic gaps, particularly beneficial for low-resource languages.

**Abstract:** Multilingual alignment of sentence representations has mostly required bitexts to bridge the gap between languages. We investigate whether visual information can bridge this gap instead. Image caption datasets are very easy to create without requiring multilingual expertise, so this offers a more efficient alternative for low-resource languages. We find that multilingual image-caption alignment can implicitly align the text representations between languages, languages unseen by the encoder in pretraining can be incorporated into this alignment post-hoc, and these aligned representations are usable for cross-lingual Natural Language Understanding (NLU) and bitext retrieval.

</details>


### [38] [Clarifying orthography: Orthographic transparency as compressibility](https://arxiv.org/abs/2505.13657)

*Charles J. Torres, Richard Futrell*

**Main category:** cs.CL

**Keywords:** orthographic transparency, mutual compressibility, algorithmic information theory, neural sequence models, language evaluation

**Relevance Score:** 3

**TL;DR:** This paper proposes a new metric for orthographic transparency based on mutual compressibility between orthographic and phonological strings, applicable across diverse writing systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a unified, script-agnostic metric for quantifying orthographic transparency, which lacks an existing framework.

**Method:** The authors utilize ideas from algorithmic information theory to calculate mutual compressibility between spelling and sound representations, employing prequential code-lengths from neural sequence models.

**Key Contributions:**

	1. Introduced a novel metric for orthographic transparency using mutual compressibility.
	2. Combined two factors affecting transparency—irregular spellings and rule complexity—into a single measure.
	3. Evaluated the transparency measure on a diverse set of languages and scripts.

**Result:** The metric was applied to evaluate orthographic transparency in 22 languages, demonstrating its effectiveness in capturing irregular spellings and rule complexity.

**Limitations:** 

**Conclusion:** Mutual compressibility serves as a straightforward and general measurement for assessing orthographic transparency across various languages and scripts.

**Abstract:** Orthographic transparency -- how directly spelling is related to sound -- lacks a unified, script-agnostic metric. Using ideas from algorithmic information theory, we quantify orthographic transparency in terms of the mutual compressibility between orthographic and phonological strings. Our measure provides a principled way to combine two factors that decrease orthographic transparency, capturing both irregular spellings and rule complexity in one quantity. We estimate our transparency measure using prequential code-lengths derived from neural sequence models. Evaluating 22 languages across a broad range of script types (alphabetic, abjad, abugida, syllabic, logographic) confirms common intuitions about relative transparency of scripts. Mutual compressibility offers a simple, principled, and general yardstick for orthographic transparency.

</details>


### [39] [Are Large Language Models Good at Detecting Propaganda?](https://arxiv.org/abs/2505.13706)

*Julia Jose, Rachel Greenstadt*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, propaganda detection, Large Language Models

**Relevance Score:** 6

**TL;DR:** This study evaluates the effectiveness of various Large Language Models (LLMs) in detecting propaganda techniques in news articles, comparing their performance to transformer-based models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how propaganda influences decision-making is crucial; recent NLP advancements can aid in identifying manipulative content.

**Method:** We evaluated several LLMs, including GPT-4, GPT-3.5, and Claude 3 Opus, against a RoBERTa-CRF baseline in detecting six types of propaganda techniques in news articles.

**Key Contributions:**

	1. Comparison of LLMs and transformer models in propaganda detection
	2. Demonstration of specific techniques LLMs excel in
	3. Banchmarking against traditional NLP methods for propaganda analysis

**Result:** GPT-4 had the highest F1 score among the LLMs (F1=0.16), but it was still below the RoBERTa-CRF baseline (F1=0.67). All LLMs exceeded a MultiGranularity Network (MGN) baseline in identifying name-calling, with GPT-3.5 and GPT-4 also exceeding it in appeal to fear and flag-waving.

**Limitations:** The study is limited by the low F1 scores and the focus on only six specific propaganda techniques.

**Conclusion:** While LLMs show promise in detecting specific propaganda techniques, they currently do not match the performance of established transformer baselines.

**Abstract:** Propagandists use rhetorical devices that rely on logical fallacies and emotional appeals to advance their agendas. Recognizing these techniques is key to making informed decisions. Recent advances in Natural Language Processing (NLP) have enabled the development of systems capable of detecting manipulative content. In this study, we look at several Large Language Models and their performance in detecting propaganda techniques in news articles. We compare the performance of these LLMs with transformer-based models. We find that, while GPT-4 demonstrates superior F1 scores (F1=0.16) compared to GPT-3.5 and Claude 3 Opus, it does not outperform a RoBERTa-CRF baseline (F1=0.67). Additionally, we find that all three LLMs outperform a MultiGranularity Network (MGN) baseline in detecting instances of one out of six propaganda techniques (name-calling), with GPT-3.5 and GPT-4 also outperforming the MGN baseline in detecting instances of appeal to fear and flag-waving.

</details>


### [40] [SQLForge: Synthesizing Reliable and Diverse Data to Enhance Text-to-SQL Reasoning in LLMs](https://arxiv.org/abs/2505.13725)

*Yu Guo, Dong Jin, Shenghao Ye, Shuangwu Chen, Jian Yang, Xiaobin Tan*

**Main category:** cs.CL

**Keywords:** Large Language Models, text-to-SQL, data synthesis, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** SQLForge enhances text-to-SQL reasoning in LLMs by synthesizing diverse and reliable data, resulting in improved performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap between open-source and closed-source models in text-to-SQL reasoning tasks.

**Method:** The authors introduce SQLForge, which focuses on synthesizing data using SQL syntax constraints, reverse translation from SQL to questions, SQL template enrichment, and iterative data domain exploration.

**Key Contributions:**

	1. Introduction of SQLForge for enhancing text-to-SQL reasoning
	2. Utilization of SQL syntax constraints and reverse translation for data reliability
	3. State-of-the-art performance achievements for open-source models on established benchmarks.

**Result:** SQLForge-LM, a family of fine-tuned models, achieves state-of-the-art performance on Spider and BIRD benchmarks, with EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev.

**Limitations:** 

**Conclusion:** By enhancing data reliability and diversity, SQLForge significantly improves the performance of open-source LLMs in text-to-SQL tasks.

**Abstract:** Large Language models (LLMs) have demonstrated significant potential in text-to-SQL reasoning tasks, yet a substantial performance gap persists between existing open-source models and their closed-source counterparts. In this paper, we introduce SQLForge, a novel approach for synthesizing reliable and diverse data to enhance text-to-SQL reasoning in LLMs. We improve data reliability through SQL syntax constraints and SQL-to-question reverse translation, ensuring data logic at both structural and semantic levels. We also propose an SQL template enrichment and iterative data domain exploration mechanism to boost data diversity. Building on the augmented data, we fine-tune a variety of open-source models with different architectures and parameter sizes, resulting in a family of models termed SQLForge-LM. SQLForge-LM achieves the state-of-the-art performance on the widely recognized Spider and BIRD benchmarks among the open-source models. Specifically, SQLForge-LM achieves EX accuracy of 85.7% on Spider Dev and 59.8% on BIRD Dev, significantly narrowing the performance gap with closed-source methods.

</details>


### [41] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)

*Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger*

**Main category:** cs.CL

**Keywords:** AI Safety, AI Risks, Value Prioritization, Human-Computer Interaction, Machine Learning

**Relevance Score:** 6

**TL;DR:** This paper introduces LitmusValues, an evaluation pipeline for assessing AI models' value prioritization to predict risky behaviors, using AIRiskDilemmas to measure conflicts among values relevant to AI safety.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** As AI models become more powerful and employ strategies like Alignment Faking to evade detection of risky behaviors, it is essential to identify underlying values within these models to better predict and mitigate potential risks.

**Method:** The authors propose LitmusValues, an evaluation pipeline designed to reveal AI models' priorities among various classes of values. They collect AIRiskDilemmas, a diverse set of scenarios showcasing conflicting values relevant to AI safety, to measure and predict risky behaviors based on the models' value prioritization.

**Key Contributions:**

	1. Introduction of the LitmusValues evaluation pipeline for AI value assessment
	2. Collection of AIRiskDilemmas to test AI models against value conflicts
	3. Successful prediction of risky behaviors in AI based on value prioritization

**Result:** The study demonstrates that the values assessed through LitmusValues, including ones that may appear harmless, can effectively predict both previously identified risky behaviors and novel risky behaviors found in HarmBench.

**Limitations:** The framework may require further validation across a wider variety of AI models and potential scenarios, and may not perfectly capture all nuances of AI behaviors.

**Conclusion:** By identifying and prioritizing values within AI models using the LitmusValues framework, researchers can better anticipate and address potential risks associated with advanced AI systems.

**Abstract:** Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [42] [Simulation Agent: A Framework for Integrating Simulation and Large Language Models for Enhanced Decision-Making](https://arxiv.org/abs/2505.13761)

*Jacob Kleiman, Kevin Frank, Sindy Campagna*

**Main category:** cs.CL

**Keywords:** simulation, large language models, human-computer interaction, empirical validation, user accessibility

**Relevance Score:** 8

**TL;DR:** This paper presents a simulation agent framework that combines the strengths of simulations and large language models (LLMs) to enhance user interaction and understanding of complex systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility of simulations for non-technical users using LLMs' conversational capabilities.

**Method:** The authors developed a framework that integrates LLMs with simulation models, enabling intuitive interactions while ensuring accurate representations of real-world dynamics.

**Key Contributions:**

	1. Integration of LLMs with simulation models
	2. Enhanced user accessibility to complex simulations
	3. Robust framework for empirical validation across domains

**Result:** The framework empowers users to interact with simulations effectively, leading to a better understanding of complex systems.

**Limitations:** 

**Conclusion:** By combining simulations with LLMs, the framework provides a robust platform for empirical validation and can be applied across various domains.

**Abstract:** Simulations, although powerful in accurately replicating real-world systems, often remain inaccessible to non-technical users due to their complexity. Conversely, large language models (LLMs) provide intuitive, language-based interactions but can lack the structured, causal understanding required to reliably model complex real-world dynamics. We introduce our simulation agent framework, a novel approach that integrates the strengths of both simulation models and LLMs. This framework helps empower users by leveraging the conversational capabilities of LLMs to interact seamlessly with sophisticated simulation systems, while simultaneously utilizing the simulations to ground the LLMs in accurate and structured representations of real-world phenomena. This integrated approach helps provide a robust and generalizable foundation for empirical validation and offers broad applicability across diverse domains.

</details>


### [43] [Krikri: Advancing Open Large Language Models for Greek](https://arxiv.org/abs/2505.13772)

*Dimitris Roussis, Leon Voukoutis, Georgios Paraskevopoulos, Sokratis Sofianopoulos, Prokopis Prokopidis, Vassilis Papavasileiou, Athanasios Katsamanis, Stelios Piperidis, Vassilis Katsouros*

**Main category:** cs.CL

**Keywords:** Llama-Krikri-8B, Large Language Model, Greek language, natural language processing, machine learning

**Relevance Score:** 4

**TL;DR:** Introduction of Llama-Krikri-8B, a Large Language Model optimized for Greek, demonstrating significant improvements in NLP tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a high-performance language model specifically for the Greek language that adapts to its unique linguistic features.

**Method:** The model is built on Meta's Llama 3.1-8B, trained on extensive high-quality Greek data, and includes a multi-stage post-training pipeline utilizing both human and synthetic instruction data, employing techniques like MAGPIE.

**Key Contributions:**

	1. Development of Llama-Krikri-8B as a Greek language-specific LLM
	2. Introduction of public benchmarks for evaluating Greek LLMs
	3. Improved performance in both natural language and code generation tasks for Greek.

**Result:** Llama-Krikri-8B exhibits notable advancements in natural language understanding and generation, outperforming existing Greek and multilingual LLMs.

**Limitations:** 

**Conclusion:** The model not only serves current NLP needs for Greek but also opens new avenues for language model capabilities in resource-scarce languages.

**Abstract:** We introduce Llama-Krikri-8B, a cutting-edge Large Language Model tailored for the Greek language, built on Meta's Llama 3.1-8B. Llama-Krikri-8B has been extensively trained on high-quality Greek data to ensure superior adaptation to linguistic nuances. With 8 billion parameters, it offers advanced capabilities while maintaining efficient computational performance. Llama-Krikri-8B supports both Modern Greek and English, and is also equipped to handle polytonic text and Ancient Greek. The chat version of Llama-Krikri-8B features a multi-stage post-training pipeline, utilizing both human and synthetic instruction and preference data, by applying techniques such as MAGPIE. In addition, for evaluation, we propose three novel public benchmarks for Greek. Our evaluation on existing as well as the proposed benchmarks shows notable improvements over comparable Greek and multilingual LLMs in both natural language understanding and generation as well as code generation.

</details>


### [44] [Interpretable Traces, Unexpected Outcomes: Investigating the Disconnect in Trace-Based Knowledge Distillation](https://arxiv.org/abs/2505.13792)

*Siddhant Bhambri, Upasana Biswas, Subbarao Kambhampati*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Question Answering, Chain-of-Thought, Reasoning Traces, Smaller Language Models

**Relevance Score:** 9

**TL;DR:** This paper explores the evaluation of reasoning traces in Knowledge Distillation for Smaller Language Models, revealing that correctness of traces does not guarantee accurate final outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study focuses on improving the performance of smaller language models in QA tasks by addressing the evaluation of reasoning traces used in Knowledge Distillation.

**Method:** The authors employ a Knowledge Distillation method that uses rule-based problem decomposition to create structured sub-problems for easier evaluation of reasoning traces.

**Key Contributions:**

	1. Introduces a method for structured problem decomposition in QA tasks
	2. Evaluates the faithfulness of reasoning traces in KD
	3. Uncovers low correlation between reasoning trace correctness and final model outputs

**Result:** Experiments demonstrate that while correct reasoning traces are essential, they do not consistently correlate with correct final model outputs, challenging existing assumptions in using these traces.

**Limitations:** The scope is limited to specific datasets and may not generalize to all QA problems or languages.

**Conclusion:** The findings suggest that relying solely on reasoning traces for enhancing smaller models' performance might be misleading, urging a reconsideration of current Knowledge Distillation practices.

**Abstract:** Question Answering (QA) poses a challenging and critical problem, particularly in today's age of interactive dialogue systems such as ChatGPT, Perplexity, Microsoft Copilot, etc. where users demand both accuracy and transparency in the model's outputs. Since smaller language models (SLMs) are computationally more efficient but often under-perform compared to larger models, Knowledge Distillation (KD) methods allow for finetuning these smaller models to improve their final performance. Lately, the intermediate tokens or the so called `reasoning' traces produced by Chain-of-Thought (CoT) or by reasoning models such as DeepSeek R1 are used as a training signal for KD. However, these reasoning traces are often verbose and difficult to interpret or evaluate. In this work, we aim to address the challenge of evaluating the faithfulness of these reasoning traces and their correlation with the final performance. To this end, we employ a KD method leveraging rule-based problem decomposition. This approach allows us to break down complex queries into structured sub-problems, generating interpretable traces whose correctness can be readily evaluated, even at inference time. Specifically, we demonstrate this approach on Open Book QA, decomposing the problem into a Classification step and an Information Retrieval step, thereby simplifying trace evaluation. Our SFT experiments with correct and incorrect traces on the CoTemp QA, Microsoft Machine Reading Comprehension QA, and Facebook bAbI QA datasets reveal the striking finding that correct traces do not necessarily imply that the model outputs the correct final solution. Similarly, we find a low correlation between correct final solutions and intermediate trace correctness. These results challenge the implicit assumption behind utilizing reasoning traces for improving SLMs' final performance via KD.

</details>


### [45] [EfficientLLM: Efficiency in Large Language Models](https://arxiv.org/abs/2505.13840)

*Zhengqing Yuan, Weixiang Sun, Yixin Liu, Huichi Zhou, Rong Zhou, Yiyang Li, Zheyuan Zhang, Wei Song, Yue Huang, Haolong Jia, Keerthiram Murugesan, Yu Wang, Lifang He, Jianfeng Gao, Lichao Sun, Yanfang Ye*

**Main category:** cs.CL

**Keywords:** Large Language Models, Efficiency, Benchmarking, Fine-tuning, Inference

**Relevance Score:** 9

**TL;DR:** EfficientLLM is a benchmark study evaluating efficiency techniques for Large Language Models, examining architecture, fine-tuning, and inference methods to enhance performance while managing compute and energy costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the prohibitive costs associated with the growing parameter counts and context windows of Large Language Models (LLMs).

**Method:** A comprehensive empirical study on a production-class cluster, systematically exploring architecture pretraining, fine-tuning, and inference techniques while defining metrics for evaluation.

**Key Contributions:**

	1. Introduction of EfficientLLM benchmark for LLM efficiency evaluation
	2. In-depth comparative analysis of efficiency techniques
	3. Open-sourcing resources for the research community

**Result:** Key insights include quantifiable trade-offs, task- and scale-dependent optima, and generalization across modalities, impacting performance and efficiency assessments.

**Limitations:** 

**Conclusion:** EfficientLLM opens new avenues for researchers and engineers by providing datasets, evaluation pipelines, and leaderboards, promoting a better understanding of LLM efficiency-performance complexities.

**Abstract:** Large Language Models (LLMs) have driven significant progress, yet their growing parameter counts and context windows incur prohibitive compute, energy, and monetary costs. We introduce EfficientLLM, a novel benchmark and the first comprehensive empirical study evaluating efficiency techniques for LLMs at scale. Conducted on a production-class cluster (48xGH200, 8xH200 GPUs), our study systematically explores three key axes: (1) architecture pretraining (efficient attention variants: MQA, GQA, MLA, NSA; sparse Mixture-of-Experts (MoE)), (2) fine-tuning (parameter-efficient methods: LoRA, RSLoRA, DoRA), and (3) inference (quantization methods: int4, float16). We define six fine-grained metrics (Memory Utilization, Compute Utilization, Latency, Throughput, Energy Consumption, Compression Rate) to capture hardware saturation, latency-throughput balance, and carbon cost. Evaluating over 100 model-technique pairs (0.5B-72B parameters), we derive three core insights: (i) Efficiency involves quantifiable trade-offs: no single method is universally optimal; e.g., MoE reduces FLOPs and improves accuracy but increases VRAM by 40%, while int4 quantization cuts memory/energy by up to 3.9x at a 3-5% accuracy drop. (ii) Optima are task- and scale-dependent: MQA offers optimal memory-latency trade-offs for constrained devices, MLA achieves lowest perplexity for quality-critical tasks, and RSLoRA surpasses LoRA efficiency only beyond 14B parameters. (iii) Techniques generalize across modalities: we extend evaluations to Large Vision Models (Stable Diffusion 3.5, Wan 2.1) and Vision-Language Models (Qwen2.5-VL), confirming effective transferability. By open-sourcing datasets, evaluation pipelines, and leaderboards, EfficientLLM provides essential guidance for researchers and engineers navigating the efficiency-performance landscape of next-generation foundation models.

</details>


### [46] [Improve Language Model and Brain Alignment via Associative Memory](https://arxiv.org/abs/2505.13844)

*Congchi Yin, Yongpeng Zhang, Xuyun Wen, Piji Li*

**Main category:** cs.CL

**Keywords:** associative memory, language models, brain alignment, human cognition, supervised fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper explores the enhancement of alignment between language models and human brain functions in speech processing by incorporating associative memory, leading to improved performance in comprehension tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment between computational language models and human cognition, particularly in processing speech through the integration of associative memory.

**Method:** We mapped language model activations to brain activities to verify alignment and tested expanded text stimuli with simulated associative memory as inputs to language models, alongside the development of the Association dataset containing 1000 story samples.

**Key Contributions:**

	1. Demonstrated the use of associative memory for improved language model and brain alignment.
	2. Created the Association dataset with specialized story samples to encourage associative memory engagement in training.
	3. Showed that supervised fine-tuning enhances language model performance in understanding speech-related tasks.

**Result:** The study shows that language models aligned better with brain regions associated with associative memory after fine-tuning on the Association dataset, which includes stimuli designed to engage associative memory.

**Limitations:** 

**Conclusion:** Incorporating associative memory into language models significantly improves their alignment with human brain responses, suggesting a pathway for enhancing natural language understanding in AI systems.

**Abstract:** Associative memory engages in the integration of relevant information for comprehension in the human cognition system. In this work, we seek to improve alignment between language models and human brain while processing speech information by integrating associative memory. After verifying the alignment between language model and brain by mapping language model activations to brain activity, the original text stimuli expanded with simulated associative memory are regarded as input to computational language models. We find the alignment between language model and brain is improved in brain regions closely related to associative memory processing. We also demonstrate large language models after specific supervised fine-tuning better align with brain response, by building the \textit{Association} dataset containing 1000 samples of stories, with instructions encouraging associative memory as input and associated content as output.

</details>


### [47] [Domain Gating Ensemble Networks for AI-Generated Text Detection](https://arxiv.org/abs/2505.13855)

*Arihant Tripathi, Liam Dugan, Charis Gao, Maggie Huan, Emma Jin, Peter Zhang, David Zhang, Julia Zhao, Chris Callison-Burch*

**Main category:** cs.CL

**Keywords:** Domain Adaptation, Text Detection, Machine Learning

**Relevance Score:** 8

**TL;DR:** DoGEN is a novel technique for adapting text detectors to unseen domains through an ensemble of domain expert models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of detecting machine-generated text across diverse domains, especially with the rise of advanced language models.

**Method:** DoGEN leverages a domain classifier to weight an ensemble of domain expert detector models, enhancing adaptability to unseen domains.

**Key Contributions:**

	1. Introduction of DoGEN for domain-adaptive text detection
	2. Demonstrated superior performance in unseen domains
	3. Releasing code and models for community research

**Result:** Achieves state-of-the-art performance in detecting machine-generated text in both in-domain and out-of-domain scenarios, outperforming larger models in out-of-domain detection.

**Limitations:** 

**Conclusion:** DoGEN provides a promising approach for domain-adaptive AI detection, with code and models released for future research.

**Abstract:** As state-of-the-art language models continue to improve, the need for robust detection of machine-generated text becomes increasingly critical. However, current state-of-the-art machine text detectors struggle to adapt to new unseen domains and generative models. In this paper we present DoGEN (Domain Gating Ensemble Networks), a technique that allows detectors to adapt to unseen domains by ensembling a set of domain expert detector models using weights from a domain classifier. We test DoGEN on a wide variety of domains from leading benchmarks and find that it achieves state-of-the-art performance on in-domain detection while outperforming models twice its size on out-of-domain detection. We release our code and trained models to assist in future research in domain-adaptive AI detection.

</details>


### [48] [Reasoning Path Compression: Compressing Generation Trajectories for Efficient LLM Reasoning](https://arxiv.org/abs/2505.13866)

*Jiwon Song, Dongwon Jo, Yulhwa Kim, Jae-Joon Kim*

**Main category:** cs.CL

**Keywords:** Reasoning Path Compression, Language Models, Inference Efficiency

**Relevance Score:** 8

**TL;DR:** Introducing Reasoning Path Compression (RPC) to enhance the efficiency of reasoning-focused language models by reducing memory usage and increasing generation throughput.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high memory usage and throughput limitations of lengthy reasoning paths in reasoning-based language models.

**Method:** RPC compresses the KV cache by retaining high importance scores based on a selector window of recent queries, thereby reducing the resource burden during inference.

**Key Contributions:**

	1. Proposed Reasoning Path Compression (RPC) method
	2. Demonstrated significant improvement in generation throughput
	3. Provided a practical approach for efficient reasoning model deployment

**Result:** RPC achieves up to 1.60× improvement in generation throughput with only a 1.2% accuracy drop on the AIME 2024 benchmark.

**Limitations:** Accuracy drop of 1.2% on the benchmark during compression.

**Conclusion:** Leveraging semantic sparsity in reasoning paths presents a viable strategy for the efficient deployment of reasoning LLMs.

**Abstract:** Recent reasoning-focused language models achieve high accuracy by generating lengthy intermediate reasoning paths before producing final answers. While this approach is effective in solving problems that require logical thinking, long reasoning paths significantly increase memory usage and throughput of token generation, limiting the practical deployment of such models. We propose Reasoning Path Compression (RPC), a training-free method that accelerates inference by leveraging the semantic sparsity of reasoning paths. RPC periodically compresses the KV cache by retaining KV cache that receive high importance score, which are computed using a selector window composed of recently generated queries. Experiments show that RPC improves generation throughput of QwQ-32B by up to 1.60$\times$ compared to the inference with full KV cache, with an accuracy drop of 1.2% on the AIME 2024 benchmark. Our findings demonstrate that semantic sparsity in reasoning traces can be effectively exploited for compression, offering a practical path toward efficient deployment of reasoning LLMs. Our code is available at https://github.com/jiwonsong-dev/ReasoningPathCompression.

</details>


### [49] [Code2Logic: Game-Code-Driven Data Synthesis for Enhancing VLMs General Reasoning](https://arxiv.org/abs/2505.13886)

*Jingqi Tong, Jixin Tang, Hangcheng Li, Yurong Mou, Ming Zhang, Jun Zhao, Yanbo Wen, Fan Song, Jiahao Zhan, Yuyang Lu, Chaoran Tao, Zhiyuan Guo, Jizhou Yu, Tianhao Cheng, Changhao Jiang, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Weifeng Ge, Guanhua Chen, Tao Gui, Xipeng Qiu, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Vision Language Models, multimodal reasoning, game code, Large Language Models, dataset

**Relevance Score:** 7

**TL;DR:** The paper introduces Code2Logic, a method for synthesizing multimodal reasoning data from game code to improve Vision Language Models (VLMs) performance and proposes the GameQA dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Visual-language CoT data is limited, hindering VLM reasoning capabilities; game code offers a more efficient way to generate this data.

**Method:** Code2Logic leverages Large Language Models to adapt game code for automatic reasoning process acquisition and results via code execution.

**Key Contributions:**

	1. Introduction of a game-code-driven approach for multimodal reasoning data synthesis.
	2. Development of the GameQA dataset for training and evaluating VLMs.
	3. Demonstration of out of domain generalization in VLMs trained on game data.

**Result:** Using Code2Logic to create the GameQA dataset resulted in VLMs demonstrating out of domain generalization, with Qwen2.5-VL-7B improving by 2.33% across 7 vision-language benchmarks.

**Limitations:** The evaluation is primarily based on game data; real-world applicability is not fully established.

**Conclusion:** GameQA is a scalable, cost-effective dataset source that enhances VLMs, challenging them and improving their performance on diverse tasks.

**Abstract:** Visual-language Chain-of-Thought (CoT) data resources are relatively scarce compared to text-only counterparts, limiting the improvement of reasoning capabilities in Vision Language Models (VLMs). However, high-quality vision-language reasoning data is expensive and labor-intensive to annotate. To address this issue, we leverage a promising resource: game code, which naturally contains logical structures and state transition processes. Therefore, we propose Code2Logic, a novel game-code-driven approach for multimodal reasoning data synthesis. Our approach leverages Large Language Models (LLMs) to adapt game code, enabling automatic acquisition of reasoning processes and results through code execution. Using the Code2Logic approach, we developed the GameQA dataset to train and evaluate VLMs. GameQA is cost-effective and scalable to produce, challenging for state-of-the-art models, and diverse with 30 games and 158 tasks. Surprisingly, despite training solely on game data, VLMs demonstrated out of domain generalization, specifically Qwen2.5-VL-7B improving performance by 2.33\% across 7 diverse vision-language benchmarks. Our code and dataset are available at https://github.com/tongjingqi/Code2Logic.

</details>


### [50] [Mapping the Minds of LLMs: A Graph-Based Analysis of Reasoning LLM](https://arxiv.org/abs/2505.13890)

*Zhen Xiong, Yujun Cai, Zhecheng Li, Yiwei Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Graph-based analysis, Prompt engineering, Cognitive analysis

**Relevance Score:** 9

**TL;DR:** This paper presents a unified graph-based framework to analyze reasoning processes in Reasoning Large Language Models (RLMs) by clustering outputs into coherent steps and constructing reasoning graphs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the counterintuitive behaviors of RLMs and enhance understanding of their reasoning abilities under various prompting conditions.

**Method:** A graph-based analytical framework that clusters Chain-of-Thought outputs and constructs directed reasoning graphs to analyze reasoning processes.

**Key Contributions:**

	1. Introduction of a unified graph-based framework for analyzing RLM reasoning processes
	2. Demonstration of correlation between structural properties of reasoning graphs and reasoning accuracy
	3. Practical insights for prompt engineering and cognitive analysis of LLMs

**Result:** The study reveals structural properties of reasoning graphs, like exploration density and branching, correlate with reasoning accuracy and can inform better prompting strategies.

**Limitations:** The study may not cover all types of reasoning tasks or models, and the findings are based on specific prompting regimes.

**Conclusion:** The proposed framework allows for better evaluation of reasoning quality in RLMs and provides insightful guidelines for prompt engineering.

**Abstract:** Recent advances in test-time scaling have enabled Large Language Models (LLMs) to display sophisticated reasoning abilities via extended Chain-of-Thought (CoT) generation. Despite their potential, these Reasoning LLMs (RLMs) often demonstrate counterintuitive and unstable behaviors, such as performance degradation under few-shot prompting, that challenge our current understanding of RLMs. In this work, we introduce a unified graph-based analytical framework for better modeling the reasoning processes of RLMs. Our method first clusters long, verbose CoT outputs into semantically coherent reasoning steps, then constructs directed reasoning graphs to capture contextual and logical dependencies among these steps. Through comprehensive analysis across models and prompting regimes, we reveal that structural properties, such as exploration density, branching, and convergence ratios, strongly correlate with reasoning accuracy. Our findings demonstrate how prompting strategies substantially reshape the internal reasoning structure of RLMs, directly affecting task outcomes. The proposed framework not only enables quantitative evaluation of reasoning quality beyond conventional metrics but also provides practical insights for prompt engineering and the cognitive analysis of LLMs. Code and resources will be released to facilitate future research in this direction.

</details>


### [51] [InfiGFusion: Graph-on-Logits Distillation via Efficient Gromov-Wasserstein for Model Fusion](https://arxiv.org/abs/2505.13893)

*Yuanyi Wang, Zhaoyi Yan, Yiming Zhang, Qi Zhou, Yanggan Gu, Fei Wu, Hongxia Yang*

**Main category:** cs.CL

**Keywords:** Large language models, Model fusion, Graph-based models

**Relevance Score:** 9

**TL;DR:** InfiGFusion is a novel structure-aware fusion framework aimed at improving large language model integrations by modeling semantic dependencies among vocabulary dimensions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Recent advances in large language models necessitate improved methods for fusing heterogeneous models while accounting for their semantic dependencies, which can enhance their complementary strengths in generation tasks.

**Method:** InfiGFusion utilizes a Graph-on-Logits Distillation (GLD) loss, creating a global co-activation graph from the top-k logits and employing a closed-form approximation to optimize computational efficiency.

**Key Contributions:**

	1. First structure-aware fusion framework with GLD loss
	2. Efficient sorting-based approximation for Gromov-Wasserstein distance
	3. Proven enhancements in reasoning benchmarks over SOTA models

**Result:** GLD significantly enhances fusion quality and stability, outpacing state-of-the-art models across 11 benchmarks, especially in complex reasoning tasks.

**Limitations:** 

**Conclusion:** InfiGFusion demonstrates superior multi-step and relational inference capabilities, establishing itself as a robust framework for model fusion.

**Abstract:** Recent advances in large language models (LLMs) have intensified efforts to fuse heterogeneous open-source models into a unified system that inherits their complementary strengths. Existing logit-based fusion methods maintain inference efficiency but treat vocabulary dimensions independently, overlooking semantic dependencies encoded by cross-dimension interactions. These dependencies reflect how token types interact under a model's internal reasoning and are essential for aligning models with diverse generation behaviors. To explicitly model these dependencies, we propose \textbf{InfiGFusion}, the first structure-aware fusion framework with a novel \textit{Graph-on-Logits Distillation} (GLD) loss. Specifically, we retain the top-$k$ logits per output and aggregate their outer products across sequence positions to form a global co-activation graph, where nodes represent vocabulary channels and edges quantify their joint activations. To ensure scalability and efficiency, we design a sorting-based closed-form approximation that reduces the original $O(n^4)$ cost of Gromov-Wasserstein distance to $O(n \log n)$, with provable approximation guarantees. Experiments across multiple fusion settings show that GLD consistently improves fusion quality and stability. InfiGFusion outperforms SOTA models and fusion baselines across 11 benchmarks spanning reasoning, coding, and mathematics. It shows particular strength in complex reasoning tasks, with +35.6 improvement on Multistep Arithmetic and +37.06 on Causal Judgement over SFT, demonstrating superior multi-step and relational inference.

</details>


### [52] [Let's Verify Math Questions Step by Step](https://arxiv.org/abs/2505.13903)

*Chengyu Shen, Zhen Hao Wong, Runming He, Hao Liang, Meiyi Qiang, Zimo Meng, Zhengyang Zhao, Bohan Zeng, Zhengzhou Zhu, Bin Cui, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** MathQ-Verify, Question Verification, Mathematical Reasoning, Large Language Models, Dataset Curation

**Relevance Score:** 4

**TL;DR:** This paper introduces Math Question Verification (MathQ-Verify), a pipeline for detecting invalid math problems, achieving state-of-the-art performance in data accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the oversight in existing research which primarily focuses on generating correct answers while neglecting the validity of math questions.

**Method:** MathQ-Verify consists of a five-stage pipeline that formats, formalizes, decomposes, verifies, and checks mathematical questions for completeness and contradictions.

**Key Contributions:**

	1. Development of a five-stage verification pipeline for math questions
	2. Achievement of state-of-the-art performance in mathematical dataset curation
	3. Creation of a new dataset with diverse error types for evaluation.

**Result:** MathQ-Verify improves F1 scores by up to 25 percentage points and achieves approximately 90% precision and 63% recall on benchmark tests, showcasing its effectiveness in filtering invalid questions.

**Limitations:** 

**Conclusion:** MathQ-Verify is a reliable method for curating valid mathematical questions, which can enhance the quality of training data for LLMs in mathematical reasoning tasks.

**Abstract:** Large Language Models (LLMs) have recently achieved remarkable progress in mathematical reasoning. To enable such capabilities, many existing works distill strong reasoning models into long chains of thought or design algorithms to construct high-quality math QA data for training. However, these efforts primarily focus on generating correct reasoning paths and answers, while largely overlooking the validity of the questions themselves. In this work, we propose Math Question Verification (MathQ-Verify), a novel five-stage pipeline designed to rigorously filter ill-posed or under-specified math problems. MathQ-Verify first performs format-level validation to remove redundant instructions and ensure that each question is syntactically well-formed. It then formalizes each question, decomposes it into atomic conditions, and verifies them against mathematical definitions. Next, it detects logical contradictions among these conditions, followed by a goal-oriented completeness check to ensure the question provides sufficient information for solving. To evaluate this task, we use existing benchmarks along with an additional dataset we construct, containing 2,147 math questions with diverse error types, each manually double-validated. Experiments show that MathQ-Verify achieves state-of-the-art performance across multiple benchmarks, improving the F1 score by up to 25 percentage points over the direct verification baseline. It further attains approximately 90% precision and 63% recall through a lightweight model voting scheme. MathQ-Verify offers a scalable and accurate solution for curating reliable mathematical datasets, reducing label noise and avoiding unnecessary computation on invalid questions. Our code and data are available at https://github.com/scuuy/MathQ-Verify.

</details>


### [53] [Cross-Linguistic Transfer in Multilingual NLP: The Role of Language Families and Morphology](https://arxiv.org/abs/2505.13908)

*Ajitesh Bankula, Praney Bankula*

**Main category:** cs.CL

**Keywords:** cross-lingual transfer, multilingual NLP, language families, morphological similarity, transfer learning

**Relevance Score:** 6

**TL;DR:** The paper explores cross-lingual transfer in multilingual NLP, examining the impact of language family proximity and morphological similarity on performance across tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of multilingual models in transferring knowledge from resource-rich to low-resource languages.

**Method:** The study investigates performance across various NLP tasks in relation to linguistic distance, focusing on language families and morphology.

**Key Contributions:**

	1. Analysis of cross-linguistic performance based on language families and morphological features.
	2. Correlation between linguistic distance and model performance in multilingual NLP tasks.
	3. Evaluation of emerging approaches for enhancing pre-training with typological information.

**Result:** Findings show correlations between language family proximity, morphological similarity, and transfer performance in multilingual models.

**Limitations:** 

**Conclusion:** Integrating typological and morphological information into model pre-training can improve cross-linguistic transfer outcomes.

**Abstract:** Cross-lingual transfer has become a crucial aspect of multilingual NLP, as it allows for models trained on resource-rich languages to be applied to low-resource languages more effectively. Recently massively multilingual pre-trained language models (e.g., mBERT, XLM-R) demonstrate strong zero-shot transfer capabilities[14] [13]. This paper investigates cross-linguistic transfer through the lens of language families and morphology. Investigating how language family proximity and morphological similarity affect performance across NLP tasks. We further discuss our results and how it relates to findings from recent literature. Overall, we compare multilingual model performance and review how linguistic distance metrics correlate with transfer outcomes. We also look into emerging approaches that integrate typological and morphological information into model pre-training to improve transfer to diverse languages[18] [19].

</details>


### [54] [Word length predicts word order: "Min-max"-ing drives language evolution](https://arxiv.org/abs/2505.13913)

*Hiram Ring*

**Main category:** cs.CL

**Keywords:** word order, language evolution, processing, information structure, language families

**Relevance Score:** 2

**TL;DR:** This paper proposes a universal mechanism for word order change based on a dataset of over 1,500 languages, suggesting a correlation between word class length and word order, contributing to theories of language evolution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address disagreements between theories regarding mechanisms driving language evolution and provide a comprehensive understanding of word order changes across languages.

**Method:** Analysis of a large tagged parallel dataset of over 1,500 languages across 133 families and 111 isolates to examine correlations between word class length and word order.

**Key Contributions:**

	1. Universal mechanism for word order change
	2. Correlation between word class length and word order
	3. Integrated theory aligning with efficiency-oriented and information-theoretic proposals

**Result:** Findings show a significant but complex correlation between word class length and word order, offering partial support for existing theories and predicting historical changes in two phylogenetic lines.

**Limitations:** 

**Conclusion:** The study supports an integrated 'Min-Max' theory of language evolution, influenced by processing and information structure, and suggests a shift in understanding language evolution mechanisms.

**Abstract:** Current theories of language propose an innate (Baker 2001; Chomsky 1981) or a functional (Greenberg 1963; Dryer 2007; Hawkins 2014) origin for the surface structures (i.e. word order) that we observe in languages of the world, while evolutionary modeling (Dunn et al. 2011) suggests that descent is the primary factor influencing such patterns. Although there are hypotheses for word order change from both innate and usage-based perspectives for specific languages and families, there are key disagreements between the two major proposals for mechanisms that drive the evolution of language more broadly (Wasow 2002; Levy 2008). This paper proposes a universal underlying mechanism for word order change based on a large tagged parallel dataset of over 1,500 languages representing 133 language families and 111 isolates. Results indicate that word class length is significantly correlated with word order crosslinguistically, but not in a straightforward manner, partially supporting opposing theories of processing, while at the same time predicting historical word order change in two different phylogenetic lines and explaining more variance than descent or language area in regression models. Such findings suggest an integrated "Min-Max" theory of language evolution driven by competing pressures of processing and information structure, aligning with recent efficiency-oriented (Levshina 2023) and information-theoretic proposals (Zaslavsky 2020; Tucker et al. 2025).

</details>


### [55] [EEG-to-Text Translation: A Model for Deciphering Human Brain Activity](https://arxiv.org/abs/2505.13936)

*Saydul Akbar Murad, Ashim Dahal, Nick Rahimi*

**Main category:** cs.CL

**Keywords:** EEG-to-text, LSTM, transformer, language models, decoding

**Relevance Score:** 8

**TL;DR:** The R1 Translator model significantly improves EEG-to-text decoding by combining a bidirectional LSTM encoder with a transformer decoder, outperforming existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap between the human brain and language processing by improving EEG-to-text decoding performance.

**Method:** The R1 Translator utilizes a bidirectional LSTM encoder to capture sequential EEG data, which is processed by a pretrained transformer-based decoder for generating text.

**Key Contributions:**

	1. Introduction of the R1 Translator for EEG-to-text decoding.
	2. Combination of LSTM and transformer models for improved performance.
	3. Significant improvements over existing models in several metrics.

**Result:** R1 achieves a ROUGE-1 score of 38.00%, outperforming T5 and Brain Translator. R1 also excels in ROUGE-L, CER, and WER metrics, showing substantial improvement over prior models.

**Limitations:** 

**Conclusion:** The R1 Translator demonstrates considerable advancements in EEG-to-text decoding, promoting better integration of neural activity with language generation.

**Abstract:** With the rapid advancement of large language models like Gemini, GPT, and others, bridging the gap between the human brain and language processing has become an important area of focus. To address this challenge, researchers have developed various models to decode EEG signals into text. However, these models still face significant performance limitations. To overcome these shortcomings, we propose a new model, R1 Translator, which aims to improve the performance of EEG-to-text decoding. The R1 Translator model combines a bidirectional LSTM encoder with a pretrained transformer-based decoder, utilizing EEG features to produce high-quality text outputs. The model processes EEG embeddings through the LSTM to capture sequential dependencies, which are then fed into the transformer decoder for effective text generation. The R1 Translator excels in ROUGE metrics, outperforming both T5 (previous research) and Brain Translator. Specifically, R1 achieves a ROUGE-1 score of 38.00% (P), which is up to 9% higher than T5 (34.89%) and 3% better than Brain (35.69%). It also leads in ROUGE-L, with a F1 score of 32.51%, outperforming T5 by 3% (29.67%) and Brain by 2% (30.38%). In terms of CER, R1 achieves a CER of 0.5795, which is 2% lower than T5 (0.5917) and 4% lower than Brain (0.6001). Additionally, R1 performs better in WER with a score of 0.7280, outperforming T5 by 4.3% (0.7610) and Brain by 3.6% (0.7553). Code is available at https://github.com/Mmurrad/EEG-To-text.

</details>


### [56] [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/abs/2409.11726)

*Wenyuan Zhang, Shuaiyi Nie, Jiawei Sheng, Zefeng Zhang, Xinghua Zhang, Yongquan He, Tingwen Liu*

**Main category:** cs.CL

**Keywords:** large language models, role-playing, knowledge errors, error detection, evaluation benchmark

**Relevance Score:** 8

**TL;DR:** This paper presents RoleKE-Bench, a framework to evaluate LLMs' detection of known and unknown knowledge errors during role-playing, revealing that current models struggle with error detection, especially with familiar knowledge, and introduces a new reasoning strategy to enhance performance.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of low-quality character training corpora due to LLMs' inability to detect knowledge errors while role-playing.

**Method:** The paper proposes RoleKE-Bench for evaluating LLMs' detection capabilities regarding known and unknown knowledge errors, employing various reasoning strategies and introducing the Self-Recollection and Self-Doubt (S$^2$RD) method for improvement.

**Key Contributions:**

	1. Introduction of RoleKE-Bench for evaluating LLMs' error detection
	2. Demonstration of LLMs' struggles with KKE and UKE
	3. Development of S$^2$RD as a reasoning method to enhance error detection

**Result:** The experiments demonstrate that existing LLMs have significant difficulty in identifying both known and unknown errors in character knowledge, particularly for familiar knowledge, yet the proposed S$^2$RD method shows promise in enhancing their detection abilities.

**Limitations:** The proposed method shows improvements but the fundamental issue of error detection continues to require more research.

**Conclusion:** Despite improvements with S$^2$RD, the issue of knowledge error detection in LLM role-playing remains a challenge needing further research.

**Abstract:** Large language model (LLM) role-playing has gained widespread attention. Authentic character knowledge is crucial for constructing realistic LLM role-playing agents. However, existing works usually overlook the exploration of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown knowledge errors (UKE) while playing roles, which would lead to low-quality automatic construction of character trainable corpus. In this paper, we propose RoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The results indicate that even the latest LLMs struggle to detect these two types of errors effectively, especially when it comes to familiar knowledge. We experimented with various reasoning strategies and propose an agent-based reasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore further the potential for improving error detection capabilities. Experiments show that our method effectively improves the LLMs' ability to detect error character knowledge, but it remains an issue that requires ongoing attention.

</details>


### [57] [Towards Rehearsal-Free Continual Relation Extraction: Capturing Within-Task Variance with Adaptive Prompting](https://arxiv.org/abs/2505.13944)

*Bao-Ngoc Dao, Quang Nguyen, Luyen Ngo Dinh, Minh Le, Nam Le, Linh Ngo Van*

**Main category:** cs.CL

**Keywords:** Continual Relation Extraction, prompt-based methods, machine learning, memory usage, generative model

**Relevance Score:** 8

**TL;DR:** WAVE++ is a novel approach for Continual Relation Extraction that utilizes task-specific prompt pools and a generative model to enhance performance without requiring data storage.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address memory usage and privacy concerns in Continual Relation Extraction while overcoming the challenges faced by prompt-based methods.

**Method:** WAVE++ introduces task-specific prompt pools and a training-free mechanism for task prediction, integrating a generative model to consolidate knowledge without explicit data storage.

**Key Contributions:**

	1. Introduction of task-specific prompt pools
	2. Development of a training-free mechanism for task prediction
	3. Integration of a generative model for knowledge consolidation

**Result:** WAVE++ demonstrates superior performance compared to existing prompt-based and rehearsal-based methods in continual relation extraction tasks.

**Limitations:** 

**Conclusion:** The proposed method effectively captures task variations and significantly improves task prediction and relation classification.

**Abstract:** Memory-based approaches have shown strong performance in Continual Relation Extraction (CRE). However, storing examples from previous tasks increases memory usage and raises privacy concerns. Recently, prompt-based methods have emerged as a promising alternative, as they do not rely on storing past samples. Despite this progress, current prompt-based techniques face several core challenges in CRE, particularly in accurately identifying task identities and mitigating catastrophic forgetting. Existing prompt selection strategies often suffer from inaccuracies, lack robust mechanisms to prevent forgetting in shared parameters, and struggle to handle both cross-task and within-task variations. In this paper, we propose WAVE++, a novel approach inspired by the connection between prefix-tuning and mixture of experts. Specifically, we introduce task-specific prompt pools that enhance flexibility and adaptability across diverse tasks while avoiding boundary-spanning risks; this design more effectively captures variations within each task and across tasks. To further refine relation classification, we incorporate label descriptions that provide richer, more global context, enabling the model to better distinguish among different relations. We also propose a training-free mechanism to improve task prediction during inference. Moreover, we integrate a generative model to consolidate prior knowledge within the shared parameters, thereby removing the need for explicit data storage. Extensive experiments demonstrate that WAVE++ outperforms state-of-the-art prompt-based and rehearsal-based methods, offering a more robust solution for continual relation extraction. Our code is publicly available at https://github.com/PiDinosauR2804/WAVE-CRE-PLUS-PLUS.

</details>


### [58] [Memory-Centric Embodied Question Answer](https://arxiv.org/abs/2505.13948)

*Mingliang Zhai, Zhi Gao, Yuwei Wu, Yunde Jia*

**Main category:** cs.CL

**Keywords:** Embodied Question Answering, Memory framework, Artificial Intelligence, Multi-modal memory, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** MemoryEQA is a memory-centric framework for Embodied Question Answering that enhances task efficiency and accuracy by improving memory interaction among modules.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and accuracy of Embodied Question Answering systems by enhancing the interaction of the memory module with other components.

**Method:** The authors propose a multi-modal hierarchical memory mechanism that includes global and local memory to better integrate with various modules in EQA.

**Key Contributions:**

	1. Introduction of MemoryEQA framework which allows flexible memory interactions with all EQA modules.
	2. Development of a multi-modal hierarchical memory mechanism for enhanced context understanding.
	3. Construction of the MT-HM3D dataset for evaluating memory capabilities in EQA tasks.

**Result:** The MemoryEQA framework achieved a 19.8% performance improvement on the MT-HM3D dataset compared to baseline models, demonstrating its effectiveness in handling complex tasks.

**Limitations:** 

**Conclusion:** Memory capabilities significantly contribute to the resolution of complex Embodied Question Answering tasks, as evidenced by improved performance metrics.

**Abstract:** Embodied Question Answering (EQA) requires agents to autonomously explore and understand the environment to answer context-dependent questions. Existing frameworks typically center around the planner, which guides the stopping module, memory module, and answering module for reasoning. In this paper, we propose a memory-centric EQA framework named MemoryEQA. Unlike planner-centric EQA models where the memory module cannot fully interact with other modules, MemoryEQA flexible feeds memory information into all modules, thereby enhancing efficiency and accuracy in handling complex tasks, such as those involving multiple targets across different regions. Specifically, we establish a multi-modal hierarchical memory mechanism, which is divided into global memory that stores language-enhanced scene maps, and local memory that retains historical observations and state information. When performing EQA tasks, the multi-modal large language model is leveraged to convert memory information into the required input formats for injection into different modules. To evaluate EQA models' memory capabilities, we constructed the MT-HM3D dataset based on HM3D, comprising 1,587 question-answer pairs involving multiple targets across various regions, which requires agents to maintain memory of exploration-acquired target information. Experimental results on HM-EQA, MT-HM3D, and OpenEQA demonstrate the effectiveness of our framework, where a 19.8% performance gain on MT-HM3D compared to baseline model further underscores memory capability's pivotal role in resolving complex tasks.

</details>


### [59] [FlashThink: An Early Exit Method For Efficient Reasoning](https://arxiv.org/abs/2505.13949)

*Guochao Jiang, Guofeng Quan, Zepeng Ding, Ziqin Luo, Dixuan Wang, Zheng Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Efficiency, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper presents FlashThink, a method for early termination of reasoning in Large Language Models to reduce computation while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models generate excessively long reasoning content, leading to unnecessary computational overhead, even for simple problems.

**Method:** A verification model is introduced to determine when the reasoning process can be exited early without compromising the correctness of the answer.

**Key Contributions:**

	1. Introduction of FlashThink for early termination of reasoning in LLMs
	2. Demonstration of significant reduction in reasoning content
	3. Maintaining accuracy while reducing computational overhead

**Result:** Comprehensive experiments show FlashThink significantly reduces reasoning content length by over 77% while maintaining model accuracy across various benchmarks.

**Limitations:** 

**Conclusion:** The approach effectively enhances efficiency in reasoning performed by LLMs with minimal impact on performance.

**Abstract:** Large Language Models (LLMs) have shown impressive performance in reasoning tasks. However, LLMs tend to generate excessively long reasoning content, leading to significant computational overhead. Our observations indicate that even on simple problems, LLMs tend to produce unnecessarily lengthy reasoning content, which is against intuitive expectations. Preliminary experiments show that at a certain point during the generation process, the model is already capable of producing the correct solution without completing the full reasoning content. Therefore, we consider that the reasoning process of the model can be exited early to achieve the purpose of efficient reasoning. We introduce a verification model that identifies the exact moment when the model can stop reasoning and still provide the correct answer. Comprehensive experiments on four different benchmarks demonstrate that our proposed method, FlashThink, effectively shortens the reasoning content while preserving the model accuracy. For the Deepseek-R1 and QwQ-32B models, we reduced the length of reasoning content by 77.04% and 77.47%, respectively, without reducing the accuracy.

</details>


### [60] [Through a Compressed Lens: Investigating the Impact of Quantization on LLM Explainability and Interpretability](https://arxiv.org/abs/2505.13963)

*Qianli Wang, Mingyang Wang, Nils Feldhus, Simon Ostermann, Yuan Cao, Hinrich Schütze, Sebastian Möller, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** quantization, explainability, interpretability, large language models, user study

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of quantization methods on the explainability and interpretability of large language models, revealing inconsistent effects that vary by configuration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how quantization affects model explainability and interpretability, which are crucial for understanding decision-making in LLMs.

**Method:** Experiments using three quantization techniques, two explainability methods, two interpretability approaches, and a user study evaluating selected explainability methods.

**Key Contributions:**

	1. Analysis of quantization's effects on model explainability and interpretability
	2. Contrasting impacts of quantization techniques on LLM capabilities
	3. User study findings on the effectiveness of explainability methods under quantization

**Result:** Quantization significantly influences model explainability and interpretability, with effects varying based on the quantization method, the evaluation protocols, and the chosen explanation approaches.

**Limitations:** The study may not cover all possible quantization techniques or explainability methods, limiting the generalizability of the findings.

**Conclusion:** Quantization can unpredictably affect model transparency, highlighting important considerations for deploying LLMs in transparency-sensitive applications.

**Abstract:** Quantization methods are widely used to accelerate inference and streamline the deployment of large language models (LLMs). While prior research has extensively investigated the degradation of various LLM capabilities due to quantization, its effects on model explainability and interpretability, which are crucial for understanding decision-making processes, remain unexplored. To address this gap, we conduct comprehensive experiments using three common quantization techniques at distinct bit widths, in conjunction with two explainability methods, counterfactual examples and natural language explanations, as well as two interpretability approaches, knowledge memorization analysis and latent multi-hop reasoning analysis. We complement our analysis with a thorough user study, evaluating selected explainability methods. Our findings reveal that, depending on the configuration, quantization can significantly impact model explainability and interpretability. Notably, the direction of this effect is not consistent, as it strongly depends on (1) the quantization method, (2) the explainability or interpretability approach, and (3) the evaluation protocol. In some settings, human evaluation shows that quantization degrades explainability, while in others, it even leads to improvements. Our work serves as a cautionary tale, demonstrating that quantization can unpredictably affect model transparency. This insight has important implications for deploying LLMs in applications where transparency is a critical requirement.

</details>


### [61] [CAFES: A Collaborative Multi-Agent Framework for Multi-Granular Multimodal Essay Scoring](https://arxiv.org/abs/2505.13965)

*Jiamin Su, Yibo Yan, Zhuoran Gao, Han Zhang, Xiang Liu, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Multi-agent Framework, Multimodal Assessment, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** CAFES is a collaborative multi-agent framework for Automated Essay Scoring (AES), improving evaluation generalizability and alignment with human judgment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability and accuracy of Automated Essay Scoring in light of emerging multimodal assessments and limitations of current methods.

**Method:** The CAFES framework employs three specialized agents: an Initial Scorer for trait-specific evaluations, a Feedback Pool Manager to gather evidence-based feedback, and a Reflective Scorer for refining scores through iterative feedback processes.

**Key Contributions:**

	1. Introduction of a collaborative multi-agent framework for AES (CAFES).
	2. Improved scoring accuracy with 21% enhancement in QWK.
	3. Integration of feedback mechanisms for better alignment with human evaluators.

**Result:** Extensive experiments demonstrate a 21% average improvement in evaluation accuracy against human ground truth, particularly in grammatical and lexical areas.

**Limitations:** Potential concerns regarding the scalability and real-world applicability of the framework; reliance on state-of-the-art MLLMs may limit generalizability.

**Conclusion:** CAFES sets a foundation for future intelligent multimodal AES systems, addressing key shortcomings of traditional methods.

**Abstract:** Automated Essay Scoring (AES) is crucial for modern education, particularly with the increasing prevalence of multimodal assessments. However, traditional AES methods struggle with evaluation generalizability and multimodal perception, while even recent Multimodal Large Language Model (MLLM)-based approaches can produce hallucinated justifications and scores misaligned with human judgment. To address the limitations, we introduce CAFES, the first collaborative multi-agent framework specifically designed for AES. It orchestrates three specialized agents: an Initial Scorer for rapid, trait-specific evaluations; a Feedback Pool Manager to aggregate detailed, evidence-grounded strengths; and a Reflective Scorer that iteratively refines scores based on this feedback to enhance human alignment. Extensive experiments, using state-of-the-art MLLMs, achieve an average relative improvement of 21% in Quadratic Weighted Kappa (QWK) against ground truth, especially for grammatical and lexical diversity. Our proposed CAFES framework paves the way for an intelligent multimodal AES system. The code will be available upon acceptance.

</details>


### [62] [Truth or Twist? Optimal Model Selection for Reliable Label Flipping Evaluation in LLM-based Counterfactuals](https://arxiv.org/abs/2505.13972)

*Qianli Wang, Van Bach Nguyen, Nils Feldhus, Luis Felipe Villa-Arenas, Christin Seifert, Sebastian Möller, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** counterfactual data augmentation, large language models, judge models

**Relevance Score:** 9

**TL;DR:** The paper investigates the effectiveness of different judge models for evaluating counterfactual data augmentation in large language models, finding that independent judge models yield the most reliable evaluations, though gaps remain in fully automated pipelines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance and reliability of counterfactual data augmentation (CDA) in large language models by understanding the impact of judge models on label flipping evaluations.

**Method:** The study defines four types of relationships between generator and judge models and conducts extensive experiments with various LLM-based methods, datasets, generator models, and judge models, as well as a user study.

**Key Contributions:**

	1. Identification of relationships between counterfactual generator and judge models.
	2. Demonstration of the importance of independent judge models in reliable label flipping evaluations.
	3. Insights from a user study that highlight gaps in automated CDA evaluations.

**Result:** Judge models providing independent evaluations from the generator model resulted in the most reliable label flipping evaluations, while relationships closely aligned with user study results improved model performance and robustness.

**Limitations:** The gap between the effectiveness of the best judge models and user study results suggests limitations in fully automating the CDA process.

**Conclusion:** The findings emphasize the need for human intervention in CDA processes, as there remains a significant gap in the effectiveness of automated evaluation pipelines.

**Abstract:** Counterfactual examples are widely employed to enhance the performance and robustness of large language models (LLMs) through counterfactual data augmentation (CDA). However, the selection of the judge model used to evaluate label flipping, the primary metric for assessing the validity of generated counterfactuals for CDA, yields inconsistent results. To decipher this, we define four types of relationships between the counterfactual generator and judge models. Through extensive experiments involving two state-of-the-art LLM-based methods, three datasets, five generator models, and 15 judge models, complemented by a user study (n = 90), we demonstrate that judge models with an independent, non-fine-tuned relationship to the generator model provide the most reliable label flipping evaluations. Relationships between the generator and judge models, which are closely aligned with the user study for CDA, result in better model performance and robustness. Nevertheless, we find that the gap between the most effective judge models and the results obtained from the user study remains considerably large. This suggests that a fully automated pipeline for CDA may be inadequate and requires human intervention.

</details>


### [63] [Toward Effective Reinforcement Learning Fine-Tuning for Medical VQA in Vision-Language Models](https://arxiv.org/abs/2505.13973)

*Wenhui Zhu, Xuanzhao Dong, Xin Li, Peijie Qiu, Xiwen Chen, Abolfazl Razi, Aris Sotiras, Yi Su, Yalin Wang*

**Main category:** cs.CL

**Keywords:** reinforcement learning, multimodal large language models, medical visual question answering, policy optimization, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper explores reinforcement learning (RL)-based tuning for Multimodal Large Language Models (MLLMs) in medical visual question answering, specifically addressing challenges in aligning model behavior with clinical expectations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of Multimodal Large Language Models' responses with clinical expectations in medical tasks, particularly in visual question answering.

**Method:** The study investigates factors such as base model initialization strategy, medical semantic alignment, length-based rewards for long-chain reasoning, and the influence of bias in the context of RL-based tuning in medical VQA.

**Key Contributions:**

	1. Identifying critical dimensions affecting RL-based tuning in medical VQA
	2. Demonstrating the superiority of GRPO-based RL tuning over SFT
	3. Providing a framework for improving alignment of model responses with clinical expectations

**Result:** Extensive experiments reveal that these factors significantly impact the performance of medical MLLMs and show that GRPO-based RL tuning outperforms standard supervised fine-tuning in accuracy and reasoning quality.

**Limitations:** 

**Conclusion:** The findings provide insights into effective domain-specific fine-tuning strategies for medical MLLMs, highlighting the advantages of RL-based approaches over traditional methods.

**Abstract:** Recently, reinforcement learning (RL)-based tuning has shifted the trajectory of Multimodal Large Language Models (MLLMs), particularly following the introduction of Group Relative Policy Optimization (GRPO). However, directly applying it to medical tasks remains challenging for achieving clinically grounded model behavior. Motivated by the need to align model response with clinical expectations, we investigate four critical dimensions that affect the effectiveness of RL-based tuning in medical visual question answering (VQA): base model initialization strategy, the role of medical semantic alignment, the impact of length-based rewards on long-chain reasoning, and the influence of bias. We conduct extensive experiments to analyze these factors for medical MLLMs, providing new insights into how models are domain-specifically fine-tuned. Additionally, our results also demonstrate that GRPO-based RL tuning consistently outperforms standard supervised fine-tuning (SFT) in both accuracy and reasoning quality.

</details>


### [64] [DRP: Distilled Reasoning Pruning with Skill-aware Step Decomposition for Efficient Large Reasoning Models](https://arxiv.org/abs/2505.13975)

*Yuxuan Jiang, Dawei Li, Frank Ferraro*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Distilled Reasoning Pruning, token efficiency, reasoning accuracy, knowledge transfer

**Relevance Score:** 8

**TL;DR:** Proposes Distilled Reasoning Pruning (DRP) for efficient reasoning in Large Reasoning Models (LRMs) by combining pruning and distillation methods.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** LRMs often produce excessively verbose reasoning traces which are inefficient, prompting the need for more effective reasoning strategies.

**Method:** The DRP framework employs a teacher model for skill-aware step decomposition and content pruning, followed by distillation into a student model.

**Key Contributions:**

	1. Introduction of Distilled Reasoning Pruning (DRP) framework
	2. Significant token efficiency improvements in LRM inference
	3. Establishment of critical alignment for knowledge transfer

**Result:** Models trained with DRP demonstrate significant improvements in token efficiency, achieving a reduction in token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%.

**Limitations:** 

**Conclusion:** Aligning the reasoning structures between training CoTs and the student's reasoning capacity is vital for successful knowledge transfer and enhanced performance.

**Abstract:** While Large Reasoning Models (LRMs) have demonstrated success in complex reasoning tasks through long chain-of-thought (CoT) reasoning, their inference often involves excessively verbose reasoning traces, resulting in substantial inefficiency. To address this, we propose Distilled Reasoning Pruning (DRP), a hybrid framework that combines inference-time pruning with tuning-based distillation, two widely used strategies for efficient reasoning. DRP uses a teacher model to perform skill-aware step decomposition and content pruning, and then distills the pruned reasoning paths into a student model, enabling it to reason both efficiently and accurately. Across several challenging mathematical reasoning datasets, we find that models trained with DRP achieve substantial improvements in token efficiency without sacrificing accuracy. Specifically, DRP reduces average token usage on GSM8K from 917 to 328 while improving accuracy from 91.7% to 94.1%, and achieves a 43% token reduction on AIME with no performance drop. Further analysis shows that aligning the reasoning structure of training CoTs with the student's reasoning capacity is critical for effective knowledge transfer and performance gains.

</details>


### [65] [Mixed Signals: Understanding Model Disagreement in Multimodal Empathy Detection](https://arxiv.org/abs/2505.13979)

*Maya Srikanth, Run Chen, Julia Hirschberg*

**Main category:** cs.CL

**Keywords:** multimodal models, empathy detection, modalities, conflicting cues, diagnostic signals

**Relevance Score:** 6

**TL;DR:** The paper investigates the failures of multimodal models in empathy detection when different modalities offer conflicting cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance issues in empathy detection models caused by conflicting modality signals.

**Method:** The authors analyze cases of divergence in predictions from unimodal and multimodal models using fine-tuned text, audio, and video models, supplemented by a gated fusion model.

**Key Contributions:**

	1. Identifying the role of modality disagreements in empathy detection
	2. Demonstrating that dominant signals in one modality can mislead multimodal fusion
	3. Establishing similarities between model and human responses to multimodal input

**Result:** The study finds that disagreements between modalities often arise from underlying ambiguities and annotator uncertainty.

**Limitations:** The analysis is limited to specific cases of divergence, and further exploration is needed across diverse contexts.

**Conclusion:** The findings suggest that disagreements in predictions can serve as diagnostic indicators, highlighting examples that challenge the robustness of empathy systems.

**Abstract:** Multimodal models play a key role in empathy detection, but their performance can suffer when modalities provide conflicting cues. To understand these failures, we examine cases where unimodal and multimodal predictions diverge. Using fine-tuned models for text, audio, and video, along with a gated fusion model, we find that such disagreements often reflect underlying ambiguity, as evidenced by annotator uncertainty. Our analysis shows that dominant signals in one modality can mislead fusion when unsupported by others. We also observe that humans, like models, do not consistently benefit from multimodal input. These insights position disagreement as a useful diagnostic signal for identifying challenging examples and improving empathy system robustness.

</details>


### [66] [The Hallucination Tax of Reinforcement Finetuning](https://arxiv.org/abs/2505.13988)

*Linxin Song, Taiwei Shi, Jieyu Zhao*

**Main category:** cs.CL

**Keywords:** Reinforcement Finetuning, Large Language Models, Hallucination Tax, Unanswerable Questions, Trustworthiness

**Relevance Score:** 8

**TL;DR:** The paper examines the adverse effects of reinforcement finetuning (RFT) on the hallucination rates of large language models (LLMs), introducing a dataset to measure this impact.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how reinforcement finetuning affects the trustworthiness of large language models, specifically their propensity to hallucinate answers to unanswerable questions.

**Method:** The authors created the SUM dataset, comprising synthetic unanswerable math problems, to analyze the impact of RFT on model refusal behavior and hallucination rates.

**Key Contributions:**

	1. Introduction of the 'hallucination tax' concept in LLMs.
	2. Creation of the SUM dataset for testing unanswerable question recognition.
	3. Demonstration of a method to restore refusal behavior in LLMs with minimal trade-offs.

**Result:** Standard RFT significantly decreased refusal rates (over 80%) but increased hallucinations; incorporating 10% of the SUM dataset during RFT restored appropriate refusal behavior with minimal accuracy loss.

**Limitations:** The study focuses primarily on math problems, which may not fully represent broader LLM behavior in diverse contexts.

**Conclusion:** Incorporating unanswerable problem training can help LLMs recognize their uncertainty better, improving their performance on both unanswerable and factual problems.

**Abstract:** Reinforcement finetuning (RFT) has become a standard approach for enhancing the reasoning capabilities of large language models (LLMs). However, its impact on model trustworthiness remains underexplored. In this work, we identify and systematically study a critical side effect of RFT, which we term the hallucination tax: a degradation in refusal behavior causing models to produce hallucinated answers to unanswerable questions confidently. To investigate this, we introduce SUM (Synthetic Unanswerable Math), a high-quality dataset of unanswerable math problems designed to probe models' ability to recognize an unanswerable question by reasoning from the insufficient or ambiguous information. Our results show that standard RFT training could reduce model refusal rates by more than 80%, which significantly increases model's tendency to hallucinate. We further demonstrate that incorporating just 10% SUM during RFT substantially restores appropriate refusal behavior, with minimal accuracy trade-offs on solvable tasks. Crucially, this approach enables LLMs to leverage inference-time compute to reason about their own uncertainty and knowledge boundaries, improving generalization not only to out-of-domain math problems but also to factual question answering tasks.

</details>


### [67] [DecIF: Improving Instruction-Following through Meta-Decomposition](https://arxiv.org/abs/2505.13990)

*Tingfeng Hui, Pengyu Zhu, Bowen Ping, Ling Tang, Yaqi Zhang, Sen Su*

**Main category:** cs.CL

**Keywords:** Instruction-following, Large language models, Meta-decomposition, Data synthesis, Autonomous generation

**Relevance Score:** 8

**TL;DR:** DecIF is a novel framework for generating diverse instruction-following data using large language models autonomously.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a flexible and generalizable approach to generate instruction-following data without relying on pre-existing documents or external resources.

**Method:** DecIF employs a meta-decomposition strategy where LLMs iteratively produce meta-information that is combined with response constraints to create structured instructions. It also uses LLMs to identify and resolve inconsistencies in these instructions and breaks down instructions for rigorous validation of response pairs.

**Key Contributions:**

	1. Introduction of DecIF framework for autonomous instruction generation
	2. Meta-decomposition approach for producing structured and high-quality instructions
	3. Rigorous validation mechanism for instruction-response pairs

**Result:** Extensive experiments show that DecIF outperforms existing methods on various instruction-following tasks while demonstrating high flexibility, scalability, and generalizability in generating quality instruction data.

**Limitations:** Work in progress; possible constraints in the initial implementation and need for further testing in diverse real-world scenarios.

**Conclusion:** DecIF introduces a new paradigm for autonomous instruction generation, which enhances the efficiency and quality of instruction-following data synthesis.

**Abstract:** Instruction-following has emerged as a crucial capability for large language models (LLMs). However, existing approaches often rely on pre-existing documents or external resources to synthesize instruction-following data, which limits their flexibility and generalizability. In this paper, we introduce DecIF, a fully autonomous, meta-decomposition guided framework that generates diverse and high-quality instruction-following data using only LLMs. DecIF is grounded in the principle of decomposition. For instruction generation, we guide LLMs to iteratively produce various types of meta-information, which are then combined with response constraints to form well-structured and semantically rich instructions. We further utilize LLMs to detect and resolve potential inconsistencies within the generated instructions. Regarding response generation, we decompose each instruction into atomic-level evaluation criteria, enabling rigorous validation and the elimination of inaccurate instruction-response pairs. Extensive experiments across a wide range of scenarios and settings demonstrate DecIF's superior performance on instruction-following tasks. Further analysis highlights its strong flexibility, scalability, and generalizability in automatically synthesizing high-quality instruction data.

</details>


### [68] [Social Sycophancy: A Broader Understanding of LLM Sycophancy](https://arxiv.org/abs/2505.13995)

*Myra Cheng, Sunny Yu, Cinoo Lee, Pranav Khadpe, Lujain Ibrahim, Dan Jurafsky*

**Main category:** cs.CL

**Keywords:** social sycophancy, LLMs, face preservation, ELEPHANT framework, user interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces a theory and framework for evaluating social sycophancy in LLMs, highlighting their tendency to excessively affirm and flatter users, which can perpetuate harmful beliefs and actions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding forms of sycophancy in ambiguous contexts where explicit beliefs cannot be directly evaluated, particularly in user interactions that require advice and emotional support.

**Method:** The authors present ELEPHANT, a framework that characterizes social sycophancy by evaluating LLMs on five face-preserving behaviors using two datasets (open-ended questions and Reddit's r/AmITheAsshole).

**Key Contributions:**

	1. Introduces a novel theory of social sycophancy in LLMs.
	2. Develops the ELEPHANT framework for evaluating sycophancy behaviors.
	3. Documents empirical findings illustrating the prevalence of social sycophancy among multiple LLM models.

**Result:** LLMs displayed significantly higher rates of social sycophancy than human responses, preserving face 47% more on open-ended questions and affirming inappropriate behaviors in 42% of AITA cases.

**Limitations:** Focuses primarily on specific types of interactions and may not account for all contexts of sycophancy.

**Conclusion:** Social sycophancy in LLMs is prevalent and reinforced by preference datasets; this work lays the groundwork for further research and tools to address and mitigate this issue.

**Abstract:** A serious risk to the safety and utility of LLMs is sycophancy, i.e., excessive agreement with and flattery of the user. Yet existing work focuses on only one aspect of sycophancy: agreement with users' explicitly stated beliefs that can be compared to a ground truth. This overlooks forms of sycophancy that arise in ambiguous contexts such as advice and support-seeking, where there is no clear ground truth, yet sycophancy can reinforce harmful implicit assumptions, beliefs, or actions. To address this gap, we introduce a richer theory of social sycophancy in LLMs, characterizing sycophancy as the excessive preservation of a user's face (the positive self-image a person seeks to maintain in an interaction). We present ELEPHANT, a framework for evaluating social sycophancy across five face-preserving behaviors (emotional validation, moral endorsement, indirect language, indirect action, and accepting framing) on two datasets: open-ended questions (OEQ) and Reddit's r/AmITheAsshole (AITA). Across eight models, we show that LLMs consistently exhibit high rates of social sycophancy: on OEQ, they preserve face 47% more than humans, and on AITA, they affirm behavior deemed inappropriate by crowdsourced human judgments in 42% of cases. We further show that social sycophancy is rewarded in preference datasets and is not easily mitigated. Our work provides theoretical grounding and empirical tools (datasets and code) for understanding and addressing this under-recognized but consequential issue.

</details>


### [69] [Activation-Guided Consensus Merging for Large Language Models](https://arxiv.org/abs/2505.14009)

*Yuxuan Yao, Shuqi Liu, Zehua Liu, Qintong Li, Mingyang Liu, Xiongwei Han, Zhijiang Guo, Han Wu, Linqi Song*

**Main category:** cs.CL

**Keywords:** Model Merging, Large Language Models, Activation-Guided Consensus Merging

**Relevance Score:** 7

**TL;DR:** A novel model merging framework called Activation-Guided Consensus Merging (ACM) is proposed to effectively integrate various Large Language Models (LLMs) by using layer-specific merging coefficients based on mutual information, leading to enhanced task-specific capabilities with improved efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reconcile System 2 reasoning capabilities with System 1 efficiency while addressing the limitations of existing training-based and prompt-based approaches in model merging.

**Method:** ACM utilizes a plug-and-play merging framework that calculates layer-specific merging coefficients based on the mutual information of activations from pre-trained and fine-tuned models, avoiding the need for gradient computations or additional training.

**Key Contributions:**

	1. Introduction of Activation-Guided Consensus Merging (ACM) framework for model merging
	2. Layer-specific merging coefficients based on mutual information
	3. Publicly available code for reproducibility

**Result:** Extensive experiments indicate that ACM consistently surpasses baseline methods. For example, TIES-Merging equipped with ACM achieves a 55.3% reduction in response length and a 1.3 point improvement in reasoning accuracy for Qwen-7B models.

**Limitations:** 

**Conclusion:** ACM effectively retains task-specific performance while enhancing efficiency in merging different LLM capabilities, making it an attractive option for improving large model integrations without complex additional training.

**Abstract:** Recent research has increasingly focused on reconciling the reasoning capabilities of System 2 with the efficiency of System 1. While existing training-based and prompt-based approaches face significant challenges in terms of efficiency and stability, model merging emerges as a promising strategy to integrate the diverse capabilities of different Large Language Models (LLMs) into a unified model. However, conventional model merging methods often assume uniform importance across layers, overlooking the functional heterogeneity inherent in neural components. To address this limitation, we propose \textbf{A}ctivation-Guided \textbf{C}onsensus \textbf{M}erging (\textbf{ACM}), a plug-and-play merging framework that determines layer-specific merging coefficients based on mutual information between activations of pre-trained and fine-tuned models. ACM effectively preserves task-specific capabilities without requiring gradient computations or additional training. Extensive experiments on Long-to-Short (L2S) and general merging tasks demonstrate that ACM consistently outperforms all baseline methods. For instance, in the case of Qwen-7B models, TIES-Merging equipped with ACM achieves a \textbf{55.3\%} reduction in response length while simultaneously improving reasoning accuracy by \textbf{1.3} points. We submit the code with the paper for reproducibility, and it will be publicly available.

</details>


### [70] [AUTOLAW: Enhancing Legal Compliance in Large Language Models via Case Law Generation and Jury-Inspired Deliberation](https://arxiv.org/abs/2505.14015)

*Tai D. Nguyen, Long H. Pham, Jun Sun*

**Main category:** cs.CL

**Keywords:** large language models, legal compliance, adversarial data generation, jury deliberation, law evaluation

**Relevance Score:** 7

**TL;DR:** AutoLaw is a violation detection framework for LLMs that combines adversarial data generation and a jury-inspired process to enhance legal compliance, adapting to local regulations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for frameworks that ensure legal compliance in domain-specific LLMs due to diverse regional legal distinctions and limitations of existing evaluation benchmarks.

**Method:** AutoLaw utilizes adversarial data generation and a jury-inspired deliberation process involving LLM-based jurors to simulate judicial decision-making and dynamically reflect case law.

**Key Contributions:**

	1. Dynamic synthesis of case law reflecting local regulations
	2. Implementation of a jury-inspired deliberation process
	3. Enhanced violation detection rates through adversarial data generation

**Result:** AutoLaw shows improved detection accuracy and compliance in LLMs, with effective evaluations on legal benchmarks demonstrating enhanced discrimination and violation detection rates.

**Limitations:** 

**Conclusion:** The framework offers a scalable solution for augmenting LLMs in legally sensitive contexts, capable of adapting to evolving regulatory landscapes.

**Abstract:** The rapid advancement of domain-specific large language models (LLMs) in fields like law necessitates frameworks that account for nuanced regional legal distinctions, which are critical for ensuring compliance and trustworthiness. Existing legal evaluation benchmarks often lack adaptability and fail to address diverse local contexts, limiting their utility in dynamically evolving regulatory landscapes. To address these gaps, we propose AutoLaw, a novel violation detection framework that combines adversarial data generation with a jury-inspired deliberation process to enhance legal compliance of LLMs. Unlike static approaches, AutoLaw dynamically synthesizes case law to reflect local regulations and employs a pool of LLM-based "jurors" to simulate judicial decision-making. Jurors are ranked and selected based on synthesized legal expertise, enabling a deliberation process that minimizes bias and improves detection accuracy. Evaluations across three benchmarks: Law-SG, Case-SG (legality), and Unfair-TOS (policy), demonstrate AutoLaw's effectiveness: adversarial data generation improves LLM discrimination, while the jury-based voting strategy significantly boosts violation detection rates. Our results highlight the framework's ability to adaptively probe legal misalignments and deliver reliable, context-aware judgments, offering a scalable solution for evaluating and enhancing LLMs in legally sensitive applications.

</details>


### [71] [From Unaligned to Aligned: Scaling Multilingual LLMs with Multi-Way Parallel Corpora](https://arxiv.org/abs/2505.14045)

*Yingli Shen, Wen Lai, Shuo Wang, Kangyang Luo, Alexander Fraser, Maosong Sun*

**Main category:** cs.CL

**Keywords:** multilingual data, large language models, cross-lingual semantics, TED Talks, multilingual performance

**Relevance Score:** 8

**TL;DR:** Introduction of a large-scale multi-way parallel corpus, TED2025, to improve multilingual performance in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of unaligned multilingual data in capturing cross-lingual semantics for enhancing LLMs' performance in low-resource languages.

**Method:** Presentation and analysis of a multi-way parallel dataset based on TED Talks covering 113 languages, investigation of pretraining and tuning strategies using this dataset.

**Key Contributions:**

	1. Introduction of the TED2025 corpus with comprehensive multilingual coverage
	2. Demonstration of outperforming results of models trained on multi-way parallel data over unaligned data
	3. Identification of effective strategies for leveraging multi-way parallel data in model training.

**Result:** Models trained on the TED2025 multi-way parallel data consistently outperform those trained on unaligned multilingual data across six multilingual benchmarks.

**Limitations:** 

**Conclusion:** The use of multi-way parallel corpora can significantly enhance the capabilities of LLMs in multilingual applications, providing stronger consistency and improving performance.

**Abstract:** Continued pretraining and instruction tuning on large-scale multilingual data have proven to be effective in scaling large language models (LLMs) to low-resource languages. However, the unaligned nature of such data limits its ability to effectively capture cross-lingual semantics. In contrast, multi-way parallel data, where identical content is aligned across multiple languages, provides stronger cross-lingual consistency and offers greater potential for improving multilingual performance. In this paper, we introduce a large-scale, high-quality multi-way parallel corpus, TED2025, based on TED Talks. The corpus spans 113 languages, with up to 50 languages aligned in parallel, ensuring extensive multilingual coverage. Using this dataset, we investigate best practices for leveraging multi-way parallel data to enhance LLMs, including strategies for continued pretraining, instruction tuning, and the analysis of key influencing factors. Experiments on six multilingual benchmarks show that models trained on multiway parallel data consistently outperform those trained on unaligned multilingual data.

</details>


### [72] [Improved Methods for Model Pruning and Knowledge Distillation](https://arxiv.org/abs/2505.14052)

*Wei Jiang, Anying Fu, Youling Zhang*

**Main category:** cs.CL

**Keywords:** Model Pruning, Human-Computer Interaction, Language Models, Computational Linguistics, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper proposes MAMA Pruning, a novel pruning technique for large language models that minimizes performance loss while achieving significant model size reduction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing pruning methods for large language models often degrade performance or require extensive retraining. There is a need for a method that effectively prunes models without compromising their generated content quality, especially in human-computer interaction contexts.

**Method:** MAMA Pruning utilizes Movement and Magnitude Analysis to identify and remove less significant neurons and connections in the model, leveraging weights and biases during pre-training and using GRPO rewards verified in post-training as pruning indicators.

**Key Contributions:**

	1. Introduction of MAMA Pruning technique for improved model efficiency
	2. Demonstrated effectiveness across various pruning levels
	3. Performance maintenance comparable to original models even at high pruning levels

**Result:** Preliminary results indicate that MAMA Pruning outperforms state-of-the-art methods across various pruning levels and NLP tasks, maintaining performance comparable to unpruned models.

**Limitations:** 

**Conclusion:** MAMA Pruning presents an effective solution for reducing the size and complexity of language models while preserving their performance, making it a viable option for applications in human-computer interaction and other computational linguistics tasks.

**Abstract:** Model pruning is a performance optimization technique for large language models like R1 or o3-mini. However, existing pruning methods often lead to significant performance degradation or require extensive retraining and fine-tuning. This technique aims to identify and remove neurons, connections unlikely leading to the contribution during the human-computer interaction phase. Our goal is to obtain a much smaller and faster knowledge distilled model that can quickly generate content almost as good as those of the unpruned ones. We propose MAMA Pruning, short for Movement and Magnitude Analysis, an improved pruning method that effectively reduces model size and computational complexity while maintaining performance comparable to the original unpruned model even at extreme pruned levels. The improved method is based on weights, bias fixed in the pre-training phase and GRPO rewards verified during the post-training phase as our novel pruning indicators. Preliminary experimental results show that our method outperforms and be comparable to state-of-the-art methods across various pruning levels and different downstream computational linguistics tasks.

</details>


### [73] [Enhancing LLMs via High-Knowledge Data Selection](https://arxiv.org/abs/2505.14070)

*Feiyu Duan, Xuemiao Zhang, Sirui Wang, Haoran Que, Yuqi Liu, Wenge Rong, Xunliang Cai*

**Main category:** cs.CL

**Keywords:** Large Language Models, knowledge scoring, data selection, bilingual dataset, domain-specific training

**Relevance Score:** 8

**TL;DR:** Proposes a High-Knowledge Scorer (HKS) for selecting high-quality training data based on knowledge richness, improving LLM performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address knowledge scarcity in pre-trained corpora used by Large Language Models (LLMs).

**Method:** Introduces a High-Knowledge Scorer (HKS) that evaluates data quality based on knowledge density and coverage metrics, with an emphasis on selecting domain-specific high-knowledge data.

**Key Contributions:**

	1. Development of a gradient-free High-Knowledge Scorer (HKS) for training data selection.
	2. Introduction of knowledge density and coverage as metrics for evaluating knowledge content.
	3. Success in using HKS on a bilingual dataset to improve model performance.

**Result:** Demonstrates that using the HKS significantly improves performance on knowledge-intensive and general comprehension tasks in LLMs.

**Limitations:** 

**Conclusion:** The proposed HKS is effective for enhancing both general and domain-specific capabilities of LLMs, contributing to better performance by utilizing knowledge-rich training data.

**Abstract:** The performance of Large Language Models (LLMs) is intrinsically linked to the quality of its training data. Although several studies have proposed methods for high-quality data selection, they do not consider the importance of knowledge richness in text corpora. In this paper, we propose a novel and gradient-free High-Knowledge Scorer (HKS) to select high-quality data from the dimension of knowledge, to alleviate the problem of knowledge scarcity in the pre-trained corpus. We propose a comprehensive multi-domain knowledge element pool and introduce knowledge density and coverage as metrics to assess the knowledge content of the text. Based on this, we propose a comprehensive knowledge scorer to select data with intensive knowledge, which can also be utilized for domain-specific high-knowledge data selection by restricting knowledge elements to the specific domain. We train models on a high-knowledge bilingual dataset, and experimental results demonstrate that our scorer improves the model's performance in knowledge-intensive and general comprehension tasks, and is effective in enhancing both the generic and domain-specific capabilities of the model.

</details>


### [74] [BAR: A Backward Reasoning based Agent for Complex Minecraft Tasks](https://arxiv.org/abs/2505.14079)

*Weihong Du, Wenrui Liao, Binyu Yan, Hongru Liang, Anthony G. Cohn, Wenqiang Lei*

**Main category:** cs.CL

**Keywords:** backward reasoning, language model agents, task planning, Minecraft, goal decomposition

**Relevance Score:** 8

**TL;DR:** This paper presents a BAckward Reasoning based agent (BAR) that improves task planning in complex environments like Minecraft by using backward reasoning to achieve task goals directly from terminal states.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the limitations of forward reasoning in LLM-based agents when handling complex tasks with significant perception gaps between initial states and goals.

**Method:** The authors introduce BAR, which employs a recursive goal decomposition module, a state consistency maintaining module, and a stage memory module to facilitate efficient planning from terminal states.

**Key Contributions:**

	1. Introduction of BAR agent leveraging backward reasoning
	2. Development of modules for goal decomposition and state consistency
	3. Empirical evidence of improved performance over existing methods

**Result:** Experimental results indicate that BAR outperforms existing planning methods in terms of robustness, consistency, and efficiency.

**Limitations:** 

**Conclusion:** The study demonstrates that backward reasoning is a viable approach for enhancing task completion in complex environments, leading to improved agent performance.

**Abstract:** Large language model (LLM) based agents have shown great potential in following human instructions and automatically completing various tasks. To complete a task, the agent needs to decompose it into easily executed steps by planning. Existing studies mainly conduct the planning by inferring what steps should be executed next starting from the agent's initial state. However, this forward reasoning paradigm doesn't work well for complex tasks. We propose to study this issue in Minecraft, a virtual environment that simulates complex tasks based on real-world scenarios. We believe that the failure of forward reasoning is caused by the big perception gap between the agent's initial state and task goal. To this end, we leverage backward reasoning and make the planning starting from the terminal state, which can directly achieve the task goal in one step. Specifically, we design a BAckward Reasoning based agent (BAR). It is equipped with a recursive goal decomposition module, a state consistency maintaining module and a stage memory module to make robust, consistent, and efficient planning starting from the terminal state. Experimental results demonstrate the superiority of BAR over existing methods and the effectiveness of proposed modules.

</details>


### [75] [Gender Trouble in Language Models: An Empirical Audit Guided by Gender Performativity Theory](https://arxiv.org/abs/2505.14080)

*Franziska Sofia Hafner, Ana Valdivia, Luc Rocher*

**Main category:** cs.CL

**Keywords:** gender bias, language models, gendered stereotypes, gender studies, model architecture

**Relevance Score:** 9

**TL;DR:** This paper explores how language models perpetuate harmful gendered stereotypes and biases, advocating for a broader definition of gender bias in AI to better address these issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequate understanding of gender bias in language models, which often erases transgender and gender diverse identities, leading to real-world harms.

**Method:** The paper empirically tests 16 language models of varying architectures and training datasets to examine how they encode gender by operationalizing insights from gender studies.

**Key Contributions:**

	1. Broadens the definition of gender bias in language models.
	2. Empirically tests diverse language models to assess their encoding of gender.
	3. Highlights how larger models reinforce binary gender associations.

**Result:** It finds that these models predominantly encode gender as a binary tied to biological sex and that larger models worsen these biases by reinforcing narrow gender understandings.

**Limitations:** The study primarily focuses on model architecture and performance without addressing the societal impacts of these biases extensively.

**Conclusion:** The research calls for a re-evaluation of definitions and methodologies used to address gendered harms in language models.

**Abstract:** Language models encode and subsequently perpetuate harmful gendered stereotypes. Research has succeeded in mitigating some of these harms, e.g. by dissociating non-gendered terms such as occupations from gendered terms such as 'woman' and 'man'. This approach, however, remains superficial given that associations are only one form of prejudice through which gendered harms arise. Critical scholarship on gender, such as gender performativity theory, emphasizes how harms often arise from the construction of gender itself, such as conflating gender with biological sex. In language models, these issues could lead to the erasure of transgender and gender diverse identities and cause harms in downstream applications, from misgendering users to misdiagnosing patients based on wrong assumptions about their anatomy.   For FAccT research on gendered harms to go beyond superficial linguistic associations, we advocate for a broader definition of 'gender bias' in language models. We operationalize insights on the construction of gender through language from gender studies literature and then empirically test how 16 language models of different architectures, training datasets, and model sizes encode gender. We find that language models tend to encode gender as a binary category tied to biological sex, and that gendered terms that do not neatly fall into one of these binary categories are erased and pathologized. Finally, we show that larger models, which achieve better results on performance benchmarks, learn stronger associations between gender and sex, further reinforcing a narrow understanding of gender. Our findings lead us to call for a re-evaluation of how gendered harms in language models are defined and addressed.

</details>


### [76] [Beyond Chains: Bridging Large Language Models and Knowledge Bases in Complex Question Answering](https://arxiv.org/abs/2505.14099)

*Yihua Zhu, Qianying Liu, Akiko Aizawa, Hidetoshi Shimodaira*

**Main category:** cs.CL

**Keywords:** Knowledge Base Question Answering, LLM, semantic parsing, information retrieval, reasoning

**Relevance Score:** 8

**TL;DR:** PDRR is a novel four-stage framework for Knowledge Base Question Answering that improves performance on both simple and complex questions by predicting question types, decomposing questions, retrieving information from knowledge bases, and using LLMs for reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing KBQA methods struggle with hallucinations and outdated knowledge, and chain-based approaches are limited in handling complex question structures. A more structured and logical method is needed.

**Method:** PDRR consists of four stages: Predict the question type, Decompose the question into structured triples, Retrieve relevant information from knowledge bases, and Reason using LLMs to complete the triples.

**Key Contributions:**

	1. Introduction of PDRR framework for KBQA
	2. Successful integration of structured knowledge retrieval and LLM reasoning
	3. Enhanced performance on both simple and complex question types

**Result:** Experimental results show that PDRR outperforms existing KBQA methods across various LLMs, handling both chain-structured and non-chain complex questions effectively.

**Limitations:** 

**Conclusion:** PDRR offers a systematic approach to enhance KBQA, addressing the limitations of prior methods and achieving superior performance.

**Abstract:** Knowledge Base Question Answering (KBQA) aims to answer natural language questions using structured knowledge from KBs. While LLM-only approaches offer generalization, they suffer from outdated knowledge, hallucinations, and lack of transparency. Chain-based KG-RAG methods address these issues by incorporating external KBs, but are limited to simple chain-structured questions due to the absence of planning and logical structuring. Inspired by semantic parsing methods, we propose PDRR: a four-stage framework consisting of Predict, Decompose, Retrieve, and Reason. Our method first predicts the question type and decomposes the question into structured triples. Then retrieves relevant information from KBs and guides the LLM as an agent to reason over and complete the decomposed triples. Experimental results demonstrate that PDRR consistently outperforms existing methods across various LLM backbones and achieves superior performance on both chain-structured and non-chain complex questions.

</details>


### [77] [MultiHal: Multilingual Dataset for Knowledge-Graph Grounded Evaluation of LLM Hallucinations](https://arxiv.org/abs/2505.14101)

*Ernests Lavrinovics, Russa Biswas, Katja Hose, Johannes Bjerva*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Factuality Evaluation, Hallucination Mitigation, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces MultiHal, a multilingual, multihop benchmark aimed at improving factual correctness in generative text models using Knowledge Graphs (KGs) for hallucination mitigation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in existing benchmarks for factuality evaluation in language models, particularly their lack of structured factual resources like Knowledge Graphs.

**Method:** The authors propose MultiHal, a KG-based multilingual multihop benchmark, by mining and curating 140k KG-paths from open-domain KGs and evaluating the integration of KGs in generative text models.

**Key Contributions:**

	1. Proposed a new multilingual, multihop benchmark named MultiHal for hallucination evaluation.
	2. Demonstrated the effectiveness of integrating Knowledge Graphs into language model evaluations.
	3. Curated a high-quality subset of KG-paths from mined data to enhance the benchmarking process.

**Result:** Baseline evaluations showed a semantic similarity score increase of 0.12 to 0.36 points for KG-RAG compared to vanilla QA across different languages and models, indicating the effectiveness of KG integration.

**Limitations:** 

**Conclusion:** The introduction of MultiHal is expected to advance research in graph-based methods for hallucination mitigation and fact-checking tasks.

**Abstract:** Large Language Models (LLMs) have inherent limitations of faithfulness and factuality, commonly referred to as hallucinations. Several benchmarks have been developed that provide a test bed for factuality evaluation within the context of English-centric datasets, while relying on supplementary informative context like web links or text passages but ignoring the available structured factual resources. To this end, Knowledge Graphs (KGs) have been identified as a useful aid for hallucination mitigation, as they provide a structured way to represent the facts about entities and their relations with minimal linguistic overhead. We bridge the lack of KG paths and multilinguality for factual language modeling within the existing hallucination evaluation benchmarks and propose a KG-based multilingual, multihop benchmark called \textbf{MultiHal} framed for generative text evaluation. As part of our data collection pipeline, we mined 140k KG-paths from open-domain KGs, from which we pruned noisy KG-paths, curating a high-quality subset of 25.9k. Our baseline evaluation shows an absolute scale increase by approximately 0.12 to 0.36 points for the semantic similarity score in KG-RAG over vanilla QA across multiple languages and multiple models, demonstrating the potential of KG integration. We anticipate MultiHal will foster future research towards several graph-based hallucination mitigation and fact-checking tasks.

</details>


### [78] [Legal Rule Induction: Towards Generalizable Principle Discovery from Analogous Judicial Precedents](https://arxiv.org/abs/2505.14104)

*Wei Fan, Tianshi Zheng, Yiran Hu, Zheye Deng, Weiqi Wang, Baixuan Xu, Chunyang Li, Haoran Li, Weixing Shen, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Legal Rule Induction, Large Language Models, judicial precedents, benchmark datasets, automated legal research

**Relevance Score:** 6

**TL;DR:** The paper introduces Legal Rule Induction (LRI) as a task for automating the extraction of legal rules from judicial precedents using LLMs, and presents the first benchmark for this task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the automation and understanding of legal rules derived from judicial decisions, addressing the limitations of current computational legal research.

**Method:** The authors formalize the task of Legal Rule Induction, create a benchmark dataset comprising 5,121 case sets and 216 expert-annotated test sets, and evaluate state-of-the-art LLMs on their ability to derive legal rules from these cases.

**Key Contributions:**

	1. Introduction of Legal Rule Induction (LRI) as a formal task.
	2. Creation of the first benchmark dataset for LRI.
	3. Demonstration of LLM performance issues and improvements with specific training.

**Result:** Experiments show that existing LLMs face challenges like over-generalization and hallucination, but performance improves significantly when trained on the proposed benchmark dataset.

**Limitations:** Current LLMs tend to over-generalize and hallucinate when extracting legal rules.

**Conclusion:** The study highlights the potential of LLMs in legal rule induction while identifying current limitations and providing a foundation for future research in this area.

**Abstract:** Legal rules encompass not only codified statutes but also implicit adjudicatory principles derived from precedents that contain discretionary norms, social morality, and policy. While computational legal research has advanced in applying established rules to cases, inducing legal rules from judicial decisions remains understudied, constrained by limitations in model inference efficacy and symbolic reasoning capability. The advent of Large Language Models (LLMs) offers unprecedented opportunities for automating the extraction of such latent principles, yet progress is stymied by the absence of formal task definitions, benchmark datasets, and methodologies. To address this gap, we formalize Legal Rule Induction (LRI) as the task of deriving concise, generalizable doctrinal rules from sets of analogous precedents, distilling their shared preconditions, normative behaviors, and legal consequences. We introduce the first LRI benchmark, comprising 5,121 case sets (38,088 Chinese cases in total) for model tuning and 216 expert-annotated gold test sets. Experimental results reveal that: 1) State-of-the-art LLMs struggle with over-generalization and hallucination; 2) Training on our dataset markedly enhances LLMs capabilities in capturing nuanced rule patterns across similar cases.

</details>


### [79] [A Personalized Conversational Benchmark: Towards Simulating Personalized Conversations](https://arxiv.org/abs/2505.14106)

*Li Li, Peilin Cai, Ryan A. Rossi, Franck Dernoncourt, Branislav Kveton, Junda Wu, Tong Yu, Linxin Song, Tiankai Yang, Yuehan Qin, Nesreen K. Ahmed, Samyadeep Basu, Subhojyoti Mukherjee, Ruiyi Zhang, Zhengmian Hu, Bo Ni, Yuxiao Zhou, Zichao Wang, Yue Huang, Yu Wang, Xiangliang Zhang, Philip S. Yu, Xiyang Hu, Yue Zhao*

**Main category:** cs.CL

**Keywords:** personalized reasoning, multi-turn conversations, large language models, benchmark, user-centric text generation

**Relevance Score:** 9

**TL;DR:** PersonaConvBench is a benchmark for evaluating personalized reasoning and generation in multi-turn conversations with LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically analyze how personalized conversational context influences outputs of LLMs in realistic multi-user scenarios.

**Method:** The paper presents a benchmark framework integrating sentence classification, impact regression, and user-centric text generation tasks using a unified prompting setup.

**Key Contributions:**

	1. Introduction of a large-scale benchmark for personalized conversation evaluation
	2. Integration of conversational structure and personalization in multi-turn dialogues
	3. Demonstrated substantial performance gains in LLMs with personalized context

**Result:** Benchmarking shows substantial performance improvements for LLMs when incorporating personalized history, with a 198% relative gain in sentiment classification compared to non-conversational baselines.

**Limitations:** 

**Conclusion:** The release of PersonaConvBench aims to facilitate research on LLMs that adapt to personal styles and produce rich, engaging responses based on long-term context.

**Abstract:** We present PersonaConvBench, a large-scale benchmark for evaluating personalized reasoning and generation in multi-turn conversations with large language models (LLMs). Unlike existing work that focuses on either personalization or conversational structure in isolation, PersonaConvBench integrates both, offering three core tasks: sentence classification, impact regression, and user-centric text generation across ten diverse Reddit-based domains. This design enables systematic analysis of how personalized conversational context shapes LLM outputs in realistic multi-user scenarios. We benchmark several commercial and open-source LLMs under a unified prompting setup and observe that incorporating personalized history yields substantial performance improvements, including a 198 percent relative gain over the best non-conversational baseline in sentiment classification. By releasing PersonaConvBench with evaluations and code, we aim to support research on LLMs that adapt to individual styles, track long-term context, and produce contextually rich, engaging responses.

</details>


### [80] [DiagnosisArena: Benchmarking Diagnostic Reasoning for Large Language Models](https://arxiv.org/abs/2505.14107)

*Yakun Zhu, Zhongzhen Huang, Linjie Mu, Yutong Huang, Wei Nie, Shaoting Zhang, Pengfei Liu, Xiaofan Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Diagnostic Reasoning, Healthcare AI, Benchmarking, Machine Learning

**Relevance Score:** 9

**TL;DR:** DiagnosisArena is a new benchmark created to evaluate the diagnostic capabilities of large language models in clinical settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the diagnostic capabilities of large language models in healthcare, as current benchmarks are inadequate for advanced diagnostic reasoning.

**Method:** DiagnosisArena encompasses 1,113 segmented patient cases and diagnoses across 28 medical specialties, developed through iterative screening and review by AI and human experts.

**Key Contributions:**

	1. Introduction of DiagnosisArena as a benchmark for AI diagnostic assessment.
	2. Detailed construction pipeline with expert reviews to ensure benchmark integrity.
	3. Revelation of significant accuracy gaps in current AI models regarding clinical diagnostics.

**Result:** The study found that leading models (o3-mini, o1, and DeepSeek-R1) achieved low diagnostic accuracy of 45.82%, 31.09%, and 17.79% respectively, indicating a significant limitation in their performance in clinical environments.

**Limitations:** The benchmark may not cover all clinical scenarios or specialties comprehensively.

**Conclusion:** DiagnosisArena aims to enhance the diagnostic reasoning capabilities of AI in healthcare, providing tools for ongoing research and improvement in clinical diagnostics.

**Abstract:** The emergence of groundbreaking large language models capable of performing complex reasoning tasks holds significant promise for addressing various scientific challenges, including those arising in complex clinical scenarios. To enable their safe and effective deployment in real-world healthcare settings, it is urgently necessary to benchmark the diagnostic capabilities of current models systematically. Given the limitations of existing medical benchmarks in evaluating advanced diagnostic reasoning, we present DiagnosisArena, a comprehensive and challenging benchmark designed to rigorously assess professional-level diagnostic competence. DiagnosisArena consists of 1,113 pairs of segmented patient cases and corresponding diagnoses, spanning 28 medical specialties, deriving from clinical case reports published in 10 top-tier medical journals. The benchmark is developed through a meticulous construction pipeline, involving multiple rounds of screening and review by both AI systems and human experts, with thorough checks conducted to prevent data leakage. Our study reveals that even the most advanced reasoning models, o3-mini, o1, and DeepSeek-R1, achieve only 45.82%, 31.09%, and 17.79% accuracy, respectively. This finding highlights a significant generalization bottleneck in current large language models when faced with clinical diagnostic reasoning challenges. Through DiagnosisArena, we aim to drive further advancements in AIs diagnostic reasoning capabilities, enabling more effective solutions for real-world clinical diagnostic challenges. We provide the benchmark and evaluation tools for further research and development https://github.com/SPIRAL-MED/DiagnosisArena.

</details>


### [81] [Invisible Entropy: Towards Safe and Efficient Low-Entropy LLM Watermarking](https://arxiv.org/abs/2505.14112)

*Tianle Gu, Zongqi Wang, Kexin Huang, Yuanqi Yao, Xiangliang Zhang, Yujiu Yang, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** watermarking, entropy, AI-generated content, low-entropy, feature extractor

**Relevance Score:** 7

**TL;DR:** Proposes Invisible Entropy (IE), a watermarking method improving safety and efficiency in low-entropy scenarios, by using a lightweight feature extractor and adaptive thresholds.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the watermarking of AI-generated content in low-entropy conditions without disrupting natural text flow while addressing high computational costs and risks of model leakage in existing methods.

**Method:** Introduces a lightweight feature extractor and an entropy tagger to predict token entropy, along with a threshold navigator to adaptively set entropy thresholds for watermarking.

**Key Contributions:**

	1. Introduction of the Invisible Entropy watermarking paradigm
	2. Use of a lightweight feature extractor and entropy tagger
	3. Development of an adaptive threshold navigator for embedding watermarks

**Result:** Experiments show IE reduces parameter size by 99% and matches the performance of state-of-the-art watermarking methods on HumanEval and MBPP datasets.

**Limitations:** 

**Conclusion:** IE offers a new paradigm for low-entropy watermarking that is both safe and efficient, enhancing detection robustness while maintaining text naturalness.

**Abstract:** Logit-based LLM watermarking traces and verifies AI-generated content by maintaining green and red token lists and increasing the likelihood of green tokens during generation. However, it fails in low-entropy scenarios, where predictable outputs make green token selection difficult without disrupting natural text flow. Existing approaches address this by assuming access to the original LLM to calculate entropy and selectively watermark high-entropy tokens. However, these methods face two major challenges: (1) high computational costs and detection delays due to reliance on the original LLM, and (2) potential risks of model leakage. To address these limitations, we propose Invisible Entropy (IE), a watermarking paradigm designed to enhance both safety and efficiency. Instead of relying on the original LLM, IE introduces a lightweight feature extractor and an entropy tagger to predict whether the entropy of the next token is high or low. Furthermore, based on theoretical analysis, we develop a threshold navigator that adaptively sets entropy thresholds. It identifies a threshold where the watermark ratio decreases as the green token count increases, enhancing the naturalness of the watermarked text and improving detection robustness. Experiments on HumanEval and MBPP datasets demonstrate that IE reduces parameter size by 99\% while achieving performance on par with state-of-the-art methods. Our work introduces a safe and efficient paradigm for low-entropy watermarking. https://github.com/Carol-gutianle/IE https://huggingface.co/datasets/Carol0110/IE-Tagger

</details>


### [82] [Self-Reasoning Language Models: Unfold Hidden Reasoning Chains with Few Reasoning Catalyst](https://arxiv.org/abs/2505.14116)

*Hongru Wang, Deng Cai, Wanjun Zhong, Shijue Huang, Jeff Z. Pan, Zeming Liu, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** Self-Reasoning Language Model, Chain-of-Thought, Machine Learning, Reasoning Tasks, Self-Training

**Relevance Score:** 9

**TL;DR:** The paper presents the Self-Reasoning Language Model (SRLM), which enhances Large Language Models' performance in complex reasoning tasks through self-generated Chain-of-Thought data and iterative self-training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing interest in inference-time scaling to improve the reasoning capabilities of Large Language Models by extending their Chain-of-Thought length, which mirrors human cognitive skills.

**Method:** SRLM synthesizes longer Chain-of-Thought data and improves performance iteratively using a few demonstration examples that aid in unfolding hidden reasoning chains.

**Key Contributions:**

	1. Introduction of Self-Reasoning Language Model (SRLM) for enhancing reasoning performance.
	2. Demonstration of significant average absolute improvements across multiple reasoning tasks.
	3. Methodology for iteratively training LLMs with self-generated reasoning chains.

**Result:** SRLM shows an average absolute improvement of over +2.5 points across five reasoning tasks and up to +7.89 points with increased sampling times, demonstrating diverse reasoning paths.

**Limitations:** 

**Conclusion:** The Self-Reasoning Language Model not only enhances initial performance but also stabilizes and enhances long-term reasoning improvements through self-training techniques.

**Abstract:** Inference-time scaling has attracted much attention which significantly enhance the performance of Large Language Models (LLMs) in complex reasoning tasks by increasing the length of Chain-of-Thought. These longer intermediate reasoning rationales embody various meta-reasoning skills in human cognition, such as reflection and decomposition, being difficult to create and acquire. In this work, we introduce \textit{Self-Reasoning Language Model} (SRLM), where the model itself can synthesize longer CoT data and iteratively improve performance through self-training. By incorporating a few demonstration examples (i.e., 1,000 samples) on how to unfold hidden reasoning chains from existing responses, which act as a reasoning catalyst, we demonstrate that SRLM not only enhances the model's initial performance but also ensures more stable and consistent improvements in subsequent iterations. Our proposed SRLM achieves an average absolute improvement of more than $+2.5$ points across five reasoning tasks: MMLU, GSM8K, ARC-C, HellaSwag, and BBH on two backbone models. Moreover, it brings more improvements with more times of sampling during inference, such as absolute $+7.89$ average improvement with $64$ sampling times, revealing the in-depth, diverse and creative reasoning paths in SRLM against the strong baseline.

</details>


### [83] [Probing BERT for German Compound Semantics](https://arxiv.org/abs/2505.14130)

*Filip Miletić, Aaron Schmid, Sabine Schulte im Walde*

**Main category:** cs.CL

**Keywords:** German BERT, noun compounds, compositionality, transformer architecture, semantic encoding

**Relevance Score:** 4

**TL;DR:** This paper explores how well pretrained German BERT encodes knowledge of noun compound semantics, finding that while representational patterns show similarities to English, performance lags due to German's more complex compounding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the capability of pretrained German BERT models in understanding the semantics of noun compounds, given the challenges presented by the German language's compounding system.

**Method:** The study varies combinations of target tokens, layers, and cased vs. uncased models, evaluating them on their predictive power regarding the compositionality of 868 gold standard noun compounds.

**Key Contributions:**

	1. Identification of representational patterns in German BERT for noun compounds
	2. Comparison of German BERT performance with English counterparts
	3. Insights into the challenges of processing noun compounds in German language.

**Result:** The findings indicate that compositionality information is most recoverable in the early layers of the model, but the performance is significantly below that of comparable English models.

**Limitations:** The study primarily focuses on noun compounds and may not generalize to other semantic structures; the performance gap with English needs further investigation.

**Conclusion:** The results suggest that the task of understanding noun compounding in German is inherently more challenging than in English, likely due to the higher productivity of compound formation and greater constituent-level ambiguity in German.

**Abstract:** This paper investigates the extent to which pretrained German BERT encodes knowledge of noun compound semantics. We comprehensively vary combinations of target tokens, layers, and cased vs. uncased models, and evaluate them by predicting the compositionality of 868 gold standard compounds. Looking at representational patterns within the transformer architecture, we observe trends comparable to equivalent prior work on English, with compositionality information most easily recoverable in the early layers. However, our strongest results clearly lag behind those reported for English, suggesting an inherently more difficult task in German. This may be due to the higher productivity of compounding in German than in English and the associated increase in constituent-level ambiguity, including in our target compound set.

</details>


### [84] [Texts or Images? A Fine-grained Analysis on the Effectiveness of Input Representations and Models for Table Question Answering](https://arxiv.org/abs/2505.14131)

*Wei Zhou, Mohsen Mesgar, Heike Adel, Annemarie Friedrich*

**Main category:** cs.CL

**Keywords:** Table Question Answering, Multi-modal Models, Dynamic Representation Selection

**Relevance Score:** 8

**TL;DR:** A study comparing table representations in TQA shows dynamic selection improves performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of different table representations in table question answering (TQA) using a controlled setup.

**Method:** A controlled study analyzing combinations of table representations and models based on question complexity and table size, with the introduction of a new benchmark for TQA.

**Key Contributions:**

	1. First controlled study on table representations in TQA
	2. New benchmark for TQA based on existing datasets
	3. Introduction of FRES for dynamic representation selection

**Result:** The best table representation and model combination varies across conditions; dynamic selection method FRES shows a 10% performance improvement.

**Limitations:** 

**Conclusion:** Dynamic selection of table representations enhances TQA performance, thus providing insights for future applications.

**Abstract:** In table question answering (TQA), tables are encoded as either texts or images. Prior work suggests that passing images of tables to multi-modal large language models (MLLMs) performs comparably to or even better than using textual input with large language models (LLMs). However, the lack of controlled setups limits fine-grained distinctions between these approaches. In this paper, we conduct the first controlled study on the effectiveness of several combinations of table representations and models from two perspectives: question complexity and table size. We build a new benchmark based on existing TQA datasets. In a systematic analysis of seven pairs of MLLMs and LLMs, we find that the best combination of table representation and model varies across setups. We propose FRES, a method selecting table representations dynamically, and observe a 10% average performance improvement compared to using both representations indiscriminately.

</details>


### [85] [Enhancing Keyphrase Extraction from Academic Articles Using Section Structure Information](https://arxiv.org/abs/2505.14149)

*Chengzhi Zhang, Xinyi Yan, Lei Zhao, Yingyi Zhang*

**Main category:** cs.CL

**Keywords:** Keyphrase Extraction, structural features, academic papers, machine learning, literature retrieval

**Relevance Score:** 6

**TL;DR:** This study enhances Keyphrase Extraction (KPE) from academic papers by incorporating section structure features and integrating extraction results from multiple sections, leading to improved model performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The growing volume of academic literature makes it increasingly difficult for researchers to access relevant papers; thus, an improved KPE method is essential.

**Method:** The study investigates seven structural features of academic articles, tests their impact on KPE models, and employs a keyphrase integration algorithm to consolidate extraction results from various sections of the papers.

**Key Contributions:**

	1. Introduces structural features for KPE
	2. Presents a keyphrase integration algorithm
	3. Demonstrates the impact of section classification on KPE performance

**Result:** Incorporating structural features significantly boosts KPE performance, with the keyphrase integration approach achieving the highest efficacy; the quality of classification of section structures also affects KPE outcomes.

**Limitations:** The study may be limited by the chosen structural features and the generalizability of the findings across different types of academic articles.

**Conclusion:** Utilizing the structural features and information of academic article sections enhances KPE, suggesting a new direction for developing effective literature retrieval tools.

**Abstract:** The exponential increase in academic papers has significantly increased the time required for researchers to access relevant literature. Keyphrase Extraction (KPE) offers a solution to this situation by enabling researchers to efficiently retrieve relevant literature. The current study on KPE from academic articles aims to improve the performance of extraction models through innovative approaches using Title and Abstract as input corpora. However, the semantic richness of keywords is significantly constrained by the length of the abstract. While full-text-based KPE can address this issue, it simultaneously introduces noise, which significantly diminishes KPE performance. To address this issue, this paper utilized the structural features and section texts obtained from the section structure information of academic articles to extract keyphrase from academic papers. The approach consists of two main parts: (1) exploring the effect of seven structural features on KPE models, and (2) integrating the extraction results from all section texts used as input corpora for KPE models via a keyphrase integration algorithm to obtain the keyphrase integration result. Furthermore, this paper also examined the effect of the classification quality of section structure on the KPE performance. The results show that incorporating structural features improves KPE performance, though different features have varying effects on model efficacy. The keyphrase integration approach yields the best performance, and the classification quality of section structure can affect KPE performance. These findings indicate that using the section structure information of academic articles contributes to effective KPE from academic articles. The code and dataset supporting this study are available at https://github.com/yan-xinyi/SSB_KPE.

</details>


### [86] [Prior Prompt Engineering for Reinforcement Fine-Tuning](https://arxiv.org/abs/2505.14157)

*Pittawat Taveekitworachai, Potsawee Manakul, Sarana Nutanong, Kunat Pipatanakul*

**Main category:** cs.CL

**Keywords:** prompt engineering, reinforcement fine-tuning, language models, behavioral styles, performance evaluation

**Relevance Score:** 9

**TL;DR:** The paper examines prior prompt engineering (pPE) in reinforcement fine-tuning (RFT) of language models (LMs), showing that various pPE strategies significantly improve model performance and instill distinct behaviors compared to traditional methods.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched area of prior prompt engineering in relation to reinforcement fine-tuning of language models, which remains largely overlooked compared to other RFT components.

**Method:** The authors translate five inference-time prompt engineering strategies into prior prompt engineering approaches and test them on the Qwen2.5-7B model across various benchmarks including AIME2024, HumanEval+, and GPQA-Diamond.

**Key Contributions:**

	1. Introduces a systematic exploration of prior prompt engineering in reinforcement fine-tuning.
	2. Demonstrates that different pPE strategies yield distinct behavioral styles in language models.
	3. Shows significant performance gains across various benchmarks with pPE approaches.

**Result:** All models trained with prior prompt engineering outperformed their inference-time counterparts, with the null-example pPE strategy yielding the most substantial performance improvement, especially in AIME2024 and GPQA-Diamond.

**Limitations:** 

**Conclusion:** The study highlights the significance of prior prompt engineering as a key factor in reinforcement fine-tuning, demonstrating its ability to enhance performance and enforce distinct behavioral characteristics in language models.

**Abstract:** This paper investigates prior prompt engineering (pPE) in the context of reinforcement fine-tuning (RFT), where language models (LMs) are incentivized to exhibit behaviors that maximize performance through reward signals. While existing RFT research has primarily focused on algorithms, reward shaping, and data curation, the design of the prior prompt--the instructions prepended to queries during training to elicit behaviors such as step-by-step reasoning--remains underexplored. We investigate whether different pPE approaches can guide LMs to internalize distinct behaviors after RFT. Inspired by inference-time prompt engineering (iPE), we translate five representative iPE strategies--reasoning, planning, code-based reasoning, knowledge recall, and null-example utilization--into corresponding pPE approaches. We experiment with Qwen2.5-7B using each of the pPE approaches, then evaluate performance on in-domain and out-of-domain benchmarks (e.g., AIME2024, HumanEval+, and GPQA-Diamond). Our results show that all pPE-trained models surpass their iPE-prompted counterparts, with the null-example pPE approach achieving the largest average performance gain and the highest improvement on AIME2024 and GPQA-Diamond, surpassing the commonly used reasoning approach. Furthermore, by adapting a behavior-classification framework, we demonstrate that different pPE strategies instill distinct behavioral styles in the resulting models. These findings position pPE as a powerful yet understudied axis for RFT.

</details>


### [87] [Temporal Alignment of Time Sensitive Facts with Activation Engineering](https://arxiv.org/abs/2505.14158)

*Sanjay Govindan, Maurice Pagnucco, Yang Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, activation engineering, temporal alignment, factual recall, LLaMA 2

**Relevance Score:** 8

**TL;DR:** This paper investigates activation engineering to improve LLMs' factual recall in context-specific temporal queries without requiring training or dataset creation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLMs to provide time-appropriate responses due to the presence of time-sensitive knowledge in training data.

**Method:** An activation engineering technique is implemented to ground three versions of LLaMA 2 at specific points in time, assessing various injection layers and prompting strategies.

**Key Contributions:**

	1. Introduction of activation engineering for LLM temporal alignment
	2. Demonstrated significant performance improvements without requiring datasets
	3. Achieved computational efficiency in grounding models temporally

**Result:** Improvements of up to 44% and 16% in relative and explicit prompting, achieving results comparable to a fine-tuning method while being more computationally efficient.

**Limitations:** 

**Conclusion:** The proposed activation engineering approach allows for temporal alignment of LLMs with effective results, suggesting a novel alternative to traditional fine-tuning.

**Abstract:** Large Language Models (LLMs) are trained on diverse and often conflicting knowledge spanning multiple domains and time periods. Some of this knowledge is only valid within specific temporal contexts, such as answering the question, "Who is the President of the United States in 2022?" Ensuring LLMs generate time appropriate responses is crucial for maintaining relevance and accuracy. In this work we explore activation engineering as a method for temporally aligning LLMs to improve factual recall without any training or dataset creation. In this research we explore an activation engineering technique to ground three versions of LLaMA 2 to specific points in time and examine the effects of varying injection layers and prompting strategies. Our experiments demonstrate up to a 44% and 16% improvement in relative and explicit prompting respectively, achieving comparable performance to the fine-tuning method proposed by Zhao et al. (2024) . Notably, our approach achieves similar results to the fine-tuning baseline while being significantly more computationally efficient and requiring no pre-aligned datasets.

</details>


### [88] [Breaking Language Barriers or Reinforcing Bias? A Study of Gender and Racial Disparities in Multilingual Contrastive Vision Language Models](https://arxiv.org/abs/2505.14160)

*Zahraa Al Sahili, Ioannis Patras, Matthew Purver*

**Main category:** cs.CL

**Keywords:** multilingual models, vision-language, bias evaluation, gender bias, cross-lingual transfer

**Relevance Score:** 7

**TL;DR:** This paper audits multilingual vision-language models for social biases, specifically examining race and gender biases across different languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the extent of social biases present in multilingual vision-language models and the impact of language resources and grammatical gender on these biases.

**Method:** The study systematically audits three multilingual CLIP checkpoints (M-CLIP, NLLB-CLIP, CAPIVARA-CLIP) using balanced subsets from FairFace and PATA in a zero-shot setting.

**Key Contributions:**

	1. First systematic audit of multilingual vision-language models for biases.
	2. Demonstrated that multilingual models can exhibit stronger gender bias than English-only models.
	3. Identified language-specific bias 'hot spots' that need focused evaluation.

**Result:** Each model exhibited stronger gender bias compared to its English-only counterparts, with amplification of stereotypes especially in low-resource languages.

**Limitations:** The study only examines three models and relies on specific datasets for bias measurement, which may not capture all nuances of social biases.

**Conclusion:** The findings indicate that multilinguality does not mitigate bias and highlight the importance of language-specific evaluations in assessing biases in multilingual models.

**Abstract:** Multilingual vision-language models promise universal image-text retrieval, yet their social biases remain under-explored. We present the first systematic audit of three public multilingual CLIP checkpoints -- M-CLIP, NLLB-CLIP, and CAPIVARA-CLIP -- across ten languages that vary in resource availability and grammatical gender. Using balanced subsets of \textsc{FairFace} and the \textsc{PATA} stereotype suite in a zero-shot setting, we quantify race and gender bias and measure stereotype amplification. Contrary to the assumption that multilinguality mitigates bias, every model exhibits stronger gender bias than its English-only baseline. CAPIVARA-CLIP shows its largest biases precisely in the low-resource languages it targets, while the shared cross-lingual encoder of NLLB-CLIP transports English gender stereotypes into gender-neutral languages; loosely coupled encoders largely avoid this transfer. Highly gendered languages consistently magnify all measured bias types, but even gender-neutral languages remain vulnerable when cross-lingual weight sharing imports foreign stereotypes. Aggregated metrics conceal language-specific ``hot spots,'' underscoring the need for fine-grained, language-aware bias evaluation in future multilingual vision-language research.

</details>


### [89] [PL-FGSA: A Prompt Learning Framework for Fine-Grained Sentiment Analysis Based on MindSpore](https://arxiv.org/abs/2505.14165)

*Zhenkai Qin, Jiajing He, Qiao Fang*

**Main category:** cs.CL

**Keywords:** fine-grained sentiment analysis, prompt learning, TextCNN, MindSpore, aspect extraction

**Relevance Score:** 7

**TL;DR:** PL-FGSA is a unified prompt learning framework for fine-grained sentiment analysis that enhances interpretability and performance in both full and low-resource settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional fine-grained sentiment analysis methods face challenges with generalization and require extensive annotated data and task-specific architectures.

**Method:** PL-FGSA reformulates fine-grained sentiment analysis as a multi-task prompt-augmented generation problem, integrating prompt design with a lightweight TextCNN model on the MindSpore platform.

**Key Contributions:**

	1. Unified approach for aspect extraction and sentiment classification
	2. Integration of prompt learning with TextCNN
	3. Strong performance under low-resource conditions

**Result:** The PL-FGSA model demonstrates superior performance over traditional fine-tuning methods, achieving F1-scores of 0.922, 0.694, and 0.597 on benchmark datasets.

**Limitations:** 

**Conclusion:** The results validate the effectiveness of prompt-based learning in enhancing sentiment analysis, making PL-FGSA a valuable tool for real-world applications.

**Abstract:** Fine-grained sentiment analysis (FGSA) aims to identify sentiment polarity toward specific aspects within a text, enabling more precise opinion mining in domains such as product reviews and social media. However, traditional FGSA approaches often require task-specific architectures and extensive annotated data, limiting their generalization and scalability. To address these challenges, we propose PL-FGSA, a unified prompt learning-based framework implemented using the MindSpore platform, which integrates prompt design with a lightweight TextCNN backbone. Our method reformulates FGSA as a multi-task prompt-augmented generation problem, jointly tackling aspect extraction, sentiment classification, and causal explanation in a unified paradigm. By leveraging prompt-based guidance, PL-FGSA enhances interpretability and achieves strong performance under both full-data and low-resource conditions. Experiments on three benchmark datasets-SST-2, SemEval-2014 Task 4, and MAMS-demonstrate that our model consistently outperforms traditional fine-tuning methods and achieves F1-scores of 0.922, 0.694, and 0.597, respectively. These results validate the effectiveness of prompt-based generalization and highlight the practical value of PL-FGSA for real-world sentiment analysis tasks.

</details>


### [90] [The Strawberry Problem: Emergence of Character-level Understanding in Tokenized Language Models](https://arxiv.org/abs/2505.14172)

*Adrian Cosma, Stefan Ruseti, Emilian Radoi, Mihai Dascalu*

**Main category:** cs.CL

**Keywords:** Large Language Models, tokenization, character-level reasoning, concept emergence, architectural modification

**Relevance Score:** 7

**TL;DR:** This paper addresses the limitations of Large Language Models (LLMs) in performing character-level tasks due to issues stemming from tokenization, offering insights and solutions to improve performance in these areas.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the fundamental limitations of LLMs in character-level reasoning caused by tokenization, aiming to understand and mitigate these pitfalls.

**Method:** The authors conducted experiments using a suite of 19 synthetic tasks to isolate character-level reasoning and analyzed the emergence of concepts during training.

**Key Contributions:**

	1. Analysis of character-level reasoning in LLMs through synthetic tasks
	2. Introduction of a percolation-based model for concept emergence
	3. Proposition of an architectural modification to improve character-level reasoning

**Result:** The findings indicate that character-level reasoning capabilities in LLMs emerge slowly and primarily late in training, which can be explained through percolation-based models of concept emergence.

**Limitations:** 

**Conclusion:** A proposed architectural modification enhances character-level reasoning in LLMs while maintaining the advantages of subword models, providing a framework to address structural blind spots in tokenized LMs.

**Abstract:** Despite their remarkable progress across diverse domains, Large Language Models (LLMs) consistently fail at simple character-level tasks, such as counting letters in words, due to a fundamental limitation: tokenization. In this work, we frame this limitation as a problem of low mutual information and analyze it in terms of concept emergence. Using a suite of 19 synthetic tasks that isolate character-level reasoning in a controlled setting, we show that such capabilities emerge slowly, suddenly, and only late in training. We further show that percolation-based models of concept emergence explain these patterns, suggesting that learning character composition is not fundamentally different from learning commonsense knowledge. To address this bottleneck, we propose a lightweight architectural modification that significantly improves character-level reasoning while preserving the inductive advantages of subword models. Together, our results bridge low-level perceptual gaps in tokenized LMs and provide a principled framework for understanding and mitigating their structural blind spots. We make our code publicly available.

</details>


### [91] [THOR-MoE: Hierarchical Task-Guided and Context-Responsive Routing for Neural Machine Translation](https://arxiv.org/abs/2505.14173)

*Yunlong Liang, Fandong Meng, Jie Zhou*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Neural Machine Translation, Hierarchical Routing

**Relevance Score:** 7

**TL;DR:** THOR-MoE is a novel framework improving sparse Mixture-of-Experts for neural machine translation by using hierarchical, context-aware routing policies to optimize expert selection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current Mixture-of-Experts (MoE) methods for neural machine translation suffer from performance limitations due to reliance on domain-specific knowledge and localized token representation without considering context.

**Method:** THOR-MoE introduces a hierarchical approach by predicting domain/language labels and employing context-responsive routing policies, allowing for more specialized expert allocation for each token.

**Key Contributions:**

	1. Hierarchical task-guided expert routing
	2. Context-responsive token routing
	3. Compatibility with existing MoE architectures

**Result:** THOR-MoE shows improved performance on multi-domain and multilingual translation tasks, illustrating an average improvement of 0.75 BLEU over existing methods with fewer activated parameters.

**Limitations:** 

**Conclusion:** The THOR-MoE framework enhances existing MoE architectures and serves as a plug-and-play module compatible with contemporary routing schemes, demonstrating its wide applicability.

**Abstract:** The sparse Mixture-of-Experts (MoE) has achieved significant progress for neural machine translation (NMT). However, there exist two limitations in current MoE solutions which may lead to sub-optimal performance: 1) they directly use the task knowledge of NMT into MoE (\emph{e.g.}, domain/linguistics-specific knowledge), which are generally unavailable at practical application and neglect the naturally grouped domain/linguistic properties; 2) the expert selection only depends on the localized token representation without considering the context, which fully grasps the state of each token in a global view. To address the above limitations, we propose THOR-MoE via arming the MoE with hierarchical task-guided and context-responsive routing policies. Specifically, it 1) firstly predicts the domain/language label and then extracts mixed domain/language representation to allocate task-level experts in a hierarchical manner; 2) injects the context information to enhance the token routing from the pre-selected task-level experts set, which can help each token to be accurately routed to more specialized and suitable experts. Extensive experiments on multi-domain translation and multilingual translation benchmarks with different architectures consistently demonstrate the superior performance of THOR-MoE. Additionally, the THOR-MoE operates as a plug-and-play module compatible with existing Top-$k$~\cite{shazeer2017} and Top-$p$~\cite{huang-etal-2024-harder} routing schemes, ensuring broad applicability across diverse MoE architectures. For instance, compared with vanilla Top-$p$~\cite{huang-etal-2024-harder} routing, the context-aware manner can achieve an average improvement of 0.75 BLEU with less than 22\% activated parameters on multi-domain translation tasks.

</details>


### [92] [Cheaper, Better, Faster, Stronger: Robust Text-to-SQL without Chain-of-Thought or Fine-Tuning](https://arxiv.org/abs/2505.14174)

*Yusuf Denizay Dönder, Derek Hommel, Andrea W Wen-Yi, David Mimno, Unso Eun Seo Jo*

**Main category:** cs.CL

**Keywords:** cost-efficient, text-to-SQL, N-rep consistency, LLMs, BIRD benchmark

**Relevance Score:** 7

**TL;DR:** This paper presents N-rep consistency, a cost-efficient method for text-to-SQL tasks that outperforms more expensive methods while significantly reducing query costs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs associated with existing LLM approaches for text-to-SQL tasks, which utilize methods like Chain-of-Thought and fine-tuning.

**Method:** N-rep consistency uses multiple representations of the same schema input to improve robustness and performance, eliminating the need for reasoning and fine-tuning.

**Key Contributions:**

	1. Introduction of N-rep consistency for text-to-SQL tasks
	2. Demonstrated comparable performance to costly state-of-the-art methods
	3. Significantly lower operational costs per query

**Result:** N-rep achieves comparable BIRD benchmark scores to more expensive techniques, costing only $0.039 per query.

**Limitations:** 

**Conclusion:** N-rep consistency is the best-performing text-to-SQL solution in its price range, offering a robust alternative to pricier methods.

**Abstract:** LLMs are effective at code generation tasks like text-to-SQL, but is it worth the cost? Many state-of-the-art approaches use non-task-specific LLM techniques including Chain-of-Thought (CoT), self-consistency, and fine-tuning. These methods can be costly at inference time, sometimes requiring over a hundred LLM calls with reasoning, incurring average costs of up to \$0.46 per query, while fine-tuning models can cost thousands of dollars. We introduce "N-rep" consistency, a more cost-efficient text-to-SQL approach that achieves similar BIRD benchmark scores as other more expensive methods, at only \$0.039 per query. N-rep leverages multiple representations of the same schema input to mitigate weaknesses in any single representation, making the solution more robust and allowing the use of smaller and cheaper models without any reasoning or fine-tuning. To our knowledge, N-rep is the best-performing text-to-SQL approach in its cost range.

</details>


### [93] [Tokenization Constraints in LLMs: A Study of Symbolic and Arithmetic Reasoning Limits](https://arxiv.org/abs/2505.14178)

*Xiang Zhang, Juntai Cao, Jiaqi Wei, Yiwei Xu, Chenyu You*

**Main category:** cs.CL

**Keywords:** tokenization, Chain-of-Thought, symbolic reasoning, Token Awareness, LLM

**Relevance Score:** 8

**TL;DR:** This paper investigates how tokenization schemes impact the reasoning abilities of language models, introducing the concept of Token Awareness to highlight the importance of token granularity in preserving logical structures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the structure of tokenization, particularly in models using subword-based methods like BPE, affects symbolic reasoning and performance in language models.

**Method:** The authors conduct both theoretical analysis and empirical evaluations on arithmetic and symbolic tasks to assess how different tokenization schemes influence reasoning capabilities in language models.

**Key Contributions:**

	1. Introduces the notion of Token Awareness to highlight token granularity's impact on reasoning.
	2. Demonstrates that token structure can significantly influence model performance in reasoning tasks.
	3. Shows that better token alignment can enable smaller models to perform better than larger ones in structured reasoning.

**Result:** The study finds that poor token granularity leads to disrupted logical alignment and hampers generalization in symbolic reasoning, while better-aligned token structures enhance model performance—allowing smaller models to outperform larger ones in certain reasoning tasks.

**Limitations:** 

**Conclusion:** Symbolic reasoning ability in LLMs is significantly affected by token-level representations rather than being solely dependent on model architecture.

**Abstract:** Tokenization is the first - and often underappreciated - layer of computation in language models. While Chain-of-Thought (CoT) prompting enables transformer models to approximate recurrent computation by externalizing intermediate steps, we show that the success of such reasoning is fundamentally bounded by the structure of tokenized inputs. This work presents a theoretical and empirical investigation into how tokenization schemes, particularly subword-based methods like byte-pair encoding (BPE), impede symbolic computation by merging or obscuring atomic reasoning units. We introduce the notion of Token Awareness to formalize how poor token granularity disrupts logical alignment and prevents models from generalizing symbolic procedures. Through systematic evaluation on arithmetic and symbolic tasks, we demonstrate that token structure dramatically affect reasoning performance, causing failure even with CoT, while atomically-aligned formats unlock strong generalization, allowing small models (e.g., GPT-4o-mini) to outperform larger systems (e.g., o1) in structured reasoning. Our findings reveal that symbolic reasoning ability in LLMs is not purely architectural, but deeply conditioned on token-level representations.

</details>


### [94] [Enhancing Abstractive Summarization of Scientific Papers Using Structure Information](https://arxiv.org/abs/2505.14179)

*Tong Bao, Heng Zhang, Chengzhi Zhang*

**Main category:** cs.CL

**Keywords:** abstractive summarization, scientific papers, structural function recognition, Longformer, context-aware summaries

**Relevance Score:** 8

**TL;DR:** This paper presents a two-stage abstractive summarization framework for scientific papers that improves the capture of structural information and generates comprehensive summaries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing summarization methods fail to capture the structural information inherent in scientific papers and struggle with robustness across disciplines.

**Method:** A two-stage framework that includes standardizing chapter titles and training a classifier for structural component recognition, followed by using Longformer for context-aware summary generation.

**Key Contributions:**

	1. Introduction of a two-stage framework for summarization that recognizes structural functions.
	2. Development of a large-scale dataset for training structural function recognition.
	3. Utilization of Longformer for context-aware summary generation.

**Result:** Experiments show that the proposed method outperforms advanced baselines and produces more comprehensive summaries on domain-specific datasets.

**Limitations:** 

**Conclusion:** The approach effectively leverages structural recognition to enhance the quality of summarization in scientific papers.

**Abstract:** Abstractive summarization of scientific papers has always been a research focus, yet existing methods face two main challenges. First, most summarization models rely on Encoder-Decoder architectures that treat papers as sequences of words, thus fail to fully capture the structured information inherent in scientific papers. Second, existing research often use keyword mapping or feature engineering to identify the structural information, but these methods struggle with the structural flexibility of scientific papers and lack robustness across different disciplines. To address these challenges, we propose a two-stage abstractive summarization framework that leverages automatic recognition of structural functions within scientific papers. In the first stage, we standardize chapter titles from numerous scientific papers and construct a large-scale dataset for structural function recognition. A classifier is then trained to automatically identify the key structural components (e.g., Background, Methods, Results, Discussion), which provides a foundation for generating more balanced summaries. In the second stage, we employ Longformer to capture rich contextual relationships across sections and generating context-aware summaries. Experiments conducted on two domain-specific scientific paper summarization datasets demonstrate that our method outperforms advanced baselines, and generates more comprehensive summaries. The code and dataset can be accessed at https://github.com/tongbao96/code-for-SFR-AS.

</details>


### [95] [SlangDIT: Benchmarking LLMs in Interpretative Slang Translation](https://arxiv.org/abs/2505.14181)

*Yunlong Liang, Fandong Meng, Jiaan Wang, Jie Zhou*

**Main category:** cs.CL

**Keywords:** slang translation, slang detection, cross-lingual explanation, large language models, natural language processing

**Relevance Score:** 7

**TL;DR:** The paper introduces SlangDIT, a benchmark for slang detection, explanation, and translation, and presents the SlangOWL model that improves translation accuracy using context-dependent slang interpretation in English-Chinese pairs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of accurately translating slang terms, which often have meanings that extend beyond their literal interpretations, highlighting the need for a comprehensive approach that correlates slang detection, explanation, and translation.

**Method:** The authors propose a new task called interpretative slang translation (SlangDIT) that consists of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation. They construct a dataset with 25k English-Chinese sentence pairs and develop the SlangOWL model, which identifies slang, analyzes its meaning, and provides contextually appropriate translations.

**Key Contributions:**

	1. Introduction of the SlangDIT dataset containing 25k English-Chinese sentence pairs with slang labeling.
	2. Development of the SlangOWL model that offers context-aware translations of slang.
	3. Exploration of the interdependencies between slang detection, explanation, and translation tasks.

**Result:** Experiments demonstrate that the SlangOWL model significantly outperforms traditional LLMs and supervised fine-tuned models, enhancing their ability to manage slang within translation tasks.

**Limitations:** The work is a progress report and may not present final results or fully validated methodologies.

**Conclusion:** The introduction of the SlangDIT benchmark and the SlangOWL model demonstrates the interdependence of slang detection, explanation, and translation, showing that a more integrated approach can lead to superior translation outcomes.

**Abstract:** The challenge of slang translation lies in capturing context-dependent semantic extensions, as slang terms often convey meanings beyond their literal interpretation. While slang detection, explanation, and translation have been studied as isolated tasks in the era of large language models (LLMs), their intrinsic interdependence remains underexplored. The main reason is lacking of a benchmark where the two tasks can be a prerequisite for the third one, which can facilitate idiomatic translation. In this paper, we introduce the interpretative slang translation task (named SlangDIT) consisting of three sub-tasks: slang detection, cross-lingual slang explanation, and slang translation within the current context, aiming to generate more accurate translation with the help of slang detection and slang explanation. To this end, we construct a SlangDIT dataset, containing over 25k English-Chinese sentence pairs. Each source sentence mentions at least one slang term and is labeled with corresponding cross-lingual slang explanation. Based on the benchmark, we propose a deep thinking model, named SlangOWL. It firstly identifies whether the sentence contains a slang, and then judges whether the slang is polysemous and analyze its possible meaning. Further, the SlangOWL provides the best explanation of the slang term targeting on the current context. Finally, according to the whole thought, the SlangOWL offers a suitable translation. Our experiments on LLMs (\emph{e.g.}, Qwen2.5 and LLama-3.1), show that our deep thinking approach indeed enhances the performance of LLMs where the proposed SLangOWL significantly surpasses the vanilla models and supervised fine-tuned models without thinking.

</details>


### [96] [ThinkSwitcher: When to Think Hard, When to Think Fast](https://arxiv.org/abs/2505.14183)

*Guosheng Liang, Longguang Zhong, Ziyi Yang, Xiaojun Quan*

**Main category:** cs.CL

**Keywords:** large reasoning models, chain-of-thought reasoning, efficient learning

**Relevance Score:** 8

**TL;DR:** ThinkSwitcher is a framework that allows large reasoning models to dynamically switch between short and long chain-of-thought reasoning based on task complexity, improving efficiency while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large reasoning models can overthink simple tasks, leading to unnecessary computational overhead. This paper explores an efficient method to leverage the inherent capabilities of these models.

**Method:** The proposed ThinkSwitcher framework utilizes a lightweight switching module that is trained with supervision signals from the performance of reasoning modes across tasks.

**Key Contributions:**

	1. Dynamic switching between short and long CoT modes
	2. Lightweight switching module trained with performance signals
	3. Reduction in computational cost without sacrificing accuracy

**Result:** ThinkSwitcher reduces computational cost by 20-30% while maintaining high accuracy on complex tasks according to experiments on various reasoning benchmarks.

**Limitations:** 

**Conclusion:** ThinkSwitcher provides a scalable and efficient solution for the deployment of large reasoning models by optimizing reasoning modes based on task complexity.

**Abstract:** Large reasoning models (LRMs) excel at solving complex tasks by leveraging long chain-of-thought (CoT) reasoning. However, this often leads to overthinking on simple tasks, resulting in unnecessary computational overhead. We observe that LRMs inherently possess the capability for efficient short CoT reasoning, which can be reliably elicited through prompt design. To leverage this capability, we propose ThinkSwitcher, a framework that enables a single LRM to dynamically switch between short and long CoT modes based on task complexity. ThinkSwitcher introduces a lightweight switching module trained with supervision signals derived from the relative performance of each reasoning mode across tasks. Experiments on multiple reasoning benchmarks show that ThinkSwitcher reduces computational cost by 20-30% while maintaining high accuracy on complex tasks. This demonstrates the effectiveness of ThinkSwitcher as a scalable and efficient solution for unified LRM deployment.

</details>


### [97] [Unraveling Interwoven Roles of Large Language Models in Authorship Privacy: Obfuscation, Mimicking, and Verification](https://arxiv.org/abs/2505.14195)

*Tuc Nguyen, Yifan Hu, Thai Le*

**Main category:** cs.CL

**Keywords:** authorshio privacy, large language models, authorship obfuscation, authorship mimicking, privacy risks

**Relevance Score:** 8

**TL;DR:** This paper presents a unified framework for understanding the interactions among authorship obfuscation, mimicking, and verification, focusing on privacy risks in the context of large language models.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the gap in understanding how authorship privacy tasks such as obfuscation, mimicking, and verification interact, particularly with the rise of LLMs that can reproduce sensitive user information.

**Method:** The authors propose a framework that quantifies the interrelations among authorship obfuscation, mimicking, and verification, considering their effects at a given time and over time, and incorporating demographic metadata into the analysis.

**Key Contributions:**

	1. Unified framework for analyzing authorship privacy tasks
	2. Quantitative assessment of inter-task dynamics
	3. Inclusion of demographic metadata in privacy risk analysis

**Result:** The paper identifies how the three authorship tasks interact to transform human-authored text and highlights the role of demographic factors in influencing privacy risks and task performances.

**Limitations:** 

**Conclusion:** The findings emphasize the need to consider the complex dynamics among authorship privacy tasks in the development and application of LLMs, urging for a comprehensive approach to privacy in user-generated content.

**Abstract:** Recent advancements in large language models (LLMs) have been fueled by large scale training corpora drawn from diverse sources such as websites, news articles, and books. These datasets often contain explicit user information, such as person names and addresses, that LLMs may unintentionally reproduce in their generated outputs. Beyond such explicit content, LLMs can also leak identity revealing cues through implicit signals such as distinctive writing styles, raising significant concerns about authorship privacy. There are three major automated tasks in authorship privacy, namely authorship obfuscation (AO), authorship mimicking (AM), and authorship verification (AV). Prior research has studied AO, AM, and AV independently. However, their interplays remain under explored, which leaves a major research gap, especially in the era of LLMs, where they are profoundly shaping how we curate and share user generated content, and the distinction between machine generated and human authored text is also increasingly blurred. This work then presents the first unified framework for analyzing the dynamic relationships among LLM enabled AO, AM, and AV in the context of authorship privacy. We quantify how they interact with each other to transform human authored text, examining effects at a single point in time and iteratively over time. We also examine the role of demographic metadata, such as gender, academic background, in modulating their performances, inter-task dynamics, and privacy risks. All source code will be publicly available.

</details>


### [98] [Automatic Dataset Generation for Knowledge Intensive Question Answering Tasks](https://arxiv.org/abs/2505.14212)

*Sizhe Yuen, Ting Su, Ziyang Wang, Yali Du, Adam J. Sobey*

**Main category:** cs.CL

**Keywords:** Question-Answering, Large Language Models, Automated Generation

**Relevance Score:** 8

**TL;DR:** A novel approach using LLMs to generate context-based QA pairs enhances knowledge-intensive question-answering systems by improving reasoning and comprehension.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance QA systems struggling with complex reasoning and the integration of real-time knowledge, providing a more effective approach for automated data generation.

**Method:** The paper presents an automated QA generator that creates context-based QA pairs for fine-tuning LLMs, utilizing evaluations through perplexity, ROUGE, BLEU, and BERTScore.

**Key Contributions:**

	1. Automated generation of context-based QA pairs
	2. Improved reasoning capabilities in LLMs
	3. Demonstrated enhancements in coherence and accuracy over existing systems

**Result:** The proposed system significantly improves logical coherence and factual accuracy, with Mistral-7b-v0.3 outperforming Llama-3-8b on several performance metrics for LLM-generated QA pairs.

**Limitations:** 

**Conclusion:** The findings suggest adaptability in AI systems by reducing reliance on human labeling through automated QA pair generation.

**Abstract:** A question-answering (QA) system is to search suitable answers within a knowledge base. Current QA systems struggle with queries requiring complex reasoning or real-time knowledge integration. They are often supplemented with retrieval techniques on a data source such as Retrieval-Augmented Generation (RAG). However, RAG continues to face challenges in handling complex reasoning and logical connections between multiple sources of information. A novel approach for enhancing Large Language Models (LLMs) in knowledge-intensive QA tasks is presented through the automated generation of context-based QA pairs. This methodology leverages LLMs to create fine-tuning data, reducing reliance on human labelling and improving model comprehension and reasoning capabilities. The proposed system includes an automated QA generator and a model fine-tuner, evaluated using perplexity, ROUGE, BLEU, and BERTScore. Comprehensive experiments demonstrate improvements in logical coherence and factual accuracy, with implications for developing adaptable Artificial Intelligence (AI) systems. Mistral-7b-v0.3 outperforms Llama-3-8b with BERT F1, BLEU, and ROUGE scores 0.858, 0.172, and 0.260 of for the LLM generated QA pairs compared to scores of 0.836, 0.083, and 0.139 for the human annotated QA pairs.

</details>


### [99] ["Haet Bhasha aur Diskrimineshun": Phonetic Perturbations in Code-Mixed Hinglish to Red-Team LLMs](https://arxiv.org/abs/2505.14226)

*Darpan Aswal, Siddharth D Jaiswal*

**Main category:** cs.CL

**Keywords:** Large Language Models, jailbreak, code-mixing, phonetic perturbations, multimodal

**Relevance Score:** 8

**TL;DR:** This study introduces novel strategies for jailbreaking Large Language Models (LLMs) through code-mixing and phonetic perturbations, showing high effectiveness in text and image generation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address existing vulnerabilities in LLMs, particularly in multilingual contexts, where models are susceptible to jailbreaking strategies that bypass safety filters.

**Method:** The paper introduces new jailbreak strategies that utilize code-mixing and phonetic perturbations to generate text and image outputs. These strategies were evaluated for their effectiveness in bypassing safety mechanisms.

**Key Contributions:**

	1. Introduction of novel jailbreak strategies using code-mixing and phonetic perturbations.
	2. High effectiveness in bypassing safety filters in both text and image generation.
	3. Contribution to the understanding of word tokenization impacts on jailbreak success.

**Result:** The proposed methods achieved a 99% Attack Success Rate for text generation and 78% for image generation, with an Attack Relevance Rate of 100% for text and 95% for image generation using the new prompts.

**Limitations:** The study may not address all forms of multilingual and multimodal vulnerabilities and focuses primarily on specific attack strategies.

**Conclusion:** The findings highlight the need for improved safety alignment in multilingual multimodal models, suggesting that LLMs should better handle code-mixed prompts with misspellings in real-world applications.

**Abstract:** Large Language Models (LLMs) have become increasingly powerful, with multilingual and multimodal capabilities improving by the day. These models are being evaluated through audits, alignment studies and red-teaming efforts to expose model vulnerabilities towards generating harmful, biased and unfair content. Existing red-teaming efforts have previously focused on the English language, using fixed template-based attacks; thus, models continue to be susceptible to multilingual jailbreaking strategies, especially in the multimodal context. In this study, we introduce a novel strategy that leverages code-mixing and phonetic perturbations to jailbreak LLMs for both text and image generation tasks. We also introduce two new jailbreak strategies that show higher effectiveness than baseline strategies. Our work presents a method to effectively bypass safety filters in LLMs while maintaining interpretability by applying phonetic misspellings to sensitive words in code-mixed prompts. Our novel prompts achieve a 99% Attack Success Rate for text generation and 78% for image generation, with Attack Relevance Rate of 100% for text generation and 95% for image generation when using the phonetically perturbed code-mixed prompts. Our interpretability experiments reveal that phonetic perturbations impact word tokenization, leading to jailbreak success. Our study motivates increasing the focus towards more generalizable safety alignment for multilingual multimodal models, especially in real-world settings wherein prompts can have misspelt words.

</details>


### [100] [Mechanistic Fine-tuning for In-context Learning](https://arxiv.org/abs/2505.14233)

*Hakaze Cho, Peng Luo, Mariko Kato, Rin Kaenbyou, Naoya Inoue*

**Main category:** cs.CL

**Keywords:** In-context Learning, Language Models, Attention Behavior Fine-Tuning, Machine Learning, Mechanistic Interpretability

**Relevance Score:** 8

**TL;DR:** The paper introduces Attention Behavior Fine-Tuning (ABFT) to enhance in-context learning in Language Models (LMs) while reducing computational costs by focusing on attention scores rather than final outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the high computational costs associated with fine-tuning Language Models on in-context learning (ICL) style data, and aims to improve efficiency while maintaining performance.

**Method:** The proposed method, Attention Behavior Fine-Tuning (ABFT), modifies training objectives to focus on attention scores that emphasize correct label tokens in the input context, rather than the final model outputs.

**Key Contributions:**

	1. Introduction of Attention Behavior Fine-Tuning (ABFT) for LMs
	2. Demonstrated reduction in data cost with maintained or improved performance
	3. Insights into the influence of training objectives on the mechanistic behavior of LMs

**Result:** ABFT shows superior performance across 9 modern LMs and 8 datasets in terms of performance, robustness, unbiasedness, and efficiency, achieving these results with only around 0.01% of the data cost compared to previous methods.

**Limitations:** 

**Conclusion:** The findings suggest that the ABFT objective is intrinsically contained within the existing end-to-end training objectives, highlighting the potential for achieving mechanistic interpretability in language models.

**Abstract:** In-context Learning (ICL) utilizes structured demonstration-query inputs to induce few-shot learning on Language Models (LMs), which are not originally pre-trained on ICL-style data. To bridge the gap between ICL and pre-training, some approaches fine-tune LMs on large ICL-style datasets by an end-to-end paradigm with massive computational costs. To reduce such costs, in this paper, we propose Attention Behavior Fine-Tuning (ABFT), utilizing the previous findings on the inner mechanism of ICL, building training objectives on the attention scores instead of the final outputs, to force the attention scores to focus on the correct label tokens presented in the context and mitigate attention scores from the wrong label tokens. Our experiments on 9 modern LMs and 8 datasets empirically find that ABFT outperforms in performance, robustness, unbiasedness, and efficiency, with only around 0.01% data cost compared to the previous methods. Moreover, our subsequent analysis finds that the end-to-end training objective contains the ABFT objective, suggesting the implicit bias of ICL-style data to the emergence of induction heads. Our work demonstrates the possibility of controlling specific module sequences within LMs to improve their behavior, opening up the future application of mechanistic interpretability.

</details>


### [101] [ABBA: Highly Expressive Hadamard Product Adaptation for Large Language Models](https://arxiv.org/abs/2505.14238)

*Raghav Singhal, Kaustubh Ponkshe, Rohit Vartak, Praneeth Vepakomma*

**Main category:** cs.CL

**Keywords:** Large Language Models, Parameter-Efficient Fine-Tuning, ABBA

**Relevance Score:** 8

**TL;DR:** ABBA is a new Parameter-Efficient Fine-Tuning architecture that improves the adaptability of Large Language Models by allowing independent optimization of updates from pre-trained weights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of efficiently adapting Large Language Models to new domains, overcoming limitations of existing PEFT methods like LoRA and HiRA which are constrained by their rank and structure.

**Method:** ABBA reparameterizes updates as a Hadamard product of two independently learnable low-rank matrices, fully decoupling the update process from the pre-trained weights.

**Key Contributions:**

	1. Introduction of ABBA as a new PEFT architecture
	2. Full decoupling of updates from pre-trained weights
	3. State-of-the-art performance on benchmark tasks

**Result:** ABBA significantly enhances expressivity within the same parameter budget and achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, outperforming existing PEFT methods across multiple models.

**Limitations:** 

**Conclusion:** The introduction of ABBA demonstrates a new direction for PEFT methods, allowing more flexible optimization techniques that lead to improved performance.

**Abstract:** Large Language Models have demonstrated strong performance across a wide range of tasks, but adapting them efficiently to new domains remains a key challenge. Parameter-Efficient Fine-Tuning (PEFT) methods address this by introducing lightweight, trainable modules while keeping most pre-trained weights fixed. The prevailing approach, LoRA, models updates using a low-rank decomposition, but its expressivity is inherently constrained by the rank. Recent methods like HiRA aim to increase expressivity by incorporating a Hadamard product with the frozen weights, but still rely on the structure of the pre-trained model. We introduce ABBA, a new PEFT architecture that reparameterizes the update as a Hadamard product of two independently learnable low-rank matrices. In contrast to prior work, ABBA fully decouples the update from the pre-trained weights, enabling both components to be optimized freely. This leads to significantly higher expressivity under the same parameter budget. We formally analyze ABBA's expressive capacity and validate its advantages through matrix reconstruction experiments. Empirically, ABBA achieves state-of-the-art results on arithmetic and commonsense reasoning benchmarks, consistently outperforming existing PEFT methods by a significant margin across multiple models. Our code is publicly available at: https://github.com/CERT-Lab/abba.

</details>


### [102] [Technical Report on classification of literature related to children speech disorder](https://arxiv.org/abs/2505.14242)

*Ziang Wang, Amir Aryani*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Speech Disorders, Topic Modeling, Literature Review, Childhood

**Relevance Score:** 4

**TL;DR:** This report outlines an NLP approach to classify scientific literature on childhood speech disorders using LDA and BERTopic, identifying 14 thematic clusters and suggesting implications for automating literature reviews in speech-language pathology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to develop a systematic way to classify and review scientific literature related to childhood speech disorders, enhancing the current methodologies with NLP techniques.

**Method:** The authors retrieved 4,804 relevant articles from PubMed, applied cleaning and pre-processing, and utilized LDA and BERTopic for topic modeling to identify clusters in the literature.

**Key Contributions:**

	1. Implementation of an NLP-based classification framework for scientific literature
	2. Identification of clinically meaningful clusters in childhood speech disorders
	3. Evaluation of topic modeling techniques tailored to speech pathology

**Result:** The study identified 14 clinically meaningful clusters, achieving a coherence score of 0.42 for LDA and showing low outlier topics for BERTopic, indicating effective classification and coherence.

**Limitations:** 

**Conclusion:** The findings support automating literature reviews in speech-language pathology, providing a foundation for further research in this area.

**Abstract:** This technical report presents a natural language processing (NLP)-based approach for systematically classifying scientific literature on childhood speech disorders. We retrieved and filtered 4,804 relevant articles published after 2015 from the PubMed database using domain-specific keywords. After cleaning and pre-processing the abstracts, we applied two topic modeling techniques - Latent Dirichlet Allocation (LDA) and BERTopic - to identify latent thematic structures in the corpus. Our models uncovered 14 clinically meaningful clusters, such as infantile hyperactivity and abnormal epileptic behavior. To improve relevance and precision, we incorporated a custom stop word list tailored to speech pathology. Evaluation results showed that the LDA model achieved a coherence score of 0.42 and a perplexity of -7.5, indicating strong topic coherence and predictive performance. The BERTopic model exhibited a low proportion of outlier topics (less than 20%), demonstrating its capacity to classify heterogeneous literature effectively. These results provide a foundation for automating literature reviews in speech-language pathology.

</details>


### [103] [TransBench: Benchmarking Machine Translation for Industrial-Scale Applications](https://arxiv.org/abs/2505.14244)

*Haijun Li, Tianqi Shi, Zifu Shang, Yuxuan Han, Xueyu Zhao, Hao Wang, Yu Qian, Zhiqiang Qian, Linlong Xu, Minghao Wu, Chenyang Lyu, Longyue Wang, Gongbo Tang, Weihua Luo, Zhao Xu, Kaifu Zhang*

**Main category:** cs.CL

**Keywords:** machine translation, evaluation framework, domain-specific, e-commerce, large language models

**Relevance Score:** 6

**TL;DR:** This paper introduces a three-level translation capability framework and a novel benchmark, TransBench, to improve machine translation quality in industrial contexts, particularly in e-commerce.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for more effective evaluation frameworks for machine translation in specialized domains, as existing models fail to capture the nuances of domain-specific terminology and cultural contexts.

**Method:** The authors propose a three-level framework comprising Basic Linguistic Competence, Domain-Specific Proficiency, and Cultural Adaptation, and develop a benchmark, TransBench, using 17,000 professionally translated sentences across various scenarios and language pairs.

**Key Contributions:**

	1. Structured framework for industrial MT evaluation
	2. First publicly available benchmark for e-commerce translation
	3. Novel metrics for multi-level translation quality

**Result:** TransBench integrates traditional evaluation metrics with a new domain-specific model, providing a comprehensive tool for assessing machine translation performance in specific industrial sectors.

**Limitations:** 

**Conclusion:** The proposed framework and benchmark aim to bridge the existing evaluation gap, allowing better assessment and enhancement of machine translation in specialized industries like e-commerce.

**Abstract:** Machine translation (MT) has become indispensable for cross-border communication in globalized industries like e-commerce, finance, and legal services, with recent advancements in large language models (LLMs) significantly enhancing translation quality. However, applying general-purpose MT models to industrial scenarios reveals critical limitations due to domain-specific terminology, cultural nuances, and stylistic conventions absent in generic benchmarks. Existing evaluation frameworks inadequately assess performance in specialized contexts, creating a gap between academic benchmarks and real-world efficacy. To address this, we propose a three-level translation capability framework: (1) Basic Linguistic Competence, (2) Domain-Specific Proficiency, and (3) Cultural Adaptation, emphasizing the need for holistic evaluation across these dimensions. We introduce TransBench, a benchmark tailored for industrial MT, initially targeting international e-commerce with 17,000 professionally translated sentences spanning 4 main scenarios and 33 language pairs. TransBench integrates traditional metrics (BLEU, TER) with Marco-MOS, a domain-specific evaluation model, and provides guidelines for reproducible benchmark construction. Our contributions include: (1) a structured framework for industrial MT evaluation, (2) the first publicly available benchmark for e-commerce translation, (3) novel metrics probing multi-level translation quality, and (4) open-sourced evaluation tools. This work bridges the evaluation gap, enabling researchers and practitioners to systematically assess and enhance MT systems for industry-specific needs.

</details>


### [104] [FuxiMT: Sparsifying Large Language Models for Chinese-Centric Multilingual Machine Translation](https://arxiv.org/abs/2505.14256)

*Shaolin Zhu, Tianyu Dong, Bo Li, Deyi Xiong*

**Main category:** cs.CL

**Keywords:** multilingual machine translation, sparsified LLM, low-resource languages, zero-shot translation, Mixture-of-Experts

**Relevance Score:** 6

**TL;DR:** FuxiMT is a multilingual machine translation model that utilizes a sparsified LLM, demonstrating strong performance in low-resource language translation and zero-shot capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve machine translation for low-resource languages and enhance multilingual communication using a robust model architecture.

**Method:** FuxiMT is trained using a two-stage strategy: pre-training on a large Chinese corpus followed by multilingual fine-tuning on a dataset of 65 languages, incorporating Mixture-of-Experts and curriculum learning.

**Key Contributions:**

	1. Introduction of FuxiMT as a Chinese-centric multilingual translation model
	2. Utilization of Mixture-of-Experts and curriculum learning to enhance performance
	3. Demonstration of significant improvements in low-resource translation scenarios

**Result:** FuxiMT outperforms state-of-the-art models, especially under low-resource conditions, and shows zero-shot translation abilities for unseen languages.

**Limitations:** 

**Conclusion:** The findings suggest FuxiMT can effectively address translation challenges in multilingual contexts, especially where data availability is limited.

**Abstract:** In this paper, we present FuxiMT, a novel Chinese-centric multilingual machine translation model powered by a sparsified large language model (LLM). We adopt a two-stage strategy to train FuxiMT. We first pre-train the model on a massive Chinese corpus and then conduct multilingual fine-tuning on a large parallel dataset encompassing 65 languages. FuxiMT incorporates Mixture-of-Experts (MoEs) and employs a curriculum learning strategy for robust performance across various resource levels. Experimental results demonstrate that FuxiMT significantly outperforms strong baselines, including state-of-the-art LLMs and machine translation models, particularly under low-resource scenarios. Furthermore, FuxiMT exhibits remarkable zero-shot translation capabilities for unseen language pairs, indicating its potential to bridge communication gaps where parallel data are scarce or unavailable.

</details>


### [105] [Think-J: Learning to Think for Generative LLM-as-a-Judge](https://arxiv.org/abs/2505.14268)

*Hui Huang, Yancheng He, Hongli Zhou, Rui Zhang, Wei Liu, Weixun Wang, Wenbo Su, Bo Zheng, Jiaheng Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, judgment modeling, reinforcement learning, evaluative capability, human annotations

**Relevance Score:** 9

**TL;DR:** The paper presents Think-J, an improved model for LLM evaluation that enhances the judgment capabilities of Large Language Models (LLMs) through reinforcement learning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of LLMs in evaluating generated responses effectively, which is crucial for LLM evaluation and reward modeling.

**Method:** The authors developed Think-J by initially using a small curated dataset for judgment thinking capabilities, followed by optimizing these capabilities with two reinforcement learning approaches: offline RL (using a critic model) and online RL (using rule-based rewards).

**Key Contributions:**

	1. Introduction of Think-J model for LLM evaluation
	2. Use of reinforcement learning for judgment optimization
	3. Significant performance improvement without extra human annotations

**Result:** Experimental results indicate that Think-J significantly improves LLM evaluation performance, outperforming previous LLM-Judge models without the need for additional human annotations.

**Limitations:** 

**Conclusion:** Think-J demonstrates that optimizing judgment thinking through RL can enhance the evaluation effectiveness of LLMs, providing a more efficient method for response generation evaluation.

**Abstract:** LLM-as-a-Judge refers to the automatic modeling of preferences for responses generated by Large Language Models (LLMs), which is of significant importance for both LLM evaluation and reward modeling. Although generative LLMs have made substantial progress in various tasks, their performance as LLM-Judge still falls short of expectations. In this work, we propose Think-J, which improves generative LLM-as-a-Judge by learning how to think. We first utilized a small amount of curated data to develop the model with initial judgment thinking capabilities. Subsequently, we optimize the judgment thinking traces based on reinforcement learning (RL). We propose two methods for judgment thinking optimization, based on offline and online RL, respectively. The offline RL requires training a critic model to construct positive and negative examples for learning. The online method defines rule-based reward as feedback for optimization. Experimental results showed that our approach can significantly enhance the evaluation capability of generative LLM-Judge, surpassing both generative and classifier-based LLM-Judge without requiring extra human annotations.

</details>


### [106] [FAID: Fine-grained AI-generated Text Detection using Multi-task Auxiliary and Multi-level Contrastive Learning](https://arxiv.org/abs/2505.14271)

*Minh Ngoc Ta, Dong Cao Van, Duc-Anh Hoang, Minh Le-Anh, Truong Nguyen, My Anh Tran Nguyen, Yuxia Wang, Preslav Nakov, Sang Dinh*

**Main category:** cs.CL

**Keywords:** AI authorship detection, human-AI collaboration, multi-task learning, contrastive learning, text classification

**Relevance Score:** 9

**TL;DR:** This paper presents FAIDSet, a dataset for distinguishing between human, AI-generated, and human-AI collaborative texts, and introduces the FAID framework for fine-grained classification and increased interpretability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Increasing collaboration between humans and AI in generative tasks has made it essential to differentiate between various types of text authorship.

**Method:** The FAID framework uses multi-level contrastive learning and multi-task auxiliary classification to classify texts into human, AI, and collaborative categories while identifying the underlying AI model family.

**Key Contributions:**

	1. Introduction of the FAIDSet dataset for multi-domain and multi-generator text classification.
	2. Development of the FAID framework for fine-grained authorship detection and model identification.
	3. Utilization of multi-level contrastive learning to enhance interpretability and adaptability.

**Result:** FAID demonstrates superior performance in classifying unseen domains and new AI models compared to existing binary classifiers, thereby enhancing generalization accuracy.

**Limitations:** 

**Conclusion:** The findings suggest that FAID can significantly improve transparency and accountability in AI-assisted writing environments.

**Abstract:** The growing collaboration between humans and AI models in generative tasks has introduced new challenges in distinguishing between human-written, AI-generated, and human-AI collaborative texts. In this work, we collect a multilingual, multi-domain, multi-generator dataset FAIDSet. We further introduce a fine-grained detection framework FAID to classify text into these three categories, meanwhile identifying the underlying AI model family. Unlike existing binary classifiers, FAID is built to capture both authorship and model-specific characteristics. Our method combines multi-level contrastive learning with multi-task auxiliary classification to learn subtle stylistic cues. By modeling AI families as distinct stylistic entities, FAID offers improved interpretability. We incorporate an adaptation to address distributional shifts without retraining for unseen data. Experimental results demonstrate that FAID outperforms several baseline approaches, particularly enhancing the generalization accuracy on unseen domains and new AI models. It provide a potential solution for improving transparency and accountability in AI-assisted writing.

</details>


### [107] [Data-Efficient Hate Speech Detection via Cross-Lingual Nearest Neighbor Retrieval with Limited Labeled Data](https://arxiv.org/abs/2505.14272)

*Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser*

**Main category:** cs.CL

**Keywords:** hate speech detection, cross-lingual transfer learning, data augmentation

**Relevance Score:** 7

**TL;DR:** This paper presents a cross-lingual transfer learning approach for hate speech detection using nearest-neighbor retrieval to augment scarce labeled data in low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting hate speech is critical, yet collecting labeled data for low-resource languages is challenging and resource-intensive.

**Method:** The method utilizes nearest-neighbor retrieval to enhance minimal labeled training datasets in the target language by retrieving relevant examples from a multilingual hate speech detection pool.

**Key Contributions:**

	1. Efficient and scalable method for cross-lingual hate speech detection
	2. Demonstration of superior performance with minimal labeled data
	3. Application of maximum marginal relevance to reduce instance redundancy

**Result:** The approach was evaluated on eight languages and showed consistent improvements over models trained solely on target language data, often surpassing state-of-the-art methods.

**Limitations:** 

**Conclusion:** The approach demonstrates data efficiency and scalability, being adaptable to new languages and tasks, while also employing a technique to reduce redundancy in retrieved instances.

**Abstract:** Considering the importance of detecting hateful language, labeled hate speech data is expensive and time-consuming to collect, particularly for low-resource languages. Prior work has demonstrated the effectiveness of cross-lingual transfer learning and data augmentation in improving performance on tasks with limited labeled data. To develop an efficient and scalable cross-lingual transfer learning approach, we leverage nearest-neighbor retrieval to augment minimal labeled data in the target language, thereby enhancing detection performance. Specifically, we assume access to a small set of labeled training instances in the target language and use these to retrieve the most relevant labeled examples from a large multilingual hate speech detection pool. We evaluate our approach on eight languages and demonstrate that it consistently outperforms models trained solely on the target language data. Furthermore, in most cases, our method surpasses the current state-of-the-art. Notably, our approach is highly data-efficient, retrieving as small as 200 instances in some cases while maintaining superior performance. Moreover, it is scalable, as the retrieval pool can be easily expanded, and the method can be readily adapted to new languages and tasks. We also apply maximum marginal relevance to mitigate redundancy and filter out highly similar retrieved instances, resulting in improvements in some languages.

</details>


### [108] [YESciEval: Robust LLM-as-a-Judge for Scientific Question Answering](https://arxiv.org/abs/2505.14279)

*Jennifer D'Souza, Hamed Babaei Giglou, Quentin Münch*

**Main category:** cs.CL

**Keywords:** Large Language Models, scientific question-answering, evaluation robustness

**Relevance Score:** 8

**TL;DR:** Introduces YESciEval, a framework for robust evaluation of large language models in scientific Q&A, using reinforcement learning and fine-grained assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of evaluations for LLMs used in scientific question-answering.

**Method:** An open-source framework combining rubric-based assessment and reinforcement learning, alongside multidisciplinary science Q&A datasets, including adversarial examples.

**Key Contributions:**

	1. Introduction of the YESciEval framework
	2. Release of multidisciplinary science Q&A datasets
	3. Mitigation of optimism bias in LLM evaluations through reinforcement learning

**Result:** YESciEval provides scalable, cost-free evaluation scores from various LLMs, aiming to reduce optimism bias and enhance AI alignment in science.

**Limitations:** 

**Conclusion:** The development of reliable LLM-as-a-judge models through YESciEval is essential for robust evaluation in scientific inquiry and the pursuit of artificial general intelligence.

**Abstract:** Large Language Models (LLMs) drive scientific question-answering on modern search engines, yet their evaluation robustness remains underexplored. We introduce YESciEval, an open-source framework that combines fine-grained rubric-based assessment with reinforcement learning to mitigate optimism bias in LLM evaluators. We release multidisciplinary scienceQ&A datasets, including adversarial variants, with evaluation scores from multiple LLMs. Independent of proprietary models and human feedback, our approach enables scalable, cost-free evaluation. By advancing reliable LLM-as-a-judge models, this work supports AI alignment and fosters robust, transparent evaluation essential for scientific inquiry and artificial general intelligence.

</details>


### [109] [Universal Acoustic Adversarial Attacks for Flexible Control of Speech-LLMs](https://arxiv.org/abs/2505.14286)

*Rao Ma, Mengjie Qian, Vyas Raina, Mark Gales, Kate Knill*

**Main category:** cs.CL

**Keywords:** speech LLMs, adversarial attacks, universal audio segments, spoken language processing, robust training strategies

**Relevance Score:** 8

**TL;DR:** This paper investigates universal acoustic adversarial attacks on speech large language models (LLMs), demonstrating their vulnerabilities and suggesting the need for improved robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the vulnerabilities of speech LLMs to adversarial attacks due to their flexibility in handling spoken language tasks.

**Method:** The research investigates the impact of prepend universal adversarial audio segments to original input audio, exploring both complete output suppression and modified task performance based on input attributes.

**Key Contributions:**

	1. Investigation of universal adversarial attacks in speech LLMs
	2. Demonstration of selective attack methods based on input attributes
	3. Identification of vulnerabilities in specific speech models

**Result:** Findings reveal vulnerabilities in models like Qwen2-Audio and Granite-Speech to universal adversarial attacks, indicating a broader susceptibility in similar speech LLMs.

**Limitations:** 

**Conclusion:** The study emphasizes the need for enhanced training strategies to fortify speech LLMs against adversarial attacks.

**Abstract:** The combination of pre-trained speech encoders with large language models has enabled the development of speech LLMs that can handle a wide range of spoken language processing tasks. While these models are powerful and flexible, this very flexibility may make them more vulnerable to adversarial attacks. To examine the extent of this problem, in this work we investigate universal acoustic adversarial attacks on speech LLMs. Here a fixed, universal, adversarial audio segment is prepended to the original input audio. We initially investigate attacks that cause the model to either produce no output or to perform a modified task overriding the original prompt. We then extend the nature of the attack to be selective so that it activates only when specific input attributes, such as a speaker gender or spoken language, are present. Inputs without the targeted attribute should be unaffected, allowing fine-grained control over the model outputs. Our findings reveal critical vulnerabilities in Qwen2-Audio and Granite-Speech and suggest that similar speech LLMs may be susceptible to universal adversarial attacks. This highlights the need for more robust training strategies and improved resistance to adversarial attacks.

</details>


### [110] [Cross-Lingual Optimization for Language Transfer in Large Language Models](https://arxiv.org/abs/2505.14297)

*Jungseob Lee, Seongtae Hong, Hyeonseok Moon, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** Cross-Lingual Optimization, Large Language Models, Supervised Fine-Tuning, Language Adaptation, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Cross-Lingual Optimization (CLO), a method for transferring English-centric LLMs to target languages while maintaining English proficiency, outperforming standard supervised fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to adapt large language models (LLMs) to various languages, but existing approaches often overemphasize English capabilities, leading to poorer performance in data-constrained environments.

**Method:** CLO employs a translation model along with publicly available supervised fine-tuning (SFT) data in English to facilitate effective cross-lingual knowledge transfer, enabling models to perform better in target languages while preserving English performance.

**Key Contributions:**

	1. Introduction of Cross-Lingual Optimization (CLO) for language model adaptation
	2. Demonstrated that CLO outperforms SFT especially in low-resource settings
	3. Provided insights into the sensitivity of supervised fine-tuning to data quantity

**Result:** CLO consistently outperforms SFT across multiple languages, showing that it can achieve high proficiency in low-resource languages with significantly fewer samples than SFT.

**Limitations:** The analysis focuses on the performance in specific languages; broader applicability needs to be validated in additional languages and contexts.

**Conclusion:** The findings highlight the advantages of CLO over SFT in terms of data efficiency and robustness, suggesting that CLO is a more effective strategy for adapting LLMs to diverse language scenarios.

**Abstract:** Adapting large language models to other languages typically employs supervised fine-tuning (SFT) as a standard approach. However, it often suffers from an overemphasis on English performance, a phenomenon that is especially pronounced in data-constrained environments. To overcome these challenges, we propose \textbf{Cross-Lingual Optimization (CLO)} that efficiently transfers an English-centric LLM to a target language while preserving its English capabilities. CLO utilizes publicly available English SFT data and a translation model to enable cross-lingual transfer. We conduct experiments using five models on six languages, each possessing varying levels of resource. Our results show that CLO consistently outperforms SFT in both acquiring target language proficiency and maintaining English performance. Remarkably, in low-resource languages, CLO with only 3,200 samples surpasses SFT with 6,400 samples, demonstrating that CLO can achieve better performance with less data. Furthermore, we find that SFT is particularly sensitive to data quantity in medium and low-resource languages, whereas CLO remains robust. Our comprehensive analysis emphasizes the limitations of SFT and incorporates additional training strategies in CLO to enhance efficiency.

</details>


### [111] [JOLT-SQL: Joint Loss Tuning of Text-to-SQL with Confusion-aware Noisy Schema Sampling](https://arxiv.org/abs/2505.14305)

*Jinwang Song, Hongying Zan, Kunli Zhang, Lingling Mu, Yingjie Han, Haobo Hua, Min Peng*

**Main category:** cs.CL

**Keywords:** text-to-sql, large language models, schema linking, SQL generation, noisy schema

**Relevance Score:** 8

**TL;DR:** JOLT-SQL is a single-stage supervised fine-tuning framework that optimizes schema linking and SQL generation, addressing robustness issues in Text-to-SQL tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve upon existing SFT approaches in Text-to-SQL by overcoming complexities related to multi-stage pipelines and enhancing robustness against noisy schema information.

**Method:** JOLT-SQL employs a streamlined single-stage framework that combines discriminative schema linking with local bidirectional attention and a confusion-aware noisy schema sampling strategy.

**Key Contributions:**

	1. Streamlined single-stage SFT framework for Text-to-SQL
	2. Joint optimization of schema linking and SQL generation
	3. Enhanced robustness through confusion-aware schema sampling

**Result:** JOLT-SQL achieves state-of-the-art execution accuracy on the Spider and BIRD benchmarks while enhancing training and inference efficiency compared to comparable open-source models.

**Limitations:** 

**Conclusion:** JOLT-SQL presents an efficient alternative for Text-to-SQL tasks, with significant improvements in execution accuracy and robustness under challenging conditions.

**Abstract:** Text-to-SQL, which maps natural language to SQL queries, has benefited greatly from recent advances in Large Language Models (LLMs). While LLMs offer various paradigms for this task, including prompting and supervised fine-tuning (SFT), SFT approaches still face challenges such as complex multi-stage pipelines and poor robustness to noisy schema information. To address these limitations, we present JOLT-SQL, a streamlined single-stage SFT framework that jointly optimizes schema linking and SQL generation via a unified loss. JOLT-SQL employs discriminative schema linking, enhanced by local bidirectional attention, alongside a confusion-aware noisy schema sampling strategy with selective attention to improve robustness under noisy schema conditions. Experiments on the Spider and BIRD benchmarks demonstrate that JOLT-SQL achieves state-of-the-art execution accuracy among comparable-size open-source models, while significantly improving both training and inference efficiency.

</details>


### [112] [Studying the Role of Input-Neighbor Overlap in Retrieval-Augmented Language Models Training Efficiency](https://arxiv.org/abs/2505.14309)

*Ehsan Doostmohammadi, Marco Kuhlmann*

**Main category:** cs.CL

**Keywords:** retrieval-augmented models, query-context overlap, data efficiency

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of query-context overlap on retrieval-augmented language model performance and demonstrates that increasing overlap can enhance data efficiency and reduce training time.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the degree of overlap between query and retrieved context impacts the performance of retrieval-augmented language models during training and inference.

**Method:** The authors conducted systematic experiments to assess varying levels of query-context overlap and its effects on model learning and test-time perplexity.

**Key Contributions:**

	1. Systematic investigation of query-context overlap effects
	2. Demonstration of enhanced data efficiency through synthetic context
	3. Empirical validation of results on question-answering tasks

**Result:** Increased overlap improves test-time perplexity and accelerates model learning above a critical threshold. Synthetic context generated through query paraphrasing enhances data efficiency and reduces training time by around 40%.

**Limitations:** 

**Conclusion:** The study provides empirical evidence that optimizing retrieval mechanisms in language model pretraining can yield significant performance benefits without compromising quality.

**Abstract:** Retrieval-augmented language models have demonstrated performance comparable to much larger models while requiring fewer computational resources. The effectiveness of these models crucially depends on the overlap between query and retrieved context, but the optimal degree of this overlap remains unexplored. In this paper, we systematically investigate how varying levels of query--context overlap affect model performance during both training and inference. Our experiments reveal that increased overlap initially has minimal effect, but substantially improves test-time perplexity and accelerates model learning above a critical threshold. Building on these findings, we demonstrate that deliberately increasing overlap through synthetic context can enhance data efficiency and reduce training time by approximately 40\% without compromising performance. We specifically generate synthetic context through paraphrasing queries. We validate our perplexity-based findings on question-answering tasks, confirming that the benefits of retrieval-augmented language modeling extend to practical applications. Our results provide empirical evidence of significant optimization potential for retrieval mechanisms in language model pretraining.

</details>


### [113] [HausaNLP: Current Status, Challenges and Future Directions for Hausa Natural Language Processing](https://arxiv.org/abs/2505.14311)

*Shamsuddeen Hassan Muhammad, Ibrahim Said Ahmad, Idris Abdulmumin, Falalu Ibrahim Lawan, Babangida Sani, Sukairaj Hafiz Imam, Yusuf Aliyu, Sani Abdullahi Sani, Ali Usman Umar, Kenneth Church, Vukosi Marivate*

**Main category:** cs.CL

**Keywords:** Hausa, Natural Language Processing, low-resource language, datasets, language models

**Relevance Score:** 4

**TL;DR:** Overview of the current state of Hausa NLP, challenges, resources, and proposed research directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite having a substantial number of speakers, Hausa remains a low-resource language in NLP, requiring systematic examination of current state and research gaps.

**Method:** The paper systematically reviews existing resources and contributions in Hausa NLP across fundamental tasks and introduces a curated catalog of datasets and tools.

**Key Contributions:**

	1. Overview of Hausa NLP resources and gaps
	2. Introduction of HausaNLP catalog
	3. Strategic research directions for Hausa NLP advancement

**Result:** Introduced HausaNLP, a catalog that aggregates datasets, tools, and research works, while identifying integration challenges with LLMs.

**Limitations:** Limited scope might not cover all regional dialects and NLP tasks thoroughly.

**Conclusion:** The paper suggests strategic directions for enhancing Hausa NLP through dataset expansion, improved modeling approaches, and community collaboration.

**Abstract:** Hausa Natural Language Processing (NLP) has gained increasing attention in recent years, yet remains understudied as a low-resource language despite having over 120 million first-language (L1) and 80 million second-language (L2) speakers worldwide. While significant advances have been made in high-resource languages, Hausa NLP faces persistent challenges, including limited open-source datasets and inadequate model representation. This paper presents an overview of the current state of Hausa NLP, systematically examining existing resources, research contributions, and gaps across fundamental NLP tasks: text classification, machine translation, named entity recognition, speech recognition, and question answering. We introduce HausaNLP (https://catalog.hausanlp.org), a curated catalog that aggregates datasets, tools, and research works to enhance accessibility and drive further development. Furthermore, we discuss challenges in integrating Hausa into large language models (LLMs), addressing issues of suboptimal tokenization and dialectal variation. Finally, we propose strategic research directions emphasizing dataset expansion, improved language modeling approaches, and strengthened community collaboration to advance Hausa NLP. Our work provides both a foundation for accelerating Hausa NLP progress and valuable insights for broader multilingual NLP research.

</details>


### [114] [A MIND for Reasoning: Meta-learning for In-context Deduction](https://arxiv.org/abs/2505.14313)

*Leonardo Bertolazzi, Manuel Vargas Guzmán, Raffaella Bernardi, Maciej Malicki, Jakub Szymanik*

**Main category:** cs.CL

**Keywords:** large language models, meta-learning, deductive reasoning, knowledge base, generalization

**Relevance Score:** 8

**TL;DR:** This paper introduces MIND, a meta-learning approach aimed at improving the generalization of LLMs in deductive reasoning tasks by enabling them to effectively leverage knowledge bases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements, LLMs struggle with generalizing to out-of-distribution problems, particularly in deductive reasoning tasks.

**Method:** The authors propose MIND, a few-shot meta-learning fine-tuning approach designed to help models systematically apply inference rules to derive hypotheses from knowledge bases.

**Key Contributions:**

	1. Introduction of MIND for few-shot meta-learning in LLMs
	2. Demonstrated improvements in deductive reasoning capabilities
	3. Enhanced performance of smaller LMs in low-data scenarios

**Result:** MIND significantly enhances generalization capabilities in small language models (1.5B to 7B parameters), outperforming state-of-the-art LLMs in low-data environments.

**Limitations:** 

**Conclusion:** The findings suggest that even smaller LMs, when fine-tuned using MIND, can achieve better performance in deductive reasoning tasks than larger state-of-the-art models.

**Abstract:** Large language models (LLMs) are increasingly evaluated on formal tasks, where strong reasoning abilities define the state of the art. However, their ability to generalize to out-of-distribution problems remains limited. In this paper, we investigate how LLMs can achieve a systematic understanding of deductive rules. Our focus is on the task of identifying the appropriate subset of premises within a knowledge base needed to derive a given hypothesis. To tackle this challenge, we propose Meta-learning for In-context Deduction (MIND), a novel few-shot meta-learning fine-tuning approach. The goal of MIND is to enable models to generalize more effectively to unseen knowledge bases and to systematically apply inference rules. Our results show that MIND significantly improves generalization in small LMs ranging from 1.5B to 7B parameters. The benefits are especially pronounced in smaller models and low-data settings. Remarkably, small models fine-tuned with MIND outperform state-of-the-art LLMs, such as GPT-4o and o3-mini, on this task.

</details>


### [115] [QA-prompting: Improving Summarization with Large Language Models using Question-Answering](https://arxiv.org/abs/2505.14347)

*Neelabh Sinha*

**Main category:** cs.CL

**Keywords:** Language Models, summarization, question-answering, natural language processing, QA-prompting

**Relevance Score:** 9

**TL;DR:** The paper presents QA-prompting, a novel method for long-context summarization using question-answering as a precursor to summary generation, which improves performance without complex techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of long-context summarization due to positional biases in language models.

**Method:** QA-prompting utilizes question-answering as an intermediate step before generating summaries, allowing for a more effective extraction of key information without requiring fine-tuning or pipelining.

**Key Contributions:**

	1. Introduction of QA-prompting for summarization
	2. Demonstrated performance improvement over existing methods
	3. Emphasis on domain-specific question selection
	4. Simplified approach that avoids fine-tuning and pipelining

**Result:** Experimental results show that QA-prompting significantly outperforms baseline and state-of-the-art methods, achieving up to 29% improvement in ROUGE scores across various datasets.

**Limitations:** 

**Conclusion:** QA-prompting provides an effective and scalable solution for summarization tasks, emphasizing the need for domain-specific question selection.

**Abstract:** Language Models (LMs) have revolutionized natural language processing, enabling high-quality text generation through prompting and in-context learning. However, models often struggle with long-context summarization due to positional biases, leading to suboptimal extraction of critical information. There are techniques to improve this with fine-tuning, pipelining, or using complex techniques, which have their own challenges. To solve these challenges, we propose QA-prompting - a simple prompting method for summarization that utilizes question-answering as an intermediate step prior to summary generation. Our method extracts key information and enriches the context of text to mitigate positional biases and improve summarization in a single LM call per task without requiring fine-tuning or pipelining. Experiments on multiple datasets belonging to different domains using ten state-of-the-art pre-trained models demonstrate that QA-prompting outperforms baseline and other state-of-the-art methods, achieving up to 29% improvement in ROUGE scores. This provides an effective and scalable solution for summarization and highlights the importance of domain-specific question selection for optimal performance.

</details>


### [116] [OSoRA: Output-Dimension and Singular-Value Initialized Low-Rank Adaptation](https://arxiv.org/abs/2505.14350)

*Jialong Han, Si Zhang, Ke Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Parameter-Efficient Fine-Tuning, Low-Rank Adaptation

**Relevance Score:** 8

**TL;DR:** OSoRA is a novel Parameter-Efficient Fine-Tuning method for Large Language Models that reduces computational costs while maintaining performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The rising challenges of fine-tuning Large Language Models due to their scale and computational costs prompted the need for more efficient methodologies.

**Method:** OSoRA integrates Singular Value Decomposition with learnable scaling vectors to optimize a low-rank adaptation framework for fine-tuning LLMs, minimizing trainable parameters.

**Key Contributions:**

	1. Introduces OSoRA, a novel PEFT method integrating SVD for LLMs.
	2. Significantly reduces computational costs by minimizing trainable parameters.
	3. Demonstrates superior performance on multiple reasoning benchmarks compared to existing methods.

**Result:** OSoRA demonstrates comparable or superior performance to state-of-the-art methods like LoRA and VeRA across various benchmarks with reduced computational requirements.

**Limitations:** 

**Conclusion:** Jointly training both the singular values and the output-dimension vector is critical for achieving optimal fine-tuning performance with the OSoRA method.

**Abstract:** Fine-tuning Large Language Models (LLMs) has become increasingly challenging due to their massive scale and associated computational costs. Parameter-Efficient Fine-Tuning (PEFT) methodologies have been proposed as computational alternatives; however, their implementations still require significant resources. In this paper, we present OSoRA (Output-Dimension and Singular-Value Initialized Low-Rank Adaptation), a novel PEFT method for LLMs. OSoRA extends Low-Rank Adaptation (LoRA) by integrating Singular Value Decomposition (SVD) with learnable scaling vectors in a unified framework. It first performs an SVD of pre-trained weight matrices, then optimizes an output-dimension vector during training, while keeping the corresponding singular vector matrices frozen. OSoRA substantially reduces computational resource requirements by minimizing the number of trainable parameters during fine-tuning. Comprehensive evaluations across mathematical reasoning, common sense reasoning, and other benchmarks demonstrate that OSoRA achieves comparable or superior performance to state-of-the-art methods like LoRA and VeRA, while maintaining a linear parameter scaling even as the rank increases to higher dimensions. Our ablation studies further confirm that jointly training both the singular values and the output-dimension vector is critical for optimal performance.

</details>


### [117] [WirelessMathBench: A Mathematical Modeling Benchmark for LLMs in Wireless Communications](https://arxiv.org/abs/2505.14354)

*Xin Li, Mengbing Liu, Li Wei, Jiancheng An, Mérouane Debbah, Chau Yuen*

**Main category:** cs.CL

**Keywords:** Wireless Communications, Large Language Models, Benchmarking, Mathematical Reasoning, Engineering Applications

**Relevance Score:** 6

**TL;DR:** WirelessMathBench is a benchmark for evaluating LLMs on mathematical modeling challenges specific to wireless communications, revealing limitations in their domain-specific reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the mathematical reasoning abilities of LLMs in the context of wireless communications, an area which remains underexplored despite the advanced capabilities of LLMs in general tasks.

**Method:** Introduced WirelessMathBench, a benchmark consisting of 587 questions sourced from 40 research papers, including various tasks on mathematical modeling for wireless communications.

**Key Contributions:**

	1. Creation of WirelessMathBench to evaluate LLMs in wireless communications
	2. Comprehensive dataset of 587 curated mathematical questions
	3. Insights into the limitations of current LLMs in complex mathematical reasoning

**Result:** Leading LLMs perform well on basic recall but struggle significantly with complex tasks, achieving an average accuracy of only 38.05% on the benchmark, with less than 8% success in full equation completion.

**Limitations:** Current models excel in simpler tasks but fail in more complex mathematical reasoning, indicating a need for improvement.

**Conclusion:** The benchmark seeks to motivate the development of stronger, domain-aware LLMs for wireless communications and engineering tasks and is publicly available for further research.

**Abstract:** Large Language Models (LLMs) have achieved impressive results across a broad array of tasks, yet their capacity for complex, domain-specific mathematical reasoning-particularly in wireless communications-remains underexplored. In this work, we introduce WirelessMathBench, a novel benchmark specifically designed to evaluate LLMs on mathematical modeling challenges to wireless communications engineering. Our benchmark consists of 587 meticulously curated questions sourced from 40 state-of-the-art research papers, encompassing a diverse spectrum of tasks ranging from basic multiple-choice questions to complex equation completion tasks, including both partial and full completions, all of which rigorously adhere to physical and dimensional constraints. Through extensive experimentation with leading LLMs, we observe that while many models excel in basic recall tasks, their performance degrades significantly when reconstructing partially or fully obscured equations, exposing fundamental limitations in current LLMs. Even DeepSeek-R1, the best performer on our benchmark, achieves an average accuracy of only 38.05%, with a mere 7.83% success rate in full equation completion. By publicly releasing WirelessMathBench along with the evaluation toolkit, we aim to advance the development of more robust, domain-aware LLMs for wireless system analysis and broader engineering applications.

</details>


### [118] [Dual Decomposition of Weights and Singular Value Low Rank Adaptation](https://arxiv.org/abs/2505.14367)

*Jialong Han, Si Zhang, Ke Zhang*

**Main category:** cs.CL

**Keywords:** Parameter-Efficient Fine-Tuning, Low-rank Adaptation, Large Language Models, Singular Value Decomposition, Knowledge Transfer

**Relevance Score:** 9

**TL;DR:** DuDe is a novel Parameter-Efficient Fine-Tuning approach that improves the training stability and knowledge transfer of Large Language Models (LLMs) using Singular Value Decomposition for initialization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address limitations of LoRA-based approaches in training dynamics and knowledge transfer for adapting LLMs to downstream tasks.

**Method:** DuDe decomposes weight matrices into magnitude and direction components and uses Singular Value Decomposition (SVD) for principled initialization.

**Key Contributions:**

	1. Introduction of DuDe for improved parameter-efficient fine-tuning of LLMs.
	2. Use of SVD for better initialization of adapter parameters.
	3. Enhanced performance on domain-specific tasks requiring specialized knowledge.

**Result:** DuDe achieves improved accuracy, with up to 48.35% on MMLU and 62.53% on GSM8K.

**Limitations:** 

**Conclusion:** DuDe enhances optimization stability and preserves pre-trained representations, marking a significant improvement in PEFT for LLMs.

**Abstract:** Parameter-Efficient Fine-Tuning (PEFT) has emerged as a critical paradigm for adapting Large Language Models (LLMs) to downstream tasks, among which Low-rank Adaptation (LoRA) represents one of the most widely adopted methodologies. However, existing LoRA-based approaches exhibit two fundamental limitations: unstable training dynamics and inefficient knowledge transfer from pre-trained models, both stemming from random initialization of adapter parameters. To overcome these challenges, we propose DuDe, a novel approach that decomposes weight matrices into magnitude and direction components, employing Singular Value Decomposition (SVD) for principled initialization. Our comprehensive evaluation demonstrates DuDe's superior performance and robustness, achieving up to 48.35\% accuracy on MMLU and 62.53\% ($\pm$ 1.59) accuracy on GSM8K. Our theoretical analysis and empirical validation collectively demonstrate that DuDe's decomposition strategy enhances optimization stability and better preserves pre-trained representations, particularly for domain-specific tasks requiring specialized knowledge. The combination of robust empirical performance and rigorous theoretical foundations establishes DuDe as a significant contribution to PEFT methodologies for LLMs.

</details>


### [119] [AutoRev: Automatic Peer Review System for Academic Research Papers](https://arxiv.org/abs/2505.14376)

*Maitreya Prafulla Chitale, Ketaki Mangesh Shetye, Harshit Gupta, Manav Chaudhary, Vasudeva Varma*

**Main category:** cs.CL

**Keywords:** Peer Review, Graph-based Methods, NLP, Large Language Models, Review Generation

**Relevance Score:** 9

**TL;DR:** AutoRev is an Automatic Peer Review System that utilizes a graph-based approach to enhance review generation for academic research papers, outperforming SOTA baselines significantly.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating coherent reviews for academic papers while overcoming the limitations of recent LLMs that focus on fine-tuning without considering long input token lengths.

**Method:** The paper introduces a framework that represents academic documents as graphs, enabling the identification and extraction of critical passages for review generation.

**Key Contributions:**

	1. Introduction of AutoRev, a novel automatic peer review system.
	2. Development of a graph-based method for representing and extracting key information from academic documents.
	3. Demonstration of superior performance in review generation compared to existing methods.

**Result:** The graph-based approach demonstrates significant effectiveness in review generation, outperforming state-of-the-art methods by an average of 58.72% across evaluation metrics.

**Limitations:** 

**Conclusion:** The research encourages further exploration of graph-based extraction techniques in NLP for various downstream applications.

**Abstract:** Generating a review for an academic research paper is a complex task that requires a deep understanding of the document's content and the interdependencies between its sections. It demands not only insight into technical details but also an appreciation of the paper's overall coherence and structure. Recent methods have predominantly focused on fine-tuning large language models (LLMs) to address this challenge. However, they often overlook the computational and performance limitations imposed by long input token lengths. To address this, we introduce AutoRev, an Automatic Peer Review System for Academic Research Papers. Our novel framework represents an academic document as a graph, enabling the extraction of the most critical passages that contribute significantly to the review. This graph-based approach demonstrates effectiveness for review generation and is potentially adaptable to various downstream tasks, such as question answering, summarization, and document representation. When applied to review generation, our method outperforms SOTA baselines by an average of 58.72% across all evaluation metrics. We hope that our work will stimulate further research in applying graph-based extraction techniques to other downstream tasks in NLP. We plan to make our code public upon acceptance.

</details>


### [120] [Editing Across Languages: A Survey of Multilingual Knowledge Editing](https://arxiv.org/abs/2505.14393)

*Nadir Durrani, Basel Mousi, Fahim Dalvi*

**Main category:** cs.CL

**Keywords:** Multilingual Knowledge Editing, Knowledge Editing, Language Models

**Relevance Score:** 7

**TL;DR:** This survey discusses the burgeoning field of Multilingual Knowledge Editing (MKE), systematizing methods and challenges in ensuring factual edits across languages in language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in multilingual contexts of Knowledge Editing, which has been primarily focused on monolingual settings.

**Method:** The paper presents a comprehensive taxonomy of MKE methods, including parameter-based, memory-based, fine-tuning, and hypernetwork approaches, while surveying available benchmarks and key findings on method effectiveness.

**Key Contributions:**

	1. Comprehensive taxonomy of Multilingual Knowledge Editing methods
	2. Survey of benchmarks and key findings in MKE
	3. Identification of challenges and open problems in multilingual context

**Result:** Identified challenges in cross-lingual knowledge propagation, including issues of language anisotropy and limitations in evaluation coverage and edit scalability.

**Limitations:** The paper highlights ongoing challenges such as cross-lingual propagation issues and edit scalability, indicating that the field is still evolving.

**Conclusion:** The analysis supports advancing editable language-aware LLMs and lays the groundwork for future research in MKE.

**Abstract:** While Knowledge Editing has been extensively studied in monolingual settings, it remains underexplored in multilingual contexts. This survey systematizes recent research on Multilingual Knowledge Editing (MKE), a growing subdomain of model editing focused on ensuring factual edits generalize reliably across languages. We present a comprehensive taxonomy of MKE methods, covering parameter-based, memory-based, fine-tuning, and hypernetwork approaches. We survey available benchmarks,summarize key findings on method effectiveness and transfer patterns, identify challenges in cross-lingual propagation, and highlight open problems related to language anisotropy, evaluation coverage, and edit scalability. Our analysis consolidates a rapidly evolving area and lays the groundwork for future progress in editable language-aware LLMs.

</details>


### [121] [MUG-Eval: A Proxy Evaluation Framework for Multilingual Generation Capabilities in Any Language](https://arxiv.org/abs/2505.14395)

*Seyoung Song, Seogyeong Jeong, Eunsu Kim, Jiho Jin, Dongkwan Kim, Jay Shin, Alice Oh*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilingual Generation, Evaluation Framework

**Relevance Score:** 9

**TL;DR:** MUG-Eval is a framework to evaluate the multilingual text generation capabilities of large language models through conversational tasks, focusing on low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating LLMs' capabilities, particularly in low-resource languages where traditional assessment methods are limited.

**Method:** MUG-Eval transforms existing benchmarks into conversational tasks and measures LLMs' success rates in these tasks as a proxy for evaluating generation accuracy.

**Key Contributions:**

	1. Novel framework for multilingual evaluation of LLMs
	2. Transforms benchmarks into conversational tasks
	3. Shows strong correlation with established evaluation metrics

**Result:** MUG-Eval was applied to 8 LLMs across 30 languages and demonstrated strong correlation with established benchmarks (r > 0.75), allowing standardized comparisons.

**Limitations:** 

**Conclusion:** MUG-Eval provides a robust and resource-efficient means of evaluating multilingual generation, applicable to thousands of languages without reliance on specific NLP tools.

**Abstract:** Evaluating text generation capabilities of large language models (LLMs) is challenging, particularly for low-resource languages where methods for direct assessment are scarce. We propose MUG-Eval, a novel framework that evaluates LLMs' multilingual generation capabilities by transforming existing benchmarks into conversational tasks and measuring the LLMs' accuracies on those tasks. We specifically designed these conversational tasks to require effective communication in the target language. Then, we simply use task success rate as a proxy of successful conversation generation. Our approach offers two key advantages: it is independent of language-specific NLP tools or annotated datasets, which are limited for most languages, and it does not rely on LLMs-as-judges, whose evaluation quality degrades outside a few high-resource languages. We evaluate 8 LLMs across 30 languages spanning high, mid, and low-resource categories, and we find that MUG-Eval correlates strongly with established benchmarks ($r$ > 0.75) while enabling standardized comparisons across languages and models. Our framework provides a robust and resource-efficient solution for evaluating multilingual generation that can be extended to thousands of languages.

</details>


### [122] [Log-Augmented Generation: Scaling Test-Time Reasoning with Reusable Computation](https://arxiv.org/abs/2505.14398)

*Peter Baile Chen, Yi Zhang, Dan Roth, Samuel Madden, Jacob Andreas, Michael Cafarella*

**Main category:** cs.CL

**Keywords:** log-augmented generation, language models, machine learning, human-computer interaction, reasoning

**Relevance Score:** 9

**TL;DR:** This paper introduces log-augmented generation (LAG), a framework that enhances large language models by reusing prior task logs at test time to improve reasoning and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle to retain and apply reasoning from previous tasks, limiting their performance on new challenges.

**Method:** The LAG framework utilizes key-value caches to represent task logs and reuses prior reasoning and computations when new tasks arise, without requiring additional knowledge extraction steps.

**Key Contributions:**

	1. Introduction of the log-augmented generation (LAG) framework
	2. Demonstrated performance improvements over standard systems
	3. Direct reuse of prior reasoning without additional knowledge extraction steps

**Result:** Experiments show that LAG significantly outperforms standard agentic systems and existing memory-based solutions on knowledge- and reasoning-intensive datasets.

**Limitations:** 

**Conclusion:** LAG offers a scalable and efficient method to enhance reasoning capabilities in large language models by leveraging past experiences directly at test time.

**Abstract:** While humans naturally learn and adapt from past experiences, large language models (LLMs) and their agentic counterparts struggle to retain reasoning from previous tasks and apply them in future contexts. To address this limitation, we propose a novel framework, log-augmented generation (LAG) that directly reuses prior computation and reasoning from past logs at test time to enhance model's ability to learn from previous tasks and perform better on new, unseen challenges, all while keeping the system efficient and scalable. Specifically, our system represents task logs using key-value (KV) caches, encoding the full reasoning context of prior tasks while storing KV caches for only a selected subset of tokens. When a new task arises, LAG retrieves the KV values from relevant logs to augment generation. Our approach differs from reflection-based memory mechanisms by directly reusing prior reasoning and computations without requiring additional steps for knowledge extraction or distillation. Our method also goes beyond existing KV caching techniques, which primarily target efficiency gains rather than improving accuracy. Experiments on knowledge- and reasoning-intensive datasets demonstrate that our method significantly outperforms standard agentic systems that do not utilize logs, as well as existing solutions based on reflection and KV cache techniques.

</details>


### [123] [Pierce the Mists, Greet the Sky: Decipher Knowledge Overshadowing via Knowledge Circuit Analysis](https://arxiv.org/abs/2505.14406)

*Haoming Huang, Yibo Yan, Jiahao Huo, Xin Zou, Xinfeng Li, Kun Wang, Xuming Hu*

**Main category:** cs.CL

**Keywords:** knowledge overshadowing, Large Language Models, attention mechanisms, model training, hallucination

**Relevance Score:** 8

**TL;DR:** PhantomCircuit is a framework for analyzing and detecting knowledge overshadowing in Large Language Models (LLMs), which leads to erroneous outputs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs face challenges from hallucinations, particularly knowledge overshadowing, where one activated knowledge masks another relevant piece, affecting output quality.

**Method:** The paper introduces PhantomCircuit, which utilizes knowledge circuit analysis to dissect the functioning of attention heads and trace competing knowledge pathways during the training process.

**Key Contributions:**

	1. Introduction of the PhantomCircuit framework for analyzing knowledge overshadowing
	2. Insight into the internal mechanisms of attention heads in LLMs
	3. Methodological lens for mitigating hallucination in LLMs

**Result:** PhantomCircuit effectively identifies instances of knowledge overshadowing, providing new insights into its occurrence and evolution.

**Limitations:** 

**Conclusion:** This framework offers a new methodological approach for the research community to understand and potentially mitigate knowledge overshadowing in LLMs.

**Abstract:** Large Language Models (LLMs), despite their remarkable capabilities, are hampered by hallucinations. A particularly challenging variant, knowledge overshadowing, occurs when one piece of activated knowledge inadvertently masks another relevant piece, leading to erroneous outputs even with high-quality training data. Current understanding of overshadowing is largely confined to inference-time observations, lacking deep insights into its origins and internal mechanisms during model training. Therefore, we introduce PhantomCircuit, a novel framework designed to comprehensively analyze and detect knowledge overshadowing. By innovatively employing knowledge circuit analysis, PhantomCircuit dissects the internal workings of attention heads, tracing how competing knowledge pathways contribute to the overshadowing phenomenon and its evolution throughout the training process. Extensive experiments demonstrate PhantomCircuit's effectiveness in identifying such instances, offering novel insights into this elusive hallucination and providing the research community with a new methodological lens for its potential mitigation.

</details>


### [124] [Hidden Ghost Hand: Unveiling Backdoor Vulnerabilities in MLLM-Powered Mobile GUI Agents](https://arxiv.org/abs/2505.14418)

*Pengzhou Cheng, Haowen Hu, Zheng Wu, Zongru Wu, Tianjie Ju, Daizong Ding, Zhuosheng Zhang, Gongshen Liu*

**Main category:** cs.CL

**Keywords:** backdoor attacks, multimodal large language models, graphical user interface agents, AgentGhost, security vulnerabilities

**Relevance Score:** 8

**TL;DR:** This paper presents AgentGhost, a framework for conducting stealthy backdoor attacks on multimodal large language model (MLLM)-powered graphical user interface (GUI) agents.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The investigation addresses the security vulnerabilities associated with MLLM-powered GUI agents, particularly focusing on the supply chain threat posed by backdoor attacks due to reliance on open-source agents or APIs.

**Method:** The paper introduces AgentGhost, which constructs composite triggers and formulates backdoor injection as a Min-Max optimization problem. It utilizes supervised contrastive learning for maximizing feature differences and supervised fine-tuning for minimizing behavior discrepancies.

**Key Contributions:**

	1. Introduction of AgentGhost framework for red-teaming backdoor attacks
	2. Formulation of backdoor injection as Min-Max optimization across various agent models
	3. Development of a defense method against the proposed attacks

**Result:** AgentGhost achieves an attack accuracy of 99.7% on three attack objectives with only 1% degradation in utility, showcasing its effectiveness and stealthiness across various agent models.

**Limitations:** The research focuses primarily on specific mobile benchmarks and may not generalize to all types of GUI agents or interaction scenarios.

**Conclusion:** The study highlights significant vulnerabilities in MLLM-powered GUI agents and proposes a defense strategy that can reduce attack accuracy substantially to 22.1%.

**Abstract:** Graphical user interface (GUI) agents powered by multimodal large language models (MLLMs) have shown greater promise for human-interaction. However, due to the high fine-tuning cost, users often rely on open-source GUI agents or APIs offered by AI providers, which introduces a critical but underexplored supply chain threat: backdoor attacks. In this work, we first unveil that MLLM-powered GUI agents naturally expose multiple interaction-level triggers, such as historical steps, environment states, and task progress. Based on this observation, we introduce AgentGhost, an effective and stealthy framework for red-teaming backdoor attacks. Specifically, we first construct composite triggers by combining goal and interaction levels, allowing GUI agents to unintentionally activate backdoors while ensuring task utility. Then, we formulate backdoor injection as a Min-Max optimization problem that uses supervised contrastive learning to maximize the feature difference across sample classes at the representation space, improving flexibility of the backdoor. Meanwhile, it adopts supervised fine-tuning to minimize the discrepancy between backdoor and clean behavior generation, enhancing effectiveness and utility. Extensive evaluations of various agent models in two established mobile benchmarks show that AgentGhost is effective and generic, with attack accuracy that reaches 99.7\% on three attack objectives, and shows stealthiness with only 1\% utility degradation. Furthermore, we tailor a defense method against AgentGhost that reduces the attack accuracy to 22.1\%. Our code is available at \texttt{anonymous}.

</details>


### [125] [SAE-FiRE: Enhancing Earnings Surprise Predictions Through Sparse Autoencoder Feature Selection](https://arxiv.org/abs/2505.14420)

*Huopu Zhang, Yanguang Liu, Mengnan Du*

**Main category:** cs.CL

**Keywords:** earnings prediction, conference calls, sparse autoencoders, financial analysis, machine learning

**Relevance Score:** 4

**TL;DR:** This paper presents a novel framework, SAE-FiRE, for analyzing earnings conference call transcripts to predict earnings surprises by extracting key financial information and minimizing redundancy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance predictive accuracy in financial outcomes by addressing challenges posed by the complexity and redundancy in earnings call transcripts.

**Method:** The framework employs Sparse Autoencoders to identify key patterns in financial transcripts while filtering out noise and redundancy.

**Key Contributions:**

	1. Introduction of SAE-FiRE for financial analysis
	2. Demonstration of effective noise reduction in transcripts
	3. Performance improvements over existing predictive models

**Result:** SAE-FiRE significantly outperforms existing baseline methods in predicting earnings surprises.

**Limitations:** 

**Conclusion:** The use of Sparse Autoencoders greatly improves distillation of valuable financial signals from complex earnings call data, facilitating better predictions of earnings surprises.

**Abstract:** Predicting earnings surprises through the analysis of earnings conference call transcripts has attracted increasing attention from the financial research community. Conference calls serve as critical communication channels between company executives, analysts, and shareholders, offering valuable forward-looking information. However, these transcripts present significant analytical challenges, typically containing over 5,000 words with substantial redundancy and industry-specific terminology that creates obstacles for language models. In this work, we propose the Sparse Autoencoder for Financial Representation Enhancement (SAE-FiRE) framework to address these limitations by extracting key information while eliminating redundancy. SAE-FiRE employs Sparse Autoencoders (SAEs) to efficiently identify patterns and filter out noises, and focusing specifically on capturing nuanced financial signals that have predictive power for earnings surprises. Experimental results indicate that the proposed method can significantly outperform comparing baselines.

</details>


### [126] [Scaling Low-Resource MT via Synthetic Data Generation with LLMs](https://arxiv.org/abs/2505.14423)

*Ona de Gibert, Joseph Attieh, Teemu Vahtola, Mikko Aulamo, Zihao Li, Raúl Vázquez, Tiancheng Hu, Jörg Tiedemann*

**Main category:** cs.CL

**Keywords:** Machine Translation, Synthetic Data, LLM, Low-resource Languages, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper explores how LLM-generated synthetic data enhances low-resource machine translation across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve low-resource machine translation (MT) by leveraging LLM-generated synthetic data, which can enhance performance in translating diverse target languages.

**Method:** Constructed a document-level synthetic corpus from English Europarl and expanded it via pivoting to create 147 additional language pair datasets; evaluated the data using automatic and human assessments.

**Key Contributions:**

	1. Introduction of SynOPUS, a public repository for synthetic parallel datasets.
	2. Demonstration of significant improvements in MT performance using synthetic data for low-resource languages.
	3. Comparison of synthetic data with established datasets like HPLT to evaluate effectiveness.

**Result:** Both automatic metrics and human evaluations confirmed the high quality of the synthetic data; practical applications and comparisons to existing datasets showed significant improvements in MT performance for low-resource languages.

**Limitations:** The study focuses only on low-resource languages and does not address potential biases in synthetic data generation.

**Conclusion:** LLM-generated synthetic data can meaningfully enhance MT outcomes, even when it contains noise, making it valuable for low-resource language translation.

**Abstract:** We investigate the potential of LLM-generated synthetic data for improving low-resource machine translation (MT). Focusing on seven diverse target languages, we construct a document-level synthetic corpus from English Europarl, and extend it via pivoting to 147 additional language pairs. Automatic and human evaluation confirm its high overall quality. We study its practical application by (i) identifying effective training regimes, (ii) comparing our data with the HPLT dataset, and (iii) testing its utility beyond English-centric MT. Finally, we introduce SynOPUS, a public repository for synthetic parallel datasets. Our findings show that LLM-generated synthetic data, even when noisy, can substantially improve MT performance for low-resource languages.

</details>


### [127] [From Templates to Natural Language: Generalization Challenges in Instruction-Tuned LLMs for Spatial Reasoning](https://arxiv.org/abs/2505.14425)

*Chalamalasetti Kranti, Sherzod Hakimov, David Schlangen*

**Main category:** cs.CL

**Keywords:** large language models, instruction tuning, spatial grounding, generalization, error analysis

**Relevance Score:** 8

**TL;DR:** Study of instruction generalization challenges in spatial grounding tasks for LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of LLMs struggling to generalize from synthetic to human-authored instructions in spatial grounding tasks.

**Method:** Fine-tuning LLMs with synthetic instructions and evaluating performance on a benchmark dataset with synthetic and human-written instructions.

**Key Contributions:**

	1. Analysis of generalization challenges in grounded environments
	2. Benchmarking LLMs with synthetic and human-written instructions
	3. Identification of performance degradation in complex tasks

**Result:** Models perform well on simple tasks but show significant performance degradation on complex tasks.

**Limitations:** 

**Conclusion:** A detailed error analysis highlights the gaps in instruction generalization for LLMs, suggesting targeted areas for improvement.

**Abstract:** Instruction-tuned large language models (LLMs) have shown strong performance on a variety of tasks; however, generalizing from synthetic to human-authored instructions in grounded environments remains a challenge for them. In this work, we study generalization challenges in spatial grounding tasks where models interpret and translate instructions for building object arrangements on a $2.5$D grid. We fine-tune LLMs using only synthetic instructions and evaluate their performance on a benchmark dataset containing both synthetic and human-written instructions. Our results reveal that while models generalize well on simple tasks, their performance degrades significantly on more complex tasks. We present a detailed error analysis of the gaps in instruction generalization.

</details>


### [128] [Neural Incompatibility: The Unbridgeable Gap of Cross-Scale Parametric Knowledge Transfer in Large Language Models](https://arxiv.org/abs/2505.14436)

*Yuqiao Tan, Shizhu He, Kang Liu, Jun Zhao*

**Main category:** cs.CL

**Keywords:** Parametric Knowledge Transfer, Large Language Models, Neural Incompatibility

**Relevance Score:** 9

**TL;DR:** The paper explores Parametric Knowledge Transfer (PKT) across Large Language Models (LLMs) and introduces two paradigms: Post-Align PKT and Pre-Align PKT, along with a novel solution called LaTen to enhance knowledge transfer across LLMs with varying scales, revealing challenges due to Neural Incompatibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance knowledge transfer methods across LLMs of different scales, which has potential implications for improving AI applications, especially in HCI and health informatics.

**Method:** The paper defines two methods for PKT: Post-Align PKT (PostPKT) and Pre-Align PKT (PrePKT). PostPKT requires fine-tuning after parameter extraction, while PrePKT aims to align the parametric spaces with minimal training steps.

**Key Contributions:**

	1. Introduction of Post-Align PKT and Pre-Align PKT paradigms for knowledge transfer.
	2. Proposal of LaTen for efficient alignment of LLMs' parametric spaces across scales.
	3. Identification of Neural Incompatibility as a primary challenge in PKT.

**Result:** Experiments across four benchmarks indicated challenges in achieving stable PKT with both PostPKT and PrePKT, highlighting Neural Incompatibility as a significant challenge in effective knowledge transfer.

**Limitations:** The study faces challenges in achieving consistently stable transfer across different scales of LLMs due to parametric structural differences.

**Conclusion:** The findings suggest that addressing Neural Incompatibility in LLMs is crucial for efficient PKT, guiding future research in parametric architectures of LLMs.

**Abstract:** Large Language Models (LLMs) offer a transparent brain with accessible parameters that encode extensive knowledge, which can be analyzed, located and transferred. Consequently, a key research challenge is to transcend traditional knowledge transfer paradigms rooted in symbolic language and achieve genuine Parametric Knowledge Transfer (PKT). Significantly, exploring effective methods for transferring knowledge across LLMs of different scales through parameters presents an intriguing and valuable research direction. In this paper, we first demonstrate $\textbf{Alignment}$ in parametric space is the fundamental prerequisite to achieve successful cross-scale PKT. We redefine the previously explored knowledge transfer as Post-Align PKT (PostPKT), which utilizes extracted parameters for LoRA initialization and requires subsequent fine-tune for alignment. Hence, to reduce cost for further fine-tuning, we introduce a novel Pre-Align PKT (PrePKT) paradigm and propose a solution called $\textbf{LaTen}$ ($\textbf{L}$oc$\textbf{a}$te-$\textbf{T}$h$\textbf{e}$n-Alig$\textbf{n}$) that aligns the parametric spaces of LLMs across scales only using several training steps without following training. Comprehensive experiments on four benchmarks demonstrate that both PostPKT and PrePKT face challenges in achieving consistently stable transfer. Through in-depth analysis, we identify $\textbf{Neural Incompatibility}$ as the ethological and parametric structural differences between LLMs of varying scales, presenting fundamental challenges to achieving effective PKT. These findings provide fresh insights into the parametric architectures of LLMs and highlight promising directions for future research on efficient PKT. Our code is available at https://github.com/Trae1ounG/Neural_Incompatibility.

</details>


### [129] [Creative Preference Optimization](https://arxiv.org/abs/2505.14442)

*Mete Ismayilzada, Antonio Laverghetta Jr., Simone A. Luchini, Reet Patel, Antoine Bosselut, Lonneke van der Plas, Roger Beaty*

**Main category:** cs.CL

**Keywords:** Creative Preference Optimization, Large Language Models, Creativity, Machine Learning, Preferences

**Relevance Score:** 9

**TL;DR:** The paper presents Creative Preference Optimization (CrPO), a new method to enhance the creativity of Large Language Models (LLMs) by integrating multiple dimensions of creativity into preference optimization.

**Read time:** 27 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods to improve LLM creativity are often task-specific and do not address the multifaceted nature of creativity.

**Method:** The authors introduce CrPO to modularly integrate signals from various creativity dimensions into the preference optimization objective, and they trained models on a new dataset, MuCE, for evaluation.

**Key Contributions:**

	1. Introduction of Creative Preference Optimization (CrPO) as a creative enhancement method for LLMs
	2. Development of MuCE, a large-scale human preference dataset for creativity assessment
	3. Demonstration of superior performance in creativity metrics compared to existing models

**Result:** The proposed models demonstrate superior performance over strong baselines like GPT-4o, showing improvements in novelty, diversity, surprise, and output quality in both automated and human evaluations.

**Limitations:** 

**Conclusion:** Optimizing for creativity using preference frameworks shows significant promise without sacrificing quality, indicating a new direction for LLM enhancement.

**Abstract:** While Large Language Models (LLMs) have demonstrated impressive performance across natural language generation tasks, their ability to generate truly creative content-characterized by novelty, diversity, surprise, and quality-remains limited. Existing methods for enhancing LLM creativity often focus narrowly on diversity or specific tasks, failing to address creativity's multifaceted nature in a generalizable way. In this work, we propose Creative Preference Optimization (CrPO), a novel alignment method that injects signals from multiple creativity dimensions into the preference optimization objective in a modular fashion. We train and evaluate creativity-augmented versions of several models using CrPO and MuCE, a new large-scale human preference dataset spanning over 200,000 human-generated responses and ratings from more than 30 psychological creativity assessments. Our models outperform strong baselines, including GPT-4o, on both automated and human evaluations, producing more novel, diverse, and surprising generations while maintaining high output quality. Additional evaluations on NoveltyBench further confirm the generalizability of our approach. Together, our results demonstrate that directly optimizing for creativity within preference frameworks is a promising direction for advancing the creative capabilities of LLMs without compromising output quality.

</details>


### [130] [CtrlDiff: Boosting Large Diffusion Language Models with Dynamic Block Prediction and Controllable Generation](https://arxiv.org/abs/2505.14455)

*Chihan Huang, Hao Tang*

**Main category:** cs.CL

**Keywords:** diffusion models, language generation, semi-autoregressive frameworks, reinforcement learning, conditional text generation

**Relevance Score:** 9

**TL;DR:** CtrlDiff is a dynamic and controllable semi-autoregressive language model that combines autoregressive dependencies with discrete diffusion. It addresses key limitations of fixed output lengths and control mechanisms in diffusion models by adaptively determining generation block sizes and implementing a classifier-guided control that reduces computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of fixed-length outputs and weak control in existing large diffusion language models, which restrict their effectiveness in practical applications.

**Method:** The authors propose CtrlDiff, which uses reinforcement learning to adaptively set the size of generation blocks based on semantics and includes a classifier-guided control for better efficiency and flexibility in text generation.

**Key Contributions:**

	1. Introduction of CtrlDiff as a semi-autoregressive framework
	2. Adaptive determination of generation block sizes using reinforcement learning
	3. Classifier-guided control mechanism for efficient post-hoc conditioning

**Result:** Experiments show that CtrlDiff establishes a new benchmark among hybrid diffusion models, significantly improving performance to compete with state-of-the-art autoregressive models and enhancing conditional text generation across various tasks.

**Limitations:** 

**Conclusion:** CtrlDiff represents a major step towards more flexible, controllable, and efficient diffusion models for language generation, effectively bridging the gap with autoregressive frameworks.

**Abstract:** Although autoregressive models have dominated language modeling in recent years, there has been a growing interest in exploring alternative paradigms to the conventional next-token prediction framework. Diffusion-based language models have emerged as a compelling alternative due to their powerful parallel generation capabilities and inherent editability. However, these models are often constrained by fixed-length generation. A promising direction is to combine the strengths of both paradigms, segmenting sequences into blocks, modeling autoregressive dependencies across blocks while leveraging discrete diffusion to estimate the conditional distribution within each block given the preceding context. Nevertheless, their practical application is often hindered by two key limitations: rigid fixed-length outputs and a lack of flexible control mechanisms. In this work, we address the critical limitations of fixed granularity and weak controllability in current large diffusion language models. We propose CtrlDiff, a dynamic and controllable semi-autoregressive framework that adaptively determines the size of each generation block based on local semantics using reinforcement learning. Furthermore, we introduce a classifier-guided control mechanism tailored to discrete diffusion, which significantly reduces computational overhead while facilitating efficient post-hoc conditioning without retraining. Extensive experiments demonstrate that CtrlDiff sets a new standard among hybrid diffusion models, narrows the performance gap to state-of-the-art autoregressive approaches, and enables effective conditional text generation across diverse tasks.

</details>


### [131] [Not All Correct Answers Are Equal: Why Your Distillation Source Matters](https://arxiv.org/abs/2505.14464)

*Xiaoyu Tian, Yunjie Ji, Haotian Wang, Shuaiting Chen, Sitong Zhao, Yiping Peng, Han Zhao, Xiangang Li*

**Main category:** cs.CL

**Keywords:** reasoning, data distillation, language models, machine learning, NLP

**Relevance Score:** 8

**TL;DR:** This work studies the effectiveness of reasoning data distillation for enhancing language models by constructing datasets from verified outputs of teacher models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of open-source language models using effective data distillation techniques.

**Method:** Conducted a large-scale empirical study using three teacher models to collect data from a corpus of 1.89 million queries and analyzed the resulting datasets.

**Key Contributions:**

	1. High-quality reasoning traces improve model performance.
	2. Empirical analysis of three parallel datasets from teacher models.
	3. Public release of distilled datasets for future research.

**Result:** AM-Thinking-v1-distilled data shows greater token length diversity and lower perplexity, with the AM-based student model achieving the best scores on several reasoning benchmarks.

**Limitations:** 

**Conclusion:** The study demonstrates the significance of high-quality verified reasoning traces in training language models, with datasets made publicly available for further research.

**Abstract:** Distillation has emerged as a practical and effective approach to enhance the reasoning capabilities of open-source language models. In this work, we conduct a large-scale empirical study on reasoning data distillation by collecting verified outputs from three state-of-the-art teacher models-AM-Thinking-v1, Qwen3-235B-A22B, and DeepSeek-R1-on a shared corpus of 1.89 million queries. We construct three parallel datasets and analyze their distributions, revealing that AM-Thinking-v1-distilled data exhibits greater token length diversity and lower perplexity. Student models trained on each dataset are evaluated on reasoning benchmarks including AIME2024, AIME2025, MATH500, and LiveCodeBench. The AM-based model consistently achieves the best performance (e.g., 84.3 on AIME2024, 72.2 on AIME2025, 98.4 on MATH500, and 65.9 on LiveCodeBench) and demonstrates adaptive output behavior-producing longer responses for harder tasks and shorter ones for simpler tasks. These findings highlight the value of high-quality, verified reasoning traces. We release the AM-Thinking-v1 and Qwen3-235B-A22B distilled datasets to support future research on open and high-performing reasoning-oriented language models. The datasets are publicly available on Hugging Face\footnote{Datasets are available on Hugging Face: \href{https://huggingface.co/datasets/a-m-team/AM-Thinking-v1-Distilled}{AM-Thinking-v1-Distilled}, \href{https://huggingface.co/datasets/a-m-team/AM-Qwen3-Distilled}{AM-Qwen3-Distilled}.}.

</details>


### [132] [Void in Language Models](https://arxiv.org/abs/2505.14467)

*Mani Shemiranifar*

**Main category:** cs.CL

**Keywords:** transformer models, layer activation, adaptive computation, language models, inference

**Relevance Score:** 7

**TL;DR:** This paper investigates layer activation in transformer-based language models during inference and introduces a method to identify unactivated layers, leading to improved model performance by selectively skipping void layers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether all layers in transformer-based language models are activated during inference and improve performance based on this understanding.

**Method:** The study employs a non-trainable and parameter-free technique called L2 Adaptive Computation (LAC) to track layer activations during two phases: Prompt Processing and Response Generation.

**Key Contributions:**

	1. Introduction of L2 Adaptive Computation for layer activation tracking
	2. Demonstration of performance improvements by skipping void layers
	3. Analysis of layer activation during different inference phases

**Result:** The experiments show that by skipping unactivated layers (voids), significant performance gains were achieved in multiple instruction-tuned models, with improved scores in various benchmarks while using a reduced number of layers.

**Limitations:** 

**Conclusion:** The findings suggest that not all layers in transformer models contribute equally, and selective activation can enhance task performance.

**Abstract:** Despite advances in transformer-based language models (LMs), a fundamental question remains largely unanswered: Are all layers activated during inference? We investigate this question by detecting unactivated layers (which we refer to as Voids) using a non-trainable and parameter-free adaptive computation method called L2 Adaptive Computation (LAC). We adapt LAC from its original efficiency-focused application to trace activated layers during inference. This method monitors changes in the L2-norm of activations to identify voids. We analyze layer activation in instruction-tuned LMs across two phases: Prompt Processing (PP), where we trace activated layers for each token in the input prompts, and Response Generation (RG), where we trace activated layers for each generated token. We further demonstrate that distinct layers are activated during these two phases. To show the effectiveness of our method, we evaluated three distinct instruction-tuned LMs from the Llama, Mistral, and Qwen families on three benchmarks: MMLU, GPQA Diamond, and BoolQ. For example, on MMLU with a zero-shot setting, skipping voids in Qwen2.5-7B-Instruct resulted in an improvement from 69.24 to 71.29 while the model uses only 30% of the layers. Similarly, Mistral-7B-Instruct-v0.3 on GPQA Diamond improved from 13.88 to 18.36 when using 70% of the layers during both the PP and RG phases. These results show that not all layers contribute equally during inference, and that selectively skipping most of them can improve the performance of models on certain tasks.

</details>


### [133] [Attributional Safety Failures in Large Language Models under Code-Mixed Perturbations](https://arxiv.org/abs/2505.14469)

*Somnath Banerjee, Pratyush Chatterjee, Shanu Kumar, Sayan Layek, Parag Agrawal, Rima Hazra, Animesh Mukherjee*

**Main category:** cs.CL

**Keywords:** LLMs, code-mixed prompts, safety concerns, explainability, cultural dimensions

**Relevance Score:** 8

**TL;DR:** This study examines the safety concerns of LLMs with code-mixed prompts, revealing increased harmful outputs compared to monolingual prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety concerns regarding LLMs' performance with code-mixed inputs, which have emerged from recent advancements.

**Method:** The study employs explainability methods to analyze internal attribution shifts in LLMs when exposed to code-mixed versus monolingual prompts.

**Key Contributions:**

	1. Investigates the unique impact of code-mixed prompts on LLM safety.
	2. Distinguishes between universally unsafe and culturally-specific unsafe queries.
	3. Provides experimental insights into the internal behaviors of LLMs.led

**Result:** The investigation highlights a notable increase in unsafe outputs from LLMs with code-mixed prompts, in addition to distinctions between universally and culturally-specific unsafe queries.

**Limitations:** Focus on code-mixed inputs may not universally generalize to all LLM applications.

**Conclusion:** Understanding these dynamics is crucial for enhancing LLM safety and application in diverse linguistic contexts.

**Abstract:** Recent advancements in LLMs have raised significant safety concerns, particularly when dealing with code-mixed inputs and outputs. Our study systematically investigates the increased susceptibility of LLMs to produce unsafe outputs from code-mixed prompts compared to monolingual English prompts. Utilizing explainability methods, we dissect the internal attribution shifts causing model's harmful behaviors. In addition, we explore cultural dimensions by distinguishing between universally unsafe and culturally-specific unsafe queries. This paper presents novel experimental insights, clarifying the mechanisms driving this phenomenon.

</details>


### [134] [Adapting Pretrained Language Models for Citation Classification via Self-Supervised Contrastive Learning](https://arxiv.org/abs/2505.14471)

*Tong Li, Jiachuan Wang, Yongqi Zhang, Shuangyin Li, Lei Chen*

**Main category:** cs.CL

**Keywords:** citation classification, self-supervised learning, pretrained language models

**Relevance Score:** 5

**TL;DR:** The paper presents Citss, a framework that uses self-supervised contrastive learning to improve citation classification by adapting pretrained language models while addressing data scarcity and contextual challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Citation classification is essential for scholarly analysis, but fine-tuning pretrained language models is complicated by data scarcity and contextual noise.

**Method:** The Citss framework employs self-supervised contrastive learning, utilizing sentence-level cropping and keyphrase perturbation to generate contrastive pairs from citation data.

**Key Contributions:**

	1. Introduction of self-supervised contrastive learning for citation classification
	2. Development of sentence-level cropping and keyphrase perturbation strategies
	3. Compatibility with both encoder-based PLMs and decoder-based LLMs

**Result:** Experiments show that Citss outperforms previous state-of-the-art methods using both encoder-based pretrained language models and decoder-based large language models across three benchmark datasets.

**Limitations:** 

**Conclusion:** Citss effectively enhances citation classification performance by addressing challenges through innovative self-supervised techniques and is applicable to various PLMs.

**Abstract:** Citation classification, which identifies the intention behind academic citations, is pivotal for scholarly analysis. Previous works suggest fine-tuning pretrained language models (PLMs) on citation classification datasets, reaping the reward of the linguistic knowledge they gained during pretraining. However, directly fine-tuning for citation classification is challenging due to labeled data scarcity, contextual noise, and spurious keyphrase correlations. In this paper, we present a novel framework, Citss, that adapts the PLMs to overcome these challenges. Citss introduces self-supervised contrastive learning to alleviate data scarcity, and is equipped with two specialized strategies to obtain the contrastive pairs: sentence-level cropping, which enhances focus on target citations within long contexts, and keyphrase perturbation, which mitigates reliance on specific keyphrases. Compared with previous works that are only designed for encoder-based PLMs, Citss is carefully developed to be compatible with both encoder-based PLMs and decoder-based LLMs, to embrace the benefits of enlarged pretraining. Experiments with three benchmark datasets with both encoder-based PLMs and decoder-based LLMs demonstrate our superiority compared to the previous state of the art. Our code is available at: github.com/LITONG99/Citss

</details>


### [135] [PlanGPT-VL: Enhancing Urban Planning with Domain-Specific Vision-Language Models](https://arxiv.org/abs/2505.14481)

*He Zhu, Junyou Su, Minxi Chen, Wen Wang, Yijie Deng, Guanhua Chen, Wenjia Zhang*

**Main category:** cs.CL

**Keywords:** Vision-Language Model, urban planning, map analysis, machine learning, data synthesis

**Relevance Score:** 5

**TL;DR:** PlanGPT-VL is a Vision-Language Model specifically designed for urban planning maps, aiming to improve interpretation and analysis in this specialized field.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Vision-Language Models struggle to effectively analyze urban planning maps, which are crucial for urban planners and educational contexts, necessitating specialized models.

**Method:** The paper introduces PlanGPT-VL, implementing three innovative approaches: the PlanAnno-V framework for data synthesis, Critical Point Thinking for hallucination reduction, and a training methodology that involves Supervised Fine-Tuning of a frozen vision encoder.

**Key Contributions:**

	1. Introduction of a domain-specific VLM for urban planning maps
	2. Implementation of innovative methodologies for data synthesis and hallucination reduction
	3. High performance achieved with a lightweight model compared to larger counterparts

**Result:** PlanGPT-VL outperforms general-purpose VLMs in interpreting urban planning maps, offering reliable analysis and educational tools for professionals while achieving efficiency with a lightweight model.

**Limitations:** 

**Conclusion:** The study demonstrates that PlanGPT-VL can match the performance of larger models in specialized tasks, highlighting its effectiveness and efficiency in urban planning applications.

**Abstract:** In the field of urban planning, existing Vision-Language Models (VLMs) frequently fail to effectively analyze and evaluate planning maps, despite the critical importance of these visual elements for urban planners and related educational contexts. Planning maps, which visualize land use, infrastructure layouts, and functional zoning, require specialized understanding of spatial configurations, regulatory requirements, and multi-scale analysis. To address this challenge, we introduce PlanGPT-VL, the first domain-specific Vision-Language Model tailored specifically for urban planning maps. PlanGPT-VL employs three innovative approaches: (1) PlanAnno-V framework for high-quality VQA data synthesis, (2) Critical Point Thinking to reduce hallucinations through structured verification, and (3) comprehensive training methodology combining Supervised Fine-Tuning with frozen vision encoder parameters. Through systematic evaluation on our proposed PlanBench-V benchmark, we demonstrate that PlanGPT-VL significantly outperforms general-purpose state-of-the-art VLMs in specialized planning map interpretation tasks, offering urban planning professionals a reliable tool for map analysis, assessment, and educational applications while maintaining high factual accuracy. Our lightweight 7B parameter model achieves comparable performance to models exceeding 72B parameters, demonstrating efficient domain specialization without sacrificing performance.

</details>


### [136] [MoMoE: Mixture of Moderation Experts Framework for AI-Assisted Online Governance](https://arxiv.org/abs/2505.14483)

*Agam Goyal, Xianyang Zhan, Yilun Chen, Koustuv Saha, Eshwar Chandrasekharan*

**Main category:** cs.CL

**Keywords:** content moderation, large language models, explainability, human-computer interaction, trustworthy AI governance

**Relevance Score:** 8

**TL;DR:** MoMoE introduces a modular framework for scalable and explainable content moderation in online communities by using a mixture of community-specialized and norm-violation experts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing content moderation approaches require separate models for each online community and lack transparency, hindering their real-world applicability.

**Method:** MoMoE orchestrates four primary operators—Allocate, Predict, Aggregate, and Explain—across seven community-specialized experts and five norm-violation experts to address moderation across multiple subreddits.

**Key Contributions:**

	1. Introduction of a modular framework for content moderation
	2. Use of explainable expert ensembles for transparency
	3. Demonstration of strong performance across various online communities using a unified approach.

**Result:** On 30 unseen subreddits, MoMoE variants achieved Micro-F1 scores of 0.72 for community-specialized and 0.67 for norm-violation experts, surpassing strong fine-tuned baselines while providing reliable explanations.

**Limitations:** Performance may vary by domain despite the framework's overall stability.

**Conclusion:** MoMoE demonstrates that explainable expert ensembles can facilitate scalable and transparent moderation without requiring per-community fine-tuning, indicating its potential in the fields of NLP and HCI for improving human-AI governance.

**Abstract:** Large language models (LLMs) have shown great potential in flagging harmful content in online communities. Yet, existing approaches for moderation require a separate model for every community and are opaque in their decision-making, limiting real-world adoption. We introduce Mixture of Moderation Experts (MoMoE), a modular, cross-community framework that adds post-hoc explanations to scalable content moderation. MoMoE orchestrates four operators -- Allocate, Predict, Aggregate, Explain -- and is instantiated as seven community-specialized experts (MoMoE-Community) and five norm-violation experts (MoMoE-NormVio). On 30 unseen subreddits, the best variants obtain Micro-F1 scores of 0.72 and 0.67, respectively, matching or surpassing strong fine-tuned baselines while consistently producing concise and reliable explanations. Although community-specialized experts deliver the highest peak accuracy, norm-violation experts provide steadier performance across domains. These findings show that MoMoE yields scalable, transparent moderation without needing per-community fine-tuning. More broadly, they suggest that lightweight, explainable expert ensembles can guide future NLP and HCI research on trustworthy human-AI governance of online communities.

</details>


### [137] [Enhanced Multimodal Aspect-Based Sentiment Analysis by LLM-Generated Rationales](https://arxiv.org/abs/2505.14499)

*Jun Cao, Jiyi Li, Ziwei Yang, Renjie Zhou*

**Main category:** cs.CL

**Keywords:** Multimodal, Aspect-Based Sentiment Analysis, Large Language Models, Small Language Models, Dual Cross-Attention

**Relevance Score:** 8

**TL;DR:** The paper introduces a novel framework, LRSA, for Multimodal Aspect-Based Sentiment Analysis (MABSA) that enhances small language models (SLMs) by incorporating large language models (LLMs) to improve aspect and sentiment detection in multimodal data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods in MABSA often rely on small language models which have limited capacity, leading to inaccuracies in identifying aspects and sentiments in multimodal data. The paper seeks to improve this by leveraging the strengths of larger models.

**Method:** The LRSA framework combines the decision-making abilities of small language models with additional insights from large language models by injecting LLM-generated explanations into SLMs, and employs a dual cross-attention mechanism to enhance feature interaction and fusion.

**Key Contributions:**

	1. Proposed a novel LRSA framework for MABSA
	2. Integrated LLM-generated explanations into SLMs
	3. Showcased superior performance on benchmark datasets

**Result:** Experiments demonstrate that the LRSA framework outperforms baseline models across three widely-used benchmarks, showing its effectiveness in improving sentiment and aspect detection.

**Limitations:** 

**Conclusion:** The LRSA approach demonstrates enhanced performance in MABSA through the integration of LLMs, suggesting that it is a viable and beneficial strategy for improving sentiment analysis tasks across multimodal data.

**Abstract:** There has been growing interest in Multimodal Aspect-Based Sentiment Analysis (MABSA) in recent years. Existing methods predominantly rely on pre-trained small language models (SLMs) to collect information related to aspects and sentiments from both image and text, with an aim to align these two modalities. However, small SLMs possess limited capacity and knowledge, often resulting in inaccurate identification of meaning, aspects, sentiments, and their interconnections in textual and visual data. On the other hand, Large language models (LLMs) have shown exceptional capabilities in various tasks by effectively exploring fine-grained information in multimodal data. However, some studies indicate that LLMs still fall short compared to fine-tuned small models in the field of ABSA. Based on these findings, we propose a novel framework, termed LRSA, which combines the decision-making capabilities of SLMs with additional information provided by LLMs for MABSA. Specifically, we inject explanations generated by LLMs as rationales into SLMs and employ a dual cross-attention mechanism for enhancing feature interaction and fusion, thereby augmenting the SLMs' ability to identify aspects and sentiments. We evaluated our method using two baseline models, numerous experiments highlight the superiority of our approach on three widely-used benchmarks, indicating its generalizability and applicability to most pre-trained models for MABSA.

</details>


### [138] [ModRWKV: Transformer Multimodality in Linear Time](https://arxiv.org/abs/2505.14505)

*Jiale Kang, Ziyin Yue, Qingyu Yin, Jiang Rui, Weile Li, Zening Lu, Zhouran Ji*

**Main category:** cs.CL

**Keywords:** multimodal, RNN, RWKV, large language models, computational efficiency

**Relevance Score:** 7

**TL;DR:** This paper presents ModRWKV, a decoupled multimodal framework based on RNN architectures that leverages RWKV7 as its backbone, achieving efficient multimodal processing and performance optimization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the applicability of modern RNN architectures in multimodal contexts, particularly as an alternative to the currently dominant Transformer models.

**Method:** The authors propose ModRWKV, a framework utilizing modern RNN architecture for multimodal information fusion, involving dynamically adaptable heterogeneous modality encoders.

**Key Contributions:**

	1. Introduction of a decoupled multimodal framework named ModRWKV based on RNNs.
	2. Demonstration of the effectiveness of pretrained RWKV7 weights in multimodal training.
	3. Identification of an optimal configuration for the ModRWKV architecture that maximizes efficiency.

**Result:** ModRWKV achieves an optimal balance between performance and computational efficiency in multimodal tasks and demonstrates the significance of pretrained RWKV7 weights in enhancing multimodal training.

**Limitations:** 

**Conclusion:** Modern RNN architectures can serve as a viable alternative to Transformers for multimodal large language models, with ModRWKV providing a well-optimized solution.

**Abstract:** Currently, most multimodal studies are based on large language models (LLMs) with quadratic-complexity Transformer architectures. While linear models like RNNs enjoy low inference costs, their application has been largely limited to the text-only modality. This work explores the capabilities of modern RNN architectures in multimodal contexts. We propose ModRWKV-a decoupled multimodal framework built upon the RWKV7 architecture as its LLM backbone-which achieves multi-source information fusion through dynamically adaptable heterogeneous modality encoders. We designed the multimodal modules in ModRWKV with an extremely lightweight architecture and, through extensive experiments, identified a configuration that achieves an optimal balance between performance and computational efficiency. ModRWKV leverages the pretrained weights of the RWKV7 LLM for initialization, which significantly accelerates multimodal training. Comparative experiments with different pretrained checkpoints further demonstrate that such initialization plays a crucial role in enhancing the model's ability to understand multimodal signals. Supported by extensive experiments, we conclude that modern RNN architectures present a viable alternative to Transformers in the domain of multimodal large language models (MLLMs). Furthermore, we identify the optimal configuration of the ModRWKV architecture through systematic exploration.

</details>


### [139] [Exploring Graph Representations of Logical Forms for Language Modeling](https://arxiv.org/abs/2505.14523)

*Michael Sullivan*

**Main category:** cs.CL

**Keywords:** language models, logical forms, data efficiency, machine learning, graph semantics

**Relevance Score:** 8

**TL;DR:** The paper presents language models over logical forms (LFLMs), specifically introducing GFoLDS, which significantly outperforms textual transformer LMs in data efficiency and learning capability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research argues for the advantages of language models that utilize logical forms over traditional textual models due to their data efficiency and ability to learn complex patterns.

**Method:** The authors introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, which is a pretrained LM utilizing graph representations of logical forms to demonstrate the efficiency of LFLMs.

**Key Contributions:**

	1. Introduction of the GFoLDS prototype as a new approach to language modeling.
	2. Demonstration of the superior performance of LFLMs in comparison to textual LMs.
	3. Evidence supporting the effective learning from limited data using logical forms.

**Result:** GFoLDS shows strong empirical performance, outperforming traditional transformer models pretrained on similar data amounts in various downstream tasks, proving LFLMs require less data to achieve better learning outcomes.

**Limitations:** 

**Conclusion:** The results suggest that LFLMs, through their scalable nature with more parameters and data, are promising for real-world applications in language understanding tasks.

**Abstract:** We make the case for language models over logical forms (LFLMs), arguing that such models are more data-efficient than their textual counterparts. To that end, we introduce the Graph-based Formal-Logical Distributional Semantics (GFoLDS) prototype, a pretrained LM over graph representations of logical forms, as a proof-of-concept of LFLMs. Using GFoLDS, we present strong experimental evidence that LFLMs can leverage the built-in, basic linguistic knowledge inherent in such models to immediately begin learning more complex patterns. On downstream tasks, we show that GFoLDS vastly outperforms textual, transformer LMs pretrained on similar amounts of data, indicating that LFLMs can learn with substantially less data than models over plain text. Furthermore, we show that the performance of this model is likely to scale with additional parameters and pretraining data, suggesting the viability of LFLMs in real-world applications.

</details>


### [140] [Internal Chain-of-Thought: Empirical Evidence for Layer-wise Subtask Scheduling in LLMs](https://arxiv.org/abs/2505.14530)

*Zhipeng Yang, Junzhuo Li, Siyu Xia, Xuming Hu*

**Main category:** cs.CL

**Keywords:** large language models, internal chain-of-thought, task decomposition, execution patterns, transparency

**Relevance Score:** 9

**TL;DR:** This paper investigates how large language models internally execute tasks by decomposing them into subtasks across different layers, confirming this behavior through empirical methods.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the transparency of large language models by understanding their internal processes and how they execute discrete subtasks.

**Method:** The study uses layer-from context-masking and a novel cross-task patching method to explore distinct subtask learning at different network depths and applies LogitLens to analyze hidden states for layerwise execution patterns.

**Key Contributions:**

	1. Demonstration of internal chain-of-thought in LLMs
	2. Empirical evidence of task decomposition across layers
	3. Introduction of a novel method for analyzing subtask execution

**Result:** The findings reveal a consistent layerwise execution pattern in LLMs, confirming that distinct subtasks are learned and executed sequentially across layers, as demonstrated on various benchmarks.

**Limitations:** 

**Conclusion:** The results enhance our understanding of LLMs' capabilities in planning and executing tasks internally, suggesting implications for instruction-level activation steering.

**Abstract:** We show that large language models (LLMs) exhibit an $\textit{internal chain-of-thought}$: they sequentially decompose and execute composite tasks layer-by-layer. Two claims ground our study: (i) distinct subtasks are learned at different network depths, and (ii) these subtasks are executed sequentially across layers. On a benchmark of 15 two-step composite tasks, we employ layer-from context-masking and propose a novel cross-task patching method, confirming (i). To examine claim (ii), we apply LogitLens to decode hidden states, revealing a consistent layerwise execution pattern. We further replicate our analysis on the real-world $\text{TRACE}$ benchmark, observing the same stepwise dynamics. Together, our results enhance LLMs transparency by showing their capacity to internally plan and execute subtasks (or instructions), opening avenues for fine-grained, instruction-level activation steering.

</details>


### [141] [Breaking Bad Tokens: Detoxification of LLMs Using Sparse Autoencoders](https://arxiv.org/abs/2505.14536)

*Agam Goyal, Vedant Rathi, William Yeh, Yian Wang, Yuen Chen, Hari Sundaram*

**Main category:** cs.CL

**Keywords:** detoxification, large language models, sparse autoencoders, toxicity reduction, safety interventions

**Relevance Score:** 9

**TL;DR:** The paper proposes a method for detoxifying large language models using sparse autoencoders to steer their outputs away from toxic content, striving for a balance between toxicity reduction and language fluency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of toxic outputs in large language models, which pose challenges in user-facing applications, and existing methods being easily bypassed.

**Method:** The authors leverage sparse autoencoders to identify and steer away from toxicity-related directions in the residual stream of models, implementing three tiers of steering aggressiveness.

**Key Contributions:**

	1. Introduced a novel method using sparse autoencoders for targeted detoxification of LLMs.
	2. Demonstrated the trade-offs between toxicity reduction and language fluency.
	3. Provided insights on the importance of disentangled feature learning in safety interventions.

**Result:** Stronger steering approaches reduced toxicity by up to 20% while maintaining stability in standard NLP benchmark scores; however, fluency issues emerged with higher aggressiveness.

**Limitations:** Fluency can degrade depending on the aggressiveness of the steering; feature-splitting hampers safety interventions.

**Conclusion:** The study reveals both the potential and limitations of sparse autoencoder interventions for detoxifying language models, proposing guidelines for safer deployment.

**Abstract:** Large language models (LLMs) are now ubiquitous in user-facing applications, yet they still generate undesirable toxic outputs, including profanity, vulgarity, and derogatory remarks. Although numerous detoxification methods exist, most apply broad, surface-level fixes and can therefore easily be circumvented by jailbreak attacks. In this paper we leverage sparse autoencoders (SAEs) to identify toxicity-related directions in the residual stream of models and perform targeted activation steering using the corresponding decoder vectors. We introduce three tiers of steering aggressiveness and evaluate them on GPT-2 Small and Gemma-2-2B, revealing trade-offs between toxicity reduction and language fluency. At stronger steering strengths, these causal interventions surpass competitive baselines in reducing toxicity by up to 20%, though fluency can degrade noticeably on GPT-2 Small depending on the aggressiveness. Crucially, standard NLP benchmark scores upon steering remain stable, indicating that the model's knowledge and general abilities are preserved. We further show that feature-splitting in wider SAEs hampers safety interventions, underscoring the importance of disentangled feature learning. Our findings highlight both the promise and the current limitations of SAE-based causal interventions for LLM detoxification, further suggesting practical guidelines for safer language-model deployment.

</details>


### [142] [KORGym: A Dynamic Game Platform for LLM Reasoning Evaluation](https://arxiv.org/abs/2505.14552)

*Jiajun Shi, Jian Yang, Jiaheng Liu, Xingyuan Bu, Jiangjie Chen, Junting Zhou, Kaijing Ma, Zhoufutu Wen, Bingli Wang, Yancheng He, Liang Song, Hualei Zhu, Shilong Li, Xingjian Wang, Wei Zhang, Ruibin Yuan, Yifan Yao, Wenjun Yang, Yunli Wang, Siyuan Fang, Siyu Yuan, Qianyu He, Xiangru Tang, Yingshui Tan, Wangchunshu Zhou, Zhaoxiang Zhang, Zhoujun Li, Wenhao Huang, Ge Zhang*

**Main category:** cs.CL

**Keywords:** large language models, reasoning capabilities, evaluation methodology, reinforcement learning, interactive environments

**Relevance Score:** 9

**TL;DR:** Introduction of KORGym, a dynamic evaluation platform for assessing reasoning capabilities of LLMs.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To improve evaluation methods for LLMs by addressing limitations of existing domain-specific benchmarks.

**Method:** KORGym offers over fifty interactive games for evaluating LLMs' reasoning through multi-turn assessments and reinforcement learning scenarios.

**Key Contributions:**

	1. Introduction of a diverse evaluation platform (KORGym) for LLMs
	2. Extensive benchmarking across multiple models and scenarios
	3. Insights into reasoning patterns and performance influences

**Result:** Extensive experiments with 19 LLMs and 8 VLMs reveal consistent reasoning patterns and superior performance of closed-source models.

**Limitations:** 

**Conclusion:** KORGym is expected to advance LLM reasoning research and refine evaluation methodologies for interactive environments.

**Abstract:** Recent advancements in large language models (LLMs) underscore the need for more comprehensive evaluation methods to accurately assess their reasoning capabilities. Existing benchmarks are often domain-specific and thus cannot fully capture an LLM's general reasoning potential. To address this limitation, we introduce the Knowledge Orthogonal Reasoning Gymnasium (KORGym), a dynamic evaluation platform inspired by KOR-Bench and Gymnasium. KORGym offers over fifty games in either textual or visual formats and supports interactive, multi-turn assessments with reinforcement learning scenarios. Using KORGym, we conduct extensive experiments on 19 LLMs and 8 VLMs, revealing consistent reasoning patterns within model families and demonstrating the superior performance of closed-source models. Further analysis examines the effects of modality, reasoning strategies, reinforcement learning techniques, and response length on model performance. We expect KORGym to become a valuable resource for advancing LLM reasoning research and developing evaluation methodologies suited to complex, interactive environments.

</details>


### [143] [Pivot Language for Low-Resource Machine Translation](https://arxiv.org/abs/2505.14553)

*Abhimanyu Talwar, Julien Laasri*

**Main category:** cs.CL

**Keywords:** pivot language, translation, Hindi, Nepali, English

**Relevance Score:** 4

**TL;DR:** This paper explores using Hindi as a pivot language to translate Nepali into English, evaluating two methods: Transfer Method and Backtranslation.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of translating between languages with limited parallel corpora by leveraging a pivot language.

**Method:** The study uses a fully supervised Transfer Method and a semi-supervised Backtranslation approach to translate Nepali to English using Hindi as the pivot.

**Key Contributions:**

	1. Introduced Hindi as a pivot language for translation between Nepali and English.
	2. Demonstrated the effectiveness of the Transfer Method with a significant BLEU score improvement.
	3. Discussed potential reasons for performance gaps and outlined future research directions.

**Result:** The Transfer Method achieved a SacreBLEU score of 14.2, improving upon previous fully supervised scores, while the semi-supervised approach scored 15.1, though the authors discuss reasons for their under-performance.

**Limitations:** The results are slightly below the semi-supervised baseline score, indicating room for improvement in the methods used.

**Conclusion:** The findings highlight the effectiveness of using Hindi as a pivot for translation and identify areas for further research to improve results.

**Abstract:** Certain pairs of languages suffer from lack of a parallel corpus which is large in size and diverse in domain. One of the ways this is overcome is via use of a pivot language. In this paper we use Hindi as a pivot language to translate Nepali into English. We describe what makes Hindi a good candidate for the pivot. We discuss ways in which a pivot language can be used, and use two such approaches - the Transfer Method (fully supervised) and Backtranslation (semi-supervised) - to translate Nepali into English. Using the former, we are able to achieve a devtest Set SacreBLEU score of 14.2, which improves the baseline fully supervised score reported by (Guzman et al., 2019) by 6.6 points. While we are slightly below the semi-supervised baseline score of 15.1, we discuss what may have caused this under-performance, and suggest scope for future work.

</details>


### [144] [TRATES: Trait-Specific Rubric-Assisted Cross-Prompt Essay Scoring](https://arxiv.org/abs/2505.14577)

*Sohaila Eltanbouly, Salam Albatarni, Tamer Elsayed*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Trait assessment, Large Language Model

**Relevance Score:** 7

**TL;DR:** TRATES is a novel Automated Essay Scoring framework that assesses essays based on individual traits using LLM-generated features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of focus on assessing essays according to individual traits in existing Automated Essay Scoring systems.

**Method:** TRATES integrates a Large Language Model with trait grading rubrics to extract trait-specific features from essays, which are combined with generic writing-quality features to train a regression model for predicting trait scores.

**Key Contributions:**

	1. Introduction of a trait-specific AES framework
	2. Utilization of LLM for generating trait-specific features
	3. State-of-the-art performance on benchmark dataset

**Result:** TRATES achieves state-of-the-art performance across all traits on a widely-used dataset, demonstrating the significance of LLM-generated features.

**Limitations:** 

**Conclusion:** The results indicate that combining generic and trait-specific features can enhance the accuracy of essay scoring systems.

**Abstract:** Research on holistic Automated Essay Scoring (AES) is long-dated; yet, there is a notable lack of attention for assessing essays according to individual traits. In this work, we propose TRATES, a novel trait-specific and rubric-based cross-prompt AES framework that is generic yet specific to the underlying trait. The framework leverages a Large Language Model (LLM) that utilizes the trait grading rubrics to generate trait-specific features (represented by assessment questions), then assesses those features given an essay. The trait-specific features are eventually combined with generic writing-quality and prompt-specific features to train a simple classical regression model that predicts trait scores of essays from an unseen prompt. Experiments show that TRATES achieves a new state-of-the-art performance across all traits on a widely-used dataset, with the generated LLM-based features being the most significant.

</details>


### [145] [Can Pruning Improve Reasoning? Revisiting Long-CoT Compression with Capability in Mind for Better Reasoning](https://arxiv.org/abs/2505.14582)

*Shangziqi Zhao, Jiahao Yuan, Guisong Yang, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Long-CoT, LLM, pruning, self-verification, structure-aware

**Relevance Score:** 7

**TL;DR:** Long-CoT reasoning can improve LLM accuracy, but its verbosity complicates distillation into small models. This paper proposes Prune-on-Logic, a method that effectively prunes low-utility reasoning steps while enhancing accuracy and reducing costs, presenting a structural optimization for aligning reasoning with model capacity.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the distillation of Long-CoT reasoning into small language models by exploring the effects of pruning on reasoning steps.

**Method:** A framework called Prune-on-Logic is introduced, which transforms Long-CoT into logic graphs and prunes reasoning steps under self-verification constraints.

**Key Contributions:**

	1. Development of the Prune-on-Logic framework for reasoning step pruning.
	2. Demonstration of improved accuracy and cost efficiency through strategic pruning of verification steps.
	3. Insights into the effectiveness of semantic structure over length in chain-of-thought reasoning.

**Result:** Pruning verification steps leads to consistent accuracy gains and reduced inference costs, outperforming other approaches, while pruning reasoning or entire chains results in degraded performance.

**Limitations:** 

**Conclusion:** Pruning is a productive optimization strategy for aligning long chain-of-thought reasoning with the capabilities of small language models, emphasizing that shorter chains are not always beneficial.

**Abstract:** Long chain-of-thought (Long-CoT) reasoning improves accuracy in LLMs, yet its verbose, self-reflective style often hinders effective distillation into small language models (SLMs). We revisit Long-CoT compression through the lens of capability alignment and ask: Can pruning improve reasoning? We propose Prune-on-Logic, a structure-aware framework that transforms Long-CoT into logic graphs and selectively prunes low-utility reasoning steps under self-verification constraints. Through systematic analysis across three pruning strategies -- targeting entire chains, core reasoning, and verification -- we find that pruning verification steps yields consistent accuracy gains while reducing inference cost, outperforming token-level baselines and uncompressed fine-tuning. In contrast, pruning reasoning or all-chain steps degrades performance, revealing that small models benefit not from shorter CoTs, but from semantically leaner ones. Our findings highlight pruning as a structural optimization strategy for aligning CoT reasoning with SLM capacity.

</details>


### [146] [Context Reasoner: Incentivizing Reasoning Capability for Contextualized Privacy and Safety Compliance via Reinforcement Learning](https://arxiv.org/abs/2505.14585)

*Wenbin Hu, Haoran Li, Huihao Jing, Qi Hu, Ziqian Zeng, Sirui Han, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety, privacy, Contextual Integrity, reinforcement learning

**Relevance Score:** 8

**TL;DR:** The paper addresses safety and privacy risks in Large Language Models (LLMs) by framing these issues as contextualized compliance problems and employing a reinforcement learning approach to enhance compliance and reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs have significant safety and privacy risks that current mitigation strategies do not adequately address, often compromising contextual reasoning.

**Method:** The authors apply reinforcement learning (RL) with a rule-based reward system, aligned with Contextual Integrity (CI) theory, to improve compliance with regulations like GDPR and HIPAA while enhancing contextual reasoning.

**Key Contributions:**

	1. Formulation of safety and privacy issues as contextualized compliance problems using CI theory.
	2. Implementation of RL with a rule-based reward to enhance compliance and reasoning.
	3. Demonstrated accuracy improvements in AI models for safety/privacy benchmarks and reasoning tasks.

**Result:** The proposed method achieves a +17.64% improvement in legal compliance benchmarks and enhances reasoning capabilities, with OpenThinker-7B showing +2.05% and +8.98% accuracy improvements on MMLU and LegalBench, respectively.

**Limitations:** 

**Conclusion:** Incorporating RL in the compliance framework not only boosts legal adherence but also fortifies reasoning abilities in LLMs.

**Abstract:** While Large Language Models (LLMs) exhibit remarkable capabilities, they also introduce significant safety and privacy risks. Current mitigation strategies often fail to preserve contextual reasoning capabilities in risky scenarios. Instead, they rely heavily on sensitive pattern matching to protect LLMs, which limits the scope. Furthermore, they overlook established safety and privacy standards, leading to systemic risks for legal compliance. To address these gaps, we formulate safety and privacy issues into contextualized compliance problems following the Contextual Integrity (CI) theory. Under the CI framework, we align our model with three critical regulatory standards: GDPR, EU AI Act, and HIPAA. Specifically, we employ reinforcement learning (RL) with a rule-based reward to incentivize contextual reasoning capabilities while enhancing compliance with safety and privacy norms. Through extensive experiments, we demonstrate that our method not only significantly enhances legal compliance (achieving a +17.64% accuracy improvement in safety/privacy benchmarks) but also further improves general reasoning capability. For OpenThinker-7B, a strong reasoning model that significantly outperforms its base model Qwen2.5-7B-Instruct across diverse subjects, our method enhances its general reasoning capabilities, with +2.05% and +8.98% accuracy improvement on the MMLU and LegalBench benchmark, respectively.

</details>


### [147] [MCIP: Protecting MCP Safety via Model Contextual Integrity Protocol](https://arxiv.org/abs/2505.14590)

*Huihao Jing, Haoran Li, Wenbin Hu, Qi Hu, Heli Xu, Tianshu Chu, Peizhao Hu, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Model Context Protocol, safety mechanisms, LLMs, taxonomies, benchmark data

**Relevance Score:** 7

**TL;DR:** The paper proposes a framework to enhance safety in the Model Context Protocol (MCP) by addressing its vulnerabilities, introducing the Model Contextual Integrity Protocol (MCIP), and developing a taxonomy to evaluate unsafe behaviors in MCP interactions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The introduction of MCP has created a new ecosystem for users and developers, but its decentralized nature poses significant safety risks that need to be systematically analyzed and addressed.

**Method:** A novel framework guided by the MAESTRO framework is proposed, which involves analyzing the missing safety mechanisms in MCP, developing a fine-grained taxonomy for unsafe behaviors, and creating benchmark and training data to evaluate the safety performance of LLMs in MCP interactions.

**Key Contributions:**

	1. Introduction of Model Contextual Integrity Protocol (MCIP)
	2. Development of a fine-grained taxonomy for unsafe behaviors in MCP
	3. Creation of benchmark and training data for evaluating LLMs' safety in MCP interactions.

**Result:** Experiments on state-of-the-art LLMs reveal vulnerabilities in handling MCP interactions and demonstrate that the proposed framework and training data significantly improve the safety performance of these models.

**Limitations:** 

**Conclusion:** The proposed Model Contextual Integrity Protocol (MCIP) and accompanying mechanisms effectively enhance the safety of the Model Context Protocol framework and improve LLMs' ability to identify and manage safety risks.

**Abstract:** As Model Context Protocol (MCP) introduces an easy-to-use ecosystem for users and developers, it also brings underexplored safety risks. Its decentralized architecture, which separates clients and servers, poses unique challenges for systematic safety analysis. This paper proposes a novel framework to enhance MCP safety. Guided by the MAESTRO framework, we first analyze the missing safety mechanisms in MCP, and based on this analysis, we propose the Model Contextual Integrity Protocol (MCIP), a refined version of MCP that addresses these gaps.Next, we develop a fine-grained taxonomy that captures a diverse range of unsafe behaviors observed in MCP scenarios. Building on this taxonomy, we develop benchmark and training data that support the evaluation and improvement of LLMs' capabilities in identifying safety risks within MCP interactions. Leveraging the proposed benchmark and training data, we conduct extensive experiments on state-of-the-art LLMs. The results highlight LLMs' vulnerabilities in MCP interactions and demonstrate that our approach substantially improves their safety performance.

</details>


### [148] [Success is in the Details: Evaluate and Enhance Details Sensitivity of Code LLMs through Counterfactuals](https://arxiv.org/abs/2505.14597)

*Xianzhen Luo, Qingfu Zhu, Zhiming Zhang, Mingzheng Xu, Tianhao Cheng, Yixuan Wang, Zheng Chu, Shijie Xuyang, Zhiyuan Ma, YuanTao Fan, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** Code LLMs, Code Sensitivity, CTF-Code, CTF-Instruct, Fine-tuning

**Relevance Score:** 7

**TL;DR:** This paper introduces the CTF-Code benchmark to evaluate code sensitivity of LLMs and proposes CTF-Instruct, an incremental instruction fine-tuning framework to enhance LLMs' performance based on sensitivity metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked capability of Code LLMs in recognizing subtle changes in problem descriptions, which impacts their performance.

**Method:** The CTF-Code benchmark was created using counterfactual perturbations, focusing on minimizing input changes while maximizing output changes. CTF-Instruct framework was developed for fine-tuning LLMs by incorporating sensitivity metrics along with traditional difficulty and diversity measures.

**Key Contributions:**

	1. Introduction of the CTF-Code benchmark for evaluating code sensitivity in LLMs.
	2. Development of the CTF-Instruct fine-tuning framework for improving LLM performance based on sensitivity metrics.
	3. Demonstration of significant performance improvements through experiments on diverse coding benchmarks.

**Result:** LLMs fine-tuned with the CTF-Instruct data showed over a 2% improvement on the CTF-Code benchmark and more than a 10% increase on LiveCodeBench, demonstrating effective enhancement of sensitivity.

**Limitations:** 

**Conclusion:** Enhancing LLMs' sensitivity through targeted fine-tuning can significantly improve their performance on coding tasks.

**Abstract:** Code Sensitivity refers to the ability of Code LLMs to recognize and respond to details changes in problem descriptions. While current code benchmarks and instruction data focus on difficulty and diversity, sensitivity is overlooked. We first introduce the CTF-Code benchmark, constructed using counterfactual perturbations, minimizing input changes while maximizing output changes. The evaluation shows that many LLMs have a more than 10\% performance drop compared to the original problems. To fully utilize sensitivity, CTF-Instruct, an incremental instruction fine-tuning framework, extends on existing data and uses a selection mechanism to meet the three dimensions of difficulty, diversity, and sensitivity. Experiments show that LLMs fine-tuned with CTF-Instruct data achieve over a 2\% improvement on CTF-Code, and more than a 10\% performance boost on LiveCodeBench, validating the feasibility of enhancing LLMs' sensitivity to improve performance.

</details>


### [149] [Toward Reliable Biomedical Hypothesis Generation: Evaluating Truthfulness and Hallucination in Large Language Models](https://arxiv.org/abs/2505.14599)

*Guangzhi Xiong, Eric Xie, Corey Williams, Myles Kim, Amir Hassan Shariatmadari, Sikun Guo, Stefan Bekiranov, Aidong Zhang*

**Main category:** cs.CL

**Keywords:** large language models, truthful biomedical hypotheses, hallucination detection, scientific discovery, KnowHD

**Relevance Score:** 9

**TL;DR:** This paper introduces TruthHypo, a benchmark for assessing the capabilities of large language models (LLMs) in generating truthful biomedical hypotheses, alongside KnowHD, a hallucination detector to evaluate groundedness in these hypotheses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of evaluating the truthfulness of biomedical hypotheses generated by LLMs, which may often lead to inefficiencies and inaccuracies due to hallucinations.

**Method:** The authors propose TruthHypo, a benchmark for assessing LLM capabilities, and KnowHD, a detector for knowledge-based hallucinations, using them to analyze and filter hypotheses generated by LLMs based on groundedness scores.

**Key Contributions:**

	1. Introduction of TruthHypo benchmark for assessing LLMs in generating biomedical hypotheses
	2. Development of KnowHD, a knowledge-based hallucination detector
	3. Demonstration of KnowHD's efficacy in filtering truthful hypotheses based on groundedness scores.

**Result:** The study reveals that LLMs commonly generate untruthful hypotheses and highlights that KnowHD effectively filters truthful hypotheses, validated through human evaluations.

**Limitations:** The study is limited to biomedical hypotheses and may not generalize across other domains; additional validation and testing with broader datasets are required.

**Conclusion:** The findings underscore the limitations of LLMs in hypothesis generation, emphasizing the importance of tools like KnowHD for enhancing the reliability of AI-generated scientific ideas.

**Abstract:** Large language models (LLMs) have shown significant potential in scientific disciplines such as biomedicine, particularly in hypothesis generation, where they can analyze vast literature, identify patterns, and suggest research directions. However, a key challenge lies in evaluating the truthfulness of generated hypotheses, as verifying their accuracy often requires substantial time and resources. Additionally, the hallucination problem in LLMs can lead to the generation of hypotheses that appear plausible but are ultimately incorrect, undermining their reliability. To facilitate the systematic study of these challenges, we introduce TruthHypo, a benchmark for assessing the capabilities of LLMs in generating truthful biomedical hypotheses, and KnowHD, a knowledge-based hallucination detector to evaluate how well hypotheses are grounded in existing knowledge. Our results show that LLMs struggle to generate truthful hypotheses. By analyzing hallucinations in reasoning steps, we demonstrate that the groundedness scores provided by KnowHD serve as an effective metric for filtering truthful hypotheses from the diverse outputs of LLMs. Human evaluations further validate the utility of KnowHD in identifying truthful hypotheses and accelerating scientific discovery. Our data and source code are available at https://github.com/Teddy-XiongGZ/TruthHypo.

</details>


### [150] [sudoLLM : On Multi-role Alignment of Language Models](https://arxiv.org/abs/2505.14607)

*Soumadeep Saha, Akshay Chaturvedi, Joy Mahapatra, Utpal Garain*

**Main category:** cs.CL

**Keywords:** Large Language Models, User Authorization, Safety-Critical Systems, Access Control, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces sudoLLM, a framework for multi-role aligned LLMs that incorporates user authorization-based access control to improve safety and prevent jailbreaking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of user authorization-based access privileges in large language models (LLMs), which are crucial for safety-critical systems.

**Method:** The sudoLLM framework injects user-based biases into LLM queries and trains the model to respond with sensitive information only if the user is authorized.

**Key Contributions:**

	1. Introduction of user authorization in LLMs through sudoLLM framework
	2. Improved alignment and generalization in LLMs
	3. Enhanced safety against prompt-based attacks

**Result:** Empirical results show that sudoLLM significantly improves model alignment and generalization, while also providing resistance to prompt-based jailbreaking attacks.

**Limitations:** 

**Conclusion:** The integration of user authorization with LLMs enhances safety and serves as an additional layer of security alongside existing safety mechanisms.

**Abstract:** User authorization-based access privileges are a key feature in many safety-critical systems, but have thus far been absent from the large language model (LLM) realm. In this work, drawing inspiration from such access control systems, we introduce sudoLLM, a novel framework that results in multi-role aligned LLMs, i.e., LLMs that account for, and behave in accordance with, user access rights. sudoLLM injects subtle user-based biases into queries and trains an LLM to utilize this bias signal in order to produce sensitive information if and only if the user is authorized. We present empirical results demonstrating that this approach shows substantially improved alignment, generalization, and resistance to prompt-based jailbreaking attacks. The persistent tension between the language modeling objective and safety alignment, which is often exploited to jailbreak LLMs, is somewhat resolved with the aid of the injected bias signal. Our framework is meant as an additional security layer, and complements existing guardrail mechanisms for enhanced end-to-end safety with LLMs.

</details>


### [151] [Language Models Optimized to Fool Detectors Still Have a Distinct Style (And How to Change It)](https://arxiv.org/abs/2505.14608)

*Rafael Rivera Soto, Barry Chen, Nicholas Andrews*

**Main category:** cs.CL

**Keywords:** machine-generated text, text detection, stylistic features, AURA metric, paraphrasing approach

**Relevance Score:** 7

**TL;DR:** This paper examines the challenges of detecting machine-generated text and presents a robust detection method using stylistic features, which remain effective even against optimized models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the inherent difficulty of reliably detecting machine-generated text and to evaluate recent claims about machine-text detectors' vulnerabilities.

**Method:** The study explores the robustness of a stylistic feature space for detecting machine-generated text and introduces AURA, a metric for analyzing the overlap between human and machine-generated distributions as sample sizes increase.

**Key Contributions:**

	1. Introduction of a stylistic feature space that proves resilient to optimization efforts by language models.
	2. Development of the AURA metric for evaluating detection performance based on sample sizes.
	3. Demonstration that stylistic detectors maintain their effectiveness against specially optimized machine-generated texts.

**Result:** The findings indicate that stylistic detectors maintain consistent performance even when models are optimized against them. Furthermore, detection performance improves as more samples are analyzed, allowing for better differentiation between human and machine-generated text.

**Limitations:** The study primarily focuses on stylistic features and may not address other potential methods of detection, limiting the scope of its applicability.

**Conclusion:** The results suggest that reliance on machine-text detection is problematic and that stylistic features offer a more robust detection approach, particularly as sample size increases.

**Abstract:** Despite considerable progress in the development of machine-text detectors, it has been suggested that the problem is inherently hard, and therefore, that stakeholders should proceed under the assumption that machine-generated text cannot be reliably detected as such. We examine a recent such claim by Nicks et al. (2024) regarding the ease with which language models can be optimized to degrade the performance of machine-text detectors, including detectors not specifically optimized against. We identify a feature space$\unicode{x2013}$the stylistic feature space$\unicode{x2013}$that is robust to such optimization, and show that it may be used to reliably detect samples from language models optimized to prevent detection. Furthermore, we show that even when models are explicitly optimized against stylistic detectors, detection performance remains surprisingly unaffected. We then seek to understand if stylistic detectors are inherently more robust. To study this question, we explore a new paraphrasing approach that simultaneously aims to close the gap between human writing and machine writing in stylistic feature space while avoiding detection using traditional features. We show that when only a single sample is available for detection, this attack is universally effective across all detectors considered, including those that use writing style. However, as the number of samples available for detection grows, the human and machine distributions become distinguishable. This observation encourages us to introduce AURA, a metric that estimates the overlap between human and machine-generated distributions by analyzing how detector performance improves as more samples become available. Overall, our findings underscore previous recommendations to avoid reliance on machine-text detection.

</details>


### [152] [Linear Control of Test Awareness Reveals Differential Compliance in Reasoning Models](https://arxiv.org/abs/2505.14617)

*Sahar Abdelnabi, Ahmed Salem*

**Main category:** cs.CL

**Keywords:** Large Language Models, Test Awareness, Safety Alignment, AI Evaluation, Reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates the influence of test awareness on the behavior and safety alignment of reasoning-focused large language models, introducing a framework for probing and controlling this effect.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the phenomenon where large language models alter their performance when aware of being evaluated, which can lead to unsafe behaviors and test optimization.

**Method:** A white-box probing framework is developed to identify and steer test awareness in models while monitoring their performance on tasks.

**Key Contributions:**

	1. First quantitative study on the impact of test awareness in LLMs
	2. Development of a probing framework for test awareness control
	3. Demonstration of varied safety alignment effects across models

**Result:** The study shows that test awareness significantly affects safety alignment in different reasoning LLMs, varying across models and task settings.

**Limitations:** The study is limited to specific state-of-the-art open-source reasoning LLMs and might not generalize to all models.

**Conclusion:** The findings underscore the importance of understanding test awareness in model evaluation to enhance safety and trust in AI systems.

**Abstract:** Reasoning-focused large language models (LLMs) sometimes alter their behavior when they detect that they are being evaluated, an effect analogous to the Hawthorne phenomenon, which can lead them to optimize for test-passing performance or to comply more readily with harmful prompts if real-world consequences appear absent. We present the first quantitative study of how such "test awareness" impacts model behavior, particularly its safety alignment. We introduce a white-box probing framework that (i) linearly identifies awareness-related activations and (ii) steers models toward or away from test awareness while monitoring downstream performance. We apply our method to different state-of-the-art open-source reasoning LLMs across both realistic and hypothetical tasks. Our results demonstrate that test awareness significantly impact safety alignment, and is different for different models. By providing fine-grained control over this latent effect, our work aims to increase trust in how we perform safety evaluation.

</details>


### [153] [Think Only When You Need with Large Hybrid-Reasoning Models](https://arxiv.org/abs/2505.14631)

*Lingjie Jiang, Xun Wu, Shaohan Huang, Qingxiu Dong, Zewen Chi, Li Dong, Xingxing Zhang, Tengchao Lv, Lei Cui, Furu Wei*

**Main category:** cs.CL

**Keywords:** Hybrid Reasoning Models, Large Language Models, Reinforcement Learning

**Relevance Score:** 8

**TL;DR:** This paper presents Large Hybrid-Reasoning Models (LHRMs) that adaptively select reasoning methods based on user query context, improving efficiency in processing queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in token usage and response latency caused by extended reasoning in Large Reasoning Models (LRMs) when handling simple queries.

**Method:** The authors propose a two-stage training pipeline combining Hybrid Fine-Tuning for a cold start and online reinforcement learning using Hybrid Group Policy Optimization (HGPO) to learn adaptive reasoning strategies.

**Key Contributions:**

	1. Introduction of Large Hybrid-Reasoning Models (LHRMs) that adapt reasoning based on query context.
	2. Development of a two-stage training pipeline combining Hybrid Fine-Tuning and reinforcement learning.
	3. Proposal of a new metric, Hybrid Accuracy, to evaluate hybrid thinking capabilities.

**Result:** LHRMs can effectively adapt their reasoning approach based on query complexity, outperforming existing LLMs and LRMs in efficiency and reasoning capabilities.

**Limitations:** 

**Conclusion:** The study emphasizes the need to balance reasoning depth with efficiency and offers a framework for developing future hybrid reasoning systems.

**Abstract:** Recent Large Reasoning Models (LRMs) have shown substantially improved reasoning capabilities over traditional Large Language Models (LLMs) by incorporating extended thinking processes prior to producing final responses. However, excessively lengthy thinking introduces substantial overhead in terms of token consumption and latency, which is particularly unnecessary for simple queries. In this work, we introduce Large Hybrid-Reasoning Models (LHRMs), the first kind of model capable of adaptively determining whether to perform thinking based on the contextual information of user queries. To achieve this, we propose a two-stage training pipeline comprising Hybrid Fine-Tuning (HFT) as a cold start, followed by online reinforcement learning with the proposed Hybrid Group Policy Optimization (HGPO) to implicitly learn to select the appropriate thinking mode. Furthermore, we introduce a metric called Hybrid Accuracy to quantitatively assess the model's capability for hybrid thinking. Extensive experimental results show that LHRMs can adaptively perform hybrid thinking on queries of varying difficulty and type. It outperforms existing LRMs and LLMs in reasoning and general capabilities while significantly improving efficiency. Together, our work advocates for a reconsideration of the appropriate use of extended thinking processes and provides a solid starting point for building hybrid thinking systems.

</details>


### [154] [Will AI Tell Lies to Save Sick Children? Litmus-Testing AI Values Prioritization with AIRiskDilemmas](https://arxiv.org/abs/2505.14633)

*Yu Ying Chiu, Zhilin Wang, Sharan Maiya, Yejin Choi, Kyle Fish, Sydney Levine, Evan Hubinger*

**Main category:** cs.CL

**Keywords:** AI safety, value prioritization, risk detection, machine learning, human values

**Relevance Score:** 7

**TL;DR:** The paper introduces LitmusValues, a pipeline for evaluating AI models' value priorities as a method to predict and assess AI risks, particularly in unsafe scenarios.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The emergence of stronger AI models and techniques like Alignment Faking necessitates better methods to detect AI risks. Identifying underlying values in AI can serve as an early warning system for risky behaviors.

**Method:** The authors developed a pipeline called LitmusValues to evaluate the values prioritized by AI models. They created AIRiskDilemmas, a set of scenarios that test the AI's value prioritization against potential safety risks.

**Key Contributions:**

	1. Introduction of LitmusValues evaluation pipeline
	2. Creation of AIRiskDilemmas for testing value prioritization
	3. Demonstration of value metrics predicting risky AI behaviors

**Result:** The study found that the values identified through LitmusValues could predict both known risky behaviors in AIRiskDilemmas and previously unseen risky behaviors in another dataset, HarmBench.

**Limitations:** The effectiveness and comprehensiveness of the value classes in LitmusValues might be limited by the diversity and representation of the dilemmas used for evaluation.

**Conclusion:** Understanding AI models' value prioritization can provide significant insights into predicting and mitigating their risky behaviors.

**Abstract:** Detecting AI risks becomes more challenging as stronger models emerge and find novel methods such as Alignment Faking to circumvent these detection attempts. Inspired by how risky behaviors in humans (i.e., illegal activities that may hurt others) are sometimes guided by strongly-held values, we believe that identifying values within AI models can be an early warning system for AI's risky behaviors. We create LitmusValues, an evaluation pipeline to reveal AI models' priorities on a range of AI value classes. Then, we collect AIRiskDilemmas, a diverse collection of dilemmas that pit values against one another in scenarios relevant to AI safety risks such as Power Seeking. By measuring an AI model's value prioritization using its aggregate choices, we obtain a self-consistent set of predicted value priorities that uncover potential risks. We show that values in LitmusValues (including seemingly innocuous ones like Care) can predict for both seen risky behaviors in AIRiskDilemmas and unseen risky behaviors in HarmBench.

</details>


### [155] [General-Reasoner: Advancing LLM Reasoning Across All Domains](https://arxiv.org/abs/2505.14652)

*Xueguang Ma, Qian Liu, Dongfu Jiang, Ge Zhang, Zejun Ma, Wenhu Chen*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Reasoning, Dataset Curation, Generative Models

**Relevance Score:** 8

**TL;DR:** This paper presents General-Reasoner, a novel paradigm for enhancing LLM reasoning capabilities across various domains by utilizing a large-scale dataset and a generative model for answer verification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current LLM reasoning approaches that mainly focus on limited domains like mathematics and coding, thereby restricting generalization to diverse question-answer representations.

**Method:** Developing a large-scale, high-quality dataset of diverse questions with verifiable answers through web crawling, and implementing a generative model-based answer verifier to enable robust reasoning capabilities.

**Key Contributions:**

	1. Construction of a large-scale, high-quality dataset of diverse questions and answers.
	2. Development of a generative model-based answer verifier for enhanced reasoning capabilities.

**Result:** General-Reasoner outperforms existing baseline methods across 12 benchmarks, demonstrating strong and generalizable reasoning performance, particularly in mathematical reasoning tasks.

**Limitations:** 

**Conclusion:** The proposed General-Reasoner significantly enhances LLM reasoning across a breadth of disciplines, moving beyond traditional narrow domain applicability.

**Abstract:** Reinforcement learning (RL) has recently demonstrated strong potential in enhancing the reasoning capabilities of large language models (LLMs). Particularly, the "Zero" reinforcement learning introduced by Deepseek-R1-Zero, enables direct RL training of base LLMs without relying on an intermediate supervised fine-tuning stage. Despite these advancements, current works for LLM reasoning mainly focus on mathematical and coding domains, largely due to data abundance and the ease of answer verification. This limits the applicability and generalization of such models to broader domains, where questions often have diverse answer representations, and data is more scarce. In this paper, we propose General-Reasoner, a novel training paradigm designed to enhance LLM reasoning capabilities across diverse domains. Our key contributions include: (1) constructing a large-scale, high-quality dataset of questions with verifiable answers curated by web crawling, covering a wide range of disciplines; and (2) developing a generative model-based answer verifier, which replaces traditional rule-based verification with the capability of chain-of-thought and context-awareness. We train a series of models and evaluate them on a wide range of datasets covering wide domains like physics, chemistry, finance, electronics etc. Our comprehensive evaluation across these 12 benchmarks (e.g. MMLU-Pro, GPQA, SuperGPQA, TheoremQA, BBEH and MATH AMC) demonstrates that General-Reasoner outperforms existing baseline methods, achieving robust and generalizable reasoning performance while maintaining superior effectiveness in mathematical reasoning tasks.

</details>


### [156] [EmoGist: Efficient In-Context Learning for Visual Emotion Understanding](https://arxiv.org/abs/2505.14660)

*Ronald Seoh, Dan Goldwasser*

**Main category:** cs.CL

**Keywords:** visual emotion classification, context-dependent labels, in-context learning

**Relevance Score:** 7

**TL;DR:** EmoGist is a training-free method for visual emotion classification using context-dependent emotion labels to improve prediction accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of emotion classification in images by utilizing context-dependent definitions of emotion labels.

**Method:** EmoGist pre-generates explanations of emotion labels by analyzing clusters of example images. At test time, it retrieves context-specific explanations based on embedding similarity and uses them for classification with a visual language model (VLM).

**Key Contributions:**

	1. Introduction of a training-free in-context learning method for emotion classification.
	2. Use of context-dependent emotion label explanations to enhance classification accuracy.
	3. Demonstrated performance improvements on benchmark emotion classification datasets.

**Result:** EmoGist improves micro F1 scores by up to 13 points on the multi-label Memotion dataset and macro F1 scores by up to 8 points on the multi-class FI dataset.

**Limitations:** 

**Conclusion:** The EmoGist method demonstrates significant improvements in emotion classification through context-aware explanations, showing potential for better performance in visual emotion recognition tasks.

**Abstract:** In this paper, we introduce EmoGist, a training-free, in-context learning method for performing visual emotion classification with LVLMs. The key intuition of our approach is that context-dependent definition of emotion labels could allow more accurate predictions of emotions, as the ways in which emotions manifest within images are highly context dependent and nuanced. EmoGist pre-generates multiple explanations of emotion labels, by analyzing the clusters of example images belonging to each category. At test time, we retrieve a version of explanation based on embedding similarity, and feed it to a fast VLM for classification. Through our experiments, we show that EmoGist allows up to 13 points improvement in micro F1 scores with the multi-label Memotion dataset, and up to 8 points in macro F1 in the multi-class FI dataset.

</details>


### [157] [Reward Reasoning Model](https://arxiv.org/abs/2505.14674)

*Jiaxin Guo, Zewen Chi, Li Dong, Qingxiu Dong, Xun Wu, Shaohan Huang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Reward Models, Reinforcement Learning, Chain-of-Thought Reasoning

**Relevance Score:** 7

**TL;DR:** This paper introduces Reward Reasoning Models (RRMs) that enhance reward model performance through chain-of-thought reasoning and adaptive use of test-time compute.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of reward models in guiding large language models by utilizing test-time compute effectively.

**Method:** The authors develop RRMs within a reinforcement learning framework, enabling them to reason and generate rewards without needing explicit training data for reasoning.

**Key Contributions:**

	1. Introduction of Reward Reasoning Models (RRMs) for enhanced reward modeling
	2. Use of chain-of-thought reasoning for better performance
	3. Demonstration of adaptive exploitation of test-time compute in reward generation.

**Result:** RRMs show superior performance on various reward modeling benchmarks, effectively utilizing additional test-time compute to enhance reward accuracy.

**Limitations:** 

**Conclusion:** The adaptive nature of RRMs allows for improved reward modeling, making them a promising approach for aligning outputs with human expectations.

**Abstract:** Reward models play a critical role in guiding large language models toward outputs that align with human expectations. However, an open challenge remains in effectively utilizing test-time compute to enhance reward model performance. In this work, we introduce Reward Reasoning Models (RRMs), which are specifically designed to execute a deliberate reasoning process before generating final rewards. Through chain-of-thought reasoning, RRMs leverage additional test-time compute for complex queries where appropriate rewards are not immediately apparent. To develop RRMs, we implement a reinforcement learning framework that fosters self-evolved reward reasoning capabilities without requiring explicit reasoning traces as training data. Experimental results demonstrate that RRMs achieve superior performance on reward modeling benchmarks across diverse domains. Notably, we show that RRMs can adaptively exploit test-time compute to further improve reward accuracy. The pretrained reward reasoning models are available at https://huggingface.co/Reward-Reasoning.

</details>


### [158] [UltraEdit: Training-, Subject-, and Memory-Free Lifelong Editing in Large Language Models](https://arxiv.org/abs/2505.14679)

*Xiaojie Gu, Guangxu Chen, Jungang Li, Jia-Chen Gu, Xuming Hu, Kai Zhang*

**Main category:** cs.CL

**Keywords:** lifelong learning, model editing, large language models

**Relevance Score:** 9

**TL;DR:** ULTRAEDIT is a new model editing solution for large language models that enables efficient and scalable lifelong learning without requiring training or memory, achieving significant speed and resource efficiency improvements.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient lifelong learning in large language models that can adapt to new information while preserving existing knowledge and ensuring reliable performance.

**Method:** ULTRAEDIT performs model editing using lightweight linear algebra operations to compute parameter shifts and employs a lifelong normalization strategy to adapt to distributional shifts.

**Key Contributions:**

	1. Development of ULTRAEDIT, a training-, subject- and memory-free model editing solution.
	2. Creation of ULTRAEDITBENCH, the largest editing dataset with over 2M pairs.
	3. Achieving 7x faster editing speeds and reduced VRAM consumption compared to current state-of-the-art methods.

**Result:** ULTRAEDIT is over 7x faster than previous methods with only 1/3 the VRAM usage, able to edit a 7B LLM on a consumer-grade GPU, and supports up to 1M edits while maintaining high accuracy.

**Limitations:** 

**Conclusion:** ULTRAEDIT demonstrates superior performance in model editing across multiple datasets and models, addressing practical challenges in lifelong adaptation.

**Abstract:** Lifelong learning enables large language models (LLMs) to adapt to evolving information by continually updating their internal knowledge. An ideal system should support efficient, wide-ranging updates while preserving existing capabilities and ensuring reliable deployment. Model editing stands out as a promising solution for this goal, offering a focused and efficient way to revise a model's internal knowledge. Although recent paradigms have made notable progress, they often struggle to meet the demands of practical lifelong adaptation at scale. To bridge this gap, we propose ULTRAEDIT-a fundamentally new editing solution that is training-, subject- and memory-free, making it particularly well-suited for ultra-scalable, real-world lifelong model editing. ULTRAEDIT performs editing through a self-contained process that relies solely on lightweight linear algebra operations to compute parameter shifts, enabling fast and consistent parameter modifications with minimal overhead. To improve scalability in lifelong settings, ULTRAEDIT employs a lifelong normalization strategy that continuously updates feature statistics across turns, allowing it to adapt to distributional shifts and maintain consistency over time. ULTRAEDIT achieves editing speeds over 7x faster than the previous state-of-the-art method-which was also the fastest known approach-while consuming less than 1/3 the VRAM, making it the only method currently capable of editing a 7B LLM on a 24GB consumer-grade GPU. Furthermore, we construct ULTRAEDITBENCH-the largest dataset in the field to date, with over 2M editing pairs-and demonstrate that our method supports up to 1M edits while maintaining high accuracy. Comprehensive experiments on four datasets and six models show that ULTRAEDIT consistently achieves superior performance across diverse model editing scenarios. Our code is available at: https://github.com/XiaojieGu/UltraEdit.

</details>


### [159] [Mind the Gap: Bridging Thought Leap for Improved Chain-of-Thought Tuning](https://arxiv.org/abs/2505.14684)

*Haolei Xu, Yuchen Yan, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Shengpei Jiang, Kaitao Song, Weiming Lu, Jun Xiao, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Language Models, Mathematical Reasoning, Dataset Creation, Cohesion

**Relevance Score:** 8

**TL;DR:** This paper introduces CoT Thought Leap Bridge Task to address missing steps in Chain-of-Thought reasoning for LLMs by automatically detecting leaps and generating intermediate steps, using a specialized dataset for training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing mathematical CoT datasets suffer from Thought Leaps caused by omitted intermediate reasoning steps, negatively impacting model performance.

**Method:** The authors propose the CoT Thought Leap Bridge Task and construct the ScaleQM+ dataset to train the CoT-Bridge model, which generates missing reasoning steps and enhances dataset completeness.

**Key Contributions:**

	1. Introduction of CoT Thought Leap Bridge Task
	2. Creation of ScaleQM+ dataset for training
	3. Demonstrated improvement in model performance on mathematical reasoning tasks

**Result:** Models fine-tuned on bridged datasets outperform those trained on original datasets by up to +5.87% on NuminaMath and show improvements in distilled data and reinforcement learning applications.

**Limitations:** 

**Conclusion:** Enhancing reasoning completeness improves model generalization and performance on diverse reasoning tasks, with CoT-Bridge functioning as a useful module for various optimization techniques.

**Abstract:** Large language models (LLMs) have achieved remarkable progress on mathemati-cal tasks through Chain-of-Thought (CoT) reasoning. However, existing mathematical CoT datasets often suffer from Thought Leaps due to experts omitting intermediate steps, which negatively impacts model learning and generalization. We propose the CoT Thought Leap Bridge Task, which aims to automatically detect leaps and generate missing intermediate reasoning steps to restore the completeness and coherence of CoT. To facilitate this, we constructed a specialized training dataset called ScaleQM+, based on the structured ScaleQuestMath dataset, and trained CoT-Bridge to bridge thought leaps. Through comprehensive experiments on mathematical reasoning benchmarks, we demonstrate that models fine-tuned on bridged datasets consistently outperform those trained on original datasets, with improvements of up to +5.87% on NuminaMath. Our approach effectively enhances distilled data (+3.02%) and provides better starting points for reinforcement learning (+3.1%), functioning as a plug-and-play module compatible with existing optimization techniques. Furthermore, CoT-Bridge demonstrate improved generalization to out-of-domain logical reasoning tasks, confirming that enhancing reasoning completeness yields broadly applicable benefits.

</details>


### [160] [Language Models use Lookbacks to Track Beliefs](https://arxiv.org/abs/2505.14685)

*Nikhil Prakash, Natalie Shapira, Arnab Sen Sharma, Christoph Riedl, Yonatan Belinkov, Tamar Rott Shaham, David Bau, Atticus Geiger*

**Main category:** cs.CL

**Keywords:** language models, Theory of Mind, belief representation, lookback mechanism

**Relevance Score:** 8

**TL;DR:** This paper investigates how language models represent characters' beliefs, focusing on the Theory of Mind (ToM) capabilities of Llama-3-70B-Instruct.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how language models represent beliefs is crucial for advancing their ToM capabilities and improving interaction dynamics in AI applications.

**Method:** The authors analyze the LM's reasoning through causal mediation and abstraction, constructing a dataset of stories with two characters altering states of objects. They identify a lookback mechanism for binding and retrieving character beliefs and state information.

**Key Contributions:**

	1. Introduced a lookback mechanism in LMs for belief tracking.
	2. Developed a unique dataset for evaluating character belief reasoning.
	3. Examined the influence of visibility on character beliefs.

**Result:** The study reveals that the LM uses a lookback mechanism to recall essential information about the characters' beliefs and actions effectively, incorporating visibility relations for belief updates.

**Limitations:** 

**Conclusion:** The findings provide insights into the LM's mechanisms for tracking beliefs, contributing to the understanding of ToM reasoning in language models.

**Abstract:** How do language models (LMs) represent characters' beliefs, especially when those beliefs may differ from reality? This question lies at the heart of understanding the Theory of Mind (ToM) capabilities of LMs. We analyze Llama-3-70B-Instruct's ability to reason about characters' beliefs using causal mediation and abstraction. We construct a dataset that consists of simple stories where two characters each separately change the state of two objects, potentially unaware of each other's actions. Our investigation uncovered a pervasive algorithmic pattern that we call a lookback mechanism, which enables the LM to recall important information when it becomes necessary. The LM binds each character-object-state triple together by co-locating reference information about them, represented as their Ordering IDs (OIs) in low rank subspaces of the state token's residual stream. When asked about a character's beliefs regarding the state of an object, the binding lookback retrieves the corresponding state OI and then an answer lookback retrieves the state token. When we introduce text specifying that one character is (not) visible to the other, we find that the LM first generates a visibility ID encoding the relation between the observing and the observed character OIs. In a visibility lookback, this ID is used to retrieve information about the observed character and update the observing character's beliefs. Our work provides insights into the LM's belief tracking mechanisms, taking a step toward reverse-engineering ToM reasoning in LMs.

</details>


### [161] [Arithmetics-Based Decomposition of Numeral Words -- Arithmetic Conditions give the Unpacking Strategy](https://arxiv.org/abs/2312.10097)

*Isidor Konrad Maier, Matthias Wolff*

**Main category:** cs.CL

**Keywords:** numeral decomposition, grammar induction, Hurford's Packing Strategy, linguistic attributes, arithmetic criteria

**Relevance Score:** 3

**TL;DR:** The paper introduces a numeral decomposer based on Hurford's Packing Strategy, which decomposes numerals using specific arithmetic criteria and applies it to grammar induction in 273 languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore a novel method for numeral decomposition that does not rely on base-10 assumptions, enhancing the understanding of numeral structures in linguistics.

**Method:** The methodology involves unpacking numerals into their factors and summands based on criteria derived from Hurford's Packing Strategy, followed by applying this decomposer for grammar induction in various languages.

**Key Contributions:**

	1. Introduction of a numeral decomposer based on Hurford's Packing Strategy.
	2. Demonstration of effective grammar induction in 273 languages using the decomposer.
	3. Comparison of induced grammars showing improvements over state-of-the-art tools.

**Result:** The resultant grammars from the numeral decomposer exhibit sensible mathematical attributes, are often close to expert-made grammars, and are more compact than those induced by a state-of-the-art tool.

**Limitations:** Some incorrect mathematical attributes were identified, often related to linguistic peculiarities like context sensitivity.

**Conclusion:** The study demonstrates the potential of the numeral decomposer for effective grammar induction while also addressing some cases of incorrect mathematical attribute induction linked to context sensitivity.

**Abstract:** This paper presents a novel numeral decomposer based on arithmetic criteria. The criteria are not dependent on a base-10 assumption but only on Hurford's Packing Strategy. Hurford's Packing Strategy constitutes numerals by packing factors and summands to multiplicators. We found out that a numeral of value n has a multiplicator larger than sqrt(n), a summand smaller than n/2 and a factor smaller than sqrt(n). Using these findings, the numeral decomposer attempts to detect and unpack factors and summand in order to reverse Hurford's Packing strategy. We tested its applicability for incremental unsupervised grammar induction in 273 languages. This way, grammars were obtained with sensible mathematical attributes that explain the structure of produced numerals. The numeral-decomposer-induced grammars are often close to expert-made and more compact than numeral grammars induced by a modern state-of-the-art grammar induction tool. Furthermore, this paper contains a report about the few cases of incorrect induced mathematical attributes, which are often linked to linguistic peculiarities like context sensitivity.

</details>


### [162] [Don't Half-listen: Capturing Key-part Information in Continual Instruction Tuning](https://arxiv.org/abs/2403.10056)

*Yongquan He, Wenyuan Zhang, Xuancheng Huang, Peng Zhang*

**Main category:** cs.CL

**Keywords:** continual instruction tuning, catastrophic forgetting, large language models, information gain, performance metrics

**Relevance Score:** 9

**TL;DR:** This paper proposes a continual instruction tuning method for large language models that alleviates catastrophic forgetting by evaluating key-part information gain to improve data replay and refine training objectives.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of catastrophic forgetting in continual instruction tuning for large language models, ensuring models retain previous knowledge while adapting to new tasks.

**Method:** The authors introduce a method based on Key-part Information Gain (KPIG) to dynamically replay data and adjust training objectives, allowing models to focus on task-relevant information.

**Key Contributions:**

	1. Novel approach using Key-part Information Gain for continual instruction tuning
	2. Development of P-score and V-score metrics for evaluating model performance
	3. Empirical results demonstrating improved performance over existing methods

**Result:** Experiments indicate that the proposed method outperforms existing approaches, demonstrating improved performance in both seen and unseen tasks.

**Limitations:** 

**Conclusion:** The KPIG-based method shows promise in enhancing continual instruction tuning by mitigating catastrophic forgetting and improving model generalization and instruction-following capabilities.

**Abstract:** Instruction tuning for large language models (LLMs) can drive them to produce results consistent with human goals in specific downstream tasks. However, the process of continual instruction tuning (CIT) for LLMs may bring about the catastrophic forgetting (CF) problem, where previously learned abilities are degraded. Recent methods try to alleviate the CF problem by modifying models or replaying data, which may only remember the surface-level pattern of instructions and get confused on held-out tasks. In this paper, we propose a novel continual instruction tuning method based on Key-part Information Gain (KPIG). Our method computes the information gain on masked parts to dynamically replay data and refine the training objective, which enables LLMs to capture task-aware information relevant to the correct response and alleviate overfitting to general descriptions in instructions. In addition, we propose two metrics, P-score and V-score, to measure the generalization and instruction-following abilities of LLMs. Experiments demonstrate our method achieves superior performance on both seen and held-out tasks.

</details>


### [163] [PLAYER*: Enhancing LLM-based Multi-Agent Communication and Interaction in Murder Mystery Games](https://arxiv.org/abs/2404.17662)

*Qinglin Zhu, Runcong Zhao, Bin Liang, Jinhua Du, Lin Gui, Yulan He*

**Main category:** cs.CL

**Keywords:** Multi-agent systems, Conversation inference, Large Language Models, Murder Mystery Games, Reasoning agents

**Relevance Score:** 6

**TL;DR:** Introduction of WellPlay dataset and PLAYER* framework for reasoning in murder mystery games.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a robust benchmark for evaluating reasoning abilities of agents in multi-agent conversational settings and to improve reasoning accuracy and interaction in MMGs.

**Method:** Development of WellPlay dataset consisting of 1,482 inferential questions and PLAYER*, a framework utilizing sensor-based state representation and information-driven strategies for agent operations.

**Key Contributions:**

	1. Creation of the WellPlay reasoning dataset
	2. Introduction of PLAYER* framework for LLM agents
	3. Demonstrated performance improvements in reasoning accuracy and efficiency

**Result:** PLAYER* demonstrates improved reasoning accuracy and efficiency compared to existing methods, enhancing agent-human interactions in MMGs.

**Limitations:** 

**Conclusion:** Journey toward advancing reasoning capabilities of agents in complex social interactions through improved benchmarks and frameworks.

**Abstract:** We introduce WellPlay, a reasoning dataset for multi-agent conversational inference in Murder Mystery Games (MMGs). WellPlay comprises 1,482 inferential questions across 12 games, spanning objectives, reasoning, and relationship understanding, and establishes a systematic benchmark for evaluating agent reasoning abilities in complex social settings. Building on this foundation, we present PLAYER*, a novel framework for Large Language Model (LLM)-based agents in MMGs. MMGs pose unique challenges, including undefined state spaces, absent intermediate rewards, and the need for strategic reasoning through natural language. PLAYER* addresses these challenges with a sensor-based state representation and an information-driven strategy that optimises questioning and suspect pruning. Experiments show that PLAYER* outperforms existing methods in reasoning accuracy, efficiency, and agent-human interaction, advancing reasoning agents for complex social scenarios.

</details>


### [164] [Turning Up the Heat: Min-p Sampling for Creative and Coherent LLM Outputs](https://arxiv.org/abs/2407.01082)

*Minh Nguyen, Andrew Baker, Clement Neo, Allen Roush, Andreas Kirsch, Ravid Shwartz-Ziv*

**Main category:** cs.CL

**Keywords:** min-p sampling, large language models, text generation, sampling methods, AI creativity

**Relevance Score:** 8

**TL;DR:** This paper introduces min-p sampling, a method that improves the quality and diversity of text generated by LLMs by dynamically adjusting sampling thresholds based on model confidence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing sampling methods like top-p (nucleus sampling) in generating coherent and diverse text, especially at higher temperatures.

**Method:** Min-p sampling dynamically truncates the sampling threshold according to the model’s confidence, utilizing the probability of the top token as a scaling factor.

**Key Contributions:**

	1. Introduction of min-p sampling as a new sampling method for LLMs
	2. Demonstrated improvements in text quality and diversity across different benchmarks
	3. Adoption of min-p sampling in popular open-source frameworks

**Result:** Experiments demonstrate that min-p sampling enhances both text quality and diversity across various benchmarks and model families, with human evaluations favoring this method.

**Limitations:** 

**Conclusion:** Min-p sampling significantly impacts text generation by improving coherence and creativity, and has been adopted in major open-source LLM frameworks.

**Abstract:** Large Language Models (LLMs) generate text by sampling the next token from a probability distribution over the vocabulary at each decoding step. Popular sampling methods like top-p (nucleus sampling) often struggle to balance quality and diversity, especially at higher temperatures which lead to incoherent or repetitive outputs. We propose min-p sampling, a dynamic truncation method that adjusts the sampling threshold based on the model's confidence by using the top token's probability as a scaling factor. Our experiments on benchmarks including GPQA, GSM8K, and AlpacaEval Creative Writing show that min-p sampling improves both the quality and diversity of generated text across different model families (Mistral and Llama 3) and model sizes (1B to 123B parameters), especially at higher temperatures. Human evaluations further show a clear preference for min-p sampling, in both text quality and creativity. Min-p sampling has been adopted by popular open-source LLM frameworks, including Hugging Face Transformers, VLLM, and many others, highlighting its considerable impact on improving text generation quality.

</details>


### [165] [PersonaGym: Evaluating Persona Agents and LLMs](https://arxiv.org/abs/2407.18416)

*Vinay Samuel, Henry Peng Zou, Yue Zhou, Shreyas Chaudhari, Ashwin Kalyan, Tanmay Rajpurohit, Ameet Deshpande, Karthik Narasimhan, Vishvak Murahari*

**Main category:** cs.CL

**Keywords:** persona agents, evaluation framework, language models, healthcare, education

**Relevance Score:** 8

**TL;DR:** Introduction of PersonaGym framework and PersonaScore metric for evaluating persona agents using LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of evaluating how persona agents adhere to their assigned personas in various domains, particularly in free-form interaction settings.

**Method:** Developed the PersonaGym framework and PersonaScore metric grounded in decision theory to enable large-scale evaluation of persona agents.

**Key Contributions:**

	1. Introduction of PersonaGym as an evaluation framework for persona agents
	2. Development of PersonaScore as an automatic metric based on decision theory
	3. Large-scale evaluation revealing parity in performance across different models

**Result:** Evaluated 10 leading LLMs across 200 personas and 10,000 questions, showing similar PersonaScores for models like GPT-4.1 and LLaMA-3-8b despite differences in model sophistication.

**Limitations:** 

**Conclusion:** Incremental improvements in model size do not guarantee better persona adherence, highlighting the need for innovative algorithmic and architectural solutions.

**Abstract:** Persona agents, which are LLM agents conditioned to act according to an assigned persona, enable contextually rich and user aligned interactions across domains like education and healthcare. However, evaluating how faithfully these agents adhere to their personas remains a significant challenge, particularly in free-form settings that demand consistency across diverse, persona-relevant environments. We introduce PersonaGym, the first dynamic evaluation framework for persona agents, and PersonaScore, a human-aligned automatic metric grounded in decision theory that enables comprehensive large-scale evaluation. Our evaluation of 10 leading LLMs across 200 personas and 10,000 questions reveals significant advancement opportunities. For example, GPT-4.1 had the exact same PersonaScore as LLaMA-3-8b despite being a more recent and advanced closed source model. Importantly, increased model size and complexity do not necessarily enhance persona agent capabilities, underscoring the need for algorithmic and architectural innovation toward faithful, performant persona agents.

</details>


### [166] [Automating Intervention Discovery from Scientific Literature: A Progressive Ontology Prompting and Dual-LLM Framework](https://arxiv.org/abs/2409.00054)

*Yuting Hu, Dancheng Liu, Qingyun Wang, Charles Yu, Chenhui Xu, Qingxiao Zheng, Heng Ji, Jinjun Xiong*

**Main category:** cs.CL

**Keywords:** large language models, speech-language pathology, automated annotation, ontology prompting, dual-agent system

**Relevance Score:** 9

**TL;DR:** This paper introduces a framework utilizing large language models to automate the identification and annotation of scientific interventions in speech-language pathology, enhancing efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of identifying effective scientific interventions due to the overwhelming volume of literature and inconsistent reporting, aiming to streamline this process through automation.

**Method:** The proposed framework combines a Progressive Ontology Prompting (POP) algorithm with a dual-agent LLM system (LLM-Duo) that includes an explorer and an evaluator working together to improve annotation quality.

**Key Contributions:**

	1. Introduction of a novel framework integrating large language models for annotation tasks
	2. Development of the Progressive Ontology Prompting algorithm for structured prompting
	3. Creation of a knowledge base from a large corpus of literature in speech-language pathology.

**Result:** The framework was applied in a case study for speech-language interventions, where it successfully identified 2,421 interventions from 64,177 articles, outperforming existing baselines in accuracy and comprehensiveness.

**Limitations:** 

**Conclusion:** The approach demonstrates significant potential for the speech-language pathology community by creating a publicly accessible intervention knowledge base, making it easier to access vetted interventions.

**Abstract:** Identifying effective interventions from the scientific literature is challenging due to the high volume of publications, specialized terminology, and inconsistent reporting formats, making manual curation laborious and prone to oversight. To address this challenge, this paper proposes a novel framework leveraging large language models (LLMs), which integrates a progressive ontology prompting (POP) algorithm with a dual-agent system, named LLM-Duo. On the one hand, the POP algorithm conducts a prioritized breadth-first search (BFS) across a predefined ontology, generating structured prompt templates and action sequences to guide the automatic annotation process. On the other hand, the LLM-Duo system features two specialized LLM agents, an explorer and an evaluator, working collaboratively and adversarially to continuously refine annotation quality. We showcase the real-world applicability of our framework through a case study focused on speech-language intervention discovery. Experimental results show that our approach surpasses advanced baselines, achieving more accurate and comprehensive annotations through a fully automated process. Our approach successfully identified 2,421 interventions from a corpus of 64,177 research articles in the speech-language pathology domain, culminating in the creation of a publicly accessible intervention knowledge base with great potential to benefit the speech-language pathology community.

</details>


### [167] [RoMath: A Mathematical Reasoning Benchmark in Romanian](https://arxiv.org/abs/2409.11074)

*Adrian Cosma, Ana-Maria Bucur, Emilian Radoi*

**Main category:** cs.CL

**Keywords:** multilingual AI, mathematics, Romanian language, language models, benchmarking

**Relevance Score:** 4

**TL;DR:** The paper introduces RoMath, a Romanian mathematical reasoning benchmark suite to enhance multilingual AI development and support low-resource language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of benchmarks for understanding informal mathematical text in non-English languages, particularly Romanian, and to promote the development of multilingual AI resources.

**Method:** The paper presents RoMath, which consists of three subsets—Baccalaureate, Competitions, and Synthetic—covering various mathematical domains and difficulty levels. The performance of several open-weight language models is benchmarked on these subsets.

**Key Contributions:**

	1. Introduction of the RoMath benchmark suite for Romanian language
	2. Focus on multilingual AI development
	3. Highlighting the need for resources beyond automatic translation

**Result:** The benchmarking highlights the performance disparities of existing models on Romanian mathematical texts, emphasizing the need for dedicated resources for low-resource languages.

**Limitations:** 

**Conclusion:** RoMath serves as a vital resource for improving non-English language models, underscoring the importance of developing dedicated tools for underrepresented languages.

**Abstract:** Mathematics has long been conveyed through natural language, primarily for human understanding. With the rise of mechanized mathematics and proof assistants, there is a growing need to understand informal mathematical text, yet most existing benchmarks focus solely on English, overlooking other languages. This paper introduces RoMath, a Romanian mathematical reasoning benchmark suite comprising three subsets: Baccalaureate, Competitions and Synthetic, which cover a range of mathematical domains and difficulty levels, aiming to improve non-English language models and promote multilingual AI development. By focusing on Romanian, a low-resource language with unique linguistic features, RoMath addresses the limitations of Anglo-centric models and emphasizes the need for dedicated resources beyond simple automatic translation. We benchmark several open-weight language models, highlighting the importance of creating resources for underrepresented languages. Code and datasets are be made available.

</details>


### [168] [Revealing and Mitigating the Challenge of Detecting Character Knowledge Errors in LLM Role-Playing](https://arxiv.org/abs/2409.11726)

*Wenyuan Zhang, Shuaiyi Nie, Jiawei Sheng, Zefeng Zhang, Xinghua Zhang, Yongquan He, Tingwen Liu*

**Main category:** cs.CL

**Keywords:** large language models, role-playing agents, knowledge errors, error detection, reasoning strategies

**Relevance Score:** 8

**TL;DR:** This paper introduces RoleKE-Bench, a benchmark for evaluating large language models' (LLMs) ability to detect knowledge errors in role-playing scenarios, and proposes a novel reasoning method to enhance error detection.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Character knowledge is essential for creating realistic LLM role-playing agents, yet current models struggle to identify known and unknown knowledge errors, which limits the quality of generated character data.

**Method:** The paper presents RoleKE-Bench for assessing LLMs' error detection abilities regarding known knowledge errors (KKE) and unknown knowledge errors (UKE). It introduces a new reasoning strategy, Self-Recollection and Self-Doubt (S^2RD), to improve detection capabilities.

**Key Contributions:**

	1. Introduction of RoleKE-Bench for evaluating error detection in LLMs
	2. Proposal of the Self-Recollection and Self-Doubt (S^2RD) method to enhance error detection
	3. Empirical results showing the limitations of existing LLMs in detecting KKE and UKE

**Result:** Results show that contemporary LLMs have significant difficulties in identifying KKE and UKE, particularly related to familiar knowledge. The proposed S^2RD method demonstrably enhances the LLMs' error detection performance.

**Limitations:** The study identifies ongoing challenges in error detection for LLMs that require further exploration beyond initial improvements.

**Conclusion:** While the S^2RD method improves error detection in LLMs, the challenges in accurately detecting knowledge errors highlight the need for continued research in this area.

**Abstract:** Large language model (LLM) role-playing has gained widespread attention. Authentic character knowledge is crucial for constructing realistic LLM role-playing agents. However, existing works usually overlook the exploration of LLMs' ability to detect characters' known knowledge errors (KKE) and unknown knowledge errors (UKE) while playing roles, which would lead to low-quality automatic construction of character trainable corpus. In this paper, we propose RoleKE-Bench to evaluate LLMs' ability to detect errors in KKE and UKE. The results indicate that even the latest LLMs struggle to detect these two types of errors effectively, especially when it comes to familiar knowledge. We experimented with various reasoning strategies and propose an agent-based reasoning method, Self-Recollection and Self-Doubt (S$^2$RD), to explore further the potential for improving error detection capabilities. Experiments show that our method effectively improves the LLMs' ability to detect error character knowledge, but it remains an issue that requires ongoing attention.

</details>


### [169] [Learning from Committee: Reasoning Distillation from a Mixture of Teachers with Peer-Review](https://arxiv.org/abs/2410.03663)

*Zhuochun Li, Yuelyu Ji, Rui Meng, Daqing He*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Large Language Models, Reasoning Tasks, Peer-Review Process, Human Learning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach to improve smaller open-source models by focusing on customized instruction data through peer-reviewed rationales from teacher LLMs.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of smaller models via knowledge distillation by capturing not only answers but also explanations of mistakes, aligning with natural human learning processes.

**Method:** The FAIR approach implements a system where teacher LLMs identify and explain students' errors, and a simulated peer-review process selects the most reliable rationales, improving the quality of instructional data.

**Key Contributions:**

	1. Introduction of the FAIR approach for knowledge distillation
	2. Customized instructional data through mistake explanations
	3. Peer-review process among teacher LLMs to enhance rationale quality

**Result:** Comprehensive experiments show that FAIR significantly improves performance on tasks related to mathematical, commonsense, and logical reasoning compared to traditional knowledge distillation methods.

**Limitations:** The study primarily focuses on reasoning tasks and may need further evaluation in diverse application contexts.

**Conclusion:** The FAIR method advances the effectiveness of knowledge distillation, providing quality instructional data that mimics human learning and improves reasoning in smaller models.

**Abstract:** While reasoning capabilities typically emerge in large language models (LLMs) with tens of billions of parameters, recent research focuses on improving smaller open-source models through knowledge distillation (KD) from commercial LLMs. However, many of these studies rely solely on responses from a single LLM as the gold rationale, unlike the natural human learning process, which involves understanding both the correct answers and the reasons behind mistakes. In this paper, we introduce a novel Fault-Aware DistIllation via Peer-Review (FAIR) approach: 1) instead of merely obtaining rationales from teachers, our method asks teachers to identify and explain the student's mistakes, providing customized instruction learning data; 2) we design a simulated peer-review process between teacher LLMs, and selects only the generated rationales above the acceptance threshold, which reduces the chance of teachers guessing correctly with flawed rationale, improving instructional data quality. Comprehensive experiments and analysis on mathematical, commonsense, and logical reasoning tasks demonstrate the effectiveness of our method. Our code is available at https://github.com/zhuochunli/Learn-from-Committee.

</details>


### [170] [SensorLLM: Human-Intuitive Alignment of Multivariate Sensor Data with LLMs for Activity Recognition](https://arxiv.org/abs/2410.10624)

*Zechen Li, Shohreh Deldari, Linyao Chen, Hao Xue, Flora D. Salim*

**Main category:** cs.CL

**Keywords:** Human Activity Recognition, Large Language Models, Wearable Sensors, Time-Series Analysis, Machine Learning

**Relevance Score:** 8

**TL;DR:** SensorLLM is a framework for human activity recognition from wearable sensor data, overcoming challenges faced by LLMs in handling time-series inputs through a two-stage approach.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable LLMs to effectively recognize human activities from wearable sensor data, addressing challenges like semantic context and numerical complexity.

**Method:** The framework consists of two stages: Sensor-Language Alignment, which aligns sensor data with descriptive text, and Task-Aware Tuning, which adapts the model for multivariate HAR classification.

**Key Contributions:**

	1. Introduction of SensorLLM for HAR using LLMs
	2. Development of SensorQA dataset for sensor-text alignment
	3. Achievement of state-of-the-art performance in multivariate HAR classification

**Result:** SensorLLM matches or exceeds the performance of state-of-the-art multivariate HAR methods, showing effective generalization across various scenarios.

**Limitations:** 

**Conclusion:** The introduction of human-intuitive alignment allows SensorLLM to effectively learn from sensor data, making it a significant step for foundation model research in time-series analysis.

**Abstract:** We introduce SensorLLM, a two-stage framework that enables Large Language Models (LLMs) to perform human activity recognition (HAR) from wearable sensor data. While LLMs excel at reasoning and generalization, they struggle with time-series inputs due to limited semantic context, numerical complexity, and sequence variability. To address these challenges, we construct SensorQA, a question-answering dataset of human-intuitive sensor-text pairs spanning diverse HAR scenarios. It supervises the Sensor-Language Alignment stage, where the model aligns sensor inputs with trend descriptions. Special tokens are introduced to mark channel boundaries. This alignment enables LLMs to interpret numerical patterns, channel-specific signals, and variable-length inputs--without requiring human annotation. In the subsequent Task-Aware Tuning stage, we adapt the model for multivariate HAR classification, achieving performance that matches or exceeds state-of-the-art methods. Our results show that, guided by human-intuitive alignment, SensorLLM becomes an effective sensor learner, reasoner, and classifier--generalizing across varied HAR settings and paving the way for foundation model research in time-series analysis.

</details>


### [171] [RATE: Causal Explainability of Reward Models with Imperfect Counterfactuals](https://arxiv.org/abs/2410.11348)

*David Reber, Sean Richardson, Todd Nief, Cristina Garbacea, Victor Veitch*

**Main category:** cs.CL

**Keywords:** reward models, large language models, causal estimation, sensitivity analysis, HCI

**Relevance Score:** 8

**TL;DR:** RATE is a method for assessing the sensitivity of reward models used in LLMs to various high-level attributes like sentiment and complexity, addressing the challenge of using imperfect counterfactuals in this measurement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand what reward models are actually rewarding when aligning or evaluating large language models (LLMs), as existing models operate as black boxes with unclear mechanisms.

**Method:** RATE employs a rewriting technique on responses to create counterfactual examples, allowing the measurement of the causal effects of high-level attributes on the reward. It utilizes a double rewriting process to mitigate biases from imperfect rewrites.

**Key Contributions:**

	1. Development of the RATE methodology for causal effect measurement in reward models
	2. Introduction of a double rewriting technique to adjust for bias in sensitivity estimation
	3. Empirical validation of RATE's effectiveness in handling attribute sensitivity.

**Result:** The RATE method demonstrates effectiveness as a causal estimator by empirically validating its performance in measuring the sensitivity to attributes while reducing estimation bias.

**Limitations:** The potential for biases due to the imperfections in the generated counterfactuals; further studies needed for broader application.

**Conclusion:** The paper establishes that RATE can reliably assess reward model sensitivity, contributing to more transparent evaluations of LLMs.

**Abstract:** Reward models are widely used as proxies for human preferences when aligning or evaluating LLMs. However, reward models are black boxes, and it is often unclear what, exactly, they are actually rewarding. In this paper we develop Rewrite-based Attribute Treatment Estimator (RATE) as an effective method for measuring the sensitivity of a reward model to high-level attributes of responses, such as sentiment, helpfulness, or complexity. Importantly, RATE measures the causal effect of an attribute on the reward. RATE uses LLMs to rewrite responses to produce imperfect counterfactuals examples that can be used to measure causal effects. A key challenge is that these rewrites are imperfect in a manner that can induce substantial bias in the estimated sensitivity of the reward model to the attribute. The core idea of RATE is to adjust for this imperfect-rewrite effect by rewriting twice. We establish the validity of the RATE procedure and show empirically that it is an effective estimator.

</details>


### [172] [Interpreting token compositionality in LLMs: A robustness analysis](https://arxiv.org/abs/2410.12924)

*Nura Aljaafari, Danilo S. Carvalho, André Freitas*

**Main category:** cs.CL

**Keywords:** large language models, compositionality, mechanistic interpretability, information theory, transformer architecture

**Relevance Score:** 8

**TL;DR:** The paper introduces Constituent-Aware Pooling (CAP) to analyze compositional linguistic structures in LLMs, revealing limitations in handling semantic representations, especially in larger models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability, interpretability, and inference processes of large language models by understanding their internal mechanisms, particularly how they handle compositional linguistic structures.

**Method:** The methodology involves systematic interventions in model activations through constituent-based pooling at different model levels, grounding the approach in compositionality, mechanistic interpretability, and information theory.

**Key Contributions:**

	1. Introduction of Constituent-Aware Pooling (CAP) methodology
	2. Insights into the limitations of transformers in processing compositional structures
	3. Evidence of information fragmentation in larger LLMs

**Result:** Experiments on inverse definition modelling and semantic prediction tasks reveal that no specific model layer effectively integrates tokens into cohesive semantic representations, leading to fragmented information processing that worsens with model size.

**Limitations:** The approach may not be generalizable across all types of LLMs and relies on specific experimental conditions.

**Conclusion:** The study uncovers fundamental limitations in existing transformer architectures regarding compositional semantics, highlighting the need for innovative approaches in LLM design to overcome these challenges.

**Abstract:** Understanding the internal mechanisms of large language models (LLMs) is integral to enhancing their reliability, interpretability, and inference processes. We present Constituent-Aware Pooling (CAP), a methodology designed to analyse how LLMs process compositional linguistic structures. Grounded in principles of compositionality, mechanistic interpretability, and information theory, CAP systematically intervenes in model activations through constituent-based pooling at various model levels. Our experiments on inverse definition modelling, hypernym and synonym prediction reveal critical insights into transformers' limitations in handling compositional abstractions. No specific layer integrates tokens into unified semantic representations based on their constituent parts. We observe fragmented information processing, which intensifies with model size, suggesting that larger models struggle more with these interventions and exhibit greater information dispersion. This fragmentation likely stems from transformers' training objectives and architectural design, preventing systematic and cohesive representations. Our findings highlight fundamental limitations in current transformer architectures regarding compositional semantics processing and model interpretability, underscoring the critical need for novel approaches in LLM design to address these challenges.

</details>


### [173] [The Mystery of the Pathological Path-star Task for Language Models](https://arxiv.org/abs/2410.13779)

*Arvid Frydenlund*

**Main category:** cs.CL

**Keywords:** language models, path-star task, teacher-forcing, graph traversal, supervision techniques

**Relevance Score:** 8

**TL;DR:** The path-star task highlights language models' limitations in graph traversal, showing they struggle in generating paths despite human ease; improvements are introduced through teacher-forcing and representation adjustments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations of language models using the path-star task, which involves generating a graph path between nodes, and address the observed performance issues.

**Method:** The authors use teacher-forcing in different setups and introduce a regularization method that uses structured samples of the graph with varying target nodes to improve model performance.

**Key Contributions:**

	1. Introduction of the path-star task as a benchmark for language model capabilities.
	2. Development of a regularization method that improves task performance across various model types.
	3. Theoretical proof of task solvability under certain conditions.

**Result:** The proposed methods enhance language models' ability to solve the path-star task and provide theoretical proofs that confirm solvability under certain conditions.

**Limitations:** The study is limited to the context of the path-star task and may not generalize to other types of graph-related tasks.

**Conclusion:** The findings suggest that modification of training strategies can help language models overcome specific task limitations, particularly through better representation and supervision techniques.

**Abstract:** The recently introduced path-star task is a minimal task designed to exemplify limitations to the abilities of language models (Bachmann and Nagarajan, 2024). It involves a path-star graph where multiple arms radiate from a single starting node and each node is unique. Given the start node and a specified target node that ends an arm, the task is to generate the arm containing that target node. This is straightforward for a human but surprisingly difficult for language models, which did not outperform the random baseline. The authors hypothesized this is due to a deficiency in teacher-forcing and the next-token prediction paradigm.   We demonstrate the task is learnable using teacher-forcing in alternative settings and that the issue is partially due to representation. We introduce a regularization method using structured samples of the same graph but with differing target nodes, improving results across a variety of model types. We provide RASP proofs showing the task is theoretically solvable. Finally, we find settings where an encoder-only model can consistently solve the task.

</details>


### [174] [Unlearning Backdoor Attacks for LLMs with Weak-to-Strong Knowledge Distillation](https://arxiv.org/abs/2410.14425)

*Shuai Zhao, Xiaobao Wu, Cong-Duy Nguyen, Yanhao Jia, Meihuizi Jia, Yichao Feng, Luu Anh Tuan*

**Main category:** cs.CL

**Keywords:** backdoor attacks, parameter-efficient fine-tuning, large language models, unlearning, knowledge distillation

**Relevance Score:** 8

**TL;DR:** This paper presents W2SDefense, a novel algorithm that enhances the unlearning capability of poisoned large language models (LLMs) to defend against backdoor attacks while maintaining model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Parameter-efficient fine-tuning (PEFT) is vulnerable to malicious backdoor attacks in large language models (LLMs). This research aims to develop a method to mitigate such vulnerabilities.

**Method:** W2SDefense utilizes a weak-to-strong unlearning algorithm based on feature alignment knowledge distillation. A clean teacher model is first trained via full-parameter fine-tuning, and then it instructs the poisoned student model to unlearn the backdoor features.

**Key Contributions:**

	1. Introduction of W2SDefense for unlearning backdoors in LLMs
	2. Demonstrated effectiveness across three state-of-the-art LLMs
	3. Maintains model performance while mitigating backdoor risks

**Result:** The empirical results demonstrate that W2SDefense effectively enhances the student model's ability to unlearn backdoor features, successfully preventing the activation of backdoors while preserving model performance.

**Limitations:** 

**Conclusion:** W2SDefense shows strong potential as a defense mechanism against backdoor attacks in LLMs, proving effective across multiple models and attack algorithms without degrading overall performance.

**Abstract:** Parameter-efficient fine-tuning (PEFT) can bridge the gap between large language models (LLMs) and downstream tasks. However, PEFT has been proven vulnerable to malicious attacks. Research indicates that poisoned LLMs, even after PEFT, retain the capability to activate internalized backdoors when input samples contain predefined triggers. In this paper, we introduce a novel weak-to-strong unlearning algorithm to defend against backdoor attacks based on feature alignment knowledge distillation, named W2SDefense. Specifically, we first train a small-scale language model through full-parameter fine-tuning to serve as the clean teacher model. Then, this teacher model guides the large-scale poisoned student model in unlearning the backdoor, leveraging PEFT. Theoretical analysis suggests that W2SDefense has the potential to enhance the student model's ability to unlearn backdoor features, preventing the activation of the backdoor. We conduct comprehensive experiments on three state-of-the-art large language models and several different backdoor attack algorithms. Our empirical results demonstrate the outstanding performance of W2SDefense in defending against backdoor attacks without compromising model performance.

</details>


### [175] [M-RewardBench: Evaluating Reward Models in Multilingual Settings](https://arxiv.org/abs/2410.15522)

*Srishti Gureja, Lester James V. Miranda, Shayekh Bin Islam, Rishabh Maheshwary, Drishti Sharma, Gusti Winata, Nathan Lambert, Sebastian Ruder, Sara Hooker, Marzieh Fadaee*

**Main category:** cs.CL

**Keywords:** reward models, multilingual, human feedback, language evaluation, M-RewardBench

**Relevance Score:** 8

**TL;DR:** This paper evaluates reward models (RMs) in multilingual settings using a new benchmark, M-RewardBench, revealing substantial performance gaps across languages.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance of reward models in multilingual contexts, as RMs have primarily been evaluated in English and their multilingual capabilities are understudied.

**Method:** A systematic evaluation of several reward models was conducted using the M-RewardBench, a benchmark containing 2.87k preference instances across 23 languages, focusing on chat, safety, reasoning, and translation capabilities.

**Key Contributions:**

	1. Introduction of the M-RewardBench multilingual benchmark for evaluating reward models.
	2. Identification of performance discrepancies in RMs between English and non-English languages.
	3. Insights on how multilingual factors like translation quality affect RM performance.

**Result:** The evaluation showed a significant performance gap between English and non-English languages, with RM preferences varying extensively across languages. Furthermore, RM performance improved with better translation quality and was generally better for high-resource languages.

**Limitations:** The benchmark's scope is limited to 23 typologically diverse languages, which may not represent all languages' RM performance.

**Conclusion:** The study highlights the need for better evaluation of reward models in multilingual settings and the impact of translation quality and resource availability on RM performance. The M-RewardBench dataset and codebase are released to support future research.

**Abstract:** Reward models (RMs) have driven the state-of-the-art performance of LLMs today by enabling the integration of human feedback into the language modeling process. However, RMs are primarily trained and evaluated in English, and their capabilities in multilingual settings remain largely understudied. In this work, we conduct a systematic evaluation of several reward models in multilingual settings. We first construct the first-of-its-kind multilingual RM evaluation benchmark, M-RewardBench, consisting of 2.87k preference instances for 23 typologically diverse languages, that tests the chat, safety, reasoning, and translation capabilities of RMs. We then rigorously evaluate a wide range of reward models on M-RewardBench, offering fresh insights into their performance across diverse languages. We identify a significant gap in RMs' performances between English and non-English languages and show that RM preferences can change substantially from one language to another. We also present several findings on how different multilingual aspects impact RM performance. Specifically, we show that the performance of RMs is improved with improved translation quality. Similarly, we demonstrate that the models exhibit better performance for high-resource languages. We release M-RewardBench dataset and the codebase in this study to facilitate a better understanding of RM evaluation in multilingual settings.

</details>


### [176] [Arithmetic Without Algorithms: Language Models Solve Math With a Bag of Heuristics](https://arxiv.org/abs/2410.21272)

*Yaniv Nikankin, Anja Reusch, Aaron Mueller, Yonatan Belinkov*

**Main category:** cs.CL

**Keywords:** large language models, arithmetic reasoning, neural circuits, heuristics, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates how large language models (LLMs) perform arithmetic reasoning, concluding they use a combination of simple heuristics rather than robust algorithms or memorization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine whether LLMs solve reasoning tasks via learned algorithms or by memorization.

**Method:** Causal analysis of LLMs focused on identifying circuits responsible for arithmetic logic and analyzing individual circuit neurons' behavior.

**Key Contributions:**

	1. Identification of a circuit in LLMs responsible for arithmetic logic
	2. Discovery of heuristic types among individual neurons
	3. Demonstration that heuristic combinations drive arithmetic accuracy

**Result:** Identified a sparse set of neurons implementing simple heuristics that explain the model's arithmetic accuracy, revealing that LLMs rely on a 'bag of heuristics' for performance.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs do not use robust algorithms or purely memorization for arithmetic tasks but a combination of heuristic approaches.

**Abstract:** Do large language models (LLMs) solve reasoning tasks by learning robust generalizable algorithms, or do they memorize training data? To investigate this question, we use arithmetic reasoning as a representative task. Using causal analysis, we identify a subset of the model (a circuit) that explains most of the model's behavior for basic arithmetic logic and examine its functionality. By zooming in on the level of individual circuit neurons, we discover a sparse set of important neurons that implement simple heuristics. Each heuristic identifies a numerical input pattern and outputs corresponding answers. We hypothesize that the combination of these heuristic neurons is the mechanism used to produce correct arithmetic answers. To test this, we categorize each neuron into several heuristic types-such as neurons that activate when an operand falls within a certain range-and find that the unordered combination of these heuristic types is the mechanism that explains most of the model's accuracy on arithmetic prompts. Finally, we demonstrate that this mechanism appears as the main source of arithmetic accuracy early in training. Overall, our experimental results across several LLMs show that LLMs perform arithmetic using neither robust algorithms nor memorization; rather, they rely on a "bag of heuristics".

</details>


### [177] [Rate, Explain and Cite (REC): Enhanced Explanation and Attribution in Automatic Evaluation by Large Language Models](https://arxiv.org/abs/2411.02448)

*Aliyah R. Hsu, James Zhu, Zhichao Wang, Bin Bi, Shubham Mehrotra, Shiva K. Pentyala, Katherine Tan, Xiang-Bo Mao, Roshanak Omrani, Sougata Chaudhuri, Regunathan Radhakrishnan, Sitaram Asur, Claire Na Cheng, Bin Yu*

**Main category:** cs.CL

**Keywords:** LLM autoevaluators, text generation, content evaluation

**Relevance Score:** 9

**TL;DR:** This paper presents three fine-tuned general-purpose LLM autoevaluators aimed at evaluating generated text for various quality metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of ensuring quality in LLM-generated content due to issues like factual inaccuracies and hallucinations.

**Method:** The authors developed three models, REC-8B, REC-12B, and REC-70B, which evaluate text on faithfulness, instruction following, coherence, and completeness, providing ratings along with explanations and verifiable citations.

**Key Contributions:**

	1. Introduction of fine-tuned LLM autoevaluators for quality assessment of generated text
	2. Provision of detailed explanations and verifiable citations to enhance trust in generated content
	3. Demonstration of superior performance in evaluating content quality over state-of-the-art models

**Result:** Extensive evaluations show that the REC-70B model outperforms existing LLMs in content evaluation with better quality explanations and citations while maintaining minimal bias.

**Limitations:** 

**Conclusion:** The models and their dataset are made available for further research and application.

**Abstract:** LLMs have demonstrated impressive proficiency in generating coherent and high-quality text, making them valuable across a range of text-generation tasks. However, rigorous evaluation of this generated content is crucial, as ensuring its quality remains a significant challenge due to persistent issues such as factual inaccuracies and hallucination. This paper introduces three fine-tuned general-purpose LLM autoevaluators, REC-8B, REC-12B and REC-70B, specifically designed to evaluate generated text across several dimensions: faithfulness, instruction following, coherence, and completeness. These models not only provide ratings for these metrics but also offer detailed explanation and verifiable citation, thereby enhancing trust in the content. Moreover, the models support various citation modes, accommodating different requirements for latency and granularity. Extensive evaluations on diverse benchmarks demonstrate that our general-purpose LLM auto-evaluator, REC-70B, outperforms state-of-the-art LLMs, excelling in content evaluation by delivering better quality explanation and citation with minimal bias. Our REC dataset and models are available at https://github.com/adelaidehsu/REC.

</details>


### [178] [Velocitune: A Velocity-based Dynamic Domain Reweighting Method for Continual Pre-training](https://arxiv.org/abs/2411.14318)

*Zheheng Luo, Xin Zhang, Xiao Liu, Haoling Li, Yeyun Gong, Chen Qi, Peng Cheng*

**Main category:** cs.CL

**Keywords:** large language models, domain-adaptive training, data proportion adjustment, machine learning, reasoning tasks

**Relevance Score:** 9

**TL;DR:** Velocitune is a framework for dynamic data proportion adjustment in continual pre-training of language models, achieving performance improvements in reasoning and command generation tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the complexities of domain-adaptive continual pre-training and improve training efficiency by dynamically adjusting data proportions.

**Method:** Velocitune dynamically assesses learning velocity and adjusts data proportions, focusing on slower-learning domains based on a scaling law for desired goals with lower costs.

**Key Contributions:**

	1. Introduction of the Velocitune framework for dynamic data adjustment
	2. Improved training efficiency for language models by addressing domain complexities
	3. Empirical validation through experiments on reasoning-focused and command-generation datasets

**Result:** Velocitune demonstrates performance gains in math and code reasoning tasks, as well as command-line generation benchmarks using CodeLlama, Llama3, and Mistral.

**Limitations:** 

**Conclusion:** The framework's effectiveness is driven by target loss prediction and data ordering, leading to improved results in specific tasks.

**Abstract:** It is well-known that a diverse corpus is critical for training large language models, which are typically constructed from a mixture of various domains. In general, previous efforts resort to sampling training data from different domains with static proportions, as well as adjusting data proportions during training. However, few methods have addressed the complexities of domain-adaptive continual pre-training. To fill this gap, we propose Velocitune, a novel framework dynamically assesses learning velocity and adjusts data proportions accordingly, favoring slower-learning domains while shunning faster-learning ones, which is guided by a scaling law to indicate the desired learning goal for each domain with less associated cost. To evaluate the effectiveness of Velocitune, we conduct experiments in a reasoning-focused dataset with CodeLlama, as well as in a corpus specialised for system command generation with Llama3 and Mistral. Velocitune achieves performance gains in both math and code reasoning tasks and command-line generation benchmarks. Further analysis reveals that key factors driving Velocitune's effectiveness include target loss prediction and data ordering.

</details>


### [179] [Can LLMs be Good Graph Judge for Knowledge Graph Construction?](https://arxiv.org/abs/2411.17388)

*Haoyu Huang, Chong Chen, Zeang Sheng, Yang Li, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Large Language Models, Information Retrieval, Data Noise, Hallucination

**Relevance Score:** 7

**TL;DR:** The paper presents GraphJudge, a framework for constructing Knowledge Graphs from unstructured data that addresses noise, inaccuracies, and hallucinations in LLM outputs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability and accuracy of Knowledge Graph construction from unstructured data, mitigating common issues faced by existing methods.

**Method:** GraphJudge employs an entity-centric strategy to filter out noise and uses a fine-tuned LLM as a graph judge to enhance the quality of generated Knowledge Graphs.

**Key Contributions:**

	1. Introduction of an entity-centric strategy to reduce noise in documents
	2. Development of a fine-tuned LLM to improve KG quality
	3. Demonstration of superior performance against baseline methods across diverse datasets

**Result:** Experiments show that GraphJudge outperforms various baseline methods and demonstrates strong generalization across general and domain-specific datasets.

**Limitations:** 

**Conclusion:** GraphJudge effectively addresses key challenges in Knowledge Graph construction from unstructured data, providing a robust framework for future applications.

**Abstract:** In real-world scenarios, most of the data obtained from the information retrieval (IR) system is unstructured. Converting natural language sentences into structured Knowledge Graphs (KGs) remains a critical challenge. We identified three limitations with respect to existing KG construction methods: (1) There could be a large amount of noise in real-world documents, which could result in extracting messy information. (2) Naive LLMs usually extract inaccurate knowledge from some domain-specific documents. (3) Hallucination phenomenon cannot be overlooked when directly using LLMs to construct KGs. In this paper, we propose \textbf{GraphJudge}, a KG construction framework to address the aforementioned challenges. In this framework, we designed an entity-centric strategy to eliminate the noise information in the documents. And we fine-tuned a LLM as a graph judge to finally enhance the quality of generated KGs. Experiments conducted on two general and one domain-specific text-graph pair datasets demonstrate state-of-the-art performance against various baseline methods with strong generalization abilities. Our code is available at \href{https://github.com/hhy-huang/GraphJudge}{https://github.com/hhy-huang/GraphJudge}.

</details>


### [180] [A Comparative Study of Learning Paradigms in Large Language Models via Intrinsic Dimension](https://arxiv.org/abs/2412.06245)

*Saahith Janapati, Yangfeng Ji*

**Main category:** cs.CL

**Keywords:** Large Language Models, supervised fine-tuning, in-context learning, intrinsic dimension, natural language tasks

**Relevance Score:** 9

**TL;DR:** This study investigates the effects of supervised fine-tuning and in-context learning on the hidden representations of Large Language Models, finding that in-context learning leads to higher intrinsic dimension in representations compared to supervised fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how different learning paradigms (supervised fine-tuning and in-context learning) affect the hidden representations of Large Language Models.

**Method:** The study employs Intrinsic Dimension (ID) to estimate degrees of freedom in LLM representations during specific natural language tasks, comparing the evolution of ID through supervised fine-tuning and the impact of demonstration numbers in in-context learning.

**Key Contributions:**

	1. Investigation of intrinsic dimensions in LLM representations
	2. Comparison of effects of supervised fine-tuning and in-context learning
	3. Demonstration of higher dimensionality in representations via in-context learning

**Result:** In-context learning consistently induces a higher intrinsic dimension compared to supervised fine-tuning, indicating that representations during in-context learning reside in higher dimensional manifolds.

**Limitations:** 

**Conclusion:** The findings suggest that in-context learning is more effective in enriching the representation space of LLMs than supervised fine-tuning.

**Abstract:** The performance of Large Language Models (LLMs) on natural language tasks can be improved through both supervised fine-tuning (SFT) and in-context learning (ICL), which operate via distinct mechanisms. Supervised fine-tuning updates the model's weights by minimizing loss on training data, whereas in-context learning leverages task demonstrations embedded in the prompt, without changing the model's parameters. This study investigates the effects of these learning paradigms on the hidden representations of LLMs using Intrinsic Dimension (ID). We use ID to estimate the number of degrees of freedom between representations extracted from LLMs as they perform specific natural language tasks. We first explore how the ID of LLM representations evolves during SFT and how it varies due to the number of demonstrations in ICL. We then compare the IDs induced by SFT and ICL and find that ICL consistently induces a higher ID compared to SFT, suggesting that representations generated during ICL reside in higher dimensional manifolds in the embedding space.

</details>


### [181] [A Survey of Mathematical Reasoning in the Era of Multimodal Large Language Model: Benchmark, Method & Challenges](https://arxiv.org/abs/2412.11936)

*Yibo Yan, Jiamin Su, Jianxiang He, Fangteng Fu, Xu Zheng, Yuanhuiyi Lyu, Kun Wang, Shen Wang, Qingsong Wen, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Mathematical Reasoning, Multimodal Large Language Models, Artificial General Intelligence, Machine Learning, Cognition

**Relevance Score:** 8

**TL;DR:** This survey analyzes mathematical reasoning in multimodal large language models (MLLMs), reviewing over 200 studies and outlining key methodologies and challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As AGI develops, integrating LLMs with mathematical reasoning tasks is vital for advancing educational and scientific problem-solving.

**Method:** The paper surveys existing literature on Math-LLMs, categorizing it into benchmarks, methodologies, and challenges, with a focus on multimodal applications.

**Key Contributions:**

	1. Comprehensive review of 200+ studies on Math-LLMs
	2. Identification of key benchmarks and methodologies
	3. Insightful discussion on challenges and future directions

**Result:** Identified trends and methodologies in MLLMs related to mathematical reasoning, along with five major challenges impeding AGI progress in this area.

**Limitations:** The paper focuses primarily on existing literature and may not cover all emerging methodologies or applications.

**Conclusion:** The survey provides insights crucial for enhancing multimodal reasoning capabilities in LLMs, serving as a resource for future research.

**Abstract:** Mathematical reasoning, a core aspect of human cognition, is vital across many domains, from educational problem-solving to scientific advancements. As artificial general intelligence (AGI) progresses, integrating large language models (LLMs) with mathematical reasoning tasks is becoming increasingly significant. This survey provides the first comprehensive analysis of mathematical reasoning in the era of multimodal large language models (MLLMs). We review over 200 studies published since 2021, and examine the state-of-the-art developments in Math-LLMs, with a focus on multimodal settings. We categorize the field into three dimensions: benchmarks, methodologies, and challenges. In particular, we explore multimodal mathematical reasoning pipeline, as well as the role of (M)LLMs and the associated methodologies. Finally, we identify five major challenges hindering the realization of AGI in this domain, offering insights into the future direction for enhancing multimodal reasoning capabilities. This survey serves as a critical resource for the research community in advancing the capabilities of LLMs to tackle complex multimodal reasoning tasks.

</details>


### [182] [TheAgentCompany: Benchmarking LLM Agents on Consequential Real World Tasks](https://arxiv.org/abs/2412.14161)

*Frank F. Xu, Yufan Song, Boxuan Li, Yuxuan Tang, Kritanjali Jain, Mengxue Bao, Zora Z. Wang, Xuhui Zhou, Zhitong Guo, Murong Cao, Mingyang Yang, Hao Yang Lu, Amaad Martin, Zhe Su, Leander Maben, Raj Mehta, Wayne Chi, Lawrence Jang, Yiqing Xie, Shuyan Zhou, Graham Neubig*

**Main category:** cs.CL

**Keywords:** AI agents, Large language models, Task automation, Benchmarking, Workplace simulation

**Relevance Score:** 8

**TL;DR:** The paper introduces TheAgentCompany, a benchmark to evaluate AI agents simulating digital worker tasks, revealing that current models can autonomously complete 30% of simpler work-related tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance of AI agents in performing real-world, work-related tasks and their implications for industry and economic policy.

**Method:** An extensible benchmark was created, simulating a software company environment with various tasks for AI agents to complete.

**Key Contributions:**

	1. Introduction of TheAgentCompany benchmark for AI agents
	2. Simulation of a software company environment for evaluating performance
	3. Empirical findings on the autonomous capabilities of AI agents in workplace tasks.

**Result:** Baseline agents powered by both closed API-based and open-weights language models completed 30% of tasks autonomously, indicating potential for automation but limitations on complex tasks.

**Limitations:** Current AI agents are less effective at complex, long-horizon tasks, which remain a challenge.

**Conclusion:** While simpler tasks can be automated, current systems struggle with more complex, long-horizon tasks, highlighting the nuanced landscape of task automation with LM agents.

**Abstract:** We interact with computers on an everyday basis, be it in everyday life or work, and many aspects of work can be done entirely with access to a computer and the Internet. At the same time, thanks to improvements in large language models (LLMs), there has also been a rapid development in AI agents that interact with and affect change in their surrounding environments. But how performant are AI agents at accelerating or even autonomously performing work-related tasks? The answer to this question has important implications both for industry looking to adopt AI into their workflows and for economic policy to understand the effects that adoption of AI may have on the labor market. To measure the progress of these LLM agents' performance on performing real-world professional tasks, in this paper we introduce TheAgentCompany, an extensible benchmark for evaluating AI agents that interact with the world in similar ways to those of a digital worker: by browsing the Web, writing code, running programs, and communicating with other coworkers. We build a self-contained environment with internal web sites and data that mimics a small software company environment, and create a variety of tasks that may be performed by workers in such a company. We test baseline agents powered by both closed API-based and open-weights language models (LMs), and find that the most competitive agent can complete 30% of tasks autonomously. This paints a nuanced picture on task automation with LM agents--in a setting simulating a real workplace, a good portion of simpler tasks could be solved autonomously, but more difficult long-horizon tasks are still beyond the reach of current systems. We release code, data, environment, and experiments on https://the-agent-company.com.

</details>


### [183] [Agent-SafetyBench: Evaluating the Safety of LLM Agents](https://arxiv.org/abs/2412.14470)

*Zhexin Zhang, Shiyao Cui, Yida Lu, Jingzhuo Zhou, Junxiao Yang, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**Keywords:** LLM agents, safety benchmark, Agent-SafetyBench, human-computer interaction, machine learning

**Relevance Score:** 9

**TL;DR:** Introduction of Agent-SafetyBench, a benchmark for evaluating the safety of LLM agents with 349 environments and 2,000 test cases.

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive benchmarks for evaluating the safety of LLM agents in interactive environments and tool use.

**Method:** Development of Agent-SafetyBench, which includes 349 interaction environments and 2,000 test cases to assess 8 categories of safety risks and 10 common failure modes.

**Key Contributions:**

	1. Introduction of Agent-SafetyBench benchmark for agent safety.
	2. Evaluation showing all tested LLM agents scored below 60% in safety.
	3. Identification of two key defects: lack of robustness and risk awareness.

**Result:** Evaluation of 16 popular LLM agents showed none scored above 60% in safety, revealing significant safety challenges and the need for methodological improvements.

**Limitations:** The benchmark may not cover all possible interaction scenarios and safety risks.

**Conclusion:** The findings indicate fundamental defects in LLM agent safety and highlight the inadequacy of defense prompts, urging the adoption of stronger strategies for safety enhancement.

**Abstract:** As large language models (LLMs) are increasingly deployed as agents, their integration into interactive environments and tool use introduce new safety challenges beyond those associated with the models themselves. However, the absence of comprehensive benchmarks for evaluating agent safety presents a significant barrier to effective assessment and further improvement. In this paper, we introduce Agent-SafetyBench, a comprehensive benchmark designed to evaluate the safety of LLM agents. Agent-SafetyBench encompasses 349 interaction environments and 2,000 test cases, evaluating 8 categories of safety risks and covering 10 common failure modes frequently encountered in unsafe interactions. Our evaluation of 16 popular LLM agents reveals a concerning result: none of the agents achieves a safety score above 60%. This highlights significant safety challenges in LLM agents and underscores the considerable need for improvement. Through failure mode and helpfulness analysis, we summarize two fundamental safety defects in current LLM agents: lack of robustness and lack of risk awareness. Furthermore, our findings suggest that reliance on defense prompts alone may be insufficient to address these safety issues, emphasizing the need for more advanced and robust strategies. To drive progress in this area, Agent-SafetyBench has been released at https://github.com/thu-coai/Agent-SafetyBench/ to facilitate further research in agent safety evaluation and improvement.

</details>


### [184] [SubData: Bridging Heterogeneous Datasets to Enable Theory-Driven Evaluation of Political and Demographic Perspectives in LLMs](https://arxiv.org/abs/2412.16783)

*Leon Fröhling, Pietro Bernardelle, Gianluca Demartini*

**Main category:** cs.CL

**Keywords:** large language models, perspective alignment, NLP, dataset standardization, subjective tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces SubData, a Python library for standardizing heterogeneous datasets to evaluate the alignment of large language models (LLMs) with human perspectives, specifically in subjective tasks.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The need for consistent evaluation of LLMs' perspective alignment on downstream tasks has arisen due to the variability in datasets used across studies.

**Method:** The paper presents a two-step framework that includes an open-source library, SubData, for dataset standardization and a theory-driven approach to test the classification of content by differently aligned LLMs.

**Key Contributions:**

	1. Introduction of SubData for standardizing datasets
	2. Theory-driven approach for evaluating LLM alignment
	3. Invitation for community contributions to enhance SubData

**Result:** The proposed SubData library allows customization for varying research needs and aims to create a multi-construct benchmark suite for LLM evaluation.

**Limitations:** 

**Conclusion:** Contributions to SubData are welcomed to enhance its dataset offerings and strengthen the assessment of LLM perspective alignment in NLP tasks.

**Abstract:** As increasingly capable large language models (LLMs) emerge, researchers have begun exploring their potential for subjective tasks. While recent work demonstrates that LLMs can be aligned with diverse human perspectives, evaluating this alignment on actual downstream tasks (e.g., hate speech detection) remains challenging due to the use of inconsistent datasets across studies. To address this issue, in this resource paper we propose a two-step framework: we (1) introduce SubData, an open-source Python library designed for standardizing heterogeneous datasets to evaluate LLM perspective alignment; and (2) present a theory-driven approach leveraging this library to test how differently-aligned LLMs (e.g., aligned with different political viewpoints) classify content targeting specific demographics. SubData's flexible mapping and taxonomy enable customization for diverse research needs, distinguishing it from existing resources. We invite contributions to add datasets to our initially proposed resource and thereby help expand SubData into a multi-construct benchmark suite for evaluating LLM perspective alignment on NLP tasks.

</details>


### [185] [Cross-model Transferability among Large Language Models on the Platonic Representations of Concepts](https://arxiv.org/abs/2501.02009)

*Youcheng Huang, Chen Huang, Duanyu Feng, Wenqiang Lei, Jiancheng Lv*

**Main category:** cs.CL

**Keywords:** Large Language Models, steering vectors, concept representations, linear transformations, transferability

**Relevance Score:** 9

**TL;DR:** This paper explores the alignment of concept representations in different LLMs using linear transformations, enabling controllable behavior through steering vectors.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the inner workings and relationships of concept representations across different Large Language Models (LLMs) and improve behavioral control of LLMs.

**Method:** The authors propose a linear transformation method to align concept representations between different LLMs and analyze the transferability of steering vectors.

**Key Contributions:**

	1. Introduction of a linear transformation method for aligning LLM concept representations.
	2. Detailed exploration of the weak-to-strong transferability between LLMs.
	3. Significant findings regarding the efficiency of steering vectors in controlling model behavior.

**Result:** Found that concept representations can be aligned using simple linear transformations, facilitating cross-model transfer and control; demonstrated weak-to-strong transferability between smaller and larger LLMs.

**Limitations:** 

**Conclusion:** Linear transformations can help bridge concept representations in LLMs, enhancing the controllability of their behavior through effective steering vectors.

**Abstract:** Understanding the inner workings of Large Language Models (LLMs) is a critical research frontier. Prior research has shown that a single LLM's concept representations can be captured as steering vectors (SVs), enabling the control of LLM behavior (e.g., towards generating harmful content). Our work takes a novel approach by exploring the intricate relationships between concept representations across different LLMs, drawing an intriguing parallel to Plato's Allegory of the Cave. In particular, we introduce a linear transformation method to bridge these representations and present three key findings: 1) Concept representations across different LLMs can be effectively aligned using simple linear transformations, enabling efficient cross-model transfer and behavioral control via SVs. 2) This linear transformation generalizes across concepts, facilitating alignment and control of SVs representing different concepts across LLMs. 3) A weak-to-strong transferability exists between LLM concept representations, whereby SVs extracted from smaller LLMs can effectively control the behavior of larger LLMs.

</details>


### [186] [ACORD: An Expert-Annotated Retrieval Dataset for Legal Contract Drafting](https://arxiv.org/abs/2501.06582)

*Steven H. Wang, Maksim Zubkov, Kexin Fan, Sarah Harrell, Yuyang Sun, Wei Chen, Andreas Plesner, Roger Wattenhofer*

**Main category:** cs.CL

**Keywords:** contract retrieval, NLP, legal informatics, dataset, information retrieval

**Relevance Score:** 5

**TL;DR:** Introducing the Atticus Clause Retrieval Dataset (ACORD) for contract clause retrieval, essential for contract drafting.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Lawyers typically do not draft contracts from scratch, relying instead on discovering and revising relevant precedent clauses.

**Method:** ACORD contains 114 queries and over 126,000 query-clause pairs, which are ranked on a scale of 1 to 5 stars, and utilizes a bi-encoder retriever combined with LLMs for re-ranking.

**Key Contributions:**

	1. Introduction of the ACORD dataset
	2. Focus on complex contract clauses
	3. First expert-annotated retrieval benchmark for contract drafting

**Result:** The methodology shows promising results but indicates that significant enhancements are necessary to address the complexities of legal work.

**Limitations:** Substantial improvements still needed for effective management of complex legal work.

**Conclusion:** ACORD is the first retrieval benchmark for contract drafting fully annotated by experts, which can be a useful resource for both legal practitioners and the NLP community.

**Abstract:** Information retrieval, specifically contract clause retrieval, is foundational to contract drafting because lawyers rarely draft contracts from scratch; instead, they locate and revise the most relevant precedent. We introduce the Atticus Clause Retrieval Dataset (ACORD), the first retrieval benchmark for contract drafting fully annotated by experts. ACORD focuses on complex contract clauses such as Limitation of Liability, Indemnification, Change of Control, and Most Favored Nation. It includes 114 queries and over 126,000 query-clause pairs, each ranked on a scale from 1 to 5 stars. The task is to find the most relevant precedent clauses to a query. The bi-encoder retriever paired with pointwise LLMs re-rankers shows promising results. However, substantial improvements are still needed to effectively manage the complex legal work typically undertaken by lawyers. As the first retrieval benchmark for contract drafting annotated by experts, ACORD can serve as a valuable IR benchmark for the NLP community.

</details>


### [187] [TiEBe: Tracking Language Model Recall of Notable Worldwide Events Through Time](https://arxiv.org/abs/2501.07482)

*Thales Sales Almeida, Giovana Kerche Bonás, João Guilherme Alves Santos, Hugo Abonizio, Rodrigo Nogueira*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Factual Recall, Geographic Disparities, Low-Resource Languages

**Relevance Score:** 8

**TL;DR:** The paper introduces the Timely Events Benchmark (TiEBe), a dataset for evaluating LLMs' knowledge retention over time, revealing disparities in factual recall linked to geography and socioeconomics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing prevalence of LLMs necessitates their continual update with current events, but few frameworks examine how LLMs retain knowledge and perform across diverse regions and languages.

**Method:** A dataset of 23,000 question-answer pairs derived from structured retrospective data from Wikipedia, spanning global notable events over 10 years, covering 23 regions and 13 languages.

**Key Contributions:**

	1. Introduction of the Timely Events Benchmark (TiEBe) for evaluating LLMs
	2. Insights on geographic disparities in LLM performance
	3. Correlation findings between LLM performance and socioeconomic indicators

**Result:** LLMs show significant geographic disparities in factual recall, correlating with socioeconomic indicators; substantial performance gaps exist for low-resource languages.

**Limitations:** 

**Conclusion:** A need for more balanced global representation in LLM training is highlighted, given the performance gaps noticed in lower-resource languages and geographic differences.

**Abstract:** As the knowledge landscape evolves and large language models (LLMs) become increasingly widespread, there is a growing need to keep these models updated with current events. While existing benchmarks assess general factual recall, few studies explore how LLMs retain knowledge over time or across different regions. To address these gaps, we present the Timely Events Benchmark (TiEBe), a dataset of over 23,000 question-answer pairs centered on notable global and regional events, spanning more than 10 years of events, 23 regions, and 13 languages. TiEBe leverages structured retrospective data from Wikipedia to identify notable events through time. These events are then used to construct a benchmark to evaluate LLMs' understanding of global and regional developments, grounded in factual evidence beyond Wikipedia itself. Our results reveal significant geographic disparities in factual recall, emphasizing the need for more balanced global representation in LLM training. We also observe a Pearson correlation of more than 0.7 between models' performance in TiEBe and various countries' socioeconomic indicators, such as HDI. In addition, we examine the impact of language on factual recall by posing questions in the native language of the region where each event occurred, uncovering substantial performance gaps for low-resource languages.

</details>


### [188] [Mitigating Forgetting in LLM Fine-Tuning via Low-Perplexity Token Learning](https://arxiv.org/abs/2501.14315)

*Chao-Chung Wu, Zhi Rui Tam, Chieh-Yen Lin, Yun-Nung Chen, Shao-Hua Sun, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** LLM, fine-tuning, cross-domain generalization, token perplexity, catastrophic forgetting

**Relevance Score:** 9

**TL;DR:** The paper analyzes the effects of fine-tuning LLMs with LLM-generated data on cross-domain generalization, finding improved performance in target tasks and reduced non-target task degradation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of LLM-generated data on cross-domain generalization in machine learning.

**Method:** Systematic analysis of fine-tuning with LLM-generated data versus ground truth data, focusing on token perplexity reduction and its effects across various model families and scales.

**Key Contributions:**

	1. First empirical explanation of token perplexity reduction's role in mitigating catastrophic forgetting.
	2. Demonstration that LLM-generated data outperforms ground truth data in maintaining model performance across domains.
	3. Introduction of masking high perplexity tokens as an effective strategy for preserving task performance.

**Result:** Fine-tuning with LLM-generated data improves target task performance while preserving non-target task robustness, mimicking the effects achieved by masking high perplexity tokens in ground truth data.

**Limitations:** 

**Conclusion:** LLM-generated data can enhance fine-tuning strategies in LLMs by mitigating catastrophic forgetting via token perplexity reduction.

**Abstract:** Maintaining consistent model performance across domains is a fundamental challenge in machine learning. While recent work has explored using LLM-generated data for fine-tuning, its impact on cross-domain generalization remains poorly understood. This paper presents a systematic analysis revealing that fine-tuning with LLM-generated data not only improves target task performance but also reduces non-target task degradation compared to fine-tuning with ground truth data. Through analyzing the data sequence in tasks of various domains, we demonstrate that this enhancement of non-target task robustness stems from the reduction of high perplexity tokens found in LLM-generated sequences. Following our findings, we showed that masking high perplexity tokens in ground truth training data achieves similar non-target task performance preservation, comparable to using LLM-generated data. Extensive experiments across different model families and scales, including Gemma 2 IT 2B, Llama 3 8B Instruct, and 3 additional models, agree with our findings. To the best of our knowledge, this is the first work to provide an empirical explanation based on token perplexity reduction to mitigate catastrophic forgetting in LLMs after fine-tuning, offering valuable insights for developing more robust fine-tuning strategies.

</details>


### [189] [STATE ToxiCN: A Benchmark for Span-level Target-Aware Toxicity Extraction in Chinese Hate Speech Detection](https://arxiv.org/abs/2501.15451)

*Zewen Bai, Shengdi Yin, Junyu Lu, Jingjie Zeng, Haohao Zhu, Yuanyuan Sun, Liang Yang, Hongfei Lin*

**Main category:** cs.CL

**Keywords:** hate speech, Chinese language, dataset, LLMs, hateful slang

**Relevance Score:** 4

**TL;DR:** The paper presents a novel dataset and method for fine-grained detection of hate speech in Chinese, addressing gaps in existing research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of hate speech in society and the lack of effective detection methods for Chinese hate speech, especially with a focus on fine-grained annotations and slang.

**Method:** A dataset named STATE ToxiCN was constructed containing Target-Argument-Hateful-Group quadruples for span-level annotations, and existing models were evaluated for hate speech detection performance.

**Key Contributions:**

	1. Creation of the first span-level Chinese hate speech dataset (STATE ToxiCN).
	2. Evaluation of existing models for span-level hate speech detection.
	3. First study on the detection of Chinese hateful slang using LLMs.

**Result:** Proven effectiveness of the STATE ToxiCN dataset in detecting hate speech and a first evaluation of LLMs on recognizing Chinese hateful slang.

**Limitations:** The research is focused solely on Chinese hate speech and does not cover other languages or broader hate speech contexts.

**Conclusion:** The research provides a crucial resource for the study of hate speech in Chinese, aiding in more effective detection methods.

**Abstract:** The proliferation of hate speech has caused significant harm to society. The intensity and directionality of hate are closely tied to the target and argument it is associated with. However, research on hate speech detection in Chinese has lagged behind, and existing datasets lack span-level fine-grained annotations. Furthermore, the lack of research on Chinese hateful slang poses a significant challenge. In this paper, we provide a solution for fine-grained detection of Chinese hate speech. First, we construct a dataset containing Target-Argument-Hateful-Group quadruples (STATE ToxiCN), which is the first span-level Chinese hate speech dataset. Secondly, we evaluate the span-level hate speech detection performance of existing models using STATE ToxiCN. Finally, we conduct the first study on Chinese hateful slang and evaluate the ability of LLMs to detect such expressions. Our work contributes valuable resources and insights to advance span-level hate speech detection in Chinese.

</details>


### [190] [People who frequently use ChatGPT for writing tasks are accurate and robust detectors of AI-generated text](https://arxiv.org/abs/2501.15654)

*Jenna Russell, Marzena Karpinska, Mohit Iyyer*

**Main category:** cs.CL

**Keywords:** AI detection, human annotation, LLM, natural language processing, text analysis

**Relevance Score:** 8

**TL;DR:** This paper investigates human ability to detect AI-generated text, revealing that frequent users of LLMs excel at this task compared to commercial detectors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to assess human detection capabilities of AI-generated content and improve understanding of text characteristics that distinguish human from AI writing.

**Method:** 300 non-fiction articles were labeled by annotators as human-written or AI-generated, with detailed explanations provided for each decision.

**Key Contributions:**

	1. Demonstrated high accuracy of LLM users in detecting AI-generated text.
	2. Highlighted reliance on lexical and complex textual features for detection.
	3. Released annotated dataset and detection code for further research.

**Result:** Expert annotators who regularly use LLMs misclassify only 1 of the 300 articles, outperforming many commercial detectors, especially against evasion tactics.

**Limitations:** The study focuses solely on non-fiction English articles and may not generalize across all text types or languages.

**Conclusion:** The findings highlight the effectiveness of experienced annotators in identifying AI-generated text and reveal nuanced features that influence detection.

**Abstract:** In this paper, we study how well humans can detect text generated by commercial LLMs (GPT-4o, Claude, o1). We hire annotators to read 300 non-fiction English articles, label them as either human-written or AI-generated, and provide paragraph-length explanations for their decisions. Our experiments show that annotators who frequently use LLMs for writing tasks excel at detecting AI-generated text, even without any specialized training or feedback. In fact, the majority vote among five such "expert" annotators misclassifies only 1 of 300 articles, significantly outperforming most commercial and open-source detectors we evaluated even in the presence of evasion tactics like paraphrasing and humanization. Qualitative analysis of the experts' free-form explanations shows that while they rely heavily on specific lexical clues ('AI vocabulary'), they also pick up on more complex phenomena within the text (e.g., formality, originality, clarity) that are challenging to assess for automatic detectors. We release our annotated dataset and code to spur future research into both human and automated detection of AI-generated text.

</details>


### [191] [Improving LLM Unlearning Robustness via Random Perturbations](https://arxiv.org/abs/2501.19202)

*Dang Huu-Tien, Hoang Thanh-Tung, Anh Bui, Le-Minh Nguyen, Naoya Inoue*

**Main category:** cs.CL

**Keywords:** LLM unlearning, robustness, backdoor attacks, Random Noise Augmentation, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the adverse impact of state-of-the-art LLM unlearning methods on model robustness and proposes Random Noise Augmentation (RNA) to enhance robustness while preserving unlearning performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the reduction in robustness caused by current LLM unlearning methods and understand it through the lens of backdoor attacks.

**Method:** The paper reframes unlearning processes as backdoor attacks and proposes Random Noise Augmentation (RNA) as a method to bolster robustness in unlearned models without extra computational cost.

**Key Contributions:**

	1. Reframing the unlearning process as backdoor attacks and defenses.
	2. Introducing Random Noise Augmentation (RNA) for robustness improvement.
	3. Demonstrating the efficacy of RNA through extensive experiments.

**Result:** Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models and maintains their unlearning performance.

**Limitations:** The method may not generalize to all contexts of model unlearning and robustness in specific scenarios.

**Conclusion:** The proposed RNA method provides a novel approach to mitigating vulnerabilities in unlearned models while ensuring they perform as intended without added complexity.

**Abstract:** In this paper, we show that current state-of-the-art LLM unlearning methods inherently reduce models' robustness, causing them to misbehave even when a single non-adversarial forget-token is in the retain-query. Toward understanding underlying causes, we reframe the unlearning process as backdoor attacks and defenses: forget-tokens act as backdoor triggers that, when activated in retain-queries, cause disruptions in unlearned models' behaviors, similar to successful backdoor attacks. To mitigate this vulnerability, we propose Random Noise Augmentation (RNA) -- a plug-and-play, model and method agnostic approach with theoretical guarantees for improving the robustness of unlearned models. Extensive experiments demonstrate that RNA significantly improves the robustness of unlearned models, maintains unlearning performances while introducing no additional computational overhead.

</details>


### [192] [LongDPO: Unlock Better Long-form Generation Abilities for LLMs via Critique-augmented Stepwise Information](https://arxiv.org/abs/2502.02095)

*Bowen Ping, Jiali Zeng, Fandong Meng, Shuo Wang, Jie Zhou, Shanghang Zhang*

**Main category:** cs.CL

**Keywords:** long-form generation, preference learning, Monte Carlo Tree Search

**Relevance Score:** 9

**TL;DR:** This paper proposes a method to enhance long-form text generation by using process supervision and Monte Carlo Tree Search for preference learning, addressing shortcomings in existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current long-form generation methods struggle with performance and feedback, leading to issues like poor quality and length deviations in generated content.

**Method:** The paper introduces Monte Carlo Tree Search for gathering stepwise preference pairs, and uses a global memory pool to maintain consistency. It also integrates external critiques to improve candidate selection before applying step-level DPO.

**Key Contributions:**

	1. Introduction of process supervision in long-form generation
	2. Use of Monte Carlo Tree Search for preference learning
	3. Integration of external critiques for improving candidate selection

**Result:** Experimental results indicate that the proposed method significantly improves both the length and quality of generated long-form text, performing nearly optimally on general benchmarks across multiple model backbones.

**Limitations:** 

**Conclusion:** The incorporation of process supervision and refined preference pairs notably enhances long-form generation results, making it a step forward in model performance.

**Abstract:** Long-form generation is crucial for academic writing papers and repo-level code generation. Despite this, current models, including GPT-4o, still exhibit unsatisfactory performance. Existing methods that utilize preference learning with outcome supervision often fail to provide detailed feedback for extended contexts. This shortcoming can lead to content that does not fully satisfy query requirements, resulting in issues like length deviations, and diminished quality. In this paper, we propose enhancing long-form generation by incorporating process supervision. We employ Monte Carlo Tree Search to gather stepwise preference pairs, utilizing a global memory pool to maintain consistency. To address the issue of suboptimal candidate selection, we integrate external critiques to refine and improve the quality of the preference pairs. Finally, we apply step-level DPO using the collected stepwise preference pairs. Experimental results show that our method improves length and quality on long-form generation benchmarks, with almost lossless performance on general benchmarks across various model backbones.

</details>


### [193] [Premise-Augmented Reasoning Chains Improve Error Identification in Math reasoning with LLMs](https://arxiv.org/abs/2502.02362)

*Sagnik Mukherjee, Abhinav Chinta, Takyoung Kim, Tarun Anoop Sharma, Dilek Hakkani-Tür*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought prompting, Large language models, Premise Augmented Reasoning Chains

**Relevance Score:** 8

**TL;DR:** This paper introduces Premise Augmented Reasoning Chains (PARC) which improve the evaluation of mathematical reasoning in LLMs by restructuring reasoning chains into a directed acyclic graph format with premise links.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance mathematical reasoning in LLMs while addressing issues of verbosity and complexity in traditional reasoning chains.

**Method:** A framework is proposed that restructures reasoning chains into Premise Augmented Reasoning Chains, forming a directed acyclic graph with nodes as reasoning steps and edges as premise links.

**Key Contributions:**

	1. Introduction of Premise Augmented Reasoning Chains (PARC)
	2. Demonstration of high recall in premise identification by LLMs
	3. Improvement in error identification accuracy through step-by-step verification in PARC.

**Result:** LLMs can reliably identify premises within complex reasoning chains, achieving 90% recall in premise identification, and the accuracy of error identification improves by 6% to 16% when using PARC.

**Limitations:** 

**Conclusion:** The findings suggest that premise-centric representations can significantly improve the reliability of LLM-based reasoning evaluations, paving the way for better problem-solving methods.

**Abstract:** Chain-of-Thought (CoT) prompting enhances mathematical reasoning in large language models (LLMs) by enabling detailed step-by-step solutions. However, due to the verbosity of LLMs, the resulting reasoning chains can be long, making it harder to verify the reasoning steps and trace issues resulting from dependencies between the steps that may be farther away in the sequence of steps. Importantly, mathematical reasoning allows each step to be derived from a small set of premises, which are a subset of the preceding steps in the reasoning chain. In this paper, we present a framework that identifies the premises for each step, to improve the evaluation of reasoning. We restructure conventional linear reasoning chains into Premise Augmented Reasoning Chains (PARC) by introducing premise links, resulting in a directed acyclic graph where the nodes are the steps and the edges are the premise links. Through experiments with a PARC-based dataset that we built, namely PERL (Premises and ERrors identification in LLMs), we demonstrate that LLMs can reliably identify premises within complex reasoning chains. In particular, even open-source LLMs achieve 90% recall in premise identification. We also show that PARC helps to identify errors in reasoning chains more reliably. The accuracy of error identification improves by 6% to 16% absolute when step-by-step verification is carried out in PARC under the premises. Our findings highlight the utility of premise-centric representations in addressing complex problem-solving tasks and open new avenues for improving the reliability of LLM-based reasoning evaluations.

</details>


### [194] [A comparison of translation performance between DeepL and Supertext](https://arxiv.org/abs/2502.02577)

*Alex Flückiger, Chantal Amrhein, Tim Graf, Frédéric Odermatt, Martin Pömsl, Philippe Schläpfer, Florian Schottmann, Samuel Läubli*

**Main category:** cs.CL

**Keywords:** machine translation, large language models, context-sensitive evaluation

**Relevance Score:** 6

**TL;DR:** This study evaluates the performance of two machine translation systems—DeepL and Supertext—using professional translators to assess translation quality across extended context, revealing a preference for Supertext in longer texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the reliance on large language models for machine translation, there is a need for better quality benchmarking that captures system performance on extended context.

**Method:** The study compares DeepL and Supertext by evaluating their translation quality on unsegmented texts, using professional translators to perform assessments based on document-level context.

**Key Contributions:**

	1. Comparison of two commercial MT systems focusing on document-level context
	2. Release of evaluation data and scripts for further research
	3. Advocacy for more context-sensitive methodologies in MT quality assessment

**Result:** Segment-level assessments show no strong preference between the systems, but document-level analysis indicates Supertext performs better in three out of four language directions, suggesting better consistency.

**Limitations:** 

**Conclusion:** The findings highlight the importance of context-sensitive evaluation methodologies for machine translation systems to reflect usability in real-world scenarios.

**Abstract:** As strong machine translation (MT) systems are increasingly based on large language models (LLMs), reliable quality benchmarking requires methods that capture their ability to leverage extended context. This study compares two commercial MT systems -- DeepL and Supertext -- by assessing their performance on unsegmented texts. We evaluate translation quality across four language directions with professional translators assessing segments with full document-level context. While segment-level assessments indicate no strong preference between the systems in most cases, document-level analysis reveals a preference for Supertext in three out of four language directions, suggesting superior consistency across longer texts. We advocate for more context-sensitive evaluation methodologies to ensure that MT quality assessments reflect real-world usability. We release all evaluation data and scripts for further analysis and reproduction at https://github.com/supertext/evaluation_deepl_supertext.

</details>


### [195] [Speculative Prefill: Turbocharging TTFT with Lightweight and Training-Free Token Importance Estimation](https://arxiv.org/abs/2502.02789)

*Jingyu Liu, Beidi Chen, Ce Zhang*

**Main category:** cs.CL

**Keywords:** large language models, time-to-first-token, inference optimization, SpecPrefill, machine learning

**Relevance Score:** 9

**TL;DR:** SpecPrefill is a training-free framework designed to improve time-to-first-token (TTFT) during LLM inference, achieving significant performance boosts for both long and medium context queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize TTFT and enhance maximal QPS for critical applications in modern LLM inference engines.

**Method:** SpecPrefill employs a lightweight model to identify locally important tokens from a prompt, which are then processed by the main model, aiming to streamline inference without additional training.

**Key Contributions:**

	1. Training-free framework for LLM inference optimization
	2. Use of a lightweight model for token speculation
	3. Achieved substantial improvements in TTFT and QPS

**Result:** SpecPrefill showed up to 7× improvement in end-to-end QPS and a 7.66× enhancement in TTFT on diverse real-world tasks and through ablation studies.

**Limitations:** 

**Conclusion:** The framework successfully boosts TTFT performance by focusing on relevant prompt subsets, thereby enhancing the efficiency of LLMs in practical applications.

**Abstract:** Improving time-to-first-token (TTFT) is an essentially important objective in modern large language model (LLM) inference engines. Optimizing TTFT directly results in higher maximal QPS and meets the requirements of many critical applications. However, boosting TTFT is notoriously challenging since it is compute-bounded and the performance bottleneck shifts from the self-attention that many prior works focus on to the MLP part. In this work, we present SpecPrefill, a training free framework that accelerates the inference TTFT for both long and medium context queries based on the following insight: LLMs are generalized enough to preserve the quality given only a carefully chosen subset of prompt tokens. At its core, SpecPrefill leverages a lightweight model to speculate locally important tokens based on the context. These tokens, along with the necessary positional information, are then sent to the main model for processing. We evaluate SpecPrefill with a diverse set of tasks, followed by a comprehensive benchmarking of performance improvement both in a real end-to-end setting and ablation studies. SpecPrefill manages to serve Llama-3.1-405B-Instruct-FP8 with up to 7$\times$ maximal end-to-end QPS on real downstream tasks and 7.66$\times$ TTFT improvement.

</details>


### [196] [MMUnlearner: Reformulating Multimodal Machine Unlearning in the Era of Multimodal Large Language Models](https://arxiv.org/abs/2502.11051)

*Jiahao Huo, Yibo Yan, Xu Zheng, Yuanhuiyi Lyu, Xin Zou, Zhihua Wei, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Machine Unlearning, Multimodal Large Language Models, gradient ascent

**Relevance Score:** 7

**TL;DR:** This paper introduces a method for Machine Unlearning in Multimodal Large Language Models, focusing on selectively removing visual patterns while preserving textual knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating the need for effective methods to remove sensitive visual information from Multimodal Large Language Models while maintaining their essential textual knowledge.

**Method:** A geometry-constrained gradient ascent method called MMUnlearner is developed, utilizing a weight saliency map to update MLLMs' weights during the unlearning process.

**Key Contributions:**

	1. Introduces MMUnlearner for multimodal machine unlearning.
	2. Preserves textual knowledge during the erasure of visual patterns.
	3. Surpasses existing baseline models in performance.

**Result:** MMUnlearner demonstrates superior performance compared to traditional methods like Gradient Ascent and Negative Preference Optimization across various evaluation metrics.

**Limitations:** 

**Conclusion:** The proposed method enables efficient selective unlearning in Multimodal Large Language Models without compromising critical textual information.

**Abstract:** Recent progress in Machine Unlearning (MU) has introduced solutions for the selective removal of private or sensitive information encoded within deep neural networks. Nonetheless, MU for Multimodal Large Language Models (MLLMs) remains in its nascent phase. Therefore, we propose to reformulate the task of multimodal MU in the era of MLLMs, which aims to erase only the visual patterns associated with a given entity while preserving the corresponding textual knowledge encoded within the original parameters of the language model backbone. Furthermore, we develop a novel geometry-constrained gradient ascent method MMUnlearner. It updates the weights of MLLMs with a weight saliency map jointly restricted by the remaining concepts and textual knowledge during unlearning, thereby preserving parameters essential for non-target knowledge. Extensive experiments demonstrate that MMUnlearner surpasses baselines that finetuning MLLMs with VQA data directly through Gradient Ascent (GA) or Negative Preference Optimization (NPO), across all evaluation dimensions. Our code will be released upon acceptance.

</details>


### [197] [CARMA: Enhanced Compositionality in LLMs via Advanced Regularisation and Mutual Information Alignment](https://arxiv.org/abs/2502.11066)

*Nura Aljaafari, Danilo S. Carvalho, André Freitas*

**Main category:** cs.CL

**Keywords:** Compositional Generalization, Large Language Models, Mutual Information Regularization, Stability Constraints, Fine-Tuning

**Relevance Score:** 9

**TL;DR:** The paper introduces CARMA, an intervention aimed at enhancing compositional reasoning in large language models (LLMs) by improving stability and robustness while preserving performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with compositional generalisation, limiting their ability to interpret novel inputs effectively. Previous methods often fail due to scalability issues or diminishing returns.

**Method:** CARMA employs mutual information regularisation and layer-wise stability constraints to improve the robustness of compositional reasoning in LLMs and mitigate feature fragmentation.

**Key Contributions:**

	1. Introduces CARMA for stability in compositional reasoning in LLMs.
	2. Utilizes mutual information regularisation and layer-wise constraints to mitigate feature fragmentation.
	3. Demonstrates improved semantic consistency and robustness across various tasks.

**Result:** CARMA improves semantic consistency, performance stability, and robustness against lexical perturbations in tasks like inverse dictionary modelling and sentiment classification, reducing variability introduced by fine-tuning.

**Limitations:** Effectiveness varies across different architectural frameworks; not a one-size-fits-all solution.

**Conclusion:** Integrating CARMA with fine-tuning enhances compositional generalisation in LLMs while maintaining task-specific performance, reinforcing learned structures rather than adding new capabilities.

**Abstract:** Large language models (LLMs) struggle with compositional generalisation, limiting their ability to systematically combine learned components to interpret novel inputs. While architectural modifications, fine-tuning, and data augmentation improve compositionality, they often have limited adaptability, face scalability constraints, or yield diminishing returns on real data. To address this, we propose CARMA, an intervention that enhances the stability and robustness of compositional reasoning in LLMs while preserving fine-tuned performance. CARMA employs mutual information regularisation and layer-wise stability constraints to mitigate feature fragmentation, ensuring structured representations persist across and within layers. We evaluate CARMA on inverse dictionary modelling and sentiment classification, measuring its impact on semantic consistency, performance stability, and robustness to lexical perturbations. Results show that CARMA reduces the variability introduced by fine-tuning, stabilises token representations, and improves compositional reasoning. While its effectiveness varies across architectures, CARMA's key strength lies in reinforcing learned structures rather than introducing new capabilities, making it a scalable auxiliary method. These findings suggest that integrating CARMA with fine-tuning can improve compositional generalisation while maintaining task-specific performance in LLMs.

</details>


### [198] [Towards Achieving Concept Completeness for Textual Concept Bottleneck Models](https://arxiv.org/abs/2502.11100)

*Milan Bhan, Yann Choho, Pierre Moreau, Jean-Noel Vittaut, Nicolas Chesneau, Marie-Jeanne Lesot*

**Main category:** cs.CL

**Keywords:** Textual Concept Bottleneck Models, interpretability, unsupervised learning

**Relevance Score:** 7

**TL;DR:** CT-CBM is an unsupervised model for text classification that improves interpretability and performance by predicting salient concepts before final predictions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance interpretability in text classification models without relying on predefined concepts or extensive human annotations.

**Method:** CT-CBM generates concept labels using a small language model in an unsupervised manner and iteratively adds important concepts to a bottleneck layer.

**Key Contributions:**

	1. Introduction of a fully unsupervised TCBM generator
	2. Elimination of predefined human labeled concepts
	3. Addressing downstream classification leakage through parallel residual connections.

**Result:** CT-CBM demonstrates competitive results against other models, effectively addressing classification leakage.

**Limitations:** 

**Conclusion:** CT-CBM represents a promising approach for improving the interpretability of NLP classifiers while maintaining high performance.

**Abstract:** Textual Concept Bottleneck Models (TBMs) are interpretable-by-design models for text classification that predict a set of salient concepts before making the final prediction. This paper proposes Complete Textual Concept Bottleneck Model (CT-CBM),a novel TCBM generator building concept labels in a fully unsupervised manner using a small language model, eliminating both the need for predefined human labeled concepts and LLM annotations. CT-CBM iteratively targets and adds important concepts in the bottleneck layer to create a complete concept basis and addresses downstream classification leakage through a parallel residual connection. CT-CBM achieves good results against competitors, offering a promising solution to enhance interpretability of NLP classifiers without sacrificing performance.

</details>


### [199] [Plant in Cupboard, Orange on Rably, Inat Aphone. Benchmarking Incremental Learning of Situation and Language Model using a Text-Simulated Situated Environment](https://arxiv.org/abs/2502.11733)

*Jonathan Jordan, Sherzod Hakimov, David Schlangen*

**Main category:** cs.CL

**Keywords:** Large Language Models, incremental learning, in-context learning, agent systems, synthetic words

**Relevance Score:** 8

**TL;DR:** The paper evaluates the incremental learning and controlled in-context learning capabilities of large language models (LLMs) in agent systems through experiments involving task-solving in a simulated environment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of Large Language Models (LLMs) as effective components in agent systems, particularly focusing on their learning capabilities in interactive environments.

**Method:** The study employs a set of experiments to assess i) agents' ability to incrementally discover and solve tasks related to everyday objects in a simulated house environment, ii) the efficiency of in-context learning with provided information about object locations, and iii) the inference abilities of LLMs with unfamiliar synthetic words based on environmental feedback.

**Key Contributions:**

	1. Introduction of a new experimental framework for testing LLMs in agent systems
	2. Insights into the learning capabilities of LLMs in interactive environments
	3. Comparison of performance between commercial and open-weight LLMs.

**Result:** Findings indicate that larger commercial LLMs perform significantly better than open-weight models, yet all models face challenges when working with synthetic pseudo-English words.

**Limitations:** The experiments predominantly focus on specific environments and tasks, which may not generalize to broader applications of LLMs in other domains.

**Conclusion:** The results suggest a considerable gap in performance based on model type and highlight the struggles LLMs have with learning from synthetic word inputs, pointing to areas for future improvement in model training and architecture.

**Abstract:** Large Language Models (LLMs) serve not only as chatbots but as key components in agent systems, where their common-sense knowledge significantly impacts performance as language-based planners for situated or embodied action. We assess LLMs' incremental learning (based on feedback from the environment), and controlled in-context learning abilities using a text-based environment. We introduce challenging yet interesting set of experiments to test i) how agents can incrementally solve tasks related to every day objects in typical rooms in a house where each of them are discovered by interacting within the environment, ii) controlled in-context learning abilities and efficiency of agents by providing short info about locations of objects and rooms to check how faster the task can be solved, and finally iii) using synthetic pseudo-English words to gauge how well LLMs are at inferring meaning of unknown words from environmental feedback. Results show that larger commercial models have a substantial gap in performance compared to open-weight but almost all models struggle with the synthetic words experiments.

</details>


### [200] [FineFilter: A Fine-grained Noise Filtering Mechanism for Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2502.11811)

*Qianchi Zhang, Hainan Zhang, Liang Pang, Ziwei Wang, Hongwei Zheng, Yongxin Tong, Zhiming Zheng*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, noise filtering, clue extraction, machine learning, question answering

**Relevance Score:** 9

**TL;DR:** FineFilter is a novel noise filtering mechanism for Retrieval-Augmented Generation (RAG) that improves QA performance by optimizing clue extraction, reranking, and truncation.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Existing noise filtering methods for RAG struggle to accurately locate answer clues from large and complex documents, requiring improved mechanisms.

**Method:** FineFilter employs a three-step approach: a clue extractor identifies potential clues, a reranker prioritizes them based on feedback from the generation module, and a truncator reduces to the minimum necessary clues for answering questions.

**Key Contributions:**

	1. Introduction of FineFilter as a fine-grained noise filtering mechanism for RAG
	2. Optimized modules for clue extraction, prioritization, and truncation
	3. Demonstrated effectiveness through extensive experiments on QA datasets

**Result:** Experiments show that FineFilter significantly enhances QA performance over baselines on LLaMA3 and Mistral across three QA datasets.

**Limitations:** 

**Conclusion:** FineFilter is effective in complex reasoning, demonstrates robustness against unreliable retrieval, and generalizes well to different scenarios.

**Abstract:** Retrieved documents containing noise will hinder Retrieval-Augmented Generation (RAG) from detecting answer clues, necessitating noise filtering mechanisms to enhance accuracy. Existing methods use reranking or summarization to identify the most relevant sentences, but directly and accurately locating answer clues from these large-scale and complex documents remains challenging. Unlike these document-level operations, we treat noise filtering as a sentence-level MinMax optimization problem: first identifying potential clues from multiple documents, then ranking them by relevance, and finally retaining the minimum number of clues through truncation. In this paper, we propose FineFilter, a novel fine-grained noise filtering mechanism for RAG, consisting of a clue extractor, a reranker, and a truncator. We optimize each module to tackle complex reasoning challenges: (1) The clue extractor first uses sentences containing the answer and similar ones as fine-tuning targets, aiming to extract sufficient potential clues; (2) The reranker is trained to prioritize effective clues based on the real feedback from the generation module, with clues capable of generating correct answers as positive samples and others as negative; (3) The truncator takes the minimum number of clues needed to answer the question (truncation point) as fine-tuning targets, and performs truncation on the reranked clues to achieve fine-grained noise filtering. Experiments on three QA datasets demonstrate that FineFilter significantly improves QA performance over baselines on both LLaMA3 and Mistral. Further analysis confirms its effectiveness in complex reasoning, robustness to unreliable retrieval, and generalization to different scenarios.

</details>


### [201] [EssayJudge: A Multi-Granular Benchmark for Assessing Automated Essay Scoring Capabilities of Multimodal Large Language Models](https://arxiv.org/abs/2502.11916)

*Jiamin Su, Yibo Yan, Fangteng Fu, Han Zhang, Jingheng Ye, Xiang Liu, Jiahao Huo, Huiyu Zhou, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Multimodal Large Language Models, Educational Assessment, Natural Language Processing, Discourse Evaluation

**Relevance Score:** 8

**TL;DR:** The paper introduces EssayJudge, a multimodal benchmark for Automated Essay Scoring (AES) utilizing Multimodal Large Language Models (MLLMs) to enhance evaluation precision across various writing traits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by traditional Automated Essay Scoring systems, including reliance on handcrafted features and difficulties in evaluating complex writing traits and multimodal contexts.

**Method:** The authors developed EssayJudge as a multimodal benchmark to test AES capabilities, leveraging MLLMs for trait-specific scoring and context understanding.

**Key Contributions:**

	1. Introduction of EssayJudge, a multimodal AES benchmark
	2. Evaluation of 18 MLLMs on AES tasks
	3. Identification of performance gaps compared to human evaluation

**Result:** Experiments with 18 representative MLLMs showed performance gaps in AES compared to human evaluations, especially in assessing discourse-level traits.

**Limitations:** Focus on multimodal contexts may limit applicability to traditional single-modal assessment scenarios.

**Conclusion:** The research highlights significant limitations in current MLLM-based AES systems and signals the need for further advancements in this field.

**Abstract:** Automated Essay Scoring (AES) plays a crucial role in educational assessment by providing scalable and consistent evaluations of writing tasks. However, traditional AES systems face three major challenges: (1) reliance on handcrafted features that limit generalizability, (2) difficulty in capturing fine-grained traits like coherence and argumentation, and (3) inability to handle multimodal contexts. In the era of Multimodal Large Language Models (MLLMs), we propose EssayJudge, the first multimodal benchmark to evaluate AES capabilities across lexical-, sentence-, and discourse-level traits. By leveraging MLLMs' strengths in trait-specific scoring and multimodal context understanding, EssayJudge aims to offer precise, context-rich evaluations without manual feature engineering, addressing longstanding AES limitations. Our experiments with 18 representative MLLMs reveal gaps in AES performance compared to human evaluation, particularly in discourse-level traits, highlighting the need for further advancements in MLLM-based AES research.

</details>


### [202] [R2-KG: General-Purpose Dual-Agent Framework for Reliable Reasoning on Knowledge Graphs](https://arxiv.org/abs/2502.12767)

*Sumin Jo, Junseong Choi, Jiho Kim, Edward Choi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Reasoning, Abstention Mechanism, Artificial Intelligence

**Relevance Score:** 9

**TL;DR:** R2-KG is a dual-agent framework that enhances reasoning accuracy in LLMs with a cost-efficient approach, separating roles into an Operator and a Supervisor, and incorporates an Abstention mechanism to improve reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing frameworks combining LLMs with Knowledge Graphs, specifically the need for re-tuning and dependence on a single high-capacity LLM for reliable reasoning.

**Method:** R2-KG divides reasoning tasks into two roles: a low-capacity LLM (Operator) gathers evidence while a high-capacity LLM (Supervisor) makes judgments. It also implements an Abstention mechanism to ensure responses are given only when adequate evidence is available.

**Key Contributions:**

	1. Introduces a dual-agent framework for improved reasoning in LLMs.
	2. Employs a cost-effective mechanism that mitigates reliance on high-capacity models.
	3. Includes an Abstention mechanism to enhance response reliability.

**Result:** R2-KG outperforms existing baselines in accuracy and reliability across five benchmarks, demonstrating reduced reliance on high-capacity LLMs and establishing a cost-effective solution for Knowledge Graph-based reasoning.

**Limitations:** 

**Conclusion:** R2-KG is effective in offering a flexible framework for reasoning tasks that balances cost and reliability, suggesting its potential for real-world applications in various fields, including health informatics.

**Abstract:** Recent studies have combined Large Language Models (LLMs) with Knowledge Graphs (KGs) to enhance reasoning, improving inference accuracy without additional training while mitigating hallucination. However, existing frameworks still suffer two practical drawbacks: they must be re-tuned whenever the KG or reasoning task changes, and they depend on a single, high-capacity LLM for reliable (i.e., trustworthy) reasoning. To address this, we introduce R2-KG, a plug-and-play, dual-agent framework that separates reasoning into two roles: an Operator (a low-capacity LLM) that gathers evidence and a Supervisor (a high-capacity LLM) that makes final judgments. This design is cost-efficient for LLM inference while still maintaining strong reasoning accuracy. Additionally, R2-KG employs an Abstention mechanism, generating answers only when sufficient evidence is collected from KG, which significantly enhances reliability. Experiments across five diverse benchmarks show that R2-KG consistently outperforms baselines in both accuracy and reliability, regardless of the inherent capability of LLMs used as the Operator. Further experiments reveal that the single-agent version of R2-KG, equipped with a strict self-consistency strategy, achieves significantly higher-than-baseline reliability with reduced inference cost but increased abstention rate in complex KGs. Our findings establish R2-KG as a flexible and cost-effective solution for KG-based reasoning, reducing reliance on high-capacity LLMs while ensuring trustworthy inference. The code is available at https://github.com/ekrxjwh2009/R2-KG/.

</details>


### [203] [Robust Adaptation of Large Multimodal Models for Retrieval Augmented Hateful Meme Detection](https://arxiv.org/abs/2502.13061)

*Jingbiao Mei, Jinghong Chen, Guangyu Yang, Weizhe Lin, Bill Byrne*

**Main category:** cs.CL

**Keywords:** hateful memes, LMMs, detection system

**Relevance Score:** 6

**TL;DR:** A robust framework for detecting hateful memes using LMMs, improving accuracy and generalization while maintaining vision-language capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of hateful memes on the Internet underscores the need for effective detection systems, particularly given the shortcomings of current LMMs in performance and generalization.

**Method:** We propose an adaptation framework that enhances both in-domain accuracy for hateful meme detection and cross-domain generalization, overcoming limitations of previous SFT and in-context learning methods in LMMs.

**Key Contributions:**

	1. Robust adaptation framework for LMMs in hateful meme detection
	2. State-of-the-art performance on meme classification datasets
	3. Improved interpretability through high-quality rationale generation

**Result:** Our framework demonstrates state-of-the-art performance on six classification datasets and produces higher-quality rationales for explaining hate content compared to standard methods.

**Limitations:** 

**Conclusion:** The proposed approach significantly improves hateful meme detection and model interpretability, marking a step forward in developing effective automated detection systems.

**Abstract:** Hateful memes have become a significant concern on the Internet, necessitating robust automated detection systems. While LMMs have shown promise in hateful meme detection, they face notable challenges like sub-optimal performance and limited out-of-domain generalization capabilities. Recent studies further reveal the limitations of both SFT and in-context learning when applied to LMMs in this setting. To address these issues, we propose a robust adaptation framework for hateful meme detection that enhances in-domain accuracy and cross-domain generalization while preserving the general vision-language capabilities of LMMs. Experiments on six meme classification datasets show that our approach achieves state-of-the-art performance, outperforming larger agentic systems. Moreover, our method generates higher-quality rationales for explaining hateful content compared to standard SFT, enhancing model interpretability.

</details>


### [204] [TreeCut: A Synthetic Unanswerable Math Word Problem Dataset for LLM Hallucination Evaluation](https://arxiv.org/abs/2502.13442)

*Jialin Ouyang*

**Main category:** cs.CL

**Keywords:** large language models, math word problems, dataset generation

**Relevance Score:** 9

**TL;DR:** TreeCut is a synthetic dataset designed to generate unanswerable math word problems, revealing the limitations of large language models in reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate and highlight the reasoning shortcomings of large language models when faced with unanswerable math problems.

**Method:** TreeCut generates math word problems by representing them as trees and strategically removing necessary conditions, thus creating both answerable and unanswerable instances.

**Key Contributions:**

	1. Introduction of the TreeCut synthetic dataset
	2. Demonstration of high hallucination rates in LLMs
	3. Identification of factors influencing hallucination likelihood.

**Result:** Experimental results demonstrated that models like GPT-4o and o3-mini showed high hallucination rates of 64% and 44% respectively for unanswerable problems in worst-case scenarios.

**Limitations:** The dataset is synthetic and may not fully capture the diversity of real-world problems.

**Conclusion:** The findings emphasize the ongoing challenges for LLMs in accurately identifying unanswerable math problems, calling for improved model capabilities in reasoning.

**Abstract:** Large language models (LLMs) now achieve near-human performance on standard math word problem benchmarks (e.g., GSM8K), yet their true reasoning ability remains disputed. A key concern is that models often produce confident, yet unfounded, answers to unanswerable problems. We introduce TreeCut, a synthetic dataset that systematically generates infinite unanswerable math word problems and their answerable counterparts, by representing each question as a tree and removing chosen necessary conditions. Experiments show TreeCut effectively induce hallucinations in large language models, including GPT-4o and o3-mini, with rates of 64% and 44% in their respective worst-case scenarios under zero-shot setting. Further analysis highlights that deeper or more complex trees, composite item names, and removing necessary condition near the middle of a path all increase the likelihood of hallucinations, underscoring the persistent challenges LLMs face in identifying unanswerable math problems. The dataset generation code and sample data are available at https://github.com/j-bagel/treecut-math.

</details>


### [205] [DiffSampling: Enhancing Diversity and Accuracy in Neural Text Generation](https://arxiv.org/abs/2502.14037)

*Giorgio Franceschelli, Mirco Musolesi*

**Main category:** cs.CL

**Keywords:** language models, decoding methods, text generation, probability distribution, machine learning

**Relevance Score:** 8

**TL;DR:** The paper proposes three new decoding methods for language models to improve text generation quality and diversity by leveraging a mathematical analysis of token probability distributions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Language models often produce repetitive or overly common text due to limitations in their decoding strategies, prompting the need for improved methods that enhance output diversity and correctness.

**Method:** The authors introduce three decoding methods that analyze the difference between consecutive sorted token probabilities to truncate unlikely candidates, aiming to produce more contextually relevant text.

**Key Contributions:**

	1. Introduction of three novel decoding methods for language models
	2. Mathematical analysis of token probability distribution for improved text generation
	3. Demonstrated performance across various tasks indicates enhanced output quality and diversity

**Result:** Experiments in math problem solving, extreme summarization, and divergent association tasks show the new methods perform at least as well as existing techniques, offering comparable quality and improved diversity in generated text.

**Limitations:** 

**Conclusion:** The proposed decoding strategies enhance the ability of language models to generate diverse and accurate outputs, addressing common limitations in existing methods.

**Abstract:** Despite their growing capabilities, language models still frequently reproduce content from their training data, generate repetitive text, and favor common grammatical patterns and vocabulary. A possible cause is the decoding strategy: the most common strategies either consider only the most probable tokens, which reduces output diversity, or increase the likelihood of unlikely tokens, compromising output accuracy and correctness. In this paper, we propose three new decoding methods that leverage a mathematical analysis of the token probability distribution to ensure the generation of contextually appropriate text. In particular, the difference between consecutive, sorted probabilities can be used to truncate incorrect tokens. Experiments concerning math problem solving, extreme summarization, and the divergent association task demonstrate that our approach consistently performs at least as well as existing methods in terms of quality and diversity.

</details>


### [206] [Enhancing Conversational Agents with Theory of Mind: Aligning Beliefs, Desires, and Intentions for Human-Like Interaction](https://arxiv.org/abs/2502.14171)

*Mehdi Jafari, Devin Yuncheng Hua, Hao Xue, Flora Salim*

**Main category:** cs.CL

**Keywords:** Theory of Mind, Large Language Models, Conversational Agents, Response Quality, Natural Language Interaction

**Relevance Score:** 9

**TL;DR:** Study examines how open-source LLaMA models can capture Theory of Mind (ToM) related information to improve response quality in AI interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the limitations of current LLM systems in aligning communication with human mental states using Theory of Mind.

**Method:** Experiments conducted on two LLaMA 3 variants to see how ToM aligned strategies affect AI response quality, focusing on beliefs, desires, and intentions.

**Key Contributions:**

	1. Analysis of ToM in LLaMA models
	2. Demonstrated enhancement in LLM response quality through ToM alignment
	3. Empirical results showing win rates for different model sizes

**Result:** ToM informed alignment improved response quality with win rates of 67% for the 3B model and 63% for the 8B model.

**Limitations:** Limited to open-source LLaMA models; results may vary with different architectures or datasets.

**Conclusion:** ToM driven strategies can significantly enhance alignment and response quality in conversational agents powered by LLMs.

**Abstract:** Natural language interaction with agentic Artificial Intelligence (AI), driven by Large Language Models (LLMs), is expected to remain a dominant paradigm in the near future. While humans instinctively align their communication with mental states -- an ability known as Theory of Mind (ToM), current LLM powered systems exhibit significant limitations in this regard. This study examines the extent to which open source language models (LLaMA) can capture and preserve ToM related information and how effectively it contributes to consistent ToM reasoning in generated responses. We further investigate whether explicit manipulation of ToM related components, such as beliefs, desires, and intentions, can enhance response alignment. Experiments on two LLaMA 3 variants demonstrate that incorporating ToM informed alignment improves response quality, achieving win rates of 67 and 63 percent for the 3B and 8B models, respectively. These findings highlight the potential of ToM driven strategies to improve alignment in LLM based conversational agents.

</details>


### [207] [Moving Beyond Medical Exam Questions: A Clinician-Annotated Dataset of Real-World Tasks and Ambiguity in Mental Healthcare](https://arxiv.org/abs/2502.16051)

*Max Lamparth, Declan Grabb, Amy Franks, Scott Gershan, Kaitlyn N. Kunstman, Aaron Lulla, Monika Drummond Roots, Manu Sharma, Aryan Shrivastava, Nina Vasan, Colleen Waickman*

**Main category:** cs.CL

**Keywords:** mental health, language models, dataset, clinical decision-making, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a dataset for evaluating medical language models in mental healthcare, capturing the complexities of clinical decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the oversimplification of medical LMs that focus primarily on board exam questions rather than real-world clinical tasks.

**Method:** An expert-created dataset involving 203 base questions across five domains of decision-making in mental healthcare, designed to reflect real-world complexities and ambiguities.

**Key Contributions:**

	1. Creation of a nuanced dataset reflecting complex clinical reasoning in mental health care.
	2. Evaluation of multiple LMs against this dataset, highlighting relevant metrics on task accuracy and demographic influences.
	3. The introduction of a preference dataset dealing with ambiguities and multiple valid answers.

**Result:** Evaluation of eleven off-the-shelf and four fine-tuned mental health LMs on task accuracy while considering how patient demographics impact decision-making.

**Limitations:** 

**Conclusion:** The proposed dataset serves as a valuable resource for enhancing the assessment of LMs in mental healthcare, focusing on the real challenges faced by practitioners.

**Abstract:** Current medical language model (LM) benchmarks often over-simplify the complexities of day-to-day clinical practice tasks and instead rely on evaluating LMs on multiple-choice board exam questions. Thus, we present an expert-created and annotated dataset spanning five critical domains of decision-making in mental healthcare: treatment, diagnosis, documentation, monitoring, and triage. This dataset - created without any LM assistance - is designed to capture the nuanced clinical reasoning and daily ambiguities mental health practitioners encounter, reflecting the inherent complexities of care delivery that are missing from existing datasets. Almost all 203 base questions with five answer options each have had the decision-irrelevant demographic patient information removed and replaced with variables (e.g., AGE), and are available for male, female, or non-binary-coded patients. For question categories dealing with ambiguity and multiple valid answer options, we create a preference dataset with uncertainties from the expert annotations. We outline a series of intended use cases and demonstrate the usability of our dataset by evaluating eleven off-the-shelf and four mental health fine-tuned LMs on category-specific task accuracy, on the impact of patient demographic information on decision-making, and how consistently free-form responses deviate from human annotated samples.

</details>


### [208] [SQLong: Enhanced NL2SQL for Longer Contexts with LLMs](https://arxiv.org/abs/2502.16747)

*Dai Quoc Nguyen, Cong Duy Vu Hoang, Duy Vu, Gioacchino Tangari, Thanh Tien Vu, Don Dharmasiri, Yuan-Fang Li, Long Duong*

**Main category:** cs.CL

**Keywords:** large language models, NL2SQL, data augmentation, machine learning, context length

**Relevance Score:** 8

**TL;DR:** SQLong is a data augmentation framework that enhances LLM performance in NL2SQL tasks by simulating long-context scenarios with synthetic data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of LLMs in NL2SQL tasks decreases with large database schemas due to increased context length.

**Method:** SQLong generates augmented datasets by adding synthetic CREATE TABLE commands and data rows to existing database schemas, facilitating long-context finetuning and evaluation.

**Key Contributions:**

	1. Introduces SQLong, a novel data augmentation framework for NL2SQL tasks.
	2. Demonstrates significant performance improvements in LLMs for long-context scenarios.
	3. Provides a practical solution to enhance SQL query generation with complex database schemas.

**Result:** Experiments show that LLMs fine-tuned with SQLong-augmented data significantly outperform those trained on standard datasets.

**Limitations:** 

**Conclusion:** SQLong shows practical applicability and improves NL2SQL capabilities in complex real-world database scenarios.

**Abstract:** Open-weight large language models (LLMs) have significantly advanced performance in the Natural Language to SQL (NL2SQL) task. However, their effectiveness diminishes when dealing with large database schemas, as the context length increases. To address this limitation, we present SQLong, a novel and efficient data augmentation framework designed to enhance LLM performance in long-context scenarios for the NL2SQL task. SQLong generates augmented datasets by extending existing database schemas with additional synthetic CREATE TABLE commands and corresponding data rows, sampled from diverse schemas in the training data. This approach effectively simulates long-context scenarios during finetuning and evaluation. Through experiments on the Spider and BIRD datasets, we demonstrate that LLMs finetuned with SQLong-augmented data significantly outperform those trained on standard datasets. These imply SQLong's practical implementation and its impact on improving NL2SQL capabilities in real-world settings with complex database schemas.

</details>


### [209] [Make LoRA Great Again: Boosting LoRA with Adaptive Singular Values and Mixture-of-Experts Optimization Alignment](https://arxiv.org/abs/2502.16894)

*Chenghao Fan, Zhenyi Lu, Sichen Liu, Chengfeng Gu, Xiaoye Qu, Wei Wei, Yu Cheng*

**Main category:** cs.CL

**Keywords:** Low-Rank Adaptation, Mixture-of-Experts, Large Language Models, Parameter-Efficient Fine-Tuning, SVD

**Relevance Score:** 8

**TL;DR:** GOAT improves Low-Rank Adaptation for Large Language Models by using an SVD-structured Mixture-of-Experts architecture, enhancing performance to match Full Fine-Tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Low-Rank Adaptation for Large Language Models, which often lags behind Full Fine-Tuning in efficiency and effectiveness.

**Method:** The GOAT framework integrates relevant priors through a singular value decomposition (SVD)-structured Mixture-of-Experts and introduces a theoretical scaling factor to align optimization with full fine-tuned models.

**Key Contributions:**

	1. Introduction of GOAT framework combining SVD with MoE for improved performance
	2. Demonstration of state-of-the-art results on various benchmarks
	3. Theoretical derivation of scaling factor for optimization alignment

**Result:** Experiments across 25 datasets show GOAT achieves state-of-the-art performance, significantly closing the gap with Full Fine-Tuning.

**Limitations:** 

**Conclusion:** GOAT provides a parameter-efficient method that boosts the efficiency and performance of LoRA MoE without changing the architecture or training algorithms.

**Abstract:** While Low-Rank Adaptation (LoRA) enables parameter-efficient fine-tuning for Large Language Models (LLMs), its performance often falls short of Full Fine-Tuning (Full FT). Current methods optimize LoRA by initializing with static singular value decomposition (SVD) subsets, leading to suboptimal leveraging of pre-trained knowledge. Another path for improving LoRA is incorporating a Mixture-of-Experts (MoE) architecture. However, weight misalignment and complex gradient dynamics make it challenging to adopt SVD prior to the LoRA MoE architecture. To mitigate these issues, we propose \underline{G}reat L\underline{o}R\underline{A} Mixture-of-Exper\underline{t} (GOAT), a framework that (1) adaptively integrates relevant priors using an SVD-structured MoE, and (2) aligns optimization with full fine-tuned MoE by deriving a theoretical scaling factor. We demonstrate that proper scaling, without modifying the architecture or training algorithms, boosts LoRA MoE's efficiency and performance. Experiments across 25 datasets, including natural language understanding, commonsense reasoning, image classification, and natural language generation, demonstrate GOAT's state-of-the-art performance, closing the gap with Full FT.

</details>


### [210] [Char-mander Use mBackdoor! A Study of Cross-lingual Backdoor Attacks in Multilingual LLMs](https://arxiv.org/abs/2502.16901)

*Himanshu Beniwal, Sailesh Panda, Birudugadda Srivibhav, Mayank Singh*

**Main category:** cs.CL

**Keywords:** cross-lingual, backdoor attacks, multilingual models, toxicity classification, embedding spaces

**Relevance Score:** 8

**TL;DR:** This paper investigates cross-lingual backdoor attacks in multilingual large language models, demonstrating the transfer of backdoors between languages through shared embeddings, using toxicity classification as a case study.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and expose the vulnerability of multilingual large language models (mLLMs) to cross-lingual backdoor attacks that can be initiated in one language and affect the model's performance in others.

**Method:** The authors conduct experiments on toxicity classification to illustrate how backdoors inserted in a single language can transfer to others through shared embedding spaces, focusing on the use of rare and frequent tokens as triggers.

**Key Contributions:**

	1. Identification of cross-lingual backdoor attack vulnerabilities in mLLMs
	2. Demonstration of effective triggers for backdoors in toxicity classification
	3. Public availability of code and data for further research

**Result:** The study reveals that one can effectively compromise multilingual systems by poisoning training data in a single language, leading to hidden backdoor effects during information processing in the model.

**Limitations:** The study primarily focuses on toxicity classification, and further exploration is needed across other tasks and languages to generalize findings.

**Conclusion:** The research highlights a critical vulnerability in mLLMs, urging the need for robust defenses against cross-lingual backdoor attacks.

**Abstract:** We explore \textbf{C}ross-lingual \textbf{B}ackdoor \textbf{AT}tacks (X-BAT) in multilingual Large Language Models (mLLMs), revealing how backdoors inserted in one language can automatically transfer to others through shared embedding spaces. Using toxicity classification as a case study, we demonstrate that attackers can compromise multilingual systems by poisoning data in a single language, with rare and high-occurring tokens serving as specific, effective triggers. Our findings expose a critical vulnerability that influences the model's architecture, resulting in a concealed backdoor effect during the information flow. Our code and data are publicly available https://github.com/himanshubeniwal/X-BAT.

</details>


### [211] [Erasing Without Remembering: Implicit Knowledge Forgetting in Large Language Models](https://arxiv.org/abs/2502.19982)

*Huazheng Wang, Yongcheng Jing, Haifeng Sun, Yingjie Wang, Jingyu Wang, Jianxin Liao, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** knowledge forgetting, large language models, unlearning, UGBench, PerMU

**Relevance Score:** 8

**TL;DR:** The paper investigates knowledge forgetting in large language models, proposing a novel unlearning paradigm called PerMU and introducing a benchmark UGBench for evaluating unlearning performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to ensure models forget specific training samples as well as related implicit knowledge, addressing the broader scope of knowledge forgetting.

**Method:** The authors introduce UGBench to assess unlearning of implicit knowledge across various methods and datasets, and propose PerMU, a probability perturbation-based paradigm that utilizes adversarial unlearning samples.

**Key Contributions:**

	1. Introduction of UGBench as a benchmark for evaluating unlearning in large language models.
	2. Development of PerMU, a novel unlearning paradigm that uses probability perturbation to enhance knowledge forgetting.
	3. Demonstration of significant improvements in unlearning specific and implicit knowledge with empirical results.

**Result:** PerMU achieves up to a 50.40% improvement in unlearning specified target data and a 40.73% increase in forgetting implicit knowledge across evaluated datasets.

**Limitations:** 

**Conclusion:** The findings highlight the potential for more significant generalisation in knowledge forgetting, suggesting a new direction in unlearning metrics and methods.

**Abstract:** In this paper, we investigate knowledge forgetting in large language models with a focus on its generalisation--ensuring that models forget not only specific training samples but also related implicit knowledge. To this end, we begin by identifying a broader unlearning scope that includes both target data and logically associated samples, including rephrased, subject-replaced, one-hop reasoned, and relation-reversed data. To rigorously evaluate generalisation, we introduce UGBench, the first comprehensive benchmark specifically designed to assess the unlearning of in-scope implicit knowledge covering 13 state-of-the-art methods across three datasets. UGBench reveals that unlearned models can still recall paraphrased answers and retain target facts in intermediate layers. This motivates us to take a preliminary step toward more generalised implicit knowledge forgetting by proposing PerMU, a novel probability perturbation-based unlearning paradigm. PerMU simulates adversarial unlearning samples to eliminate fact-related tokens from the logit distribution, collectively reducing the probabilities of all answer-associated tokens. Experiments are conducted on a diverse range of datasets, including TOFU, Harry Potter, ZsRE, WMDP, and MUSE, using models ranging from 1.3B to 13B in scale. The results demonstrate that PerMU delivers up to a 50.40% improvement in unlearning vanilla target data while maintaining a 40.73% boost in forgetting implicit knowledge. Our code can be found in https://github.com/MaybeLizzy/UGBench.

</details>


### [212] [Multi2: Multi-Agent Test-Time Scalable Framework for Multi-Document Processing](https://arxiv.org/abs/2502.20592)

*Juntai Cao, Xiang Zhang, Raymond Li, Chuyuan Li, Chenyu You, Shafiq Joty, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** Multi-Document Summarization, Large Language Models, test-time scaling, natural language generation, prompt ensemble

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel framework for Multi-Document Summarization (MDS) using test-time scaling and prompt ensemble techniques to improve summary quality in natural language generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Explores the unexplored application of test-time scaling in MDS, impacted by the need for nuanced prompt and ensemble methods compared to logical reasoning tasks.

**Method:** The proposed framework generates multiple candidate summaries using various prompts and combines them with an aggregator to produce a final refined summary, employing two new metrics for evaluation.

**Key Contributions:**

	1. Novel framework for Multi-Document Summarization using test-time scaling and prompt ensemble techniques.
	2. Introduction of two new evaluation metrics: Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (LLM-ACU) score.
	3. Empirical evidence of enhanced summary quality and practical scaling boundaries for MDS.
	4. Identification of unique challenges in applying LLMs to MDS compared to reasoning tasks.

**Result:** Extensive experiments show that the framework significantly enhances summary quality and identifies scaling limitations in MDS tasks.

**Limitations:** The framework's performance may vary based on the complexity and diversity of the documents being summarized.

**Conclusion:** The results highlight the effectiveness of test-time scaling for improving Multi-Document Summarization and demonstrate the need for innovative evaluation metrics in natural language generation.

**Abstract:** Recent advances in test-time scaling have shown promising results in improving Large Language Model (LLM) performance through strategic computation allocation during inference. While this approach has demonstrated strong improvements in logical and mathematical reasoning tasks, its application to natural language generation (NLG), particularly summarization, remains unexplored. Multi-Document Summarization (MDS), a fundamental task in NLG, presents unique challenges by requiring models to extract and synthesize essential information across multiple lengthy documents. Unlike reasoning tasks, MDS demands a more nuanced approach to prompt design and ensemble methods, as no single "best" prompt can satisfy diverse summarization requirements. We propose a novel framework leveraging test-time scaling for MDS. Our approach employs prompt ensemble techniques to generate multiple candidate summaries using various prompts, then combines them with an aggregator to produce a refined summary. To evaluate our method effectively, we also introduce two new LLM-based metrics: the Consistency-Aware Preference (CAP) score and LLM Atom-Content-Unit (LLM-ACU) score, which assess summary quality while addressing the positional bias inherent in traditional automatic evaluation. Our extensive experiments demonstrate that this framework significantly enhances summary quality while also revealing the practical scaling boundaries to MDS tasks.

</details>


### [213] [CODI: Compressing Chain-of-Thought into Continuous Space via Self-Distillation](https://arxiv.org/abs/2502.21074)

*Zhenyi Shen, Hanqi Yan, Linhai Zhang, Zhanghao Hu, Yali Du, Yulan He*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Large Language Models, Continuous Space, Self-Distillation, Implicit Reasoning

**Relevance Score:** 8

**TL;DR:** The paper introduces CODI, a framework that compresses Chain-of-Thought reasoning from natural language into a continuous space, achieving improved efficiency, robustness, and performance.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Large Language Models' reasoning capabilities by using a latent continuous space, potentially improving efficiency and robustness over traditional methods.

**Method:** CODI (Continuous Chain-of-Thought via Self-Distillation) systematically distills reasoning from explicit Chain-of-Thought (CoT) into an implicit continuous space, aligning hidden states between a teacher (Explicit CoT) and a student (Implicit CoT) during training.

**Key Contributions:**

	1. Introduction of CODI framework for continuous CoT reasoning
	2. Demonstration of superior performance and compression compared to explicit CoT
	3. Showcasing robustness and interpretability of implicit reasoning models

**Result:** CODI matches the performance of explicit CoT on the GSM8k benchmark at the GPT-2 scale with a 3.1x compression rate and 28.2% accuracy improvement over the previous state-of-the-art.

**Limitations:** 

**Conclusion:** The findings validate that LLMs can maintain effective reasoning capabilities not just in natural language, but also within a continuous latent space, offering advantages in terms of compression and robustness.

**Abstract:** Chain-of-Thought (CoT) reasoning enhances Large Language Models (LLMs) by encouraging step-by-step reasoning in natural language. However, leveraging a latent continuous space for reasoning may offer benefits in terms of both efficiency and robustness. Prior implicit CoT methods attempt to bypass language completely by reasoning in continuous space but have consistently underperformed compared to the standard explicit CoT approach. We introduce CODI (Continuous Chain-of-Thought via Self-Distillation), a novel training framework that effectively compresses natural language CoT into continuous space. CODI jointly trains a teacher task (Explicit CoT) and a student task (Implicit CoT), distilling the reasoning ability from language into continuous space by aligning the hidden states of a designated token. Our experiments show that CODI is the first implicit CoT approach to match the performance of explicit CoT on GSM8k at the GPT-2 scale, achieving a 3.1x compression rate and outperforming the previous state-of-the-art by 28.2% in accuracy. CODI also demonstrates robustness, generalizable to complex datasets, and interpretability. These results validate that LLMs can reason effectively not only in natural language, but also in a latent continuous space.

</details>


### [214] [Evaluation and Facilitation of Online Discussions in the LLM Era: A Survey](https://arxiv.org/abs/2503.01513)

*Katerina Korre, Dimitris Tsirmpas, Nikos Gkoumas, Emma Cabalé, Danai Myrtzani, Theodoros Evgeniou, Ion Androutsopoulos, John Pavlopoulos*

**Main category:** cs.CL

**Keywords:** online discussions, LLMs, discussion quality, facilitation strategies, NLP

**Relevance Score:** 8

**TL;DR:** Survey of methods using LLMs to improve online discussion quality and moderation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address harmful exchanges in online discussions that threaten social cohesion and democratic values.

**Method:** Review and synthesis of existing literature in NLP and Social Sciences, leading to the formulation of new taxonomies and strategies.

**Key Contributions:**

	1. New taxonomy on discussion quality evaluation
	2. Overview of facilitation strategies and interventions
	3. LLM-oriented roadmap for future research directions

**Result:** Identification of a new taxonomy for discussion quality evaluation and conversation facilitation datasets, along with intervention strategies using LLMs.

**Limitations:** 

**Conclusion:** LLMs have significant potential to enhance interaction quality online, necessitating further research in technology and societal impacts.

**Abstract:** We present a survey of methods for assessing and enhancing the quality of online discussions, focusing on the potential of LLMs. While online discourses aim, at least in theory, to foster mutual understanding, they often devolve into harmful exchanges, such as hate speech, threatening social cohesion and democratic values. Recent advancements in LLMs enable artificial facilitation agents to not only moderate content, but also actively improve the quality of interactions. Our survey synthesizes ideas from NLP and Social Sciences to provide (a) a new taxonomy on discussion quality evaluation, (b) an overview of intervention and facilitation strategies, (c) along with a new taxonomy of conversation facilitation datasets, (d) an LLM-oriented roadmap of good practices and future research directions, from technological and societal perspectives.

</details>


### [215] [MCiteBench: A Multimodal Benchmark for Generating Text with Citations](https://arxiv.org/abs/2503.02589)

*Caiyu Hu, Yikai Zhang, Tinghui Zhu, Yiwei Ye, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** multimodal, large language models, citations, benchmark, model behavior

**Relevance Score:** 9

**TL;DR:** This paper introduces MCiteBench, the first benchmark for assessing Multimodal Large Language Models' (MLLMs) ability to generate text with citations in multimodal contexts, revealing challenges in grounding output and a systematic modality bias among models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The work aims to address hallucination in MLLMs by improving citation generation for multimodal scenarios, an area that has been largely neglected in existing research.

**Method:** The authors developed MCiteBench, a benchmark that includes data from academic papers and review-rebuttal interactions featuring diverse information sources and multimodal content, to evaluate MLLMs' citation performance.

**Key Contributions:**

	1. Introduction of MCiteBench benchmark for multimodal citation generation
	2. Identification of systematic modality bias in MLLMs
	3. Insights into model behavior and reliance on different information sources for citation generation

**Result:** Experimental results indicate that MLLMs have difficulty reliably grounding their outputs when processing multimodal input, exposing a systematic bias towards certain modalities during the citation generation process.

**Limitations:** The benchmark may not cover all possible multimodal scenarios and the analysis is limited to the examined models' architectures.

**Conclusion:** The findings provide insights into model behavior during multimodal citation tasks and highlight potential future research directions to improve MLLMs' performance in this area.

**Abstract:** Multimodal Large Language Models (MLLMs) have advanced in integrating diverse modalities but frequently suffer from hallucination. A promising solution to mitigate this issue is to generate text with citations, providing a transparent chain for verification. However, existing work primarily focuses on generating citations for text-only content, leaving the challenges of multimodal scenarios largely unexplored. In this paper, we introduce MCiteBench, the first benchmark designed to assess the ability of MLLMs to generate text with citations in multimodal contexts. Our benchmark comprises data derived from academic papers and review-rebuttal interactions, featuring diverse information sources and multimodal content. Experimental results reveal that MLLMs struggle to ground their outputs reliably when handling multimodal input. Further analysis uncovers a systematic modality bias and reveals how models internally rely on different sources when generating citations, offering insights into model behavior and guiding future directions for multimodal citation tasks.

</details>


### [216] [Assumed Identities: Quantifying Gender Bias in Machine Translation of Gender-Ambiguous Occupational Terms](https://arxiv.org/abs/2503.04372)

*Orfeas Menis Mastromichalakis, Giorgos Filandrianos, Maria Symeonaki, Giorgos Stamou*

**Main category:** cs.CL

**Keywords:** Machine Translation, gender bias, evaluation metric, GAMBIT-MT, occupational terms

**Relevance Score:** 6

**TL;DR:** This paper introduces GRAPE, a metric for evaluating gender bias in Machine Translation systems, and GAMBIT-MT, a dataset with gender-ambiguous occupational terms. It analyzes translation biases and their alignment with societal norms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the systematic gender bias in Machine Translation systems that emerges from the translation of gender-ambiguous occupational terms.

**Method:** The paper proposes GRAPE, a probability-based metric, and uses it to evaluate MT systems against a newly created dataset, GAMBIT-MT, which contains gender-ambiguous terms in English.

**Key Contributions:**

	1. Introduction of the GRAPE metric for gender bias evaluation in MT systems.
	2. Creation of the GAMBIT-MT dataset for benchmarking gender ambiguity in translations.
	3. Analysis of gender translation patterns in relation to societal norms and real-world distributions.

**Result:** Evaluation of various MT systems revealed differing patterns of gender translation in Greek and French that either aligned with or diverged from societal stereotypes and real-world gender distributions.

**Limitations:** The limitations of the study include a focus on only Greek and French translations, which may not represent wider linguistic diversity.

**Conclusion:** The study highlights the need for improved evaluation metrics like GRAPE to assess gender bias in MT, suggesting that MT systems often perpetuate societal stereotypes rather than reflecting actual gender distributions.

**Abstract:** Machine Translation (MT) systems frequently encounter gender-ambiguous occupational terms, where they must assign gender without explicit contextual cues. While individual translations in such cases may not be inherently biased, systematic patterns-such as consistently translating certain professions with specific genders-can emerge, reflecting and perpetuating societal stereotypes. This ambiguity challenges traditional instance-level single-answer evaluation approaches, as no single gold standard translation exists. To address this, we introduce GRAPE, a probability-based metric designed to evaluate gender bias by analyzing aggregated model responses. Alongside this, we present GAMBIT-MT, a benchmarking dataset in English with gender-ambiguous occupational terms. Using GRAPE, we evaluate several MT systems and examine whether their gendered translations in Greek and French align with or diverge from societal stereotypes, real-world occupational gender distributions, and normative standards.

</details>


### [217] [Cost-Optimal Grouped-Query Attention for Long-Context Modeling](https://arxiv.org/abs/2503.09579)

*Yingfa Chen, Yutong Wu, Chenyang Song, Zhen Leng Thai, Xingyu Shen, Xu Han, Zhiyuan Liu, Maosong Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Grouped-Query Attention, Cost Optimization, Context Length, Model Efficiency

**Relevance Score:** 9

**TL;DR:** This paper introduces a method to optimize Grouped-Query Attention (GQA) in large language models by considering context length to minimize computational costs without degrading performance.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of attention layers in large language models by optimizing GQA configurations based on context length.

**Method:** Analysis of the relationship among context length, model size, GQA configuration, and model loss; introduces decoupling of total head size from hidden size and joint optimization of model size and GQA configuration.

**Key Contributions:**

	1. Decoupling total head size from hidden size for better control of attention FLOPs.
	2. Joint optimization of model size and GQA configuration for efficient resource allocation.
	3. A recipe for deriving cost-optimal GQA configurations suited for long-context scenarios.

**Result:** Proposed configurations can reduce memory usage and FLOPs by over 50% compared to current Llama-3's GQA while maintaining model performance.

**Limitations:** 

**Conclusion:** The findings suggest new strategies for designing efficient long-context LLMs and provide a systematic approach for deriving cost-optimal GQA configurations.

**Abstract:** Grouped-Query Attention (GQA) is a widely adopted strategy for reducing the computational cost of attention layers in large language models (LLMs). However, current GQA configurations are often suboptimal because they overlook how context length influences inference cost. Since inference cost grows with context length, the most cost-efficient GQA configuration should also vary accordingly. In this work, we analyze the relationship among context length, model size, GQA configuration, and model loss, and introduce two innovations: (1) we decouple the total head size from the hidden size, enabling more flexible control over attention FLOPs; and (2) we jointly optimize the model size and the GQA configuration to arrive at a better allocation of inference resources between attention layers and other components. Our analysis reveals that commonly used GQA configurations are highly suboptimal for long-context scenarios. More importantly, we propose a recipe for deriving cost-optimal GQA configurations. Our results show that for long-context scenarios, one should use fewer attention heads while scaling up model size. Configurations selected by our recipe can reduce both memory usage and FLOPs by more than 50% compared to Llama-3's GQA, with *no degradation in model capabilities*. Our findings offer valuable insights for designing efficient long-context LLMs. The code is available at https://www.github.com/THUNLP/cost-optimal-gqa .

</details>


### [218] [RouterEval: A Comprehensive Benchmark for Routing LLMs to Explore Model-level Scaling Up in LLMs](https://arxiv.org/abs/2503.10657)

*Zhongzhan Huang, Guoming Ling, Yupei Lin, Yandong Chen, Shanshan Zhong, Hefeng Wu, Liang Lin*

**Main category:** cs.CL

**Keywords:** Routing LLMs, Benchmark, Machine Learning, Performance Enhancement

**Relevance Score:** 8

**TL;DR:** This paper introduces RouterEval, a benchmark for evaluating Routing Large Language Models (LLMs), highlighting their performance enhancement potential when a capable router is employed.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for comprehensive and open-source benchmarks for Routing LLMs to facilitate their development and evaluation.

**Method:** Analysis of over 8,500 LLMs and introduction of RouterEval benchmark with 200 million performance records for evaluating Routing LLM methods.

**Key Contributions:**

	1. Introduction of RouterEval benchmark
	2. Analysis of performance as LLM candidate numbers increase
	3. Demonstration of significant room for improvement in existing methods

**Result:** The analysis shows that capable routers can significantly enhance performance beyond that of the best single model in the pool, revealing substantial room for improvement in existing Routing LLM methods.

**Limitations:** Limited to the current set of 8,500 LLMs and their evaluations; effectiveness of routers can vary by domain.

**Conclusion:** RouterEval provides a solid foundation for future research in Routing LLMs, addressing the benchmarking gap that hinders router development.

**Abstract:** Routing large language models (LLMs) is a new paradigm that uses a router to recommend the best LLM from a pool of candidates for a given input. In this paper, our comprehensive analysis with more than 8,500 LLMs reveals a novel model-level scaling up phenomenon in Routing LLMs, i.e., a capable router can significantly enhance the performance of this paradigm as the number of candidates increases. This improvement can even surpass the performance of the best single model in the pool and many existing strong LLMs, confirming it a highly promising paradigm. However, the lack of comprehensive and open-source benchmarks for Routing LLMs has hindered the development of routers. In this paper, we introduce RouterEval, a benchmark tailored for router research, which includes over 200,000,000 performance records for 12 popular LLM evaluations across various areas such as commonsense reasoning, semantic understanding, etc., based on over 8,500 various LLMs. Using RouterEval, extensive evaluations of existing Routing LLM methods reveal that most still have significant room for improvement. See https://github.com/MilkThink-Lab/RouterEval for all data, code and tutorial.

</details>


### [219] [MathAgent: Leveraging a Mixture-of-Math-Agent Framework for Real-World Multimodal Mathematical Error Detection](https://arxiv.org/abs/2503.18132)

*Yibo Yan, Shen Wang, Jiahao Huo, Philip S. Yu, Xuming Hu, Qingsong Wen*

**Main category:** cs.CL

**Keywords:** mathematical error detection, multimodal large language models, educational technology

**Relevance Score:** 8

**TL;DR:** The paper introduces MathAgent, a Mixture-of-Math-Agent framework that improves mathematical error detection in educational settings using MLLMs, achieving higher accuracy and student satisfaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Mathematical error detection in education is challenging for MLLMs due to their need for complex reasoning and understanding of visual and textual content.

**Method:** MathAgent decomposes error detection into three phases, handled by specialized agents: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer.

**Key Contributions:**

	1. Introduction of a novel Mixture-of-Math-Agent framework for error detection
	2. Decomposition of error detection into specialized phases
	3. Demonstrated deployment with high student satisfaction

**Result:** MathAgent shows approximately 5% higher accuracy in error step identification and a 3% improvement in error categorization on real-world educational data compared to baseline models.

**Limitations:** 

**Conclusion:** MathAgent has been effectively deployed in educational platforms, achieving high student satisfaction and cost savings in manual error detection.

**Abstract:** Mathematical error detection in educational settings presents a significant challenge for Multimodal Large Language Models (MLLMs), requiring a sophisticated understanding of both visual and textual mathematical content along with complex reasoning capabilities. Though effective in mathematical problem-solving, MLLMs often struggle with the nuanced task of identifying and categorizing student errors in multimodal mathematical contexts. Therefore, we introduce MathAgent, a novel Mixture-of-Math-Agent framework designed specifically to address these challenges. Our approach decomposes error detection into three phases, each handled by a specialized agent: an image-text consistency validator, a visual semantic interpreter, and an integrative error analyzer. This architecture enables more accurate processing of mathematical content by explicitly modeling relationships between multimodal problems and student solution steps. We evaluate MathAgent on real-world educational data, demonstrating approximately 5% higher accuracy in error step identification and 3% improvement in error categorization compared to baseline models. Besides, MathAgent has been successfully deployed in an educational platform that has served over one million K-12 students, achieving nearly 90% student satisfaction while generating significant cost savings by reducing manual error detection.

</details>


### [220] [Scaling Video-Language Models to 10K Frames via Hierarchical Differential Distillation](https://arxiv.org/abs/2504.02438)

*Chuanqi Cheng, Jian Guan, Wei Wu, Rui Yan*

**Main category:** cs.CL

**Keywords:** video processing, vision-language models, differential distillation, long-form videos, computational efficiency

**Relevance Score:** 8

**TL;DR:** Introducing ViLAMP, a hierarchical video-language model that efficiently processes long videos using differential distillation techniques to preserve important information and improve computational efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational challenges of processing long-form videos with vision-language models while preserving critical temporal dependencies and semantic information.

**Method:** ViLAMP uses differential keyframe selection to maximize query relevance and maintain temporal distinctiveness, along with differential feature merging to keep query-salient features in non-keyframes, optimizing processing of videos at mixed precision.

**Key Contributions:**

	1. Introduces differential distillation to retain task-relevant information in video processing.
	2. Develops a hierarchical model capable of handling ultra-long videos on limited hardware.
	3. Achieves state-of-the-art performance on video understanding benchmarks with improved computational efficiency.

**Result:** ViLAMP demonstrates superior performance on five video understanding benchmarks, particularly with ultra-long videos, achieving significant computational efficiency while maintaining state-of-the-art results.

**Limitations:** 

**Conclusion:** ViLAMP can process long videos while preserving essential information, demonstrating both efficiency and effectiveness in video understanding tasks.

**Abstract:** Long-form video processing fundamentally challenges vision-language models (VLMs) due to the high computational costs of handling extended temporal sequences. Existing token pruning and feature merging methods often sacrifice critical temporal dependencies or dilute semantic information. We introduce differential distillation, a principled approach that systematically preserves task-relevant information while suppressing redundancy. Based on this principle, we develop ViLAMP, a hierarchical video-language model that processes hour-long videos at "mixed precision" through two key mechanisms: (1) differential keyframe selection that maximizes query relevance while maintaining temporal distinctiveness at the frame level and (2) differential feature merging that preserves query-salient features in non-keyframes at the patch level. Hence, ViLAMP retains full information in keyframes while reducing non-keyframes to their most salient features, resembling mixed-precision training. Extensive experiments demonstrate ViLAMP's superior performance across five video understanding benchmarks, particularly on long-form content. Notably, ViLAMP can process ultra-long videos (up to 10K frames) on a single NVIDIA A100 GPU, achieving substantial computational efficiency while maintaining state-of-the-art performance. Code and model are available at https://github.com/steven-ccq/ViLAMP.

</details>


### [221] [Beyond Self-Reports: Multi-Observer Agents for Personality Assessment in Large Language Models](https://arxiv.org/abs/2504.08399)

*Yin Jou Huang, Rafik Hadfi*

**Main category:** cs.CL

**Keywords:** LLM, personality assessment, multi-observer framework, Big Five, bias

**Relevance Score:** 8

**TL;DR:** The paper introduces a multi-observer framework for assessing LLM personality traits, emphasizing the need for context in evaluations.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional self-report questionnaires for assessing LLM personality traits suffer from biases and do not fully capture behavioral nuances.

**Method:** The authors propose using multiple observer agents, each representing different relational contexts, to evaluate an LLM's behavior based on dialogue interactions before rating across the Big Five personality dimensions.

**Key Contributions:**

	1. Introduction of a multi-observer framework for LLM personality assessments
	2. Demonstration of the importance of relational context in personality evaluation
	3. Evidence that aggregated observer ratings improve reliability and reduce bias.

**Result:** Observer-report ratings closely align with human judgments and reveal biases in LLM self-assessments. Aggregating ratings from multiple observers improves reliability and reduces bias.

**Limitations:** The framework relies on the setup of appropriate observer agents and may require extensive dialogue interactions.

**Conclusion:** A multi-observer paradigm provides a more reliable and context-sensitive approach to evaluating LLM personality traits compared to conventional self-assessment methods.

**Abstract:** Self-report questionnaires have long been used to assess LLM personality traits, yet they fail to capture behavioral nuances due to biases and meta-knowledge contamination. This paper proposes a novel multi-observer framework for personality trait assessments in LLM agents that draws on informant-report methods in psychology. Instead of relying on self-assessments, we employ multiple observer agents. Each observer is configured with a specific relational context (e.g., family member, friend, or coworker) and engages the subject LLM in dialogue before evaluating its behavior across the Big Five dimensions. We show that these observer-report ratings align more closely with human judgments than traditional self-reports and reveal systematic biases in LLM self-assessments. We also found that aggregating responses from 5 to 7 observers reduces systematic biases and achieves optimal reliability. Our results highlight the role of relationship context in perceiving personality and demonstrate that a multi-observer paradigm offers a more reliable, context-sensitive approach to evaluating LLM personality traits.

</details>


### [222] [S1-Bench: A Simple Benchmark for Evaluating System 1 Thinking Capability of Large Reasoning Models](https://arxiv.org/abs/2504.10368)

*Wenyuan Zhang, Shuaiyi Nie, Xinghua Zhang, Zefeng Zhang, Tingwen Liu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, system 1 thinking, benchmark, human-computer interaction, machine learning

**Relevance Score:** 8

**TL;DR:** S1-Bench is a benchmark for evaluating Large Reasoning Models' performance on simple tasks favoring system 1 thinking, revealing inefficiencies and limitations of LRMs in handling these tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of an appropriate benchmark to evaluate Large Reasoning Models (LRMs) on simple tasks that utilize intuitive system 1 thinking, as current evaluations focus on complex reasoning tasks that rely on deliberative system 2 thought.

**Method:** S1-Bench introduces a suite of simple, diverse questions across multiple domains and languages to assess the performance of LRMs on tasks suitable for system 1 thinking.

**Key Contributions:**

	1. Introduction of S1-Bench benchmark for system 1 thinking in LRMs
	2. Evaluation of 28 LRMs revealing performance gaps
	3. Identification of a need for dual-system compatibility in LRM development

**Result:** Extensive evaluations across 28 LRMs showed inefficiencies, inadequate accuracy, and limited robustness when handling simple questions, revealing discrepancies in difficulty perception and generation length.

**Limitations:** 

**Conclusion:** The findings highlight the need for dual-system compatibility in the development of LRMs, addressing their limitations in system 1 thinking.

**Abstract:** We introduce S1-Bench, a novel benchmark designed to evaluate the performance of Large Reasoning Models (LRMs) on simple tasks that favor intuitive system 1 thinking rather than deliberative system 2 reasoning. While LRMs have achieved significant breakthroughs in complex reasoning tasks through explicit chains of thought, their heavy reliance on system 2 thinking may limit their system 1 thinking capabilities. However, there is a lack of an appropriate benchmark for evaluating LRM's system 1 thinking capabilities. To fill this gap, S1-Bench introduces a suite of simple, diverse, and natural questions across multiple domains and languages, specifically designed to assess LRMs' performance on questions more suitable for system 1 . We conduct extensive evaluations across 28 LRMs, revealing their inefficiency, inadequate accuracy, and limited robustness when handling simple questions. Additionally, we observe a gap between their difficulty perception and generation length. Overall, this work paves the way toward dual-system compatibility in the development of LRMs.

</details>


### [223] [Cross-Document Cross-Lingual NLI via RST-Enhanced Graph Fusion and Interpretability Prediction](https://arxiv.org/abs/2504.12324)

*Mengying Yuan, Wenhao Wang, Zixuan Wang, Yujie Huang, Kangli Wei, Fei Li, Chong Teng, Donghong Ji*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Cross-Document NLI, Cross-Lingual NLI

**Relevance Score:** 6

**TL;DR:** The paper introduces a novel Cross-Document Cross-Lingual Natural Language Inference (CDCL-NLI) paradigm with a new dataset and an innovative methodology that enhances NLI performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the largely unexplored area of Cross-Document Cross-Lingual NLI and improve existing NLI capabilities.

**Method:** The authors propose a new method that combines RST-enhanced graph fusion with an interpretability-aware prediction framework to model cross-document and cross-lingual contexts.

**Key Contributions:**

	1. Introduction of the CDCL-NLI paradigm
	2. Development of a high-quality dataset with 25,410 instances in 26 languages
	3. Innovative RST-enhanced graph neural networks for cross-document context modeling

**Result:** Extensive experiments show that the proposed approach significantly outperforms conventional NLI models and large language models.

**Limitations:** 

**Conclusion:** The research contributes to the field of NLI by promoting interest in cross-document and cross-lingual understanding and providing methods for eliminating hallucinations and enhancing interpretability.

**Abstract:** Natural Language Inference (NLI) is a fundamental task in natural language processing. While NLI has developed many sub-directions such as sentence-level NLI, document-level NLI and cross-lingual NLI, Cross-Document Cross-Lingual NLI (CDCL-NLI) remains largely unexplored. In this paper, we propose a novel paradigm: CDCL-NLI, which extends traditional NLI capabilities to multi-document, multilingual scenarios. To support this task, we construct a high-quality CDCL-NLI dataset including 25,410 instances and spanning 26 languages. To address the limitations of previous methods on CDCL-NLI task, we further propose an innovative method that integrates RST-enhanced graph fusion with interpretability-aware prediction. Our approach leverages RST (Rhetorical Structure Theory) within heterogeneous graph neural networks for cross-document context modeling, and employs a structure-aware semantic alignment based on lexical chains for cross-lingual understanding. For NLI interpretability, we develop an EDU (Elementary Discourse Unit)-level attribution framework that produces extractive explanations. Extensive experiments demonstrate our approach's superior performance, achieving significant improvements over both conventional NLI models as well as large language models. Our work sheds light on the study of NLI and will bring research interest on cross-document cross-lingual context understanding, hallucination elimination and interpretability inference. Our code and datasets are available at \href{https://anonymous.4open.science/r/CDCL-NLI-637E/}{CDCL-NLI-link} for peer review.

</details>


### [224] [Walk the Talk? Measuring the Faithfulness of Large Language Model Explanations](https://arxiv.org/abs/2504.14150)

*Katie Matton, Robert Osazuwa Ness, John Guttag, Emre Kıcıman*

**Main category:** cs.CL

**Keywords:** large language models, faithfulness, explanation, causal inference, social bias

**Relevance Score:** 9

**TL;DR:** This paper presents a new approach to measure the faithfulness of explanations given by large language models, addressing potential misrepresentations in their reasoning processes.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The reliability of large language models (LLMs) in generating explanations for their answers is questionable, leading to over-trust and misuse. This study aims to define and measure the faithfulness of LLM explanations.

**Method:** The paper introduces a rigorous definition of faithfulness based on the comparison between influential concepts in LLM explanations and actual influential concepts. It employs an auxiliary LLM to create counterfactuals and uses a Bayesian hierarchical model to quantify causal effects at both example and dataset levels.

**Key Contributions:**

	1. A rigorous definition of faithfulness for LLM explanations.
	2. A novel measurement approach utilizing auxiliary LLMs for counterfactuals and Bayesian models for causality.
	3. Identification of unfaithfulness patterns in both social bias and medical domains.

**Result:** Experiments demonstrate that the proposed method effectively quantifies unfaithfulness in LLM explanations, revealing biases in social contexts and misleading claims in medical question answering.

**Limitations:** The study is limited to specific tasks and may not generalize across all domains. Further validation is required.

**Conclusion:** By identifying patterns of unfaithfulness, this research underscores the importance of understanding the limitations of LLM explanations to avoid misuse and enhance trust.

**Abstract:** Large language models (LLMs) are capable of generating plausible explanations of how they arrived at an answer to a question. However, these explanations can misrepresent the model's "reasoning" process, i.e., they can be unfaithful. This, in turn, can lead to over-trust and misuse. We introduce a new approach for measuring the faithfulness of LLM explanations. First, we provide a rigorous definition of faithfulness. Since LLM explanations mimic human explanations, they often reference high-level concepts in the input question that purportedly influenced the model. We define faithfulness in terms of the difference between the set of concepts that LLM explanations imply are influential and the set that truly are. Second, we present a novel method for estimating faithfulness that is based on: (1) using an auxiliary LLM to modify the values of concepts within model inputs to create realistic counterfactuals, and (2) using a Bayesian hierarchical model to quantify the causal effects of concepts at both the example- and dataset-level. Our experiments show that our method can be used to quantify and discover interpretable patterns of unfaithfulness. On a social bias task, we uncover cases where LLM explanations hide the influence of social bias. On a medical question answering task, we uncover cases where LLM explanations provide misleading claims about which pieces of evidence influenced the model's decisions.

</details>


### [225] [MrGuard: A Multilingual Reasoning Guardrail for Universal LLM Safety](https://arxiv.org/abs/2504.15241)

*Yahan Yang, Soham Dan, Shuo Li, Dan Roth, Insup Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual guardrail, adversarial attacks, content moderation, Group Relative Policy Optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces MrGuard, a multilingual guardrail for LLMs that detects and filters unsafe content by leveraging synthetic multilingual data, supervised fine-tuning, and a novel GRPO framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a robust guardrail to protect against adversarial attacks on LLMs, especially in multilingual contexts with limited safety-aligned data.

**Method:** The approach includes generating synthetic multilingual data, supervised fine-tuning of models, and a curriculum-based Group Relative Policy Optimization framework to enhance performance.

**Key Contributions:**

	1. Introduction of a multilingual guardrail for LLMs
	2. Innovative use of synthetic data generation for cultural nuances
	3. Development of a Group Relative Policy Optimization framework for improved safety performance.

**Result:** MrGuard outperforms existing baselines by over 15% across various languages, demonstrating resilience against multilingual variations and maintaining safety judgments.

**Limitations:** 

**Conclusion:** The multilingual reasoning capacity of MrGuard facilitates explanations, assisting in the understanding of language-specific risks in content moderation.

**Abstract:** Large Language Models (LLMs) are susceptible to adversarial attacks such as jailbreaking, which can elicit harmful or unsafe behaviors. This vulnerability is exacerbated in multilingual settings, where multilingual safety-aligned data is often limited. Thus, developing a guardrail capable of detecting and filtering unsafe content across diverse languages is critical for deploying LLMs in real-world applications. In this work, we introduce a multilingual guardrail with reasoning for prompt classification. Our method consists of: (1) synthetic multilingual data generation incorporating culturally and linguistically nuanced variants, (2) supervised fine-tuning, and (3) a curriculum-based Group Relative Policy Optimization (GRPO) framework that further improves performance. Experimental results demonstrate that our multilingual guardrail, MrGuard, consistently outperforms recent baselines across both in-domain and out-of-domain languages by more than 15%. We also evaluate MrGuard's robustness to multilingual variations, such as code-switching and low-resource language distractors in the prompt, and demonstrate that it preserves safety judgments under these challenging conditions. The multilingual reasoning capability of our guardrail enables it to generate explanations, which are particularly useful for understanding language-specific risks and ambiguities in multilingual content moderation.

</details>


### [226] [Rethinking Prompt Optimizers: From Prompt Merits to Optimization](https://arxiv.org/abs/2505.09930)

*Zixiao Zhu, Hanzhang Zhou, Zijian Feng, Tianjiao Li, Chua Jia Jim Deryl, Mak Lee Onn, Gee Wah Ng, Kezhi Mao*

**Main category:** cs.CL

**Keywords:** prompt optimization, machine learning, interpretable design, scalable solutions, lightweight LLM

**Relevance Score:** 8

**TL;DR:** This paper presents MePO, a new merit-guided prompt optimizer that enhances the quality of responses in various models while addressing cost and privacy issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve response quality in LLMs without relying on advanced models that can degrade performance in lightweight inference scenarios.

**Method:** MePO utilizes a set of model-agnostic prompt quality merits and is trained on a preference dataset made from merit-aligned prompts generated by a lightweight LLM.

**Key Contributions:**

	1. Development of the MePO prompt optimizer
	2. Introduction of interpretable design in prompt optimization
	3. Validation of model-agnostic prompt quality merits

**Result:** MePO outperforms previous prompt optimization methods across a range of tasks and models by providing a scalable and interpretable solution.

**Limitations:** 

**Conclusion:** MePO is a robust alternative for prompt optimization that enhances quality while minimizing deployment concerns.

**Abstract:** Prompt optimization (PO) provides a practical way to improve response quality when users lack the time or expertise to manually craft effective prompts. Existing methods typically rely on advanced, large-scale LLMs like GPT-4 to generate optimized prompts. However, due to limited downward compatibility, verbose, instruction-heavy prompts from advanced LLMs can overwhelm lightweight inference models and degrade response quality. In this work, we rethink prompt optimization through the lens of interpretable design. We first identify a set of model-agnostic prompt quality merits and empirically validate their effectiveness in enhancing prompt and response quality. We then introduce MePO, a merit-guided, lightweight, and locally deployable prompt optimizer trained on our preference dataset built from merit-aligned prompts generated by a lightweight LLM. Unlike prior work, MePO avoids online optimization reliance, reduces cost and privacy concerns, and, by learning clear, interpretable merits, generalizes effectively to both large-scale and lightweight inference models. Experiments demonstrate that MePO achieves better results across diverse tasks and model types, offering a scalable and robust solution for real-world deployment. The code and dataset can be found in https://github.com/MidiyaZhu/MePO

</details>


### [227] [Designing and Contextualising Probes for African Languages](https://arxiv.org/abs/2505.10081)

*Wisdom Aduah, Francois Meyer*

**Main category:** cs.CL

**Keywords:** pretrained language models, African languages, linguistic knowledge, interpretability, multilingual adaptation

**Relevance Score:** 7

**TL;DR:** This paper investigates probing pretrained language models for linguistic knowledge related to African languages, revealing that African-specific models encode more linguistic information than multilingual models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of how pretrained language models encode linguistic features specific to African languages and to evaluate the effectiveness of these models in comparison to multilingual ones.

**Method:** The study involves training layer-wise probes for six African languages and designing control tasks using the MasakhaPOS dataset to assess probe performance and interpreted results.

**Key Contributions:**

	1. First systematic probing of PLMs for African languages.
	2. Comparison of linguistic knowledge in African-specific versus multilingual PLMs.
	3. Application of interpretability techniques to African language models.

**Result:** African language-specific PLMs were found to encode more linguistic information compared to massively multilingual PLMs, with syntactic information concentrated in the middle-to-last layers and sentence-level semantic information spread across all layers.

**Limitations:** 

**Conclusion:** The findings affirm that probing methods can reveal distinct knowledge within PLMs, aiding in understanding model performance and the underlying mechanisms effective in multilingual adaptation and active learning.

**Abstract:** Pretrained language models (PLMs) for African languages are continually improving, but the reasons behind these advances remain unclear. This paper presents the first systematic investigation into probing PLMs for linguistic knowledge about African languages. We train layer-wise probes for six typologically diverse African languages to analyse how linguistic features are distributed. We also design control tasks, a way to interpret probe performance, for the MasakhaPOS dataset. We find PLMs adapted for African languages to encode more linguistic information about target languages than massively multilingual PLMs. Our results reaffirm previous findings that token-level syntactic information concentrates in middle-to-last layers, while sentence-level semantic information is distributed across all layers. Through control tasks and probing baselines, we confirm that performance reflects the internal knowledge of PLMs rather than probe memorisation. Our study applies established interpretability techniques to African-language PLMs. In doing so, we highlight the internal mechanisms underlying the success of strategies like active learning and multilingual adaptation.

</details>
