# 2025-07-24

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 43]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Assessing Medical Training Skills via Eye and Head Movements](https://arxiv.org/abs/2507.16819)

*Kayhan Latifzadeh, Luis A. Leiva, Klen Čopič Pucihar, Matjaž Kljun, Iztok Devetak, Lili Steblovnik*

**Main category:** cs.HC

**Keywords:** Eye tracking, Clinical training, Skill assessment, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper explores the use of eye and head tracking to assess and improve clinical skills in healthcare professionals during simulated training sessions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To gain insights into skill development in clinical settings and enhance training methodologies.

**Method:** Analysis of eye and head movements during simulated baby delivery training sessions using metrics like pupillary response rate and fixation duration.

**Key Contributions:**

	1. Efficacy of eye and head tracking in skill differentiation
	2. High F1 and AUC scores demonstrating predictive power
	3. Foundation for computational models in clinical skill assessment

**Result:** Eye and head tracking metrics effectively distinguish between trained and untrained practitioners during clinical tasks, achieving high F1 scores and AUC values.

**Limitations:** 

**Conclusion:** The study supports the potential of eye-tracking technology as a valuable tool for skill assessment and enhancement in clinical training.

**Abstract:** We examined eye and head movements to gain insights into skill development in clinical settings. A total of 24 practitioners participated in simulated baby delivery training sessions. We calculated key metrics, including pupillary response rate, fixation duration, or angular velocity. Our findings indicate that eye and head tracking can effectively differentiate between trained and untrained practitioners, particularly during labor tasks. For example, head-related features achieved an F1 score of 0.85 and AUC of 0.86, whereas pupil-related features achieved F1 score of 0.77 and AUC of 0.85. The results lay the groundwork for computational models that support implicit skill assessment and training in clinical settings by using commodity eye-tracking glasses as a complementary device to more traditional evaluation methods such as subjective scores.

</details>


### [2] [Write, Rank, or Rate: Comparing Methods for Studying Visualization Affordances](https://arxiv.org/abs/2507.17024)

*Chase Stokes, Kylie Lin, Cindy Xiong Bearfield*

**Main category:** cs.HC

**Keywords:** chart affordances, human-computer interaction, large language models

**Relevance Score:** 8

**TL;DR:** The paper explores scalable methodologies for assessing chart affordances through various elicitation methods, including a case study using GPT-4o as a proxy for human interpretation.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the labor-intensive nature of analyzing reader interactions with information visualizations by identifying effective research methodologies for studying chart affordances.

**Method:** The study tests four elicitation methods: free response, visualization ranking, conclusion ranking, and salience rating, comparing their effectiveness in gathering reader interpretations of different types of charts.

**Key Contributions:**

	1. Testing and comparing four elicitation methods for chart interpretation
	2. Introduction of large language models as human proxies in visualization studies
	3. Highlighting the influence of participant bias in ranking methodologies

**Result:** None of the tested elicitation methods fully replicated affordances from free-response conclusions; however, combinations of ranking and rating methods can act as effective proxies.

**Limitations:** GPT-4o faced severe constraints outside of the salience rating methodology, suggesting limitations in its use as a proxy.

**Conclusion:** The findings indicate significant discrepancies between methods and the importance of carefully selecting and combining methodologies while considering their trade-offs.

**Abstract:** A growing body of work on visualization affordances highlights how specific design choices shape reader takeaways from information visualizations. However, mapping the relationship between design choices and reader conclusions often requires labor-intensive crowdsourced studies, generating large corpora of free-response text for analysis. To address this challenge, we explored alternative scalable research methodologies to assess chart affordances. We test four elicitation methods from human-subject studies: free response, visualization ranking, conclusion ranking, and salience rating, and compare their effectiveness in eliciting reader interpretations of line charts, dot plots, and heatmaps. Overall, we find that while no method fully replicates affordances observed in free-response conclusions, combinations of ranking and rating methods can serve as an effective proxy at a broad scale. The two ranking methodologies were influenced by participant bias towards certain chart types and the comparison of suggested conclusions. Rating conclusion salience could not capture the specific variations between chart types observed in the other methods. To supplement this work, we present a case study with GPT-4o, exploring the use of large language models (LLMs) to elicit human-like chart interpretations. This aligns with recent academic interest in leveraging LLMs as proxies for human participants to improve data collection and analysis efficiency. GPT-4o performed best as a human proxy for the salience rating methodology but suffered from severe constraints in other areas. Overall, the discrepancies in affordances we found between various elicitation methodologies, including GPT-4o, highlight the importance of intentionally selecting and combining methods and evaluating trade-offs.

</details>


### [3] [Evaluation of the effects of frame time variation on VR task performance](https://arxiv.org/abs/2507.17139)

*Benjamin Watson, Victoria Spaulding, Neff Walker, William Ribarsky*

**Main category:** cs.HC

**Keywords:** frame time variations, task performance, virtual environments, immersive applications, closed loop tasks

**Relevance Score:** 7

**TL;DR:** Study on how frame time variations impact task performance in virtual environments (VEs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the effects of frame time variations is essential for designers of virtual environments and immersive applications.

**Method:** The study involved testing both open and closed loop tasks in a virtual environment, measuring the impact of varying frame times on task performance.

**Key Contributions:**

	1. First study to analyze the effects of frame time variations on task performance in VEs.
	2. Identifies significant impacts of frame time variations on closed loop tasks at immersive frame rates.
	3. Provides insights for the design of immersive applications

**Result:** Large deviations in frame times do not significantly affect performance for many applications; however, at lower frame thresholds, variations impact closed loop task performance significantly.

**Limitations:** The study focuses on specific types of tasks and environments; results may not generalize to all virtual environment designs.

**Conclusion:** Designers must consider frame time variations, especially at lower limits to ensure immersive experiences in VEs.

**Abstract:** We present a first study of the effects of frame time variations, in both deviation around mean frame times and period of fluctuation, on task performance in a virtual environment (VE). Chosen are open and closed loop tasks that are typical for current applications or likely to be prominent in future ones. The results show that at frame times in the range deemed acceptable for many applications, fairly large deviations in amplitude over a fairly wide range of periods do not significantly affect task performance. However, at a frame time often considered a minimum for immersive VR, frame time variations do produce significant effects on closed loop task performance. The results will be of use to designers of VEs and immersive applications, who often must control frame time variations due to large fluctuations of complexity (graphical and otherwise) in the VE.

</details>


### [4] [HypoChainer: A Collaborative System Combining LLMs and Knowledge Graphs for Hypothesis-Driven Scientific Discovery](https://arxiv.org/abs/2507.17209)

*Haoran Jiang, Shaohan Shi, Yunjie Yao, Chang Jiang, Quan Li*

**Main category:** cs.HC

**Keywords:** hypothesis generation, graph neural networks, large language models, knowledge graphs, biomedical research

**Relevance Score:** 8

**TL;DR:** HypoChainer is a visualization framework that integrates human expertise with LLMs and knowledge graphs to enhance the process of hypothesis generation and validation in scientific discovery.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in integrating vast and heterogeneous knowledge in biomedical research and drug development, which are overwhelmed by complex biological systems and limitations of traditional research methods.

**Method:** HypoChainer operates in three stages: exploration and contextualization using retrieval-augmented LLMs and dimensionality reduction, hypothesis chain formation with expert examination of KG relationships, and validation prioritization based on KG-supported evidence.

**Key Contributions:**

	1. Integration of human expertise with LLMs and KGs for hypothesis generation.
	2. Three-stage process enhances scalability and interpretability of scientific discovery.
	3. Demonstrated effectiveness through case studies and expert interviews.

**Result:** Case studies demonstrate HypoChainer's effectiveness in supporting interpretable and scalable scientific discovery, allowing for better hypothesis generation and validation driven by LLMs and expert insights.

**Limitations:** 

**Conclusion:** HypoChainer shows promise in enhancing the reliability and efficiency of hypothesis generation and validation through a collaborative framework that combines human and machine intelligence.

**Abstract:** Modern scientific discovery faces growing challenges in integrating vast and heterogeneous knowledge critical to breakthroughs in biomedicine and drug development. Traditional hypothesis-driven research, though effective, is constrained by human cognitive limits, the complexity of biological systems, and the high cost of trial-and-error experimentation. Deep learning models, especially graph neural networks (GNNs), have accelerated prediction generation, but the sheer volume of outputs makes manual selection for validation unscalable. Large language models (LLMs) offer promise in filtering and hypothesis generation, yet suffer from hallucinations and lack grounding in structured knowledge, limiting their reliability. To address these issues, we propose HypoChainer, a collaborative visualization framework that integrates human expertise, LLM-driven reasoning, and knowledge graphs (KGs) to enhance hypothesis generation and validation. HypoChainer operates in three stages: First, exploration and contextualization -- experts use retrieval-augmented LLMs (RAGs) and dimensionality reduction to navigate large-scale GNN predictions, assisted by interactive explanations. Second, hypothesis chain formation -- experts iteratively examine KG relationships around predictions and semantically linked entities, refining hypotheses with LLM and KG suggestions. Third, validation prioritization -- refined hypotheses are filtered based on KG-supported evidence to identify high-priority candidates for experimentation, with visual analytics further strengthening weak links in reasoning. We demonstrate HypoChainer's effectiveness through case studies in two domains and expert interviews, highlighting its potential to support interpretable, scalable, and knowledge-grounded scientific discovery.

</details>


### [5] [OceanVive: An Immersive Visualization System for Communicating Complex Oceanic Phenomena](https://arxiv.org/abs/2507.17218)

*Yang Ouyang, Yuchen Wu, Xiyuan Wang, Laixin Xie, Weicong Cheng, Jianping Gan, Quan Li, Xiaojuan Ma*

**Main category:** cs.HC

**Keywords:** ocean visualization, immersive system, science communication

**Relevance Score:** 2

**TL;DR:** OceanVive is an immersive visualization system designed to improve the communication of complex oceanic phenomena such as hypoxia and acidification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Marine science struggles to effectively communicate complex oceanic phenomena due to limitations of traditional visualization and reporting methods.

**Method:** The system uses a table-sized tablet combined with a large screen to create navigable spatial narratives, incorporating adaptive visual encodings and storytelling techniques.

**Key Contributions:**

	1. Development of an immersive visualization system for ocean data
	2. Integration of adaptive visual encodings and storytelling in scientific communication
	3. Validation through expert interviews to demonstrate efficacy

**Result:** Experts validate OceanVive as a tool that enhances science communication and fosters public understanding of ocean changes.

**Limitations:** 

**Conclusion:** OceanVive shows promise in transforming how complex ocean datasets are presented and interpreted by the public.

**Abstract:** Communicating the complexity of oceanic phenomena-such as hypoxia and acidification-poses a persistent challenge for marine science. Despite advances in sensing technologies and computational models, conventional formats like static visualizations and text-based reports often fall short in conveying the dynamics of ocean changes. To address this gap, we present OceanVive, an immersive and interactive visualization system that transforms complex ocean datasets into navigable spatial narratives. OceanVive incorporates an exploratory panel on a table-sized tablet for managing immersive content on a large screen and integrates adaptive visual encodings, contextual storytelling, and intuitive navigation pathways to support effective communication. We validate the system through expert interviews, demonstrating its potential to enhance science communication and promote deeper public understanding.

</details>


### [6] [A "watch your replay videos" reflection assignment on comparing programming without versus with generative AI: learning about programming, critical AI use and limitations, and reflection](https://arxiv.org/abs/2507.17226)

*Sarah "Magz" Fernandez, Greg L Nelson*

**Main category:** cs.HC

**Keywords:** Generative AI, programming education, video reflections, metacognition, software engineering

**Relevance Score:** 8

**TL;DR:** This paper explores a novel assignment using video reflections in software engineering education to help students understand the impact of generative AI on their programming processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in computing education where most interventions focus on using generative AI rather than understanding its implications on programming.

**Method:** A comparative video reflection assignment was designed, where students recorded their programming sessions first without and then with generative AI, followed by thematic analysis of their reflections.

**Key Contributions:**

	1. Developed a comparative video reflection assignment to enhance understanding of AI in programming education.
	2. Found that students learned to critically engage with AI rather than simply using it as a tool.
	3. Demonstrated the importance of metacognitive skills in programming with and without the aid of AI.

**Result:** Students gained insights into their programming behavior, learning to critically evaluate their use of AI, recognize patterns, and improve their programming processes.

**Limitations:** 

**Conclusion:** Structured reflection on programming videos fosters metacognitive skills and lifelong learning, essential for adapting to generative AI in software development.

**Abstract:** Generative AI is disrupting computing education. Most interventions focus on teaching GenAI use rather than helping students understand how AI changes their programming process. We designed and deployed a novel comparative video reflection assignment adapting the Describe, Examine, then Articulate Learning (DEAL) framework. In an introductory software engineering course, students recorded themselves programming during their team project two times: first without, then with using generative AI. Students then analyzed their own videos using a scaffolded set of reflection questions, including on their programming process and human, internet, and AI help-seeking. We conducted a qualitative thematic analysis of the reflections, finding students developed insights about planning, debugging, and help-seeking behaviors that transcended AI use. Students reported learning to slow down and understand before writing or generating code, recognized patterns in their problem-solving approaches, and articulated specific process improvements. Students also learned and reflected on AI limits and downsides, and strategies to use AI more critically, including better prompting but also to benefit their learning instead of just completing tasks. Unexpectedly, the comparative reflection also scaffolded reflection on programming not involving AI use, and even led to students spontaneously setting future goals to adopt video and other regular reflection. This work demonstrates structured reflection on programming session videos can develop metacognitive skills essential for programming with and without generative AI and also lifelong learning in our evolving field.

</details>


### [7] [Designing for Learning with Generative AI is a Wicked Problem: An Illustrative Longitudinal Qualitative Case Series](https://arxiv.org/abs/2507.17230)

*Clara Scalzer, Saurav Pokhrel, Sara Hunt, Greg L Nelson*

**Main category:** cs.HC

**Keywords:** Generative AI, Education, Ethics, Learning, Career Development

**Relevance Score:** 8

**TL;DR:** This study explores the complexities of integrating generative AI into education, highlighting how promoting GenAI skills can conflict with students' ethical concerns and learning outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges computing educators face in preparing students for careers increasingly influenced by generative AI, focusing on their learning, motivation, ethics, and career development.

**Method:** A longitudinal qualitative study analyzing students' experiences in a GenAI-integrated creative media course, involving extensive instruction on critical GenAI use, ethics, bias, and industry impacts.

**Key Contributions:**

	1. Identifies the conflict between GenAI proficiency and ethical concerns in students.
	2. Highlights the complexities of student motivation and learning in the context of GenAI.
	3. Proposes a multi-dimensional approach to addressing educational challenges related to GenAI.

**Result:** The study reveals that while increasing GenAI skills can lead to ethical compromises, enhancing ethical awareness can hinder skill development, resulting in a complex relationship among learning, ethics, and career confidence.

**Limitations:** The study is based on a qualitative analysis of just two students, which may limit the generalizability of the findings.

**Conclusion:** Supporting student development in the era of GenAI requires a multi-dimensional approach to evaluation and design, rather than optimizing specific separate elements like learning, skills, ethics, or motivation.

**Abstract:** Students continue their education when they feel their learning is meaningful and relevant for their future careers. Computing educators now face the challenge of preparing students for careers increasingly shaped by generative AI (GenAI) with the goals of supporting their learning, motivation, ethics, and career development. Our longitudinal qualitative study of students in a GenAI-integrated creative media course shows how this is a "wicked" problem: progress on one goal can then impede progress on other goals. Students developed concerning patterns despite extensive instruction in critical and ethical GenAI use including prompt engineering, ethics and bias, and industry panels on GenAI's career impact. We present an analysis of two students' experiences to showcase this complexity. Increasing GenAI use skills can lower ethics; for example, Pat started from purposefully avoiding GenAI use, to dependency. He described himself as a "notorious cheater" who now uses GenAi to "get all the right answers" while acknowledging he's learning less. Increasing ethical awareness can lower the learning of GenAI use skills; for example, Jay's newfound environmental concerns led to self-imposed usage limits that impeded skill development, and new serious fears that GenAI would eliminate creative careers they had been passionate about. Increased GenAI proficiency, a potential career skill, did not improve their career confidence. These findings suggest that supporting student development in the GenAI era is a "wicked" problem requiring multi-dimensional evaluation and design, rather than optimizing learning, GenAI skills, ethics, or career motivation individually.

</details>


### [8] [High-Density EEG Enables the Fastest Visual Brain-Computer Interfaces](https://arxiv.org/abs/2507.17242)

*Gege Ming, Weihua Pei, Sen Tian, Xiaogang Chen, Xiaorong Gao, Yijun Wang*

**Main category:** cs.HC

**Keywords:** Brain-computer interface, high-density EEG, information transfer rate, spatiotemporal information, visual stimuli

**Relevance Score:** 4

**TL;DR:** This study presents a novel frequency-phase-space fusion encoding method for improving information transfer rates in brain-computer interface (BCI) systems using high-density EEG recordings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current visual BCI systems have low information transfer rates due to limitations in spatial resolution and underutilization of spatial information.

**Method:** The study employed a frequency-phase-space fusion encoding method with 256-channel high-density EEG recordings to enhance BCI performance.

**Key Contributions:**

	1. Development of a new frequency-phase-space fusion encoding method
	2. Demonstration of significant improvements in information transfer rates
	3. Empirical validation of high-speed BCI systems using 256-channel EEG recordings

**Result:** The proposed system exhibited significantly increased theoretical information transfer rates: 83.66%, 79.99%, and 55.50% enhancements for different electrode configurations compared to the traditional setup, and an actual ITR of 472.7 bpm.

**Limitations:** 

**Conclusion:** The findings highlight the critical importance of high-density EEG in effectively decoding spatiotemporal information from visual stimuli, leading to faster and more efficient BCIs.

**Abstract:** Brain-computer interface (BCI) technology establishes a direct communication pathway between the brain and external devices. Current visual BCI systems suffer from insufficient information transfer rates (ITRs) for practical use. Spatial information, a critical component of visual perception, remains underexploited in existing systems because the limited spatial resolution of recording methods hinders the capture of the rich spatiotemporal dynamics of brain signals. This study proposed a frequency-phase-space fusion encoding method, integrated with 256-channel high-density electroencephalogram (EEG) recordings, to develop high-speed BCI systems. In the classical frequency-phase encoding 40-target BCI paradigm, the 256-66, 128-32, and 64-21 electrode configurations brought theoretical ITR increases of 83.66%, 79.99%, and 55.50% over the traditional 64-9 setup. In the proposed frequency-phase-space encoding 200-target BCI paradigm, these increases climbed to 195.56%, 153.08%, and 103.07%. The online BCI system achieved an average actual ITR of 472.7 bpm. This study demonstrates the essential role and immense potential of high-density EEG in decoding the spatiotemporal information of visual stimuli.

</details>


### [9] [Reality Proxy: Fluid Interactions with Real-World Objects in MR via Abstract Representations](https://arxiv.org/abs/2507.17248)

*Xiaoan Liu, Difan Jia, Xianhao Carton Liu, Mar Gonzalez-Franco, Chen Zhu-Tian*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Human-Computer Interaction, Proxies, AI, User Interaction

**Relevance Score:** 8

**TL;DR:** The paper introduces Reality Proxy, a Mixed Reality system that decouples interaction from physical constraints by using proxies—abstract representations of real-world objects—to enhance user interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The difficulties in interacting with real-world objects in Mixed Reality often arise when they are crowded, distant, or occluded, necessitating a need for improved interaction methods.

**Method:** The paper presents Reality Proxy, a system that shifts interaction from physical objects to abstract proxies during selection, using AI to enrich these proxies with semantic attributes and spatial relationships.

**Key Contributions:**

	1. Introduction of semi-abstract proxies for interaction in MR
	2. AI-enhanced proxy attributes and relationships
	3. Versatile application scenarios including multi-drone control

**Result:** Reality Proxy enables innovative interactions such as skimming, attribute-based filtering, and complex multi-object selections across various scenarios including office information retrieval and multi-drone control.

**Limitations:** 

**Conclusion:** The expert evaluation indicates that proxy-based abstractions provide a powerful interaction paradigm for future Mixed Reality systems, enhancing usability and utility.

**Abstract:** Interacting with real-world objects in Mixed Reality (MR) often proves difficult when they are crowded, distant, or partially occluded, hindering straightforward selection and manipulation. We observe that these difficulties stem from performing interaction directly on physical objects, where input is tightly coupled to their physical constraints. Our key insight is to decouple interaction from these constraints by introducing proxies-abstract representations of real-world objects. We embody this concept in Reality Proxy, a system that seamlessly shifts interaction targets from physical objects to their proxies during selection. Beyond facilitating basic selection, Reality Proxy uses AI to enrich proxies with semantic attributes and hierarchical spatial relationships of their corresponding physical objects, enabling novel and previously cumbersome interactions in MR - such as skimming, attribute-based filtering, navigating nested groups, and complex multi object selections - all without requiring new gestures or menu systems. We demonstrate Reality Proxy's versatility across diverse scenarios, including office information retrieval, large-scale spatial navigation, and multi-drone control. An expert evaluation suggests the system's utility and usability, suggesting that proxy-based abstractions offer a powerful and generalizable interaction paradigm for future MR systems.

</details>


### [10] [EventLines: Time Compression for Discrete Event Timelines](https://arxiv.org/abs/2507.17320)

*Yuet Ling Wong, Niklas Elmqvist*

**Main category:** cs.HC

**Keywords:** event sequences, dynamic time scaling, visualization, temporal perception, User study

**Relevance Score:** 5

**TL;DR:** Introduction of EventLines, a technique for visualizing discrete event sequences with dynamic time scaling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the representation of discrete event sequences that exhibit bursty behavior, which standard timeline charts fail to show effectively.

**Method:** EventLines employs dynamic adjustments to the time scale based on the distribution of events, enhancing visual clarity by using the time axis's visual representation.

**Key Contributions:**

	1. Introduction of a novel dynamic time scaling technique for event sequences
	2. Findings from a study on graphical perception related to time visualization
	3. Demonstration of improved clarity and efficiency in the representation of bursty event data

**Result:** The study shows that EventLines allows for a more efficient use of screen space and enhances the perception of temporal data.

**Limitations:** 

**Conclusion:** EventLines provides a promising solution for visualizing event sequences by adapting the time scale to the data, which could improve user comprehension in various applications.

**Abstract:** Discrete event sequences serve as models for numerous real-world datasets, including publications over time, project milestones, and medication dosing during patient treatments. These event sequences typically exhibit bursty behavior, where events cluster together in rapid succession, interspersed with periods of inactivity. Standard timeline charts with linear time axes fail to adequately represent such data, resulting in cluttered regions during event bursts while leaving other areas unutilized. We introduce EventLines, a novel technique that dynamically adjusts the time scale to match the underlying event distribution, enabling more efficient use of screen space. To address the challenges of non-linear time scaling, EventLines employs the time axis's visual representation itself to communicate the varying scale. We present findings from a crowdsourced graphical perception study that examines how different time scale representations influence temporal perception.

</details>


### [11] [Layered Interactions: Exploring Non-Intrusive Digital Craftsmanship Design Through Lacquer Art Interfaces](https://arxiv.org/abs/2507.17430)

*Yan Dong, Hanjie Yu, Yanran Chen, Zipeng Zhang, Qiong Wu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Lacquerware, Craftsmanship, Interactive Interfaces, Design Approach

**Relevance Score:** 7

**TL;DR:** This paper presents Layered Interactions, a design approach that combines HCI technology with traditional lacquerware craftsmanship to create interactive interfaces.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the integration of technology with traditional craftsmanship, enhancing accessibility for artisans and modernizing the craft.

**Method:** The study involves the development of a lacquerware toolkit, user experiments, and semi-structured interviews to explore the design approach and its implications.

**Key Contributions:**

	1. Introduction of Layered Interactions approach merging HCI with craftsmanship.
	2. Development of a toolkit for lacquerware artisans to integrate technology.
	3. Demonstration of enhanced emotional and material qualities in interactive designs.

**Result:** The approach effectively integrates interactive circuits within lacquerware, demonstrating enhanced emotional qualities and material adaptability of interactive interfaces.

**Limitations:** 

**Conclusion:** Layered Interactions fosters collaboration between artisans and technologists, providing a new perspective for the HCI community regarding materiality and design.

**Abstract:** Integrating technology with the distinctive characteristics of craftsmanship has become a key issue in the field of digital craftsmanship. This paper introduces Layered Interactions, a design approach that seamlessly merges Human-Computer Interaction (HCI) technologies with traditional lacquerware craftsmanship. By leveraging the multi-layer structure and material properties of lacquerware, we embed interactive circuits and integrate programmable hardware within the layers, creating tangible interfaces that support diverse interactions. This method enhances the adaptability and practicality of traditional crafts in modern digital contexts. Through the development of a lacquerware toolkit, along with user experiments and semi-structured interviews, we demonstrate that this approach not only makes technology more accessible to traditional artisans but also enhances the materiality and emotional qualities of interactive interfaces. Additionally, it fosters mutual learning and collaboration between artisans and technologists. Our research introduces a cross-disciplinary perspective to the HCI community, broadening the material and design possibilities for interactive interfaces.

</details>


### [12] [SDC-Net: A Domain Adaptation Framework with Semantic-Dynamic Consistency for Cross-Subject EEG Emotion Recognition](https://arxiv.org/abs/2507.17524)

*Jiahao Tang, Youjun Li, Xiangting Fan, Yangxuan Zheng, Siyuan Lu, Xueping Li, Peng Fang, Chenxi Li, Zi-Gang Huang*

**Main category:** cs.HC

**Keywords:** EEG, emotion recognition, domain adaptation, brain-computer interface, unsupervised learning

**Relevance Score:** 8

**TL;DR:** This paper presents an unsupervised Semantic-Dynamic Consistency domain adaptation network (SDC-Net) for EEG-based emotion recognition, addressing challenges in inter-subject variability and lack of labeled data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The practical deployment of EEG-based emotion recognition systems is hindered by inter-subject variability and limited labeled datasets, necessitating a robust method for cross-subject emotion recognition.

**Method:** The proposed SDC-Net employs a Same-Subject Same-Trial Mixup strategy for data augmentation, a dynamic distribution alignment module using reproducing kernel Hilbert space (RKHS), and a dual-domain similarity consistency learning mechanism to enhance emotion recognition accuracy.

**Key Contributions:**

	1. Introduced Same-Subject Same-Trial Mixup for enhanced data diversity.
	2. Developed a dynamic distribution alignment module in RKHS for robust adaptation.
	3. Implemented a dual-domain similarity consistency learning mechanism for effective emotion decoding.

**Result:** Extensive experiments on EEG benchmark datasets show that SDC-Net outperforms existing unsupervised domain adaptation methods in emotion recognition for both cross-subject and cross-session scenarios.

**Limitations:** 

**Conclusion:** SDC-Net significantly enhances the performance and generalization of EEG emotion recognition, paving the way for improved personalized affective brain-computer interfaces.

**Abstract:** Electroencephalography(EEG) based emotion recognition holds great promise for affective brain-computer interfaces (aBCIs), yet practical deployment remains challenging due to substantial inter-subject variability and the lack of labeled data in target domains. To overcome these limitations, we present a novel unsupervised Semantic-Dynamic Consistency domain adaptation network for fully label-free cross-subject EEG emotion recognition. First, we introduce a Same-Subject Same-Trial Mixup strategy that generates augmented samples via intra-trial interpolation, enhancing data diversity while explicitly preserving individual identity to mitigate label ambiguity. Second, we construct a dynamic distribution alignment module in reproducing kernel Hilbert space (RKHS), jointly aligning marginal and conditional distributions through multi-objective kernel mean embedding, and leveraging a confidence-aware pseudo-labeling strategy to ensure stable adaptation. Third, we propose a dual-domain similarity consistency learning mechanism that enforces cross-domain structural constraints based on latent pairwise similarities, enabling semantic boundary learning without relying on temporal synchronization or label priors. To validate the effectiveness and robustness of the proposed SDC-Net, extensive experiments are conducted on three widely used EEG benchmark datasets: SEED, SEED-IV, and Faced. Comparative results against existing unsupervised domain adaptation methods demonstrate that SDC-Net achieves state-of-the-art performance in emotion recognition under both cross-subject and cross-session conditions. This advancement significantly improves the accuracy and generalization capability of emotion decoding, and lays a solid foundation for real-world applications of personalized affective brain-computer interfaces (aBCIs). The source code will be released at https://github.com/XuanSuTrum/SDC-Net.

</details>


### [13] [Anticipate, Simulate, Reason (ASR): A Comprehensive Generative AI Framework for Combating Messaging Scams](https://arxiv.org/abs/2507.17543)

*Xue Wen Tan, Kenneth See, Stanley Kok*

**Main category:** cs.HC

**Keywords:** scam detection, generative AI, user-centered design, large language models, fraud prevention

**Relevance Score:** 8

**TL;DR:** The paper introduces the Anticipate, Simulate, Reason (ASR) framework for improving scam detection in messaging through generative AI, using a specific model (ScamGPT-J) trained on scam conversation data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user security and financial safety against increasing messaging scams by providing a proactive identification and understanding of such scams.

**Method:** The ASR framework uses large language models to simulate scam conversations and predict scammer responses, offering real-time, interpretable support for users.

**Key Contributions:**

	1. Introduction of the ASR framework for scam detection
	2. Development of ScamGPT-J, a language model fine-tuned for scam conversations
	3. Empirical evidence of demographic patterns affecting user vulnerability to scams

**Result:** Experimental evaluation shows that the ASR framework significantly improves scam detection, especially for job scams, and identifies demographic patterns in user vulnerability.

**Limitations:** 

**Conclusion:** The findings highlight the need for user-centered design in AI systems, as those most at risk are often the least open to AI support, underscoring the importance of addressing user perceptions in fraud prevention efforts.

**Abstract:** The rapid growth of messaging scams creates an escalating challenge for user security and financial safety. In this paper, we present the Anticipate, Simulate, Reason (ASR) framework, a generative AI method that enables users to proactively identify and comprehend scams within instant messaging platforms. Using large language models, ASR predicts scammer responses, creates realistic scam conversations, and delivers real-time, interpretable support to end-users. We develop ScamGPT-J, a domain-specific language model fine-tuned on a new, high-quality dataset of scam conversations covering multiple scam types. Thorough experimental evaluation shows that the ASR framework substantially enhances scam detection, particularly in challenging contexts such as job scams, and uncovers important demographic patterns in user vulnerability and perceptions of AI-generated assistance. Our findings reveal a contradiction where those most at risk are often least receptive to AI support, emphasizing the importance of user-centered design in AI-driven fraud prevention. This work advances both the practical and theoretical foundations for interpretable, human-centered AI systems in combating evolving digital threats.

</details>


### [14] [Explainable AI for Collaborative Assessment of 2D/3D Registration Quality](https://arxiv.org/abs/2507.17597)

*Sue Min Cho, Alexander Do, Russell H. Taylor, Mathias Unberath*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Explainable AI, Surgical Navigation

**Relevance Score:** 8

**TL;DR:** This paper proposes an AI framework for verifying 2D/3D registration quality in surgeries, integrating explainability features to enhance human decision-making and operator trust.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The digital transformation in surgery raises the need for safeguarding patient safety through human verification of algorithmic outputs, especially in critical surgical decisions.

**Method:** The study introduces a novel AI framework emphasizing explainability for 2D/3D registration quality verification, comparing AI-only, human-only, human-AI, and human-XAI conditions.

**Key Contributions:**

	1. First AI framework for 2D/3D registration quality verification
	2. Introduction of explainable AI features
	3. Comparison of AI and human decision-making in surgical settings

**Result:** The study finds that explainability features modestly increase user trust but do not surpass the performance of AI alone, suggesting the potential for improving human-AI collaboration for future systems.

**Limitations:** Explainability features improve user trust but not statistical performance; further algorithmic design needed for better outcomes.

**Conclusion:** While the proposed XAI approach shows promise for enhancing decision-making and quality assurance in surgical contexts, further work is needed to improve algorithmic design and collaboration dynamics.

**Abstract:** As surgery embraces digital transformation--integrating sophisticated imaging, advanced algorithms, and robotics to support and automate complex sub-tasks--human judgment of system correctness remains a vital safeguard for patient safety. This shift introduces new "operator-type" roles tasked with verifying complex algorithmic outputs, particularly at critical junctures of the procedure, such as the intermediary check before drilling or implant placement. A prime example is 2D/3D registration, a key enabler of image-based surgical navigation that aligns intraoperative 2D images with preoperative 3D data. Although registration algorithms have advanced significantly, they occasionally yield inaccurate results. Because even small misalignments can lead to revision surgery or irreversible surgical errors, there is a critical need for robust quality assurance. Current visualization-based strategies alone have been found insufficient to enable humans to reliably detect 2D/3D registration misalignments. In response, we propose the first artificial intelligence (AI) framework trained specifically for 2D/3D registration quality verification, augmented by explainability features that clarify the model's decision-making. Our explainable AI (XAI) approach aims to enhance informed decision-making for human operators by providing a second opinion together with a rationale behind it. Through algorithm-centric and human-centered evaluations, we systematically compare four conditions: AI-only, human-only, human-AI, and human-XAI. Our findings reveal that while explainability features modestly improve user trust and willingness to override AI errors, they do not exceed the standalone AI in aggregate performance. Nevertheless, future work extending both the algorithmic design and the human-XAI collaboration elements holds promise for more robust quality assurance of 2D/3D registration.

</details>


### [15] [Mindfulness Meditation and Respiration: Accelerometer-Based Respiration Rate and Mindfulness Progress Estimation to Enhance App Engagement and Mindfulness Skills](https://arxiv.org/abs/2507.17688)

*Mohammad Nur Hossain Khan, David creswell, Jordan Albert, Patrick O'Connell, Shawn Fallon, Mathew Polowitz, Xuhai "orson" Xu, Bashima islam*

**Main category:** cs.HC

**Keywords:** Mindfulness, Biosignal feedback, User engagement, Respiration tracking, Skill estimation

**Relevance Score:** 8

**TL;DR:** This paper investigates the use of respiration biosignal feedback and mindfulness skill estimation to enhance user engagement in smartphone-based mindfulness apps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of sustaining long-term user engagement in digital meditation by leveraging smartphone sensors for feedback and skill estimation.

**Method:** Development of an accelerometer-based respiration tracking algorithm and a quantitative framework for estimating mindfulness skills, tested through a user study comparing biosignal feedback with standard app usage.

**Key Contributions:**

	1. Introduction of a smartphone-based respiration tracking algorithm
	2. First quantitative framework for estimating mindfulness skills
	3. Demonstrated improvement in usability through user studies

**Result:** Respiration feedback improves system usability, with the tracking model achieving a mean absolute error of 1.6 breaths per minute and mindfulness skill estimation having F1 scores of 80-84% in skill progression.

**Limitations:** 

**Conclusion:** Integrating respiration tracking and mindfulness estimation into commercial apps can significantly enhance digital mindfulness training experience.

**Abstract:** Mindfulness training is widely recognized for its benefits in reducing depression, anxiety, and loneliness. With the rise of smartphone-based mindfulness apps, digital meditation has become more accessible, but sustaining long-term user engagement remains a challenge. This paper explores whether respiration biosignal feedback and mindfulness skill estimation enhance system usability and skill development. We develop a smartphone's accelerometer-based respiration tracking algorithm, eliminating the need for additional wearables. Unlike existing methods, our approach accurately captures slow breathing patterns typical of mindfulness meditation. Additionally, we introduce the first quantitative framework to estimate mindfulness skills-concentration, sensory clarity, and equanimity-based on accelerometer-derived respiration data. We develop and test our algorithms on 261 mindfulness sessions in both controlled and real-world settings. A user study comparing an experimental group receiving biosignal feedback with a control group using a standard app shows that respiration feedback enhances system usability. Our respiration tracking model achieves a mean absolute error (MAE) of 1.6 breaths per minute, closely aligning with ground truth data, while our mindfulness skill estimation attains F1 scores of 80-84% in tracking skill progression. By integrating respiration tracking and mindfulness estimation into a commercial app, we demonstrate the potential of smartphone sensors to enhance digital mindfulness training.

</details>


### [16] [DataWink: Reusing and Adapting SVG-based Visualization Examples with Large Multimodal Models](https://arxiv.org/abs/2507.17734)

*Liwenhan Xie, Yanna Lin, Can Liu, Huamin Qu, Xinhuan Shu*

**Main category:** cs.HC

**Keywords:** data visualization, user interface, multimodal models, example-driven design, HCI

**Relevance Score:** 7

**TL;DR:** DataWink is a system that enables users to create custom visualizations by adapting existing high-quality examples, facilitating the visualization process for users without design expertise.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist users without design expertise in creating aesthetically pleasing data visualizations, thus democratizing the visualization creation process.

**Method:** DataWink utilizes large multimodal models to extract data encoding from SVG-based visualization examples, enabling users to adapt visualizations through a conversational agent and interactive widgets.

**Key Contributions:**

	1. Introduction of DataWink for custom visualization creation
	2. Use of large multimodal models for data encoding extraction
	3. User study validating effectiveness and learnability of the system

**Result:** User studies demonstrate that DataWink is effective and learnable for personalized authoring tasks, highlighting the viability of example-driven approaches in visualization.

**Limitations:** 

**Conclusion:** DataWink shows promise in making data visualization accessible to users lacking design skills by allowing customizable adaptations of existing models, thereby improving user engagement with visualization tools.

**Abstract:** Creating aesthetically pleasing data visualizations remains challenging for users without design expertise or familiarity with visualization tools. To address this gap, we present DataWink, a system that enables users to create custom visualizations by adapting high-quality examples. Our approach combines large multimodal models (LMMs) to extract data encoding from existing SVG-based visualization examples, featuring an intermediate representation of visualizations that bridges primitive SVG and visualization programs. Users may express adaptation goals to a conversational agent and control the visual appearance through widgets generated on demand. With an interactive interface, users can modify both data mappings and visual design elements while maintaining the original visualization's aesthetic quality. To evaluate DataWink, we conduct a user study (N=12) with replication and free-form exploration tasks. As a result, DataWink is recognized for its learnability and effectiveness in personalized authoring tasks. Our results demonstrate the potential of example-driven approaches for democratizing visualization creation.

</details>


### [17] [The Paradox of Spreadsheet Self-Efficacy: Social Incentives for Informal Knowledge Sharing in End-User Programming](https://arxiv.org/abs/2408.08068)

*Qing, Xia, Advait Sarkar, Duncan P. Brumby, Anna Cox*

**Main category:** cs.HC

**Keywords:** Knowledge Sharing, Spreadsheet, Self-efficacy, Reputational Gains, User Design

**Relevance Score:** 5

**TL;DR:** This paper examines factors influencing informal knowledge sharing among spreadsheet users, emphasizing personal, social, and software-related variables.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how personal (self-efficacy), social (reputational gains, trust), and software-related (codification effort) factors influence knowledge sharing intentions among end-user programmers.

**Method:** Multiple regression analysis based on survey data from 100 spreadsheet users in administrative and finance roles.

**Key Contributions:**

	1. Identification of personal and social factors affecting knowledge sharing
	2. Quantitative analysis of spreadsheet users' sharing intentions
	3. Insights into spreadsheet design improvements for fostering knowledge sharing

**Result:** High self-efficacy in spreadsheet use and perceived reputational gains led to higher knowledge sharing intentions, while perceived effort in knowledge codification decreased these intentions.

**Limitations:** 

**Conclusion:** Designing for social and personal variables can encourage knowledge sharing among experienced users, which has implications for spreadsheet design.

**Abstract:** Informal Knowledge Sharing (KS) is vital for end-user programmers to gain expertise. To better understand how personal (self-efficacy), social (reputational gains, trust between colleagues), and software-related (codification effort) variables influence spreadsheet KS intention, we conducted a multiple regressions analysis based on survey data from spreadsheet users (n=100) in administrative and finance roles. We found that high levels of spreadsheet self-efficacy and a perception that sharing would result in reputational gains predicted higher KS intention, but individuals who found knowledge codification effortful showed lower KS intention. We also observed that regardless of occupation, users tended to report a lower sense of self-efficacy in their general spreadsheet proficiency, despite also reporting high self-efficacy in spreadsheet use for job-related contexts. Our findings suggest that acknowledging and designing for these social and personal variables can help avoid situations where experienced individuals refrain unnecessarily from sharing, with implications for spreadsheet design.

</details>


### [18] [Image memorability predicts social media virality and externally-associated commenting](https://arxiv.org/abs/2409.14659)

*Shikang Peng, Wilma A. Bainbridge*

**Main category:** cs.HC

**Keywords:** image memorability, social media engagement, virality, neural networks, semantic distinctiveness

**Relevance Score:** 4

**TL;DR:** This study investigates the relationship between image memorability and virality on social media, particularly through the analysis of Reddit image posts.

**Read time:** 47 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why certain images achieve more engagement on social media, specifically regarding their memorability and how it may predict viral potential.

**Method:** Analyzed 1,247 Reddit image posts using the neural network ResMem to assess memorability and correlated that with virality metrics such as the number of comments, while controlling for image categories using ResNet-152.

**Key Contributions:**

	1. Establishes a link between image memorability and social media virality.
	2. Identifies semantic distinctiveness as a key factor in memorability and virality.
	3. Provides insights into the types of comments associated with memorable versus non-memorable images.

**Result:** Memorable images are linked to increased engagement, specifically more comments, and this effect persists when controlling for categories. Additionally, memorable images prompted more neutral-affect comments and were associated with semantic distinctiveness.

**Limitations:** Limited to images from Reddit and specific virality metrics; may not generalize to all social media platforms or image types.

**Conclusion:** Memorability is a significant predictor of social media virality, suggesting that visual features and cognitive interactions influence online engagement.

**Abstract:** Visual content on social media plays a key role in entertainment and information sharing, yet some images gain more engagement than others. We propose that image memorability - the ability to be remembered - may predict viral potential. Using 1,247 Reddit image posts across three timepoints, we assessed memorability with neural network ResMem and correlated the predicted memorability scores with virality metrics. Memorable images are consistently associated with more comments, even after controlling for image categories with ResNet-152. Semantic analysis revealed that memorable images relate to more neutral-affect comments, suggesting a distinct pathway to virality from emotional contents. Additionally, visual consistency analysis showed that memorable posts inspired diverse, externally-associated comments. By analyzing ResMem's layers, we found that semantic distinctiveness was key to both memorability and virality even after accounting for image category effects. This study highlights memorability as a unique correlate of social media virality, offering insights into how visual features and human cognitive behavioral interactions are associated with online engagement.

</details>


### [19] [Alleviating Seasickness through Brain-Computer Interface-based Attention Shift](https://arxiv.org/abs/2501.08518)

*Xiaoyu Bao, Kailin Xu, Jiawei Zhu, Haiyun Huang, Kangning Li, Qiyun Huang, Yuanqing Li*

**Main category:** cs.HC

**Keywords:** brain-computer interface, seasickness, attention shift, EEG analysis, nonpharmaceutical intervention

**Relevance Score:** 3

**TL;DR:** This study develops an AI-driven brain-computer interface (BCI) to alleviate seasickness by promoting attention shift, with significant participant effectiveness reported.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the widespread problem of seasickness affecting passenger comfort and maritime operations.

**Method:** An AI-driven BCI was developed, incorporating tasks like breath counting, tested in a real-world nautical experiment with 43 participants across feedback sessions.

**Key Contributions:**

	1. Development of an AI-driven BCI for seasickness intervention
	2. Real-world testing with significant participant feedback
	3. EEG analysis supporting BCI effectiveness in attention shift

**Result:** 81.39% of participants found the BCI intervention effective; EEG analysis showed regulation of motion sickness EEG signatures and improved attentional focus.

**Limitations:** 

**Conclusion:** This BCI approach offers a novel, portable, nonpharmacological solution for seasickness, expanding BCI applications.

**Abstract:** Seasickness poses a widespread problem that adversely impacts both passenger comfort and the operational efficiency of maritime crews. Although attention shift has been proposed as a potential method to alleviate symptoms of motion sickness, its efficacy remains to be rigorously validated, especially in maritime environments. In this study, we develop an AI-driven brain-computer interface (BCI) to realize sustained and practical attention shift by incorporating tasks such as breath counting. Forty-three participants completed a real-world nautical experiment consisting of a real-feedback session, a resting session, and a pseudo-feedback session. Notably, 81.39\% of the participants reported that the BCI intervention was effective. EEG analysis revealed that the proposed system can effectively regulate motion sickness EEG signatures, such as an decrease in total band power, along with an increase in theta relative power and a decrease in beta relative power. Furthermore, an indicator of attentional focus, the theta/beta ratio, exhibited a significant reduction during the real-feedback session, providing further evidence to support the effectiveness of the BCI in shifting attention. Collectively, this study presents a novel nonpharmacological, portable, and effective approach for seasickness intervention, which has the potential to open up a brand-new application domain for BCIs.

</details>


### [20] [Animal Interaction with Autonomous Mobility Systems: Designing for Multi-Species Coexistence](https://arxiv.org/abs/2507.16258)

*Tram Thi Minh Tran, Xinyan Yu, Marius Hoggenmueller, Callum Parker, Paul Schmitt, Julie Stephany Berrio Perez, Stewart Worrall, Martin Tomitsch*

**Main category:** cs.HC

**Keywords:** Autonomous Mobility, Animal-Computer Interaction, Multispecies Design, HCI, Ethics

**Relevance Score:** 3

**TL;DR:** The paper investigates how autonomous mobility systems affect animal interactions, emphasizing the importance of including non-human perspectives in design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited understanding of how non-human species perceive and are affected by autonomous mobility systems designed primarily for human interaction.

**Method:** A multi-method approach combining a scoping review of 45 articles, online ethnography of 39 YouTube videos and 11 Reddit discussions, and interviews with 8 experts.

**Key Contributions:**

	1. Highlights the intersection of Animal-Computer Interaction with autonomous mobility design.
	2. Presents a comprehensive analysis of animal interactions with autonomous systems.
	3. Proposes design and policy recommendations for multispecies coexistence.

**Result:** Identified five key areas of concern: Physical Impact, Behavioural Effects, Accessibility Concerns, Ethics and Regulations, and Urban Disturbance.

**Limitations:** 

**Conclusion:** The study provides design and policy directions for fostering multispecies coexistence with autonomous systems by considering the needs and perspectives of non-human species.

**Abstract:** Autonomous mobility systems increasingly operate in environments shared with animals, from urban pets to wildlife. However, their design has largely focused on human interaction, with limited understanding of how non-human species perceive, respond to, or are affected by these systems. Motivated by research in Animal-Computer Interaction (ACI) and more-than-human design, this study investigates animal interactions with autonomous mobility through a multi-method approach combining a scoping review (45 articles), online ethnography (39 YouTube videos and 11 Reddit discussions), and expert interviews (8 participants). Our analysis surfaces five key areas of concern: Physical Impact (e.g., collisions, failures to detect), Behavioural Effects (e.g., avoidance, stress), Accessibility Concerns (particularly for service animals), Ethics and Regulations, and Urban Disturbance. We conclude with design and policy directions aimed at supporting multispecies coexistence in the age of autonomous systems. This work underscores the importance of incorporating non-human perspectives to ensure safer, more inclusive futures for all species.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [A Unifying Scheme for Extractive Content Selection Tasks](https://arxiv.org/abs/2507.16922)

*Shmuel Amar, Ori Shapira, Aviv Slobodkin, Ido Dagan*

**Main category:** cs.CL

**Keywords:** NLP, Content Selection, Instruction-guided Learning, Benchmarking, Transfer Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a unified framework for content selection tasks in NLP, proposing instruction-guided content selection (IGCS) and establishing a benchmark, igcsbench, alongside a large synthetic dataset to enhance performance across tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a unified framework for content selection tasks, which have been studied in isolation, and to address the limitations in model performance and evaluation metrics.

**Method:** The work proposes the IGCS framework which encapsulates task definitions and requests as instructions for language models. It establishes igcsbench as a benchmark and develops synthetic datasets for transfer learning.

**Key Contributions:**

	1. Introduction of instruction-guided content selection (IGCS) as a unified framework.
	2. Development of igcsbench, the first benchmark for diverse content selection tasks.
	3. Creation of a large synthetic dataset to improve transfer learning in content selection.

**Result:** The introduction of IGCS and igcsbench has shown improved performance in content selection tasks through transfer learning, regardless of the availability of dedicated training data.

**Limitations:** 

**Conclusion:** The proposed resources and methods are valuable for advancing content selection models and addressing common inference time issues in LLM-based approaches.

**Abstract:** A broad range of NLP tasks involve selecting relevant text spans from given source texts. Despite this shared objective, such \textit{content selection} tasks have traditionally been studied in isolation, each with its own modeling approaches, datasets, and evaluation metrics. In this work, we propose \textit{instruction-guided content selection (IGCS)} as a beneficial unified framework for such settings, where the task definition and any instance-specific request are encapsulated as instructions to a language model. To promote this framework, we introduce \igcsbench{}, the first unified benchmark covering diverse content selection tasks. Further, we create a large generic synthetic dataset that can be leveraged for diverse content selection tasks, and show that transfer learning with these datasets often boosts performance, whether dedicated training for the targeted task is available or not. Finally, we address generic inference time issues that arise in LLM-based modeling of content selection, assess a generic evaluation metric, and overall propose the utility of our resources and methods for future content selection models. Models and datasets available at https://github.com/shmuelamar/igcs.

</details>


### [22] [AI-based Clinical Decision Support for Primary Care: A Real-World Study](https://arxiv.org/abs/2507.16947)

*Robert Korom, Sarah Kiptinness, Najib Adan, Kassim Said, Catherine Ithuli, Oliver Rotich, Boniface Kimani, Irene King'ori, Stellah Kamau, Elizabeth Atemba, Muna Aden, Preston Bowman, Michael Sharman, Rebecca Soskin Hicks, Rebecca Distler, Johannes Heidecke, Rahul K. Arora, Karan Singhal*

**Main category:** cs.CL

**Keywords:** clinical decision support, large language models, health informatics, workflow integration, error reduction

**Relevance Score:** 9

**TL;DR:** The study evaluates the effectiveness of AI Consult, an LLM-based clinical decision support tool, in reducing diagnostic and treatment errors in live care settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the impact of AI Consult in improving clinical decision-making and documentation accuracy in primary care.

**Method:** A quality improvement study analyzing patient visit outcomes with and without access to AI Consult across 15 clinics, involving 39,849 patient visits.

**Key Contributions:**

	1. Demonstrated the effectiveness of LLM-based decision support in real-world healthcare settings.
	2. Provided empirical evidence of reduced clinical errors with AI Consult use.
	3. Outlined a practical framework for implementing AI tools in clinical workflows.

**Result:** Clinicians using AI Consult made 16% fewer diagnostic errors and 13% fewer treatment errors, potentially preventing 22,000 diagnostic and 29,000 treatment errors annually at Penda Health.

**Limitations:** 

**Conclusion:** The study demonstrates the efficacy of LLM-based clinical decision support in reducing errors and emphasizes the importance of integrating such tools into clinical workflows.

**Abstract:** We evaluate the impact of large language model-based clinical decision support in live care. In partnership with Penda Health, a network of primary care clinics in Nairobi, Kenya, we studied AI Consult, a tool that serves as a safety net for clinicians by identifying potential documentation and clinical decision-making errors. AI Consult integrates into clinician workflows, activating only when needed and preserving clinician autonomy. We conducted a quality improvement study, comparing outcomes for 39,849 patient visits performed by clinicians with or without access to AI Consult across 15 clinics. Visits were rated by independent physicians to identify clinical errors. Clinicians with access to AI Consult made relatively fewer errors: 16% fewer diagnostic errors and 13% fewer treatment errors. In absolute terms, the introduction of AI Consult would avert diagnostic errors in 22,000 visits and treatment errors in 29,000 visits annually at Penda alone. In a survey of clinicians with AI Consult, all clinicians said that AI Consult improved the quality of care they delivered, with 75% saying the effect was "substantial". These results required a clinical workflow-aligned AI Consult implementation and active deployment to encourage clinician uptake. We hope this study demonstrates the potential for LLM-based clinical decision support tools to reduce errors in real-world settings and provides a practical framework for advancing responsible adoption.

</details>


### [23] [Harnessing RLHF for Robust Unanswerability Recognition and Trustworthy Response Generation in LLMs](https://arxiv.org/abs/2507.16951)

*Shuyuan Lin, Lei Duan, Philip Hughes, Yuxuan Sheng*

**Main category:** cs.CL

**Keywords:** Conversational Information Retrieval, Large Language Models, Unanswerable Questions, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This paper presents SALU, a Self-Aware LLM for handling unanswerable questions in conversational information retrieval by integrating unanswerability detection within the generative process.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** CIR systems struggle with unanswerable questions, risking misleading outputs and hallucinations.

**Method:** SALU employs a multi-task learning framework combined with a reinforcement learning phase that incorporates human feedback to reduce hallucinations and improve accuracy.

**Key Contributions:**

	1. Introduction of Self-Aware LLM for unanswerability in CIR
	2. Multi-task learning integration for QA and abstention
	3. Reinforcement learning with human feedback to boost accuracy and reduce hallucination

**Result:** SALU outperforms traditional LLM-classifier systems in accuracy for answering and abstaining from questions and demonstrates higher reliability as per human evaluations.

**Limitations:** 

**Conclusion:** SALU shows significant improvements in managing unanswerable queries, reducing hallucination while enhancing overall factuality and reliability.

**Abstract:** Conversational Information Retrieval (CIR) systems, while offering intuitive access to information, face a significant challenge: reliably handling unanswerable questions to prevent the generation of misleading or hallucinated content. Traditional approaches often rely on external classifiers, which can introduce inconsistencies with the core generative Large Language Models (LLMs). This paper introduces Self-Aware LLM for Unanswerability (SALU), a novel approach that deeply integrates unanswerability detection directly within the LLM's generative process. SALU is trained using a multi-task learning framework for both standard Question Answering (QA) and explicit abstention generation for unanswerable queries. Crucially, it incorporates a confidence-score-guided reinforcement learning with human feedback (RLHF) phase, which explicitly penalizes hallucinated responses and rewards appropriate abstentions, fostering intrinsic self-awareness of knowledge boundaries. Through extensive experiments on our custom-built C-IR_Answerability dataset, SALU consistently outperforms strong baselines, including hybrid LLM-classifier systems, in overall accuracy for correctly answering or abstaining from questions. Human evaluation further confirms SALU's superior reliability, achieving high scores in factuality, appropriate abstention, and, most importantly, a dramatic reduction in hallucination, demonstrating its ability to robustly "know when to say 'I don't know'."

</details>


### [24] [Text-to-SPARQL Goes Beyond English: Multilingual Question Answering Over Knowledge Graphs through Human-Inspired Reasoning](https://arxiv.org/abs/2507.16971)

*Aleksandr Perevalov, Andreas Both*

**Main category:** cs.CL

**Keywords:** multilingual natural-language processing, knowledge graphs, query refinement

**Relevance Score:** 6

**TL;DR:** mKGQAgent is a framework that converts natural language questions into SPARQL queries through modular subtasks, utilizing a coordinated LLM agent workflow.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of accessing knowledge via multilingual natural-language interfaces and transforming natural-language input into queries for information retrieval.

**Method:** mKGQAgent uses a human-inspired framework that breaks down the conversion task into modular, interpretable subtasks involving planning, entity linking, and query refinement, supported by in-context learning from an experience pool.

**Key Contributions:**

	1. Introduction of mKGQAgent framework for KGQA
	2. Modular and interpretable subtasks for query conversion
	3. First-place performance in Text2SPARQL challenge 2025

**Result:** mKGQAgent achieved first place in the Text2SPARQL challenge 2025, demonstrating its effectiveness on DBpedia- and Corporate-based KGQA benchmarks.

**Limitations:** 

**Conclusion:** The work contributes to the development of human-like reasoning systems in multilingual semantic parsing.

**Abstract:** Accessing knowledge via multilingual natural-language interfaces is one of the emerging challenges in the field of information retrieval and related ones. Structured knowledge stored in knowledge graphs can be queried via a specific query language (e.g., SPARQL). Therefore, one needs to transform natural-language input into a query to fulfill an information need. Prior approaches mostly focused on combining components (e.g., rule-based or neural-based) that solve downstream tasks and come up with an answer at the end. We introduce mKGQAgent, a human-inspired framework that breaks down the task of converting natural language questions into SPARQL queries into modular, interpretable subtasks. By leveraging a coordinated LLM agent workflow for planning, entity linking, and query refinement - guided by an experience pool for in-context learning - mKGQAgent efficiently handles multilingual KGQA. Evaluated on the DBpedia- and Corporate-based KGQA benchmarks within the Text2SPARQL challenge 2025, our approach took first place among the other participants. This work opens new avenues for developing human-like reasoning systems in multilingual semantic parsing.

</details>


### [25] [Leveraging Synthetic Data for Question Answering with Multilingual LLMs in the Agricultural Domain](https://arxiv.org/abs/2507.16974)

*Rishemjit Kaur, Arshdeep Singh Bhankhar, Surangika Ranathunga, Jashanpreet Singh Salh, Sudhir Rajput, Vidhi, Kashish Mahendra, Bhavika Berwal, Ritesh Kumar*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Synthetic datasets, Agricultural advisory systems

**Relevance Score:** 6

**TL;DR:** This paper discusses the generation of multilingual synthetic agricultural datasets and the fine-tuning of language-specific LLMs to improve agricultural advisory services in various languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide accurate agricultural information in native languages for farmers, as existing LLMs offer generic and imprecise advice due to a lack of domain-specific training.

**Method:** The study generates multilingual synthetic datasets from agriculture-specific documents in English, Hindi, and Punjabi, and fine-tunes language-specific LLMs using these datasets.

**Key Contributions:**

	1. Generation of multilingual synthetic agricultural datasets
	2. Fine-tuning of language-specific LLMs
	3. Demonstration of improvements in accuracy and relevance of agricultural advisories

**Result:** Fine-tuned models show significant improvements in factual accuracy, relevance, and consensus in agricultural knowledge compared to baseline models.

**Limitations:** Limited to the specific languages and contexts covered in the datasets; may not address all agricultural domains or languages.

**Conclusion:** Synthetic data-driven, language-specific fine-tuning enhances LLM performance in agriculture, facilitating better advisory services for multilingual communities.

**Abstract:** Enabling farmers to access accurate agriculture-related information in their native languages in a timely manner is crucial for the success of the agriculture field. Although large language models (LLMs) can be used to implement Question Answering (QA) systems, simply using publicly available general-purpose LLMs in agriculture typically offer generic advisories, lacking precision in local and multilingual contexts due to insufficient domain-specific training and scarcity of high-quality, region-specific datasets. Our study addresses these limitations by generating multilingual synthetic agricultural datasets (English, Hindi, Punjabi) from agriculture-specific documents and fine-tuning language-specific LLMs. Our evaluation on curated multilingual datasets demonstrates significant improvements in factual accuracy, relevance, and agricultural consensus for the fine-tuned models compared to their baseline counterparts. These results highlight the efficacy of synthetic data-driven, language-specific fine-tuning as an effective strategy to improve the performance of LLMs in agriculture, especially in multilingual and low-resource settings. By enabling more accurate and localized agricultural advisory services, this study provides a meaningful step toward bridging the knowledge gap in AI-driven agricultural solutions for diverse linguistic communities.

</details>


### [26] [Obscured but Not Erased: Evaluating Nationality Bias in LLMs via Name-Based Bias Benchmarks](https://arxiv.org/abs/2507.16989)

*Giulio Pelosio, Devesh Batra, Noémie Bovey, Robert Hankache, Cristovao Iglesias, Greig Cowan, Raad Khraishi*

**Main category:** cs.CL

**Keywords:** Large Language Models, bias, natural language processing, cultural names, AI systems

**Relevance Score:** 8

**TL;DR:** This paper examines how substituting explicit nationality labels with culturally indicative names affects bias and accuracy in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research investigates latent biases in LLMs without explicit demographic markers, focusing on real-world implications of these biases.

**Method:** A novel name-based benchmarking approach derived from the Bias Benchmark for QA (BBQ) dataset was used to compare bias magnitude and accuracy across different LLMs.

**Key Contributions:**

	1. Introduction of a name-based benchmarking approach for LLMs
	2. Demonstration of bias differences between small and large models
	3. Insights into error retention rates in ambiguous contexts

**Result:** Smaller models show higher bias and lower accuracy than larger models; for example, Claude Haiku had a bias score of 9% compared to 3.5% for Claude Sonnet, with significant differences in accuracy.

**Limitations:** 

**Conclusion:** The study emphasizes the resilience of biases in LLMs, which has critical implications for AI system development in diverse contexts.

**Abstract:** Large Language Models (LLMs) can exhibit latent biases towards specific nationalities even when explicit demographic markers are not present. In this work, we introduce a novel name-based benchmarking approach derived from the Bias Benchmark for QA (BBQ) dataset to investigate the impact of substituting explicit nationality labels with culturally indicative names, a scenario more reflective of real-world LLM applications. Our novel approach examines how this substitution affects both bias magnitude and accuracy across a spectrum of LLMs from industry leaders such as OpenAI, Google, and Anthropic. Our experiments show that small models are less accurate and exhibit more bias compared to their larger counterparts. For instance, on our name-based dataset and in the ambiguous context (where the correct choice is not revealed), Claude Haiku exhibited the worst stereotypical bias scores of 9%, compared to only 3.5% for its larger counterpart, Claude Sonnet, where the latter also outperformed it by 117.7% in accuracy. Additionally, we find that small models retain a larger portion of existing errors in these ambiguous contexts. For example, after substituting names for explicit nationality references, GPT-4o retains 68% of the error rate versus 76% for GPT-4o-mini, with similar findings for other model providers, in the ambiguous context. Our research highlights the stubborn resilience of biases in LLMs, underscoring their profound implications for the development and deployment of AI systems in diverse, global contexts.

</details>


### [27] [Multi-Label Classification with Generative AI Models in Healthcare: A Case Study of Suicidality and Risk Factors](https://arxiv.org/abs/2507.17009)

*Ming Huang, Zehan Li, Yan Hu, Wanjing Wang, Andrew Wen, Scott Lane, Salih Selek, Lokesh Shahani, Rodrigo Machado-Vieira, Jair Soares, Hua Xu, Hongfang Liu*

**Main category:** cs.CL

**Keywords:** suicidality, large language models, classification, electronic health records, AI in healthcare

**Relevance Score:** 9

**TL;DR:** This study leverages generative large language models (LLMs) for multi-label classification of suicidality-related factors from psychiatric electronic health records, achieving high performance metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the urgent need for early identification of suicidality-related factors (SrFs) in the context of a global mental health crisis.

**Method:** Employs generative LLMs (GPT-3.5 and GPT-4.5) for multi-label classification of SrFs, incorporating advanced evaluation techniques for better error analysis.

**Key Contributions:**

	1. Novel end-to-end generative multi-label classification pipeline
	2. Introduction of advanced evaluation methods for clinical tasks
	3. Analysis of systematic error patterns in AI-generated classifications

**Result:** Finetuned GPT-3.5 reached 0.94 partial match accuracy and 0.91 F1 score; GPT-4.5 showed improved performance across diverse label sets, highlighting systematic error patterns.

**Limitations:** 

**Conclusion:** The research demonstrates the potential of generative AI in clinical classification tasks and suggests methods for structuring EHR data for enhanced clinical research.

**Abstract:** Suicide remains a pressing global health crisis, with over 720,000 deaths annually and millions more affected by suicide ideation (SI) and suicide attempts (SA). Early identification of suicidality-related factors (SrFs), including SI, SA, exposure to suicide (ES), and non-suicidal self-injury (NSSI), is critical for timely intervention. While prior studies have applied AI to detect SrFs in clinical notes, most treat suicidality as a binary classification task, overlooking the complexity of cooccurring risk factors. This study explores the use of generative large language models (LLMs), specifically GPT-3.5 and GPT-4.5, for multi-label classification (MLC) of SrFs from psychiatric electronic health records (EHRs). We present a novel end to end generative MLC pipeline and introduce advanced evaluation methods, including label set level metrics and a multilabel confusion matrix for error analysis. Finetuned GPT-3.5 achieved top performance with 0.94 partial match accuracy and 0.91 F1 score, while GPT-4.5 with guided prompting showed superior performance across label sets, including rare or minority label sets, indicating a more balanced and robust performance. Our findings reveal systematic error patterns, such as the conflation of SI and SA, and highlight the models tendency toward cautious over labeling. This work not only demonstrates the feasibility of using generative AI for complex clinical classification tasks but also provides a blueprint for structuring unstructured EHR data to support large scale clinical research and evidence based medicine.

</details>


### [28] [Can External Validation Tools Improve Annotation Quality for LLM-as-a-Judge?](https://arxiv.org/abs/2507.17015)

*Arduin Findeis, Floris Weers, Guoli Yin, Ke Ye, Ruoming Pang, Tom Gunter*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-Computer Interaction, Tool-using Systems, Response Evaluation, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper proposes a tool-using agentic system to enhance the evaluation of large language models (LLMs) by using external validation via web-search and code execution, particularly for challenging domains such as long-form factual, math, and code tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of feedback on LLM responses, especially in domains where traditional evaluation metrics are inadequate or biased, such as factual accuracy versus writing quality.

**Method:** The authors developed a system that integrates tool usage (web-search and code execution) to provide ground-truth validation for evaluating responses, thereby reducing reliance on internal LLM knowledge.

**Key Contributions:**

	1. Introduction of a tool-using agentic system for LLM evaluation.
	2. Demonstration of improved feedback quality in challenging response domains.
	3. Highlighting sensitivity of performance to evaluation parameters.

**Result:** Experimental results show that the proposed system can enhance performance in several domains but reveals that performance is highly sensitive to parameters like prompts.

**Limitations:** Not all domains benefited from the external tools; results showed variability depending on specific contexts.

**Conclusion:** External tools can improve LLM evaluation performance but are not universally effective, indicating the need for better annotator benchmarks and more controlled evaluation methods.

**Abstract:** Pairwise preferences over model responses are widely collected to evaluate and provide feedback to large language models (LLMs). Given two alternative model responses to the same input, a human or AI annotator selects the "better" response. This approach can provide feedback for domains where other hard-coded metrics are difficult to obtain (e.g., chat response quality), thereby helping model evaluation or training. However, for some domains high-quality pairwise comparisons can be tricky to obtain - from AI and humans. For example, for responses with many factual statements, annotators may disproportionately weigh writing quality rather than underlying facts. In this work, we explore augmenting standard AI annotator systems with additional tools to improve performance on three challenging response domains: long-form factual, math and code tasks. We propose a tool-using agentic system to provide higher quality feedback on these domains. Our system uses web-search and code execution to ground itself based on external validation, independent of the LLM's internal knowledge and biases. We provide extensive experimental results evaluating our method across the three targeted response domains as well as general annotation tasks, using RewardBench (incl. AlpacaEval and LLMBar), RewardMath, as well as three new datasets for domains with saturated pre-existing datasets. Our results indicate that external tools can indeed improve performance in many, but not all, cases. More generally, our experiments highlight the sensitivity of performance to simple parameters (e.g., prompt) and the need for improved (non-saturated) annotator benchmarks. We share our code at https://github.com/apple/ml-agent-evaluator.

</details>


### [29] [Evolutionary Feature-wise Thresholding for Binary Representation of NLP Embeddings](https://arxiv.org/abs/2507.17025)

*Soumen Sinha, Shahryar Rahnamayan, Azam Asilian Bidgoli*

**Main category:** cs.CL

**Keywords:** text embedding, binary representations, NLP, machine learning, binarization

**Relevance Score:** 7

**TL;DR:** The paper proposes a Coordinate Search-based optimization framework for creating binary embeddings in NLP, which identifies optimal thresholds for features, enhancing accuracy and efficiency in text representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient text embedding techniques in large-scale NLP applications where storage and computational efficiency are critical.

**Method:** A Coordinate Search-based optimization framework that identifies feature-specific optimal thresholds for converting continuous embeddings into binary representations.

**Key Contributions:**

	1. Introduction of a new Coordinate Search-based optimization framework for feature-specific threshold identification
	2. Demonstrated improved performance of binary embeddings in NLP applications
	3. Versatility of the technique for various machine learning domains

**Result:** Binary embeddings generated using the proposed method outperform traditional binarization methods in terms of accuracy across various NLP tasks and datasets.

**Limitations:** 

**Conclusion:** The proposed technique for generating binary representations is versatile, applicable beyond NLP, and shows promise for improving performance in machine learning applications.

**Abstract:** Efficient text embedding is crucial for large-scale natural language processing (NLP) applications, where storage and computational efficiency are key concerns. In this paper, we explore how using binary representations (barcodes) instead of real-valued features can be used for NLP embeddings derived from machine learning models such as BERT. Thresholding is a common method for converting continuous embeddings into binary representations, often using a fixed threshold across all features. We propose a Coordinate Search-based optimization framework that instead identifies the optimal threshold for each feature, demonstrating that feature-specific thresholds lead to improved performance in binary encoding. This ensures that the binary representations are both accurate and efficient, enhancing performance across various features. Our optimal barcode representations have shown promising results in various NLP applications, demonstrating their potential to transform text representation. We conducted extensive experiments and statistical tests on different NLP tasks and datasets to evaluate our approach and compare it to other thresholding methods. Binary embeddings generated using using optimal thresholds found by our method outperform traditional binarization methods in accuracy. This technique for generating binary representations is versatile and can be applied to any features, not just limited to NLP embeddings, making it useful for a wide range of domains in machine learning applications.

</details>


### [30] [CogDual: Enhancing Dual Cognition of LLMs via Reinforcement Learning with Implicit Rule-Based Rewards](https://arxiv.org/abs/2507.17147)

*Cheng Liu, Yifei Lu, Fanghua Ye, Jian Li, Xingyu Chen, Feiliang Ren, Zhaopeng Tu, Xiaolong Li*

**Main category:** cs.CL

**Keywords:** Role-Playing Language Agents, Cognitive Psychology, Reinforcement Learning

**Relevance Score:** 7

**TL;DR:** CogDual is a Role-Playing Language Agent that enhances character consistency and contextual alignment using a cognize-then-respond reasoning paradigm, outperforming existing models in various benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing models for Role-Playing Language Agents often rely on prompt engineering and supervised fine-tuning, neglecting the cognitive mechanisms behind character behaviors.

**Method:** CogDual employs a cognize-then-respond reasoning paradigm, modeling both situational awareness and self-awareness, along with reinforcement learning and general-purpose reward schemes for text generation.

**Key Contributions:**

	1. Introduction of the cognize-then-respond reasoning paradigm.
	2. Joint modeling of situational and self-awareness in response generation.
	3. Use of reinforcement learning with novel reward schemes for improved performance.

**Result:** CogDual outperforms existing baselines on the CoSER benchmark and generalizes well across diverse role-playing tasks.

**Limitations:** 

**Conclusion:** CogDual represents a significant advancement in Role-Playing Language Agents, effectively integrating cognitive psychology principles.

**Abstract:** Role-Playing Language Agents (RPLAs) have emerged as a significant application direction for Large Language Models (LLMs). Existing approaches typically rely on prompt engineering or supervised fine-tuning to enable models to imitate character behaviors in specific scenarios, but often neglect the underlying \emph{cognitive} mechanisms driving these behaviors. Inspired by cognitive psychology, we introduce \textbf{CogDual}, a novel RPLA adopting a \textit{cognize-then-respond } reasoning paradigm. By jointly modeling external situational awareness and internal self-awareness, CogDual generates responses with improved character consistency and contextual alignment. To further optimize the performance, we employ reinforcement learning with two general-purpose reward schemes designed for open-domain text generation. Extensive experiments on the CoSER benchmark, as well as Cross-MR and LifeChoice, demonstrate that CogDual consistently outperforms existing baselines and generalizes effectively across diverse role-playing tasks.

</details>


### [31] [SKA-Bench: A Fine-Grained Benchmark for Evaluating Structured Knowledge Understanding of LLMs](https://arxiv.org/abs/2507.17178)

*Zhiqiang Liu, Enpei Niu, Yin Hua, Mengshu Sun, Lei Liang, Huajun Chen, Wen Zhang*

**Main category:** cs.CL

**Keywords:** Structured Knowledge, Large Language Models, Benchmarking, QA Benchmark, Noise Robustness

**Relevance Score:** 9

**TL;DR:** The paper introduces SKA-Bench, a benchmark for evaluating large language models' understanding of structured knowledge, highlighting their shortcomings in various capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a rigorous evaluation of large language models' understanding of structured knowledge forms, as existing evaluations are limited and not comprehensive.

**Method:** The paper presents SKA-Bench, a three-stage pipeline for creating benchmark instances that test LLM capabilities in areas like noise robustness and information integration.

**Key Contributions:**

	1. Introduction of SKA-Bench as a comprehensive benchmark for structured knowledge understanding
	2. Detailed evaluation of LLM capabilities across multiple structured knowledge formats
	3. Identification of specific challenges faced by existing models in SK understanding

**Result:** Empirical evaluations reveal that current LLMs struggle significantly with structured knowledge understanding and their performance varies based on noise and order of the knowledge units.

**Limitations:** The evaluation is limited to 8 LLMs and may not represent all possible variations and contexts of structured knowledge understanding.

**Conclusion:** The findings underscore the challenges LLMs face in structured knowledge understanding, indicating the necessity for improved models and approaches in this domain.

**Abstract:** Although large language models (LLMs) have made significant progress in understanding Structured Knowledge (SK) like KG and Table, existing evaluations for SK understanding are non-rigorous (i.e., lacking evaluations of specific capabilities) and focus on a single type of SK. Therefore, we aim to propose a more comprehensive and rigorous structured knowledge understanding benchmark to diagnose the shortcomings of LLMs. In this paper, we introduce SKA-Bench, a Structured Knowledge Augmented QA Benchmark that encompasses four widely used structured knowledge forms: KG, Table, KG+Text, and Table+Text. We utilize a three-stage pipeline to construct SKA-Bench instances, which includes a question, an answer, positive knowledge units, and noisy knowledge units. To evaluate the SK understanding capabilities of LLMs in a fine-grained manner, we expand the instances into four fundamental ability testbeds: Noise Robustness, Order Insensitivity, Information Integration, and Negative Rejection. Empirical evaluations on 8 representative LLMs, including the advanced DeepSeek-R1, indicate that existing LLMs still face significant challenges in understanding structured knowledge, and their performance is influenced by factors such as the amount of noise, the order of knowledge units, and hallucination phenomenon. Our dataset and code are available at https://github.com/Lza12a/SKA-Bench.

</details>


### [32] [FinGAIA: An End-to-End Benchmark for Evaluating AI Agents in Finance](https://arxiv.org/abs/2507.17186)

*Lingfeng Zeng, Fangqi Lou, Zixuan Wang, Jiajie Xu, Jinyi Niu, Mengping Li, Yifan Dong, Qi Qi, Wei Zhang, Ziwei Yang, Jun Han, Ruilun Feng, Ruiqi Hu, Lejie Zhang, Zhengbo Feng, Yicheng Ren, Xin Guo, Zhaowei Liu, Dongpo Cheng, Weige Cai, Liwen Zhang*

**Main category:** cs.CL

**Keywords:** AI agents, financial tasks, evaluation benchmark, ChatGPT, error analysis

**Relevance Score:** 2

**TL;DR:** This paper presents FinGAIA, a benchmark for evaluating AI agents in finance, consisting of 407 tasks across various financial sub-domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and promote AI agents' capabilities in automating tasks in the financial sector, where current capabilities remain underexplored.

**Method:** Evaluated 10 mainstream AI agents in a zero-shot setting using a set of 407 financial tasks organized into three levels of scenario depth.

**Key Contributions:**

	1. Introduction of the FinGAIA benchmark for AI in finance
	2. Identification of recurring failure patterns in AI agents
	3. Comprehensive task design spanning multiple financial sub-domains

**Result:** The best-performing agent, ChatGPT, achieved an accuracy of 48.9%, which is significantly lower than financial experts by over 35 percentage points.

**Limitations:** The benchmark is novel but inherently limited to the specific tasks designed and may not cover all financial scenarios.

**Conclusion:** FinGAIA establishes a foundational benchmark for AI agents in finance, highlighting key areas of improvement and directing future research.

**Abstract:** The booming development of AI agents presents unprecedented opportunities for automating complex tasks across various domains. However, their multi-step, multi-tool collaboration capabilities in the financial sector remain underexplored. This paper introduces FinGAIA, an end-to-end benchmark designed to evaluate the practical abilities of AI agents in the financial domain. FinGAIA comprises 407 meticulously crafted tasks, spanning seven major financial sub-domains: securities, funds, banking, insurance, futures, trusts, and asset management. These tasks are organized into three hierarchical levels of scenario depth: basic business analysis, asset decision support, and strategic risk management. We evaluated 10 mainstream AI agents in a zero-shot setting. The best-performing agent, ChatGPT, achieved an overall accuracy of 48.9\%, which, while superior to non-professionals, still lags financial experts by over 35 percentage points. Error analysis has revealed five recurring failure patterns: Cross-modal Alignment Deficiency, Financial Terminological Bias, Operational Process Awareness Barrier, among others. These patterns point to crucial directions for future research. Our work provides the first agent benchmark closely related to the financial domain, aiming to objectively assess and promote the development of agents in this crucial field. Partial data is available at https://github.com/SUFE-AIFLM-Lab/FinGAIA.

</details>


### [33] [The Pluralistic Moral Gap: Understanding Judgment and Value Differences between Humans and Large Language Models](https://arxiv.org/abs/2507.17216)

*Giuseppe Russo, Debora Nozza, Paul Röttger, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** Large Language Models, moral judgments, Dynamic Moral Profiling

**Relevance Score:** 9

**TL;DR:** This paper introduces the Moral Dilemma Dataset, analyzing how well Large Language Models (LLMs) align with human moral judgments, finding significant gaps in alignment and value diversity, and proposes a method to improve this alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how closely LLMs match human moral judgments as people use them for moral advice, revealing potential influence on decision-making.

**Method:** A benchmark dataset of 1,618 moral dilemmas paired with human judgments was created and analyzed, comparing distributions of LLM and human responses, and a novel method called Dynamic Moral Profiling was introduced to enhance alignment.

**Key Contributions:**

	1. Introduction of the Moral Dilemma Dataset
	2. Discovery of a pluralistic moral gap between LLMs and human judgments
	3. Development of Dynamic Moral Profiling to enhance LLM alignment with human values

**Result:** LLMs align with human moral judgments only when there's high consensus and show a sharper decline in alignment with increased human disagreement; LLMs utilize a narrower set of moral values compared to humans.

**Limitations:** The focus is primarily on alignment under specific conditions, and the results may not generalize across all types of moral dilemmas.

**Conclusion:** Dynamic Moral Profiling improves model alignment and value diversity, addressing the identified moral gap.

**Abstract:** People increasingly rely on Large Language Models (LLMs) for moral advice, which may influence humans' decisions. Yet, little is known about how closely LLMs align with human moral judgments. To address this, we introduce the Moral Dilemma Dataset, a benchmark of 1,618 real-world moral dilemmas paired with a distribution of human moral judgments consisting of a binary evaluation and a free-text rationale. We treat this problem as a pluralistic distributional alignment task, comparing the distributions of LLM and human judgments across dilemmas. We find that models reproduce human judgments only under high consensus; alignment deteriorates sharply when human disagreement increases. In parallel, using a 60-value taxonomy built from 3,783 value expressions extracted from rationales, we show that LLMs rely on a narrower set of moral values than humans. These findings reveal a pluralistic moral gap: a mismatch in both the distribution and diversity of values expressed. To close this gap, we introduce Dynamic Moral Profiling (DMP), a Dirichlet-based sampling method that conditions model outputs on human-derived value profiles. DMP improves alignment by 64.3% and enhances value diversity, offering a step toward more pluralistic and human-aligned moral guidance from LLMs.

</details>


### [34] [CLARIFID: Improving Radiology Report Generation by Reinforcing Clinically Accurate Impressions and Enforcing Detailed Findings](https://arxiv.org/abs/2507.17234)

*Kyeongkyu Lee, Seonghwan Yoon, Hongki Lim*

**Main category:** cs.CL

**Keywords:** radiology report generation, diagnostic correctness, multi-view learning

**Relevance Score:** 9

**TL;DR:** CLARIFID is a framework that optimizes diagnostic correctness in radiology report generation by employing a structured approach that mimics expert workflows and utilizes multi-view data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to improve the reliability and factual correctness of automatically generated radiology reports to alleviate radiologists' workload and enhance diagnostic comprehensiveness.

**Method:** CLARIFID learns the logical flow of radiology reports, fine-tunes its model using Proximal Policy Optimization, ensures a reasoning-aware decoding process for coherent clinical reasoning, and integrates multiple chest X-ray views with a vision-transformer-based encoder.

**Key Contributions:**

	1. Direct optimization of diagnostic correctness
	2. Reasoning-aware decoding process
	3. Integration of multiple views through a vision transformer

**Result:** CLARIFID achieves superior clinical efficacy compared to existing methods on standard NLG metrics and clinically relevant scores, demonstrating improved reliability in report generation.

**Limitations:** 

**Conclusion:** The proposed framework not only improves the factual correctness of radiology reports but also enhances their coherence and comprehensiveness, making it a significant advancement in automated medical report generation.

**Abstract:** Automatic generation of radiology reports has the potential to alleviate radiologists' significant workload, yet current methods struggle to deliver clinically reliable conclusions. In particular, most prior approaches focus on producing fluent text without effectively ensuring the factual correctness of the reports and often rely on single-view images, limiting diagnostic comprehensiveness. We propose CLARIFID, a novel framework that directly optimizes diagnostic correctness by mirroring the two-step workflow of experts. Specifically, CLARIFID (1) learns the logical flow from Findings to Impression through section-aware pretraining, (2) is fine-tuned with Proximal Policy Optimization in which the CheXbert F1 score of the Impression section serves as the reward, (3) enforces reasoning-aware decoding that completes "Findings" before synthesizing the "Impression", and (4) fuses multiple chest X-ray views via a vision-transformer-based multi-view encoder. During inference, we apply a reasoning-aware next-token forcing strategy followed by report-level re-ranking, ensuring that the model first produces a comprehensive Findings section before synthesizing the Impression and thereby preserving coherent clinical reasoning. Experimental results on the MIMIC-CXR dataset demonstrate that our method achieves superior clinical efficacy and outperforms existing baselines on both standard NLG metrics and clinically aware scores.

</details>


### [35] [Triple X: A LLM-Based Multilingual Speech Recognition System for the INTERSPEECH2025 MLC-SLM Challenge](https://arxiv.org/abs/2507.17288)

*Miaomiao Gao, Xiaoxiao Xiang, Yiwen Guo*

**Main category:** cs.CL

**Keywords:** speech recognition, multilingual, conversational AI, large language models, machine learning

**Relevance Score:** 7

**TL;DR:** The Triple X speech recognition system optimizes accuracy in multilingual conversational scenarios using an encoder-adapter-LLM architecture, achieving competitive WER performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance speech recognition accuracy in multilingual settings, particularly in conversational scenarios.

**Method:** The approach utilizes an innovative encoder-adapter-LLM architecture along with a multi-stage training strategy leveraging extensive multilingual audio datasets.

**Key Contributions:**

	1. Introduction of a novel encoder-adapter-LLM architecture
	2. Application of a multi-stage training strategy
	3. Demonstrated competitive WER performance in a multi-lingual conversational context

**Result:** Achieved competitive Word Error Rate (WER) results, ranking second in the MLC-SLM Challenge.

**Limitations:** 

**Conclusion:** The proposed system effectively combines LLM reasoning capabilities with domain-specific adaptations to improve multilingual speech recognition.

**Abstract:** This paper describes our Triple X speech recognition system submitted to Task 1 of the Multi-Lingual Conversational Speech Language Modeling (MLC-SLM) Challenge. Our work focuses on optimizing speech recognition accuracy in multilingual conversational scenarios through an innovative encoder-adapter-LLM architecture. This framework harnesses the powerful reasoning capabilities of text-based large language models while incorporating domain-specific adaptations. To further enhance multilingual recognition performance, we adopted a meticulously designed multi-stage training strategy leveraging extensive multilingual audio datasets. Experimental results demonstrate that our approach achieves competitive Word Error Rate (WER) performance on both dev and test sets, obtaining second place in the challenge ranking.

</details>


### [36] [Millions of $\text{GeAR}$-s: Extending GraphRAG to Millions of Documents](https://arxiv.org/abs/2507.17399)

*Zhili Shen, Chenxin Diao, Pascual Merita, Pavlos Vougiouklis, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, graph-based methods, SIGIR 2025 LiveRAG Challenge

**Relevance Score:** 6

**TL;DR:** The paper adapts the GeAR graph-based retrieval-augmented generation method and evaluates its performance on the SIGIR 2025 LiveRAG Challenge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore broader applicability of graph-based approaches in retrieval-augmented generation (RAG) beyond specific tasks.

**Method:** Adaptation of the GeAR method for graph-based retrieval-augmented generation and performance evaluation on a live challenge dataset.

**Key Contributions:**

	1. Adaptation of the GeAR retrieval-augmented generation model for broader datasets.
	2. Evaluation of performance in a live challenge setting.
	3. Identification of limitations in the current application of graph-based RAG methods.

**Result:** The paper presents findings on the performance and limitations of GeAR in the context of the LiveRAG Challenge.

**Limitations:** Limited evidence of general applicability across broader datasets outside specific tasks.

**Conclusion:** The study highlights the performance of GeAR and discusses its limitations, indicating avenues for future research.

**Abstract:** Recent studies have explored graph-based approaches to retrieval-augmented generation, leveraging structured or semi-structured information -- such as entities and their relations extracted from documents -- to enhance retrieval. However, these methods are typically designed to address specific tasks, such as multi-hop question answering and query-focused summarisation, and therefore, there is limited evidence of their general applicability across broader datasets. In this paper, we aim to adapt a state-of-the-art graph-based RAG solution: $\text{GeAR}$ and explore its performance and limitations on the SIGIR 2025 LiveRAG Challenge.

</details>


### [37] [Investigating Subjective Factors of Argument Strength: Storytelling, Emotions, and Hedging](https://arxiv.org/abs/2507.17409)

*Carlotta Quensel, Neele Falk, Gabriella Lapesa*

**Main category:** cs.CL

**Keywords:** argument strength, subjectivity, NLP, emotion, storytelling

**Relevance Score:** 4

**TL;DR:** The paper conducts a regression analysis to explore how subjective factors such as emotions, storytelling, and hedging affect argument strength in NLP, highlighting contrasting impacts on objective and subjective quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in large-scale analyses of subjective features and their relation to argument strength in NLP, emphasizing subjectivity's growing relevance.

**Method:** Regression analysis on two datasets annotated for objective argument quality and subjective persuasion, evaluating automated annotation methods for subjective features.

**Key Contributions:**

	1. Development of annotated datasets with subjective features
	2. Comparative evaluation of automated annotation methods
	3. Novel insights into the effects of subjective features on argument strength

**Result:** The analysis reveals that storytelling and hedging influence argument quality differently, with emotions' effects varying by rhetorical context rather than domain.

**Limitations:** 

**Conclusion:** Understanding the impact of subjective features on arguments can enhance techniques in NLP, particularly in argument mining.

**Abstract:** In assessing argument strength, the notions of what makes a good argument are manifold. With the broader trend towards treating subjectivity as an asset and not a problem in NLP, new dimensions of argument quality are studied. Although studies on individual subjective features like personal stories exist, there is a lack of large-scale analyses of the relation between these features and argument strength. To address this gap, we conduct regression analysis to quantify the impact of subjective factors $-$ emotions, storytelling, and hedging $-$ on two standard datasets annotated for objective argument quality and subjective persuasion. As such, our contribution is twofold: at the level of contributed resources, as there are no datasets annotated with all studied dimensions, this work compares and evaluates automated annotation methods for each subjective feature. At the level of novel insights, our regression analysis uncovers different patterns of impact of subjective features on the two facets of argument strength encoded in the datasets. Our results show that storytelling and hedging have contrasting effects on objective and subjective argument quality, while the influence of emotions depends on their rhetoric utilization rather than the domain.

</details>


### [38] [Each to Their Own: Exploring the Optimal Embedding in RAG](https://arxiv.org/abs/2507.17442)

*Shiting Chen, Zijian Zhao, Jinsong Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Confident RAG, embedding models, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents two approaches, Mixture-Embedding RAG and Confident RAG, to enhance Retrieval-Augmented Generation (RAG) by leveraging multiple embedding models to improve response quality from Large Language Models (LLMs).

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of responses generated by LLMs when using Retrieval-Augmented Generation (RAG) due to varying performance from different embedding models.

**Method:** The paper proposes two methods: Mixture-Embedding RAG, which selects retrievals from multiple embedding models based on standardized similarity, and Confident RAG, which generates multiple responses using different models and selects the highest confidence response.

**Key Contributions:**

	1. Introduction of Mixture-Embedding RAG and Confident RAG methods
	2. Demonstration of average improvements in response quality over existing methods
	3. Potential for release of practical code to facilitate further research in the area.

**Result:** Confident RAG shows average improvements of approximately 10% over vanilla LLMs and 5% over RAG, indicating a significant enhancement in response quality.

**Limitations:** 

**Conclusion:** Confident RAG serves as an efficient plug-and-play solution for improving responses in various domains using LLMs.

**Abstract:** Recently, as Large Language Models (LLMs) have fundamentally impacted various fields, the methods for incorporating up-to-date information into LLMs or adding external knowledge to construct domain-specific models have garnered wide attention. Retrieval-Augmented Generation (RAG), serving as an inference-time scaling method, is notable for its low cost and minimal effort for parameter tuning. However, due to heterogeneous training data and model architecture, the variant embedding models used in RAG exhibit different benefits across various areas, often leading to different similarity calculation results and, consequently, varying response quality from LLMs. To address this problem, we propose and examine two approaches to enhance RAG by combining the benefits of multiple embedding models, named Mixture-Embedding RAG and Confident RAG. Mixture-Embedding RAG simply sorts and selects retrievals from multiple embedding models based on standardized similarity; however, it does not outperform vanilla RAG. In contrast, Confident RAG generates responses multiple times using different embedding models and then selects the responses with the highest confidence level, demonstrating average improvements of approximately 10% and 5% over vanilla LLMs and RAG, respectively. The consistent results across different LLMs and embedding models indicate that Confident RAG is an efficient plug-and-play approach for various domains. We will release our code upon publication.

</details>


### [39] [MultiNRC: A Challenging and Native Multilingual Reasoning Evaluation Benchmark for LLMs](https://arxiv.org/abs/2507.17476)

*Alexander R. Fabbri, Diego Mares, Jorge Flores, Meher Mankikar, Ernesto Hernandez, Dean Lee, Bing Liu, Chen Xing*

**Main category:** cs.CL

**Keywords:** multilingual reasoning, large language models, cultural context, benchmark evaluation, native reasoning

**Relevance Score:** 8

**TL;DR:** The paper introduces the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark assessing LLMs on culturally grounded reasoning questions in multiple languages and evaluates the performance of leading LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing multilingual reasoning benchmarks that are biased towards English language and culture by creating a benchmark for assessing reasoning capabilities in native languages.

**Method:** MultiNRC was developed to include over 1,000 reasoning questions across four categories: linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and culturally relevant math reasoning. It evaluates 14 leading LLMs and compares their performances on the original multilingual questions and their English equivalents.

**Key Contributions:**

	1. Introduction of a new multilingual benchmark (MultiNRC) focused on native reasoning questions
	2. Systematic evaluation of multiple LLMs across various reasoning tasks
	3. Insights into LLMs' strengths and weaknesses in handling cultural and linguistic reasoning

**Result:** Current LLMs showed poor performance on MultiNRC, with none scoring above 50%. There were notable differences in strengths and weaknesses across linguistic, cultural, and logical reasoning tasks, with better performance in math reasoning in English compared to original languages (an increase of 10%).

**Limitations:** The benchmark currently includes only three languages (French, Spanish, and Chinese), which may limit its generalizability.

**Conclusion:** The study highlights persistent challenges for LLMs in native multilingual reasoning and illustrates the necessity for culturally grounded evaluation methods.

**Abstract:** Although recent Large Language Models (LLMs) have shown rapid improvement on reasoning benchmarks in English, the evaluation of such LLMs' multilingual reasoning capability across diverse languages and cultural contexts remains limited. Existing multilingual reasoning benchmarks are typically constructed by translating existing English reasoning benchmarks, biasing these benchmarks towards reasoning problems with context in English language/cultures. In this work, we introduce the Multilingual Native Reasoning Challenge (MultiNRC), a benchmark designed to assess LLMs on more than 1,000 native, linguistic and culturally grounded reasoning questions written by native speakers in French, Spanish, and Chinese. MultiNRC covers four core reasoning categories: language-specific linguistic reasoning, wordplay & riddles, cultural/tradition reasoning, and math reasoning with cultural relevance. For cultural/tradition reasoning and math reasoning with cultural relevance, we also provide English equivalent translations of the multilingual questions by manual translation from native speakers fluent in English. This set of English equivalents can provide a direct comparison of LLM reasoning capacity in other languages vs. English on the same reasoning questions. We systematically evaluate current 14 leading LLMs covering most LLM families on MultiNRC and its English equivalent set. The results show that (1) current LLMs are still not good at native multilingual reasoning, with none scoring above 50% on MultiNRC; (2) LLMs exhibit distinct strengths and weaknesses in handling linguistic, cultural, and logical reasoning tasks; (3) Most models perform substantially better in math reasoning in English compared to in original languages (+10%), indicating persistent challenges with culturally grounded knowledge.

</details>


### [40] [Seed LiveInterpret 2.0: End-to-end Simultaneous Speech-to-speech Translation with Your Voice](https://arxiv.org/abs/2507.17527)

*Shanbo Cheng, Yu Bao, Zhichao Huang, Yu Lu, Ningxin Peng, Lu Xu, Runsheng Yu, Rong Cao, Ting Han, Zeyang Li, Sitong Liu, Shengtao Ma, Shiguang Pan, Jiongchen Xiao, Nuo Xu, Meng Yang, Rong Ye, Yiming Yu, Ruofei Zhang, Wanyi Zhang, Wenhao Zhu, Liehao Zou, Lu Lu, Yuxuan Wang, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** Simultaneous Interpretation, Speech Generation, Voice Cloning, Translation Quality, Latency Reduction

**Relevance Score:** 4

**TL;DR:** Introduction of Seed-LiveInterpret 2.0, an advanced simultaneous interpretation model that improves speech-to-speech generation quality and reduces latency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address longstanding challenges in the translation industry related to simultaneous interpretation, including transcription quality, real-time speech generation, and translation latency.

**Method:** The model employs a duplex speech-to-speech understanding-generating framework, utilizing large-scale pretraining and reinforcement learning techniques.

**Key Contributions:**

	1. Introduction of a novel duplex framework for speech-to-speech interpretation
	2. Significant improvement in translation accuracy and latency
	3. Demonstrated effectiveness validated by human interpreters

**Result:** Seed-LiveInterpret 2.0 achieves over 70% correctness in complex scenarios, significantly improving translation quality and reducing latency from nearly 10 seconds to 3 seconds.

**Limitations:** 

**Conclusion:** The model is demonstrated to outperform existing commercial solutions, enhancing usability in practical applications.

**Abstract:** Simultaneous Interpretation (SI) represents one of the most daunting frontiers in the translation industry, with product-level automatic systems long plagued by intractable challenges: subpar transcription and translation quality, lack of real-time speech generation, multi-speaker confusion, and translated speech inflation, especially in long-form discourses. In this study, we introduce Seed-LiveInterpret 2.0, an end-to-end SI model that delivers high-fidelity, ultra-low-latency speech-to-speech generation with voice cloning capabilities. As a fully operational product-level solution, Seed-LiveInterpret 2.0 tackles these challenges head-on through our novel duplex speech-to-speech understanding-generating framework. Experimental results demonstrate that through large-scale pretraining and reinforcement learning, the model achieves a significantly better balance between translation accuracy and latency, validated by human interpreters to exceed 70% correctness in complex scenarios. Notably, Seed-LiveInterpret 2.0 outperforms commercial SI solutions by significant margins in translation quality, while slashing the average latency of cloned speech from nearly 10 seconds to a near-real-time 3 seconds, which is around a near 70% reduction that drastically enhances practical usability.

</details>


### [41] [Synthetic Voice Data for Automatic Speech Recognition in African Languages](https://arxiv.org/abs/2507.17578)

*Brian DeRenzi, Anna Dixon, Mohamed Aymane Farhi, Christian Resch*

**Main category:** cs.CL

**Keywords:** synthetic voice corpora, Automatic Speech Recognition, African languages, text-to-speech, language models

**Relevance Score:** 7

**TL;DR:** This paper presents a systematic assessment of large-scale synthetic voice corpora for African Automatic Speech Recognition (ASR), focusing on language processing using LLMs and TTS. It provides methods, findings, and implications for low-resource languages.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of speech technology for over 2300 languages in Africa by creating synthetic voice data to improve ASR capabilities.

**Method:** A three-step process involving LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning was used. ASR improvements were evaluated on languages such as Hausa, Dholuo, and Chichewa.

**Key Contributions:**

	1. Creation of synthetic voice corpora for African languages
	2. Demonstration of ASR performance improvement using a combination of synthetic and real data
	3. Public release of data and models for further research

**Result:** Synthetic text for eight out of ten languages showed high readability scores. ASR performance improved significantly with the use of synthetic and real data combinations, achieving competitive results between model configurations.

**Limitations:** The need for more robust reviewer protocols and accurate evaluation datasets to ensure reliable ASR performance assessments.

**Conclusion:** The study highlights the potential of synthetic data for enhancing ASR in low-resource African languages and emphasizes the need for improved evaluation protocols and datasets.

**Abstract:** Speech technology remains out of reach for most of the over 2300 languages in Africa. We present the first systematic assessment of large-scale synthetic voice corpora for African ASR. We apply a three-step process: LLM-driven text creation, TTS voice synthesis, and ASR fine-tuning. Eight out of ten languages for which we create synthetic text achieved readability scores above 5 out of 7. We evaluated ASR improvement for three (Hausa, Dholuo, Chichewa) and created more than 2,500 hours of synthetic voice data at below 1% of the cost of real data. Fine-tuned Wav2Vec-BERT-2.0 models trained on 250h real and 250h synthetic Hausa matched a 500h real-data-only baseline, while 579h real and 450h to 993h synthetic data created the best performance. We also present gender-disaggregated ASR performance evaluation. For very low-resource languages, gains varied: Chichewa WER improved about 6.5% relative with a 1:2 real-to-synthetic ratio; a 1:1 ratio for Dholuo showed similar improvements on some evaluation data, but not on others. Investigating intercoder reliability, ASR errors and evaluation datasets revealed the need for more robust reviewer protocols and more accurate evaluation data. All data and models are publicly released to invite further work to improve synthetic data for African languages.

</details>


### [42] [A Hybrid Early-Exit Algorithm for Large Language Models Based on Space Alignment Decoding (SPADE)](https://arxiv.org/abs/2507.17618)

*Bowen Zheng, Ming Ma, Zhongqiao Lin, Tianming Yang*

**Main category:** cs.CL

**Keywords:** large language models, early-exit algorithms, decoding methods

**Relevance Score:** 8

**TL;DR:** This paper introduces SPADE, a novel decoding method that aligns intermediate layer outputs with the output layer in large language models to improve inference efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models are expensive to run, and early-exit algorithms can lower costs but often degrade performance.

**Method:** The proposed SPADE method aligns intermediate layer representations with the output layer by using a minimally reduced sequence. Additionally, a hybrid early-exit algorithm is designed to evaluate confidence levels for more efficient real-time inference.

**Key Contributions:**

	1. Introduction of SPADE for alignment of intermediate and output layer representations
	2. Development of a hybrid early-exit algorithm to optimize inference costs
	3. Utilization of confidence metrics based on entropy for decision making

**Result:** SPADE significantly reduces inference costs while maintaining high output accuracy compared to traditional methods.

**Limitations:** 

**Conclusion:** This approach provides a scalable solution for deploying large language models in practical applications without sacrificing performance.

**Abstract:** Large language models are computationally expensive due to their deep structures. Prior research has shown that intermediate layers contain sufficient information to generate accurate answers, leading to the development of early-exit algorithms that reduce inference costs by terminating computation at earlier layers. However, these methods often suffer from poor performance due to misalignment between intermediate and output layer representations that lead to decoding inaccuracy. To address these challenges, we propose SPADE (SPace Alignment DEcoding), a novel decoding method that aligns intermediate layer representations with the output layer by propagating a minimally reduced sequence consisting of only the start token and the answer token. We further optimize the early-exit decision-making process by training a linear approximation of SPADE that computes entropy-based confidence metrics. Putting them together, we create a hybrid early-exit algorithm that monitors confidence levels and stops inference at intermediate layers while using SPADE to generate high-quality outputs. This approach significantly reduces inference costs without compromising accuracy, offering a scalable and efficient solution for deploying large language models in real-world applications.

</details>


### [43] [WSM: Decay-Free Learning Rate Schedule via Checkpoint Merging for LLM Pre-training](https://arxiv.org/abs/2507.17634)

*Changxin Tian, Jiapeng Wang, Qian Zhao, Kunlong Chen, Jia Liu, Ziqi Liu, Jiaxin Mao, Wayne Xin Zhao, Zhiqiang Zhang, Jun Zhou*

**Main category:** cs.CL

**Keywords:** learning rate scheduling, model merging, machine learning, model performance

**Relevance Score:** 5

**TL;DR:** The paper introduces WSM, a framework connecting learning rate scheduling and model merging for improved model performance without traditional decay.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance learning rate scheduling by eliminating decay phases while maintaining competitive model performance.

**Method:** A unifying theoretical framework called Warmup-Stable and Merge (WSM) that connects decay strategies to model averaging schemes is developed.

**Key Contributions:**

	1. Introduction of the WSM framework that links learning rate scheduling with model merging.
	2. Identification of merge duration as a critical factor influencing model performance.
	3. Demonstration of superior performance of WSM compared to existing methods across various benchmarks.

**Result:** WSM shows significant improvements in multiple benchmarks, outperforming the traditional WSD approach by +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro.

**Limitations:** 

**Conclusion:** The study indicates that merge duration is crucial for model performance and suggests WSM as a promising method for long-term model refinement.

**Abstract:** Recent advances in learning rate (LR) scheduling have demonstrated the effectiveness of decay-free approaches that eliminate the traditional decay phase while maintaining competitive performance. Model merging techniques have emerged as particularly promising solutions in this domain. We present Warmup-Stable and Merge (WSM), a general framework that establishes a formal connection between learning rate decay and model merging. WSM provides a unified theoretical foundation for emulating various decay strategies-including cosine decay, linear decay and inverse square root decay-as principled model averaging schemes, while remaining fully compatible with diverse optimization methods. Through extensive experiments, we identify merge duration-the training window for checkpoint aggregation-as the most critical factor influencing model performance, surpassing the importance of both checkpoint interval and merge quantity. Our framework consistently outperforms the widely-adopted Warmup-Stable-Decay (WSD) approach across multiple benchmarks, achieving significant improvements of +3.5% on MATH, +2.9% on HumanEval, and +5.5% on MMLU-Pro. The performance advantages extend to supervised fine-tuning scenarios, highlighting WSM's potential for long-term model refinement.

</details>


### [44] [Who Attacks, and Why? Using LLMs to Identify Negative Campaigning in 18M Tweets across 19 Countries](https://arxiv.org/abs/2507.17636)

*Victor Hartman, Petter Törnberg*

**Main category:** cs.CL

**Keywords:** negative campaigning, zero-shot learning, cross-lingual, Large Language Models, political communication

**Relevance Score:** 4

**TL;DR:** This study uses zero-shot Large Language Models (LLMs) for cross-lingual classification of negative campaigning in political communication, analyzing 18 million tweets across 19 European countries.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high cost and limited scalability of existing classification methods in political competition research, particularly in the context of negative campaigning.

**Method:** The study employs zero-shot Large Language Models (LLMs) for classifying negative campaigning across multiple languages using benchmark datasets.

**Key Contributions:**

	1. Introduction of zero-shot LLMs for cross-lingual classification of negative campaigning.
	2. Conducting the largest cross-national study of negative campaigning with 18 million tweets.
	3. Identification of consistent patterns in negative messaging across different political parties in Europe.

**Result:** LLMs achieved performance comparable to that of native-speaking human coders and outperformed traditional supervised machine learning methods; it also identified that governing parties engage less in negative messaging compared to radical right parties.

**Limitations:** The study primarily focuses on tweets from parliamentarians and may not encompass broader public sentiment.

**Conclusion:** The research highlights the ability of LLMs to facilitate scalable and replicable studies in political communication across different languages and cultures.

**Abstract:** Negative campaigning is a central feature of political competition, yet empirical research has been limited by the high cost and limited scalability of existing classification methods. This study makes two key contributions. First, it introduces zero-shot Large Language Models (LLMs) as a novel approach for cross-lingual classification of negative campaigning. Using benchmark datasets in ten languages, we demonstrate that LLMs achieve performance on par with native-speaking human coders and outperform conventional supervised machine learning approaches. Second, we leverage this novel method to conduct the largest cross-national study of negative campaigning to date, analyzing 18 million tweets posted by parliamentarians in 19 European countries between 2017 and 2022. The results reveal consistent cross-national patterns: governing parties are less likely to use negative messaging, while ideologically extreme and populist parties -- particularly those on the radical right -- engage in significantly higher levels of negativity. These findings advance our understanding of how party-level characteristics shape strategic communication in multiparty systems. More broadly, the study demonstrates the potential of LLMs to enable scalable, transparent, and replicable research in political communication across linguistic and cultural contexts.

</details>


### [45] [Towards Greater Leverage: Scaling Laws for Efficient Mixture-of-Experts Language Models](https://arxiv.org/abs/2507.17702)

*Changxin Tian, Kunlong Chen, Jia Liu, Ziqi Liu, Zhiqiang Zhang, Jun Zhou*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Efficiency Leverage, Large Language Models, scaling laws, computational efficiency

**Relevance Score:** 8

**TL;DR:** The paper introduces Efficiency Leverage (EL) as a metric for evaluating the computational advantage of Mixture-of-Experts (MoE) architectures, revealing relationships between MoE configurations and EL through empirical research on models with up to 28B parameters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To find a reliable metric for predicting the model capacity of various MoE configurations, addressing a critical gap in understanding how architectural choices impact efficiency.

**Method:** An empirical study was conducted involving the training of over 300 models with varying parameters, focusing on the expert activation ratio, total compute budget, and expert granularity to derive a unified scaling law for EL.

**Key Contributions:**

	1. Introduction of Efficiency Leverage as a new metric for MoE efficiency
	2. Empirical analysis involving 300+ models leading to predictable scaling laws
	3. Development of a pilot model demonstrating significant resource efficiency.

**Result:** The study finds that EL correlates with expert activation ratio and total compute budget, typically following predictable power laws, while expert granularity influences EL non-linearly. A pilot model, Ling-mini-beta, demonstrated significant resource savings while matching performance with a larger dense model.

**Limitations:** 

**Conclusion:** The findings provide a solid, empirically-backed foundation for efficiently scaling MoE models, offering insights into architectural optimization.

**Abstract:** Mixture-of-Experts (MoE) has become a dominant architecture for scaling Large Language Models (LLMs) efficiently by decoupling total parameters from computational cost. However, this decoupling creates a critical challenge: predicting the model capacity of a given MoE configurations (e.g., expert activation ratio and granularity) remains an unresolved problem. To address this gap, we introduce Efficiency Leverage (EL), a metric quantifying the computational advantage of an MoE model over a dense equivalent. We conduct a large-scale empirical study, training over 300 models up to 28B parameters, to systematically investigate the relationship between MoE architectural configurations and EL. Our findings reveal that EL is primarily driven by the expert activation ratio and the total compute budget, both following predictable power laws, while expert granularity acts as a non-linear modulator with a clear optimal range. We integrate these discoveries into a unified scaling law that accurately predicts the EL of an MoE architecture based on its configuration. To validate our derived scaling laws, we designed and trained Ling-mini-beta, a pilot model for Ling-2.0 series with only 0.85B active parameters, alongside a 6.1B dense model for comparison. When trained on an identical 1T high-quality token dataset, Ling-mini-beta matched the performance of the 6.1B dense model while consuming over 7x fewer computational resources, thereby confirming the accuracy of our scaling laws. This work provides a principled and empirically-grounded foundation for the scaling of efficient MoE models.

</details>


### [46] [TyDi QA-WANA: A Benchmark for Information-Seeking Question Answering in Languages of West Asia and North Africa](https://arxiv.org/abs/2507.17709)

*Parker Riley, Siamak Shakeri, Waleed Ammar, Jonathan H. Clark*

**Main category:** cs.CL

**Keywords:** question-answering, dataset, language varieties, cultural relevance, natural language processing

**Relevance Score:** 6

**TL;DR:** TyDi QA-WANA is a question-answering dataset with 28K examples across 10 language varieties from western Asia and northern Africa, designed for evaluating models' abilities to use large text contexts in answering questions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to create a dataset that allows for culturally relevant question-answering tasks in multiple language varieties without translation biases.

**Method:** Data was collected by eliciting information-seeking questions directly in each language variety and pairing them with relevant articles.

**Key Contributions:**

	1. Creation of a large-scale, culturally relevant question-answering dataset.
	2. Evaluation of baseline models that utilize large text contexts for question answering.
	3. Availability of code and data for community use.

**Result:** Two baseline models were tested on the dataset, establishing benchmarks for future research.

**Limitations:** 

**Conclusion:** The dataset is released to enable the research community to improve question-answering systems using large contexts tailored to specific language and cultural features.

**Abstract:** We present TyDi QA-WANA, a question-answering dataset consisting of 28K examples divided among 10 language varieties of western Asia and northern Africa. The data collection process was designed to elicit information-seeking questions, where the asker is genuinely curious to know the answer. Each question in paired with an entire article that may or may not contain the answer; the relatively large size of the articles results in a task suitable for evaluating models' abilities to utilize large text contexts in answering questions. Furthermore, the data was collected directly in each language variety, without the use of translation, in order to avoid issues of cultural relevance. We present performance of two baseline models, and release our code and data to facilitate further improvement by the research community.

</details>


### [47] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)

*Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt*

**Main category:** cs.CL

**Keywords:** AI telephone surveying, voice AI, quantitative research, survey methodology, respondent experience

**Relevance Score:** 7

**TL;DR:** The paper presents an AI telephone surveying system that uses voice-enabled AI technologies to conduct quantitative surveys, enhancing respondent experience and methodological rigor.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of voice-enabled AI systems provides a new mode for quantitative survey research, allowing for increased scale and interactivity in data collection.

**Method:** An AI system built using large language models and speech technologies was tested in two pilot surveys among the SSRS Opinion Panel, following strict research methodologies.

**Key Contributions:**

	1. Development of a novel AI telephone surveying system utilizing LLM and ASR technologies
	2. Validation of effectiveness through pilot surveys and comparison with human-administered surveys
	3. Identification of factors influencing survey completion and satisfaction rates.

**Result:** The system demonstrated improved survey completion rates, lower break-off rates, and higher respondent satisfaction, particularly with shorter instruments and adaptive AI interviewers.

**Limitations:** The study is limited to the specific context of the SSRS Opinion Panel and may require further testing across diverse populations.

**Conclusion:** AI-driven telephone surveying can effectively enhance the quality and efficiency of quantitative research by providing a more natural interaction for respondents.

**Abstract:** With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.   We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.   To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.

</details>


### [48] [From Feedback to Checklists: Grounded Evaluation of AI-Generated Clinical Notes](https://arxiv.org/abs/2507.17717)

*Karen Zhou, John Giorgi, Pranav Mani, Peng Xu, Davis Liang, Chenhao Tan*

**Main category:** cs.CL

**Keywords:** AI-generated notes, clinical evaluation, human feedback, checklist, healthcare

**Relevance Score:** 9

**TL;DR:** This paper proposes a systematic pipeline to create checklists for evaluating AI-generated clinical notes based on real user feedback, outperforming baseline methods in various measures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating the quality of AI-generated clinical notes is challenging due to subjectivity and limitations of existing automated metrics.

**Method:** The authors developed a pipeline that distills user feedback into structured evaluation checklists that are interpretable and grounded in human preferences, then evaluated its performance against existing methods.

**Key Contributions:**

	1. Development of a systematic pipeline for checklist creation from real user feedback
	2. Demonstration of enhanced evaluation effectiveness over baseline methods
	3. Practical application in identifying low-quality clinical notes

**Result:** The checklist derived from feedback showed superior coverage, diversity, and predictive power for human ratings compared to baseline approaches.

**Limitations:** 

**Conclusion:** The feedback-derived checklist is robust and aligns closely with clinician preferences, providing practical value in identifying clinical notes that do not meet quality standards.

**Abstract:** AI-generated clinical notes are increasingly used in healthcare, but evaluating their quality remains a challenge due to high subjectivity and limited scalability of expert review. Existing automated metrics often fail to align with real-world physician preferences. To address this, we propose a pipeline that systematically distills real user feedback into structured checklists for note evaluation. These checklists are designed to be interpretable, grounded in human feedback, and enforceable by LLM-based evaluators. Using deidentified data from over 21,000 clinical encounters, prepared in accordance with the HIPAA safe harbor standard, from a deployed AI medical scribe system, we show that our feedback-derived checklist outperforms baseline approaches in our offline evaluations in coverage, diversity, and predictive power for human ratings. Extensive experiments confirm the checklist's robustness to quality-degrading perturbations, significant alignment with clinician preferences, and practical value as an evaluation methodology. In offline research settings, the checklist can help identify notes likely to fall below our chosen quality thresholds.

</details>


### [49] [AI Telephone Surveying: Automating Quantitative Data Collection with an AI Interviewer](https://arxiv.org/abs/2507.17718)

*Danny D. Leybzon, Shreyas Tirumala, Nishant Jain, Summer Gillen, Michael Jackson, Cameron McPhee, Jennifer Schmidt*

**Main category:** cs.CL

**Keywords:** AI telephone surveying, voice AI, quantitative research

**Relevance Score:** 7

**TL;DR:** This paper presents an AI-powered system for conducting quantitative surveys via telephone, focusing on enhancing respondent experience through voice AI.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of voice-enabled AI systems enables a new methodology for conducting quantitative surveys, aiming for a blend of interactivity and methodological rigor.

**Method:** An AI system utilizing large language models (LLM), automatic speech recognition (ASR), and speech synthesis was built to conduct phone interviews, adhering to research best practices in quantitative studies.

**Key Contributions:**

	1. Development of an AI system for telephone surveying using LLM, ASR, and speech synthesis.
	2. Validation of the system through pilot surveys that compare AI-administered and human-administered surveys.
	3. Identification of key metrics for survey effectiveness such as completion rates and respondent satisfaction.

**Result:** Pilot surveys showed that using responsive AI interviewers and shorter survey instruments can significantly improve completion rates, reduce break-off rates, and increase respondent satisfaction.

**Limitations:** 

**Conclusion:** The findings indicate the potential of voice AI in enhancing quantitative survey research, suggesting a shift from traditional methods to AI-based approaches.

**Abstract:** With the rise of voice-enabled artificial intelligence (AI) systems, quantitative survey researchers have access to a new data-collection mode: AI telephone surveying. By using AI to conduct phone interviews, researchers can scale quantitative studies while balancing the dual goals of human-like interactivity and methodological rigor. Unlike earlier efforts that used interactive voice response (IVR) technology to automate these surveys, voice AI enables a more natural and adaptive respondent experience as it is more robust to interruptions, corrections, and other idiosyncrasies of human speech.   We built and tested an AI system to conduct quantitative surveys based on large language models (LLM), automatic speech recognition (ASR), and speech synthesis technologies. The system was specifically designed for quantitative research, and strictly adhered to research best practices like question order randomization, answer order randomization, and exact wording.   To validate the system's effectiveness, we deployed it to conduct two pilot surveys with the SSRS Opinion Panel and followed-up with a separate human-administered survey to assess respondent experiences. We measured three key metrics: the survey completion rates, break-off rates, and respondent satisfaction scores. Our results suggest that shorter instruments and more responsive AI interviewers may contribute to improvements across all three metrics studied.

</details>


### [50] [Megrez2 Technical Report](https://arxiv.org/abs/2507.17728)

*Boxun Li, Yadong Li, Zhiyuan Li, Congyi Liu, Weilin Liu, Guowei Niu, Zheyue Tan, Haiyang Xu, Zhuyu Yao, Tao Yuan, Dong Zhou, Yueqing Zhuang, Bo Zhao, Guohao Dai, Yu Wang*

**Main category:** cs.CL

**Keywords:** Megrez2, language model, expert sharing, device deployment, efficient inference

**Relevance Score:** 8

**TL;DR:** Megrez2 is a lightweight, high-performance language model optimized for device-native deployment, featuring cross-layer expert sharing and pre-gated routing for enhanced efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create an efficient language model architecture that can be deployed on resource-constrained devices without sacrificing quality.

**Method:** Introduces a cross-layer expert sharing mechanism to reduce parameter count and employs pre-gated routing for memory-efficient expert loading.

**Key Contributions:**

	1. Introduction of cross-layer expert sharing mechanism
	2. Pre-gated routing for memory-efficient loading
	3. Demonstration of competitive performance with fewer parameters

**Result:** Megrez2-Preview, with 3B activated and 7.5B stored parameters, outperforms or matches larger models across various language tasks, demonstrating strong accuracy and efficiency.

**Limitations:** 

**Conclusion:** The Megrez2 architecture strikes a balance between performance and resource efficiency, making it suitable for real-world applications.

**Abstract:** We present Megrez2, a novel lightweight and high-performance language model architecture optimized for device native deployment. Megrez2 introduces a novel cross-layer expert sharing mechanism, which significantly reduces total parameter count by reusing expert modules across adjacent transformer layers while maintaining most of the model's capacity. It also incorporates pre-gated routing, enabling memory-efficient expert loading and faster inference. As the first instantiation of the Megrez2 architecture, we introduce the Megrez2-Preview model, which is pre-trained on a 5-trillion-token corpus and further enhanced through supervised fine-tuning and reinforcement learning with verifiable rewards. With only 3B activated and 7.5B stored parameters, Megrez2-Preview demonstrates competitive or superior performance compared to larger models on a wide range of tasks, including language understanding, instruction following, mathematical reasoning, and code generation. These results highlight the effectiveness of the Megrez2 architecture to achieve a balance between accuracy, efficiency, and deployability, making it a strong candidate for real-world, resource-constrained applications.

</details>


### [51] [Pretraining on the Test Set Is No Longer All You Need: A Debate-Driven Approach to QA Benchmarks](https://arxiv.org/abs/2507.17747)

*Linbo Cao, Jinman Zhao*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Machine Learning, Language Models, Evaluation, Benchmarking

**Relevance Score:** 8

**TL;DR:** Proposes a debate-driven evaluation paradigm for QA datasets to reduce data contamination and improve reasoning assessment of language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standard QA benchmarks are saturated, raising issues of data contamination and the costs associated with dataset creation.

**Method:** Transforms QA datasets into structured adversarial debates involving multi-round argumentation between models, adjudicated by a judge model.

**Key Contributions:**

	1. An evaluation pipeline to convert QA tasks into debate-based assessments.
	2. A public benchmark demonstrating effectiveness on MMLU-Pro questions.

**Result:** The approach leads to dramatic improvements in a fine-tuned model's accuracy on traditional tasks, but highlights poorer performance during debates, indicating a stronger emphasis on reasoning.

**Limitations:** Focus on a specific subset of questions may limit generalizability and practical application.

**Conclusion:** This framework offers a sustainable method for assessing reasoning abilities of language models, suggesting that relying solely on pretraining from test data is not sufficient anymore.

**Abstract:** As frontier language models increasingly saturate standard QA benchmarks, concerns about data contamination, memorization, and escalating dataset creation costs persist. We propose a debate-driven evaluation paradigm that transforms any existing QA dataset into structured adversarial debates--where one model is given the official answer to defend, and another constructs and defends an alternative answer--adjudicated by a judge model blind to the correct solution. By forcing multi-round argumentation, this approach substantially increases difficulty while penalizing shallow memorization, yet reuses QA items to reduce curation overhead. We make two main contributions: (1) an evaluation pipeline to systematically convert QA tasks into debate-based assessments, and (2) a public benchmark that demonstrates our paradigm's effectiveness on a subset of MMLU-Pro questions, complete with standardized protocols and reference models. Empirical results validate the robustness of the method and its effectiveness against data contamination--a Llama 3.1 model fine-tuned on test questions showed dramatic accuracy improvements (50% -> 82%) but performed worse in debates. Results also show that even weaker judges can reliably differentiate stronger debaters, highlighting how debate-based evaluation can scale to future, more capable systems while maintaining a fraction of the cost of creating new benchmarks. Overall, our framework underscores that "pretraining on the test set is no longer all you need," offering a sustainable path for measuring the genuine reasoning ability of advanced language models.

</details>


### [52] [Multi-Level Explanations for Generative Language Models](https://arxiv.org/abs/2403.14459)

*Lucas Monteiro Paes, Dennis Wei, Hyo Jin Do, Hendrik Strobelt, Ronny Luss, Amit Dhurandhar, Manish Nagireddy, Karthikeyan Natesan Ramamurthy, Prasanna Sattigeri, Werner Geyer, Soumya Ghosh*

**Main category:** cs.CL

**Keywords:** Large Language Models, Interpretability, Attribution Methods

**Relevance Score:** 9

**TL;DR:** The paper presents Multi-Level Explanations for Generative Language Models (MExGen), a technique designed to provide explanations for context-grounded text generation in LLMs, improving understanding of their responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for better understanding of how LLMs generate responses, particularly in context-grounded tasks like summarization and question-answering, where current methods fall short.

**Method:** MExGen assigns scores to parts of the context to quantify their influence on the model's output, extending existing attribution methods (LIME, SHAP) to address challenges related to high inference costs and long inputs.

**Key Contributions:**

	1. Introduction of MExGen for explaining LLM outputs in context-aware tasks.
	2. Enhanced attribution methodology for LLMs beyond existing methods like LIME and SHAP.
	3. Comprehensive evaluation demonstrating improved explanation fidelity.

**Result:** MExGen provides more faithful explanations of generated outputs compared to alternatives, including self-explanations from LLMs, as demonstrated through systematic evaluations.

**Limitations:** 

**Conclusion:** The framework improves the interpretability of LLMs in context-grounded tasks and the open-sourced code enables further exploration and application.

**Abstract:** Despite the increasing use of large language models (LLMs) for context-grounded tasks like summarization and question-answering, understanding what makes an LLM produce a certain response is challenging. We propose Multi-Level Explanations for Generative Language Models (MExGen), a technique to provide explanations for context-grounded text generation. MExGen assigns scores to parts of the context to quantify their influence on the model's output. It extends attribution methods like LIME and SHAP to LLMs used in context-grounded tasks where (1) inference cost is high, (2) input text is long, and (3) the output is text. We conduct a systematic evaluation, both automated and human, of perturbation-based attribution methods for summarization and question answering. The results show that our framework can provide more faithful explanations of generated output than available alternatives, including LLM self-explanations. We open-source code for MExGen as part of the ICX360 toolkit: https://github$.$com/IBM/ICX360.

</details>


### [53] [Impact of Stickers on Multimodal Sentiment and Intent in Social Media: A New Task, Dataset and Baseline](https://arxiv.org/abs/2405.08427)

*Yuanchen Shi, Biao Ma, Longyin Zhang, Fang Kong*

**Main category:** cs.CL

**Keywords:** multimodal, sentiment analysis, intent recognition, stickers, social media

**Relevance Score:** 7

**TL;DR:** This paper introduces MSAIRS, a new framework for multimodal sentiment analysis and intent recognition using stickers in chat messages, along with a novel dataset and an effective multimodal model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The use of stickers in social media for expressing sentiment and intent is significant, yet underexplored in research.

**Method:** We propose the MSAIRS task and introduce a multimodal dataset of Chinese chat records and stickers. We develop a joint model, MMSAIR, with differential vector construction and cascaded attention for better sentiment and intent recognition.

**Key Contributions:**

	1. Proposal of the MSAIRS task providing a new research direction.
	2. Introduction of a novel multimodal dataset for sentiment analysis and intent recognition.
	3. Development of an effective multimodal model, MMSAIR, enhancing analysis accuracy.

**Result:** MMSAIR outperforms traditional methods and advanced MLLMs in sentiment and intent recognition accuracy, highlighting the unique challenges of sticker interpretation.

**Limitations:** Focuses only on Chinese chat records and stickers, limiting generalizability to other languages or contexts.

**Conclusion:** Jointly modeling sentiment and intent improves recognition accuracy, and our dataset and model contribute to better understanding the impact of stickers.

**Abstract:** Stickers are increasingly used in social media to express sentiment and intent. Despite their significant impact on sentiment analysis and intent recognition, little research has been conducted in this area. To address this gap, we propose a new task: \textbf{M}ultimodal chat \textbf{S}entiment \textbf{A}nalysis and \textbf{I}ntent \textbf{R}ecognition involving \textbf{S}tickers (MSAIRS). Additionally, we introduce a novel multimodal dataset containing Chinese chat records and stickers excerpted from several mainstream social media platforms. Our dataset includes paired data with the same text but different stickers, the same sticker but different contexts, and various stickers consisting of the same images with different texts, allowing us to better understand the impact of stickers on chat sentiment and intent. We also propose an effective multimodal joint model, MMSAIR, featuring differential vector construction and cascaded attention mechanisms for enhanced multimodal fusion. Our experiments demonstrate the necessity and effectiveness of jointly modeling sentiment and intent, as they mutually reinforce each other's recognition accuracy. MMSAIR significantly outperforms traditional models and advanced MLLMs, demonstrating the challenge and uniqueness of sticker interpretation in social media. Our dataset and code are available on https://github.com/FakerBoom/MSAIRS-Dataset.

</details>


### [54] [Is text normalization relevant for classifying medieval charters?](https://arxiv.org/abs/2408.16446)

*Florian Atzenhofer-Baumgartner, Tamás Kovács*

**Main category:** cs.CL

**Keywords:** text normalization, classification, medieval charters, machine learning, document analysis

**Relevance Score:** 3

**TL;DR:** This study assesses the impact of historical text normalization on classifying medieval charters, revealing minimal benefits for location tasks and diminished dating accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how historical text normalization affects the classification process of medieval charters, focusing on dating and locating tasks.

**Method:** Evaluation of various classifiers, including traditional and transformer-based models, on a dataset of Middle High German charters, with and without text normalization.

**Key Contributions:**

	1. Assessment of normalization effects on medieval charter classification
	2. Comparison of various classification models including transformer-based
	3. Recommendations for selective normalization in textual analysis

**Result:** Normalization slightly enhances locating tasks but reduces accuracy for dating; support vector machines and gradient boosting outperform other models, challenging the effectiveness of transformers.

**Limitations:** Results are specific to Middle High German charters and may not generalize to other texts.

**Conclusion:** A selective approach to historical text normalization is recommended, as preserving certain textual features is vital for effective classification in document analysis.

**Abstract:** This study examines the impact of historical text normalization on the classification of medieval charters, specifically focusing on document dating and locating. Using a data set of Middle High German charters from a digital archive, we evaluate various classifiers, including traditional and transformer-based models, with and without normalization. Our results indicate that the given normalization minimally improves locating tasks but reduces accuracy for dating, implying that original texts contain crucial features that normalization may obscure. We find that support vector machines and gradient boosting outperform other models, questioning the efficiency of transformers for this use case. Results suggest a selective approach to historical text normalization, emphasizing the significance of preserving some textual characteristics that are critical for classification tasks in document analysis.

</details>


### [55] [Beyond Profile: From Surface-Level Facts to Deep Persona Simulation in LLMs](https://arxiv.org/abs/2502.12988)

*Zixiao Wang, Duzhen Zhang, Ishita Agrawal, Shen Gao, Le Song, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** persona simulation, large language models, character representation, machine learning, natural language processing

**Relevance Score:** 8

**TL;DR:** CharacterBot is a model designed for persona simulation in language models, effectively replicating distinct thought processes and linguistic styles, demonstrated through the case study of writer Lu Xun.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for persona simulation in LLMs fall short by focusing solely on biographical data and limited dialogue, necessitating a more comprehensive understanding of characters.

**Method:** CharacterBot employs a holistic approach that includes a pre-training task for linguistic structure mastery and three fine-tuning tasks focused on question answering and style transfer, complemented by a CharLoRA parameter updating mechanism for optimized learning.

**Key Contributions:**

	1. Introduction of CharacterBot model for deeper persona simulation
	2. Proposal of CharLoRA parameter updating mechanism
	3. Demonstrated effectiveness through case study of Lu Xun's writing style

**Result:** CharacterBot demonstrates significant improvements in linguistic accuracy and opinion comprehension compared to baseline models, evaluated through tailored metrics.

**Limitations:** 

**Conclusion:** The study showcases the potential of CharacterBot in enhancing character persona simulation in language models, encouraging further exploration in this domain.

**Abstract:** Previous approaches to persona simulation large language models (LLMs) have typically relied on learning basic biographical information, or using limited role-play dialogue datasets to capture a character's responses. However, a holistic representation of an individual goes beyond surface-level facts or conversations to deeper thoughts and thinking. In this work, we introduce CharacterBot, a model designed to replicate both the linguistic patterns and distinctive thought processes of a character. Using Lu Xun, a renowned Chinese writer, as a case study, we propose four training tasks derived from his 17 essay collections. These include a pre-training task focused on mastering external linguistic structures and knowledge, as well as three fine-tuning tasks: multiple-choice question answering, generative question answering, and style transfer, each aligning the LLM with Lu Xun's internal ideation and writing style. To optimize learning across these tasks, we introduce a CharLoRA parameter updating mechanism, where a general linguistic style expert collaborates with other task-specific experts to better study both the language style and the understanding of deeper thoughts. We evaluate CharacterBot on three tasks for linguistic accuracy and opinion comprehension, demonstrating that it significantly outperforms the baselines on our adapted metrics. We hope that this work inspires future research on deep character persona simulation LLM.

</details>


### [56] [An Efficient and Precise Training Data Construction Framework for Process-supervised Reward Model in Mathematical Reasoning](https://arxiv.org/abs/2503.02382)

*Wei Sun, Qianlong Du, Fuwei Cui, Jiajun Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical reasoning, process supervision, Epic50k, adaptive binary search

**Relevance Score:** 8

**TL;DR:** This paper presents EpicPRM, a framework that improves the reasoning capabilities of Large Language Models by efficiently annotating intermediate reasoning steps and constructing a high-quality dataset called Epic50k.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance mathematical reasoning in Large Language Models (LLMs) and address inefficiencies and costs associated with existing training data construction methods.

**Method:** EpicPRM introduces an adaptive binary search algorithm for annotating each reasoning step based on its contribution, leading to the creation of a useful training dataset, Epic50k.

**Key Contributions:**

	1. Introduction of the EpicPRM framework for process supervision
	2. Creation of the Epic50k dataset with 50k annotated reasoning steps
	3. Improvement in reasoning performance of LLMs using the new dataset

**Result:** The Epic50k dataset consists of 50k high-quality annotated intermediate reasoning steps, which significantly improves the performance of the PRM compared to other available datasets.

**Limitations:** 

**Conclusion:** The EpicPRM framework not only optimizes the annotation process but also allows for the development of a substantial dataset that boosts the reasoning abilities of LLMs.

**Abstract:** Enhancing the mathematical reasoning capabilities of Large Language Models (LLMs) is of great scientific and practical significance. Researchers typically employ process-supervised reward models (PRMs) to guide the reasoning process, effectively improving the models' reasoning abilities. However, existing methods for constructing process supervision training data, such as manual annotation and per-step Monte Carlo estimation, are often costly or suffer from poor quality. To address these challenges, this paper introduces a framework called EpicPRM, which annotates each intermediate reasoning step based on its quantified contribution and uses an adaptive binary search algorithm to enhance both annotation precision and efficiency. Using this approach, we efficiently construct a high-quality process supervision training dataset named Epic50k, consisting of 50k annotated intermediate steps. Compared to other publicly available datasets, the PRM trained on Epic50k demonstrates significantly superior performance. Getting Epic50k at https://github.com/xiaolizh1/EpicPRM.

</details>


### [57] [Visualising Policy-Reward Interplay to Inform Zeroth-Order Preference Optimisation of Large Language Models](https://arxiv.org/abs/2503.03460)

*Alessio Galatolo, Zhenbang Dai, Katie Winkle, Meriem Beloucif*

**Main category:** cs.CL

**Keywords:** Zeroth-Order optimization, Preference Optimization, Large Language Models, SPSA, generative tasks

**Relevance Score:** 8

**TL;DR:** This paper introduces ZOPrO, a novel zeroth-order optimization algorithm for Preference Optimization in Large Language Models (LLMs), which enhances convergence times and reward signals in generative tasks such as summarization and machine translation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational intensity of fine-tuning LLMs with first-order methods and to explore the application of Zeroth-Order optimization in complex generative tasks beyond classification.

**Method:** The authors adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to improve convergence in Preference Optimization for LLMs.

**Key Contributions:**

	1. Introduction of ZOPrO for Preference Optimization in LLMs
	2. Application of SPSA with targeted sampling to accelerate convergence
	3. Demonstrated applicability to generative tasks beyond classification

**Result:** Experiments show that ZOPrO enhances reward signals and achieves comparable convergence times to first-order methods in tasks like summarization, machine translation, and conversational assistants.

**Limitations:** While ZOPrO achieves significant improvements, it does not outperform all state-of-the-art methods in certain scenarios.

**Conclusion:** ZOPrO is the first application of Zeroth-Order methods to Preference Optimization in LLMs, offering insights and techniques that pave the way for future research in this largely unexplored area.

**Abstract:** Fine-tuning Large Language Models (LLMs) with first-order methods like back-propagation is computationally intensive. Zeroth-Order (ZO) optimisation uses function evaluations instead of gradients, reducing memory usage, but suffers from slow convergence in high-dimensional models. As a result, ZO research in LLMs has mostly focused on classification, overlooking more complex generative tasks. In this paper, we introduce ZOPrO, a novel ZO algorithm designed for Preference Optimisation in LLMs. We begin by analysing the interplay between policy and reward models during traditional (first-order) Preference Optimisation, uncovering patterns in their relative updates. Guided by these insights, we adapt Simultaneous Perturbation Stochastic Approximation (SPSA) with a targeted sampling strategy to accelerate convergence. Through experiments on summarisation, machine translation, and conversational assistants, we demonstrate that our method consistently enhances reward signals while achieving convergence times comparable to first-order methods. While it falls short of some state-of-the-art methods, our work is the first to apply Zeroth-Order methods to Preference Optimisation in LLMs, going beyond classification tasks and paving the way for a largely unexplored research direction. Code and visualisations are available at https://github.com/alessioGalatolo/VisZOPrO

</details>


### [58] [ORANSight-2.0: Foundational LLMs for O-RAN](https://arxiv.org/abs/2503.05200)

*Pranshav Gajjar, Vijay K. Shah*

**Main category:** cs.CL

**Keywords:** Open Radio Access Networks, Large Language Models, O-RAN, AI in telecommunications, Instruction tuning

**Relevance Score:** 4

**TL;DR:** ORANSight-2.0 introduces specialized foundational LLMs for Open Radio Access Networks (O-RAN), improving performance for domain-specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of Large Language Models (LLMs) in Open Radio Access Networks (O-RAN) is limited due to the lack of domain-specific models, which are necessary to address unique technical challenges.

**Method:** ORANSight-2.0 develops specialized foundational LLMs by fine-tuning 18 models across five open-source frameworks, using a newly introduced RAG-based framework for instruction-tuning.

**Key Contributions:**

	1. Introduction of domain-specific foundational LLMs for O-RAN
	2. Development of RANSTRUCT, a novel RAG-based tuning framework
	3. Creation of the srsRANBench benchmark for evaluating model performance

**Result:** The initiative enhances performance in O-RAN-specific tasks and reduces dependence on proprietary models, supported by the creation of high-quality instruction-tuning datasets.

**Limitations:** 

**Conclusion:** The developed srsRANBench benchmark serves as a tool for evaluating code generation and understanding in the context of a widely used 5G O-RAN stack, validating the effectiveness of ORANSight-2.0.

**Abstract:** Despite the transformative impact of Large Language Models (LLMs) across critical domains such as healthcare, customer service, and business marketing, their integration into Open Radio Access Networks (O-RAN) remains limited. This gap is primarily due to the absence of domain-specific foundational models, with existing solutions often relying on general-purpose LLMs that fail to address the unique challenges and technical intricacies of O-RAN. To bridge this gap, we introduce ORANSight-2.0 (O-RAN Insights), a pioneering initiative to develop specialized foundational LLMs tailored for O-RAN. Built on 18 models spanning five open-source LLM frameworks -- Mistral, Qwen, Llama, Phi, and Gemma -- ORANSight-2.0 fine-tunes models ranging from 1B to 70B parameters, significantly reducing reliance on proprietary, closed-source models while enhancing performance in O-RAN-specific tasks. At the core of ORANSight-2.0 is RANSTRUCT, a novel Retrieval-Augmented Generation (RAG)-based instruction-tuning framework that employs two LLM agents -- a Mistral-based Question Generator and a Qwen-based Answer Generator -- to create high-quality instruction-tuning datasets. The generated dataset is then used to fine-tune the 18 pre-trained open-source LLMs via QLoRA. To evaluate ORANSight-2.0, we introduce srsRANBench, a novel benchmark designed for code generation and codebase understanding in the context of srsRAN, a widely used 5G O-RAN stack.

</details>


### [59] [Towards Detecting Persuasion on Social Media: From Model Development to Insights on Persuasion Strategies](https://arxiv.org/abs/2503.13844)

*Elyas Meguellati, Stefano Civelli, Pietro Bernardelle, Shazia Sadiq, Irwin King, Gianluca Demartini*

**Main category:** cs.CL

**Keywords:** persuasive text detection, political advertising, social media, elections, transparency

**Relevance Score:** 2

**TL;DR:** This paper presents a lightweight model for detecting persuasive text in political advertising, showcasing its application on a dataset from the Australian Federal Election 2022.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to enhance voter awareness and ensure transparency in democratic processes by detecting persuasive elements in political advertising.

**Method:** The study introduces a lightweight model for persuasive text detection, validated through state-of-the-art performance in the SemEval 2023 Task 3, and applies it on the annotated Australian Federal Election 2022 Facebook Ads dataset.

**Key Contributions:**

	1. Development of a lightweight model achieving state-of-the-art results in persuasive text detection
	2. Application of the model on a newly annotated dataset from the Australian Federal Election 2022
	3. Identification of persuasive strategies used in political advertising on social media.

**Result:** The fine-tuned model reveals distinct patterns in political campaign persuasion strategies, including funding strategies, word choices, demographic targeting, and temporal shifts leading to election day.

**Limitations:** 

**Conclusion:** The findings highlight the need for domain-specific modeling in social media persuasion analysis and advocate for increased transparency and accountability in digital political campaigns.

**Abstract:** Political advertising plays a pivotal role in shaping public opinion and influencing electoral outcomes, often through subtle persuasive techniques embedded in broader propaganda strategies. Detecting these persuasive elements is crucial for enhancing voter awareness and ensuring transparency in democratic processes. This paper presents an integrated approach that bridges model development and real-world application through two interconnected studies. First, we introduce a lightweight model for persuasive text detection that achieves state-of-the-art performance in Subtask 3 of SemEval 2023 Task 3 while requiring significantly fewer computational resources and training data than existing methods. Second, we demonstrate the model's practical utility by collecting the Australian Federal Election 2022 Facebook Ads (APA22) dataset, partially annotating a subset for persuasion, and fine-tuning the model to adapt from mainstream news to social media content. We then apply the fine-tuned model to label the remainder of the APA22 dataset, revealing distinct patterns in how political campaigns leverage persuasion through different funding strategies, word choices, demographic targeting, and temporal shifts in persuasion intensity as election day approaches. Our findings not only underscore the necessity of domain-specific modeling for analyzing persuasion on social media but also show how uncovering these strategies can enhance transparency, inform voters, and promote accountability in digital campaigns.

</details>


### [60] [WAKENLLM: Evaluating Reasoning Potential and Stability in LLMs via Fine-Grained Benchmarking](https://arxiv.org/abs/2507.16199)

*Zipeng Ling, Yuehao Tang, Shuliang Liu, Junqi Yang, Shenghong Fu, Yao Wan, Kejia Huang, Chen Huang, Zhichao Hou, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vague Perception, Unknown responses, Guided stimulation, LLM reasoning

**Relevance Score:** 8

**TL;DR:** The paper introduces a framework to analyze and improve Large Language Models' (LLMs) responses designated as Unknown, distinguishing between genuine indeterminacy and solvable problems due to model incapacity, referred to as Vague Perception.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify and quantify why LLMs output Unknown responses, separating cases of true indeterminacy from model incapacity to improve LLM reasoning capabilities.

**Method:** A framework that quantifies Unknown responses, tests guided stimulation methods to convert them into correct Known or Unknown responses, and assesses theoretical accuracy across various LLMs.

**Key Contributions:**

	1. Introduction of the Vague Perception concept in LLMs
	2. A framework to analyze Unknown responses systematically
	3. Demonstration of methods to improve reasoning in LLMs using guided stimulation

**Result:** The proposed method successfully distinguishes sources of uncertainty in LLM outputs and shows potential for improving reasoning accuracy through guided stimulation.

**Limitations:** 

**Conclusion:** By addressing the Vague Perception phenomenon, this paper provides insights into LLM reasoning limits and methods for enhancing their decision-making capabilities.

**Abstract:** Large Language Models (LLMs) frequently output the label Unknown, yet current evaluations focus almost exclusively on whether such answers are honest rather than why they arise. This blurs two distinct cases: (i) an input that is genuinely indeterminate and (ii) a solvable problem that the model fails to resolve. We call this phenomenon Vague Perception. And thus we introduce a framework that quantifies the proportion of Unknown responses attributable to model incapacity and tests whether guided stimulation can convert them into either correct Known or correct Unknown with valid reasoning. By separating these sources of uncertainty, our method provides a clearer picture of LLM reasoning limits and their potential for improvement. As we get a theoretical accuracy of reasoning task on different LLMs, we apply different methods to test whether the model can reach the accuracy given a baseline framework. Our work is meaningful in exploring the potential reasoning ability of LLMs and providing a new perspective on solving the Vague Perception phenomenon.

</details>


### [61] [Language Detection by Means of the Minkowski Norm: Identification Through Character Bigrams and Frequency Analysis](https://arxiv.org/abs/2507.16284)

*Paul-Andrei Pogăcean, Sanda-Maria Avram*

**Main category:** cs.CL

**Keywords:** language identification, monograms, bigrams, frequency analysis, non-AI methods

**Relevance Score:** 4

**TL;DR:** This research presents a classical frequency-based algorithm for language identification that outperforms AI-driven models in accuracy for certain text lengths.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore non-AI-based approaches to language identification that have been overshadowed by the focus on AI-powered models.

**Method:** A mathematical implementation leveraging monograms and bigrams frequency rankings derived from linguistic research, applied to a variety of text datasets.

**Key Contributions:**

	1. Introduction of a frequency-based algorithm for language identification
	2. Demonstrated effectiveness across diverse text genres and lengths
	3. Provided a comparative analysis with AI-based models

**Result:** Achieves over 80% accuracy on texts shorter than 150 characters and 100% accuracy for longer texts.

**Limitations:** 

**Conclusion:** Classical frequency-based methods are effective and scalable alternatives for language detection compared to AI-driven models.

**Abstract:** The debate surrounding language identification has gained renewed attention in recent years, especially with the rapid evolution of AI-powered language models. However, the non-AI-based approaches to language identification have been overshadowed. This research explores a mathematical implementation of an algorithm for language determinism by leveraging monograms and bigrams frequency rankings derived from established linguistic research. The datasets used comprise texts varying in length, historical period, and genre, including short stories, fairy tales, and poems. Despite these variations, the method achieves over 80\% accuracy on texts shorter than 150 characters and reaches 100\% accuracy for longer texts. These results demonstrate that classical frequency-based approaches remain effective and scalable alternatives to AI-driven models for language detection.

</details>


### [62] [Test-Time-Matching: Decouple Personality, Memory, and Linguistic Style in LLM-based Role-Playing Language Agent](https://arxiv.org/abs/2507.16799)

*Xiaoyu Zhan, Xinyu Fu, Hao Sun, Yuanqi Li, Jie Guo, Yanwen Guo*

**Main category:** cs.CL

**Keywords:** large language models, role-playing, context engineering, Test-Time-Matching, dialogue generation

**Relevance Score:** 9

**TL;DR:** The paper introduces Test-Time-Matching (TTM), a training-free framework for enhancing role-playing in large language models (LLMs) by effectively decoupling character traits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for deeper immersion in role-play applications involving LLMs, especially for iconic figures, amid challenges in existing methods.

**Method:** TTM employs a three-stage generation pipeline to decouple character features into personality, memory, and linguistic style, optimized for role-playing without fine-tuning.

**Key Contributions:**

	1. Introduction of a training-free role-playing framework (TTM) for LLMs.
	2. Decoupling of character features for improved dialogue control.
	3. Evaluation shows high performance in expressive and stylistically consistent dialogue generation.

**Result:** TTM achieves high-fidelity role-playing performance and can seamlessly combine diverse linguistic styles and variations in character traits.

**Limitations:** 

**Conclusion:** The method demonstrates outstanding results in generating expressive and stylistically consistent dialogues, fulfilling the immersive role-play requirement effectively.

**Abstract:** The rapid advancement of large language models (LLMs) has enabled role-playing language agents to demonstrate significant potential in various applications. However, relying solely on prompts and contextual inputs often proves insufficient for achieving deep immersion in specific roles, particularly well-known fictional or public figures. On the other hand, fine-tuning-based approaches face limitations due to the challenges associated with data collection and the computational resources required for training, thereby restricting their broader applicability. To address these issues, we propose Test-Time-Matching (TTM), a training-free role-playing framework through test-time scaling and context engineering. TTM uses LLM agents to automatically decouple a character's features into personality, memory, and linguistic style. Our framework involves a structured, three-stage generation pipeline that utilizes these features for controlled role-playing. It achieves high-fidelity role-playing performance, also enables seamless combinations across diverse linguistic styles and even variations in personality and memory. We evaluate our framework through human assessment, and the results demonstrate that our method achieves the outstanding performance in generating expressive and stylistically consistent character dialogues.

</details>


### [63] [Agentar-Fin-R1: Enhancing Financial Intelligence through Domain Expertise, Training Efficiency, and Advanced Reasoning](https://arxiv.org/abs/2507.16802)

*Yanjun Zheng, Xiyang Du, Longfei Liao, Xiaoke Zhao, Zhaowen Zhou, Bo Zhang, Jiawei Liu, Xiang Qi, Zhe Li, Zhiqiang Zhang, Wei Wang, Peng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial Applications, Reasoning Capabilities

**Relevance Score:** 4

**TL;DR:** Introduction of Agentar-Fin-R1, a series of financial large language models designed to enhance reasoning and adaptation in financial applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for improved reasoning capabilities and trustworthiness in financial large language models, as current models struggle with complex financial scenarios.

**Method:** Development of the Agentar-Fin-R1 models based on Qwen3 with a structured optimization approach including a financial task label system and a trustworthiness assurance framework.

**Key Contributions:**

	1. Introduction of a novel finova evaluation benchmark for financial reasoning
	2. Enhancements in reasoning capabilities and reliability for financial applications
	3. High-quality data governance and synthesis for training efficiency

**Result:** The models show improved performance on financial benchmarks (Fineva, FinEval, FinanceIQ) and general reasoning datasets (MATH-500, GPQA-diamond), demonstrating state-of-the-art capabilities in financial domains and general reasoning.

**Limitations:** 

**Conclusion:** Agentar-Fin-R1 provides an effective and trustworthy solution for high-stakes financial applications, with new evaluation benchmarks established for real-world deployment.

**Abstract:** Large Language Models (LLMs) exhibit considerable promise in financial applications; however, prevailing models frequently demonstrate limitations when confronted with scenarios that necessitate sophisticated reasoning capabilities, stringent trustworthiness criteria, and efficient adaptation to domain-specific requirements. We introduce the Agentar-Fin-R1 series of financial large language models (8B and 32B parameters), specifically engineered based on the Qwen3 foundation model to enhance reasoning capabilities, reliability, and domain specialization for financial applications. Our optimization approach integrates a high-quality, systematic financial task label system with a comprehensive multi-layered trustworthiness assurance framework. This framework encompasses high-quality trustworthy knowledge engineering, multi-agent trustworthy data synthesis, and rigorous data validation governance. Through label-guided automated difficulty-aware optimization, tow-stage training pipeline, and dynamic attribution systems, we achieve substantial improvements in training efficiency. Our models undergo comprehensive evaluation on mainstream financial benchmarks including Fineva, FinEval, and FinanceIQ, as well as general reasoning datasets such as MATH-500 and GPQA-diamond. To thoroughly assess real-world deployment capabilities, we innovatively propose the Finova evaluation benchmark, which focuses on agent-level financial reasoning and compliance verification. Experimental results demonstrate that Agentar-Fin-R1 not only achieves state-of-the-art performance on financial tasks but also exhibits exceptional general reasoning capabilities, validating its effectiveness as a trustworthy solution for high-stakes financial applications. The Finova bench is available at https://github.com/antgroup/Finova.

</details>
