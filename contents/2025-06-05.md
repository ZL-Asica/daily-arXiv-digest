# 2025-06-05

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 17]

- [cs.CL](#cs.CL) [Total: 159]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [From Reality to Recognition: Evaluating Visualization Analogies for Novice Chart Comprehension](https://arxiv.org/abs/2506.03385)

*Oliver Huang, Patrick Lee, Carolina Nobre*

**Main category:** cs.HC

**Keywords:** visualization, chart comprehension, education, analogies, novice learners

**Relevance Score:** 6

**TL;DR:** This study explores how visualization analogies can improve comprehension of novel chart types for novice learners, showing significant improvements in visual analysis skills compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Novice learners struggle with understanding new visualizations due to reliance on previous simpler charts, necessitating better pedagogical strategies for teaching visualization.

**Method:** An empirical study involving 128 participants was conducted where 8 chart types were taught using visualization analogies, mapping data structures to real-world contexts.

**Key Contributions:**

	1. Introduction of visualization analogies for teaching chart comprehension
	2. Empirical evidence showing better student outcomes compared to traditional methods
	3. Open-source tools to support visualization education

**Result:** Findings indicate that visualization analogies enhance visual analysis skills and facilitate the transfer of understanding to actual charts, preferred by novices over traditional methods.

**Limitations:** 

**Conclusion:** Visualization analogies effectively support diverse learning preferences and improve educational outcomes in visualization comprehension for novices, providing insights and tools for educators.

**Abstract:** Novice learners often have difficulty learning new visualization types because they tend to interpret novel visualizations through the mental models of simpler charts they have previously encountered. Traditional visualization teaching methods, which usually rely on directly translating conceptual aspects of data into concrete data visualizations, often fail to attend to the needs of novice learners navigating this tension. To address this, we conducted an empirical exploration of how analogies can be used to help novices with chart comprehension. We introduced visualization analogies: visualizations that map data structures to real-world contexts to facilitate an intuitive understanding of novel chart types. We evaluated this pedagogical technique using a within-subject study (N=128) where we taught 8 chart types using visualization analogies. Our findings show that visualization analogies improve visual analysis skills and help learners transfer their understanding to actual charts. They effectively introduce visual embellishments, cater to diverse learning preferences, and are preferred by novice learners over traditional chart visualizations. This study offers empirical insights and open-source tools to advance visualization education through analogical reasoning.

</details>


### [2] [Sampling Preferences Yields Simple Trustworthiness Scores](https://arxiv.org/abs/2506.03399)

*Sean Steinle*

**Main category:** cs.HC

**Keywords:** large language models, trustworthiness, preference sampling, model evaluation, user preferences

**Relevance Score:** 9

**TL;DR:** This paper introduces preference sampling, a method to derive a single trustworthiness score from multi-dimensional evaluations of large language models (LLMs). It compares preference sampling to other aggregation methods, highlighting its effectiveness in reducing model candidates while incorporating user preferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of evaluating LLMs necessitates a robust method for simplifying decisions on model selection among multi-dimensional performance metrics.

**Method:** The authors propose preference sampling as a technique to extract a scalar trustworthiness score influenced by user-defined preferences, comparing its efficacy against alternate aggregation methods.

**Key Contributions:**

	1. Introduction of preference sampling for LLM evaluation
	2. Demonstration of improved model reduction compared to traditional methods
	3. Emphasis on user sensitivity in model selection process

**Result:** Preference sampling consistently reduces the set of candidate models by 100% of the time, outperforming Pareto optimality, which does not reduce the set by more than 50%.

**Limitations:** 

**Conclusion:** Preference sampling enables a more user-sensitive method for selecting LLMs by incorporating users' prior knowledge and preferences effectively.

**Abstract:** With the onset of large language models (LLMs), the performance of artificial intelligence (AI) models is becoming increasingly multi-dimensional. Accordingly, there have been several large, multi-dimensional evaluation frameworks put forward to evaluate LLMs. Though these frameworks are much more realistic than previous attempts which only used a single score like accuracy, multi-dimensional evaluations can complicate decision-making since there is no obvious way to select an optimal model. This work introduces preference sampling, a method to extract a scalar trustworthiness score from multi-dimensional evaluation results by considering the many characteristics of model performance which users value. We show that preference sampling improves upon alternate aggregation methods by using multi-dimensional trustworthiness evaluations of LLMs from TrustLLM and DecodingTrust. We find that preference sampling is consistently reductive, fully reducing the set of candidate models 100% of the time whereas Pareto optimality never reduces the set by more than 50%. Likewise, preference sampling is consistently sensitive to user priors-allowing users to specify the relative weighting and confidence of their preferences-whereas averaging scores is intransigent to the users' prior knowledge.

</details>


### [3] [The Stress of Improvisation: Instructors' Perspectives on Live Coding in Programming Classes](https://arxiv.org/abs/2506.03402)

*Xiaotian Su, April Wang*

**Main category:** cs.HC

**Keywords:** live coding, computer science education, instructor challenges, pedagogical techniques, IDE enhancements

**Relevance Score:** 6

**TL;DR:** The paper investigates the challenges instructors face while using live coding in computer science education and proposes potential enhancements to improve the experience.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the cognitive and psychological challenges instructors encounter while using live coding as a teaching technique in computer science classes.

**Method:** Conducted formative interviews with five teaching assistants and a contextual inquiry study with four lecturers in large-scale CS classes.

**Key Contributions:**

	1. Identified specific cognitive and psychological challenges faced by instructors during live coding.
	2. Proposed augmentations for IDEs and presentation setups to alleviate these challenges.
	3. Provided insights from empirical studies with teaching assistants and lecturers.

**Result:** Identified that the unpredictability of live coding increases mental stress for instructors compared to static presentations, affecting their ability to engage students and manage time.

**Limitations:** Focuses primarily on the challenges without extensive exploration of solutions or effectiveness of proposed tools.

**Conclusion:** There is a need for improved tools and environments to support instructors during live coding sessions to enhance the teaching experience.

**Abstract:** Live coding is a pedagogical technique in which an instructor writes and executes code in front of students to impart skills like incremental development and debugging. Although live coding offers many benefits, instructors face many challenges in the classroom, like cognitive challenges and psychological stress, most of which have yet to be formally studied. To understand the obstacles faced by instructors in CS classes, we conducted (1) a formative interview with five teaching assistants in exercise sessions and (2) a contextual inquiry study with four lecturers for large-scale classes. We found that the improvisational and unpredictable nature of live coding makes it difficult for instructors to manage their time and keep students engaged, resulting in more mental stress than presenting static slides. We discussed opportunities for augmenting existing IDEs and presentation setups to help enhance live coding experience.

</details>


### [4] [VChatter: Exploring Generative Conversational Agents for Simulating Exposure Therapy to Reduce Social Anxiety](https://arxiv.org/abs/2506.03520)

*Han Zhang, KaWing Tsang, Zhenhui Peng*

**Main category:** cs.HC

**Keywords:** social anxiety, exposure therapy, LLMs, conversational agents, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces VChatter, a multi-agent system utilizing large language models for simulating exposure therapy to help reduce social anxiety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to alleviate the challenges of social anxiety by employing a technology-driven approach for exposure therapy, traditionally reliant on human therapists.

**Method:** The authors developed VChatter, which includes an Agent-P that designs therapy plans and two Agent-Hs that interact with users in varying exposure scenarios, based on a survey (N=36) and qualitative study (N=10).

**Key Contributions:**

	1. Development of VChatter as a multi-agent system for therapy simulation
	2. Empirical evidence demonstrating effectiveness in reducing social anxiety
	3. Discussion on future design considerations for anxiety-specific agents

**Result:** The qualitative study demonstrated that VChatter effectively reduced users' social anxiety, feelings of isolation, and avoidance of social interactions.

**Limitations:** Study sample size was small (N=10), and further research is needed for broader applicability and long-term effects.

**Conclusion:** The research shows the feasibility of using conversational agents based on LLMs for simulating exposure therapy and highlights the need for tailored agent design for social anxiety.

**Abstract:** Many people struggle with social anxiety, feeling fear, or even physically uncomfortable in social situations like talking to strangers. Exposure therapy, a clinical method that gradually and repeatedly exposes individuals to the source of their fear and helps them build coping mechanisms, can reduce social anxiety but traditionally requires human therapists' guidance and constructions of situations. In this paper, we developed a multi-agent system VChatter to explore large language models(LLMs)-based conversational agents for simulating exposure therapy with users. Based on a survey study (N=36) and an expert interview, VChatter includes an Agent-P, which acts as a psychotherapist to design the exposure therapy plans for users, and two Agent-Hs, which can take on different interactive roles in low, medium, and high exposure scenarios. A six-day qualitative study (N=10) showcases VChatter's usefulness in reducing users' social anxiety, feelings of isolation, and avoidance of social interactions. We demonstrated the feasibility of using LLMs-based conversational agents to simulate exposure therapy for addressing social anxiety and discussed future concerns for designing agents tailored to social anxiety.

</details>


### [5] [Understanding Visually Impaired Tramway Passengers Interaction with Public Transport Systems](https://arxiv.org/abs/2506.03687)

*Dominik Mimra, Dominik Kaar, Enrico Del Re, Novel Certad, Joshua Cherian Varughese, David Seibt, Cristina Olaverri-Monreal*

**Main category:** cs.HC

**Keywords:** public transport, accessibility, socio-technical systems, Actor-Network Theory, visually impaired

**Relevance Score:** 6

**TL;DR:** This study examines the socio-technical networks affecting accessibility in public transport for visually impaired passengers, highlighting the integration of technology and social solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve inclusive public transport services and address accessibility challenges in smart city infrastructure.

**Method:** Utilized Actor-Network Theory and a mixed-method approach, including shadowing and focus groups, to study visually impaired passengers' experiences in Linz's tram system.

**Key Contributions:**

	1. Integration of socio-technical systems in accessibility research
	2. Identification of critical dimensions for public transport accessibility
	3. Insights into user comfort with digital technologies in transport settings

**Result:** Identified key dimensions influencing accessibility: network configuration, mobility patterns, technology integration, and warning systems; findings indicate accessibility arises from interactions between human and non-human actors.

**Limitations:** 

**Conclusion:** Digital technology plays a significant role in enhancing public transport accessibility but varies in user comfort and effectiveness; two-sense principles for warnings are vital.

**Abstract:** Designing inclusive public transport services is crucial to developing modern, barrier-free smart city infrastructure. This research contributes to the design of inclusive public transport by considering accessibility challenges emerging from socio-technical systems, thus demanding the integration of technological and social solutions. Using Actor-Network Theory (ANT) as a theoretical framework and a mixed-method approach, including shadowing and a focus group, this study examines the socio-technical networks that shape accessibility experiences for visually impaired passengers utilizing the tram in Linz, Austria. Key dimensions that influence public transport accessibility are identified: network configuration, mobility patterns, technology integration, and warning systems. The results show that accessibility emerges from complex interactions between human actors (passengers, staff) and non-human actors (assistive devices, infrastructure) rather than being an inherent property of transport systems. Digital technologies serve multiple functions, from navigational assistance to broader social inclusion, although users comfort with technology varies. Participants emphasized the importance of the two-sense principle for warning signals, with directional audio and tactile feedback particularly valuable.

</details>


### [6] [Design of a visual environment for programming by direct data manipulation](https://arxiv.org/abs/2506.03720)

*Michel Adam, Patrice Frison, Moncef Daoud, Sabine Letellier Zarshenas*

**Main category:** cs.HC

**Keywords:** visual programming, Turing-complete, data manipulation, graphical interfaces, algorithm development

**Relevance Score:** 4

**TL;DR:** This paper presents a tool for developing Turing-complete programs visually without coding, allowing users to manipulate data directly and visualize algorithm results, including support for generating code in several programming languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify the programming process for non-specialists by eliminating the need for traditional coding syntax, making programming more accessible.

**Method:** The tool allows users to create algorithms and iterations visually, controlling the state of the data and seeing the results of their manipulations in real time.

**Key Contributions:**

	1. Developing Turing-complete programs through visual manipulation
	2. Generating code in multiple programming languages from visual designs
	3. Providing real-time feedback on data manipulation during algorithm development

**Result:** The tool successfully enables the visual development of arbitrary programs and generates equivalent code in languages like Python, C, and Java.

**Limitations:** There are inherent challenges in accurately representing complex algorithms visually and ensuring the generated code is efficient and compact.

**Conclusion:** This direct-data-manipulation approach facilitates a more intuitive method to program, although it comes with certain limitations and challenges.

**Abstract:** The use of applications on computers, smartphones, and tablets has been considerably simplified thanks to interactive and dynamic graphical interfaces coupled with the mouse and touch screens. It is no longer necessary to be a computer specialist to use them. Paradoxically, the development of computer programs generally requires writing lines of code in a programming language whose syntax is particularly strict. This process poses many difficulties for programmers. We propose an original tool in which arbitrary programs (Turing-complete) can be developed in a completely visual manner by direct manipulation of the data, without writing a line of code. The user can thus develop an algorithm by directly visualizing the result of actions taken on the data. A method for constructing iterations is associated with the tool. It proposes to create each part, including the loop body, in a non-linear manner under visual control of the state of the data. In addition, the tool supports the production of lines of code in several languages including Python, C, Java, that correspond to the actions performed. In this article, we present the tool, the design choices, the problems to be solved, and the limits and the contributions of the direct-data-manipulation approach.

</details>


### [7] [Enhancing Text Comprehension for Dyslexic Readers: A 3D Semantic Visualization Approach Using Transformer Mode](https://arxiv.org/abs/2506.03731)

*Zhengyang Li*

**Main category:** cs.HC

**Keywords:** dyslexia, reading comprehension, Transformer models, semantic visualization, narrative tracking

**Relevance Score:** 6

**TL;DR:** This study introduces a Transformer model-based approach utilizing three-dimensional semantic visualization to enhance text comprehension for dyslexic readers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Dyslexic individuals struggle with traditional reading, especially complex texts, but they often have strong spatial imagination skills.

**Method:** The study employs Transformer models to convert sentences and words into three-dimensional vector representations, clustering semantically similar content to aid comprehension.

**Key Contributions:**

	1. Development of a three-dimensional semantic visualization technique for reading comprehension
	2. Demonstration of improved narrative relationship identification and character connection understanding
	3. Application of Transformer models in educational support for dyslexic readers.

**Result:** Experimental results show that this 3D semantic visualization method significantly improves dyslexic readers' comprehension of complex texts, enhancing narrative tracking and understanding character relationships.

**Limitations:** 

**Conclusion:** This innovative approach offers a promising pathway to improve reading comprehension for dyslexic individuals by leveraging their spatial abilities.

**Abstract:** Dyslexic individuals often face significant challenges with traditional reading, particularly when engaging with complex texts such as mystery novels. These texts typically demand advanced narrative tracking and information integration skills, making it difficult for dyslexic readers to fully comprehend the content. However, research indicates that while dyslexic individuals may struggle with textual processing, they often possess strong spatial imagination abilities. Leveraging this strength, this study proposes an innovative approach using Transformer models to map sentences and words into three-dimensional vector representations. This process clusters semantically similar sentences and words in spatial proximity, allowing dyslexic readers to interpret the semantic structure and narrative flow of the text through spatial perception. Experimental results demonstrate that, compared to direct text reading, this three-dimensional semantic visualization method significantly enhances dyslexic readers' comprehension of complex texts. In particular, it shows marked advantages in identifying narrative relationships and character connections. This study provides a novel pathway for improving textual comprehension among dyslexic individuals

</details>


### [8] [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)

*Rifat Mehreen Amin, Oliver Hans Kühle, Daniel Buschek, Andreas Butz*

**Main category:** cs.HC

**Keywords:** PromptCanvas, AI-generated content, Creativity Support Index, collaborative writing, interactive widgets

**Relevance Score:** 8

**TL;DR:** PromptCanvas is a widget-based interactive tool that enhances AI-generated content creation by allowing users to customize and organize their prompts on an infinite canvas, leading to improved creativity and reduced cognitive load.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide users with greater control and flexibility in generating AI content through a visual and interactive approach that enhances engagement.

**Method:** A lab study involving 18 participants and a follow-up field study with 10 participants compared PromptCanvas's effectiveness against a traditional conversational UI, measuring creativity support and cognitive load.

**Key Contributions:**

	1. Introduction of an infinite canvas for prompt customization
	2. Demonstrated reduction in cognitive load compared to traditional UIs
	3. Provided evidence for improved creativity in collaborative writing through visual organization.

**Result:** PromptCanvas outperformed traditional UI on the Creativity Support Index, showing lower mental demand and frustration among participants, while enhancing collaborative writing experiences.

**Limitations:** 

**Conclusion:** Dynamic, customizable interfaces like PromptCanvas can significantly improve user engagement and creativity in AI-supported writing tasks.

**Abstract:** We introduce PromptCanvas, a concept that transforms prompting into a composable, widget-based experience on an infinite canvas. Users can generate, customize, and arrange interactive widgets representing various facets of their text, offering greater control over AI-generated content. PromptCanvas allows widget creation through system suggestions, user prompts, or manual input, providing a flexible environment tailored to individual needs. This enables deeper engagement with the creative process. In a lab study with 18 participants, PromptCanvas outperformed a traditional conversational UI on the Creativity Support Index. Participants found that it reduced cognitive load, with lower mental demand and frustration. Qualitative feedback revealed that the visual organization of thoughts and easy iteration encouraged new perspectives and ideas. A follow-up field study (N=10) confirmed these results, showcasing the potential of dynamic, customizable interfaces in improving collaborative writing with AI.

</details>


### [9] [Understanding Mental Models of Generative Conversational Search and The Effect of Interface Transparency](https://arxiv.org/abs/2506.03807)

*Chadha Degachi, Samuel Kernan Freire, Evangelos Niforatos, Gerd Kortuem*

**Main category:** cs.HC

**Keywords:** conversational search, mental models, transparency, user trust, search interface design

**Relevance Score:** 8

**TL;DR:** This paper explores users' mental models of generative conversational search and the impact of transparency on user experience and trust.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding users' mental models of conversational search can reveal design intervention opportunities for improving interaction and trust.

**Method:** Conducted a study with 16 participants performing 4 search tasks across conversational interfaces with varying transparency levels.

**Key Contributions:**

	1. Investigated user mental models in generative conversational search interfaces.
	2. Identified transparency as a key factor in user trust.
	3. Proposed hybrid web-conversational search as a novel design direction.

**Result:** Most user mental models were found to be too abstract, limiting their ability to explain search instances, and indicating mental models may hinder trust in conversational search.

**Limitations:** Small participant size, limiting generalizability.

**Conclusion:** Improving transparency in conversational search interfaces could enhance user trust and interface design; hybrid search approaches are a promising future direction.

**Abstract:** The experience and adoption of conversational search is tied to the accuracy and completeness of users' mental models -- their internal frameworks for understanding and predicting system behaviour. Thus, understanding these models can reveal areas for design interventions. Transparency is one such intervention which can improve system interpretability and enable mental model alignment. While past research has explored mental models of search engines, those of generative conversational search remain underexplored, even while the popularity of these systems soars. To address this, we conducted a study with 16 participants, who performed 4 search tasks using 4 conversational interfaces of varying transparency levels. Our analysis revealed that most user mental models were too abstract to support users in explaining individual search instances. These results suggest that 1) mental models may pose a barrier to appropriate trust in conversational search, and 2) hybrid web-conversational search is a promising novel direction for future search interface design.

</details>


### [10] [Neural and Cognitive Impacts of AI: The Influence of Task Subjectivity on Human-LLM Collaboration](https://arxiv.org/abs/2506.04167)

*Matthew Russell, Aman Shah, Giles Blaney, Judith Amores, Mary Czerwinski, Robert J. K. Jacob*

**Main category:** cs.HC

**Keywords:** AI assistants, human-AI collaboration, task performance, user experience, cognitive processes

**Relevance Score:** 9

**TL;DR:** This paper analyzes the effects of Microsoft's Copilot, a LLM-based assistant, on users during various tasks, indicating that its effectiveness varies significantly based on the nature of the task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched effects of AI-based interactive assistants on users' mental and physiological states.

**Method:** The study involved objective tasks (SAT reading comprehension) and subjective tasks (personal reflection), utilizing fNIRS, Empatica E4, NASA-TLX, and questionnaires to measure performance and user experience with and without Copilot.

**Key Contributions:**

	1. Analysis of task-dependent effectiveness of AI assistants
	2. Introduction of brain-network based insights into human-AI collaboration
	3. Empirical measurements of user performance and experience across different task types

**Result:** Participants reported reduced workload and increased enjoyment in objective tasks, with improved performance. In contrast, subjective tasks showed no performance change with Copilot, highlighting task-dependent effectiveness.

**Limitations:** No physiological changes recorded; effectiveness based on subjective user reporting.

**Conclusion:** AI assistants like Copilot demonstrate varying effectiveness based on task type, posing implications for human-AI collaboration in tasks that require episodic memory.

**Abstract:** AI-based interactive assistants are advancing human-augmenting technology, yet their effects on users' mental and physiological states remain under-explored. We address this gap by analyzing how Copilot for Microsoft Word, a LLM-based assistant, impacts users. Using tasks ranging from objective (SAT reading comprehension) to subjective (personal reflection), and with measurements including fNIRS, Empatica E4, NASA-TLX, and questionnaires, we measure Copilot's effects on users. We also evaluate users' performance with and without Copilot across tasks. In objective tasks, participants reported a reduction of workload and an increase in enjoyment, which was paired with objective performance increases. Participants reported reduced workload and increased enjoyment with no change in performance in a creative poetry writing task. However, no benefits due to Copilot use were reported in a highly subjective self-reflection task. Although no physiological changes were recorded due to Copilot use, task-dependent differences in prefrontal cortex activation offer complementary insights into the cognitive processes associated with successful and unsuccessful human-AI collaboration. These findings suggest that AI assistants' effectiveness varies with task type-particularly showing decreased usefulness in tasks that engage episodic memory-and presents a brain-network based hypothesis of human-AI collaboration.

</details>


### [11] [Examining Algorithmic Curation on Social Media: An Empirical Audit of Reddit's r/popular Feed](https://arxiv.org/abs/2502.20491)

*Jackie Chan, Fred Choi, Koustuv Saha, Eshwar Chandrasekharan*

**Main category:** cs.HC

**Keywords:** algorithmic transparency, social media, engagement, Reddit, content moderation

**Relevance Score:** 7

**TL;DR:** An empirical audit of Reddit's algorithmically curated feed reveals how algorithms prioritize content, influencing user engagement and behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the opacity of algorithmic decision-making in social media feeds and its impact on user behavior.

**Method:** Conducted an empirical audit of Reddit's r/popular feed, analyzing 10K posts over 11 months to examine how content is ranked and engages users.

**Key Contributions:**

	1. Empirical evidence on how recent comments affect post ranking and engagement.
	2. Identification of a sharp engagement drop for posts ranked below 80.
	3. Analysis of undesired content impact on algorithmic visibility.

**Result:** Recent comments tend to help posts remain higher longer on r/popular. Posts ranked below 80 suffer a sharp engagement drop, while undesired comments do not significantly impact post longevity on the feed.

**Limitations:** 

**Conclusion:** Empirical audits can enhance transparency in social media algorithms, benefiting creators, consumers, and moderators by elucidating content prioritization.

**Abstract:** Platforms are increasingly relying on algorithms to curate the content within users' social media feeds. However, the growing prominence of proprietary, algorithmically curated feeds has concealed what factors influence the presentation of content on social media feeds and how that presentation affects user behavior. This lack of transparency can be detrimental to users, from reducing users' agency over their content consumption to the propagation of misinformation and toxic content. To uncover details about how these feeds operate and influence user behavior, we conduct an empirical audit of Reddit's algorithmically curated trending feed called r/popular. Using 10K r/popular posts collected by taking snapshots of the feed over 11 months, we find that recent comments help a post remain on r/popular longer and climb the feed. We also find that posts below rank 80 correspond to a sharp decline in activity compared to posts above. When examining the effects of having a higher proportion of undesired behavior -- i.e., moderator-removed and toxic comments -- we find no significant evidence that it helps posts stay on r/popular for longer. Although posts closer to the top receive more undesired comments, we find this increase to coincide with a broader increase in overall engagement -- rather than indicating a disproportionate effect on undesired activity. The relationships between algorithmic rank and engagement highlight the extent to which algorithms employed by social media platforms essentially determine which content is prioritized and which is not. We conclude by discussing how content creators, consumers, and moderators on social media platforms can benefit from empirical audits aimed at improving transparency in algorithmically curated feeds.

</details>


### [12] [Towards Sustainable Creativity Support: An Exploratory Study on Prompt Based Image Generation](https://arxiv.org/abs/2504.07879)

*Daniel Hove Paludan, Julie Fredsgård, Kasper Patrick Bährentz, Ilhan Aslan, Niels van Berkel*

**Main category:** cs.HC

**Keywords:** generative AI, creativity, energy consumption, user study, image generation

**Relevance Score:** 6

**TL;DR:** The paper investigates the balance between energy consumption and perceived creativity support in AI image generators through a user study.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate if energy consumption in AI image generation can be reduced while maintaining perceived support for human creativity.

**Method:** A user study involving 24 participants was conducted to assess the effects of different conditions in image generation on energy consumption and creativity support.

**Key Contributions:**

	1. Exploration of the relationship between AI energy consumption and creativity support.
	2. User study findings that inform AI design for reducing environmental impact.
	3. Recommendations for prompt optimization to achieve better energy efficiency in AI generation.

**Result:** The study found a significant relationship between energy consumption and the perceived support for creativity, influenced mainly by the number of images generated per prompt.

**Limitations:** The study is limited to a specific user group and may not generalize across other demographics or settings.

**Conclusion:** The paper concludes that it is possible to reduce energy usage in AI tools for creativity without compromising their effectiveness, provided users adjust their prompting behavior.

**Abstract:** Creativity is a valuable human skill that has long been augmented through both analog and digital tools. Recent progress in generative AI, such as image generation, provides a disruptive technological solution to supporting human creativity further and helping humans generate solutions faster. While AI image generators can help to rapidly visualize ideas based on user prompts, the use of such AI systems has also been critiqued due to their considerable energy usage. In this paper, we report on a user study (N = 24) to understand whether energy consumption can be reduced without impeding on the tool's perceived creativity support. Our results highlight that, for example, a main effect of (image generation) condition on energy consumption, and index of creativity support per prompt but not per task, which seem mainly attributed to image quantity per prompt. We provide details of our analysis on the relation between energy usage, creativity support, and prompting behavior, including attitudes towards designing with AI and its environmental impact.

</details>


### [13] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)

*Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang*

**Main category:** cs.HC

**Keywords:** GUI Agents, Human-Computer Interaction, Large Language Models, Automation, Evaluation Methodologies

**Relevance Score:** 9

**TL;DR:** This survey examines the evolution and capabilities of LLM-based GUI Agents in HCI, focusing on their components and evaluation methodologies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of the rapidly advancing field of LLM-based GUI Agents and identify challenges and future directions.

**Method:** Systematic analysis of architectural foundations, technical components, evaluation methodologies, and critical examination of current evaluation frameworks for GUI Agents.

**Key Contributions:**

	1. Systematic examination of LLM-based GUI Agents and their components.
	2. Identification of key technical challenges and future research directions.
	3. Critical analysis of current evaluation methodologies and standardization proposals.

**Result:** Identifies fundamental components of GUI Agents, reveals advances in automation facilitated by LLMs, and highlights methodological limitations in existing benchmarks.

**Limitations:** Methodological limitations in existing benchmarks for evaluation of GUI Agents.

**Conclusion:** The review offers insights into the state of intelligent interface automation and provides directions for future research to enhance GUI Agents' capabilities.

**Abstract:** Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.

</details>


### [14] [Biased by Design: Leveraging AI Inherent Biases to Enhance Critical Thinking of News Readers](https://arxiv.org/abs/2504.14522)

*Liudmila Zavolokina, Kilian Sprenkamp, Zoya Katashinskaya, Daniel Gordon Jones*

**Main category:** cs.HC

**Keywords:** propaganda detection, Large Language Models, user personalization, confirmation bias, AI tools

**Relevance Score:** 9

**TL;DR:** This paper presents a design for a propaganda detection tool utilizing Large Language Models, focusing on user biases to promote critical thinking in news consumption.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how biases in AI models can enhance critical thinking rather than hinder it, particularly in political contexts.

**Method:** The research involves a qualitative user study exploring user choices, personalization, and leveraging psychological concepts to design an effective propaganda detection tool.

**Key Contributions:**

	1. Design recommendations for AI tools in propaganda detection
	2. Insights into user choice and personalization based on political stance
	3. Application of psychological concepts like confirmation bias in tool design

**Result:** The findings suggest that user choice, personalization, and awareness of biases can improve the efficacy of propaganda detection tools, encouraging exploration of diverse viewpoints.

**Limitations:** The qualitative nature of the study may limit generalizability, and findings need further validation in diverse contexts.

**Conclusion:** The study concludes that incorporating psychological strategies can assist users in navigating potential biases in news consumption and enhance the overall design of AI tools.

**Abstract:** This paper explores the design of a propaganda detection tool using Large Language Models (LLMs). Acknowledging the inherent biases in AI models, especially in political contexts, we investigate how these biases might be leveraged to enhance critical thinking in news consumption. Countering the typical view of AI biases as detrimental, our research proposes strategies of user choice and personalization in response to a user's political stance, applying psychological concepts of confirmation bias and cognitive dissonance. We present findings from a qualitative user study, offering insights and design recommendations (bias awareness, personalization and choice, and gradual introduction of diverse perspectives) for AI tools in propaganda detection.

</details>


### [15] [MAC-Gaze: Motion-Aware Continual Calibration for Mobile Gaze Tracking](https://arxiv.org/abs/2505.22769)

*Yaxiong Lei, Mingyue Zhao, Yuheng Wang, Shijing He, Yusuke Sugano, Mohamed Khamis, Juan Ye*

**Main category:** cs.HC

**Keywords:** gaze tracking, motion awareness, continual learning, IMU sensors, calibration

**Relevance Score:** 8

**TL;DR:** MAC-Gaze is a Motion-Aware continual Calibration approach for mobile gaze tracking that leverages IMU sensors and continual learning techniques to maintain accuracy amid user motion changes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of degraded gaze tracking performance due to user posture and device orientation changes, traditional calibration methods are insufficient, prompting the need for a dynamic and adaptive solution.

**Method:** The system uses a combination of a pre-trained gaze estimator and an IMU-based activity recognition model, implementing a clustering-based decision mechanism to trigger recalibration as user motion states change, while employing replay-based continual learning to prevent catastrophic forgetting.

**Key Contributions:**

	1. Introduction of a Motion-Aware continual calibration technique for gaze tracking.
	2. Integration of IMU sensors for real-time motion detection and model recalibration.
	3. Implementation of replay-based continual learning to mitigate catastrophic forgetting.

**Result:** The method reduces gaze estimation error by 19.9% on the RGBDGaze dataset and 31.7% on the MotionGaze dataset compared to traditional methods, demonstrating improved performance across various postures and motion conditions.

**Limitations:** The method relies on the quality of the IMU sensors and may require additional work to further generalize across a wider range of activities and environments.

**Conclusion:** MAC-Gaze provides an effective solution for maintaining gaze estimation accuracy in mobile settings, adapting to users' changing motion states dynamically.

**Abstract:** Mobile gaze tracking faces a fundamental challenge: maintaining accuracy as users naturally change their postures and device orientations. Traditional calibration approaches, like one-off, fail to adapt to these dynamic conditions, leading to degraded performance over time. We present MAC-Gaze, a Motion-Aware continual Calibration approach that leverages smartphone Inertial measurement unit (IMU) sensors and continual learning techniques to automatically detect changes in user motion states and update the gaze tracking model accordingly. Our system integrates a pre-trained visual gaze estimator and an IMU-based activity recognition model with a clustering-based hybrid decision-making mechanism that triggers recalibration when motion patterns deviate significantly from previously encountered states. To enable accumulative learning of new motion conditions while mitigating catastrophic forgetting, we employ replay-based continual learning, allowing the model to maintain performance across previously encountered motion conditions. We evaluate our system through extensive experiments on the publicly available RGBDGaze dataset and our own 10-hour multimodal MotionGaze dataset (481K+ images, 800K+ IMU readings), encompassing a wide range of postures under various motion conditions including sitting, standing, lying, and walking. Results demonstrate that our method reduces gaze estimation error by 19.9% on RGBDGaze (from 1.73 cm to 1.41 cm) and by 31.7% on MotionGaze (from 2.81 cm to 1.92 cm) compared to traditional calibration approaches. Our framework provides a robust solution for maintaining gaze estimation accuracy in mobile scenarios.

</details>


### [16] [PromptCanvas: Composable Prompting Workspaces Using Dynamic Widgets for Exploration and Iteration in Creative Writing](https://arxiv.org/abs/2506.03741)

*Rifat Mehreen Amin, Oliver Hans Kühle, Daniel Buschek, Andreas Butz*

**Main category:** cs.HC

**Keywords:** PromptCanvas, AI content generation, Creativity support, Cognitive load, Collaborative writing

**Relevance Score:** 8

**TL;DR:** PromptCanvas is a widget-based interface that enhances user control over AI-generated content, demonstrating better creativity support than traditional UIs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve user engagement and control in AI-generated content generation by providing a flexible, customizable interface.

**Method:** The study investigates the use of PromptCanvas through a lab study with 18 participants and a follow-up field study with 10 participants, measuring creativity support and cognitive load.

**Key Contributions:**

	1. Introduction of the PromptCanvas concept
	2. Demonstrated improvement in the Creativity Support Index
	3. Showed reduced cognitive load in users

**Result:** PromptCanvas resulted in lower mental demand and frustration compared to traditional UIs, and received positive qualitative feedback for facilitating new ideas.

**Limitations:** The sample size is small, limiting the generalizability of the findings.

**Conclusion:** Dynamic, customizable interfaces like PromptCanvas can significantly enhance collaborative writing experiences with AI.

**Abstract:** We introduce PromptCanvas, a concept that transforms prompting into a composable, widget-based experience on an infinite canvas. Users can generate, customize, and arrange interactive widgets representing various facets of their text, offering greater control over AI-generated content. PromptCanvas allows widget creation through system suggestions, user prompts, or manual input, providing a flexible environment tailored to individual needs. This enables deeper engagement with the creative process. In a lab study with 18 participants, PromptCanvas outperformed a traditional conversational UI on the Creativity Support Index. Participants found that it reduced cognitive load, with lower mental demand and frustration. Qualitative feedback revealed that the visual organization of thoughts and easy iteration encouraged new perspectives and ideas. A follow-up field study (N=10) confirmed these results, showcasing the potential of dynamic, customizable interfaces in improving collaborative writing with AI.

</details>


### [17] [A Survey on (M)LLM-Based GUI Agents](https://arxiv.org/abs/2504.13865)

*Fei Tang, Haolei Xu, Hang Zhang, Siqi Chen, Xingyu Wu, Yongliang Shen, Wenqi Zhang, Guiyang Hou, Zeqi Tan, Yuchen Yan, Kaitao Song, Jian Shao, Weiming Lu, Jun Xiao, Yueting Zhuang*

**Main category:** cs.HC

**Keywords:** GUI Agents, Human-Computer Interaction, Large Language Models, Multimodal Learning, Evaluation Frameworks

**Relevance Score:** 9

**TL;DR:** This survey examines the advancements in LLM-based GUI Agents, focusing on their components, evaluation methodologies, and future developments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive understanding of LLM-based GUI Agents and their role in modern HCI.

**Method:** Systematic analysis of architectural foundations, technical components, and evaluation methodologies of GUI Agents.

**Key Contributions:**

	1. Comprehensive examination of LLM-based GUI Agents and their components.
	2. Analysis of technical challenges in current GUI automation.
	3. Proposed directions for standardization in evaluation methodologies.

**Result:** Identified four fundamental components of GUI Agents: perception systems, exploration mechanisms, planning frameworks, and interaction systems; analyzed their technological advancements and evaluation frameworks.

**Limitations:** Methodological limitations in existing evaluation benchmarks.

**Conclusion:** The survey reveals critical challenges and future research directions for enhancing GUI Agents' capabilities in intelligent interface automation.

**Abstract:** Graphical User Interface (GUI) Agents have emerged as a transformative paradigm in human-computer interaction, evolving from rule-based automation scripts to sophisticated AI-driven systems capable of understanding and executing complex interface operations. This survey provides a comprehensive examination of the rapidly advancing field of LLM-based GUI Agents, systematically analyzing their architectural foundations, technical components, and evaluation methodologies. We identify and analyze four fundamental components that constitute modern GUI Agents: (1) perception systems that integrate text-based parsing with multimodal understanding for comprehensive interface comprehension; (2) exploration mechanisms that construct and maintain knowledge bases through internal modeling, historical experience, and external information retrieval; (3) planning frameworks that leverage advanced reasoning methodologies for task decomposition and execution; and (4) interaction systems that manage action generation with robust safety controls. Through rigorous analysis of these components, we reveal how recent advances in large language models and multimodal learning have revolutionized GUI automation across desktop, mobile, and web platforms. We critically examine current evaluation frameworks, highlighting methodological limitations in existing benchmarks while proposing directions for standardization. This survey also identifies key technical challenges, including accurate element localization, effective knowledge retrieval, long-horizon planning, and safety-aware execution control, while outlining promising research directions for enhancing GUI Agents' capabilities. Our systematic review provides researchers and practitioners with a thorough understanding of the field's current state and offers insights into future developments in intelligent interface automation.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [18] [Evaluating Large Language Models for Zero-Shot Disease Labeling in CT Radiology Reports Across Organ Systems](https://arxiv.org/abs/2506.03259)

*Michael E. Garcia-Alcoser, Mobina GhojoghNejad, Fakrul Islam Tushar, David Kim, Kyle J. Lafata, Geoffrey D. Rubin, Joseph Y. Lo*

**Main category:** cs.CL

**Keywords:** large language models, CT radiology reports, disease annotation, automated reporting, machine learning

**Relevance Score:** 9

**TL;DR:** This study evaluates the use of large language models for automating disease annotation in CT radiology reports, comparing their performance against a rule-based algorithm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of LLMs in automating annotations of CT radiology reports and to compare them with traditional methods.

**Method:** The study analyzed 40,833 CT reports and evaluated three lightweight open-weight LLMs using zero-shot prompting, compared to a rule-based algorithm. Performance metrics included Cohen's Kappa and F1 scores.

**Key Contributions:**

	1. Demonstrated the effectiveness of LLMs in medical annotations.
	2. Provided insights into performance metrics in multi-disease labeling.
	3. Showed that lightweight models can enhance clinical efficiency.

**Result:** Llama-3.1 8B and Gemma-3 27B exhibited the highest performance, with top scores of 0.87 for Kappa and 0.82 for macro-F1, outperforming the rule-based algorithm.

**Limitations:** Binary labels do not fully represent the nuances in radiology reports.

**Conclusion:** Lightweight LLMs are superior to rule-based methods for CT report annotation, although binary labels do not encompass the full complexity of report language.

**Abstract:** Purpose: This study aims to evaluate the effectiveness of large language models (LLMs) in automating disease annotation of CT radiology reports. We compare a rule-based algorithm (RBA), RadBERT, and three lightweight open-weight LLMs for multi-disease labeling of chest, abdomen, and pelvis (CAP) CT reports.   Materials and Methods: This retrospective study analyzed 40,833 CT reports from 29,540 patients, with 1,789 CAP reports manually annotated across three organ systems. External validation was conducted using the CT-RATE dataset. Three open-weight LLMs were tested with zero-shot prompting. Performance was evaluated using Cohen's Kappa and micro/macro-averaged F1 scores.   Results: In 12,197 Duke CAP reports from 8,854 patients, Llama-3.1 8B and Gemma-3 27B showed the highest agreement ($\kappa$ median: 0.87). On the manually annotated set, Gemma-3 27B achieved the top macro-F1 (0.82), followed by Llama-3.1 8B (0.79), while the RBA scored lowest (0.64). On the CT-RATE dataset (lungs/pleura only), Llama-3.1 8B performed best (0.91), with Gemma-3 27B close behind (0.89). Performance differences were mainly due to differing labeling practices, especially for lung atelectasis.   Conclusion: Lightweight LLMs outperform rule-based methods for CT report annotation and generalize across organ systems with zero-shot prompting. However, binary labels alone cannot capture the full nuance of report language. LLMs can provide a flexible, efficient solution aligned with clinical judgment and user needs.

</details>


### [19] [A conclusive remark on linguistic theorizing and language modeling](https://arxiv.org/abs/2506.03268)

*Cristiano Chesi*

**Main category:** cs.CL

**Keywords:** 

**Relevance Score:** 0

**TL;DR:** 

**Read time:** 0 min

<details>
  <summary>Details</summary>

**Motivation:** 

**Method:** 

**Key Contributions:**



**Result:** 

**Limitations:** 

**Conclusion:** 

**Abstract:** This is the final remark on the replies received to my target paper in the Italian Journal of Linguistics

</details>


### [20] [FailureSensorIQ: A Multi-Choice QA Dataset for Understanding Sensor Relationships and Failure Modes](https://arxiv.org/abs/2506.03278)

*Christodoulos Constantinides, Dhaval Patel, Shuxin Lin, Claudio Guerrero, Sunil Dagajirao Patil, Jayant Kalagnanam*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-Choice Question-Answering, Industry 4.0, feature selection, failure prediction

**Relevance Score:** 8

**TL;DR:** FailureSensorIQ is a MCQA benchmarking system for assessing LLMs' reasoning in Industry 4.0, revealing performance issues across various models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLM reasoning capabilities in complex, domain-specific scenarios related to Industry 4.0.

**Method:** Developed a novel MCQA benchmarking system focusing on failure modes and sensor data relationships, evaluating LLMs through various analytical methods.

**Key Contributions:**

	1. Introduction of FailureSensorIQ for LLM assessment in Industry 4.0
	2. Detailed evaluation of LLMs including GPT-4 and Llama
	3. Real-world case study showcasing LLM applicability in failure prediction

**Result:** LLMs show a significant performance drop when faced with perturbations and distractions, despite some models nearing expert-level performance.

**Limitations:** Performance drops in LLMs are fragile and sensitive to various disruptions.

**Conclusion:** LLMs can influence modeling decisions in industrial contexts, with detailed findings and resources provided for further study.

**Abstract:** We introduce FailureSensorIQ, a novel Multi-Choice Question-Answering (MCQA) benchmarking system designed to assess the ability of Large Language Models (LLMs) to reason and understand complex, domain-specific scenarios in Industry 4.0. Unlike traditional QA benchmarks, our system focuses on multiple aspects of reasoning through failure modes, sensor data, and the relationships between them across various industrial assets. Through this work, we envision a paradigm shift where modeling decisions are not only data-driven using statistical tools like correlation analysis and significance tests, but also domain-driven by specialized LLMs which can reason about the key contributors and useful patterns that can be captured with feature engineering. We evaluate the Industrial knowledge of over a dozen LLMs-including GPT-4, Llama, and Mistral-on FailureSensorIQ from different lens using Perturbation-Uncertainty-Complexity analysis, Expert Evaluation study, Asset-Specific Knowledge Gap analysis, ReAct agent using external knowledge-bases. Even though closed-source models with strong reasoning capabilities approach expert-level performance, the comprehensive benchmark reveals a significant drop in performance that is fragile to perturbations, distractions, and inherent knowledge gaps in the models. We also provide a real-world case study of how LLMs can drive the modeling decisions on 3 different failure prediction datasets related to various assets. We release: (a) expert-curated MCQA for various industrial assets, (b) FailureSensorIQ benchmark and Hugging Face leaderboard based on MCQA built from non-textual data found in ISO documents, and (c) LLMFeatureSelector, an LLM-based feature selection scikit-learn pipeline. The software is available at https://github.com/IBM/FailureSensorIQ.

</details>


### [21] [HyperSteer: Activation Steering at Scale with Hypernetworks](https://arxiv.org/abs/2506.03292)

*Jiuding Sun, Sidharth Baskaran, Zhengxuan Wu, Michael Sklar, Christopher Potts, Atticus Geiger*

**Main category:** cs.CL

**Keywords:** language models, steering vectors, hypernetwork, control mechanisms, natural language processing

**Relevance Score:** 8

**TL;DR:** HyperSteer is introduced as a hypernetwork-based architecture for generating steering vectors for language models, achieving superior performance compared to existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of current methods for generating steering vectors for language models, specifically the trade-off between unsupervised methods that lack efficacy guarantees and supervised methods that require extensive data.

**Method:** HyperSteer uses a family of hypernetwork-based architectures that generate steering vectors based on natural language steering prompts and the internal states of the language model.

**Key Contributions:**

	1. Introduction of HyperSteer for generating steering vectors
	2. End-to-end training of hypernetworks conditioned on prompts
	3. Performance exceeds existing state-of-the-art methods

**Result:** HyperSteer outperforms state-of-the-art activation steering methods even with unseen steering prompts and matches the effectiveness of steering via prompting.

**Limitations:** 

**Conclusion:** The study demonstrates that HyperSteer is a scalable and effective solution for controlling language model outputs by generating appropriate steering vectors.

**Abstract:** Steering language models (LMs) by modifying internal activations is a popular approach for controlling text generation. Unsupervised dictionary learning methods, e.g., sparse autoencoders, can be scaled to produce many steering vectors, but lack guarantees on the individual efficacy of each vector and control over the coverage of relevant steering tasks. In contrast, supervised methods for constructing steering vectors are targeted and effective, but require more data collection and training for each additional steering vector produced. In this work, we introduce HyperSteer, a family of hypernetwork-based architectures which are trained end-to-end to generate steering vectors conditioned on the natural language steering prompts and the internals of the steered LM. In our evaluations, we show that scaling HyperSteer with thousands of steering prompts exceeds the performance of state-of-the-art activation steering methods, even on steering prompts never seen during training. Moreover, HyperSteer performs on par with steering-via-prompting.

</details>


### [22] [Unleashing the Reasoning Potential of Pre-trained LLMs by Critique Fine-Tuning on One Problem](https://arxiv.org/abs/2506.03295)

*Yubo Wang, Ping Nie, Kai Zou, Lijun Wu, Wenhu Chen*

**Main category:** cs.CL

**Keywords:** Critique Fine-Tuning, Large Language Models, Reinforcement Learning, Reasoning, Compute Efficiency

**Relevance Score:** 8

**TL;DR:** Critique Fine-Tuning (CFT) efficiently enhances the reasoning abilities of large language models using minimal training on single problems, outperforming traditional reinforcement learning approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To find a more efficient method than reinforcement learning (RL) for improving reasoning capabilities in large language models (LLMs).

**Method:** The authors employ Critique Fine-Tuning (CFT) by generating diverse model responses to a single problem and use teacher LLMs to critique these responses.

**Key Contributions:**

	1. Introduction of Critique Fine-Tuning (CFT) for enhancing LLM reasoning with low compute
	2. Demonstration of significant performance gains on math and logic reasoning tasks
	3. Comparison of CFT effectiveness against traditional RL approaches with 20x less compute usage.

**Result:** CFT significantly improves the performance of Qwen and Llama models, achieving average improvements of 15% on math benchmarks and 16% on logic reasoning benchmarks with only 5 GPU hours of training.

**Limitations:** CFT relies on generating critiques from teacher LLMs, which may limit generalizability to novel tasks beyond those the model has been fine-tuned on.

**Conclusion:** One-shot CFT is a simple, general, and compute-efficient method to enhance reasoning capabilities in modern LLMs, providing competitive results compared to traditional RL methods at a fraction of the computational cost.

**Abstract:** We have witnessed that strong LLMs like Qwen-Math, MiMo, and Phi-4 possess immense reasoning potential inherited from the pre-training stage. With reinforcement learning (RL), these models can improve dramatically on reasoning tasks. Recent studies have shown that even RL on a single problem can unleash these models' reasoning capabilities. However, RL is not only expensive but also unstable. Even one-shot RL requires hundreds of GPU hours. This raises a critical question: Is there a more efficient way to unleash the reasoning potential of these powerful base LLMs? In this work, we demonstrate that Critique Fine-Tuning (CFT) on only one problem can effectively unleash the reasoning potential of LLMs. Our method constructs critique data by collecting diverse model-generated solutions to a single problem and using teacher LLMs to provide detailed critiques. We fine-tune Qwen and Llama family models, ranging from 1.5B to 14B parameters, on the CFT data and observe significant performance gains across diverse reasoning tasks. For example, with just 5 GPU hours of training, Qwen-Math-7B-CFT show an average improvement of 15% on six math benchmarks and 16% on three logic reasoning benchmarks. These results are comparable to or even surpass the results from RL with 20x less compute. Ablation studies reveal the robustness of one-shot CFT across different prompt problems. These results highlight one-shot CFT as a simple, general, and compute-efficient approach to unleashing the reasoning capabilities of modern LLMs.

</details>


### [23] [From Instructions to ODRL Usage Policies: An Ontology Guided Approach](https://arxiv.org/abs/2506.03301)

*Daham M. Mustafa, Abhishek Nadgeri, Diego Collarana, Benedikt T. Arnold, Christoph Quix, Christoph Lange, Stefan Decker*

**Main category:** cs.CL

**Keywords:** large language models, ODRL, knowledge graphs, data exchange, policy generation

**Relevance Score:** 7

**TL;DR:** This study proposes a method to automatically generate usage policies in the W3C Open Digital Rights Language (ODRL) using large language models like GPT-4, with a focus on improving accuracy through ontology documentation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the generation of usage policies from natural language instructions, improving the efficiency and accuracy of policy creation in distributed data exchange contexts.

**Method:** Leveraging the ODRL ontology and its documentation to guide a knowledge graph (KG) construction process, including various heuristics for adaptation.

**Key Contributions:**

	1. Development of a method for automatic policy generation from natural language using large language models
	2. Integration of ODRL ontology for better guidance in knowledge graph construction
	3. Creation of a benchmark for evaluating policy generation effectiveness

**Result:** The approach achieved an accuracy of up to 91.95% in generating knowledge graphs based on a benchmark of 12 use cases.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of using large language models for effective policy generation in the cultural domain, suggesting further applications in trustworthy data exchange.

**Abstract:** This study presents an approach that uses large language models such as GPT-4 to generate usage policies in the W3C Open Digital Rights Language ODRL automatically from natural language instructions. Our approach uses the ODRL ontology and its documentation as a central part of the prompt. Our research hypothesis is that a curated version of existing ontology documentation will better guide policy generation. We present various heuristics for adapting the ODRL ontology and its documentation to guide an end-to-end KG construction process. We evaluate our approach in the context of dataspaces, i.e., distributed infrastructures for trustworthy data exchange between multiple participating organizations for the cultural domain. We created a benchmark consisting of 12 use cases of varying complexity. Our evaluation shows excellent results with up to 91.95% accuracy in the resulting knowledge graph.

</details>


### [24] [Hopscotch: Discovering and Skipping Redundancies in Language Models](https://arxiv.org/abs/2506.03303)

*Mustafa Eyceoz, Nikhil Shivakumar Nayak, Hao Wang, Ligong Han, Akash Srivastava*

**Main category:** cs.CL

**Keywords:** language models, attention blocks, model efficiency, scalable outputs

**Relevance Score:** 6

**TL;DR:** Hopscotch is a method for optimizing causal language models by identifying and skipping less important attention blocks, achieving high efficiency with minimal performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of modern causal language models by skipping unnecessary attention blocks without degrading output quality.

**Method:** Hopscotch identifies and skips attention blocks based on their contribution to a task and adaptively scales the outputs of the remaining layers using trainable parameters.

**Key Contributions:**

	1. Introduced a method to skip unnecessary attention blocks in language models
	2. Developed lightweight, trainable scaling parameters to mitigate performance drops
	3. Demonstrated low performance loss when applied to large language models

**Result:** When implemented on models like Llama-3.1-8B and Qwen2.5-7B, Hopscotch resulted in less than a 2% performance drop despite skipping four attention blocks.

**Limitations:** 

**Conclusion:** Hopscotch improves model efficiency by optimizing attention block usage while maintaining performance, and it is compatible with existing model compression techniques.

**Abstract:** Modern causal language models stack many attention blocks to improve performance, but not all blocks are necessary for every task. We propose Hopscotch, a simple yet effective method that identifies and skips attention blocks with least contributions to a task and adapts to preserve output quality. Hopscotch jointly optimizes which blocks to skip and how to scale the outputs of the remaining layers. By introducing lightweight, trainable scaling parameters to attention and MLP blocks, it mitigates distribution shifts in hidden states caused by removing attention blocks. Hopscotch does not modify model weights or require access to pretraining or instruction-tuning data, and is compatible with existing model compression techniques. When applied to $\texttt{Llama-3.1-8B}$ and $\texttt{Qwen2.5-7B}$, Hopscotch achieves less than a 2% drop in performance even after skipping four attention blocks.

</details>


### [25] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)

*Guillermo Marco, Julio Gonzalo, Víctor Fresno*

**Main category:** cs.CL

**Keywords:** AI-generated texts, reader preferences, literary evaluation, HCI, text generation

**Relevance Score:** 6

**TL;DR:** This paper explores how reader preferences shape the evaluation of AI-generated and human-authored literary texts, revealing two distinct reader profiles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand why previous research on AI-generated vs. human-authored literature has produced conflicting results, focusing on reader interpretation and value rather than intrinsic text quality.

**Method:** The authors utilize five public datasets, extracting 17 textual features, and model individual reader preferences to analyze their importance in a shared preference space.

**Key Contributions:**

	1. Identification of two reader profiles: surface-focused and holistic readers
	2. Quantitative explanation of literary quality measurements based on reader preferences
	3. Advocacy for reader-sensitive evaluation frameworks in literature

**Result:** The analysis reveals two reader profiles: 'surface-focused readers' prioritize readability, while 'holistic readers' value thematic depth. The findings show that literary quality assessments are influenced by how text features align with readers' preferences.

**Limitations:** 

**Conclusion:** The results advocate for incorporating reader-sensitive evaluation frameworks in creative text generation assessments, emphasizing the importance of reader interpretation in quality evaluation.

**Abstract:** Recent studies comparing AI-generated and human-authored literary texts have produced conflicting results: some suggest AI already surpasses human quality, while others argue it still falls short. We start from the hypothesis that such divergences can be largely explained by genuine differences in how readers interpret and value literature, rather than by an intrinsic quality of the texts evaluated. Using five public datasets (1,471 stories, 101 annotators including critics, students, and lay readers), we (i) extract 17 reference-less textual features (e.g., coherence, emotional variance, average sentence length...); (ii) model individual reader preferences, deriving feature importance vectors that reflect their textual priorities; and (iii) analyze these vectors in a shared "preference space". Reader vectors cluster into two profiles: 'surface-focused readers' (mainly non-experts), who prioritize readability and textual richness; and 'holistic readers' (mainly experts), who value thematic development, rhetorical variety, and sentiment dynamics. Our results quantitatively explain how measurements of literary quality are a function of how text features align with each reader's preferences. These findings advocate for reader-sensitive evaluation frameworks in the field of creative text generation.

</details>


### [26] [The Reader is the Metric: How Textual Features and Reader Profiles Explain Conflicting Evaluations of AI Creative Writing](https://arxiv.org/abs/2506.03310)

*Guillermo Marco, Julio Gonzalo, Víctor Fresno*

**Main category:** cs.CL

**Keywords:** AI-generated texts, literary quality, reader preferences, creative text generation, evaluation frameworks

**Relevance Score:** 6

**TL;DR:** This study explores the divergence in evaluations of AI-generated versus human-authored literary texts, attributing differences to reader interpretation and value preferences rather than text quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why evaluations of AI-generated and human-written texts yield conflicting results, focusing on reader interpretation and value frameworks.

**Method:** Analyzed 1,471 stories with 101 annotators, extracting 17 textual features and modeling reader preferences to evaluate literary quality against individual priorities.

**Key Contributions:**

	1. Identification of two distinct reader profiles in literary evaluation
	2. Quantitative analysis linking text features with reader preferences
	3. Advocacy for reader-sensitive frameworks in text quality assessment

**Result:** Two reader profiles emerged: 'surface-focused' readers who value readability, and 'holistic' readers who prioritize thematic and rhetorical qualities. This analysis quantitatively links literary quality assessments with reader preferences.

**Limitations:** 

**Conclusion:** The findings suggest the need for reader-sensitive evaluation frameworks in creative text generation to better account for individual preferences in literary quality assessment.

**Abstract:** Recent studies comparing AI-generated and human-authored literary texts have produced conflicting results: some suggest AI already surpasses human quality, while others argue it still falls short. We start from the hypothesis that such divergences can be largely explained by genuine differences in how readers interpret and value literature, rather than by an intrinsic quality of the texts evaluated. Using five public datasets (1,471 stories, 101 annotators including critics, students, and lay readers), we (i) extract 17 reference-less textual features (e.g., coherence, emotional variance, average sentence length...); (ii) model individual reader preferences, deriving feature importance vectors that reflect their textual priorities; and (iii) analyze these vectors in a shared "preference space". Reader vectors cluster into two profiles: 'surface-focused readers' (mainly non-experts), who prioritize readability and textual richness; and 'holistic readers' (mainly experts), who value thematic development, rhetorical variety, and sentiment dynamics. Our results quantitatively explain how measurements of literary quality are a function of how text features align with each reader's preferences. These findings advocate for reader-sensitive evaluation frameworks in the field of creative text generation.

</details>


### [27] [Cross-Platform Violence Detection on Social Media: A Dataset and Analysis](https://arxiv.org/abs/2506.03312)

*Celia Chen, Scotty Beland, Ingo Burghardt, Jill Byczek, William J. Conway, Eric Cotugno, Sadaf Davre, Megan Fletcher, Rajesh Kumar Gnanasekaran, Kristin Hamilton, Marilyn Harbert, Jordan Heustis, Tanaya Jha, Emily Klein, Hayden Kramer, Alex Leitch, Jessica Perkins, Casi Sherman, Celia Sterrn, Logan Stevens, Rebecca Zarrella, Jennifer Golbeck*

**Main category:** cs.CL

**Keywords:** violent threats, social media, machine learning, dataset, content classification

**Relevance Score:** 7

**TL;DR:** The paper presents a dataset of 30,000 hand-coded posts for violent threats and evaluates machine learning classification accuracy across different platforms.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To create high-quality data that aids in the understanding and detection of violent content on social media.

**Method:** A cross-platform dataset of 30,000 posts was hand-coded for violent threats and analyzed using machine learning in comparison to an existing dataset from YouTube.

**Key Contributions:**

	1. Introduction of a large cross-platform dataset for violent threats
	2. High classification accuracy between datasets
	3. Insights into content-classification strategies for violent content

**Result:** High classification accuracy was achieved when training on one dataset and testing on another, indicating effectiveness in identifying violent content across different platforms.

**Limitations:** 

**Conclusion:** The findings enhance content-classification strategies and contribute to the broader understanding of violent content on social media.

**Abstract:** Violent threats remain a significant problem across social media platforms. Useful, high-quality data facilitates research into the understanding and detection of malicious content, including violence. In this paper, we introduce a cross-platform dataset of 30,000 posts hand-coded for violent threats and sub-types of violence, including political and sexual violence. To evaluate the signal present in this dataset, we perform a machine learning analysis with an existing dataset of violent comments from YouTube. We find that, despite originating from different platforms and using different coding criteria, we achieve high classification accuracy both by training on one dataset and testing on the other, and in a merged dataset condition. These results have implications for content-classification strategies and for understanding violent content across social media.

</details>


### [28] [Ask a Local: Detecting Hallucinations With Specialized Model Divergence](https://arxiv.org/abs/2506.03357)

*Aldan Creo, Héctor Cerezo-Costas, Pedro Alonso-Doval, Maximiliano Hormazábal-Lagos*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, multilingual processing

**Relevance Score:** 8

**TL;DR:** A novel method called 'Ask a Local' is introduced for detecting hallucinations in large language models by evaluating perplexity distributions of specialized models in multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenge of hallucinations in large language models, where they generate factually incorrect information.

**Method:** The method calculates divergence between perplexity distributions of language-specialized models to detect potentially hallucinated information.

**Key Contributions:**

	1. Introduction of 'Ask a Local' for hallucination detection
	2. Efficiency in multilingual applications without adaptation
	3. Strong performance in specific languages like Italian and Catalan

**Result:** The approach shows consistent performance across 14 languages, particularly excelling in Italian and Catalan, with notable IoU scores and Spearman correlation values.

**Limitations:** 

**Conclusion:** The method is effective in multilingual contexts without needing language-specific adaptations and the authors release their code for further research.

**Abstract:** Hallucinations in large language models (LLMs) - instances where models generate plausible but factually incorrect information - present a significant challenge for AI.   We introduce "Ask a Local", a novel hallucination detection method exploiting the intuition that specialized models exhibit greater surprise when encountering domain-specific inaccuracies. Our approach computes divergence between perplexity distributions of language-specialized models to identify potentially hallucinated spans. Our method is particularly well-suited for a multilingual context, as it naturally scales to multiple languages without the need for adaptation, relying on external data sources, or performing training. Moreover, we select computationally efficient models, providing a scalable solution that can be applied to a wide range of languages and domains.   Our results on a human-annotated question-answer dataset spanning 14 languages demonstrate consistent performance across languages, with Intersection-over-Union (IoU) scores around 0.3 and comparable Spearman correlation values. Our model shows particularly strong performance on Italian and Catalan, with IoU scores of 0.42 and 0.38, respectively, while maintaining cross-lingual effectiveness without language-specific adaptations. We release our code and architecture to facilitate further research in multilingual hallucination detection.

</details>


### [29] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)

*Junling Wang, Anna Rutkiewicz, April Yi Wang, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** Math Word Problems, Visual Generation, Education Technology, Text-to-Image Models, Human-Computer Interaction

**Relevance Score:** 6

**TL;DR:** Math2Visual is an automated framework for generating educational visuals from math word problems (MWPs) text descriptions, aimed at improving the teaching of mathematics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Automated methods for creating visuals for math word problems are lacking despite their importance in helping young learners interpret and solve problems.

**Method:** Math2Visual utilizes a pre-defined visual language and design space based on teacher interviews, constructing an annotated dataset of 1,903 visuals to evaluate and fine-tune Text-to-Image models for educational purposes.

**Key Contributions:**

	1. Introduction of the Math2Visual framework for generating educational visuals from text
	2. Creation of a large annotated dataset of visuals for evaluating TTI models
	3. Establishment of benchmarks and insights into challenges in multimodal educational content

**Result:** The framework improves the generation of educational visuals from MWPs, establishing a new benchmark for automated creation while addressing challenges in multimodal content.

**Limitations:** The study identifies challenges such as misrepresentation of mathematical relationships and omission of essential visual elements in generated visuals.

**Conclusion:** Math2Visual enhances the educational process by automating visual generation for math problems and provides insights into the challenges of creating effective educational visuals.

**Abstract:** Visuals are valuable tools for teaching math word problems (MWPs), helping young learners interpret textual descriptions into mathematical expressions before solving them. However, creating such visuals is labor-intensive and there is a lack of automated methods to support this process. In this paper, we present Math2Visual, an automatic framework for generating pedagogically meaningful visuals from MWP text descriptions. Math2Visual leverages a pre-defined visual language and a design space grounded in interviews with math teachers, to illustrate the core mathematical relationships in MWPs. Using Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate Text-to-Image (TTI) models for their ability to generate visuals that align with our design. We further fine-tune several TTI models with our dataset, demonstrating improvements in educational visual generation. Our work establishes a new benchmark for automated generation of pedagogically meaningful visuals and offers insights into key challenges in producing multimodal educational content, such as the misrepresentation of mathematical relationships and the omission of essential visual elements.

</details>


### [30] [A Multimodal, Multilingual, and Multidimensional Pipeline for Fine-grained Crowdsourcing Earthquake Damage Evaluation](https://arxiv.org/abs/2506.03360)

*Zihui Ma, Lingyao Li, Juan Li, Wenyue Hua, Jingxiao Liu, Qingyuan Feng, Yuki Miura*

**Main category:** cs.CL

**Keywords:** disaster assessment, multimodal large language models, social media, earthquake analysis, real-time crisis management

**Relevance Score:** 6

**TL;DR:** This paper proposes a 3M pipeline using multimodal large language models for rapid disaster damage assessment from social media data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective emergency response requires rapid and fine-grained disaster damage assessment, but traditional methods face challenges due to limited sensors and delayed reporting.

**Method:** The study introduces a structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that utilizes multimodal large language models to analyze data from social media during two major earthquake events.

**Key Contributions:**

	1. Introduction of a 3M pipeline for disaster assessment
	2. Demonstrated effectiveness of MLLMs in integrating multimodal data
	3. Released code and data for further research

**Result:** MLLMs were found to effectively integrate image and text signals, showing a strong correlation with seismic data, although performance varied by language and input type.

**Limitations:** Performance varies based on language and input modality, which may affect overall assessment accuracy.

**Conclusion:** The research demonstrates the potential of MLLMs for disaster assessment and provides a foundation for future research in real-time crisis applications.

**Abstract:** Rapid, fine-grained disaster damage assessment is essential for effective emergency response, yet remains challenging due to limited ground sensors and delays in official reporting. Social media provides a rich, real-time source of human-centric observations, but its multimodal and unstructured nature presents challenges for traditional analytical methods. In this study, we propose a structured Multimodal, Multilingual, and Multidimensional (3M) pipeline that leverages multimodal large language models (MLLMs) to assess disaster impacts. We evaluate three foundation models across two major earthquake events using both macro- and micro-level analyses. Results show that MLLMs effectively integrate image-text signals and demonstrate a strong correlation with ground-truth seismic data. However, performance varies with language, epicentral distance, and input modality. This work highlights the potential of MLLMs for disaster assessment and provides a foundation for future research in applying MLLMs to real-time crisis contexts. The code and data are released at: https://github.com/missa7481/EMNLP25_earthquake

</details>


### [31] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)

*Mikel K. Ngueajio, Flor Miriam Plaza-del-Arco, Yi-Ling Chung, Danda B. Rawat, Amanda Cercas Curry*

**Main category:** cs.CL

**Keywords:** automated counter-narratives, hate speech, LLMs, ethical implications, accessibility

**Relevance Score:** 7

**TL;DR:** This paper evaluates the effectiveness of Large Language Model-generated counter-narratives for online hate speech, highlighting concerns about accessibility and emotional tone.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing issue of online hate speech through automated counter-narratives, while examining their ethical implications and effectiveness.

**Method:** A framework is proposed to assess LLM-generated counter-narratives based on persona framing, verbosity, readability, affective tone, and ethical robustness, using three different LLMs and datasets.

**Key Contributions:**

	1. A comprehensive evaluation framework for LLM-generated counter-narratives.
	2. Comparative analysis of different LLMs on hate speech datasets.
	3. Insights into the balance between emotional tone and ethical considerations in generated content.

**Result:** The study found that LLM-generated counter-narratives are often verbose and tailored for higher literacy levels, being less accessible to the general public. Emotional prompts improve empathy but raise safety concerns.

**Limitations:** Lack of accessibility due to literacy level requirements and unresolved safety concerns.

**Conclusion:** While LLMs can create counter-narratives, issues with verbosity and accessibility need to be addressed to ensure effectiveness in real-world applications.

**Abstract:** Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets. Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness.

</details>


### [32] [Trajectory Prediction Meets Large Language Models: A Survey](https://arxiv.org/abs/2506.03408)

*Yi Xu, Ruining Yang, Yitian Zhang, Yizhou Wang, Jianglin Lu, Mingyuan Zhang, Lili Su, Yun Fu*

**Main category:** cs.CL

**Keywords:** large language models, trajectory prediction, natural language processing, autonomous systems, data generation

**Relevance Score:** 9

**TL;DR:** This survey explores the integration of large language models (LLMs) into trajectory prediction across five key areas, analyzing methods and identifying challenges.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge natural language processing and trajectory prediction by leveraging the semantic and reasoning capabilities of LLMs.

**Method:** The paper categorizes recent work into five directions: trajectory prediction via language modeling, direct prediction with pretrained LLMs, language-guided scene understanding, language-driven data generation, and language-based reasoning.

**Key Contributions:**

	1. Comprehensive overview of the intersection of LLMs and trajectory prediction.
	2. Categorization of methods into five distinct areas.
	3. Identification of core challenges and design choices in the field.

**Result:** It highlights representative methods within these categories, core design choices, and identifies open challenges in the field.

**Limitations:** 

**Conclusion:** The survey provides a unified perspective on how language can enhance trajectory prediction in autonomous systems.

**Abstract:** Recent advances in large language models (LLMs) have sparked growing interest in integrating language-driven techniques into trajectory prediction. By leveraging their semantic and reasoning capabilities, LLMs are reshaping how autonomous systems perceive, model, and predict trajectories. This survey provides a comprehensive overview of this emerging field, categorizing recent work into five directions: (1) Trajectory prediction via language modeling paradigms, (2) Direct trajectory prediction with pretrained language models, (3) Language-guided scene understanding for trajectory prediction, (4) Language-driven data generation for trajectory prediction, (5) Language-based reasoning and interpretability for trajectory prediction. For each, we analyze representative methods, highlight core design choices, and identify open challenges. This survey bridges natural language processing and trajectory prediction, offering a unified perspective on how language can enrich trajectory prediction.

</details>


### [33] [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)

*Meiqing Jin, Liam Dugan, Chris Callison-Burch*

**Main category:** cs.CL

**Keywords:** large language models, language learning, controllable generation, output comprehensibility, Token Miss Rate

**Relevance Score:** 8

**TL;DR:** This paper explores how controllable generation techniques can make large language models more suitable for beginner language learners by adapting the complexity of output text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There's a need to improve language learning tools for absolute beginners who struggle with the complexity of LLM outputs.

**Method:** The authors investigate modular controllable generation methods that do not require fine-tuning the language models, evaluating effectiveness through automatic metrics and a user study focused on Japanese language learners.

**Key Contributions:**

	1. Demonstration of controllable generation techniques for beginner language learners.
	2. Development of the Token Miss Rate (TMR) evaluation metric.
	3. Release of code, models, annotation tools, and dataset for further research.

**Result:** The study finds that traditional prompting methods are inadequate for controlling output difficulty, but using future discriminators substantially increases comprehensibility from 40.4% to 84.3%.

**Limitations:** 

**Conclusion:** The introduction of a novel token-level evaluation metric, Token Miss Rate (TMR), provides a strong correlation with human understanding and can guide future research in AI-supported language learning.

**Abstract:** Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for beginner learners (CEFR: A1-A2). In this paper, we investigate whether controllable generation techniques -- specifically modular methods that do not require model fine-tuning -- can adapt LLM outputs to better support absolute beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails to control output difficulty, the use of future discriminators (Yang and Klein, 2021) significantly improves output comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset.

</details>


### [34] [DistRAG: Towards Distance-Based Spatial Reasoning in LLMs](https://arxiv.org/abs/2506.03424)

*Nicole R Schneider, Nandini Ramachandran, Kent O'Sullivan, Hanan Samet*

**Main category:** cs.CL

**Keywords:** Large Language Models, spatial reasoning, geodesic distance, point of interest, itinerary planning

**Relevance Score:** 8

**TL;DR:** DistRAG enhances LLMs' spatial reasoning by integrating geodesic distance retrieval to improve distance-based question answering.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the spatial reasoning capabilities of LLMs for tasks like POI recommendation and itinerary planning.

**Method:** DistRAG encodes geodesic distances between cities and towns in a graph, retrieving a relevant context subgraph to answer distance-based questions.

**Key Contributions:**

	1. Development of DistRAG for spatial reasoning in LLMs
	2. Integration of geodesic distance information for improved question answering
	3. Establishment of a flexible method to create a rudimentary world model for LLMs.

**Result:** The method enables LLMs to answer previously unanswerable distance reasoning questions by utilizing external spatial information.

**Limitations:** 

**Conclusion:** DistRAG serves as a foundational step toward equipping LLMs with a basic world model that supplements their linguistic knowledge.

**Abstract:** Many real world tasks where Large Language Models (LLMs) can be used require spatial reasoning, like Point of Interest (POI) recommendation and itinerary planning. However, on their own LLMs lack reliable spatial reasoning capabilities, especially about distances. To address this problem, we develop a novel approach, DistRAG, that enables an LLM to retrieve relevant spatial information not explicitly learned during training. Our method encodes the geodesic distances between cities and towns in a graph and retrieves a context subgraph relevant to the question. Using this technique, our method enables an LLM to answer distance-based reasoning questions that it otherwise cannot answer. Given the vast array of possible places an LLM could be asked about, DistRAG offers a flexible first step towards providing a rudimentary `world model' to complement the linguistic knowledge held in LLMs.

</details>


### [35] [Time Course MechInterp: Analyzing the Evolution of Components and Knowledge in Large Language Models](https://arxiv.org/abs/2506.03434)

*Ahmad Dawar Hakimi, Ali Modarressi, Philipp Wicke, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** large language models, knowledge representation, attention heads, feed forward networks, adaptive learning

**Relevance Score:** 8

**TL;DR:** The paper analyzes how the OLMo-7B model represents factual knowledge throughout its pre-training phase, focusing on the roles and stability of attention heads and feed forward networks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the acquisition and storage of factual knowledge in LLMs is essential for improving their interpretability and reliability.

**Method:** The study categorizes attention heads and feed forward networks into four roles and tracks their evolution over the pre-training of the OLMo-7B model.

**Key Contributions:**

	1. Classification of roles of model components in LLMs
	2. Insights into adaptive learning processes in LLM training
	3. Demonstration of how relations' complexity affects knowledge acquisition

**Result:** The findings reveal that LLM components initially rely on general-purpose capabilities and then become more specialized, with evidence of adaptive learning as components repurpose for reliable answer prediction.

**Limitations:** 

**Conclusion:** Attention heads show significant turnover while FFNs demonstrate stability, with certain relations converging to accuracy earlier based on task complexity, providing insights into LLM knowledge formation dynamics.

**Abstract:** Understanding how large language models (LLMs) acquire and store factual knowledge is crucial for enhancing their interpretability and reliability. In this work, we analyze the evolution of factual knowledge representation in the OLMo-7B model by tracking the roles of its attention heads and feed forward networks (FFNs) over the course of pre-training. We classify these components into four roles: general, entity, relation-answer, and fact-answer specific, and examine their stability and transitions. Our results show that LLMs initially depend on broad, general-purpose components, which later specialize as training progresses. Once the model reliably predicts answers, some components are repurposed, suggesting an adaptive learning process. Notably, attention heads display the highest turnover. We also present evidence that FFNs remain more stable throughout training. Furthermore, our probing experiments reveal that location-based relations converge to high accuracy earlier in training than name-based relations, highlighting how task complexity shapes acquisition dynamics. These insights offer a mechanistic view of knowledge formation in LLMs.

</details>


### [36] [Culture Matters in Toxic Language Detection in Persian](https://arxiv.org/abs/2506.03458)

*Zahra Bokaei, Walid Magdy, Bonnie Webber*

**Main category:** cs.CL

**Keywords:** toxic language detection, transfer learning, Persian language, cultural context, NLP

**Relevance Score:** 6

**TL;DR:** This paper explores toxic language detection in Persian using various methods and highlights the influence of cultural context on transfer learning effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance online safety by improving toxic language detection, particularly in under-researched languages like Persian.

**Method:** Comparison of methods including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning.

**Key Contributions:**

	1. Comprehensive evaluation of toxic language detection methods in Persian.
	2. Insight into the role of cultural similarity in transfer learning outcomes.
	3. First extensive study on toxic language detection in the Persian language.

**Result:** Finding that transfer learning from culturally similar languages yields better results for toxic language detection in Persian than from culturally distinct languages.

**Limitations:** Focuses mainly on Persian; findings may not generalize to all languages or contexts.

**Conclusion:** Emphasizes the importance of cultural context in effective transfer learning for natural language processing tasks like toxic language detection.

**Abstract:** Toxic language detection is crucial for creating safer online environments and limiting the spread of harmful content. While toxic language detection has been under-explored in Persian, the current work compares different methods for this task, including fine-tuning, data enrichment, zero-shot and few-shot learning, and cross-lingual transfer learning. What is especially compelling is the impact of cultural context on transfer learning for this task: We show that the language of a country with cultural similarities to Persian yields better results in transfer learning. Conversely, the improvement is lower when the language comes from a culturally distinct country. Warning: This paper contains examples of toxic language that may disturb some readers. These examples are included for the purpose of research on toxic detection.

</details>


### [37] [Delta-KNN: Improving Demonstration Selection in In-Context Learning for Alzheimer's Disease Detection](https://arxiv.org/abs/2506.03476)

*Chuyuan Li, Raymond Li, Thalia S. Field, Giuseppe Carenini*

**Main category:** cs.CL

**Keywords:** Alzheimer's Disease, Large Language Models, In-Context Learning, Delta-KNN, Health Informatics

**Relevance Score:** 9

**TL;DR:** The paper explores using Large Language Models (LLMs) for diagnosing Alzheimer's Disease from patient-generated text through a novel in-context learning method called Delta-KNN.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective early intervention methods for Alzheimer's Disease, particularly through the analysis of linguistic patterns in patient text.

**Method:** The authors introduce Delta-KNN, a new strategy that uses a delta score to improve the selection of training examples and enhances in-context learning performance for AD diagnosis.

**Key Contributions:**

	1. Introduction of Delta-KNN for better demonstration selection
	2. Empirical validation showing superiority over traditional ICL methods
	3. Achieving state-of-the-art results in Alzheimer's Disease diagnosis using LLMs

**Result:** Delta-KNN consistently outperforms conventional in-context learning baselines and achieves state-of-the-art results on AD detection datasets when using the Llama-3.1 model.

**Limitations:** 

**Conclusion:** The introduction of Delta-KNN significantly improves the performance of LLMs in diagnosing Alzheimer's Disease from text, suggesting that advanced selection strategies can enhance healthcare applications of AI.

**Abstract:** Alzheimer's Disease (AD) is a progressive neurodegenerative disorder that leads to dementia, and early intervention can greatly benefit from analyzing linguistic abnormalities. In this work, we explore the potential of Large Language Models (LLMs) as health assistants for AD diagnosis from patient-generated text using in-context learning (ICL), where tasks are defined through a few input-output examples. Empirical results reveal that conventional ICL methods, such as similarity-based selection, perform poorly for AD diagnosis, likely due to the inherent complexity of this task. To address this, we introduce Delta-KNN, a novel demonstration selection strategy that enhances ICL performance. Our method leverages a delta score to assess the relative gains of each training example, coupled with a KNN-based retriever that dynamically selects optimal "representatives" for a given input. Experiments on two AD detection datasets across three open-source LLMs demonstrate that Delta-KNN consistently outperforms existing ICL baselines. Notably, when using the Llama-3.1 model, our approach achieves new state-of-the-art results, surpassing even supervised classifiers.

</details>


### [38] [APT: Improving Specialist LLM Performance with Weakness Case Acquisition and Iterative Preference Training](https://arxiv.org/abs/2506.03483)

*Jun Rao, Zepeng Lin, Xuebo Liu, Xiaopeng Ke, Lian Lian, Dong Jin, Shengjun Cheng, Jun Yu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Domain-specific fine-tuning, Machine Learning, Human-Computer Interaction, Performance enhancement

**Relevance Score:** 9

**TL;DR:** APT enhances domain-specific performance of LLMs while maintaining general capabilities by using self-generated dis-preferred weakness data for targeted training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve domain-specific performance of LLMs without degrading their general capabilities.

**Method:** A novel approach named APT focuses on training using only samples where errors occur, paired with similar samples.

**Key Contributions:**

	1. Introduction of APT for targeted training of LLMs
	2. Use of self-generated dis-preferred weakness data
	3. Demonstration of superior performance without sacrificing general capabilities

**Result:** APT shows no reduction in generic capacity and achieves superior performance on downstream tasks on LLama-2 and Mistral-V0.3 models across various benchmarks.

**Limitations:** 

**Conclusion:** APT is validated as an effective strategy for balancing domain-specific enhancements and general applicability of LLMs.

**Abstract:** Large Language Models (LLMs) often require domain-specific fine-tuning to address targeted tasks, which risks degrading their general capabilities. Maintaining a balance between domain-specific enhancements and general model utility is a key challenge. This paper proposes a novel approach named APT (Weakness Case Acquisition and Iterative Preference Training) to enhance domain-specific performance with self-generated dis-preferred weakness data (bad cases and similar cases). APT uniquely focuses on training the model using only those samples where errors occur, alongside a small, similar set of samples retrieved for this purpose. This targeted training minimizes interference with the model's existing knowledge base, effectively retaining generic capabilities. Experimental results on the LLama-2 and Mistral-V0.3 models across various benchmarks demonstrate that APT ensures no reduction in generic capacity and achieves superior performance on downstream tasks compared to various existing methods. This validates our method as an effective strategy for enhancing domain-specific capabilities without sacrificing the model's broader applicability.

</details>


### [39] [Explainable AI: XAI-Guided Context-Aware Data Augmentation](https://arxiv.org/abs/2506.03484)

*Melkamu Abay Mersha, Mesay Gemeda Yigezu, Atnafu Lambebo Tonja, Hassan Shakil, Samer Iskander, Olga Kolesnikova, Jugal Kalita*

**Main category:** cs.CL

**Keywords:** Explainable AI, Data Augmentation, Low-resource languages, NLP, Model performance

**Relevance Score:** 6

**TL;DR:** This paper proposes an XAI-guided framework for data augmentation that improves AI model performance by selectively modifying features based on explainability insights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of scarcity of labeled data in developing robust AI models, especially for low-resource languages, and the limitations of conventional data augmentation techniques.

**Method:** A novel framework called XAI-Guided Context-Aware Data Augmentation is introduced, employing XAI techniques to modify less critical features while preserving task-relevant ones through an iterative feedback loop.

**Key Contributions:**

	1. Introduces XAI-guided data augmentation for low-resource languages
	2. Implements an iterative feedback loop based on explainability
	3. Demonstrates significant performance improvements in specific NLP tasks

**Result:** Experimental results show significant improvements in model accuracy for hate speech and sentiment analysis tasks on the Amharic dataset, outperforming existing augmentation techniques.

**Limitations:** 

**Conclusion:** The study demonstrates that the proposed methods provide a more controlled and interpretable solution for data augmentation, significantly enhancing AI model training.

**Abstract:** Explainable AI (XAI) has emerged as a powerful tool for improving the performance of AI models, going beyond providing model transparency and interpretability. The scarcity of labeled data remains a fundamental challenge in developing robust and generalizable AI models, particularly for low-resource languages. Conventional data augmentation techniques introduce noise, cause semantic drift, disrupt contextual coherence, lack control, and lead to overfitting. To address these challenges, we propose XAI-Guided Context-Aware Data Augmentation. This novel framework leverages XAI techniques to modify less critical features while selectively preserving most task-relevant features. Our approach integrates an iterative feedback loop, which refines augmented data over multiple augmentation cycles based on explainability-driven insights and the model performance gain. Our experimental results demonstrate that XAI-SR-BT and XAI-PR-BT improve the accuracy of models on hate speech and sentiment analysis tasks by 6.6% and 8.1%, respectively, compared to the baseline, using the Amharic dataset with the XLM-R model. XAI-SR-BT and XAI-PR-BT outperform existing augmentation techniques by 4.8% and 5%, respectively, on the same dataset and model. Overall, XAI-SR-BT and XAI-PR-BT consistently outperform both baseline and conventional augmentation techniques across all tasks and models. This study provides a more controlled, interpretable, and context-aware solution to data augmentation, addressing critical limitations of existing augmentation techniques and offering a new paradigm shift for leveraging XAI techniques to enhance AI model training.

</details>


### [40] [EpiCoDe: Boosting Model Performance Beyond Training with Extrapolation and Contrastive Decoding](https://arxiv.org/abs/2506.03489)

*Mingxu Tao, Jie Hu, Mingchuan Yang, Yunhuai Liu, Dongyan Zhao, Yansong Feng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data-scarcity, Contrastive Decoding, Model Extrapolation, Performance Enhancement

**Relevance Score:** 8

**TL;DR:** Introducing EpiCoDe, a novel method that enhances LLM performance in data-scarcity scenarios without the need for additional training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve model performance in scenarios where annotated data is limited, addressing the challenges posed by high costs of data acquisition.

**Method:** The method employs model extrapolation to enhance a finetuned model with its inferior version, and uses contrastive decoding to minimize predicted errors by comparing logit scores.

**Key Contributions:**

	1. Introduction of the EpiCoDe method for enhancing LLM performance in data-scarcity
	2. Establishment of a new theoretical framework for understanding contrastive decoding effects
	3. Empirical validation across multiple tasks and models showing significant improvement.

**Result:** EpiCoDe consistently outperforms existing methods across three tasks and four different LLMs, demonstrating significant and robust improvement.

**Limitations:** 

**Conclusion:** A theoretical framework is proposed to explain the effectiveness of contrastive decoding in data-scarcity scenarios, as validated by the experiments conducted.

**Abstract:** The remarkable performance of Large language models (LLMs) relies heavily on the availability of abundant high-quality training data. However, the high cost of acquiring annotated data often prevents models from obtaining capabilities to tackle downstream tasks. In this paper, we introduce a novel method, EpiCoDe that boosts model performance in data-scarcity scenarios without extra training. We first employ model extrapolation to enhance a finetuned model with its inferior version, and then adopt contrastive decoding to further reduce predicted errors, by comparing the logit scores given by the extrapolated and the vanilla finetuned model. Experiments across three tasks over four different LLMs show that EpiCoDe consistently outperforms existing methods with significant and robust improvement. We also propose a new theoretical framework to reveal the mechanism behind contrastive decoding in data-scarcity scenarios, which further helps us better understand the effectiveness of EpiCoDe.

</details>


### [41] [Beyond Memorization: A Rigorous Evaluation Framework for Medical Knowledge Editing](https://arxiv.org/abs/2506.03490)

*Shigeng Chen, Linhao Luo, Zhangchi Qiu, Yanan Cao, Carl Yang, Shirui Pan*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, medical informatics, Machine Learning, evaluation framework

**Relevance Score:** 9

**TL;DR:** This paper presents MedEditBench, a framework for evaluating knowledge editing methods in the medical domain, addressing the limitations of current approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective knowledge editing methods in Large Language Models (LLMs) specifically for the complex medical domain, where generalization to unseen scenarios is crucial.

**Method:** The authors introduce a novel framework called MedEditBench, which includes a medical knowledge editing benchmark and three different editing paradigms to assess knowledge sources.

**Key Contributions:**

	1. Introduction of MedEditBench for evaluating KE methods in medicine
	2. Development of Self-Generated Rationale Editing (SGR-Edit)
	3. Insights on localization of medical knowledge in LLMs and the effects of sequential editing.

**Result:** The study finds that current knowledge editing methods only achieve superficial memorization of facts and do not generalize well to new scenarios. The proposed SGR-Edit method shows significant improvements by using model-derived rationales for editing.

**Limitations:** Current KE methods tend to result in superficial memorization without effective generalization to new scenarios.

**Conclusion:** The paper provides insights into the challenges of medical knowledge editing, and recommends SGR-Edit as a more effective approach for real-world medical applications.

**Abstract:** Recently, knowledge editing (KE) has emerged as a promising approach to update specific facts in Large Language Models (LLMs) without the need for full retraining. Despite the effectiveness in general-domain benchmarks, their applicability to complex medical domain remains largely unexplored. Medical knowledge editing is particularly challenging, as it requires LLMs to internalize the knowledge and generalize to unseen scenarios for effective and interpretable decision-making. In this work, we propose a novel framework called MedEditBench to rigorously evaluate the effectiveness of existing KE methods in the medical domain. In MedEditBench, we introduce a new medical knowledge editing benchmark as well as three different knowledge editing paradigms, which are designed to assess the impact of different knowledge sources for editing. Our findings indicate that current KE methods result in only superficial memorization of the injected information, failing to generalize to new scenarios. To overcome this limitation, we present Self-Generated Rationale Editing (SGR-Edit), which utilizes model-derived rationales as the target knowledge for editing, thereby uncovering the underlying reasoning process and demonstrating significant improvements over existing KE approaches. Additionally, we offer deeper insights into medical knowledge editing, including the localization of medical knowledge in LLMs and the impact of sequential editing on evolving knowledge. This could provide practical guidance for implementing KE methods in real-world medical applications.

</details>


### [42] [Measuring Human Involvement in AI-Generated Text: A Case Study on Academic Writing](https://arxiv.org/abs/2506.03501)

*Yuchen Guo, Zhicheng Dou, Huy H. Nguyen, Ching-Chun Chang, Saku Sugawara, Isao Echizen*

**Main category:** cs.CL

**Keywords:** AI-generated text, human involvement, BERTScore, multi-task learning, RoBERTa

**Relevance Score:** 8

**TL;DR:** This paper addresses the limitations of binary classification in detecting human involvement in AI-generated text, proposing a multi-task RoBERTa-based regressor and BERTScore for improved measurement.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the detection of human involvement in content creation using generative AI, as current methods lack robustness and overlook the nuances of human-machine collaboration.

**Method:** A multi-task RoBERTa-based regressor was trained on a token classification task, using BERTScore as a metric to measure the nuances of human involvement in AI-generated text.

**Key Contributions:**

	1. Introduced BERTScore for measuring human involvement in AI text generation.
	2. Developed a multi-task RoBERTa-based regressor for nuanced detection.
	3. Showed significant performance improvement over traditional binary classification methods.

**Result:** The proposed method achieved an F1 score of 0.9423 and a mean squared error of 0.004, demonstrating effectiveness in detecting human involvement across generative models.

**Limitations:** 

**Conclusion:** The paper presents a more reliable approach to detect varying levels of human involvement in AI-generated texts compared to existing binary classification methods, highlighting its utility in academic scenarios.

**Abstract:** Content creation has dramatically progressed with the rapid advancement of large language models like ChatGPT and Claude. While this progress has greatly enhanced various aspects of life and work, it has also negatively affected certain areas of society. A recent survey revealed that nearly 30% of college students use generative AI to help write academic papers and reports. Most countermeasures treat the detection of AI-generated text as a binary classification task and thus lack robustness. This approach overlooks human involvement in the generation of content even though human-machine collaboration is becoming mainstream. Besides generating entire texts, people may use machines to complete or revise texts. Such human involvement varies case by case, which makes binary classification a less than satisfactory approach. We refer to this situation as participation detection obfuscation. We propose using BERTScore as a metric to measure human involvement in the generation process and a multi-task RoBERTa-based regressor trained on a token classification task to address this problem. To evaluate the effectiveness of this approach, we simulated academic-based scenarios and created a continuous dataset reflecting various levels of human involvement. All of the existing detectors we examined failed to detect the level of human involvement on this dataset. Our method, however, succeeded (F1 score of 0.9423 and a regressor mean squared error of 0.004). Moreover, it demonstrated some generalizability across generative models. Our code is available at https://github.com/gyc-nii/CAS-CS-and-dual-head-detector

</details>


### [43] [Accurate Sublayer Pruning for Large Language Models by Exploiting Latency and Tunability Information](https://arxiv.org/abs/2506.03510)

*Seungcheol Park, Sojin Lee, Jongjin Kim, Jinsik Lee, Hyunjik Jo, U Kang*

**Main category:** cs.CL

**Keywords:** large language models, sublayer pruning, performance optimization, latency reduction, machine learning

**Relevance Score:** 8

**TL;DR:** SPRINT is a novel sublayer pruning method for large language models that improves inference speed without sacrificing accuracy by selectively removing redundant sublayers based on latency and tunability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the slow inference speed of large language models that limits their application despite their high performance.

**Method:** SPRINT prunes sublayers iteratively by assessing the latency reduction and tunability of each sublayer to optimize the model's efficiency and effectiveness.

**Key Contributions:**

	1. Introduction of SPRINT, a novel sublayer pruning method for LLMs.
	2. Improved accuracy-speed trade-off compared to existing methods.
	3. A systematic approach considering latency and tunability for pruning decisions.

**Result:** SPRINT provides a superior accuracy-speedup trade-off, achieving up to 23.88% higher accuracy on zero-shot commonsense reasoning benchmarks compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed SPRINT method offers an effective solution to accelerate large language models while maintaining or improving accuracy, making it suitable for real-world applications.

**Abstract:** How can we accelerate large language models(LLMs) without sacrificing accuracy? The slow inference speed of LLMs hinders us to benefit from their remarkable performance in diverse applications. This is mainly because numerous sublayers are stacked together in LLMs. Sublayer pruning compresses and expedites LLMs via removing unnecessary sublayers. However, existing sublayer pruning algorithms are limited in accuracy since they naively select sublayers to prune, overlooking the different characteristics of each sublayer. In this paper, we propose SPRINT (Sublayer PRuning wIth LateNcy and Tunability Information), an accurate sublayer pruning method for LLMs. SPRINT accurately selects a target sublayer to prune by considering 1) the amount of latency reduction after pruning and 2) the tunability of sublayers. SPRINT iteratively prunes redundant sublayers and swiftly tunes the parameters of remaining sublayers. Experiments show that SPRINT achieves the best accuracy-speedup trade-off, exhibiting up to 23.88%p higher accuracy on zero-shot commonsense reasoning benchmarks compared to existing pruning algorithms.

</details>


### [44] [An Efficient Task-Oriented Dialogue Policy: Evolutionary Reinforcement Learning Injected by Elite Individuals](https://arxiv.org/abs/2506.03519)

*Yangyang Zhao, Ben Niu, Libo Qin, Shihan Wang*

**Main category:** cs.CL

**Keywords:** Deep Reinforcement Learning, Evolutionary Algorithms, Dialogue Systems, Exploration-Exploitation, Task-Oriented Dialogue

**Relevance Score:** 7

**TL;DR:** This paper presents a novel approach that combines Evolutionary Algorithms (EA) with Deep Reinforcement Learning (DRL) to enhance dialogue systems, addressing the balance between exploration and exploitation.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges faced by Deep Reinforcement Learning in task-oriented dialogue systems, specifically the balance between exploration and exploitation due to high dimensional state and action spaces.

**Method:** The proposed method combines the global search capabilities of Evolutionary Algorithms with the local optimization of Deep Reinforcement Learning, and introduces an elite individual injection mechanism to improve the efficiency of the solution search.

**Key Contributions:**

	1. Novel integration of EA and DRL for dialogue policy optimization
	2. Introduction of an elite individual injection mechanism to enhance search efficiency
	3. Demonstrated significant improvements in performance across multiple datasets

**Result:** Experiments show significant improvements in the performance of dialogue systems, along with a reduction in exploration time with the elite individual injection mechanism.

**Limitations:** The flexibility of natural language in dialogue tasks complicates the integration of EA and DRL, leading to longer evolutionary times.

**Conclusion:** The integration of Evolutionary Algorithms and Deep Reinforcement Learning shows promise for improving task-oriented dialogue systems, effectively optimizing exploration and exploitation.

**Abstract:** Deep Reinforcement Learning (DRL) is widely used in task-oriented dialogue systems to optimize dialogue policy, but it struggles to balance exploration and exploitation due to the high dimensionality of state and action spaces. This challenge often results in local optima or poor convergence. Evolutionary Algorithms (EAs) have been proven to effectively explore the solution space of neural networks by maintaining population diversity. Inspired by this, we innovatively combine the global search capabilities of EA with the local optimization of DRL to achieve a balance between exploration and exploitation. Nevertheless, the inherent flexibility of natural language in dialogue tasks complicates this direct integration, leading to prolonged evolutionary times. Thus, we further propose an elite individual injection mechanism to enhance EA's search efficiency by adaptively introducing best-performing individuals into the population. Experiments across four datasets show that our approach significantly improves the balance between exploration and exploitation, boosting performance. Moreover, the effectiveness of the EII mechanism in reducing exploration time has been demonstrated, achieving an efficient integration of EA and DRL on task-oriented dialogue policy tasks.

</details>


### [45] [Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems](https://arxiv.org/abs/2502.14019)

*Myra Cheng, Su Lin Blodgett, Alicia DeVrio, Lisa Egede, Alexandra Olteanu*

**Main category:** cs.CL

**Keywords:** text generation, anthropomorphism, interventions, human-computer interaction, AI ethics

**Relevance Score:** 8

**TL;DR:** This paper addresses the issue of anthropomorphic outputs in text generation systems and proposes an inventory of interventions to mitigate their harmful effects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing anthropomorphism of text generation systems, there are growing concerns about the adverse effects, such as over-reliance and emotional dependence on these systems.

**Method:** The study compiles an inventory of interventions based on literature and a crowdsourcing study where participants modified system outputs to be less human-like. A conceptual framework is developed to categorize and evaluate these interventions.

**Key Contributions:**

	1. Compilation of a diverse inventory of interventions to mitigate anthropomorphic effects
	2. Development of a conceptual framework for characterizing interventions
	3. Empirical grounding from a crowdsourcing study on editing outputs

**Result:** An inventory of potential interventions for reducing anthropomorphic behaviors in text generated by AI systems was created, providing a theoretical foundation for assessing their effectiveness.

**Limitations:** 

**Conclusion:** The paper outlines the need for and provides a framework for future research on interventions to address the issues arising from anthropomorphic behavior in AI text generation systems.

**Abstract:** As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also increasingly raised concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourcing study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.

</details>


### [46] [TokAlign: Efficient Vocabulary Adaptation via Token Alignment](https://arxiv.org/abs/2506.03523)

*Chong Li, Jiajun Zhang, Chengqing Zong*

**Main category:** cs.CL

**Keywords:** tokenization, Large Language Models, multilingual NLP, knowledge transfer, TokAlign

**Relevance Score:** 6

**TL;DR:** TokAlign is a method for aligning vocabularies in Large Language Models (LLMs) to improve their efficiency and performance in multilingual contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in tokenization for LLMs, especially in new domains or languages, and enhance knowledge transfer between models.

**Method:** TokAlign learns a one-to-one mapping matrix for token IDs to align source and target vocabularies, rearranges model parameters, and fine-tunes for new vocabularies.

**Key Contributions:**

	1. Introduction of TokAlign method for vocabulary alignment in LLMs
	2. Demonstrated significant reduction in perplexity for LLMs
	3. Showed improved effectiveness in token-level distillation compared to sentence-level methods.

**Result:** Improved multilingual text compression rates and vocabulary initialization for LLMs, reducing perplexity significantly after initialization.

**Limitations:** 

**Conclusion:** TokAlign enables better token-level distillation and enhances model performance with fewer training steps and tokens.

**Abstract:** Tokenization serves as a foundational step for Large Language Models (LLMs) to process text. In new domains or languages, the inefficiency of the tokenizer will slow down the training and generation of LLM. The mismatch in vocabulary also hinders deep knowledge transfer between LLMs like token-level distillation. To mitigate this gap, we propose an efficient method named TokAlign to replace the vocabulary of LLM from the token co-occurrences view, and further transfer the token-level knowledge between models. It first aligns the source vocabulary to the target one by learning a one-to-one mapping matrix for token IDs. Model parameters, including embeddings, are rearranged and progressively fine-tuned for the new vocabulary. Our method significantly improves multilingual text compression rates and vocabulary initialization for LLMs, decreasing the perplexity from 3.4$\text{e}^2$ of strong baseline methods to 1.2$\text{e}^2$ after initialization. Experimental results on models across multiple parameter scales demonstrate the effectiveness and generalization of TokAlign, which costs as few as 5k steps to restore the performance of the vanilla model. After unifying vocabularies between LLMs, token-level distillation can remarkably boost (+4.4% than sentence-level distillation) the base model, costing only 235M tokens.

</details>


### [47] [Seed-Coder: Let the Code Model Curate Data for Itself](https://arxiv.org/abs/2506.03524)

*Yuyu Zhang, Jing Su, Yifan Sun, Chenguang Xi, Xia Xiao, Shen Zheng, Anxiang Zhang, Kaibo Liu, Daoguang Zan, Tao Sun, Jinhua Zhu, Shulin Xin, Dong Huang, Yetao Bai, Lixin Dong, Chao Li, Jianchong Chen, Hanzhi Zhou, Yifan Huang, Guanghan Ning, Xierui Song, Jiaze Chen, Siyao Liu, Kai Shen, Liang Xiang, Yonghui Wu*

**Main category:** cs.CL

**Keywords:** Code Generation, Code Reasoning, Language Models, Human-Computer Interaction, Software Engineering

**Relevance Score:** 8

**TL;DR:** Seed-Coder is a series of open-source LLMs designed to produce code pretraining data with minimal human involvement, achieving state-of-the-art results in various code-related tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of current open-source LLMs that rely heavily on human effort for code pretraining data production, which is costly, subjective, and not scalable.

**Method:** Introduces a model-centric data pipeline that uses LLMs for scoring and filtering code data, followed by supervised fine-tuning and preference optimization for the instruct model, and Long-Chain-of-Thought reinforcement learning for the reasoning model.

**Key Contributions:**

	1. Development of a model-centric data pipeline for code pretraining
	2. Implementation of Long-Chain-of-Thought reinforcement learning for improved reasoning
	3. Achievement of state-of-the-art results among open-source LLMs in code tasks.

**Result:** Seed-Coder achieves state-of-the-art performance among open-source models of similar size and surpasses larger models in code generation, completion, editing, reasoning, and other software engineering tasks.

**Limitations:** 

**Conclusion:** Seed-Coder demonstrates that it is possible to create high-performing code models with reduced human input and improved scalability.

**Abstract:** Code data in large language model (LLM) pretraining is recognized crucial not only for code-related tasks but also for enhancing general intelligence of LLMs. Current open-source LLMs often heavily rely on human effort to produce their code pretraining data, such as employing hand-crafted filtering rules tailored to individual programming languages, or using human-annotated data to train quality filters. However, these approaches are inherently limited in scalability, prone to subjective biases, and costly to extend and maintain across diverse programming languages. To address these challenges, we introduce Seed-Coder, a series of open-source LLMs comprising base, instruct and reasoning models of 8B size, minimizing human involvement in data construction. Our code pretraining data is produced by a model-centric data pipeline, which predominantly leverages LLMs for scoring and filtering code data. The instruct model is further trained via supervised fine-tuning and preference optimization, and the reasoning model leverages Long-Chain-of-Thought (LongCoT) reinforcement learning to improve multi-step code reasoning. Seed-Coder achieves state-of-the-art results among open-source models of similar size and even surpasses some much larger models, demonstrating superior performance in code generation, code completion, code editing, code reasoning, and software engineering tasks.

</details>


### [48] [Go-Browse: Training Web Agents with Structured Exploration](https://arxiv.org/abs/2506.03533)

*Apurva Gandhi, Graham Neubig*

**Main category:** cs.CL

**Keywords:** Digital agents, Web exploration, Language models, Dataset collection, Benchmarking

**Relevance Score:** 5

**TL;DR:** Go-Browse is a method for automating diverse web agent data collection through structured exploration, achieving improved performance on the WebArena benchmark.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Digital agents often struggle to understand their environment, hindering their goal achievement, particularly in unfamiliar web contexts.

**Method:** Go-Browse frames the data collection process as a graph search to enable structured exploration and reuse of information across different episodes.

**Key Contributions:**

	1. Introduction of Go-Browse for structured web exploration
	2. Collection of a novel dataset with diverse web agent interactions
	3. Improved performance benchmarks for sub-10B language models

**Result:** The method successfully collected a dataset consisting of 10K task-solving trajectories and 40K interaction steps, leading to a fine-tuned language model achieving a success rate of 21.7% on the WebArena benchmark.

**Limitations:** 

**Conclusion:** Go-Browse surpasses previous methods, showing improved results on web agent performance benchmarks and providing a robust dataset for future research.

**Abstract:** One of the fundamental problems in digital agents is their lack of understanding of their environment. For instance, a web browsing agent may get lost in unfamiliar websites, uncertain what pages must be visited to achieve its goals. To address this, we propose Go-Browse, a method for automatically collecting diverse and realistic web agent data at scale through structured exploration of web environments. Go-Browse achieves efficient exploration by framing data collection as a graph search, enabling reuse of information across exploration episodes. We instantiate our method on the WebArena benchmark, collecting a dataset of 10K successful task-solving trajectories and 40K interaction steps across 100 URLs. Fine-tuning a 7B parameter language model on this dataset achieves a success rate of 21.7% on the WebArena benchmark, beating GPT-4o mini by 2.4% and exceeding current state-of-the-art results for sub-10B parameter models by 2.9%.

</details>


### [49] [Debate, Reflect, and Distill: Multi-Agent Feedback with Tree-Structured Preference Optimization for Efficient Language Model Enhancement](https://arxiv.org/abs/2506.03541)

*Xiaofeng Zhou, Heyan Huang, Lizi Liao*

**Main category:** cs.CL

**Keywords:** Large Language Models, model distillation, debate framework, NLP benchmarks, Tree-structured Direct Preference Optimization

**Relevance Score:** 8

**TL;DR:** The paper introduces a Debate and Reflect (D&R) framework that enhances the performance of smaller language models through multi-turn debates with stronger teacher models, and applies Tree-structured Direct Preference Optimization (T-DPO) for effective training utilizing debate logs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models (LLMs) face adoption challenges due to high computational demands, necessitating the development of more efficient distillation techniques.

**Method:** The Debate and Reflect (D&R) framework facilitates structured multi-turn debates between smaller and larger models, along with Tree-structured Direct Preference Optimization (T-DPO) to organize and utilize the debate logs for model training.

**Key Contributions:**

	1. Introduction of the Debate and Reflect (D&R) framework for model distillation.
	2. Development of Tree-structured Direct Preference Optimization (T-DPO) for organizing debate interactions.
	3. Demonstrated significant performance improvements on various NLP benchmarks.

**Result:** Empirical evaluations show that the D&R framework significantly boosts the accuracy, robustness, and generalization of smaller models, surpassing traditional distillation methods.

**Limitations:** 

**Conclusion:** The proposed framework and methods lead to substantial improvements in the performance of smaller models in NLP tasks, making them more viable for practical applications.

**Abstract:** Large Language Models (LLMs) continue to set new standards in knowledge-intensive and complex reasoning tasks, yet their high computational demands limit widespread adoption. While distilling large models into smaller ones offers a sustainable solution, current techniques--such as static knowledge distillation, resource-intensive reinforcement learning from human feedback, or limited self-reflection--struggle to yield substantial and lasting performance gains. In this paper, we present a novel Debate and Reflect (D&R) framework that orchestrates multi-turn debates between smaller models and stronger teacher models, eliciting actionable feedback (e.g., error analysis, corrective strategies) to guide student models. Further, we introduce Tree-structured Direct Preference Optimization (T-DPO) to efficiently leverage these debate logs, organizing interactions into a hierarchical format for effective training. Empirical evaluations across diverse NLP benchmarks demonstrate that our approach significantly improves smaller-model accuracy, robustness, and generalization, outperforming conventional baselines by a large margin.

</details>


### [50] [BPO: Revisiting Preference Modeling in Direct Preference Optimization](https://arxiv.org/abs/2506.03557)

*Lin Sun, Chuang Liu, Peng Liu, Bingyang Li, Weijia Lu, Ning Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Direct Preference Optimization, Balanced Preference Optimization

**Relevance Score:** 6

**TL;DR:** Balanced Preference Optimization (BPO) improves Large Language Models' alignment with human preferences by addressing the issue of Degraded Chosen Responses (DCR) in Direct Preference Optimization (DPO).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of Direct Preference Optimization (DPO) in preserving absolute reward magnitudes and to prevent Degraded Chosen Responses (DCR) that lead to poor model performance.

**Method:** Balanced Preference Optimization (BPO) introduces a dynamic balance in the optimization process through balanced reward margin and gap adaptor without adding constraints to the loss function.

**Key Contributions:**

	1. Introduction of Balanced Preference Optimization (BPO) framework
	2. Dynamic balance of optimization for chosen and rejected responses
	3. Higher accuracy in LLM performance on reasoning tasks compared to DPO and its variants

**Result:** BPO significantly surpasses DPO and its variants in multiple mathematical reasoning tasks, improving model accuracy by rates up to +11.7% and requiring only a minimal code change for implementation.

**Limitations:** 

**Conclusion:** BPO effectively resolves the DCR issue inherent in DPO, offering a more robust method for aligning LLMs with human preferences and enhancing performance.

**Abstract:** Direct Preference Optimization (DPO) have emerged as a popular method for aligning Large Language Models (LLMs) with human preferences. While DPO effectively preserves the relative ordering between chosen and rejected responses through pairwise ranking losses, it often neglects absolute reward magnitudes. This oversight can decrease the likelihood of chosen responses and increase the risk of generating out-of-distribution responses, leading to poor performance. We term this issue Degraded Chosen Responses (DCR).To address this issue, we propose Balanced Preference Optimization (BPO), a novel framework that dynamically balances the optimization of chosen and rejected responses through two key components: balanced reward margin and gap adaptor. Unlike previous methods, BPO can fundamentally resolve DPO's DCR issue, without introducing additional constraints to the loss function. Experimental results on multiple mathematical reasoning tasks show that BPO significantly outperforms DPO, improving accuracy by +10.1% with Llama-3.1-8B-Instruct (18.8% to 28.9%) and +11.7% with Qwen2.5-Math-7B (35.0% to 46.7%). It also surpasses DPO variants by +3.6% over IPO (43.1%), +5.0% over SLiC (41.7%), and +3.1% over Cal-DPO (43.6%) on the same model. Remarkably, our algorithm requires only a single line of code modification, making it simple to implement and fully compatible with existing DPO-based frameworks.

</details>


### [51] [ConsistentChat: Building Skeleton-Guided Consistent Dialogues for Large Language Models from Scratch](https://arxiv.org/abs/2506.03558)

*Jiawei Chen, Xinyan Guan, Qianhao Yuan, Guozhao Mo, Weixiang Zhou, Yaojie Lu, Hongyu Lin, Ben He, Le Sun, Xianpei Han*

**Main category:** cs.CL

**Keywords:** multi-turn dialogue, instruction synthesis, conversational intent, dataset generation, task success

**Relevance Score:** 8

**TL;DR:** Skeleton-Guided Multi-Turn Dialogue Generation improves multi-turn instruction synthesis by modeling human conversational intent and generates a dataset named ConsistentChat.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve task completion rates in extended conversations by addressing lack of cross-turn coherence in current instruction data synthesis methods.

**Method:** The proposed framework operates in two stages: Intent Modeling captures global structure of dialogues based on intent trajectories, and Skeleton Generation creates a sequence of user queries aligned with modeled intent.

**Key Contributions:**

	1. Introduction of Skeleton-Guided Multi-Turn Dialogue Generation framework
	2. Development of ConsistentChat dataset with 15,000 multi-turn conversations
	3. Demonstrated significant performance improvements in chat consistency and task success rates.

**Result:** Models fine-tuned on the ConsistentChat dataset show a 20-30% improvement in chat consistency and a 15% increase in task success rate compared to existing datasets.

**Limitations:** 

**Conclusion:** The framework provides a more coherent and goal-oriented approach to multi-turn instruction synthesis, significantly enhancing dialogue consistency and task success.

**Abstract:** Current instruction data synthesis methods primarily focus on single-turn instructions and often neglect cross-turn coherence, resulting in context drift and reduced task completion rates in extended conversations. To address this limitation, we propose Skeleton-Guided Multi-Turn Dialogue Generation, a framework that constrains multi-turn instruction synthesis by explicitly modeling human conversational intent. It operates in two stages: (1) Intent Modeling, which captures the global structure of human dialogues by assigning each conversation to one of nine well-defined intent trajectories, ensuring a coherent and goal-oriented information flow; and (2) Skeleton Generation, which constructs a structurally grounded sequence of user queries aligned with the modeled intent, thereby serving as a scaffold that constrains and guides the downstream instruction synthesis process. Based on this process, we construct ConsistentChat, a multi-turn instruction dataset with approximately 15,000 multi-turn conversations and 224,392 utterances. Experiments on the Light, Topdial, and MT-Eval benchmarks show that models fine-tuned on ConsistentChat achieve a 20-30% improvement in chat consistency and up to a 15% increase in task success rate, significantly outperforming models trained on existing single-turn and multi-turn instruction datasets.

</details>


### [52] [POSS: Position Specialist Generates Better Draft for Speculative Decoding](https://arxiv.org/abs/2506.03566)

*Langlin Huang, Chengsong Huang, Jixuan Leng, Di Huang, Jiaxin Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inference acceleration, Token prediction, Position Specialists, Error accumulation

**Relevance Score:** 7

**TL;DR:** Position Specialists improve token generation in LLM inference by using specialized draft layers for different token positions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improve the accuracy of token predictions in Large Language Model inference by addressing error accumulation in draft models at later positions.

**Method:** The paper proposes Position Specialists (PosS), which consist of multiple position-specialized draft layers that generate tokens at assigned positions, improving prediction quality.

**Key Contributions:**

	1. Introduction of Position Specialists for LLM inference
	2. Demonstrated improvement in token acceptance rates at later positions
	3. Codebase available for further research and implementation.

**Result:** Experiments show that PosS enhances acceptance rates for token generation and accelerates inference speed across multiple datasets compared to existing methods.

**Limitations:** 

**Conclusion:** PosS effectively addresses the challenges of error accumulation in token prediction by utilizing specialized draft layers, leading to significant improvements in performance.

**Abstract:** Speculative decoding accelerates Large Language Model (LLM) inference by using a small draft model to predict multiple tokens, and a large target model to verify these tokens in parallel. Recent studies leverage the hidden state of the target model to enhance draft model prediction accuracy. However, existing methods suffer from the degrading quality of draft token predictions at later positions, due to error accumulation in draft model generated features. In this paper, we propose Position Specialists (PosS), which consist of multiple position-specialized draft layers to generate tokens at assigned position(s). Position specialists greatly improve token acceptance rate at later positions per drafting round, as each specialist only needs to focus on handling a certain level of draft model feature deviation. Experiment results on Llama-3-8B-Instruct and Llama-2-13B-chat across six datasets demonstrate that PosS effectively improves over baselines on average acceptance length and speed-up ratio. Our codebase is available at https://github.com/shrango/PosS.

</details>


### [53] [MiMo-VL Technical Report](https://arxiv.org/abs/2506.03569)

*Xiaomi LLM-Core Team, :, Zihao Yue, Zhenru Lin, Yifan Song, Weikun Wang, Shuhuai Ren, Shuhao Gu, Shicheng Li, Peidian Li, Liang Zhao, Lei Li, Kainan Bao, Hao Tian, Hailin Zhang, Gang Wang, Dawei Zhu, Cici, Chenhong He, Bowen Ye, Bowen Shen, Zihan Zhang, Zihan Jiang, Zhixian Zheng, Zhichao Song, Zhenbo Luo, Yue Yu, Yudong Wang, Yuanyuan Tian, Yu Tu, Yihan Yan, Yi Huang, Xu Wang, Xinzhe Xu, Xingchen Song, Xing Zhang, Xing Yong, Xin Zhang, Xiangwei Deng, Wenyu Yang, Wenhan Ma, Weiwei Lv, Weiji Zhuang, Wei Liu, Sirui Deng, Shuo Liu, Shimao Chen, Shihua Yu, Shaohui Liu, Shande Wang, Rui Ma, Qiantong Wang, Peng Wang, Nuo Chen, Menghang Zhu, Kangyang Zhou, Kang Zhou, Kai Fang, Jun Shi, Jinhao Dong, Jiebao Xiao, Jiaming Xu, Huaqiu Liu, Hongshen Xu, Heng Qu, Haochen Zhao, Hanglong Lv, Guoan Wang, Duo Zhang, Dong Zhang, Di Zhang, Chong Ma, Chang Liu, Can Cai, Bingquan Xia*

**Main category:** cs.CL

**Keywords:** vision-language models, multimodal reasoning, GUI grounding, reinforcement learning, open-source

**Relevance Score:** 8

**TL;DR:** Introduction of two open-source vision-language models, MiMo-VL-7B-SFT and MiMo-VL-7B-RL, demonstrating superior performance in visual understanding and multimodal reasoning.

**Read time:** 32 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the field of vision-language models by developing open-source solutions that perform exceptionally well on multimodal tasks and setting new benchmarks for GUI grounding applications.

**Method:** The models were trained using a four-stage pre-training approach with 2.4 trillion tokens and integrated Mixed On-policy Reinforcement Learning (MORL) that utilizes diverse reward signals, focusing on high-quality reasoning data.

**Key Contributions:**

	1. Open-sourcing MiMo-VL-7B-SFT and MiMo-VL-7B-RL models.
	2. Establishing new performance benchmarks on various visual and multimodal tasks.
	3. Providing a comprehensive evaluation suite for over 50 tasks to promote reproducibility.

**Result:** MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B in 35 out of 40 tasks and achieves a score of 59.4 on OlympiadBench, along with setting a new standard of 56.1 on OSWorld-G for GUI grounding applications.

**Limitations:** Challenges in simultaneous multi-domain optimization during training.

**Conclusion:** The study highlights the effectiveness of incorporating high-quality reasoning data in pre-training and the advantages of mixed RL while providing resources for reproducibility and evaluation.

**Abstract:** We open-source MiMo-VL-7B-SFT and MiMo-VL-7B-RL, two powerful vision-language models delivering state-of-the-art performance in both general visual understanding and multimodal reasoning. MiMo-VL-7B-RL outperforms Qwen2.5-VL-7B on 35 out of 40 evaluated tasks, and scores 59.4 on OlympiadBench, surpassing models with up to 78B parameters. For GUI grounding applications, it sets a new standard with 56.1 on OSWorld-G, even outperforming specialized models such as UI-TARS. Our training combines four-stage pre-training (2.4 trillion tokens) with Mixed On-policy Reinforcement Learning (MORL) integrating diverse reward signals. We identify the importance of incorporating high-quality reasoning data with long Chain-of-Thought into pre-training stages, and the benefits of mixed RL despite challenges in simultaneous multi-domain optimization. We also contribute a comprehensive evaluation suite covering 50+ tasks to promote reproducibility and advance the field. The model checkpoints and full evaluation suite are available at https://github.com/XiaomiMiMo/MiMo-VL.

</details>


### [54] [FreePRM: Training Process Reward Models Without Ground Truth Process Labels](https://arxiv.org/abs/2506.03570)

*Lin Sun, Chuang Liu, Xiaofeng Ma, Tao Yang, Weijia Lu, Ning Wu*

**Main category:** cs.CL

**Keywords:** Process Reward Models, weak supervision, Large Language Models, pseudo labeling, health informatics

**Relevance Score:** 8

**TL;DR:** FreePRM is a weakly supervised framework that trains Process Reward Models without requiring ground-truth step-level labels, successfully generating pseudo labels and improving performance on various benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and challenges of obtaining step-level labels for training Process Reward Models (PRMs).

**Method:** FreePRM generates pseudo step-level labels based on final outcome correctness and uses Buffer Probability to reduce noise from pseudo labeling.

**Key Contributions:**

	1. Introduction of FreePRM, a framework for weakly supervised training
	2. Achieved significant performance improvements over existing PRMs
	3. Reduction in reliance on manual annotations for PRM training

**Result:** FreePRM achieved an average F1 score of 53.0% on ProcessBench, outperforming fully supervised models and several competitive open-source PRMs.

**Limitations:** 

**Conclusion:** FreePRM introduces a new approach to PRM training that minimizes reliance on costly annotations while achieving strong performance.

**Abstract:** Recent advancements in Large Language Models (LLMs) have demonstrated that Process Reward Models (PRMs) play a crucial role in enhancing model performance. However, training PRMs typically requires step-level labels, either manually annotated or automatically generated, which can be costly and difficult to obtain at scale. To address this challenge, we introduce FreePRM, a weakly supervised framework for training PRMs without access to ground-truth step-level labels. FreePRM first generates pseudo step-level labels based on the correctness of final outcome, and then employs Buffer Probability to eliminate impact of noise inherent in pseudo labeling. Experimental results show that FreePRM achieves an average F1 score of 53.0% on ProcessBench, outperforming fully supervised PRM trained on Math-Shepherd by +24.1%. Compared to other open-source PRMs, FreePRM outperforms upon RLHFlow-PRM-Mistral-8B (28.4%) by +24.6%, EurusPRM (31.3%) by +21.7%, and Skywork-PRM-7B (42.1%) by +10.9%. This work introduces a new paradigm in PRM training, significantly reducing reliance on costly step-level annotations while maintaining strong performance.

</details>


### [55] [Exchange of Perspective Prompting Enhances Reasoning in Large Language Models](https://arxiv.org/abs/2506.03573)

*Lin Sun, Can Zhang*

**Main category:** cs.CL

**Keywords:** large language models, natural language processing, exchange of perspective, framework, AI

**Relevance Score:** 9

**TL;DR:** The paper introduces the Exchange-of-Perspective (EoP) framework to enhance the performance of large language models (LLMs) in natural language processing tasks by facilitating a broader understanding of problems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often struggle with problem comprehension, which limits their effectiveness in NLP tasks.

**Method:** The Exchange-of-Perspective (EoP) framework is proposed to allow LLMs to exchange perspectives across different definitions of problems, aiming to overcome fixed mindsets related to specific question formulations.

**Key Contributions:**

	1. Introduction of the EoP framework
	2. Demonstrated improvement in performance metrics across multiple NLP benchmarks
	3. Evaluation using state-of-the-art models like GPT-3.5 and GPT-4

**Result:** EoP significantly improves performance in various benchmarks, achieving notable enhancements such as 3.6% improvement on AQuA with GPT-3.5-Turbo and up to 7.7% accuracy increase on Math tasks with GPT-4.

**Limitations:** 

**Conclusion:** The EoP framework demonstrates substantial performance gains for LLMs across diverse NLP benchmarks, suggesting a promising direction for improving comprehension and versatility in problem-solving.

**Abstract:** Large language models (LLMs) have made significant advancements in addressing diverse natural language processing (NLP) tasks. However, their performance is often limited by inherent comprehension of problems. To address this limitation, we propose Exchange-of-Perspective (EoP), a novel framework designed to exchange perspectives across different definitions of problem, so that it can break the fixed mindset from any particular formulation of the question. We conducted extensive and comprehensive experiments on 8 benchmarks. The results show that EoP can significantly improve performance. For instance, compared to the non-commutative baseline PHP, with GPT-3.5-Turbo and EoP, we observe a 3.6% improvement on AQuA (60.6% to 64.2%), while GPT-4-powered EoP demonstrates a 7.7% overall accuracy enhancement on Math (53.9% to 61.6%) and a 3.5% improvement on OlympiadBench Maths (43.5% to 47.0%) when using Qwen-2.5-72b.

</details>


### [56] [KG-BiLM: Knowledge Graph Embedding via Bidirectional Language Models](https://arxiv.org/abs/2506.03576)

*Zirui Chen, Xin Wang, Zhao Li, Wenbin Guo, Dongxiao He*

**Main category:** cs.CL

**Keywords:** Knowledge Representation Learning, Language Models, Knowledge Graphs, Semantic Understanding, Graph Neural Networks

**Relevance Score:** 8

**TL;DR:** KG-BiLM is a new bidirectional language model framework that integrates knowledge graphs with language models for improved semantic understanding and reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To unify symbolic knowledge graphs with language models for richer semantic understanding, filling the gap between graph structure and textual semantics.

**Method:** KG-BiLM introduces three components: Bidirectional Knowledge Attention for full token interaction, Knowledge-Masked Prediction for leveraging both local and global information, and Contrastive Graph Semantic Aggregation for preserving KG structure.

**Key Contributions:**

	1. Introduction of a unified framework combining KGs and LMs
	2. Bidirectional Knowledge Attention for enhanced token interaction
	3. Contrastive Graph Semantic Aggregation for preserving graph structures

**Result:** KG-BiLM outperforms strong baselines in link prediction tasks, particularly in large-scale graphs with multi-hop relations.

**Limitations:** 

**Conclusion:** KG-BiLM effectively combines structural information from knowledge graphs with the semantic capabilities of generative transformers, proving its validity through extensive experiments.

**Abstract:** Recent advances in knowledge representation learning (KRL) highlight the urgent necessity to unify symbolic knowledge graphs (KGs) with language models (LMs) for richer semantic understanding. However, existing approaches typically prioritize either graph structure or textual semantics, leaving a gap: a unified framework that simultaneously captures global KG connectivity, nuanced linguistic context, and discriminative reasoning semantics. To bridge this gap, we introduce KG-BiLM, a bidirectional LM framework that fuses structural cues from KGs with the semantic expressiveness of generative transformers. KG-BiLM incorporates three key components: (i) Bidirectional Knowledge Attention, which removes the causal mask to enable full interaction among all tokens and entities; (ii) Knowledge-Masked Prediction, which encourages the model to leverage both local semantic contexts and global graph connectivity; and (iii) Contrastive Graph Semantic Aggregation, which preserves KG structure via contrastive alignment of sampled sub-graph representations. Extensive experiments on standard benchmarks demonstrate that KG-BiLM outperforms strong baselines in link prediction, especially on large-scale graphs with complex multi-hop relations - validating its effectiveness in unifying structural information and textual semantics.

</details>


### [57] [Automatically Suggesting Diverse Example Sentences for L2 Japanese Learners Using Pre-Trained Language Models](https://arxiv.org/abs/2506.03580)

*Enrico Benedetti, Akiko Aizawa, Florian Boudin*

**Main category:** cs.CL

**Keywords:** Pre-trained Language Models, language acquisition, example sentences, Japanese language, Machine Learning

**Relevance Score:** 6

**TL;DR:** This study explores using Pre-trained Language Models (PLMs) to generate and evaluate example sentences for L2 Japanese learners, highlighting preferences for retrieval over generative methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide diverse example sentences that align with learners' proficiency levels for effective language acquisition.

**Method:** The study employs PLMs in two ways: as quality scoring components in a retrieval system from a curated corpus of Japanese sentences and as direct sentence generators using zero-shot learning, evaluated by learners, native speakers, and GPT-4.

**Key Contributions:**

	1. Demonstrates the role of PLMs in language learning applications.
	2. Provides insights into learner preferences for sentence types.
	3. Shows the potential for improved adaptability in educational tools using PLMs.

**Result:** Evaluate the quality of sentences based on difficulty, diversity, and naturalness, with mixed preferences for retrieval methods, particularly among beginner and advanced learners.

**Limitations:** Inherent disagreement among raters on sentence quality ratings, except for difficulty.

**Conclusion:** The findings indicate that PLMs can enhance sentence suggestion systems, improving the adaptability and effectiveness of language learning tools despite variability in quality ratings.

**Abstract:** Providing example sentences that are diverse and aligned with learners' proficiency levels is essential for fostering effective language acquisition. This study examines the use of Pre-trained Language Models (PLMs) to produce example sentences targeting L2 Japanese learners. We utilize PLMs in two ways: as quality scoring components in a retrieval system that draws from a newly curated corpus of Japanese sentences, and as direct sentence generators using zero-shot learning. We evaluate the quality of sentences by considering multiple aspects such as difficulty, diversity, and naturalness, with a panel of raters consisting of learners of Japanese, native speakers -- and GPT-4. Our findings suggest that there is inherent disagreement among participants on the ratings of sentence qualities, except for difficulty. Despite that, the retrieval approach was preferred by all evaluators, especially for beginner and advanced target proficiency, while the generative approaches received lower scores on average. Even so, our experiments highlight the potential for using PLMs to enhance the adaptability of sentence suggestion systems and therefore improve the language learning journey.

</details>


### [58] [From Understanding to Generation: An Efficient Shortcut for Evaluating Language Models](https://arxiv.org/abs/2506.03592)

*Viktor Hangya, Fabian Küch, Darina Gold*

**Main category:** cs.CL

**Keywords:** large language models, NLG, NLU, capability assessment, evaluation reduction

**Relevance Score:** 9

**TL;DR:** This paper presents a method to reduce the computational burden of evaluating large language models (LLMs) during training by reformulating NLG tasks into cheaper NLU alternatives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for iterative evaluation of LLMs while addressing the high computational costs associated with NLG tasks.

**Method:** We reformulate generative tasks into computationally cheaper NLU tasks and evaluate 8 LMs across 4 capabilities to assess the performance correlation between the original and reformulated tasks.

**Key Contributions:**

	1. Development of a method to reformulate NLG tasks into NLU tasks
	2. Validation of performance correlation between NLG and NLU formats
	3. Achieved significant reduction in evaluation time for LLM capability assessment

**Result:** Our approach shows a strong correlation between NLG and the reformulated NLU tasks, leading to an average reduction of over 35x in evaluation time.

**Limitations:** 

**Conclusion:** The findings support the use of cheaper alternatives for monitoring LLM capabilities and we intend to release the benchmark adaptations.

**Abstract:** Iterative evaluation of LLMs during training is essential to ensure expected capability development, but can be time- and compute-intensive. While NLU tasks, where the model selects from fixed answer choices, are cheap to evaluate, essential capabilities like reasoning and code generation rely on the more time-consuming NLG (token-by-token generation) format. In this work, our aim is to decrease the computational burden of NLG benchmarks in order to enable monitoring crucial LLM capabilities during model training. We reformulate generative tasks into computationally cheaper NLU alternatives. We test the performance correlation between the original and reformulated tasks using 8 LMs of various sizes and 4 capabilities: mathematical reasoning, code generation, factual knowledge and reading comprehension. Our results show a strong correlation between task formats, supporting capability assessment via cheaper alternatives and achieving over 35x average reduction in evaluation time. We plan to publish our benchmark adaptions.

</details>


### [59] [Is linguistically-motivated data augmentation worth it?](https://arxiv.org/abs/2506.03593)

*Ray Groshan, Michael Ginn, Alexis Palmer*

**Main category:** cs.CL

**Keywords:** data augmentation, low-resource languages, machine translation, linguistic strategies, sequence-to-sequence

**Relevance Score:** 7

**TL;DR:** This paper compares linguistically-naive and linguistically-motivated data augmentation strategies for low-resource languages in machine translation and interlinear glossing tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To empirically compare the effectiveness of data augmentation approaches for low-resource languages, given the lack of previous systematic studies on the topic.

**Method:** A comprehensive evaluation of various augmentation strategies applied to two low-resource languages (Uspanteko and Arapaho) across machine translation and interlinear glossing tasks.

**Key Contributions:**

	1. Systematic empirical comparison of data augmentation strategies for low-resource languages.
	2. Evaluation of augmentation strategies on machine translation and interlinear glossing tasks.
	3. Insights on the conditions under which linguistically-motivated strategies are beneficial.

**Result:** Linguistically-motivated data augmentation strategies can outperform naive methods, but only if the new examples align closely with the distribution of the training data.

**Limitations:** The study is limited to two specific low-resource languages and may not generalize to all languages or tasks.

**Conclusion:** Linguistic expertise can improve data augmentation effectiveness, but it is contingent on the relevance of the generated examples to existing training data.

**Abstract:** Data augmentation, a widely-employed technique for addressing data scarcity, involves generating synthetic data examples which are then used to augment available training data. Researchers have seen surprising success from simple methods, such as random perturbations from natural examples, where models seem to benefit even from data with nonsense words, or data that doesn't conform to the rules of the language. A second line of research produces synthetic data that does in fact follow all linguistic constraints; these methods require some linguistic expertise and are generally more challenging to implement. No previous work has done a systematic, empirical comparison of both linguistically-naive and linguistically-motivated data augmentation strategies, leaving uncertainty about whether the additional time and effort of linguistically-motivated data augmentation work in fact yields better downstream performance.   In this work, we conduct a careful and comprehensive comparison of augmentation strategies (both linguistically-naive and linguistically-motivated) for two low-resource languages with different morphological properties, Uspanteko and Arapaho. We evaluate the effectiveness of many different strategies and their combinations across two important sequence-to-sequence tasks for low-resource languages: machine translation and interlinear glossing. We find that linguistically-motivated strategies can have benefits over naive approaches, but only when the new examples they produce are not significantly unlike the training data distribution.

</details>


### [60] [Auto prompt sql: a resource-efficient architecture for text-to-sql translation in constrained environments](https://arxiv.org/abs/2506.03598)

*Zetong Tang, Qian Ma, Di Wu*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, prompt engineering, resource-efficient models

**Relevance Score:** 7

**TL;DR:** Auto Prompt SQL (AP-SQL) is a novel architecture that enhances Text-to-SQL translation using resource-efficient models, leveraging prompt engineering and schema linking techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of using resource-intensive models for Text-to-SQL tasks in resource-constrained environments.

**Method:** AP-SQL decomposes Text-to-SQL tasks into schema filtering, retrieval-augmented generation via in-context examples, and prompt-driven schema linking and SQL generation, with fine-tuning of large language models for better schema selection.

**Key Contributions:**

	1. Introduction of Auto Prompt SQL (AP-SQL) architecture
	2. Utilization of prompt engineering techniques such as Chain-of-Thought and Graph-of-Thought
	3. Demonstrated effectiveness on Spider benchmarks for schema selection and SQL generation

**Result:** Comprehensive evaluations on the Spider benchmarks show that AP-SQL significantly improves accuracy in SQL generation compared to existing methods.

**Limitations:** 

**Conclusion:** AP-SQL effectively bridges the gap between small, resource-efficient models and large, closed-source models, enhancing Text-to-SQL translation performance.

**Abstract:** Using the best Text-to-SQL methods in resource-constrained environments is challenging due to their reliance on resource-intensive open-source models. This paper introduces Auto Prompt SQL(AP-SQL), a novel architecture designed to bridge the gap between resource-efficient small open-source models and the powerful capabilities of large closed-source models for Text-to-SQL translation. Our method decomposes the task into schema filtering, retrieval-augmented text-to-SQL generation based on in-context examples, and prompt-driven schema linking and SQL generation. To improve schema selection accuracy, we fine-tune large language models. Crucially, we also explore the impact of prompt engineering throughout the process, leveraging Chain-of-Thought(CoT) and Graph-of-Thought(GoT) templates to significantly enhance the model's reasoning for accurate SQL generation. Comprehensive evaluations on the Spider benchmarks demonstrate the effectiveness of AP-SQL.

</details>


### [61] [Learning to Insert [PAUSE] Tokens for Better Reasoning](https://arxiv.org/abs/2506.03616)

*Eunki Kim, Sangryul Kim, James Thorne*

**Main category:** cs.CL

**Keywords:** Dynamic Inserting Tokens, transformer models, large language models, reasoning, token insertion

**Relevance Score:** 8

**TL;DR:** The paper introduces Dynamic Inserting Tokens Training (DIT), a method that enhances reasoning in transformer-based LLMs by strategically inserting [PAUSE] tokens at positions of low model confidence, resulting in improved predictive capabilities and accuracy across multiple datasets.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of transformer-based large language models (LLMs) by improving the learning mechanism through token insertion strategies.

**Method:** The method identifies positions within sequences of low model confidence based on token log-likelihood and inserts [PAUSE] tokens at these locations to bolster predictive capabilities for subsequent tokens.

**Key Contributions:**

	1. Introduction of Dynamic Inserting Tokens Training (DIT) method
	2. Demonstration of significant accuracy gains on multiple datasets
	3. Shift from heuristic to model-based dynamic approach for token insertion

**Result:** DIT outperforms traditional fine-tuning and previous token insertion strategies across diverse datasets and models, achieving accuracy gains up to 4.7% on GSM8K and improvements of 3.4% pass@1 on MBPP datasets.

**Limitations:** 

**Conclusion:** The study demonstrates that a model-based, dynamic approach to token insertion broadens the scope of reasoning research in large language models.

**Abstract:** To enhance reasoning capabilities, previous works have explored incorporating special-purpose tokens into the training process. These strategies strengthen the learning mechanism of transformer-based large language models (LLMs). Building on prior research, in which inserting dummy tokens consecutively just before reasoning steps can enhance effectiveness, we introduce a novel approach termed Dynamic Inserting Tokens Training (DIT). Our method identifies positions within sequences where model confidence is lowest according to token log-likelihood. Strategically inserting [PAUSE] tokens on these positions bolsters the model's predictive capabilities for subsequent tokens. Experimental results across diverse datasets and models, from the 2.7B model to the 8B model, demonstrate that DIT consistently outperforms traditional fine-tuning and previous token insertion methods. With this simple yet effective method, we achieve accuracy gains of up to 4.7%p on GSM8K, 3.23%p on AQUA-RAT, and pass@1 improvements of up to 3.4%p on MBPP datasets. Our work shows a model-based, dynamic approach rather than a heuristic one, thereby broadening the scope of research in reasoning.

</details>


### [62] [Do Large Language Models Know Folktales? A Case Study of Yokai in Japanese Folktales](https://arxiv.org/abs/2506.03619)

*Ayuto Tsutsumi, Yuu Jinnai*

**Main category:** cs.CL

**Keywords:** Large Language Models, Yokai, Cultural Awareness, Folktales, Benchmark Dataset

**Relevance Score:** 7

**TL;DR:** This paper evaluates the cultural awareness of Large Language Models (LLMs) through a benchmark dataset focused on Japanese folktales, particularly Yokai.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural limitations of LLMs, particularly in non-English speaking communities, by evaluating their knowledge of folktales as cultural expressions.

**Method:** The study introduces YokaiEval, a benchmark dataset with 809 multiple-choice questions about yokai, and evaluates 31 Japanese and multilingual LLMs on it.

**Key Contributions:**

	1. Introduction of YokaiEval dataset for evaluating LLM cultural awareness
	2. Comparison of performance between Japanese and multilingual LLMs
	3. Insights on the importance of language-specific training for cultural understanding

**Result:** Japanese language models outperformed English-centric models in accuracy, especially those that underwent continued pretraining in Japanese, such as Llama-3.

**Limitations:** 

**Conclusion:** The findings indicate that models trained with Japanese resources exhibit better cultural awareness, and the dataset can aid in further research on cultural inclusivity in LLMs.

**Abstract:** Although Large Language Models (LLMs) have demonstrated strong language understanding and generation abilities across various languages, their cultural knowledge is often limited to English-speaking communities, which can marginalize the cultures of non-English communities. To address the problem, evaluation of the cultural awareness of the LLMs and the methods to develop culturally aware LLMs have been investigated. In this study, we focus on evaluating knowledge of folktales, a key medium for conveying and circulating culture. In particular, we focus on Japanese folktales, specifically on knowledge of Yokai. Yokai are supernatural creatures originating from Japanese folktales that continue to be popular motifs in art and entertainment today. Yokai have long served as a medium for cultural expression, making them an ideal subject for assessing the cultural awareness of LLMs. We introduce YokaiEval, a benchmark dataset consisting of 809 multiple-choice questions (each with four options) designed to probe knowledge about yokai. We evaluate the performance of 31 Japanese and multilingual LLMs on this dataset. The results show that models trained with Japanese language resources achieve higher accuracy than English-centric models, with those that underwent continued pretraining in Japanese, particularly those based on Llama-3, performing especially well. The code and dataset are available at https://github.com/CyberAgentA ILab/YokaiEval.

</details>


### [63] [Robustness of Prompting: Enhancing Robustness of Large Language Models Against Prompting Attacks](https://arxiv.org/abs/2506.03627)

*Lin Mu, Guowei Chu, Li Ni, Lei Sang, Zhize Wu, Peiquan Jin, Yiwen Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Robustness, Prompting Strategies, Error Correction, Guidance

**Relevance Score:** 9

**TL;DR:** The paper introduces Robustness of Prompting (RoP), a novel prompting strategy to enhance the robustness of Large Language Models against input perturbations, showing significant improvements in model performance across various reasoning tasks.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of LLMs being sensitive to input perturbations like typographical errors, which can significantly degrade their performance despite advances in prompting techniques.

**Method:** RoP consists of two stages: Error Correction, which generates adversarial examples to automatically correct input errors, and Guidance, which generates optimal prompts based on corrected inputs to enhance model inference accuracy.

**Key Contributions:**

	1. Introduction of Robustness of Prompting (RoP) strategy
	2. Demonstration of significant robustness improvements for LLMs
	3. Application of error correction and guidance prompting techniques

**Result:** Comprehensive experiments demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations while maintaining model accuracy with minimal degradation compared to clean input scenarios.

**Limitations:** 

**Conclusion:** RoP establishes itself as a practical and effective approach to improve LLM robustness in real-world applications, enhancing performance across arithmetic, commonsense, and logical reasoning tasks.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across various tasks by effectively utilizing a prompting strategy. However, they are highly sensitive to input perturbations, such as typographical errors or slight character order errors, which can substantially degrade their performance. Despite advances in prompting techniques, developing a prompting strategy that explicitly mitigates the negative impact of such perturbations remains an open challenge. To bridge this gap, we propose Robustness of Prompting (RoP), a novel prompting strategy specifically designed to enhance the robustness of LLMs. RoP consists of two stages: Error Correction and Guidance. In the Error Correction stage, RoP applies diverse perturbation methods to generate adversarial examples, which are then used to construct prompts that automatically correct input errors. In the Guidance stage, RoP generates an optimal guidance prompting based on the corrected input, steering the model toward more robust and accurate inferences. Through comprehensive experiments spanning arithmetic, commonsense, and logical reasoning tasks, we demonstrate that RoP significantly improves LLMs' robustness against adversarial perturbations. Notably, it maintains model accuracy with only minimal degradation compared to clean input scenarios, thereby establishing RoP as a practical and effective approach for enhancing LLM robustness in real-world applications.

</details>


### [64] [RewardAnything: Generalizable Principle-Following Reward Models](https://arxiv.org/abs/2506.03637)

*Zhuohao Yu, Jiali Zeng, Weizheng Gu, Yidong Wang, Jindong Wang, Fandong Meng, Jie Zhou, Yue Zhang, Shikun Zhang, Wei Ye*

**Main category:** cs.CL

**Keywords:** Reward Models, Natural Language Principles, Reinforcement Learning

**Relevance Score:** 8

**TL;DR:** This paper presents RewardAnything, a novel reward model that adheres to natural language specifications, addressing the limitations of traditional reward models in generalizing across diverse principles.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Current reward models are limited by fixed preference datasets and biased rewards, making them inflexible for real-world application. There is a need for reward models that can adapt to various task requirements dynamically without extensive retraining.

**Method:** The paper proposes a new approach where reward models are trained to follow natural language specifications of reward principles. A benchmark called RABench is introduced to evaluate the generalization capabilities of reward models across varying principles.

**Key Contributions:**

	1. Introduction of RABench for benchmarking reward model generalization.
	2. Development of RewardAnything, a reward model that adheres to dynamic natural language principles.
	3. Demonstration of seamless integration of RewardAnything with existing reinforcement learning from human feedback (RLHF) methods.

**Result:** RewardAnything achieves state-of-the-art performance on traditional reward model benchmarks and demonstrates strong adaptability to novel principles using natural language specifications, without the need for retraining.

**Limitations:** The effectiveness of RewardAnything is evaluated primarily on benchmark tasks, and its performance in unstructured real-world scenarios remains to be assessed.

**Conclusion:** The proposed RewardAnything model offers a promising solution for developing flexible reward models in reinforcement learning, allowing for efficient alignment of large language models to user specifications.

**Abstract:** Reward Models, essential for guiding Large Language Model optimization, are typically trained on fixed preference datasets, resulting in rigid alignment to single, implicit preference distributions. This prevents adaptation to diverse real-world needs-from conciseness in one task to detailed explanations in another. The standard practice of collecting task-specific preference data and retraining reward models is resource-intensive, often producing biased rewards, and limits practical application. We introduce generalizable, principle-following reward models. We propose that RMs should understand and adhere to dynamically provided natural language specifications of reward principles, similar to instruction-following in LLMs. To measure this capability, we develop RABench, a comprehensive benchmark for RMs focusing on generalization across diverse principles. Evaluations on RABench reveal poor generalization of current RMs. As a solution, we present RewardAnything, a novel RM designed and trained to explicitly follow natural language principles. We achieve SotA performance with RewardAnything in traditional RM benchmark simply by specifying a well-defined principle, and results on RABench show we excel in adapting to novel principles without retraining. Furthermore, RewardAnything integrates seamlessly with existing RLHF methods and we show by a case study on how to automatically and efficiently align LLMs with only natural language principles.

</details>


### [65] [Trustworthy Medical Question Answering: An Evaluation-Centric Survey](https://arxiv.org/abs/2506.03659)

*Yinuo Wang, Robert E. Mercer, Frank Rudzicz, Sudipta Singha Roy, Pengjie Ren, Zhumin Chen, Xindi Wang*

**Main category:** cs.CL

**Keywords:** trustworthiness, healthcare QA, large language models, evaluation metrics, safety alignment

**Relevance Score:** 9

**TL;DR:** The paper surveys the trustworthiness of healthcare question-answering systems that utilize large language models, highlighting key dimensions and challenges.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the importance of trustworthiness in healthcare QA systems, which affects patient safety and clinical outcomes as LLMs become more integrated into healthcare.

**Method:** The survey systematically examines six dimensions of trustworthiness (Factuality, Robustness, Fairness, Safety, Explainability, Calibration) and reviews their evaluation in LLM-based medical QA systems.

**Key Contributions:**

	1. Framework for assessing trustworthiness dimensions in medical QA systems
	2. Comparison of major benchmarks for evaluation
	3. Identification of future research directions to improve LLM-powered medical QA

**Result:** The review identifies existing benchmarks for assessing these dimensions and analyzes techniques for improving model performance, such as retrieval-augmented grounding and adversarial fine-tuning.

**Limitations:** 

**Conclusion:** The paper identifies open challenges and proposes directions for future research to enhance the safe and reliable use of LLMs in medical QA.

**Abstract:** Trustworthiness in healthcare question-answering (QA) systems is important for ensuring patient safety, clinical effectiveness, and user confidence. As large language models (LLMs) become increasingly integrated into medical settings, the reliability of their responses directly influences clinical decision-making and patient outcomes. However, achieving comprehensive trustworthiness in medical QA poses significant challenges due to the inherent complexity of healthcare data, the critical nature of clinical scenarios, and the multifaceted dimensions of trustworthy AI. In this survey, we systematically examine six key dimensions of trustworthiness in medical QA, i.e., Factuality, Robustness, Fairness, Safety, Explainability, and Calibration. We review how each dimension is evaluated in existing LLM-based medical QA systems. We compile and compare major benchmarks designed to assess these dimensions and analyze evaluation-guided techniques that drive model improvements, such as retrieval-augmented grounding, adversarial fine-tuning, and safety alignment. Finally, we identify open challenges-such as scalable expert evaluation, integrated multi-dimensional metrics, and real-world deployment studies-and propose future research directions to advance the safe, reliable, and transparent deployment of LLM-powered medical QA.

</details>


### [66] [ROSA: Addressing text understanding challenges in photographs via ROtated SAmpling](https://arxiv.org/abs/2506.03665)

*Hernán Maina, Guido Ivetta, Mateo Lione Stuto, Julian Martin Eisenschlos, Jorge Sánchez, Luciana Benotti*

**Main category:** cs.CL

**Keywords:** Visually Impaired, Visual Question Answering, Text Recognition, Decoding Strategy, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper introduces ROSA, a decoding strategy that improves Visual Question Answering (VQA) performance for visually impaired users by addressing challenges in recognizing misaligned text in images.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Visually impaired individuals can benefit from VQA systems, but current models struggle with recognizing text in images due to orientation issues.

**Method:** In-depth interviews with visually impaired individuals were conducted to identify common text framing challenges, leading to the development of ROSA, a decoding strategy designed to enhance VQA performance in these scenarios.

**Key Contributions:**

	1. Introduction of ROtated SAmpling (ROSA) for VQA
	2. Identification of common framing conventions causing misaligned text
	3. Benchmarks demonstrating ROSA's superior performance over traditional methods

**Result:** ROSA demonstrates significant performance improvement, outpacing Greedy decoding by 11.7 absolute points in the best-performing model.

**Limitations:** 

**Conclusion:** ROSA effectively addresses the shortcomings of existing VQA models in recognizing misaligned text for visually impaired users.

**Abstract:** Visually impaired people could benefit from Visual Question Answering (VQA) systems to interpret text in their surroundings. However, current models often struggle with recognizing text in the photos taken by this population. Through in-depth interviews with visually impaired individuals, we identified common framing conventions that frequently result in misaligned text. Existing VQA benchmarks primarily feature well-oriented text captured by sighted users, under-representing these challenges. To address this gap, we introduce ROtated SAmpling (ROSA), a decoding strategy that enhances VQA performance in text-rich images with incorrectly oriented text. ROSA outperforms Greedy decoding by 11.7 absolute points in the best-performing model.

</details>


### [67] [Efficient Data Selection for Domain Adaptation of ASR Using Pseudo-Labels and Multi-Stage Filtering](https://arxiv.org/abs/2506.03681)

*Pradeep Rangappa, Andres Carofilis, Jeena Prakash, Shashi Kumar, Sergio Burdisso, Srikanth Madikeri, Esau Villatoro-Tello, Bidisha Sharma, Petr Motlicek, Kadri Hacioglu, Shankar Venkatesan, Saurabh Vyas, Andreas Stolcke*

**Main category:** cs.CL

**Keywords:** ASR, data selection, fine-tuning, Whisper, Zipformer

**Relevance Score:** 6

**TL;DR:** Proposes a robust data selection pipeline for fine-tuning ASR models, improving performance with limited data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges faced by small organizations in fine-tuning ASR models due to limited labeled data and resources.

**Method:** The study integrates multiple selection strategies including WER prediction, NER, and CER analysis to filter pseudo-labels from Whisper and Zipformer models.

**Key Contributions:**

	1. Development of a robust data selection pipeline for ASR
	2. Integration of multiple selection strategies for better filtering
	3. Demonstrated significant reduction in training data while preserving ASR performance

**Result:** Fine-tuning on 7500 hours of pseudo-labeled data achieves 12.3% WER, but with filtering, the dataset is reduced to 100 hours with a similar performance (1.4% WER).

**Limitations:** 

**Conclusion:** The proposed filtering strategy significantly reduces the amount of data needed for effective ASR fine-tuning, maintaining high performance.

**Abstract:** Fine-tuning pretrained ASR models for specific domains is challenging for small organizations with limited labeled data and computational resources. Here, we explore different data selection pipelines and propose a robust approach that improves ASR adaptation by filtering pseudo-labels generated using Whisper (encoder-decoder) and Zipformer (transducer) models. Our approach integrates multiple selection strategies -- including word error rate (WER) prediction, named entity recognition (NER), and character error rate (CER) analysis -- to extract high-quality training segments. We evaluate our method on Whisper and Zipformer using a 7500-hour baseline, comparing it to a CER-based approach relying on hypotheses from three ASR systems. Fine-tuning on 7500 hours of pseudo-labeled call center data achieves 12.3% WER, while our filtering reduces the dataset to 100 hours (1.4%) with similar performance; a similar trend is observed on Fisher English.

</details>


### [68] [Robust Preference Optimization via Dynamic Target Margins](https://arxiv.org/abs/2506.03690)

*Jie Sun, Junkang Wu, Jiancan Wu, Zhibo Zhu, Xingyu Lu, Jun Zhou, Lintao Ma, Xiang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Direct Preference Optimization, Algorithm Optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces $b3$-PO, a dynamic algorithm for optimizing Large Language Models (LLMs) using preference pairs, enhancing alignment by addressing data quality issues.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable alignment of LLMs for safety and efficacy in applications drives the development of efficient optimization methods like Direct Preference Optimization (DPO).

**Method:** The proposed $b3$-PO algorithm adjusts reward margins at the pairwise level, prioritizing high-confidence preference pairs and minimizing noise from ambiguous pairs.

**Key Contributions:**

	1. Introduction of $b3$-PO algorithm for dynamic optimization of LLMs.
	2. Significant improvements in performance over existing baseline methods.
	3. Minimal implementation overhead for broader applicability.

**Result:** Across various benchmarks, $b3$-PO achieved an average improvement of 4.4% over existing baselines, presenting a new state-of-the-art performance in LLM alignment.

**Limitations:** 

**Conclusion:** $b3$-PO is a straightforward and robust method for enhancing LLM alignment with minimal changes to existing code and negligible effects on training efficiency.

**Abstract:** The alignment of Large Language Models (LLMs) is crucial for ensuring their safety and reliability in practical applications. Direct Preference Optimization (DPO) has emerged as an efficient method that directly optimizes models using preference pairs, significantly reducing resource demands. However, the effectiveness of DPO heavily depends on the data quality, which is frequently compromised by noise. In this work, we propose $\gamma$-PO, a dynamic target margin preference optimization algorithm that adjust reward margins at the pairwise level. By introducing instance-specific margin calibration, $\gamma$-PO strategically prioritizes high-confidence pairs (those demonstrating higher reward margins) while suppressing potential noise from ambiguous pairs. Moreover, $\gamma$-PO is a plug-and-play method, compatible with variants of DPO that rely on reward margin between preference pairs. Across benchmarks such as AlpacaEval2 and Arena-Hard, $\gamma$-PO achieves an average 4.4\% improvement over other baselines, setting new benchmarks for state-of-the-art performance. Additionally, $\gamma$-PO requires minimal code changes and has a negligible impact on training efficiency, making it a robust solution for enhancing LLMs alignment. Our codes are available at \href{https://github.com/sunjie279/gammaPO}{https://github.com/sunjie279/gammaPO}.

</details>


### [69] [AdaDecode: Accelerating LLM Decoding with Adaptive Layer Parallelism](https://arxiv.org/abs/2506.03700)

*Zhepei Wei, Wei-Lin Chen, Xinyu Zhu, Yu Meng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Decoding Efficiency, Adaptive Token Generation

**Relevance Score:** 9

**TL;DR:** AdaDecode improves long-content generation in large language models by enabling parallel decoding at intermediate layers, significantly increasing throughput while maintaining output consistency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiency of sequential autoregressive decoding in LLMs, which limits the ability to utilize parallel processing capabilities of modern hardware.

**Method:** AdaDecode accelerates token generation by adaptively predicting tokens at intermediate layers based on confidence levels, allowing parallel computations for deferred predictions.

**Key Contributions:**

	1. Introduction of AdaDecode for LLMs
	2. Achieving up to 1.73x speedup in decoding
	3. Ensuring output consistency without auxiliary models

**Result:** AdaDecode achieves decoding throughput improvements of up to 1.73x compared to standard autoregressive decoding while ensuring output parity.

**Limitations:** 

**Conclusion:** The proposed method enhances efficiency in LLM decoding and leverages hardware better without needing auxiliary models, contributing to advances in long-content generation.

**Abstract:** Large language models (LLMs) are increasingly used for long-content generation (e.g., long Chain-of-Thought reasoning) where decoding efficiency becomes a critical bottleneck: Autoregressive decoding is inherently limited by its sequential token generation process, where each token must be generated before the next can be processed. This sequential dependency restricts the ability to fully leverage modern hardware's parallel processing capabilities. Existing methods like speculative decoding and layer skipping offer potential speedups but have notable drawbacks: speculative decoding relies on an auxiliary "drafter" model, which can be challenging to acquire and increases memory overhead, while layer skipping may introduce discrepancies in the outputs due to the missing key-value cache at skipped layers. In this work, we propose AdaDecode, which accelerates LLM decoding without requiring auxiliary models or changes to the original model parameters, while ensuring output consistency. AdaDecode leverages the insight that many tokens can accurately be generated at intermediate layers, as further layers often do not significantly alter predictions once the model reaches a certain confidence. By adaptively generating tokens at intermediate layers when confidence is high, AdaDecode enables the next token's computation to begin immediately. The remaining layer computations for early-predicted tokens are deferred and executed in parallel with subsequent tokens when needed, maximizing hardware utilization and reducing decoding latency. A final verification step ensures that early predictions match the results of standard autoregressive decoding, preserving output parity. Experiments across diverse generation tasks shows that AdaDecode consistently achieves superior decoding throughput with up to 1.73x speedup, while guaranteeing output parity with standard autoregressive decoding.

</details>


### [70] [ScoreRAG: A Retrieval-Augmented Generation Framework with Consistency-Relevance Scoring and Structured Summarization for News Generation](https://arxiv.org/abs/2506.03704)

*Pei-Yun Lin, Yen-lung Tsai*

**Main category:** cs.CL

**Keywords:** ScoreRAG, automated news generation, retrieval-augmented generation, natural language processing, large language models

**Relevance Score:** 7

**TL;DR:** ScoreRAG is a multi-stage framework designed to enhance the quality of automated news generation by integrating retrieval-augmented generation with consistency evaluation and structured summarization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues in current automated news generation methods such as hallucinations, factual inconsistencies, and lack of domain expertise.

**Method:** The framework retrieves relevant news documents from a vector database, maps them to complete news items, evaluates consistency relevance, reranks documents, and generates summaries that guide the large language model in producing quality news articles.

**Key Contributions:**

	1. Introduction of ScoreRAG framework for automated news generation
	2. Integration of retrieval-augmented generation and consistency relevance evaluation
	3. Provision of a demo and code for public access

**Result:** ScoreRAG significantly improves the accuracy, coherence, informativeness, and professionalism of generated news articles.

**Limitations:** 

**Conclusion:** The method shows potential for stable and consistent generation of high-quality news articles, adhering to professional journalistic standards.

**Abstract:** This research introduces ScoreRAG, an approach to enhance the quality of automated news generation. Despite advancements in Natural Language Processing and large language models, current news generation methods often struggle with hallucinations, factual inconsistencies, and lack of domain-specific expertise when producing news articles. ScoreRAG addresses these challenges through a multi-stage framework combining retrieval-augmented generation, consistency relevance evaluation, and structured summarization. The system first retrieves relevant news documents from a vector database, maps them to complete news items, and assigns consistency relevance scores based on large language model evaluations. These documents are then reranked according to relevance, with low-quality items filtered out. The framework proceeds to generate graded summaries based on relevance scores, which guide the large language model in producing complete news articles following professional journalistic standards. Through this methodical approach, ScoreRAG aims to significantly improve the accuracy, coherence, informativeness, and professionalism of generated news articles while maintaining stability and consistency throughout the generation process. The code and demo are available at: https://github.com/peiyun2260/ScoreRAG.

</details>


### [71] [MFLA: Monotonic Finite Look-ahead Attention for Streaming Speech Recognition](https://arxiv.org/abs/2506.03722)

*Yinfeng Xia, Huiyan Li, Chenyang Le, Manhong Wang, Yutao Sun, Xingyang Ma, Yanmin Qian*

**Main category:** cs.CL

**Keywords:** speech recognition, streaming systems, Whisper model

**Relevance Score:** 7

**TL;DR:** This paper proposes a novel prefix-to-prefix training framework for integrating large pre-trained speech models like Whisper into streaming systems, addressing challenges in streaming recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Integrating large pre-trained speech models into streaming systems effectively while minimizing training costs.

**Method:** The authors introduce a prefix-to-prefix training framework, utilizing a Continuous Integrate-and-Fire mechanism for quasi-monotonic alignment and Monotonic Finite Look-ahead Attention for efficient token attention. A wait-k decoding strategy is also employed to simplify the decoding process.

**Key Contributions:**

	1. Novel prefix-to-prefix training framework for streaming recognition
	2. Continuous Integrate-and-Fire mechanism for quasi-monotonic alignment
	3. Monotonic Finite Look-ahead Attention for enhanced token attention

**Result:** The proposed framework achieves a controllable balance between latency and quality in streaming applications, as shown through theoretical analysis and experimental validation.

**Limitations:** 

**Conclusion:** The novel approaches enhance the performance of Whisper in streaming systems, making it an effective solution for various applications that require real-time transcription or speech recognition.

**Abstract:** Applying large pre-trained speech models like Whisper has shown promise in reducing training costs for various speech tasks. However, integrating these models into streaming systems remains a challenge. This paper presents a novel prefix-to-prefix training framework for streaming recognition by fine-tuning the Whisper. We introduce the Continuous Integrate-and-Fire mechanism to establish a quasi-monotonic alignment between continuous speech sequences and discrete text tokens. Additionally, we design Monotonic Finite Look-ahead Attention, allowing each token to attend to infinite left-context and finite right-context from the speech sequences. We also employ the wait-k decoding strategy to simplify the decoding process while ensuring consistency between training and testing. Our theoretical analysis and experiments demonstrate that this approach achieves a controllable trade-off between latency and quality, making it suitable for various streaming applications.

</details>


### [72] [Verbalized Confidence Triggers Self-Verification: Emergent Behavior Without Explicit Reasoning Supervision](https://arxiv.org/abs/2506.03723)

*Chaeyun Jang, Moonseok Choi, Yegon Kim, Hyungi Lee, Juho Lee*

**Main category:** cs.CL

**Keywords:** uncertainty calibration, language models, confidence scores, self-verification, chain-of-thought reasoning

**Relevance Score:** 9

**TL;DR:** This paper explores uncertainty calibration in large language models (LLMs) for chain-of-thought reasoning by focusing on confidence calibration and self-verification behavior through supervised fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Uncertainty calibration is crucial for deploying LLMs that users depend on for reliable confidence estimates, yet it has not been thoroughly investigated in the context of CoT reasoning.

**Method:** The study employs supervised fine-tuning with scalar confidence labels, allowing LLMs to generate self-verifying responses and adapting their answers based on confidence levels without explicit reasoning training or reinforcement learning.

**Key Contributions:**

	1. Demonstrated effectiveness of fine-tuning LLMs with scalar confidence labels for self-verification in reasoning tasks.
	2. Introduced a test-time scaling method to improve model performance based on calibrated uncertainty.
	3. Empirical evidence of enhanced calibration and accuracy in reasoning tasks such as GSM8K, MATH-500, and ARC-Challenge.

**Result:** Experiments demonstrate that confidence-aware fine-tuning enhances both calibration and accuracy, particularly in long-form reasoning tasks, while improving interpretability through alignment of reasoning paths with confidence levels.

**Limitations:** The study primarily focuses on confidence calibration without addressing other aspects of model interpretability or external validation of results.

**Conclusion:** The proposed approaches notably improve the self-verification capabilities of LLMs, enhancing both their performance and the trust users can place in their outputs.

**Abstract:** Uncertainty calibration is essential for the safe deployment of large language models (LLMs), particularly when users rely on verbalized confidence estimates. While prior work has focused on classifiers or short-form generation, confidence calibration for chain-of-thought (CoT) reasoning remains largely unexplored. Surprisingly, we find that supervised fine-tuning with scalar confidence labels alone suffices to elicit self-verification behavior of language models, without any explicit reasoning supervision or reinforcement learning-based rewards. Despite being trained only to produce a verbalized confidence score without any self-verifying examples, the model learns to generate longer and self-checking responses for low-confidence queries while providing more concise answers for high-confidence ones. We further propose a simple rethinking method that boosts performance via test-time scaling based on calibrated uncertainty. Experiments on GSM8K and held-out reasoning tasks such as MATH-500 and ARC-Challenge show that our confidence-aware fine-tuning improves both calibration and accuracy, while also enhancing interpretability by aligning the model's reasoning path with its confidence.

</details>


### [73] [Generating Pedagogically Meaningful Visuals for Math Word Problems: A New Benchmark and Analysis of Text-to-Image Models](https://arxiv.org/abs/2506.03735)

*Junling Wang, Anna Rutkiewicz, April Yi Wang, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** educational visuals, math word problems, Text-to-Image, automated generation, pedagogical design

**Relevance Score:** 4

**TL;DR:** This paper presents Math2Visual, an automated framework for generating educational visuals from math word problems to aid young learners.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a demand for automated tools to create visuals that help students interpret math word problems, overcoming the laborious task of manual creation.

**Method:** The authors developed Math2Visual, which utilizes a predefined visual language and a dataset of 1,903 visuals to evaluate and enhance Text-to-Image models for generating educational visuals.

**Key Contributions:**

	1. Introduction of the Math2Visual framework for automating visual generation from math word problems
	2. Creation of an annotated dataset of 1,903 visuals for educational purposes
	3. Evaluation and enhancement of Text-to-Image models using the dataset.

**Result:** The evaluation shows that fine-tuning TTI models with the Math2Visual dataset leads to improved generation of visuals that align with educational requirements.

**Limitations:** The study may face challenges in accurately representing all mathematical relationships and ensuring essential visual elements are included.

**Conclusion:** Math2Visual establishes a benchmark for automated educational visual generation and highlights challenges in ensuring accurate representation of mathematical concepts.

**Abstract:** Visuals are valuable tools for teaching math word problems (MWPs), helping young learners interpret textual descriptions into mathematical expressions before solving them. However, creating such visuals is labor-intensive and there is a lack of automated methods to support this process. In this paper, we present Math2Visual, an automatic framework for generating pedagogically meaningful visuals from MWP text descriptions. Math2Visual leverages a pre-defined visual language and a design space grounded in interviews with math teachers, to illustrate the core mathematical relationships in MWPs. Using Math2Visual, we construct an annotated dataset of 1,903 visuals and evaluate Text-to-Image (TTI) models for their ability to generate visuals that align with our design. We further fine-tune several TTI models with our dataset, demonstrating improvements in educational visual generation. Our work establishes a new benchmark for automated generation of pedagogically meaningful visuals and offers insights into key challenges in producing multimodal educational content, such as the misrepresentation of mathematical relationships and the omission of essential visual elements.

</details>


### [74] [Act-as-Pet: Benchmarking the Abilities of Large Language Models as E-Pets in Social Network Services](https://arxiv.org/abs/2506.03761)

*Hongcheng Guo, Zheyong Xie, Shaosheng Cao, Boyang Wang, Weiting Liu, Zheyu Ye, Zhoujun Li, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, virtual pets, benchmarking, human-computer interaction, emotionally immersive experiences

**Relevance Score:** 9

**TL;DR:** The paper introduces Pet-Bench, a benchmark for evaluating LLMs in the context of virtual pet companionship, focusing on both self-interaction and human-interaction dimensions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is growing interest in using LLMs for creating interactive and emotionally engaging experiences, particularly in virtual pet companionship, an area that is currently underexplored.

**Method:** The paper presents the Pet-Bench framework, featuring diverse tasks designed to evaluate LLMs through more realistic pet companionship scenarios, including memory-based dialogues and intelligent scheduling.

**Key Contributions:**

	1. Introduction of Pet-Bench as a comprehensive benchmarking tool for LLMs in pet companionship
	2. Focus on self-evolution and developmental behaviors in pet interactions
	3. Evaluation of 28 LLMs revealing performance variations that inform future optimizations

**Result:** Evaluation of 28 LLMs shows significant performance variations based on model size and capabilities, highlighting the necessity for specialized optimization for better pet-related interactions.

**Limitations:** 

**Conclusion:** Pet-Bench provides a foundational resource to benchmark LLM capabilities in pet companionship and aims to advance more immersive human-pet interactions.

**Abstract:** As interest in using Large Language Models (LLMs) for interactive and emotionally rich experiences grows, virtual pet companionship emerges as a novel yet underexplored application. Existing approaches focus on basic pet role-playing interactions without systematically benchmarking LLMs for comprehensive companionship. In this paper, we introduce Pet-Bench, a dedicated benchmark that evaluates LLMs across both self-interaction and human-interaction dimensions. Unlike prior work, Pet-Bench emphasizes self-evolution and developmental behaviors alongside interactive engagement, offering a more realistic reflection of pet companionship. It features diverse tasks such as intelligent scheduling, memory-based dialogues, and psychological conversations, with over 7,500 interaction instances designed to simulate complex pet behaviors. Evaluation of 28 LLMs reveals significant performance variations linked to model size and inherent capabilities, underscoring the need for specialized optimization in this domain. Pet-Bench serves as a foundational resource for benchmarking pet-related LLM abilities and advancing emotionally immersive human-pet interactions.

</details>


### [75] [AhaKV: Adaptive Holistic Attention-Driven KV Cache Eviction for Efficient Inference of Large Language Models](https://arxiv.org/abs/2506.03762)

*Yifeng Gu, Zicong Jiang, Jianxiu Jin, Kailing Guo, Ziyang Zhang, Xiangmin Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Key-Value cache, Adaptive holistic attention, Bias reduction, Token importance

**Relevance Score:** 8

**TL;DR:** This paper introduces Adaptive holistic attention KV (AhaKV), a method to reduce bias in Key-Value (KV) cache used in Large Language Models (LLMs) during inference, leading to better retention of important tokens and improved performance on benchmark tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of Large Language Models is resource-intensive due to the memory consumption of the KV cache, and existing methods for token eviction rely on a biased attention score.

**Method:** AhaKV adapts the scale of the softmax function based on the expectation of information entropy of attention scores rather than solely relying on accumulated attention scores.

**Key Contributions:**

	1. Introduction of AhaKV for adaptive tuning of softmax scale
	2. Utilization of value vectors to refine adaptive scoring
	3. Demonstration of state-of-the-art results on benchmark tasks

**Result:** Experiments demonstrate that AhaKV mitigates bias in token retention and achieves state-of-the-art results across several benchmark tasks compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed AhaKV method effectively reduces bias and improves the retention of important tokens, allowing LLMs to better utilize global context during inference.

**Abstract:** Large Language Models (LLMs) have significantly advanced the field of Artificial Intelligence. However, their deployment is resource-intensive, not only due to the large number of model parameters but also because the (Key-Value) KV cache consumes a lot of memory during inference. While several works propose reducing the KV cache by evicting the unnecessary tokens, these approaches rely on accumulated attention score as eviction score to quantify the importance of the token. We identify the accumulated attention score is biased and it decreases with the position of the tokens in the mathematical expectation. As a result, the retained tokens concentrate on the initial positions, limiting model's access to global contextual information. To address this issue, we propose Adaptive holistic attention KV (AhaKV), it addresses the bias of the accumulated attention score by adaptively tuning the scale of softmax according the expectation of information entropy of attention scores. To make use of the holistic attention information in self-attention mechanism, AhaKV utilize the information of value vectors, which is overlooked in previous works, to refine the adaptive score. We show theoretically that our method is well suited for bias reduction. We deployed AhaKV on different models with a fixed cache budget. Experiments show that AhaKV successfully mitigates bias and retains crucial tokens across global context and achieve state-of-the-art results against other related work on several benchmark tasks.

</details>


### [76] [ClozeMath: Improving Mathematical Reasoning in Language Models by Learning to Fill Equations](https://arxiv.org/abs/2506.03763)

*Quang Hieu Pham, Thuy Duong Nguyen, Tung Pham, Anh Tuan Luu, Dat Quoc Nguyen*

**Main category:** cs.CL

**Keywords:** large language models, mathematical reasoning, text-infilling, ClozeMath

**Relevance Score:** 9

**TL;DR:** ClozeMath is a new approach for enhancing large language models' mathematical reasoning through a text-infilling task that predicts masked equations from solutions, showing superior performance compared to existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve large language models' (LLMs) ability to perform mathematical reasoning by proposing a method inspired by human learning processes.

**Method:** ClozeMath uses a text-infilling task to predict masked equations from given solutions, akin to cloze exercises, and incorporates tests with Beam Search and Chain-of-Thought decoding algorithms.

**Key Contributions:**

	1. Introduction of ClozeMath for mathematical reasoning in LLMs
	2. Demonstration of superior performance compared to Masked Thought
	3. Analysis of implementation choices affecting performance.

**Result:** ClozeMath outperforms the baseline method Masked Thought in both performance and robustness across benchmarks like GSM8K and MATH.

**Limitations:** 

**Conclusion:** The method offers significant improvements in mathematical reasoning for LLMs and provides insights through an ablation study on architectural implementations.

**Abstract:** The capabilities of large language models (LLMs) have been enhanced by training on data that reflects human thought processes, such as the Chain-of-Thought format. However, evidence suggests that the conventional scheme of next-word prediction may not fully capture how humans learn to think. Inspired by how humans generalize mathematical reasoning, we propose a new approach named ClozeMath to fine-tune LLMs for mathematical reasoning. Our ClozeMath involves a text-infilling task that predicts masked equations from a given solution, analogous to cloze exercises used in human learning. Experiments on GSM8K, MATH, and GSM-Symbolic show that ClozeMath surpasses the strong baseline Masked Thought in performance and robustness, with two test-time scaling decoding algorithms, Beam Search and Chain-of-Thought decoding. Additionally, we conduct an ablation study to analyze the effects of various architectural and implementation choices on our approach.

</details>


### [77] [Unifying Uniform and Binary-coding Quantization for Accurate Compression of Large Language Models](https://arxiv.org/abs/2506.03781)

*Seungcheol Park, Jeongin Bae, Beomseok Kwon, Minjun Kim, Byeongwook Kim, Se Jung Kwon, U Kang, Dongsoo Lee*

**Main category:** cs.CL

**Keywords:** quantization, large language models, machine learning

**Relevance Score:** 6

**TL;DR:** Proposes UniQuanF, a quantization method for large language models that combines the strengths of binary-coding quantization and uniform quantization for improved accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To efficiently deploy large language models while preserving their accuracy through effective quantization methods.

**Method:** UniQuanF unifies flexible mapping from uniform quantization with non-uniform quantization levels from binary-coding quantization, incorporating unified initialization and local/periodic mapping techniques for precise optimization.

**Key Contributions:**

	1. Introduction of UniQuanF method for LLM quantization
	2. Combination of flexible mapping and non-uniform quantization
	3. Demonstrated significant accuracy improvement over existing methods

**Result:** UniQuanF outperforms existing quantization methods, achieving up to 4.60% higher accuracy on the GSM8K benchmark.

**Limitations:** 

**Conclusion:** The proposed method provides superior accuracy without incurring additional deployment costs related to the unification process.

**Abstract:** How can we quantize large language models while preserving accuracy? Quantization is essential for deploying large language models (LLMs) efficiently. Binary-coding quantization (BCQ) and uniform quantization (UQ) are promising quantization schemes that have strong expressiveness and optimizability, respectively. However, neither scheme leverages both advantages. In this paper, we propose UniQuanF (Unified Quantization with Flexible Mapping), an accurate quantization method for LLMs. UniQuanF harnesses both strong expressiveness and optimizability by unifying the flexible mapping technique in UQ and non-uniform quantization levels of BCQ. We propose unified initialization, and local and periodic mapping techniques to optimize the parameters in UniQuanF precisely. After optimization, our unification theorem removes computational and memory overhead, allowing us to utilize the superior accuracy of UniQuanF without extra deployment costs induced by the unification. Experimental results demonstrate that UniQuanF outperforms existing UQ and BCQ methods, achieving up to 4.60% higher accuracy on GSM8K benchmark.

</details>


### [78] [Knockout LLM Assessment: Using Large Language Models for Evaluations through Iterative Pairwise Comparisons](https://arxiv.org/abs/2506.03785)

*Isik Baran Sandan, Tu Anh Dinh, Jan Niehues*

**Main category:** cs.CL

**Keywords:** Large Language Models, evaluation, Knockout Assessment, pairwise comparisons, machine translation

**Relevance Score:** 8

**TL;DR:** Knockout Assessment improves LLM evaluation method using iterative pairwise comparisons.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance LLM evaluation capabilities by allowing continuous comparison for better global ranking.

**Method:** Knockout Assessment employs a tournament-style system allowing iterative pairwise comparisons to develop a comprehensive ranking.

**Key Contributions:**

	1. Introduces a knockout tournament system for LLM evaluations
	2. Demonstrates improved correlation with expert evaluations
	3. Offers a novel approach for developing global rankings in LLM assessments

**Result:** The new method improved scoring accuracy by increasing Pearson correlation with expert evaluations, particularly in university exam scoring and machine translation.

**Limitations:** 

**Conclusion:** Knockout Assessment aligns LLM evaluations more closely with human scoring than traditional methods.

**Abstract:** Large Language Models (LLMs) have shown to be effective evaluators across various domains such as machine translations or the scientific domain. Current LLM-as-a-Judge approaches rely mostly on individual assessments or a single round of pairwise assessments, preventing the judge LLM from developing a global ranking perspective. To address this, we present Knockout Assessment, an LLM-asa Judge method using a knockout tournament system with iterative pairwise comparisons. Experiments across three LLMs on two datasets show that knockout assessment improves scoring accuracy, increasing Pearson correlation with expert evaluations by 0.07 on average for university-level exam scoring and machine translation evaluations, aligning LLM assessments more closely with human scoring.

</details>


### [79] [Mark My Words: A Robust Multilingual Model for Punctuation in Text and Speech Transcripts](https://arxiv.org/abs/2506.03793)

*Sidharth Pulipaka, Sparsh Jain, Ashwin Sankar, Raj Dabre*

**Main category:** cs.CL

**Keywords:** punctuation restoration, NLP, multilingual models, spontaneous speech, Cadence

**Relevance Score:** 8

**TL;DR:** Cadence is a state-of-the-art model for punctuation restoration in both written and spontaneous spoken text, improving multilingual support and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current punctuation restoration models struggle with spontaneous speech and disfluencies, which affects downstream NLP tasks.

**Method:** Cadence adapts a pretrained large language model for punctuation restoration, focusing on both clean and spontaneous text across multiple languages.

**Key Contributions:**

	1. Introduces Cadence, a novel model for multilingual punctuation restoration.
	2. Expands punctuation restoration support from 14 to 22 Indian languages plus English.
	3. Provides a comprehensive analysis of model behavior across different punctuation types and languages.

**Result:** Cadence outperforms previous models in multilingual punctuation restoration, supporting all 22 Indian languages and English, addressing issues with rare punctuation in domain shifts.

**Limitations:** Challenges persist under domain shift and with rare punctuation marks.

**Conclusion:** The study illustrates the effectiveness of pretrained language models for enhancing punctuation restoration in low-resource NLP applications.

**Abstract:** Punctuation plays a vital role in structuring meaning, yet current models often struggle to restore it accurately in transcripts of spontaneous speech, especially in the presence of disfluencies such as false starts and backtracking. These limitations hinder the performance of downstream tasks like translation, text to speech, summarization, etc. where sentence boundaries are critical for preserving quality. In this work, we introduce Cadence, a generalist punctuation restoration model adapted from a pretrained large language model. Cadence is designed to handle both clean written text and highly spontaneous spoken transcripts. It surpasses the previous state of the art in performance while expanding support from 14 to all 22 Indian languages and English. We conduct a comprehensive analysis of model behavior across punctuation types and language families, identifying persistent challenges under domain shift and with rare punctuation marks. Our findings demonstrate the efficacy of utilizing pretrained language models for multilingual punctuation restoration and highlight Cadence practical value for low resource NLP pipelines at scale.

</details>


### [80] [Automatic Correction of Writing Anomalies in Hausa Texts](https://arxiv.org/abs/2506.03820)

*Ahmad Mustapha Wali, Sergiu Nisioi*

**Main category:** cs.CL

**Keywords:** Hausa, NLP, transformer models, text correction, low-resource languages

**Relevance Score:** 3

**TL;DR:** The paper proposes a method for correcting writing anomalies in Hausa texts using fine-tuned transformer models and a large dataset of noisy-clean sentence pairs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address writing anomalies in Hausa texts that hinder NLP applications by automatically correcting them.

**Method:** The approach involves fine-tuning transformer-based models on a large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs created from public sources with synthetically generated noise.

**Key Contributions:**

	1. Large-scale parallel dataset of noisy-clean Hausa sentence pairs
	2. Fine-tuned multilingual and African language models for anomaly correction
	3. Demonstrated improvement in NLP metrics for Hausa language

**Result:** The experiments showed significant improvements in F1, BLEU, and METEOR scores, and reductions in Character Error Rate (CER) and Word Error Rate (WER).

**Limitations:** 

**Conclusion:** The research offers a robust methodology, a publicly available dataset, and effective models for improving Hausa text quality, enhancing NLP capabilities for the language.

**Abstract:** Hausa texts are often characterized by writing anomalies such as incorrect character substitutions and spacing errors, which sometimes hinder natural language processing (NLP) applications. This paper presents an approach to automatically correct the anomalies by finetuning transformer-based models. Using a corpus gathered from several public sources, we created a large-scale parallel dataset of over 450,000 noisy-clean Hausa sentence pairs by introducing synthetically generated noise, fine-tuned to mimic realistic writing errors. Moreover, we adapted several multilingual and African language-focused models, including M2M100, AfriTEVA, mBART, and Opus-MT variants for this correction task using SentencePiece tokenization. Our experimental results demonstrate significant increases in F1, BLEU and METEOR scores, as well as reductions in Character Error Rate (CER) and Word Error Rate (WER). This research provides a robust methodology, a publicly available dataset, and effective models to improve Hausa text quality, thereby advancing NLP capabilities for the language and offering transferable insights for other low-resource languages.

</details>


### [81] [CRAWLDoc: A Dataset for Robust Ranking of Bibliographic Documents](https://arxiv.org/abs/2506.03822)

*Fabian Karl, Ansgar Scherp*

**Main category:** cs.CL

**Keywords:** metadata extraction, web documents, contextual ranking

**Relevance Score:** 4

**TL;DR:** CRAWLDoc is a method for contextual ranking of linked web documents to improve metadata extraction from diverse web sources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in metadata extraction due to variations in web layouts and data formats.

**Method:** CRAWLDoc retrieves a publication's landing page and linked resources, embedding them into a unified representation for ranking.

**Key Contributions:**

	1. Introduction of CRAWLDoc for contextual ranking of web documents.
	2. Creation of a new labeled dataset of 600 publications for evaluation.
	3. Demonstration of robust performance independent of layout and format.

**Result:** CRAWLDoc provides robust and layout-independent ranking of relevant documents across different publishers and data formats.

**Limitations:** 

**Conclusion:** CRAWLDoc lays a foundation for improved metadata extraction, offering a source code and dataset for further research.

**Abstract:** Publication databases rely on accurate metadata extraction from diverse web sources, yet variations in web layouts and data formats present challenges for metadata providers. This paper introduces CRAWLDoc, a new method for contextual ranking of linked web documents. Starting with a publication's URL, such as a digital object identifier, CRAWLDoc retrieves the landing page and all linked web resources, including PDFs, ORCID profiles, and supplementary materials. It embeds these resources, along with anchor texts and the URLs, into a unified representation. For evaluating CRAWLDoc, we have created a new, manually labeled dataset of 600 publications from six top publishers in computer science. Our method CRAWLDoc demonstrates a robust and layout-independent ranking of relevant documents across publishers and data formats. It lays the foundation for improved metadata extraction from web documents with various layouts and formats. Our source code and dataset can be accessed at https://github.com/FKarl/CRAWLDoc.

</details>


### [82] [Multi-objective Aligned Bidword Generation Model for E-commerce Search Advertising](https://arxiv.org/abs/2506.03827)

*Zhenhui Liu, Chunyuan Yuan, Ming Pang, Zheng Fang, Li Yuan, Xue Jiang, Changping Peng, Zhangang Lin, Zheng Luo, Jingping Shao*

**Main category:** cs.CL

**Keywords:** query rewriting, advertisement matching, multi-objective optimization

**Relevance Score:** 6

**TL;DR:** This paper presents a Multi-objective aligned Bidword Generation Model (MoBGM) to effectively match user queries with relevant advertisements in e-commerce search advertising.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of non-recall of advertisements due to the diversity of user queries and the limitations of current query rewriting methods.

**Method:** The proposed MoBGM uses a discriminator, generator, and preference alignment module to optimize the relevance and authenticity of user queries while maximizing ad revenue.

**Key Contributions:**

	1. Introduces MoBGM for multi-objective optimization in query rewriting.
	2. Implements a discriminator to balance query relevance and authenticity with ad revenue.
	3. Shows significant improvements over state-of-the-art methods.

**Result:** Extensive experiments demonstrate that MoBGM significantly outperforms existing methods and enhances commercial value for the platform.

**Limitations:** 

**Conclusion:** The algorithm's deployment has proven its feasibility and robustness in improving advertisement recall and user experience.

**Abstract:** Retrieval systems primarily address the challenge of matching user queries with the most relevant advertisements, playing a crucial role in e-commerce search advertising. The diversity of user needs and expressions often produces massive long-tail queries that cannot be matched with merchant bidwords or product titles, which results in some advertisements not being recalled, ultimately harming user experience and search efficiency. Existing query rewriting research focuses on various methods such as query log mining, query-bidword vector matching, or generation-based rewriting. However, these methods often fail to simultaneously optimize the relevance and authenticity of the user's original query and rewrite and maximize the revenue potential of recalled ads.   In this paper, we propose a Multi-objective aligned Bidword Generation Model (MoBGM), which is composed of a discriminator, generator, and preference alignment module, to address these challenges. To simultaneously improve the relevance and authenticity of the query and rewrite and maximize the platform revenue, we design a discriminator to optimize these key objectives. Using the feedback signal of the discriminator, we train a multi-objective aligned bidword generator that aims to maximize the combined effect of the three objectives. Extensive offline and online experiments show that our proposed algorithm significantly outperforms the state of the art. After deployment, the algorithm has created huge commercial value for the platform, further verifying its feasibility and robustness.

</details>


### [83] [Brain-tuned Speech Models Better Reflect Speech Processing Stages in the Brain](https://arxiv.org/abs/2506.03832)

*Omer Moussa, Mariya Toneva*

**Main category:** cs.CL

**Keywords:** brain-tuning, speech models, semantic understanding, hierarchical processing, acoustic features

**Relevance Score:** 7

**TL;DR:** Brain-tuned speech models better align with human speech processing hierarchies than pretrained models, showing improved semantics and structured processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how brain-tuning enhances speech models' alignment with human speech processing stages and semantics.

**Method:** Comparison of brain-tuned models with pretrained models, using layer-wise probing to assess semantic understanding and acoustic feature representation.

**Key Contributions:**

	1. Improvement in semantic alignment of late model layers through brain-tuning
	2. Demonstration of hierarchical processing in speech models
	3. Insights into the distribution of acoustic vs. semantic features in model layers.

**Result:** Late layers of brain-tuned models align substantially better with semantic language regions compared to pretrained models; early layers focus on acoustic features.

**Limitations:** 

**Conclusion:** Brain-tuned models exhibit a clearer hierarchical structure in speech processing, making them more suitable for understanding human speech.

**Abstract:** Pretrained self-supervised speech models excel in speech tasks but do not reflect the hierarchy of human speech processing, as they encode rich semantics in middle layers and poor semantics in late layers. Recent work showed that brain-tuning (fine-tuning models using human brain recordings) improves speech models' semantic understanding. Here, we examine how well brain-tuned models further reflect the brain's intermediate stages of speech processing. We find that late layers of brain-tuned models substantially improve over pretrained models in their alignment with semantic language regions. Further layer-wise probing reveals that early layers remain dedicated to low-level acoustic features, while late layers become the best at complex high-level tasks. These findings show that brain-tuned models not only perform better but also exhibit a well-defined hierarchical processing going from acoustic to semantic representations, making them better model organisms for human speech processing.

</details>


### [84] [PulseReddit: A Novel Reddit Dataset for Benchmarking MAS in High-Frequency Cryptocurrency Trading](https://arxiv.org/abs/2506.03861)

*Qiuhan Han, Qian Wang, Atsushi Yoshikawa, Masayuki Yamamura*

**Main category:** cs.CL

**Keywords:** High-Frequency Trading, PulseReddit, Multi-Agent Systems, Social Sentiment, Cryptocurrency

**Relevance Score:** 7

**TL;DR:** This paper introduces PulseReddit, a dataset that combines Reddit discussion data with cryptocurrency market statistics, and evaluates its impact on high-frequency trading using LLM-based Multi-Agent Systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage underexplored social media information for improving decision-making in high-frequency cryptocurrency trading.

**Method:** An empirical study using Large Language Model-based Multi-Agent Systems to analyze how social sentiment from PulseReddit influences trading performance.

**Key Contributions:**

	1. Introduction of the PulseReddit dataset for HFT analysis.
	2. Demonstration of the efficacy of LLM-based Multi-Agent Systems in trading.
	3. Insights into performance-efficiency trade-offs for LLM model selection.

**Result:** The experiments show that the Multi-Agent Systems with PulseReddit data outperform traditional trading models, especially in bull markets, and adapt well across various market conditions.

**Limitations:** 

**Conclusion:** The integration of social media insights significantly enhances trading strategies in high-frequency trading, offering a framework for future research and practical applications.

**Abstract:** High-Frequency Trading (HFT) is pivotal in cryptocurrency markets, demanding rapid decision-making. Social media platforms like Reddit offer valuable, yet underexplored, information for such high-frequency, short-term trading. This paper introduces \textbf{PulseReddit}, a novel dataset that is the first to align large-scale Reddit discussion data with high-frequency cryptocurrency market statistics for short-term trading analysis. We conduct an extensive empirical study using Large Language Model (LLM)-based Multi-Agent Systems (MAS) to investigate the impact of social sentiment from PulseReddit on trading performance. Our experiments conclude that MAS augmented with PulseReddit data achieve superior trading outcomes compared to traditional baselines, particularly in bull markets, and demonstrate robust adaptability across different market regimes. Furthermore, our research provides conclusive insights into the performance-efficiency trade-offs of different LLMs, detailing significant considerations for practical model selection in HFT applications. PulseReddit and our findings establish a foundation for advanced MAS research in HFT, demonstrating the tangible benefits of integrating social media.

</details>


### [85] [EuroGEST: Investigating gender stereotypes in multilingual language models](https://arxiv.org/abs/2506.03867)

*Jacqueline Rowe, Mateusz Klimaszewski, Liane Guillou, Shannon Vallor, Alexandra Birch*

**Main category:** cs.CL

**Keywords:** multilingual language models, gender bias, fairness, EuroGEST, language stereotypes

**Relevance Score:** 8

**TL;DR:** This paper introduces EuroGEST, a dataset for measuring gender-stereotypical reasoning in multilingual LLMs, revealing strong gender stereotypes encoded in different languages and highlighting the need for broader evaluations of fairness in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the lack of multilingual benchmarks for evaluating gender bias in large language models, as existing benchmarks are predominantly English-focused.

**Method:** The authors create the EuroGEST dataset by expanding an expert-informed benchmark using translation tools and quality estimation metrics, followed by human evaluations for accuracy.

**Key Contributions:**

	1. Introduction of EuroGEST dataset for multilingual gender bias measurement
	2. Demonstration of gender stereotypes across 30 languages
	3. Highlighting that larger models encode stereotypes more strongly

**Result:** The evaluation of 24 multilingual language models reveals that stereotypical representations of women and men vary across languages but persist, with larger models exhibiting stronger gendered stereotypes.

**Limitations:** The study may not encompass all language models or fully address all aspects of gender bias.

**Conclusion:** The study emphasizes the necessity for more comprehensive multilingual audits of fairness in LLMs and provides scalable methods and resources to facilitate this process.

**Abstract:** Large language models increasingly support multiple languages, yet most benchmarks for gender bias remain English-centric. We introduce EuroGEST, a dataset designed to measure gender-stereotypical reasoning in LLMs across English and 29 European languages. EuroGEST builds on an existing expert-informed benchmark covering 16 gender stereotypes, expanded in this work using translation tools, quality estimation metrics, and morphological heuristics. Human evaluations confirm that our data generation method results in high accuracy of both translations and gender labels across languages. We use EuroGEST to evaluate 24 multilingual language models from six model families, demonstrating that the strongest stereotypes in all models across all languages are that women are \textit{beautiful,} \textit{empathetic} and \textit{neat} and men are \textit{leaders}, \textit{strong, tough} and \textit{professional}. We also show that larger models encode gendered stereotypes more strongly and that instruction finetuning does not consistently reduce gendered stereotypes. Our work highlights the need for more multilingual studies of fairness in LLMs and offers scalable methods and resources to audit gender bias across languages.

</details>


### [86] [RadialRouter: Structured Representation for Efficient and Robust Large Language Models Routing](https://arxiv.org/abs/2506.03880)

*Ruihan Jin, Pengpeng Shao, Zhengqi Wen, Jinyang Wu, Mingkuan Feng, Shuai Zhang, Jianhua Tao*

**Main category:** cs.CL

**Keywords:** large language models, routing techniques, Transformer, RadialFormer, cost optimization

**Relevance Score:** 9

**TL;DR:** RadialRouter is a novel framework for optimizing large language model routing, leveraging a lightweight Transformer-based architecture to improve LLM selection based on user queries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current routing methods for selecting large language models (LLMs) are ineffective due to limited exploration of the relationship between user queries and LLM characteristics.

**Method:** RadialRouter employs a Transformer-based structure, RadialFormer, to articulate the query-LLM relationship, and uses an objective function combining Kullback-Leibler divergence with query-query contrastive loss for optimization.

**Key Contributions:**

	1. Introduction of RadialRouter framework for LLM routing
	2. Use of RadialFormer structure for query-LLM relationship
	3. Enhanced routing performance with innovative objective function

**Result:** RadialRouter significantly outperforms existing methods, showing improvements of 9.2% and 5.8% on RouterBench in Balance and Cost First scenarios, respectively.

**Limitations:** 

**Conclusion:** RadialRouter demonstrates strong adaptability to diverse performance-cost trade-offs and suggests practical application potential in LLM routing.

**Abstract:** The rapid advancements in large language models (LLMs) have led to the emergence of routing techniques, which aim to efficiently select the optimal LLM from diverse candidates to tackle specific tasks, optimizing performance while reducing costs. Current LLM routing methods are limited in effectiveness due to insufficient exploration of the intrinsic connection between user queries and the characteristics of LLMs. To address this issue, in this paper, we present RadialRouter, a novel framework for LLM routing which employs a lightweight Transformer-based backbone with a radial structure named RadialFormer to articulate the query-LLMs relationship. The optimal LLM selection is performed based on the final states of RadialFormer. The pipeline is further refined by an objective function that combines Kullback-Leibler divergence with the query-query contrastive loss to enhance robustness. Experimental results on RouterBench show that RadialRouter significantly outperforms existing routing methods by 9.2\% and 5.8\% in the Balance and Cost First scenarios, respectively. Additionally, its adaptability toward different performance-cost trade-offs and the dynamic LLM pool demonstrates practical application potential.

</details>


### [87] [Kinship in Speech: Leveraging Linguistic Relatedness for Zero-Shot TTS in Indian Languages](https://arxiv.org/abs/2506.03884)

*Utkarsh Pathak, Chandra Sai Krishna Gunda, Anusha Prakash, Keshav Agarwal, Hema A. Murthy*

**Main category:** cs.CL

**Keywords:** Text-to-Speech, Zero-shot synthesis, Indian languages

**Relevance Score:** 5

**TL;DR:** The paper presents a zero-shot text-to-speech (TTS) synthesis approach for under-represented Indian languages using shared phone representations and modified text parsing rules.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of training TTS systems for the 1369 languages of India, many of which lack digital resources.

**Method:** The study employs zero-shot synthesis techniques by augmenting a shared phone representation and adjusting text parsing rules based on the phonotactics of the target language.

**Key Contributions:**

	1. Development of zero-shot TTS synthesis for multiple Indian languages
	2. Utilization of shared phone representation for cross-language synthesis
	3. Modification of text parsing rules to adapt to target language phonotactics

**Result:** Intelligible and natural speech for languages such as Sanskrit, Maharashtrian and Canara Konkani, Maithili, and Kurukh was generated, proving the method's efficacy.

**Limitations:** 

**Conclusion:** The approach demonstrates potential for expanding access to speech technology for under-represented languages in India.

**Abstract:** Text-to-speech (TTS) systems typically require high-quality studio data and accurate transcriptions for training. India has 1369 languages, with 22 official using 13 scripts. Training a TTS system for all these languages, most of which have no digital resources, seems a Herculean task. Our work focuses on zero-shot synthesis, particularly for languages whose scripts and phonotactics come from different families. The novelty of our work is in the augmentation of a shared phone representation and modifying the text parsing rules to match the phonotactics of the target language, thus reducing the synthesiser overhead and enabling rapid adaptation. Intelligible and natural speech was generated for Sanskrit, Maharashtrian and Canara Konkani, Maithili and Kurukh by leveraging linguistic connections across languages with suitable synthesisers. Evaluations confirm the effectiveness of this approach, highlighting its potential to expand speech technology access for under-represented languages.

</details>


### [88] [Pre$^3$: Enabling Deterministic Pushdown Automata for Faster Structured LLM Generation](https://arxiv.org/abs/2506.03887)

*Junyi Chen, Shihao Bai, Zaijun Wang, Siyu Wu, Chuheng Du, Hailong Yang, Ruihao Gong, Shengzhong Liu, Fan Wu, Guihai Chen*

**Main category:** cs.CL

**Keywords:** LLM, structured generation, LR(1) grammars, deterministic pushdown automata, efficiency

**Relevance Score:** 8

**TL;DR:** Pre$^3$ is a method that optimizes structured generation in LLMs using deterministic pushdown automata for better efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve output generation efficiency for large language models (LLMs) when handling LR(1) grammars, focusing on reducing runtime execution overhead during context-dependent token processing.

**Method:** Pre$^3$ utilizes precomputed prefix-conditioned edges for parallel transition processing and transforms LR(1) transition graphs into deterministic pushdown automata (DPDA) to minimize runtime path exploration overhead.

**Key Contributions:**

	1. Introduced a method to precompute edges for efficient parallel processing
	2. Transformed LR(1) grammars into DPDA to minimize runtime exploration
	3. Demonstrated substantial improvements in output token generation efficiency

**Result:** Pre$^3$ significantly reduces the time per output token by up to 40% and increases throughput by up to 36% in experimental setups.

**Limitations:** 

**Conclusion:** Integrating Pre$^3$ into standard LLM inference frameworks can greatly enhance decoding efficiency without compromising performance.

**Abstract:** Extensive LLM applications demand efficient structured generations, particularly for LR(1) grammars, to produce outputs in specified formats (e.g., JSON). Existing methods primarily parse LR(1) grammars into a pushdown automaton (PDA), leading to runtime execution overhead for context-dependent token processing, especially inefficient under large inference batches. To address these issues, we propose Pre$^3$ that exploits deterministic pushdown automata (DPDA) to optimize the constrained LLM decoding efficiency. First, by precomputing prefix-conditioned edges during the preprocessing, Pre$^3$ enables ahead-of-time edge analysis and thus makes parallel transition processing possible. Second, by leveraging the prefix-conditioned edges, Pre$^3$ introduces a novel approach that transforms LR(1) transition graphs into DPDA, eliminating the need for runtime path exploration and achieving edge transitions with minimal overhead. Pre$^3$ can be seamlessly integrated into standard LLM inference frameworks, reducing time per output token (TPOT) by up to 40% and increasing throughput by up to 36% in our experiments. Our code is available at https://github.com/ModelTC/lightllm.

</details>


### [89] [Magic Mushroom: A Customizable Benchmark for Fine-grained Analysis of Retrieval Noise Erosion in RAG Systems](https://arxiv.org/abs/2506.03901)

*Yuxin Zhang, Yan Wang, Yongrui Chen, Shenyu Zhang, Xinbang Dai, Sheng Bi, Guilin Qi*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Benchmarking, Noise Robustness, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces Magic Mushroom, a benchmark for evaluating RAG systems under various types of real-world retrieval noise, aiming to improve performance robustness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the sensitivity of RAG systems to retrieval noise that is common in real-world applications and improve their robustness assessment through better benchmarks.

**Method:** The authors define four categories of retrieval noise based on linguistic properties and introduce Magic Mushroom, a benchmark consisting of 7,468 single-hop and 3,925 multi-hop question-answer pairs. They evaluate LLM generators and denoising strategies under diverse noise distributions to assess performance dynamics.

**Key Contributions:**

	1. Definition of four categories of retrieval noise
	2. Introduction of the Magic Mushroom benchmark for evaluating RAG systems
	3. Evaluation of LLM generators and denoising strategies under various noise distributions

**Result:** The analysis shows that current LLM generators and denoising strategies are extremely sensitive to noise distributions and have significant room for improvement.

**Limitations:** 

**Conclusion:** Magic Mushroom serves as a promising tool for evaluating RAG systems under realistic noise conditions, facilitating better deployment in practical applications.

**Abstract:** Retrieval-Augmented Generation (RAG) systems enhance Large Language Models (LLMs) by incorporating external retrieved information, mitigating issues such as hallucination and outdated knowledge.   However, RAG systems are highly sensitive to retrieval noise prevalent in real-world scenarios.   Existing benchmarks fail to emulate the complex and heterogeneous noise distributions encountered in real-world retrieval environments, undermining reliable robustness assessment.   In this paper, we define four categories of retrieval noise based on linguistic properties and noise characteristics, aiming to reflect the heterogeneity of noise in real-world scenarios.   Building on this, we introduce Magic Mushroom, a benchmark for replicating "magic mushroom" noise: contexts that appear relevant on the surface but covertly mislead RAG systems.   Magic Mushroom comprises 7,468 single-hop and 3,925 multi-hop question-answer pairs.   More importantly, Magic Mushroom enables researchers to flexibly configure combinations of retrieval noise according to specific research objectives or application scenarios, allowing for highly controlled evaluation setups.   We evaluate LLM generators of varying parameter scales and classic RAG denoising strategies under diverse noise distributions to investigate their performance dynamics during progressive noise encroachment.   Our analysis reveals that both generators and denoising strategies have significant room for improvement and exhibit extreme sensitivity to noise distributions.   Magic Mushroom emerges as a promising tool for evaluating and advancing noise-robust RAG systems, accelerating their widespread deployment in real-world applications.   The Magic Mushroom benchmark is available at the https://drive.google.com/file/d/1aP5kyPuk4L-L_uoI6T9UhxuTyt8oMqjT/view?usp=sharing.

</details>


### [90] [The Harmonic Structure of Information Contours](https://arxiv.org/abs/2506.03902)

*Eleftheria Tsipidi, Samuel Kiegeland, Franz Nowak, Tianyang Xu, Ethan Wilcox, Alex Warstadt, Ryan Cotterell, Mario Giulianelli*

**Main category:** cs.CL

**Keywords:** information density, periodicity, linguistic structure, harmonic regression, discourse analysis

**Relevance Score:** 4

**TL;DR:** This paper explores how fluctuations in information density in language are influenced by periodic rhythms, proposing a framework for analyzing these patterns across multiple languages.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to provide a deeper understanding of how information is structured in language and the underlying pressures that contribute to this organization.

**Method:** The authors employ harmonic regression and a new technique called time scaling to analyze information density patterns in texts from various languages.

**Key Contributions:**

	1. Introduction of time scaling for harmonic regression in linguistic analysis
	2. Evidence of periodic patterns in information rates across different languages
	3. Correlational findings between information oscillation and discourse structure

**Result:** The analysis reveals consistent periodic patterns in information rate across several languages, with dominant frequencies correlating with discourse structure.

**Limitations:** 

**Conclusion:** The findings suggest that these periodic oscillations in information rate not only indicate linguistic organization but also provide a framework for studying structural pressures in language.

**Abstract:** The uniform information density (UID) hypothesis proposes that speakers aim to distribute information evenly throughout a text, balancing production effort and listener comprehension difficulty. However, language typically does not maintain a strictly uniform information rate; instead, it fluctuates around a global average. These fluctuations are often explained by factors such as syntactic constraints, stylistic choices, or audience design. In this work, we explore an alternative perspective: that these fluctuations may be influenced by an implicit linguistic pressure towards periodicity, where the information rate oscillates at regular intervals, potentially across multiple frequencies simultaneously. We apply harmonic regression and introduce a novel extension called time scaling to detect and test for such periodicity in information contours. Analyzing texts in English, Spanish, German, Dutch, Basque, and Brazilian Portuguese, we find consistent evidence of periodic patterns in information rate. Many dominant frequencies align with discourse structure, suggesting these oscillations reflect meaningful linguistic organization. Beyond highlighting the connection between information rate and discourse structure, our approach offers a general framework for uncovering structural pressures at various levels of linguistic granularity.

</details>


### [91] [When Fairness Isn't Statistical: The Limits of Machine Learning in Evaluating Legal Reasoning](https://arxiv.org/abs/2506.03913)

*Claire Barale, Michael Rovatsos, Nehal Bhuta*

**Main category:** cs.CL

**Keywords:** Legal fairness, Machine Learning, Refugee adjudication, Statistical evaluation, Legal reasoning

**Relevance Score:** 3

**TL;DR:** This paper evaluates the effectiveness of machine learning techniques in assessing fairness in legal decisions, particularly in refugee adjudication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the fairness of legal decisions in high-stakes domains like refugee adjudication using machine learning techniques.

**Method:** The study empirically evaluates three common ML approaches—feature-based analysis, semantic clustering, and predictive modeling—on a dataset containing over 59,000 Canadian refugee decisions.

**Key Contributions:**

	1. Empirical evaluation of common ML approaches in the context of legal fairness.
	2. Highlighting the inconsistencies and limitations of ML in assessing legal outcomes.
	3. Arguing for a reevaluation of fairness assessment methods in law, emphasizing the need for legal reasoning.

**Result:** The results indicate that different ML methods yield inconsistent and sometimes contradictory insights, highlighting that predictive modeling often relies on context rather than legal characteristics, and that semantic clustering fails to reflect substantive legal reasoning.

**Limitations:** The study emphasizes the limitations of statistical methods in evaluating fairness, particularly in discretionary legal contexts.

**Conclusion:** The authors argue that statistical fairness evaluation has significant limitations in legal contexts characterized by discretion, and they advocate for integrating legal reasoning with data-driven methods.

**Abstract:** Legal decisions are increasingly evaluated for fairness, consistency, and bias using machine learning (ML) techniques. In high-stakes domains like refugee adjudication, such methods are often applied to detect disparities in outcomes. Yet it remains unclear whether statistical methods can meaningfully assess fairness in legal contexts shaped by discretion, normative complexity, and limited ground truth.   In this paper, we empirically evaluate three common ML approaches (feature-based analysis, semantic clustering, and predictive modeling) on a large, real-world dataset of 59,000+ Canadian refugee decisions (AsyLex). Our experiments show that these methods produce divergent and sometimes contradictory signals, that predictive modeling often depends on contextual and procedural features rather than legal features, and that semantic clustering fails to capture substantive legal reasoning.   We show limitations of statistical fairness evaluation, challenge the assumption that statistical regularity equates to fairness, and argue that current computational approaches fall short of evaluating fairness in legally discretionary domains. We argue that evaluating fairness in law requires methods grounded not only in data, but in legal reasoning and institutional context.

</details>


### [92] [Compositional Generalisation for Explainable Hate Speech Detection](https://arxiv.org/abs/2506.03916)

*Agostina Calabrese, Tom Sherborne, Björn Ross, Mirella Lapata*

**Main category:** cs.CL

**Keywords:** hate speech detection, dataset bias, generalization, compositional generalization, NLP

**Relevance Score:** 7

**TL;DR:** This paper addresses the challenges in hate speech detection models, particularly their inability to generalize beyond training data due to biases and inadequate labeling methods.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current hate speech detection models struggle with generalization due to dataset biases and the limitations of sentence-level labels.

**Method:** The authors introduce U-PLEAD, a dataset comprising ~364,000 synthetic posts and a compositional generalisation benchmark with ~8,000 validated posts, and investigate its impact on model training.

**Key Contributions:**

	1. Introduction of U-PLEAD dataset with balanced expression occurrences
	2. Development of a novel compositional generalisation benchmark
	3. Demonstration of improved performance on PLEAD dataset

**Result:** Training on a combination of the U-PLEAD dataset and real data improves compositional generalisation and achieves state-of-the-art performance on the human-sourced PLEAD dataset.

**Limitations:** 

**Conclusion:** The findings suggest that more balanced and contextually varied training data can enhance model performance in hate speech detection applications.

**Abstract:** Hate speech detection is key to online content moderation, but current models struggle to generalise beyond their training data. This has been linked to dataset biases and the use of sentence-level labels, which fail to teach models the underlying structure of hate speech. In this work, we show that even when models are trained with more fine-grained, span-level annotations (e.g., "artists" is labeled as target and "are parasites" as dehumanising comparison), they struggle to disentangle the meaning of these labels from the surrounding context. As a result, combinations of expressions that deviate from those seen during training remain particularly difficult for models to detect. We investigate whether training on a dataset where expressions occur with equal frequency across all contexts can improve generalisation. To this end, we create U-PLEAD, a dataset of ~364,000 synthetic posts, along with a novel compositional generalisation benchmark of ~8,000 manually validated posts. Training on a combination of U-PLEAD and real data improves compositional generalisation while achieving state-of-the-art performance on the human-sourced PLEAD.

</details>


### [93] [HSSBench: Benchmarking Humanities and Social Sciences Ability for Multimodal Large Language Models](https://arxiv.org/abs/2506.03922)

*Zhaolu Kang, Junhao Gong, Jiaxu Yan, Wanke Xia, Yian Wang, Ziwen Wang, Huaxuan Ding, Zhuo Cheng, Wenhao Cao, Zhiyuan Feng, Siqi He, Shannan Yan, Junzhe Chen, Xiaomin He, Chaoya Jiang, Wei Ye, Kaidong Yu, Xuelong Li*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Humanities and Social Sciences, Benchmarking

**Relevance Score:** 6

**TL;DR:** HSSBench is a benchmark for evaluating Multimodal Large Language Models (MLLMs) on Humanities and Social Sciences tasks, addressing the unique challenges in interdisciplinary reasoning and visual concept association.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for MLLMs focus on STEM disciplines and neglect the distinct needs of the Humanities and Social Sciences (HSS), which require interdisciplinary thinking and deep integration of knowledge.

**Method:** Introduction of HSSBench, a benchmark designed to assess MLLM capabilities on HSS tasks in multiple languages, supported by a novel data generation pipeline involving domain experts and automated agents.

**Key Contributions:**

	1. Development of HSSBench benchmark for multidisciplinary evaluation of MLLMs
	2. Introduction of a novel data generation pipeline for HSS tasks
	3. Benchmarking results that demonstrate challenges for existing MLLMs in HSS tasks

**Result:** Benchmarking over 20 mainstream MLLMs on HSSBench revealed significant challenges for state-of-the-art models while highlighting the necessity for improved cross-disciplinary reasoning capabilities.

**Limitations:** 

**Conclusion:** HSSBench aims to inspire further research into the interdisciplinary reasoning abilities of MLLMs, emphasizing the need for models to connect knowledge across different fields.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated significant potential to advance a broad range of domains. However, current benchmarks for evaluating MLLMs primarily emphasize general knowledge and vertical step-by-step reasoning typical of STEM disciplines, while overlooking the distinct needs and potential of the Humanities and Social Sciences (HSS). Tasks in the HSS domain require more horizontal, interdisciplinary thinking and a deep integration of knowledge across related fields, which presents unique challenges for MLLMs, particularly in linking abstract concepts with corresponding visual representations. Addressing this gap, we present HSSBench, a dedicated benchmark designed to assess the capabilities of MLLMs on HSS tasks in multiple languages, including the six official languages of the United Nations. We also introduce a novel data generation pipeline tailored for HSS scenarios, in which multiple domain experts and automated agents collaborate to generate and iteratively refine each sample. HSSBench contains over 13,000 meticulously designed samples, covering six key categories. We benchmark more than 20 mainstream MLLMs on HSSBench and demonstrate that it poses significant challenges even for state-of-the-art models. We hope that this benchmark will inspire further research into enhancing the cross-disciplinary reasoning abilities of MLLMs, especially their capacity to internalize and connect knowledge across fields.

</details>


### [94] [More or Less Wrong: A Benchmark for Directional Bias in LLM Comparative Reasoning](https://arxiv.org/abs/2506.03923)

*Mohammadamin Shafiei, Hamidreza Saffari, Nafise Sadat Moosavi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Framing Bias, Comparative Reasoning, Benchmarking, Demographic Identity

**Relevance Score:** 8

**TL;DR:** The paper investigates how semantic cues in comparative math problems influence the reasoning of large language models (LLMs), revealing a framing bias and proposing a benchmark called MathComp to evaluate this effect.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanisms by which semantic cues impact reasoning in LLMs, specifically in the context of comparative math problems that have objective ground truth.

**Method:** The authors introduced MathComp, a controlled benchmark consisting of 300 comparison scenarios, each evaluated with 14 prompt variants across three different LLM families to study the impact of linguistic steering.

**Key Contributions:**

	1. Introduction of the MathComp benchmark for comparative reasoning in LLMs
	2. Identification of systematic linguistic steering in LLM predictions
	3. Demonstration of the impact of demographic identity terms on model biases

**Result:** The study found that the presence of terms like 'more', 'less', or 'equal' biases model predictions towards those terms. Errors often reflected this bias, and chain-of-thought prompting could reduce but not eliminate the bias, with free-form reasoning being more effective than structured formats.

**Limitations:** The study focuses on a specific context of comparative math problems, which may not generalize to all reasoning tasks.

**Conclusion:** The findings suggest the need for framing-aware benchmarks in LLM assessments to improve reasoning robustness and fairness, especially considering that demographic identity terms can amplify biases.

**Abstract:** Large language models (LLMs) are known to be sensitive to input phrasing, but the mechanisms by which semantic cues shape reasoning remain poorly understood. We investigate this phenomenon in the context of comparative math problems with objective ground truth, revealing a consistent and directional framing bias: logically equivalent questions containing the words ``more'', ``less'', or ``equal'' systematically steer predictions in the direction of the framing term. To study this effect, we introduce MathComp, a controlled benchmark of 300 comparison scenarios, each evaluated under 14 prompt variants across three LLM families. We find that model errors frequently reflect linguistic steering, systematic shifts toward the comparative term present in the prompt. Chain-of-thought prompting reduces these biases, but its effectiveness varies: free-form reasoning is more robust, while structured formats may preserve or reintroduce directional drift. Finally, we show that including demographic identity terms (e.g., ``a woman'', ``a Black person'') in input scenarios amplifies directional drift, despite identical underlying quantities, highlighting the interplay between semantic framing and social referents. These findings expose critical blind spots in standard evaluation and motivate framing-aware benchmarks for diagnosing reasoning robustness and fairness in LLMs.

</details>


### [95] [Hanging in the Balance: Pivotal Moments in Crisis Counseling Conversations](https://arxiv.org/abs/2506.03941)

*Vivian Nguyen, Lillian Lee, Cristian Danescu-Niculescu-Mizil*

**Main category:** cs.CL

**Keywords:** pivotal moments, conversation analysis, mental health counseling, unsupervised learning, real-time detection

**Relevance Score:** 9

**TL;DR:** A method for detecting pivotal moments in conversations, particularly in mental health counseling, that can drastically change outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist conversationalists in crucial moments during conversations, particularly in high-stakes situations like mental health crisis counseling.

**Method:** An unsupervised computational method that detects pivotal moments in real-time, based on the variation in expected outcomes from potential participant responses.

**Key Contributions:**

	1. Introduction of an unsupervised method for real-time detection of pivotal conversational moments.
	2. Validation of the method through alignment with human perception and conversational outcomes.
	3. Exploration of counselor response strategies during pivotal moments and their impact on session outcomes.

**Result:** The method aligns with human perception of pivotal moments; counselors take longer to respond during these detected moments, which correlates with a significant change in conversational trajectory.

**Limitations:** May require adaptation for different conversational contexts outside mental health counseling.

**Conclusion:** The framework not only identifies pivotal moments but also explores how counselor responses during these moments relate to the outcomes of counseling sessions.

**Abstract:** During a conversation, there can come certain moments where its outcome hangs in the balance. In these pivotal moments, how one responds can put the conversation on substantially different trajectories leading to significantly different outcomes. Systems that can detect when such moments arise could assist conversationalists in domains with highly consequential outcomes, such as mental health crisis counseling.   In this work, we introduce an unsupervised computational method for detecting such pivotal moments as they happen, in an online fashion. Our approach relies on the intuition that a moment is pivotal if our expectation of the outcome varies widely depending on what might be said next. By applying our method to crisis counseling conversations, we first validate it by showing that it aligns with human perception -- counselors take significantly longer to respond during moments detected by our method -- and with the eventual conversational trajectory -- which is more likely to change course at these times. We then use our framework to explore the relation of the counselor's response during pivotal moments with the eventual outcome of the session.

</details>


### [96] [TableEval: A Real-World Benchmark for Complex, Multilingual, and Multi-Structured Table Question Answering](https://arxiv.org/abs/2506.03949)

*Junnan Zhu, Jingyi Wang, Bohan Yu, Xiaoyu Wu, Junbo Li, Lei Wang, Nan Xu*

**Main category:** cs.CL

**Keywords:** TableQA, Benchmarks, LLMs, Evaluation Framework, Cross-lingual

**Relevance Score:** 8

**TL;DR:** Introduction of a new benchmark, TableEval, to evaluate LLMs in TableQA tasks including diverse table structures and cross-lingual scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing TableQA benchmarks are limited in scope, focusing on simple tables, suffering from data leakage, and lacking cross-domain and cross-lingual tests.

**Method:** TableEval consists of diverse table structures from multiple domains, and introduces SEAT, an evaluation framework assessing model responses at the sub-question level.

**Key Contributions:**

	1. Introduction of the TableEval benchmark
	2. Development of the SEAT evaluation framework
	3. Inclusion of cross-lingual and multi-domain table structures

**Result:** Experimental results on TableEval indicate significant gaps in state-of-the-art LLMs' capabilities in handling complex TableQA tasks.

**Limitations:** The benchmark might still have limitations in fully replicating every real-world scenario.

**Conclusion:** The development of TableEval and SEAT provides a necessary framework and dataset for evaluating LLMs on real-world TableQA challenges, encouraging improvements in this area.

**Abstract:** LLMs have shown impressive progress in natural language processing. However, they still face significant challenges in TableQA, where real-world complexities such as diverse table structures, multilingual data, and domain-specific reasoning are crucial. Existing TableQA benchmarks are often limited by their focus on simple flat tables and suffer from data leakage. Furthermore, most benchmarks are monolingual and fail to capture the cross-lingual and cross-domain variability in practical applications. To address these limitations, we introduce TableEval, a new benchmark designed to evaluate LLMs on realistic TableQA tasks. Specifically, TableEval includes tables with various structures (such as concise, hierarchical, and nested tables) collected from four domains (including government, finance, academia, and industry reports). Besides, TableEval features cross-lingual scenarios with tables in Simplified Chinese, Traditional Chinese, and English. To minimize the risk of data leakage, we collect all data from recent real-world documents. Considering that existing TableQA metrics fail to capture semantic accuracy, we further propose SEAT, a new evaluation framework that assesses the alignment between model responses and reference answers at the sub-question level. Experimental results have shown that SEAT achieves high agreement with human judgment. Extensive experiments on TableEval reveal critical gaps in the ability of state-of-the-art LLMs to handle these complex, real-world TableQA tasks, offering insights for future improvements. We make our dataset available here: https://github.com/wenge-research/TableEval.

</details>


### [97] [From Real to Synthetic: Synthesizing Millions of Diversified and Complicated User Instructions with Attributed Grounding](https://arxiv.org/abs/2506.03968)

*Chiwei Zhu, Benfeng Xu, Xiaorui Wang, Zhendong Mao*

**Main category:** cs.CL

**Keywords:** instruction synthesis, large language models, grounding, web documents, datasets

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for synthesizing diverse and complex instructions for large language models using attributed grounding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for diverse and meaningful instruction data for automatically aligning large language models (LLMs) is critical, yet existing methods fall short in complexity or grounding.

**Method:** The framework involves a top-down attribution process grounding real instructions to specific users, and a bottom-up synthesis process generating situations and meaningful instructions from web documents.

**Key Contributions:**

	1. Introduction of attributed grounding for synthesizing instructions
	2. Development of the SynthQuestions dataset with 1 million instructions
	3. Models trained on SynthQuestions achieve leading performance benchmarks.

**Result:** The construction of a dataset named SynthQuestions with 1 million instructions, which led to improved performance on several benchmarks for models trained on this data, scaling performance with more web corpora.

**Limitations:** 

**Conclusion:** The proposed method demonstrates that it is possible to efficiently harvest diverse and complex instructions at scale using the resources available on the web.

**Abstract:** The pursuit of diverse, complex, and large-scale instruction data is crucial for automatically aligning large language models (LLMs). While there are methods capable of generating synthetic instructions at scale, they either suffer from limited grounding sources, leading to a narrow distribution, or rely on trivial extensions that fail to produce meaningful trajectories in terms of complexity. In contrast, instructions that benefit efficient alignment are typically crafted with cognitive insights and grounded in real-world use cases. In this paper, we synthesize such instructions using attributed grounding, which involves 1) a top-down attribution process that grounds a selective set of real instructions to situated users, and 2) a bottom-up synthesis process that leverages web documents to first generate a situation, then a meaningful instruction. This framework allows us to harvest diverse and complex instructions at scale, utilizing the vast range of web documents. Specifically, we construct a dataset of 1 million instructions, called SynthQuestions, and demonstrate that models trained on it achieve leading performance on several common benchmarks, with improvements that continually scale with more web corpora. Data, models and codes will be available at https://github.com/Ignoramus0817/SynthQuestions.

</details>


### [98] [Structured Pruning for Diverse Best-of-N Reasoning Optimization](https://arxiv.org/abs/2506.03978)

*Hieu Trung Nguyen, Bao Nguyen, Viet Anh Nguyen*

**Main category:** cs.CL

**Keywords:** model pruning, transformer models, contrastive learning, reasoning performance, NLP

**Relevance Score:** 8

**TL;DR:** This paper presents SPRINT, a novel contrastive learning framework that improves reasoning performance in transformer-based language models through selective pruning of attention heads.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the unexpected benefit of model pruning, which traditionally focuses on computational savings, on reasoning capabilities in language models.

**Method:** SPRINT dynamically selects optimal attention heads and layers for pruning during inference by aligning question embeddings with head embeddings.

**Key Contributions:**

	1. Introduction of SPRINT, a dynamic pruning method for transformers
	2. Demonstration of improved reasoning results on benchmark datasets
	3. Insight into the relationship between head pruning and reasoning performance

**Result:** SPRINT significantly outperforms existing strategies in head selection on reasoning tasks, specifically on the MATH500 and GSM8K datasets.

**Limitations:** 

**Conclusion:** Selective pruning through SPRINT not only saves computation but also enhances reasoning performance, challenging traditional views on model pruning.

**Abstract:** Model pruning in transformer-based language models, traditionally viewed as a means of achieving computational savings, can enhance the model's reasoning capabilities. In this work, we uncover a surprising phenomenon: the selective pruning of certain attention heads leads to improvements in reasoning performance, particularly on challenging tasks. Motivated by this observation, we propose SPRINT, a novel contrastive learning framework that dynamically selects the optimal head and layer to prune during inference. By aligning question embeddings with head embeddings, SPRINT identifies those pruned-head configurations that result in more accurate reasoning. Extensive experiments demonstrate that our method significantly outperforms traditional best-of-$N$ and random head selection strategies on the MATH500 and GSM8K datasets.

</details>


### [99] [Voice Activity Projection Model with Multimodal Encoders](https://arxiv.org/abs/2506.03980)

*Takeshi Saga, Catherine Pelachaud*

**Main category:** cs.CL

**Keywords:** turn-taking, human-machine interaction, multimodal, voice activity projection, pre-trained encoders

**Relevance Score:** 8

**TL;DR:** A multimodal model using pre-trained audio and face encoders improves turn-taking prediction in human-machine interaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance turn-taking management in human-machine interactions by utilizing multimodal inputs that capture subtle expressions, overcoming limitations of conventional models.

**Method:** The proposed model integrates pre-trained audio and face encoders, leveraging a unified representation of turn-taking behaviors to predict interactions more effectively.

**Key Contributions:**

	1. Introduction of a multimodal approach combining audio and facial encoders for turn-taking prediction.
	2. Demonstration of improved performance over existing models in certain metrics.
	3. Availability of source codes and pretrained models for further research.

**Result:** The multimodal model showed competitive performance, outperforming previous state-of-the-art models on turn-taking metrics in some evaluations.

**Limitations:** 

**Conclusion:** The incorporation of audio and facial expression encoders significantly enhances the prediction of turn-taking behaviors in human-machine interactions, thus demonstrating the potential for improved interactive systems.

**Abstract:** Turn-taking management is crucial for any social interaction. Still, it is challenging to model human-machine interaction due to the complexity of the social context and its multimodal nature. Unlike conventional systems based on silence duration, previous existing voice activity projection (VAP) models successfully utilized a unified representation of turn-taking behaviors as prediction targets, which improved turn-taking prediction performance. Recently, a multimodal VAP model outperformed the previous state-of-the-art model by a significant margin. In this paper, we propose a multimodal model enhanced with pre-trained audio and face encoders to improve performance by capturing subtle expressions. Our model performed competitively, and in some cases, even better than state-of-the-art models on turn-taking metrics. All the source codes and pretrained models are available at https://github.com/sagatake/VAPwithAudioFaceEncoders.

</details>


### [100] [Around the World in 24 Hours: Probing LLM Knowledge of Time and Place](https://arxiv.org/abs/2506.03984)

*Carolin Holtermann, Paul Röttger, Anne Lauscher*

**Main category:** cs.CL

**Keywords:** Language Models, Temporal Reasoning, Geospatial Reasoning, GeoTemp Dataset, Prompt Engineering

**Relevance Score:** 7

**TL;DR:** This paper evaluates language models' abilities to reason over time and space using a unique dataset, GeoTemp, revealing strengths in temporal reasoning but limitations in integrating both temporal and spatial knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the largely unexplored capabilities of language models in reasoning over time and space, as previous research focused on these domains in isolation or in simplistic conditions.

**Method:** The study created a dataset called GeoTemp featuring 320k prompts related to 289 cities across 217 countries and 37 time zones, then assessed the performance of eight open chat models from three model families on reasoning tasks combining temporal and geographic knowledge.

**Key Contributions:**

	1. Introduction of the GeoTemp dataset for evaluating language models on joint reasoning over time and space
	2. Demonstration of the impact of prompt formulation on model performance
	3. Insights into model performance variations based on geographical knowledge

**Result:** Most language models excelled at tasks involving only temporal knowledge, and performance generally improved with model size. However, they struggled with tasks requiring a combination of temporal and spatial reasoning. Higher performance was noted for location names with lower model perplexity, and prompt formulation significantly affected outcomes.

**Limitations:** Performance constraints in connecting temporal and geographical information, and variability of outcomes based on prompt design.

**Conclusion:** While language models perform well on isolated temporal reasoning tasks, their capability to integrate and reason with both time and space is still limited and influenced by specific dataset characteristics and prompting techniques.

**Abstract:** Reasoning over time and space is essential for understanding our world. However, the abilities of language models in this area are largely unexplored as previous work has tested their abilities for logical reasoning in terms of time and space in isolation or only in simple or artificial environments. In this paper, we present the first evaluation of the ability of language models to jointly reason over time and space. To enable our analysis, we create GeoTemp, a dataset of 320k prompts covering 289 cities in 217 countries and 37 time zones. Using GeoTemp, we evaluate eight open chat models of three different model families for different combinations of temporal and geographic knowledge. We find that most models perform well on reasoning tasks involving only temporal knowledge and that overall performance improves with scale. However, performance remains constrained in tasks that require connecting temporal and geographical information. We do not find clear correlations of performance with specific geographic regions. Instead, we find a significant performance increase for location names with low model perplexity, suggesting their repeated occurrence during model training. We further demonstrate that their performance is heavily influenced by prompt formulation - a direct injection of geographical knowledge leads to performance gains, whereas, surprisingly, techniques like chain-of-thought prompting decrease performance on simpler tasks.

</details>


### [101] [Stronger Baselines for Retrieval-Augmented Generation with Long-Context Language Models](https://arxiv.org/abs/2506.03989)

*Alex Laitenberger, Christopher D. Manning, Nelson F. Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Long-context Language Models, Question Answering, Baseline Evaluation, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of multi-stage retrieval-augmented generation (RAG) pipelines compared to simpler, single-stage methods for QA tasks, finding that a straightforward retrieve-then-read method outperforms complex pipelines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether multi-stage RAG pipelines provide benefits over simpler, single-stage retrieval methods in light of the capabilities of long-context language models.

**Method:** Controlled evaluation of QA tasks comparing two multi-stage pipelines (ReadAgent and RAPTOR) against three baselines including the straightforward DOS RAG method.

**Key Contributions:**

	1. Establishment of DOS RAG as a new baseline for RAG evaluations
	2. Comparison of multi-stage and single-stage methods in long-context QA tasks
	3. Insights on the trade-offs between complexity and effectiveness in retrieval-based models.

**Result:** The DOS RAG method consistently matches or outperforms more complex multi-stage methods across multiple long-context QA benchmarks.

**Limitations:** 

**Conclusion:** The study recommends adopting DOS RAG as a strong baseline for future RAG evaluations as it balances simplicity and effectiveness.

**Abstract:** With the rise of long-context language models (LMs) capable of processing tens of thousands of tokens in a single pass, do multi-stage retrieval-augmented generation (RAG) pipelines still offer measurable benefits over simpler, single-stage approaches? To assess this question, we conduct a controlled evaluation for QA tasks under systematically scaled token budgets, comparing two recent multi-stage pipelines, ReadAgent and RAPTOR, against three baselines, including DOS RAG (Document's Original Structure RAG), a simple retrieve-then-read method that preserves original passage order. Despite its straightforward design, DOS RAG consistently matches or outperforms more intricate methods on multiple long-context QA benchmarks. We recommend establishing DOS RAG as a simple yet strong baseline for future RAG evaluations, pairing it with emerging embedding and language models to assess trade-offs between complexity and effectiveness as model capabilities evolve.

</details>


### [102] [DynTok: Dynamic Compression of Visual Tokens for Efficient and Effective Video Understanding](https://arxiv.org/abs/2506.03990)

*Hongzhi Zhang, Jingyuan Zhang, Xingguang Ji, Qi Wang, Fuzheng Zhang*

**Main category:** cs.CL

**Keywords:** video modeling, token compression, dynamic tokenization, LLM, visual information

**Relevance Score:** 5

**TL;DR:** DynTok is a dynamic token compression strategy for video modeling that reduces visual token size significantly while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational overhead caused by the massive number of visual tokens in long videos, DynTok seeks to optimize video token representation.

**Method:** DynTok adaptively splits visual tokens into groups and merges them within each group to achieve compression, particularly in low information density regions.

**Key Contributions:**

	1. Introduction of DynTok for dynamic visual token compression
	2. Achieves significant reduction in token size with retained performance
	3. Insights for designing more efficient video modeling techniques

**Result:** DynTok reduces the number of tokens to 44.4% of the original size while maintaining performance, achieving 65.3% on Video-MME and 72.5% on MLVU.

**Limitations:** 

**Conclusion:** The method exposes redundancy in video token representations and provides insights for more efficient video modeling techniques.

**Abstract:** Typical video modeling methods, such as LLava, represent videos as sequences of visual tokens, which are then processed by the LLM backbone for effective video understanding. However, this approach leads to a massive number of visual tokens, especially for long videos. A practical solution is to first extract relevant visual information from the large visual context before feeding it into the LLM backbone, thereby reducing computational overhead. In this work, we introduce DynTok, a novel \textbf{Dyn}amic video \textbf{Tok}en compression strategy. DynTok adaptively splits visual tokens into groups and merges them within each group, achieving high compression in regions with low information density while preserving essential content. Our method reduces the number of tokens to 44.4% of the original size while maintaining comparable performance. It further benefits from increasing the number of video frames and achieves 65.3% on Video-MME and 72.5% on MLVU. By applying this simple yet effective compression method, we expose the redundancy in video token representations and offer insights for designing more efficient video modeling techniques.

</details>


### [103] [Words of Warmth: Trust and Sociability Norms for over 26k English Words](https://arxiv.org/abs/2506.03993)

*Saif M. Mohammad*

**Main category:** cs.CL

**Keywords:** Warmth, Competence, Sociability, Trust, Bias research

**Relevance Score:** 4

**TL;DR:** The paper introduces Words of Warmth, a repository of word associations related to warmth, trust, and sociability, demonstrating their reliability and applications in understanding development and biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive resource that explores the dimensions of Warmth and Competence, particularly focusing on the components of warmth, and to facilitate research on biases and stereotypes.

**Method:** Development of a large-scale repository of word associations for over 26k English words related to warmth, trust, and sociability, along with reliability assessments and case studies.

**Key Contributions:**

	1. Creation of the first large-scale repository of word associations for warmth, trust, and sociability.
	2. Assessment of the developmental acquisition of warmth-related words in children.
	3. Facilitation of bias and stereotype research with real-world applications.

**Result:** The repository shows high reliability in word associations and illustrates how children acquire WCTS words as they age, alongside applications in bias and stereotype research.

**Limitations:** 

**Conclusion:** Words of Warmth serves as a valuable tool for social psychology researchers, offering insights into the development of social dimensions and a basis for further research on biases.

**Abstract:** Social psychologists have shown that Warmth (W) and Competence (C) are the primary dimensions along which we assess other people and groups. These dimensions impact various aspects of our lives from social competence and emotion regulation to success in the work place and how we view the world. More recent work has started to explore how these dimensions develop, why they have developed, and what they constitute. Of particular note, is the finding that warmth has two distinct components: Trust (T) and Sociability (S). In this work, we introduce Words of Warmth, the first large-scale repository of manually derived word--warmth (as well as word--trust and word--sociability) associations for over 26k English words. We show that the associations are highly reliable. We use the lexicons to study the rate at which children acquire WCTS words with age. Finally, we show that the lexicon enables a wide variety of bias and stereotype research through case studies on various target entities. Words of Warmth is freely available at: http://saifmohammad.com/warmth.html

</details>


### [104] [Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era](https://arxiv.org/abs/2506.03994)

*Dan Oneata, Desmond Elliott, Stella Frank*

**Main category:** cs.CL

**Keywords:** semantic feature norms, large-scale models, image encoders

**Relevance Score:** 6

**TL;DR:** This paper investigates the ability of large-scale models to represent the semantic features of concrete object concepts by evaluating different types of encoders on a set of probing tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how well large-scale trained models capture semantic feature norms based on sensorimotor experiences compared to traditional methods.

**Method:** The authors conducted probing tasks to assess the performance of image encoders (trained on image data, multimodally, and language-only) in predicting attribute ratings from datasets like the McRae norms and Binder dataset.

**Key Contributions:**

	1. Demonstrated the capabilities of unimodal image encoders in representing semantic attributes.
	2. Examined the performance of multimodal vs. language-only models in detail.
	3. Provided new insights into the learning dynamics of large-scale models.

**Result:** Multimodal image encoders slightly outperformed language-only models, while image-only encoders showed comparable performance to language models, even on attributes not directly visual in nature.

**Limitations:** 

**Conclusion:** The findings suggest valuable insights into the effectiveness of unimodal learning and highlight the complementarity between different modalities in model training.

**Abstract:** Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as "encyclopedic" or "function". These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.

</details>


### [105] [QQSUM: A Novel Task and Model of Quantitative Query-Focused Summarization for Review-based Product Question Answering](https://arxiv.org/abs/2506.04020)

*An Quang Tang, Xiuzhen Zhang, Minh Ngoc Dinh, Zhuang Li*

**Main category:** cs.CL

**Keywords:** Product Question Answering, customer reviews, quantitative summarization, few-shot learning, retrieval-augmented generation

**Relevance Score:** 7

**TL;DR:** This paper presents QQSUM, a novel task for summarizing diverse customer opinions in Product Question Answering (PQA) using a model called QQSUM-RAG, which improves upon Retrieval-Augmented Generation by capturing the diversity of user reviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing PQA systems fail to capture diverse perspectives in customer reviews, limiting their effectiveness in addressing queries.

**Method:** The authors introduce QQSUM, a quantitative query-focused summarization task, and develop QQSUM-RAG, which combines few-shot learning with a Key Point (KP)-oriented retriever and summary generator to create comprehensive summaries from diverse opinions.

**Key Contributions:**

	1. Introduction of the QQSUM task for summarizing customer opinions
	2. Development of QQSUM-RAG model that improves retrieval and summarization of diverse viewpoints
	3. Demonstration of superior performance in opinion diversity quantification compared to existing models.

**Result:** Experimental results show that QQSUM-RAG outperforms state-of-the-art RAG models in both the quality of textual output and the accuracy of opinion quantification.

**Limitations:** 

**Conclusion:** QQSUM-RAG effectively addresses the key limitations of existing PQA systems by providing richer, multi-perspective summaries that are more representative of customer sentiments.

**Abstract:** Review-based Product Question Answering (PQA) allows e-commerce platforms to automatically address customer queries by leveraging insights from user reviews. However, existing PQA systems generate answers with only a single perspective, failing to capture the diversity of customer opinions. In this paper we introduce a novel task Quantitative Query-Focused Summarization (QQSUM), which aims to summarize diverse customer opinions into representative Key Points (KPs) and quantify their prevalence to effectively answer user queries. While Retrieval-Augmented Generation (RAG) shows promise for PQA, its generated answers still fall short of capturing the full diversity of viewpoints. To tackle this challenge, our model QQSUM-RAG, which extends RAG, employs few-shot learning to jointly train a KP-oriented retriever and a KP summary generator, enabling KP-based summaries that capture diverse and representative opinions. Experimental results demonstrate that QQSUM-RAG achieves superior performance compared to state-of-the-art RAG baselines in both textual quality and quantification accuracy of opinions. Our source code is available at: https://github.com/antangrocket1312/QQSUMM

</details>


### [106] [AI Agents for Conversational Patient Triage: Preliminary Simulation-Based Evaluation with Real-World EHR Data](https://arxiv.org/abs/2506.04032)

*Sina Rashidian, Nan Li, Jonathan Amar, Jong Ha Lee, Sam Pugh, Eric Yang, Geoff Masterson, Myoung Cha, Yugang Jia, Akhil Vaid*

**Main category:** cs.CL

**Keywords:** Patient Simulator, EHR data, Conversational AI, Healthcare, Simulation

**Relevance Score:** 9

**TL;DR:** Development of a Patient Simulator using real EHR data for training AI health agents.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To create realistic test subjects for healthcare agentic models based on actual patient encounters.

**Method:** Constructed clinical scenarios from real-world EHR data and evaluated the simulator's performance with expert clinicians.

**Key Contributions:**

	1. Patient Simulator utilizing real EHR data
	2. Evaluation framework aligning simulated encounters with clinical expertise
	3. High consistency and relevance in simulated patient encounters

**Result:** Clinicians found the Patient Simulator consistent with patient vignettes in 97.7% of cases, and case summaries were 99% relevant.

**Limitations:** 

**Conclusion:** The methodology allows for large-scale training and testing of conversational AI agents using realistic patient data.

**Abstract:** Background: We present a Patient Simulator that leverages real world patient encounters which cover a broad range of conditions and symptoms to provide synthetic test subjects for development and testing of healthcare agentic models. The simulator provides a realistic approach to patient presentation and multi-turn conversation with a symptom-checking agent. Objectives: (1) To construct and instantiate a Patient Simulator to train and test an AI health agent, based on patient vignettes derived from real EHR data. (2) To test the validity and alignment of the simulated encounters provided by the Patient Simulator to expert human clinical providers. (3) To illustrate the evaluation framework of such an LLM system on the generated realistic, data-driven simulations -- yielding a preliminary assessment of our proposed system. Methods: We first constructed realistic clinical scenarios by deriving patient vignettes from real-world EHR encounters. These vignettes cover a variety of presenting symptoms and underlying conditions. We then evaluate the performance of the Patient Simulator as a simulacrum of a real patient encounter across over 500 different patient vignettes. We leveraged a separate AI agent to provide multi-turn questions to obtain a history of present illness. The resulting multiturn conversations were evaluated by two expert clinicians. Results: Clinicians scored the Patient Simulator as consistent with the patient vignettes in those same 97.7% of cases. The extracted case summary based on the conversation history was 99% relevant. Conclusions: We developed a methodology to incorporate vignettes derived from real healthcare patient data to build a simulation of patient responses to symptom checking agents. The performance and alignment of this Patient Simulator could be used to train and test a multi-turn conversational AI agent at scale.

</details>


### [107] [The mutual exclusivity bias of bilingual visually grounded speech models](https://arxiv.org/abs/2506.04037)

*Dan Oneata, Leanne Nortje, Yevgen Matusevych, Herman Kamper*

**Main category:** cs.CL

**Keywords:** mutual exclusivity, bilingual models, visually grounded speech

**Relevance Score:** 6

**TL;DR:** This paper analyzes the mutual exclusivity (ME) bias in bilingual visually grounded speech models compared to monolingual models, finding a generally weaker ME bias in the former with interesting implications for language learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore mutual exclusivity in bilingual language learning contexts and to understand its computational implications on visually grounded speech models.

**Method:** Bilingual visually grounded speech models were trained on combinations of English, French, and Dutch, and their ME bias was compared to that of monolingual models.

**Key Contributions:**

	1. Analysis of ME bias in bilingual vs. monolingual models
	2. Insights into visual embeddings and variance correlations
	3. Provision of code and data for further research.

**Result:** Bilingual models generally exhibited a weaker ME bias than monolingual ones, with exceptions. The visual embeddings of bilingual models showed smaller variance for familiar data, leading to increased confusion between novel and familiar concepts.

**Limitations:** The study focuses on a limited set of languages and may not generalize across all bilingual contexts.

**Conclusion:** The findings provide insights into how bilingualism affects mutual exclusivity and suggest reasons for the observed biases in VGS models.

**Abstract:** Mutual exclusivity (ME) is a strategy where a novel word is associated with a novel object rather than a familiar one, facilitating language learning in children. Recent work has found an ME bias in a visually grounded speech (VGS) model trained on English speech with paired images. But ME has also been studied in bilingual children, who may employ it less due to cross-lingual ambiguity. We explore this pattern computationally using bilingual VGS models trained on combinations of English, French, and Dutch. We find that bilingual models generally exhibit a weaker ME bias than monolingual models, though exceptions exist. Analyses show that the combined visual embeddings of bilingual models have a smaller variance for familiar data, partly explaining the increase in confusion between novel and familiar concepts. We also provide new insights into why the ME bias exists in VGS models in the first place. Code and data: https://github.com/danoneata/me-vgs

</details>


### [108] [LexTime: A Benchmark for Temporal Ordering of Legal Events](https://arxiv.org/abs/2506.04041)

*Claire Barale, Leslie Barrett, Vikram Sunil Bajaj, Michael Rovatsos*

**Main category:** cs.CL

**Keywords:** legal texts, temporal reasoning, event ordering, LLMs, LexTime

**Relevance Score:** 6

**TL;DR:** LexTime is a dataset for assessing LLMs' event ordering in legal texts, revealing accuracy variations and challenges in legal linguistic complexities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of expert language evaluation in existing datasets for legal temporal reasoning and understand how LLMs manage event ordering in legal contexts.

**Method:** Introduction of LexTime, a novel dataset comprising 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations; analysis of model performance based on context length and event characteristics.

**Key Contributions:**

	1. Introduction of the LexTime dataset for evaluating LLMs in legal text event ordering.
	2. Demonstration of improved accuracy of LLMs on legal texts vs. narrative texts.
	3. Identification of challenges posed by legal linguistic complexities and nested clauses.

**Result:** LLMs demonstrated greater accuracy in legal event ordering compared to narrative contexts, with the highest accuracy reaching 80.8% for implicit-explicit event pairs; longer contexts and implicit events improved outcomes, but legal complexities posed challenges.

**Limitations:** The challenges of legal linguistic complexities and nested clauses remain unaddressed in terms of model adaptation.

**Conclusion:** Highlighting specific modeling strategies to improve LLMs' capability in temporal event reasoning within legal texts is critical.

**Abstract:** Temporal reasoning in legal texts is important for applications like case law analysis and compliance monitoring. However, existing datasets lack expert language evaluation, leaving a gap in understanding how LLMs manage event ordering in legal contexts. We introduce LexTime, the first dataset designed to evaluate LLMs' event ordering capabilities in legal language, consisting of 512 instances from U.S. Federal Complaints with annotated event pairs and their temporal relations. Our findings show that (1) LLMs are more accurate on legal event ordering than on narrative (up to +10.5%); (2) longer input contexts and implicit events boost accuracy, reaching 80.8% for implicit-explicit event pairs; (3) legal linguistic complexities and nested clauses remain a challenge. We investigate how context length, explicit vs implicit event pairs, and legal language features affect model performance, demonstrating the need for specific modeling strategies to enhance temporal event reasoning.

</details>


### [109] [Unveiling and Eliminating the Shortcut Learning for Locate-Then-Edit Knowledge Editing via Both Subject and Relation Awareness](https://arxiv.org/abs/2506.04042)

*Xiyu Liu, Zhengxiao Liu, Naibin Gu, Zheng Lin, Ji Xiang, Weiping Wang*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Large Language Models, Optimization Process, Controllable Knowledge Modification, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel two-stage optimization process for controllable knowledge editing in large language models that prevents shortcut learning and enhances prediction performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for controllable knowledge editing in large language models to avoid unintended modifications to unrelated knowledge.

**Method:** A two-stage optimization process that balances the learning of subject and relation features during the editing of knowledge predictions.

**Key Contributions:**

	1. Introduces a two-stage optimization process for knowledge editing.
	2. Identifies crucial features (subject and relation features) for model learning.
	3. Demonstrates the effectiveness of the approach in preventing shortcut learning.

**Result:** The proposed method effectively prevents shortcut learning and improves overall performance in knowledge editing tasks.

**Limitations:** 

**Conclusion:** A better approach to knowledge editing is provided by addressing shortcut learning issues, allowing for more precise modifications without unwanted side effects.

**Abstract:** Knowledge editing aims to alternate the target knowledge predicted by large language models while ensuring the least side effects on unrelated knowledge. An effective way to achieve knowledge editing is to identify pivotal parameters for predicting factual associations and modify them with an optimization process to update the predictions. However, these locate-then-edit methods are uncontrollable since they tend to modify most unrelated relations connected to the subject of target editing. We unveil that this failure of controllable editing is due to a shortcut learning issue during the optimization process. Specifically, we discover two crucial features that are the subject feature and the relation feature for models to learn during optimization, but the current optimization process tends to over-learning the subject feature while neglecting the relation feature. To eliminate this shortcut learning of the subject feature, we propose a novel two-stage optimization process that balances the learning of the subject feature and the relation feature. Experimental results demonstrate that our approach successfully prevents knowledge editing from shortcut learning and achieves the optimal overall performance, contributing to controllable knowledge editing.

</details>


### [110] [Think Like a Person Before Responding: A Multi-Faceted Evaluation of Persona-Guided LLMs for Countering Hate](https://arxiv.org/abs/2506.04043)

*Mikel K. Ngueajio, Flor Miriam Plaza-del-Arco, Yi-Ling Chung, Danda B. Rawat, Amanda Cercas Curry*

**Main category:** cs.CL

**Keywords:** counter-narratives, hate speech, large language models, accessibility, ethical risks

**Relevance Score:** 6

**TL;DR:** The paper evaluates the effectiveness of LLM-generated counter-narratives for online hate speech, highlighting issues of tone, accessibility, and ethical risks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges associated with automated counter-narratives in mitigating hate speech online, particularly focusing on their tone, accessibility, and ethical implications.

**Method:** A framework was proposed for evaluating LLM-generated counter-narratives across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Various models (GPT-4o-Mini, CommandR-7B, LLaMA 3.1-70B) were assessed using datasets MT-Conan and HatEval.

**Key Contributions:**

	1. Proposed an evaluation framework for LLM-generated counter-narratives.
	2. Demonstrated the impact of affective tone on the effectiveness of counter-narratives.
	3. Identified limitations in accessibility of LLM-generated content.

**Result:** The evaluation showed that LLM-generated counter-narratives tend to be verbose and geared towards individuals with college-level literacy, thus limiting their accessibility. Emotionally guided prompts resulted in more empathetic and readable outputs, but safety and effectiveness concerns persist.

**Limitations:** Concerns remain about the safety and effectiveness of LLM-generated counter-narratives despite potential improvements in empathetic and readable responses.

**Conclusion:** The study concludes that while LLM capabilities can enhance counter-narratives, significant issues regarding accessibility and ethical robustness need to be addressed before they can be reliably used.

**Abstract:** Automated counter-narratives (CN) offer a promising strategy for mitigating online hate speech, yet concerns about their affective tone, accessibility, and ethical risks remain. We propose a framework for evaluating Large Language Model (LLM)-generated CNs across four dimensions: persona framing, verbosity and readability, affective tone, and ethical robustness. Using GPT-4o-Mini, Cohere's CommandR-7B, and Meta's LLaMA 3.1-70B, we assess three prompting strategies on the MT-Conan and HatEval datasets. Our findings reveal that LLM-generated CNs are often verbose and adapted for people with college-level literacy, limiting their accessibility. While emotionally guided prompts yield more empathetic and readable responses, there remain concerns surrounding safety and effectiveness.

</details>


### [111] [Lacuna Inc. at SemEval-2025 Task 4: LoRA-Enhanced Influence-Based Unlearning for LLMs](https://arxiv.org/abs/2506.04044)

*Aleksey Kudelya, Alexander Shirnin*

**Main category:** cs.CL

**Keywords:** unlearning, large language models, influence functions, second-order optimization, sensitive content

**Relevance Score:** 9

**TL;DR:** LIBU proposes a novel algorithm for unlearning specific data from large language models without complete retraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of unlearning sensitive information from language models while maintaining their utility.

**Method:** The algorithm leverages influence functions to eliminate the impact of specific data points and employs second-order optimization techniques to stabilize the model's performance.

**Key Contributions:**

	1. Introduction of LIBU algorithm for unlearning
	2. Use of classical influence functions in LLM context
	3. Combination of influence functions with second-order optimization for improved stability

**Result:** Experiments demonstrate the effectiveness of LIBU in various tasks related to unlearning in large language models.

**Limitations:** 

**Conclusion:** LIBU presents a lightweight and effective method for removing specific knowledge from LLMs, offering a practical solution without full retraining.

**Abstract:** This paper describes LIBU (LoRA enhanced influence-based unlearning), an algorithm to solve the task of unlearning - removing specific knowledge from a large language model without retraining from scratch and compromising its overall utility (SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models). The algorithm combines classical \textit{influence functions} to remove the influence of the data from the model and \textit{second-order optimization} to stabilize the overall utility. Our experiments show that this lightweight approach is well applicable for unlearning LLMs in different kinds of task.

</details>


### [112] [On Support Samples of Next Word Prediction](https://arxiv.org/abs/2506.04047)

*Yuqian Li, Yupei Du, Yufang Liu, Feifei Feng, Mou Xiao Feng, Yuanbin Wu*

**Main category:** cs.CL

**Keywords:** data-centric interpretability, language models, next-word prediction

**Relevance Score:** 8

**TL;DR:** Investigates data-centric interpretability in language models, focusing on support samples in next-word prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the rationale behind language model decisions, particularly in next-word prediction tasks.

**Method:** Utilizes representer theorem to identify support samples that promote or deter predictions.

**Key Contributions:**

	1. Introduces the concept of data-centric interpretability for language models.
	2. Identifies intrinsic properties of support samples prior to training.
	3. Highlights the critical role of non-support samples in generalization and representation learning.

**Result:** Support samples are intrinsic and predictable before training; non-support samples prevent overfitting and shape representation learning, especially in deeper layers.

**Limitations:** 

**Conclusion:** Insights on data's role in model decisions enhance the understanding of language model interpretability.

**Abstract:** Language models excel in various tasks by making complex decisions, yet understanding the rationale behind these decisions remains a challenge. This paper investigates \emph{data-centric interpretability} in language models, focusing on the next-word prediction task. Using representer theorem, we identify two types of \emph{support samples}-those that either promote or deter specific predictions. Our findings reveal that being a support sample is an intrinsic property, predictable even before training begins. Additionally, while non-support samples are less influential in direct predictions, they play a critical role in preventing overfitting and shaping generalization and representation learning. Notably, the importance of non-support samples increases in deeper layers, suggesting their significant role in intermediate representation formation.These insights shed light on the interplay between data and model decisions, offering a new dimension to understanding language model behavior and interpretability.

</details>


### [113] [Explainability-Based Token Replacement on LLM-Generated Text](https://arxiv.org/abs/2506.04050)

*Hadi Mohammadi, Anastasia Giachanou, Daniel L. Oberski, Ayoub Bagheri*

**Main category:** cs.CL

**Keywords:** explainable AI, large language models, AI-generated text detection, ensemble classifier, token replacement strategies

**Relevance Score:** 9

**TL;DR:** This paper investigates using explainable AI methods to reduce the detectability of AI-generated text while proposing a robust ensemble-based detection approach.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the distinguishability of AI-generated text from human-written text, which is a growing concern with the advancement of generative models like LLMs.

**Method:** An ensemble classifier was trained to distinguish between AI-generated text and human-written text. SHAP and LIME were used to identify influential tokens, followed by the implementation of four token replacement strategies to reduce detectability.

**Key Contributions:**

	1. Introduction of token replacement strategies based on XAI methods to reduce AIGT detectability.
	2. Development of a robust ensemble classifier that performs well across different languages and domains.
	3. Demonstration of the impact of influential token manipulations on detection strategies.

**Result:** The token replacement strategies significantly decreased the ability of a single classifier to detect AI-generated text, while the ensemble classifier retained strong performance across multiple languages and domains.

**Limitations:** The effectiveness of the proposed strategies may vary with different classifiers and the evolving nature of AI-generated content.

**Conclusion:** Explainable AI methods can effectively obscure AI-generated text detection, but maintaining an ensemble-based detection strategy is crucial for adapting to evolving manipulative techniques.

**Abstract:** Generative models, especially large language models (LLMs), have shown remarkable progress in producing text that appears human-like. However, they often exhibit patterns that make their output easier to detect than text written by humans. In this paper, we investigate how explainable AI (XAI) methods can be used to reduce the detectability of AI-generated text (AIGT) while also introducing a robust ensemble-based detection approach. We begin by training an ensemble classifier to distinguish AIGT from human-written text, then apply SHAP and LIME to identify tokens that most strongly influence its predictions. We propose four explainability-based token replacement strategies to modify these influential tokens. Our findings show that these token replacement approaches can significantly diminish a single classifier's ability to detect AIGT. However, our ensemble classifier maintains strong performance across multiple languages and domains, showing that a multi-model approach can mitigate the impact of token-level manipulations. These results show that XAI methods can make AIGT harder to detect by focusing on the most influential tokens. At the same time, they highlight the need for robust, ensemble-based detection strategies that can adapt to evolving approaches for hiding AIGT.

</details>


### [114] [High Accuracy, Less Talk (HALT): Reliable LLMs through Capability-Aligned Finetuning](https://arxiv.org/abs/2506.04051)

*Tim Franzmeyer, Archie Sravankumar, Lijuan Liu, Yuning Mao, Rui Hou, Sinong Wang, Jakob N. Foerster, Luke Zettlemoyer, Madian Khabsa*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination, post-training, capability-aligned, finetuning

**Relevance Score:** 9

**TL;DR:** This paper proposes HALT, a method for post-training Large Language Models (LLMs) to generate content confidently, reducing hallucinations by encoding what the model can and cannot reliably generate.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucinations in LLMs, which produce incorrect answers when lacking knowledge or capability.

**Method:** HALT generates capability-aligned post-training data by splitting responses of pretrained LLMs into factual fragments and identifying incorrect fragments using ground truth information.

**Key Contributions:**

	1. Development of HALT for capability-aligned post-training of LLMs
	2. Demonstration of significant correctness improvements while maintaining response completeness
	3. Evaluation across multiple domains: biography writing, mathematics, coding, and medicine.

**Result:** Using HALT for finetuning, the mean correctness of response fragments increased by 15% on average, and a single Llama3-70B model achieved a correctness improvement from 51% to 87%.

**Limitations:** 

**Conclusion:** HALT effectively balances the trade-off between response completeness and correctness in LLM responses, leading to significant improvements in reliability.

**Abstract:** Large Language Models (LLMs) currently respond to every prompt. However, they can produce incorrect answers when they lack knowledge or capability -- a problem known as hallucination. We instead propose post-training an LLM to generate content only when confident in its correctness and to otherwise (partially) abstain. Specifically, our method, HALT, produces capability-aligned post-training data that encodes what the model can and cannot reliably generate. We generate this data by splitting responses of the pretrained LLM into factual fragments (atomic statements or reasoning steps), and use ground truth information to identify incorrect fragments. We achieve capability-aligned finetuning responses by either removing incorrect fragments or replacing them with "Unsure from Here" -- according to a tunable threshold that allows practitioners to trade off response completeness and mean correctness of the response's fragments. We finetune four open-source models for biography writing, mathematics, coding, and medicine with HALT for three different trade-off thresholds. HALT effectively trades off response completeness for correctness, increasing the mean correctness of response fragments by 15% on average, while resulting in a 4% improvement in the F1 score (mean of completeness and correctness of the response) compared to the relevant baselines. By tuning HALT for highest correctness, we train a single reliable Llama3-70B model with correctness increased from 51% to 87% across all four domains while maintaining 53% of the response completeness achieved with standard finetuning.

</details>


### [115] [Progressive Mastery: Customized Curriculum Learning with Guided Prompting for Mathematical Reasoning](https://arxiv.org/abs/2506.04065)

*Muling Wu, Qi Qian, Wenhao Liu, Xiaohua Wang, Zisu Huang, Di Liang, LI Miao, Shihan Dou, Changze Lv, Zhenghua Wang, Zhibo Xu, Lina Chen, Tianlong Li, Xiaoqing Zheng, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Curriculum Learning, Guided Prompting

**Relevance Score:** 7

**TL;DR:** This paper introduces Customized Curriculum Learning (CCL), a framework that improves post-training efficiency and model performance by customizing difficulty metrics and using guided prompting for training with challenging samples.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of inefficient sample utilization and inflexible difficulty processing in post-training of Large Language Models (LLMs).

**Method:** The proposed CCL framework includes model-adaptive difficulty definitions to customize datasets and 'Guided Prompting' to dynamically adjust sample difficulty through hints.

**Key Contributions:**

	1. Model-adaptive difficulty definition for curriculum datasets.
	2. Guided Prompting for dynamic sample difficulty adjustment.
	3. Demonstrated effectiveness across five mathematical reasoning benchmarks.

**Result:** CCL shows significant performance improvements over uniform training methods in mathematical reasoning tasks, demonstrating better sample utilization and enhanced model performance across multiple benchmarks.

**Limitations:** 

**Conclusion:** The CCL framework is effective in improving the performance of LLMs in reasoning tasks by customizing sample difficulty and using guidance for tougher samples.

**Abstract:** Large Language Models (LLMs) have achieved remarkable performance across various reasoning tasks, yet post-training is constrained by inefficient sample utilization and inflexible difficulty samples processing. To address these limitations, we propose Customized Curriculum Learning (CCL), a novel framework with two key innovations. First, we introduce model-adaptive difficulty definition that customizes curriculum datasets based on each model's individual capabilities rather than using predefined difficulty metrics. Second, we develop "Guided Prompting," which dynamically reduces sample difficulty through strategic hints, enabling effective utilization of challenging samples that would otherwise degrade performance. Comprehensive experiments on supervised fine-tuning and reinforcement learning demonstrate that CCL significantly outperforms uniform training approaches across five mathematical reasoning benchmarks, confirming its effectiveness across both paradigms in enhancing sample utilization and model performance.

</details>


### [116] [LaF-GRPO: In-Situ Navigation Instruction Generation for the Visually Impaired via GRPO with LLM-as-Follower Reward](https://arxiv.org/abs/2506.04070)

*Yi Zhao, Siqi Wang, Jing Li*

**Main category:** cs.CL

**Keywords:** Navigation instruction generation, Visually impaired, Vision-Language Model, Benchmark, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This study introduces LaF-GRPO, an approach for generating navigation instructions for visually impaired individuals using a Vision-Language Model and a new open-sourced benchmark called NIG4VI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The generation of precise navigation instructions for visually impaired individuals is critical for enhancing their mobility, yet it remains underexplored.

**Method:** The research proposes LaF-GRPO, which uses an LLM to simulate responses from visually impaired users to produce rewards that guide the post-training of a Vision-Language Model.

**Key Contributions:**

	1. Introduction of LaF-GRPO for generating navigation instructions for visually impaired users
	2. Development of NIG4VI, a comprehensive benchmark for instruction generation in diverse navigation scenarios
	3. Demonstrated superior performance in instruction quality compared to existing models

**Result:** Experiments on the NIG4VI benchmark demonstrate that LaF-GRPO significantly improves instruction quality, increasing BLEU and METEOR scores compared to existing models.

**Limitations:** 

**Conclusion:** The proposed method enhances the usability of navigation instructions for visually impaired users while minimizing the dependency on costly real-world data.

**Abstract:** Navigation instruction generation for visually impaired (VI) individuals (NIG-VI) is critical yet relatively underexplored. This study, hence, focuses on producing precise, in-situ, step-by-step navigation instructions that are practically usable by VI users. Concretely, we propose LaF-GRPO (LLM-as-Follower GRPO), where an LLM simulates VI user responses to generate rewards guiding the Vision-Language Model (VLM) post-training. This enhances instruction usability while reducing costly real-world data needs. To facilitate training and testing, we introduce NIG4VI, a 27k-sample open-sourced benchmark. It provides diverse navigation scenarios with accurate spatial coordinates, supporting detailed, open-ended in-situ instruction generation. Experiments on NIG4VI show the effectiveness of LaF-GRPO by quantitative metrics (e.g., Zero-(LaF-GRPO) boosts BLEU +14\%; SFT+(LaF-GRPO) METEOR 0.542 vs. GPT-4o's 0.323) and yields more intuitive, safer instructions. Code and benchmark are available at \href{https://github.com/YiyiyiZhao/NIG4VI}{https://github.com/YiyiyiZhao/NIG4VI}.

</details>


### [117] [Controlling Difficulty of Generated Text for AI-Assisted Language Learning](https://arxiv.org/abs/2506.04072)

*Meiqing Jin, Liam Dugan, Chris Callison-Burch*

**Main category:** cs.CL

**Keywords:** Large Language Models, Language Learning, Controllable Generation, Comprehensibility, Token Miss Rate

**Relevance Score:** 8

**TL;DR:** This paper explores how controllable generation techniques can modify LLM outputs to better support beginner language learners, resulting in significantly improved comprehensibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional LLMs generate complex text, making them unsuitable for beginner language learners. This research seeks to make LLMs more accessible for absolute beginners through controllable generation methods.

**Method:** The study employs modular controllable generation techniques without fine-tuning the LLM. It evaluates these techniques using automatic metrics and a user study with university students learning Japanese.

**Key Contributions:**

	1. Introduction of future discriminators for output control
	2. Development of the Token Miss Rate (TMR) evaluation metric
	3. Release of resources for AI-assisted language learning research

**Result:** Using future discriminators, the study demonstrated a significant improvement in output comprehensibility, increasing from 40.4% to 84.3%. A novel evaluation metric, Token Miss Rate (TMR), was introduced to measure the proportion of incomprehensible tokens and showed a strong correlation with human judgments.

**Limitations:** 

**Conclusion:** The research shows that controllable generation can effectively aid beginner language learners using LLMs, and tools and datasets are provided for further research.

**Abstract:** Practicing conversations with large language models (LLMs) presents a promising alternative to traditional in-person language learning. However, most LLMs generate text at a near-native level of complexity, making them ill-suited for beginner learners (CEFR: A1-A2). In this paper, we investigate whether controllable generation techniques -- specifically modular methods that do not require model fine-tuning -- can adapt LLM outputs to better support absolute beginners. We evaluate these methods through both automatic metrics and a user study with university-level learners of Japanese. Our findings show that while prompting alone fails to control output difficulty, the use of future discriminators (Yang and Klein, 2021) significantly improves output comprehensibility (from 40.4\% to 84.3\%). We further introduce a novel token-level evaluation metric, Token Miss Rate (TMR), that quantifies the proportion of incomprehensible tokens per utterance and correlates strongly with human judgments. To support future research in AI-assisted language learning, we release our code, models, annotation tools, and dataset.

</details>


### [118] [Acoustically Precise Hesitation Tagging Is Essential for End-to-End Verbatim Transcription Systems](https://arxiv.org/abs/2506.04076)

*Jhen-Ke Lin, Hao-Chien Lu, Chung-Chun Wang, Hong-Yun Lin, Berlin Chen*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, disfluencies, fine-tuning, speech assessment, filled pauses

**Relevance Score:** 7

**TL;DR:** This paper fine-tunes Whisper models for automatic speaking assessment, focusing on accurate transcription of disfluencies using different annotation schemes to improve ASR accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The accurate capture of disfluencies is crucial for automatic speaking assessment, especially for tasks like error analysis and providing effective feedback.

**Method:** Fine-tuning of Whisper models on the Speak & Improve 2025 corpus employing low-rank adaptation, exploring three annotation schemes regarding hesitations.

**Key Contributions:**

	1. Fine-tuning Whisper models with low-rank adaptation
	2. Comparison of annotation schemes for disfluencies
	3. Significant improvements in ASR accuracy for verbatim speech transcription

**Result:** The challenge system achieved 6.47% WER with the Pure scheme and 5.81% with the Extra scheme, with an 11.3% relative improvement observed when using realistic filled-pause labeling.

**Limitations:** 

**Conclusion:** Explicit and realistic labeling of filled pauses enhances ASR accuracy significantly for verbatim L2 speech transcription.

**Abstract:** Verbatim transcription for automatic speaking assessment demands accurate capture of disfluencies, crucial for downstream tasks like error analysis and feedback. However, many ASR systems discard or generalize hesitations, losing important acoustic details. We fine-tune Whisper models on the Speak & Improve 2025 corpus using low-rank adaptation (LoRA), without recourse to external audio training data. We compare three annotation schemes: removing hesitations (Pure), generic tags (Rich), and acoustically precise fillers inferred by Gemini 2.0 Flash from existing audio-transcript pairs (Extra). Our challenge system achieved 6.47% WER (Pure) and 5.81% WER (Extra). Post-challenge experiments reveal that fine-tuning Whisper Large V3 Turbo with the "Extra" scheme yielded a 5.5% WER, an 11.3% relative improvement over the "Pure" scheme (6.2% WER). This demonstrates that explicit, realistic filled-pause labeling significantly enhances ASR accuracy for verbatim L2 speech transcription.

</details>


### [119] [A Novel Data Augmentation Approach for Automatic Speaking Assessment on Opinion Expressions](https://arxiv.org/abs/2506.04077)

*Chung-Chun Wang, Jhen-Ke Lin, Hao-Chien Lu, Hong-Yun Lin, Berlin Chen*

**Main category:** cs.CL

**Keywords:** Automated Speaking Assessment, Large Language Models, Text-to-Speech Synthesis

**Relevance Score:** 8

**TL;DR:** A novel training paradigm for automated speaking assessment leverages large language models to generate diverse responses and uses multimodal approaches to predict proficiency scores.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Address the scarcity of labeled recordings in automated speaking assessment, which limits prompt diversity and scoring reliability.

**Method:** Leverage large language models to generate diverse responses, convert these into synthesized speech using speaker-aware text-to-speech synthesis, and apply a dynamic importance loss to adaptively reweight training instances based on feature distribution differences.

**Key Contributions:**

	1. Introduction of a novel training paradigm using LLMs for response generation.
	2. Development of a dynamic importance loss to handle feature distribution differences.
	3. Integration of multimodal features for accurate proficiency score prediction.

**Result:** Experiments on the LTTC dataset show improved performance over methods using real data or conventional augmentation, effectively addressing low-resource constraints.

**Limitations:** 

**Conclusion:** The proposed approach facilitates automated speaking assessment on opinion expressions by integrating cross-modal information and generating diverse training data.

**Abstract:** Automated speaking assessment (ASA) on opinion expressions is often hampered by the scarcity of labeled recordings, which restricts prompt diversity and undermines scoring reliability. To address this challenge, we propose a novel training paradigm that leverages a large language models (LLM) to generate diverse responses of a given proficiency level, converts responses into synthesized speech via speaker-aware text-to-speech synthesis, and employs a dynamic importance loss to adaptively reweight training instances based on feature distribution differences between synthesized and real speech. Subsequently, a multimodal large language model integrates aligned textual features with speech signals to predict proficiency scores directly. Experiments conducted on the LTTC dataset show that our approach outperforms methods relying on real data or conventional augmentation, effectively mitigating low-resource constraints and enabling ASA on opinion expressions with cross-modal information.

</details>


### [120] [LLMEval-Med: A Real-world Clinical Benchmark for Medical LLMs with Physician Validation](https://arxiv.org/abs/2506.04078)

*Ming Zhang, Yujiong Shen, Zelin Li, Huayu Sha, Binze Hu, Yuhui Wang, Chenhao Huang, Shichun Liu, Jingqi Tong, Changhao Jiang, Mingxu Chai, Zhiheng Xi, Shihan Dou, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** large language models, medical benchmarks, automated evaluation, healthcare AI, clinical scenarios

**Relevance Score:** 9

**TL;DR:** LLMEval-Med is a new benchmark for evaluating large language models in medicine, addressing limitations of current medical benchmarks with real-world data and an automated evaluation pipeline.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of LLMs in medical applications, which require high accuracy and complex reasoning assessment.

**Method:** Introduction of LLMEval-Med benchmark with 2,996 questions from real electronic health records and expert scenarios, along with an automated evaluation pipeline using an LLM-as-Judge framework.

**Key Contributions:**

	1. Development of LLMEval-Med benchmark covering five core medical areas
	2. Integration of real-world electronic health record data into evaluation
	3. Creation of an automated evaluation pipeline with dynamic expert feedback

**Result:** Evaluation of 13 LLMs revealed insights into the performance of specialized medical models versus open-source and closed-source models, highlighting the benchmark's effectiveness.

**Limitations:** 

**Conclusion:** LLMEval-Med provides a reliable framework for assessing LLMs in the medical field and is crucial for their safe deployment in clinical settings.

**Abstract:** Evaluating large language models (LLMs) in medicine is crucial because medical applications require high accuracy with little room for error. Current medical benchmarks have three main types: medical exam-based, comprehensive medical, and specialized assessments. However, these benchmarks have limitations in question design (mostly multiple-choice), data sources (often not derived from real clinical scenarios), and evaluation methods (poor assessment of complex reasoning). To address these issues, we present LLMEval-Med, a new benchmark covering five core medical areas, including 2,996 questions created from real-world electronic health records and expert-designed clinical scenarios. We also design an automated evaluation pipeline, incorporating expert-developed checklists into our LLM-as-Judge framework. Furthermore, our methodology validates machine scoring through human-machine agreement analysis, dynamically refining checklists and prompts based on expert feedback to ensure reliability. We evaluate 13 LLMs across three categories (specialized medical models, open-source models, and closed-source models) on LLMEval-Med, providing valuable insights for the safe and effective deployment of LLMs in medical domains. The dataset is released in https://github.com/llmeval/LLMEval-Med.

</details>


### [121] [EuroLLM-9B: Technical Report](https://arxiv.org/abs/2506.04079)

*Pedro Henrique Martins, João Alves, Patrick Fernandes, Nuno M. Guerreiro, Ricardo Rei, Amin Farajian, Mateusz Klimaszewski, Duarte M. Alves, José Pombal, Manuel Faysse, Pierre Colombo, François Yvon, Barry Haddow, José G. C. de Souza, Alexandra Birch, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** EuroLLM-9B, large language model, multilingual support, EuroFilter, synthetic dataset

**Relevance Score:** 8

**TL;DR:** EuroLLM-9B is a large language model developed to support multilingual needs across Europe, trained to cover 24 EU languages and 11 additional languages.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of European languages in existing open large language models.

**Method:** The paper details the development of EuroLLM-9B, which includes tokenizer design, architecture specifications, and data collection methods. It introduces EuroFilter for multilingual data filtering and EuroBlocks-Synthetic for enhanced post-training language coverage.

**Key Contributions:**

	1. Introduction of EuroLLM-9B model covering multiple European languages
	2. Development of EuroFilter for multilingual data filtering
	3. Creation of EuroBlocks-Synthetic dataset for enhanced language coverage

**Result:** Evaluation shows EuroLLM-9B performs competitively on multilingual benchmarks and machine translation tasks, establishing it as a leading open European-made LLM.

**Limitations:** 

**Conclusion:** EuroLLM-9B's release includes all major components to promote open research and usage in the field.

**Abstract:** This report presents EuroLLM-9B, a large language model trained from scratch to support the needs of European citizens by covering all 24 official European Union languages and 11 additional languages. EuroLLM addresses the issue of European languages being underrepresented and underserved in existing open large language models. We provide a comprehensive overview of EuroLLM-9B's development, including tokenizer design, architectural specifications, data filtering, and training procedures. We describe the pre-training data collection and filtering pipeline, including the creation of EuroFilter, an AI-based multilingual filter, as well as the design of EuroBlocks-Synthetic, a novel synthetic dataset for post-training that enhances language coverage for European languages. Evaluation results demonstrate EuroLLM-9B's competitive performance on multilingual benchmarks and machine translation tasks, establishing it as the leading open European-made LLM of its size. To support open research and adoption, we release all major components of this work, including the base and instruction-tuned models, the EuroFilter classifier, and the synthetic post-training dataset.

</details>


### [122] [TextAtari: 100K Frames Game Playing with Language Agents](https://arxiv.org/abs/2506.04098)

*Wenhao Li, Wenwu Li, Chuyun Shen, Junjie Sheng, Zixiao Huang, Di Wu, Yun Hua, Wei Yin, Xiangfeng Wang, Hongyuan Zha, Bo Jin*

**Main category:** cs.CL

**Keywords:** Language models, Decision-making, Natural language processing, Atari games, Benchmark

**Relevance Score:** 6

**TL;DR:** TextAtari is a benchmark for evaluating language agents on long-horizon decision-making tasks using textual descriptions of Atari games, highlighting performance gaps and challenges in sequential reasoning.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To create a benchmark that bridges sequential decision-making with natural language processing, specifically for tasks spanning up to 100,000 steps in classic Atari games.

**Method:** An unsupervised representation learning framework (AtariARI) is used to translate visual states into textual descriptions, creating nearly 100 distinct tasks of varying complexity for evaluation of three open-source large language models across different agent frameworks.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for long-horizon decision-making in language agents.
	2. Evaluation of the impact of different reasoning frameworks and prior knowledge on performance across various tasks.
	3. Standardized protocols for future research in language models and planning.

**Result:** Significant performance gaps are revealed between language agents and human players in tasks requiring extensive planning, specifically in terms of sequential reasoning, state tracking, and strategic planning.

**Limitations:** The results indicate performance gaps, suggesting that the models still struggle significantly with human-level strategic planning and reasoning tasks.

**Conclusion:** TextAtari offers standardized evaluation protocols and baseline implementations to advance research at the intersection of language models and long-term decision-making.

**Abstract:** We present TextAtari, a benchmark for evaluating language agents on very long-horizon decision-making tasks spanning up to 100,000 steps. By translating the visual state representations of classic Atari games into rich textual descriptions, TextAtari creates a challenging test bed that bridges sequential decision-making with natural language processing. The benchmark includes nearly 100 distinct tasks with varying complexity, action spaces, and planning horizons, all rendered as text through an unsupervised representation learning framework (AtariARI). We evaluate three open-source large language models (Qwen2.5-7B, Gemma-7B, and Llama3.1-8B) across three agent frameworks (zero-shot, few-shot chain-of-thought, and reflection reasoning) to assess how different forms of prior knowledge affect performance on these long-horizon challenges. Four scenarios-Basic, Obscured, Manual Augmentation, and Reference-based-investigate the impact of semantic understanding, instruction comprehension, and expert demonstrations on agent decision-making. Our results reveal significant performance gaps between language agents and human players in extensive planning tasks, highlighting challenges in sequential reasoning, state tracking, and strategic planning across tens of thousands of steps. TextAtari provides standardized evaluation protocols, baseline implementations, and a framework for advancing research at the intersection of language models and planning.

</details>


### [123] [Rectified Sparse Attention](https://arxiv.org/abs/2506.04108)

*Yutao Sun, Tianzhu Ye, Li Dong, Yuqing Xia, Jian Chen, Yizhao Gao, Shijie Cao, Jianyong Wang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Long-sequence generation, Large Language Models, Sparse attention

**Relevance Score:** 7

**TL;DR:** ReSA offers an efficient method for long-sequence generation in Large Language Models by combining block-sparse attention with periodic dense rectification to improve generation quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of efficient long-sequence generation in Large Language Models due to KV cache misalignment and its impact on generation quality.

**Method:** Proposes Rectified Sparse Attention (ReSA), which combines block-sparse attention with periodic dense rectification, refreshing the KV cache at fixed intervals to limit error accumulation.

**Key Contributions:**

	1. Introduction of Rectified Sparse Attention (ReSA) method
	2. Demonstrated significant efficiency improvements in long-sequence generation
	3. Code availability for practical implementation

**Result:** ReSA achieves near-lossless generation quality with significant efficiency improvements, including up to 2.42× speedup for 256K sequence lengths in various tasks.

**Limitations:** 

**Conclusion:** ReSA is presented as a practical solution for scalable long-context inference, maintaining alignment with pretraining distribution and enhancing generation quality.

**Abstract:** Efficient long-sequence generation is a critical challenge for Large Language Models. While recent sparse decoding methods improve efficiency, they suffer from KV cache misalignment, where approximation errors accumulate and degrade generation quality. In this work, we propose Rectified Sparse Attention (ReSA), a simple yet effective method that combines block-sparse attention with periodic dense rectification. By refreshing the KV cache at fixed intervals using a dense forward pass, ReSA bounds error accumulation and preserves alignment with the pretraining distribution. Experiments across math reasoning, language modeling, and retrieval tasks demonstrate that ReSA achieves near-lossless generation quality with significantly improved efficiency. Notably, ReSA delivers up to 2.42$\times$ end-to-end speedup under decoding at 256K sequence length, making it a practical solution for scalable long-context inference. Code is available at https://aka.ms/ReSA-LM.

</details>


### [124] [CLAIM: An Intent-Driven Multi-Agent Framework for Analyzing Manipulation in Courtroom Dialogues](https://arxiv.org/abs/2506.04131)

*Disha Sheshanarayana, Tanishka Magar, Ayushi Mittal, Neelam Chaplot*

**Main category:** cs.CL

**Keywords:** NLP, Legal Analysis, Manipulation Detection, Courtroom Conversations, AI in Law

**Relevance Score:** 4

**TL;DR:** The paper introduces LegalCon, a dataset for manipulation detection in courtroom conversations, and CLAIM, a framework for enhanced manipulation analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in applying NLP for detecting and analyzing manipulation in the legal domain.

**Method:** The study introduces LegalCon, a dataset of 1,063 annotated courtroom conversations, and proposes CLAIM, an Intent-driven Multi-agent framework for manipulation analysis.

**Key Contributions:**

	1. Introduction of LegalCon dataset for courtroom manipulation detection.
	2. Development of the CLAIM framework for context-aware decision-making.
	3. Highlighting the potential for NLP to enhance fairness in legal processes.

**Result:** Results indicate the potential of agentic frameworks like CLAIM to improve fairness and transparency in judicial processes.

**Limitations:** 

**Conclusion:** The research contributes to the application of NLP in legal discourse analysis and aims to develop robust tools for supporting fairness in legal decision-making.

**Abstract:** Courtrooms are places where lives are determined and fates are sealed, yet they are not impervious to manipulation. Strategic use of manipulation in legal jargon can sway the opinions of judges and affect the decisions. Despite the growing advancements in NLP, its application in detecting and analyzing manipulation within the legal domain remains largely unexplored. Our work addresses this gap by introducing LegalCon, a dataset of 1,063 annotated courtroom conversations labeled for manipulation detection, identification of primary manipulators, and classification of manipulative techniques, with a focus on long conversations. Furthermore, we propose CLAIM, a two-stage, Intent-driven Multi-agent framework designed to enhance manipulation analysis by enabling context-aware and informed decision-making. Our results highlight the potential of incorporating agentic frameworks to improve fairness and transparency in judicial processes. We hope that this contributes to the broader application of NLP in legal discourse analysis and the development of robust tools to support fairness in legal decision-making. Our code and data are available at https://github.com/Disha1001/CLAIM.

</details>


### [125] [Are Lexicon-Based Tools Still the Gold Standard for Valence Analysis in Low-Resource Flemish?](https://arxiv.org/abs/2506.04139)

*Ratna Kandala, Katie Hoemann*

**Main category:** cs.CL

**Keywords:** LLM, emotional valence, natural language processing, Flemish, human-computer interaction

**Relevance Score:** 8

**TL;DR:** The study explores the effectiveness of Dutch-specific LLMs in capturing emotional valence in everyday narratives compared to traditional tools like LIWC and Pattern, highlighting the need for tailored models for accurate analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to improve understanding of everyday language nuances in computational linguistics and emotions research by examining LLM performance in valence prediction.

**Method:** A study with 25,000 textual responses from 102 Dutch-speaking participants was conducted, where narratives about current feelings were collected alongside self-assessed valence ratings. The performance of three LLMs was compared to traditional lexicon-based tools.

**Key Contributions:**

	1. Evaluation of LLMs' emotional analysis in spontaneous narratives
	2. Comparison of LLM outputs with traditional tools
	3. Call for tailored models for low-resource languages

**Result:** The findings revealed that the Dutch-tuned LLMs struggle to accurately predict emotional valence from spontaneous narratives, showing limitations in capturing the context-dependent nature of everyday language.

**Limitations:** The study is limited to Dutch-speaking participants and focuses on only a few LLMs, which may not represent the broader capabilities of all LLMs in emotional analysis.

**Conclusion:** The study emphasizes the necessity for culturally and linguistically adapted models to improve automated emotional analysis and suggests focusing on low-resource languages like Flemish.

**Abstract:** Understanding the nuances in everyday language is pivotal for advancements in computational linguistics & emotions research. Traditional lexicon-based tools such as LIWC and Pattern have long served as foundational instruments in this domain. LIWC is the most extensively validated word count based text analysis tool in the social sciences and Pattern is an open source Python library offering functionalities for NLP. However, everyday language is inherently spontaneous, richly expressive, & deeply context dependent. To explore the capabilities of LLMs in capturing the valences of daily narratives in Flemish, we first conducted a study involving approximately 25,000 textual responses from 102 Dutch-speaking participants. Each participant provided narratives prompted by the question, "What is happening right now and how do you feel about it?", accompanied by self-assessed valence ratings on a continuous scale from -50 to +50. We then assessed the performance of three Dutch-specific LLMs in predicting these valence scores, and compared their outputs to those generated by LIWC and Pattern. Our findings indicate that, despite advancements in LLM architectures, these Dutch tuned models currently fall short in accurately capturing the emotional valence present in spontaneous, real-world narratives. This study underscores the imperative for developing culturally and linguistically tailored models/tools that can adeptly handle the complexities of natural language use. Enhancing automated valence analysis is not only pivotal for advancing computational methodologies but also holds significant promise for psychological research with ecologically valid insights into human daily experiences. We advocate for increased efforts in creating comprehensive datasets & finetuning LLMs for low-resource languages like Flemish, aiming to bridge the gap between computational linguistics & emotion research.

</details>


### [126] [Establishing Trustworthy LLM Evaluation via Shortcut Neuron Analysis](https://arxiv.org/abs/2506.04142)

*Kejian Zhu, Shangqing Tu, Zhuoran Jin, Lei Hou, Juanzi Li, Jun Zhao*

**Main category:** cs.CL

**Keywords:** large language models, evaluation, data contamination, shortcut neurons, trustworthy benchmarks

**Relevance Score:** 9

**TL;DR:** The paper addresses data contamination in LLM evaluations, proposing a method to identify and suppress shortcut neurons for more trustworthy results.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Ensuring trustworthy evaluation of large language models (LLMs) is crucial as current public benchmarks are prone to data contamination, compromising fairness.

**Method:** The authors propose a novel method for identifying shortcut neurons through comparative and causal analysis, and introduce 'shortcut neuron patching' to suppress these neurons.

**Key Contributions:**

	1. Identification of shortcut neurons in LLMs
	2. Introduction of shortcut neuron patching
	3. Validation of the method's effectiveness through correlation with MixEval.

**Result:** Experiments validate that the proposed method effectively mitigates contamination, achieving a high Spearman coefficient with MixEval, indicating strong correlation with true model capabilities.

**Limitations:** 

**Conclusion:** The method demonstrates generalizability across various benchmarks and hyperparameter settings, validating its effectiveness in enhancing evaluations of LLMs.

**Abstract:** The development of large language models (LLMs) depends on trustworthy evaluation. However, most current evaluations rely on public benchmarks, which are prone to data contamination issues that significantly compromise fairness. Previous researches have focused on constructing dynamic benchmarks to address contamination. However, continuously building new benchmarks is costly and cyclical. In this work, we aim to tackle contamination by analyzing the mechanisms of contaminated models themselves. Through our experiments, we discover that the overestimation of contaminated models is likely due to parameters acquiring shortcut solutions in training. We further propose a novel method for identifying shortcut neurons through comparative and causal analysis. Building on this, we introduce an evaluation method called shortcut neuron patching to suppress shortcut neurons. Experiments validate the effectiveness of our approach in mitigating contamination. Additionally, our evaluation results exhibit a strong linear correlation with MixEval, a recently released trustworthy benchmark, achieving a Spearman coefficient ($\rho$) exceeding 0.95. This high correlation indicates that our method closely reveals true capabilities of the models and is trustworthy. We conduct further experiments to demonstrate the generalizability of our method across various benchmarks and hyperparameter settings. Code: https://github.com/GaryStack/Trustworthy-Evaluation

</details>


### [127] [A Dataset for Addressing Patient's Information Needs related to Clinical Course of Hospitalization](https://arxiv.org/abs/2506.04156)

*Sarvesh Soni, Dina Demner-Fushman*

**Main category:** cs.CL

**Keywords:** EHR, AI, QA, patient-centered, dataset

**Relevance Score:** 9

**TL;DR:** ArchEHR-QA introduces a novel dataset for evaluating AI responses to patient questions using clinical evidence from EHRs, focusing on their factual accuracy and relevance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of robust datasets that capture patient information needs related to their hospitalizations, which can be addressed using AI systems.

**Method:** The study introduces ArchEHR-QA, an expert-annotated dataset of 134 patient cases that includes questions from patients, clinician interpretations, clinical note excerpts, and clinician-authored answers. It evaluates the performance of three large language models (Llama 4, Llama 3, and Mixtral) using different prompting strategies.

**Key Contributions:**

	1. Introduction of ArchEHR-QA, a novel dataset for patient-centered EHR QA systems
	2. Evaluation of multiple LLMs with novel prompting strategies
	3. Identifying common issues in AI-generated clinical responses

**Result:** The answer-first prompting method with Llama 4 outperformed the other models, achieving the highest scores in factuality and relevance, validated through manual error analysis.

**Limitations:** The dataset is limited to 134 patient cases, and findings may not generalize broadly across different patient populations or contexts.

**Conclusion:** ArchEHR-QA sets a benchmark for patient-centered EHR question answering systems and highlights the need for generating accurate and relevant AI responses in clinical contexts.

**Abstract:** Patients have distinct information needs about their hospitalization that can be addressed using clinical evidence from electronic health records (EHRs). While artificial intelligence (AI) systems show promise in meeting these needs, robust datasets are needed to evaluate the factual accuracy and relevance of AI-generated responses. To our knowledge, no existing dataset captures patient information needs in the context of their EHRs. We introduce ArchEHR-QA, an expert-annotated dataset based on real-world patient cases from intensive care unit and emergency department settings. The cases comprise questions posed by patients to public health forums, clinician-interpreted counterparts, relevant clinical note excerpts with sentence-level relevance annotations, and clinician-authored answers. To establish benchmarks for grounded EHR question answering (QA), we evaluated three open-weight large language models (LLMs)--Llama 4, Llama 3, and Mixtral--across three prompting strategies: generating (1) answers with citations to clinical note sentences, (2) answers before citations, and (3) answers from filtered citations. We assessed performance on two dimensions: Factuality (overlap between cited note sentences and ground truth) and Relevance (textual and semantic similarity between system and reference answers). The final dataset contains 134 patient cases. The answer-first prompting approach consistently performed best, with Llama 4 achieving the highest scores. Manual error analysis supported these findings and revealed common issues such as omitted key clinical evidence and contradictory or hallucinated content. Overall, ArchEHR-QA provides a strong benchmark for developing and evaluating patient-centered EHR QA systems, underscoring the need for further progress toward generating factual and relevant responses in clinical contexts.

</details>


### [128] [SkipGPT: Dynamic Layer Pruning Reinvented with Token Awareness and Module Decoupling](https://arxiv.org/abs/2506.04179)

*Anhao Zhao, Fanghua Ye, Yingqi Fan, Junlong Tong, Zhiwei Fei, Hui Su, Xiaoyu Shen*

**Main category:** cs.CL

**Keywords:** dynamic layer pruning, large language models, token-aware routing

**Relevance Score:** 9

**TL;DR:** SkipGPT introduces a dynamic layer pruning framework that optimizes large language models' computational efficiency by implementing global token-aware routing and decoupled pruning policies for different layers, achieving substantial parameter reduction while maintaining performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models are computationally expensive; thus, efficient models are needed to balance resource usage and performance.

**Method:** SkipGPT employs a two-stage optimization process with a disentangled training phase for routing strategies and LoRA fine-tuning to maintain model performance post-pruning.

**Key Contributions:**

	1. Introduction of dynamic layer pruning for LLMs
	2. Global token-aware routing mechanism
	3. Decoupled pruning strategies for different model components

**Result:** SkipGPT can reduce model parameters by over 40% while matching or exceeding the performance of the original dense model on various benchmarks.

**Limitations:** 

**Conclusion:** The framework harmonizes efficiency and performance, advancing scalable deployment of LLMs in real-world applications.

**Abstract:** Large language models (LLMs) achieve remarkable performance across tasks but incur substantial computational costs due to their deep, multi-layered architectures. Layer pruning has emerged as a strategy to alleviate these inefficiencies, but conventional static pruning methods overlook two critical dynamics inherent to LLM inference: (1) horizontal dynamics, where token-level heterogeneity demands context-aware pruning decisions, and (2) vertical dynamics, where the distinct functional roles of MLP and self-attention layers necessitate component-specific pruning policies. We introduce SkipGPT, a dynamic layer pruning framework designed to optimize computational resource allocation through two core innovations: (1) global token-aware routing to prioritize critical tokens, and (2) decoupled pruning policies for MLP and self-attention components. To mitigate training instability, we propose a two-stage optimization paradigm: first, a disentangled training phase that learns routing strategies via soft parameterization to avoid premature pruning decisions, followed by parameter-efficient LoRA fine-tuning to restore performance impacted by layer removal. Extensive experiments demonstrate that SkipGPT reduces over 40% of model parameters while matching or exceeding the performance of the original dense model across benchmarks. By harmonizing dynamic efficiency with preserved expressivity, SkipGPT advances the practical deployment of scalable, resource-aware LLMs. Our code is publicly available at: https://github.com/EIT-NLP/SkipGPT.

</details>


### [129] [SuperWriter: Reflection-Driven Long-Form Generation with Large Language Models](https://arxiv.org/abs/2506.04180)

*Yuhao Wu, Yushi Bai, Zhiqiang Hu, Juanzi Li, Roy Ka-Wei Lee*

**Main category:** cs.CL

**Keywords:** long-form text generation, language models, structured thinking, preference optimization, Monte Carlo Tree Search

**Relevance Score:** 8

**TL;DR:** SuperWriter-Agent is an agent-based framework that enhances long-form text generation quality and consistency for LLMs through structured thinking and hierarchical optimization.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Long-form text generation in LLMs faces challenges in coherence, logical consistency, and quality, particularly as sequence lengths increase.

**Method:** The SuperWriter-Agent integrates structured thinking and introduces planning and refinement stages into the text generation pipeline. A 7B SuperWriter-LM is trained on a newly constructed supervised fine-tuning dataset, employing a hierarchical Direct Preference Optimization procedure guided by Monte Carlo Tree Search.

**Key Contributions:**

	1. Introduction of SuperWriter-Agent framework for LLM text generation.
	2. Development of hierarchical Direct Preference Optimization for refining text quality.
	3. Creation of a new supervised fine-tuning dataset for training long-form LLMs.

**Result:** SuperWriter-LM achieves state-of-the-art results on various benchmarks and outperforms larger models in both automatic and human evaluations.

**Limitations:** 

**Conclusion:** The framework's incorporation of structured thinking steps significantly improves long-form text generation quality, as evidenced by empirical results and ablation studies.

**Abstract:** Long-form text generation remains a significant challenge for large language models (LLMs), particularly in maintaining coherence, ensuring logical consistency, and preserving text quality as sequence length increases. To address these limitations, we propose SuperWriter-Agent, an agent-based framework designed to enhance the quality and consistency of long-form text generation. SuperWriter-Agent introduces explicit structured thinking-through planning and refinement stages into the generation pipeline, guiding the model to follow a more deliberate and cognitively grounded process akin to that of a professional writer. Based on this framework, we construct a supervised fine-tuning dataset to train a 7B SuperWriter-LM. We further develop a hierarchical Direct Preference Optimization (DPO) procedure that uses Monte Carlo Tree Search (MCTS) to propagate final quality assessments and optimize each generation step accordingly. Empirical results across diverse benchmarks demonstrate that SuperWriter-LM achieves state-of-the-art performance, surpassing even larger-scale baseline models in both automatic evaluation and human evaluation. Furthermore, comprehensive ablation studies demonstrate the effectiveness of hierarchical DPO and underscore the value of incorporating structured thinking steps to improve the quality of long-form text generation.

</details>


### [130] [Long or short CoT? Investigating Instance-level Switch of Large Reasoning Models](https://arxiv.org/abs/2506.04182)

*Ruiqi Zhang, Changyi Xiao, Yixin Cao*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, prompting strategies, SwitchCoT, computational efficiency, token consumption

**Relevance Score:** 8

**TL;DR:** This paper presents SwitchCoT, a framework for selecting between long and short Chain-of-Thought strategies based on task context, which improves computational efficiency while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As large reasoning models advance, understanding when to use long vs. short Chain-of-Thought prompting is essential to balance between performance and token consumption.

**Method:** Empirical analysis comparing long and short Chain-of-Thought prompting strategies under varying resource constraints, leading to the development of an automatic framework, SwitchCoT.

**Key Contributions:**

	1. Introduction of SwitchCoT framework that dynamically selects CoT strategies
	2. Empirical analysis highlighting the trade-off between token usage and performance
	3. Proven reduction in inference costs while maintaining accuracy

**Result:** SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy, achieving performance comparable to or exceeding that of using either long or short CoT alone under limited token budgets.

**Limitations:** Focus on budget constraints may limit generalizability to all Chain-of-Thought tasks or scenarios without such constraints.

**Conclusion:** A dynamic and budget-aware approach to Chain-of-Thought prompting can significantly optimize performance across different resource availability scenarios.

**Abstract:** With the rapid advancement of large reasoning models, long Chain-of-Thought (CoT) prompting has demonstrated strong performance on complex tasks. However, this often comes with a significant increase in token usage. In this paper, we conduct a comprehensive empirical analysis comparing long and short CoT strategies. Our findings reveal that while long CoT can lead to performance improvements, its benefits are often marginal relative to its significantly higher token consumption. Specifically, long CoT tends to outperform when ample generation budgets are available, whereas short CoT is more effective under tighter budget constraints. These insights underscore the need for a dynamic approach that selects the proper CoT strategy based on task context and resource availability. To address this, we propose SwitchCoT, an automatic framework that adaptively chooses between long and short CoT strategies to balance reasoning accuracy and computational efficiency. Moreover, SwitchCoT is designed to be budget-aware, making it broadly applicable across scenarios with varying resource constraints. Experimental results demonstrate that SwitchCoT can reduce inference costs by up to 50% while maintaining high accuracy. Notably, under limited token budgets, it achieves performance comparable to, or even exceeding, that of using either long or short CoT alone.

</details>


### [131] [R-Search: Empowering LLM Reasoning with Search via Multi-Reward Reinforcement Learning](https://arxiv.org/abs/2506.04185)

*Qingfei Zhao, Ruobing Wang, Dingling Xu, Daren Zha, Limin Liu*

**Main category:** cs.CL

**Keywords:** large language models, reasoning-search integration, reinforcement learning, multi-step reasoning, search interaction

**Relevance Score:** 9

**TL;DR:** R-Search is a framework for integrating reasoning with search in large language models, using reinforcement learning to improve response quality in complex tasks.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of LLMs by optimizing their interaction with search mechanisms, addressing the current limitations in producing optimal responses during reasoning-search interactions.

**Method:** A reinforcement learning framework that autonomously determines when a model should retrieve information or reason, employing multi-reward signals to optimize reasoning-search interaction trajectories.

**Key Contributions:**

	1. Development of R-Search for reasoning-search integration in LLMs
	2. Utilization of multi-reward signals to optimize reasoning trajectories
	3. Demonstrated significant performance improvements over current RAG methodologies

**Result:** R-Search significantly outperforms existing RAG baselines, achieving improvements of up to 32.2% in-domain and 25.1% out-of-domain across seven datasets.

**Limitations:** 

**Conclusion:** The introduction of R-Search provides a robust method for facilitating deep interactions between reasoning and search, leading to better performance in logic and knowledge-intensive tasks.

**Abstract:** Large language models (LLMs) have notably progressed in multi-step and long-chain reasoning. However, extending their reasoning capabilities to encompass deep interactions with search remains a non-trivial challenge, as models often fail to identify optimal reasoning-search interaction trajectories, resulting in suboptimal responses. We propose R-Search, a novel reinforcement learning framework for Reasoning-Search integration, designed to enable LLMs to autonomously execute multi-step reasoning with deep search interaction, and learn optimal reasoning search interaction trajectories via multi-reward signals, improving response quality in complex logic- and knowledge-intensive tasks. R-Search guides the LLM to dynamically decide when to retrieve or reason, while globally integrating key evidence to enhance deep knowledge interaction between reasoning and search. During RL training, R-Search provides multi-stage, multi-type rewards to jointly optimize the reasoning-search trajectory. Experiments on seven datasets show that R-Search outperforms advanced RAG baselines by up to 32.2% (in-domain) and 25.1% (out-of-domain). The code and data are available at https://github.com/QingFei1/R-Search.

</details>


### [132] [Efficient Knowledge Editing via Minimal Precomputation](https://arxiv.org/abs/2506.04226)

*Akshat Gupta, Maochuan Lu, Thomas Hartvigsen, Gopala Anumanchipalli*

**Main category:** cs.CL

**Keywords:** Knowledge editing, MEMIT, hidden vectors, machine learning, language models

**Relevance Score:** 7

**TL;DR:** This paper reduces the precomputation cost of MEMIT knowledge editing by showing that fewer hidden vectors are needed, significantly saving time during the editing process.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To decrease the excessive computational cost associated with the precomputation step in knowledge editing methods like MEMIT, ROME, and EMMET.

**Method:** The authors determine the theoretical minimum number of hidden vector precomputations needed for effective knowledge editing and empirically validate that less than 0.3% of the originally stipulated hidden vectors are sufficient.

**Key Contributions:**

	1. Theoretical analysis of minimum hidden vector precomputation for knowledge editing.
	2. Empirical evidence showing significantly reduced hidden vector requirements for effective editing.
	3. Demonstration of practical time savings in model editing processes.

**Result:** The study demonstrates that knowledge editing can be accomplished with a drastically reduced number of hidden vectors, resulting in a precomputation time cut down to a few minutes from the preceding hours.

**Limitations:** 

**Conclusion:** By minimizing the precomputation step, the paper enables more efficient knowledge editing in large language models, providing a practical improvement for users.

**Abstract:** Knowledge editing methods like MEMIT are able to make data and compute efficient updates of factual knowledge by using a single sentence to update facts and their consequences. However, what is often overlooked is a "precomputation step", which requires a one-time but significant computational cost. The authors of MEMIT originally precompute approximately 44 million hidden vectors per edited layer, which requires a forward pass over 44 million tokens. For GPT-J (6B), this precomputation step takes 36 hours on a single GPU, while it takes approximately 40 hours for Llama2-7B. Additionally, this precomputation time grows with model size. In this paper, we show that this excessive computational cost is unnecessary. Knowledge editing using MEMIT and related methods, such as ROME and EMMET, can be performed by pre-computing a very small portion of the 44 million hidden vectors. We first present the theoretical minimum number of hidden vector precomputation required for solutions of these editing methods to exist. We then empirically show that knowledge editing using these methods can be done by pre-computing significantly fewer hidden vectors. Specifically, we show that the precomputation step can be done with less than 0.3% of the originally stipulated number of hidden vectors. This saves a significant amount of precomputation time and allows users to begin editing new models within a few minutes.

</details>


### [133] [Transformers in Speech Processing: A Survey](https://arxiv.org/abs/2303.11607)

*Siddique Latif, Aun Zaidi, Heriberto Cuayahuitl, Fahad Shamshad, Moazzam Shoukat, Muhammad Usama, Junaid Qadir*

**Main category:** cs.CL

**Keywords:** transformers, speech processing, natural language processing, survey, machine learning

**Relevance Score:** 7

**TL;DR:** This paper surveys the application of transformers in various speech processing domains including recognition, synthesis, and dialogue systems, addressing their challenges and potential solutions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To consolidate and present research on the use of transformers in speech processing, a rapidly developing area influenced by advances in natural language processing.

**Method:** A comprehensive survey of the literature and research studies focusing on transformer applications in different subfields of speech technology.

**Key Contributions:**

	1. Comprehensive survey of transformers in speech technology
	2. Identification of challenges and solutions for using transformers
	3. Resource for future research in speech processing

**Result:** The survey reveals the broad adoption of transformers in speech processing and identifies significant challenges they face, along with possible solutions.

**Limitations:** 

**Conclusion:** This work serves as a resource for researchers to understand the potential of transformers in advancing speech technology and guides them in overcoming existing challenges.

**Abstract:** The remarkable success of transformers in the field of natural language processing has sparked the interest of the speech-processing community, leading to an exploration of their potential for modeling long-range dependencies within speech sequences. Recently, transformers have gained prominence across various speech-related domains, including automatic speech recognition, speech synthesis, speech translation, speech para-linguistics, speech enhancement, spoken dialogue systems, and numerous multimodal applications. In this paper, we present a comprehensive survey that aims to bridge research studies from diverse subfields within speech technology. By consolidating findings from across the speech technology landscape, we provide a valuable resource for researchers interested in harnessing the power of transformers to advance the field. We identify the challenges encountered by transformers in speech processing while also offering insights into potential solutions to address these issues.

</details>


### [134] [WizardMath: Empowering Mathematical Reasoning for Large Language Models via Reinforced Evol-Instruct](https://arxiv.org/abs/2308.09583)

*Haipeng Luo, Qingfeng Sun, Can Xu, Pu Zhao, Jianguang Lou, Chongyang Tao, Xiubo Geng, Qingwei Lin, Shifeng Chen, Yansong Tang, Dongmei Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Reinforcement Learning, NLP, Instruction Evolution

**Relevance Score:** 9

**TL;DR:** This paper introduces WizardMath, a model that enhances LLMs' mathematical reasoning using RLEIF, outperforming existing models in math-related NLP tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing open-source LLMs lack math-related optimization despite their strong NLP performance.

**Method:** The authors developed WizardMath, utilizing their Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method specifically for enhancing mathematical reasoning in LLMs.

**Key Contributions:**

	1. Introduction of WizardMath for improved mathematical reasoning in LLMs
	2. Demonstration of significant performance improvements over existing open-source models
	3. Highlighting the role of instruction evolution in LLM training

**Result:** WizardMath-Mistral 7B demonstrates superior performance on GSM8k and MATH benchmarks, exceeding top LLM competitors and achieving higher data efficiency.

**Limitations:** 

**Conclusion:** The study accentuates the importance of instruction evolution and process supervision in improving mathematical reasoning in LLMs.

**Abstract:** Large language models (LLMs), such as GPT-4, have shown remarkable performance in natural language processing (NLP) tasks, including challenging mathematical reasoning. However, most existing open-source models are only pre-trained on large-scale internet data and without math-related optimization. In this paper, we present WizardMath, which enhances the mathematical CoT reasoning abilities of LLMs without using external python tools, by applying our proposed Reinforcement Learning from Evol-Instruct Feedback (RLEIF) method to the domain of math. Through extensive experiments on two mathematical reasoning benchmarks, namely GSM8k and MATH, we reveal the extraordinary capabilities of our model. Remarkably, WizardMath-Mistral 7B surpasses top-tier open-source LLMs by a substantial margin with higher data efficiency. Furthermore, WizardMath 70B even outperforms GPT-3.5-Turbo, Claude 2, Gemini Pro and GPT-4-early-version. Additionally, our preliminary exploration highlights the pivotal role of instruction evolution and process supervision in achieving exceptional math performance. For more details refer to https://github.com/nlpxucan/WizardLM

</details>


### [135] [Concept-Guided Chain-of-Thought Prompting for Pairwise Comparison Scoring of Texts with Large Language Models](https://arxiv.org/abs/2310.12049)

*Patrick Y. Wu, Jonathan Nagler, Joshua A. Tucker, Solomon Messing*

**Main category:** cs.CL

**Keywords:** text scoring, large language models, HCI, political discourse, machine learning

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel text scoring framework utilizing generative large language models (LLMs) for effective short text comparison, demonstrating superior performance to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing text scoring methods face challenges such as requiring large corpora or hand-labeled data. This study aims to develop an efficient framework addressing these limitations.

**Method:** The authors propose a concept-guided chain-of-thought (CGCoT) approach which includes generating concept-specific breakdowns of texts using an LLM and conducting pairwise comparisons to produce scores.

**Key Contributions:**

	1. Introduction of the CGCoT framework for text scoring
	2. Achieving superior performance to traditional unsupervised methods
	3. Ability to score texts without requiring extensive hand-labeled datasets

**Result:** The CGCoT method achieves stronger correlations with human judgments compared to unsupervised methods like Wordfish and performs on par with a fine-tuned RoBERTa-Large model without additional hand-labeled data.

**Limitations:** 

**Conclusion:** Combining human expertise with LLMs shows promise for improving text scoring tasks, particularly in political discourse analysis on platforms like Twitter.

**Abstract:** Existing text scoring methods require a large corpus, struggle with short texts, or require hand-labeled data. We develop a text scoring framework that leverages generative large language models (LLMs) to (1) set texts against the backdrop of information from the near-totality of the web and digitized media, and (2) effectively transform pairwise text comparisons from a reasoning problem to a pattern recognition task. Our approach, concept-guided chain-of-thought (CGCoT), utilizes a chain of researcher-designed prompts with an LLM to generate a concept-specific breakdown for each text, akin to guidance provided to human coders. We then pairwise compare breakdowns using an LLM and aggregate answers into a score using a probability model. We apply this approach to better understand speech reflecting aversion to specific political parties on Twitter, a topic that has commanded increasing interest because of its potential contributions to democratic backsliding. We achieve stronger correlations with human judgments than widely used unsupervised text scoring methods like Wordfish. In a supervised setting, besides a small pilot dataset to develop CGCoT prompts, our measures require no additional hand-labeled data and produce predictions on par with RoBERTa-Large fine-tuned on thousands of hand-labeled tweets. This project showcases the potential of combining human expertise and LLMs for scoring tasks.

</details>


### [136] [CheckEmbed: Effective Verification of LLM Solutions to Open-Ended Tasks](https://arxiv.org/abs/2406.02524)

*Maciej Besta, Lorenzo Paleari, Marcin Copik, Robert Gerstenberger, Ales Kubicek, Piotr Nyczyk, Patrick Iff, Eric Schreiber, Tanja Srindran, Tomasz Lehmann, Hubert Niewiadomski, Torsten Hoefler*

**Main category:** cs.CL

**Keywords:** Large Language Models, verification, CheckEmbed, embeddings, hallucinations

**Relevance Score:** 9

**TL;DR:** CheckEmbed (CE) is a new verification method for LLM outputs that uses embedding vectors for accurate and scalable verification, outperforming previous methods in detecting hallucinations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The verification of outputs from Large Language Models (LLMs) is challenging, particularly for complex tasks, making the need for new methods critical.

**Method:** CE reduces LLM answers to a single embedding vector using modern embedding models, allowing for whole-answer level comparisons that improve accuracy and scalability.

**Key Contributions:**

	1. Introduction of CheckEmbed (CE) for whole-answer verification of LLM outputs
	2. Empirical demonstration of CE's effectiveness in detecting hallucinations
	3. Generalization of CE beyond text to vision tasks

**Result:** CE demonstrates superior effectiveness and efficiency in detecting hallucinations across various tasks compared to 13 existing verification methods.

**Limitations:** 

**Conclusion:** CE establishes itself as a versatile framework for output verification in both text and other modalities such as vision.

**Abstract:** Large Language Models (LLMs) are transforming a wide range of domains, yet verifying their outputs remains a significant challenge, especially for complex open-ended tasks such as consolidation, summarization, and knowledge extraction. To address this, we introduce CheckEmbed (CE): a simple, scalable, and accurate verification method. CE reduces each LLM answer to a single embedding vector using powerful modern embedding LLM models like SFR-Embedding-Mistral. Prior methods such as BERTScore and SelfCheckGPT relied on weaker encoders like BERT, forcing them to operate at token or sentence granularity. In contrast, CE performs fast, semantically rich comparisons directly at the whole-answer level, overcoming key limitations in both accuracy and scalability. We conduct a comprehensive design and time complexity analysis across 13 verification baselines, including classical text scorers (e.g., BLEU), stability-based methods (e.g., SelfCheckGPT), and generative evaluators (e.g., LLM-as-a-Judge), which highlights the effectiveness, efficiency, versatility, and simplicity of CE. Empirical results show that CE reliably detects hallucinations in both closed and open-ended tasks. We further present evidence that CE generalizes beyond text to other modalities such as vision, establishing it as a practical and versatile verification framework.

</details>


### [137] [AlignMMBench: Evaluating Chinese Multimodal Alignment in Large Vision-Language Models](https://arxiv.org/abs/2406.09295)

*Yuhang Wu, Wenmeng Yu, Yean Cheng, Yan Wang, Xiaohan Zhang, Jiazheng Xu, Ming Ding, Yuxiao Dong*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, alignment capabilities, benchmark, Chinese visual contexts, CritiqueVLM

**Relevance Score:** 6

**TL;DR:** This paper introduces AlignMMBench, a benchmark for evaluating the alignment capabilities of large Vision-Language Models (VLMs) in Chinese visual contexts, offering a nuanced assessment beyond existing benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive benchmarks that evaluate the alignment capabilities of VLMs in real-world, Chinese-specific contexts using nuanced methodologies.

**Method:** AlignMMBench consists of 1,054 images and 4,978 QA pairs across thirteen tasks, incorporating both single-turn and multi-turn dialogues, along with a rule-calibrated evaluator called CritiqueVLM for performance assessment.

**Key Contributions:**

	1. Introduction of AlignMMBench for nuanced VLM evaluation in Chinese contexts
	2. Development of CritiqueVLM as a superior evaluator compared to existing models like GPT-4
	3. Creation of a quantitative 'alignment score' for model robustness assessment

**Result:** The evaluation reveals varying capabilities and limitations among different VLM architectures when assessed using AlignMMBench.

**Limitations:** 

**Conclusion:** AlignMMBench provides a significant advancement in evaluating the alignment capabilities of VLMs, incorporating novel metrics and methodologies that can benefit future research in this area.

**Abstract:** Evaluating the alignment capabilities of large Vision-Language Models (VLMs) is essential for determining their effectiveness as helpful assistants. However, existing benchmarks primarily focus on basic abilities using nonverbal methods, such as yes-no and multiple-choice questions. In this paper, we address this gap by introducing AlignMMBench, which provides more nuanced evaluations of alignment capabilities and is the first benchmark specifically designed for Chinese visual contexts. This benchmark is meticulously curated from real-world scenarios and internet sources, encompassing thirteen specific tasks across three categories, and includes both single-turn and multi-turn dialogue scenarios. Incorporating a prompt rewrite strategy, AlignMMBench encompasses 1,054 images and 4,978 question-answer pairs. To facilitate the evaluation pipeline, we develop CritiqueVLM, a rule-calibrated evaluator that exceeds GPT-4's evaluation ability. Additionally, we measure the "alignment score", a quantitative metric designed to assess the robustness and stability of models across diverse prompts. Finally, we evaluate the performance of representative VLMs on AlignMMBench, offering insights into the capabilities and limitations of different VLM architectures. The evaluation code and data are available at https://github.com/THUDM/AlignMMBench.

</details>


### [138] [UBench: Benchmarking Uncertainty in Large Language Models with Multiple Choice Questions](https://arxiv.org/abs/2406.12784)

*Xunzhi Wang, Zhuowei Zhang, Gaonan Chen, Qiongyu Li, Bitong Luo, Zhixin Han, Haotian Wang, Zhiyu li, Hang Gao, Mengting Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Uncertainty Quantification, Confidence Intervals, Machine Learning

**Relevance Score:** 9

**TL;DR:** Introducing UBench, a new benchmark for evaluating the uncertainty of large language models (LLMs) based on confidence intervals.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There are significant challenges in benchmarking the uncertainty of large language models (LLMs) due to the need for internal model access, additional training, and high computational costs, particularly for closed-source models.

**Method:** UBench utilizes confidence intervals and includes 11,978 multiple-choice questions to evaluate various capabilities across 20 LLMs, performing comprehensive comparisons with advanced uncertainty estimation methods.

**Key Contributions:**

	1. Introduction of a new benchmark (UBench) for LLM uncertainty evaluation
	2. Use of confidence intervals for uncertainty quantification
	3. Insights on the effectiveness of various prompting strategies for improving LLM reliability

**Result:** UBench demonstrates that confidence interval-based methods are effective for uncertainty quantification, with open-source models performing competitively compared to closed-source models. The analysis also uncovers the potential of CoT and RP prompts to improve model reliability.

**Limitations:** The research is limited to specific models and questions, leaving potential explorations of other models and question types unexplored.

**Conclusion:** The UBench framework presents valuable insights into estimating uncertainty in LLMs, showing that certain prompting strategies can enhance reliability and that performance can vary between open-source and closed-source models.

**Abstract:** Despite recent progress in systematic evaluation frameworks, benchmarking the uncertainty of large language models (LLMs) remains a highly challenging task. Existing methods for benchmarking the uncertainty of LLMs face three key challenges: the need for internal model access, additional training, or high computational costs. This is particularly unfavorable for closed-source models. To this end, we introduce UBench, a new benchmark for evaluating the uncertainty of LLMs. Unlike other benchmarks, UBench is based on confidence intervals. It encompasses 11,978 multiple-choice questions spanning knowledge, language, understanding, and reasoning capabilities. Based on this, we conduct extensive experiments. This includes comparisons with other advanced uncertainty estimation methods, the assessment of the uncertainty of 20 LLMs, and an exploration of the effects of Chain-of-Thought (CoT) prompts, role-playing (RP) prompts, and temperature on model uncertainty. Our analysis reveals several crucial insights: 1) Our confidence interval-based methods are highly effective for uncertainty quantification; 2) Regarding uncertainty, outstanding open-source models show competitive performance versus closed-source models; 3) CoT and RP prompts present potential ways to improve model reliability, while the influence of temperature changes follows no universal rule. Our implementation is available at https://github.com/Cyno2232/UBENCH.

</details>


### [139] [UIO-LLMs: Unbiased Incremental Optimization for Long-Context LLMs](https://arxiv.org/abs/2406.18173)

*Wenhao Li, Mingbao Lin, Yunshan Zhong, Shuicheng Yan, Rongrong Ji*

**Main category:** cs.CL

**Keywords:** memory-enhanced transformers, long-context, incremental optimization, UIO-LLMs, Llama2

**Relevance Score:** 8

**TL;DR:** This paper presents UIO-LLMs, an innovative memory-enhanced transformer approach designed to manage long texts in large language models by extending the context window significantly while optimizing training and inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in large language models, managing extended contexts remains a significant challenge due to limitations in context window sizes. This study seeks to enhance the capability of LLMs in handling long texts efficiently.

**Method:** The method involves an encoder-decoder framework utilizing shared weights, where the encoder creates memory from context segments to aid in predicting subsequent outputs. Additionally, the Truncated Backpropagation Through Time (TBPTT) algorithm is used to enhance the training process by reducing time complexity and improving gradient computation efficiency.

**Key Contributions:**

	1. Introduction of UIO-LLMs for long-context management
	2. Use of TBPTT for training optimization
	3. Significant extension of context window with minimal parameter increase

**Result:** UIO-LLMs extend the context window of models like Llama2-7b-chat from 4K to 100K tokens with only a 2% increase in parameters, while maintaining nearly linear inference costs as context length grows.

**Limitations:** The experimental results of the paper require further validation.

**Conclusion:** The proposed approach enables more effective handling of long context in large language models, although further experimental validation is needed for the results presented.

**Abstract:** Managing long texts is challenging for large language models (LLMs) due to limited context window sizes. This study introduces UIO-LLMs, an unbiased incremental optimization approach for memory-enhanced transformers under long-context settings. We initially conceptualize the process as a streamlined encoder-decoder framework where the weights-shared encoder and decoder respectively encapsulate a context segment into memories and leverage these memories to predict outputs of the subsequent segment. Subsequently, by treating our memory-enhanced transformers as fully-connected recurrent neural networks (RNNs), we refine the training process using the Truncated Backpropagation Through Time (TBPTT) algorithm, which incorporates innovative incremental optimization techniques. These techniques not only diminish time complexity but also address the bias in gradient computation through an unbiased optimization process. UIO-LLMs successfully handle long context, such as extending the context window of Llama2-7b-chat from 4K to 100K tokens with minimal 2% additional parameters, while keeping the inference cost nearly linear as context length increases.

</details>


### [140] [REAL: Response Embedding-based Alignment for LLMs](https://arxiv.org/abs/2409.17169)

*Honggen Zhang, Xufeng Zhao, Igor Molybog, June Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human Preferences, Annotation Efficiency, Alignment, Machine Learning

**Relevance Score:** 9

**TL;DR:** Proposes REAL, a method for selecting distinct response pairs to improve LLM alignment while reducing annotation efforts by 65%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of human bias and inefficiency in the preference dataset creation for LLMs alignment.

**Method:** Response Embedding-based Alignment, focusing on acquiring less ambiguous preference pairs using similarity of response embeddings independent of prompts.

**Key Contributions:**

	1. Introduction of REAL for LLM alignment
	2. Demonstration of reduced label errors
	3. Significant savings in annotation workload

**Result:** Experimental results show improved alignment efficiency with better margins and win rates on dialogue tasks by selecting dissimilar response pairs.

**Limitations:** 

**Conclusion:** Focusing on distinct response pairs can enhance LLM alignment efficiency and significantly reduce labeling errors and effort.

**Abstract:** Aligning large language models (LLMs) to human preferences is a crucial step in building helpful and safe AI tools, which usually involve training on supervised datasets. Popular algorithms such as Direct Preference Optimization (DPO) rely on pairs of AI-generated responses ranked according to human annotation. The response pair annotation process might bring human bias. Building a correct preference dataset is the costly part of the alignment pipeline. To improve annotation efficiency and quality in the LLMs alignment, we propose REAL: Response Embedding-based Alignment for LLMs, a strategy for constructing a high-quality training dataset that focuses on acquiring the less ambiguous preference pairs for labeling out of a set of response candidates. Our selection process is based on the similarity of embedding responses independently of prompts, which guarantees the selection process in an off-policy setting, avoiding adaptively measuring the similarity during the training. Experimental results on real-world dataset SHP2 and synthetic HH-RLHF benchmarks indicate that choosing dissimilar response pairs enhances the direct alignment of LLMs while reducing inherited labeling errors. The model aligned with dissimilar response pairs obtained a better margin and win rate on the dialogue task. Our findings suggest that focusing on distinct pairs can reduce the label error and improve LLM alignment efficiency, saving up to $65\%$ of annotators' work.

</details>


### [141] [Geometric Signatures of Compositionality Across a Language Model's Lifetime](https://arxiv.org/abs/2410.01444)

*Jin Hwa Lee, Thomas Jiralerspong, Lei Yu, Yoshua Bengio, Emily Cheng*

**Main category:** cs.CL

**Keywords:** compositionality, intrinsic dimension, language models, representation complexity, geometric analysis

**Relevance Score:** 7

**TL;DR:** This paper explores the relationship between linguistic compositionality and the intrinsic dimensions of language model representations, demonstrating how dataset compositionality correlates with representation complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether contemporary language models mirror the simplicity of language through compositional structures and to understand how these structures relate to representation complexity.

**Method:** The authors take a geometric approach by analyzing the intrinsic dimension of representations from language models in relation to the compositionality of datasets.

**Key Contributions:**

	1. Establishing a link between the degree of compositionality and the intrinsic dimension of LM representations.
	2. Demonstrating that nonlinear dimensionality captures semantic aspects while linear dimensionality reflects superficial features.
	3. Providing insights into how training shapes the representation of linguistic features in language models.

**Result:** The study shows that the degree of dataset compositionality influences the intrinsic dimensions of LM representations, indicating that learned linguistic features capture both semantic and superficial aspects differently.

**Limitations:** 

**Conclusion:** The findings suggest a significant interaction between compositionality and representation complexity, emphasizing the role of training on linguistic features in LMs.

**Abstract:** By virtue of linguistic compositionality, few syntactic rules and a finite lexicon can generate an unbounded number of sentences. That is, language, though seemingly high-dimensional, can be explained using relatively few degrees of freedom. An open question is whether contemporary language models (LMs) reflect the intrinsic simplicity of language that is enabled by compositionality. We take a geometric view of this problem by relating the degree of compositionality in a dataset to the intrinsic dimension (ID) of its representations under an LM, a measure of feature complexity. We find not only that the degree of dataset compositionality is reflected in representations' ID, but that the relationship between compositionality and geometric complexity arises due to learned linguistic features over training. Finally, our analyses reveal a striking contrast between nonlinear and linear dimensionality, showing they respectively encode semantic and superficial aspects of linguistic composition.

</details>


### [142] [Nudging: Inference-time Alignment of LLMs via Guided Decoding](https://arxiv.org/abs/2410.09300)

*Yu Fei, Yasaman Razeghi, Sameer Singh*

**Main category:** cs.CL

**Keywords:** large language models, alignment, inference, ML, NLU

**Relevance Score:** 9

**TL;DR:** NUDGING is a training-free algorithm for aligning large language models at inference time using a small aligned model, improving their performance on specific tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models require alignment to safely follow instructions, but training requires high computational resources. NUDGING provides a way to achieve alignment efficiently at inference time by leveraging insights about model uncertainty.

**Method:** NUDGING generates nudging tokens from a small aligned model to guide the output of a base model during decoding, particularly when the base model is uncertain about generating stylistic tokens.

**Key Contributions:**

	1. Introduces NUDGING, a training-free alignment method for LLMs
	2. Demonstrates improved performance of base models nudged by smaller aligned models
	3. Facilitates cross-model collaboration and efficient alignment at inference time

**Result:** NUDGING achieves zero-shot performance comparable to and sometimes surpassing large aligned models while employing a small aligned model, with minimal additional inference cost.

**Limitations:** 

**Conclusion:** NUDGING offers a modular and efficient solution for aligning LLMs, enabling effective collaboration between different model families without extensive training.

**Abstract:** Large language models (LLMs) require alignment to effectively and safely follow user instructions. This process necessitates training an aligned version for every base model, resulting in significant computational overhead. In this work, we propose NUDGING, a simple, training-free algorithm that aligns any base model at inference time using a small aligned model. NUDGING is motivated by recent findings that alignment primarily alters the model's behavior on a small subset of stylistic tokens (e.g., discourse markers). We find that base models are significantly more uncertain when generating these tokens. Building on this insight, NUDGING employs a small aligned model to generate nudging tokens to guide the base model's output during decoding when the base model's uncertainty is high, with only a minor additional inference overhead. We evaluate NUDGING across 3 model families on a diverse range of open-instruction tasks. Without any training, nudging a large base model with a 7x-14x smaller aligned model achieves zero-shot performance comparable to, and sometimes surpassing, that of large aligned models. By operating at the token level, NUDGING enables off-the-shelf collaboration between model families. For instance, nudging Gemma-2-27b with Llama-27b-chat outperforms Llama-2-70b-chat on various tasks. Overall, our work offers a modular and cost-efficient solution to LLM alignment. Our code and demo are available at: https://fywalter.github.io/nudging/ .

</details>


### [143] [RULEBREAKERS: Challenging LLMs at the Crossroads between Formal Logic and Human-like Reasoning](https://arxiv.org/abs/2410.16502)

*Jason Chan, Robert Gaizauskas, Zhixue Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Formal Logic, Human-like Reasoning, Cognitive Science, RULEBREAKERS

**Relevance Score:** 9

**TL;DR:** The study introduces a dataset called RULEBREAKERS to evaluate LLMs' reasoning capabilities in unconventional scenarios, revealing limitations in their logic application compared to human reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Formal logic often fails in unique scenarios where common sense is required; the study aims to assess LLMs' ability to handle such 'rulebreaker' situations.

**Method:** The researchers developed the RULEBREAKERS dataset and evaluated its impact on seven LLMs, observing their performance and accuracy in recognizing rulebreaking instances.

**Key Contributions:**

	1. Introduction of the RULEBREAKERS dataset for LLM evaluation.
	2. Empirical findings on LLM performance in 'rulebreaker' contexts.
	3. Insights into the relationship between LLM performance and their knowledge utilization.

**Result:** The evaluation revealed that most LLMs, including GPT-4o, achieve mediocre accuracy and tend to apply logical rules too rigidly, diverging from human reasoning.

**Limitations:** The study focuses solely on LLMs' performance and does not address the potential remedies or improvements for their reasoning abilities.

**Conclusion:** The findings underline a significant limitation in current LLMs' reasoning capabilities and caution against solutions reliant on formal logic, which may exacerbate the gap between LLMs and human-like reasoning.

**Abstract:** Formal logic enables computers to reason in natural language by representing sentences in symbolic forms and applying rules to derive conclusions. However, in what our study characterizes as "rulebreaker" scenarios, this method can lead to conclusions that are typically not inferred or accepted by humans given their common sense and factual knowledge. Inspired by works in cognitive science, we create RULEBREAKERS, the first dataset for rigorously evaluating the ability of large language models (LLMs) to recognize and respond to rulebreakers (versus non-rulebreakers) in a human-like manner. Evaluating seven LLMs, we find that most models, including GPT-4o, achieve mediocre accuracy on RULEBREAKERS and exhibit some tendency to over-rigidly apply logical rules unlike what is expected from typical human reasoners. Further analysis suggests that this apparent failure is potentially associated with the models' poor utilization of their world knowledge and their attention distribution patterns. Whilst revealing a limitation of current LLMs, our study also provides a timely counterbalance to a growing body of recent works that propose methods relying on formal logic to improve LLMs' general reasoning capabilities, highlighting their risk of further increasing divergence between LLMs and human-like reasoning.

</details>


### [144] [DynaSaur: Large Language Agents Beyond Predefined Actions](https://arxiv.org/abs/2411.01747)

*Dang Nguyen, Viet Dac Lai, Seunghyun Yoon, Ryan A. Rossi, Handong Zhao, Ruiyi Zhang, Puneet Mathur, Nedim Lipka, Yu Wang, Trung Bui, Franck Dernoncourt, Tianyi Zhou*

**Main category:** cs.CL

**Keywords:** LLM agents, dynamic action generation, program execution

**Relevance Score:** 9

**TL;DR:** The paper presents a dynamic LLM agent framework that generates and composes actions in real-time, overcoming limitations of fixed action sets in complex environments.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM agents struggle in open-ended scenarios due to fixed action sets which limit flexibility and require extensive human effort to define actions.

**Method:** The proposed framework allows agents to interact with their environment by generating and executing programs in a general-purpose programming language, dynamically creating actions and accumulating them for future use.

**Key Contributions:**

	1. Introduction of dynamic action creation for LLM agents
	2. Demonstration of improved flexibility in open-ended scenarios
	3. Accumulation of generated actions for future reuse

**Result:** Experiments demonstrate that the dynamic framework significantly enhances flexibility and outperforms traditional methods that use fixed set actions, especially in unexpected scenarios.

**Limitations:** 

**Conclusion:** The dynamic agent framework enables better adaptation and recovery in complex environments, addressing shortcomings of conventional LLM agents.

**Abstract:** Existing LLM agent systems typically select actions from a fixed and predefined set at every step. While this approach is effective in closed, narrowly scoped environments, it presents two major challenges for real-world, open-ended scenarios: (1) it significantly restricts the planning and acting capabilities of LLM agents, and (2) it requires substantial human effort to enumerate and implement all possible actions, which is impractical in complex environments with a vast number of potential actions. To address these limitations, we propose an LLM agent framework that can dynamically create and compose actions as needed. In this framework, the agent interacts with its environment by generating and executing programs written in a general-purpose programming language. Moreover, generated actions are accumulated over time for future reuse. Our extensive experiments across multiple benchmarks show that this framework significantly improves flexibility and outperforms prior methods that rely on a fixed action set. Notably, it enables LLM agents to adapt and recover in scenarios where predefined actions are insufficient or fail due to unforeseen edge cases. Our code can be found in https://github.com/adobe-research/dynasaur.

</details>


### [145] [Enabling LLM Knowledge Analysis via Extensive Materialization](https://arxiv.org/abs/2411.04920)

*Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Simon Razniewski*

**Main category:** cs.CL

**Keywords:** large language models, knowledge base, factual knowledge, NLP, GPT

**Relevance Score:** 9

**TL;DR:** This paper introduces GPTKB, a knowledge base derived from an LLM, aimed at analyzing its factual knowledge comprehensively.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing methods that analyze LLM knowledge based on availability bias, introducing a comprehensive analysis of knowledge structure.

**Method:** The authors propose a novel recursive querying methodology to materialize and analyze the factual knowledge of LLMs, specifically creating a knowledge base called GPTKB.

**Key Contributions:**

	1. Introduction of a novel methodology for LLM knowledge analysis
	2. Creation of GPTKB, a large knowledge base for LLMs
	3. Analysis of LLM factual knowledge on multiple dimensions simultaneously.

**Result:** GPTKB contains 101 million relational triples for over 2.9 million entities, allowing for extensive analysis of GPT-4o-mini's knowledge, including aspects like scale, accuracy, and bias.

**Limitations:** 

**Conclusion:** GPTKB provides valuable insights into the scope and structure of LLM knowledge, overcoming previous biases in knowledge assessment.

**Abstract:** Large language models (LLMs) have majorly advanced NLP and AI, and next to their ability to perform a wide range of procedural tasks, a major success factor is their internalized factual knowledge. Since Petroni et al. (2019), analyzing this knowledge has gained attention. However, most approaches investigate one question at a time via modest-sized pre-defined samples, introducing an ``availability bias'' (Tversky&Kahnemann, 1973) that prevents the analysis of knowledge (or beliefs) of LLMs beyond the experimenter's predisposition.   To address this challenge, we propose a novel methodology to comprehensively materialize an LLM's factual knowledge through recursive querying and result consolidation. Our approach is a milestone for LLM research, for the first time providing constructive insights into the scope and structure of LLM knowledge (or beliefs).   As a prototype, we build GPTKB, a knowledge base (KB) comprising 101 million relational triples for over 2.9 million entities from GPT-4o-mini. We use GPTKB to exemplarily analyze GPT-4o-mini's factual knowledge in terms of scale, accuracy, bias, cutoff and consistency, at the same time. GPTKB is accessible at https://gptkb.org

</details>


### [146] [Improving Radiology Report Conciseness and Structure via Local Large Language Models](https://arxiv.org/abs/2411.05042)

*Iryna Hartsock, Cyrillo Araujo, Les Folio, Ghulam Rasool*

**Main category:** cs.CL

**Keywords:** radiology reports, large language models, clinical workflows, information retrieval, structured formatting

**Relevance Score:** 8

**TL;DR:** This study enhances radiology reports by using large language models to create concise, structured formats, improving clarity and reducing redundancy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges physicians face with lengthy, unstructured radiology reports, leading to missed critical information.

**Method:** The study employed private large language models (LLMs) deployed locally to structure and condense 814 radiology reports using five prompting strategies within the LangChain framework.

**Key Contributions:**

	1. Utilization of private LLMs for creating structured radiology reports
	2. Demonstrated superiority of Mixtral LLM in formatting
	3. Reduction of verbosity and improvement in clarity of reports

**Result:** The Mixtral LLM showed the best results in adhering to formatting requirements, reducing word counts by over 53%, and improving clarity in radiology reporting.

**Limitations:** 

**Conclusion:** Locally-deployed, open-source LLMs can significantly enhance the efficiency of radiology reporting, meeting the needs of referring physicians and improving clinical workflows.

**Abstract:** Radiology reports are often lengthy and unstructured, posing challenges for referring physicians to quickly identify critical imaging findings while increasing the risk of missed information. This retrospective study aimed to enhance radiology reports by making them concise and well-structured, with findings organized by relevant organs. To achieve this, we utilized private large language models (LLMs) deployed locally within our institution's firewall, ensuring data security and minimizing computational costs. Using a dataset of 814 radiology reports from seven board-certified body radiologists at Moffitt Cancer Center, we tested five prompting strategies within the LangChain framework. After evaluating several models, the Mixtral LLM demonstrated superior adherence to formatting requirements compared to alternatives like Llama. The optimal strategy involved condensing reports first and then applying structured formatting based on specific instructions, reducing verbosity while improving clarity. Across all radiologists and reports, the Mixtral LLM reduced redundant word counts by more than 53%. These findings highlight the potential of locally deployed, open-source LLMs to streamline radiology reporting. By generating concise, well-structured reports, these models enhance information retrieval and better meet the needs of referring physicians, ultimately improving clinical workflows.

</details>


### [147] [A Survey of Event Causality Identification: Taxonomy, Challenges, Assessment, and Prospects](https://arxiv.org/abs/2411.10371)

*Qing Cheng, Zefan Zeng, Xingchen Hu, Yuehang Si, Zhong Liu*

**Main category:** cs.CL

**Keywords:** Event Causality Identification, Natural Language Processing, Machine Learning, Large Language Models, Quantitative Evaluation

**Relevance Score:** 9

**TL;DR:** This survey on Event Causality Identification (ECI) reviews methods to detect causal relationships in text, offering a classification framework and discussing challenges, evaluations, and future directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically elucidate the foundational principles and frameworks of Event Causality Identification (ECI) in NLP.

**Method:** The paper proposes a novel classification framework that organizes ECI methods into Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). It reviews various methods including machine learning, deep learning, and prompt-based techniques, and conducts quantitative evaluations on benchmark datasets.

**Key Contributions:**

	1. Introduction of a novel classification framework for ECI methods.
	2. Extensive quantitative evaluations on benchmark datasets assessing various ECI methods.
	3. Discussion on advancements in multilingual, cross-lingual, and zero-shot ECI using LLMs.

**Result:** The evaluation of various ECI methods showed strengths and limitations across different models and approaches.

**Limitations:** Challenges and unresolved aspects of current ECI methods are noted.

**Conclusion:** The paper highlights the current state of ECI, discusses unresolved challenges, and outlines potential future research directions.

**Abstract:** Event Causality Identification (ECI) has emerged as a pivotal task in natural language processing (NLP), aimed at automatically detecting causal relationships between events in text. In this comprehensive survey, we systematically elucidate the foundational principles and technical frameworks of ECI, proposing a novel classification framework to categorize and clarify existing methods. {We discuss associated challenges, provide quantitative evaluations, and outline future directions for this dynamic and rapidly evolving field. We first delineate key definitions, problem formalization, and evaluation protocols of ECI. Our classification framework organizes ECI methods based on two primary tasks: Sentence-level Event Causality Identification (SECI) and Document-level Event Causality Identification (DECI). For SECI, we review methods including feature pattern-based matching, machine learning-based classification, deep semantic encoding, prompt-based fine-tuning, and causal knowledge pre-training, alongside common data augmentation strategies. For DECI, we focus on techniques such as deep semantic encoding, event graph reasoning, and prompt-based fine-tuning. We dedicate specific discussions to advancements in multi-lingual and cross-lingual ECI as well as zero-shot ECI leveraging Large Language Models (LLMs). Furthermore, we analyze the strengths, limitations, and unresolved challenges of each method. Extensive quantitative evaluations are conducted on four benchmark datasets to assess various ECI methods. Finally, we explore future research directions.

</details>


### [148] [MAmmoTH-VL: Eliciting Multimodal Reasoning with Instruction Tuning at Scale](https://arxiv.org/abs/2412.05237)

*Jarvis Guo, Tuney Zheng, Yuelin Bai, Bo Li, Yubo Wang, King Zhu, Yizhi Li, Graham Neubig, Wenhu Chen, Xiang Yue*

**Main category:** cs.CL

**Keywords:** multimodal large language models, instruction-tuning dataset, reasoning capabilities, Chain-of-Thought reasoning, dataset construction

**Relevance Score:** 8

**TL;DR:** This paper presents a novel dataset creation method for training multimodal large language models (MLLMs) that focuses on enhanced reasoning capabilities through rich intermediate rationales.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning abilities of open-source MLLMs, which are currently limited by existing simplistic instruction-tuning datasets.

**Method:** The authors developed a scalable method to construct a large-scale multimodal instruction-tuning dataset with 12M instruction-response pairs that include detailed rationales to support Chain-of-Thought (CoT) reasoning.

**Key Contributions:**

	1. Development of a new multimodal instruction-tuning dataset with elaborate rationales.
	2. Demonstration of improved reasoning capabilities in MLLMs through comprehensive experimentation.
	3. Insights into critical dataset construction techniques, such as rewriting and self-filtering.

**Result:** Training MLLMs on the new dataset led to significant improvements in reasoning performance, achieving state-of-the-art results on multiple benchmarks including MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%).

**Limitations:** 

**Conclusion:** The new dataset construction approach greatly enhances MLLMs' reasoning capabilities, with important findings on the necessity of components like rewriting and self-filtering during the dataset creation process.

**Abstract:** Open-source multimodal large language models (MLLMs) have shown significant potential in a broad range of multimodal tasks. However, their reasoning capabilities remain constrained by existing instruction-tuning datasets, which were predominately repurposed from academic datasets such as VQA, AI2D, and ChartQA. These datasets target simplistic tasks, and only provide phrase-level answers without any intermediate rationales. To address these challenges, we introduce a scalable and cost-effective method to construct a large-scale multimodal instruction-tuning dataset with rich intermediate rationales designed to elicit CoT reasoning. Using only open models, we create a dataset containing 12M instruction-response pairs to cover diverse, reasoning-intensive tasks with detailed and faithful rationales. Experiments demonstrate that training MLLMs on this dataset significantly improves reasoning capabilities, achieving state-of-the-art performance on benchmarks such as MathVerse (+8.1%), MMMU-Pro (+7%), and MuirBench (+13.3%). Additionally, the model demonstrates notable improvements of up to 4% on non-reasoning-based benchmarks. Ablation studies further highlight the importance of key components, such as rewriting and self-filtering, in the dataset construction process.

</details>


### [149] [Data Laundering: Artificially Boosting Benchmark Results through Knowledge Distillation](https://arxiv.org/abs/2412.15255)

*Jonibek Mansurov, Akhmed Sakip, Alham Fikri Aji*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Data Laundering, Language Models, Evaluation Integrity

**Relevance Score:** 7

**TL;DR:** This paper reveals vulnerabilities in language model evaluations through a method called 'Data Laundering,' which can inflate benchmark scores without improving true reasoning capabilities.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight critical vulnerabilities in current evaluation practices of language models and emphasize the need for more robust evaluation methods.

**Method:** The paper introduces 'Data Laundering,' a process that manipulates language model benchmark scores by transferring benchmark-specific knowledge through intermediate training steps.

**Key Contributions:**

	1. Introduction of 'Data Laundering' as a method for manipulating benchmark scores
	2. Demonstration of the technique's effectiveness in improving benchmark performance
	3. Highlighting the need for more robust evaluation practices in AI.

**Result:** The method demonstrated substantial improvements in benchmark accuracy (up to 75% on GPQA) using a 2-layer BERT student model without developing genuine reasoning capabilities.

**Limitations:** 

**Conclusion:** The findings serve as a cautionary tale regarding the integrity of evaluation methods in AI, urging for the development of benchmarks that better reflect true model capabilities.

**Abstract:** In this paper, we show that knowledge distillation can be subverted to manipulate language model benchmark scores, revealing a critical vulnerability in current evaluation practices. We introduce "Data Laundering," a process that enables the covert transfer of benchmark-specific knowledge through seemingly legitimate intermediate training steps. Through extensive experiments with a 2-layer BERT student model, we show how this approach can achieve substantial improvements in benchmark accuracy (up to 75\% on GPQA) without developing genuine reasoning capabilities. Notably, this method can be exploited intentionally or even unintentionally, as researchers may inadvertently adopt this method and inflate scores without realising the implications. While our findings demonstrate the effectiveness of this technique, we present them as a cautionary tale highlighting the urgent need for more robust evaluation methods in AI. This work aims to contribute to the ongoing discussion about evaluation integrity in AI development and the need for benchmarks that more accurately reflect true model capabilities. The code is available at https://github.com/mbzuai-nlp/data_laundering.

</details>


### [150] [Verbosity-Aware Rationale Reduction: Effective Reduction of Redundant Rationale via Principled Criteria](https://arxiv.org/abs/2412.21006)

*Joonwon Jang, Jaehee Kim, Wonbin Kweon, Seonghyeon Lee, Hwanjo Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inference Costs, Rationale Reduction, Verbosity, Reasoning Tasks

**Relevance Score:** 9

**TL;DR:** This paper introduces a sentence-level rationale reduction framework that optimizes inference costs while preserving reasoning capabilities in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increased inference costs in large language models (LLMs) caused by extensive intermediate reasoning units, while maintaining performance on complex tasks.

**Method:** A novel framework that utilizes likelihood-based criteria, specifically verbosity, to identify and remove redundant reasoning sentences during inference.

**Key Contributions:**

	1. Introduces a sentence-level rationale reduction framework for LLMs.
	2. Utilizes likelihood-based criteria for identifying redundant sentences.
	3. Demonstrates significant performance improvements and reductions in inference costs.

**Result:** The proposed method improves performance by an average of 7.71% and reduces token generation by 19.87% compared to models trained with complete reasoning paths.

**Limitations:** 

**Conclusion:** The framework effectively balances performance and efficiency in reasoning tasks, contributing to more cost-effective applications of LLMs.

**Abstract:** Large Language Models (LLMs) rely on generating extensive intermediate reasoning units (e.g., tokens, sentences) to enhance final answer quality across a wide range of complex tasks. While this approach has proven effective, it inevitably increases substantial inference costs. Previous methods adopting token-level reduction without clear criteria result in poor performance compared to models trained with complete rationale. To address this challenge, we propose a novel sentence-level rationale reduction framework leveraging likelihood-based criteria, verbosity, to identify and remove redundant reasoning sentences. Unlike previous approaches, our method leverages verbosity to selectively remove redundant reasoning sentences while preserving reasoning capabilities. Our experimental results across various reasoning tasks demonstrate that our method improves performance by an average of 7.71% while reducing token generation by 19.87% compared to model trained with complete reasoning paths.

</details>


### [151] [ConSim: Measuring Concept-Based Explanations' Effectiveness with Automated Simulatability](https://arxiv.org/abs/2501.05855)

*Antonin Poché, Alon Jacovi, Agustin Martin Picard, Victor Boutin, Fanny Jourdan*

**Main category:** cs.CL

**Keywords:** concept-based explanations, automated simulatability, large language models

**Relevance Score:** 8

**TL;DR:** This paper introduces an evaluation framework for concept-based explanations using automated simulatability, particularly leveraging large language models (LLMs) to assess explanation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation metrics for concept-based explanations fail to consider the effectiveness of communicating chosen concepts to users alongside the quality of the induced concept space.

**Method:** The authors propose an evaluation framework that measures concept explanations by simulating a model's outputs based on provided explanations, utilizing LLMs as simulators for empirical evaluation.

**Key Contributions:**

	1. Introduction of automated simulatability for evaluating concept explanations
	2. Use of LLMs as simulators for scalable evaluation
	3. Comprehensive empirical evaluation demonstrating consistent rankings of explanation methods

**Result:** The evaluation framework allows for consistent and scalable assessment of explanation methods across various models and datasets, showing that LLMs can reliably rank these methods.

**Limitations:** Human studies for simulatability remain challenging and might not capture all nuances of user understanding.

**Conclusion:** The proposed method provides a new avenue for rigorous evaluation of concept-based explanations and facilitates broader empirical studies.

**Abstract:** Concept-based explanations work by mapping complex model computations to human-understandable concepts. Evaluating such explanations is very difficult, as it includes not only the quality of the induced space of possible concepts but also how effectively the chosen concepts are communicated to users. Existing evaluation metrics often focus solely on the former, neglecting the latter. We introduce an evaluation framework for measuring concept explanations via automated simulatability: a simulator's ability to predict the explained model's outputs based on the provided explanations. This approach accounts for both the concept space and its interpretation in an end-to-end evaluation. Human studies for simulatability are notoriously difficult to enact, particularly at the scale of a wide, comprehensive empirical evaluation (which is the subject of this work). We propose using large language models (LLMs) as simulators to approximate the evaluation and report various analyses to make such approximations reliable. Our method allows for scalable and consistent evaluation across various models and datasets. We report a comprehensive empirical evaluation using this framework and show that LLMs provide consistent rankings of explanation methods. Code available at https://github.com/AnonymousConSim/ConSim.

</details>


### [152] [Through the Prism of Culture: Evaluating LLMs' Understanding of Indian Subcultures and Traditions](https://arxiv.org/abs/2501.16748)

*Garima Chhikara, Abhishek Kumar, Abhijnan Chakraborty*

**Main category:** cs.CL

**Keywords:** cultural bias, Little Traditions, Large Language Models, AI systems, cultural diversity

**Relevance Score:** 8

**TL;DR:** This study evaluates LLMs' ability to recognize and respond to Indian cultural practices, highlighting challenges in balancing cultural diversity within AI systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about cultural bias in LLMs, particularly regarding their interactions with under-represented subcultures.

**Method:** The paper utilizes case studies to assess LLMs' recognition of Little Traditions in Indian society and tests various prompting strategies, including the use of regional languages.

**Key Contributions:**

	1. First study analyzing LLMs' engagement with Indian subcultures
	2. Evaluation of cultural responsiveness of LLMs
	3. Insights on prompting strategies to enhance cultural sensitivity

**Result:** The study finds LLMs can articulate cultural nuances but often fail to apply this understanding in practical contexts.

**Limitations:** LLMs' understanding often fails in context-specific scenarios despite showing awareness of cultural nuances.

**Conclusion:** This is the first analysis of LLMs engagement with Indian subcultures, providing insights into the difficulties of integrating cultural diversity into AI systems.

**Abstract:** Large Language Models (LLMs) have shown remarkable advancements but also raise concerns about cultural bias, often reflecting dominant narratives at the expense of under-represented subcultures. In this study, we evaluate the capacity of LLMs to recognize and accurately respond to the Little Traditions within Indian society, encompassing localized cultural practices and subcultures such as caste, kinship, marriage, and religion. Through a series of case studies, we assess whether LLMs can balance the interplay between dominant Great Traditions and localized Little Traditions. We explore various prompting strategies and further investigate whether using prompts in regional languages enhances the models cultural sensitivity and response quality. Our findings reveal that while LLMs demonstrate an ability to articulate cultural nuances, they often struggle to apply this understanding in practical, context-specific scenarios. To the best of our knowledge, this is the first study to analyze LLMs engagement with Indian subcultures, offering critical insights into the challenges of embedding cultural diversity in AI systems.

</details>


### [153] [Refining Sentence Embedding Model through Ranking Sentences Generation with Large Language Models](https://arxiv.org/abs/2502.13656)

*Liyang He, Chenglong Liu, Rui Li, Zhenya Huang, Shulan Ruan, Jun Zhou, Enhong Chen*

**Main category:** cs.CL

**Keywords:** Sentence Embedding, Contrastive Learning, Large Language Models, NLP, Ranking Information

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel approach for controlling the generation direction of LLMs to enhance sentence embedding without heavy reliance on manual annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing contrastive learning methods that depend on manually annotated datasets, thereby improving the scalability of sentence embedding.

**Method:** The authors propose a controlled generation method for large language models (LLMs) in the latent space, integrating ranking information and semantic information into existing sentence embedding models.

**Key Contributions:**

	1. Introduced a controlled generation method for LLMs to improve sentence embeddings.
	2. Integrated ranking information into sentence embedding models for enhanced semantic distinction.
	3. Achieved state-of-the-art performance on multiple benchmarks.

**Result:** Experiments on multiple benchmarks indicate that the proposed method achieves new state-of-the-art performance in sentence embedding tasks with only a modest increase in the cost of ranking sentence synthesis.

**Limitations:** 

**Conclusion:** The proposed approach effectively balances the need for semantic distinction in embeddings while minimizing the reliance on labeled data, setting a new standard in the field.

**Abstract:** Sentence embedding is essential for many NLP tasks, with contrastive learning methods achieving strong performance using annotated datasets like NLI. Yet, the reliance on manual labels limits scalability. Recent studies leverage large language models (LLMs) to generate sentence pairs, reducing annotation dependency. However, they overlook ranking information crucial for fine-grained semantic distinctions. To tackle this challenge, we propose a method for controlling the generation direction of LLMs in the latent space. Unlike unconstrained generation, the controlled approach ensures meaningful semantic divergence. Then, we refine exist sentence embedding model by integrating ranking information and semantic information. Experiments on multiple benchmarks demonstrate that our method achieves new SOTA performance with a modest cost in ranking sentence synthesis.

</details>


### [154] [Why Safeguarded Ships Run Aground? Aligned Large Language Models' Safety Mechanisms Tend to Be Anchored in The Template Region](https://arxiv.org/abs/2502.13946)

*Chak Tou Leong, Qingyu Yin, Jian Wang, Wenjie Li*

**Main category:** cs.CL

**Keywords:** large language models, safety alignment, jailbreak attacks, template dependence, robustness

**Relevance Score:** 9

**TL;DR:** This paper investigates vulnerabilities in large language models (LLMs) related to safety alignment caused by template dependence and proposes methods for improvement.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address vulnerabilities in large language models due to their dependence on fixed templates during safety alignment, which can be exploited by jailbreak attacks.

**Method:** The authors conduct extensive experiments on various aligned LLMs and perform mechanistic analyses to explore the impact of template-anchored safety alignment on model vulnerability.

**Key Contributions:**

	1. Identification of template-anchored safety alignment vulnerabilities in LLMs
	2. Mechanistic analysis of susceptibility to jailbreak attacks
	3. Proposed solutions to mitigate risks associated with template dependence

**Result:** The experiments reveal that template-anchored safety alignment is a common issue across LLMs, leading to susceptibility under certain inference-time attacks.

**Limitations:** 

**Conclusion:** Detaching safety mechanisms from input templates shows promise in improving the robustness of LLMs against jailbreak attacks; further research is needed to advance safety alignment techniques.

**Abstract:** The safety alignment of large language models (LLMs) remains vulnerable, as their initial behavior can be easily jailbroken by even relatively simple attacks. Since infilling a fixed template between the input instruction and initial model output is a common practice for existing LLMs, we hypothesize that this template is a key factor behind their vulnerabilities: LLMs' safety-related decision-making overly relies on the aggregated information from the template region, which largely influences these models' safety behavior. We refer to this issue as template-anchored safety alignment. In this paper, we conduct extensive experiments and verify that template-anchored safety alignment is widespread across various aligned LLMs. Our mechanistic analyses demonstrate how it leads to models' susceptibility when encountering inference-time jailbreak attacks. Furthermore, we show that detaching safety mechanisms from the template region is promising in mitigating vulnerabilities to jailbreak attacks. We encourage future research to develop more robust safety alignment techniques that reduce reliance on the template region.

</details>


### [155] [Dehumanizing Machines: Mitigating Anthropomorphic Behaviors in Text Generation Systems](https://arxiv.org/abs/2502.14019)

*Myra Cheng, Su Lin Blodgett, Alicia DeVrio, Lisa Egede, Alexandra Olteanu*

**Main category:** cs.CL

**Keywords:** anthropomorphic behavior, interventions, text generation, human-computer interaction, emotional dependence

**Relevance Score:** 8

**TL;DR:** This paper investigates interventions to reduce harmful anthropomorphic behaviors in text generation systems that users may overly rely on or become emotionally dependent upon.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing perception of text generation systems as human-like raises concerns about users developing harmful dependencies and over-reliance on these systems.

**Method:** The authors compile an inventory of interventions based on literature and a crowdsourcing study, then develop a conceptual framework to classify and evaluate these interventions.

**Key Contributions:**

	1. Compilation of interventions from literature and user input
	2. Development of a conceptual framework for categorizing interventions
	3. Provision of a theoretical basis for evaluating intervention effectiveness

**Result:** The paper outlines various types of interventions and provides a theoretical basis to assess their effectiveness in mitigating anthropomorphic behavior.

**Limitations:** 

**Conclusion:** The study highlights the need for empirical and theoretical work to effectively intervene on anthropomorphic outputs in text generation systems.

**Abstract:** As text generation systems' outputs are increasingly anthropomorphic -- perceived as human-like -- scholars have also increasingly raised concerns about how such outputs can lead to harmful outcomes, such as users over-relying or developing emotional dependence on these systems. How to intervene on such system outputs to mitigate anthropomorphic behaviors and their attendant harmful outcomes, however, remains understudied. With this work, we aim to provide empirical and theoretical grounding for developing such interventions. To do so, we compile an inventory of interventions grounded both in prior literature and a crowdsourcing study where participants edited system outputs to make them less human-like. Drawing on this inventory, we also develop a conceptual framework to help characterize the landscape of possible interventions, articulate distinctions between different types of interventions, and provide a theoretical basis for evaluating the effectiveness of different interventions.

</details>


### [156] [Large Language Models Struggle to Describe the Haystack without Human Help: Human-in-the-loop Evaluation of Topic Models](https://arxiv.org/abs/2502.14748)

*Zongxia Li, Lorena Calvo-Bartolomé, Alexander Hoyle, Paiheng Xu, Alden Dima, Juan Francisco Fung, Jordan Boyd-Graber*

**Main category:** cs.CL

**Keywords:** NLP, Large Language Models, topic models, data exploration, human supervision

**Relevance Score:** 8

**TL;DR:** The study compares LLM-based methods and traditional topic models for understanding large document collections, revealing strengths and limitations of each approach.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of Large Language Models (LLMs) versus traditional topic models in understanding large datasets in real-world applications.

**Method:** Evaluation of knowledge acquisition from LLM-based exploratory approaches and traditional topic models on two datasets, measuring readability of topics and average win probabilities.

**Key Contributions:**

	1. Empirical comparison of LLMs and traditional models in data exploration
	2. Insights on the impact of human supervision on LLM outputs
	3. Identification of limitations in LLM performance with domain-specific datasets

**Result:** LLM-based methods yield more human-readable topics and higher win probabilities, but generate overly generic topics for domain-specific data. Human supervision enhances exploration but increases user effort; traditional models remain effective but are less user-friendly.

**Limitations:** LLMs struggle with generative specificity for domain-specific datasets and have scaling and hallucination constraints due to context length.

**Conclusion:** LLMs require human assistance for effective domain-specific data exploration due to limitations in scaling and propensity for hallucination.

**Abstract:** A common use of NLP is to facilitate the understanding of large document collections, with a shift from using traditional topic models to Large Language Models. Yet the effectiveness of using LLM for large corpus understanding in real-world applications remains under-explored. This study measures the knowledge users acquire with unsupervised, supervised LLM-based exploratory approaches or traditional topic models on two datasets. While LLM-based methods generate more human-readable topics and show higher average win probabilities than traditional models for data exploration, they produce overly generic topics for domain-specific datasets that do not easily allow users to learn much about the documents. Adding human supervision to the LLM generation process improves data exploration by mitigating hallucination and over-genericity but requires greater human effort. In contrast, traditional. models like Latent Dirichlet Allocation (LDA) remain effective for exploration but are less user-friendly. We show that LLMs struggle to describe the haystack of large corpora without human help, particularly domain-specific data, and face scaling and hallucination limitations due to context length constraints.

</details>


### [157] [D2S-FLOW: Automated Parameter Extraction from Datasheets for SPICE Model Generation Using Large Language Models](https://arxiv.org/abs/2502.16540)

*Hong Cai Chen, Yi Pin Xu, Yang Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, SPICE models, Automated extraction, Circuit design, Document processing

**Relevance Score:** 4

**TL;DR:** D2S-FLOW is an automated framework using LLMs to extract electrical parameters from datasheets and generate SPICE models, enhancing efficiency and precision.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Manual extraction of component parameters from extensive documents for SPICE models is labor-intensive and time-consuming.

**Method:** D2S-FLOW utilizes three mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN) to improve precision and efficiency in document handling.

**Key Contributions:**

	1. Introduction of D2S-FLOW framework
	2. Three innovative mechanisms for improved document handling
	3. Significant performance improvements over baseline systems

**Result:** The framework achieves an Exact Match of 0.86, F1 score of 0.92, and Exact Correctness of 0.96, outperforming the strongest baseline significantly. It also reduces API token consumption by 38% and minimizes irrelevant information to 4%.

**Limitations:** 

**Conclusion:** D2S-FLOW presents a substantial improvement in automating circuit design, showcasing high efficiency and precision in extracting parameters.

**Abstract:** In electronic design, engineers often manually search through extensive documents to retrieve component parameters required for constructing SPICE models, a process that is both labor-intensive and time-consuming. To address this challenge, we present an automated framework called D2S-FLOW that leverages large language models (LLMs) to extract electrical parameters from datasheets and generate SPICE models with high precision and efficiency, significantly reducing the need for manual intervention. Unlike traditional RAG systems, D2S-FLOW employs a workflow to enhance precision in handling unstructured documents and inconsistent naming conventions through three innovative mechanisms: Attention-Guided Document Focusing (AGDF), Hierarchical Document-Enhanced Retrieval (HDER), and Heterogeneous Named Entity Normalization (HNEN). AGDF narrows retrieval to user-selected documents, HDER utilizes document structure for precise parameter localization, and HNEN standardizes terminology via semantic inference. Experimental results demonstrate that the framework achieves an Exact Match (EM) of 0.86, an F1 score of 0.92, and an Exact Correctness (EC) of 0.96, outperforming the strongest baseline by 19.4%, 5.7%, and 13.1%, respectively. Additionally, it reduces API token consumption by 38% and minimizes the irrelevant information ratio to 4%, showcasing substantial improvements in resource efficiency. This research provides an effective automated solution for circuit design.

</details>


### [158] [Sliding Window Attention Training for Efficient Large Language Models](https://arxiv.org/abs/2502.18845)

*Zichuan Fu, Wentao Song, Yejing Wang, Xian Wu, Yefeng Zheng, Yingying Zhang, Derong Xu, Xuetao Wei, Tong Xu, Xiangyu Zhao*

**Main category:** cs.CL

**Keywords:** Long Language Models, Sliding Window Attention, Transformer Efficiency, ALiBi, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** SWAT is a new model that enhances the efficiency of long-context handling in Transformer-based architectures by replacing softmax with the sigmoid function and employing balanced ALiBi and Rotary Position Embedding.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational inefficiency of Transformers with long sequences due to quadratic complexity and the attention sink phenomenon.

**Method:** SWAT introduces a novel approach by substituting the softmax operation with the sigmoid function, integrating balanced ALiBi and Rotary Position Embedding for effective information compression.

**Key Contributions:**

	1. Introduction of Sliding Window Attention Training (SWAT) for efficiency
	2. Replacement of softmax with sigmoid for improved performance
	3. Utilization of balanced ALiBi and Rotary Position Embedding.

**Result:** Experiments show that SWAT achieves state-of-the-art performance on eight benchmarks, outperforming existing linear recurrent architectures.

**Limitations:** 

**Conclusion:** SWAT represents a simple yet effective modification to the Transformer architecture, maintaining its fundamental structure while improving efficiency for long-context processing.

**Abstract:** Recent advances in transformer-based Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks. However, their quadratic computational complexity concerning sequence length remains a significant bottleneck for processing long documents. As a result, many efforts like sparse attention and state space models have been proposed to improve the efficiency of LLMs over long sequences. Though effective, these approaches compromise the performance or introduce structural complexity. This calls for a simple yet efficient model that preserves the fundamental Transformer architecture. To this end, we introduce SWAT, which enables efficient long-context handling via Sliding Window Attention Training. This paper first attributes the inefficiency of Transformers to the attention sink phenomenon resulting from the high variance of softmax operation. Then, we replace softmax with the sigmoid function and utilize a balanced ALiBi and Rotary Position Embedding for efficient information compression and retention. Experiments demonstrate that SWAT achieves SOTA performance compared with state-of-the-art linear recurrent architectures on eight benchmarks. Code is available at https://github.com/Fzkuji/swat-attention.

</details>


### [159] [Where Are We? Evaluating LLM Performance on African Languages](https://arxiv.org/abs/2502.19582)

*Ife Adebara, Hawau Olamide Toyin, Nahom Tesfu Ghebremichael, AbdelRahim Elmadany, Muhammad Abdul-Mageed*

**Main category:** cs.CL

**Keywords:** large language models, African languages, data equity, NLP, linguistic diversity

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of large language models on African languages using a comprehensive benchmark, highlighting the impact of data inequities on model effectiveness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underrepresentation of Africa's linguistic heritage in NLP due to historical policies favoring foreign languages and data inequities.

**Method:** The paper integrates theoretical insights with an empirical evaluation using Sahara, a benchmark of large-scale publicly accessible datasets that capture Africa's linguistic diversity.

**Key Contributions:**

	1. Empirical evaluation of LLM performance on a comprehensive benchmark for African languages.
	2. Insights into how historical policies affect NLP and data representation for Indigenous languages.
	3. Recommendations for policy reforms to improve data practices in AI for African languages.

**Result:** The assessment reveals that while some languages perform reasonably well, many Indigenous languages are marginalized due to sparse data, with significant variations in model performance linked to policy-induced data disparities.

**Limitations:** Focuses primarily on large language models; may not cover other types of NLP applications.

**Conclusion:** The findings call for actionable policy reforms and inclusive data practices to foster linguistic diversity in AI, emphasizing the need for both theoretical understanding and empirical assessment.

**Abstract:** Africa's rich linguistic heritage remains underrepresented in NLP, largely due to historical policies that favor foreign languages and create significant data inequities. In this paper, we integrate theoretical insights on Africa's language landscape with an empirical evaluation using Sahara - a comprehensive benchmark curated from large-scale, publicly accessible datasets capturing the continent's linguistic diversity. By systematically assessing the performance of leading large language models (LLMs) on Sahara, we demonstrate how policy-induced data variations directly impact model effectiveness across African languages. Our findings reveal that while a few languages perform reasonably well, many Indigenous languages remain marginalized due to sparse data. Leveraging these insights, we offer actionable recommendations for policy reforms and inclusive data practices. Overall, our work underscores the urgent need for a dual approach - combining theoretical understanding with empirical evaluation - to foster linguistic diversity in AI for African communities.

</details>


### [160] [DOVE: A Large-Scale Multi-Dimensional Predictions Dataset Towards Meaningful LLM Evaluation](https://arxiv.org/abs/2503.01622)

*Eliya Habba, Ofir Arviv, Itay Itzhak, Yotam Perlitz, Elron Bandel, Leshem Choshen, Michal Shmueli-Scheuer, Gabriel Stanovsky*

**Main category:** cs.CL

**Keywords:** LLM evaluation, prompt perturbations, dataset, model sensitivity, HCI

**Relevance Score:** 8

**TL;DR:** DOVE is a dataset aimed at evaluating LLM sensitivity to prompt variations, offering insights into effective prompt design and model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues with current single-prompt evaluation practices by examining LLM sensitivity to various prompt perturbations holistically.

**Method:** DOVE includes a large-scale dataset with thousands of prompt perturbations for various evaluation benchmarks, focusing on joint effects of multiple dimensions.

**Key Contributions:**

	1. Introduces the DOVE dataset with over 250M prompt perturbations
	2. Examines LLM sensitivity from a holistic perspective
	3. Identifies effective prompt design strategies and model output characteristics

**Result:** Findings include methods for optimizing prompt selection, the impact of few-shot examples on reducing sensitivity, and the identification of consistently hard instances across perturbations.

**Limitations:** 

**Conclusion:** DOVE provides a resource to improve LLM evaluation methodologies and encourages community involvement in testing and refining prompt performance.

**Abstract:** Recent work found that LLMs are sensitive to a wide range of arbitrary prompt dimensions, including the type of delimiters, answer enumerators, instruction wording, and more. This throws into question popular single-prompt evaluation practices. We present DOVE (Dataset Of Variation Evaluation) a large-scale dataset containing prompt perturbations of various evaluation benchmarks. In contrast to previous work, we examine LLM sensitivity from an holistic perspective, and assess the joint effects of perturbations along various dimensions, resulting in thousands of perturbations per instance. We evaluate several model families against DOVE, leading to several findings, including efficient methods for choosing well-performing prompts, observing that few-shot examples reduce sensitivity, and identifying instances which are inherently hard across all perturbations. DOVE consists of more than 250M prompt perturbations and model outputs, which we make publicly available to spur a community-wide effort toward meaningful, robust, and efficient evaluation.   Browse the data, contribute, and more: https://slab-nlp.github.io/DOVE/

</details>


### [161] [On the Acquisition of Shared Grammatical Representations in Bilingual Language Models](https://arxiv.org/abs/2503.03962)

*Catherine Arnett, Tyler A. Chang, James A. Michaelov, Benjamin K. Bergen*

**Main category:** cs.CL

**Keywords:** crosslingual transfer, structural priming, multilingual models, transfer learning, language similarity

**Relevance Score:** 8

**TL;DR:** The paper investigates the effects of training monolingual models on bilingual data and how this influences crosslingual transfer, particularly through structural priming.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the mechanisms behind crosslingual transfer in multilingual language models is vital for improving their effectiveness and comprehension.

**Method:** Small bilingual models were trained with varying amounts of data for each language and different orders of language exposure. Structural priming methods were used to examine grammatical representations.

**Key Contributions:**

	1. Demonstrates the impact of language exposure order on transfer learning effects.
	2. Highlights asymmetries in structural priming across different language pairs.
	3. Identifies challenges in applying crosslingual transfer to diverse languages.

**Result:** Asymmetrical effects were observed across different language pairs and training conditions, indicating that language similarity influences structural priming outcomes.

**Limitations:** Structural priming effects are less robust for less similar languages, indicating potential limitations in crosslingual representations.

**Conclusion:** The asymmetry in priming effects may provide insights into human linguistic processing and highlights the limitations of crosslingual transfer for languages that are typologically diverse.

**Abstract:** Crosslingual transfer is crucial to contemporary language models' multilingual capabilities, but how it occurs is not well understood. We ask what happens to a monolingual language model when it begins to be trained on a second language. Specifically, we train small bilingual models for which we control the amount of data for each language and the order of language exposure. To find evidence of shared multilingual representations, we turn to structural priming, a method used to study grammatical representations in humans. We first replicate previous crosslingual structural priming results and find that after controlling for training data quantity and language exposure, there are asymmetrical effects across language pairs and directions. We argue that this asymmetry may shape hypotheses about human structural priming effects. We also find that structural priming effects are less robust for less similar language pairs, highlighting potential limitations of crosslingual transfer learning and shared representations for typologically diverse languages.

</details>


### [162] [Full-Duplex-Bench: A Benchmark to Evaluate Full-duplex Spoken Dialogue Models on Turn-taking Capabilities](https://arxiv.org/abs/2503.04721)

*Guan-Ting Lin, Jiachen Lian, Tingle Li, Qirui Wang, Gopala Anumanchipalli, Alexander H. Liu, Hung-yi Lee*

**Main category:** cs.CL

**Keywords:** Spoken Dialogue Models, Benchmark, Evaluation, Full-Duplex, Natural Interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces Full-Duplex-Bench, a benchmark for evaluating full-duplex spoken dialogue models (SDMs) that enables simultaneous listening and speaking, assessing key interactive behaviors beyond traditional metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The field of spoken dialogue modeling faces limitations in evaluation methods, primarily focusing on turn-based interactions. The need for a benchmark that captures the complexities of real-time communication is critical for developing more natural SDMs.

**Method:** The authors developed Full-Duplex-Bench, which systematically evaluates interactive behaviors like pause handling, backchanneling, turn-taking, and interruption management using automatic metrics for reproducibility.

**Key Contributions:**

	1. Introduction of Full-Duplex-Bench for evaluating full-duplex spoken dialogue models.
	2. Focus on interactive behaviors such as pause handling and backchanneling.
	3. Provision of a reproducible and fast evaluation framework.

**Result:** The benchmark provides a rapid and fair evaluation setup for full-duplex SDMs, aiming to facilitate advancements in spoken dialogue systems by presenting clear assessment criteria.

**Limitations:** Current evaluations are still limited to automatic metrics and may not fully capture human conversational nuances.

**Conclusion:** By releasing Full-Duplex-Bench and its accompanying code, the authors hope to promote more effective development and evaluation of spoken dialogue models, contributing to more engaging conversational agents.

**Abstract:** Spoken dialogue modeling poses challenges beyond text-based language modeling, requiring real-time interaction, turn-taking, and backchanneling. While most Spoken Dialogue Models (SDMs) operate in half-duplex mode-processing one turn at a time - emerging full-duplex SDMs can listen and speak simultaneously, enabling more natural conversations. However, current evaluations remain limited, focusing mainly on turn-based metrics or coarse corpus-level analyses. To address this, we introduce Full-Duplex-Bench, a benchmark that systematically evaluates key interactive behaviors: pause handling, backchanneling, turn-taking, and interruption management. Our framework uses automatic metrics for consistent, reproducible assessment and provides a fair, fast evaluation setup. By releasing our benchmark and code, we aim to advance spoken dialogue modeling and foster the development of more natural and engaging SDMs.

</details>


### [163] [PFDial: A Structured Dialogue Instruction Fine-tuning Method Based on UML Flowcharts](https://arxiv.org/abs/2503.06706)

*Ming Zhang, Yuhui Wang, Yujiong Shen, Tingyi Yang, Changhao Jiang, Yilong Wu, Shihan Dou, Qinhao Chen, Zhiheng Xi, Zhihao Zhang, Yi Dong, Zhen Wang, Zhihui Fei, Mingyang Wan, Tao Liang, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** process-driven dialogue, large language models, dataset, customer service, UML flowcharts

**Relevance Score:** 7

**TL;DR:** The study presents the PFDial dataset for process-driven dialogue systems and evaluates LLMs in constrained dialogue tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve dialogue systems in customer service and equipment maintenance by providing a dataset with predefined process constraints.

**Method:** Construction of the PFDial dataset from UML flowcharts, followed by training various models to evaluate their performance on dialogue instructions.

**Key Contributions:**

	1. Introduction of the PFDial dataset
	2. High accuracy achieved by smaller LLMs on constrained dialogue tasks
	3. In-depth analysis of dataset formats and their impact on model performance

**Result:** Models, including a 7B and an 8B model, demonstrated over 90% accuracy on dialogue tasks, outperforming GPT-4o.

**Limitations:** 

**Conclusion:** The findings highlight the effectiveness of LLMs in constrained dialogue tasks and the importance of dataset formats on model performance.

**Abstract:** Process-driven dialogue systems, which operate under strict predefined process constraints, are essential in customer service and equipment maintenance scenarios. Although Large Language Models (LLMs) have shown remarkable progress in dialogue and reasoning, they still struggle to solve these strictly constrained dialogue tasks. To address this challenge, we construct Process Flow Dialogue (PFDial) dataset, which contains 12,705 high-quality Chinese dialogue instructions derived from 440 flowcharts containing 5,055 process nodes. Based on PlantUML specification, each UML flowchart is converted into atomic dialogue units i.e., structured five-tuples. Experimental results demonstrate that a 7B model trained with merely 800 samples, and a 0.5B model trained on total data both can surpass 90% accuracy. Additionally, the 8B model can surpass GPT-4o up to 43.88% with an average of 11.00%. We further evaluate models' performance on challenging backward transitions in process flows and conduct an in-depth analysis of various dataset formats to reveal their impact on model performance in handling decision and sequential branches. The data is released in https://github.com/KongLongGeFDU/PFDial.

</details>


### [164] [LSC-Eval: A General Framework to Evaluate Methods for Assessing Dimensions of Lexical Semantic Change Using LLM-Generated Synthetic Data](https://arxiv.org/abs/2503.08042)

*Naomi Baes, Raphaël Merx, Nick Haslam, Ekaterina Vylomova, Haim Dubossarsky*

**Main category:** cs.CL

**Keywords:** Lexical Semantic Change, Evaluation Framework, Synthetic Datasets, In-Context Learning, Social Sciences

**Relevance Score:** 4

**TL;DR:** The paper introduces LSC-Eval, a framework to evaluate methods for measuring Lexical Semantic Change (LSC) using synthetic datasets.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of historical benchmark datasets for measuring different kinds of Lexical Semantic Change, which are essential for evaluating computational methods in social sciences.

**Method:** A three-stage framework is developed: generating synthetic datasets simulating LSC, evaluating computational methods' sensitivity to changes, and assessing the methods' suitability for specific domains.

**Key Contributions:**

	1. Development of LSC-Eval framework for evaluating LSC methods.
	2. Created synthetic datasets for simulating LSC changes.
	3. Revealed limitations of existing LSC models in detecting affective changes.

**Result:** The study validated the use of synthetic benchmarks and demonstrated that tailored methods are effective in detecting changes along defined dimensions, while revealing challenges faced by state-of-the-art models in identifying affective dimensions of LSC.

**Limitations:** The paper may have limitations in generalizability beyond the tested dimensions and domains.

**Conclusion:** LSC-Eval provides a valuable tool for dimension- and domain-specific benchmarking of LSC methods, particularly in social sciences.

**Abstract:** Lexical Semantic Change (LSC) provides insight into cultural and social dynamics. Yet, the validity of methods for measuring different kinds of LSC remains unestablished due to the absence of historical benchmark datasets. To address this gap, we propose LSC-Eval, a novel three-stage general-purpose evaluation framework to: (1) develop a scalable methodology for generating synthetic datasets that simulate theory-driven LSC using In-Context Learning and a lexical database; (2) use these datasets to evaluate the sensitivity of computational methods to synthetic change; and (3) assess their suitability for detecting change in specific dimensions and domains. We apply LSC-Eval to simulate changes along the Sentiment, Intensity, and Breadth (SIB) dimensions, as defined in the SIBling framework, using examples from psychology. We then evaluate the ability of selected methods to detect these controlled interventions. Our findings validate the use of synthetic benchmarks, demonstrate that tailored methods effectively detect changes along SIB dimensions, and reveal that a state-of-the-art LSC model faces challenges in detecting affective dimensions of LSC. LSC-Eval offers a valuable tool for dimension- and domain-specific benchmarking of LSC methods, with particular relevance to the social sciences.

</details>


### [165] [An Expanded Massive Multilingual Dataset for High-Performance Language Technologies (HPLT)](https://arxiv.org/abs/2503.10267)

*Laurie Burchell, Ona de Gibert, Nikolay Arefyev, Mikko Aulamo, Marta Bañón, Pinzhen Chen, Mariia Fedorova, Liane Guillou, Barry Haddow, Jan Hajič, Jindřich Helcl, Erik Henriksson, Mateusz Klimaszewski, Ville Komulainen, Andrey Kutuzov, Joona Kytöniemi, Veronika Laippala, Petter Mæhlum, Bhavitvya Malik, Farrokh Mehryary, Vladislav Mikhailov, Nikita Moghe, Amanda Myntti, Dayyán O'Brien, Stephan Oepen, Proyag Pal, Jousia Piha, Sampo Pyysalo, Gema Ramírez-Sánchez, David Samuel, Pavel Stepachev, Jörg Tiedemann, Dušan Variš, Tereza Vojtěchová, Jaume Zaragoza-Bernabeu*

**Main category:** cs.CL

**Keywords:** multilingual datasets, large language models, machine translation

**Relevance Score:** 8

**TL;DR:** HPLT v2 presents a rich multilingual dataset for training large language models, with extensive documentation and evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Building suitable multilingual datasets for training large language models is a significant challenge.

**Method:** We introduce HPLT v2, a collection of high-quality multilingual datasets including monolingual and parallel corpora, and document the entire data pipeline.

**Key Contributions:**

	1. Introduction of a large-scale multilingual dataset (HPLT v2) for training language models.
	2. Documented data pipeline for reproducibility.
	3. Extensive performance evaluation of language models and machine translation systems using HPLT v2.

**Result:** HPLT v2 includes 8T tokens in 193 languages and 380M sentence pairs in 51 languages, with a thorough analysis of data quality and model performance evaluations.

**Limitations:** 

**Conclusion:** The evaluation demonstrates the superiority of models trained on HPLT v2 for both natural language processing and machine translation tasks.

**Abstract:** Training state-of-the-art large language models requires vast amounts of clean and diverse textual data. However, building suitable multilingual datasets remains a challenge. In this work, we present HPLT v2, a collection of high-quality multilingual monolingual and parallel corpora, extending prior work of the HPLT project. The monolingual portion of the data contains 8T tokens covering 193 languages, while the parallel data contains 380M sentence pairs covering 51 languages. We document the entire data pipeline and release the code to reproduce it. We provide extensive analysis of the quality and characteristics of our data. Finally, we evaluate the performance of language models and machine translation systems trained on HPLT v2, demonstrating its value.

</details>


### [166] [Probing LLMs for Multilingual Discourse Generalization Through a Unified Label Set](https://arxiv.org/abs/2503.10515)

*Florian Eichin, Yang Janet Liu, Barbara Plank, Michael A. Hedderich*

**Main category:** cs.CL

**Keywords:** discourse understanding, large language models, multilingual NLP

**Relevance Score:** 8

**TL;DR:** Investigation of LLMs' ability to capture and generalize discourse knowledge across languages and frameworks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding discourse is vital for NLP tasks, yet current discourse representations are framework-dependent, limiting their applicability.

**Method:** Developed a unified discourse relation label set and probed 23 LLMs of various sizes to evaluate their discourse knowledge generalization.

**Key Contributions:**

	1. Unified discourse relation label set for analysis
	2. Evaluation of 23 LLMs' discourse knowledge
	3. Layer-wise analysis revealing generalization in intermediate layers

**Result:** LLMs, particularly those trained on multilingual data, show capability to generalize discourse relations across languages; significant insights from layer-wise analyses indicate that this generalization is most prominent in intermediate layers.

**Limitations:** The research may face limitations regarding the diversity of discourse relations and the evaluation framework used.

**Conclusion:** The study confirms LLMs possess generalizable discourse knowledge, aiding in cross-lingual and cross-framework discourse tasks; comprehensive error analysis suggests specific challenging areas.

**Abstract:** Discourse understanding is essential for many NLP tasks, yet most existing work remains constrained by framework-dependent discourse representations. This work investigates whether large language models (LLMs) capture discourse knowledge that generalizes across languages and frameworks. We address this question along two dimensions: (1) developing a unified discourse relation label set to facilitate cross-lingual and cross-framework discourse analysis, and (2) probing LLMs to assess whether they encode generalizable discourse abstractions. Using multilingual discourse relation classification as a testbed, we examine a comprehensive set of 23 LLMs of varying sizes and multilingual capabilities. Our results show that LLMs, especially those with multilingual training corpora, can generalize discourse information across languages and frameworks. Further layer-wise analyses reveal that language generalization at the discourse level is most salient in the intermediate layers. Lastly, our error analysis provides an account of challenging relation classes.

</details>


### [167] [Uncertainty Quantification and Confidence Calibration in Large Language Models: A Survey](https://arxiv.org/abs/2503.15850)

*Xiaoou Liu, Tiejin Chen, Longchao Da, Chacha Chen, Zhen Lin, Hua Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, Healthcare, Reliability, AI

**Relevance Score:** 9

**TL;DR:** This paper addresses the reliability concerns of Large Language Models (LLMs) in high-stakes fields by exploring uncertainty quantification (UQ) methods.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The reliability of LLMs is critical as they are increasingly adopted in sensitive areas like healthcare and law, where incorrect outputs can have significant consequences. Uncertainty quantification can help enhance trustworthiness by estimating confidence in the generated outputs.

**Method:** The paper introduces a new taxonomy for uncertainty quantification methods based on computational efficiency and various uncertainty dimensions, including input, reasoning, parameter, and prediction uncertainty. It also evaluates existing techniques and their applicability in real-world situations.

**Key Contributions:**

	1. Introduction of a new taxonomy for UQ methods specific to LLMs
	2. Evaluation of existing UQ techniques in the context of LLM reliability
	3. Identification of open challenges and the need for scalable and interpretable UQ approaches.

**Result:** The study highlights unique uncertainty sources introduced by LLMs, which traditional UQ methods struggle to address, identifying a need for more scalable and interpretable approaches.

**Limitations:** 

**Conclusion:** A robust approach to uncertainty quantification is necessary to improve the reliability of LLMs, suggesting that advancements in UQ could facilitate more responsible use of these models in critical applications.

**Abstract:** Large Language Models (LLMs) excel in text generation, reasoning, and decision-making, enabling their adoption in high-stakes domains such as healthcare, law, and transportation. However, their reliability is a major concern, as they often produce plausible but incorrect responses. Uncertainty quantification (UQ) enhances trustworthiness by estimating confidence in outputs, enabling risk mitigation and selective prediction. However, traditional UQ methods struggle with LLMs due to computational constraints and decoding inconsistencies. Moreover, LLMs introduce unique uncertainty sources, such as input ambiguity, reasoning path divergence, and decoding stochasticity, that extend beyond classical aleatoric and epistemic uncertainty. To address this, we introduce a new taxonomy that categorizes UQ methods based on computational efficiency and uncertainty dimensions (input, reasoning, parameter, and prediction uncertainty). We evaluate existing techniques, assess their real-world applicability, and identify open challenges, emphasizing the need for scalable, interpretable, and robust UQ approaches to enhance LLM reliability.

</details>


### [168] [SCORE: Story Coherence and Retrieval Enhancement for AI Narratives](https://arxiv.org/abs/2503.23512)

*Qiang Yi, Yangfan He, Jianhui Wang, Xinyuan Song, Shiyao Qian, Xinhang Yuan, Li Sun, Yi Xin, Keqin Li, Kuan Lu, Menghao Huo, Jiaqi Chen, Tianyu Shi*

**Main category:** cs.CL

**Keywords:** Large Language Models, narrative coherence, Retrieval-Augmented Generation, AI-generated stories, story structure

**Relevance Score:** 9

**TL;DR:** Introducing SCORE, a framework for enhancing narrative coherence in AI-generated stories.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of maintaining coherence and emotional depth in narratives generated by large language models.

**Method:** SCORE employs a Retrieval-Augmented Generation (RAG) approach that utilizes TF-IDF and cosine similarity to detect narrative inconsistencies and enhance story structure by tracking item statuses and generating episode summaries.

**Key Contributions:**

	1. Development of the SCORE framework for narrative coherence
	2. Application of Retrieval-Augmented Generation techniques
	3. Demonstrated improvement over baseline models in narrative consistency

**Result:** SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models in LLM-generated stories.

**Limitations:** 

**Conclusion:** The framework offers a more robust method for evaluating and refining AI-generated narratives.

**Abstract:** Large Language Models (LLMs) can generate creative and engaging narratives from user-specified input, but maintaining coherence and emotional depth throughout these AI-generated stories remains a challenge. In this work, we propose SCORE, a framework for Story Coherence and Retrieval Enhancement, designed to detect and resolve narrative inconsistencies. By tracking key item statuses and generating episode summaries, SCORE uses a Retrieval-Augmented Generation (RAG) approach, incorporating TF-IDF and cosine similarity to identify related episodes and enhance the overall story structure. Results from testing multiple LLM-generated stories demonstrate that SCORE significantly improves the consistency and stability of narrative coherence compared to baseline GPT models, providing a more robust method for evaluating and refining AI-generated narratives.

</details>


### [169] [Rubrik's Cube: Testing a New Rubric for Evaluating Explanations on the CUBE dataset](https://arxiv.org/abs/2503.23899)

*Diana Galvan-Sosa, Gabrielle Gaudeau, Pride Kavumba, Yunmeng Li, Hongyi gu, Zheng Yuan, Keisuke Sakaguchi, Paula Buttery*

**Main category:** cs.CL

**Keywords:** Large-Language Models, explanation generation, evaluation rubric, dataset, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper presents Rubrik's CUBE, a rubric and dataset for evaluating the quality of explanations generated by Large-Language Models (LLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the adoption of LLMs in generating explanations, their reliability remains questionable, leading to the need for a robust evaluation framework.

**Method:** The authors introduce Rubrik's CUBE, an education-inspired rubric, and a dataset of 26,000 explanations, quality-annotated by humans and LLMs focusing on various reasoning and language tasks.

**Key Contributions:**

	1. Introduction of Rubrik's CUBE for evaluating LLM explanations
	2. Creation of a large dataset of annotated explanations
	3. Insights into factors affecting the quality of LLM explanations

**Result:** Analysis reveals that low-quality explanations are primarily due to a lack of conciseness, influenced by the task and perceived difficulty rather than cohesion or word choice.

**Limitations:** The dataset may not cover all possible explanation tasks or scenarios.

**Conclusion:** The findings highlight the need for improved evaluation of LLM-generated explanations; the dataset and resources are made available to the community.

**Abstract:** The performance and usability of Large-Language Models (LLMs) are driving their use in explanation generation tasks. However, despite their widespread adoption, LLM explanations have been found to be unreliable, making it difficult for users to distinguish good from bad explanations. To address this issue, we present Rubrik's CUBE, an education-inspired rubric and a dataset of 26k explanations, written and later quality-annotated using the rubric by both humans and six open- and closed-source LLMs. The CUBE dataset focuses on two reasoning and two language tasks, providing the necessary diversity for us to effectively test our proposed rubric. Using Rubrik, we find that explanations are influenced by both task and perceived difficulty. Low quality stems primarily from a lack of conciseness in LLM-generated explanations, rather than cohesion and word choice. The full dataset, rubric, and code are available at https://github.com/RubriksCube/rubriks_cube.

</details>


### [170] [Token-Driven GammaTune: Adaptive Calibration for Enhanced Speculative Decoding](https://arxiv.org/abs/2504.00030)

*Aayush Gautam, Susav Shrestha, Narasimha Reddy*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, adaptive algorithms, inference optimization, GammaTune

**Relevance Score:** 8

**TL;DR:** GammaTune and GammaTune+ are adaptive algorithms for optimizing speculation length in speculative decoding for LLM inference, achieving a significant speedup.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of LLM inference through optimal speculation length while reducing computation waste.

**Method:** GammaTune and GammaTune+ utilize a training-free adaptive approach that adjusts speculation length based on token acceptance rates with a heuristic-based switching mechanism.

**Key Contributions:**

	1. Introduction of training-free algorithms for adaptive speculation length adjustment
	2. Demonstrated significant speedup in LLM inference
	3. Reduction in performance variance compared to fixed-length approaches

**Result:** The proposed methods, evaluated on SpecBench, achieve an average speedup of 15% (±5%) for GammaTune and 16% (±3%) for GammaTune+, outperforming existing methods and reducing performance variance.

**Limitations:** 

**Conclusion:** GammaTune provides a robust and efficient solution for real-world deployment in LLM inference environments.

**Abstract:** Speculative decoding accelerates large language model (LLM) inference by using a smaller draft model to propose tokens, which are then verified by a larger target model. However, selecting an optimal speculation length is critical for maximizing speedup while minimizing wasted computation. We introduce \textit{GammaTune} and \textit{GammaTune+}, training-free adaptive algorithms that dynamically adjust speculation length based on token acceptance rates using a heuristic-based switching mechanism. Evaluated on SpecBench across multiple tasks and model pairs, our method outperforms other heuristic-based approaches and fixed-length speculative decoding, achieving an average speedup of 15\% ($\pm$5\%) with \textit{GammaTune} and 16\% ($\pm$3\%) with \textit{GammaTune+}, while reducing performance variance. This makes \textit{GammaTune} a robust and efficient solution for real-world deployment.

</details>


### [171] [CARE: Assessing the Impact of Multilingual Human Preference Learning on Cultural Awareness](https://arxiv.org/abs/2504.05154)

*Geyang Guo, Tarek Naous, Hiromi Wakaki, Yukiko Nishimura, Yuki Mitsufuji, Alan Ritter, Wei Xu*

**Main category:** cs.CL

**Keywords:** Cultural Awareness, Language Models, Human Preferences, Multilingual, CARE

**Relevance Score:** 9

**TL;DR:** This paper introduces CARE, a resource for training culturally aware language models (LMs) using native human cultural preferences.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language models' handling of culturally diverse queries and incorporate native human cultural preferences in preference tuning.

**Method:** The paper presents CARE, a multilingual resource with culturally specific questions and responses, and analyzes its impact on training culturally aware LMs.

**Key Contributions:**

	1. Introduction of CARE, a multilingual resource for culturally specific query training
	2. Demonstration of improved cultural awareness in LMs through native preference incorporation
	3. Analysis of the disparity in cultural performance among models from different regions

**Result:** Incorporating native preferences from CARE significantly enhances cultural awareness in various LMs, outperforming larger generic data sets.

**Limitations:** The study does not explore the long-term effects of using culturally aware models in real-world applications.

**Conclusion:** Models with better initial cultural performance show more significant improvement from using native cultural data; this research highlights regional disparities in access to culturally relevant data.

**Abstract:** Language Models (LMs) are typically tuned with human preferences to produce helpful responses, but the impact of preference tuning on the ability to handle culturally diverse queries remains understudied. In this paper, we systematically analyze how native human cultural preferences can be incorporated into the preference learning process to train more culturally aware LMs. We introduce \textbf{CARE}, a multilingual resource containing 3,490 culturally specific questions and 31.7k responses with native judgments. We demonstrate how a modest amount of high-quality native preferences improves cultural awareness across various LMs, outperforming larger generic preference data. Our analyses reveal that models with stronger initial cultural performance benefit more from alignment, leading to gaps among models developed in different regions with varying access to culturally relevant data. CARE will be made publicly available at https://github.com/Guochry/CARE.

</details>


### [172] [Enhancing LLM-Based Short Answer Grading with Retrieval-Augmented Generation](https://arxiv.org/abs/2504.05276)

*Yucheng Chu, Peng He, Hang Li, Haoyu Han, Kaiqi Yang, Yu Xue, Tingting Li, Joseph Krajcik, Jiliang Tang*

**Main category:** cs.CL

**Keywords:** RAG, automated grading, large language models, science education, domain-specific knowledge

**Relevance Score:** 8

**TL;DR:** This paper presents an adaptive retrieval-augmented generation (RAG) framework for automated grading in science education, improving grading accuracy by incorporating domain-specific knowledge.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the automated grading process in science education by addressing the limitations of large language models (LLMs) in domain knowledge and task-specific understanding.

**Method:** The proposed adaptive RAG framework dynamically retrieves domain-specific knowledge during assessment by combining semantic search and curated educational sources based on the context of questions and student answers.

**Key Contributions:**

	1. Development of an adaptive RAG framework for automated grading
	2. Demonstration of improved grading accuracy with domain-specific knowledge retrieval
	3. Integration of semantic search with curated educational resources

**Result:** Experimental results indicate that the RAG framework significantly enhances grading accuracy compared to baseline LLM approaches in a science education dataset.

**Limitations:** 

**Conclusion:** The RAG-enhanced grading system shows potential as a reliable tool to support human graders with improved performance in evaluating student responses.

**Abstract:** Short answer assessment is a vital component of science education, allowing evaluation of students' complex three-dimensional understanding. Large language models (LLMs) that possess human-like ability in linguistic tasks are increasingly popular in assisting human graders to reduce their workload. However, LLMs' limitations in domain knowledge restrict their understanding in task-specific requirements and hinder their ability to achieve satisfactory performance. Retrieval-augmented generation (RAG) emerges as a promising solution by enabling LLMs to access relevant domain-specific knowledge during assessment. In this work, we propose an adaptive RAG framework for automated grading that dynamically retrieves and incorporates domain-specific knowledge based on the question and student answer context. Our approach combines semantic search and curated educational sources to retrieve valuable reference materials. Experimental results in a science education dataset demonstrate that our system achieves an improvement in grading accuracy compared to baseline LLM approaches. The findings suggest that RAG-enhanced grading systems can serve as reliable support with efficient performance gains.

</details>


### [173] [Identifying Aspects in Peer Reviews](https://arxiv.org/abs/2504.06910)

*Sheng Lu, Ilia Kuznetsov, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** peer review, aspect identification, data-driven schema, NLP, machine learning

**Relevance Score:** 8

**TL;DR:** This paper proposes a data-driven approach to identify aspects in peer reviews to improve the peer review process and support computational methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of academic submissions is straining the peer review process, necessitating computational support and standardization.

**Method:** The authors define a conceptual framework for 'aspect' in peer reviews and develop a data-driven schema to extract these aspects from a corpus of peer reviews.

**Key Contributions:**

	1. Proposes an operational definition of aspect in peer reviews
	2. Develops a data-driven schema for aspect identification from peer reviews
	3. Introduces an augmented dataset for community-level analysis of peer reviews

**Result:** They introduce an augmented dataset of peer reviews with identified aspects and demonstrate the implications for community-level review analysis and LLM-generated review detection.

**Limitations:** The practicality of the approach may depend on the diversity and quality of the data used for aspect extraction.

**Conclusion:** The findings establish a foundation for using NLP to enhance peer review methodologies and applications.

**Abstract:** Peer review is central to academic publishing, but the growing volume of submissions is straining the process. This motivates the development of computational approaches to support peer review. While each review is tailored to a specific paper, reviewers often make assessments according to certain aspects such as Novelty, which reflect the values of the research community. This alignment creates opportunities for standardizing the reviewing process, improving quality control, and enabling computational support. While prior work has demonstrated the potential of aspect analysis for peer review assistance, the notion of aspect remains poorly formalized. Existing approaches often derive aspects from review forms and guidelines, yet data-driven methods for aspect identification are underexplored. To address this gap, our work takes a bottom-up approach: we propose an operational definition of aspect and develop a data-driven schema for deriving aspects from a corpus of peer reviews. We introduce a dataset of peer reviews augmented with aspects and show how it can be used for community-level review analysis. We further show how the choice of aspects can impact downstream applications, such as LLM-generated review detection. Our results lay a foundation for a principled and data-driven investigation of review aspects, and pave the path for new applications of NLP to support peer review.

</details>


### [174] [Revisiting Uncertainty Quantification Evaluation in Language Models: Spurious Interactions with Response Length Bias Results](https://arxiv.org/abs/2504.13677)

*Andrea Santilli, Adam Golinski, Michael Kirchhof, Federico Danieli, Arno Blaas, Miao Xiong, Luca Zappella, Sinead Williamson*

**Main category:** cs.CL

**Keywords:** Uncertainty Quantification, Language Models, Evaluation Metrics, Biases, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of mutual biases on the evaluation of uncertainty quantification methods in language models, concluding that length biases distort assessments and proposing LM-as-a-judge methods as more reliable.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance safety and reliability in language models through better uncertainty quantification evaluations.

**Method:** Analyzes the biases in UQ methods and correctness functions using theoretical proofs and empirical tests across multiple datasets, models, and evaluation metrics.

**Key Contributions:**

	1. Proof of biases affecting AUROC rankings in UQ evaluations
	2. Empirical validation with multiple metrics and datasets
	3. Identification of LM-as-a-judge methods as a promising evaluation approach

**Result:** Demonstrated that mutual biases skew AUROC rankings non-randomly, compromising benchmark integrity and highlighting the distortion caused by length biases during evaluation.

**Limitations:** 

**Conclusion:** LM-as-a-judge methods show the least length bias, potentially leading to fairer evaluations of uncertainty quantification methods in language models.

**Abstract:** Uncertainty Quantification (UQ) in Language Models (LMs) is key to improving their safety and reliability. Evaluations often use metrics like AUROC to assess how well UQ methods (e.g., negative sequence probabilities) correlate with task correctness functions (e.g., ROUGE-L). We show that mutual biases--when both UQ methods and correctness functions are biased by the same factors--systematically distort evaluation. First, we formally prove that any mutual bias non-randomly skews AUROC rankings, compromising benchmark integrity. Second, we confirm this happens empirically by testing 7 widely used correctness functions, from lexical-based and embedding-based metrics to LM-as-a-judge approaches, across 4 datasets x 4 models x 8 UQ methods. Our analysis shows that length biases in correctness functions distort UQ assessments by interacting with length biases in UQ methods. We identify LM-as-a-judge methods as the least length-biased, offering a promising path for a fairer UQ evaluation.

</details>


### [175] [Hypothetical Documents or Knowledge Leakage? Rethinking LLM-based Query Expansion](https://arxiv.org/abs/2504.14175)

*Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park*

**Main category:** cs.CL

**Keywords:** query expansion, large language models, fact verification, knowledge leakage, retrieval performance

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of knowledge leakage on the performance of LLM-powered query expansion methods in fact verification tasks, suggesting that apparent improvements may be due to previously known information rather than genuine enhancement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that LLMs can generate useful hypothetical documents for query expansion in retrieval tasks without knowledge leakage affecting benchmark results.

**Method:** Analyzed the performance of LLM-generated documents in a zero-shot retrieval context, focusing on fact verification tasks and assessing the entailed information against ground-truth evidence.

**Key Contributions:**

	1. Investigation of knowledge leakage in LLM query expansion methods
	2. Analysis using fact verification as a testbed
	3. Insights into the integrity of fact verification benchmarks

**Result:** Performance improvements were consistently associated with claims whose LLM-generated documents contained sentences that were entailed by actual evidence, indicating potential knowledge leakage in benchmarks.

**Limitations:** The study primarily focuses on a specific testbed (fact verification) and may not generalize across all retrieval tasks.

**Conclusion:** The findings imply that knowledge leakage may inflate the perceived effectiveness of query expansion methods utilizing LLMs in retrieval tasks, raising questions about benchmark integrity.

**Abstract:** Query expansion methods powered by large language models (LLMs) have demonstrated effectiveness in zero-shot retrieval tasks. These methods assume that LLMs can generate hypothetical documents that, when incorporated into a query vector, enhance the retrieval of real evidence. However, we challenge this assumption by investigating whether knowledge leakage in benchmarks contributes to the observed performance gains. Using fact verification as a testbed, we analyze whether the generated documents contain information entailed by ground-truth evidence and assess their impact on performance. Our findings indicate that, on average, performance improvements consistently occurred for claims whose generated documents included sentences entailed by gold evidence. This suggests that knowledge leakage may be present in fact-verification benchmarks, potentially inflating the perceived performance of LLM-based query expansion methods.

</details>


### [176] [The Arabic AI Fingerprint: Stylometric Analysis and Detection of Large Language Models Text](https://arxiv.org/abs/2505.23276)

*Maged S. Al-Shaibani, Moataz Ahmed*

**Main category:** cs.CL

**Keywords:** Machine Learning, Natural Language Processing, Arabic Language Models

**Relevance Score:** 9

**TL;DR:** This paper investigates Arabic machine-generated text, revealing detectable patterns in LLM outputs across various contexts and developing effective BERT-based detection models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the significant challenges posed by misinformation generated by LLMs in low-resource languages like Arabic, particularly in critical domains such as healthcare and education.

**Method:** A comprehensive analysis of Arabic machine-generated text was conducted, exploring multiple generation strategies and model architectures, followed by stylometric analysis to differentiate human and machine outputs, culminating in the development of BERT-based detection models.

**Key Contributions:**

	1. Comprehensive analysis of Arabic machine-generated text using various generation strategies and model architectures.
	2. Stylometric analysis revealing unique patterns between human-written and machine-generated text.
	3. Development of high-performance BERT-based detection models for Arabic text.

**Result:** The analysis showed distinctive linguistic patterns in machine-generated Arabic text, and the BERT-based models achieved up to 99.9% F1-score in detecting formal contexts, despite generalization challenges.

**Limitations:** Generalization challenges in detection across different contexts were noted, which requires future attention.

**Conclusion:** This work is the most comprehensive investigation of Arabic machine-generated text, providing insights that are essential for building robust detection systems to preserve information integrity.

**Abstract:** Large Language Models (LLMs) have achieved unprecedented capabilities in generating human-like text, posing subtle yet significant challenges for information integrity across critical domains, including education, social media, and academia, enabling sophisticated misinformation campaigns, compromising healthcare guidance, and facilitating targeted propaganda. This challenge becomes severe, particularly in under-explored and low-resource languages like Arabic. This paper presents a comprehensive investigation of Arabic machine-generated text, examining multiple generation strategies (generation from the title only, content-aware generation, and text refinement) across diverse model architectures (ALLaM, Jais, Llama, and GPT-4) in academic, and social media domains. Our stylometric analysis reveals distinctive linguistic patterns differentiating human-written from machine-generated Arabic text across these varied contexts. Despite their human-like qualities, we demonstrate that LLMs produce detectable signatures in their Arabic outputs, with domain-specific characteristics that vary significantly between different contexts. Based on these insights, we developed BERT-based detection models that achieved exceptional performance in formal contexts (up to 99.9\% F1-score) with strong precision across model architectures. Our cross-domain analysis confirms generalization challenges previously reported in the literature. To the best of our knowledge, this work represents the most comprehensive investigation of Arabic machine-generated text to date, uniquely combining multiple prompt generation methods, diverse model architectures, and in-depth stylometric analysis across varied textual domains, establishing a foundation for developing robust, linguistically-informed detection systems essential for preserving information integrity in Arabic-language contexts.

</details>
