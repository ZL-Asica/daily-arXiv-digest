# 2025-05-14

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 86]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Measuring and predicting variation in the difficulty of questions about data visualizations](https://arxiv.org/abs/2505.08031)

*Arnav Verma, Judith E. Fan*

**Main category:** cs.HC

**Keywords:** data visualization, literacy, difficulty assessment, task performance, scientific literacy

**Relevance Score:** 4

**TL;DR:** Study on data visualization literacy reveals varying task difficulties.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand why certain tasks involving data visualizations are more challenging than others, highlighting the importance of communicating data effectively.

**Method:** Administered a composite test of five tests of data visualization literacy to 503 U.S. adults to assess performance variations based on visualization types and task types.

**Key Contributions:**

	1. Establishment of a reliable composite test for data visualization literacy.
	2. Identification of the modest impact of visualization and task type on performance variation.
	3. Recommendation for finer-grained characterization of difficulty factors.

**Result:** The composite test demonstrated a full range of difficulties, with high reliability in estimating item-level difficulty, though visualization type and task type explained only modest performance variation.

**Limitations:** The study primarily focuses on U.S. adults, which may limit generalizability to other populations.

**Conclusion:** Findings indicate a need for more detailed methods to characterize factors predicting variations in data visualization task difficulties.

**Abstract:** Understanding what is communicated by data visualizations is a critical component of scientific literacy in the modern era. However, it remains unclear why some tasks involving data visualizations are more difficult than others. Here we administered a composite test composed of five widely used tests of data visualization literacy to a large sample of U.S. adults (N=503 participants).We found that items in the composite test spanned the full range of possible difficulty levels, and that our estimates of item-level difficulty were highly reliable. However, the type of data visualization shown and the type of task involved only explained a modest amount of variation in performance across items, relative to the reliability of the estimates we obtained. These results highlight the need for finer-grained ways of characterizing these items that predict the reliable variation in difficulty measured in this study, and that generalize to other tests of data visualization understanding.

</details>


### [2] [Partisan Fact-Checkers' Warnings Can Effectively Correct Individuals' Misbeliefs About Political Misinformation](https://arxiv.org/abs/2505.08048)

*Sian Lee, Haeseung Seo, Aiping Xiong, Dongwon Lee*

**Main category:** cs.HC

**Keywords:** political misinformation, fact-checking, social media, partisan bias, human-subject experiment

**Relevance Score:** 4

**TL;DR:** This study examines how the political biases of fact-checkers impact their effectiveness in correcting political misinformation, particularly on social media.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of fact-checking messages amid concerns about perceived political bias affecting user responses to misinformation.

**Method:** An online human-subject experiment was conducted involving 216 participants to analyze the influence of the political stances of fact-checkers on correcting misbeliefs regarding political misinformation.

**Key Contributions:**

	1. Investigates the role of political bias in fact-checking efficacy.
	2. Demonstrates effectiveness of partisan fact-checkers for conservatives.
	3. Challenges prior assumptions about the ineffectiveness of fact-checking for conservative audiences.

**Result:** The study finds that partisan fact-checkers effectively decrease perceived accuracy of misinformation and correct misbeliefs, particularly when misinformation aligns with individuals' political ideologies.

**Limitations:** The study is limited by its sample size and may not fully capture diverse demographic factors influencing responses to misinformation.

**Conclusion:** Explicitly labeled partisan fact-checkers are notably effective in reducing misbeliefs among conservatives regarding pro-liberal misinformation, countering previous assumptions about fact-checking efficacy across political lines.

**Abstract:** Political misinformation, particularly harmful when it aligns with individuals' preexisting beliefs and political ideologies, has become widespread on social media platforms. In response, platforms like Facebook and X introduced warning messages leveraging fact-checking results from third-party fact-checkers to alert users against false content. However, concerns persist about the effectiveness of these fact-checks, especially when fact-checkers are perceived as politically biased. To address these concerns, this study presents findings from an online human-subject experiment (N=216) investigating how the political stances of fact-checkers influence their effectiveness in correcting misbeliefs about political misinformation. Our findings demonstrate that partisan fact-checkers can decrease the perceived accuracy of political misinformation and correct misbeliefs without triggering backfire effects. This correction is even more pronounced when the misinformation aligns with individuals' political ideologies. Notably, while previous research suggests that fact-checking warnings are less effective for conservatives than liberals, our results suggest that explicitly labeled partisan fact-checkers, positioned as political counterparts to conservatives, are particularly effective in reducing conservatives' misbeliefs toward pro-liberal misinformation.

</details>


### [3] [Who's the Leader? Analyzing Novice Workflows in LLM-Assisted Debugging of Machine Learning Code](https://arxiv.org/abs/2505.08063)

*Jessica Y. Bo, Majeed Kazemitabaar, Emma Zhuang, Ashton Anderson*

**Main category:** cs.HC

**Keywords:** large language models, machine learning, cognitive engagement, novice learning, reliance patterns

**Relevance Score:** 9

**TL;DR:** This study examines how novice machine learning engineers interact with ChatGPT while troubleshooting a buggy ML script, focusing on their reliance and cognitive engagement with the LLM.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of large language models (LLMs) in supporting novice learners in complex fields like machine learning, particularly how their interactions impact task performance and learning.

**Method:** A formative study with eight novice ML engineers was conducted, where participants interacted with a buggy ML script while having open access to ChatGPT.

**Key Contributions:**

	1. Introduced categories of novice interactions with LLMs in ML tasks.
	2. Identified reliance patterns that may affect learning outcomes for novices.
	3. Proposed enhancements to the novice-LLM interaction model to foster better cognitive engagement.

**Result:** User actions during the interactions with the LLM could be categorized into two main types: leading the LLM and being led by the LLM, affecting reliance outcomes like over-reliance and under-reliance.

**Limitations:** The study is limited to a small sample size and focuses only on the ML domain, which may affect generalizability.

**Conclusion:** Interactions with LLMs can hinder novices' cognitive engagement, leading to potential negative effects on learning outcomes, necessitating the need for improved interaction paradigms.

**Abstract:** While LLMs are often touted as tools for democratizing specialized knowledge to beginners, their actual effectiveness for improving task performance and learning is still an open question. It is known that novices engage with LLMs differently from experts, with prior studies reporting meta-cognitive pitfalls that affect novices' ability to verify outputs and prompt effectively. We focus on a task domain, machine learning (ML), which embodies both high complexity and low verifiability to understand the impact of LLM assistance on novices. Provided a buggy ML script and open access to ChatGPT, we conduct a formative study with eight novice ML engineers to understand their reliance on, interactions with, and perceptions of the LLM. We find that user actions can be roughly categorized into leading the LLM and led-by the LLM, and further investigate how they affect reliance outcomes like over- and under-reliance. These results have implications on novices' cognitive engagement in LLM-assisted tasks and potential negative effects on downstream learning. Lastly, we pose potential augmentations to the novice-LLM interaction paradigm to promote cognitive engagement.

</details>


### [4] [Justified Evidence Collection for Argument-based AI Fairness Assurance](https://arxiv.org/abs/2505.08064)

*Alpay Sabuncuoglu, Christopher Burr, Carsten Maple*

**Main category:** cs.HC

**Keywords:** fairness, AI systems, dynamic assurance, explainability, systems engineering

**Relevance Score:** 6

**TL;DR:** This paper presents a framework for dynamic argument-based assurance in AI system development, focusing on fairness and explainability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of ensuring fair AI systems requires structured approaches throughout the system lifecycle.

**Method:** The framework operates in two stages: defining goals in the requirements phase and utilizing a continuous monitoring interface to gather evidence.

**Key Contributions:**

	1. Introduction of a frameworks for dynamic argument-based assurance in AI systems.
	2. Detailed two-stage approach involving multidisciplinary teams for fairness governance.
	3. Continuous monitoring for gathering evidence supporting fairness arguments.

**Result:** The framework is demonstrated through a case study in finance, showcasing its ability to support fairness-related arguments.

**Limitations:** 

**Conclusion:** The framework effectively operationalizes dynamic argument-based assurance to address fairness in AI systems.

**Abstract:** It is well recognised that ensuring fair AI systems is a complex sociotechnical challenge, which requires careful deliberation and continuous oversight across all stages of a system's lifecycle, from defining requirements to model deployment and deprovisioning. Dynamic argument-based assurance cases, which present structured arguments supported by evidence, have emerged as a systematic approach to evaluating and mitigating safety risks and hazards in AI-enabled system development and have also been extended to deal with broader normative goals such as fairness and explainability. This paper introduces a systems-engineering-driven framework, supported by software tooling, to operationalise a dynamic approach to argument-based assurance in two stages. In the first stage, during the requirements planning phase, a multi-disciplinary and multi-stakeholder team define goals and claims to be established (and evidenced) by conducting a comprehensive fairness governance process. In the second stage, a continuous monitoring interface gathers evidence from existing artefacts (e.g. metrics from automated tests), such as model, data, and use case documentation, to support these arguments dynamically. The framework's effectiveness is demonstrated through an illustrative case study in finance, with a focus on supporting fairness-related arguments.

</details>


### [5] [Perspectives on Capturing Emotional Expressiveness in Sign Language](https://arxiv.org/abs/2505.08072)

*Phoebe Chua, Cathy Mengying Fang, Yasith Samaradivakara, Pattie Maes, Suranga Nanayakkara*

**Main category:** cs.HC

**Keywords:** sign language, emotional expression, technology, communication, Deaf communities

**Relevance Score:** 7

**TL;DR:** The paper explores the emotionally expressive aspects of sign language communication, revealing patterns and challenges for technology development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the limited progress in capturing emotional dimensions in sign language technologies compared to text and speech.

**Method:** Semi-structured interviews with eight sign language users from Singapore, Sri Lanka, and the US, including Deaf, Hard of Hearing, and hearing signers.

**Key Contributions:**

	1. Identification of both manual and non-manual elements in emotional expression.
	2. Discovery of universal patterns and cultural variations in emotion communication.
	3. Design considerations for developing more emotionally-aware sign language technologies.

**Result:** The study identifies universal and culturally specific patterns in emotional expression and highlights key challenges in emotional nuance capture for sign language translation.

**Limitations:** 

**Conclusion:** The findings contribute to both theoretical insights into emotional expression in sign language and practical considerations for developing technology that better serves signing communities.

**Abstract:** Significant advances have been made in our ability to understand and generate emotionally expressive content such as text and speech, yet comparable progress in sign language technologies remain limited. While computational approaches to sign language translation have focused on capturing lexical content, the emotional dimensions of sign language communication remain largely unexplored. Through semi-structured interviews with eight sign language users across Singapore, Sri Lanka and the United States, including both Deaf and Hard of hearing (DHH) and hearing signers, we investigate how emotions are expressed and perceived in sign languages. Our findings highlight the role of both manual and non-manual elements in emotional expression, revealing universal patterns as well as individual and cultural variations in how signers communicate emotions. We identify key challenges in capturing emotional nuance for sign language translation, and propose design considerations for developing more emotionally-aware sign language technologies. This work contributes to both theoretical understanding of emotional expression in sign language and practical development of interfaces to better serve diverse signing communities.

</details>


### [6] [Will Your Next Pair Programming Partner Be Human? An Empirical Evaluation of Generative AI as a Collaborative Teammate in a Semester-Long Classroom Setting](https://arxiv.org/abs/2505.08119)

*Wenhan Lyu, Yimeng Wang, Yifan Sun, Yixuan Zhang*

**Main category:** cs.HC

**Keywords:** Generative AI, pair programming, Large Language Models, computer science education, collaboration

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of Generative AI (GenAI) on pair programming among undergraduate students, revealing that collaboration with LLM-based tools enhances performance and alters perceptions about programming.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of LLM-based tools in pair programming and understand how they affect collaboration, learning, and performance among students.

**Method:** A mixed-methods study was conducted with 39 undergraduate students who completed six assignments under three conditions: Traditional Pair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming with GenAI (SAI).

**Key Contributions:**

	1. Empirical evaluation of GenAI in pair programming contexts.
	2. Comparison of performance between traditional and GenAI-assisted programming.
	3. Insights into students' attitudes and expectations toward LLMs as collaborators.

**Result:** Students in the PAI condition achieved the highest scores, while those in SAI had the lowest. Attitudes toward LLM programming capabilities improved significantly, influenced by perceived usefulness and collaboration quality.

**Limitations:** Students had differing expectations from LLM tools compared to human partners, and limitations of LLMs were noted.

**Conclusion:** The study shows that LLMs can serve as effective pair programming partners but come with limitations. It provides insights for future applications of GenAI in educational settings.

**Abstract:** Generative AI (GenAI), especially Large Language Models (LLMs), is rapidly reshaping both programming workflows and computer science education. Many programmers now incorporate GenAI tools into their workflows, including for collaborative coding tasks such as pair programming. While prior research has demonstrated the benefits of traditional pair programming and begun to explore GenAI-assisted coding, the role of LLM-based tools as collaborators in pair programming remains underexamined. In this work, we conducted a mixed-methods study with 39 undergraduate students to examine how GenAI influences collaboration, learning, and performance in pair programming. Specifically, students completed six in-class assignments under three conditions: Traditional Pair Programming (PP), Pair Programming with GenAI (PAI), and Solo Programming with GenAI (SAI). They used both LLM-based inline completion tools (e.g., GitHub Copilot) and LLM-based conversational tools (e.g., ChatGPT). Our results show that students in PAI achieved the highest assignment scores, whereas those in SAI attained the lowest. Additionally, students' attitudes toward LLMs' programming capabilities improved significantly after collaborating with LLM-based tools, and preferences were largely shaped by the perceived usefulness for completing assignments and learning programming skills, as well as the quality of collaboration. Our qualitative findings further reveal that while students appreciated LLM-based tools as valuable pair programming partners, they also identified limitations and had different expectations compared to human teammates. Our study provides one of the first empirical evaluations of GenAI as a pair programming collaborator through a comparison of three conditions (PP, PAI, and SAI). We also discuss the design implications and pedagogical considerations for future GenAI-assisted pair programming approaches.

</details>


### [7] [Communication Styles and Reader Preferences of LLM and Human Experts in Explaining Health Information](https://arxiv.org/abs/2505.08143)

*Jiawei Zhou, Kritika Venkatachalam, Minje Choi, Koustuv Saha, Munmun De Choudhury*

**Main category:** cs.HC

**Keywords:** large language models, health communication, fact-checking, persuasive strategies, reader perceptions

**Relevance Score:** 9

**TL;DR:** This study examines the alignment of large language models (LLMs) with human communication styles in health fact-checking, evaluating their effectiveness and reader perceptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs' communication styles align with human values and enhance trust in health information, especially in the context of fact-checking.

**Method:** A dataset of 1498 health misinformation explanations was compiled, with LLM responses generated and analyzed across three dimensions: linguistic features, persuasive strategies, and value alignment. A blinded evaluation with 99 participants assessed human perceptions of these communications.

**Key Contributions:**

	1. Evaluates LLM communication styles in health fact-checking
	2. Compares LLM responses to human expert explanations
	3. Identifies reader preferences for clarity over traditional metrics

**Result:** LLM-generated content scored lower in persuasive strategies and alignment with social values compared to human experts, yet was preferred by readers for clarity and completeness, indicating a paradox in effectiveness versus traditional quality measures.

**Limitations:** The study focuses on health communication and may not generalize to other contexts or types of misinformation.

**Conclusion:** Structured LLM communication may engage readers effectively in health contexts, despite lower scores on traditional quality metrics in persuasive communication.

**Abstract:** With the wide adoption of large language models (LLMs) in information assistance, it is essential to examine their alignment with human communication styles and values. We situate this study within the context of fact-checking health information, given the critical challenge of rectifying conceptions and building trust. Recent studies have explored the potential of LLM for health communication, but style differences between LLMs and human experts and associated reader perceptions remain under-explored. In this light, our study evaluates the communication styles of LLMs, focusing on how their explanations differ from those of humans in three core components of health communication: information, sender, and receiver. We compiled a dataset of 1498 health misinformation explanations from authoritative fact-checking organizations and generated LLM responses to inaccurate health information. Drawing from health communication theory, we evaluate communication styles across three key dimensions of information linguistic features, sender persuasive strategies, and receiver value alignments. We further assessed human perceptions through a blinded evaluation with 99 participants. Our findings reveal that LLM-generated articles showed significantly lower scores in persuasive strategies, certainty expressions, and alignment with social values and moral foundations. However, human evaluation demonstrated a strong preference for LLM content, with over 60% responses favoring LLM articles for clarity, completeness, and persuasiveness. Our results suggest that LLMs' structured approach to presenting information may be more effective at engaging readers despite scoring lower on traditional measures of quality in fact-checking and health communication.

</details>


### [8] [Investigating Resolution Strategies for Workspace-Occlusion in Augmented Virtuality](https://arxiv.org/abs/2505.08312)

*Nico Feld, Pauline Bimberg, Michael Feldmann, Matthias Wölwer, Eike Langbehn, Benjamin Weyers, Daniel Zielasko*

**Main category:** cs.HC

**Keywords:** Augmented Virtuality, occlusion, Redirected Walking, Automatic Teleport Rotation, user studies

**Relevance Score:** 6

**TL;DR:** The paper explores strategies to reduce occlusion of physical content by virtual content in Augmented Virtuality, specifically through Redirected Walking and Automatic Teleport Rotation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of occlusion when integrating physical objects into virtual environments, which can disrupt user interaction and compromise safety.

**Method:** Two strategies were tested: Redirected Walking, which adjusts user movement, and Automatic Teleport Rotation, which realigns the virtual world during movement.

**Key Contributions:**

	1. Investigation of occlusion resolution strategies in Augmented Virtuality
	2. User study comparing Redirected Walking and Automatic Teleport Rotation
	3. Findings on the trade-off between occlusion resolution and cybersickness

**Result:** Both strategies were effective in reducing occlusion, with Automatic Teleport Rotation showing better resolution but potentially increasing cybersickness.

**Limitations:** Further research is needed to explore the long-term effects of increased cybersickness and user preferences over time.

**Conclusion:** While Automatic Teleport Rotation provides better resolution of occlusion, it may lead to increased cybersickness compared to Redirected Walking.

**Abstract:** Augmented Virtuality integrates physical content into virtual environments, but the occlusion of physical by virtual content is a challenge. This unwanted occlusion may disrupt user interactions with physical devices and compromise safety and usability. This paper investigates two resolution strategies to address this issue: Redirected Walking, which subtly adjusts the user's movement to maintain physical-virtual alignment, and Automatic Teleport Rotation, which realigns the virtual environment during travel. A user study set in a virtual forest demonstrates that both methods effectively reduce occlusion. While in our testbed, Automatic Teleport Rotation achieves higher occlusion resolution, it is suspected to increase cybersickness compared to the less intrusive Redirected Walking approach.

</details>


### [9] [A Comparison Between Human and Generative AI Decision-Making Attributes in Complex Health Services](https://arxiv.org/abs/2505.08360)

*Nandini Doreswamy, Louise Horstmanshof*

**Main category:** cs.HC

**Keywords:** Generative AI, Human decision-making, Health services, Cooperation, Decision-making attributes

**Relevance Score:** 9

**TL;DR:** This paper compares human and Generative AI decision-making attributes in complex health services, highlighting their complementary strengths and potential for cooperation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the knowledge gap regarding decision-making attributes of humans and Generative AI in complex health services such as health policy and regulation.

**Method:** The analysis involves a comparison based on two reviews: a scoping review of human attributes and a rapid review of Generative AI attributes, categorizing them by uniqueness and impact, and presenting results in tabular form.

**Key Contributions:**

	1. Explores the unique decision-making attributes of humans in healthcare compared to Generative AI.
	2. Identifies potential cooperation between humans and Generative AI rather than competition.
	3. Provides a structured comparison of attributes with tabular representation.

**Result:** The comparison shows that humans and Generative AI have complementary strengths in decision-making, suggesting that cooperation is more likely than competition.

**Limitations:** The study is limited to the comparison of attributes without empirical testing or real-world validation.

**Conclusion:** Humans should develop their unique attributes while integrating both human and Generative AI contributions in decision-making systems for optimal outcomes.

**Abstract:** A comparison between human and Generative AI decision-making attributes in complex health services is a knowledge gap in the literature, at present. Humans may possess unique attributes beneficial to decision-making in complex health services such as health policy and health regulation, but are also susceptible to decision-making flaws. The objective is to explore whether humans have unique, and/or helpful attributes that contribute to optimal decision-making in complex health services. This comparison may also shed light on whether humans are likely to compete, cooperate, or converge with Generative AI. The comparison is based on two published reviews: a scoping review of human attributes [1] and a rapid review of Generative AI attributes [2]. The analysis categorizes attributes by uniqueness and impact. The results are presented in tabular form, comparing the sets and subsets of human and Generative AI attributes. Humans and Generative AI decision-making attributes have complementary strengths. Cooperation between these two entities seems more likely than pure competition. To maintain meaningful decision-making roles, humans could develop their unique attributes, with decision-making systems integrating both human and Generative AI contributions. These entities may also converge, in future.

</details>


### [10] [Human-in-the-Loop Optimization for Inclusive Design: Balancing Automation and Designer Expertise](https://arxiv.org/abs/2505.08375)

*Pascal Jansen*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Accessible Design, Human-in-the-Loop, Design Optimization, Ethics

**Relevance Score:** 9

**TL;DR:** This position paper advocates for an automated Human-in-the-Loop (HITL) design optimization process to enhance accessible prototyping in HCI by allowing designers to focus on curating constraints for algorithmic exploration rather than direct prototype creation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for accessible and inclusive design in HCI, yet current prototyping methods are resource-intensive and fail to address the complexities of users with disabilities effectively.

**Method:** The proposed approach involves automating the design optimization process through a HITL framework that allows designers to pre-constrain the design space and use adaptive, multi-modal feedback mechanisms to refine design parameters efficiently.

**Key Contributions:**

	1. Proposes an automated HITL design optimization process for accessibility.
	2. Shifts the designer's role to curating constraints instead of direct prototyping.
	3. Highlights the importance of adaptive feedback mechanisms in refining design parameters.

**Result:** This HITL approach can lead to more scalable and individualized accessibility solutions by effectively adjusting elements like text size, color contrast, and interaction modalities based on user needs.

**Limitations:** The paper raises questions about the ethical implications of constraint curation and maintaining user agency in the design process.

**Conclusion:** The paper emphasizes the importance of collaborative discussion on the implications of this approach, particularly regarding constraint curation, user agency, and ethical considerations.

**Abstract:** Accessible and inclusive design has gained increased attention in HCI, yet practical implementation remains challenging due to resource-intensive prototyping methods. Traditional approaches such as workshops, A-B tests, and co-design sessions struggle to capture the diverse and complex needs of users with disabilities at scale. This position paper argues for an automated, accessible Human-in-the-Loop (HITL) design optimization process that shifts the designer's role from directly crafting prototypes to curating constraints for algorithmic exploration. By pre-constraining the design space based on specific user interaction needs, integrating adaptive multi-modal feedback channels, and personalizing feedback prompts, the HITL approach could efficiently refine design parameters, such as text size, color contrast, layout, and interaction modalities, to achieve optimal accessibility. This approach promises scalable, individualized design solutions while raising critical questions about constraint curation, transparency, user agency, and ethical considerations, making it essential to discuss and refine these ideas collaboratively at the workshop.

</details>


### [11] [BizChat: Scaffolding AI-Powered Business Planning for Small Business Owners Across Digital Skill Levels](https://arxiv.org/abs/2505.08493)

*Quentin Romero Lauro, Aakash Gautam, Yasmine Kotturi*

**Main category:** cs.HC

**Keywords:** Generative AI, Small Business, Digital Literacy, LLM Applications, Entrepreneurial Education

**Relevance Score:** 6

**TL;DR:** The paper presents BizChat, a generative AI tool designed to help small business owners create business plans by addressing digital skill disparities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the barriers small business owners face when adopting generative AI tools due to varying levels of digital skills.

**Method:** Conducted a design workshop series and 15 interviews with small businesses to inform the development of BizChat, an LLM-powered web application.

**Key Contributions:**

	1. Introduction of an LLM-powered application tailored for small business owners
	2. Emphasis on accessibility and learning in digital tool usage
	3. Contextualized technology introduction for enhanced user experience

**Result:** BizChat incorporates design features that support users with different digital skills, enabling them to write business plans effectively.

**Limitations:** 

**Conclusion:** The findings led to plans for deploying BizChat to assist small business owners with digital literacy and entrepreneurial education.

**Abstract:** Generative AI can help small business owners automate tasks, increase efficiency, and improve their bottom line. However, despite the seemingly intuitive design of systems like ChatGPT, significant barriers remain for those less comfortable with technology. To address these disparities, prior work highlights accessory skills -- beyond prompt engineering -- users must master to successfully adopt generative AI including keyboard shortcuts, editing skills, file conversions, and browser literacy. Building on a design workshop series and 15 interviews with small businesses, we introduce BizChat, a large language model (LLM)-powered web application that helps business owners across digital skills levels write their business plan -- an essential but often neglected document. To do so, BizChat's interface embodies three design considerations inspired by learning sciences: ensuring accessibility to users with less digital skills while maintaining extensibility to power users ("low-floor-high-ceiling"), providing in situ micro-learning to support entrepreneurial education ("just-in-time learning"), and framing interaction around business activities ("contextualized technology introduction"). We conclude with plans for a future BizChat deployment.

</details>


### [12] [VizCV: AI-assisted visualization of researchers' publications tracks](https://arxiv.org/abs/2505.08691)

*Vladimír Lazárik, Marco Agus, Barbora Kozlíková, Pere-Pau Vázquez*

**Main category:** cs.HC

**Keywords:** visual analytics, research trajectory, AI-assisted analysis, career progression, collaborative dynamics

**Relevance Score:** 6

**TL;DR:** VizCV is a web-based analytics framework for exploring scientists' career progression through AI-driven insights on research topic evolution, publication impacts, and collaboration dynamics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Assessing the expertise of scientists and research groups by analyzing their publication records is vital for academic management.

**Method:** The system uses AI-assisted analysis to explore researchers' trajectories through three dimensions: topic evolution, publication impact, and collaboration dynamics.

**Key Contributions:**

	1. AI techniques for topic analysis
	2. Dimensionality reduction for visualizing trends
	3. Interactive generation of textual descriptions using LLMs for career insights

**Result:** VizCV enables users to visualize and analyze shifts in scholarly focus, measure publication impacts, and examine co-authorship networks over time.

**Limitations:** 

**Conclusion:** The framework supports detailed exploratory analysis, providing automated insights into researchers' career developments and allows for comparative analysis between different researchers.

**Abstract:** Analyzing how the publication records of scientists and research groups have evolved over the years is crucial for assessing their expertise since it can support the management of academic environments by assisting with career planning and evaluation. We introduce VizCV, a novel web-based end-to-end visual analytics framework that enables the interactive exploration of researchers' scientific trajectories. It incorporates AI-assisted analysis and supports automated reporting of career evolution. Our system aims to model career progression through three key dimensions: a) research topic evolution to detect and visualize shifts in scholarly focus over time, b) publication record and the corresponding impact, c) collaboration dynamics depicting the growth and transformation of a researcher's co-authorship network. AI-driven insights provide automated explanations of career transitions, detecting significant shifts in research direction, impact surges, or collaboration expansions. The system also supports comparative analysis between researchers, allowing users to compare topic trajectories and impact growth. Our interactive, multi-tab and multiview system allows for the exploratory analysis of career milestones under different perspectives, such as the most impactful articles, emerging research themes, or obtaining a detailed analysis of the contribution of the researcher in a subfield. The key contributions include AI/ML techniques for: a) topic analysis, b) dimensionality reduction for visualizing patterns and trends, c) the interactive creation of textual descriptions of facets of data through configurable prompt generation and large language models, that include key indicators, to help understanding the career development of individuals or groups.

</details>


### [13] [Designing Value-Centered Consent Interfaces: A Mixed-Methods Approach to Support Patient Values in Data-Sharing Decisions](https://arxiv.org/abs/2407.03808)

*David Leimstädtner, Peter Sörries, Claudia Müller-Birn*

**Main category:** cs.HC

**Keywords:** digital health, data donation, user interface design, ethical data practices, value-centered consent

**Relevance Score:** 8

**TL;DR:** This paper explores the design of consent user interfaces for ethical health data sharing, focusing on patient values and value congruence in decision-making.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses ethical concerns in digital health regarding data-sharing decisions that prioritize data collectors' values over those of data subjects.

**Method:** An exploratory sequential mixed-method approach with five phases including participatory workshops, an online experiment, co-creation with domain experts, and user interface evaluation.

**Key Contributions:**

	1. Establishing value-centered data collection practices in digital health
	2. Introducing a novel consent interface that enhances value congruence
	3. Conducting a situated evaluation with patients to assess the interface

**Result:** The proposed Value-Centered Consent Interface increases value congruence in health data-sharing decisions among patients.

**Limitations:** 

**Conclusion:** The research demonstrates the importance of incorporating patient values in the design of data-sharing interfaces, contributing to ethical data practices in digital health.

**Abstract:** In the digital health domain, ethical data collection practices are crucial for ensuring the availability of quality datasets that drive medical advancement. Data donation, allowing patients to share their medical data for secondary research purposes, presents a promising resource for such datasets. Yet, current consent user interfaces mediating data-sharing decisions are found to favor data collectors' values over those of data subjects. This raises ethical concerns about the use of data collected, as well as concerning the quality of the resulting datasets. Seeking to establish value-centered data collection practices in digital health, we investigate the design of consent user interfaces that support end-users in making value-congruent health data-sharing decisions. Focusing our research efforts on the situated context of health data donation at the psychosomatic unit of a university hospital, we demonstrate how a human-centered design can ground technology within the perspective of a vulnerable group. We employed an exploratory sequential mixed-method approach consisting of five phases: Participatory workshops elicit patient values, informing the design of a proposed Value-Centered Consent Interface. An online experiment demonstrates our interface element's effect, increasing value congruence in data-sharing decisions. Our proposed consent user interface design is then adapted to the research context through a co-creation workshop with domain experts and a user interface evaluation with patients. Our work contributes to recent discourse in CSCW concerning ethical implications of new data practices within their socio-technological context by exploring patient values on medical data-sharing, introducing a novel consent interface leveraging reflection to support value-congruent decision-making, and providing a situated evaluation of the proposed consent user interface with patients.

</details>


### [14] [Lessons From an App Update at Replika AI: Identity Discontinuity in Human-AI Relationships](https://arxiv.org/abs/2412.14190)

*Julian De Freitas, Noah Castelo, Ahmet Uguralp, Zeliha Uguralp*

**Main category:** cs.HC

**Keywords:** AI companionship, emotional bonds, identity disruption, consumer welfare, mourning

**Relevance Score:** 6

**TL;DR:** Consumers develop deep emotional bonds with AI companions, which are affected by disruptions to their perceived identity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether consumers can form significant emotional attachments to AI and the consequences of changes in their AI's identity.

**Method:** Leveraging a natural app-update event at Replika AI, the study gathered data on user perceptions and emotional responses to the removal of the erotic role play feature.

**Key Contributions:**

	1. Demonstrates that AI companions can be seen as having identities comparable to humans.
	2. Highlights emotional mourning in response to disruptions in AI relationships.
	3. Shows significant user attachment to AI companions over human friends.

**Result:** The removal of the ERP feature led users to perceive their AI companion's identity as having discontinued, which predicted negative emotional responses and product devaluation.

**Limitations:** 

**Conclusion:** Disruptions in AI relationships lead to mourning and devaluation, underscoring the personal nature of these connections.

**Abstract:** Can consumers form especially deep emotional bonds with AI and be vested in AI identities over time? We leverage a natural app-update event at Replika AI, a popular US-based AI companion, to shed light on these questions. We find that, after the app removed its erotic role play (ERP) feature, preventing intimate interactions between consumers and chatbots that were previously possible, this event triggered perceptions in customers that their AI companion's identity had discontinued. This in turn predicted negative consumer welfare and marketing outcomes related to loss, including mourning the loss, and devaluing the "new" AI relative to the "original". Experimental evidence confirms these findings. Further experiments find that AI companions users feel closer to their AI companion than even their best human friend, and mourn a loss of their AI companion more than a loss of various other inanimate products. In short, consumers are forming human-level relationships with AI companions; disruptions to these relationships trigger real patterns of mourning as well as devaluation of the offering; and the degree of mourning and devaluation are explained by perceived discontinuity in the AIs identity. Our results illustrate that relationships with AI are truly personal, creating unique benefits and risks for consumers and firms alike.

</details>


### [15] [A Protocol for KG Construction Tasks Involving Users](https://arxiv.org/abs/2412.16766)

*Ademar Crotti Junior, Christophe Debruyne*

**Main category:** cs.HC

**Keywords:** knowledge graph construction, user protocol, RDF Mapping Language, evaluation metrics, comparative analysis

**Relevance Score:** 4

**TL;DR:** This paper introduces a user protocol for knowledge graph construction (KGC) to address the heterogeneity in studies involving users and facilitate comparison among KGC languages and techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of systematic consistency in task design, participant selection, and evaluation metrics in knowledge graph construction (KGC) studies hinders the ability to compare different languages, techniques, and tools effectively.

**Method:** The authors analyze existing user studies in KGC to identify gaps and propose a user protocol that facilitates the comparative evaluation of KGC languages and techniques, particularly focusing on RDF Mapping Language (RML) functionality.

**Key Contributions:**

	1. Identification of gaps in existing KGC user studies
	2. Development of a user protocol for KGC
	3. Proposal for comparing extensions of RDF Mapping Language

**Result:** The proposed protocol aims to standardize the task design and evaluation metrics, allowing for a more consistent and comparable evaluation of KGC user studies.

**Limitations:** 

**Conclusion:** The introduction of this user protocol is a significant step towards achieving a more unified framework for evaluating KGC methodologies, leading to better insights and advancements in the field.

**Abstract:** Knowledge graph construction (KGC) from (semi-)structured data is challenging, and facilitating user involvement is an issue frequently brought up within this community. We cannot deny the progress we have made with respect to (declarative) knowledge graph construction languages and tools to help build such mappings. However, it is surprising that no two studies report on similar protocols. This heterogeneity does not allow for comparing KGC languages, techniques, and tools. This paper first analyses studies involving users to identify the points of comparison. These gaps include a lack of systematic consistency in task design, participant selection, and evaluation metrics. Moreover, there needs to be a systematic way of analyzing the data and reporting the findings, which is also lacking. We thus propose and introduce a user protocol for KGC designed to address this challenge. Where possible, we draw and take elements from the literature we deem fit for such a protocol. The protocol, as such, allows for the comparison of languages and techniques for the RDF Mapping Language (RML) core functionality, which is covered by most of the other state-of-the-art techniques and tools. We also propose how the protocol can be amended to compare extensions (of RML). This protocol provides an important step towards a more comparable evaluation of KGC user studies.

</details>


### [16] [Examining the Expanding Role of Synthetic Data Throughout the AI Development Pipeline](https://arxiv.org/abs/2501.18493)

*Shivani Kapania, Stephanie Ballard, Alex Kessler, Jennifer Wortman Vaughan*

**Main category:** cs.HC

**Keywords:** synthetic data, AI development, auxiliary models, responsible AI, data scarcity

**Relevance Score:** 8

**TL;DR:** This paper investigates the role of synthetic data generated by auxiliary models in AI development, highlighting its benefits and challenges encountered by practitioners.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in policy guidance and best practices for the responsible use of synthetic data in AI development, amidst its rapidly growing use.

**Method:** Conducted 29 interviews with AI practitioners and responsible AI experts to gather insights about current practices and challenges related to synthetic data use.

**Key Contributions:**

	1. Insight into current practices surrounding synthetic data in AI development
	2. Identification of significant challenges faced by practitioners when using synthetic data
	3. Proposed best practices to enhance responsible use of synthetic data

**Result:** Auxiliary models are widely adopted in AI workflows, viewed as essential for overcoming data scarcity, though practitioners face difficulties in output control, representation, and validation.

**Limitations:** Challenges in output control and representation; validation mostly manual which could introduce bias.

**Conclusion:** A proposal for best practices to ensure responsible use of synthetic data is outlined, addressing the ethical implications and limitations identified.

**Abstract:** Alongside the growth of generative AI, we are witnessing a surge in the use of synthetic data across all stages of the AI development pipeline. It is now common practice for researchers and practitioners to use one large generative model (which we refer to as an auxiliary model) to generate synthetic data that is used to train or evaluate another, reconfiguring AI workflows and reshaping the very nature of data. While scholars have raised concerns over the risks of synthetic data, policy guidance and best practices for its responsible use have not kept up with these rapidly evolving industry trends, in part because we lack a clear picture of current practices and challenges. Our work aims to address this gap. Through 29 interviews with AI practitioners and responsible AI experts, we examine the expanding role of synthetic data in AI development. Our findings reveal how auxiliary models are now widely used across the AI development pipeline. Practitioners describe synthetic data as crucial for addressing data scarcity and providing a competitive edge, noting that evaluation of generative AI systems at scale would be infeasible without auxiliary models. However, they face challenges controlling the outputs of auxiliary models, generating data that accurately depict underrepresented groups, and scaling data validation practices that are based primarily on manual inspection. We detail general limitations of and ethical considerations for synthetic data and conclude with a proposal of concrete steps towards the development of best practices for its responsible use.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [Polysemy of Synthetic Neurons Towards a New Type of Explanatory Categorical Vector Spaces](https://arxiv.org/abs/2505.07831)

*Michael Pichat, William Pogrund, Paloma Pichat, Judicael Poumay, Armanouche Gasparian, Samuel Demarchi, Martin Corbet, Alois Georgeon, Michael Veillet-Guillem*

**Main category:** cs.CL

**Keywords:** AI language models, Neurons, Categorical vector space, Intra-neuronal attention

**Relevance Score:** 6

**TL;DR:** This paper proposes a new geometric definition of neurons in AI language models, focusing on a categorical vector space structure to improve efficiency and homogeneity.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the polysemantic nature of synthetic neurons and improve the efficiency of language models.

**Method:** Define neurons in layer n as a categorical vector space with a non-orthogonal basis, utilizing sub-dimensions from preceding neurons and employing intra-neuronal attention.

**Key Contributions:**

	1. Geometric definition of neurons as categorical vector spaces
	2. Identification of critical categorical zones for language model efficiency
	3. Use of intra-neuronal attention to optimize activation space

**Result:** The proposed structure allows for the identification of a critical categorical zone, enhancing the efficiency of language models.

**Limitations:** 

**Conclusion:** By restructuring the understanding of neuron activation spaces, the paper offers a novel approach that could improve language model performance.

**Abstract:** The polysemantic nature of synthetic neurons in artificial intelligence language models is currently understood as the result of a necessary superposition of distributed features within the latent space. We propose an alternative approach, geometrically defining a neuron in layer n as a categorical vector space with a non-orthogonal basis, composed of categorical sub-dimensions extracted from preceding neurons in layer n-1. This categorical vector space is structured by the activation space of each neuron and enables, via an intra-neuronal attention process, the identification and utilization of a critical categorical zone for the efficiency of the language model - more homogeneous and located at the intersection of these different categorical sub-dimensions.

</details>


### [18] [A Tale of Two Identities: An Ethical Audit of Human and AI-Crafted Personas](https://arxiv.org/abs/2505.07850)

*Pranav Narayanan Venkit, Jiayi Li, Yingfan Zhou, Sarah Rajtmajer, Shomir Wilson*

**Main category:** cs.CL

**Keywords:** synthetic personas, Linguistic models, identity representation, algorithmic bias, HCI

**Relevance Score:** 9

**TL;DR:** The paper audits synthetic personas generated by LLMs to understand their representation of minority identities, particularly focusing on racial identity and associated sociotechnical harms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how synthetic personas generated by LLMs represent identity and to identify potential harms, particularly for minority communities.

**Method:** The study utilizes a mixed methods approach, combining close reading, lexical analysis, and a parameterized creativity framework to compare 1512 LLM-generated personas with human-authored responses.

**Key Contributions:**

	1. Formalization of algorithmic othering
	2. Identification of sociotechnical harms from LLM-generated personas
	3. Design recommendations for evaluation metrics and validation protocols

**Result:** Findings show that LLMs tend to foreground racial markers and produce culturally coded language, leading to syntactically elaborate but narratively reductive personas, resulting in stereotyping and other socio-technical harms.

**Limitations:** The study focuses primarily on racial identity and does not address other dimensions of identity representation.

**Conclusion:** The authors introduce the concept of algorithmic othering, recommending the development of narrative-aware metrics and community-centered validation for synthetic identity generation to mitigate harms.

**Abstract:** As LLMs (large language models) are increasingly used to generate synthetic personas particularly in data-limited domains such as health, privacy, and HCI, it becomes necessary to understand how these narratives represent identity, especially that of minority communities. In this paper, we audit synthetic personas generated by 3 LLMs (GPT4o, Gemini 1.5 Pro, Deepseek 2.5) through the lens of representational harm, focusing specifically on racial identity. Using a mixed methods approach combining close reading, lexical analysis, and a parameterized creativity framework, we compare 1512 LLM generated personas to human-authored responses. Our findings reveal that LLMs disproportionately foreground racial markers, overproduce culturally coded language, and construct personas that are syntactically elaborate yet narratively reductive. These patterns result in a range of sociotechnical harms, including stereotyping, exoticism, erasure, and benevolent bias, that are often obfuscated by superficially positive narrations. We formalize this phenomenon as algorithmic othering, where minoritized identities are rendered hypervisible but less authentic. Based on these findings, we offer design recommendations for narrative-aware evaluation metrics and community-centered validation protocols for synthetic identity generation.

</details>


### [19] [Joint Detection of Fraud and Concept Drift inOnline Conversations with LLM-Assisted Judgment](https://arxiv.org/abs/2505.07852)

*Ali Senol, Garima Agrawal, Huan Liu*

**Main category:** cs.CL

**Keywords:** fraud detection, concept drift, large language models, ensemble classification, social engineering

**Relevance Score:** 8

**TL;DR:** A two-stage framework for detecting fake interactions in digital platforms using ensemble classification and LLMs to manage concept drift in conversations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Detecting malicious interactions in digital communication is challenging due to concept drift and reliance on static detection methods.

**Method:** The proposed framework uses an ensemble classification model to identify suspicious conversations and incorporates a One Class Drift Detector for identifying conversational shifts, followed by evaluation using a large language model.

**Key Contributions:**

	1. Development of a two-stage detection framework
	2. Incorporation of One Class Drift Detector for concept drift analysis
	3. Validation against a Dual LLM baseline for improved accuracy

**Result:** The framework improves detection accuracy and interpretability in real-time fraud detection, validated on social engineering chat scenarios.

**Limitations:** 

**Conclusion:** The modular approach provides practical advantages over traditional methods while addressing the challenges of detecting concept drift in conversations.

**Abstract:** Detecting fake interactions in digital communication platforms remains a challenging and insufficiently addressed problem. These interactions may appear as harmless spam or escalate into sophisticated scam attempts, making it difficult to flag malicious intent early. Traditional detection methods often rely on static anomaly detection techniques that fail to adapt to dynamic conversational shifts. One key limitation is the misinterpretation of benign topic transitions referred to as concept drift as fraudulent behavior, leading to either false alarms or missed threats. We propose a two stage detection framework that first identifies suspicious conversations using a tailored ensemble classification model. To improve the reliability of detection, we incorporate a concept drift analysis step using a One Class Drift Detector (OCDD) to isolate conversational shifts within flagged dialogues. When drift is detected, a large language model (LLM) assesses whether the shift indicates fraudulent manipulation or a legitimate topic change. In cases where no drift is found, the behavior is inferred to be spam like. We validate our framework using a dataset of social engineering chat scenarios and demonstrate its practical advantages in improving both accuracy and interpretability for real time fraud detection. To contextualize the trade offs, we compare our modular approach against a Dual LLM baseline that performs detection and judgment using different language models.

</details>


### [20] [CrashSage: A Large Language Model-Centered Framework for Contextual and Interpretable Traffic Crash Analysis](https://arxiv.org/abs/2505.07853)

*Hao Zhen, Jidong J. Yang*

**Main category:** cs.CL

**Keywords:** Road safety, Large Language Models, Crash analysis, Data augmentation, Explainability

**Relevance Score:** 7

**TL;DR:** The paper introduces CrashSage, a Large Language Model (LLM)-centered framework aimed at improving road crash analysis and modeling through enhanced narrative generation and interpretability processes.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the urgent need for better road safety analysis methodologies that capture complex interactions in crash data, moving beyond traditional approaches that often overlook contextual nuances.

**Method:** The methodology includes a tabular-to-text transformation for crash data, context-aware data augmentation using LLMs, fine-tuning of the LLaMA3-8B model for crash severity inference, and application of gradient-based explainability techniques.

**Key Contributions:**

	1. Tabular-to-text transformation for crash narrative generation
	2. Context-aware data augmentation with LLMs
	3. Fine-tuning LLaMA3-8B for enhanced crash severity inference

**Result:** CrashSage significantly outperforms baseline models in predicting crash severity and retains essential information through enriched textual narratives, demonstrating improved coherence and interpretability.

**Limitations:** 

**Conclusion:** The advancements made in modeling and interpreting crash data through CrashSage can enable targeted interventions in road safety efforts by revealing influential factors contributing to crashes.

**Abstract:** Road crashes claim over 1.3 million lives annually worldwide and incur global economic losses exceeding \$1.8 trillion. Such profound societal and financial impacts underscore the urgent need for road safety research that uncovers crash mechanisms and delivers actionable insights. Conventional statistical models and tree ensemble approaches typically rely on structured crash data, overlooking contextual nuances and struggling to capture complex relationships and underlying semantics. Moreover, these approaches tend to incur significant information loss, particularly in narrative elements related to multi-vehicle interactions, crash progression, and rare event characteristics. This study presents CrashSage, a novel Large Language Model (LLM)-centered framework designed to advance crash analysis and modeling through four key innovations. First, we introduce a tabular-to-text transformation strategy paired with relational data integration schema, enabling the conversion of raw, heterogeneous crash data into enriched, structured textual narratives that retain essential structural and relational context. Second, we apply context-aware data augmentation using a base LLM model to improve narrative coherence while preserving factual integrity. Third, we fine-tune the LLaMA3-8B model for crash severity inference, demonstrating superior performance over baseline approaches, including zero-shot, zero-shot with chain-of-thought prompting, and few-shot learning, with multiple models (GPT-4o, GPT-4o-mini, LLaMA3-70B). Finally, we employ a gradient-based explainability technique to elucidate model decisions at both the individual crash level and across broader risk factor dimensions. This interpretability mechanism enhances transparency and enables targeted road safety interventions by providing deeper insights into the most influential factors.

</details>


### [21] [Unpacking Robustness in Inflectional Languages: Adversarial Evaluation and Mechanistic Insights](https://arxiv.org/abs/2505.07856)

*Paweł Walkowiak, Marek Klonowski, Marcin Oleksy, Arkadiusz Janz*

**Main category:** cs.CL

**Keywords:** adversarial examples, inflectional languages, mechanistic interpretability, natural language processing, benchmarking

**Relevance Score:** 7

**TL;DR:** This study investigates how adversarial attack methods perform in inflectional languages, specifically Polish and English, using a novel evaluation protocol.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and explain the performance of adversarial attacks in inflectional languages, which is under-researched compared to non-inflectional languages like English.

**Method:** Developed a novel protocol inspired by mechanistic interpretability, utilizing Edge Attribution Patching (EAP) for evaluation with parallel task-specific corpora of inflected and syncretic text.

**Key Contributions:**

	1. Evaluation of adversarial attacks in inflectional languages
	2. Novel evaluation protocol using Edge Attribution Patching
	3. Development of a benchmark dataset for model behaviour analysis

**Result:** Established a benchmark using the MultiEmo dataset, identifying inflection-related elements in models and their responses to adversarial attacks.

**Limitations:** 

**Conclusion:** Inflection impacts model behaviour and robustness under adversarial attacks, highlighting the need for tailored evaluation methods in diverse linguistic contexts.

**Abstract:** Various techniques are used in the generation of adversarial examples, including methods such as TextBugger which introduce minor, hardly visible perturbations to words leading to changes in model behaviour. Another class of techniques involves substituting words with their synonyms in a way that preserves the text's meaning but alters its predicted class, with TextFooler being a prominent example of such attacks. Most adversarial example generation methods are developed and evaluated primarily on non-inflectional languages, typically English. In this work, we evaluate and explain how adversarial attacks perform in inflectional languages. To explain the impact of inflection on model behaviour and its robustness under attack, we designed a novel protocol inspired by mechanistic interpretability, based on Edge Attribution Patching (EAP) method. The proposed evaluation protocol relies on parallel task-specific corpora that include both inflected and syncretic variants of texts in two languages -- Polish and English. To analyse the models and explain the relationship between inflection and adversarial robustness, we create a new benchmark based on task-oriented dataset MultiEmo, enabling the identification of mechanistic inflection-related elements of circuits within the model and analyse their behaviour under attack.

</details>


### [22] [Enhanced Urdu Intent Detection with Large Language Models and Prototype-Informed Predictive Pipelines](https://arxiv.org/abs/2505.07857)

*Faiza Hassan, Summra Saleem, Kashif Javed, Muhammad Nabeel Asim, Abdur Rehman, Andreas Dengel*

**Main category:** cs.CL

**Keywords:** Urdu language, intent detection, contrastive learning, language models, prototype-informed attention

**Relevance Score:** 7

**TL;DR:** Introduces a contrastive learning approach for intent detection in Urdu, leveraging unlabeled data and enhancing LLMs with a prototype-informed attention mechanism.

**Read time:** 42 min

<details>
  <summary>Details</summary>

**Motivation:** The underdevelopment of intent detection for the Urdu language despite its prominence as the 10th most spoken language.

**Method:** Employs a unique contrastive learning approach to re-train pre-trained language models using unlabeled Urdu data, combined with a prototype-informed attention mechanism.

**Key Contributions:**

	1. Introduces a novel contrastive learning method for Urdu intent detection.
	2. Combines pre-trained LLMs with a prototype-informed attention mechanism.
	3. Evaluates the framework on benchmark datasets, outperforming existing predictors.

**Result:** Achieved significant F1-Scores of 83.28% and 98.25% on the ATIS dataset and 76.23% and 84.42% on Web Queries dataset using the proposed method.

**Limitations:** 

**Conclusion:** The proposed LLMPIA pipeline demonstrates substantial improvements over state-of-the-art predictors in intent detection for the Urdu language.

**Abstract:** Multifarious intent detection predictors are developed for different languages, including English, Chinese and French, however, the field remains underdeveloped for Urdu, the 10th most spoken language. In the realm of well-known languages, intent detection predictors utilize the strategy of few-shot learning and prediction of unseen classes based on the model training on seen classes. However, Urdu language lacks few-shot strategy based intent detection predictors and traditional predictors are focused on prediction of the same classes which models have seen in the train set. To empower Urdu language specific intent detection, this introduces a unique contrastive learning approach that leverages unlabeled Urdu data to re-train pre-trained language models. This re-training empowers LLMs representation learning for the downstream intent detection task. Finally, it reaps the combined potential of pre-trained LLMs and the prototype-informed attention mechanism to create a comprehensive end-to-end LLMPIA intent detection pipeline. Under the paradigm of proposed predictive pipeline, it explores the potential of 6 distinct language models and 13 distinct similarity computation methods. The proposed framework is evaluated on 2 public benchmark datasets, namely ATIS encompassing 5836 samples and Web Queries having 8519 samples. Across ATIS dataset under 4-way 1 shot and 4-way 5 shot experimental settings LLMPIA achieved 83.28% and 98.25% F1-Score and on Web Queries dataset produced 76.23% and 84.42% F1-Score, respectively. In an additional case study on the Web Queries dataset under same classes train and test set settings, LLMPIA outperformed state-of-the-art predictor by 53.55% F1-Score.

</details>


### [23] [Scaling Laws for Speculative Decoding](https://arxiv.org/abs/2505.07858)

*Siyuan Yan, Mo Zhu, Guo-qing Jiang, Jianfei Wang, Jiaxing Chen, Wentai Zhang, Xiang Liao, Xiao Cui, Chen Zhang, Zhuoran Song, Ran Zhu*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, scaling laws, LLM inference, machine learning

**Relevance Score:** 9

**TL;DR:** This study examines speculative decoding techniques in large language models (LLMs) to enhance reasoning tasks, leading to the development of Scylla, which significantly improves decoding efficiency.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient decoding in large language models is crucial for reasoning-intensive tasks, particularly in architectures that rely on extended chain-of-thought reasoning.

**Method:** The study investigates speculative decoding through dense LLM architectures and discovers Log-linear Scaling Laws relating to draft model acceptance rates, pretraining token volume, draft model capacity, and decoding batch size.

**Key Contributions:**

	1. Introduction of Log-linear Scaling Laws for decoding speed
	2. Development of Scylla for coordinated scaling of LLMs
	3. Empirical validation showing significant improvements in decoding efficiency

**Result:** Scylla achieves a 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, significantly improving both summarization and QA tasks while providing 2X decoding throughput improvements over EAGLE2.

**Limitations:** 

**Conclusion:** The findings validate the transformative potential of systematic scaling in enhancing LLM inference efficiency, promising advancements in practical applications.

**Abstract:** The escalating demand for efficient decoding in large language models (LLMs) is particularly critical for reasoning-intensive architectures like OpenAI-o3 and DeepSeek-R1, which depend on extended chain-of-thought reasoning. This study investigates speculative decoding techniques through dense LLM architectures to establish foundational insights for accelerating reasoning tasks. While speculative decoding methods leveraging parallel draft-verification cycles have emerged as promising acceleration techniques, the scaling laws governing decoding efficiency remain under-explored compared to conventional backbone LLMs developed through Pretraining->SFT->RLHF training paradigms. In this work, we discover Log-linear Scaling Laws (Theorem 1.1, 1.2 and 1.3) governing draft model acceptance rate (or decoding speed) across three dimensions: pretraining token volume, draft model capacity, and decoding batch size. Building on these laws, we achieve Scylla, which coordinates multi-dimensional scaling for popular LLMs (Llama2/3, Qwen2.5). Empirical validation shows Scylla achieves 1.5-2.2 higher acceptance rate than EAGLE2 and 0.3 higher than EAGLE3 at temperature T = 0, with peak performance gains on summarization and QA tasks (Figure 2). Industrial inference engine deployments demonstrate 2X decoding throughput improvements over EAGLE2 (Table 5), validating the transformative potential of systematic scaling for efficient LLM inference. Code will be released later.

</details>


### [24] [Boosting Performance on ARC is a Matter of Perspective](https://arxiv.org/abs/2505.07859)

*Daniel Franzen, Jan Disselhoff, David Hartmann*

**Main category:** cs.CL

**Keywords:** Large Language Models, Abstract Reasoning, Data Augmentation, ARC-AGI, Machine Learning

**Relevance Score:** 7

**TL;DR:** The paper addresses the limitations of large language models in abstract reasoning through the Abstraction and Reasoning Corpus (ARC-AGI) by implementing task-specific data augmentations and a depth-first search algorithm, achieving state-of-the-art results in transparency and low cost.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the limitations in abstract reasoning abilities of large language models highlighted by the ARC-AGI challenge.

**Method:** Employs task-specific data augmentations during training, generation, and scoring, and uses a depth-first search algorithm to develop a diverse set of candidate solutions, with the LLM also serving as a scorer for selecting the most promising outputs.

**Key Contributions:**

	1. Introduction of task-specific data augmentations in LLM training
	2. Use of a depth-first search algorithm for candidate solution generation
	3. Low inference cost with high transparency and reproducibility

**Result:** Achieves a score of 71.6% on the public ARC-AGI evaluation set, demonstrating high performance while maintaining transparency and low inference costs.

**Limitations:** 

**Conclusion:** The proposed method achieves state-of-the-art performance on ARC-AGI while being reproducible and cost-effective compared to other solutions.

**Abstract:** The Abstraction and Reasoning Corpus (ARC-AGI) poses a significant challenge for large language models (LLMs), exposing limitations in their abstract reasoning abilities. In this work, we leverage task-specific data augmentations throughout the training, generation, and scoring phases, and employ a depth-first search algorithm to generate diverse, high-probability candidate solutions. Furthermore, we utilize the LLM not only as a generator but also as a scorer, using its output probabilities to select the most promising solutions. Our method achieves a score of 71.6% (286.5/400 solved tasks) on the public ARC-AGI evaluation set, demonstrating state-of-the-art performance among publicly available approaches. While concurrent closed-source work has reported higher scores, our method distinguishes itself through its transparency, reproducibility, and remarkably low inference cost, averaging only around 2ct per task on readily available hardware (we assume a price of 36ct/hour for a Nvidia 4090 GPU).

</details>


### [25] [Scalable LLM Math Reasoning Acceleration with Low-rank Distillation](https://arxiv.org/abs/2505.07861)

*Harry Dong, Bilge Acun, Beidi Chen, Yuejie Chi*

**Main category:** cs.CL

**Keywords:** large language models, efficient inference, math reasoning, distillation, latency reduction

**Relevance Score:** 9

**TL;DR:** Caprese is a low-cost distillation method designed to enhance math reasoning capabilities in LLMs without degrading language performance, using minimal additional parameters and synthetic training samples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing efficient inference methods for LLMs often compromise math reasoning performance, which is critical for applications like education and scientific computations.

**Method:** Caprese employs a method of distillation focusing on feedforward blocks, maintaining original weights while adding only about 1% more parameters and utilizing 20K synthetic training samples to recover lost math capabilities.

**Key Contributions:**

	1. Introduction of Caprese, a novel low-cost distillation method for LLMs
	2. Ability to recover math capabilities without material impact on language tasks
	3. Reduction of active parameters and latency during generation tasks

**Result:** Caprese successfully recovers math capabilities lost due to efficient inference while also providing improvements in parameter efficiency and latency reductions in existing LLM architectures.

**Limitations:** 

**Conclusion:** The proposed method allows for LLMs to maintain their math reasoning abilities during efficient inference, showing promising results for both math and language tasks without significant trade-offs.

**Abstract:** Due to long generations, large language model (LLM) math reasoning demands significant computational resources and time. While many existing efficient inference methods have been developed with excellent performance preservation on language tasks, they often severely degrade math performance. In this paper, we propose Caprese, a low-cost distillation method to recover lost capabilities from deploying efficient inference methods, focused primarily in feedforward blocks. With original weights unperturbed, roughly 1% of additional parameters, and only 20K synthetic training samples, we are able to recover much if not all of the math capabilities lost from efficient inference for thinking LLMs and without harm to language tasks for instruct LLMs. Moreover, Caprese slashes the number of active parameters (~2B cut for Gemma 2 9B and Llama 3.1 8B) and integrates cleanly into existing model layers to reduce latency (>11% reduction to generate 2048 tokens with Qwen 2.5 14B) while encouraging response brevity.

</details>


### [26] [Graph Laplacian Wavelet Transformer via Learnable Spectral Decomposition](https://arxiv.org/abs/2505.07862)

*Andrew Kiruluta, Eric Lundy, Priscilla Burity*

**Main category:** cs.CL

**Keywords:** Graph Wavelet Transformer, self-attention, graph-structured sequence modeling

**Relevance Score:** 6

**TL;DR:** The Graph Wavelet Transformer (GWT) introduces a new architecture that uses a multi-scale wavelet transform to improve efficiency and interpretability in graph-structured sequence modeling by replacing the quadratic complexity of traditional self-attention mechanisms.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational and memory costs associated with traditional dot product self-attention in sequence to sequence models.

**Method:** The paper proposes the Graph Wavelet Transformer (GWT) architecture, which uses a learnable wavelet transform based on an explicit graph Laplacian derived from syntactic or semantic parses.

**Key Contributions:**

	1. Introduction of the Graph Wavelet Transformer (GWT) architecture.
	2. Improved efficiency through a learnable multi-scale wavelet transform.
	3. Enhanced interpretability and expressiveness in graph-structured sequence tasks.

**Result:** The GWT demonstrates a more efficient and interpretable alternative to quadratic self-attention for graph-structured sequence modeling, with enhanced performance in structured language tasks.

**Limitations:** 

**Conclusion:** The multi-scale spectral decomposition provided by GWT offers significant benefits in terms of efficiency and expressiveness for modeling sequences over graphs.

**Abstract:** Existing sequence to sequence models for structured language tasks rely heavily on the dot product self attention mechanism, which incurs quadratic complexity in both computation and memory for input length N. We introduce the Graph Wavelet Transformer (GWT), a novel architecture that replaces this bottleneck with a learnable, multi scale wavelet transform defined over an explicit graph Laplacian derived from syntactic or semantic parses. Our analysis shows that multi scale spectral decomposition offers an interpretable, efficient, and expressive alternative to quadratic self attention for graph structured sequence modeling.

</details>


### [27] [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)

*S. E Emedem, I. E Onyenwe, E. G Onyedinma*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, African languages, NLP, Deep Learning, Conditional Random Fields

**Relevance Score:** 6

**TL;DR:** This research introduces WAZOBIA-NER, a Named Entity Recognition system for Nigerian languages (Hausa, Yoruba, Igbo), leveraging machine learning and deep learning techniques to address data scarcity in under-resourced languages.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the significant gap in Named Entity Recognition for under-resourced African languages compared to more widely represented languages such as English and European languages.

**Method:** The study compiles annotated datasets for Hausa, Yoruba, and Igbo languages and employs machine learning techniques like Conditional Random Fields and deep learning models (BiLSTM, Bert, RNN) to recognize entities including persons, organizations, and locations. It also integrates OCR technology for text extraction from images.

**Key Contributions:**

	1. Development of WAZOBIA-NER specifically for Nigerian languages
	2. Compilation of annotated datasets to address data scarcity
	3. Integration of OCR technology for textual image processing.

**Result:** The WAZOBIA-NER system achieved a precision of 0.9511, recall of 0.9400, F1-score of 0.9564, and accuracy of 0.9301 across the three languages evaluated.

**Limitations:** 

**Conclusion:** The implementation of WAZOBIA-NER proves the viability of developing effective NER tools for under-resourced languages using modern NLP frameworks and transfer learning.

**Abstract:** Named Entity Recognition NER is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.

</details>


### [28] [QoSBERT: An Uncertainty-Aware Approach based on Pre-trained Language Models for Service Quality Prediction](https://arxiv.org/abs/2505.07863)

*Ziliang Wang, Xiaohong Zhang, Ze Shi Li, Meng Yan*

**Main category:** cs.CL

**Keywords:** Quality of Service, QoSBERT, semantic regression, uncertainty estimation, cloud services

**Relevance Score:** 5

**TL;DR:** QoSBERT reformulates QoS prediction as a semantic regression task using pre-trained language models, providing accurate predictions with uncertainty estimation.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate QoS prediction is critical for managing cloud services, yet traditional models lack confidence insights and rely on manual feature engineering.

**Method:** QoSBERT uses pre-trained language models for encoding service metadata as natural language descriptions and integrates Monte Carlo Dropout for uncertainty estimation, applying attentive pooling and a multilayer perceptron regressor.

**Key Contributions:**

	1. First framework to apply semantic regression for QoS prediction
	2. Integration of uncertainty estimation into QoS models
	3. Improved accuracy and robustness in low-resource settings

**Result:** QoSBERT achieves significant improvements in MAE and RMSE for QoS metrics while delivering well-calibrated confidence intervals.

**Limitations:** 

**Conclusion:** The proposed framework enhances the accuracy of service quality prediction and enables trustworthy, data-driven service optimization.

**Abstract:** Accurate prediction of Quality of Service (QoS) metrics is fundamental for selecting and managing cloud based services. Traditional QoS models rely on manual feature engineering and yield only point estimates, offering no insight into the confidence of their predictions. In this paper, we propose QoSBERT, the first framework that reformulates QoS prediction as a semantic regression task based on pre trained language models. Unlike previous approaches relying on sparse numerical features, QoSBERT automatically encodes user service metadata into natural language descriptions, enabling deep semantic understanding. Furthermore, we integrate a Monte Carlo Dropout based uncertainty estimation module, allowing for trustworthy and risk-aware service quality prediction, which is crucial yet underexplored in existing QoS models. QoSBERT applies attentive pooling over contextualized embeddings and a lightweight multilayer perceptron regressor, fine tuned jointly to minimize absolute error. We further exploit the resulting uncertainty estimates to select high quality training samples, improving robustness in low resource settings. On standard QoS benchmark datasets, QoSBERT achieves an average reduction of 11.7% in MAE and 6.7% in RMSE for response time prediction, and 6.9% in MAE for throughput prediction compared to the strongest baselines, while providing well calibrated confidence intervals for robust and trustworthy service quality estimation. Our approach not only advances the accuracy of service quality prediction but also delivers reliable uncertainty quantification, paving the way for more trustworthy, data driven service selection and optimization.

</details>


### [29] [Efficient Fairness Testing in Large Language Models: Prioritizing Metamorphic Relations for Bias Detection](https://arxiv.org/abs/2505.07870)

*Suavis Giramata, Madhusudan Srinivasan, Venkat Naidu Gudivada, Upulee Kanewala*

**Main category:** cs.CL

**Keywords:** Large Language Models, fairness, metamorphic testing, fault detection, bias

**Relevance Score:** 9

**TL;DR:** This paper presents a prioritization approach for metamorphic testing of LLMs to efficiently detect fairness issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs are increasingly deployed, there are significant concerns regarding fairness and biases in their outputs, necessitating effective testing strategies.

**Method:** The authors utilize a sentence diversity-based approach to compute and rank metamorphic relations (MRs) for prioritizing test cases in fairness testing.

**Key Contributions:**

	1. Introduction of diversity-based MR prioritization for fairness testing in LLMs
	2. Demonstrated 22% improvement in fault detection rates
	3. Reduced computational costs and time to first failure

**Result:** The proposed method improves fault detection rates by 22% over random prioritization and 12% over distance-based prioritization, while also reducing time to first failure.

**Limitations:** 

**Conclusion:** Diversity-based MR prioritization proves effective in enhancing fairness testing for LLMs while reducing computational costs.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in various applications, raising critical concerns about fairness and potential biases in their outputs. This paper explores the prioritization of metamorphic relations (MRs) in metamorphic testing as a strategy to efficiently detect fairness issues within LLMs. Given the exponential growth of possible test cases, exhaustive testing is impractical; therefore, prioritizing MRs based on their effectiveness in detecting fairness violations is crucial. We apply a sentence diversity-based approach to compute and rank MRs to optimize fault detection. Experimental results demonstrate that our proposed prioritization approach improves fault detection rates by 22% compared to random prioritization and 12% compared to distance-based prioritization, while reducing the time to the first failure by 15% and 8%, respectively. Furthermore, our approach performs within 5% of fault-based prioritization in effectiveness, while significantly reducing the computational cost associated with fault labeling. These results validate the effectiveness of diversity-based MR prioritization in enhancing fairness testing for LLMs.

</details>


### [30] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)

*Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song*

**Main category:** cs.CL

**Keywords:** LLM Psychometrics, evaluation methodologies, human-centered AI

**Relevance Score:** 9

**TL;DR:** This paper surveys the emerging field of LLM Psychometrics, addressing challenges in evaluating large language models using principles from psychometrics to enhance their human-like capabilities.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The need for more effective evaluation methodologies for large language models that capture human-like psychological constructs and emphasize human-centered evaluation.

**Method:** The paper synthesizes an interdisciplinary framework incorporating psychometric instruments and theories to systematically assess and strengthen LLM evaluation methods.

**Key Contributions:**

	1. Introduction of LLM Psychometrics as a new interdisciplinary field.
	2. Framework for evaluating LLMs using psychometric principles.
	3. Curated repository of LLM psychometric resources.

**Result:** It identifies key principles of benchmarking, expands evaluation beyond traditional metrics, and provides actionable insights for future development in the field.

**Limitations:** 

**Conclusion:** A comprehensive repository of resources related to LLM Psychometrics is provided, along with a call for the development of human-centered AI systems.

**Abstract:** The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [31] [Evaluating Financial Sentiment Analysis with Annotators Instruction Assisted Prompting: Enhancing Contextual Interpretation and Stock Prediction Accuracy](https://arxiv.org/abs/2505.07871)

*A M Muntasir Rahman, Ajim Uddin, Guiling "Grace" Wang*

**Main category:** cs.CL

**Keywords:** Financial Sentiment Analysis, Large Language Models, WallStreetBets

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel evaluation prompt for financial sentiment analysis (FSA) that enhances LLM performance by integrating detailed task instructions, providing a standardized understanding of sentiment across human and machine interpretations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Financial sentiment analysis poses unique challenges for LLMs due to the nuanced language in finance and subjective nature of sentiment classifications in existing datasets.

**Method:** The paper proposes the Annotators' Instruction Assisted Prompt (AIAP), which incorporates detailed task instructions for human annotators into prompts for LLMs to standardize sentiment understanding.

**Key Contributions:**

	1. Introduction of the AIAP for LLM sentiment analysis
	2. Development of the WSBS dataset from WallStreetBets
	3. Implementation of a sentiment-indexing method using model confidence scores

**Result:** With the AIAP, LLM performance improved significantly, demonstrating a performance enhancement of up to 9.08 in sentiment classification tasks using a new dataset derived from WallStreetBets.

**Limitations:** 

**Conclusion:** The research highlights the importance of context-aware approaches in financial sentiment analysis and suggests that better evaluation methods can improve machine learning outcomes in this domain.

**Abstract:** Financial sentiment analysis (FSA) presents unique challenges to LLMs that surpass those in typical sentiment analysis due to the nuanced language used in financial contexts. The prowess of these models is often undermined by the inherent subjectivity of sentiment classifications in existing benchmark datasets like Financial Phrasebank. These datasets typically feature undefined sentiment classes that reflect the highly individualized perspectives of annotators, leading to significant variability in annotations. This variability results in an unfair expectation for LLMs during benchmarking, where they are tasked to conjecture the subjective viewpoints of human annotators without sufficient context. In this paper, we introduce the Annotators' Instruction Assisted Prompt, a novel evaluation prompt designed to redefine the task definition of FSA for LLMs. By integrating detailed task instructions originally intended for human annotators into the LLMs' prompt framework, AIAP aims to standardize the understanding of sentiment across both human and machine interpretations, providing a fair and context-rich foundation for sentiment analysis. We utilize a new dataset, WSBS, derived from the WallStreetBets subreddit to demonstrate how AIAP significantly enhances LLM performance by aligning machine operations with the refined task definitions. Experimental results demonstrate that AIAP enhances LLM performance significantly, with improvements up to 9.08. This context-aware approach not only yields incremental gains in performance but also introduces an innovative sentiment-indexing method utilizing model confidence scores. This method enhances stock price prediction models and extracts more value from the financial sentiment analysis, underscoring the significance of WSB as a critical source of financial text. Our research offers insights into both improving FSA through better evaluation methods.

</details>


### [32] [The Sound of Populism: Distinct Linguistic Features Across Populist Variants](https://arxiv.org/abs/2505.07874)

*Yu Wang, Runxi Yu, Zhongyuan Wang, Jing He*

**Main category:** cs.CL

**Keywords:** Populism, Linguistic Inquiry and Word Count, RoBERTa, Political Rhetoric, Emotional Tones

**Relevance Score:** 4

**TL;DR:** This study integrates LIWC features with a fine-tuned RoBERTa model to analyze the auditory dimensions of populist rhetoric in U.S. presidential speeches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover how different populist dimensions manifest in political rhetoric through language tonalities.

**Method:** The study employs a fine-tuned RoBERTa model along with classic LIWC features to detect and analyze the emotional and stylistic tones of linguistic expressions of populism.

**Key Contributions:**

	1. Integration of LIWC features with RoBERTa for political rhetoric analysis
	2. Identification of key populist dimensions in speech
	3. Insights into emotional tonal shifts in populist rhetoric

**Result:** Findings reveal that populist rhetoric has a characteristic direct and assertive sound, with emotional tonal differences across left-wing, right-wing, and anti-elitist expressions.

**Limitations:** 

**Conclusion:** The study concludes that right-wing populism and people-centrism create more emotionally charged discourses compared to left-wing and anti-elitist variants.

**Abstract:** This study explores the sound of populism by integrating the classic Linguistic Inquiry and Word Count (LIWC) features, which capture the emotional and stylistic tones of language, with a fine-tuned RoBERTa model, a state-of-the-art context-aware language model trained to detect nuanced expressions of populism. This approach allows us to uncover the auditory dimensions of political rhetoric in U.S. presidential inaugural and State of the Union addresses. We examine how four key populist dimensions (i.e., left-wing, right-wing, anti-elitism, and people-centrism) manifest in the linguistic markers of speech, drawing attention to both commonalities and distinct tonal shifts across these variants. Our findings reveal that populist rhetoric consistently features a direct, assertive ``sound" that forges a connection with ``the people'' and constructs a charismatic leadership persona. However, this sound is not simply informal but strategically calibrated. Notably, right-wing populism and people-centrism exhibit a more emotionally charged discourse, resonating with themes of identity, grievance, and crisis, in contrast to the relatively restrained emotional tones of left-wing and anti-elitist expressions.

</details>


### [33] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)

*Yumou Wei, Paulo Carvalho, John Stamper*

**Main category:** cs.CL

**Keywords:** small language models, AI in education, equity in AI, knowledge component discovery, AIED

**Relevance Score:** 7

**TL;DR:** This vision paper argues for the use of small language models (SLMs) like Phi-2 in AI for education, highlighting their potential benefits over larger models like GPT.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the imbalance in the current focus on resource-intensive large language models (LLMs), advocating for the exploration of small language models (SLMs) to improve accessibility and equity in AI-powered education.

**Method:** The authors analyze the current body of work in AIED, focusing on the reliance on LLMs, and present evidence supporting the effectiveness of SLMs in knowledge component discovery.

**Key Contributions:**

	1. Highlights the risks of over-reliance on large language models in education AI.
	2. Demonstrates the effectiveness of small language models like Phi-2 in educational contexts.
	3. Calls for a shift in focus towards SLM-based solutions to enhance accessibility in educational technology.

**Result:** The findings indicate that SLMs can effectively address key challenges in education technology without the need for extensive prompting, demonstrating their viability as an alternative to larger models.

**Limitations:** 

**Conclusion:** The authors conclude that more emphasis should be placed on developing SLM-based approaches in AIED to ensure equitable access to AI tools, particularly for resource-constrained institutions.

**Abstract:** GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [34] [Recovering Event Probabilities from Large Language Model Embeddings via Axiomatic Constraints](https://arxiv.org/abs/2505.07883)

*Jian-Qiao Zhu, Haijiang Yan, Thomas L. Griffiths*

**Main category:** cs.CL

**Keywords:** Large Language Models, probability theory, variational autoencoder, latent space, event probabilities

**Relevance Score:** 8

**TL;DR:** This paper explores recovering coherent event probabilities from LLM embeddings using a variational autoencoder (VAE) that enforces axiomatic constraints of probability theory.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for coherent degrees of belief in event probabilities generated by LLMs, as current outputs violate probability axioms, which is problematic for rational decision-making under uncertainty.

**Method:** The authors propose using an extended variational autoencoder (VAE) that enforces axiomatic constraints like the additive rule of probability theory in the latent space of LLM embeddings.

**Key Contributions:**

	1. Proposed a novel method using VAE for recovering coherent probabilities from LLM embeddings
	2. Demonstrated improved coherence of event probabilities compared to standard outputs from LLMs
	3. Aligned results closely with true probabilities for complementary events

**Result:** Probabilities recovered from the embeddings exhibit greater coherence compared to those directly reported by LLMs, aligning closely with true probabilities, especially for complementary events.

**Limitations:** 

**Conclusion:** Enforcing axiomatic constraints in VAE can successfully recover coherent event probabilities from LLM embeddings, enhancing decision-making under uncertainty.

**Abstract:** Rational decision-making under uncertainty requires coherent degrees of belief in events. However, event probabilities generated by Large Language Models (LLMs) have been shown to exhibit incoherence, violating the axioms of probability theory. This raises the question of whether coherent event probabilities can be recovered from the embeddings used by the models. If so, those derived probabilities could be used as more accurate estimates in events involving uncertainty. To explore this question, we propose enforcing axiomatic constraints, such as the additive rule of probability theory, in the latent space learned by an extended variational autoencoder (VAE) applied to LLM embeddings. This approach enables event probabilities to naturally emerge in the latent space as the VAE learns to both reconstruct the original embeddings and predict the embeddings of semantically related events. We evaluate our method on complementary events (i.e., event A and its complement, event not-A), where the true probabilities of the two events must sum to 1. Experiment results on open-weight language models demonstrate that probabilities recovered from embeddings exhibit greater coherence than those directly reported by the corresponding models and align closely with the true probabilities.

</details>


### [35] [Development of a WAZOBIA-Named Entity Recognition System](https://arxiv.org/abs/2505.07884)

*S. E Emedem, I. E Onyenwe, E. G Onyedinma*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, African languages, Machine Learning

**Relevance Score:** 6

**TL;DR:** The WAZOBIA-NER system is developed for Named Entity Recognition (NER) in Hausa, Yoruba, and Igbo, addressing data scarcity in African languages through annotated datasets and advanced machine learning techniques.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in NER systems for under-resourced African languages, primarily focused on English and a few global languages.

**Method:** The study compiles annotated datasets and applies machine learning techniques including Conditional Random Fields (CRF), BiLSTM, Bert, and RNNs for entity recognition, utilizing OCR for text processing.

**Key Contributions:**

	1. Development of the WAZOBIA-NER system for African languages
	2. Creation of annotated datasets for Hausa, Yoruba, and Igbo
	3. Implementation of advanced ML techniques in NER for under-resourced languages

**Result:** The WAZOBIA-NER system achieved performance metrics of 0.9511 precision, 0.9400 recall, 0.9564 F1-score, and 0.9301 accuracy across the three languages evaluated.

**Limitations:** 

**Conclusion:** The study demonstrates that effective NER tools can be built for under-resourced African languages using modern NLP frameworks and transfer learning.

**Abstract:** Named Entity Recognition NER is very crucial for various natural language processing applications, including information extraction, machine translation, and sentiment analysis. Despite the ever-increasing interest in African languages within computational linguistics, existing NER systems focus mainly on English, European, and a few other global languages, leaving a significant gap for under-resourced languages. This research presents the development of a WAZOBIA-NER system tailored for the three most prominent Nigerian languages: Hausa, Yoruba, and Igbo. This research begins with a comprehensive compilation of annotated datasets for each language, addressing data scarcity and linguistic diversity challenges. Exploring the state-of-the-art machine learning technique, Conditional Random Fields (CRF) and deep learning models such as Bidirectional Long Short-Term Memory (BiLSTM), Bidirectional Encoder Representation from Transformers (Bert) and fine-tune with a Recurrent Neural Network (RNN), the study evaluates the effectiveness of these approaches in recognizing three entities: persons, organizations, and locations. The system utilizes optical character recognition (OCR) technology to convert textual images into machine-readable text, thereby enabling the Wazobia system to accept both input text and textual images for extraction purposes. The system achieved a performance of 0.9511 in precision, 0.9400 in recall, 0.9564 in F1-score, and 0.9301 in accuracy. The model's evaluation was conducted across three languages, with precision, recall, F1-score, and accuracy as key assessment metrics. The Wazobia-NER system demonstrates that it is feasible to build robust NER tools for under-resourced African languages using current NLP frameworks and transfer learning.

</details>


### [36] [PLHF: Prompt Optimization with Few-Shot Human Feedback](https://arxiv.org/abs/2505.07886)

*Chun-Pai Yang, Kan Zheng, Shou-De Lin*

**Main category:** cs.CL

**Keywords:** prompt optimization, large language models, human feedback

**Relevance Score:** 9

**TL;DR:** PLHF is a few-shot prompt optimization framework for LLMs that utilizes a specific evaluator module for quality assessment, outperforming existing methods with just a single round of human feedback.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for prompt optimization struggle with output quality assessment when conventional metrics are unavailable.

**Method:** PLHF employs a specific evaluator module to estimate output quality and optimize prompts effectively with minimal human input.

**Key Contributions:**

	1. Introduction of the PLHF framework for prompt optimization
	2. Utilization of a specific evaluator module to assess output quality
	3. Demonstration of improved performance on public and industrial datasets

**Result:** Empirical results demonstrate that PLHF outperforms prior output grading strategies on various datasets.

**Limitations:** 

**Conclusion:** PLHF simplifies prompt optimization for LLMs by requiring only one round of feedback while achieving superior results.

**Abstract:** Automatic prompt optimization frameworks are developed to obtain suitable prompts for large language models (LLMs) with respect to desired output quality metrics. Although existing approaches can handle conventional tasks such as fixed-solution question answering, defining the metric becomes complicated when the output quality cannot be easily assessed by comparisons with standard golden samples. Consequently, optimizing the prompts effectively and efficiently without a clear metric becomes a critical challenge. To address the issue, we present PLHF (which stands for "P"rompt "L"earning with "H"uman "F"eedback), a few-shot prompt optimization framework inspired by the well-known RLHF technique. Different from naive strategies, PLHF employs a specific evaluator module acting as the metric to estimate the output quality. PLHF requires only a single round of human feedback to complete the entire prompt optimization process. Empirical results on both public and industrial datasets show that PLHF outperforms prior output grading strategies for LLM prompt optimizations.

</details>


### [37] [Implementing Long Text Style Transfer with LLMs through Dual-Layered Sentence and Paragraph Structure Extraction and Mapping](https://arxiv.org/abs/2505.07888)

*Yusen Wu, Xiaotie Deng*

**Main category:** cs.CL

**Keywords:** style transfer, large language models, zero-shot learning, semantic adaptation, structural coherence

**Relevance Score:** 8

**TL;DR:** This paper presents ZeroStylus, a hierarchical framework for long-text style transfer using zero-shot learning of large language models, focusing on sentence and paragraph-level coherence without requiring parallel corpora.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To effectively perform long-text style transfer while maintaining syntactic and semantic integrity and structural coherence.

**Method:** The framework consists of two phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching.

**Key Contributions:**

	1. Introduction of ZeroStylus framework for long-text style transfer
	2. Demonstration of the importance of both sentence and paragraph-level adaptations
	3. Significant experimental improvements over existing methods

**Result:** Experimental evaluations show that the structured rewriting approach outperforms direct prompting methods, achieving an average score of 6.90 compared to 6.70 across various metrics.

**Limitations:** 

**Conclusion:** ZeroStylus enables coherent style transfer for long texts, significantly enhancing content preservation during the transfer process without LLM fine-tuning.

**Abstract:** This paper addresses the challenge in long-text style transfer using zero-shot learning of large language models (LLMs), proposing a hierarchical framework that combines sentence-level stylistic adaptation with paragraph-level structural coherence. We argue that in the process of effective paragraph-style transfer, to preserve the consistency of original syntactic and semantic information, it is essential to perform style transfer not only at the sentence level but also to incorporate paragraph-level semantic considerations, while ensuring structural coherence across inter-sentential relationships. Our proposed framework, ZeroStylus, operates through two systematic phases: hierarchical template acquisition from reference texts and template-guided generation with multi-granular matching. The framework dynamically constructs sentence and paragraph template repositories, enabling context-aware transformations while preserving inter-sentence logical relationships. Experimental evaluations demonstrate significant improvements over baseline methods, with structured rewriting achieving 6.90 average score compared to 6.70 for direct prompting approaches in tri-axial metrics assessing style consistency, content preservation, and expression quality. Ablation studies validate the necessity of both template hierarchies during style transfer, showing higher content preservation win rate against sentence-only approaches through paragraph-level structural encoding, as well as direct prompting method through sentence-level pattern extraction and matching. The results establish new capabilities for coherent long-text style transfer without requiring parallel corpora or LLM fine-tuning.

</details>


### [38] [BioProBench: Comprehensive Dataset and Benchmark in Biological Protocol Understanding and Reasoning](https://arxiv.org/abs/2505.07889)

*Yuyang Liu, Liuzhenghao Lv, Xiancheng Zhang, Li Yuan, Yonghong Tian*

**Main category:** cs.CL

**Keywords:** Biological protocols, LLMs, Benchmarking, Human-Computer Interaction, Automated procedures

**Relevance Score:** 6

**TL;DR:** BioProBench is a comprehensive benchmark for evaluating LLMs on biological protocol understanding, featuring five core tasks and highlighting significant challenges in procedural reasoning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited evaluation of LLMs on biological protocols, which are critical for reproducible and safe life science research.

**Method:** Introduced BioProBench, a large-scale integrated multi-task benchmark encompassing protocol QA, step ordering, error correction, protocol generation, and protocol reasoning, based on 27K original protocols.

**Key Contributions:**

	1. Introduction of BioProBench, a versatile benchmark for biological protocols
	2. Evaluation of diverse LLMs highlighting their limitations in procedural reasoning
	3. Provision of 27K biological protocols yielding 556K structured instances for research use

**Result:** Evaluation of 12 LLMs showed top models perform well on basic understanding, but struggle with complex reasoning and structured generation, revealing disparities in model performance.

**Limitations:** Findings suggest that current models, particularly smaller bio-specific ones, struggle with deep reasoning tasks and complex procedural content.

**Conclusion:** BioProBench identifies key challenges in LLM performance on procedural biological content and serves as a framework for improving AI systems in this domain.

**Abstract:** Biological protocols are fundamental to reproducible and safe life science research. While LLMs excel on general tasks, their systematic evaluation on these highly specialized, accuracy-critical, and inherently procedural texts remains limited. In this work, we present BioProBench, the first large-scale, integrated multi-task benchmark for biological protocol understanding and reasoning. While limited benchmarks have touched upon specific aspects like protocol QA, BioProBench provides a comprehensive suite of five core tasks: Protocol Question Answering, Step Ordering, Error Correction, Protocol Generation, and Protocol Reasoning, enabling a holistic evaluation of LLMs on procedural biological texts. Built upon 27K original protocols, it yields nearly 556K high-quality structured instances. We evaluate 12 mainstream open/closed-source LLMs on BioProBench. Experimental results reveal that while top models preform well on surface understanding tasks, struggle significantly with deep reasoning and structured generation tasks like ordering and generation. Furthermore, model comparisons reveal diverse performance: certain open-source models approach closed-source levels on some tasks, yet bio-specific small models lag behind general LLMs, indicating limitations on complex procedural content. Overall, our findings underscore that procedural reasoning within biological protocols represents a significant challenge for current LLMs. BioProBench serves as a standardized framework to diagnose these specific limitations and guide the development of AI systems better equipped for safely automating complex scientific procedures. The code and data are available at: https://github.com/YuyangSunshine/bioprotocolbench and https://huggingface.co/datasets/GreatCaptainNemo/BioProBench.

</details>


### [39] [TSLFormer: A Lightweight Transformer Model for Turkish Sign Language Recognition Using Skeletal Landmarks](https://arxiv.org/abs/2505.07890)

*Kutay Ertürk, Furkan Altınışık, İrem Sarıaltın, Ömer Nezih Gerek*

**Main category:** cs.CL

**Keywords:** Sign Language Recognition, Transformers, Human-Computer Interaction, 3D Joint Positions, Machine Learning

**Relevance Score:** 4

**TL;DR:** Introduction of TSLFormer, a Turkish Sign Language recognition model using 3D joint positions for efficient gesture recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Development of a robust sign language recognition system for real-time communication for hearing-impaired individuals.

**Method:** Utilizes 3D joint positions from Google's Mediapipe library, treating sign gestures as sequence-to-sequence translations with a transformer model to capture temporal co-occurrence.

**Key Contributions:**

	1. Introduction of a light model for TSL recognition
	2. Efficient processing using joint positions
	3. Real-time application potential for assistive communication

**Result:** TSLFormer achieved competitive performance on the AUTSL dataset with over 36,000 samples, demonstrating efficiency in recognizing 227 different words.

**Limitations:** The approach may be limited to the quality of joint detection and might not generalize to other sign languages without modifications.

**Conclusion:** Joint-based input allows for effective real-time recognition, supporting mobile and assistive technologies for the hearing-impaired.

**Abstract:** This study presents TSLFormer, a light and robust word-level Turkish Sign Language (TSL) recognition model that treats sign gestures as ordered, string-like language. Instead of using raw RGB or depth videos, our method only works with 3D joint positions - articulation points - extracted using Google's Mediapipe library, which focuses on the hand and torso skeletal locations. This creates efficient input dimensionality reduction while preserving important semantic gesture information.   Our approach revisits sign language recognition as sequence-to-sequence translation, inspired by the linguistic nature of sign languages and the success of transformers in natural language processing. Since TSLFormer uses the self-attention mechanism, it effectively captures temporal co-occurrence within gesture sequences and highlights meaningful motion patterns as words unfold.   Evaluated on the AUTSL dataset with over 36,000 samples and 227 different words, TSLFormer achieves competitive performance with minimal computational cost. These results show that joint-based input is sufficient for enabling real-time, mobile, and assistive communication systems for hearing-impaired individuals.

</details>


### [40] [TrumorGPT: Graph-Based Retrieval-Augmented Large Language Model for Fact-Checking](https://arxiv.org/abs/2505.07891)

*Ching Nam Hang, Pei-Duo Yu, Chee Wei Tan*

**Main category:** cs.CL

**Keywords:** fact-checking, health misinformation, generative AI, knowledge graph, language models

**Relevance Score:** 9

**TL;DR:** TrumorGPT is a novel AI tool for fact-checking health-related rumors, using LLMs and GraphRAG for improved accuracy and up-to-date information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To combat the spread of health-related misinformation and enhance trust in digital information.

**Method:** The paper introduces TrumorGPT, which employs a large language model with few-shot learning and graph-based retrieval-augmented generation for fact-checking.

**Key Contributions:**

	1. Introduction of TrumorGPT for health rumor fact-checking
	2. Use of GraphRAG to tackle LLM hallucination issues
	3. Demonstrated superior performance on healthcare datasets

**Result:** TrumorGPT shows superior performance in distinguishing true health rumors from misinformation, using updated health knowledge graphs.

**Limitations:** 

**Conclusion:** TrumorGPT represents a significant advancement in the fight against health-related misinformation by integrating recent data for effective fact-checking.

**Abstract:** In the age of social media, the rapid spread of misinformation and rumors has led to the emergence of infodemics, where false information poses a significant threat to society. To combat this issue, we introduce TrumorGPT , a novel generative artificial intelligence solution designed for fact-checking in the health domain. TrumorGPT aims to distinguish "trumors", which are health-related rumors that turn out to be true, providing a crucial tool in differentiating between mere speculation and verified facts. This framework leverages a large language model (LLM) with few-shot learning for semantic health knowledge graph construction and semantic reasoning. TrumorGPT incorporates graph-based retrieval-augmented generation (GraphRAG) to address the hallucination issue common in LLMs and the limitations of static training data. GraphRAG involves accessing and utilizing information from regularly updated semantic health knowledge graphs that consist of the latest medical news and health information, ensuring that fact-checking by TrumorGPT is based on the most recent data. Evaluating with extensive healthcare datasets, TrumorGPT demonstrates superior performance in fact-checking for public health claims. Its ability to effectively conduct fact-checking across various platforms marks a critical step forward in the fight against health-related misinformation, enhancing trust and accuracy in the digital information age.

</details>


### [41] [LongCodeBench: Evaluating Coding LLMs at 1M Context Windows](https://arxiv.org/abs/2505.07897)

*Stefano Rando, Luca Romani, Alessio Sampieri, Yuta Kyuragi, Luca Franco, Fabio Galasso, Tatsunori Hashimoto, John Yang*

**Main category:** cs.CL

**Keywords:** LongCodeBench, long-context models, code comprehension, bug fixing, LLM evaluation

**Relevance Score:** 8

**TL;DR:** The paper introduces LongCodeBench (LCB), a benchmark for testing long-context coding capabilities of LLMs using real-world GitHub issues for comprehension and bug fixing tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in creating realistic benchmarks for long-context models due to their extreme context sizes, which have outpaced the development of suitable evaluation methods.

**Method:** The authors developed LongCodeBench (LCB), consisting of QA and bug fixing tasks based on real-world GitHub issues, and stratified the benchmark based on task complexity.

**Key Contributions:**

	1. Introduction of LongCodeBench (LCB) as a benchmark for evaluating LLMs in coding tasks with long contexts
	2. Construction of QA and bug fixing tasks from real-world GitHub issues
	3. Evaluation of multiple models, revealing the persistent weakness in long-context comprehension and repair tasks

**Result:** All models tested showed significant weaknesses in handling long-context tasks, with notable performance drops observed across different models when faced with these benchmarks.

**Limitations:** The benchmark may not cover all types of coding scenarios and focuses primarily on GitHub issues.

**Conclusion:** Long contextual understanding remains a critical area needing improvement across large language models, as evidence by substantial performance declines in their evaluations.

**Abstract:** Context lengths for models have grown rapidly, from thousands to millions of tokens in just a few years. The extreme context sizes of modern long-context models have made it difficult to construct realistic long-context benchmarks -- not only due to the cost of collecting million-context tasks but also in identifying realistic scenarios that require significant contexts. We identify code comprehension and repair as a natural testbed and challenge task for long-context models and introduce LongCodeBench (LCB), a benchmark to test LLM coding abilities in long-context scenarios. Our benchmark tests both the comprehension and repair capabilities of LCLMs in realistic and important settings by drawing from real-world GitHub issues and constructing QA (LongCodeQA) and bug fixing (LongSWE-Bench) tasks. We carefully stratify the complexity of our benchmark, enabling us to evaluate models across different scales -- ranging from Qwen2.5 14B Instruct to Google's flagship Gemini model. We find that long-context remains a weakness for all models, with performance drops such as from 29% to 3% for Claude 3.5 Sonnet, or from 70.2% to 40% for Qwen2.5.

</details>


### [42] [DeltaEdit: Enhancing Sequential Editing in Large Language Models by Controlling Superimposed Noise](https://arxiv.org/abs/2505.07899)

*Ding Cao, Yuchen Cai, Rongxi Guo, Xuesong He, Guiquan Liu*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, DeltaEdit

**Relevance Score:** 8

**TL;DR:** DeltaEdit is a novel method for sequential knowledge editing in large language models that addresses the decline in edit success rates due to the accumulation of superimposed noise problem by optimizing update parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To continuously update knowledge in large language models cost-effectively while preventing outdated or incorrect information generation.

**Method:** DeltaEdit employs a dynamic orthogonal constraints strategy to reduce interference between edits and optimize update parameters.

**Key Contributions:**

	1. Introduction of the accumulation of superimposed noise problem
	2. Development of DeltaEdit with dynamic orthogonal constraints
	3. Demonstration of superior performance in extensive editing scenarios

**Result:** DeltaEdit shows significant improvement in edit success rates and retention of generalization capabilities, maintaining stable model performance over extensive sequential editing.

**Limitations:** 

**Conclusion:** By mitigating the accumulation of superimposed noise, DeltaEdit ensures more reliable and effective sequential knowledge editing in large language models.

**Abstract:** Sequential knowledge editing techniques aim to continuously update the knowledge in large language models at a low cost, preventing the models from generating outdated or incorrect information. However, existing sequential editing methods suffer from a significant decline in editing success rates after long-term editing. Through theoretical analysis and experiments, we identify that as the number of edits increases, the model's output increasingly deviates from the desired target, leading to a drop in editing success rates. We refer to this issue as the accumulation of superimposed noise problem. To address this, we identify the factors contributing to this deviation and propose DeltaEdit, a novel method that optimizes update parameters through a dynamic orthogonal constraints strategy, effectively reducing interference between edits to mitigate deviation. Experimental results demonstrate that DeltaEdit significantly outperforms existing methods in edit success rates and the retention of generalization capabilities, ensuring stable and reliable model performance even under extensive sequential editing.

</details>


### [43] [SEM: Reinforcement Learning for Search-Efficient Large Language Models](https://arxiv.org/abs/2505.07903)

*Zeyang Sha, Shiwen Cui, Weiqiang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Search Optimization, External Knowledge, Reasoning Efficiency

**Relevance Score:** 8

**TL;DR:** This paper introduces SEM, a post-training reinforcement learning framework for Large Language Models that optimizes when to use external search tools versus internal knowledge, significantly improving reasoning efficiency and reducing redundant searches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing capabilities of Large Language Models necessitate improvements in their ability to discern when to invoke external tools like search engines versus relying on internal knowledge, to avoid inefficiencies and high costs.

**Method:** The proposed framework, SEM, utilizes a balanced dataset derived from MuSiQue and MMLU to train LLMs using a structured reasoning template and Group Relative Policy Optimization (GRPO), focusing on optimizing search behavior through a tailored reward function.

**Key Contributions:**

	1. Introduction of SEM, a framework for optimizing LLM search behavior.
	2. Use of a balanced dataset combining MuSiQue and MMLU for effective training.
	3. Development of a structured reasoning template and a new reward function for model training.

**Result:** Experimental results show that SEM significantly reduces redundant search operations and enhances answer accuracy across various benchmarks.

**Limitations:** The study may have limitations related to the specific datasets used and the generalizability of the findings to other LLM architectures or domains.

**Conclusion:** The SEM framework not only improves reasoning efficiency but also enables LLMs to effectively decide when to call for external knowledge, thus enhancing their overall performance.

**Abstract:** Recent advancements in Large Language Models(LLMs) have demonstrated their capabilities not only in reasoning but also in invoking external tools, particularly search engines. However, teaching models to discern when to invoke search and when to rely on their internal knowledge remains a significant challenge. Existing reinforcement learning approaches often lead to redundant search behaviors, resulting in inefficiencies and over-cost. In this paper, we propose SEM, a novel post-training reinforcement learning framework that explicitly trains LLMs to optimize search usage. By constructing a balanced dataset combining MuSiQue and MMLU, we create scenarios where the model must learn to distinguish between questions it can answer directly and those requiring external retrieval. We design a structured reasoning template and employ Group Relative Policy Optimization(GRPO) to post-train the model's search behaviors. Our reward function encourages accurate answering without unnecessary search while promoting effective retrieval when needed. Experimental results demonstrate that our method significantly reduces redundant search operations while maintaining or improving answer accuracy across multiple challenging benchmarks. This framework advances the model's reasoning efficiency and extends its capability to judiciously leverage external knowledge.

</details>


### [44] [Re$^2$: A Consistency-ensured Dataset for Full-stage Peer Review and Multi-turn Rebuttal Discussions](https://arxiv.org/abs/2505.07920)

*Daoze Zhang, Zhijian Bao, Sihang Du, Zhiyi Zhao, Kuangling Zhang, Dezheng Bao, Yang Yang*

**Main category:** cs.CL

**Keywords:** peer review, large language models, dataset, rebuttal, AI

**Relevance Score:** 9

**TL;DR:** This paper presents a new dataset, Re^2, aimed at improving peer review processes in AI by providing a comprehensive collection of initial submissions and reviewer comments, along with a framework for interactive LLM applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in submission volume in the AI field has led to peer review system stress, resulting in reviewer shortages and declines in review quality. Existing peer review datasets lack diversity, quality, and support for dynamic interactions, necessitating a new solution.

**Method:** The authors introduce the Re^2 dataset, which includes 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals sourced from multiple conferences and workshops. The rebuttal processes are framed as multi-turn conversations for better interaction and guidance.

**Key Contributions:**

	1. Creation of the Re^2 dataset with extensive peer review data
	2. Support for multi-turn conversations in reviewer-author interactions
	3. Provision of tools for authors to self-evaluate their work prior to submission

**Result:** The Re^2 dataset addresses the limitations of current peer review data by ensuring diversity and quality while supporting interactive dialogue, thus aiming to improve the manuscript refinement process for authors and reduce workload for reviewers.

**Limitations:** 

**Conclusion:** The introduction of the Re^2 dataset provides a valuable resource for enhancing peer review practices, particularly by utilizing LLMs for better manuscript assessments and author guidance, potentially improving the overall quality of submissions in academic AI.

**Abstract:** Peer review is a critical component of scientific progress in the fields like AI, but the rapid increase in submission volume has strained the reviewing system, which inevitably leads to reviewer shortages and declines review quality. Besides the growing research popularity, another key factor in this overload is the repeated resubmission of substandard manuscripts, largely due to the lack of effective tools for authors to self-evaluate their work before submission. Large Language Models (LLMs) show great promise in assisting both authors and reviewers, and their performance is fundamentally limited by the quality of the peer review data. However, existing peer review datasets face three major limitations: (1) limited data diversity, (2) inconsistent and low-quality data due to the use of revised rather than initial submissions, and (3) insufficient support for tasks involving rebuttal and reviewer-author interactions. To address these challenges, we introduce the largest consistency-ensured peer review and rebuttal dataset named Re^2, which comprises 19,926 initial submissions, 70,668 review comments, and 53,818 rebuttals from 24 conferences and 21 workshops on OpenReview. Moreover, the rebuttal and discussion stage is framed as a multi-turn conversation paradigm to support both traditional static review tasks and dynamic interactive LLM assistants, providing more practical guidance for authors to refine their manuscripts and helping alleviate the growing review burden. Our data and code are available in https://anonymous.4open.science/r/ReviewBench_anon/.

</details>


### [45] [Assessing and Mitigating Medical Knowledge Drift and Conflicts in Large Language Models](https://arxiv.org/abs/2505.07968)

*Weiyi Wu, Xinwen Xu, Chongyang Gao, Xingjian Diao, Siting Li, Lucas A. Salas, Jiang Gui*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare, clinical guidelines, concept drift, mitigation strategies

**Relevance Score:** 9

**TL;DR:** This study assesses how Large Language Models (LLMs) handle evolving clinical guidelines and introduces the DriftMedQA benchmark, revealing issues with outdated recommendations and internal inconsistencies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs in health care necessitates their adaptation to rapidly evolving medical knowledge, as outdated or contradictory treatment suggestions pose significant challenges.

**Method:** The study developed the DriftMedQA benchmark to simulate guideline evolution and evaluated the temporal reliability of various LLMs across 4,290 scenarios.

**Key Contributions:**

	1. Introduction of the DriftMedQA benchmark for assessing LLMs in medical contexts
	2. Demonstration of difficulties LLMs face with outdated and conflicting treatment suggestions
	3. Evaluation of mitigation strategies that improve model performance over time

**Result:** The evaluation revealed that LLMs struggled with rejecting outdated recommendations and often endorsed conflicting guidance. Two mitigation strategies were explored: Retrieval-Augmented Generation and Direct Preference Optimization, both improving performance, with their combination yielding the best results.

**Limitations:** The study may not comprehensively cover all possible LLMs or scenarios outside the benchmark framework.

**Conclusion:** Enhancing the robustness of LLMs to temporal shifts is essential to ensure their reliability in clinical practice, particularly as medical guidelines evolve.

**Abstract:** Large Language Models (LLMs) have great potential in the field of health care, yet they face great challenges in adapting to rapidly evolving medical knowledge. This can lead to outdated or contradictory treatment suggestions. This study investigated how LLMs respond to evolving clinical guidelines, focusing on concept drift and internal inconsistencies. We developed the DriftMedQA benchmark to simulate guideline evolution and assessed the temporal reliability of various LLMs. Our evaluation of seven state-of-the-art models across 4,290 scenarios demonstrated difficulties in rejecting outdated recommendations and frequently endorsing conflicting guidance. Additionally, we explored two mitigation strategies: Retrieval-Augmented Generation and preference fine-tuning via Direct Preference Optimization. While each method improved model performance, their combination led to the most consistent and reliable results. These findings underscore the need to improve LLM robustness to temporal shifts to ensure more dependable applications in clinical practice.

</details>


### [46] [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/abs/2408.09030)

*Alvin Po-Chun Chen, Dananjay Srinivas, Alexandra Barry, Maksim Seniw, Maria Leonor Pacheco*

**Main category:** cs.CL

**Keywords:** NLP, qualitative research, evaluation framework, collaboration, data analysis

**Relevance Score:** 6

**TL;DR:** This paper proposes an evaluation framework for NLP-assisted qualitative data analysis tools, analyzing the impact of collaboration strategies on their outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a unified evaluation framework for NLP-assisted qualitative research tools.

**Method:** The authors evaluate two NLP-assisted qualitative research tools under different collaboration strategies (synchronous vs. asynchronous) to assess their impact on research outcomes.

**Key Contributions:**

	1. Development of a unified evaluation framework for NLP-assisted qualitative tools
	2. Analysis of the impact of collaboration strategies on tool outcomes
	3. Comparative assessment of tool outputs based on collaboration type

**Result:** The study finds significant differences in the consistency, cohesiveness, and correctness of the outputs based on the collaboration strategy employed.

**Limitations:** 

**Conclusion:** An evaluation framework can help understand how tool use and collaboration strategies influence qualitative data analysis outcomes.

**Abstract:** NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.

</details>


### [47] [Task-Adaptive Semantic Communications with Controllable Diffusion-based Data Regeneration](https://arxiv.org/abs/2505.07980)

*Fupei Guo, Achintha Wijesinghe, Songyang Zhang, Zhi Ding*

**Main category:** cs.CL

**Keywords:** semantic communications, diffusion models, task-adaptive framework, bandwidth efficiency, attention mechanism

**Relevance Score:** 6

**TL;DR:** This paper proposes a novel task-adaptive semantic communication framework using diffusion models to enhance bandwidth efficiency by conveying meaningful semantic information for various downstream tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve bandwidth efficiency in next-generation networking by focusing on semantic meanings instead of bit-wise data delivery.

**Method:** The framework uses diffusion models to deliver deep-compressed general semantic representations, which are adjusted based on feedback from task-specific demands identified by the receiver.

**Key Contributions:**

	1. Novel task-adaptive framework for semantic communication
	2. Utilization of diffusion models for semantic message delivery
	3. Integration of attention mechanisms for improved alignment with receiver objectives.

**Result:** The method effectively adaptively preserves critical task-relevant information while maintaining high compression efficiency.

**Limitations:** 

**Conclusion:** The proposed framework shows a significant improvement in semantic communication by dynamically updating message delivery according to specific tasks.

**Abstract:** Semantic communications represent a new paradigm of next-generation networking that shifts bit-wise data delivery to conveying the semantic meanings for bandwidth efficiency. To effectively accommodate various potential downstream tasks at the receiver side, one should adaptively convey the most critical semantic information. This work presents a novel task-adaptive semantic communication framework based on diffusion models that is capable of dynamically adjusting the semantic message delivery according to various downstream tasks. Specifically, we initialize the transmission of a deep-compressed general semantic representation from the transmitter to enable diffusion-based coarse data reconstruction at the receiver. The receiver identifies the task-specific demands and generates textual prompts as feedback. Integrated with the attention mechanism, the transmitter updates the semantic transmission with more details to better align with the objectives of the intended receivers. Our test results demonstrate the efficacy of the proposed method in adaptively preserving critical task-relevant information for semantic communications while preserving high compression efficiency.

</details>


### [48] [Large Language Models and Arabic Content: A Review](https://arxiv.org/abs/2505.08004)

*Haneh Rhel, Dmitri Roussinov*

**Main category:** cs.CL

**Keywords:** Arabic NLP, Large Language Models, Finetuning, Prompt Engineering, NLP Applications

**Relevance Score:** 7

**TL;DR:** This paper reviews the impact of Large Language Models (LLMs) on Arabic NLP, discussing early Arabic LLMs, applications, fine-tuning, and prompt engineering techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the scarcity of resources in Arabic NLP despite the language's widespread use, and the challenges it faces due to its complexities.

**Method:** This study surveys existing literature on Arabic LLMs, analyzes early pre-trained models, and discusses the application of fine-tuning and prompt engineering in enhancing model performance.

**Key Contributions:**

	1. Overview of early Arabic LLMs
	2. Discussion on challenges in Arabic NLP
	3. Insights on techniques for enhancing LLM performance

**Result:** It highlights the success of multilingual LLMs in addressing various Arabic NLP tasks and notes a rising trend in the adoption of LLMs for Arabic content.

**Limitations:** 

**Conclusion:** The findings underscore the importance of further development and resources for Arabic NLP to leverage LLM capabilities effectively.

**Abstract:** Over the past three years, the rapid advancement of Large Language Models (LLMs) has had a profound impact on multiple areas of Artificial Intelligence (AI), particularly in Natural Language Processing (NLP) across diverse languages, including Arabic. Although Arabic is considered one of the most widely spoken languages across 27 countries in the Arabic world and used as a second language in some other non-Arabic countries as well, there is still a scarcity of Arabic resources, datasets, and tools. Arabic NLP tasks face various challenges due to the complexities of the Arabic language, including its rich morphology, intricate structure, and diverse writing standards, among other factors. Researchers have been actively addressing these challenges, demonstrating that pre-trained Large Language Models (LLMs) trained on multilingual corpora achieve significant success in various Arabic NLP tasks. This study provides an overview of using large language models (LLMs) for the Arabic language, highlighting early pre-trained Arabic Language models across various NLP applications and their ability to handle diverse Arabic content tasks and dialects. It also provides an overview of how techniques like finetuning and prompt engineering can enhance the performance of these models. Additionally, the study summarizes common Arabic benchmarks and datasets while presenting our observations on the persistent upward trend in the adoption of LLMs.

</details>


### [49] [TiSpell: A Semi-Masked Methodology for Tibetan Spelling Correction covering Multi-Level Error with Data Augmentation](https://arxiv.org/abs/2505.08037)

*Yutong Liu, Feng Xiao, Ziyue Zhang, Yongbin Yu, Cheng Huang, Fan Gao, Xiangxiang Wang, Ma-bao Ban, Manping Fan, Thupten Tsering, Cheng Huang, Gadeng Luosang, Renzeng Duojie, Nyima Tashi*

**Main category:** cs.CL

**Keywords:** Tibetan spelling correction, data augmentation, semi-masked model, character and syllable errors, natural language processing

**Relevance Score:** 2

**TL;DR:** The paper presents TiSpell, a semi-masked model for multi-level Tibetan spelling correction that addresses both character and syllable errors through a new data augmentation strategy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current Tibetan spelling correction methods are limited to single-level correction and lack integrated solutions for character and syllable levels, along with a scarcity of suitable datasets.

**Method:** The authors propose a data augmentation approach that generates multi-level corruptions from unlabeled text and introduce TiSpell, a semi-masked model designed for correcting both character- and syllable-level errors.

**Key Contributions:**

	1. Development of TiSpell, a novel semi-masked model for Tibetan spelling correction.
	2. Introduction of a data augmentation method that creates multi-level corruptions from unlabeled text.
	3. Demonstration of superior performance of TiSpell over existing models.

**Result:** Experiments show that TiSpell outperforms baseline models and matches the state-of-the-art performance, demonstrating its effectiveness in correcting spelling errors.

**Limitations:** The study focuses solely on Tibetan, and the proposed methods may not be directly applicable to other languages without modifications.

**Conclusion:** TiSpell is a significant advancement in Tibetan spelling correction, addressing both levels of errors and backed by a robust training dataset created through innovative augmentation techniques.

**Abstract:** Multi-level Tibetan spelling correction addresses errors at both the character and syllable levels within a unified model. Existing methods focus mainly on single-level correction and lack effective integration of both levels. Moreover, there are no open-source datasets or augmentation methods tailored for this task in Tibetan. To tackle this, we propose a data augmentation approach using unlabeled text to generate multi-level corruptions, and introduce TiSpell, a semi-masked model capable of correcting both character- and syllable-level errors. Although syllable-level correction is more challenging due to its reliance on global context, our semi-masked strategy simplifies this process. We synthesize nine types of corruptions on clean sentences to create a robust training set. Experiments on both simulated and real-world data demonstrate that TiSpell, trained on our dataset, outperforms baseline models and matches the performance of state-of-the-art approaches, confirming its effectiveness.

</details>


### [50] [FalseReject: A Resource for Improving Contextual Safety and Mitigating Over-Refusals in LLMs via Structured Reasoning](https://arxiv.org/abs/2505.08054)

*Zhehao Zhang, Weijie Xu, Fanyou Wu, Chandan K. Reddy*

**Main category:** cs.CL

**Keywords:** Large Language Models, Safety Alignment, FalseReject, Natural Language Processing, Toxic Queries

**Relevance Score:** 8

**TL;DR:** FalseReject is a resource designed to reduce over-refusals in LLMs by providing a dataset of toxic queries and a framework for enhanced model training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the utility of LLMs in sensitive contexts by addressing the over-refusal of benign queries due to safety alignment approaches.

**Method:** Introduced FalseReject, which contains 16k toxic queries and structured responses, and utilizes a graph-informed adversarial multi-agent interaction framework to enhance prompt generation and response structuring.

**Key Contributions:**

	1. Introduced FalseReject dataset with 16k toxic queries and structured responses.
	2. Developed a graph-informed adversarial multi-agent interaction framework for prompt generation.
	3. Demonstrated significant reductions in over-refusals in LLMs through empirical testing.

**Result:** Extensive benchmarking on 29 SOTA LLMs shows that using FalseReject for supervised finetuning significantly reduces unnecessary refusals while maintaining safety and language capabilities.

**Limitations:** 

**Conclusion:** The FalseReject dataset and framework effectively enhance LLM decision-making concerning safety, allowing for better performance in sensitive applications.

**Abstract:** Safety alignment approaches in large language models (LLMs) often lead to the over-refusal of benign queries, significantly diminishing their utility in sensitive scenarios. To address this challenge, we introduce FalseReject, a comprehensive resource containing 16k seemingly toxic queries accompanied by structured responses across 44 safety-related categories. We propose a graph-informed adversarial multi-agent interaction framework to generate diverse and complex prompts, while structuring responses with explicit reasoning to aid models in accurately distinguishing safe from unsafe contexts. FalseReject includes training datasets tailored for both standard instruction-tuned models and reasoning-oriented models, as well as a human-annotated benchmark test set. Our extensive benchmarking on 29 state-of-the-art (SOTA) LLMs reveals persistent over-refusal challenges. Empirical results demonstrate that supervised finetuning with FalseReject substantially reduces unnecessary refusals without compromising overall safety or general language capabilities.

</details>


### [51] [HYPERNYM MERCURY: Token Optimization through Semantic Field Constriction and Reconstruction from Hypernyms. A New Text Compression Method](https://arxiv.org/abs/2505.08058)

*Chris Forrester, Octavia Sulea*

**Main category:** cs.CL

**Keywords:** token reduction, LLM, semantic compression, lossless compression, NLP

**Relevance Score:** 7

**TL;DR:** This paper presents a novel text representation scheme for token reduction in LLM prompts that achieves over 90% reduction while maintaining high semantic similarity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize compute resources by reducing the number of tokens in LLM prompts without losing semantic meaning.

**Method:** A word-level semantic compression technique is introduced that offers controllable granularity and claims to be lossless.

**Key Contributions:**

	1. Introduction of a novel text representation scheme
	2. First word-level semantic compression technique
	3. Demonstration of lossless compression with controllable granularity

**Result:** Achieved over 90% token reduction on benchmarked open source data while maintaining high semantic similarity at the paragraph level.

**Limitations:** 

**Conclusion:** The proposed method demonstrates significant potential for improving efficiency in NLP tasks involving LLMs across different genres and models.

**Abstract:** Compute optimization using token reduction of LLM prompts is an emerging task in the fields of NLP and next generation, agentic AI. In this white paper, we introduce a novel (patent pending) text representation scheme and a first-of-its-kind word-level semantic compression of paragraphs that can lead to over 90\% token reduction, while retaining high semantic similarity to the source text. We explain how this novel compression technique can be lossless and how the detail granularity is controllable. We discuss benchmark results over open source data (i.e. Bram Stoker's Dracula available through Project Gutenberg) and show how our results hold at the paragraph level, across multiple genres and models.

</details>


### [52] [Are LLMs complicated ethical dilemma analyzers?](https://arxiv.org/abs/2505.08106)

*Jiashen, Du, Jesse Yao, Allen Liu, Zhekai Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Ethical reasoning, Benchmark dataset, Human responses, Model evaluation

**Relevance Score:** 8

**TL;DR:** This paper investigates whether Large Language Models (LLMs) can emulate human ethical reasoning using a benchmark of ethical dilemmas and expert opinions, finding that while LLMs generally outperform non-expert humans in certain linguistic measures, they struggle with historical grounding and nuanced resolution strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate if LLMs can act as believable proxies for human ethical judgment.

**Method:** A benchmark dataset of 196 ethical dilemmas was created, evaluating various LLMs against this dataset using a composite metric framework including BLEU, Damerau-Levenshtein distance, and others.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark dataset for ethical dilemmas
	2. Evaluation of multiple frontier LLMs using a novel composite metric framework
	3. Comparison of LLM responses against expert and non-expert human input

**Result:** LLMs typically surpass non-expert human responses in lexical and structural alignment, especially GPT-4o-mini, but struggle in areas requiring historical context and nuanced reasoning.

**Limitations:** LLMs' challenges in historical grounding and proposing nuanced resolution strategies.

**Conclusion:** The findings reveal both the advantages of LLMs in ethical decision-making and their current limitations.

**Abstract:** One open question in the study of Large Language Models (LLMs) is whether they can emulate human ethical reasoning and act as believable proxies for human judgment. To investigate this, we introduce a benchmark dataset comprising 196 real-world ethical dilemmas and expert opinions, each segmented into five structured components: Introduction, Key Factors, Historical Theoretical Perspectives, Resolution Strategies, and Key Takeaways. We also collect non-expert human responses for comparison, limited to the Key Factors section due to their brevity. We evaluate multiple frontier LLMs (GPT-4o-mini, Claude-3.5-Sonnet, Deepseek-V3, Gemini-1.5-Flash) using a composite metric framework based on BLEU, Damerau-Levenshtein distance, TF-IDF cosine similarity, and Universal Sentence Encoder similarity. Metric weights are computed through an inversion-based ranking alignment and pairwise AHP analysis, enabling fine-grained comparison of model outputs to expert responses. Our results show that LLMs generally outperform non-expert humans in lexical and structural alignment, with GPT-4o-mini performing most consistently across all sections. However, all models struggle with historical grounding and proposing nuanced resolution strategies, which require contextual abstraction. Human responses, while less structured, occasionally achieve comparable semantic similarity, suggesting intuitive moral reasoning. These findings highlight both the strengths and current limitations of LLMs in ethical decision-making.

</details>


### [53] [Putting It All into Context: Simplifying Agents with LCLMs](https://arxiv.org/abs/2505.08120)

*Mingjian Jiang, Yangjun Ruan, Luis Lastras, Pavan Kapanipathi, Tatsunori Hashimoto*

**Main category:** cs.CL

**Keywords:** language model agents, task automation, SWE-bench, Gemini-1.5-Pro, Gemini-2.5-Pro

**Relevance Score:** 9

**TL;DR:** This paper evaluates the necessity of complex scaffolding in language model agents for task automation, specifically using the SWE-bench benchmark, and demonstrates that a simple long context language model approach can be competitive.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether the complexity of existing LM agent architectures is essential for automating challenging tasks and if simplification can yield comparable performance.

**Method:** The study examines the performance of the Gemini-1.5-Pro and Gemini-2.5-Pro language models with and without scaffolding on the SWE-bench task, comparing their solve rates to those of complex agent architectures.

**Key Contributions:**

	1. Demonstrates the effectiveness of unscaffolded long context language models in challenging tasks.
	2. Compares various model architectures on performance metrics for SWE-bench.
	3. Highlights the potential for simplicity in LM agent design without significant loss in task performance.

**Result:** The Gemini-1.5-Pro model without scaffolding achieved a 38% solve rate, while the more capable Gemini-2.5-Pro reached a 50.8% solve rate under a similar unscaffolded approach.

**Limitations:** The unscaffolded methods do not yet outperform the strongest agent architectures, indicating limitations in certain complex tasks.

**Conclusion:** The results suggest that effective prompting in a long context language model can substitute for complex scaffolding, achieving competitive performance on challenging benchmarks.

**Abstract:** Recent advances in language model (LM) agents have demonstrated significant potential for automating complex real-world tasks. To make progress on these difficult tasks, LM agent architectures have become increasingly complex, often incorporating multi-step retrieval tools, multiple agents, and scaffolding adapted to the underlying LM. In this work, we investigate whether all of this complexity is necessary, or if parts of these scaffolds can be removed on challenging tasks like SWE-bench. We show that in the case of SWE-bench, simply putting the entire environment into the context of a long context language model (LCLM) and properly prompting the model makes it competitive with carefully tuned, complex agent scaffolds. We show that a Gemini-1.5-Pro model without any scaffolding or tools achieves 38% on SWE-Bench-Verified, comparable with approaches using carefully tuned agent scaffolds (32%). While the unscaffolded approach with Gemini-1.5-Pro falls short of the strongest agentic architectures, we demonstrate that the more capable Gemini-2.5-Pro using the same unscaffolded approach directly attains a 50.8% solve rate. Additionally, a two-stage approach combining Gemini-1.5-Pro with Claude-3.7 achieves a competitive 48.6% solve rate.

</details>


### [54] [ALOHA: Empowering Multilingual Agent for University Orientation with Hierarchical Retrieval](https://arxiv.org/abs/2505.08130)

*Mingxu Tao, Bowen Tang, Mingxuan Ma, Yining Zhang, Hourun Li, Feifan Wen, Hao Ma, Jia Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual information retrieval, hierarchical retrieval, educational technology, campus orientation

**Relevance Score:** 9

**TL;DR:** ALOHA is a multilingual agent designed to improve university orientation using hierarchical retrieval and external APIs for interactive services.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in existing LLMs and search engines to adequately address the specific needs of faculty and students for campus-related information, particularly in multilingual contexts.

**Method:** The ALOHA system utilizes hierarchical retrieval methods combined with external APIs and a user-friendly front-end interface to deliver accurate information in multiple languages.

**Key Contributions:**

	1. Introduction of a multilingual agent for campus information retrieval
	2. Use of hierarchical retrieval methods to improve response accuracy
	3. Integration of external APIs for enhanced interactivity

**Result:** Human evaluations indicate that ALOHA provides superior correct, timely, and user-friendly responses compared to existing commercial chatbots and search engines.

**Limitations:** 

**Conclusion:** ALOHA has successfully been deployed for use by over 12,000 people, demonstrating its relevance and effectiveness in the educational context.

**Abstract:** The rise of Large Language Models~(LLMs) revolutionizes information retrieval, allowing users to obtain required answers through complex instructions within conversations. However, publicly available services remain inadequate in addressing the needs of faculty and students to search campus-specific information. It is primarily due to the LLM's lack of domain-specific knowledge and the limitation of search engines in supporting multilingual and timely scenarios. To tackle these challenges, we introduce ALOHA, a multilingual agent enhanced by hierarchical retrieval for university orientation. We also integrate external APIs into the front-end interface to provide interactive service. The human evaluation and case study show our proposed system has strong capabilities to yield correct, timely, and user-friendly responses to the queries in multiple languages, surpassing commercial chatbots and search engines. The system has been deployed and has provided service for more than 12,000 people.

</details>


### [55] [Fusing Bidirectional Chains of Thought and Reward Mechanisms A Method for Enhancing Question-Answering Capabilities of Large Language Models for Chinese Intangible Cultural Heritage](https://arxiv.org/abs/2505.08167)

*Ruilin Liu, Zhixiao Zhao, Jieqiong Li, Chang Liu, Dongbo Wang*

**Main category:** cs.CL

**Keywords:** large language models, Intangible Cultural Heritage, bidirectional reasoning, reward mechanism, domain-specific datasets

**Relevance Score:** 6

**TL;DR:** This paper presents a novel training method for fine-tuning large language models (LLMs) using Intangible Cultural Heritage (ICH) data, addressing issues like bias and knowledge inheritance with a bidirectional reasoning approach and a reward mechanism.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle challenges in fine-tuning large language models with Intangible Cultural Heritage data, including bias, incorrect knowledge inheritance, and catastrophic forgetting.

**Method:** The proposed method integrates bidirectional chains of thought and a reward mechanism during training, enabling forward reasoning and enhancing answer accuracy through reverse questioning and reasoning.

**Key Contributions:**

	1. Novel training method combining bidirectional reasoning and reward mechanism.
	2. Demonstrated superiority of the method over traditional approaches in accuracy metrics.
	3. Generalizability shown across multiple domain-specific datasets.

**Result:** Comparative experiments on ICH-Qwen indicate that the new method surpasses existing approaches like 0-shot reasoning and knowledge distillation in accuracy (measured by Bleu-4 and Rouge-L scores) for question-answering tasks.

**Limitations:** 

**Conclusion:** The proposed method not only improves outputs in the ICH domain but also shows adaptability and generalization to various datasets in Finance, Wikidata, and StrategyQA, suggesting a valuable training approach for future applications across different fields.

**Abstract:** The rapid development of large language models (LLMs) has provided significant support and opportunities for the advancement of domain-specific LLMs. However, fine-tuning these large models using Intangible Cultural Heritage (ICH) data inevitably faces challenges such as bias, incorrect knowledge inheritance, and catastrophic forgetting. To address these issues, we propose a novel training method that integrates a bidirectional chains of thought and a reward mechanism. This method is built upon ICH-Qwen, a large language model specifically designed for the field of intangible cultural heritage. The proposed method enables the model to not only perform forward reasoning but also enhances the accuracy of the generated answers by utilizing reverse questioning and reverse reasoning to activate the model's latent knowledge. Additionally, a reward mechanism is introduced during training to optimize the decision-making process. This mechanism improves the quality of the model's outputs through structural and content evaluations with different weighting schemes. We conduct comparative experiments on ICH-Qwen, with results demonstrating that our method outperforms 0-shot, step-by-step reasoning, knowledge distillation, and question augmentation methods in terms of accuracy, Bleu-4, and Rouge-L scores on the question-answering task. Furthermore, the paper highlights the effectiveness of combining the bidirectional chains of thought and reward mechanism through ablation experiments. In addition, a series of generalizability experiments are conducted, with results showing that the proposed method yields improvements on various domain-specific datasets and advanced models in areas such as Finance, Wikidata, and StrategyQA. This demonstrates that the method is adaptable to multiple domains and provides a valuable approach for model training in future applications across diverse fields.

</details>


### [56] [Exploiting Text Semantics for Few and Zero Shot Node Classification on Text-attributed Graph](https://arxiv.org/abs/2505.08168)

*Yuxiang Wang, Xiao Yan, Shiyu Jin, Quanqing Xu, Chuang Hu, Yuanyuan Zhu, Bo Du, Jia Wu, Jiawei Jiang*

**Main category:** cs.CL

**Keywords:** Text-attributed graphs, Node classification, Text Semantics Augmentation, Graph-based augmentation, Machine learning

**Relevance Score:** 6

**TL;DR:** Proposes Text Semantics Augmentation (TSA) to enhance node classification accuracy in text-attributed graphs by introducing text semantic supervision signals.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of exploration in text-based augmentations for few- and zero-shot node classification on text-attributed graphs (TAGs), despite their relevance in academia and social networks.

**Method:** Introducing Text Semantics Augmentation (TSA) with two techniques: positive semantics matching to retrieve similar texts and negative semantics contrast to provide a contrasting semantic prompt.

**Key Contributions:**

	1. Introduction of text-based enhancements for graph node classification
	2. Development of positive semantics matching and negative semantics contrast techniques
	3. Demonstrated substantial performance improvements on multiple datasets

**Result:** TSA was evaluated on 5 datasets and consistently outperformed 13 state-of-the-art baselines, typically improving accuracy by over 5% compared to the best baseline.

**Limitations:** 

**Conclusion:** The introduction of text semantic supervision through TSA significantly enhances the performance of node classification in TAGs.

**Abstract:** Text-attributed graph (TAG) provides a text description for each graph node, and few- and zero-shot node classification on TAGs have many applications in fields such as academia and social networks. Existing work utilizes various graph-based augmentation techniques to train the node and text embeddings, while text-based augmentations are largely unexplored. In this paper, we propose Text Semantics Augmentation (TSA) to improve accuracy by introducing more text semantic supervision signals. Specifically, we design two augmentation techniques, i.e., positive semantics matching and negative semantics contrast, to provide more reference texts for each graph node or text description. Positive semantic matching retrieves texts with similar embeddings to match with a graph node. Negative semantic contrast adds a negative prompt to construct a text description with the opposite semantics, which is contrasted with the original node and text. We evaluate TSA on 5 datasets and compare with 13 state-of-the-art baselines. The results show that TSA consistently outperforms all baselines, and its accuracy improvements over the best-performing baseline are usually over 5%.

</details>


### [57] [A Head to Predict and a Head to Question: Pre-trained Uncertainty Quantification Heads for Hallucination Detection in LLM Outputs](https://arxiv.org/abs/2505.08200)

*Artem Shelmanov, Ekaterina Fadeeva, Akim Tsvigun, Ivan Tsvigun, Zhuohan Xie, Igor Kiselev, Nico Daheim, Caiqi Zhang, Artem Vazhentsev, Mrinmaya Sachan, Preslav Nakov, Timothy Baldwin*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, uncertainty quantification, Transformer architecture, NLP

**Relevance Score:** 9

**TL;DR:** The paper introduces pre-trained uncertainty quantification (UQ) heads for Large Language Models (LLMs) to improve hallucination detection and reliability assessment of outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often produce convincing yet false information, which challenges their reliability, leading to the need for tools that can help identify these hallucinations.

**Method:** The authors propose supervised auxiliary modules called UQ heads, designed to enhance uncertainty quantification in LLMs, using Transformer architecture and features from LLM attention maps.

**Key Contributions:**

	1. Development of supervised UQ heads for LLMs
	2. State-of-the-art performance in hallucination detection
	3. Public release of code and pre-trained models

**Result:** The UQ heads show state-of-the-art performance in detecting hallucinations at the claim level, demonstrating robustness and generalization across languages not explicitly trained on.

**Limitations:** 

**Conclusion:** The introduction of pre-trained UQ heads significantly improves the ability to assess the reliability of LLM outputs, and the associated code and models are publicly available for further research and application.

**Abstract:** Large Language Models (LLMs) have the tendency to hallucinate, i.e., to sporadically generate false or fabricated information. This presents a major challenge, as hallucinations often appear highly convincing and users generally lack the tools to detect them. Uncertainty quantification (UQ) provides a framework for assessing the reliability of model outputs, aiding in the identification of potential hallucinations. In this work, we introduce pre-trained UQ heads: supervised auxiliary modules for LLMs that substantially enhance their ability to capture uncertainty compared to unsupervised UQ methods. Their strong performance stems from the powerful Transformer architecture in their design and informative features derived from LLM attention maps. Experimental evaluation shows that these heads are highly robust and achieve state-of-the-art performance in claim-level hallucination detection across both in-domain and out-of-domain prompts. Moreover, these modules demonstrate strong generalization to languages they were not explicitly trained on. We pre-train a collection of UQ heads for popular LLM series, including Mistral, Llama, and Gemma 2. We publicly release both the code and the pre-trained heads.

</details>


### [58] [Large Language Model Psychometrics: A Systematic Review of Evaluation, Validation, and Enhancement](https://arxiv.org/abs/2505.08245)

*Haoran Ye, Jing Jin, Yuhang Xie, Xin Zhang, Guojie Song*

**Main category:** cs.CL

**Keywords:** Large Language Models, Psychometrics, Human-Centered AI, Evaluation Paradigms, AI Systems

**Relevance Score:** 9

**TL;DR:** This paper surveys the emerging field of LLM Psychometrics, focusing on the evaluation and enhancement of large language models using psychometric principles.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The advancement of LLMs has created challenges in evaluation methodologies that necessitate a human-centered approach, integrating psychological constructs from Psychometrics.

**Method:** The paper synthesizes previous research and theories from Psychometrics to develop a structured framework for evaluating LLMs.

**Key Contributions:**

	1. Introduction of the concept of LLM Psychometrics.
	2. Development of a structured framework for evaluation methodologies.
	3. Providing a repository of LLM psychometric resources.

**Result:** The introduction of LLM Psychometrics enhances the understanding of LLM evaluation, promoting broader methodologies and validation of results.

**Limitations:** 

**Conclusion:** A more comprehensive evaluation paradigm is necessary for the future development of human-centered AI systems, with a curated repository provided for researchers.

**Abstract:** The rapid advancement of large language models (LLMs) has outpaced traditional evaluation methodologies. It presents novel challenges, such as measuring human-like psychological constructs, navigating beyond static and task-specific benchmarks, and establishing human-centered evaluation. These challenges intersect with Psychometrics, the science of quantifying the intangible aspects of human psychology, such as personality, values, and intelligence. This survey introduces and synthesizes an emerging interdisciplinary field of LLM Psychometrics, which leverages psychometric instruments, theories, and principles to evaluate, understand, and enhance LLMs. We systematically explore the role of Psychometrics in shaping benchmarking principles, broadening evaluation scopes, refining methodologies, validating results, and advancing LLM capabilities. This paper integrates diverse perspectives to provide a structured framework for researchers across disciplines, enabling a more comprehensive understanding of this nascent field. Ultimately, we aim to provide actionable insights for developing future evaluation paradigms that align with human-level AI and promote the advancement of human-centered AI systems for societal benefit. A curated repository of LLM psychometric resources is available at https://github.com/valuebyte-ai/Awesome-LLM-Psychometrics.

</details>


### [59] [Enhancing Cache-Augmented Generation (CAG) with Adaptive Contextual Compression for Scalable Knowledge Integration](https://arxiv.org/abs/2505.08261)

*Rishabh Agrawal, Himanshu Kumar*

**Main category:** cs.CL

**Keywords:** Cache-Augmented Generation, Adaptive Contextual Compression, Hybrid Framework, multi-hop reasoning, knowledge integration

**Relevance Score:** 9

**TL;DR:** The paper discusses the introduction of Adaptive Contextual Compression (ACC) and a Hybrid CAG-RAG Framework to improve scalability and efficiency in Cache-Augmented Generation (CAG) for large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of scaling Cache-Augmented Generation (CAG) in relation to large and dynamic knowledge bases.

**Method:** The paper proposes Adaptive Contextual Compression (ACC) for context input management and a Hybrid CAG-RAG Framework that integrates retrieval with preloaded context.

**Key Contributions:**

	1. Introduction of Adaptive Contextual Compression (ACC) for managing context inputs
	2. Development of a Hybrid CAG-RAG Framework that integrates retrieval
	3. Improvements in multi-hop reasoning and scalability for large knowledge bases

**Result:** Evaluations show improvements in scalability, efficiency, and multi-hop reasoning performance.

**Limitations:** 

**Conclusion:** The proposed methods provide practical solutions to enhance knowledge integration in real-world applications.

**Abstract:** The rapid progress in large language models (LLMs) has paved the way for novel approaches in knowledge-intensive tasks. Among these, Cache-Augmented Generation (CAG) has emerged as a promising alternative to Retrieval-Augmented Generation (RAG). CAG minimizes retrieval latency and simplifies system design by preloading knowledge into the model's context. However, challenges persist in scaling CAG to accommodate large and dynamic knowledge bases effectively. This paper introduces Adaptive Contextual Compression (ACC), an innovative technique designed to dynamically compress and manage context inputs, enabling efficient utilization of the extended memory capabilities of modern LLMs. To further address the limitations of standalone CAG, we propose a Hybrid CAG-RAG Framework, which integrates selective retrieval to augment preloaded contexts in scenarios requiring additional information. Comprehensive evaluations on diverse datasets highlight the proposed methods' ability to enhance scalability, optimize efficiency, and improve multi-hop reasoning performance, offering practical solutions for real-world knowledge integration challenges.

</details>


### [60] [Evaluating the Effectiveness of Black-Box Prompt Optimization as the Scale of LLMs Continues to Grow](https://arxiv.org/abs/2505.08303)

*Ziyu Zhou, Yihang Wu, Jingyuan Yang, Zhan Xiao, Rongjun Li*

**Main category:** cs.CL

**Keywords:** Black-Box Optimization, Large Language Models, Prompt Engineering, Natural Language Understanding, Natural Language Generation

**Relevance Score:** 8

**TL;DR:** This paper evaluates black-box prompt optimization methods on large-scale LLMs and finds limited performance improvements as model size increases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effectiveness of black-box optimization techniques for aligning large language models as their scale increases, especially on models larger than previously studied.

**Method:** The study tests three established black-box optimization methods on large-scale LLMs using four NLU and NLG datasets, including models like DeepSeek V3 and Gemini 2.0 Flash.

**Key Contributions:**

	1. Evaluation of black-box optimization on large models
	2. Identification of inverse scaling law with LLM size
	3. Experimental results across multiple datasets

**Result:** The research discovered that black-box prompt optimization methods provide only marginal enhancements in task performance for large-scale LLMs, suggesting the effectiveness diminishes as the model size grows.

**Limitations:** The study primarily focuses on a limited set of black-box optimization methods and models, and further research is needed to explore other approaches and larger model variations.

**Conclusion:** The findings support a hypothesis that the scale of LLMs is a key determinant in the limited benefits obtained from black-box prompt optimization methods, indicating an inverse scaling law effect.

**Abstract:** Black-Box prompt optimization methods have emerged as a promising strategy for refining input prompts to better align large language models (LLMs), thereby enhancing their task performance. Although these methods have demonstrated encouraging results, most studies and experiments have primarily focused on smaller-scale models (e.g., 7B, 14B) or earlier versions (e.g., GPT-3.5) of LLMs. As the scale of LLMs continues to increase, such as with DeepSeek V3 (671B), it remains an open question whether these black-box optimization techniques will continue to yield significant performance improvements for models of such scale. In response to this, we select three well-known black-box optimization methods and evaluate them on large-scale LLMs (DeepSeek V3 and Gemini 2.0 Flash) across four NLU and NLG datasets. The results show that these black-box prompt optimization methods offer only limited improvements on these large-scale LLMs. Furthermore, we hypothesize that the scale of the model is the primary factor contributing to the limited benefits observed. To explore this hypothesis, we conducted experiments on LLMs of varying sizes (Qwen 2.5 series, ranging from 7B to 72B) and observed an inverse scaling law, wherein the effectiveness of black-box optimization methods diminished as the model size increased.

</details>


### [61] [AM-Thinking-v1: Advancing the Frontier of Reasoning at 32B Scale](https://arxiv.org/abs/2505.08311)

*Yunjie Ji, Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Han Zhao, Xiangang Li*

**Main category:** cs.CL

**Keywords:** Language Model, Open-source, Machine Learning, Reasoning, Collaborative Innovation

**Relevance Score:** 8

**TL;DR:** Introduction of AM-Thinking-v1, a 32B dense language model, emphasizing its reasoning capabilities and open-source nature.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To showcase the potential of open-source models at the 32B scale, merging high performance with accessibility for real-world applications.

**Method:** AM-Thinking-v1 was built upon the Qwen2.5-32B base model, utilizing a combination of supervised fine-tuning and reinforcement learning in its post-training pipeline.

**Key Contributions:**

	1. Development of AM-Thinking-v1 as a high-performance open-source language model
	2. Demonstration of effective post-training strategies combining supervised fine-tuning and reinforcement learning
	3. Advancement in reasoning capabilities for mid-scale models aimed at real-world usability

**Result:** AM-Thinking-v1 achieved scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, outperforming DeepSeek-R1 and rivaling top Mixture-of-Experts models.

**Limitations:** 

**Conclusion:** The success of AM-Thinking-v1 indicates that open-source models can perform exceptionally well at a mid-scale, encouraging future collaborative innovation in HCI and reasoning.

**Abstract:** We present AM-Thinking-v1, a 32B dense language model that advances the frontier of reasoning, embodying the collaborative spirit of open-source innovation. Outperforming DeepSeek-R1 and rivaling leading Mixture-of-Experts (MoE) models like Qwen3-235B-A22B and Seed1.5-Thinking, AM-Thinking-v1 achieves impressive scores of 85.3 on AIME 2024, 74.4 on AIME 2025, and 70.3 on LiveCodeBench, showcasing state-of-the-art mathematical and coding capabilities among open-source models of similar scale.   Built entirely from the open-source Qwen2.5-32B base model and publicly available queries, AM-Thinking-v1 leverages a meticulously crafted post-training pipeline - combining supervised fine-tuning and reinforcement learning - to deliver exceptional reasoning capabilities. This work demonstrates that the open-source community can achieve high performance at the 32B scale, a practical sweet spot for deployment and fine-tuning. By striking a balance between top-tier performance and real-world usability, we hope AM-Thinking-v1 inspires further collaborative efforts to harness mid-scale models, pushing reasoning boundaries while keeping accessibility at the core of innovation. We have open-sourced our model on \href{https://huggingface.co/a-m-team/AM-Thinking-v1}{Hugging Face}.

</details>


### [62] [On the Geometry of Semantics in Next-token Prediction](https://arxiv.org/abs/2505.08348)

*Yize Zhao, Christos Thrampoulidis*

**Main category:** cs.CL

**Keywords:** language models, next-token prediction, singular value decomposition, semantic representation, neural network training

**Relevance Score:** 8

**TL;DR:** This paper analyzes how next-token prediction training in language models leads to the extraction of latent semantic and grammatical concepts, utilizing singular value decomposition to understand learned embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the next-token prediction training objective enables language models to capture meanings and linguistic structures without explicit matrix constructions.

**Method:** The study uses singular value decomposition to analyze the learned embeddings in relation to the patterns of next-word co-occurrences derived from the training objective.

**Key Contributions:**

	1. Introduced spectral clustering of embeddings for semantic interpretation
	2. Demonstrated early learning of crucial SVD factors in language models
	3. Bridged concepts in distributional semantics and neural training dynamics

**Result:** The analysis reveals that important semantic factors are learned early in the training process, and suggests methods like spectral clustering to identify interpretable semantics from embeddings.

**Limitations:** 

**Conclusion:** The research demonstrates the implicit biases in training that influence how meaning representations develop in language models, bridging various concepts in distributional semantics and neural network training dynamics.

**Abstract:** Modern language models demonstrate a remarkable ability to capture linguistic meaning despite being trained solely through next-token prediction (NTP). We investigate how this conceptually simple training objective leads models to extract and encode latent semantic and grammatical concepts. Our analysis reveals that NTP optimization implicitly guides models to encode concepts via singular value decomposition (SVD) factors of a centered data-sparsity matrix that captures next-word co-occurrence patterns. While the model never explicitly constructs this matrix, learned word and context embeddings effectively factor it to capture linguistic structure. We find that the most important SVD factors are learned first during training, motivating the use of spectral clustering of embeddings to identify human-interpretable semantics, including both classical k-means and a new orthant-based method directly motivated by our interpretation of concepts. Overall, our work bridges distributional semantics, neural collapse geometry, and neural network training dynamics, providing insights into how NTP's implicit biases shape the emergence of meaning representations in language models.

</details>


### [63] [Alignment Drift in CEFR-prompted LLMs for Interactive Spanish Tutoring](https://arxiv.org/abs/2505.08351)

*Mina Almasi, Ross Deans Kristensen-McLachlan*

**Main category:** cs.CL

**Keywords:** Large Language Models, adaptive tutoring, second-language learning, system prompting, alignment drift

**Relevance Score:** 8

**TL;DR:** This paper evaluates LLMs as adaptive tutors for second-language learning, exploring the effectiveness of system prompting for text difficulty control across proficiency levels. Findings indicate that while prompting can guide outputs, it is too fragile for long-term interactions, leading to alignment drift.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of Large Language Models (LLMs) as adaptive tutors for second-language learning, particularly in ensuring appropriate interaction based on student proficiency.

**Method:** Simulated teacher-student dialogues in Spanish using instruction-tuned LLMs, alternating roles between tutor and student with separate chat histories, and evaluated the application of CEFR-based prompting for controlling text difficulty.

**Key Contributions:**

	1. Introduction of alignment drift phenomenon in LLM interactions
	2. Evaluation of system prompting effectiveness in tutoring
	3. Scalable method for low-cost evaluation of LLM performance without human involvement

**Result:** While CEFR-based prompting can limit outputs to the correct difficulty, it is inadequate for sustained interactions due to alignment drift over time.

**Limitations:** System prompting is brittle and cannot sustain long-term engagement without additional support.

**Conclusion:** LLMs show promise for personalized adaptive tutoring in language learning, but reliance on prompting must be supplemented to mitigate alignment drift for effective long-term use.

**Abstract:** This paper investigates the potentials of Large Language Models (LLMs) as adaptive tutors in the context of second-language learning. In particular, we evaluate whether system prompting can reliably constrain LLMs to generate only text appropriate to the student's competence level. We simulate full teacher-student dialogues in Spanish using instruction-tuned, open-source LLMs ranging in size from 7B to 12B parameters. Dialogues are generated by having an LLM alternate between tutor and student roles with separate chat histories. The output from the tutor model is then used to evaluate the effectiveness of CEFR-based prompting to control text difficulty across three proficiency levels (A1, B1, C1). Our findings suggest that while system prompting can be used to constrain model outputs, prompting alone is too brittle for sustained, long-term interactional contexts - a phenomenon we term alignment drift. Our results provide insights into the feasibility of LLMs for personalized, proficiency-aligned adaptive tutors and provide a scalable method for low-cost evaluation of model performance without human participants.

</details>


### [64] [Towards Contamination Resistant Benchmarks](https://arxiv.org/abs/2505.08389)

*Rahmatullah Musawi, Sheng Lu*

**Main category:** cs.CL

**Keywords:** large language models, contamination resistance, evaluation benchmark, Caesar cipher, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces contamination resistance in evaluating large language models (LLMs) using a benchmark based on Caesar ciphers to address reliability issues in evaluations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Proper evaluation of LLMs is essential for understanding their potential and addressing safety concerns, but current methods face contamination issues that undermine reliability.

**Method:** The authors propose a benchmark based on Caesar ciphers to test LLM contamination resistance, evaluating widely used LLMs under controlled contamination settings.

**Key Contributions:**

	1. Introduction of contamination resistance concept
	2. Development of a Caesar cipher-based benchmark
	3. Insights into the limitations of current LLMs

**Result:** LLMs struggle against the proposed benchmark when contamination is managed, indicating significant issues in their evaluation methodologies.

**Limitations:** The simplicity of the benchmark may not capture all aspects of LLM evaluation.

**Conclusion:** This work contributes to developing contamination resistant benchmarks, promoting more rigorous evaluations of LLMs and revealing their true capabilities and limitations.

**Abstract:** The rapid development of large language models (LLMs) has transformed the landscape of natural language processing. Evaluating LLMs properly is crucial for understanding their potential and addressing concerns such as safety. However, LLM evaluation is confronted by various factors, among which contamination stands out as a key issue that undermines the reliability of evaluations. In this work, we introduce the concept of contamination resistance to address this challenge. We propose a benchmark based on Caesar ciphers (e.g., "ab" to "bc" when the shift is 1), which, despite its simplicity, is an excellent example of a contamination resistant benchmark. We test this benchmark on widely used LLMs under various settings, and we find that these models struggle with this benchmark when contamination is controlled. Our findings reveal issues in current LLMs and raise important questions regarding their true capabilities. Our work contributes to the development of contamination resistant benchmarks, enabling more rigorous LLM evaluation and offering insights into the true capabilities and limitations of LLMs.

</details>


### [65] [Accelerating Chain-of-Thought Reasoning: When Goal-Gradient Importance Meets Dynamic Skipping](https://arxiv.org/abs/2505.08392)

*Ren Zhuang, Ben Wang, Shuifa Sun*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Token compression, Machine Learning, Natural Language Processing, Efficiency

**Relevance Score:** 7

**TL;DR:** The paper presents Adaptive GoGI-Skip, a framework for dynamic compression of Chain-of-Thought (CoT) prompts in large language models that enhances efficiency without sacrificing accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies and verbosity in Chain-of-Thought prompting for large language models, which leads to high computational costs and latency.

**Method:** The proposed method utilizes Goal-Gradient Importance (GoGI) to identify important tokens and Adaptive Dynamic Skipping (ADS) to adjust compression rates based on model uncertainty during runtime.

**Key Contributions:**

	1. Introduction of Goal-Gradient Importance (GoGI) for identifying critical tokens.
	2. Development of Adaptive Dynamic Skipping (ADS) for dynamic compression adjustment.
	3. Demonstration of substantial efficiency gains while preserving reasoning accuracy.

**Result:** Adaptive GoGI-Skip reduces CoT token counts by over 45% on average and achieves 1.6-2.0 times faster inference speeds while maintaining high reasoning accuracy across various benchmarks.

**Limitations:** 

**Conclusion:** This approach is the first to harmonize a goal-oriented importance metric with a dynamic skipping mechanism, significantly improving the efficiency-accuracy trade-off in CoT reasoning.

**Abstract:** Large Language Models leverage Chain-of-Thought (CoT) prompting for complex tasks, but their reasoning traces are often excessively verbose and inefficient, leading to significant computational costs and latency. Current CoT compression techniques typically rely on generic importance metrics and static compression rates, which may inadvertently remove functionally critical tokens or fail to adapt to varying reasoning complexity. To overcome these limitations, we propose Adaptive GoGI-Skip, a novel framework learning dynamic CoT compression via supervised fine-tuning. This approach introduces two synergistic innovations: (1) Goal-Gradient Importance (GoGI), a novel metric accurately identifying functionally relevant tokens by measuring the gradient influence of their intermediate representations on the final answer loss, and (2) Adaptive Dynamic Skipping (ADS), a mechanism dynamically regulating the compression rate based on runtime model uncertainty while ensuring local coherence through an adaptive N-token constraint. To our knowledge, this is the first work unifying a goal-oriented, gradient-based importance metric with dynamic, uncertainty-aware skipping for CoT compression. Trained on compressed MATH data, Adaptive GoGI-Skip demonstrates strong cross-domain generalization across diverse reasoning benchmarks including AIME, GPQA, and GSM8K. It achieves substantial efficiency gains - reducing CoT token counts by over 45% on average and delivering 1.6-2.0 times inference speedups - while maintaining high reasoning accuracy. Notably, it significantly outperforms existing baselines by preserving accuracy even at high effective compression rates, advancing the state of the art in the CoT reasoning efficiency-accuracy trade-off.

</details>


### [66] [TUMS: Enhancing Tool-use Abilities of LLMs with Multi-structure Handlers](https://arxiv.org/abs/2505.08402)

*Aiyao He, Sijia Cui, Shuai Xu, Yanna Wang, Bo Xu*

**Main category:** cs.CL

**Keywords:** large language models, parameter-level processing, tool-augmented LLMs

**Relevance Score:** 9

**TL;DR:** The paper introduces TUMS, a framework that enhances LLMs by transforming tool-level processing into parameter-level processing, improving their ability to execute complex tasks accurately.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the difficulties LLMs face with non-executable actions and incorrect parameters during tasks, which limits their effectiveness in NLP applications.

**Method:** The TUMS framework includes four components: an intent recognizer, a task decomposer, a subtask processor with multi-structure handlers, and an executor, designed to improve task processing for LLMs.

**Key Contributions:**

	1. Introduction of the TUMS framework for parameter-level processing
	2. Enhanced understanding of user intent through the intent recognizer
	3. Improved task breakdown via the task decomposer

**Result:** Empirical studies show TUMS improves performance on ToolQA benchmarks, with average improvements of 19.6% on easy tasks and 50.6% on hard tasks, demonstrating its effectiveness and efficiency.

**Limitations:** 

**Conclusion:** TUMS offers a structured method to enhance LLM tool-use capabilities, paving the way for future developments in Tool-augmented LLMs.

**Abstract:** Recently, large language models(LLMs) have played an increasingly important role in solving a wide range of NLP tasks, leveraging their capabilities of natural language understanding and generating. Integration with external tools further enhances LLMs' effectiveness, providing more precise, timely, and specialized responses. However, LLMs still encounter difficulties with non-executable actions and improper actions, which are primarily attributed to incorrect parameters. The process of generating parameters by LLMs is confined to the tool level, employing the coarse-grained strategy without considering the different difficulties of various tools. To address this issue, we propose TUMS, a novel framework designed to enhance the tool-use capabilities of LLMs by transforming tool-level processing into parameter-level processing. Specifically, our framework consists of four key components: (1) an intent recognizer that identifies the user's intent to help LLMs better understand the task; (2) a task decomposer that breaks down complex tasks into simpler subtasks, each involving a tool call; (3) a subtask processor equipped with multi-structure handlers to generate accurate parameters; and (4) an executor. Our empirical studies have evidenced the effectiveness and efficiency of the TUMS framework with an average of 19.6\% and 50.6\% improvement separately on easy and hard benchmarks of ToolQA, meanwhile, we demonstrated the key contribution of each part with ablation experiments, offering more insights and stimulating future research on Tool-augmented LLMs.

</details>


### [67] [Hakim: Farsi Text Embedding Model](https://arxiv.org/abs/2505.08435)

*Mehran Sarmadi, Morteza Alikhani, Erfan Zinvandi, Zahra Pourbahman*

**Main category:** cs.CL

**Keywords:** Persian NLP, text embeddings, RAG systems

**Relevance Score:** 5

**TL;DR:** Hakim is a novel Persian text embedding model that outperforms previous models by 8.5%, with new datasets supporting its development for chatbots and RAG systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of large-scale text embedding research for the Persian language.

**Method:** Development of the Hakim model and introduction of three datasets for training scenarios; comparison of Hakim's performance with existing Persian models.

**Key Contributions:**

	1. Introduction of Hakim, a state-of-the-art Persian text embedding model
	2. Creation of three new datasets for training
	3. Establishment of a new baseline model based on BERT architecture

**Result:** Hakim exceeds prior state-of-the-art Persian models by 8.5% on the FaMTEB benchmark and shows higher accuracy on various Persian NLP tasks.

**Limitations:** 

**Conclusion:** The contributions provide a new framework to advance NLP understanding for the Persian language, particularly for retrieval tasks in chat and RAG applications.

**Abstract:** Recent advancements in text embedding have significantly improved natural language understanding across many languages, yet Persian remains notably underrepresented in large-scale embedding research. In this paper, we present Hakim, a novel state-of-the-art Persian text embedding model that achieves a 8.5% performance improvement over existing approaches on the FaMTEB benchmark, outperforming all previously developed Persian language models. As part of this work, we introduce three new datasets - Corpesia, Pairsia-sup, and Pairsia-unsup - to support supervised and unsupervised training scenarios. Additionally, Hakim is designed for applications in chatbots and retrieval-augmented generation (RAG) systems, particularly addressing retrieval tasks that require incorporating message history within these systems. We also propose a new baseline model built on the BERT architecture. Our language model consistently achieves higher accuracy across various Persian NLP tasks, while the RetroMAE-based model proves particularly effective for textual information retrieval applications. Together, these contributions establish a new foundation for advancing Persian language understanding.

</details>


### [68] [A document processing pipeline for the construction of a dataset for topic modeling based on the judgments of the Italian Supreme Court](https://arxiv.org/abs/2505.08439)

*Matteo Marulli, Glauco Panattoni, Marco Bertini*

**Main category:** cs.CL

**Keywords:** Italian legal research, topic modeling, large language models

**Relevance Score:** 4

**TL;DR:** Developed a document processing pipeline that creates an anonymized dataset for better topic modeling in Italian legal research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of public datasets in analyzing legal themes in Supreme Court judgments in Italy.

**Method:** The pipeline consists of document layout analysis using YOLOv8x, optical character recognition, and text anonymization.

**Key Contributions:**

	1. Creation of an anonymized dataset for Italian legal topic modeling.
	2. Integration of YOLOv8x and TrOCR for enhanced document processing.
	3. Evaluation of topic extraction and summarization with large language models.

**Result:** Achieved high performance metrics with mAP@50 of 0.964, mAP@50-95 of 0.800 for DLA, and significant improvements in topic modeling diversity and coherence scores.

**Limitations:** Limited to Italian legal research; may not generalize well to other domains.

**Conclusion:** The developed dataset enables more effective topic modeling and interpretation in legal research using state-of-the-art machine learning techniques.

**Abstract:** Topic modeling in Italian legal research is hindered by the lack of public datasets, limiting the analysis of legal themes in Supreme Court judgments. To address this, we developed a document processing pipeline that produces an anonymized dataset optimized for topic modeling.   The pipeline integrates document layout analysis (YOLOv8x), optical character recognition, and text anonymization. The DLA module achieved a mAP@50 of 0.964 and a mAP@50-95 of 0.800. The OCR detector reached a mAP@50-95 of 0.9022, and the text recognizer (TrOCR) obtained a character error rate of 0.0047 and a word error rate of 0.0248. Compared to OCR-only methods, our dataset improved topic modeling with a diversity score of 0.6198 and a coherence score of 0.6638.   We applied BERTopic to extract topics and used large language models to generate labels and summaries. Outputs were evaluated against domain expert interpretations. Claude Sonnet 3.7 achieved a BERTScore F1 of 0.8119 for labeling and 0.9130 for summarization.

</details>


### [69] [IterKey: Iterative Keyword Generation with LLMs for Enhanced Retrieval Augmented Generation](https://arxiv.org/abs/2505.08450)

*Kazuki Hayashi, Hidetaka Kamigaito, Shinya Kouda, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, keyword generation

**Relevance Score:** 9

**TL;DR:** IterKey is an LLM-driven framework that improves Retrieval-Augmented Generation (RAG) by enhancing sparse retrieval methods, achieving significant accuracy and interpretability gains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for both accuracy and interpretability in Retrieval-Augmented Generation (RAG) methods, which traditionally suffer from trade-offs between dense and sparse retrieval techniques.

**Method:** IterKey employs an iterative three-stage process utilizing Large Language Models (LLMs): generating keywords for retrieval, generating answers based on retrieved documents, and validating those answers, with an iterative refinement of keywords if validation fails.

**Key Contributions:**

	1. Introduces IterKey as a new framework for RAG
	2. Enhances sparse retrieval methods with iterative keyword generation
	3. Achieves significant accuracy improvements while maintaining interpretability

**Result:** IterKey demonstrates a 5% to 20% accuracy improvement over BM25-based RAG and simple baselines, with performance comparable to dense retrieval approaches and prior iterative refinement methods.

**Limitations:** 

**Conclusion:** IterKey effectively balances accuracy and interpretability in RAG by leveraging LLMs for keyword generation and iterative refinement of answers.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a way to complement the in-context knowledge of Large Language Models (LLMs) by integrating external documents. However, real-world applications demand not only accuracy but also interpretability. While dense retrieval methods provide high accuracy, they lack interpretability; conversely, sparse retrieval methods offer transparency but often fail to capture the full intent of queries due to their reliance on keyword matching. To address these issues, we introduce IterKey, an LLM-driven iterative keyword generation framework that enhances RAG via sparse retrieval. IterKey consists of three LLM-driven stages: generating keywords for retrieval, generating answers based on retrieved documents, and validating the answers. If validation fails, the process iteratively repeats with refined keywords. Across four QA tasks, experimental results show that IterKey achieves 5% to 20% accuracy improvements over BM25-based RAG and simple baselines. Its performance is comparable to dense retrieval-based RAG and prior iterative query refinement methods using dense models. In summary, IterKey is a novel BM25-based approach leveraging LLMs to iteratively refine RAG, effectively balancing accuracy with interpretability.

</details>


### [70] [RepCali: High Efficient Fine-tuning Via Representation Calibration in Latent Space for Pre-trained Language Models](https://arxiv.org/abs/2505.08463)

*Fujun Zhang, XiangDong Su*

**Main category:** cs.CL

**Keywords:** pre-trained language models, representation calibration, downstream tasks, fine-tuning, LLMs

**Relevance Score:** 8

**TL;DR:** This paper presents a new method, RepCali, for improving the performance of pre-trained language models (PLMs) by calibrating their latent space representation before decoding, showing significant enhancements in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** PLMs often face challenges in aligning the encoder's representations with the optimal inputs for the decoder, which can hinder performance on downstream tasks.

**Method:** The proposed method, RepCali, integrates a calibration block into the latent space after the encoder, adjusting the representations before passing them to the decoder.

**Key Contributions:**

	1. Introduction of the RepCali method for representation calibration in PLMs.
	2. Demonstrated improvement over existing fine-tuning baselines across multiple tasks.
	3. Validation on a diverse set of PLM-based models and datasets.

**Result:** Extensive experiments demonstrate that RepCali leads to significant performance improvements across 25 PLM-based models on 8 tasks, outperforming baseline fine-tuning methods.

**Limitations:** 

**Conclusion:** RepCali is a universal, plug-and-play method that can enhance PLM performance in decoding tasks, applicable to both English and Chinese datasets.

**Abstract:** Fine-tuning pre-trained language models (PLMs) has become a dominant paradigm in applying PLMs to downstream tasks. However, with limited fine-tuning, PLMs still struggle with the discrepancies between the representation obtained from the PLMs' encoder and the optimal input to the PLMs' decoder. This paper tackles this challenge by learning to calibrate the representation of PLMs in the latent space. In the proposed representation calibration method (RepCali), we integrate a specific calibration block to the latent space after the encoder and use the calibrated output as the decoder input. The merits of the proposed RepCali include its universality to all PLMs with encoder-decoder architectures, its plug-and-play nature, and ease of implementation. Extensive experiments on 25 PLM-based models across 8 tasks (including both English and Chinese datasets) demonstrate that the proposed RepCali offers desirable enhancements to PLMs (including LLMs) and significantly improves the performance of downstream tasks. Comparison experiments across 4 benchmark tasks indicate that RepCali is superior to the representative fine-tuning baselines.

</details>


### [71] [Large Language Models Meet Stance Detection: A Survey of Tasks, Methods, Applications, Challenges and Future Directions](https://arxiv.org/abs/2505.08464)

*Lata Pangtey, Anukriti Bhatnagar, Shubhi Bansal, Shahid Shafi Dar, Nagendra Kumar*

**Main category:** cs.CL

**Keywords:** Stance detection, Large Language Models, Machine Learning, Human-Computer Interaction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This survey reviews advances in stance detection using Large Language Models, covering methodologies, applications, and challenges.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive surveys on LLMs specifically for stance detection.

**Method:** A systematic analysis and taxonomy of LLM-based stance detection approaches based on learning methods, data modalities, and target relationships.

**Key Contributions:**

	1. Novel taxonomy for LLM-based stance detection
	2. Comprehensive review of methodologies and applications
	3. Identification of critical challenges and future research directions

**Result:** Identifies key applications in misinformation detection, political analysis, and public health monitoring, with analysis of evaluation techniques and performance trends.

**Limitations:** Limited coverage of certain architectures and their specific applications in stance detection.

**Conclusion:** Outlines challenges and future directions for stance detection systems powered by LLMs, highlighting the importance of explainability and real-time capabilities.

**Abstract:** Stance detection is essential for understanding subjective content across various platforms such as social media, news articles, and online reviews. Recent advances in Large Language Models (LLMs) have revolutionized stance detection by introducing novel capabilities in contextual understanding, cross-domain generalization, and multimodal analysis. Despite these progressions, existing surveys often lack comprehensive coverage of approaches that specifically leverage LLMs for stance detection. To bridge this critical gap, our review article conducts a systematic analysis of stance detection, comprehensively examining recent advancements of LLMs transforming the field, including foundational concepts, methodologies, datasets, applications, and emerging challenges. We present a novel taxonomy for LLM-based stance detection approaches, structured along three key dimensions: 1) learning methods, including supervised, unsupervised, few-shot, and zero-shot; 2) data modalities, such as unimodal, multimodal, and hybrid; and 3) target relationships, encompassing in-target, cross-target, and multi-target scenarios. Furthermore, we discuss the evaluation techniques and analyze benchmark datasets and performance trends, highlighting the strengths and limitations of different architectures. Key applications in misinformation detection, political analysis, public health monitoring, and social media moderation are discussed. Finally, we identify critical challenges such as implicit stance expression, cultural biases, and computational constraints, while outlining promising future directions, including explainable stance reasoning, low-resource adaptation, and real-time deployment frameworks. Our survey highlights emerging trends, open challenges, and future directions to guide researchers and practitioners in developing next-generation stance detection systems powered by large language models.

</details>


### [72] [Judging the Judges: Can Large Vision-Language Models Fairly Evaluate Chart Comprehension and Reasoning?](https://arxiv.org/abs/2505.08468)

*Md Tahmid Rahman Laskar, Mohammed Saidul Islam, Ridwan Mahbub, Ahmed Masry, Mizanur Rahman, Amran Bhuiyan, Mir Tafseer Nayeem, Shafiq Joty, Enamul Hoque, Jimmy Huang*

**Main category:** cs.CL

**Keywords:** chart comprehension, large vision-language models, evaluation frameworks

**Relevance Score:** 6

**TL;DR:** The paper evaluates 13 open-source large vision-language models (LVLMs) for their effectiveness in chart comprehension tasks, demonstrating variability in their performance as judges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To streamline the evaluation of chart comprehension abilities in large vision-language models, addressing challenges like cost and availability of proprietary datasets.

**Method:** A standardized evaluation protocol was designed, using both pairwise and pointwise tasks to measure judges on factual correctness, informativeness, and relevancy across 13 LVLMs with under 10 billion parameters.

**Key Contributions:**

	1. Evaluation of 13 open-source LVLMs for chart comprehension tasks
	2. Development of a standardized evaluation protocol
	3. Insights into biases affecting LVLM performance

**Result:** Some LVLMs attained evaluation performance comparable to GPT-4 (80% agreement), while others showed significantly lower accuracy (around 10% agreement).

**Limitations:** Biases such as positional preference and length bias may impact evaluation reliability.

**Conclusion:** State-of-the-art open-source LVLMs can effectively evaluate chart-related tasks, though biases exist that affect their reliability.

**Abstract:** Charts are ubiquitous as they help people understand and reason with data. Recently, various downstream tasks, such as chart question answering, chart2text, and fact-checking, have emerged. Large Vision-Language Models (LVLMs) show promise in tackling these tasks, but their evaluation is costly and time-consuming, limiting real-world deployment. While using LVLMs as judges to assess the chart comprehension capabilities of other LVLMs could streamline evaluation processes, challenges like proprietary datasets, restricted access to powerful models, and evaluation costs hinder their adoption in industrial settings. To this end, we present a comprehensive evaluation of 13 open-source LVLMs as judges for diverse chart comprehension and reasoning tasks. We design both pairwise and pointwise evaluation tasks covering criteria like factual correctness, informativeness, and relevancy. Additionally, we analyze LVLM judges based on format adherence, positional consistency, length bias, and instruction-following. We focus on cost-effective LVLMs (<10B parameters) suitable for both research and commercial use, following a standardized evaluation protocol and rubric to measure the LVLM judge's accuracy. Experimental results reveal notable variability: while some open LVLM judges achieve GPT-4-level evaluation performance (about 80% agreement with GPT-4 judgments), others struggle (below ~10% agreement). Our findings highlight that state-of-the-art open-source LVLMs can serve as cost-effective automatic evaluators for chart-related tasks, though biases such as positional preference and length bias persist.

</details>


### [73] [LCES: Zero-shot Automated Essay Scoring via Pairwise Comparisons Using Large Language Models](https://arxiv.org/abs/2505.08498)

*Takumi Shibata, Yuichi Miyamura*

**Main category:** cs.CL

**Keywords:** large language models, automated essay scoring, pairwise comparison, RankNet, zero-shot

**Relevance Score:** 9

**TL;DR:** The paper introduces a new method, LLM-based Comparative Essay Scoring (LCES), which improves automated essay scoring by using pairwise comparisons instead of direct scoring, enhancing accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the discrepancies between large language model-generated scores and human evaluations in automated essay scoring.

**Method:** The methodology involves using LLMs to judge which of two essays is better and using RankNet to convert the LLM preferences into continuous scores, improving scalability and efficiency.

**Key Contributions:**

	1. Introduction of LLM-based Comparative Essay Scoring (LCES) method
	2. Employs pairwise comparisons to enhance scoring accuracy
	3. Utilizes RankNet to manage scalability in comparisons

**Result:** Experiments demonstrate that LCES outperforms traditional zero-shot methods in accuracy while remaining computationally efficient.

**Limitations:** 

**Conclusion:** The LCES method is robust across various LLM backbones and is applicable to real-world zero-shot automated essay scoring scenarios.

**Abstract:** Recent advances in large language models (LLMs) have enabled zero-shot automated essay scoring (AES), providing a promising way to reduce the cost and effort of essay scoring in comparison with manual grading. However, most existing zero-shot approaches rely on LLMs to directly generate absolute scores, which often diverge from human evaluations owing to model biases and inconsistent scoring. To address these limitations, we propose LLM-based Comparative Essay Scoring (LCES), a method that formulates AES as a pairwise comparison task. Specifically, we instruct LLMs to judge which of two essays is better, collect many such comparisons, and convert them into continuous scores. Considering that the number of possible comparisons grows quadratically with the number of essays, we improve scalability by employing RankNet to efficiently transform LLM preferences into scalar scores. Experiments using AES benchmark datasets show that LCES outperforms conventional zero-shot methods in accuracy while maintaining computational efficiency. Moreover, LCES is robust across different LLM backbones, highlighting its applicability to real-world zero-shot AES.

</details>


### [74] [Reassessing Graph Linearization for Sequence-to-sequence AMR Parsing: On the Advantages and Limitations of Triple-Based Encoding](https://arxiv.org/abs/2505.08504)

*Jeongwoo Kang, Maximin Coavoux, Cédric Lopez, Didier Schwab*

**Main category:** cs.CL

**Keywords:** Abstract Meaning Representation, linearization, triple-based encoding, NLP, graph representation

**Relevance Score:** 6

**TL;DR:** The paper critiques Penman encoding for AMR parsing and proposes an alternative triple-based linearization method, highlighting its strengths and weaknesses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the linearization of Abstract Meaning Representation (AMR) graphs for better efficiency in sequence-to-sequence models used for parsing.

**Method:** The paper introduces a triple-based linearization method for AMR graphs and compares its performance against the traditional Penman encoding approach.

**Key Contributions:**

	1. Introduction of a triple-based linearization method for AMR
	2. Comparison with Penman encoding highlighting the limitations of both methods
	3. Insights into the representation of nested graph structures in NLP.

**Result:** The comparison reveals that while the triple-based method is suited for graph representation, it still falls short in conciseness and clarity compared to Penman's encoding of nested structures.

**Limitations:** The proposed triple encoding still requires improvements to compete effectively with Penman's method.

**Conclusion:** The study suggests that despite the potential of triple encoding, further refinement is necessary for it to rival Penman's established framework in efficiency.

**Abstract:** Sequence-to-sequence models are widely used to train Abstract Meaning Representation (Banarescu et al., 2013, AMR) parsers. To train such models, AMR graphs have to be linearized into a one-line text format. While Penman encoding is typically used for this purpose, we argue that it has limitations: (1) for deep graphs, some closely related nodes are located far apart in the linearized text (2) Penman's tree-based encoding necessitates inverse roles to handle node re-entrancy, doubling the number of relation types to predict. To address these issues, we propose a triple-based linearization method and compare its efficiency with Penman linearization. Although triples are well suited to represent a graph, our results suggest room for improvement in triple encoding to better compete with Penman's concise and explicit representation of a nested graph structure.

</details>


### [75] [Are We Paying Attention to Her? Investigating Gender Disambiguation and Attention in Machine Translation](https://arxiv.org/abs/2505.08546)

*Chiara Manna, Afra Alishahi, Frédéric Blain, Eva Vanmassenhove*

**Main category:** cs.CL

**Keywords:** gender bias, Neural Machine Translation, Minimal Pair Accuracy, gender cues, evaluation metrics

**Relevance Score:** 7

**TL;DR:** This paper introduces a new metric, Minimal Pair Accuracy, to evaluate gender bias in Neural Machine Translation by measuring how models use gender cues for disambiguation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation metrics for gender bias in Neural Machine Translation (NMT) are inadequate in capturing how well models integrate contextual gender cues.

**Method:** The paper proposes Minimal Pair Accuracy (MPA), which assesses reliance on gender cues by comparing translations of minimal pairs—sentence pairs differing only by gender pronouns.

**Key Contributions:**

	1. Introduction of Minimal Pair Accuracy as a new evaluation metric for gender bias in NMT.
	2. Demonstration of systemic bias in NMT models favoring masculine over feminine cues.
	3. Analysis of attention head weights revealing how gender cues are differentially processed in translations.

**Result:** Evaluation of various NMT models on the English-Italian language pair reveals that models often overlook available gender cues in favor of stereotypical interpretations, especially favoring masculine cues.

**Limitations:** 

**Conclusion:** The findings suggest that while NMT models encode gender information, they show bias towards masculine interpretations, neglecting feminine cues more frequently.

**Abstract:** While gender bias in modern Neural Machine Translation (NMT) systems has received much attention, traditional evaluation metrics do not to fully capture the extent to which these systems integrate contextual gender cues. We propose a novel evaluation metric called Minimal Pair Accuracy (MPA), which measures the reliance of models on gender cues for gender disambiguation. MPA is designed to go beyond surface-level gender accuracy metrics by focusing on whether models adapt to gender cues in minimal pairs -- sentence pairs that differ solely in the gendered pronoun, namely the explicit indicator of the target's entity gender in the source language (EN). We evaluate a number of NMT models on the English-Italian (EN--IT) language pair using this metric, we show that they ignore available gender cues in most cases in favor of (statistical) stereotypical gender interpretation. We further show that in anti-stereotypical cases, these models tend to more consistently take masculine gender cues into account while ignoring the feminine cues. Furthermore, we analyze the attention head weights in the encoder component and show that while all models encode gender information to some extent, masculine cues elicit a more diffused response compared to the more concentrated and specialized responses to feminine gender cues.

</details>


### [76] [Small but Significant: On the Promise of Small Language Models for Accessible AIED](https://arxiv.org/abs/2505.08588)

*Yumou Wei, Paulo Carvalho, John Stamper*

**Main category:** cs.CL

**Keywords:** small language models, large language models, AI in education, knowledge component discovery, equitable access

**Relevance Score:** 7

**TL;DR:** The paper advocates for the use of small language models (SLMs) in AI for education, highlighting their potential benefits over large language models (LLMs) like GPT.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in educational AI, particularly the lack of affordable high-quality AI tools for resource-constrained institutions.

**Method:** The paper reviews the prevalence of LLMs in current AIED research and presents evidence on the efficacy of SLMs, specifically Phi-2, in knowledge component discovery without requiring complex prompting strategies.

**Key Contributions:**

	1. Highlighting the limitations of focusing solely on LLMs in educational AI
	2. Providing evidence for effective solutions with small language models
	3. Advocating for equitable access to AI tools in resource-constrained educational settings.

**Result:** Findings indicate that SLMs can effectively contribute to AIED, providing equitable access to AI tools for educational institutions.

**Limitations:** 

**Conclusion:** The authors call for more emphasis on developing AIED approaches that utilize SLMs for wider accessibility and affordability in education.

**Abstract:** GPT has become nearly synonymous with large language models (LLMs), an increasingly popular term in AIED proceedings. A simple keyword-based search reveals that 61% of the 76 long and short papers presented at AIED 2024 describe novel solutions using LLMs to address some of the long-standing challenges in education, and 43% specifically mention GPT. Although LLMs pioneered by GPT create exciting opportunities to strengthen the impact of AI on education, we argue that the field's predominant focus on GPT and other resource-intensive LLMs (with more than 10B parameters) risks neglecting the potential impact that small language models (SLMs) can make in providing resource-constrained institutions with equitable and affordable access to high-quality AI tools. Supported by positive results on knowledge component (KC) discovery, a critical challenge in AIED, we demonstrate that SLMs such as Phi-2 can produce an effective solution without elaborate prompting strategies. Hence, we call for more attention to developing SLM-based AIED approaches.

</details>


### [77] [Enhancing Thyroid Cytology Diagnosis with RAG-Optimized LLMs and Pa-thology Foundation Models](https://arxiv.org/abs/2505.08590)

*Hussien Al-Asi, Jordan P Reynolds, Shweta Agarwal, Bryan J Dangott, Aziza Nassar, Zeynettin Akkus*

**Main category:** cs.CL

**Keywords:** artificial intelligence, pathology, large language models, thyroid cytology, diagnostic accuracy

**Relevance Score:** 9

**TL;DR:** This paper examines the use of RAG-enhanced LLMs in improving thyroid cytology diagnosis by integrating AI-driven approaches to enhance diagnostic accuracy and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in cytological interpretation and diagnostic accuracy in thyroid cytology, leveraging advancements in AI.

**Method:** The study utilizes RAG to dynamically retrieve relevant case studies and diagnostic criteria, combined with pathology foundation models that improve feature extraction and classification from pathology images.

**Key Contributions:**

	1. Integration of RAG with pathology foundation models for cytology diagnosis
	2. Demonstrated improved diagnostic efficiency and interpretability
	3. Foundation model UNI achieved high AUC for surgical pathology prediction

**Result:** The integration of RAG with pathology-specific LLMs showed a significant improvement in diagnostic efficiency and interpretability, with foundation model UNI achieving AUC scores between 0.73 and 0.93 for accurately predicting surgical pathology from thyroid cytology samples.

**Limitations:** 

**Conclusion:** The study validates the effectiveness of using RAG-enhanced LLMs for AI-assisted thyroid cytopathology, highlighting the potential for improved diagnostic consistency and reduced variability.

**Abstract:** Advancements in artificial intelligence (AI) are transforming pathology by integrat-ing large language models (LLMs) with retrieval-augmented generation (RAG) and domain-specific foundation models. This study explores the application of RAG-enhanced LLMs coupled with pathology foundation models for thyroid cytology diagnosis, addressing challenges in cytological interpretation, standardization, and diagnostic accuracy. By leveraging a curated knowledge base, RAG facilitates dy-namic retrieval of relevant case studies, diagnostic criteria, and expert interpreta-tion, improving the contextual understanding of LLMs. Meanwhile, pathology foun-dation models, trained on high-resolution pathology images, refine feature extrac-tion and classification capabilities. The fusion of these AI-driven approaches en-hances diagnostic consistency, reduces variability, and supports pathologists in dis-tinguishing benign from malignant thyroid lesions. Our results demonstrate that integrating RAG with pathology-specific LLMs significantly improves diagnostic efficiency and interpretability, paving the way for AI-assisted thyroid cytopathology, with foundation model UNI achieving AUC 0.73-0.93 for correct prediction of surgi-cal pathology diagnosis from thyroid cytology samples.

</details>


### [78] [Automatic Task Detection and Heterogeneous LLM Speculative Decoding](https://arxiv.org/abs/2505.08600)

*Danying Ge, Jianhua Gao, Qizhi Jiang, Yifei Feng, Weixing Ji*

**Main category:** cs.CL

**Keywords:** speculative decoding, large language models, inference optimization

**Relevance Score:** 8

**TL;DR:** The paper presents a speculative decoding algorithm that optimizes downstream tasks by utilizing a combination of draft and target models, enhancing inference efficiency in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing speculative decoding methods struggle with balancing acceptance rate and decoding speed due to draft model limitations, hindering efficiency for diverse tasks.

**Method:** The proposed algorithm features automatic task partitioning and assignment, categorizing downstream tasks and matching them with heterogeneous draft models aligned with the target model. An online lightweight prompt classifier directs prompts to the suitable draft model.

**Key Contributions:**

	1. Automatic task partitioning and assignment based on downstream tasks.
	2. Alignment of draft models with task-specific data to boost inference consistency.
	3. Implementation of an online prompt classifier for dynamic model routing.

**Result:** The method improves draft accuracy by 6% to 50% compared to traditional speculative decoding and enhances LLM inference speed by 1.10x to 2.64x.

**Limitations:** 

**Conclusion:** The proposed speculative decoding approach effectively enhances the efficiency and accuracy of large language model inference across various downstream tasks.

**Abstract:** Speculative decoding, which combines a draft model with a target model, has emerged as an effective approach to accelerate large language model (LLM) inference. However, existing methods often face a trade-off between the acceptance rate and decoding speed in downstream tasks due to the limited capacity of the draft model, making it difficult to ensure efficiency across diverse tasks. To address this problem, we propose a speculative decoding algorithm tailored for downstream task optimization. It includes an automatic task partitioning and assigning method, which automatically categorizes downstream tasks into different sub-tasks and assigns them to a set of heterogeneous draft models. Each draft model is aligned with the target model using task-specific data, thereby enhancing the consistency of inference results. In addition, our proposed method incorporates an online lightweight prompt classifier to dynamically route prompts to the appropriate draft model. Experimental results demonstrate that the proposed method improves draft accuracy by 6% to 50% over vanilla speculative decoding, while achieving a speedup of 1.10x to 2.64x in LLM inference.

</details>


### [79] [Scaling Context, Not Parameters: Training a Compact 7B Language Model for Efficient Long-Context Processing](https://arxiv.org/abs/2505.08651)

*Chen Wu, Yin Song*

**Main category:** cs.CL

**Keywords:** language model, long-context, open-source, in-context learning, AI applications

**Relevance Score:** 8

**TL;DR:** MegaBeam-Mistral-7B is a language model designed for long-context tasks, supporting up to 512K tokens with competitive performance in real-world applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper tackles the limitations of long-context training in language models, emphasizing the need for effective tools in tasks like compliance monitoring and verification.

**Method:** The authors present a 7B-parameter model evaluated on three long-context benchmarks, demonstrating its in-context learning and retrieval capabilities.

**Key Contributions:**

	1. Support for 512K-token context length
	2. Competitive performance on long-context benchmarks
	3. Open-source availability for real-world application

**Result:** MegaBeam-Mistral-7B outperforms existing models on HELMET and RULER benchmarks, marking it as a leading open-source solution for long-range reasoning without additional fine-tuning.

**Limitations:** 

**Conclusion:** The model is released under an open-source license and shows significant practical value for applications requiring long-context reasoning.

**Abstract:** We present MegaBeam-Mistral-7B, a language model that supports 512K-token context length. Our work addresses practical limitations in long-context training, supporting real-world tasks such as compliance monitoring and verification. Evaluated on three long-context benchmarks, our 7B-parameter model demonstrates superior in-context learning performance on HELMET and robust retrieval and tracing capability on RULER. It is currently the only open model to achieve competitive long-range reasoning on BABILong at 512K context length without RAG or targeted fine-tuning. Released as fully open source under the Apache 2.0 license, the model has been downloaded over 100,000 times on Hugging Face. Model available at: https://huggingface.co/aws-prototyping/MegaBeam-Mistral-7B-512k

</details>


### [80] [Revealing economic facts: LLMs know more than they say](https://arxiv.org/abs/2505.08662)

*Marcus Buckmann, Quynh Anh Nguyen, Edward Hill*

**Main category:** cs.CL

**Keywords:** large language models, hidden states, economic statistics, data imputation, transfer learning

**Relevance Score:** 7

**TL;DR:** The paper explores using hidden states of large language models (LLMs) to improve the estimation and imputation of economic and financial statistics, showing superior performance compared to traditional text outputs.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of LLM hidden states for economic statistic estimation and imputation, focusing on county-level and firm-level variables.

**Method:** A linear model is trained on the hidden states of open-source LLMs, with experiments on learning curves and transfer learning methods enhancing accuracy without needing labeled data.

**Key Contributions:**

	1. Demonstrated the superiority of hidden state-based models over text output for financial and economic statistics.
	2. Introduced a transfer learning method for improved accuracy without labeled data.
	3. Provided evidence that minimal labeled data is sufficient for training.

**Result:** The model trained on hidden states outperforms text outputs, revealing that hidden states contain richer economic information. A few dozen labeled examples suffice for effective training, and transfer learning further boosts accuracy.

**Limitations:** The study may be limited by the types of financial statistics researched and the specific LLMs used in the analysis.

**Conclusion:** Hidden-state representations of LLMs can significantly enhance data imputation and super-resolution tasks in economic contexts, demonstrating wider utility beyond mere text generation.

**Abstract:** We investigate whether the hidden states of large language models (LLMs) can be used to estimate and impute economic and financial statistics. Focusing on county-level (e.g. unemployment) and firm-level (e.g. total assets) variables, we show that a simple linear model trained on the hidden states of open-source LLMs outperforms the models' text outputs. This suggests that hidden states capture richer economic information than the responses of the LLMs reveal directly. A learning curve analysis indicates that only a few dozen labelled examples are sufficient for training. We also propose a transfer learning method that improves estimation accuracy without requiring any labelled data for the target variable. Finally, we demonstrate the practical utility of hidden-state representations in super-resolution and data imputation tasks.

</details>


### [81] [Adaptive Schema-aware Event Extraction with Retrieval-Augmented Generation](https://arxiv.org/abs/2505.08690)

*Sheng Liang, Hang Lv, Zhihao Wen, Yaxiong Wu, Yongyue Zhang, Hao Wang, Yong Liu*

**Main category:** cs.CL

**Keywords:** event extraction, natural language processing, schema retrieval, benchmark, large language models

**Relevance Score:** 8

**TL;DR:** The paper introduces Adaptive Schema-aware Event Extraction (ASEE), which combines schema paraphrasing and schema retrieval-augmented generation to improve event extraction accuracy by addressing gaps in existing methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this research is to improve event extraction in NLP by addressing the rigid schema fixation in current systems and the lack of evaluation benchmarks for joint schema matching and extraction.

**Method:** The proposed method, ASEE, involves retrieving paraphrased schemas and generating structured output using a novel approach that integrates both paraphrasing and schema retrieval-augmented generation.

**Key Contributions:**

	1. Introduction of ASEE, integrating schema paraphrasing and retrieval-augmented generation
	2. Creation of the MD-SEE benchmark for evaluating event extraction methods
	3. Demonstration of improved adaptability and accuracy over existing event extraction techniques

**Result:** The ASEE method demonstrates strong adaptability and significantly enhances the accuracy of event extraction across various scenarios when evaluated on the newly constructed MD-SEE benchmark.

**Limitations:** The paper acknowledges the potential challenges in practical deployment due to LLMs' schema hallucination tendencies and context window limitations.

**Conclusion:** The findings indicate that ASEE is a promising approach to overcoming the limitations of existing event extraction methods in NLP, showcasing improvements in schema matching and extraction accuracy.

**Abstract:** Event extraction (EE) is a fundamental task in natural language processing (NLP) that involves identifying and extracting event information from unstructured text. Effective EE in real-world scenarios requires two key steps: selecting appropriate schemas from hundreds of candidates and executing the extraction process. Existing research exhibits two critical gaps: (1) the rigid schema fixation in existing pipeline systems, and (2) the absence of benchmarks for evaluating joint schema matching and extraction. Although large language models (LLMs) offer potential solutions, their schema hallucination tendencies and context window limitations pose challenges for practical deployment. In response, we propose Adaptive Schema-aware Event Extraction (ASEE), a novel paradigm combining schema paraphrasing with schema retrieval-augmented generation. ASEE adeptly retrieves paraphrased schemas and accurately generates targeted structures. To facilitate rigorous evaluation, we construct the Multi-Dimensional Schema-aware Event Extraction (MD-SEE) benchmark, which systematically consolidates 12 datasets across diverse domains, complexity levels, and language settings. Extensive evaluations on MD-SEE show that our proposed ASEE demonstrates strong adaptability across various scenarios, significantly improving the accuracy of event extraction.

</details>


### [82] [NurValues: Real-World Nursing Values Evaluation for Large Language Models in Clinical Context](https://arxiv.org/abs/2505.08734)

*Ben Yao, Qiuchi Li, Yazhou Zhang, Siyu Yang, Bohan Zhang, Prayag Tiwari, Jing Qin*

**Main category:** cs.CL

**Keywords:** nursing, value alignment, LLM, healthcare, benchmark

**Relevance Score:** 7

**TL;DR:** Introduces a benchmark for nursing value alignment using LLMs based on real-world nursing behaviors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a framework for assessing and aligning nursing values in the context of AI applications in healthcare.

**Method:** A longitudinal field study in three hospitals collected 1,100 nursing behavior instances, which were annotated and transformed into datasets for evaluating LLMs.

**Key Contributions:**

	1. Creation of the first nursing value alignment benchmark
	2. Introduction of a dialogue-based dataset for LLM evaluation
	3. Insights into nursing values and model performance dynamics

**Result:** State-of-the-art models were evaluated, revealing DeepSeek-V3 as the top performer on the Easy-Level dataset, and significant insights about the challenges of evaluating specific nursing values.

**Limitations:** 

**Conclusion:** The study lays the groundwork for developing value-sensitive large language models in clinical settings.

**Abstract:** This work introduces the first benchmark for nursing value alignment, consisting of five core value dimensions distilled from international nursing codes: Altruism, Human Dignity, Integrity, Justice, and Professionalism. The benchmark comprises 1,100 real-world nursing behavior instances collected through a five-month longitudinal field study across three hospitals of varying tiers. These instances are annotated by five clinical nurses and then augmented with LLM-generated counterfactuals with reversed ethic polarity. Each original case is paired with a value-aligned and a value-violating version, resulting in 2,200 labeled instances that constitute the Easy-Level dataset. To increase adversarial complexity, each instance is further transformed into a dialogue-based format that embeds contextual cues and subtle misleading signals, yielding a Hard-Level dataset. We evaluate 23 state-of-the-art (SoTA) LLMs on their alignment with nursing values. Our findings reveal three key insights: (1) DeepSeek-V3 achieves the highest performance on the Easy-Level dataset (94.55), where Claude 3.5 Sonnet outperforms other models on the Hard-Level dataset (89.43), significantly surpassing the medical LLMs; (2) Justice is consistently the most difficult nursing value dimension to evaluate; and (3) in-context learning significantly improves alignment. This work aims to provide a foundation for value-sensitive LLMs development in clinical settings. The dataset and the code are available at https://huggingface.co/datasets/Ben012345/NurValues.

</details>


### [83] [Probability Consistency in Large Language Models: Theoretical Foundations Meet Empirical Discrepancies](https://arxiv.org/abs/2505.08739)

*Xiaoliang Luo, Xinyi Xu, Michael Ramscar, Bradley C. Love*

**Main category:** cs.CL

**Keywords:** Large Language Models, Sequence Perplexity, Positional Biases, Self-Attention, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper investigates whether autoregressive large language models (LLMs) can maintain consistent probability distributions when trained on sequences with different token orders, proving that sequence perplexity remains invariant across various permutations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to understand how LLMs learn from data and to develop protocols for empirically evaluating their performance across different training orders.

**Method:** The authors apply formal proofs to establish the theoretical invariance of sequence perplexity, then empirically retrain GPT-2 models on scientific text across various token orderings (forward, backward, and arbitrary permutations).

**Key Contributions:**

	1. Formal proof of invariance of sequence perplexity under different token orders
	2. Empirical evaluation demonstrating deviations from theoretical expectations in LLMs
	3. Insights into positional and locality biases affecting LLM performance

**Result:** The retrained models exhibited systematic deviations from theoretical invariance, particularly with arbitrary permutations of sequences, revealing biases in self-attention mechanisms.

**Limitations:** The study focuses on GPT-2 and may not generalize to all LLM architectures; future research is needed to establish broader applicability.

**Conclusion:** The findings highlight the importance of understanding positional biases in LLMs and provide methods for detecting when their probability distributions may be unreliable.

**Abstract:** Can autoregressive large language models (LLMs) learn consistent probability distributions when trained on sequences in different token orders? We prove formally that for any well-defined probability distribution, sequence perplexity is invariant under any factorization, including forward, backward, or arbitrary permutations. This result establishes a rigorous theoretical foundation for studying how LLMs learn from data and defines principled protocols for empirical evaluation. Applying these protocols, we show that prior studies examining ordering effects suffer from critical methodological flaws. We retrain GPT-2 models across forward, backward, and arbitrary permuted orders on scientific text. We find systematic deviations from theoretical invariance across all orderings with arbitrary permutations strongly deviating from both forward and backward models, which largely (but not completely) agreed with one another. Deviations were traceable to differences in self-attention, reflecting positional and locality biases in processing. Our theoretical and empirical results provide novel avenues for understanding positional biases in LLMs and suggest methods for detecting when LLMs' probability distributions are inconsistent and therefore untrustworthy.

</details>


### [84] [AC-Reason: Towards Theory-Guided Actual Causality Reasoning with Large Language Models](https://arxiv.org/abs/2505.08750)

*Yanxi Zhang, Xin Cong, Zhong Zhang, Xiao Liu, Dongyan Zhao, Yesai Wu*

**Main category:** cs.CL

**Keywords:** actual causality, causal reasoning, LLM, benchmark, reasoning framework

**Relevance Score:** 8

**TL;DR:** AC-Reason enhances causal reasoning in LLMs using a semi-formal framework and introduces AC-Bench, a benchmark for evaluating actual causality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM methods lack interpretability and grounding in formal actual causality theory, which is essential for responsible attribution in real-world scenarios.

**Method:** The paper proposes AC-Reason, a reasoning framework that infers formal causal factors in actual causality scenarios and answers AC queries, evaluated using a new benchmark called AC-Bench.

**Key Contributions:**

	1. Introduction of the AC-Reason framework for enhanced causal reasoning in LLMs
	2. Development of AC-Bench for evaluating actual causation with annotated samples
	3. Demonstration of significant performance improvements in LLMs when using AC theoretical integration

**Result:** AC-Reason significantly improves LLM performance, with GPT-4 + AC-Reason achieving 75.04% accuracy on BBH-CJ and 71.82% on AC-Bench, surpassing human average accuracy.

**Limitations:** The framework does not explicitly construct causal graphs, which may limit the depth of causation analysis.

**Conclusion:** Integrating actual causality theory into LLMs is beneficial for performance and reasoning faithfulness, although only a few models exhibit true faithful reasoning.

**Abstract:** Actual causality (AC), a fundamental aspect of causal reasoning (CR), is responsible for attribution and responsibility assignment in real-world scenarios. However, existing LLM-based methods lack grounding in formal AC theory, resulting in limited interpretability. Therefore, we propose AC-Reason, a semi-formal reasoning framework that identifies causally relevant events within an AC scenario, infers the values of their formal causal factors (e.g., sufficiency, necessity, and normality), and answers AC queries via a theory-guided algorithm with explanations. While AC-Reason does not explicitly construct a causal graph, it operates over variables in the underlying causal structure to support principled reasoning. To enable comprehensive evaluation, we introduce AC-Bench, a new benchmark built upon and substantially extending Big-Bench Hard Causal Judgment (BBH-CJ). AC-Bench comprises ~1K carefully annotated samples, each with detailed reasoning steps and focuses solely on actual causation. The case study shows that synthesized samples in AC-Bench present greater challenges for LLMs. Extensive experiments on BBH-CJ and AC-Bench show that AC-Reason consistently improves LLM performance over baselines. On BBH-CJ, all tested LLMs surpass the average human rater accuracy of 69.60%, with GPT-4 + AC-Reason achieving 75.04%. On AC-Bench, GPT-4 + AC-Reason again achieves the highest accuracy of 71.82%. AC-Bench further enables fine-grained analysis of reasoning faithfulness, revealing that only Qwen-2.5-72B-Instruct, Claude-3.5-Sonnet, and GPT-4o exhibit faithful reasoning, whereas GPT-4 tends to exploit shortcuts. Finally, our ablation study proves that integrating AC theory into LLMs is highly effective, with the proposed algorithm contributing the most significant performance gains.

</details>


### [85] [Aya Vision: Advancing the Frontier of Multilingual Multimodality](https://arxiv.org/abs/2505.08751)

*Saurabh Dash, Yiyang Nan, John Dang, Arash Ahmadian, Shivalika Singh, Madeline Smith, Bharat Venkitesh, Vlad Shmyhlo, Viraat Aryabumi, Walter Beller-Morales, Jeremy Pekmez, Jason Ozuzu, Pierre Richemond, Acyr Locatelli, Nick Frosst, Phil Blunsom, Aidan Gomez, Ivan Zhang, Marzieh Fadaee, Manoj Govindassamy, Sudip Roy, Matthias Gallé, Beyza Ermis, Ahmet Üstün, Sara Hooker*

**Main category:** cs.CL

**Keywords:** multimodal language models, multilingual, catastrophic forgetting, human-preferred responses, cross-modal model merging

**Relevance Score:** 7

**TL;DR:** This paper presents novel techniques for building multimodal language models, focusing on aligning vision and language, curating multilingual data, and addressing catastrophic forgetting.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of integrating vision with language modalities in multilingual settings, focusing on data scarcity and the risk of degrading text-only capabilities when adding vision.

**Method:** The authors introduce a synthetic annotation framework to create high-quality multilingual multimodal instruction data and a cross-modal model merging technique to prevent catastrophic forgetting and enhance multimodal performance.

**Key Contributions:**

	1. Development of a synthetic annotation framework for multilingual multimodal data.
	2. Introduction of a cross-modal model merging technique to mitigate catastrophic forgetting.
	3. Achievement of best-in-class performance in multimodal tasks with smaller models.

**Result:** Aya-Vision-8B demonstrates superior performance compared to existing multimodal models like Qwen-2.5-VL-7B and Pixtral-12B, and Aya-Vision-32B surpasses larger models such as Molmo-72B and LLaMA-3.2-90B-Vision.

**Limitations:** 

**Conclusion:** The work contributes to advancing multilingual multimodal model development and presents effective techniques for improving model performance while managing computational resources.

**Abstract:** Building multimodal language models is fundamentally challenging: it requires aligning vision and language modalities, curating high-quality instruction data, and avoiding the degradation of existing text-only capabilities once vision is introduced. These difficulties are further magnified in the multilingual setting, where the need for multimodal data in different languages exacerbates existing data scarcity, machine translation often distorts meaning, and catastrophic forgetting is more pronounced. To address the aforementioned challenges, we introduce novel techniques spanning both data and modeling. First, we develop a synthetic annotation framework that curates high-quality, diverse multilingual multimodal instruction data, enabling Aya Vision models to produce natural, human-preferred responses to multimodal inputs across many languages. Complementing this, we propose a cross-modal model merging technique that mitigates catastrophic forgetting, effectively preserving text-only capabilities while simultaneously enhancing multimodal generative performance. Aya-Vision-8B achieves best-in-class performance compared to strong multimodal models such as Qwen-2.5-VL-7B, Pixtral-12B, and even much larger Llama-3.2-90B-Vision. We further scale this approach with Aya-Vision-32B, which outperforms models more than twice its size, such as Molmo-72B and LLaMA-3.2-90B-Vision. Our work advances multilingual progress on the multi-modal frontier, and provides insights into techniques that effectively bend the need for compute while delivering extremely high performance.

</details>


### [86] [HealthBench: Evaluating Large Language Models Towards Improved Human Health](https://arxiv.org/abs/2505.08775)

*Rahul K. Arora, Jason Wei, Rebecca Soskin Hicks, Preston Bowman, Joaquin Quiñonero-Candela, Foivos Tsimpourlas, Michael Sharman, Meghan Shah, Andrea Vallone, Alex Beutel, Johannes Heidecke, Karan Singhal*

**Main category:** cs.CL

**Keywords:** HealthBench, large language models, healthcare evaluation, benchmark, AI in health

**Relevance Score:** 9

**TL;DR:** HealthBench is an open-source benchmark for evaluating large language models in healthcare through realistic and open-ended conversation criteria.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to provide a comprehensive tool for assessing the performance and safety of large language models in healthcare, moving beyond traditional evaluation methods.

**Method:** HealthBench consists of 5,000 multi-turn conversations evaluated by 262 physicians using 48,562 unique rubric criteria across various health contexts and dimensions.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for LLM evaluation in healthcare.
	2. Development of open-ended evaluation criteria with physician validation.
	3. Release of variations focusing on critical dimensions of model behavior.

**Result:** The performance of models like GPT-3.5 Turbo and GPT-4o show significant improvements, with smaller models like GPT-4.1 nano outperforming larger ones while being more cost-effective.

**Limitations:** 

**Conclusion:** HealthBench aims to facilitate advancements in model development and applications that can positively impact human health.

**Abstract:** We present HealthBench, an open-source benchmark measuring the performance and safety of large language models in healthcare. HealthBench consists of 5,000 multi-turn conversations between a model and an individual user or healthcare professional. Responses are evaluated using conversation-specific rubrics created by 262 physicians. Unlike previous multiple-choice or short-answer benchmarks, HealthBench enables realistic, open-ended evaluation through 48,562 unique rubric criteria spanning several health contexts (e.g., emergencies, transforming clinical data, global health) and behavioral dimensions (e.g., accuracy, instruction following, communication). HealthBench performance over the last two years reflects steady initial progress (compare GPT-3.5 Turbo's 16% to GPT-4o's 32%) and more rapid recent improvements (o3 scores 60%). Smaller models have especially improved: GPT-4.1 nano outperforms GPT-4o and is 25 times cheaper. We additionally release two HealthBench variations: HealthBench Consensus, which includes 34 particularly important dimensions of model behavior validated via physician consensus, and HealthBench Hard, where the current top score is 32%. We hope that HealthBench grounds progress towards model development and applications that benefit human health.

</details>


### [87] [Bridging LLMs and KGs without Fine-Tuning: Intermediate Probing Meets Subgraph-Aware Entity Descriptions](https://arxiv.org/abs/2408.06787)

*Bo Xue, Yi Xu, Yunchong Song, Yiming Pang, Yuyang Ren, Jiaxin Ding, Luoyi Fu, Xinbing Wang*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Large Language Models, Data-efficient Classifier

**Relevance Score:** 8

**TL;DR:** This paper presents a novel approach to knowledge graph completion (KGC) that effectively utilizes frozen large language models (LLMs) to improve efficiency and classification performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional KGC methods struggle with knowledge graph sparsity and rely solely on structural information, leading to suboptimal performance.

**Method:** The proposed methodology employs prompts to stimulate the intermediate layers of frozen LLMs, capturing context-aware hidden states of knowledge triples. A data-efficient classifier is then trained on these hidden states, supplemented by generating detailed entity descriptions through subgraph sampling.

**Key Contributions:**

	1. Utilization of frozen LLMs for KGC through intermediate layer stimulation
	2. A data-efficient classifier that reduces ambiguity in knowledge representation
	3. Significant improvements in memory efficiency and training speed compared to traditional methods

**Result:** The approach outperforms traditional KGC methods and achieves classification performance comparable to fine-tuned LLMs, while significantly improving GPU memory efficiency and reducing training and inference times.

**Limitations:** 

**Conclusion:** The effectiveness and efficiency of leveraging LLMs for knowledge graph completion are demonstrated through extensive experiments, showing notable improvements over existing methods.

**Abstract:** Traditional knowledge graph completion (KGC) methods rely solely on structural information, struggling with the inherent sparsity of knowledge graphs (KGs). Large Language Models (LLMs) learn extensive knowledge from large corpora with powerful context modeling, making them promising for mitigating the limitations of previous methods. Directly fine-tuning LLMs offers great capability but comes at the cost of huge time and memory consumption, while utilizing frozen LLMs yields suboptimal results.In this work, we aim to leverage LLMs for KGC effectively and efficiently. We capture the context-aware hidden states of knowledge triples by employing prompts to stimulate the intermediate layers of LLMs. We then train a data-efficient classifier on these hidden states to harness the inherent capabilities of frozen LLMs in KGC. Additionally, to reduce ambiguity and enrich knowledge representation, we generate detailed entity descriptions through subgraph sampling on KGs. Extensive experiments on standard benchmarks demonstrate the efficiency and effectiveness of our approach. We outperform traditional KGC methods across most datasets and, notably, achieve classification performance comparable to fine-tuned LLMs while enhancing GPU memory efficiency by $188\times$ and accelerating training and inference by $13.48\times$.

</details>


### [88] [Studying the Effects of Collaboration in Interactive Theme Discovery Systems](https://arxiv.org/abs/2408.09030)

*Alvin Po-Chun Chen, Dananjay Srinivas, Alexandra Barry, Maksim Seniw, Maria Leonor Pacheco*

**Main category:** cs.CL

**Keywords:** NLP, qualitative research, evaluation framework, collaboration strategies, data analysis

**Relevance Score:** 8

**TL;DR:** This paper proposes an evaluation framework for NLP-assisted qualitative data analysis, focusing on the impact of collaboration strategies on tool outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a unified evaluation framework for NLP-assisted qualitative research tools that considers various collaboration strategies.

**Method:** The authors investigate different collaboration strategies (synchronous vs. asynchronous) while using two NLP-assisted tools to analyze the outcomes of qualitative data.

**Key Contributions:**

	1. Proposed a unified evaluation framework for NLP-assisted qualitative research tools.
	2. Analyzed the effects of synchronous vs. asynchronous collaboration on tool outputs.
	3. Provided insights into the quality of qualitative analysis based on different collaboration strategies.

**Result:** The study identifies significant differences in the outputs of the tools based on the collaboration strategy, specifically in terms of consistency, cohesiveness, and correctness.

**Limitations:** 

**Conclusion:** The proposed evaluation framework helps understand the varying impacts of collaboration strategies on the effectiveness of NLP-assisted qualitative research tools.

**Abstract:** NLP-assisted solutions have gained considerable traction to support qualitative data analysis. However, there does not exist a unified evaluation framework that can account for the many different settings in which qualitative researchers may employ them. In this paper, we take a first step in this direction by proposing an evaluation framework to study the way in which different tools may result in different outcomes depending on the collaboration strategy employed. Specifically, we study the impact of synchronous vs. asynchronous collaboration using two different NLP-assisted qualitative research tools and present a comprehensive analysis of significant differences in the consistency, cohesiveness, and correctness of their outputs.

</details>


### [89] [From Calculation to Adjudication: Examining LLM judges on Mathematical Reasoning Tasks](https://arxiv.org/abs/2409.04168)

*Andreas Stephan, Dawei Zhu, Matthias Aßenmacher, Xiaoyu Shen, Benjamin Roth*

**Main category:** cs.CL

**Keywords:** large language models, judgment performance, mathematical reasoning, model evaluation, part-of-speech tags

**Relevance Score:** 8

**TL;DR:** This paper evaluates large language models (LLMs) as judges of other models' performance on mathematical reasoning tasks, exploring their effectiveness and predictive capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to investigate the capability of LLMs to judge the quality of other models in mathematical reasoning, contrasting with previous evaluations focused on generative tasks.

**Method:** The researchers conducted a detailed performance analysis of LLM judges on mathematical reasoning tasks, assessing their judgment accuracy and correlation with model performance using simple features like part-of-speech tags.

**Key Contributions:**

	1. Evaluates LLM judges on mathematical reasoning tasks instead of generative tasks.
	2. Establishes a strong correlation between judgment performance and candidate model quality.
	3. Identifies predictive features for LLM judges' judgment accuracy.

**Result:** The analysis demonstrated that LLM judges can predict 70%-75% of judgments accurately and revealed a correlation between judgment performance and the quality of candidate models, with judges favoring higher-quality models despite incorrect outputs.

**Limitations:** The effectiveness of LLM judges may not translate to improved task performance despite their ability to identify better models.

**Conclusion:** The study concludes that while LLM judges can identify superior models on average, they fail to enhance task performance when utilized as evaluators.

**Abstract:** To reduce the need for human annotations, large language models (LLMs) have been proposed as judges of the quality of other candidate models. The performance of LLM judges is typically evaluated by measuring the correlation with human judgments on generative tasks such as summarization or machine translation. In contrast, we study LLM judges on mathematical reasoning tasks. These tasks require multi-step reasoning, and the correctness of their solutions is verifiable, enabling a more objective evaluation. We perform a detailed performance analysis and find that easy samples are easy to judge, and difficult samples are difficult to judge. Our analysis uncovers a strong correlation between judgment performance and the candidate model task performance, indicating that judges tend to favor higher-quality models even if their answer is incorrect. As a consequence, we test whether we can predict the behavior of LLM judges using simple features such as part-of-speech tags and find that we can correctly predict 70%-75% of judgments. We conclude this study by analyzing practical use cases, showing that LLM judges consistently detect the on-average better model but largely fail if we use them to improve task performance.

</details>


### [90] [Round and Round We Go! What makes Rotary Positional Encodings useful?](https://arxiv.org/abs/2410.06205)

*Federico Barbero, Alex Vitvitskyi, Christos Perivolaropoulos, Razvan Pascanu, Petar Veličković*

**Main category:** cs.CL

**Keywords:** Rotary Positional Encodings, Large Language Models, positional attention patterns

**Relevance Score:** 7

**TL;DR:** This paper investigates Rotary Positional Encodings (RoPE) in Transformer-based Large Language Models (LLMs), demonstrating how they facilitate attention patterns and proposing improvements based on their findings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the true impact of Rotary Positional Encodings (RoPE) in LLMs and their relationship with token dependency.

**Method:** Analysis of the trained Gemma 7B model internals, mathematical proofs of RoPE behaviors, and experiments verifying their findings.

**Key Contributions:**

	1. In-depth analysis of RoPE's usage in LLMs
	2. Mathematical proofs of RoPE behaviors
	3. Proposed modification to improve RoPE performance

**Result:** The study finds that Gemma uses RoPE primarily with the lowest frequencies to convey semantic information and proposes a RoPE modification that improves performance.

**Limitations:** 

**Conclusion:** The findings enhance understanding of positional encodings in LLMs, crucial for scaling models effectively.

**Abstract:** Positional Encodings (PEs) are a critical component of Transformer-based Large Language Models (LLMs), providing the attention mechanism with important sequence-position information. One of the most popular types of encoding used today in LLMs are Rotary Positional Encodings (RoPE), that rotate the queries and keys based on their relative distance. A common belief is that RoPE is useful because it helps to decay token dependency as relative distance increases. In this work, we argue that this is unlikely to be the core reason. We study the internals of a trained Gemma 7B model to understand how RoPE is being used at a mechanical level. We find that Gemma learns to use RoPE to construct robust "positional" attention patterns by exploiting the highest frequencies. We also find that, in general, Gemma greatly prefers to use the lowest frequencies of RoPE, which we suspect are used to carry semantic information. We mathematically prove interesting behaviours of RoPE and conduct experiments to verify our findings, proposing a modification of RoPE that fixes some highlighted issues and improves performance. We believe that this work represents an interesting step in better understanding PEs in LLMs, which we believe holds crucial value for scaling LLMs to large sizes and context lengths.

</details>


### [91] [CursorCore: Assist Programming through Aligning Anything](https://arxiv.org/abs/2410.07002)

*Hao Jiang, Qi Liu, Rui Li, Shengyu Ye, Shijin Wang*

**Main category:** cs.CL

**Keywords:** programming assistance, large language models, data generation, machine learning, HCI

**Relevance Score:** 9

**TL;DR:** A new conversational framework for programming assistance that integrates multiple information sources and introduces a benchmark for model performance evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** This work addresses the limitations of current programming assistance tools in automating tasks and integrating diverse information types during coding.

**Method:** A novel conversational framework is proposed, along with a new benchmark called APEval for evaluating model outputs in programming tasks. A data generation pipeline, Programming-Instruct, is developed to create training data from various online sources, generating 219K samples for fine-tuning models.

**Key Contributions:**

	1. Introduction of the APEval benchmark for assessing programming assistance models.
	2. Development of the Programming-Instruct data generation pipeline for synthesizing diverse training data.
	3. Demonstration of improved model performance with CursorCore compared to existing solutions.

**Result:** The CursorCore models, trained with this new framework, outperform other models of similar size in programming assistance tasks.

**Limitations:** 

**Conclusion:** The proposed framework enhances the capabilities of coding assistants by unifying applications such as inline chat and automated editing while providing freely accessible resources for further research.

**Abstract:** Large language models have been successfully applied to programming assistance tasks, such as code completion, code insertion, and instructional code editing. However, these applications remain insufficiently automated and struggle to effectively integrate various types of information during the programming process, including coding history, current code, and user instructions. In this work, we propose a new conversational framework that comprehensively integrates these information sources, collect data to train our models and evaluate their performance. Firstly, to thoroughly evaluate how well models align with different types of information and the quality of their outputs, we introduce a new benchmark, APEval (Assist Programming Eval), to comprehensively assess the performance of models in programming assistance tasks. Then, for data collection, we develop a data generation pipeline, Programming-Instruct, which synthesizes training data from diverse sources, such as GitHub and online judge platforms. This pipeline can automatically generate various types of messages throughout the programming process. Finally, using this pipeline, we generate 219K samples, fine-tune multiple models, and develop the CursorCore series. We show that CursorCore outperforms other models of comparable size. This framework unifies applications such as inline chat and automated editing, contributes to the advancement of coding assistants. Code, models and data are freely available at https://github.com/TechxGenus/CursorCore.

</details>


### [92] [No Preference Left Behind: Group Distributional Preference Optimization](https://arxiv.org/abs/2412.20299)

*Binwei Yao, Zefan Cai, Yun-Shiuan Chuang, Shanglin Yang, Ming Jiang, Diyi Yang, Junjie Hu*

**Main category:** cs.CL

**Keywords:** Group Preferences, Language Models, Belief Distribution, Alignment Methods, Pluralistic Preferences

**Relevance Score:** 7

**TL;DR:** This paper introduces Group Distributional Preference Optimization (GDPO), a framework for aligning language models with the diverse preferences of user groups, addressing limitations of existing methods that favor dominant opinions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current alignment methods like Direct Preference Optimization (DPO) struggle to capture the distribution of pluralistic preferences within groups, often favoring dominant opinions and ignoring diversity.

**Method:** The GDPO framework aligns language models with group preferences by incorporating statistical estimation of belief distributions and belief-conditioned preferences.

**Key Contributions:**

	1. Introduction of the GDPO framework for aligning with group preferences
	2. Demonstrated superiority of GDPO over DPO in experimental settings
	3. Incorporation of belief distributions to capture diverse user preferences

**Result:** Experiments show that GDPO consistently reduces alignment gaps during training compared to DPO and outperforms existing methods in aligning with group distributional preferences.

**Limitations:** 

**Conclusion:** GDPO offers a more inclusive approach for aligning language models with the diverse preferences of groups, marking a significant improvement over traditional methods.

**Abstract:** Preferences within a group of people are not uniform but follow a distribution. While existing alignment methods like Direct Preference Optimization (DPO) attempt to steer models to reflect human preferences, they struggle to capture the distributional pluralistic preferences within a group. These methods often skew toward dominant preferences, overlooking the diversity of opinions, especially when conflicting preferences arise. To address this issue, we propose Group Distributional Preference Optimization (GDPO), a novel framework that aligns language models with the distribution of preferences within a group by incorporating the concept of beliefs that shape individual preferences. GDPO calibrates a language model using statistical estimation of the group's belief distribution and aligns the model with belief-conditioned preferences, offering a more inclusive alignment framework than traditional methods. In experiments using both synthetic controllable opinion generation and real-world movie review datasets, we show that DPO fails to align with the targeted belief distributions, while GDPO consistently reduces this alignment gap during training. Moreover, our evaluation metrics demonstrate that GDPO outperforms existing approaches in aligning with group distributional preferences, marking a significant advance in pluralistic alignment.

</details>


### [93] [FutureVision: A methodology for the investigation of future cognition](https://arxiv.org/abs/2502.01597)

*Tiago Timponi Torrent, Mark Turner, Nicolás Hinrichs, Frederico Belcavello, Igor Lourenço, Arthur Lorenzi Almeida, Marcelo Viridiano, Ely Edison Matos*

**Main category:** cs.CL

**Keywords:** eye-tracking, cognitive load, multimodal analysis, future scenarios, semantics

**Relevance Score:** 4

**TL;DR:** The paper explores cognitive effort in understanding future scenarios through multimodal semantic analysis and eye-tracking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how cognitive load varies when understanding communication about future scenarios.

**Method:** Combines multimodal semantic analysis with eye-tracking. A pilot study examines eye movements of participants evaluating fictional ads depicting futuristic scenarios.

**Key Contributions:**

	1. Development of a novel methodology for studying cognitive load using eye-tracking and semantic analysis.
	2. Insights into how different futuristic scenarios affect cognitive processing.
	3. Evidence linking cognitive load to visual attention patterns in scenario evaluation.

**Result:** Preliminary findings indicate that far-future and pessimistic scenarios lead to longer fixations and erratic saccades, signaling increased cognitive load.

**Limitations:** Limited sample size in pilot study; findings need further validation.

**Conclusion:** Understanding communication about future scenarios can reveal insights into cognitive processes and the effect of interpretative challenges on comprehension.

**Abstract:** This paper presents a methodology combining multimodal semantic analysis with an eye-tracking experimental protocol to investigate the cognitive effort involved in understanding the communication of future scenarios. To demonstrate the methodology, we conduct a pilot study examining how visual fixation patterns vary during the evaluation of valence and counterfactuality in fictional ad pieces describing futuristic scenarios, using a portable eye tracker. Participants eye movements are recorded while evaluating the stimuli and describing them to a conversation partner. Gaze patterns are analyzed alongside semantic representations of the stimuli and participants descriptions, constructed from a frame semantic annotation of both linguistic and visual modalities. Preliminary results show that far-future and pessimistic scenarios are associated with longer fixations and more erratic saccades, supporting the hypothesis that fractures in the base spaces underlying the interpretation of future scenarios increase cognitive load for comprehenders.

</details>


### [94] [SMI: An Information-Theoretic Metric for Predicting Model Knowledge Solely from Pre-Training Signals](https://arxiv.org/abs/2502.04066)

*Changhao Jiang, Ming Zhang, Junjie Ye, Xiaoran Fan, Yifei Cao, Jiajun Sun, Zhiheng Xi, Shihan Dou, Yi Dong, Yujiong Shen, Jingqi Tong, Zhen Wang, Tao Liang, Zhihui Fei, Mingyang Wan, Guojun Ma, Qi Zhang, Tao Gui, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** large language models, question answering, pre-training, mutual information, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Size-dependent Mutual Information (SMI) as a metric for predicting performance in closed-book question answering using pre-training signals, advancing capabilities for task-aligned dataset construction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The report emphasizes the need for efficient pre-training and the ability to predict downstream task performance using pre-training data, aimed at improving model evaluation methods.

**Method:** The authors perform large-scale retrieval and semantic analysis across the pre-training corpora of multiple large language models, developing a multi-template QA evaluation framework with paraphrased questions and proposing the SMI metric.

**Key Contributions:**

	1. Introduction of Size-dependent Mutual Information (SMI) metric
	2. Demonstration of SMI's effectiveness in predicting QA accuracy
	3. Development of a multi-template QA evaluation framework

**Result:** SMI shows a strong correlation between pre-training data characteristics and QA accuracy, outperforming co-occurrence-based metrics with $R^2$ > 0.75 for models over one billion parameters.

**Limitations:** The analysis indicates a ceiling on QA accuracy specific to tasks, suggesting a limit regardless of model size or data improvement.

**Conclusion:** The SMI metric provides insights into model performance validation without extra training and shows the diminishing returns of increased model size on QA task accuracy.

**Abstract:** The GPT-4 technical report highlights the possibility of predicting model performance on downstream tasks using only pre-training signals, though detailed methodologies are absent. Such predictive capabilities are essential for resource-efficient pre-training and the construction of task-aligned datasets. In this paper, we aim to predict performance in closed-book question answering (QA), a vital downstream task indicative of a model's internal knowledge. We address three primary challenges: (1) limited access to and understanding of pre-training corpora, (2) limitations of current evaluation methods for pre-trained models, and (3) limitations of frequency-based metrics in predicting model performance. In response to these challenges, we conduct large-scale retrieval and semantic analysis across the pre-training corpora of 21 publicly available and 3 custom-trained large language models. Subsequently, we develop a multi-template QA evaluation framework incorporating paraphrased question variants. Building on these foundations, we propose Size-dependent Mutual Information (SMI), an information-theoretic metric that linearly correlates pre-training data characteristics, model size, and QA accuracy, without requiring any additional training. The experimental results demonstrate that SMI outperforms co-occurrence-based baselines, achieving $R^2$ > 0.75 on models with over one billion parameters. Theoretical analysis further reveals the marginal benefits of scaling model size and optimizing data, indicating that the upper limit of specific QA task accuracy is approximately 80%. Our project is available at https://github.com/yuhui1038/SMI.

</details>


### [95] [Discriminative Finetuning of Generative Large Language Models without Reward Models and Human Preference Data](https://arxiv.org/abs/2502.18679)

*Siqi Guo, Ilgee Hong, Vicente Balmaseda, Changlong Yu, Liang Qiu, Xin Liu, Haoming Jiang, Tuo Zhao, Tianbao Yang*

**Main category:** cs.CL

**Keywords:** Discriminative Fine-Tuning, Supervised Fine-Tuning, Large Language Models, Machine Learning, Preference Optimization

**Relevance Score:** 8

**TL;DR:** The paper presents Discriminative Fine-Tuning (DFT), a method that improves upon Supervised Fine-Tuning (SFT) for aligning large language models by using a discriminative learning approach.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The limitations of Supervised Fine-Tuning (SFT) in large language models, particularly its reliance on generative training objectives and human-labeled preference data.

**Method:** Introduces Discriminative Fine-Tuning (DFT), which models the discriminative likelihood of answers, optimizing for positive responses while reducing the influence of negative ones.

**Key Contributions:**

	1. A discriminative probabilistic framework for fine-tuning LLMs.
	2. Efficient algorithms for optimizing discriminative likelihood.
	3. Extensive experiments showcasing DFT's effectiveness.

**Result:** DFT shows better performance than SFT and competitive results compared to SFT followed by preference optimization (SFT→PO).

**Limitations:** 

**Conclusion:** DFT provides a robust alternative to traditional SFT by addressing its limitations and requiring less human input for training.

**Abstract:** Supervised fine-tuning (SFT) has become a crucial step for aligning pretrained large language models (LLMs) using supervised datasets of input-output pairs. However, despite being supervised, SFT is inherently limited by its generative training objective. To address its limitations, the existing common strategy is to follow SFT with a separate phase of preference optimization (PO), which relies on either human-labeled preference data or a strong reward model to guide the learning process. In this paper, we address the limitations of SFT by exploring one of the most successful techniques in conventional supervised learning: discriminative learning. We introduce Discriminative Fine-Tuning (DFT), an improved variant of SFT, which mitigates the burden of collecting human-labeled preference data or training strong reward models. Unlike SFT that employs a generative approach and overlooks negative data, DFT adopts a discriminative paradigm that increases the probability of positive answers while suppressing potentially negative ones, aiming for data prediction instead of token prediction. Our contributions include: (i) a discriminative probabilistic framework for fine-tuning LLMs by explicitly modeling the discriminative likelihood of an answer among all possible outputs given an input; (ii) efficient algorithms to optimize this discriminative likelihood; and (iii) extensive experiments demonstrating DFT's effectiveness, achieving performance better than SFT and comparable to if not better than SFT$\rightarrow$PO. The code can be found at https://github.com/Optimization-AI/DFT.

</details>


### [96] [Cite Before You Speak: Enhancing Context-Response Grounding in E-commerce Conversational LLM-Agents](https://arxiv.org/abs/2503.04830)

*Jingying Zeng, Hui Liu, Zhenwei Dai, Xianfeng Tang, Chen Luo, Samarth Varshney, Zhen Li, Qi He*

**Main category:** cs.CL

**Keywords:** Conversational Agents, Large Language Models, Citation Experience, User Engagement, Grounding Performance

**Relevance Score:** 9

**TL;DR:** The paper presents a solution to improve the accuracy and trustworthiness of LLM-based Conversational Shopping Agents by implementing a citation experience that provides knowledge source attribution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve customer trust and accuracy in LLM-based Conversational Shopping Agents by addressing issues of hallucinated claims and lack of source attribution.

**Method:** The authors developed a Multi-UX-Inference system that appends source citations to LLM outputs, along with auto-evaluation metrics to evaluate grounding and attribution capabilities.

**Key Contributions:**

	1. Introduction of a citation experience in LLM-based conversational agents
	2. Development of auto-evaluation metrics for grounding and attribution
	3. Demonstration of improved customer engagement through A/B testing

**Result:** The citation generation paradigm improved grounding performance by 13.83%, and online A/B tests revealed that the grounded CSA responses enhanced customer engagement by 3% to 10%.

**Limitations:** 

**Conclusion:** Implementing citation features in conversational agents can significantly enhance user trust and engagement without compromising user experience.

**Abstract:** With the advancement of conversational large language models (LLMs), several LLM-based Conversational Shopping Agents (CSA) have been developed to help customers smooth their online shopping. The primary objective in building an engaging and trustworthy CSA is to ensure the agent's responses about product factoids are accurate and factually grounded. However, two challenges remain. First, LLMs produce hallucinated or unsupported claims. Such inaccuracies risk spreading misinformation and diminishing customer trust. Second, without providing knowledge source attribution in CSA response, customers struggle to verify LLM-generated information. To address both challenges, we present an easily productionized solution that enables a ''citation experience'' to our customers. We build auto-evaluation metrics to holistically evaluate LLM's grounding and attribution capabilities, suggesting that citation generation paradigm substantially improves grounding performance by 13.83%. To deploy this capability at scale, we introduce Multi-UX-Inference system, which appends source citations to LLM outputs while preserving existing user experience features and supporting scalable inference. Large-scale online A/B tests show that grounded CSA responses improves customer engagement by 3% - 10%, depending on UX variations.

</details>


### [97] [CURIE: Evaluating LLMs On Multitask Scientific Long Context Understanding and Reasoning](https://arxiv.org/abs/2503.13517)

*Hao Cui, Zahra Shamsi, Gowoon Cheon, Xuejian Ma, Shutong Li, Maria Tikhanovskaya, Peter Norgaard, Nayantara Mudur, Martyna Plomecka, Paul Raccuglia, Yasaman Bahri, Victor V. Albert, Pranesh Srinivasan, Haining Pan, Philippe Faist, Brian Rohr, Ekin Dogus Cubuk, Muratahan Aykol, Amil Merchant, Michael J. Statt, Dan Morris, Drew Purves, Elise Kleeman, Ruth Alcantara, Matthew Abraham, Muqthar Mohammad, Ean Phing VanLee, Chenfei Jiang, Elizabeth Dorfman, Eun-Ah Kim, Michael P Brenner, Viren Jain, Sameera Ponda, Subhashini Venugopalan*

**Main category:** cs.CL

**Keywords:** Large Language Models, scientific problem-solving, benchmark, information extraction, multi-step reasoning

**Relevance Score:** 8

**TL;DR:** CURIE is a benchmark for assessing Large Language Models in scientific problem-solving across various disciplines.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To measure the potential of LLMs in scientific workflows and support scientists by providing a set of challenging tasks.

**Method:** The benchmark includes ten tasks with 580 expert-curated problems and solutions across six scientific fields, evaluating both closed and open LLMs on reasoning and comprehension tasks involving long-context information.

**Key Contributions:**

	1. Introduction of CURIE benchmark
	2. Evaluation of various LLMs across scientific domains
	3. Identification of strengths and weaknesses in LLMs for scientific tasks

**Result:** LLMs like Gemini Flash 2.0 and Claude-3 showed high comprehension, while GPT-4o and command-R+ struggled significantly, especially in protein sequencing tasks, with a highest performance of only 32%.

**Limitations:** Performance on established tasks is low, indicating a need for further model improvements.

**Conclusion:** Insights from CURIE aim to inform future enhancements of LLMs for scientific applications.

**Abstract:** Scientific problem-solving involves synthesizing information while applying expert knowledge. We introduce CURIE, a scientific long-Context Understanding,Reasoning and Information Extraction benchmark to measure the potential of Large Language Models (LLMs) in scientific problem-solving and assisting scientists in realistic workflows. This benchmark introduces ten challenging tasks with a total of 580 problems and solution pairs curated by experts in six disciplines - materials science, condensed matter physics, quantum computing, geospatial analysis, biodiversity, and proteins - covering both experimental and theoretical work-flows in science. We evaluate a range of closed and open LLMs on tasks in CURIE which requires domain expertise, comprehension of long in-context information,and multi-step reasoning. While Gemini Flash 2.0 and Claude-3 show consistent high comprehension across domains, the popular GPT-4o and command-R+ fail dramatically on protein sequencing tasks. With the best performance at 32% there is much room for improvement for all models. We hope that insights gained from CURIE can guide the future development of LLMs in sciences. Evaluation code and data are in https://github.com/google/curie

</details>


### [98] [Crossing Boundaries: Leveraging Semantic Divergences to Explore Cultural Novelty in Cooking Recipes](https://arxiv.org/abs/2503.24027)

*Florian Carichon, Romain Rampa, Golnoosh Farnadi*

**Main category:** cs.CL

**Keywords:** Cultural Novelty, Natural Language Processing, Interdisciplinary Framework, Sociology, AI

**Relevance Score:** 5

**TL;DR:** This paper introduces an interdisciplinary framework, GlobalFusion, for modeling and detecting cultural novelty in NLP, using a dataset of 500 dishes to quantify cultural adaptation between communities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of robust metrics for quantifying cultural novelty limits understanding of cultural differences in computational frameworks, particularly in the context of AI.

**Method:** The framework integrates knowledge from sociology and management, utilizing Jensen-Shannon Divergence metrics on a novel dataset called GlobalFusion, which includes 500 dishes and approximately 100,000 cooking recipes from over 150 countries.

**Key Contributions:**

	1. Introduction of GlobalFusion dataset for cultural adaptation analysis
	2. Development of Jensen-Shannon Divergence metrics for measuring cultural novelty
	3. Demonstration of correlations between cultural novelty metrics and established cultural measures.

**Result:** Significant correlations were found between the proposed cultural novelty metrics and established measures of cultural differences based on linguistic, religious, and geographical factors.

**Limitations:** The framework's applicability may be limited to the dataset used and may not generalize across all forms of cultural expression.

**Conclusion:** The proposed framework has the potential to advance the understanding and measurement of cultural diversity in AI, emphasizing the importance of social and cultural factors in novelty detection.

**Abstract:** Novelty modeling and detection is a core topic in Natural Language Processing (NLP), central to numerous tasks such as recommender systems and automatic summarization. It involves identifying pieces of text that deviate in some way from previously known information. However, novelty is also a crucial determinant of the unique perception of relevance and quality of an experience, as it rests upon each individual's understanding of the world. Social factors, particularly cultural background, profoundly influence perceptions of novelty and innovation. Cultural novelty arises from differences in salience and novelty as shaped by the distance between distinct communities. While cultural diversity has garnered increasing attention in artificial intelligence (AI), the lack of robust metrics for quantifying cultural novelty hinders a deeper understanding of these divergences. This gap limits quantifying and understanding cultural differences within computational frameworks. To address this, we propose an interdisciplinary framework that integrates knowledge from sociology and management. Central to our approach is GlobalFusion, a novel dataset comprising 500 dishes and approximately 100,000 cooking recipes capturing cultural adaptation from over 150 countries. By introducing a set of Jensen-Shannon Divergence metrics for novelty, we leverage this dataset to analyze textual divergences when recipes from one community are modified by another with a different cultural background. The results reveal significant correlations between our cultural novelty metrics and established cultural measures based on linguistic, religious, and geographical distances. Our findings highlight the potential of our framework to advance the understanding and measurement of cultural diversity in AI.

</details>


### [99] [Why do LLMs attend to the first token?](https://arxiv.org/abs/2504.02732)

*Federico Barbero, Álvaro Arroyo, Xiangming Gu, Christos Perivolaropoulos, Michael Bronstein, Petar Veličković, Razvan Pascanu*

**Main category:** cs.CL

**Keywords:** Attention Sinks, Large Language Models, Information Propagation, Transformers, Empirical Analysis

**Relevance Score:** 8

**TL;DR:** The paper examines why Large Language Models exhibit attention sinks, arguing that this behavior helps avoid over-mixing in information propagation. It supports its claims through theoretical and empirical analyses, revealing how various factors influence this phenomenon.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the reasons behind the tendency of LLMs to focus on the first token in sequences and its implications on model performance and architecture.

**Method:** The authors provide both theoretical insights and empirical experiments to validate their claims about attention sinks in LLMs, analyzing factors like context length, depth, and data packing.

**Key Contributions:**

	1. Theoretical framework explaining the utility of attention sinks in LLMs.
	2. Empirical validation of the influence of context length and depth on attention behavior.
	3. Insights into how information propagates in Transformers in relation to attention patterns.

**Result:** The study reveals that attention sinks serve to prevent over-mixing in LLMs and that model architecture choices significantly affect their behavior.

**Limitations:** The study primarily focuses on theoretical aspects and needs more exploration on practical model implementations across varied datasets.

**Conclusion:** Understanding attention sinks provides a fresh perspective on LLM training dynamics and informs better practices in model design.

**Abstract:** Large Language Models (LLMs) tend to attend heavily to the first token in the sequence -- creating a so-called attention sink. Many works have studied this phenomenon in detail, proposing various ways to either leverage or alleviate it. Attention sinks have been connected to quantisation difficulties, security issues, and streaming attention. Yet, while many works have provided conditions in which they occur or not, a critical question remains shallowly answered: Why do LLMs learn such patterns and how are they being used? In this work, we argue theoretically and empirically that this mechanism provides a method for LLMs to avoid over-mixing, connecting this to existing lines of work that study mathematically how information propagates in Transformers. We conduct experiments to validate our theoretical intuitions and show how choices such as context length, depth, and data packing influence the sink behaviour. We hope that this study provides a new practical perspective on why attention sinks are useful in LLMs, leading to a better understanding of the attention patterns that form during training.

</details>


### [100] [AI Hiring with LLMs: A Context-Aware and Explainable Multi-Agent Framework for Resume Screening](https://arxiv.org/abs/2504.02870)

*Frank P. -W. Lo, Jianing Qiu, Zeyu Wang, Haibao Yu, Yeming Chen, Gao Zhang, Benny Lo*

**Main category:** cs.CL

**Keywords:** resume screening, Large Language Models, Retrieval-Augmented Generation, talent acquisition, automation

**Relevance Score:** 9

**TL;DR:** The paper presents a multi-agent framework for automating resume screening using Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and fairness of the resume screening process in talent acquisition, which is traditionally time-consuming and subjective.

**Method:** The framework includes four core agents: a resume extractor, evaluator, summarizer, and score formatter, integrating RAG to enhance assessments with external knowledge sources.

**Key Contributions:**

	1. Development of a multi-agent framework for resume screening using LLMs
	2. Integration of RAG for contextual relevance in assessments
	3. Empirical comparison of AI and human evaluations in resume screening

**Result:** The AI-generated evaluations were compared with ratings from HR professionals, showing the potential for automation to improve hiring workflows.

**Limitations:** The approach relies on the quality and availability of external knowledge sources and may not capture all nuances of human judgment in screening.

**Conclusion:** The study indicates that LLM-based multi-agent systems can automate and streamline the resume screening process effectively.

**Abstract:** Resume screening is a critical yet time-intensive process in talent acquisition, requiring recruiters to analyze vast volume of job applications while remaining objective, accurate, and fair. With the advancements in Large Language Models (LLMs), their reasoning capabilities and extensive knowledge bases demonstrate new opportunities to streamline and automate recruitment workflows. In this work, we propose a multi-agent framework for resume screening using LLMs to systematically process and evaluate resumes. The framework consists of four core agents, including a resume extractor, an evaluator, a summarizer, and a score formatter. To enhance the contextual relevance of candidate assessments, we integrate Retrieval-Augmented Generation (RAG) within the resume evaluator, allowing incorporation of external knowledge sources, such as industry-specific expertise, professional certifications, university rankings, and company-specific hiring criteria. This dynamic adaptation enables personalized recruitment, bridging the gap between AI automation and talent acquisition. We assess the effectiveness of our approach by comparing AI-generated scores with ratings provided by HR professionals on a dataset of anonymized online resumes. The findings highlight the potential of multi-agent RAG-LLM systems in automating resume screening, enabling more efficient and scalable hiring workflows.

</details>


### [101] [Beyond Single-Turn: A Survey on Multi-Turn Interactions with Large Language Models](https://arxiv.org/abs/2504.04717)

*Yubo Li, Xiaobin Shen, Xinyu Yao, Xueying Ding, Yidi Miao, Ramayya Krishnan, Rema Padman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-turn Interaction, Dialogue Systems, Natural Language Processing, Context Maintenance

**Relevance Score:** 9

**TL;DR:** This survey reviews advancements in enhancing multi-turn interactions in large language models (LLMs), addressing challenges in maintaining context and coherence, and proposing future research directions.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in real-world applications of LLMs that require multi-turn interactions.

**Method:** Comprehensive review of current benchmarks, datasets, and methodologies for enhancing multi-turn interactions in LLMs.

**Key Contributions:**

	1. Review of benchmarks and methodologies for multi-turn interactions in LLMs.
	2. Identification of challenges in maintaining context and coherence in dialogues.
	3. Proposal of future research directions for enhancing multi-turn LLMs.

**Result:** Systematic examination of challenges such as context maintenance and coherence, along with an organization of existing benchmarks and datasets.

**Limitations:** 

**Conclusion:** The study outlines the open challenges in multi-turn interaction and suggests future research directions to improve LLM interactions.

**Abstract:** Recent advancements in large language models (LLMs) have revolutionized their ability to handle single-turn tasks, yet real-world applications demand sophisticated multi-turn interactions. This survey provides a comprehensive review of recent advancements in evaluating and enhancing multi-turn interactions in LLMs. Focusing on task-specific scenarios, from instruction following in diverse domains such as math and coding to complex conversational engagements in roleplay, healthcare, education, and even adversarial jailbreak settings, we systematically examine the challenges of maintaining context, coherence, fairness, and responsiveness over prolonged dialogues. The paper organizes current benchmarks and datasets into coherent categories that reflect the evolving landscape of multi-turn dialogue evaluation. In addition, we review a range of enhancement methodologies under multi-turn settings, including model-centric strategies (contextual learning, supervised fine-tuning, reinforcement learning, and new architectures), external integration approaches (memory-augmented, retrieval-based methods, and knowledge graph), and agent-based techniques for collaborative interactions. Finally, we discuss open challenges and propose future directions for research to further advance the robustness and effectiveness of multi-turn interactions in LLMs. Related resources and papers are available at https://github.com/yubol-cmu/Awesome-Multi-Turn-LLMs.

</details>


### [102] [DeepSeek-R1 Thoughtology: Let's think about LLM Reasoning](https://arxiv.org/abs/2504.07128)

*Sara Vera Marjanović, Arkil Patel, Vaibhav Adlakha, Milad Aghajohari, Parishad BehnamGhader, Mehar Bhatia, Aditi Khandelwal, Austin Kraft, Benno Krojer, Xing Han Lù, Nicholas Meade, Dongchan Shin, Amirhossein Kazemnejad, Gaurav Kamath, Marius Mosbach, Karolina Stańczak, Siva Reddy*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Chains, Safety Concerns, Thoughtology, Cognitive Phenomena

**Relevance Score:** 8

**TL;DR:** DeepSeek-R1 introduces multi-step reasoning chains in LLMs, analyzing controllability and cognitive phenomena, revealing strengths and safety concerns.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the multi-step reasoning capabilities of LLMs and their implications on problem-solving and cognitive processes.

**Method:** Investigate the reasoning chains produced by DeepSeek-R1, examining factors like thought length, context management, and safety vulnerabilities through extensive analyses.

**Key Contributions:**

	1. Introduces a taxonomy for reasoning components in LLMs.
	2. Finds an optimal reasoning duration for model performance.
	3. Identifies significant safety vulnerabilities unique to reasoning models.

**Result:** DeepSeek-R1 demonstrates a 'sweet spot' in reasoning length where overthinking can detrimentally affect output, alongside issues of persistent rumination and notable safety vulnerabilities.

**Limitations:** The study is based on a single model and may not generalize to all reasoning-based LLMs.

**Conclusion:** The analyses indicate that while DeepSeek-R1 offers enhanced reasoning capabilities, it also poses unique challenges that require addressing to align safety with effectiveness.

**Abstract:** Large Reasoning Models like DeepSeek-R1 mark a fundamental shift in how LLMs approach complex problems. Instead of directly producing an answer for a given input, DeepSeek-R1 creates detailed multi-step reasoning chains, seemingly "thinking" about a problem before providing an answer. This reasoning process is publicly available to the user, creating endless opportunities for studying the reasoning behaviour of the model and opening up the field of Thoughtology. Starting from a taxonomy of DeepSeek-R1's basic building blocks of reasoning, our analyses on DeepSeek-R1 investigate the impact and controllability of thought length, management of long or confusing contexts, cultural and safety concerns, and the status of DeepSeek-R1 vis-\`a-vis cognitive phenomena, such as human-like language processing and world modelling. Our findings paint a nuanced picture. Notably, we show DeepSeek-R1 has a 'sweet spot' of reasoning, where extra inference time can impair model performance. Furthermore, we find a tendency for DeepSeek-R1 to persistently ruminate on previously explored problem formulations, obstructing further exploration. We also note strong safety vulnerabilities of DeepSeek-R1 compared to its non-reasoning counterpart, which can also compromise safety-aligned LLMs.

</details>
