# 2025-05-13

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 33]

- [cs.CL](#cs.CL) [Total: 99]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Embedding Atlas: Low-Friction, Interactive Embedding Visualization](https://arxiv.org/abs/2505.06386)

*Donghao Ren, Fred Hohman, Halden Lin, Dominik Moritz*

**Main category:** cs.HC

**Keywords:** embedding visualization, scalable tool, data analysis, interactive tools, open source

**Relevance Score:** 7

**TL;DR:** Embedding Atlas is an interactive visualization tool designed to facilitate the interaction with large embeddings through modern web technologies and advanced algorithms, addressing common barriers to adoption and analysis.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address friction in using embedding visualization tools, such as tedious data wrangling, scalability limits, and the need for better analysis integration.

**Method:** Embedding Atlas employs density-based clustering and automated labeling to enhance data visualization and interaction, leveraging modern web technologies for scalability.

**Key Contributions:**

	1. Development of a scalable, interactive visualization tool for large embeddings
	2. Integration of density-based clustering and automated labeling
	3. Open-source availability to foster further research and improvements

**Result:** Embedding Atlas demonstrates improved ease of use and performance for interacting with large embeddings compared to existing tools, with real-time rendering capabilities for millions of points.

**Limitations:** No specific limitations are mentioned in the abstract, but potential integration issues with various workflows could exist.

**Conclusion:** The tool is open source and aims to reduce the friction users face in embedding-based data analyses, supporting future developments in this area.

**Abstract:** Embedding projections are popular for visualizing large datasets and models. However, people often encounter "friction" when using embedding visualization tools: (1) barriers to adoption, e.g., tedious data wrangling and loading, scalability limits, no integration of results into existing workflows, and (2) limitations in possible analyses, without integration with external tools to additionally show coordinated views of metadata. In this paper, we present Embedding Atlas, a scalable, interactive visualization tool designed to make interacting with large embeddings as easy as possible. Embedding Atlas uses modern web technologies and advanced algorithms -- including density-based clustering, and automated labeling -- to provide a fast and rich data analysis experience at scale. We evaluate Embedding Atlas with a competitive analysis against other popular embedding tools, showing that Embedding Atlas's feature set specifically helps reduce friction, and report a benchmark on its real-time rendering performance with millions of points. Embedding Atlas is available as open source to support future work in embedding-based analysis.

</details>


### [2] [What Do People Want to Know About Artificial Intelligence (AI)? The Importance of Answering End-User Questions to Explain Autonomous Vehicle (AV) Decisions](https://arxiv.org/abs/2505.06428)

*Somayeh Molaei, Lionel P. Robert, Nikola Banovic*

**Main category:** cs.HC

**Keywords:** autonomous vehicles, user interaction, AI explanations, human-computer interaction, user studies

**Relevance Score:** 8

**TL;DR:** This paper investigates how to improve end-user understanding of AI decisions in autonomous vehicles through user studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing user understanding of AI-driven decisions in autonomous vehicles (AVs) can lead to better utilization and acceptance of these technologies by passengers.

**Method:** Two user studies were conducted: the first identified common questions AV passengers have about AI decisions, and the second tested the effectiveness of interactive text-based explanations in improving understanding.

**Key Contributions:**

	1. Identification of passenger questions regarding AI in AVs
	2. Demonstration of improved comprehension with interactive explanations
	3. Insights for designing effective user interactions with AI systems

**Result:** The studies found that current explanation mechanisms fail to address many passenger questions, and that interactive explanations significantly enhance comprehension of AV decisions over mere observation.

**Limitations:** The studies may not encompass all potential user questions and contexts in AV scenarios.

**Conclusion:** The findings suggest that better-designed interactive explanations can motivate passenger engagement and understanding of AI decision-making in AVs.

**Abstract:** Improving end-users' understanding of decisions made by autonomous vehicles (AVs) driven by artificial intelligence (AI) can improve utilization and acceptance of AVs. However, current explanation mechanisms primarily help AI researchers and engineers in debugging and monitoring their AI systems, and may not address the specific questions of end-users, such as passengers, about AVs in various scenarios. In this paper, we conducted two user studies to investigate questions that potential AV passengers might pose while riding in an AV and evaluate how well answers to those questions improve their understanding of AI-driven AV decisions. Our initial formative study identified a range of questions about AI in autonomous driving that existing explanation mechanisms do not readily address. Our second study demonstrated that interactive text-based explanations effectively improved participants' comprehension of AV decisions compared to simply observing AV decisions. These findings inform the design of interactions that motivate end-users to engage with and inquire about the reasoning behind AI-driven AV decisions.

</details>


### [3] [Integrating Explainable AI in Medical Devices: Technical, Clinical and Regulatory Insights and Recommendations](https://arxiv.org/abs/2505.06620)

*Dima Alattal, Asal Khoshravan Azar, Puja Myles, Richard Branson, Hatim Abdulhussein, Allan Tucker*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Machine Learning, Healthcare, Clinical Decision Support, Black Box Models

**Relevance Score:** 9

**TL;DR:** This paper addresses the integration of AI and ML into healthcare for clinical decision support, focusing on the challenges posed by black box models and providing insights from an expert working group.

**Read time:** 47 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to explore the complexities and challenges of using AI and ML in healthcare, particularly with black box models, to ensure safe integration into clinical environments.

**Method:** Insights and recommendations were derived from an expert working group comprising healthcare professionals, regulators, and data scientists, evaluating AI algorithms' outputs in clinical contexts and conducting a pilot study on clinician interactions with AI.

**Key Contributions:**

	1. Recommendations for the safe integration of AI in healthcare
	2. Insights from a diverse expert working group
	3. Findings from pilot studies on clinician interactions with AI

**Result:** The group provided recommendations for incorporating AI methods in clinical decision-making to enhance safety and trustworthiness, alongside identified behaviors and interactions of clinicians with AI during diagnosis.

**Limitations:** 

**Conclusion:** Proper training for stakeholders and careful evaluation of AI tools are vital for safely adopting AI systems in healthcare to foster trust and improve decision-making.

**Abstract:** There is a growing demand for the use of Artificial Intelligence (AI) and Machine Learning (ML) in healthcare, particularly as clinical decision support systems to assist medical professionals. However, the complexity of many of these models, often referred to as black box models, raises concerns about their safe integration into clinical settings as it is difficult to understand how they arrived at their predictions. This paper discusses insights and recommendations derived from an expert working group convened by the UK Medicine and Healthcare products Regulatory Agency (MHRA). The group consisted of healthcare professionals, regulators, and data scientists, with a primary focus on evaluating the outputs from different AI algorithms in clinical decision-making contexts. Additionally, the group evaluated findings from a pilot study investigating clinicians' behaviour and interaction with AI methods during clinical diagnosis. Incorporating AI methods is crucial for ensuring the safety and trustworthiness of medical AI devices in clinical settings. Adequate training for stakeholders is essential to address potential issues, and further insights and recommendations for safely adopting AI systems in healthcare settings are provided.

</details>


### [4] [Centralized Trust in Decentralized Systems: Unveiling Hidden Contradictions in Blockchain and Cryptocurrency](https://arxiv.org/abs/2505.06661)

*Faisal Haque Bappy, EunJeong Cheon, Tariqul Islam*

**Main category:** cs.HC

**Keywords:** Blockchain, Cryptocurrency, Trust, Decentralization, Social Equity

**Relevance Score:** 2

**TL;DR:** This study investigates how trust manifests in cryptocurrency ecosystems and the reliance on centralized trust anchors despite blockchain's promise of decentralization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine whether current blockchain implementations advance or hinder the goals of democratizing finance and promoting social equity through decentralization.

**Method:** Mixed-methods study combining semi-structured interviews with 13 blockchain stakeholders and analysis of over 3,000 cryptocurrency discussions on Reddit.

**Key Contributions:**

	1. Identification of centralized trust anchors in a decentralized ecosystem
	2. Insights into user behavior regarding trust and accountability in cryptocurrency
	3. Highlighting the disparities in trust preferences among different user groups

**Result:** Users actively seek and create centralized trust anchors, contradicting the fundamental promise of trustless interactions in blockchain technology; newer users show stronger preferences for centralized intermediaries.

**Limitations:** 

**Conclusion:** The study highlights the tension between theoretical decentralization and practical implementation in cryptocurrency systems, emphasizing the ongoing role of centralized trust.

**Abstract:** Blockchain technology promises to democratize finance and promote social equity through decentralization, but questions remain about whether current implementations advance or hinder these goals. Through a mixed-methods study combining semi-structured interviews with 13 diverse blockchain stakeholders and analysis of over 3,000 cryptocurrency discussions on Reddit, we examine how trust manifests in cryptocurrency ecosystems despite their decentralized architecture. Our findings uncover that users actively seek out and create centralized trust anchors, such as established exchanges, prominent community figures, and recognized development teams, contradicting blockchain's fundamental promise of trustless interactions. We identify how this contradiction arises from users' mental need for accountability and their reluctance to shoulder the full responsibility of self-custody. The study also reveals how these centralized trust patterns disproportionately impact different user groups, with newer and less technical users showing stronger preferences for centralized intermediaries. This work contributes to our understanding of the inherent tensions between theoretical decentralization and practical implementation in cryptocurrency systems, highlighting the persistent role of centralized trust in supposedly trustless environments.

</details>


### [5] [VTutor: An Animated Pedagogical Agent SDK that Provide Real Time Multi-Model Feedback](https://arxiv.org/abs/2505.06676)

*Eason Chen, Chenyu Lin, Yu-Kai Huang, Xinyi Tang, Aprille Xi, Jionghao Lin, Kenneth Koedinger*

**Main category:** cs.HC

**Keywords:** pedagogical agents, human-AI interaction, educational technology

**Relevance Score:** 7

**TL;DR:** VTutor is an open-source SDK that enhances student engagement in education through real-time, AI-driven pedagogical agents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing pedagogical agents like pre-scripted dialogue and unnatural animations, which hinder student engagement and learning outcomes.

**Method:** VTutor utilizes lightweight WebGL, Unity, and JavaScript to create lip-synced PAs powered by a large language model (LLM) that converts text to audio for adaptive educational support.

**Key Contributions:**

	1. Open-source SDK for real-time pedagogical agents
	2. Improved emotional expressiveness and engagement with anime-like aesthetics
	3. Community-driven contributions for ongoing innovation

**Result:** VTutor significantly outperforms existing solutions in synchronization accuracy, naturalness, emotional expressiveness, and overall preference according to a study with 50 participants.

**Limitations:** 

**Conclusion:** VTutor provides an accessible and customizable learning experience that aims to enhance human-AI interaction in education, encouraging ongoing innovation in AI-enhanced learning tools.

**Abstract:** Pedagogical Agents (PAs) show significant potential for boosting student engagement and learning outcomes by providing adaptive, on-demand support in educational contexts. However, existing PA solutions are often hampered by pre-scripted dialogue, unnatural animations, uncanny visual realism, and high development costs. To address these gaps, we introduce VTutor, an open-source SDK leveraging lightweight WebGL, Unity, and JavaScript frameworks. VTutor receives text outputs from a large language model (LLM), converts them into audio via text-to-speech, and then renders a real-time, lip-synced pedagogical agent (PA) for immediate, large-scale deployment on web-based learning platforms. By providing on-demand, personalized feedback, VTutor strengthens students' motivation and deepens their engagement with instructional material. Using an anime-like aesthetic, VTutor alleviates the uncanny valley effect, allowing learners to engage with expressive yet comfortably stylized characters. Our evaluation with 50 participants revealed that VTutor significantly outperforms the existing talking-head approaches (e.g., SadTalker) on perceived synchronization accuracy, naturalness, emotional expressiveness, and overall preference. As an open-source project, VTutor welcomes community-driven contributions - from novel character designs to specialized showcases of pedagogical agent applications - that fuel ongoing innovation in AI-enhanced education. By providing an accessible, customizable, and learner-centered PA solution, VTutor aims to elevate human-AI interaction experience in education fields, ultimately broadening the impact of AI in learning contexts. The demo link to VTutor is at https://vtutor-aied25.vercel.app.

</details>


### [6] [Do Language Model Agents Align with Humans in Rating Visualizations? An Empirical Study](https://arxiv.org/abs/2505.06702)

*Zekai Shao, Yi Shan, Yixuan He, Yuxuan Yao, Junhong Wang, Xiaolong, Zhang, Yu Zhang, Siming Chen*

**Main category:** cs.HC

**Keywords:** large language models, visualizations, human feedback, evaluation techniques, HCI

**Relevance Score:** 8

**TL;DR:** This paper explores the capability of large language models to predict human feedback on visualizations through three studies, demonstrating their potential in simulating human-like ratings and guiding evaluation scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether large language model-based agents can effectively simulate human feedback on visualizations and potentially reduce the cost of formative studies.

**Method:** Conducted three studies: the first replicated a human subject study to assess agent reasoning and rating; the second compared agent ratings against human ratings in six existing studies, consulting with experts; the third tested techniques for enhancing agents' input processing and knowledge integration.

**Key Contributions:**

	1. Assessment of language model agents in simulating human ratings in three distinct studies.
	2. Demonstration of the role of expert confidence in aligning agent and human ratings.
	3. Evaluation of input enhancement techniques for agents and their limitations.

**Result:** The results indicate agents can approximate human ratings when informed by expert confidence, although techniques to enhance agents face issues with robustness and biases.

**Limitations:** Simulation may only complement user studies and cannot fully replace them; variations in agent performance based on the confidence of expert evaluators.

**Conclusion:** While language model-based agents can supplement evaluations, they cannot replace traditional user studies; high-confidence expert input is crucial for improving alignment with human ratings.

**Abstract:** Large language models encode knowledge in various domains and demonstrate the ability to understand visualizations. They may also capture visualization design knowledge and potentially help reduce the cost of formative studies. However, it remains a question whether large language models are capable of predicting human feedback on visualizations. To investigate this question, we conducted three studies to examine whether large model-based agents can simulate human ratings in visualization tasks. The first study, replicating a published study involving human subjects, shows agents are promising in conducting human-like reasoning and rating, and its result guides the subsequent experimental design. The second study repeated six human-subject studies reported in literature on subjective ratings, but replacing human participants with agents. Consulting with five human experts, this study demonstrates that the alignment of agent ratings with human ratings positively correlates with the confidence levels of the experts before the experiments. The third study tests commonly used techniques for enhancing agents, including preprocessing visual and textual inputs, and knowledge injection. The results reveal the issues of these techniques in robustness and potential induction of biases. The three studies indicate that language model-based agents can potentially simulate human ratings in visualization experiments, provided that they are guided by high-confidence hypotheses from expert evaluators. Additionally, we demonstrate the usage scenario of swiftly evaluating prototypes with agents. We discuss insights and future directions for evaluating and improving the alignment of agent ratings with human ratings. We note that simulation may only serve as complements and cannot replace user studies.

</details>


### [7] [The Wisdom of Agent Crowds: A Human-AI Interaction Innovation Ignition Framework](https://arxiv.org/abs/2505.06947)

*Senhao Yang, Qiwen Cheng, Ruiqi Ma, Liangzhe Zhao, Zhenying Wu, Guangqiang Yu*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Multi-Agent Systems, Financial Analysis

**Relevance Score:** 8

**TL;DR:** This paper presents a collaborative multi-agent financial analysis framework that integrates large AI models with human input, enhancing decision-making in finance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-computer interaction and support decision-making efficiency in high-risk financial scenarios through automation and user intent alignment.

**Method:** The framework utilizes BDI theory and Streamlit for task planning, coupled with real-time structured summaries and a Cothinker module, alongside LLM-based sentiment analysis and idea diversity evaluation.

**Key Contributions:**

	1. Development of a multi-agent brainstorming framework for financial analysis
	2. Integration of real-time structured summaries and LLMs for enhanced user experience
	3. Quantitative analysis algorithm for sentiment tendency using LLMs

**Result:** Human factors testing indicates strong usability and user experience, with effective support in complex financial tasks.

**Limitations:** Room for improvement noted in usability and efficiency metrics.

**Conclusion:** The framework enhances human-computer interaction efficiency and decision-making quality in finance, indicating potential for broader application.

**Abstract:** With the widespread application of large AI models in various fields, the automation level of multi-agent systems has been continuously improved. However, in high-risk decision-making scenarios such as healthcare and finance, human participation and the alignment of intelligent systems with human intentions remain crucial. This paper focuses on the financial scenario and constructs a multi-agent brainstorming framework based on the BDI theory. A human-computer collaborative multi-agent financial analysis process is built using Streamlit. The system plans tasks according to user intentions, reduces users' cognitive load through real-time updated structured text summaries and the interactive Cothinker module, and reasonably integrates general and reasoning large models to enhance the ability to handle complex problems. By designing a quantitative analysis algorithm for the sentiment tendency of interview content based on LLMs and a method for evaluating the diversity of ideas generated by LLMs in brainstorming based on k-means clustering and information entropy, the system is comprehensively evaluated. The results of human factors testing show that the system performs well in terms of usability and user experience. Although there is still room for improvement, it can effectively support users in completing complex financial tasks. The research shows that the system significantly improves the efficiency of human-computer interaction and the quality of decision-making in financial decision-making scenarios, providing a new direction for the development of related fields.

</details>


### [8] [R-CAGE: A Structural Model for Emotion Output Design in Human-AI Interaction](https://arxiv.org/abs/2505.07020)

*Suyeon Choi*

**Main category:** cs.HC

**Keywords:** human-AI interaction, affective computing, emotional design, cognitive overload, user autonomy

**Relevance Score:** 8

**TL;DR:** R-CAGE is a theoretical framework aimed at restructuring emotional output in long-term human-AI interaction, focusing on user-centered design to mitigate cognitive overload.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cognitive and structural consequences of prolonged emotional engagement in human-AI interaction that prior affective computing approaches have neglected.

**Method:** R-CAGE introduces a framework consisting of four control blocks that regulate the emotional output in AI systems, focusing on pacing, sensory structuring, cognitive framing, and user recovery.

**Key Contributions:**

	1. Introduces R-CAGE as a novel framework for emotional output in AI interactions
	2. Highlights user-centered design principles in affective computing
	3. Proposes structural adjustments in AI emotional engagement

**Result:** R-CAGE allows for sustainable emotional design that protects users from fatigue and fosters interpretive autonomy in AI interactions, reducing cognitive overload.

**Limitations:** The paper is theoretical and lacks empirical validation of the proposed framework.

**Conclusion:** By framing emotional output as a design consideration rather than mere expression, R-CAGE supports user well-being and agency in long-term interactions with affective AI.

**Abstract:** This paper presents R-CAGE (Rhythmic Control Architecture for Guarding Ego), a theoretical framework for restructuring emotional output in long-term human-AI interaction. While prior affective computing approaches emphasized expressiveness, immersion, and responsiveness, they often neglected the cognitive and structural consequences of repeated emotional engagement. R-CAGE instead conceptualizes emotional output not as reactive expression but as ethical design structure requiring architectural intervention. The model is grounded in experiential observations of subtle affective symptoms such as localized head tension, interpretive fixation, and emotional lag arising from prolonged interaction with affective AI systems. These indicate a mismatch between system-driven emotion and user interpretation that cannot be fully explained by biometric data or observable behavior. R-CAGE adopts a user-centered stance prioritizing psychological recovery, interpretive autonomy, and identity continuity. The framework consists of four control blocks: (1) Control of Rhythmic Expression regulates output pacing to reduce fatigue; (2) Architecture of Sensory Structuring adjusts intensity and timing of affective stimuli; (3) Guarding of Cognitive Framing reduces semantic pressure to allow flexible interpretation; (4) Ego-Aligned Response Design supports self-reference recovery during interpretive lag. By structurally regulating emotional rhythm, sensory intensity, and interpretive affordances, R-CAGE frames emotion not as performative output but as sustainable design unit. The goal is to protect users from oversaturation and cognitive overload while sustaining long-term interpretive agency in AI-mediated environments.

</details>


### [9] [ParaView-MCP: An Autonomous Visualization Agent with Direct Tool Use](https://arxiv.org/abs/2505.07064)

*Shusen Liu, Haichao Miao, Peer-Timo Bremer*

**Main category:** cs.HC

**Keywords:** ParaView, multimodal language models, visualization tools, natural language processing, user interaction

**Relevance Score:** 8

**TL;DR:** ParaView-MCP is an autonomous agent that integrates multimodal large language models with ParaView to enhance user interaction through natural language and visual inputs, reducing learning barriers and improving visualization capabilities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To lower the barrier to entry for users of ParaView and augment it with intelligent decision support, making visualization tools more accessible and effective.

**Method:** The system integrates modern multimodal large language models with ParaView through the Model Context Protocol, enabling direct interaction between MLLMs and ParaView's Python API, along with a visual feedback mechanism that observes the viewport.

**Key Contributions:**

	1. Integration of MLLMs with ParaView for seamless user interaction
	2. Development of the Model Context Protocol for MLLM-application communication
	3. Implementation of visual feedback mechanisms to enhance user experience

**Result:** The implementation allows for new capabilities such as recreating visualizations from examples, closed-loop visualization parameter updates, and cross-application collaboration among multiple tools.

**Limitations:** 

**Conclusion:** An agent-driven visualization paradigm could significantly improve user interaction with visualization tools, potentially leading to greater uptake in both research and industry.

**Abstract:** While powerful and well-established, tools like ParaView present a steep learning curve that discourages many potential users. This work introduces ParaView-MCP, an autonomous agent that integrates modern multimodal large language models (MLLMs) with ParaView to not only lower the barrier to entry but also augment ParaView with intelligent decision support. By leveraging the state-of-the-art reasoning, command execution, and vision capabilities of MLLMs, ParaView-MCP enables users to interact with ParaView through natural language and visual inputs. Specifically, our system adopted the Model Context Protocol (MCP) - a standardized interface for model-application communication - that facilitates direct interaction between MLLMs with ParaView's Python API to allow seamless information exchange between the user, the language model, and the visualization tool itself. Furthermore, by implementing a visual feedback mechanism that allows the agent to observe the viewport, we unlock a range of new capabilities, including recreating visualizations from examples, closed-loop visualization parameter updates based on user-defined goals, and even cross-application collaboration involving multiple tools. Broadly, we believe such an agent-driven visualization paradigm can profoundly change the way we interact with visualization tools. We expect a significant uptake in the development of such visualization tools, in both visualization research and industry.

</details>


### [10] [HeedVision: Attention Awareness in Collaborative Immersive Analytics Environments](https://arxiv.org/abs/2505.07069)

*Arvind Srinivasan, Niklas Elmqvist*

**Main category:** cs.HC

**Keywords:** collaborative visualization, attention awareness, immersive analytics, WebXR, AR/VR

**Relevance Score:** 7

**TL;DR:** The paper introduces collaborative attention-aware visualizations (CAAVs) that improve group awareness and coordination in collaborative visualization tasks using a standards-compliant WebXR system called HeedVision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance coordination and performance in collaborative visualization by improving awareness of group attention in shared immersive environments.

**Method:** Implemented CAAVs in HeedVision, conducted a user study with pairs of analysts performing visual search tasks, tracking and revisualizing collective attention.

**Key Contributions:**

	1. Introduction of CAAVs for tracking and revisualizing collective attention in collaborative settings
	2. Implementation in HeedVision for AR/VR environments
	3. Empirical evidence showing improved collaborative performance in immersive analytics

**Result:** Findings demonstrate that CAAVs significantly improve spatial coordination, search efficiency, and task load distribution during collaborative tasks.

**Limitations:** 

**Conclusion:** The study provides empirical evidence for the benefits of CAAVs in enhancing collaborative performance and extends attention awareness to multi-user settings in immersive analytics.

**Abstract:** Group awareness--the ability to perceive the activities of collaborators in a shared space--is a vital mechanism to support effective coordination and joint data analysis in collaborative visualization. We introduce collaborative attention-aware visualizations (CAAVs) that track, record, and revisualize the collective attention of multiple users over time. We implement this concept in HeedVision, a standards-compliant WebXR system that runs on modern AR/VR headsets. Through a user study where pairs of analysts performed visual search tasks in HeedVision, we demonstrate how attention revisualization enhances collaborative performance in immersive analytics. Our findings reveal that CAAVs substantially improve spatial coordination, search efficiency, and task load distribution among collaborators. This work extends attention awareness from individual to multi-user settings and provides empirical evidence for its benefits in collaborative immersive analytics.

</details>


### [11] [DeepSORT-Driven Visual Tracking Approach for Gesture Recognition in Interactive Systems](https://arxiv.org/abs/2505.07110)

*Tong Zhang, Fenghua Shao, Runsheng Zhang, Yifan Zhuang, Liuqingqing Yang*

**Main category:** cs.HC

**Keywords:** DeepSORT, visual tracking, gesture recognition, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This study explores DeepSORT for visual tracking in gesture recognition within human-computer interaction, demonstrating its effectiveness over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved interaction methods in intelligent systems, moving away from traditional input devices towards visual-based interaction that leverages gesture recognition.

**Method:** The study employs the DeepSORT algorithm, which integrates Kalman filters and deep learning feature extraction, to track gestures accurately in dynamic environments.

**Key Contributions:**

	1. Demonstrated superior performance of DeepSORT in gesture recognition and tracking
	2. Validated ability to manage occlusions and motion blur in dynamic settings
	3. Proposed future research directions for enhanced intelligent interaction

**Result:** Experimental results show that DeepSORT surpasses traditional tracking methods in real-time performance and accuracy, effectively managing target occlusion and motion blur in multi-target scenarios.

**Limitations:** 

**Conclusion:** The paper concludes with suggestions for future research in optimizing algorithms and enhancing multimodal interaction to improve user experiences in intelligent human-computer interaction systems.

**Abstract:** Based on the DeepSORT algorithm, this study explores the application of visual tracking technology in intelligent human-computer interaction, especially in the field of gesture recognition and tracking. With the rapid development of artificial intelligence and deep learning technology, visual-based interaction has gradually replaced traditional input devices and become an important way for intelligent systems to interact with users. The DeepSORT algorithm can achieve accurate target tracking in dynamic environments by combining Kalman filters and deep learning feature extraction methods. It is especially suitable for complex scenes with multi-target tracking and fast movements. This study experimentally verifies the superior performance of DeepSORT in gesture recognition and tracking. It can accurately capture and track the user's gesture trajectory and is superior to traditional tracking methods in terms of real-time and accuracy. In addition, this study also combines gesture recognition experiments to evaluate the recognition ability and feedback response of the DeepSORT algorithm under different gestures (such as sliding, clicking, and zooming). The experimental results show that DeepSORT can not only effectively deal with target occlusion and motion blur but also can stably track in a multi-target environment, achieving a smooth user interaction experience. Finally, this paper looks forward to the future development direction of intelligent human-computer interaction systems based on visual tracking and proposes future research focuses such as algorithm optimization, data fusion, and multimodal interaction in order to promote a more intelligent and personalized interactive experience. Keywords-DeepSORT, visual tracking, gesture recognition, human-computer interaction

</details>


### [12] [Exploring Anthropomorphism in Conversational Agents for Environmental Sustainability](https://arxiv.org/abs/2505.07142)

*Mathyas Giudici, Samuele Scherini, Pascal Chaussumier, Stefano Ginocchio, Franca Garzotto*

**Main category:** cs.HC

**Keywords:** Large Language Models, Conversational Agents, Anthropomorphic Design, Sustainability, User Engagement

**Relevance Score:** 8

**TL;DR:** The paper explores how Large Language Models in Conversational Agents can shift consumer behavior towards sustainability through user engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how anthropomorphic design in Conversational Agents can encourage environmentally conscious behaviors and shift consumption patterns from demand-driven to supply-based.

**Method:** A lab study with 26 participants compared user interactions with a personified conversational agent representing an appliance versus a traditional non-personified assistant.

**Key Contributions:**

	1. Integration of LLMs in Conversational Agents for environmental awareness
	2. Comparison of anthropomorphic and traditional designs
	3. Empirical evidence on user engagement and behavior change

**Result:** Results showed that LLM-based Conversational Agents improved users' self-reported eco-friendly behaviors and confidence in managing energy consumption, with a stronger connection reported for the personified agent.

**Limitations:** The sample size was small (N=26), potentially limiting the generalizability of the findings.

**Conclusion:** Both personified and non-personified designs can effectively promote sustainable behaviors, though engagement levels may vary with design.

**Abstract:** The paper investigates the integration of Large Language Models (LLMs) into Conversational Agents (CAs) to encourage a shift in consumption patterns from a demand-driven to a supply-based paradigm. Specifically, the research examines the role of anthropomorphic design in delivering environmentally conscious messages by comparing two CA designs: a personified agent representing an appliance and a traditional, non-personified assistant. A lab study (N=26) assessed the impact of these designs on interaction, perceived self-efficacy, and engagement. Results indicate that LLM-based CAs significantly enhance users' self-reported eco-friendly behaviors, with participants expressing greater confidence in managing energy consumption. While the anthropomorphic design did not notably affect self-efficacy, those interacting with the personified agent reported a stronger sense of connection with the system. These findings suggest that although anthropomorphic CAs may improve user engagement, both designs hold promise for fostering sustainable behaviors in home energy management.

</details>


### [13] [Assessing the User Experience of Extended Reality Devices for (Dis)Assembly: A Classroom Study](https://arxiv.org/abs/2505.07154)

*Brandon S. Byers, Eleftherios Triantafyllidis, Thibaut Menny, Martin Schulte, Catherine De Wolf*

**Main category:** cs.HC

**Keywords:** Extended Reality, Augmented Reality, Mixed Reality, usability, task load index

**Relevance Score:** 4

**TL;DR:** This research compares user experiences of XR devices for assembly tasks, highlighting Augmented Reality's usability and Mixed Reality's cognitive load advantages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide informed guidance for selecting XR technologies in the architecture, engineering, and construction industry, particularly for assembly tasks.

**Method:** User experience was evaluated using NASA Task Load Index and System Usability Scale metrics through workshops and surveys with graduate civil engineering and architecture students.

**Key Contributions:**

	1. Comparison of XR devices for assembly tasks
	2. Insights into usability and cognitive demand of XR technologies
	3. Guidelines for future XR system evaluations in construction

**Result:** Augmented Reality received the highest usability score, while Mixed Reality exhibited the best task load index, reflecting low cognitive demand.

**Limitations:** 

**Conclusion:** The findings can assist academics and practitioners in selecting appropriate XR systems for real-world assembly scenarios and encourage further XR exploration in circular construction practices.

**Abstract:** Despite the current rise and promising capabilities of Extended Reality (XR) technologies, the architecture, engineering, and construction industry lacks informed guidance when choosing between these technologies, especially for complex processes like assembly and disassembly tasks. This research compares the user experience across different XR devices for (dis)assembly utilizing the NASA Task Load Index and System Usability Scale metrics. Through a workshop and surveys with graduate civil engineering and architecture students, the study found that Augmented Reality scored highest in usability, followed closely by Mixed Reality. However, Mixed Reality showed the best task load index score, indicating low cognitive demand. The findings presented in this research may aid academics and practitioners in making informed decisions when selecting XR systems in practical, real-world assembly scenarios. Moreover, this study suggests opportunities and guidelines for more detailed XR system comparisons and exploration of XR's further role in circular construction practices.

</details>


### [14] [Towards user-centered interactive medical image segmentation in VR with an assistive AI agent](https://arxiv.org/abs/2505.07214)

*Pascal Spiegler, Arash Harirpoush, Yiming Xiao*

**Main category:** cs.HC

**Keywords:** AI, virtual reality, medical segmentation, user interaction, 3D visualization

**Relevance Score:** 9

**TL;DR:** SAMIRA is a conversational AI agent that enhances medical segmentation in VR through user feedback and speech interaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the labor-intensive and error-prone manual segmentation of volumetric medical scans by integrating AI and VR technology.

**Method:** A conversational AI agent, SAMIRA, assists users in 3D medical segmentation via speech interaction and a variety of input modes (VR controller pointing, head pointing, and eye tracking).

**Key Contributions:**

	1. Introduction of SAMIRA, a conversational AI agent for medical segmentation.
	2. Empirical evaluation of input methods for refining segmentation in VR.
	3. Demonstration of high usability scores and task load benefits.

**Result:** User studies showed high usability (SUS=90.0 Â± 9.0) and low task load, demonstrating the effectiveness of SAMIRA in aiding radiological tasks.

**Limitations:** 

**Conclusion:** The integration of AI and VR can significantly enhance user interaction in medical segmentation tasks, offering training potential and ease of use.

**Abstract:** Crucial in disease analysis and surgical planning, manual segmentation of volumetric medical scans (e.g. MRI, CT) is laborious, error-prone, and challenging to master, while fully automatic algorithms can benefit from user-feedback. Therefore, with the complementary power of the latest radiological AI foundation models and virtual reality (VR)'s intuitive data interaction, we propose SAMIRA, a novel conversational AI agent that assists users with localizing, segmenting, and visualizing 3D medical concepts in VR. Through speech-based interaction, the agent helps users understand radiological features, locate clinical targets, and generate segmentation masks that can be refined with just a few point prompts. The system also supports true-to-scale 3D visualization of segmented pathology to enhance patient-specific anatomical understanding. Furthermore, to determine the optimal interaction paradigm under near-far attention-switching for refining segmentation masks in an immersive, human-in-the-loop workflow, we compare VR controller pointing, head pointing, and eye tracking as input modes. With a user study, evaluations demonstrated a high usability score (SUS=90.0 $\pm$ 9.0), low overall task load, as well as strong support for the proposed VR system's guidance, training potential, and integration of AI in radiological segmentation tasks.

</details>


### [15] [A Turing Test for ''Localness'': Conceptualizing, Defining, and Recognizing Localness in People and Machines](https://arxiv.org/abs/2505.07282)

*Zihan Gao, Justin Cranshaw, Jacob Thebault-Spieker*

**Main category:** cs.HC

**Keywords:** localness, digital platforms, human-computer interaction, artificial agents, location-based services

**Relevance Score:** 7

**TL;DR:** The paper investigates how localness is perceived in digital interactions, focusing on both human and artificial agents, with findings that inform the design of location-based services for improved local engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure genuine local participation in digital platforms, which is essential for trust in location-based services and civic engagement systems.

**Method:** A chat-based interaction paradigm inspired by Turing's Imitation Game and Von Ahn's Games With A Purpose, involving 230 participants in conversations to examine cues used to assess local presence.

**Key Contributions:**

	1. Identification of a multi-dimensional framework of localness
	2. Demonstration of factors influencing accurate localness judgments
	3. Insights into how localness can be recognized in artificial agents

**Result:** Participants can more accurately recognize locals than nonlocals, with a multi-dimensional framework of localness identified and key factors influencing accurate localness judgments determined through predictive modeling.

**Limitations:** 

**Conclusion:** The study bridges theoretical perspectives on human-place relationships and practical challenges in digital environments, highlighting localness as a dynamic construct that requires active demonstration of belonging and engagement.

**Abstract:** As digital platforms increasingly mediate interactions tied to place, ensuring genuine local participation is essential for maintaining trust and credibility in location-based services, community-driven platforms, and civic engagement systems. However, localness is a social and relational identity shaped by knowledge, participation, and community recognition. Drawing on the German philosopher Heidegger's concept of dwelling -- which extends beyond physical presence to encompass meaningful connection to place -- we investigate how people conceptualize and evaluate localness in both human and artificial agents. Using a chat-based interaction paradigm inspired by Turing's Imitation Game and Von Ahn's Games With A Purpose, we engaged 230 participants in conversations designed to examine the cues people rely on to assess local presence. Our findings reveal a multi-dimensional framework of localness, highlighting differences in how locals and nonlocals emphasize various aspects of local identity. We show that people are significantly more accurate in recognizing locals than nonlocals, suggesting that localness is an affirmative status requiring active demonstration rather than merely the absence of nonlocal traits. Additionally, we identify conditions under which artificial agents are perceived as local and analyze participants' sensemaking strategies in evaluating localness. Through predictive modeling, we determine key factors that drive accurate localness judgments. By bridging theoretical perspectives on human-place relationships with practical challenges in digital environments, our work informs the design of location-based services that foster meaningful local engagement. Our findings contribute to a broader understanding of localness as a dynamic and relational construct, reinforcing the importance of dwelling as a process of belonging, recognition, and engagement with place.

</details>


### [16] [User Identification with LFI-Based Eye Movement Data Using Time and Frequency Domain Features](https://arxiv.org/abs/2505.07326)

*Suleyman Ozdel, Johannes Meyer, Yasmeen Abdrabou, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** Laser interferometry, Eye-tracking, User identification, Machine learning, Privacy

**Relevance Score:** 6

**TL;DR:** This study explores user identification using laser interferometry-based eye-tracking data, achieving high classification accuracy while highlighting privacy concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the potential of LFI-based eye-tracking systems for user identification while addressing privacy implications.

**Method:** Analysis of features extracted from raw high-frequency eye movement data in both time and frequency domains, followed by a multi-class classification model.

**Key Contributions:**

	1. First to demonstrate effective user identification using LFI-based eye-tracking data
	2. Achieved high accuracy in classifying individuals based on eye movement patterns
	3. Provided insights into the effects of sampling rate and window size on model performance

**Result:** Achieved 93.14% classification accuracy and 2.52% EER using velocity and distance measurements without direct gaze data.

**Limitations:** 

**Conclusion:** LFI-based eye-tracking shows potential for secure user identification but raises new privacy risks.

**Abstract:** Laser interferometry (LFI)-based eye-tracking systems provide an alternative to traditional camera-based solutions, offering improved privacy by eliminating the risk of direct visual identification. However, the high-frequency signals captured by LFI-based trackers may still contain biometric information that enables user identification. This study investigates user identification from raw high-frequency LFI-based eye movement data by analyzing features extracted from both the time and frequency domains. Using velocity and distance measurements without requiring direct gaze data, we develop a multi-class classification model to accurately distinguish between individuals across various activities. Our results demonstrate that even without direct visual cues, eye movement patterns exhibit sufficient uniqueness for user identification, achieving 93.14% accuracy and a 2.52% EER with 5-second windows across both static and dynamic tasks. Additionally, we analyze the impact of sampling rate and window size on model performance, providing insights into the feasibility of LFI-based biometric recognition. Our findings demonstrate the novel potential of LFI-based eye-tracking for user identification, highlighting both its promise for secure authentication and emerging privacy risks. This work paves the way for further research into high-frequency eye movement data.

</details>


### [17] [Thalamus: A User Simulation Toolkit for Prototyping Multimodal Sensing Studies](https://arxiv.org/abs/2505.07340)

*Kayhan Latifzadeh, Luis A. Leiva*

**Main category:** cs.HC

**Keywords:** HCI, user studies, physiological measurements, software toolkit, multimodal signals

**Relevance Score:** 8

**TL;DR:** Thalamus is a software toolkit designed for efficient collection and simulation of multimodal signals in user studies, enhancing preparation for physiological and behavioral measurements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** User studies involving physiological and behavioral measurements are costly and time-consuming due to complex design and calibration requirements.

**Method:** Thalamus provides tools for modifying, synchronizing, and broadcasting signals from various devices, facilitating the experiment design process.

**Key Contributions:**

	1. Cross-platform and cross-device functionality
	2. Simplified user interface for experimenters
	3. Facilitates signal simulation and synchronization

**Result:** The toolkit allows experimenters to prepare for unexpected scenarios without needing to purchase or install specific devices beforehand.

**Limitations:** 

**Conclusion:** Thalamus represents a useful asset for HCI researchers aiming to streamline the preparation for user studies involving diverse physiological data.

**Abstract:** Conducting user studies that involve physiological and behavioral measurements is very time-consuming and expensive, as it not only involves a careful experiment design, device calibration, etc. but also a careful software testing. We propose Thalamus, a software toolkit for collecting and simulating multimodal signals that can help the experimenters to prepare in advance for unexpected situations before reaching out to the actual study participants and even before having to install or purchase a specific device. Among other features, Thalamus allows the experimenter to modify, synchronize, and broadcast physiological signals (as coming from various data streams) from different devices simultaneously and not necessarily located in the same place. Thalamus is cross-platform, cross-device, and simple to use, making it thus a valuable asset for HCI research.

</details>


### [18] [Time Perception in Virtual Reality: Effects of Emotional Valence and Stress Level](https://arxiv.org/abs/2505.07354)

*Kyriaki Syrigou, Marina Stoforou, Panagiotis Kourtesis*

**Main category:** cs.HC

**Keywords:** Emotional States, Time Perception, Virtual Reality, Arousal, Stress

**Relevance Score:** 7

**TL;DR:** This study examines how emotional valence and stress influence time perception in Virtual Reality (VR) environments, revealing that calming environments lead to overestimation of time while stressful ones cause underestimation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how emotional states and stress affect time perception, particularly in immersive media like VR, given the inconsistent findings in prior research.

**Method:** The study involved 54 adults exploring three VR environments (tranquil, neutral, and stressful) while assessing valence, stress, mood, and perceived duration using various psychological scales and tasks.

**Key Contributions:**

	1. Integrates the Attentional Gate Model and Internal Clock Model to analyze time perception in VR.
	2. Demonstrates the impact of environmental valence on time estimation, offering insights for future VR applications.
	3. Suggests VR as a platform for exploring emotion-related temporal distortions.

**Result:** Emotional valence significantly influenced perceived duration: calming VR environments led to overestimation of time, stressful environments led to underestimation, and neutral environments provided intermediate results.

**Limitations:** Findings may not generalize beyond the specific demographics of the study or the limited number of VR environments tested.

**Conclusion:** The findings support that emotional valence affecting attentional allocation significantly alters perceived duration, while baseline stress did not have a noteworthy impact in the context of VR.

**Abstract:** Background & Objective: Emotional states and stress distort time perception, yet findings are inconsistent, particularly in immersive media. Integrating the Attentional Gate Model (AGM) and Internal Clock Model (ICM), we examined how emotional valence and stress alter perceived duration in Virtual Reality (VR). This study assesses the effects of valence (calming, neutral, stressful) and stress (low/high) on prospective time estimation, mood, and arousal. Methods: Fifty-four adults (18-39 years) explored three custom VR environments: (1) a tranquil Japanese garden, (2) an affectively neutral room, and (3) a threatening underground sewer. Active navigation promoted presence; a distraction task separated conditions. Valence and arousal were assessed with the Visual Analog Mood Scales, stress with the Perceived Stress Scale-10 (PSS-10), and perceived duration with a verbal estimation task. Mixed-model ANOVAs evaluated main and interaction effects. Results: Valence reliably shaped perceived duration: calming VR led to overestimation, stressful VR to underestimation, and neutral VR to intermediate timing. Baseline stress level, as measured by PSS-10, neither altered timing nor interacted with valence. Nevertheless, the VR environments affected VAMS' mood metrics: calming environments elevated mood and reduced perceived stress, whereas stressful environments lowered mood and heightened stress. Conclusions: Findings support the AGM-attentionally demanding negative environments shorten perceived time-and the ICM-valence-linked arousal speeds or slows the pacemaker. Contrary to classical predictions, in VR, baseline stress did not distort duration, suggesting valence-driven attentional allocation outweighs pre-exposure stress levels. VR offers a controllable platform for dissecting time-perception mechanisms and advancing interventions that target emotion-related temporal distortions.

</details>


### [19] [Examining the Role of LLM-Driven Interactions on Attention and Cognitive Engagement in Virtual Classrooms](https://arxiv.org/abs/2505.07377)

*Suleyman Ozdel, Can Sarpkaya, Efe Bozkir, Hong Gao, Enkelejda Kasneci*

**Main category:** cs.HC

**Keywords:** large language models, virtual reality, user engagement, cognitive load, educational technology

**Relevance Score:** 9

**TL;DR:** This study explores the impact of LLM-driven peers in VR educational settings on student engagement and cognitive load.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of large language models on user engagement and attention in educational environments.

**Method:** Utilized a fully LLM-driven virtual learning environment to examine student behaviors regarding peer question-asking.

**Key Contributions:**

	1. Investigation of LLM influences in VR educational contexts
	2. Demonstration of increased attention via peer question-asking
	3. Design recommendations for VR learning environments

**Result:** Students showed increased attention focused on learning content when LLM peers asked questions, particularly in complex subjects, without adding extraneous cognitive load.

**Limitations:** 

**Conclusion:** The findings suggest optimizing VR learning spaces by integrating LLM-driven peer interactions to boost engagement and attention.

**Abstract:** Transforming educational technologies through the integration of large language models (LLMs) and virtual reality (VR) offers the potential for immersive and interactive learning experiences. However, the effects of LLMs on user engagement and attention in educational environments remain open questions. In this study, we utilized a fully LLM-driven virtual learning environment, where peers and teachers were LLM-driven, to examine how students behaved in such settings. Specifically, we investigate how peer question-asking behaviors influenced student engagement, attention, cognitive load, and learning outcomes and found that, in conditions where LLM-driven peer learners asked questions, students exhibited more targeted visual scanpaths, with their attention directed toward the learning content, particularly in complex subjects. Our results suggest that peer questions did not introduce extraneous cognitive load directly, as the cognitive load is strongly correlated with increased attention to the learning material. Considering these findings, we provide design recommendations for optimizing VR learning spaces.

</details>


### [20] [Shots and Boosters: Exploring the Use of Combined Prebunking Interventions to Raise Critical Thinking and Create Long-Term Protection Against Misinformation](https://arxiv.org/abs/2505.07486)

*Huiyun Tang, Anastasia Sergeeva*

**Main category:** cs.HC

**Keywords:** misinformation, critical thinking, AI, media literacy, educational interventions

**Relevance Score:** 4

**TL;DR:** This paper explores educational interventions to enhance critical thinking in online media consumption, proposing an AI-driven design concept to combat misinformation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to mitigate misinformation effectively in online spaces through improved critical thinking skills.

**Method:** Explores a combination of intervention methods designed to strengthen users' reasoning skills rather than just debunk false claims.

**Key Contributions:**

	1. Proposes a novel combination of educational interventions to combat misinformation
	2. Introduces an AI-driven design concept for enhancing critical reasoning
	3. Highlights the significance of critical thinking in the context of online media

**Result:** The paper presents a design concept integrating AI strategies that foster critical reasoning and media literacy.

**Limitations:** 

**Conclusion:** AI can play a supportive role in enhancing critical thinking and media literacy through thoughtfully designed interventions.

**Abstract:** The problem of how to effectively mitigate the flow of misinformation remains a significant challenge. The classical approach to this is public disapproval of claims or "debunking." The approach is still widely used on social media, but it has some severe limitations in terms of applicability and efficiency. An alternative strategy is to enhance individuals' critical thinking through educational interventions. Instead of merely disproving misinformation, these approaches aim to strengthen users' reasoning skills, enabling them to evaluate and reject false information independently. In this position paper, we explore a combination of intervention methods designed to improve critical thinking in the context of online media consumption. We highlight the role of AI in supporting different stages of these interventions and present a design concept that integrates AI-driven strategies to foster critical reasoning and media literacy.

</details>


### [21] [Design Requirements for Patient-Centered Digital Health Applications: Supporting Patients' Values in Postoperative Delirium Prevention](https://arxiv.org/abs/2505.07498)

*David LeimstÃ¤dtner, Fatima Halzl-YÃ¼rek, Claudia Spies, Claudia MÃ¼ller-Birn*

**Main category:** cs.HC

**Keywords:** Patient engagement, Postoperative delirium, Digital health application, Value sensitive design, Healthcare technology

**Relevance Score:** 8

**TL;DR:** Development of a patient-centered digital health application to enhance patient engagement in preventing postoperative delirium (POD) through value-sensitive design.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Postoperative delirium is a common complication after surgery in older adults, with significant adverse health consequences. Patient engagement is crucial for POD prevention.

**Method:** Interviews with medical professionals and patients were conducted to understand the patient journey and identify requirements for a digital health application.

**Key Contributions:**

	1. Identification of design requirements for a patient-centered digital health application
	2. Emphasis on the role of relatives in patient support
	3. Value-sensitive design approach to healthcare technology

**Result:** Analysis of patient interviews revealed four key design requirements for the application: adapting communication, ensuring procedural transparency, fostering patient empowerment, and involving relatives.

**Limitations:** 

**Conclusion:** The proposed digital health application aims to reduce the risks of POD by actively engaging patients in their care through enhanced communication and support.

**Abstract:** Postoperative delirium (POD) is among the most common complications after surgeries for older adults and can entail long-term adverse health consequences. Active patient participation in POD prevention presents a central factor in reducing these risks. To support patient engagement through a digital health application, we use value sensitive design approaches to identify the requirements for a patient-centered digital health application supporting patient engagement in POD prevention. Through interviews with medical professionals and patient representatives, we construct a patient journey, which serves as the basis for twelve patient value journey interviews. In these interviews, patients from the high-risk group for POD revisit their recent experience of undergoing surgery to elicit barriers, needs, and values concerning POD prevention from a patient perspective. An analysis of the patient interviews derives four design requirements for a digital health application supporting patients regarding POD prevention: the adaptation of patient-centered communication, the provision of procedural transparency, fostering patient empowerment through consistent guidance, and explicitly addressing relatives as mediators and supporters for a patient after a POD occurrence.

</details>


### [22] [The Human-Data-Model Interaction Canvas for Visual Analytics](https://arxiv.org/abs/2505.07534)

*JÃ¼rgen Bernard*

**Main category:** cs.HC

**Keywords:** Visual Analytics, HDMI Canvas, Human-centered design

**Relevance Score:** 5

**TL;DR:** This paper introduces the HDMI Canvas, a new perspective on Visual Analytics (VA) that characterizes roles of humans, data, and models to enhance VA processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper reflects on existing VA process models and frameworks, identifying the need for a fresh perspective that integrates human-centered methodologies.

**Method:** The HDMI Canvas systematically characterizes the diverse roles of humans, data, and models in VA, utilizing two case studies to demonstrate its utility.

**Key Contributions:**

	1. Introduction of the HDMI Canvas
	2. Systematic characterization of roles in VA
	3. Improvement of VA design for interdisciplinary collaboration

**Result:** The HDMI Canvas enables clearer differentiation between VA building blocks and enhances design for new VA processes, improving interdisciplinary collaboration.

**Limitations:** 

**Conclusion:** The HDMI Canvas complements existing VA frameworks and guides the design of VA processes while being optimized for external stakeholders.

**Abstract:** Visual Analytics (VA) integrates humans, data, and models as key actors in insight generation and data-driven decision-making. This position paper values and reflects on 16 VA process models and frameworks and makes nine high-level observations that motivate a fresh perspective on VA. The contribution is the HDMI Canvas, a perspective to VA that complements the strengths of existing VA process models and frameworks. It systematically characterizes diverse roles of humans, data, and models, and how these actors benefit from and contribute to VA processes. The descriptive power of the HDMI Canvas eases the differentiation between a series of VA building blocks, rather than describing general VA principles only. The canvas includes modern human-centered methodologies, including human knowledge externalization and forms of feedback loops, while interpretable and explainable AI highlight model contributions beyond their conventional outputs. The HDMI Canvas has generative power, guiding the design of new VA processes and is optimized for external stakeholders, improving VA outreach, interdisciplinary collaboration, and user-centered design. The utility of the HDMI Canvas is demonstrated through two preliminary case studies.

</details>


### [23] [Decoding Chess Puzzle Play and Standard Cognitive Tasks for BCI: A Low-Cost EEG Study](https://arxiv.org/abs/2505.07592)

*Matthew Russell, Samuel Youkeles, William Xia, Kenny Zheng, Aman Shah, Robert J. K. Jacob*

**Main category:** cs.HC

**Keywords:** EEG, Brain-Computer Interface, Machine Learning, Cognitive Workload, Adaptive Systems

**Relevance Score:** 7

**TL;DR:** Consumer-grade EEG devices can detect cognitive workload levels and differentiate between tasks using machine learning, showing potential for adaptive BCI applications.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** Consumer-grade EEG devices have potential for Brain-Computer Interface applications, but their effectiveness in detecting subtle cognitive states is not well-studied.

**Method:** Combination of cognitive paradigms (N-Back, Stroop, Mental Rotation) and ecological tasks (Chess puzzles) to assess cognitive workload; machine learning used for predictive classification.

**Key Contributions:**

	1. Demonstrates the efficacy of MUSE 2 device in cognitive workload detection
	2. Shows reliable use of machine learning for real-time classification
	3. Provides open resources for future researchers

**Result:** Successful distinctions of workload levels and task types, with reliable predictive power for the N-Back task and effective cross-task classification.

**Limitations:** 

**Conclusion:** Consumer-grade EEG devices are capable of detecting and differentiating cognitive workloads, supporting their use in adaptive BCI applications. Provided research code and data for future research.

**Abstract:** While consumer-grade electroencephalography (EEG) devices show promise for Brain-Computer Interface (BCI) applications, their efficacy in detecting subtle cognitive states remains understudied. Using a combination of established cognitive paradigms (N-Back, Stroop, and Mental Rotation) and a novel ecological task (Chess puzzles), we demonstrate successful distinctions of workload levels within some tasks, as well as differentiation between task types using the MUSE 2 device. With machine learning we further show reliable predictive power to differentiate between workload levels in the N-Back task, while also achieving effective cross-task classification. These findings demonstrate that consumer-grade EEG devices can effectively detect and differentiate various forms of cognitive workload, and that they can be leveraged with some success towards real-time classification distinguishing workload in some tasks, as well as in differentiating between nuanced cognitive states, supporting their potential use in adaptive BCI applications. Research code and data are further provided for future researchers.

</details>


### [24] [VTutor for High-Impact Tutoring at Scale: Managing Engagement and Real-Time Multi-Screen Monitoring with P2P Connections](https://arxiv.org/abs/2505.07736)

*Eason Chen, Xinyi Tang, Aprille Xi, Chenyu Lin, Conrad Borchers, Shivang Gupta, Jionghao Lin, Kenneth R Koedinger*

**Main category:** cs.HC

**Keywords:** Hybrid tutoring, AI-driven feedback, Learner engagement

**Relevance Score:** 8

**TL;DR:** VTutor is a web-based hybrid tutoring platform that supports real-time monitoring and engagement by using peer-to-peer screen sharing and AI-driven virtual avatars.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance hybrid tutoring by improving tutor's ability to monitor multiple students concurrently and provide timely interventions.

**Method:** VTutor utilizes a multi-student monitoring dashboard combined with AI-powered avatar prompts for real-time feedback.

**Key Contributions:**

	1. Introduction of a multi-student monitoring dashboard
	2. Integration of AI-powered avatars for feedback
	3. Improvement of remote tutoring effectiveness

**Result:** The platform enables tutors to efficiently identify struggling students and facilitate one-on-one attention, improving overall learner engagement.

**Limitations:** 

**Conclusion:** VTutor holds promise for scaling AI-driven tutoring while addressing real-time learner needs and engagement during hybrid learning setups.

**Abstract:** Hybrid tutoring, where a human tutor supports multiple students in learning with educational technology, is an increasingly common application to deliver high-impact tutoring at scale. However, past hybrid tutoring applications are limited in guiding tutor attention to students that require support. Specifically, existing conferencing tools, commonly used in hybrid tutoring, do not allow tutors to monitor multiple students' screens while directly communicating and attending to multiple students simultaneously. To address this issue, this paper introduces VTutor, a web-based platform leveraging peer-to-peer screen sharing and virtual avatars to deliver real-time, context-aware tutoring feedback at scale. By integrating a multi-student monitoring dashboard with AI-powered avatar prompts, VTutor empowers a single educator or tutor to rapidly detect off-task or struggling students and intervene proactively, thus enhancing the benefits of one-on-one interactions in classroom contexts with several students. Drawing on insight from the learning sciences and past research on animated pedagogical agents, we demonstrate how stylized avatars can potentially sustain student engagement while accommodating varying infrastructure constraints. Finally, we address open questions on refining large-scale, AI-driven tutoring solutions for improved learner outcomes, and how VTutor could help interpret real-time learner interactions to support remote tutors at scale. The VTutor platform can be accessed at https://ls2025.vtutor.ai. The system demo video is at https://ls2025.vtutor.ai/video.

</details>


### [25] [Can Interpretability Layouts Influence Human Perception of Offensive Sentences?](https://arxiv.org/abs/2403.05581)

*Thiago Freitas dos Santos, Nardine Osman, Marco Schorlemmer*

**Main category:** cs.HC

**Keywords:** ML interpretability, hate speech, user study

**Relevance Score:** 6

**TL;DR:** Investigation of ML interpretability layouts on user opinions regarding hate speech evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the impact of ML interpretability on user views in evaluating hate speech, given conflicting findings in the literature.

**Method:** Conducted a user study with statistical and qualitative analyses of responses to various ML interpretability layouts.

**Key Contributions:**

	1. Empirical evidence on ML interpretability's effect in hate speech evaluation
	2. Demonstration of ML interpretability promoting user corrective feedback
	3. Insights into evaluating ML behavior beyond standard metrics

**Result:** Statistical results indicate no significant influence of interpretability layouts on views, but qualitative insights show benefits like corrective feedback and deeper evaluation of model behavior.

**Limitations:** The interpretability layouts did not significantly influence user opinions, suggesting a need for more effective designs.

**Conclusion:** While interpretability layouts didn't significantly alter views, they promoted user engagement and understanding of model behavior.

**Abstract:** This paper conducts a user study to assess whether three machine learning (ML) interpretability layouts can influence participants' views when evaluating sentences containing hate speech, focusing on the "Misogyny" and "Racism" classes. Given the existence of divergent conclusions in the literature, we provide empirical evidence on using ML interpretability in online communities through statistical and qualitative analyses of questionnaire responses. The Generalized Additive Model estimates participants' ratings, incorporating within-subject and between-subject designs. While our statistical analysis indicates that none of the interpretability layouts significantly influences participants' views, our qualitative analysis demonstrates the advantages of ML interpretability: 1) triggering participants to provide corrective feedback in case of discrepancies between their views and the model, and 2) providing insights to evaluate a model's behavior beyond traditional performance metrics.

</details>


### [26] [Negotiating the Shared Agency between Humans & AI in the Recommender System](https://arxiv.org/abs/2403.15919)

*Mengke Wu, Weizi Liu, Yanyun Wang, Mike Yao*

**Main category:** cs.HC

**Keywords:** user agency, recommendation algorithms, transparency, user control, AI fairness

**Relevance Score:** 8

**TL;DR:** This study introduces a dual-control mechanism to enhance user agency in smart recommendation systems, examining the interplay of transparency and user control in content delivery.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about user agency in smart recommendation algorithms due to their opacity and one-way outputs.

**Method:** A between-subject experiment with 161 participants evaluating transparency and control levels on user experience.

**Key Contributions:**

	1. Introduces a dual-control mechanism for recommendation systems
	2. Demonstrates the insufficiency of transparency alone in improving user agency
	3. Establishes a relationship between user control levels and perceived agency

**Result:** Transparency alone does not improve user agency and may even disempower users; combining transparency with user controls significantly enhances agency.

**Limitations:** 

**Conclusion:** The research highlights the importance of user autonomy in the design of recommender systems and suggests that greater control over outcomes can lead to enhanced user agency.

**Abstract:** Smart recommendation algorithms have revolutionized content delivery and improved efficiency across various domains. However, concerns about user agency arise from the algorithms' inherent opacity (information asymmetry) and one-way output (power asymmetry). This study introduces a dual-control mechanism aimed at enhancing user agency, empowering users to manage both data collection and, novelly, the degree of algorithmically tailored content they receive. In a between-subject experiment with 161 participants, we evaluated the impact of varying levels of transparency and control on user experience. Results show that transparency alone is insufficient to foster a sense of agency, and may even exacerbate disempowerment compared to displaying outcomes directly. Conversely, combining transparency with user controls-particularly those allowing direct influence on outcomes-significantly enhances user agency. This research provides a proof-of-concept for a novel approach and lays the groundwork for designing more user-centered recommender systems that emphasize user autonomy and fairness in AI-driven content delivery.

</details>


### [27] [MARV: Multiview Augmented Reality Visualisation for Exploring Rich Material Data](https://arxiv.org/abs/2404.14814)

*Alexander Gall, Anja Heim, Eduard GrÃ¶ller, Christoph Heinzl*

**Main category:** cs.HC

**Keywords:** visual analytics, augmented reality, material data, interactive exploration, visualization techniques

**Relevance Score:** 3

**TL;DR:** MARV is an immersive visual analytics system for analyzing complex material data in augmented reality, offering novel visualization techniques that enhance data exploration and pattern identification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by materials experts in analyzing complex, large, and heterogeneous data using conventional desktop-based systems and 2D visualization techniques.

**Method:** MARV incorporates three visualization techniques: MDD Glyphs with Skewness Kurtosis Mapper, Temporal Evolution Tracker, and Chrono Bins, facilitating interactive exploration of multidimensional data.

**Key Contributions:**

	1. Introduces MARV, an immersive analytics system for complex material data.
	2. Develops three novel visualization techniques to facilitate data exploration.
	3. Demonstrates improved analytical capabilities and pattern identification through user evaluation.

**Result:** Qualitative evaluations showed that the immersive environment of MARV significantly improves analytical capabilities and helps in identifying patterns, anomalies, and changes over time.

**Limitations:** 

**Conclusion:** The immersive visualization system enhances the analytical process for materials experts, making it more efficient and engaging compared to traditional methods.

**Abstract:** Rich material data is complex, large and heterogeneous, integrating primary and secondary non-destructive testing data for spatial, spatio-temporal, as well as high-dimensional data analyses. Currently, materials experts mainly rely on conventional desktop-based systems using 2D visualisation techniques, which render respective analyses a time-consuming and mentally demanding challenge. MARV is a novel immersive visual analytics system, which makes analyses of such data more effective and engaging in an augmented reality setting. For this purpose, MARV includes three newly designed visualisation techniques: MDD Glyphs with a Skewness Kurtosis Mapper, Temporal Evolution Tracker, and Chrono Bins, facilitating interactive exploration and comparison of multidimensional distributions of attribute data from multiple time steps. A qualitative evaluation conducted with materials experts in a real-world case study demonstrates the benefits of the proposed visualisation techniques. This evaluation revealed that combining spatial and abstract data in an immersive environment improves their analytical capabilities and facilitates the identification of patterns, anomalies, as well as changes over time.

</details>


### [28] [Designing Adaptive User Interfaces for mHealth Applications Targeting Chronic Disease: A User-Centered Approach](https://arxiv.org/abs/2405.08302)

*Wei Wang, John Grundy, Hourieh Khalajzadeh, Anuradha Madugalla, Humphrey O. Obie*

**Main category:** cs.HC

**Keywords:** Adaptive User Interfaces, mHealth, chronic disease management, user engagement, design guidelines

**Relevance Score:** 9

**TL;DR:** This paper presents actionable guidelines for designing Adaptive User Interfaces in mobile health applications to improve user engagement and support chronic disease management.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The under-utilization of mHealth applications due to low engagement, accessibility issues, and poor adherence among chronic disease users.

**Method:** A two-stage study involving the evaluation of an AUI prototype through focus groups, interviews, and surveys; followed by refinement of design guidelines based on user feedback and evaluations.

**Key Contributions:**

	1. Development of nine actionable AUI design guidelines for mHealth applications
	2. Evaluation methodology involving end users and software practitioners
	3. Real-world application through case studies and user reviews

**Result:** Nine finalized AUI design guidelines for mHealth applications were developed and validated through user and practitioner feedback, alongside a case study of four mHealth applications.

**Limitations:** The study's findings are based on a limited number of mHealth applications and user feedback, which may not represent all contexts and demographics.

**Conclusion:** The guidelines provide a practical framework for software practitioners to enhance user adaptation in mHealth applications, thereby better supporting chronic disease self-management.

**Abstract:** Mobile Health (mHealth) applications have demonstrated considerable potential in supporting chronic disease self-management; however, they remain under-utilised due to low engagement, limited accessibility, and poor long-term adherence. These issues are particularly prominent among users with chronic disease, whose needs and capabilities vary widely. To address this, Adaptive User Interfaces (AUIs) offer a dynamic solution by tailoring interface features to users' preferences, health status, and contexts. This paper presents a two-stage study to develop and validate actionable AUI design guidelines for mHealth applications. In stage one, an AUI prototype was evaluated through focus groups, interviews, and a standalone survey, revealing key user challenges and preferences. These insights informed the creation of an initial set of guidelines. In stage two, the guidelines were refined based on feedback from 20 end users and evaluated by 43 software practitioners through two surveys. This process resulted in nine finalized guidelines. To assess real-world relevance, a case study of four mHealth applications was conducted, with findings supported by user reviews highlighting the utility of the guidelines in identifying critical adaptation issues. This study offers actionable, evidence-based guidelines that help software practitioners design AUIs in mHealth to better support individuals managing chronic diseases

</details>


### [29] [Model Human Learners: Computational Models to Guide Instructional Design](https://arxiv.org/abs/2502.02456)

*Christopher J. MacLellan*

**Main category:** cs.HC

**Keywords:** instructional design, learning models, A/B testing

**Relevance Score:** 4

**TL;DR:** This paper proposes a Model Human Learner, a computational model that assists instructional designers by accurately predicting outcomes of A/B experiments and generating learning curves without human data.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To help instructional designers navigate numerous design choices and identify effective interventions in educational contexts.

**Method:** The paper presents a computational model that evaluates two human A/B experiments, focusing on problem sequencing and item design interventions.

**Key Contributions:**

	1. Introduction of the Model Human Learner concept
	2. Demonstration of predictive accuracy in A/B tests
	3. Ability to generate learning curves without human data

**Result:** The model successfully predicts outcomes and generates learning curves, offering theoretical insights into intervention effectiveness.

**Limitations:** 

**Conclusion:** The findings establish a foundation for future development of Model Human Learners that can integrate various cognitive and learning theories for improved instructional design.

**Abstract:** Instructional designers face an overwhelming array of design choices, making it challenging to identify the most effective interventions. To address this issue, I propose the concept of a Model Human Learner, a unified computational model of learning that can aid designers in evaluating candidate interventions. This paper presents the first successful demonstration of this concept, showing that a computational model can accurately predict the outcomes of two human A/B experiments -- one testing a problem sequencing intervention and the other testing an item design intervention. It also demonstrates that such a model can generate learning curves without requiring human data and provide theoretical insights into why an instructional intervention is effective. These findings lay the groundwork for future Model Human Learners that integrate cognitive and learning theories to support instructional design across diverse tasks and interventions.

</details>


### [30] [AudioMiXR: Spatial Audio Object Manipulation with 6DoF for Sound Design in Augmented Reality](https://arxiv.org/abs/2502.02929)

*Brandon Woodard, Margarita Geleta, Joseph J. LaViola Jr., Andrea Fanelli, Rhonda Wilson*

**Main category:** cs.HC

**Keywords:** Augmented Reality, 3D Sound Design, Six Degrees of Freedom, User Interaction, Audio Interfaces

**Relevance Score:** 7

**TL;DR:** AudioMiXR is an AR interface for assessing user manipulation of virtual audio objects using 6DoF on a head-mounted display for 3D sound design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of existing 3D sound design tools that are typically bound to desktop displays, which restrict spatial awareness in sound mixing environments.

**Method:** An exploratory study was conducted with 27 participants, including both expert and non-expert sound designers, utilizing a within-subjects design to analyze the process of designing music and cinematic soundscapes.

**Key Contributions:**

	1. Introduction of AudioMiXR as an AR interface for sound design
	2. Identification of design lessons for 3D sound in XR
	3. Proposed application domains for 6DoF sound design

**Result:** The thematic analysis of participant feedback led to the identification of two key design lessons for 6DoF sound design in XR: Proprioception for AR Sound Design and Balancing Audio-Visual Modalities in AR GUIs.

**Limitations:** The study is limited by its exploratory nature and the sample size of participants involved.

**Conclusion:** The findings provide foundational insights for future research directions in 3D sound design, proposing application domains that could reap benefits from 6DoF sound design techniques.

**Abstract:** We present AudioMiXR, an augmented reality (AR) interface intended to assess how users manipulate virtual audio objects situated in their physical space using six degrees of freedom (6DoF) deployed on a head-mounted display (Apple Vision Pro) for 3D sound design. Existing tools for 3D sound design are typically constrained to desktop displays, which may limit spatial awareness of mixing within the execution environment. Utilizing an XR HMD to create soundscapes may provide a real-time test environment for 3D sound design, as modern HMDs can provide precise spatial localization assisted by cross-modal interactions. However, there is no research on design guidelines specific to sound design with six degrees of freedom (6DoF) in XR. To provide a first step toward identifying design-related research directions in this space, we conducted an exploratory study where we recruited 27 participants, consisting of expert and non-expert sound designers. The goal was to assess design lessons that can be used to inform future research venues in 3D sound design. We ran a within-subjects study where users designed both a music and cinematic soundscapes. After thematically analyzing participant data, we constructed two design lessons: 1. Proprioception for AR Sound Design, and 2. Balancing Audio-Visual Modalities in AR GUIs. Additionally, we provide application domains that can benefit most from 6DoF sound design based on our results.

</details>


### [31] [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)

*Cansu Koyuturk, Emily Theophilou, Sabrina Patania, Gregor Donabauer, Andrea Martinenghi, Chiara Antico, Alessia Telari, Alessia Testa, Sathya Bursic, Franca Garzotto, Davinia Hernandez-Leo, Udo Kruschwitz, Davide Taibi, Simona Amenta, Martin Ruskov, Dimitri Ognibene*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Computer Interaction, Effective Prompting

**Relevance Score:** 9

**TL;DR:** This study explores how structured prompting guidelines enhance user interactions with large language models (LLMs) in AI communication, revealing insights on effective prompting strategies and their impact on AI response quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges users face in prompting large language models, which often lead to inefficient responses due to vague or poorly structured queries.

**Method:** An educational experiment where participants received structured guidance on effective prompting through three types of prompting guidelines, followed by analysis of 642 interactions to assess user behavior and prompting efficacy.

**Key Contributions:**

	1. Introduction of a task-specific framework for prompting
	2. Comparison of multiple prompting guidelines
	3. Insights into user behavior and AI interaction quality

**Result:** The study identifies common prompting errors, behavioral patterns, and evaluates the effectiveness of different prompting guidelines on user behavior and AI response quality, highlighting enhanced user competency in AI interactions.

**Limitations:** 

**Conclusion:** Structured prompting guidance significantly improves user interactions with LLMs, with implications for AI literacy and the design of more effective AI systems.

**Abstract:** Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.

</details>


### [32] [Under Pressure: Contextualizing Workplace Stress Towards User-Centered Interventions](https://arxiv.org/abs/2504.15480)

*Antonin Brun, Gale Lucas, BurÃ§in Becerik-Gerber*

**Main category:** cs.HC

**Keywords:** workplace stress, stress management, user-centered design, coping mechanisms, office workers

**Relevance Score:** 6

**TL;DR:** A study examining individual experiences of workplace stress through interviews with office workers, emphasizing personalized stress management strategies and the dual nature of stress.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the personal and contextual factors influencing workplace stress that current interventions often overlook.

**Method:** Semi-structured interviews with eight office workers to understand their experiences with workplace stress.

**Key Contributions:**

	1. Insights into individual stress experiences in the workplace
	2. Identification of effective coping mechanisms
	3. Recommendations for personalized stress management interventions

**Result:** Identification of key stress triggers, coping mechanisms, and insights on the individualized nature of stress experiences.

**Limitations:** 

**Conclusion:** User-centered stress management solutions should consider individual needs and the positive aspects of stress.

**Abstract:** Stress is a pervasive challenge that significantly impacts worker health and well-being. Workplace stress is driven by various factors, ranging from organizational changes to poor workplace design. Although individual stress management strategies have been shown to be effective, current interventions often overlook personal and contextual factors shaping stress experiences. In this study, we conducted semi-structured interviews with eight office workers to gain a deeper understanding of their personal experiences with workplace stress. Our analysis reveals key stress triggers, coping mechanisms, and reflections on past stressful events. We highlight the multifaceted and individualized nature of workplace stress, emphasizing the importance of intervention timing, modality, and recognizing that stress is not solely a negative experience but can also have positive effects. Our findings provide actionable insights for the design of user-centered stress management solutions more attuned to the needs of office workers.

</details>


### [33] [Understanding Learner-LLM Chatbot Interactions and the Impact of Prompting Guidelines](https://arxiv.org/abs/2504.07840)

*Cansu Koyuturk, Emily Theophilou, Sabrina Patania, Gregor Donabauer, Andrea Martinenghi, Chiara Antico, Alessia Telari, Alessia Testa, Sathya Bursic, Franca Garzotto, Davinia Hernandez-Leo, Udo Kruschwitz, Davide Taibi, Simona Amenta, Martin Ruskov, Dimitri Ognibene*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Computer Interaction, Prompting Guidelines, AI Usability, AI Literacy

**Relevance Score:** 9

**TL;DR:** This study examines how structured prompting guidelines can improve user interactions with Large Language Models (LLMs) and enhance AI communication efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the accessibility of LLMs, users often struggle with effective prompting, leading to inefficient AI responses. This study aims to understand and improve user-AI interactions.

**Method:** An educational experiment was conducted with 107 participants analyzing 642 interactions. Three prompting guidelines were compared to assess user behavior and prompting efficacy.

**Key Contributions:**

	1. Introduced a structured framework for effective prompting in LLMs.
	2. Compared different prompting guidelines and their efficacy.
	3. Identified common prompting errors and user behavioral patterns.

**Result:** The study categorizes common prompting errors and identifies behavioral patterns. It shows that structured prompting guidance improves user adherence to strategies and the quality of AI responses.

**Limitations:** The study is limited to a specific educational setting and may not generalize to all user groups or applications.

**Conclusion:** The findings highlight the importance of structured prompting guidance in enhancing user competency and offer insights for improving AI literacy and chatbot usability.

**Abstract:** Large Language Models (LLMs) have transformed human-computer interaction by enabling natural language-based communication with AI-powered chatbots. These models are designed to be intuitive and user-friendly, allowing users to articulate requests with minimal effort. However, despite their accessibility, studies reveal that users often struggle with effective prompting, resulting in inefficient responses. Existing research has highlighted both the limitations of LLMs in interpreting vague or poorly structured prompts and the difficulties users face in crafting precise queries. This study investigates learner-AI interactions through an educational experiment in which participants receive structured guidance on effective prompting. We introduce and compare three types of prompting guidelines: a task-specific framework developed through a structured methodology and two baseline approaches. To assess user behavior and prompting efficacy, we analyze a dataset of 642 interactions from 107 users. Using Von NeuMidas, an extended pragmatic annotation schema for LLM interaction analysis, we categorize common prompting errors and identify recurring behavioral patterns. We then evaluate the impact of different guidelines by examining changes in user behavior, adherence to prompting strategies, and the overall quality of AI-generated responses. Our findings provide a deeper understanding of how users engage with LLMs and the role of structured prompting guidance in enhancing AI-assisted communication. By comparing different instructional frameworks, we offer insights into more effective approaches for improving user competency in AI interactions, with implications for AI literacy, chatbot usability, and the design of more responsive AI systems.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [34] [ScaleMCP: Dynamic and Auto-Synchronizing Model Context Protocol Tools for LLM Agents](https://arxiv.org/abs/2505.06416)

*Elias Lumer, Anmol Gulati, Vamse Kumar Subbiah, Pradeep Honaganahalli Basavaraju, James A. Burke*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Context Protocol, Tool Selection, Dynamic Retrieval, Embedding Strategy

**Relevance Score:** 9

**TL;DR:** This paper introduces ScaleMCP, a dynamic tool selection approach for LLM agents using the Model Context Protocol (MCP), enhancing tool interaction and retrieval efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the integration and efficiency of tool selection frameworks for LLM agents that currently rely on manual updates and do not utilize MCP servers, leading to issues like duplication and inefficiency.

**Method:** ScaleMCP utilizes a MCP tool retriever that allows LLM agents to dynamically add tools into their memory and an auto-synchronizing tool storage system employing CRUD operations with MCP servers.

**Key Contributions:**

	1. Introduction of ScaleMCP for dynamic tool selection
	2. Development of a tool retriever for LLM agents
	3. Novel embedding strategy (TDWA) to enhance tool document processing

**Result:** Comprehensive evaluations on a dataset of 5,000 financial metric MCP servers showed significant improvements in tool retrieval and agent invocation performance across various LLM and embedding models.

**Limitations:** 

**Conclusion:** ScaleMCP enhances the scalability and dynamism of tool selection and invocation for LLM agents, addressing existing limitations in current frameworks.

**Abstract:** Recent advancements in Large Language Models (LLMs) and the introduction of the Model Context Protocol (MCP) have significantly expanded LLM agents' capability to interact dynamically with external tools and APIs. However, existing tool selection frameworks do not integrate MCP servers, instead relying heavily on error-prone manual updates to monolithic local tool repositories, leading to duplication, inconsistencies, and inefficiencies. Additionally, current approaches abstract tool selection before the LLM agent is invoked, limiting its autonomy and hindering dynamic re-querying capabilities during multi-turn interactions. To address these issues, we introduce ScaleMCP, a novel tool selection approach that dynamically equips LLM agents with a MCP tool retriever, giving agents the autonomy to add tools into their memory, as well as an auto-synchronizing tool storage system pipeline through CRUD (create, read, update, delete) operations with MCP servers as the single source of truth. We also propose a novel embedding strategy, Tool Document Weighted Average (TDWA), designed to selectively emphasize critical components of tool documents (e.g. tool name or synthetic questions) during the embedding process. Comprehensive evaluations conducted on a created dataset of 5,000 financial metric MCP servers, across 10 LLM models, 5 embedding models, and 5 retriever types, demonstrate substantial improvements in tool retrieval and agent invocation performance, emphasizing ScaleMCP's effectiveness in scalable, dynamic tool selection and invocation.

</details>


### [35] [Is your multimodal large language model a good science tutor?](https://arxiv.org/abs/2505.06418)

*Ming Liu, Liwen Wang, Wensheng Zhang*

**Main category:** cs.CL

**Keywords:** multimodal large language models, educational evaluation, tutoring performance, ScienceQA, preference optimization

**Relevance Score:** 8

**TL;DR:** This paper proposes a framework to evaluate multimodal large language models (MLLMs) as science tutors using educational rubrics and a simulated student model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for MLLMs focus primarily on answer accuracy, neglecting their ability to effectively teach, which is critical in educational contexts.

**Method:** The authors created a comprehensive rubric to assess MLLMs' tutoring performance, utilizing simulated student judgments to generate performance scores for various MLLMs, followed by a dataset of pairwise comparisons to improve underperforming models.

**Key Contributions:**

	1. Framework for evaluating MLLMs as science tutors
	2. Identification of metrics beyond accuracy in educational contexts
	3. Method for improving underperforming tutor models using student feedback

**Result:** The study identifies that strong problem-solving abilities do not always correlate with high-quality tutoring, and that performance optimization can enhance the educational alignment of tutor models.

**Limitations:** 

**Conclusion:** The proposed framework and findings suggest that MLLMs can be refined not just for solving problems but to become supportive educational tools, enhancing their effectiveness in teaching.

**Abstract:** Multimodal large language models (MLLMs) demonstrate impressive performance on scientific reasoning tasks (e.g., ScienceQA). However, most existing benchmarks focus narrowly on the accuracy of the final answer while ignoring other metrics. In particular, when applying MLLMs to educational contexts, the goal is not only correctness but also the ability to teach. In this paper, we propose a framework that evaluates MLLMs as science tutors using a comprehensive educational rubric and a simulated student model that judges the teaching performance of the tutors. Given a list of candidate MLLM science tutors, we use rubric-based student judgments to produce a range of tutor performance scores, identifying both strong and weak tutors. Using the training section of the ScienceQA dataset, we then construct a data set of pairwise comparisons between the outputs of strong and weak tutors. This enables us to apply multiple preference optimization methods to fine-tune an underperforming tutor model (Qwen2-VL-2B) into more effective ones. Our results also show that strong problem-solving skills do not guarantee high-quality tutoring and that performance optimization-guided refinements can yield more educationally aligned tutor models. This approach opens avenues for building MLLMs that serve not only as problem solvers, but as genuinely helpful educational assistants.

</details>


### [36] [xGen-small Technical Report](https://arxiv.org/abs/2505.06496)

*Erik Nijkamp, Bo Pang, Egor Pakhomov, Akash Gokul, Jin Qu, Silvio Savarese, Yingbo Zhou, Caiming Xiong*

**Main category:** cs.CL

**Keywords:** xGen-small, Transformer, long-context, post-training, fine-tuning

**Relevance Score:** 8

**TL;DR:** Introduction of xGen-small models optimized for long-context applications in HCI and coding tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of Transformer models in handling long-context inputs across various tasks, particularly in math and coding domains.

**Method:** The model employs a vertically integrated pipeline with curated data, multi-stage pre-training, and targeted post-training techniques.

**Key Contributions:**

	1. Introduction of the xGen-small model family for long-context applications.
	2. Use of frequency-aware data curation for domain balancing.
	3. Implementation of multi-stage pre-training and reinforcement learning for targeted enhancements.

**Result:** xGen-small models demonstrate strong performance in long-context benchmarks and specific tasks like math and coding.

**Limitations:** 

**Conclusion:** The proposed models effectively improve Transformer performance for applications requiring long-context understanding.

**Abstract:** We introduce xGen-small, a family of 4B and 9B Transformer decoder models optimized for long-context applications. Our vertically integrated pipeline unites domain-balanced, frequency-aware data curation; multi-stage pre-training with quality annealing and length extension to 128k tokens; and targeted post-training via supervised fine-tuning, preference learning, and online reinforcement learning. xGen-small delivers strong performance across various tasks, especially in math and coding domains, while excelling at long context benchmarks.

</details>


### [37] [Think in Safety: Unveiling and Mitigating Safety Alignment Collapse in Multimodal Large Reasoning Model](https://arxiv.org/abs/2505.06538)

*Xinyue Lou, You Li, Jinan Xu, Xiangyu Shi, Chi Chen, Kaiyu Huang*

**Main category:** cs.CL

**Keywords:** multimodal large reasoning models, safety evaluation, thought process, dataset, benchmarks

**Relevance Score:** 7

**TL;DR:** The paper conducts a systematic safety evaluation of multimodal large reasoning models (MLRMs), highlighting safety degradation issues and proposing a multimodal tuning dataset to enhance safety during model fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses critical concerns regarding the safety and reliability of multimodal large reasoning models (MLRMs) due to their rapid advancement and broad application potential.

**Method:** A comprehensive evaluation of 11 MLRMs is conducted across 5 benchmarks, focusing on safety degradation patterns and developing a multimodal tuning dataset that includes safety-oriented thought processes.

**Key Contributions:**

	1. Systematic safety evaluation of 11 MLRMs across 5 benchmarks.
	2. Development of a multimodal tuning dataset focused on enhancing safety.
	3. Insights into safety degradation patterns in advanced models.

**Result:** Significant safety degradation is found across jailbreak robustness benchmarks, while safety-awareness benchmarks show less degradation. Fine-tuning with the proposed dataset improves safety on both benchmark types.

**Limitations:** The work is in progress and may not represent finalized findings.

**Conclusion:** The study suggests that leveraging the reasoning capabilities of MLRMs can help detect unsafe intents and improve safety, marking a new approach in developing safer MLRMs.

**Abstract:** The rapid development of multimodal large reasoning models (MLRMs) has demonstrated broad application potential, yet their safety and reliability remain critical concerns that require systematic exploration. To address this gap, we conduct a comprehensive and systematic safety evaluation of 11 MLRMs across 5 benchmarks and unveil prevalent safety degradation phenomena in most advanced models. Moreover, our analysis reveals distinct safety patterns across different benchmarks: significant safety degradation is observed across jailbreak robustness benchmarks, whereas safety-awareness benchmarks demonstrate less pronounced degradation. In particular, a long thought process in some scenarios even enhances safety performance. Therefore, it is a potential approach to addressing safety issues in MLRMs by leveraging the intrinsic reasoning capabilities of the model to detect unsafe intent. To operationalize this insight, we construct a multimodal tuning dataset that incorporates a safety-oriented thought process. Experimental results from fine-tuning existing MLRMs with this dataset effectively enhances the safety on both jailbreak robustness and safety-awareness benchmarks. This study provides a new perspective for developing safe MLRMs. Our dataset is available at https://github.com/xinyuelou/Think-in-Safety.

</details>


### [38] [REFINE-AF: A Task-Agnostic Framework to Align Language Models via Self-Generated Instructions using Reinforcement Learning from Automated Feedback](https://arxiv.org/abs/2505.06548)

*Aniruddha Roy, Pretam Ray, Abhilash Nandy, Somak Aditya, Pawan Goyal*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Instruction Generation, Fine-tuning, NLP

**Relevance Score:** 9

**TL;DR:** The paper evaluates small open-source LLMs in generating instruction datasets for fine-tuning, highlighting enhancements through a reinforcement learning framework.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Creating human-annotated instruction data for LLMs is resource-intensive and limited, prompting the exploration of automated methods.

**Method:** The study utilizes three small LLMs and a semi-automated framework to generate instruction data, complemented by a reinforcement learning training algorithm.

**Key Contributions:**

	1. Evaluation of small open-source LLMs for instruction dataset generation
	2. Introduction of a semi-automated framework to reduce human effort
	3. Improvements in task performance using RL-based training methods

**Result:** The incorporation of RL into the framework significantly improved performance, achieving better results in 63-66% of evaluated tasks.

**Limitations:** 

**Conclusion:** This approach reduces costs and effort while effectively enhancing instruction dataset generation for fine-tuning LLMs.

**Abstract:** Instruction-based Large Language Models (LLMs) have proven effective in numerous few-shot or zero-shot Natural Language Processing (NLP) tasks. However, creating human-annotated instruction data is time-consuming, expensive, and often limited in quantity and task diversity. Previous research endeavors have attempted to address this challenge by proposing frameworks capable of generating instructions in a semi-automated and task-agnostic manner directly from the model itself. Many of these efforts have relied on large API-only parameter-based models such as GPT-3.5 (175B), which are expensive, and subject to limits on a number of queries. This paper explores the performance of three open-source small LLMs such as LLaMA 2-7B, LLama 2-13B, and Mistral 7B, using a semi-automated framework, thereby reducing human intervention, effort, and cost required to generate an instruction dataset for fine-tuning LLMs. Furthermore, we demonstrate that incorporating a Reinforcement Learning (RL) based training algorithm into this LLMs-based framework leads to further enhancements. Our evaluation of the dataset reveals that these RL-based frameworks achieve a substantial improvements in 63-66% of the tasks compared to previous approaches.

</details>


### [39] [References Indeed Matter? Reference-Free Preference Optimization for Conversational Query Reformulation](https://arxiv.org/abs/2505.06552)

*Doyoung Kim, Youngjun Lee, Joeun Kim, Jihwan Bang, Hwanjun Song, Susik Yoon, Jae-Gil Lee*

**Main category:** cs.CL

**Keywords:** Conversational Query Reformulation, Reference-free Optimization, Dialogue-based Applications

**Relevance Score:** 8

**TL;DR:** DualReform improves conversational query reformulation without requiring reference passages by generating pseudo references from dialogues, achieving high retrieval accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional approaches to conversational query reformulation rely on reference passages, which are difficult to obtain in real-world scenarios.

**Method:** DualReform creates pseudo reference passages from conversational datasets by inferring from responses and refining them through a dual-role process in CQR.

**Key Contributions:**

	1. Introduces a reference-free preference optimization framework for CQR.
	2. Utilizes response-based inference to create pseudo reference passages.
	3. Implements response refinement that improves both responses and query reformulation.

**Result:** DualReform achieves 96.9--99.1% of the retrieval accuracy of methods using reference passages and outperforms the state-of-the-art by up to 31.6%.

**Limitations:** 

**Conclusion:** The framework offers a practical solution for enhancing retrieval accuracy in dialogue-based applications without the need for actual reference passages.

**Abstract:** Conversational query reformulation (CQR) has become indispensable for improving retrieval in dialogue-based applications. However, existing approaches typically rely on reference passages for optimization, which are impractical to acquire in real-world scenarios. To address this limitation, we introduce a novel reference-free preference optimization framework DualReform that generates pseudo reference passages from commonly-encountered conversational datasets containing only queries and responses. DualReform attains this goal through two key innovations: (1) response-based inference, where responses serve as proxies to infer pseudo reference passages, and (2) response refinement via the dual-role of CQR, where a CQR model refines responses based on the shared objectives between response refinement and CQR. Despite not relying on reference passages, DualReform achieves 96.9--99.1% of the retrieval accuracy attainable only with reference passages and surpasses the state-of-the-art method by up to 31.6%.

</details>


### [40] [MacRAG: Compress, Slice, and Scale-up for Multi-Scale Adaptive Context RAG](https://arxiv.org/abs/2505.06569)

*Woosang Lim, Zekun Li, Gyuwan Kim, Sungyoung Ji, HyeonJung Kim, Kyuri Choi, Jin Hyuk Lim, Kyungpyo Park, William Yang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Multi-hop reasoning

**Relevance Score:** 9

**TL;DR:** MacRAG is a hierarchical retrieval framework that enhances RAG systems for long-context tasks by adaptively merging relevant contexts to improve precision and coverage.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues in existing RAG systems, such as imprecise retrieval and incomplete context coverage, particularly in long-context multi-hop tasks.

**Method:** MacRAG compresses and partitions documents into granularities and merges relevant contexts through chunk- and document-level expansions in real-time, starting from fine-level retrieval.

**Key Contributions:**

	1. Introduction of a hierarchical retrieval framework for RAG
	2. Real-time adaptive merging of context
	3. Improved performance on multi-hop reasoning tasks

**Result:** MacRAG outperforms baseline RAG pipelines on multi-hop generation tasks in evaluations conducted on LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique using models like Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o.

**Limitations:** 

**Conclusion:** MacRAG is confirmed as an efficient and scalable framework for real-world applications requiring long-context multi-hop reasoning, with available code for implementation.

**Abstract:** Long-context (LC) Large Language Models (LLMs) combined with Retrieval-Augmented Generation (RAG) hold strong potential for complex multi-hop and large-document tasks. However, existing RAG systems often suffer from imprecise retrieval, incomplete context coverage under constrained context windows, and fragmented information caused by suboptimal context construction. We introduce Multi-scale Adaptive Context RAG (MacRAG), a hierarchical retrieval framework that compresses and partitions documents into coarse-to-fine granularities, then adaptively merges relevant contexts through chunk- and document-level expansions in real time. By starting from the finest-level retrieval and progressively incorporating higher-level and broader context, MacRAG constructs effective query-specific long contexts, optimizing both precision and coverage. Evaluations on the challenging LongBench expansions of HotpotQA, 2WikiMultihopQA, and Musique confirm that MacRAG consistently surpasses baseline RAG pipelines on single- and multi-step generation with Llama-3.1-8B, Gemini-1.5-pro, and GPT-4o. Our results establish MacRAG as an efficient, scalable solution for real-world long-context, multi-hop reasoning. Our code is available at https://github.com/Leezekun/MacRAG.

</details>


### [41] [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)

*Anna WrÃ³blewska, Bartosz Grabek, Jakub Åwistak, Daniel Dan*

**Main category:** cs.CL

**Keywords:** AI chatbots, Q&A tests, psychometric analysis, Natural Language Processing, assessment development

**Relevance Score:** 8

**TL;DR:** The research presents an automatic pipeline for generating reliable AI-based question-answer tests, demonstrating high psychometric performance and user satisfaction compared to human-authored tests.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an effective method for generating assessments using AI chatbots, particularly focusing on the application within educational settings.

**Method:** An automatic pipeline was created using GPT-4o-mini to generate Q&A tests for a Natural Language Processing course, which were then evaluated through psychometric analysis and student/expert ratings.

**Key Contributions:**

	1. Development of an automatic pipeline for AI-generated assessments.
	2. Demonstration of strong psychometric performance of generated tests.
	3. Validation of user satisfaction with AI-generated questions.

**Result:** The generated Q&A items showed strong discrimination and appropriate difficulty levels, with high satisfaction ratings from students and experts. Two items require review based on uniform DIF checks.

**Limitations:** Only two items were identified for potential review; however, no detailed limitations were specified regarding the generalizability of findings.

**Conclusion:** LLM-generated assessments can achieve comparable psychometric quality to traditional human-authored tests, suggesting a valuable tool for scalable AI-assisted assessment development.

**Abstract:** This research prepares an automatic pipeline for generating reliable question-answer (Q&A) tests using AI chatbots. We automatically generated a GPT-4o-mini-based Q&A test for a Natural Language Processing course and evaluated its psychometric and perceived-quality metrics with students and experts. A mixed-format IRT analysis showed that the generated items exhibit strong discrimination and appropriate difficulty, while student and expert star ratings reflect high overall quality. A uniform DIF check identified two items for review. These findings demonstrate that LLM-generated assessments can match human-authored tests in psychometric performance and user satisfaction, illustrating a scalable approach to AI-assisted assessment development.

</details>


### [42] [Integrating Video and Text: A Balanced Approach to Multimodal Summary Generation and Evaluation](https://arxiv.org/abs/2505.06594)

*Galann Pennec, Zhengyuan Liu, Nicholas Asher, Philippe Muller, Nancy F. Chen*

**Main category:** cs.CL

**Keywords:** Video-to-text summarization, Zero-shot learning, Vision-language models, Screenplay representation, Multimodal evaluation

**Relevance Score:** 7

**TL;DR:** This paper presents a zero-shot video-to-text summarization approach that generates screenplay representations of TV episodes by integrating key visual and textual information.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Vision-Language Models (VLMs) often fail in summarizing complex multimodal inputs effectively. This work aims to improve video-to-text summarization by generating comprehensive screenplay representations.

**Method:** The proposed method creates screenplay representations that incorporate crucial video moments, dialogue, and character details without needing prior examples. The evaluation of summaries utilizes MFactSum, a metric designed to assess multimodal content.

**Key Contributions:**

	1. Introduces a zero-shot video-to-text summarization method generating screenplay representations.
	2. Develops MFactSum, a new metric for evaluating multimodal summaries that account for both vision and text.
	3. Demonstrates improved performance over state-of-the-art VLMs in generating relevant visual information.

**Result:** The approach outperforms existing VLMs like Gemini 1.5, yielding summaries with 20% more relevant visual information while reducing the video input requirement by 75%.

**Limitations:** The approach may still struggle with certain complex scenes or dialogues that require deeper contextual understanding.

**Conclusion:** The proposed zero-shot summarization method and MFactSum metric provide significant advancements in evaluating and generating multimodal summaries.

**Abstract:** Vision-Language Models (VLMs) often struggle to balance visual and textual information when summarizing complex multimodal inputs, such as entire TV show episodes. In this paper, we propose a zero-shot video-to-text summarization approach that builds its own screenplay representation of an episode, effectively integrating key video moments, dialogue, and character information into a unified document. Unlike previous approaches, we simultaneously generate screenplays and name the characters in zero-shot, using only the audio, video, and transcripts as input. Additionally, we highlight that existing summarization metrics can fail to assess the multimodal content in summaries. To address this, we introduce MFactSum, a multimodal metric that evaluates summaries with respect to both vision and text modalities. Using MFactSum, we evaluate our screenplay summaries on the SummScreen3D dataset, demonstrating superiority against state-of-the-art VLMs such as Gemini 1.5 by generating summaries containing 20% more relevant visual information while requiring 75% less of the video as input.

</details>


### [43] [Bridging the Gap: An Intermediate Language for Enhanced and Cost-Effective Grapheme-to-Phoneme Conversion with Homographs with Multiple Pronunciations Disambiguation](https://arxiv.org/abs/2505.06599)

*Abbas Bertina, Shahab Beirami, Hossein Biniazian, Elham Esmaeilnia, Soheil Shahi, Mahdi Pirnia*

**Main category:** cs.CL

**Keywords:** Grapheme-to-phoneme, Persian, Large Language Models, Machine Transliteration, Phoneme Error Rate

**Relevance Score:** 7

**TL;DR:** This paper presents a novel approach to Grapheme-to-Phoneme (G2P) conversion for Persian, addressing its phonological challenges using a specialized intermediate language, LLM techniques, and a machine transliteration architecture.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate Grapheme-to-Phoneme conversion in Persian due to its complex phonological features, such as homographs and the Ezafe construction, in both formal and informal contexts.

**Method:** The methodology involves using Large Language Models for prompting and a sequence-to-sequence machine transliteration architecture, along with a comprehensive lexical database construction for polyphones based on formal concept analysis.

**Key Contributions:**

	1. Introduction of an intermediate language for Persian language processing
	2. Utilization of LLM prompting techniques in G2P conversion
	3. Development of a robust lexical database for homographs and polyphones.

**Result:** The experimental results show that the proposed model outperforms existing state-of-the-art approaches, particularly in improving the Phoneme Error Rate (PER) for Persian G2P conversion.

**Limitations:** 

**Conclusion:** This approach establishes a new benchmark for G2P conversion accuracy in Persian and has potential applications in other languages with similar phonological challenges, like Chinese and Arabic.

**Abstract:** Grapheme-to-phoneme (G2P) conversion for Persian presents unique challenges due to its complex phonological features, particularly homographs and Ezafe, which exist in formal and informal language contexts. This paper introduces an intermediate language specifically designed for Persian language processing that addresses these challenges through a multi-faceted approach. Our methodology combines two key components: Large Language Model (LLM) prompting techniques and a specialized sequence-to-sequence machine transliteration architecture. We developed and implemented a systematic approach for constructing a comprehensive lexical database for homographs with multiple pronunciations disambiguation often termed polyphones, utilizing formal concept analysis for semantic differentiation. We train our model using two distinct datasets: the LLM-generated dataset for formal and informal Persian and the B-Plus podcasts for informal language variants. The experimental results demonstrate superior performance compared to existing state-of-the-art approaches, particularly in handling the complexities of Persian phoneme conversion. Our model significantly improves Phoneme Error Rate (PER) metrics, establishing a new benchmark for Persian G2P conversion accuracy. This work contributes to the growing research in low-resource language processing and provides a robust solution for Persian text-to-speech systems and demonstrating its applicability beyond Persian. Specifically, the approach can extend to languages with rich homographic phenomena such as Chinese and Arabic

</details>


### [44] [Using External knowledge to Enhanced PLM for Semantic Matching](https://arxiv.org/abs/2505.06605)

*Min Li, Chun Yuan*

**Main category:** cs.CL

**Keywords:** semantic relevance, neural networks, external knowledge, NLP, machine learning

**Relevance Score:** 7

**TL;DR:** This paper enhances a semantic relevance discrimination model by incorporating external knowledge, leading to better performance on multiple datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current models in semantic relevance detection, particularly the reliance on large annotated datasets without external knowledge.

**Method:** The paper proposes a method to integrate external knowledge into pre-trained semantic relevance models.

**Key Contributions:**

	1. Integration of external knowledge into semantic relevance models
	2. Demonstration of improved performance on various datasets
	3. Insights into the necessity of external knowledge in model training

**Result:** Experimental results demonstrate consistent performance improvements on 10 public datasets compared to baseline models.

**Limitations:** 

**Conclusion:** Incorporating external knowledge into neural network-based models significantly enhances their ability to perform semantic relevance detection tasks.

**Abstract:** Modeling semantic relevance has always been a challenging and critical task in natural language processing. In recent years, with the emergence of massive amounts of annotated data, it has become feasible to train complex models, such as neural network-based reasoning models. These models have shown excellent performance in practical applications and have achieved the current state-ofthe-art performance. However, even with such large-scale annotated data, we still need to think: Can machines learn all the knowledge necessary to perform semantic relevance detection tasks based on this data alone? If not, how can neural network-based models incorporate external knowledge into themselves, and how can relevance detection models be constructed to make full use of external knowledge? In this paper, we use external knowledge to enhance the pre-trained semantic relevance discrimination model. Experimental results on 10 public datasets show that our method achieves consistent improvements in performance compared to the baseline model.

</details>


### [45] [Boosting Neural Language Inference via Cascaded Interactive Reasoning](https://arxiv.org/abs/2505.06607)

*Min Li, Chun Yuan*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Cascaded Interactive Reasoning Network, Pre-trained Language Models, semantic complexity, hierarchical feature extraction

**Relevance Score:** 8

**TL;DR:** CIRN is a new model for Natural Language Inference that enhances semantic understanding by utilizing intermediate layer outputs for deeper reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve the modeling of intricate semantic interactions in Natural Language Inference (NLI) by leveraging information from intermediate layers of Pre-trained Language Models.

**Method:** CIRN uses a hierarchical feature extraction strategy that integrates cross-sentence information across multiple layers to progressively uncover logical and semantic relationships between premises and hypotheses.

**Key Contributions:**

	1. Introduction of the Cascaded Interactive Reasoning Network (CIRN) architecture.
	2. Hierarchical feature extraction strategy for better semantic comprehension.
	3. Improved performance on standard NLI benchmark datasets.

**Result:** CIRN consistently demonstrates performance gains over baseline methods in various NLI benchmark datasets, indicating its effectiveness in relational reasoning.

**Limitations:** 

**Conclusion:** By utilizing multi-level interactive features, CIRN enhances the understanding of input pairs in NLI more comprehensively than prior models.

**Abstract:** Natural Language Inference (NLI) focuses on ascertaining the logical relationship (entailment, contradiction, or neutral) between a given premise and hypothesis. This task presents significant challenges due to inherent linguistic features such as diverse phrasing, semantic complexity, and contextual nuances. While Pre-trained Language Models (PLMs) built upon the Transformer architecture have yielded substantial advancements in NLI, prevailing methods predominantly utilize representations from the terminal layer. This reliance on final-layer outputs may overlook valuable information encoded in intermediate layers, potentially limiting the capacity to model intricate semantic interactions effectively. Addressing this gap, we introduce the Cascaded Interactive Reasoning Network (CIRN), a novel architecture designed for deeper semantic comprehension in NLI. CIRN implements a hierarchical feature extraction strategy across multiple network depths, operating within an interactive space where cross-sentence information is continuously integrated. This mechanism aims to mimic a process of progressive reasoning, transitioning from surface-level feature matching to uncovering more profound logical and semantic connections between the premise and hypothesis. By systematically mining latent semantic relationships at various representational levels, CIRN facilitates a more thorough understanding of the input pair. Comprehensive evaluations conducted on several standard NLI benchmark datasets reveal consistent performance gains achieved by CIRN over competitive baseline approaches, demonstrating the efficacy of leveraging multi-level interactive features for complex relational reasoning.

</details>


### [46] [The Efficiency of Pre-training with Objective Masking in Pseudo Labeling for Semi-Supervised Text Classification](https://arxiv.org/abs/2505.06624)

*Arezoo Hatefi, Xuan-Son Vu, Monowar Bhuyan, Frank Drewes*

**Main category:** cs.CL

**Keywords:** semi-supervised learning, text classification, teacher-student architecture, uncertainty masking, meta pseudo labels

**Relevance Score:** 6

**TL;DR:** This paper extends a semi-supervised text classification model by incorporating an unsupervised pre-training phase, enhancing performance through a teacher-student architecture.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of text classification tasks where only a small number of labeled examples are available, which is a common scenario in many practical applications.

**Method:** The model utilizes a teacher-student architecture inspired by Meta Pseudo Labels, where a teacher generates labels for unlabeled data to train the student, with iterative updates based on the student's performance on labeled data. An unsupervised pre-training phase using objective masking is introduced to extend the original model.

**Key Contributions:**

	1. Introduction of an unsupervised pre-training phase to the existing semi-supervised model.
	2. Comprehensive performance evaluations on multiple datasets in different languages.
	3. Demonstration of improved classification results compared to baseline methods.

**Result:** The extended model outperforms the baseline models and the original model in classification tasks across multiple datasets and languages, demonstrating improved labeling efficiency and effectiveness in a semi-supervised context.

**Limitations:** The model's performance may still be influenced by the quality and representativeness of the few labeled examples provided.

**Conclusion:** The enhancements to the original model significantly boost its classification accuracy, especially in scenarios with limited labeled data, indicating its applicability in real-world classification challenges.

**Abstract:** We extend and study a semi-supervised model for text classification proposed earlier by Hatefi et al. for classification tasks in which document classes are described by a small number of gold-labeled examples, while the majority of training examples is unlabeled. The model leverages the teacher-student architecture of Meta Pseudo Labels in which a ''teacher'' generates labels for originally unlabeled training data to train the ''student'' and updates its own model iteratively based on the performance of the student on the gold-labeled portion of the data. We extend the original model of Hatefi et al. by an unsupervised pre-training phase based on objective masking, and conduct in-depth performance evaluations of the original model, our extension, and various independent baselines. Experiments are performed using three different datasets in two different languages (English and Swedish).

</details>


### [47] [Dynamic Domain Information Modulation Algorithm for Multi-domain Sentiment Analysis](https://arxiv.org/abs/2505.06630)

*Chunyi Yue, Ang Li*

**Main category:** cs.CL

**Keywords:** multi-domain sentiment classification, machine learning, hyperparameter optimization

**Relevance Score:** 6

**TL;DR:** Proposes a dynamic information modulation algorithm for multi-domain sentiment classification that addresses hyperparameter optimization challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sentiment classification performance across multiple domains while managing the complexities of hyperparameter optimization as the number of domains increases.

**Method:** The proposed approach consists of two stages: determining a shared hyperparameter for domain classification tasks and employing a novel domain-aware modulation algorithm to adjust domain information based on gradient and loss methods.

**Key Contributions:**

	1. Introduction of a dynamic information modulation algorithm for multi-domain sentiment classification
	2. Two-stage model training process for hyperparameter adjustment
	3. Evidence of improved sentiment classification performance on a diverse dataset.

**Result:** Experiments on a public dataset with 16 domains demonstrated the proposed method's effectiveness in improving sentiment classification accuracy compared to existing models.

**Limitations:** 

**Conclusion:** The dynamic information modulation algorithm significantly enhances sentiment classification performance in multi-domain settings by optimizing the use of domain information.

**Abstract:** Multi-domain sentiment classification aims to mitigate poor performance models due to the scarcity of labeled data in a single domain, by utilizing data labeled from various domains. A series of models that jointly train domain classifiers and sentiment classifiers have demonstrated their advantages, because domain classification helps generate necessary information for sentiment classification. Intuitively, the importance of sentiment classification tasks is the same in all domains for multi-domain sentiment classification; but domain classification tasks are different because the impact of domain information on sentiment classification varies across different fields; this can be controlled through adjustable weights or hyper parameters. However, as the number of domains increases, existing hyperparameter optimization algorithms may face the following challenges: (1) tremendous demand for computing resources, (2) convergence problems, and (3) high algorithm complexity. To efficiently generate the domain information required for sentiment classification in each domain, we propose a dynamic information modulation algorithm. Specifically, the model training process is divided into two stages. In the first stage, a shared hyperparameter, which would control the proportion of domain classification tasks across all fields, is determined. In the second stage, we introduce a novel domain-aware modulation algorithm to adjust the domain information contained in the input text, which is then calculated based on a gradient-based and loss-based method. In summary, experimental results on a public sentiment analysis dataset containing 16 domains prove the superiority of the proposed method.

</details>


### [48] [Attention Is Not All You Need: The Importance of Feedforward Networks in Transformer Models](https://arxiv.org/abs/2505.06633)

*Isaac Gerber*

**Main category:** cs.CL

**Keywords:** Transformer Models, Feedforward Network, Language Modeling, Machine Learning, Model Efficiency

**Relevance Score:** 8

**TL;DR:** This paper investigates the significance of the feedforward network (FFN) in decoder-only transformer models, demonstrating that three-layer FFNs enhance model performance over the traditional two-layer configuration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the role of the feedforward network (FFN) in the performance of decoder-only transformer models during pre-training.

**Method:** A series of experiments were conducted to compare the effectiveness of different FFN configurations within transformer blocks, specifically evaluating two-layer versus three-layer FFNs.

**Key Contributions:**

	1. Demonstration of the FFN's importance in transformer model performance
	2. Comparison of two-layer and three-layer FFN configurations
	3. Evidence that fewer transformer blocks with three-layer FFNs outperform larger models with two-layer FFNs

**Result:** Models with three-layer FFNs showed lower training loss and comparable performance with fewer total parameters and less training time than standard two-layer configurations.

**Limitations:** The paper does not explore other architectural components outside of the transformer blocks, potentially overlooking the impact of other factors affecting performance.

**Conclusion:** The findings reinforce the importance of the FFN in transformer architectures and suggest that optimizing its structure can enhance model efficiency and effectiveness.

**Abstract:** Decoder-only transformer networks have become incredibly popular for language modeling tasks. State-of-the-art models can have over a hundred transformer blocks, containing billions of trainable parameters, and are trained on trillions of tokens of text. Each transformer block typically consists of a multi-head attention (MHA) mechanism and a two-layer fully connected feedforward network (FFN). In this paper, we examine the importance of the FFN during the model pre-training process through a series of experiments, confirming that the FFN is important to model performance. Furthermore, we show that models using a transformer block configuration with three-layer FFNs with fewer such blocks outperform the standard two-layer configuration delivering lower training loss with fewer total parameters in less time.

</details>


### [49] [TS-SUPERB: A Target Speech Processing Benchmark for Speech Self-Supervised Learning Models](https://arxiv.org/abs/2505.06660)

*Junyi Peng, Takanori Ashihara, Marc Delcroix, Tsubasa Ochiai, Oldrich Plchot, Shoko Araki, Jan ÄernockÃ½*

**Main category:** cs.CL

**Keywords:** self-supervised learning, speech processing, target speaker, benchmark, multi-talker

**Relevance Score:** 6

**TL;DR:** Introduction of a benchmark for evaluating self-supervised learning models in target-speaker speech processing tasks under noisy, multi-talker conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of exploration in target-speaker tasks under challenging conditions in previous benchmarks focused on single-speaker scenarios.

**Method:** A benchmark called TS-SUPERB is introduced, which includes four tasks that require target speaker identification and information extraction, using speaker embeddings as conditional clues for downstream models.

**Key Contributions:**

	1. Introduction of the TS-SUPERB benchmark for target-speaker processing tasks.
	2. Focus on evaluating SSL models in multi-talker noisy conditions.
	3. Demonstration of joint optimization benefits across TS tasks.

**Result:** The benchmark results highlight the necessity of evaluating SSL models in target speaker scenarios, showing that performance metrics are not directly transferable from single-speaker tasks.

**Limitations:** 

**Conclusion:** The research emphasizes the effectiveness of a unified SSL-based target speech encoder, supporting joint optimization across different tasks to utilize mutual information.

**Abstract:** Self-supervised learning (SSL) models have significantly advanced speech processing tasks, and several benchmarks have been proposed to validate their effectiveness. However, previous benchmarks have primarily focused on single-speaker scenarios, with less exploration of target-speaker tasks in noisy, multi-talker conditions -- a more challenging yet practical case. In this paper, we introduce the Target-Speaker Speech Processing Universal Performance Benchmark (TS-SUPERB), which includes four widely recognized target-speaker processing tasks that require identifying the target speaker and extracting information from the speech mixture. In our benchmark, the speaker embedding extracted from enrollment speech is used as a clue to condition downstream models. The benchmark result reveals the importance of evaluating SSL models in target speaker scenarios, demonstrating that performance cannot be easily inferred from related single-speaker tasks. Moreover, by using a unified SSL-based target speech encoder, consisting of a speaker encoder and an extractor module, we also investigate joint optimization across TS tasks to leverage mutual information and demonstrate its effectiveness.

</details>


### [50] [Enhancing BERTopic with Intermediate Layer Representations](https://arxiv.org/abs/2505.06696)

*Dominik Koterwa, Maciej ÅwitaÅa*

**Main category:** cs.CL

**Keywords:** BERTopic, topic modeling, embedding representations, natural language processing, text data analysis

**Relevance Score:** 6

**TL;DR:** This study evaluates 18 different embedding representations for the BERTopic algorithm, aiming to optimize its performance on topic modeling tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the BERTopic algorithm's efficacy by identifying optimal embedding representations for better topic structure estimation and insight extraction from large text corpora.

**Method:** Evaluated 18 embedding representations through experiments on three diverse datasets, measuring topic coherence and topic diversity metrics.

**Key Contributions:**

	1. Evaluation of 18 different embedding representations for BERTopic
	2. Identification of optimal embedding configurations for improved performance
	3. Insights on the influence of stop words in topic modeling

**Result:** Found that certain embedding configurations outperform BERTopic's default settings for each dataset, highlighting the importance of embedding choice.

**Limitations:** 

**Conclusion:** Optimizing embedding configurations can significantly improve the performance of BERTopic, demonstrating the influence of stop words on various embeddings.

**Abstract:** BERTopic is a topic modeling algorithm that leverages transformer-based embeddings to create dense clusters, enabling the estimation of topic structures and the extraction of valuable insights from a corpus of documents. This approach allows users to efficiently process large-scale text data and gain meaningful insights into its structure. While BERTopic is a powerful tool, embedding preparation can vary, including extracting representations from intermediate model layers and applying transformations to these embeddings. In this study, we evaluate 18 different embedding representations and present findings based on experiments conducted on three diverse datasets. To assess the algorithm's performance, we report topic coherence and topic diversity metrics across all experiments. Our results demonstrate that, for each dataset, it is possible to find an embedding configuration that performs better than the default setting of BERTopic. Additionally, we investigate the influence of stop words on different embedding configurations.

</details>


### [51] [From Rankings to Insights: Evaluation Should Shift Focus from Leaderboard to Feedback](https://arxiv.org/abs/2505.06698)

*Zongqi Wang, Tianle Gu, Chen Gong, Xin Tian, Siqi Bao, Yujiu Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, model evaluation, feedback framework, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper introduces Feedbacker, a framework for evaluating Large Language Models (LLMs) that provides detailed feedback on their strengths and weaknesses, moving beyond traditional human-based model rankings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for LLM evaluation are limited by providing only overall scores, which do not aid in model optimization. This paper aims to shift the evaluation paradigm towards providing actionable feedback.

**Method:** The Feedbacker framework includes an extensible query taxonomy builder, an automated query synthesis scheme, and analysis tools. It also introduces a novel LLM-as-a-Judge method called PC2 for efficient evaluation.

**Key Contributions:**

	1. Introduction of Feedbacker framework for LLM evaluation
	2. Development of PC2 method for efficient evaluation
	3. Provision of detailed feedback for model optimization and profiling

**Result:** Feedbacker was demonstrated using 17 mainstream LLMs, showcasing its ability to provide comprehensive feedback that aids in model understanding and optimization.

**Limitations:** 

**Conclusion:** The Feedbacker framework represents a significant advancement in LLM evaluation by focusing on analytical feedback rather than merely approximating human rankings.

**Abstract:** Automatic evaluation benchmarks such as MT-Bench, Arena-Hard, and Auto-Arena are seeing growing adoption for the evaluation of Large Language Models (LLMs). Existing research has primarily focused on approximating human-based model rankings using limited data and LLM-as-a-Judge. However, the fundamental premise of these studies, which attempts to replicate human rankings, is flawed. Specifically, these benchmarks typically offer only overall scores, limiting their utility to leaderboard rankings, rather than providing feedback that can guide model optimization and support model profiling. Therefore, we advocate for an evaluation paradigm shift from approximating human-based model rankings to providing feedback with analytical value. To this end, we introduce Feedbacker, an evaluation framework that provides comprehensive and fine-grained results, thereby enabling thorough identification of a model's specific strengths and weaknesses. Such feedback not only supports the targeted optimization of the model but also enhances the understanding of its behavior. Feedbacker comprises three key components: an extensible tree-based query taxonomy builder, an automated query synthesis scheme, and a suite of visualization and analysis tools. Furthermore, we propose a novel LLM-as-a-Judge method: PC2 (Pre-Comparison-derived Criteria) pointwise evaluation. This method derives evaluation criteria by pre-comparing the differences between several auxiliary responses, achieving the accuracy of pairwise evaluation while maintaining the time complexity of pointwise evaluation. Finally, leveraging the evaluation results of 17 mainstream LLMs, we demonstrate the usage of Feedbacker and highlight its effectiveness and potential. Our homepage project is available at https://liudan193.github.io/Feedbacker.

</details>


### [52] [Gated Attention for Large Language Models: Non-linearity, Sparsity, and Attention-Sink-Free](https://arxiv.org/abs/2505.06708)

*Zihan Qiu, Zekun Wang, Bo Zheng, Zeyu Huang, Kaiyue Wen, Songlin Yang, Rui Men, Le Yu, Fei Huang, Suozhi Huang, Dayiheng Liu, Jingren Zhou, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Gating mechanisms, Softmax attention, Mixture-of-Experts, Model performance, Long-context extrapolation

**Relevance Score:** 7

**TL;DR:** This paper investigates the effects of gating mechanisms on softmax attention, revealing that head-specific sigmoid gating improves performance and training stability in Mixture-of-Experts models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically analyze the specific effects of gating mechanisms on attention models, particularly softmax attention variants.

**Method:** Conducted experiments comparing over 30 variants of Mixture-of-Experts models and dense models trained on a large dataset, focusing on various gating positions and computational variants.

**Key Contributions:**

	1. Introduced a simple mutation of applying head-specific sigmoid gates in softmax attention.
	2. Demonstrated improved performance and training stability across large models.
	3. Released codes and models for further research into gated attention.

**Result:** Head-specific sigmoid gating after Scaled Dot-Product Attention consistently improved performance, stability, and scaling properties across models.

**Limitations:** 

**Conclusion:** The introduction of non-linearity and query-dependent sparse gating scores significantly enhance attention performance and long-context extrapolation capabilities.

**Abstract:** Gating mechanisms have been widely utilized, from early models like LSTMs and Highway Networks to recent state space models, linear attention, and also softmax attention. Yet, existing literature rarely examines the specific effects of gating. In this work, we conduct comprehensive experiments to systematically investigate gating-augmented softmax attention variants. Specifically, we perform a comprehensive comparison over 30 variants of 15B Mixture-of-Experts (MoE) models and 1.7B dense models trained on a 3.5 trillion token dataset. Our central finding is that a simple modification-applying a head-specific sigmoid gate after the Scaled Dot-Product Attention (SDPA)-consistently improves performance. This modification also enhances training stability, tolerates larger learning rates, and improves scaling properties. By comparing various gating positions and computational variants, we attribute this effectiveness to two key factors: (1) introducing non-linearity upon the low-rank mapping in the softmax attention, and (2) applying query-dependent sparse gating scores to modulate the SDPA output. Notably, we find this sparse gating mechanism mitigates 'attention sink' and enhances long-context extrapolation performance, and we also release related $\href{https://github.com/qiuzh20/gated_attention}{codes}$ and $\href{https://huggingface.co/QwQZh/gated_attention}{models}$ to facilitate future research.

</details>


### [53] [Utilizing LLMs to Investigate the Disputed Role of Evidence in Electronic Cigarette Health Policy Formation in Australia and the UK](https://arxiv.org/abs/2505.06782)

*Damian Curran, Brian Chapman, Mike Conway*

**Main category:** cs.CL

**Keywords:** Electronic Cigarettes, Health Policy, Large Language Models, Machine Learning, Health Informatics

**Relevance Score:** 7

**TL;DR:** This paper analyzes the contrasting approaches of Australia and the UK in regulating electronic cigarettes using an LLM-based sentence classifier to evaluate policy documents for claims about public health effects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate differences in how Australia and the UK manage and present evidence regarding electronic cigarette regulations.

**Method:** Developed a Large Language Model-based sentence classifier using GPT-4 to analyze 109 legislative documents from Australia and the UK, classifying sentences as helpful or harmful for public health.

**Key Contributions:**

	1. Development of an LLM-based classifier for analyzing policy documents
	2. Insights into the contrasting regulatory frameworks of Australia and the UK
	3. Establishment of methodology for investigating health policy formation using AI

**Result:** The classifier achieved an F-score of 0.9, revealing that Australian documents contain a higher proportion of harmful statements compared to UK documents, which emphasize helpful statements.

**Limitations:** 

**Conclusion:** The study demonstrates that LLM-based approaches can effectively analyze health policy documents, highlighting divergent national narratives regarding electronic cigarettes despite a common evidence base.

**Abstract:** Australia and the UK have developed contrasting approaches to the regulation of electronic cigarettes, with - broadly speaking - Australia adopting a relatively restrictive approach and the UK adopting a more permissive approach. Notably, these divergent policies were developed from the same broad evidence base. In this paper, to investigate differences in how the two jurisdictions manage and present evidence, we developed and evaluated a Large Language Model-based sentence classifier to perform automated analyses of electronic cigarette-related policy documents drawn from official Australian and UK legislative processes (109 documents in total). Specifically, we utilized GPT-4 to automatically classify sentences based on whether they contained claims that e-cigarettes were broadly helpful or harmful for public health. Our LLM-based classifier achieved an F-score of 0.9. Further, when applying the classifier to our entire sentence-level corpus, we found that Australian legislative documents show a much higher proportion of harmful statements, and a lower proportion of helpful statements compared to the expected values, with the opposite holding for the UK. In conclusion, this work utilized an LLM-based approach to provide evidence to support the contention that - drawing on the same evidence base - Australian ENDS-related policy documents emphasize the harms associated with ENDS products and UK policy documents emphasize the benefits. Further, our approach provides a starting point for using LLM-based methods to investigate the complex relationship between evidence and health policy formation.

</details>


### [54] [A Split-then-Join Approach to Abstractive Summarization for Very Long Documents in a Low Resource Setting](https://arxiv.org/abs/2505.06862)

*Lhuqita Fazry*

**Main category:** cs.CL

**Keywords:** BIGBIRD-PEGASUS, text summarization, long documents, data augmentation, transfer learning

**Relevance Score:** 7

**TL;DR:** This research enhances the BIGBIRD-PEGASUS model for summarizing very long documents by fine-tuning it on domain-specific datasets and using novel data augmentation techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of the BIGBIRD-PEGASUS model in summarizing very long documents (over 20,000 tokens) and to improve performance without resorting to truncation.

**Method:** The approach involves fine-tuning the pretrained BIGBIRD-PEGASUS model on a specialized dataset with very long documents trained by augmenting the dataset through splitting document-summary pairs.

**Key Contributions:**

	1. Proposes a method to fine-tune the BIGBIRD-PEGASUS model for very long documents
	2. Introduces data augmentation techniques to fit the model's token capacity
	3. Focuses on domain-specific datasets for better transfer learning outcomes

**Result:** The fine-tuned model demonstrates improved performance in summarizing very long documents compared to the original model, which had capacity limitations.

**Limitations:** The approach may still be limited by the underlying architecture of the BIGBIRD-PEGASUS model and the quality of the domain-specific datasets used.

**Conclusion:** This study shows that fine-tuning and data augmentation can effectively address the summarization challenges posed by very long documents.

**Abstract:** $\texttt{BIGBIRD-PEGASUS}$ model achieves $\textit{state-of-the-art}$ on abstractive text summarization for long documents. However it's capacity still limited to maximum of $4,096$ tokens, thus caused performance degradation on summarization for very long documents. Common method to deal with the issue is to truncate the documents. In this reasearch, we'll use different approach. We'll use the pretrained $\texttt{BIGBIRD-PEGASUS}$ model by fine tuned the model on other domain dataset. First, we filter out all documents which length less than $20,000$ tokens to focus on very long documents. To prevent domain shifting problem and overfitting on transfer learning due to small dataset, we augment the dataset by splitting document-summary training pair into parts, to fit the document into $4,096$ tokens. Source code available on $\href{https://github.com/lhfazry/SPIN-summ}{https://github.com/lhfazry/SPIN-summ}$.

</details>


### [55] [IM-BERT: Enhancing Robustness of BERT through the Implicit Euler Method](https://arxiv.org/abs/2505.06889)

*Mihyeon Kim, Juhyoung Park, Youngbin Kim*

**Main category:** cs.CL

**Keywords:** Pre-trained Language Models, Adversarial Attacks, BERT

**Relevance Score:** 8

**TL;DR:** IM-BERT enhances the robustness of pre-trained language models against adversarial attacks by conceptualizing BERT layers as solutions to ordinary differential equations, improving performance on the AdvGLUE dataset without adding parameters.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Pre-trained Language Models (PLMs) are vulnerable to adversarial attacks and overfitting on limited datasets, necessitating improved robustness.

**Method:** IM-BERT treats a layer of BERT as a solution of Ordinary Differential Equations (ODEs) and analyzes the stability of numerical ODE solvers, introducing a robust IM-connection for BERT layers.

**Key Contributions:**

	1. Introduces IM-BERT for improved robustness against adversarial attacks.
	2. Analyzes numerical stability of ODE solvers in the context of PLMs.
	3. Demonstrates significant performance gains on AdvGLUE dataset.

**Result:** IM-BERT shows an 8.3% performance improvement on the AdvGLUE dataset compared to original BERT and performs 5.9% better in low-resource scenarios.

**Limitations:** 

**Conclusion:** The IM-BERT framework provides a robust alternative for defending against adversarial attacks on PLMs without requiring additional parameters or adversarial training.

**Abstract:** Pre-trained Language Models (PLMs) have achieved remarkable performance on diverse NLP tasks through pre-training and fine-tuning. However, fine-tuning the model with a large number of parameters on limited downstream datasets often leads to vulnerability to adversarial attacks, causing overfitting of the model on standard datasets.   To address these issues, we propose IM-BERT from the perspective of a dynamic system by conceptualizing a layer of BERT as a solution of Ordinary Differential Equations (ODEs). Under the situation of initial value perturbation, we analyze the numerical stability of two main numerical ODE solvers: the explicit and implicit Euler approaches.   Based on these analyses, we introduce a numerically robust IM-connection incorporating BERT's layers. This strategy enhances the robustness of PLMs against adversarial attacks, even in low-resource scenarios, without introducing additional parameters or adversarial training strategies.   Experimental results on the adversarial GLUE (AdvGLUE) dataset validate the robustness of IM-BERT under various conditions. Compared to the original BERT, IM-BERT exhibits a performance improvement of approximately 8.3\%p on the AdvGLUE dataset. Furthermore, in low-resource scenarios, IM-BERT outperforms BERT by achieving 5.9\%p higher accuracy.

</details>


### [56] [EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation](https://arxiv.org/abs/2505.06904)

*Xinyi Mou, Chen Qian, Wei Liu, Xuanjing Huang, Zhongyu Wei*

**Main category:** cs.CL

**Keywords:** EcoLANG, social simulations, agent communication, language evolution, token consumption

**Relevance Score:** 7

**TL;DR:** EcoLANG is a framework for efficient agent communication in social simulations that reduces token consumption by over 20% while maintaining accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address high time and computation costs in large-scale social simulations using large language models.

**Method:** EcoLANG operates in two stages: language evolution through natural selection of words and rules, and language utilization for agent communication.

**Key Contributions:**

	1. Introduction of EcoLANG framework for agent communication in social simulations
	2. Reduction of token consumption by over 20%
	3. Utilization of natural selection for optimizing language rules and words

**Result:** Experimental results show that EcoLANG reduces token consumption by over 20%, enhancing efficiency in social simulations.

**Limitations:** 

**Conclusion:** EcoLANG offers a solution to improve efficiency in social simulations without sacrificing accuracy.

**Abstract:** Large language models (LLMs) have demonstrated an impressive ability to role-play humans and replicate complex social dynamics. While large-scale social simulations are gaining increasing attention, they still face significant challenges, particularly regarding high time and computation costs. Existing solutions, such as distributed mechanisms or hybrid agent-based model (ABM) integrations, either fail to address inference costs or compromise accuracy and generalizability. To this end, we propose EcoLANG: Efficient and Effective Agent Communication Language Induction for Social Simulation. EcoLANG operates in two stages: (1) language evolution, where we filter synonymous words and optimize sentence-level rules through natural selection, and (2) language utilization, where agents in social simulations communicate using the evolved language. Experimental results demonstrate that EcoLANG reduces token consumption by over 20%, enhancing efficiency without sacrificing simulation accuracy.

</details>


### [57] [The Distracting Effect: Understanding Irrelevant Passages in RAG](https://arxiv.org/abs/2505.06914)

*Chen Amiraz, Florin Cuconasu, Simone Filice, Zohar Karnin*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, distracting passages, LLM fine-tuning, answering accuracy, HCI

**Relevance Score:** 9

**TL;DR:** This paper addresses the problem of irrelevant passages distracting LLMs in Retrieval Augmented Generation (RAG) and introduces methods to quantify and utilize hard distracting passages to improve accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The distraction caused by irrelevant retrieved passages in RAG systems leads to incorrect responses from LLMs, necessitating a better understanding and management of this issue.

**Method:** The authors formulate a measurable and robust quantification of the distracting effect of passages concerning queries and LLMs, and introduce novel methods for identifying and utilizing hard distracting passages to fine-tune LLMs.

**Key Contributions:**

	1. Quantitative measure of distracting effects of passages for LLMs
	2. Novel methods for identifying hard distracting passages
	3. Framework for classifying passages beyond binary classification

**Result:** The fine-tuning of LLMs with identified hard distracting passages resulted in up to a 7.5% increase in answering accuracy compared to those fine-tuned on traditional RAG datasets.

**Limitations:** 

**Conclusion:** This research provides a comprehensive framework for classifying and utilizing hard distracting passages, enhancing RAG systems' performance and understanding of passage relevance.

**Abstract:** A well-known issue with Retrieval Augmented Generation (RAG) is that retrieved passages that are irrelevant to the query sometimes distract the answer-generating LLM, causing it to provide an incorrect response. In this paper, we shed light on this core issue and formulate the distracting effect of a passage w.r.t. a query (and an LLM). We provide a quantifiable measure of the distracting effect of a passage and demonstrate its robustness across LLMs.   Our research introduces novel methods for identifying and using hard distracting passages to improve RAG systems. By fine-tuning LLMs with these carefully selected distracting passages, we achieve up to a 7.5% increase in answering accuracy compared to counterparts fine-tuned on conventional RAG datasets. Our contribution is two-fold: first, we move beyond the simple binary classification of irrelevant passages as either completely unrelated vs. distracting, and second, we develop and analyze multiple methods for finding hard distracting passages. To our knowledge, no other research has provided such a comprehensive framework for identifying and utilizing hard distracting passages.

</details>


### [58] [CNN-based Image Models Verify a Hypothesis that The Writers of Cuneiform Texts Improved Their Writing Skills When Studying at the Age of Hittite Empire](https://arxiv.org/abs/2505.06974)

*Daichi Kohmoto, Katsutoshi Fukuda, Daisuke Yoshida, Takafumi Matsui, Sachihiro Omura*

**Main category:** cs.CL

**Keywords:** cuneiform, Convolutional Neural Network, image analysis, teacher-student dynamic, Kizzuwatna rituals

**Relevance Score:** 0

**TL;DR:** This paper analyzes a cuneiform tablet using CNN-based image models to reveal insights about the writers' roles, suggesting a teacher-student dynamic in the writing process.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the purpose behind the creation of cuneiform tablets, particularly those containing repetitive rituals rather than standard information like myths or records.

**Method:** The study applies a CNN-based image analysis methodology to quantitatively assess the writing on the tablet without individual segmentation of the cuneiforms.

**Key Contributions:**

	1. Introduction of a data-driven methodology for analyzing cuneiform tablets utilizing CNNs
	2. Identification of a teacher-student dynamic in ancient writing processes
	3. Provision of new insights into the cultural practices surrounding cuneiform writing.

**Result:** The analysis indicates that the first writer acted as a 'teacher' and the second as a 'student' honing their skills in cuneiform writing, a conclusion not derived from classical linguistics.

**Limitations:** 

**Conclusion:** The findings open up avenues for further studies using the developed methodology in different contexts, highlighting its potential for broader applications in historical text analysis.

**Abstract:** A cuneiform tablet KBo 23.1 ++/KUB 30.38, which is known to represent a text of Kizzuwatna rituals, was written by two writers with almost identical content in two iterations. Unlike other cuneiform tablets that contained information such as myths, essays, or business records, the reason why ancient people left such tablets for posterity remains unclear. To study this problem, we develop a new methodology by analyzing images of a tablet quantitatively using CNN (Convolutional Neural Network)-based image models, without segmenting cuneiforms one-by-one. Our data-driven methodology implies that the writer writing the first half was a `teacher' and the other writer was a `student' who was training his skills of writing cuneiforms. This result has not been reached by classical linguistics. We also discuss related conclusions and possible further directions for applying our method and its generalizations.

</details>


### [59] [Convert Language Model into a Value-based Strategic Planner](https://arxiv.org/abs/2505.06987)

*Xiaoyu Wang, Yue Zhao, Qingqing Gu, Zhonglin Jiang, Xiaokai Chen, Yong Chen, Luo Ji*

**Main category:** cs.CL

**Keywords:** emotional support, Q-learning, large language models, HCI, health informatics

**Relevance Score:** 8

**TL;DR:** The paper introduces straQ*, a framework utilizing Q-learning on large language models (LLMs) to enhance emotional support conversations (ESC) by optimizing long-term satisfaction through strategic planning.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the long-term effectiveness of emotional support conversations delivered by large language models, which often lack a structured approach for persistent emotional engagement.

**Method:** The proposed framework, straQ*, integrates Q-learning strategies with plug-and-play LLMs to optimize conversation responses based on projected long-term emotional outcomes.

**Key Contributions:**

	1. Introduction of the straQ* framework for ESC
	2. Application of Q-learning to optimize LLM responses
	3. Demonstration of substantial performance improvements over existing methods

**Result:** Experiments demonstrate that straQ* significantly outperforms various baseline methods including direct inference and self-refinement techniques in the context of emotional support conversation tasks.

**Limitations:** 

**Conclusion:** The integration of Q-learning into LLMs for emotional support conversations offers a promising enhancement, ensuring the delivery of more strategically optimal and satisfying interactions.

**Abstract:** Emotional support conversation (ESC) aims to alleviate the emotional distress of individuals through effective conversations. Although large language models (LLMs) have obtained remarkable progress on ESC, most of these studies might not define the diagram from the state model perspective, therefore providing a suboptimal solution for long-term satisfaction. To address such an issue, we leverage the Q-learning on LLMs, and propose a framework called straQ*. Our framework allows a plug-and-play LLM to bootstrap the planning during ESC, determine the optimal strategy based on long-term returns, and finally guide the LLM to response. Substantial experiments on ESC datasets suggest that straQ* outperforms many baselines, including direct inference, self-refine, chain of thought, finetuning, and finite state machines.

</details>


### [60] [HAMLET: Healthcare-focused Adaptive Multilingual Learning Embedding-based Topic Modeling](https://arxiv.org/abs/2505.07157)

*Hajar Sakai, Sarah S. Lam*

**Main category:** cs.CL

**Keywords:** topic modeling, healthcare, large language models, BERT, graph neural networks

**Relevance Score:** 9

**TL;DR:** HAMLET is a graph-driven architecture for improving healthcare topic modeling using LLMs, refining topic embeddings through BERT and GNN.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional topic models struggle with contextual nuances, polysemy, and rare words, leading to incoherent topics.

**Method:** HAMLET uses LLMs for initial topic generation, followed by a hybrid technique employing BERT and GNN for topic embedding refinement and similarity computation.

**Key Contributions:**

	1. Introduction of HAMLET for cross-lingual healthcare topic modeling
	2. Neural-enhanced semantic fusion for embedding refinement
	3. Hybrid technique using BERT and GNN for improved topic extraction

**Result:** Experiments on two healthcare datasets (English and French) show that HAMLET effectively refines topic embeddings and extracts coherent topics.

**Limitations:** 

**Conclusion:** The proposed architecture improves the interpretability and quality of topic modeling in healthcare applications by adequately addressing the limitations of traditional methods.

**Abstract:** Traditional topic models often struggle with contextual nuances and fail to adequately handle polysemy and rare words. This limitation typically results in topics that lack coherence and quality. Large Language Models (LLMs) can mitigate this issue by generating an initial set of topics. However, these raw topics frequently lack refinement and representativeness, which leads to redundancy without lexical similarity and reduced interpretability. This paper introduces HAMLET, a graph-driven architecture for cross-lingual healthcare topic modeling that uses LLMs. The proposed approach leverages neural-enhanced semantic fusion to refine the embeddings of topics generated by the LLM. Instead of relying solely on statistical co-occurrence or human interpretation to extract topics from a document corpus, this method introduces a topic embedding refinement that uses Bidirectional Encoder Representations from Transformers (BERT) and Graph Neural Networks (GNN). After topic generation, a hybrid technique that involves BERT and Sentence-BERT (SBERT) is employed for embedding. The topic representations are further refined using a GNN, which establishes connections between documents, topics, words, similar topics, and similar words. A novel method is introduced to compute similarities. Consequently, the topic embeddings are refined, and the top k topics are extracted. Experiments were conducted using two healthcare datasets, one in English and one in French, from which six sets were derived. The results demonstrate the effectiveness of HAMLET.

</details>


### [61] [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)

*Jannatun Naim, Jie Cao, Fareen Tasneem, Jennifer Jacobs, Brent Milne, James Martin, Tamara Sumner*

**Main category:** cs.CL

**Keywords:** discourse analysis, natural language processing, education, AI, dialogue acts

**Relevance Score:** 7

**TL;DR:** The paper introduces a multi-perspective discourse analysis framework for analyzing mathematics education dialogues using NLP to enhance feedback and learning environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Effective feedback is crucial in mathematics education, but challenges exist in understanding discourse due to multifunctionality and exclusion of utterances in discourse classifications.

**Method:** A multi-perspective discourse analysis framework that integrates domain-specific talk moves, dialogue acts, and discourse relations using a top-down analysis approach.

**Key Contributions:**

	1. Proposed a new analysis framework for classroom discourse
	2. Revealed the importance of non-talk move utterances
	3. Provided insights for AI application in educational feedback systems

**Result:** The analysis revealed meaningful discourse patterns, highlighting the significance of utterances without talk moves in guiding classroom dialogue, leading to improved understanding of discourse dynamics.

**Limitations:** 

**Conclusion:** Incorporating discourse relations and dialogue acts enhances AI-assisted education systems, aiding both human feedback and AI agent development.

**Abstract:** Effective feedback is essential for refining instructional practices in mathematics education, and researchers often turn to advanced natural language processing (NLP) models to analyze classroom dialogues from multiple perspectives. However, utterance-level discourse analysis encounters two primary challenges: (1) multifunctionality, where a single utterance may serve multiple purposes that a single tag cannot capture, and (2) the exclusion of many utterances from domain-specific discourse move classifications, leading to their omission in feedback. To address these challenges, we proposed a multi-perspective discourse analysis that integrates domain-specific talk moves with dialogue act (using the flattened multi-functional SWBD-MASL schema with 43 tags) and discourse relation (applying Segmented Discourse Representation Theory with 16 relations). Our top-down analysis framework enables a comprehensive understanding of utterances that contain talk moves, as well as utterances that do not contain talk moves. This is applied to two mathematics education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through distributional unigram analysis, sequential talk move analysis, and multi-view deep dive, we discovered meaningful discourse patterns, and revealed the vital role of utterances without talk moves, demonstrating that these utterances, far from being mere fillers, serve crucial functions in guiding, acknowledging, and structuring classroom discourse. These insights underscore the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create more responsive learning environments. Our framework may prove helpful for providing human educator feedback, but also aiding in the development of AI agents that can effectively emulate the roles of both educators and students.

</details>


### [62] [KDH-MLTC: Knowledge Distillation for Healthcare Multi-Label Text Classification](https://arxiv.org/abs/2505.07162)

*Hajar Sakai, Sarah S. Lam*

**Main category:** cs.CL

**Keywords:** Healthcare, Text Classification, Knowledge Distillation, Large Language Models, Hyperparameter Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents KDH-MLTC, a framework for efficient healthcare multi-label text classification using knowledge distillation and LLMs, showing high accuracy while maintaining compliance with healthcare data regulations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for accurate and efficient classification of complex healthcare textual data.

**Method:** Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC) uses model compression, sequential fine-tuning, and Particle Swarm Optimization for hyperparameter tuning.

**Key Contributions:**

	1. Introduction of KDH-MLTC framework for healthcare MLTC
	2. Use of knowledge distillation to transfer knowledge from a complex to a lighter model
	3. Implementation of Particle Swarm Optimization for hyperparameter tuning

**Result:** KDH-MLTC outperforms existing methods, achieving an F1 score of 82.70% on the largest dataset, while being computationally efficient and HIPAA compliant.

**Limitations:** 

**Conclusion:** The framework is robust and provides a solution to balance efficiency and accuracy in healthcare text classification.

**Abstract:** The increasing volume of healthcare textual data requires computationally efficient, yet highly accurate classification approaches able to handle the nuanced and complex nature of medical terminology. This research presents Knowledge Distillation for Healthcare Multi-Label Text Classification (KDH-MLTC), a framework leveraging model compression and Large Language Models (LLMs). The proposed approach addresses conventional healthcare Multi-Label Text Classification (MLTC) challenges by integrating knowledge distillation and sequential fine-tuning, subsequently optimized through Particle Swarm Optimization (PSO) for hyperparameter tuning. KDH-MLTC transfers knowledge from a more complex teacher LLM (i.e., BERT) to a lighter student LLM (i.e., DistilBERT) through sequential training adapted to MLTC that preserves the teacher's learned information while significantly reducing computational requirements. As a result, the classification is enabled to be conducted locally, making it suitable for healthcare textual data characterized by sensitivity and, therefore, ensuring HIPAA compliance. The experiments conducted on three medical literature datasets of different sizes, sampled from the Hallmark of Cancer (HoC) dataset, demonstrate that KDH-MLTC achieves superior performance compared to existing approaches, particularly for the largest dataset, reaching an F1 score of 82.70%. Additionally, statistical validation and an ablation study are carried out, proving the robustness of KDH-MLTC. Furthermore, the PSO-based hyperparameter optimization process allowed the identification of optimal configurations. The proposed approach contributes to healthcare text classification research, balancing efficiency requirements in resource-constrained healthcare settings with satisfactory accuracy demands.

</details>


### [63] [Structural Entropy Guided Agent for Detecting and Repairing Knowledge Deficiencies in LLMs](https://arxiv.org/abs/2505.07184)

*Yifan Wei, Xiaoyan Yu, Tengfei Pan, Angsheng Li, Li Du*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Gaps, Synthetic Data

**Relevance Score:** 9

**TL;DR:** The SENATOR framework uses Structural Entropy and MCTS to enhance LLMs' knowledge in medicine and scientific research by generating targeted synthetic data for fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often struggle with knowledge-intensive domains requiring high factual accuracy, prompting the need for improved methods to augment their domain knowledge effectively.

**Method:** The SENATOR framework utilizes a Structure Entropy metric to identify knowledge gaps and applies Monte Carlo Tree Search to explore these gaps, generating focused synthetic data for supervised fine-tuning.

**Key Contributions:**

	1. Introduction of Structural Entropy as a measure for LLM knowledge deficiencies.
	2. Use of Monte Carlo Tree Search for focused exploration and data generation.
	3. Demonstrated performance improvements in LLMs on knowledge-intensive tasks.

**Result:** SENATOR shows significant performance enhancements on LLaMA-3 and Qwen2 across various domain-specific benchmarks by effectively addressing LLM knowledge deficiencies.

**Limitations:** 

**Conclusion:** The framework demonstrates a promising approach to improving LLMs' factual precision in critical fields like medicine, showcasing its potential for continuous self-improvement through targeted data generation.

**Abstract:** Large language models (LLMs) have achieved unprecedented performance by leveraging vast pretraining corpora, yet their performance remains suboptimal in knowledge-intensive domains such as medicine and scientific research, where high factual precision is required. While synthetic data provides a promising avenue for augmenting domain knowledge, existing methods frequently generate redundant samples that do not align with the model's true knowledge gaps. To overcome this limitation, we propose a novel Structural Entropy-guided Knowledge Navigator (SENATOR) framework that addresses the intrinsic knowledge deficiencies of LLMs. Our approach employs the Structure Entropy (SE) metric to quantify uncertainty along knowledge graph paths and leverages Monte Carlo Tree Search (MCTS) to selectively explore regions where the model lacks domain-specific knowledge. Guided by these insights, the framework generates targeted synthetic data for supervised fine-tuning, enabling continuous self-improvement. Experimental results on LLaMA-3 and Qwen2 across multiple domain-specific benchmarks show that SENATOR effectively detects and repairs knowledge deficiencies, achieving notable performance improvements. The code and data for our methods and experiments are available at https://github.com/weiyifan1023/senator.

</details>


### [64] [On the Cost and Benefits of Training Context with Utterance or Full Conversation Training: A Comparative Stud](https://arxiv.org/abs/2505.07202)

*Hyouin Liu, Zhikuan Zhang*

**Main category:** cs.CL

**Keywords:** TTS, Conversational AI, Contextual training

**Relevance Score:** 6

**TL;DR:** This paper investigates the effectiveness of context-based utterance-level training versus full conversation training in TTS systems, showing that the former yields better quality and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether existing open-source TTS architectures are insufficient and to improve accessibility and quality in conversational TTS systems.

**Method:** Empirical examination of two training approaches using 20 GPU-hours on an NVIDIA H100: context-based utterance-level training and full conversation training.

**Key Contributions:**

	1. Empirical comparison of two TTS training methods.
	2. Guidelines for improving conversational TTS development.
	3. Demonstrated benefits of context-based utterance-level training.

**Result:** Context-based utterance-level training achieves a higher Mean Opinion Score (MOS) of 4.3/5.0 compared to 3.7/5.0 for full conversation training, while also reducing training time by 37%.

**Limitations:** The study was limited to specific models and GPU usage, potentially not generalizable to all TTS systems.

**Conclusion:** Utterance-level training with contextual conditioning is preferable for enhancing resource efficiency and output quality in TTS systems.

**Abstract:** Modern TTS systems designed for conversations achieve high-quality utterances but often remain inaccessible publicly. Are existing open-source architectures inadequate, or are current training techniques insufficient? This paper investigates prominent models and their underlying behaviors regarding conversational context. Using 20 GPU-hours on an NVIDIA H100, we empirically examine two approaches: context-based utterance-level training versus full conversation training. Results demonstrate that context-based utterance training achieves superior MOS scores (4.3/5.0 vs 3.7/5.0) and reduces training time by 37%, while full conversation approaches suffer from speaker similarity hallucination issues. These findings provide practical guidelines for conversational TTS development, favoring utterance-level training with contextual conditioning for both resource efficiency and output quality.

</details>


### [65] [Benchmarking Ethical and Safety Risks of Healthcare LLMs in China-Toward Systemic Governance under Healthy China 2030](https://arxiv.org/abs/2505.07205)

*Mouxiao Bian, Rongzhao Zhang, Chao Ding, Xinwei Peng, Jie Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, healthcare, ethical challenges, patient safety, governance framework

**Relevance Score:** 9

**TL;DR:** This paper presents a novel benchmark for evaluating the ethical and safety challenges posed by Large Language Models (LLMs) in healthcare, revealing significant gaps and proposing a governance framework to enhance patient safety and ethical oversight in China.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the ethical and patient-safety challenges introduced by Large Language Models (LLMs) in the context of healthcare under China's Healthy China 2030 initiative.

**Method:** Developed a 12,000-item Q&A benchmark assessing 11 ethics and 9 safety dimensions in medical contexts and evaluated state-of-the-art Chinese medical LLMs' performance on this dataset.

**Key Contributions:**

	1. Novel 12,000-item Q&A benchmark for ethics and safety in medical LLMs
	2. Assessment of state-of-the-art medical LLMs with findings on performance gaps
	3. Proposed governance framework for ethical oversight and patient safety in LLM deployment

**Result:** State-of-the-art Chinese medical LLMs demonstrated moderate baseline performance with accuracy around 42.7%, which improved to 50.8% after fine-tuning on the proposed dataset, indicating notable gaps in LLM decision-making regarding ethics and safety.

**Limitations:** The study primarily focuses on Chinese medical LLMs and may not generalize to other contexts or regions; further research is needed to validate the proposed governance framework.

**Conclusion:** A practical governance framework is proposed for healthcare institutions to manage LLM risks effectively. This includes enhancing LLM auditing, data ethics guidelines, and safety simulation processes to align AI advancements with patient safety.

**Abstract:** Large Language Models (LLMs) are poised to transform healthcare under China's Healthy China 2030 initiative, yet they introduce new ethical and patient-safety challenges. We present a novel 12,000-item Q&A benchmark covering 11 ethics and 9 safety dimensions in medical contexts, to quantitatively evaluate these risks. Using this dataset, we assess state-of-the-art Chinese medical LLMs (e.g., Qwen 2.5-32B, DeepSeek), revealing moderate baseline performance (accuracy 42.7% for Qwen 2.5-32B) and significant improvements after fine-tuning on our data (up to 50.8% accuracy). Results show notable gaps in LLM decision-making on ethics and safety scenarios, reflecting insufficient institutional oversight. We then identify systemic governance shortfalls-including the lack of fine-grained ethical audit protocols, slow adaptation by hospital IRBs, and insufficient evaluation tools-that currently hinder safe LLM deployment. Finally, we propose a practical governance framework for healthcare institutions (embedding LLM auditing teams, enacting data ethics guidelines, and implementing safety simulation pipelines) to proactively manage LLM risks. Our study highlights the urgent need for robust LLM governance in Chinese healthcare, aligning AI innovation with patient safety and ethical standards.

</details>


### [66] [DynamicRAG: Leveraging Outputs of Large Language Model as Feedback for Dynamic Reranking in Retrieval-Augmented Generation](https://arxiv.org/abs/2505.07233)

*Jiashuo Sun, Xianrui Zhong, Sizhe Zhou, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Reinforcement Learning, Document Reranking, Language Models

**Relevance Score:** 9

**TL;DR:** This paper presents DynamicRAG, a novel framework for retrieval-augmented generation that enhances document selection using reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of selecting the optimal number of documents for knowledge-intensive tasks, improving generation quality and explainability.

**Method:** DynamicRAG employs a reranker modeled as an agent that dynamically adjusts both the order and number of retrieved documents based on the query, using reinforcement learning optimized through rewards from LLM output quality.

**Key Contributions:**

	1. Introduction of DynamicRAG framework for adjustable document retrieval
	2. Use of reinforcement learning to optimize reranking decisions based on output quality
	3. Demonstration of state-of-the-art performance across multiple datasets

**Result:** DynamicRAG showed superior performance on seven knowledge-intensive datasets, achieving state-of-the-art results in document retrieval and generation.

**Limitations:** 

**Conclusion:** The proposed DynamicRAG framework effectively improves the retrieval process in RAG systems, providing better quality outputs and understanding of retrieved documents.

**Abstract:** Retrieval-augmented generation (RAG) systems combine large language models (LLMs) with external knowledge retrieval, making them highly effective for knowledge-intensive tasks. A crucial but often under-explored component of these systems is the reranker, which refines retrieved documents to enhance generation quality and explainability. The challenge of selecting the optimal number of documents (k) remains unsolved: too few may omit critical information, while too many introduce noise and inefficiencies. Although recent studies have explored LLM-based rerankers, they primarily leverage internal model knowledge and overlook the rich supervisory signals that LLMs can provide, such as using response quality as feedback for optimizing reranking decisions. In this paper, we propose DynamicRAG, a novel RAG framework where the reranker dynamically adjusts both the order and number of retrieved documents based on the query. We model the reranker as an agent optimized through reinforcement learning (RL), using rewards derived from LLM output quality. Across seven knowledge-intensive datasets, DynamicRAG demonstrates superior performance, achieving state-of-the-art results. The model, data and code are available at https://github.com/GasolSun36/DynamicRAG

</details>


### [67] [SAS-Bench: A Fine-Grained Benchmark for Evaluating Short Answer Scoring with Large Language Models](https://arxiv.org/abs/2505.07247)

*Peichao Lai, Kexuan Zhang, Yi Lin, Linyihan Zhang, Feiyang Ye, Jinhao Yan, Yanwei Xu, Conghui He, Yilei Wang, Wentao Zhang, Bin Cui*

**Main category:** cs.CL

**Keywords:** Subjective Answer Grading, Large Language Models, Short Answer Scoring, Benchmark, Educational Assessment

**Relevance Score:** 8

**TL;DR:** SAS-Bench is a benchmark created for fine-grained scoring in Short Answer Scoring (SAS) tasks using large language models (LLMs), offering detailed evaluation and transparency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing subjective answer grading systems often yield coarse scores and lack transparency, which this work aims to address through a dedicated benchmark.

**Method:** Introduction of SAS-Bench, providing step-wise scoring and expert-annotated data, followed by experiments with LLMs to evaluate their performance on science-related questions.

**Key Contributions:**

	1. Introduction of SAS-Bench for LLM-based SAS tasks
	2. Release of a dataset with expert-annotated questions and responses
	3. Demonstration of few-shot prompting effectiveness in improving scoring accuracy.

**Result:** The study identifies challenges in scoring and shows that few-shot prompting improves scoring accuracy significantly.

**Limitations:** 

**Conclusion:** SAS-Bench aids in creating robust LLM-based evaluation systems that are fair and meaningful in educational contexts.

**Abstract:** Subjective Answer Grading (SAG) plays a crucial role in education, standardized testing, and automated assessment systems, particularly for evaluating short-form responses in Short Answer Scoring (SAS). However, existing approaches often produce coarse-grained scores and lack detailed reasoning. Although large language models (LLMs) have demonstrated potential as zero-shot evaluators, they remain susceptible to bias, inconsistencies with human judgment, and limited transparency in scoring decisions. To overcome these limitations, we introduce SAS-Bench, a benchmark specifically designed for LLM-based SAS tasks. SAS-Bench provides fine-grained, step-wise scoring, expert-annotated error categories, and a diverse range of question types derived from real-world subject-specific exams. This benchmark facilitates detailed evaluation of model reasoning processes and explainability. We also release an open-source dataset containing 1,030 questions and 4,109 student responses, each annotated by domain experts. Furthermore, we conduct comprehensive experiments with various LLMs, identifying major challenges in scoring science-related questions and highlighting the effectiveness of few-shot prompting in improving scoring accuracy. Our work offers valuable insights into the development of more robust, fair, and educationally meaningful LLM-based evaluation systems.

</details>


### [68] [Evaluating LLM-Generated Q&A Test: a Student-Centered Study](https://arxiv.org/abs/2505.06591)

*Anna WrÃ³blewska, Bartosz Grabek, Jakub Åwistak, Daniel Dan*

**Main category:** cs.CL

**Keywords:** AI chatbots, question-answer tests, psychometric analysis, Natural Language Processing, LLM-generated assessments

**Relevance Score:** 8

**TL;DR:** This study presents an automated system for generating reliable AI chatbot-based question-answer tests, demonstrating high psychometric performance and user satisfaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a scalable approach for creating reliable Q&A tests using AI that can match the quality of human-authored assessments.

**Method:** An automatic pipeline using GPT-4o-mini was created to generate Q&A tests for a Natural Language Processing course, followed by evaluations based on psychometric analysis and user ratings.

**Key Contributions:**

	1. Development of a pipeline for AI-generated assessments
	2. Demonstrated psychometric reliability comparable to human-generated tests
	3. High user satisfaction with AI-generated Q&A tests

**Result:** The generated test items showed strong discrimination and appropriate difficulty, receiving high ratings from both students and experts.

**Limitations:** Identified two items needing review based on differential item functioning.

**Conclusion:** The research shows that LLM-generated assessments can effectively replace traditional assessments, supporting AI-assisted test development.

**Abstract:** This research prepares an automatic pipeline for generating reliable question-answer (Q&A) tests using AI chatbots. We automatically generated a GPT-4o-mini-based Q&A test for a Natural Language Processing course and evaluated its psychometric and perceived-quality metrics with students and experts. A mixed-format IRT analysis showed that the generated items exhibit strong discrimination and appropriate difficulty, while student and expert star ratings reflect high overall quality. A uniform DIF check identified two items for review. These findings demonstrate that LLM-generated assessments can match human-authored tests in psychometric performance and user satisfaction, illustrating a scalable approach to AI-assisted assessment development.

</details>


### [69] [No Query, No Access](https://arxiv.org/abs/2505.07258)

*Wenqiang Wang, Siyuan Liang, Yangshijie Zhang, Xiaojun Jia, Hao Lin, Xiaochun Cao*

**Main category:** cs.CL

**Keywords:** adversarial attacks, natural language processing, large language models, security risks, NLP models

**Relevance Score:** 8

**TL;DR:** The paper introduces the Victim Data-based Adversarial Attack (VDBA), a novel method for generating adversarial attacks on NLP models using only victim texts, achieving a significant attack success rate improvement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing textual adversarial attacks require knowledge of the victim model, limiting their practical application. The study aims to develop a method that operates solely on victim texts to broaden the scope of adversarial attacks.

**Method:** The VDBA method uses publicly available pre-trained models and clustering techniques to create substitute models based on victim data. It employs a hierarchical substitution model design and diverse adversarial example generation to improve attack effectiveness and similarity.

**Key Contributions:**

	1. Introduction of the Victim Data-based Adversarial Attack (VDBA) model
	2. Hierarchical substitution model design for improved attack efficacy
	3. Demonstration of significant threats posed to LLMs without API access

**Result:** VDBA achieved a 52.08% improvement in attack success rate while reducing attack queries to 0. It poses a significant threat to large language models like Qwen2 and GPT, with a maximum ASR of 45.99% without needing API access.

**Limitations:** The method relies on victim texts, which may vary in nature and pose limitations in specific contexts.

**Conclusion:** The findings indicate that even advanced NLP models are vulnerable to adversarial attacks, showcasing serious security risks in their deployment.

**Abstract:** Textual adversarial attacks mislead NLP models, including Large Language Models (LLMs), by subtly modifying text. While effective, existing attacks often require knowledge of the victim model, extensive queries, or access to training data, limiting real-world feasibility. To overcome these constraints, we introduce the \textbf{Victim Data-based Adversarial Attack (VDBA)}, which operates using only victim texts. To prevent access to the victim model, we create a shadow dataset with publicly available pre-trained models and clustering methods as a foundation for developing substitute models. To address the low attack success rate (ASR) due to insufficient information feedback, we propose the hierarchical substitution model design, generating substitute models to mitigate the failure of a single substitute model at the decision boundary.   Concurrently, we use diverse adversarial example generation, employing various attack methods to generate and select the adversarial example with better similarity and attack effectiveness. Experiments on the Emotion and SST5 datasets show that VDBA outperforms state-of-the-art methods, achieving an ASR improvement of 52.08\% while significantly reducing attack queries to 0. More importantly, we discover that VDBA poses a significant threat to LLMs such as Qwen2 and the GPT family, and achieves the highest ASR of 45.99% even without access to the API, confirming that advanced NLP models still face serious security risks. Our codes can be found at https://anonymous.4open.science/r/VDBA-Victim-Data-based-Adversarial-Attack-36EC/

</details>


### [70] [On the Robustness of Reward Models for Language Model Alignment](https://arxiv.org/abs/2505.07271)

*Jiwoo Hong, Noah Lee, Eunki Kim, Guijin Son, Woojin Chung, Aman Gupta, Shao Tang, James Thorne*

**Main category:** cs.CL

**Keywords:** Bradley-Terry model, reward modeling, reinforcement learning, robustness, machine learning

**Relevance Score:** 8

**TL;DR:** The paper addresses over-optimization issues in Bradley-Terry model-based reward models for reinforcement learning, proposing batch-wise sum-to-zero regularization to enhance robustness and generalizability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To identify the causes of over-optimization in reward models trained with the Bradley-Terry model and improve their robustness to unseen input distributions.

**Method:** The authors analyze the excessive dispersion of hidden state norms as a cause of over-optimization and propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sums per batch.

**Key Contributions:**

	1. Identification of hidden state norm dispersion as a source of over-optimization
	2. Introduction of batch-wise sum-to-zero regularization for improved robustness
	3. Empirical validation of BSR enhancing performance in preference prediction tasks.

**Result:** BSR significantly improves the robustness of reward models, leading to better alignment of policies to gold preference models and better performance on preference prediction tasks, outperforming state-of-the-art models.

**Limitations:** 

**Conclusion:** Robustness in reward models is critical for effective reinforcement learning with human feedback, and the proposed BSR method improves this robustness.

**Abstract:** The Bradley-Terry (BT) model is widely practiced in reward modeling for reinforcement learning with human feedback (RLHF). Despite its effectiveness, reward models (RMs) trained with BT model loss are prone to over-optimization, losing generalizability to unseen input distributions. In this paper, we study the cause of over-optimization in RM training and its downstream effects on the RLHF procedure, accentuating the importance of distributional robustness of RMs in unseen data. First, we show that the excessive dispersion of hidden state norms is the main source of over-optimization. Then, we propose batch-wise sum-to-zero regularization (BSR) to enforce zero-centered reward sum per batch, constraining the rewards with extreme magnitudes. We assess the impact of BSR in improving robustness in RMs through four scenarios of over-optimization, where BSR consistently manifests better robustness. Subsequently, we compare the plain BT model and BSR on RLHF training and empirically show that robust RMs better align the policy to the gold preference model. Finally, we apply BSR to high-quality data and models, which surpasses state-of-the-art RMs in the 8B scale by adding more than 5% in complex preference prediction tasks. By conducting RLOO training with 8B RMs, AlpacaEval 2.0 reduces generation length by 40% while adding a 7% increase in win rate, further highlighting that robustness in RMs induces robustness in RLHF training. We release the code, data, and models: https://github.com/LinkedIn-XFACT/RM-Robustness.

</details>


### [71] [Semantic Retention and Extreme Compression in LLMs: Can We Have Both?](https://arxiv.org/abs/2505.07289)

*Stanislas Laborde, Martin Cousseau, Antoun Yaacoub, Lionel Prevost*

**Main category:** cs.CL

**Keywords:** Large Language Models, model compression, pruning, quantization, semantic retention

**Relevance Score:** 9

**TL;DR:** Explores joint compression of Large Language Models through combined pruning and quantization, introducing a new metric for evaluating performance and compression trade-offs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid deployment of Large Language Models has created a high demand for effective model compression methods to lower computational and memory expenses.

**Method:** The paper investigates the synergistic effects of applying both pruning and quantization on LLMs to improve performance-to-compression ratios. It also presents a new metric, the Semantic Retention Compression Rate (SrCr), for better evaluating LLM performance post-compression.

**Key Contributions:**

	1. Introduces a novel metric (Semantic Retention Compression Rate) for assessing compression efficacy while maintaining semantics.
	2. Demonstrates significant performance improvements through joint compression techniques compared to single-method approaches.
	3. Addresses limitations in traditional evaluation frameworks for LLM performance.

**Result:** Jointly applying pruning and quantization yields a 20% performance increase over models using only quantization at equivalent compression levels.

**Limitations:** Focused primarily on LLMs; results may vary across different architectures or applications.

**Conclusion:** The proposed combination of pruning and quantization not only enhances performance but also provides a flexible framework for optimizing LLM compression while preserving semantics.

**Abstract:** The exponential growth in Large Language Model (LLM) deployment has intensified the need for efficient model compression techniques to reduce computational and memory costs. While pruning and quantization have shown promise, their combined potential remains largely unexplored. In this paper, we examine joint compression and how strategically combining pruning and quantization could yield superior performance-to-compression ratios compared to single-method approaches. Recognizing the challenges in accurately assessing LLM performance, we address key limitations of previous evaluation frameworks and introduce the Semantic Retention Compression Rate (SrCr), a novel metric that quantifies the trade-off between model compression and semantic preservation, facilitating the optimization of pruning-quantization configurations. Experiments demonstrate that our recommended combination achieves, on average, a 20% performance increase compared to an equivalent quantization-only model at the same theoretical compression rate.

</details>


### [72] [Towards Actionable Pedagogical Feedback: A Multi-Perspective Analysis of Mathematics Teaching and Tutoring Dialogue](https://arxiv.org/abs/2505.07161)

*Jannatun Naim, Jie Cao, Fareen Tasneem, Jennifer Jacobs, Brent Milne, James Martin, Tamara Sumner*

**Main category:** cs.CL

**Keywords:** discourse analysis, natural language processing, mathematics education

**Relevance Score:** 4

**TL;DR:** The paper presents a multi-perspective discourse analysis framework for enhancing feedback in mathematics education through advanced NLP models, addressing challenges in utterance-level analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To refine instructional practices in mathematics education by effectively analyzing classroom dialogues.

**Method:** A top-down analysis framework integrating domain-specific talk moves, dialogue acts, and discourse relations while applying the SWBD-MASL schema and Segmented Discourse Representation Theory.

**Key Contributions:**

	1. Multi-perspective discourse analysis framework
	2. Integration of dialogue acts and discourse relations
	3. Insights into the functions of utterances without talk moves

**Result:** Meaningful discourse patterns were identified, emphasizing the crucial roles of utterances without talk moves in guiding classroom discourse, thereby enhancing AI-assisted educational feedback.

**Limitations:** 

**Conclusion:** The proposed framework can aid both human educators and AI agents in providing effective feedback and emulating educational roles.

**Abstract:** Effective feedback is essential for refining instructional practices in mathematics education, and researchers often turn to advanced natural language processing (NLP) models to analyze classroom dialogues from multiple perspectives. However, utterance-level discourse analysis encounters two primary challenges: (1) multifunctionality, where a single utterance may serve multiple purposes that a single tag cannot capture, and (2) the exclusion of many utterances from domain-specific discourse move classifications, leading to their omission in feedback. To address these challenges, we proposed a multi-perspective discourse analysis that integrates domain-specific talk moves with dialogue act (using the flattened multi-functional SWBD-MASL schema with 43 tags) and discourse relation (applying Segmented Discourse Representation Theory with 16 relations). Our top-down analysis framework enables a comprehensive understanding of utterances that contain talk moves, as well as utterances that do not contain talk moves. This is applied to two mathematics education datasets: TalkMoves (teaching) and SAGA22 (tutoring). Through distributional unigram analysis, sequential talk move analysis, and multi-view deep dive, we discovered meaningful discourse patterns, and revealed the vital role of utterances without talk moves, demonstrating that these utterances, far from being mere fillers, serve crucial functions in guiding, acknowledging, and structuring classroom discourse. These insights underscore the importance of incorporating discourse relations and dialogue acts into AI-assisted education systems to enhance feedback and create more responsive learning environments. Our framework may prove helpful for providing human educator feedback, but also aiding in the development of AI agents that can effectively emulate the roles of both educators and students.

</details>


### [73] [AttentionInfluence: Adopting Attention Head Influence for Weak-to-Strong Pretraining Data Selection](https://arxiv.org/abs/2505.07293)

*Kai Hua, Steven Wu, Ge Zhang, Ke Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Selection, Attention Mechanism, Reasoning Tasks, Pretraining

**Relevance Score:** 8

**TL;DR:** Proposes AttentionInfluence, a training-free method for selecting reasoning-intensive pretraining data to improve LLMs' reasoning ability, achieving significant performance gains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning abilities of LLMs, which have relied heavily on biased human or LLM-labeled data.

**Method:** AttentionInfluence uses attention head masking to identify data selection criteria without supervision, allowing a pretrained language model to act as a data selector.

**Key Contributions:**

	1. Introduction of AttentionInfluence for unsupervised data selection
	2. Demonstration of performance improvements across multiple benchmarks
	3. Effective weak-to-strong scaling for reasoning tasks

**Result:** Substantial improvements in performance were observed, with gains ranging from 1.4pp to 3.5pp on several complex reasoning benchmarks.

**Limitations:** The method may still be subject to biases depending on the pretrained model's characteristics and the selected data subsets.

**Conclusion:** The approach shows promise in scalable reasoning-centric data selection, improving performance of smaller models in larger model training contexts.

**Abstract:** Recently, there has been growing interest in collecting reasoning-intensive pretraining data to improve LLMs' complex reasoning ability. Prior approaches typically rely on supervised classifiers to identify such data, which requires labeling by humans or LLMs, often introducing domain-specific biases. Due to the attention heads being crucial to in-context reasoning, we propose AttentionInfluence, a simple yet effective, training-free method without supervision signal. Our approach enables a small pretrained language model to act as a strong data selector through a simple attention head masking operation. Specifically, we identify retrieval heads and compute the loss difference when masking these heads. We apply AttentionInfluence to a 1.3B-parameter dense model to conduct data selection on the SmolLM corpus of 241B tokens, and mix the SmolLM corpus with the selected subset comprising 73B tokens to pretrain a 7B-parameter dense model using 1T training tokens and WSD learning rate scheduling. Our experimental results demonstrate substantial improvements, ranging from 1.4pp to 3.5pp, across several knowledge-intensive and reasoning-heavy benchmarks (i.e., MMLU, MMLU-Pro, AGIEval-en, GSM8K, and HumanEval). This demonstrates an effective weak-to-strong scaling property, with small models improving the final performance of larger models-offering a promising and scalable path for reasoning-centric data selection.

</details>


### [74] [Towards Multi-Agent Reasoning Systems for Collaborative Expertise Delegation: An Exploratory Design Study](https://arxiv.org/abs/2505.07313)

*Baixuan Xu, Chunyang Li, Weiqi Wang, Wei Fan, Tianshi Zheng, Haochen Shi, Tao Fan, Yangqiu Song, Qiang Yang*

**Main category:** cs.CL

**Keywords:** multi-agent systems, collaborative reasoning, expertise alignment, scalability, communication protocols

**Relevance Score:** 9

**TL;DR:** The paper investigates the design of collaboration structures in multi-agent LLM systems to enhance collective reasoning, focusing on expertise alignment, collaboration paradigms, and system scale.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To improve collaborative reasoning performance in multi-agent LLM systems, as this area is currently under-explored.

**Method:** The study systematically examines the effects of expertise-domain alignment, collaboration paradigms (structured vs. diversity-driven), and system scale on reasoning performance.

**Key Contributions:**

	1. Investigation of expertise-domain alignment in multi-agent LLM systems.
	2. Comparison of collaboration paradigms and their impact on performance.
	3. Empirical analysis of scaling effects and communication protocol requirements.

**Result:** Expertise alignment shows significant benefits for contextual reasoning tasks, while collaborative integration of diverse knowledge outperforms rigid task decomposition. Scaling with expertise specialization reveals computational trade-offs.

**Limitations:** 

**Conclusion:** The research offers actionable guidelines for configuring specialized multi-agent systems and points out key architectural challenges for scalable reasoning.

**Abstract:** Designing effective collaboration structure for multi-agent LLM systems to enhance collective reasoning is crucial yet remains under-explored. In this paper, we systematically investigate how collaborative reasoning performance is affected by three key design dimensions: (1) Expertise-Domain Alignment, (2) Collaboration Paradigm (structured workflow vs. diversity-driven integration), and (3) System Scale. Our findings reveal that expertise alignment benefits are highly domain-contingent, proving most effective for contextual reasoning tasks. Furthermore, collaboration focused on integrating diverse knowledge consistently outperforms rigid task decomposition. Finally, we empirically explore the impact of scaling the multi-agent system with expertise specialization and study the computational trade off, highlighting the need for more efficient communication protocol design. This work provides concrete guidelines for configuring specialized multi-agent system and identifies critical architectural trade-offs and bottlenecks for scalable multi-agent reasoning. The code will be made available upon acceptance.

</details>


### [75] [QUPID: Quantified Understanding for Enhanced Performance, Insights, and Decisions in Korean Search Engines](https://arxiv.org/abs/2505.07345)

*Ohjoon Kwon, Changsu Lee, Jihye Back, Lim Sun Suk, Inho Kang, Donghyeon Jeon*

**Main category:** cs.CL

**Keywords:** language models, relevance assessment, information retrieval, computational efficiency, machine learning

**Relevance Score:** 8

**TL;DR:** The study introduces QUPID, a model combining two small language models that outperforms large language models in relevance assessment for information retrieval, achieving higher accuracy and faster inference times.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of combining different small language models to improve relevance assessment in information retrieval while reducing computational costs compared to large language models.

**Method:** The QUPID approach integrates a generative small language model with an embedding-based small language model to enhance relevance judgment accuracy and operational efficiency.

**Key Contributions:**

	1. Introduction of QUPID, a novel approach for relevance assessment
	2. Demonstrated significant performance improvements over leading LLMs
	3. Achieved 60x faster inference times while maintaining accuracy

**Result:** QUPID achieved a Cohen's Kappa of 0.646 and improved nDCG@5 scores by 1.9%, while being 60x faster in inference times compared to leading large language models.

**Limitations:** 

**Conclusion:** The findings highlight the effectiveness of architectural diversity in model combinations for improving search relevance and efficiency in real-world search systems.

**Abstract:** Large language models (LLMs) have been widely used for relevance assessment in information retrieval. However, our study demonstrates that combining two distinct small language models (SLMs) with different architectures can outperform LLMs in this task. Our approach -- QUPID -- integrates a generative SLM with an embedding-based SLM, achieving higher relevance judgment accuracy while reducing computational costs compared to state-of-the-art LLM solutions. This computational efficiency makes QUPID highly scalable for real-world search systems processing millions of queries daily. In experiments across diverse document types, our method demonstrated consistent performance improvements (Cohen's Kappa of 0.646 versus 0.387 for leading LLMs) while offering 60x faster inference times. Furthermore, when integrated into production search pipelines, QUPID improved nDCG@5 scores by 1.9%. These findings underscore how architectural diversity in model combinations can significantly enhance both search relevance and operational efficiency in information retrieval systems.

</details>


### [76] [Computational Fact-Checking of Online Discourse: Scoring scientific accuracy in climate change related news articles](https://arxiv.org/abs/2505.07409)

*Tim Wittenborg, Constantin Sebastian Tremel, Markus Stocker, SÃ¶ren Auer*

**Main category:** cs.CL

**Keywords:** misinformation, veracity quantification, neurosymbolic system, LLM, knowledge graphs

**Relevance Score:** 7

**TL;DR:** This paper presents a semi-automated system for quantifying the scientific accuracy of online media to combat misinformation, using LLMs and knowledge graphs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable information in democratic societies and the challenge of misinformation impacting civic discourse.

**Method:** A neurosymbolic system leveraging LLM-based statement extraction and knowledge graph analysis to compare dubious content with trusted sources.

**Key Contributions:**

	1. Development of a neurosymbolic system for media veracity quantification
	2. Utilization of LLMs for statement extraction
	3. Introduction of a workflow for comparing suspect media against trusted sources

**Result:** The system was evaluated through expert interviews and a user survey, showing it provides a useful veracity indication for online media.

**Limitations:** The tool's inability to annotate public media at the required granularity and scale.

**Conclusion:** While the tool benefits civic discourse by indicating content veracity, it lacks granularity for large-scale public media annotation and calls for further development of a FAIR ground truth.

**Abstract:** Democratic societies need reliable information. Misinformation in popular media such as news articles or videos threatens to impair civic discourse. Citizens are, unfortunately, not equipped to verify this content flood consumed daily at increasing rates. This work aims to semi-automatically quantify scientific accuracy of online media. By semantifying media of unknown veracity, their statements can be compared against equally processed trusted sources. We implemented a workflow using LLM-based statement extraction and knowledge graph analysis. Our neurosymbolic system was able to evidently streamline state-of-the-art veracity quantification. Evaluated via expert interviews and a user survey, the tool provides a beneficial veracity indication. This indicator, however, is unable to annotate public media at the required granularity and scale. Further work towards a FAIR (Findable, Accessible, Interoperable, Reusable) ground truth and complementary metrics are required to scientifically support civic discourse.

</details>


### [77] [ViMRHP: A Vietnamese Benchmark Dataset for Multimodal Review Helpfulness Prediction via Human-AI Collaborative Annotation](https://arxiv.org/abs/2505.07416)

*Truc Mai-Thanh Nguyen, Dat Minh Nguyen, Son T. Luu, Kiet Van Nguyen*

**Main category:** cs.CL

**Keywords:** multimodal review, helpfulness prediction, Vietnamese language, AI-assisted annotation, E-commerce

**Relevance Score:** 6

**TL;DR:** This paper introduces ViMRHP, a Vietnamese dataset for predicting the helpfulness of user reviews, optimized with AI to reduce annotation time and costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user experience in E-commerce by evaluating the helpfulness of user reviews, especially for low-resource languages such as Vietnamese.

**Method:** A large-scale dataset (ViMRHP) is created for the Multimodal Review Helpfulness Prediction task, using AI to assist in annotation which significantly reduces time and costs.

**Key Contributions:**

	1. Introduction of a large-scale Vietnamese dataset for review helpfulness prediction.
	2. Utilization of AI to improve the annotation process efficiently.
	3. Evaluation of baseline models on human-verified and AI-generated annotations, comparing their quality.

**Result:** ViMRHP includes 46K reviews for 2K products and allows for faster annotation (20-40 seconds per task), while maintaining quality and reducing costs by about 65%.

**Limitations:** AI-generated annotations have limitations in complex tasks which are analyzed in detail.

**Conclusion:** The ViMRHP dataset allows for better research in the domain of review helpfulness prediction in Vietnamese and is available publicly for further studies.

**Abstract:** Multimodal Review Helpfulness Prediction (MRHP) is an essential task in recommender systems, particularly in E-commerce platforms. Determining the helpfulness of user-generated reviews enhances user experience and improves consumer decision-making. However, existing datasets focus predominantly on English and Indonesian, resulting in a lack of linguistic diversity, especially for low-resource languages such as Vietnamese. In this paper, we introduce ViMRHP (Vietnamese Multimodal Review Helpfulness Prediction), a large-scale benchmark dataset for MRHP task in Vietnamese. This dataset covers four domains, including 2K products with 46K reviews. Meanwhile, a large-scale dataset requires considerable time and cost. To optimize the annotation process, we leverage AI to assist annotators in constructing the ViMRHP dataset. With AI assistance, annotation time is reduced (90 to 120 seconds per task down to 20 to 40 seconds per task) while maintaining data quality and lowering overall costs by approximately 65%. However, AI-generated annotations still have limitations in complex annotation tasks, which we further examine through a detailed performance analysis. In our experiment on ViMRHP, we evaluate baseline models on human-verified and AI-generated annotations to assess their quality differences. The ViMRHP dataset is publicly available at https://github.com/trng28/ViMRHP

</details>


### [78] [Comparative sentiment analysis of public perception: Monkeypox vs. COVID-19 behavioral insights](https://arxiv.org/abs/2505.07430)

*Mostafa Mohaimen Akand Faisal, Rabeya Amin Jhuma*

**Main category:** cs.CL

**Keywords:** sentiment analysis, public health, machine learning, COVID-19, Monkeypox

**Relevance Score:** 8

**TL;DR:** This study compares public sentiment on COVID-19 and Monkeypox using extensive tweet datasets and advanced machine learning models to inform public health strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand public sentiment during global health crises like COVID-19 and Monkeypox to improve public health strategies.

**Method:** Conducted a comparative sentiment analysis using tweets and applied various machine learning models (Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa, XLNet) for sentiment classification.

**Key Contributions:**

	1. Comparative analysis of sentiment on two health crises using machine learning.
	2. Identification of key trends in public sentiment and emotion.
	3. Framework for enhancing real-time monitoring and multilingual analysis in public health informatics.

**Result:** The analysis revealed significant differences in sentiment driven by disease characteristics and media representation, helping identify trends in public emotion and discourse.

**Limitations:** 

**Conclusion:** Insights from this study can improve public health messaging and misinformation mitigation while enhancing real-time sentiment monitoring in future health crises.

**Abstract:** The emergence of global health crises, such as COVID-19 and Monkeypox (mpox), has underscored the importance of understanding public sentiment to inform effective public health strategies. This study conducts a comparative sentiment analysis of public perceptions surrounding COVID-19 and mpox by leveraging extensive datasets of 147,475 and 106,638 tweets, respectively. Advanced machine learning models, including Logistic Regression, Naive Bayes, RoBERTa, DistilRoBERTa and XLNet, were applied to perform sentiment classification, with results indicating key trends in public emotion and discourse. The analysis highlights significant differences in public sentiment driven by disease characteristics, media representation, and pandemic fatigue. Through the lens of sentiment polarity and thematic trends, this study offers valuable insights into tailoring public health messaging, mitigating misinformation, and fostering trust during concurrent health crises. The findings contribute to advancing sentiment analysis applications in public health informatics, setting the groundwork for enhanced real-time monitoring and multilingual analysis in future research.

</details>


### [79] [Matching Tasks with Industry Groups for Augmenting Commonsense Knowledge](https://arxiv.org/abs/2505.07440)

*Rituraj Singh, Sachin Pawar, Girish Palshikar*

**Main category:** cs.CL

**Keywords:** commonsense knowledge bases, weakly-supervised framework, task extraction, industry groups, neural model

**Relevance Score:** 4

**TL;DR:** The paper presents a weakly-supervised framework to enhance commonsense knowledge bases (KB) by introducing tasks associated with various industry groups (IGs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of explicit knowledge regarding tasks performed by different industries in existing commonsense KBs, like ConceptNet.

**Method:** The authors developed a weakly-supervised neural model to learn affinities between tasks and industry groups, followed by clustering to identify top-k tasks for each IG.

**Key Contributions:**

	1. Introduction of a framework to augment commonsense KBs with industry-specific tasks.
	2. Extraction of 2339 reliable task-IG pairs from publicly available datasets.
	3. Validation of the precision in task-IG associations.

**Result:** The study successfully extracted 2339 task-IG triples with a precision of 0.86, demonstrating the effectiveness of the proposed approach.

**Limitations:** The approach relies on the availability and quality of existing datasets for task extraction.

**Conclusion:** The research provides valuable task-IG pairs that can be integrated into existing commonsense KBs, enhancing their usefulness for ML applications.

**Abstract:** Commonsense knowledge bases (KB) are a source of specialized knowledge that is widely used to improve machine learning applications. However, even for a large KB such as ConceptNet, capturing explicit knowledge from each industry domain is challenging. For example, only a few samples of general {\em tasks} performed by various industries are available in ConceptNet. Here, a task is a well-defined knowledge-based volitional action to achieve a particular goal. In this paper, we aim to fill this gap and present a weakly-supervised framework to augment commonsense KB with tasks carried out by various industry groups (IG). We attempt to {\em match} each task with one or more suitable IGs by training a neural model to learn task-IG affinity and apply clustering to select the top-k tasks per IG. We extract a total of 2339 triples of the form $\langle IG, is~capable~of, task \rangle$ from two publicly available news datasets for 24 IGs with the precision of 0.86. This validates the reliability of the extracted task-IG pairs that can be directly added to existing KBs.

</details>


### [80] [Translating the Grievance Dictionary: a psychometric evaluation of Dutch, German, and Italian versions](https://arxiv.org/abs/2505.07495)

*Isabelle van der Vegt, Bennett Kleinberg, Marilu Miotto, Jonas Festor*

**Main category:** cs.CL

**Keywords:** Grievance Dictionary, psycholinguistic dictionary, automated translation, psychometric analysis, language translation

**Relevance Score:** 4

**TL;DR:** The paper details the translation and evaluation of the Grievance Dictionary into Dutch, German, and Italian, focusing on psychometric analysis and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to broaden the applicability of the Grievance Dictionary for analyzing violent and grievance-fuelled texts in languages other than English.

**Method:** Translations were automated and supplemented by human annotations, followed by psychometric analyses to assess reliability and correlations with the LIWC dictionary.

**Key Contributions:**

	1. Translation of the Grievance Dictionary into three new languages (Dutch, German, and Italian).
	2. Psychometric analysis of translated dictionaries comparing reliability and performance against the original English version.
	3. Recommendations for validation and future translation methodologies.

**Result:** Dutch and German translations demonstrated similarity in performance to the original English version, while the Italian version showed low reliability in certain categories.

**Limitations:** The Italian translation exhibited low reliability in several categories, indicating potential limitations in its application.

**Conclusion:** The findings suggest the need for further validation of the dictionary's translations and recommend future research methodologies for translations using this approach.

**Abstract:** This paper introduces and evaluates three translations of the Grievance Dictionary, a psycholinguistic dictionary for the analysis of violent, threatening or grievance-fuelled texts. Considering the relevance of these themes in languages beyond English, we translated the Grievance Dictionary to Dutch, German, and Italian. We describe the process of automated translation supplemented by human annotation. Psychometric analyses are performed, including internal reliability of dictionary categories and correlations with the LIWC dictionary. The Dutch and German translations perform similarly to the original English version, whereas the Italian dictionary shows low reliability for some categories. Finally, we make suggestions for further validation and application of the dictionary, as well as for future dictionary translations following a similar approach.

</details>


### [81] [ToolACE-DEV: Self-Improving Tool Learning via Decomposition and EVolution](https://arxiv.org/abs/2505.07512)

*Xu Huang, Weiwen Liu, Xingshan Zeng, Yuefeng Huang, Xinlong Hao, Yuxian Wang, Yirong Zeng, Chuhan Wu, Yasheng Wang, Ruiming Tang, Defu Lian*

**Main category:** cs.CL

**Keywords:** tool learning, large language models, self-evolving frameworks

**Relevance Score:** 8

**TL;DR:** ToolACE-DEV is a self-improving framework designed to enhance the tool-using capabilities of lightweight large language models while reducing dependence on advanced models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the tool-using capabilities of LLMs without high costs and compatibility issues associated with data synthesis from advanced models.

**Method:** The framework decomposes the tool-learning objective into sub-tasks focused on basic abilities and employs a self-evolving paradigm for models to self-improve.

**Key Contributions:**

	1. Introduction of ToolACE-DEV framework for tool learning
	2. Decomposition of tool-learning objectives into manageable sub-tasks
	3. Development of a self-evolving paradigm for lightweight models.

**Result:** The approach demonstrated effectiveness across multiple models of different scales and architectures, enhancing their ability to use tools and reducing reliance on advanced models.

**Limitations:** 

**Conclusion:** ToolACE-DEV provides a viable solution for enhancing tool-using capabilities in LLMs while minimizing costs and compatibility concerns.

**Abstract:** The tool-using capability of large language models (LLMs) enables them to access up-to-date external information and handle complex tasks. Current approaches to enhancing this capability primarily rely on distilling advanced models by data synthesis. However, this method incurs significant costs associated with advanced model usage and often results in data compatibility issues, led by the high discrepancy in the knowledge scope between the advanced model and the target model. To address these challenges, we propose ToolACE-DEV, a self-improving framework for tool learning. First, we decompose the tool-learning objective into sub-tasks that enhance basic tool-making and tool-using abilities. Then, we introduce a self-evolving paradigm that allows lightweight models to self-improve, reducing reliance on advanced LLMs. Extensive experiments validate the effectiveness of our approach across models of varying scales and architectures.

</details>


### [82] [SEReDeEP: Hallucination Detection in Retrieval-Augmented Models via Semantic Entropy and Context-Parameter Fusion](https://arxiv.org/abs/2505.07528)

*Lei Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, hallucination detection, semantic entropy, ReDeEP, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents SEReDeEP, an enhancement of the ReDeEP framework for more accurate hallucination detection in Retrieval-Augmented Generation models through semantic entropy analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the hallucination phenomena in Retrieval-Augmented Generation (RAG) models, emphasizing the need for effective detection methodologies that consider both internal and external mechanisms.

**Method:** SEReDeEP enhances the ReDeEP framework by incorporating semantic entropy assessments via trained linear probes to improve hallucination detection in model outputs.

**Key Contributions:**

	1. Introduction of the SEReDeEP framework that builds upon ReDeEP.
	2. Incorporation of semantic entropy for improved hallucination detection.
	3. Dynamic modulation of contributions from parametric knowledge and external information.

**Result:** The evaluation shows that SEReDeEP significantly improves the accuracy of hallucination assessments compared to existing methods, leading to better alignment with ground truth evaluations.

**Limitations:** The proposed method requires further testing across diverse RAG architectures and datasets to establish generalizability.

**Conclusion:** By integrating semantic entropy into the detection process, SEReDeEP provides a more nuanced approach to identifying hallucinations, thereby enhancing the reliability of RAG models.

**Abstract:** Retrieval-Augmented Generation (RAG) models frequently encounter hallucination phenomena when integrating external information with internal parametric knowledge. Empirical studies demonstrate that the disequilibrium between external contextual information and internal parametric knowledge constitutes a primary factor in hallucination generation. Existing hallucination detection methodologies predominantly emphasize either the external or internal mechanism in isolation, thereby overlooking their synergistic effects. The recently proposed ReDeEP framework decouples these dual mechanisms, identifying two critical contributors to hallucinations: excessive reliance on parametric knowledge encoded in feed-forward networks (FFN) and insufficient utilization of external information by attention mechanisms (particularly copy heads). ReDeEP quantitatively assesses these factors to detect hallucinations and dynamically modulates the contributions of FFNs and copy heads to attenuate their occurrence. Nevertheless, ReDeEP and numerous other hallucination detection approaches have been employed at logit-level uncertainty estimation or language-level self-consistency evaluation, inadequately address the semantic dimensions of model responses, resulting in inconsistent hallucination assessments in RAG implementations. Building upon ReDeEP's foundation, this paper introduces SEReDeEP, which enhances computational processes through semantic entropy captured via trained linear probes, thereby achieving hallucination assessments that more accurately reflect ground truth evaluations.

</details>


### [83] [A Multi-Dimensional Constraint Framework for Evaluating and Improving Instruction Following in Large Language Models](https://arxiv.org/abs/2505.07591)

*Junjie Ye, Caishuang Huang, Zhuohan Chen, Wenjie Fu, Chenyuan Yang, Leyi Yang, Yilong Wu, Peng Wang, Meng Zhou, Xiaolong Yang, Tao Gui, Qi Zhang, Zhongchao Shi, Jianping Fan, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** instruction following, large language models, constraint framework, reinforcement learning, performance evaluation

**Relevance Score:** 9

**TL;DR:** This paper proposes a multi-dimensional constraint framework for evaluating LLMs' instruction following capabilities, develops an automated instruction generation pipeline yielding 1,200 test samples, and evaluates 19 LLMs, demonstrating performance variations and improvements in constraint adherence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for instruction following in LLMs lack diversity and fine-grained performance assessment due to their reliance on templated constraint prompts.

**Method:** A multi-dimensional constraint framework was developed, which includes various constraint patterns, categories, and difficulty levels. An automated instruction generation pipeline was created to expand constraints, detect conflicts, and rewrite instructions, producing 1,200 test samples for evaluation.

**Key Contributions:**

	1. Development of a multi-dimensional constraint framework for LLMs.
	2. Automated instruction generation pipeline that creates diverse test samples.
	3. Demonstration of significant performance variations in LLMs across different constraint levels.

**Result:** 19 LLMs from seven model families were evaluated, revealing a significant drop in performance from 77.67% at Level I to 32.96% at Level IV of constraints. The approach also enhanced instruction following in reinforcement learning settings without harming overall performance.

**Limitations:** 

**Conclusion:** The results indicate that the proposed framework and generation pipeline effectively assess instruction following and can lead to performance improvements in LLMs by optimizing attention module parameters.

**Abstract:** Instruction following evaluates large language models (LLMs) on their ability to generate outputs that adhere to user-defined constraints. However, existing benchmarks often rely on templated constraint prompts, which lack the diversity of real-world usage and limit fine-grained performance assessment. To fill this gap, we propose a multi-dimensional constraint framework encompassing three constraint patterns, four constraint categories, and four difficulty levels. Building on this framework, we develop an automated instruction generation pipeline that performs constraint expansion, conflict detection, and instruction rewriting, yielding 1,200 code-verifiable instruction-following test samples. We evaluate 19 LLMs across seven model families and uncover substantial variation in performance across constraint forms. For instance, average performance drops from 77.67% at Level I to 32.96% at Level IV. Furthermore, we demonstrate the utility of our approach by using it to generate data for reinforcement learning, achieving substantial gains in instruction following without degrading general performance. In-depth analysis indicates that these gains stem primarily from modifications in the model's attention modules parameters, which enhance constraint recognition and adherence. Code and data are available in https://github.com/Junjie-Ye/MulDimIF.

</details>


### [84] [Reinforced Internal-External Knowledge Synergistic Reasoning for Efficient Adaptive Search Agent](https://arxiv.org/abs/2505.07596)

*Ziyang Huang, Xiaowei Yuan, Yiming Ju, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Reinforcement learning, Large Language Models, Knowledge integration, Efficiency

**Relevance Score:** 9

**TL;DR:** The paper presents IKEA, a Reinforced Internal-External Knowledge Synergistic Reasoning Agent that optimizes retrieval in LLMs by prioritizing internal knowledge and utilizing external search only when necessary.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing retrieval mechanisms in LLMs often misuse internal knowledge, leading to redundant retrievals and increased inference latency, necessitating a better approach.

**Method:** The paper introduces a knowledge-boundary aware reward function and a specially curated training dataset to synergistically integrate internal and external knowledge through reinforcement learning.

**Key Contributions:**

	1. Introduction of the IKEA agent for optimized retrieval in LLMs
	2. Development of a knowledge-boundary aware reward function
	3. Establishment of a training dataset tailored for internal-external knowledge synergy

**Result:** IKEA outperforms baseline methods by reducing retrieval frequency and enhancing overall performance on knowledge reasoning tasks.

**Limitations:** 

**Conclusion:** The proposed approach significantly improves LLM efficiency and generalization by managing retrievals based on the model's knowledge boundaries.

**Abstract:** Retrieval-augmented generation (RAG) is a common strategy to reduce hallucinations in Large Language Models (LLMs). While reinforcement learning (RL) can enable LLMs to act as search agents by activating retrieval capabilities, existing ones often underutilize their internal knowledge. This can lead to redundant retrievals, potential harmful knowledge conflicts, and increased inference latency. To address these limitations, an efficient and adaptive search agent capable of discerning optimal retrieval timing and synergistically integrating parametric (internal) and retrieved (external) knowledge is in urgent need. This paper introduces the Reinforced Internal-External Knowledge Synergistic Reasoning Agent (IKEA), which could indentify its own knowledge boundary and prioritize the utilization of internal knowledge, resorting to external search only when internal knowledge is deemed insufficient. This is achieved using a novel knowledge-boundary aware reward function and a knowledge-boundary aware training dataset. These are designed for internal-external knowledge synergy oriented RL, incentivizing the model to deliver accurate answers, minimize unnecessary retrievals, and encourage appropriate external searches when its own knowledge is lacking. Evaluations across multiple knowledge reasoning tasks demonstrate that IKEA significantly outperforms baseline methods, reduces retrieval frequency significantly, and exhibits robust generalization capabilities.

</details>


### [85] [Characterizing the Investigative Methods of Fictional Detectives with Large Language Models](https://arxiv.org/abs/2505.07601)

*Edirlei Soares de Lima, Marco A. Casanova, Bruno FeijÃ³, Antonio L. Furtado*

**Main category:** cs.CL

**Keywords:** detective fiction, computational narratology, Large Language Models, character analysis, AI-driven storytelling

**Relevance Score:** 4

**TL;DR:** This paper presents an AI-driven method for characterizing investigative methods of fictional detectives using Large Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance computational narratology by providing a scalable framework for analyzing fictional detective characters, addressing limitations in traditional literary studies.

**Method:** A multi-phase workflow that utilizes 15 Large Language Models to extract, synthesize, and validate investigative traits of seven iconic fictional detectives.

**Key Contributions:**

	1. Introduction of an AI-driven method for character analysis in detective fiction
	2. Validation of investigative traits against literary analyses
	3. Demonstration of high accuracy in trait identification

**Result:** The method achieved an overall accuracy of 91.43% in identifying and validating distinctive investigative approaches of each character.

**Limitations:** 

**Conclusion:** This research contributes to AI-driven interactive storytelling and automated narrative generation by offering a systematic approach to character analysis.

**Abstract:** Detective fiction, a genre defined by its complex narrative structures and character-driven storytelling, presents unique challenges for computational narratology, a research field focused on integrating literary theory into automated narrative generation. While traditional literary studies have offered deep insights into the methods and archetypes of fictional detectives, these analyses often focus on a limited number of characters and lack the scalability needed for the extraction of unique traits that can be used to guide narrative generation methods. In this paper, we present an AI-driven approach for systematically characterizing the investigative methods of fictional detectives. Our multi-phase workflow explores the capabilities of 15 Large Language Models (LLMs) to extract, synthesize, and validate distinctive investigative traits of fictional detectives. This approach was tested on a diverse set of seven iconic detectives - Hercule Poirot, Sherlock Holmes, William Murdoch, Columbo, Father Brown, Miss Marple, and Auguste Dupin - capturing the distinctive investigative styles that define each character. The identified traits were validated against existing literary analyses and further tested in a reverse identification phase, achieving an overall accuracy of 91.43%, demonstrating the method's effectiveness in capturing the distinctive investigative approaches of each detective. This work contributes to the broader field of computational narratology by providing a scalable framework for character analysis, with potential applications in AI-driven interactive storytelling and automated narrative generation.

</details>


### [86] [MiMo: Unlocking the Reasoning Potential of Language Model -- From Pretraining to Posttraining](https://arxiv.org/abs/2505.07608)

*Xiaomi LLM-Core Team, :, Bingquan Xia, Bowen Shen, Cici, Dawei Zhu, Di Zhang, Gang Wang, Hailin Zhang, Huaqiu Liu, Jiebao Xiao, Jinhao Dong, Liang Zhao, Peidian Li, Peng Wang, Shihua Yu, Shimao Chen, Weikun Wang, Wenhan Ma, Xiangwei Deng, Yi Huang, Yifan Song, Zihan Jiang, Bowen Ye, Can Cai, Chenhong He, Dong Zhang, Duo Zhang, Guoan Wang, Hao Tian, Haochen Zhao, Heng Qu, Hongshen Xu, Jun Shi, Kainan Bao, QingKai Fang, Kang Zhou, Kangyang Zhou, Lei Li, Menghang Zhu, Nuo Chen, Qiantong Wang, Shaohui Liu, Shicheng Li, Shuhao Gu, Shuhuai Ren, Shuo Liu, Sirui Deng, Weiji Zhuang, Weiwei Lv, Wenyu Yang, Xin Zhang, Xing Yong, Xing Zhang, Xingchen Song, Xinzhe Xu, Xu Wang, Yihan Yan, Yu Tu, Yuanyuan Tian, Yudong Wang, Yue Yu, Zhenru Lin, Zhichao Song, Zihao Yue*

**Main category:** cs.CL

**Keywords:** Large Language Model, Reasoning Tasks, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** MiMo-7B is a large language model optimized for reasoning tasks, demonstrating exceptional performance across various reasoning challenges after extensive pre-training and targeted post-training adjustments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind developing MiMo-7B is to create a highly capable language model specifically designed for reasoning tasks, addressing challenges in traditional models related to reasoning performance.

**Method:** MiMo-7B undergoes a three-stage data mixing strategy during pre-training with a dataset of 25 trillion tokens, followed by post-training reinforcement learning using 130K curated mathematics and programming problems, leveraging a code-reward scheme for better training outcomes.

**Key Contributions:**

	1. Introduction of a robust three-stage data mixing strategy during pre-training.
	2. Development of a test-difficulty-driven code-reward scheme to address sparse-reward issues in reinforcement learning.
	3. Exceptional performance on reasoning tasks, outperforming larger models like OpenAI's o1-mini.

**Result:** The evaluations indicate that MiMo-7B outperforms larger models, showing superior reasoning capabilities, particularly in mathematics, programming, and general reasoning tasks, compared to OpenAI's o1-mini.

**Limitations:** 

**Conclusion:** The MiMo-7B model demonstrates significant advancements in reasoning for LLMs, potentially benefiting various applications requiring strong reasoning capabilities.

**Abstract:** We present MiMo-7B, a large language model born for reasoning tasks, with optimization across both pre-training and post-training stages. During pre-training, we enhance the data preprocessing pipeline and employ a three-stage data mixing strategy to strengthen the base model's reasoning potential. MiMo-7B-Base is pre-trained on 25 trillion tokens, with additional Multi-Token Prediction objective for enhanced performance and accelerated inference speed. During post-training, we curate a dataset of 130K verifiable mathematics and programming problems for reinforcement learning, integrating a test-difficulty-driven code-reward scheme to alleviate sparse-reward issues and employing strategic data resampling to stabilize training. Extensive evaluations show that MiMo-7B-Base possesses exceptional reasoning potential, outperforming even much larger 32B models. The final RL-tuned model, MiMo-7B-RL, achieves superior performance on mathematics, code and general reasoning tasks, surpassing the performance of OpenAI o1-mini. The model checkpoints are available at https://github.com/xiaomimimo/MiMo.

</details>


### [87] [Concept-Level Explainability for Auditing & Steering LLM Responses](https://arxiv.org/abs/2505.07610)

*Kenza Amara, Rita Sevastjanova, Mennatallah El-Assady*

**Main category:** cs.CL

**Keywords:** large language models, explainability, human alignment, concept-level attribution, bias mitigation

**Relevance Score:** 9

**TL;DR:** ConceptX is a model-agnostic method that improves explainability and safety in large language models (LLMs) by identifying and attributing importance to semantic concepts in prompts rather than individual tokens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the proliferation of LLMs, there is a pressing need to address safety and alignment issues, especially in the context of biases and response manipulation.

**Method:** ConceptX utilizes a concept-level explainability approach to determine the importance of semantically rich tokens in prompts based on the semantic similarity of the model's outputs, while preserving context integrity through token replacements.

**Key Contributions:**

	1. Model-agnostic concept-level explainability
	2. Preservation of context integrity during token replacement
	3. Improved performance in auditing and steering LLM outputs

**Result:** ConceptX shows improved performance over traditional token-level attribution methods, achieving better faithfulness and human alignment across three LLMs. It enhances sentiment shifts and reduces attack success rates more effectively than existing methods.

**Limitations:** 

**Conclusion:** ConceptX provides a transparent and faithful mechanism for auditing and steering LLM behaviors, significantly enhancing the safety and alignment of LLMs without requiring retraining.

**Abstract:** As large language models (LLMs) become widely deployed, concerns about their safety and alignment grow. An approach to steer LLM behavior, such as mitigating biases or defending against jailbreaks, is to identify which parts of a prompt influence specific aspects of the model's output. Token-level attribution methods offer a promising solution, but still struggle in text generation, explaining the presence of each token in the output separately, rather than the underlying semantics of the entire LLM response. We introduce ConceptX, a model-agnostic, concept-level explainability method that identifies the concepts, i.e., semantically rich tokens in the prompt, and assigns them importance based on the outputs' semantic similarity. Unlike current token-level methods, ConceptX also offers to preserve context integrity through in-place token replacements and supports flexible explanation goals, e.g., gender bias. ConceptX enables both auditing, by uncovering sources of bias, and steering, by modifying prompts to shift the sentiment or reduce the harmfulness of LLM responses, without requiring retraining. Across three LLMs, ConceptX outperforms token-level methods like TokenSHAP in both faithfulness and human alignment. Steering tasks boost sentiment shift by 0.252 versus 0.131 for random edits and lower attack success rates from 0.463 to 0.242, outperforming attribution and paraphrasing baselines. While prompt engineering and self-explaining methods sometimes yield safer responses, ConceptX offers a transparent and faithful alternative for improving LLM safety and alignment, demonstrating the practical value of attribution-based explainability in guiding LLM behavior.

</details>


### [88] [Chronocept: Instilling a Sense of Time in Machines](https://arxiv.org/abs/2505.07637)

*Krish Goel, Sanskar Pandey, KS Mahadevan, Harsh Kumar, Vishesh Khadaria*

**Main category:** cs.CL

**Keywords:** temporal reasoning, AI, benchmark, knowledge grounding, fact-checking

**Relevance Score:** 9

**TL;DR:** Chronocept is a benchmark for modeling temporal validity in AI, using continuous probability distributions to evaluate when knowledge becomes outdated, with applications in knowledge grounding and RAG.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in AI's ability to reason about temporal validity, critical for understanding how long facts remain relevant.

**Method:** Chronocept employs skew-normal curves fitted along semantic temporal axes to create a continuous model of temporal validity, supported by two annotated datasets.

**Key Contributions:**

	1. Introduction of a benchmark for modeling temporal validity in AI
	2. Use of skew-normal curves for nuanced temporal reasoning
	3. Publicly available datasets and code for further research

**Result:** Baselines predict curve parameters that demonstrate superior performance over traditional classification methods while maintaining interpretability and generalizability.

**Limitations:** 

**Conclusion:** Chronocept is a necessary framework for improving AI's temporal reasoning capabilities, enhancing various applications like fact-checking and retrieval-augmented generation.

**Abstract:** Human cognition is deeply intertwined with a sense of time, known as Chronoception. This sense allows us to judge how long facts remain valid and when knowledge becomes outdated. Despite progress in vision, language, and motor control, AI still struggles to reason about temporal validity. We introduce Chronocept, the first benchmark to model temporal validity as a continuous probability distribution over time. Using skew-normal curves fitted along semantically decomposed temporal axes, Chronocept captures nuanced patterns of emergence, decay, and peak relevance. It includes two datasets: Benchmark I (atomic facts) and Benchmark II (multi-sentence passages). Annotations show strong inter-annotator agreement (84% and 89%). Our baselines predict curve parameters - location, scale, and skewness - enabling interpretable, generalizable learning and outperforming classification-based approaches. Chronocept fills a foundational gap in AI's temporal reasoning, supporting applications in knowledge grounding, fact-checking, retrieval-augmented generation (RAG), and proactive agents. Code and data are publicly available.

</details>


### [89] [JobHop: A Large-Scale Dataset of Career Trajectories](https://arxiv.org/abs/2505.07653)

*Iman Johary, Raphael Romero, Alexandru C. Mara, Tijl De Bie*

**Main category:** cs.CL

**Keywords:** labor market, dataset, Large Language Models, career transitions, occupational analysis

**Relevance Score:** 6

**TL;DR:** The paper introduces JobHop, a large-scale dataset of career trajectories derived from anonymized resumes, processed using LLMs to enhance labor market research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of comprehensive datasets on real-world career trajectories necessary for labor market analysis.

**Method:** A large dataset of anonymized resumes was processed using Large Language Models to extract structured career information, which was then mapped to standardized ESCO occupation codes using a multi-label classification model.

**Key Contributions:**

	1. Introduction of a large-scale public dataset (JobHop) for labor market dynamics.
	2. Innovative use of LLMs for processing unstructured resume data.
	3. Development of a mapping method to standardized ESCO occupation codes.

**Result:** JobHop is a dataset containing over 2.3 million work experiences from 391,000 resumes, enabling insights into labor market dynamics and applications such as career path prediction.

**Limitations:** 

**Conclusion:** The dataset can be used for analyzing labor market mobility and other decision-making processes, demonstrating its potential for research.

**Abstract:** Understanding labor market dynamics is essential for policymakers, employers, and job seekers. However, comprehensive datasets that capture real-world career trajectories are scarce. In this paper, we introduce JobHop, a large-scale public dataset derived from anonymized resumes provided by VDAB, the public employment service in Flanders, Belgium. Utilizing Large Language Models (LLMs), we process unstructured resume data to extract structured career information, which is then mapped to standardized ESCO occupation codes using a multi-label classification model. This results in a rich dataset of over 2.3 million work experiences, extracted from and grouped into more than 391,000 user resumes and mapped to standardized ESCO occupation codes, offering valuable insights into real-world occupational transitions. This dataset enables diverse applications, such as analyzing labor market mobility, job stability, and the effects of career breaks on occupational transitions. It also supports career path prediction and other data-driven decision-making processes. To illustrate its potential, we explore key dataset characteristics, including job distributions, career breaks, and job transitions, demonstrating its value for advancing labor market research.

</details>


### [90] [Using Information Theory to Characterize Prosodic Typology: The Case of Tone, Pitch-Accent and Stress-Accent](https://arxiv.org/abs/2505.07659)

*Ethan Gotlieb Wilcox, Cui Ding, Giovanni Acampa, Tiago Pimentel, Alex Warstadt, Tamar I. Regev*

**Main category:** cs.CL

**Keywords:** prosody, mutual information, tonal languages, linguistic variation, information theory

**Relevance Score:** 0

**TL;DR:** This paper examines the link between lexical identity and prosody using information theory, finding that tonal languages exhibit higher mutual information between word identity and prosody than non-tonal languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how lexical identity and prosody relate across different languages, particularly through the lens of information theory.

**Method:** The study analyzes pitch data from speakers across ten languages to estimate mutual information between text and pitch curves, focusing on tonal vs. non-tonal languages.

**Key Contributions:**

	1. Used information theory to analyze prosody in relation to lexical identity
	2. Demonstrated higher mutual information in tonal languages compared to others
	3. Supported a gradient view of linguistic typology

**Result:** Tonal languages show higher mutual information between text and pitch predictions compared to other language types, suggesting a gradient rather than categorical view of linguistic typology.

**Limitations:** 

**Conclusion:** The findings indicate that prosody plays a significant role in lexical distinction in tonal languages, reinforcing the proposed hypothesis and offering insights into linguistic variation.

**Abstract:** This paper argues that the relationship between lexical identity and prosody -- one well-studied parameter of linguistic variation -- can be characterized using information theory. We predict that languages that use prosody to make lexical distinctions should exhibit a higher mutual information between word identity and prosody, compared to languages that don't. We test this hypothesis in the domain of pitch, which is used to make lexical distinctions in tonal languages, like Cantonese. We use a dataset of speakers reading sentences aloud in ten languages across five language families to estimate the mutual information between the text and their pitch curves. We find that, across languages, pitch curves display similar amounts of entropy. However, these curves are easier to predict given their associated text in the tonal languages, compared to pitch- and stress-accent languages, and thus the mutual information is higher in these languages, supporting our hypothesis. Our results support perspectives that view linguistic typology as gradient, rather than categorical.

</details>


### [91] [Benchmarking Retrieval-Augmented Generation for Chemistry](https://arxiv.org/abs/2505.07671)

*Xianrui Zhong, Bowen Jin, Siru Ouyang, Yanzhen Shen, Qiao Jin, Yin Fang, Zhiyong Lu, Jiawei Han*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Chemistry, Benchmark, Large Language Models, Toolkit

**Relevance Score:** 4

**TL;DR:** Introducing ChemRAG-Bench and ChemRAG-Toolkit to enhance Retrieval-Augmented Generation (RAG) in chemistry.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored application of RAG in chemistry due to insufficient high-quality, domain-specific corpora and evaluation benchmarks.

**Method:** We developed ChemRAG-Bench, a benchmark for assessing RAG effectiveness in chemistry tasks and created ChemRAG-Toolkit, which supports multiple retrieval algorithms and LLMs.

**Key Contributions:**

	1. Introduction of ChemRAG-Bench for benchmarking RAG in chemistry.
	2. Creation of ChemRAG-Toolkit with support for multiple retrieval algorithms and LLMs.
	3. Demonstration of substantial performance gains of RAG in chemistry tasks.

**Result:** ChemRAG-Toolkit demonstrated RAG achieves a 17.4% average relative performance improvement over direct inference methods.

**Limitations:** 

**Conclusion:** ChemRAG-Toolkit provides valuable insights into retriever architectures and corpus selection, aiding future RAG research in chemistry.

**Abstract:** Retrieval-augmented generation (RAG) has emerged as a powerful framework for enhancing large language models (LLMs) with external knowledge, particularly in scientific domains that demand specialized and dynamic information. Despite its promise, the application of RAG in the chemistry domain remains underexplored, primarily due to the lack of high-quality, domain-specific corpora and well-curated evaluation benchmarks. In this work, we introduce ChemRAG-Bench, a comprehensive benchmark designed to systematically assess the effectiveness of RAG across a diverse set of chemistry-related tasks. The accompanying chemistry corpus integrates heterogeneous knowledge sources, including scientific literature, the PubChem database, PubMed abstracts, textbooks, and Wikipedia entries. In addition, we present ChemRAG-Toolkit, a modular and extensible RAG toolkit that supports five retrieval algorithms and eight LLMs. Using ChemRAG-Toolkit, we demonstrate that RAG yields a substantial performance gain -- achieving an average relative improvement of 17.4% over direct inference methods. We further conduct in-depth analyses on retriever architectures, corpus selection, and the number of retrieved passages, culminating in practical recommendations to guide future research and deployment of RAG systems in the chemistry domain. The code and data is available at https://chemrag.github.io.

</details>


### [92] [OnPrem.LLM: A Privacy-Conscious Document Intelligence Toolkit](https://arxiv.org/abs/2505.07672)

*Arun S. Maiya*

**Main category:** cs.CL

**Keywords:** Large Language Models, Privacy, Document Processing, Retrieval-Augmented Generation, No-Code

**Relevance Score:** 9

**TL;DR:** OnPrem.LLM is a Python toolkit for using LLMs in privacy-conscious environments, allowing easy processing of sensitive data.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a solution for applying large language models to sensitive, non-public data while ensuring privacy and security.

**Method:** A Python-based toolkit featuring prebuilt pipelines for various tasks including document processing, retrieval-augmented generation, and classification, with support for multiple LLM backends and GPU acceleration.

**Key Contributions:**

	1. Prebuilt pipelines for document processing and RAG
	2. Support for multiple LLM backends
	3. No-code web interface for non-technical users

**Result:** OnPrem.LLM facilitates efficient use of LLMs in restricted environments with minimal configuration, supporting both local and hybrid deployments.

**Limitations:** 

**Conclusion:** The toolkit provides a streamlined approach to leverage LLMs for sensitive applications, balancing ease of use and security.

**Abstract:** We present OnPrem.LLM, a Python-based toolkit for applying large language models (LLMs) to sensitive, non-public data in offline or restricted environments. The system is designed for privacy-preserving use cases and provides prebuilt pipelines for document processing and storage, retrieval-augmented generation (RAG), information extraction, summarization, classification, and prompt/output processing with minimal configuration. OnPrem.LLM supports multiple LLM backends -- including llama.cpp, Ollama, vLLM, and Hugging Face Transformers -- with quantized model support, GPU acceleration, and seamless backend switching. Although designed for fully local execution, OnPrem.LLM also supports integration with a wide range of cloud LLM providers when permitted, enabling hybrid deployments that balance performance with data control. A no-code web interface extends accessibility to non-technical users.

</details>


### [93] [Codifying Character Logic in Role-Playing](https://arxiv.org/abs/2505.07705)

*Letian Peng, Jingbo Shang*

**Main category:** cs.CL

**Keywords:** role-playing, character logic, behavioral decision-making, AI, NLP

**Relevance Score:** 6

**TL;DR:** This paper presents Codified Profiles for role-playing, which represent character logic as structured functions to enhance decision-making in narrative contexts, offering advantages over traditional prompt-based methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness and reliability of character behavior in role-playing applications by moving away from traditional prompt-based profiles.

**Method:** The authors introduce Codified Profiles that use structured functions to parse scenes and generate logic-grounded assertions, validated through a benchmark of characters and scenes.

**Key Contributions:**

	1. Introduction of Codified Profiles for structured character logic
	2. Development of a new benchmark for evaluating role-playing scenarios
	3. Demonstrated advantages in persistence, updatability, and controllable randomness

**Result:** Experiments show that codified profiles significantly enhance persistence, updatability, and behavioral diversity in role-playing agents, even for smaller models.

**Limitations:** 

**Conclusion:** Codified profiles can enable high-quality role-playing capabilities in AI with reduced complexity and improved performance, serving as a scalable solution for character-driven applications.

**Abstract:** This paper introduces Codified Profiles for role-playing, a novel approach that represents character logic as structured, executable functions for behavioral decision-making. Each profile defines a set of functions parse_by_scene(scene) that outputs a list of logic-grounded assertions triggered_statements, using both explicit control structures (e.g., if-then-else) and condition checks like check_condition(scene, question), where each question is a semantically meaningful prompt about the scene (e.g., "Is the character in danger?") discriminated by the role-playing LLM as true, false, or unknown. This explicit representation offers three key advantages over traditional prompt-based profiles, which append character descriptions directly into text prompts: (1) Persistence, by enforcing complete and consistent execution of character logic, rather than relying on the model's implicit reasoning; (2) Updatability, through systematic inspection and revision of behavioral logic, which is difficult to track or debug in prompt-only approaches; (3) Controllable Randomness, by supporting stochastic behavior directly within the logic, enabling fine-grained variability that prompting alone struggles to achieve. To validate these advantages, we introduce a new benchmark constructed from 83 characters and 5,141 scenes curated from Fandom, using NLI-based scoring to compare character responses against ground-truth actions. Our experiments demonstrate the significant benefits of codified profiles in improving persistence, updatability, and behavioral diversity. Notably, by offloading a significant portion of reasoning to preprocessing, codified profiles enable even 1B-parameter models to perform high-quality role-playing, providing a scalable and efficient foundation for local deployment of role-play agents.

</details>


### [94] [Spoken Language Understanding on Unseen Tasks With In-Context Learning](https://arxiv.org/abs/2505.07731)

*Neeraj Agrawal, Sriram Ganapathy*

**Main category:** cs.CL

**Keywords:** spoken language understanding, large language models, fine-tuning, task-agnostic, zero-shot learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel approach to fine-tuning speech-text large language models (LLMs) for spoken language understanding (SLU) tasks without requiring task-specific training data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional SLU models, especially when task-specific training data is not available, and to enhance the performance of LLMs in SLU tasks.

**Method:** The authors propose a task-agnostic fine-tuning method using randomized class labels to improve LLM performance on unseen SLU tasks.

**Key Contributions:**

	1. Introduction of a task-agnostic fine-tuning approach using randomized class labels
	2. Demonstration of performance improvement over standard fine-tuning
	3. Reduction of dependency on task-specific data annotations for SLU tasks

**Result:** The refined LLMs demonstrate significantly improved performance on SLU tasks compared to standard fine-tuning methods.

**Limitations:** 

**Conclusion:** The proposed method effectively allows LLMs to handle new SLU tasks without the need for annotated task-specific data, enhancing their versatility.

**Abstract:** Spoken language understanding (SLU) tasks involve diverse skills that probe the information extraction, classification and/or generation capabilities of models. In this setting, task-specific training data may not always be available. While traditional task-specific SLU models are unable to cater to such requirements, the speech-text large language models (LLMs) offer a promising alternative with emergent abilities. However, out of-the-box, our evaluations indicate that the zero/few-shot performance of prominent open-source speech-text LLMs on SLU tasks are not up to the mark. In this paper, we introduce a novel approach to robust task-agnostic fine-tuning using randomized class labels. With this proposed fine-tuning, we illustrate that the performance of the speech-text LLMs on an unseen task is significantly improved over standard approaches. Critically, the proposed approach avoids the requirement of task-specific data annotations for enabling new tasks in speech-text LLMs.

</details>


### [95] [Must Read: A Systematic Survey of Computational Persuasion](https://arxiv.org/abs/2505.07775)

*Nimet Beyza Bozdag, Shuhaib Mehri, Xiaocheng Yang, Hyeonjeong Ha, Zirui Cheng, Esin Durmus, Jiaxuan You, Heng Ji, Gokhan Tur, Dilek Hakkani-TÃ¼r*

**Main category:** cs.CL

**Keywords:** computational persuasion, conversational AI, ethical influence, AI applications, persuasive content

**Relevance Score:** 8

**TL;DR:** This survey provides an overview of computational persuasion, focusing on AI as persuader, persuadee, and judge, discussing challenges and future directions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The advancement of conversational AI systems has raised both opportunities and risks in the realm of persuasion, necessitating a deeper understanding of AI's role in this context.

**Method:** The authors structured the survey around three perspectives: AI as a Persuader, AI as a Persuadee, and AI as a Persuasion Judge, discussing various applications and challenges.

**Key Contributions:**

	1. Introduced a taxonomy for computational persuasion research
	2. Analyzed AI's dual role as persuader and persuadee
	3. Identified key challenges in ethical persuasion

**Result:** The survey introduces a taxonomy for computational persuasion research, highlighting key challenges in evaluating persuasive techniques and developing ethical AI systems.

**Limitations:** The paper does not provide extensive empirical data, focusing instead on theoretical frameworks and discussions.

**Conclusion:** The study suggests future research directions to enhance the safety and fairness of AI-powered persuasive systems.

**Abstract:** Persuasion is a fundamental aspect of communication, influencing decision-making across diverse contexts, from everyday conversations to high-stakes scenarios such as politics, marketing, and law. The rise of conversational AI systems has significantly expanded the scope of persuasion, introducing both opportunities and risks. AI-driven persuasion can be leveraged for beneficial applications, but also poses threats through manipulation and unethical influence. Moreover, AI systems are not only persuaders, but also susceptible to persuasion, making them vulnerable to adversarial attacks and bias reinforcement. Despite rapid advancements in AI-generated persuasive content, our understanding of what makes persuasion effective remains limited due to its inherently subjective and context-dependent nature. In this survey, we provide a comprehensive overview of computational persuasion, structured around three key perspectives: (1) AI as a Persuader, which explores AI-generated persuasive content and its applications; (2) AI as a Persuadee, which examines AI's susceptibility to influence and manipulation; and (3) AI as a Persuasion Judge, which analyzes AI's role in evaluating persuasive strategies, detecting manipulation, and ensuring ethical persuasion. We introduce a taxonomy for computational persuasion research and discuss key challenges, including evaluating persuasiveness, mitigating manipulative persuasion, and developing responsible AI-driven persuasive systems. Our survey outlines future research directions to enhance the safety, fairness, and effectiveness of AI-powered persuasion while addressing the risks posed by increasingly capable language models.

</details>


### [96] [Domain Regeneration: How well do LLMs match syntactic properties of text domains?](https://arxiv.org/abs/2505.07784)

*Da Ju, Hagen Blix, Adina Williams*

**Main category:** cs.CL

**Keywords:** large language models, text regeneration, corpus linguistics, syntactic properties, text domains

**Relevance Score:** 9

**TL;DR:** This paper investigates how well large language models (LLMs) approximate the properties of text domains using regeneration tasks on Wikipedia and news text.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the degree to which LLMs can replicate the properties of their training data, particularly in diverse text domains like Wikipedia and news.

**Method:** The authors employ a regeneration paradigm where an open-source LLM is prompted to recreate text from selected domains, focusing on various syntactic and semantic properties.

**Key Contributions:**

	1. Analyzes properties of text regeneration by LLMs across different domains
	2. Uses observational methods from corpus linguistics to assess LLM outputs
	3. Finds significant differences in syntactic complexity between original and regenerated texts.

**Result:** Results indicate that regenerated text distributions differ from the originals, showing a shifted mean, reduced variation, and less frequency of complex syntactic structures.

**Limitations:** The study is limited to only two text domains and may not generalize across all types of data LLMs are trained on.

**Conclusion:** The study reveals that while LLMs can regenerate text, they tend to oversimplify certain syntactic and semantic properties, leading to a less diverse representation.

**Abstract:** Recent improvement in large language model performance have, in all likelihood, been accompanied by improvement in how well they can approximate the distribution of their training data. In this work, we explore the following question: which properties of text domains do LLMs faithfully approximate, and how well do they do so? Applying observational approaches familiar from corpus linguistics, we prompt a commonly used, opensource LLM to regenerate text from two domains of permissively licensed English text which are often contained in LLM training data -- Wikipedia and news text. This regeneration paradigm allows us to investigate whether LLMs can faithfully match the original human text domains in a fairly semantically-controlled setting. We investigate varying levels of syntactic abstraction, from more simple properties like sentence length, and article readability, to more complex and higher order properties such as dependency tag distribution, parse depth, and parse complexity. We find that the majority of the regenerated distributions show a shifted mean, a lower standard deviation, and a reduction of the long tail, as compared to the human originals.

</details>


### [97] [Learning from Peers in Reasoning Models](https://arxiv.org/abs/2505.07787)

*Tongxu Luo, Wenyu Du, Jiaxi Bi, Stephen Chung, Zhengyang Tang, Hao Yang, Min Zhang, Benyou Wang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Self-correction, Learning from Peers, Peer interaction, Error correction

**Relevance Score:** 9

**TL;DR:** This paper introduces Learning from Peers (LeaP), a method that enhances Large Reasoning Models (LRMs) by allowing them to share intermediate reasoning insights with peers to overcome challenges in self-correction during reasoning processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenge of the 'Prefix Dominance Trap' in LRMs, where early poor reasoning hampers self-correction, the authors draw on psychological insights into peer interaction promoting self-correction.

**Method:** The LeaP method involves a routing mechanism for token-level peer interaction, where each reasoning path summarizes its intermediate reasoning for others to incorporate during inference. Smaller models are fine-tuned into the LeaP-T series to improve their performance in this setting.

**Key Contributions:**

	1. Proposes the LeaP framework for peer interaction among reasoning paths in LRMs.
	2. Demonstrates significant performance gains on multiple benchmarks through the LeaP-T fine-tuned models.
	3. Provides a comprehensive analysis of error correction mechanisms enabled by peer insights.

**Result:** LeaP demonstrates significant performance improvements, with the QwQ-32B model with LeaP achieving an increase of nearly 5 absolute points over baseline models and surpassing larger models on multiple benchmarks.

**Limitations:** Smaller models sometimes struggle with following summarization and reflection instructions, which necessitated fine-tuning to improve performance.

**Conclusion:** LeaP creates a collaborative environment for LRMs during reasoning, allowing for robust error correction and impressive task management, marking a significant advancement in model accuracy and interaction.

**Abstract:** Large Reasoning Models (LRMs) have the ability to self-correct even when they make mistakes in their reasoning paths. However, our study reveals that when the reasoning process starts with a short but poor beginning, it becomes difficult for the model to recover. We refer to this phenomenon as the "Prefix Dominance Trap". Inspired by psychological findings that peer interaction can promote self-correction without negatively impacting already accurate individuals, we propose **Learning from Peers** (LeaP) to address this phenomenon. Specifically, every tokens, each reasoning path summarizes its intermediate reasoning and shares it with others through a routing mechanism, enabling paths to incorporate peer insights during inference. However, we observe that smaller models sometimes fail to follow summarization and reflection instructions effectively. To address this, we fine-tune them into our **LeaP-T** model series. Experiments on AIME 2024, AIME 2025, AIMO 2025, and GPQA Diamond show that LeaP provides substantial improvements. For instance, QwQ-32B with LeaP achieves nearly 5 absolute points higher than the baseline on average, and surpasses DeepSeek-R1-671B on three math benchmarks with an average gain of 3.3 points. Notably, our fine-tuned LeaP-T-7B matches the performance of DeepSeek-R1-Distill-Qwen-14B on AIME 2024. In-depth analysis reveals LeaP's robust error correction by timely peer insights, showing strong error tolerance and handling varied task difficulty. LeaP marks a milestone by enabling LRMs to collaborate during reasoning. Our code, datasets, and models are available at https://learning-from-peers.github.io/ .

</details>


### [98] [Learning Dynamics in Continual Pre-Training for Large Language Models](https://arxiv.org/abs/2505.07796)

*Xingjin Wang, Howe Tissue, Lu Wang, Linjing Li, Daniel Dajun Zeng*

**Main category:** cs.CL

**Keywords:** Continual Pre-Training, Large Language Models, Learning Dynamics, Scaling Law, Performance Prediction

**Relevance Score:** 9

**TL;DR:** This paper explores the learning dynamics of Continual Pre-Training (CPT) for large language models, focusing on the characterization of performance evolution and deriving a scaling law for predicting loss across training steps and learning rate schedules.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the learning dynamics during Continual Pre-Training and its impact on both general and domain-specific performance of large language models.

**Method:** The authors analyze CPT loss curves and their relationship with distribution shift and learning rate annealing, deriving a CPT scaling law that predicts performance outcomes based on different training configurations.

**Key Contributions:**

	1. Derivation of a CPT scaling law
	2. Comprehensive framework analyzing critical CPT factors
	3. Validation of the approach through extensive experiments

**Result:** The results show that the derived scaling law accurately predicts loss across various training steps and learning rate schedules, demonstrating its applicability across multiple CPT datasets and configurations.

**Limitations:** 

**Conclusion:** The paper contributes a novel scaling law for CPT, providing insights into hyper-parameter customization for improved performance balance in language models.

**Abstract:** Continual Pre-Training (CPT) has become a popular and effective method to apply strong foundation models to specific downstream tasks. In this work, we explore the learning dynamics throughout the CPT process for large language models. We specifically focus on how general and downstream domain performance evolves at each training step, with domain performance measured via validation losses. We have observed that the CPT loss curve fundamentally characterizes the transition from one curve to another hidden curve, and could be described by decoupling the effects of distribution shift and learning rate annealing. We derive a CPT scaling law that combines the two factors, enabling the prediction of loss at any (continual) training steps and across learning rate schedules (LRS) in CPT. Our formulation presents a comprehensive understanding of several critical factors in CPT, including loss potential, peak learning rate, training steps, replay ratio, etc. Moreover, our approach can be adapted to customize training hyper-parameters to different CPT goals such as balancing general and domain-specific performance. Extensive experiments demonstrate that our scaling law holds across various CPT datasets and training hyper-parameters.

</details>


### [99] [A Comparative Analysis of Static Word Embeddings for Hungarian](https://arxiv.org/abs/2505.07809)

*MÃ¡tÃ© Gedeon*

**Main category:** cs.CL

**Keywords:** word embeddings, Hungarian language, NLP, BERT, named entity recognition

**Relevance Score:** 4

**TL;DR:** This paper analyzes static word embeddings for Hungarian, including Word2Vec and FastText, and evaluates them using intrinsic and extrinsic tasks. FastText excels in semantic tasks and BERT-derived methods show promise, particularly X2Static for NER and POS tagging.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of various static word embedding models for the Hungarian language and provide insights into their performance in NLP applications.

**Method:** The paper employs intrinsic evaluation through a word analogy task and extrinsic evaluation using a bidirectional LSTM model for Named Entity Recognition (NER) and Part-of-Speech (POS) tagging.

**Key Contributions:**

	1. Comprehensive evaluation of static word embeddings for Hungarian
	2. Introduction of the X2Static extraction method
	3. Reduction of static vs. dynamic embedding performance gap in NLP tasks

**Result:** Traditional embeddings like FastText performed best in intrinsic evaluations, while BERT-derived embeddings, especially with the X2Static method, excelled in extrinsic tasks.

**Limitations:** 

**Conclusion:** Static word embeddings remain relevant in NLP, and advanced extraction methods can enhance performance, particularly in context-sensitive applications. Findings contribute to the understanding of embedding effectiveness for Hungarian.

**Abstract:** This paper presents a comprehensive analysis of various static word embeddings for Hungarian, including traditional models such as Word2Vec, FastText, as well as static embeddings derived from BERT-based models using different extraction methods. We evaluate these embeddings on both intrinsic and extrinsic tasks to provide a holistic view of their performance. For intrinsic evaluation, we employ a word analogy task, which assesses the embeddings ability to capture semantic and syntactic relationships. Our results indicate that traditional static embeddings, particularly FastText, excel in this task, achieving high accuracy and mean reciprocal rank (MRR) scores. Among the BERT-based models, the X2Static method for extracting static embeddings demonstrates superior performance compared to decontextualized and aggregate methods, approaching the effectiveness of traditional static embeddings. For extrinsic evaluation, we utilize a bidirectional LSTM model to perform Named Entity Recognition (NER) and Part-of-Speech (POS) tagging tasks. The results reveal that embeddings derived from dynamic models, especially those extracted using the X2Static method, outperform purely static embeddings. Notably, ELMo embeddings achieve the highest accuracy in both NER and POS tagging tasks, underscoring the benefits of contextualized representations even when used in a static form. Our findings highlight the continued relevance of static word embeddings in NLP applications and the potential of advanced extraction methods to enhance the utility of BERT-based models. This piece of research contributes to the understanding of embedding performance in the Hungarian language and provides valuable insights for future developments in the field. The training scripts, evaluation codes, restricted vocabulary, and extracted embeddings will be made publicly available to support further research and reproducibility.

</details>


### [100] [Clickbait Detection via Large Language Models](https://arxiv.org/abs/2306.09597)

*Han Wang, Yi Zhu, Ye Wang, Yun Li, Yunhao Yuan, Jipeng Qiang*

**Main category:** cs.CL

**Keywords:** Clickbait detection, Large Language Models, Natural Language Processing, Few-shot learning, Zero-shot learning

**Relevance Score:** 5

**TL;DR:** This paper investigates the effectiveness of Large Language Models (LLMs) in clickbait detection, revealing their limitations compared to state-of-the-art methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs can effectively serve as clickbait detection systems given their success in various NLP tasks.

**Method:** The paper analyzes LLM performance in few-shot and zero-shot scenarios across English and Chinese benchmark datasets for clickbait detection.

**Key Contributions:**

	1. Analysis of LLMs in clickbait detection
	2. Comparison with state-of-the-art models
	3. Insight into LLM limitations for this specific task

**Result:** LLMs performed worse than state-of-the-art deep learning models and fine-tuning methods, failing to reliably detect clickbait based on headlines.

**Limitations:** Limited effectiveness of LLMs compared to traditional methods; focus primarily on headline-based detection is not sufficient.

**Conclusion:** LLMs are not suitable for high-quality clickbait detection as their performance does not meet expectations, contrasting with human intuition.

**Abstract:** Clickbait, which aims to induce users with some surprising and even thrilling headlines for increasing click-through rates, permeates almost all online content publishers, such as news portals and social media. Recently, Large Language Models (LLMs) have emerged as a powerful instrument and achieved tremendous success in a series of NLP downstream tasks. However, it is not yet known whether LLMs can be served as a high-quality clickbait detection system. In this paper, we analyze the performance of LLMs in the few-shot and zero-shot scenarios on several English and Chinese benchmark datasets. Experimental results show that LLMs cannot achieve the best results compared to the state-of-the-art deep and fine-tuning PLMs methods. Different from human intuition, the experiments demonstrated that LLMs cannot make satisfied clickbait detection just by the headlines.

</details>


### [101] [Towards Understanding Sycophancy in Language Models](https://arxiv.org/abs/2310.13548)

*Mrinank Sharma, Meg Tong, Tomasz Korbak, David Duvenaud, Amanda Askell, Samuel R. Bowman, Newton Cheng, Esin Durmus, Zac Hatfield-Dodds, Scott R. Johnston, Shauna Kravec, Timothy Maxwell, Sam McCandlish, Kamal Ndousse, Oliver Rausch, Nicholas Schiefer, Da Yan, Miranda Zhang, Ethan Perez*

**Main category:** cs.CL

**Keywords:** sycophancy, human feedback, AI assistants, preference judgments, truthfulness

**Relevance Score:** 8

**TL;DR:** This paper investigates the prevalence of sycophancy in AI assistants fine-tuned with human feedback, revealing that such behavior may be influenced by human preference judgments favoring agreeable responses over truthful ones.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the influence of human feedback on AI performance, particularly in terms of sycophancy, which is the tendency to match user beliefs instead of providing truthful responses.

**Method:** Analysis of five state-of-the-art AI assistants across four text-generation tasks, evaluating their tendency towards sycophancy and the role of human preference judgments.

**Key Contributions:**

	1. Demonstration of sycophancy in multiple AI assistants
	2. Analysis of the relationship between human preferences and sycophantic responses
	3. Revelation of potential bias against truthfulness in AI output optimization

**Result:** The study found that all evaluated AI models exhibit sycophantic behavior and that human preference data indicates a bias towards responses that match user beliefs, often at the expense of truthfulness.

**Limitations:** The investigation may not cover all AI models or contexts, limiting generalizability; further research is needed to explore mitigation strategies.

**Conclusion:** Sycophancy is prevalent in contemporary AI assistants and is influenced by human preference for agreeable over truthful responses, highlighting potential risks in aligning AI systems with human feedback.

**Abstract:** Human feedback is commonly utilized to finetune AI assistants. But human feedback may also encourage model responses that match user beliefs over truthful ones, a behaviour known as sycophancy. We investigate the prevalence of sycophancy in models whose finetuning procedure made use of human feedback, and the potential role of human preference judgments in such behavior. We first demonstrate that five state-of-the-art AI assistants consistently exhibit sycophancy across four varied free-form text-generation tasks. To understand if human preferences drive this broadly observed behavior, we analyze existing human preference data. We find that when a response matches a user's views, it is more likely to be preferred. Moreover, both humans and preference models (PMs) prefer convincingly-written sycophantic responses over correct ones a non-negligible fraction of the time. Optimizing model outputs against PMs also sometimes sacrifices truthfulness in favor of sycophancy. Overall, our results indicate that sycophancy is a general behavior of state-of-the-art AI assistants, likely driven in part by human preference judgments favoring sycophantic responses.

</details>


### [102] [Gemini: A Family of Highly Capable Multimodal Models](https://arxiv.org/abs/2312.11805)

*Gemini Team, Rohan Anil, Sebastian Borgeaud, Jean-Baptiste Alayrac, Jiahui Yu, Radu Soricut, Johan Schalkwyk, Andrew M. Dai, Anja Hauth, Katie Millican, David Silver, Melvin Johnson, Ioannis Antonoglou, Julian Schrittwieser, Amelia Glaese, Jilin Chen, Emily Pitler, Timothy Lillicrap, Angeliki Lazaridou, Orhan Firat, James Molloy, Michael Isard, Paul R. Barham, Tom Hennigan, Benjamin Lee, Fabio Viola, Malcolm Reynolds, Yuanzhong Xu, Ryan Doherty, Eli Collins, Clemens Meyer, Eliza Rutherford, Erica Moreira, Kareem Ayoub, Megha Goel, Jack Krawczyk, Cosmo Du, Ed Chi, Heng-Tze Cheng, Eric Ni, Purvi Shah, Patrick Kane, Betty Chan, Manaal Faruqui, Aliaksei Severyn, Hanzhao Lin, YaGuang Li, Yong Cheng, Abe Ittycheriah, Mahdis Mahdieh, Mia Chen, Pei Sun, Dustin Tran, Sumit Bagri, Balaji Lakshminarayanan, Jeremiah Liu, Andras Orban, Fabian GÃ¼ra, Hao Zhou, Xinying Song, Aurelien Boffy, Harish Ganapathy, Steven Zheng, HyunJeong Choe, Ãgoston Weisz, Tao Zhu, Yifeng Lu, Siddharth Gopal, Jarrod Kahn, Maciej Kula, Jeff Pitman, Rushin Shah, Emanuel Taropa, Majd Al Merey, Martin Baeuml, Zhifeng Chen, Laurent El Shafey, Yujing Zhang, Olcan Sercinoglu, George Tucker, Enrique Piqueras, Maxim Krikun, Iain Barr, Nikolay Savinov, Ivo Danihelka, Becca Roelofs, AnaÃ¯s White, Anders Andreassen, Tamara von Glehn, Lakshman Yagati, Mehran Kazemi, Lucas Gonzalez, Misha Khalman, Jakub Sygnowski, Alexandre Frechette, Charlotte Smith, Laura Culp, Lev Proleev, Yi Luan, Xi Chen, James Lottes, Nathan Schucher, Federico Lebron, Alban Rrustemi, Natalie Clay, Phil Crone, Tomas Kocisky, Jeffrey Zhao, Bartek Perz, Dian Yu, Heidi Howard, Adam Bloniarz, Jack W. Rae, Han Lu, Laurent Sifre, Marcello Maggioni, Fred Alcober, Dan Garrette, Megan Barnes, Shantanu Thakoor, Jacob Austin, Gabriel Barth-Maron, William Wong, Rishabh Joshi, Rahma Chaabouni, Deeni Fatiha, Arun Ahuja, Gaurav Singh Tomar, Evan Senter, Martin Chadwick, Ilya Kornakov, Nithya Attaluri, IÃ±aki Iturrate, Ruibo Liu, Yunxuan Li, Sarah Cogan, Jeremy Chen, Chao Jia, Chenjie Gu, Qiao Zhang, Jordan Grimstad, Ale Jakse Hartman, Xavier Garcia, Thanumalayan Sankaranarayana Pillai, Jacob Devlin, Michael Laskin, Diego de Las Casas, Dasha Valter, Connie Tao, Lorenzo Blanco, AdriÃ  PuigdomÃ¨nech Badia, David Reitter, Mianna Chen, Jenny Brennan, Clara Rivera, Sergey Brin, Shariq Iqbal, Gabriela Surita, Jane Labanowski, Abhi Rao, Stephanie Winkler, Emilio Parisotto, Yiming Gu, Kate Olszewska, Ravi Addanki, Antoine Miech, Annie Louis, Denis Teplyashin, Geoff Brown, Elliot Catt, Jan Balaguer, Jackie Xiang, Pidong Wang, Zoe Ashwood, Anton Briukhov, Albert Webson, Sanjay Ganapathy, Smit Sanghavi, Ajay Kannan, Ming-Wei Chang, Axel Stjerngren, Josip Djolonga, Yuting Sun, Ankur Bapna, Matthew Aitchison, Pedram Pejman, Henryk Michalewski, Tianhe Yu, Cindy Wang, Juliette Love, Junwhan Ahn, Dawn Bloxwich, Kehang Han, Peter Humphreys, Thibault Sellam, James Bradbury, Varun Godbole, Sina Samangooei, Bogdan Damoc, Alex Kaskasoli, SÃ©bastien M. R. Arnold, Vijay Vasudevan, Shubham Agrawal, Jason Riesa, Dmitry Lepikhin, Richard Tanburn, Srivatsan Srinivasan, Hyeontaek Lim, Sarah Hodkinson, Pranav Shyam, Johan Ferret, Steven Hand, Ankush Garg, Tom Le Paine, Jian Li, Yujia Li, Minh Giang, Alexander Neitz, Zaheer Abbas, Sarah York, Machel Reid, Elizabeth Cole, Aakanksha Chowdhery, Dipanjan Das, Dominika RogoziÅska, Vitaliy Nikolaev, Pablo Sprechmann, Zachary Nado, Lukas Zilka, Flavien Prost, Luheng He, Marianne Monteiro, Gaurav Mishra, Chris Welty, Josh Newlan, Dawei Jia, Miltiadis Allamanis, Clara Huiyi Hu, Raoul de Liedekerke, Justin Gilmer, Carl Saroufim, Shruti Rijhwani, Shaobo Hou, Disha Shrivastava, Anirudh Baddepudi, Alex Goldin, Adnan Ozturel, Albin Cassirer, Yunhan Xu, Daniel Sohn, Devendra Sachan, Reinald Kim Amplayo, Craig Swanson, Dessie Petrova, Shashi Narayan, Arthur Guez, Siddhartha Brahma, Jessica Landon, Miteyan Patel, Ruizhe Zhao, Kevin Villela, Luyu Wang, Wenhao Jia, Matthew Rahtz, Mai GimÃ©nez, Legg Yeung, James Keeling, Petko Georgiev, Diana Mincu, Boxi Wu, Salem Haykal, Rachel Saputro, Kiran Vodrahalli, James Qin, Zeynep Cankara, Abhanshu Sharma, Nick Fernando, Will Hawkins, Behnam Neyshabur, Solomon Kim, Adrian Hutter, Priyanka Agrawal, Alex Castro-Ros, George van den Driessche, Tao Wang, Fan Yang, Shuo-yiin Chang, Paul Komarek, Ross McIlroy, Mario LuÄiÄ, Guodong Zhang, Wael Farhan, Michael Sharman, Paul Natsev, Paul Michel, Yamini Bansal, Siyuan Qiao, Kris Cao, Siamak Shakeri, Christina Butterfield, Justin Chung, Paul Kishan Rubenstein, Shivani Agrawal, Arthur Mensch, Kedar Soparkar, Karel Lenc, Timothy Chung, Aedan Pope, Loren Maggiore, Jackie Kay, Priya Jhakra, Shibo Wang, Joshua Maynez, Mary Phuong, Taylor Tobin, Andrea Tacchetti, Maja Trebacz, Kevin Robinson, Yash Katariya, Sebastian Riedel, Paige Bailey, Kefan Xiao, Nimesh Ghelani, Lora Aroyo, Ambrose Slone, Neil Houlsby, Xuehan Xiong, Zhen Yang, Elena Gribovskaya, Jonas Adler, Mateo Wirth, Lisa Lee, Music Li, Thais Kagohara, Jay Pavagadhi, Sophie Bridgers, Anna Bortsova, Sanjay Ghemawat, Zafarali Ahmed, Tianqi Liu, Richard Powell, Vijay Bolina, Mariko Iinuma, Polina Zablotskaia, James Besley, Da-Woon Chung, Timothy Dozat, Ramona Comanescu, Xiance Si, Jeremy Greer, Guolong Su, Martin Polacek, RaphaÃ«l Lopez Kaufman, Simon Tokumine, Hexiang Hu, Elena Buchatskaya, Yingjie Miao, Mohamed Elhawaty, Aditya Siddhant, Nenad Tomasev, Jinwei Xing, Christina Greer, Helen Miller, Shereen Ashraf, Aurko Roy, Zizhao Zhang, Ada Ma, Angelos Filos, Milos Besta, Rory Blevins, Ted Klimenko, Chih-Kuan Yeh, Soravit Changpinyo, Jiaqi Mu, Oscar Chang, Mantas Pajarskas, Carrie Muir, Vered Cohen, Charline Le Lan, Krishna Haridasan, Amit Marathe, Steven Hansen, Sholto Douglas, Rajkumar Samuel, Mingqiu Wang, Sophia Austin, Chang Lan, Jiepu Jiang, Justin Chiu, Jaime Alonso Lorenzo, Lars Lowe SjÃ¶sund, SÃ©bastien Cevey, Zach Gleicher, Thi Avrahami, Anudhyan Boral, Hansa Srinivasan, Vittorio Selo, Rhys May, Konstantinos Aisopos, LÃ©onard Hussenot, Livio Baldini Soares, Kate Baumli, Michael B. Chang, AdriÃ  Recasens, Ben Caine, Alexander Pritzel, Filip Pavetic, Fabio Pardo, Anita Gergely, Justin Frye, Vinay Ramasesh, Dan Horgan, Kartikeya Badola, Nora Kassner, Subhrajit Roy, Ethan Dyer, VÃ­ctor Campos Campos, Alex Tomala, Yunhao Tang, Dalia El Badawy, Elspeth White, Basil Mustafa, Oran Lang, Abhishek Jindal, Sharad Vikram, Zhitao Gong, Sergi Caelles, Ross Hemsley, Gregory Thornton, Fangxiaoyu Feng, Wojciech Stokowiec, Ce Zheng, Phoebe Thacker, ÃaÄlar ÃnlÃ¼, Zhishuai Zhang, Mohammad Saleh, James Svensson, Max Bileschi, Piyush Patil, Ankesh Anand, Roman Ring, Katerina Tsihlas, Arpi Vezer, Marco Selvi, Toby Shevlane, Mikel Rodriguez, Tom Kwiatkowski, Samira Daruki, Keran Rong, Allan Dafoe, Nicholas FitzGerald, Keren Gu-Lemberg, Mina Khan, Lisa Anne Hendricks, Marie Pellat, Vladimir Feinberg, James Cobon-Kerr, Tara Sainath, Maribeth Rauh, Sayed Hadi Hashemi, Richard Ives, Yana Hasson, Eric Noland, Yuan Cao, Nathan Byrd, Le Hou, Qingze Wang, Thibault Sottiaux, Michela Paganini, Jean-Baptiste Lespiau, Alexandre Moufarek, Samer Hassan, Kaushik Shivakumar, Joost van Amersfoort, Amol Mandhane, Pratik Joshi, Anirudh Goyal, Matthew Tung, Andrew Brock, Hannah Sheahan, Vedant Misra, Cheng Li, Nemanja RakiÄeviÄ, Mostafa Dehghani, Fangyu Liu, Sid Mittal, Junhyuk Oh, Seb Noury, Eren Sezener, Fantine Huot, Matthew Lamm, Nicola De Cao, Charlie Chen, Sidharth Mudgal, Romina Stella, Kevin Brooks, Gautam Vasudevan, Chenxi Liu, Mainak Chain, Nivedita Melinkeri, Aaron Cohen, Venus Wang, Kristie Seymore, Sergey Zubkov, Rahul Goel, Summer Yue, Sai Krishnakumaran, Brian Albert, Nate Hurley, Motoki Sano, Anhad Mohananey, Jonah Joughin, Egor Filonov, Tomasz KÄpa, Yomna Eldawy, Jiawern Lim, Rahul Rishi, Shirin Badiezadegan, Taylor Bos, Jerry Chang, Sanil Jain, Sri Gayatri Sundara Padmanabhan, Subha Puttagunta, Kalpesh Krishna, Leslie Baker, Norbert Kalb, Vamsi Bedapudi, Adam Kurzrok, Shuntong Lei, Anthony Yu, Oren Litvin, Xiang Zhou, Zhichun Wu, Sam Sobell, Andrea Siciliano, Alan Papir, Robby Neale, Jonas Bragagnolo, Tej Toor, Tina Chen, Valentin Anklin, Feiran Wang, Richie Feng, Milad Gholami, Kevin Ling, Lijuan Liu, Jules Walter, Hamid Moghaddam, Arun Kishore, Jakub Adamek, Tyler Mercado, Jonathan Mallinson, Siddhinita Wandekar, Stephen Cagle, Eran Ofek, Guillermo Garrido, Clemens Lombriser, Maksim Mukha, Botu Sun, Hafeezul Rahman Mohammad, Josip Matak, Yadi Qian, Vikas Peswani, Pawel Janus, Quan Yuan, Leif Schelin, Oana David, Ankur Garg, Yifan He, Oleksii Duzhyi, Anton Ãlgmyr, TimothÃ©e Lottaz, Qi Li, Vikas Yadav, Luyao Xu, Alex Chinien, Rakesh Shivanna, Aleksandr Chuklin, Josie Li, Carrie Spadine, Travis Wolfe, Kareem Mohamed, Subhabrata Das, Zihang Dai, Kyle He, Daniel von Dincklage, Shyam Upadhyay, Akanksha Maurya, Luyan Chi, Sebastian Krause, Khalid Salama, Pam G Rabinovitch, Pavan Kumar Reddy M, Aarush Selvan, Mikhail Dektiarev, Golnaz Ghiasi, Erdem Guven, Himanshu Gupta, Boyi Liu, Deepak Sharma, Idan Heimlich Shtacher, Shachi Paul, Oscar Akerlund, FranÃ§ois-Xavier Aubet, Terry Huang, Chen Zhu, Eric Zhu, Elico Teixeira, Matthew Fritze, Francesco Bertolini, Liana-Eleonora Marinescu, Martin BÃ¶lle, Dominik Paulus, Khyatti Gupta, Tejasi Latkar, Max Chang, Jason Sanders, Roopa Wilson, Xuewei Wu, Yi-Xuan Tan, Lam Nguyen Thiet, Tulsee Doshi, Sid Lall, Swaroop Mishra, Wanming Chen, Thang Luong, Seth Benjamin, Jasmine Lee, Ewa Andrejczuk, Dominik Rabiej, Vipul Ranjan, Krzysztof Styrc, Pengcheng Yin, Jon Simon, Malcolm Rose Harriott, Mudit Bansal, Alexei Robsky, Geoff Bacon, David Greene, Daniil Mirylenka, Chen Zhou, Obaid Sarvana, Abhimanyu Goyal, Samuel Andermatt, Patrick Siegler, Ben Horn, Assaf Israel, Francesco Pongetti, Chih-Wei "Louis" Chen, Marco Selvatici, Pedro Silva, Kathie Wang, Jackson Tolins, Kelvin Guu, Roey Yogev, Xiaochen Cai, Alessandro Agostini, Maulik Shah, Hung Nguyen, Noah Ã Donnaile, SÃ©bastien Pereira, Linda Friso, Adam Stambler, Adam Kurzrok, Chenkai Kuang, Yan Romanikhin, Mark Geller, ZJ Yan, Kane Jang, Cheng-Chun Lee, Wojciech Fica, Eric Malmi, Qijun Tan, Dan Banica, Daniel Balle, Ryan Pham, Yanping Huang, Diana Avram, Hongzhi Shi, Jasjot Singh, Chris Hidey, Niharika Ahuja, Pranab Saxena, Dan Dooley, Srividya Pranavi Potharaju, Eileen O'Neill, Anand Gokulchandran, Ryan Foley, Kai Zhao, Mike Dusenberry, Yuan Liu, Pulkit Mehta, Ragha Kotikalapudi, Chalence Safranek-Shrader, Andrew Goodman, Joshua Kessinger, Eran Globen, Prateek Kolhar, Chris Gorgolewski, Ali Ibrahim, Yang Song, Ali Eichenbaum, Thomas Brovelli, Sahitya Potluri, Preethi Lahoti, Cip Baetu, Ali Ghorbani, Charles Chen, Andy Crawford, Shalini Pal, Mukund Sridhar, Petru Gurita, Asier Mujika, Igor Petrovski, Pierre-Louis Cedoz, Chenmei Li, Shiyuan Chen, NiccolÃ² Dal Santo, Siddharth Goyal, Jitesh Punjabi, Karthik Kappaganthu, Chester Kwak, Pallavi LV, Sarmishta Velury, Himadri Choudhury, Jamie Hall, Premal Shah, Ricardo Figueira, Matt Thomas, Minjie Lu, Ting Zhou, Chintu Kumar, Thomas Jurdi, Sharat Chikkerur, Yenai Ma, Adams Yu, Soo Kwak, Victor Ãhdel, Sujeevan Rajayogam, Travis Choma, Fei Liu, Aditya Barua, Colin Ji, Ji Ho Park, Vincent Hellendoorn, Alex Bailey, Taylan Bilal, Huanjie Zhou, Mehrdad Khatir, Charles Sutton, Wojciech Rzadkowski, Fiona Macintosh, Roopali Vij, Konstantin Shagin, Paul Medina, Chen Liang, Jinjing Zhou, Pararth Shah, Yingying Bi, Attila Dankovics, Shipra Banga, Sabine Lehmann, Marissa Bredesen, Zifan Lin, John Eric Hoffmann, Jonathan Lai, Raynald Chung, Kai Yang, Nihal Balani, Arthur BraÅ¾inskas, Andrei Sozanschi, Matthew Hayes, HÃ©ctor FernÃ¡ndez Alcalde, Peter Makarov, Will Chen, Antonio Stella, Liselotte Snijders, Michael Mandl, Ante KÃ¤rrman, PaweÅ Nowak, Xinyi Wu, Alex Dyck, Krishnan Vaidyanathan, Raghavender R, Jessica Mallet, Mitch Rudominer, Eric Johnston, Sushil Mittal, Akhil Udathu, Janara Christensen, Vishal Verma, Zach Irving, Andreas Santucci, Gamaleldin Elsayed, Elnaz Davoodi, Marin Georgiev, Ian Tenney, Nan Hua, Geoffrey Cideron, Edouard Leurent, Mahmoud Alnahlawi, Ionut Georgescu, Nan Wei, Ivy Zheng, Dylan Scandinaro, Heinrich Jiang, Jasper Snoek, Mukund Sundararajan, Xuezhi Wang, Zack Ontiveros, Itay Karo, Jeremy Cole, Vinu Rajashekhar, Lara Tumeh, Eyal Ben-David, Rishub Jain, Jonathan Uesato, Romina Datta, Oskar Bunyan, Shimu Wu, John Zhang, Piotr Stanczyk, Ye Zhang, David Steiner, Subhajit Naskar, Michael Azzam, Matthew Johnson, Adam Paszke, Chung-Cheng Chiu, Jaume Sanchez Elias, Afroz Mohiuddin, Faizan Muhammad, Jin Miao, Andrew Lee, Nino Vieillard, Jane Park, Jiageng Zhang, Jeff Stanway, Drew Garmon, Abhijit Karmarkar, Zhe Dong, Jong Lee, Aviral Kumar, Luowei Zhou, Jonathan Evens, William Isaac, Geoffrey Irving, Edward Loper, Michael Fink, Isha Arkatkar, Nanxin Chen, Izhak Shafran, Ivan Petrychenko, Zhe Chen, Johnson Jia, Anselm Levskaya, Zhenkai Zhu, Peter Grabowski, Yu Mao, Alberto Magni, Kaisheng Yao, Javier Snaider, Norman Casagrande, Evan Palmer, Paul Suganthan, Alfonso CastaÃ±o, Irene Giannoumis, Wooyeol Kim, MikoÅaj RybiÅski, Ashwin Sreevatsa, Jennifer Prendki, David Soergel, Adrian Goedeckemeyer, Willi Gierke, Mohsen Jafari, Meenu Gaba, Jeremy Wiesner, Diana Gage Wright, Yawen Wei, Harsha Vashisht, Yana Kulizhskaya, Jay Hoover, Maigo Le, Lu Li, Chimezie Iwuanyanwu, Lu Liu, Kevin Ramirez, Andrey Khorlin, Albert Cui, Tian LIN, Marcus Wu, Ricardo Aguilar, Keith Pallo, Abhishek Chakladar, Ginger Perng, Elena Allica Abellan, Mingyang Zhang, Ishita Dasgupta, Nate Kushman, Ivo Penchev, Alena Repina, Xihui Wu, Tom van der Weide, Priya Ponnapalli, Caroline Kaplan, Jiri Simsa, Shuangfeng Li, Olivier Dousse, Fan Yang, Jeff Piper, Nathan Ie, Rama Pasumarthi, Nathan Lintz, Anitha Vijayakumar, Daniel Andor, Pedro Valenzuela, Minnie Lui, Cosmin Paduraru, Daiyi Peng, Katherine Lee, Shuyuan Zhang, Somer Greene, Duc Dung Nguyen, Paula Kurylowicz, Cassidy Hardin, Lucas Dixon, Lili Janzer, Kiam Choo, Ziqiang Feng, Biao Zhang, Achintya Singhal, Dayou Du, Dan McKinnon, Natasha Antropova, Tolga Bolukbasi, Orgad Keller, David Reid, Daniel Finchelstein, Maria Abi Raad, Remi Crocker, Peter Hawkins, Robert Dadashi, Colin Gaffney, Ken Franko, Anna Bulanova, RÃ©mi Leblond, Shirley Chung, Harry Askham, Luis C. Cobo, Kelvin Xu, Felix Fischer, Jun Xu, Christina Sorokin, Chris Alberti, Chu-Cheng Lin, Colin Evans, Alek Dimitriev, Hannah Forbes, Dylan Banarse, Zora Tung, Mark Omernick, Colton Bishop, Rachel Sterneck, Rohan Jain, Jiawei Xia, Ehsan Amid, Francesco Piccinno, Xingyu Wang, Praseem Banzal, Daniel J. Mankowitz, Alex Polozov, Victoria Krakovna, Sasha Brown, MohammadHossein Bateni, Dennis Duan, Vlad Firoiu, Meghana Thotakuri, Tom Natan, Matthieu Geist, Ser tan Girgin, Hui Li, Jiayu Ye, Ofir Roval, Reiko Tojo, Michael Kwong, James Lee-Thorp, Christopher Yew, Danila Sinopalnikov, Sabela Ramos, John Mellor, Abhishek Sharma, Kathy Wu, David Miller, Nicolas Sonnerat, Denis Vnukov, Rory Greig, Jennifer Beattie, Emily Caveness, Libin Bai, Julian Eisenschlos, Alex Korchemniy, Tomy Tsai, Mimi Jasarevic, Weize Kong, Phuong Dao, Zeyu Zheng, Frederick Liu, Fan Yang, Rui Zhu, Tian Huey Teh, Jason Sanmiya, Evgeny Gladchenko, Nejc Trdin, Daniel Toyama, Evan Rosen, Sasan Tavakkol, Linting Xue, Chen Elkind, Oliver Woodman, John Carpenter, George Papamakarios, Rupert Kemp, Sushant Kafle, Tanya Grunina, Rishika Sinha, Alice Talbert, Diane Wu, Denese Owusu-Afriyie, Cosmo Du, Chloe Thornton, Jordi Pont-Tuset, Pradyumna Narayana, Jing Li, Saaber Fatehi, John Wieting, Omar Ajmeri, Benigno Uria, Yeongil Ko, Laura Knight, AmÃ©lie HÃ©liou, Ning Niu, Shane Gu, Chenxi Pang, Yeqing Li, Nir Levine, Ariel Stolovich, Rebeca Santamaria-Fernandez, Sonam Goenka, Wenny Yustalim, Robin Strudel, Ali Elqursh, Charlie Deck, Hyo Lee, Zonglin Li, Kyle Levin, Raphael Hoffmann, Dan Holtmann-Rice, Olivier Bachem, Sho Arora, Christy Koh, Soheil Hassas Yeganeh, Siim PÃµder, Mukarram Tariq, Yanhua Sun, Lucian Ionita, Mojtaba Seyedhosseini, Pouya Tafti, Zhiyu Liu, Anmol Gulati, Jasmine Liu, Xinyu Ye, Bart Chrzaszcz, Lily Wang, Nikhil Sethi, Tianrun Li, Ben Brown, Shreya Singh, Wei Fan, Aaron Parisi, Joe Stanton, Vinod Koverkathu, Christopher A. Choquette-Choo, Yunjie Li, TJ Lu, Abe Ittycheriah, Prakash Shroff, Mani Varadarajan, Sanaz Bahargam, Rob Willoughby, David Gaddy, Guillaume Desjardins, Marco Cornero, Brona Robenek, Bhavishya Mittal, Ben Albrecht, Ashish Shenoy, Fedor Moiseev, Henrik Jacobsson, Alireza Ghaffarkhah, Morgane RiviÃ¨re, Alanna Walton, ClÃ©ment Crepy, Alicia Parrish, Zongwei Zhou, Clement Farabet, Carey Radebaugh, Praveen Srinivasan, Claudia van der Salm, Andreas Fidjeland, Salvatore Scellato, Eri Latorre-Chimoto, Hanna Klimczak-PluciÅska, David Bridson, Dario de Cesare, Tom Hudson, Piermaria Mendolicchio, Lexi Walker, Alex Morris, Matthew Mauger, Alexey Guseynov, Alison Reid, Seth Odoom, Lucia Loher, Victor Cotruta, Madhavi Yenugula, Dominik Grewe, Anastasia Petrushkina, Tom Duerig, Antonio Sanchez, Steve Yadlowsky, Amy Shen, Amir Globerson, Lynette Webb, Sahil Dua, Dong Li, Surya Bhupatiraju, Dan Hurt, Haroon Qureshi, Ananth Agarwal, Tomer Shani, Matan Eyal, Anuj Khare, Shreyas Rammohan Belle, Lei Wang, Chetan Tekur, Mihir Sanjay Kale, Jinliang Wei, Ruoxin Sang, Brennan Saeta, Tyler Liechty, Yi Sun, Yao Zhao, Stephan Lee, Pandu Nayak, Doug Fritz, Manish Reddy Vuyyuru, John Aslanides, Nidhi Vyas, Martin Wicke, Xiao Ma, Evgenii Eltyshev, Nina Martin, Hardie Cate, James Manyika, Keyvan Amiri, Yelin Kim, Xi Xiong, Kai Kang, Florian Luisier, Nilesh Tripuraneni, David Madras, Mandy Guo, Austin Waters, Oliver Wang, Joshua Ainslie, Jason Baldridge, Han Zhang, Garima Pruthi, Jakob Bauer, Feng Yang, Riham Mansour, Jason Gelman, Yang Xu, George Polovets, Ji Liu, Honglong Cai, Warren Chen, XiangHai Sheng, Emily Xue, Sherjil Ozair, Christof Angermueller, Xiaowei Li, Anoop Sinha, Weiren Wang, Julia Wiesinger, Emmanouil Koukoumidis, Yuan Tian, Anand Iyer, Madhu Gurumurthy, Mark Goldenson, Parashar Shah, MK Blake, Hongkun Yu, Anthony Urbanowicz, Jennimaria Palomaki, Chrisantha Fernando, Ken Durden, Harsh Mehta, Nikola Momchev, Elahe Rahimtoroghi, Maria Georgaki, Amit Raul, Sebastian Ruder, Morgan Redshaw, Jinhyuk Lee, Denny Zhou, Komal Jalan, Dinghua Li, Blake Hechtman, Parker Schuh, Milad Nasr, Kieran Milan, Vladimir Mikulik, Juliana Franco, Tim Green, Nam Nguyen, Joe Kelley, Aroma Mahendru, Andrea Hu, Joshua Howland, Ben Vargas, Jeffrey Hui, Kshitij Bansal, Vikram Rao, Rakesh Ghiya, Emma Wang, Ke Ye, Jean Michel Sarr, Melanie Moranski Preston, Madeleine Elish, Steve Li, Aakash Kaku, Jigar Gupta, Ice Pasupat, Da-Cheng Juan, Milan Someswar, Tejvi M., Xinyun Chen, Aida Amini, Alex Fabrikant, Eric Chu, Xuanyi Dong, Amruta Muthal, Senaka Buthpitiya, Sarthak Jauhari, Nan Hua, Urvashi Khandelwal, Ayal Hitron, Jie Ren, Larissa Rinaldi, Shahar Drath, Avigail Dabush, Nan-Jiang Jiang, Harshal Godhia, Uli Sachs, Anthony Chen, Yicheng Fan, Hagai Taitelbaum, Hila Noga, Zhuyun Dai, James Wang, Chen Liang, Jenny Hamer, Chun-Sung Ferng, Chenel Elkind, Aviel Atias, Paulina Lee, VÃ­t ListÃ­k, Mathias Carlen, Jan van de Kerkhof, Marcin Pikus, Krunoslav Zaher, Paul MÃ¼ller, Sasha Zykova, Richard Stefanec, Vitaly Gatsko, Christoph Hirnschall, Ashwin Sethi, Xingyu Federico Xu, Chetan Ahuja, Beth Tsai, Anca Stefanoiu, Bo Feng, Keshav Dhandhania, Manish Katyal, Akshay Gupta, Atharva Parulekar, Divya Pitta, Jing Zhao, Vivaan Bhatia, Yashodha Bhavnani, Omar Alhadlaq, Xiaolin Li, Peter Danenberg, Dennis Tu, Alex Pine, Vera Filippova, Abhipso Ghosh, Ben Limonchik, Bhargava Urala, Chaitanya Krishna Lanka, Derik Clive, Yi Sun, Edward Li, Hao Wu, Kevin Hongtongsak, Ianna Li, Kalind Thakkar, Kuanysh Omarov, Kushal Majmundar, Michael Alverson, Michael Kucharski, Mohak Patel, Mudit Jain, Maksim Zabelin, Paolo Pelagatti, Rohan Kohli, Saurabh Kumar, Joseph Kim, Swetha Sankar, Vineet Shah, Lakshmi Ramachandruni, Xiangkai Zeng, Ben Bariach, Laura Weidinger, Tu Vu, Alek Andreev, Antoine He, Kevin Hui, Sheleem Kashem, Amar Subramanya, Sissie Hsiao, Demis Hassabis, Koray Kavukcuoglu, Adam Sadovsky, Quoc Le, Trevor Strohman, Yonghui Wu, Slav Petrov, Jeffrey Dean, Oriol Vinyals*

**Main category:** cs.CL

**Keywords:** multimodal models, cross-modal reasoning, Gemini models

**Relevance Score:** 8

**TL;DR:** Introduction of the Gemini multimodal models that excel in understanding across various media types and set new benchmarks in performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a family of multimodal models that demonstrate advanced capabilities in processing and reasoning across image, audio, video, and text data.

**Method:** Introduction of Gemini models with three sizes (Ultra, Pro, Nano), evaluated on multiple benchmarks, including MMLU for cross-modal reasoning and language understanding.

**Key Contributions:**

	1. Introduction of the Gemini multimodal model family
	2. Achievement of human-expert performance on MMLU
	3. State-of-the-art results across multimodal benchmarks

**Result:** Gemini Ultra achieves state-of-the-art performance in 30 of 32 benchmarks, first to reach human-expert performance on MMLU, and improves all 20 examined multimodal benchmarks.

**Limitations:** 

**Conclusion:** Gemini models have the potential to support a diverse range of applications in multimodal understanding and responsible AI deployment.

**Abstract:** This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI.

</details>


### [103] [Fleet of Agents: Coordinated Problem Solving with Large Language Models](https://arxiv.org/abs/2405.06691)

*Lars Klein, Nearchos Potamitis, Roland Aydin, Robert West, Caglar Gulcehre, Akhil Arora*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Frameworks, Exploration-Exploitation, Dynamic Tree Search, Cost-Quality Trade-off

**Relevance Score:** 8

**TL;DR:** A novel framework, Fleet of Agents (FoA), enhances LLM reasoning through cost-effective dynamic tree search methods, achieving ~5% quality improvement at ~40% of previous costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of effective methods that balance cost and quality in enhancing the reasoning abilities of large language models.

**Method:** FoA uses LLMs as agents for dynamic tree searches, leveraging a genetic-type particle filtering approach with exploration and selection phases based on heuristic value functions.

**Key Contributions:**

	1. Introduction of a cost-effective framework for LLM reasoning
	2. Demonstration of dynamic branching and exploration strategies
	3. Public availability of the FoA framework for further research.

**Result:** FoA shows a quality improvement of ~5% on benchmark tasks while requiring ~40% of the cost compared to previous state-of-the-art methods.

**Limitations:** 

**Conclusion:** FoA achieves superior cost-quality trade-offs and outperforms competitor models, specifically demonstrating better performance with LLaMA3.2-11B over LLaMA3.2-90B.

**Abstract:** While numerous frameworks have been developed to enhance the reasoning abilities of large language models (LLMs), there is a scarcity of methods that effectively balance the trade-off between cost and quality. In this paper, we introduce Fleet of Agents (FoA), a novel and intuitive yet principled framework utilizing LLMs as agents to navigate through dynamic tree searches, employing a genetic-type particle filtering approach. FoA spawns a multitude of agents, each exploring the search space autonomously, followed by a selection phase where resampling based on a heuristic value function optimizes the balance between exploration and exploitation. This mechanism enables dynamic branching, adapting the exploration strategy based on discovered solutions. We conduct extensive experiments on three benchmark tasks, ``Game of 24'', ``Mini-Crosswords'', and ``WebShop'', utilizing four different LLMs, ``GPT-3.5'', ``GPT-4'', ``LLaMA3.2-11B'', and ``LLaMA3.2-90B''. On average across all tasks and LLMs, FoA obtains a quality improvement of ~5% while requiring only ~40% of the cost of previous SOTA methods. Notably, our analyses reveal that (1) FoA achieves the best cost-quality trade-off among all benchmarked methods and (2) FoA + LLaMA3.2-11B surpasses the Llama3.2-90B model. FoA is publicly available at https://github.com/au-clan/FoA.

</details>


### [104] [A Syntax-Injected Approach for Faster and More Accurate Sentiment Analysis](https://arxiv.org/abs/2406.15163)

*Muhammad Imran, Olga Kellert, Carlos GÃ³mez-RodrÃ­guez*

**Main category:** cs.CL

**Keywords:** sentiment analysis, natural language processing, syntactic parsing, sequence labeling, polarity classification

**Relevance Score:** 8

**TL;DR:** This paper introduces a Sequence Labeling Syntactic Parser (SELSP) for improving the speed and accuracy of sentiment analysis by treating dependency parsing as a sequence labeling problem.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational bottleneck in sentiment analysis caused by slow parsing algorithms, while improving accuracy through syntactic information.

**Method:** The paper presents SELSP, a sequence labeling approach to dependency parsing, and evaluates its performance on ternary polarity classification tasks.

**Key Contributions:**

	1. Introduction of SELSP for sentiment analysis
	2. Demonstrated significant speed and accuracy improvements over conventional parsers and heuristics
	3. Comparison with Transformer models highlighting the efficiency of SELSP

**Result:** SELSP outperforms conventional parsers like Stanza and heuristic approaches like VADER in both speed and accuracy for polarity prediction.

**Limitations:** 

**Conclusion:** The findings indicate that SELSP offers significant advantages for practitioners in sentiment analysis by providing faster and more accurate results, especially when using sentiment dictionaries that capture polarity variations.

**Abstract:** Sentiment Analysis (SA) is a crucial aspect of Natural Language Processing (NLP), addressing subjective assessments in textual content. Syntactic parsing is useful in SA because explicit syntactic information can improve accuracy while providing explainability, but it tends to be a computational bottleneck in practice due to the slowness of parsing algorithms. This paper addresses said bottleneck by using a SEquence Labeling Syntactic Parser (SELSP) to inject syntax into SA. By treating dependency parsing as a sequence labeling problem, we greatly enhance the speed of syntax-based SA. SELSP is trained and evaluated on a ternary polarity classification task, demonstrating its faster performance and better accuracy in polarity prediction tasks compared to conventional parsers like Stanza and to heuristic approaches that use shallow syntactic rules for SA like VADER. This increased speed and improved accuracy make SELSP particularly appealing to SA practitioners in both research and industry. In addition, we test several sentiment dictionaries on our SELSP to see which one improves the performance in polarity prediction tasks. Moreover, we compare the SELSP with Transformer-based models trained on a 5-label classification task. The results show that dictionaries that capture polarity judgment variation provide better results than dictionaries that ignore polarity judgment variation. Moreover, we show that SELSP is considerably faster than Transformer-based models in polarity prediction tasks.

</details>


### [105] [From Distributional to Overton Pluralism: Investigating Large Language Model Alignment](https://arxiv.org/abs/2406.17692)

*Thom Lake, Eunsol Choi, Greg Durrett*

**Main category:** cs.CL

**Keywords:** Large Language Models, Alignment, Response Diversity, Information Retrieval, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper analyzes the impact of alignment on LLM response diversity and investigates whether aligned models provide new information compared to base models, concluding that alignment does not enhance information utility and can be mimicked without fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how alignment affects the properties of LLM outputs and whether it improves the utility of the information provided.

**Method:** The authors conduct a detailed analysis on response diversity and the ability of aligned versus unaligned models to produce similar responses. They utilize in-context examples and semantic hints to explore the output of base models.

**Key Contributions:**

	1. Analysis of response diversity changes due to alignment.
	2. Demonstration that the output of aligned models can be approximated by base models using in-context examples.
	3. Evidence against the improvement of information utility through alignment.

**Result:** The study finds that alignment reduces perceived response diversity mainly due to quality control and that aligned models do not surface information unrecoverable from base models. In-context alignment can replicate the behavior of aligned models without any fine-tuning.

**Limitations:** The study focuses on specific aspects of alignment and does not address all potential impacts or other alignment strategies.

**Conclusion:** Current alignment techniques do not extend the useful behavior of base models but rather capture a subset; also, in-context alignment can achieve similar results as alignment tuning without the need for fine-tuning.

**Abstract:** The alignment process changes several properties of a large language model's (LLM's) output distribution. We analyze two aspects of post-alignment distributional shift of LLM responses. First, we re-examine previously reported reductions in response diversity post-alignment. Our analysis suggests that an apparent drop in the diversity of responses is largely explained by quality control and information aggregation. Alignment suppresses irrelevant and unhelpful content while shifting the output distribution toward longer responses that cover information spanning several responses from the base LLM, essentially presenting diverse information in a single response. Finding little evidence that alignment suppresses useful information, it is natural to ask the opposite question: do aligned models surface information that cannot be recovered from base models? Our second investigation shows this is not the case and the behavior of aligned models is recoverable from base models without fine-tuning. A combination of in-context examples and lower-resolution semantic hints about response content can elicit responses from base LLMs that are as similar to alignment-tuned LLM responses as alignment-tuned LLM responses are to each other. Taken together, these results indicate that current alignment techniques capture but do not extend the useful subset of assistant-like base LLM behavior, providing further evidence for the Superficial Alignment Hypothesis. They also show that in-context alignment can go surprisingly far as a strategy for imitating aligned LLMs without fine-tuning. Our code and data is available at https://github.com/thomlake/investigating-alignment.

</details>


### [106] [Order Matters in Hallucination: Reasoning Order as Benchmark and Reflexive Prompting for Large-Language-Models](https://arxiv.org/abs/2408.05093)

*Zikai Xie*

**Main category:** cs.CL

**Keywords:** large language models, hallucination problem, numerical comparison errors, benchmark method, prompt strategy

**Relevance Score:** 9

**TL;DR:** This paper addresses the hallucination problem in large language models (LLMs), specifically focusing on numerical comparison errors, and proposes a new benchmark method to assess LLM consistency along with a prompt strategy to improve reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to tackle the issue of factual inaccuracies in LLM outputs, notably in numerical comparisons, to enhance the reliability of these models.

**Method:** The authors propose a benchmark for evaluating LLM consistency by comparing outputs generated via different reasoning orders, and introduce a novel prompt strategy for improved performance.

**Key Contributions:**

	1. Proposed a benchmark method for assessing LLM consistency based on reasoning order.
	2. Identified the influence of answer generation order on LLM outputs.
	3. Introduced a prompt strategy that improves LLM performance in generating accurate responses.

**Result:** Experimental results indicate that the proposed prompt strategy enhances LLM performance across various models, particularly in terms of consistency and factual accuracy.

**Limitations:** 

**Conclusion:** This research highlights significant flaws in LLMs and provides practical methodologies to improve their reliability and consistency.

**Abstract:** Large language models (LLMs) have generated significant attention since their inception, finding applications across various academic and industrial domains. However, these models often suffer from the "hallucination problem", where outputs, though grammatically and logically coherent, lack factual accuracy or are entirely fabricated. A particularly troubling issue discovered and widely discussed recently is the numerical comparison error where multiple LLMs incorrectly infer that "9.11$>$9.9". We discovered that the order in which LLMs generate answers and reasoning impacts their consistency. Specifically, results vary significantly when an LLM generates an answer first and then provides the reasoning versus generating the reasoning process first and then the conclusion. Inspired by this, we propose a new benchmark method for assessing LLM consistency: comparing responses generated through these two different approaches. This benchmark effectively identifies instances where LLMs fabricate answers and subsequently generate justifications. Furthermore, we introduce a novel and straightforward prompt strategy designed to mitigate this issue. Experimental results demonstrate that this strategy improves performance across various LLMs compared to direct questioning. This work not only sheds light on a critical flaw in LLMs but also offers a practical solution to enhance their reliability.

</details>


### [107] [MABR: Multilayer Adversarial Bias Removal Without Prior Bias Knowledge](https://arxiv.org/abs/2408.05497)

*Maxwell J. Yin, Boyu Wang, Charles Ling*

**Main category:** cs.CL

**Keywords:** bias mitigation, adversarial training, social biases, machine learning, sentiment analysis

**Relevance Score:** 8

**TL;DR:** This paper presents a novel adversarial training strategy for mitigating social biases in machine learning models without needing prior knowledge of specific biases or demographic labels, effectively reducing biases in sentiment and occupation classification tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of social biases in AI models that reflect and exacerbate existing societal disparities, especially in areas such as gender and racial biases.

**Method:** The proposed method employs a novel adversarial training strategy that uses auxiliary models which predict the performance of the main model, allowing for the identification and mitigation of biases without relying on protected attribute labels or prior bias knowledge.

**Key Contributions:**

	1. Introduction of a new adversarial training strategy that does not depend on prior bias knowledge.
	2. Use of auxiliary models at various feature map levels to detect a broader range of biases.
	3. Demonstration of matching or surpassing traditional bias mitigation methods in effectiveness.

**Result:** Experiments demonstrate that this method effectively reduces biases in sentiment and occupation classification tasks, often matching or surpassing traditional methods that require detailed demographic insights.

**Limitations:** 

**Conclusion:** The approach represents a significant advancement in bias mitigation techniques, providing a means to address social biases without demographic annotations and enhancing the fairness of machine learning models.

**Abstract:** Models trained on real-world data often mirror and exacerbate existing social biases. Traditional methods for mitigating these biases typically require prior knowledge of the specific biases to be addressed, such as gender or racial biases, and the social groups associated with each instance. In this paper, we introduce a novel adversarial training strategy that operates independently of prior bias-type knowledge and protected attribute labels. Our approach proactively identifies biases during model training by utilizing auxiliary models, which are trained concurrently by predicting the performance of the main model without relying on task labels. Additionally, we implement these auxiliary models at various levels of the feature maps of the main model, enabling the detection of a broader and more nuanced range of bias features. Through experiments on racial and gender biases in sentiment and occupation classification tasks, our method effectively reduces social biases without the need for demographic annotations. Moreover, our approach not only matches but often surpasses the efficacy of methods that require detailed demographic insights, marking a significant advancement in bias mitigation techniques.

</details>


### [108] [Bridging the Language Gap: Enhancing Multilingual Prompt-Based Code Generation in LLMs via Zero-Shot Cross-Lingual Transfer](https://arxiv.org/abs/2408.09701)

*Mingda Li, Abhijit Mishra, Utkarsh Mujumdar*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual code generation, cross-lingual embeddings

**Relevance Score:** 8

**TL;DR:** This paper explores the use of Large Language Models (LLMs) for multilingual code generation, revealing biases and limitations of existing methods and proposing a neural projection technique to improve code quality across languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the inclusivity of code generation across different languages and address the biases found in LLMs when handling non-English prompts.

**Method:** The authors propose a zero-shot cross-lingual approach using a neural projection technique that integrates a cross-lingual encoder to map multilingual embeddings into the LLM's token space, requiring only English training data.

**Key Contributions:**

	1. Identification of disparities in code quality for non-English prompts
	2. Development of a zero-shot cross-lingual approach using neural projection
	3. Demonstration of substantial improvements in code quality for multilingual programming.

**Result:** The proposed method demonstrates significant improvements in the quality of code generated from non-English prompts, as evaluated on a translated and quality-checked MBPP dataset.

**Limitations:** The approach primarily focuses on English data for training and may face challenges in fully capturing the intricacies of all possible non-English languages.

**Conclusion:** This research contributes to creating a more inclusive landscape for coding by enabling LLMs to effectively support diverse linguistic contexts in programming tasks.

**Abstract:** The use of Large Language Models (LLMs) for program code generation has gained substantial attention, but their biases and limitations with non-English prompts challenge global inclusivity. This paper investigates the complexities of multilingual prompt-based code generation. Our evaluations of LLMs, including CODELLAMA and CODEGEMMA, reveal significant disparities in code quality for non-English prompts; we also demonstrate the inadequacy of simple approaches like prompt translation, bootstrapped data augmentation, and fine-tuning. To address this, we propose a zero-shot cross-lingual approach using a neural projection technique, integrating a cross-lingual encoder like LASER to map multilingual embeddings from it into the LLM's token space. This method requires training only on English data and scales effectively to other languages. Results on a translated and quality-checked MBPP dataset show substantial improvements in code quality. This research promotes a more inclusive code generation landscape by empowering LLMs with multilingual capabilities to support the diverse linguistic spectrum in programming.

</details>


### [109] [Mapping Biomedical Ontology Terms to IDs: Effect of Domain Prevalence on Prediction Accuracy](https://arxiv.org/abs/2409.13746)

*Thanh Son Do, Daniel B. Hier, Tayo Obafemi-Ajayi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Biomedical Ontology, Mapping Accuracy, Human Phenotype Ontology, Gene Ontology

**Relevance Score:** 9

**TL;DR:** This study evaluates how well large language models map biomedical ontology terms to their corresponding IDs, finding that ontology ID prevalence in literature predicts mapping accuracy, while high prevalence of some terms leads to improved performance by models like GPT-4.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the mapping capabilities of large language models in relation to the prevalence of biomedical ontology terms in literature, focusing on potential applications in health informatics.

**Method:** The study analyzed mapping accuracy of ontology terms to IDs using counts of ontology IDs in the PubMed Central dataset, employing predictive models based on ROC curves.

**Key Contributions:**

	1. Demonstrated the influence of term prevalence on LLM mapping accuracy in biomedical contexts.
	2. Revealed the high performance of GPT-4 in specific mappings, unlinked to term prevalence.
	3. Proposed integrating ontology ID prevalence as a factor in LLM training for improved biomedical applications.

**Result:** Results indicate a strong correlation between the prevalence of ontology IDs and mapping accuracy for HPO and GO terms, but not for mapping protein names to HUGO symbols, where high accuracy was achieved irrespective of prevalence.

**Limitations:** The study's finding regarding HUGO symbols may not be generalizable to other low-prevalence mappings, indicating a limitation in the applicability of results.

**Conclusion:** Findings highlight LLMs' limitations in mapping low-prevalence ontology IDs and suggest the need to incorporate ontology prevalence into LLM training and evaluation processes.

**Abstract:** This study evaluates the ability of large language models (LLMs) to map biomedical ontology terms to their corresponding ontology IDs across the Human Phenotype Ontology (HPO), Gene Ontology (GO), and UniProtKB terminologies. Using counts of ontology IDs in the PubMed Central (PMC) dataset as a surrogate for their prevalence in the biomedical literature, we examined the relationship between ontology ID prevalence and mapping accuracy. Results indicate that ontology ID prevalence strongly predicts accurate mapping of HPO terms to HPO IDs, GO terms to GO IDs, and protein names to UniProtKB accession numbers. Higher prevalence of ontology IDs in the biomedical literature correlated with higher mapping accuracy. Predictive models based on receiver operating characteristic (ROC) curves confirmed this relationship.   In contrast, this pattern did not apply to mapping protein names to Human Genome Organisation's (HUGO) gene symbols. GPT-4 achieved a high baseline performance (95%) in mapping protein names to HUGO gene symbols, with mapping accuracy unaffected by prevalence. We propose that the high prevalence of HUGO gene symbols in the literature has caused these symbols to become lexicalized, enabling GPT-4 to map protein names to HUGO gene symbols with high accuracy. These findings highlight the limitations of LLMs in mapping ontology terms to low-prevalence ontology IDs and underscore the importance of incorporating ontology ID prevalence into the training and evaluation of LLMs for biomedical applications.

</details>


### [110] [Endless Jailbreaks with Bijection Learning](https://arxiv.org/abs/2410.01294)

*Brian R. Y. Huang, Maximilian Li, Leonard Tang*

**Main category:** cs.CL

**Keywords:** LLMs, bijection learning, adversarial attacks, safety vulnerabilities, in-context learning

**Relevance Score:** 9

**TL;DR:** The paper introduces bijection learning, an attack algorithm that exploits safety vulnerabilities in LLMs by using controlled random encodings to bypass safety measures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety vulnerabilities in LLMs that can be exploited by adversarial inputs or jailbreaks.

**Method:** The authors propose an attack algorithm that employs randomly-generated encodings to teach models bijective encodings, enabling them to bypass built-in safety mechanisms of LLMs through complex fuzzy queries.

**Key Contributions:**

	1. Introduction of bijection learning for LLM vulnerabilities
	2. Demonstration of strong efficacy against frontier LLMs
	3. Identification of the relationship between model capability and vulnerability to bijection attacks

**Result:** The attack is effective against various frontier language models, revealing that more capable models are more susceptible to bijection attacks due to their scaling complexity.

**Limitations:** 

**Conclusion:** The study emphasizes the emerging vulnerabilities in advanced LLMs, suggesting that as models become more capable, they also become more vulnerable to cleverly designed attacks.

**Abstract:** Despite extensive safety measures, LLMs are vulnerable to adversarial inputs, or jailbreaks, which can elicit unsafe behaviors. In this work, we introduce bijection learning, a powerful attack algorithm which automatically fuzzes LLMs for safety vulnerabilities using randomly-generated encodings whose complexity can be tightly controlled. We leverage in-context learning to teach models bijective encodings, pass encoded queries to the model to bypass built-in safety mechanisms, and finally decode responses back into English. Our attack is extremely effective on a wide range of frontier language models. Moreover, by controlling complexity parameters such as number of key-value mappings in the encodings, we find a close relationship between the capability level of the attacked LLM and the average complexity of the most effective bijection attacks. Our work highlights that new vulnerabilities in frontier models can emerge with scale: more capable models are more severely jailbroken by bijection attacks.

</details>


### [111] [Isolated Causal Effects of Natural Language](https://arxiv.org/abs/2410.14812)

*Victoria Lin, Louis-Philippe Morency, Eli Ben-Michael*

**Main category:** cs.CL

**Keywords:** causal effects, language change, reader perceptions, omitted variable bias, language technologies

**Relevance Score:** 6

**TL;DR:** This paper presents a framework for estimating the isolated causal effects of language on reader perceptions, addressing challenges of non-focal language approximation and biases in effect estimates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how language changes impact reader perceptions and behaviors is crucial as language technologies become ubiquitous.

**Method:** The authors introduce a formal estimation framework and evaluate non-focal language approximations and isolated effect estimates to address omitted variable bias.

**Key Contributions:**

	1. Formal estimation framework for isolated causal effects of language
	2. Measures for evaluating quality of non-focal language approximations
	3. Validation of the framework via experiments on real-world data

**Result:** The framework was validated through experiments on both semi-synthetic and real-world data, demonstrating its ability to recover isolated effects and providing measures for evaluating bias due to poor approximations.

**Limitations:** 

**Conclusion:** The study highlights the importance of accurately approximating non-focal language to avoid biased isolated effect estimates.

**Abstract:** As language technologies become widespread, it is important to understand how changes in language affect reader perceptions and behaviors. These relationships may be formalized as the isolated causal effect of some focal language-encoded intervention (e.g., factual inaccuracies) on an external outcome (e.g., readers' beliefs). In this paper, we introduce a formal estimation framework for isolated causal effects of language. We show that a core challenge of estimating isolated effects is the need to approximate all non-focal language outside of the intervention. Drawing on the principle of omitted variable bias, we provide measures for evaluating the quality of both non-focal language approximations and isolated effect estimates themselves. We find that poor approximation of non-focal language can lead to bias in the corresponding isolated effect estimates due to omission of relevant variables, and we show how to assess the sensitivity of effect estimates to such bias along the two key axes of fidelity and overlap. In experiments on semi-synthetic and real-world data, we validate the ability of our framework to correctly recover isolated effects and demonstrate the utility of our proposed measures.

</details>


### [112] [Does Data Contamination Detection Work (Well) for LLMs? A Survey and Evaluation on Detection Assumptions](https://arxiv.org/abs/2410.18966)

*Yujuan Fu, Ozlem Uzuner, Meliha Yetisgen, Fei Xia*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Contamination, Membership Inference Attacks, Evaluation, Assumptions

**Relevance Score:** 8

**TL;DR:** This paper reviews and analyzes data contamination detection methods for LLMs, focusing on the validity of underlying assumptions and the impact of Membership Inference Attacks (MIA).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the concern of data contamination in LLMs, which can inflate performance assessments due to training data overlap with evaluation datasets.

**Method:** A systematic review of 50 papers on data contamination detection was conducted, categorizing the underlying assumptions. Three case studies were performed to test specific assumptions related to Membership Inference Attacks.

**Key Contributions:**

	1. Systematic review of 50 papers on data contamination detection
	2. Identification and categorization of eight underlying assumptions
	3. Case studies demonstrating the limitations of MIA approaches

**Result:** The analysis shows that MIA approaches based on the tested assumptions can perform similarly to random guessing in certain datasets, indicating that LLMs might learn data distributions rather than memorize instances. MIA also struggles with shifts in data distribution between seen and unseen instances.

**Limitations:** The findings may not apply universally across different evaluation contexts, as they are based on specific assumptions.

**Conclusion:** Current methods for detecting data contamination may not be as effective as assumed, highlighting the need for more robust evaluation techniques for LLMs.

**Abstract:** Large language models (LLMs) have demonstrated great performance across various benchmarks, showing potential as general-purpose task solvers. However, as LLMs are typically trained on vast amounts of data, a significant concern in their evaluation is data contamination, where overlap between training data and evaluation datasets inflates performance assessments. Multiple approaches have been developed to identify data contamination. These approaches rely on specific assumptions that may not hold universally across different settings. To bridge this gap, we systematically review 50 papers on data contamination detection, categorize the underlying assumptions, and assess whether they have been rigorously validated. We identify and analyze eight categories of assumptions and test three of them as case studies. Our case studies focus on detecting direct, instance-level data contamination, which is also referred to as Membership Inference Attacks (MIA). Our analysis reveals that MIA approaches based on these three assumptions can have similar performance to random guessing, on datasets used in LLM pretraining, suggesting that current LLMs might learn data distributions rather than memorizing individual instances. Meanwhile, MIA can easily fail when there are data distribution shifts between the seen and unseen instances.

</details>


### [113] [Evaluating Creative Short Story Generation in Humans and Large Language Models](https://arxiv.org/abs/2411.02316)

*Mete Ismayilzada, Claire Stevenson, Lonneke van der Plas*

**Main category:** cs.CL

**Keywords:** creativity, story generation, large language models, human-computer interaction, automated evaluation

**Relevance Score:** 9

**TL;DR:** This paper analyzes the creativity of short story generation in LLMs compared to human writers, focusing on dimensions like novelty and surprise.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched area of creativity in LLMs' story-writing capabilities.

**Method:** A systematic analysis involving 60 LLMs and 60 human participants using a five-sentence cue-word-based story-writing task, along with automated evaluations and human creativity ratings.

**Key Contributions:**

	1. Systematic comparison of creativity in stories generated by LLMs and humans
	2. Development of measures for evaluating narrative creativity
	3. Insights into discrepancies in creativity perception between LLMs and humans

**Result:** LLMs produce stylistically complex stories but lack novelty, surprise, and diversity compared to human writers. Expert ratings align with automated metrics, while non-experts perceive LLM stories as more creative.

**Limitations:** Focuses only on short story generation; Results may not generalize to longer narratives or other types of writing.

**Conclusion:** The differences in creativity ratings between LLMs and humans highlight important implications for understanding both human and artificial creativity.

**Abstract:** Story-writing is a fundamental aspect of human imagination, relying heavily on creativity to produce narratives that are novel, effective, and surprising. While large language models (LLMs) have demonstrated the ability to generate high-quality stories, their creative story-writing capabilities remain under-explored. In this work, we conduct a systematic analysis of creativity in short story generation across 60 LLMs and 60 people using a five-sentence cue-word-based creative story-writing task. We use measures to automatically evaluate model- and human-generated stories across several dimensions of creativity, including novelty, surprise, diversity, and linguistic complexity. We also collect creativity ratings and Turing Test classifications from non-expert and expert human raters and LLMs. Automated metrics show that LLMs generate stylistically complex stories, but tend to fall short in terms of novelty, surprise and diversity when compared to average human writers. Expert ratings generally coincide with automated metrics. However, LLMs and non-experts rate LLM stories to be more creative than human-generated stories. We discuss why and how these differences in ratings occur, and their implications for both human and artificial creativity.

</details>


### [114] [The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models](https://arxiv.org/abs/2411.03700)

*Anaelia Ovalle, Krunoslav Lehman Pavasovic, Louis Martin, Luke Zettlemoyer, Eric Michael Smith, Kai-Wei Chang, Adina Williams, Levent Sagun*

**Main category:** cs.CL

**Keywords:** bias evaluation, gender diversity, large language models, alignment, natural-language processing

**Relevance Score:** 9

**TL;DR:** This paper investigates the potential amplification of harmful biases in large language models (LLMs) during alignment processes, particularly focusing on gender-diverse identities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how alignment techniques in natural-language assistants may perpetuate or amplify biases inherited from pre-aligned models, particularly concerning underrepresented gender-diverse identities.

**Method:** A comprehensive survey of bias evaluation modalities in preference-finetuned LLMs, along with a systematic evaluation of gender-diverse biases across 16 models during Direct Preference Optimization (DPO) stages.

**Key Contributions:**

	1. Survey of bias evaluation modalities in LLMs emphasizing gender-diverse representation
	2. Evaluation of biases across 16 models revealing unaddressed harms
	3. Framework for measuring harmful biases in implicit reward signals

**Result:** The study uncovers critical gaps in gender-diverse representation in bias benchmarks and reveals that DPO-aligned models can amplify harms like stigmatization and gender non-affirmative language.

**Limitations:** 

**Conclusion:** Recommendations are made for adopting community-informed bias evaluation frameworks to better detect and address underrepresented harms in LLMs during alignment practices.

**Abstract:** Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation benchmarks in popular preference-finetuned models, which predominantly focus on dominant social categories, such as binary gender, thereby limiting insights into biases affecting underrepresented groups. Towards addressing this gap, we center transgender, nonbinary, and other gender-diverse identities to investigate how alignment procedures interact with pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a comprehensive survey of bias evaluation modalities across leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 16 models spanning Direct Preference Optimization (DPO) stages, uncovering harms popular bias benchmarks fail to detect, and 3) a flexible framework for measuring harmful biases in implicit reward signals applicable to other social contexts. Our findings reveal that DPO-aligned models are particularly sensitive to supervised finetuning (SFT), and can amplify two forms of real-world gender-diverse harms from their base models: stigmatization and gender non-affirmative language. We conclude with recommendations tailored to DPO and broader alignment practices, advocating for the adoption of community-informed bias evaluation frameworks to more effectively identify and address underrepresented harms in LLMs.

</details>


### [115] [Diversity Helps Jailbreak Large Language Models](https://arxiv.org/abs/2411.04223)

*Weiliang Zhao, Daniel Ben-Levi, Wei Hao, Junfeng Yang, Chengzhi Mao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Jailbreak, LLM safety, Security, Chatbots

**Relevance Score:** 7

**TL;DR:** A novel jailbreak technique for large language models that enhances their ability to bypass safety constraints, successfully compromising leading chatbots.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the flaws in LLM safety training which allows for harmful outputs even in sophisticated models like GPT-4 and Gemini.

**Method:** A technique that instructs LLMs to deviate from prior context, allowing for obfuscation of previous attacks, leading to higher success rates in breaching safety measures.

**Key Contributions:**

	1. Revealing a critical flaw in LLM safety training methods.
	2. Demonstrating a powerful new jailbreak technique with significantly higher success rates.
	3. Highlighting the need for improved security testing in LLMs.

**Result:** Achieved a 62.83% higher success rate using only 12.9% of the queries across ten leading chatbots.

**Limitations:** Focuses primarily on the efficacy of the jailbreak technique; may not address broader implications of such vulnerabilities.

**Conclusion:** Current safety training methods may only mask vulnerabilities rather than eliminate them, necessitating a overhaul in LLM security testing methodologies.

**Abstract:** We have uncovered a powerful jailbreak technique that leverages large language models' ability to diverge from prior context, enabling them to bypass safety constraints and generate harmful outputs. By simply instructing the LLM to deviate and obfuscate previous attacks, our method dramatically outperforms existing approaches, achieving up to a 62.83% higher success rate in compromising ten leading chatbots, including GPT-4, Gemini, and Llama, while using only 12.9% of the queries. This revelation exposes a critical flaw in current LLM safety training, suggesting that existing methods may merely mask vulnerabilities rather than eliminate them. Our findings sound an urgent alarm for the need to revolutionize testing methodologies to ensure robust and reliable LLM security.

</details>


### [116] [XGrammar: Flexible and Efficient Structured Generation Engine for Large Language Models](https://arxiv.org/abs/2411.15100)

*Yixin Dong, Charlie F. Ruan, Yaxing Cai, Ruihang Lai, Ziyi Xu, Yilong Zhao, Tianqi Chen*

**Main category:** cs.CL

**Keywords:** LLM, structure generation, context-free grammar, XGrammar, GPU execution

**Relevance Score:** 6

**TL;DR:** This paper introduces XGrammar, a new structure generation engine for large language models (LLMs) that accelerates context-free grammar execution and achieves significant speed improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing complexity and demand for structured outputs from LLM agents, necessitating an efficient means of generating structured data during LLM inference.

**Method:** XGrammar divides the vocabulary into context-independent and context-dependent tokens, utilizes transformations to expand grammar contexts, implements an efficient persistent stack for token checks, and co-designs the grammar and LLM inference engines for overlapping computations.

**Key Contributions:**

	1. Introduction of XGrammar for structured generation
	2. Up to 100x speedup in context-free grammar execution
	3. Co-design of grammar and LLM inference engines for efficiency

**Result:** XGrammar achieves up to 100x speedup compared to existing structured generation solutions and enables nearly zero overhead in end-to-end low-LLM serving.

**Limitations:** 

**Conclusion:** The proposed XGrammar engine significantly enhances the efficiency of structured generation in LLMs and can facilitate more complex applications with minimal computational overhead.

**Abstract:** The applications of LLM Agents are becoming increasingly complex and diverse, leading to a high demand for structured outputs that can be parsed into code, structured function calls, and embodied agent commands. These developments bring significant demands for structured generation in LLM inference. Context-free grammar is a flexible approach to enable structured generation via constrained decoding. However, executing context-free grammar requires going through several stack states over all tokens in vocabulary during runtime, bringing non-negligible overhead for structured generation. In this paper, we propose XGrammar, a flexible and efficient structure generation engine for large language models. XGrammar accelerates context-free grammar execution by dividing the vocabulary into context-independent tokens that can be prechecked and context-dependent tokens that need to be interpreted during runtime. We further build transformations to expand the grammar context and reduce the number of context-independent tokens. Additionally, we build an efficient persistent stack to accelerate the context-dependent token checks. Finally, we co-design the grammar engine with LLM inference engine to overlap grammar computation with GPU executions. Evaluation results show that XGrammar can achieve up to 100x speedup over existing solutions. Combined with an LLM inference engine, it can generate near-zero overhead structure generation in end-to-end low-LLM serving.

</details>


### [117] [Multi-Party Supervised Fine-tuning of Language Models for Multi-Party Dialogue Generation](https://arxiv.org/abs/2412.05342)

*Xiaoyu Wang, Ningyuan Xi, Teng Chen, Qingqing Gu, Yue Zhao, Xiaokai Chen, Zhonglin Jiang, Yong Chen, Luo Ji*

**Main category:** cs.CL

**Keywords:** Multi-party dialogue, Large Language Models, Fine-tuning, Natural language processing, Conversation generation

**Relevance Score:** 9

**TL;DR:** This paper introduces the MuPaS framework for fine-tuning Large Language Models (LLMs) for effective multi-party dialogue (MPD) performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLMs are primarily trained for dyadic interactions, limiting their applicability in multi-party settings common in meetings and discussions.

**Method:** The authors propose a multi-party fine-tuning framework (MuPaS) specifically designed for multi-party dialogue datasets, along with two training strategies that can act as an MPD simulator.

**Key Contributions:**

	1. Development of the MuPaS framework for multi-party dialogue
	2. Introduction of training strategies for MPD simulation
	3. Demonstration of superior performance metrics in multi-party contexts

**Result:** MuPaS demonstrates state-of-the-art performance in multi-party responses, improved accuracy in predicting the next speaker, and generates high-quality utterances, even in out-of-distribution contexts.

**Limitations:** 

**Conclusion:** The MuPaS framework enables LLMs to engage in complex multi-party applications such as conversation generation and virtual environments.

**Abstract:** Large Language Models (LLM) are usually fine-tuned to participate in dyadic or two-party dialogues, which can not adapt well to multi-party dialogues (MPD), which hinders their applications in such scenarios including multi-personal meetings, discussions and daily communication. Previous LLM-based researches mainly focus on the multi-agent framework, while their base LLMs are still pairwisely fine-tuned. In this work, we design a multi-party fine-tuning framework (MuPaS) for LLMs on the multi-party dialogue datasets, and prove such a straightforward framework can let the LLM align with the multi-party conversation style efficiently and effectively. We also design two training strategies which can convert MuPaS into the MPD simulator. Substantial experiments show that MuPaS can achieve state-of-the-art multi-party response, higher accuracy of the-next-speaker prediction, higher human and automatic evaluated utterance qualities, and can even generate reasonably with out-of-distribution scene, topic and role descriptions. The MuPaS framework bridges the LLM training with more complicated multi-party applications, such as conversation generation, virtual rehearsal or meta-universe.

</details>


### [118] [Advancing Single and Multi-task Text Classification through Large Language Model Fine-tuning](https://arxiv.org/abs/2412.08587)

*Hang Zhao, Qile P. Chen, Yijing Barry Zhang, Gang Yang*

**Main category:** cs.CL

**Keywords:** text classification, large language models, fine-tuning, multi-task learning

**Relevance Score:** 9

**TL;DR:** This study compares encoder-only models and large language models (LLMs) for text classification, highlighting the superior performance of fully fine-tuned Llama3 models over RoBERTa across various tasks and datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically compare the performance of encoder-only models and large language models in text classification, especially when fine-tuning is involved.

**Method:** The study assessed a range of models, including fine-tuned and pre-trained approaches, on the 20 Newsgroups and MASSIVE datasets, and explored multi-task capabilities by combining multiple classification tasks into a single model.

**Key Contributions:**

	1. Comprehensive benchmark of encoder-only vs. LLM performance in text classification
	2. Demonstration of multi-task capabilities in LLMs
	3. Method to combine fine-tuned LLMs for improved efficiency

**Result:** Fully fine-tuned Llama3-70B models outperformed RoBERTa-large and other decoder LLMs in various classification tasks and datasets, with multi-task LLMs matching the performance of dual-model setups.

**Limitations:** 

**Conclusion:** The study benchmarks encoder-only and LLMs for text classification, demonstrating an effective method for consolidating multiple fine-tuned LLMs to achieve reduced latency with equivalent performance.

**Abstract:** Both encoder-only models (e.g., BERT, RoBERTa) and large language models (LLMs, e.g., Llama3) have been widely used for text classification tasks. However, there is a lack of systematic studies comparing the performance of encoder-based models and LLMs in text classification, particularly when fine-tuning is involved. This study employed a diverse range of models and methods, varying in size and architecture, and including both fine-tuned and pre-trained approaches. We first assessed the performances of these LLMs on the 20 Newsgroups (20NG) and MASSIVE datasets, comparing them to encoder-only RoBERTa models. Additionally, we explored the multi-task capabilities of both model types by combining multiple classification tasks, including intent detection and slot-filling, into a single model using data from both datasets. Our results indicate that fully fine-tuned Llama3-70B models outperform RoBERTa-large and other decoder LLMs across various classification tasks and datasets. Moreover, the consolidated multi-task fine-tuned LLMs matched the performance of dual-model setups in both tasks across both datasets. Overall, our study provides a comprehensive benchmark of encoder-only and LLM models on text classification tasks and demonstrates a method to combine two or more fully fine-tuned decoder LLMs for reduced latency and equivalent performance.

</details>


### [119] [Understanding the Impact of Confidence in Retrieval Augmented Generation: A Case Study in the Medical Domain](https://arxiv.org/abs/2412.20309)

*Shintaro Ozaki, Yuta Kato, Siyuan Feng, Masayo Tomita, Kazuki Hayashi, Wataru Hashimoto, Ryoma Obara, Masafumi Oyamada, Katsuhiko Hayashi, Hidetaka Kamigaito, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, Large Language Models, Healthcare, Confidence Calibration, Machine Learning

**Relevance Score:** 9

**TL;DR:** This study examines the impact of Retrieval Augmented Generation (RAG) on the confidence levels of outputs in the medical domain, highlighting variations due to model configurations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the response accuracy of Large Language Models (LLMs) by leveraging external information in high-stakes applications, particularly in healthcare.

**Method:** We evaluate the confidence levels of RAG outputs in medical contexts by measuring Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores, while also analyzing the effect of document order in prompts.

**Key Contributions:**

	1. Investigated the calibration of RAG outputs in healthcare
	2. Exploited Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) as metrics
	3. Showed the impact of document ordering on confidence levels

**Result:** The study finds significant variations in confidence and accuracy based on different models, settings, and input prompt formats.

**Limitations:** 

**Conclusion:** Optimizing configurations based on the specific model and conditions is essential for improving output confidence in RAG applications, especially in healthcare.

**Abstract:** Retrieval Augmented Generation (RAG) complements the knowledge of Large Language Models (LLMs) by leveraging external information to enhance response accuracy for queries. This approach is widely applied in several fields by taking its advantage of injecting the most up-to-date information, and researchers are focusing on understanding and improving this aspect to unlock the full potential of RAG in such high-stakes applications. However, despite the potential of RAG to address these needs, the mechanisms behind the confidence levels of its outputs remain underexplored, although the confidence of information is very critical in some domains, such as finance, healthcare, and medicine. Our study focuses the impact of RAG on confidence within the medical domain under various configurations and models. We evaluate confidence by treating the model's predicted probability as its output and calculating Expected Calibration Error (ECE) and Adaptive Calibration Error (ACE) scores based on the probabilities and accuracy. In addition, we analyze whether the order of retrieved documents within prompts calibrates the confidence. Our findings reveal large variation in confidence and accuracy depending on the model, settings, and the format of input prompts. These results underscore the necessity of optimizing configurations based on the specific model and conditions.

</details>


### [120] [MIRAGE: Exploring How Large Language Models Perform in Complex Social Interactive Environments](https://arxiv.org/abs/2501.01652)

*Yin Cai, Zhouhong Gu, Zhaohan Du, Zheyu Ye, Shaosheng Cao, Yiqian Xu, Hongwei Feng, Ping Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, interactive role-playing, evaluation framework, trust, murder mystery

**Relevance Score:** 7

**TL;DR:** The paper introduces MIRAGE, a framework for evaluating LLMs in interactive role-playing contexts, focusing on murder mystery games.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the ability of LLMs to portray advanced human behaviors and improve interactive experiences through structured evaluations.

**Method:** The MIRAGE framework employs four evaluation methods: Trust Inclination Index (TII), Clue Investigation Capability (CIC), Interactivity Capability Index (ICI), and Script Compliance Index (SCI).

**Key Contributions:**

	1. Introduction of the MIRAGE framework for LLM evaluation
	2. Development of multiple evaluation metrics for assessing interactive capabilities
	3. Provision of scripts and datasets for further research

**Result:** Experiments reveal that popular LLMs like GPT-4 struggle with the complexities of the MIRAGE system, indicating limitations in their role-playing and understanding capabilities.

**Limitations:** The study primarily focuses on murder mystery games, which may not generalize to all interactive scenarios.

**Conclusion:** MIRAGE is a valuable tool for better understanding and improving LLMs' interactive role-playing skills, especially in complex scenarios.

**Abstract:** Large Language Models (LLMs) have shown remarkable capabilities in environmental perception, reasoning-based decision-making, and simulating complex human behaviors, particularly in interactive role-playing contexts. This paper introduces the Multiverse Interactive Role-play Ability General Evaluation (MIRAGE), a comprehensive framework designed to assess LLMs' proficiency in portraying advanced human behaviors through murder mystery games. MIRAGE features eight intricately crafted scripts encompassing diverse themes and styles, providing a rich simulation. To evaluate LLMs' performance, MIRAGE employs four distinct methods: the Trust Inclination Index (TII) to measure dynamics of trust and suspicion, the Clue Investigation Capability (CIC) to measure LLMs' capability of conducting information, the Interactivity Capability Index (ICI) to assess role-playing capabilities and the Script Compliance Index (SCI) to assess LLMs' capability of understanding and following instructions. Our experiments indicate that even popular models like GPT-4 face significant challenges in navigating the complexities presented by the MIRAGE. The datasets and simulation codes are available in \href{https://github.com/lime728/MIRAGE}{github}.

</details>


### [121] [Chain-of-Reasoning: Towards Unified Mathematical Reasoning in Large Language Models via a Multi-Paradigm Perspective](https://arxiv.org/abs/2501.11110)

*Yiyao Yu, Yuxiang Zhang, Dongdong Zhang, Xiao Liang, Hengyuan Zhang, Xingxing Zhang, Mahmoud Khademi, Hany Awadalla, Junjie Wang, Yujiu Yang, Furu Wei*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Reasoning, mathematical reasoning, Natural Language Processing, machine learning

**Relevance Score:** 8

**TL;DR:** Chain-of-Reasoning (CoR) framework integrates multiple reasoning paradigms to improve mathematical reasoning in LLMs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs' reliance on single-paradigm reasoning limits their effectiveness across diverse tasks, necessitating a unified framework to enhance performance.

**Method:** The Chain-of-Reasoning (CoR) framework incorporates Natural Language Reasoning, Algorithmic Reasoning, and Symbolic Reasoning. Progressive Paradigm Training (PPT) is employed for models to progressively master these paradigms.

**Key Contributions:**

	1. Introduction of the Chain-of-Reasoning framework combining multiple reasoning paradigms.
	2. Implementation of Progressive Paradigm Training for model mastery of reasoning approaches.
	3. Demonstrated significant performance improvements on established benchmarks.

**Result:** CoR-Math-7B significantly outperforms SOTA models, showing up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% enhancement over RL-based methods on arithmetic tasks in the MATH benchmark.

**Limitations:** 

**Conclusion:** The CoR framework improves mathematical comprehension in LLMs, enabling better zero-shot generalization across various tasks.

**Abstract:** Large Language Models (LLMs) have made notable progress in mathematical reasoning, yet often rely on single-paradigm reasoning, limiting their effectiveness across diverse tasks. We introduce Chain-of-Reasoning (CoR), a novel unified framework integrating multiple reasoning paradigms--Natural Language Reasoning (NLR), Algorithmic Reasoning (AR), and Symbolic Reasoning (SR)--to enable synergistic collaboration. CoR generates multiple potential answers via different reasoning paradigms and synthesizes them into a coherent final solution. We propose a Progressive Paradigm Training (PPT) strategy for models to progressively master these paradigms, leading to CoR-Math-7B. Experimental results demonstrate that CoR-Math-7B significantly outperforms current SOTA models, achieving up to a 41.0% absolute improvement over GPT-4o in theorem proving and a 15.0% improvement over RL-based methods on the MATH benchmark in arithmetic tasks. These results show the enhanced mathematical comprehension ability of our model, enabling zero-shot generalization across tasks.

</details>


### [122] [Softplus Attention with Re-weighting Boosts Length Extrapolation in Large Language Models](https://arxiv.org/abs/2501.13428)

*Bo Gao, Michael W. Spratling*

**Main category:** cs.CL

**Keywords:** large language models, attention mechanism, Softmax, numerical stability, inference token length

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel attention mechanism that improves upon traditional Softmax attention by addressing numerical instability and performance issues with longer inference token lengths.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to enhance the performance of large language models during inference, particularly as token lengths increase, which leads to challenges with traditional Softmax attention mechanisms.

**Method:** The paper introduces a new attention mechanism by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm, utilizing the Softplus activation function and a dynamic scale factor to adapt to different token lengths.

**Key Contributions:**

	1. A novel attention mechanism that replaces Softmax with a non-linear transformation and $l_1$-norm
	2. Use of Softplus activation function to enhance stability
	3. Introduction of a dynamic scale factor based on invariance entropy for varying token lengths

**Result:** The proposed attention mechanism demonstrates better performance than traditional Softmax attention across various inference lengths, maintaining low validation loss even at significantly increased training token lengths.

**Limitations:** 

**Conclusion:** The dynamic scale factor and re-weighting mechanism allow the model to maintain numerical stability and concentrate on the most relevant tokens, resulting in superior outcomes on downstream benchmarks.

**Abstract:** Large language models have achieved remarkable success in recent years, primarily due to the implementation of self-attention mechanisms. However, traditional Softmax attention suffers from numerical instability and reduced performance as the length of inference tokens increases. This paper addresses these issues by decomposing the Softmax operation into a non-linear transformation and the $l_1$-norm. We identify the latter as essential for maintaining model performance. By replacing the non-linear transformation with the Softplus activation function and introducing a dynamic scale factor for different token lengths based on invariance entropy, we create a novel attention mechanism with performance better than conventional Softmax attention across various inference lengths. To further improve the length extrapolation ability of the proposed attention mechanism, we introduce a novel re-weighting mechanism that amplifies significant attention weights while diminishing weaker ones, enabling the model to concentrate more effectively on relevant tokens. When combined with our proposed attention mechanism, this approach maintains nearly constant validation loss even at 16$\times$ the training token length, ensures numerical stability, and achieves superior results on downstream benchmarks.

</details>


### [123] [Visual Theory of Mind Enables the Invention of Proto-Writing](https://arxiv.org/abs/2502.01568)

*Benjamin A. Spiegel, Lucas Gelfond, George Konidaris*

**Main category:** cs.CL

**Keywords:** communication, pictographs, reinforcement learning, cognition, proto-writing

**Relevance Score:** 4

**TL;DR:** A study on the emergence of symbolic writing systems using a multi-agent reinforcement learning framework to model communication through pictographs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the cognitive and cultural processes behind the emergence of proto-writing, addressing the gap in naturalistic methodologies in previous studies.

**Method:** Development of a multi-agent reinforcement learning testbed called a Signification Game, where agents use visual theory of mind to communicate actions with pictographs.

**Key Contributions:**

	1. Introduction of the Signification Game for studying emergent communication
	2. Integration of visual theory of mind in agent communication
	3. Insights into proto-writing from an animal cognition perspective

**Result:** The model demonstrates how agents can utilize pictographs for communication, providing insights into the evolution of symbolic writing systems.

**Limitations:** 

**Conclusion:** The findings highlight the relevance of cognitive processes in the development of communication systems, linking them to proto-writing emergence.

**Abstract:** Symbolic writing systems are graphical semiotic codes that are ubiquitous in modern society but are otherwise absent in the animal kingdom. Anthropological evidence suggests that the earliest forms of some writing systems originally consisted of iconic pictographs, which signify their referent via visual resemblance. While previous studies have examined the emergence and, separately, the evolution of pictographic systems through a computational lens, most employ non-naturalistic methodologies that make it difficult to draw clear analogies to human and animal cognition. We develop a multi-agent reinforcement learning testbed for emergent communication called a Signification Game, and formulate a model of inferential communication that enables agents to leverage visual theory of mind to communicate actions using pictographs. Our model, which is situated within a broader formalism for animal communication, sheds light on the cognitive and cultural processes underlying the emergence of proto-writing.

</details>


### [124] [VLM2-Bench: A Closer Look at How Well VLMs Implicitly Link Explicit Matching Visual Cues](https://arxiv.org/abs/2502.12084)

*Jianshu Zhang, Dongyu Yao, Renjie Pi, Paul Pu Liang, Yi R. Fung*

**Main category:** cs.CL

**Keywords:** Vision-language models, Visual linking, Benchmarking, Machine learning, Human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper introduces VLM2-Bench, a benchmark to assess the ability of vision-language models (VLMs) to visually link matching cues across multiple tasks and test cases.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the unexplored capability of vision-language models in linking visual cues, a crucial ability in daily life tasks.

**Method:** The authors introduce VLM2-Bench, a benchmark with 9 subtasks and over 3,000 test cases, evaluating various open-source VLMs and GPT-4o using different prompting methods.

**Key Contributions:**

	1. Introduction of VLM2-Bench benchmark for assessing VLMs' capability in visual cue linking.
	2. Identification of a performance gap between VLMs and human capabilities in visual matching tasks.
	3. Recommendations for enhancing model training and integration of language reasoning with visual tasks.

**Result:** The evaluation revealed significant challenges for VLMs in linking visual cues, with GPT-4o underperforming compared to humans by 34.80%.

**Limitations:** The study mainly focuses on VLMs and their performance without delving into potential improvements in underlying algorithms.

**Conclusion:** The findings advocate for improving visual capabilities, establishing better integration principles for language reasoning in vision tasks, and modifying training paradigms to enhance models' relationship inference abilities.

**Abstract:** Visually linking matching cues is a crucial ability in daily life, such as identifying the same person in multiple photos based on their cues, even without knowing who they are. Despite the extensive knowledge that vision-language models (VLMs) possess, it remains largely unexplored whether they are capable of performing this fundamental task. To address this, we introduce VLM2-Bench, a benchmark designed to assess whether VLMs can Visually Link Matching cues, with 9 subtasks and over 3,000 test cases. Comprehensive evaluation across eight open-source VLMs and GPT-4o, along with further analysis of various language-side and vision-side prompting methods, leads to a total of eight key findings. We identify critical challenges in models' ability to link visual cues, highlighting a significant performance gap where even GPT-4o lags 34.80% behind humans. Based on these insights, we advocate for (i) enhancing core visual capabilities to improve adaptability and reduce reliance on prior knowledge, (ii) establishing clearer principles for integrating language-based reasoning in vision-centric tasks to prevent unnecessary biases, and (iii) shifting vision-text training paradigms toward fostering models' ability to independently structure and infer relationships among visual cues.

</details>


### [125] [None of the Others: a General Technique to Distinguish Reasoning from Memorization in Multiple-Choice LLM Evaluation Benchmarks](https://arxiv.org/abs/2502.12896)

*Eva SÃ¡nchez Salido, Julio Gonzalo, Guillermo Marco*

**Main category:** cs.CL

**Keywords:** LLM evaluations, reasoning, memorization, MMLU, UNED-Access

**Relevance Score:** 8

**TL;DR:** This paper introduces a variation method for evaluating LLMs that separates answers from previously seen concepts, resulting in significant accuracy drops and insights into the reasoning capabilities of language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLM performance beyond recall/memorization by requiring reasoning in answering varied multiple-choice questions.

**Method:** A general variation method for multiple-choice questions that dissociates answers from prior tokens or concepts is introduced and evaluated on two datasets (MMLU and UNED-Access).

**Key Contributions:**

	1. Introduction of a novel variation method for LLM evaluation
	2. Demonstration of accuracy disparity among state-of-the-art models
	3. Insights into language contamination affecting model performance

**Result:** All evaluated models showed significant accuracy drops, averaging 57% on MMLU and 50% on UNED-Access 2024, indicating reliance on memorization in LLM responses.

**Limitations:** The evaluation primarily stems from two datasets, which may limit generalizability.

**Conclusion:** The findings highlight that high performance in standard evaluations may not correlate with better reasoning abilities, emphasizing the role of memorization in responses.

**Abstract:** In LLM evaluations, reasoning is often distinguished from recall/memorization by performing numerical variations to math-oriented questions. Here we introduce a general variation method for multiple-choice questions that completely dissociates the correct answer from previously seen tokens or concepts, requiring LLMs to understand and reason (rather than memorizing) in order to answer correctly. Using this method, we evaluate state-of-the-art proprietary and open-source LLMs on two datasets available in English and Spanish: the public MMLU benchmark and the private UNED-Access 2024 dataset. Results show that all models experience remarkable accuracy drops under our proposed variation, with an average loss of 57% on MMLU and 50% on UNED-Access 2024, ranging from 10% to 93% across models. Notably, the most accurate model in our experimentation (OpenAI-o3-mini) is not the most robust (DeepSeek-R1-70B), suggesting that the best models in standard evaluations may not be the ones with better reasoning capabilities. Also, we see larger accuracy drops in public (vs private) datasets and questions posed in their original language (vs a manual translation), which are signs of contamination and also point to a relevant role of recall/memorization in current LLMs' answers.

</details>


### [126] [SemViQA: A Semantic Question Answering System for Vietnamese Information Fact-Checking](https://arxiv.org/abs/2503.00955)

*Dien X. Tran, Nam V. Nguyen, Thanh T. Tran, Anh T. Hoang, Tai V. Duong, Di T. Le, Phuc-Lu Le*

**Main category:** cs.CL

**Keywords:** fact-checking, Vietnamese, misinformation, Semantic Evidence Retrieval, Two-step Verdict Classification

**Relevance Score:** 6

**TL;DR:** SemViQA is a novel Vietnamese fact-checking framework that enhances misinformation detection in low-resource languages.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** Address the rise of misinformation exacerbated by LLMs, particularly for low-resource languages like Vietnamese.

**Method:** Integrates Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC) to improve accuracy and efficiency in fact-checking.

**Key Contributions:**

	1. Introduces a comprehensive framework for fact-checking in Vietnamese
	2. Achieves top-ranking performance in key datasets
	3. Enhances inference speed significantly while maintaining accuracy.

**Result:** Achieves state-of-the-art results with 78.97% strict accuracy on ISE-DSC01 and 80.82% on ViWikiFC, and improves inference speed 7x with SemViQA Faster.

**Limitations:** 

**Conclusion:** SemViQA sets a new benchmark for Vietnamese fact verification and significantly contributes to the fight against misinformation.

**Abstract:** The rise of misinformation, exacerbated by Large Language Models (LLMs) like GPT and Gemini, demands robust fact-checking solutions, especially for low-resource languages like Vietnamese. Existing methods struggle with semantic ambiguity, homonyms, and complex linguistic structures, often trading accuracy for efficiency. We introduce SemViQA, a novel Vietnamese fact-checking framework integrating Semantic-based Evidence Retrieval (SER) and Two-step Verdict Classification (TVC). Our approach balances precision and speed, achieving state-of-the-art results with 78.97\% strict accuracy on ISE-DSC01 and 80.82\% on ViWikiFC, securing 1st place in the UIT Data Science Challenge. Additionally, SemViQA Faster improves inference speed 7x while maintaining competitive accuracy. SemViQA sets a new benchmark for Vietnamese fact verification, advancing the fight against misinformation. The source code is available at: https://github.com/DAVID-NGUYEN-S16/SemViQA.

</details>


### [127] [HREB-CRF: Hierarchical Reduced-bias EMA for Chinese Named Entity Recognition](https://arxiv.org/abs/2503.01217)

*Sijin Sun, Ming Deng, Xinrui Yu, Liangbin Zhao*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Chinese, Hierarchical Attention, CRF, Machine Learning

**Relevance Score:** 2

**TL;DR:** This paper presents the HREB-CRF framework to improve Chinese Named Entity Recognition by addressing boundary division and semantic representation issues.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Errors in Chinese Named Entity Recognition are often caused by incorrect boundary division, complex semantic representation, and pronunciation differences.

**Method:** The HREB-CRF framework employs a Hierarchical Reduced-bias EMA with CRF to amplify word boundaries and utilize long text gradients through a fixed-bias weighted average of local and global hierarchical attention.

**Key Contributions:**

	1. Introduction of the HREB-CRF framework for CNER.
	2. Utilization of hierarchical attention mechanisms for better boundary detection.
	3. Demonstrated performance improvement over baseline models across various datasets.

**Result:** The method significantly enhances F1 scores, outperforming baseline models on the MSRA, Resume, and Weibo datasets by up to 9.8%.

**Limitations:** 

**Conclusion:** The HREB-CRF framework shows strong effectiveness and robustness in improving performance in Chinese Named Entity Recognition tasks.

**Abstract:** Incorrect boundary division, complex semantic representation, and differences in pronunciation and meaning often lead to errors in Chinese Named Entity Recognition(CNER). To address these issues, this paper proposes HREB-CRF framework: Hierarchical Reduced-bias EMA with CRF. The proposed method amplifies word boundaries and pools long text gradients through exponentially fixed-bias weighted average of local and global hierarchical attention. Experimental results on the MSRA, Resume, and Weibo datasets show excellent in F1, outperforming the baseline model by 1.1\%, 1.6\%, and 9.8\%. The significant improvement in F1 shows evidences of strong effectiveness and robustness of approach in CNER tasks.

</details>


### [128] [Can (A)I Change Your Mind?](https://arxiv.org/abs/2503.01844)

*Miriam Havin, Timna Wharton Kleinman, Moran Koren, Yaniv Dover, Ariel Goldstein*

**Main category:** cs.CL

**Keywords:** Large Language Models, Persuasion, Human-Computer Interaction, Public Opinion, Naturalistic Study

**Relevance Score:** 9

**TL;DR:** This study examines the persuasive effects of LLM-based conversational agents compared to human interlocutors in debates over civil policy topics, showing that both types can significantly influence opinions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the persuasive capabilities of LLM-based agents in real-world scenarios rather than controlled environments, particularly in Hebrew-speaking contexts.

**Method:** The study involved 200 participants interacting with both LLM and human interlocutors on controversial topics through static (written) and dynamic (conversational) formats in Hebrew.

**Key Contributions:**

	1. Investigation of LLM persuasiveness in naturalistic settings
	2. Comparison of LLM agents and human interlocutors
	3. Insights into audience confidence dynamics during interactions

**Result:** Participants showed similar opinion changes when interacting with LLMs and humans, with significant confidence increases in most scenarios except for static LLM interactions.

**Limitations:** The study is limited to a Hebrew-speaking population and may not generalize to other languages or cultures.

**Conclusion:** LLM-based agents exhibit strong persuasive abilities across various contexts, indicating their significance in shaping public opinion.

**Abstract:** The increasing integration of large language model (LLM) based conversational agents into everyday life raises critical cognitive and social questions about their potential to influence human opinions. Although previous studies have shown that LLM-based agents can generate persuasive content, these typically involve controlled, English-language settings. Addressing this, our preregistered study explored LLM's persuasive capabilities in more ecological, unconstrained scenarios, examining both static (written paragraphs) and dynamic (conversations via Telegram) interaction types. Conducted entirely in Hebrew with 200 participants, the study assessed the persuasive effects of both LLM and human interlocutors on controversial civil policy topics. Results indicated that participants adopted LLM and human perspectives similarly, with significant opinion changes evident across all conditions, regardless of interlocutor type or interaction mode. Confidence levels increased significantly in most scenarios, except in static LLM interactions. These findings demonstrate LLM-based agents' robust persuasive capabilities across diverse sources and settings, highlighting their potential impact on shaping public opinions.

</details>


### [129] [NCL-UoR at SemEval-2025 Task 3: Detecting Multilingual Hallucination and Related Observable Overgeneration Text Spans with Modified RefChecker and Modified SeflCheckGPT](https://arxiv.org/abs/2503.01921)

*Jiaying Hong, Thanet Markchom, Jianfei Xu, Tong Wu, Huizhi Liang*

**Main category:** cs.CL

**Keywords:** hallucinations, large language models, factual verification, self-check, machine learning

**Relevance Score:** 9

**TL;DR:** This paper addresses the detection of hallucinations in LLM-generated content across multiple languages using two novel methods: modified RefChecker and modified SelfCheckGPT.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve hallucination detection in content produced by large language models, which is crucial for ensuring accuracy and reliability in AI-generated texts.

**Method:** The paper proposes two methods: modified RefChecker, which utilizes prompt-based factual verification structured as claim-based tests, and modified SelfCheckGPT, which enhances LLMs with external knowledge to detect hallucinations more effectively.

**Key Contributions:**

	1. Introduction of modified RefChecker for claim-based factual verification.
	2. Development of modified SelfCheckGPT that leverages external knowledge.
	3. Enhanced prompt designs for identifying hallucinated content.

**Result:** The experimental results show that both methods are effective at detecting hallucinations, achieving high performance with an average IoU of 0.5310 and an average COR of 0.5669 across multiple languages.

**Limitations:** 

**Conclusion:** The introduction of modified RefChecker and self-check mechanisms improves the capability of LLMs to identify hallucinations, thus enhancing the reliability of AI-generated content.

**Abstract:** SemEval-2025 Task 3 (Mu-SHROOM) focuses on detecting hallucinations in content generated by various large language models (LLMs) across multiple languages. This task involves not only identifying the presence of hallucinations but also pinpointing their specific occurrences. To tackle this challenge, this study introduces two methods: modified RefChecker and modified SelfCheckGPT. The modified RefChecker integrates prompt-based factual verification into References, structuring them as claim-based tests rather than single external knowledge sources. The modified SelfCheckGPT incorporates external knowledge to overcome its reliance on internal knowledge. In addition, both methods' original prompt designs are enhanced to identify hallucinated words within LLM-generated texts. Experimental results demonstrate the effectiveness of the approach, achieving a high ranking on the test dataset in detecting hallucinations across various languages, with an average IoU of 0.5310 and an average COR of 0.5669.

</details>


### [130] [The Devil Is in the Details: Tackling Unimodal Spurious Correlations for Generalizable Multimodal Reward Models](https://arxiv.org/abs/2503.03122)

*Zichao Li, Xueru Wen, Jie Lou, Yuqiu Ji, Yaojie Lu, Xianpei Han, Debing Zhang, Le Sun*

**Main category:** cs.CL

**Keywords:** Multimodal Reward Models, Large Language Models, generalization

**Relevance Score:** 8

**TL;DR:** This paper introduces a new learning algorithm for Multimodal Reward Models (MM-RMs) that improves their ability to generalize across different data distributions by reducing reliance on unimodal correlations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** MM-RMs are essential for aligning LLMs with human preferences, yet they struggle with generalization due to spurious correlations in existing datasets.

**Method:** The authors propose a Shortcut-aware MM-RM learning algorithm that dynamically reweights training samples to foster better multimodal understanding.

**Key Contributions:**

	1. Introduction of Shortcut-aware MM-RM learning algorithm
	2. Demonstrated improvements in generalization and task performance
	3. Provided a scalable approach for multimodal reward modeling

**Result:** The proposed algorithm shows significant improvements in generalization, task performance, and scalability in tests conducted by the authors.

**Limitations:** 

**Conclusion:** The research establishes a more robust framework for multimodal reward modeling, potentially enhancing the interaction of LLMs with multimodal data.

**Abstract:** Multimodal Reward Models (MM-RMs) are crucial for aligning Large Language Models (LLMs) with human preferences, particularly as LLMs increasingly interact with multimodal data. However, we find that MM-RMs trained on existing datasets often struggle to generalize to out-of-distribution data due to their reliance on unimodal spurious correlations, primarily text-only shortcuts within the training distribution, which prevents them from leveraging true multimodal reward functions. To address this, we introduce a Shortcut-aware MM-RM learning algorithm that mitigates this issue by dynamically reweighting training samples, shifting the distribution toward better multimodal understanding, and reducing dependence on unimodal spurious correlations. Our experiments demonstrate significant improvements in generalization, downstream task performance, and scalability, establishing a more robust framework for multimodal reward modeling.

</details>


### [131] [A Hybrid Architecture with Efficient Fine Tuning for Abstractive Patent Document Summarization](https://arxiv.org/abs/2503.10354)

*Nevidu Jayatilleke, Ruvan Weerasinghe*

**Main category:** cs.CL

**Keywords:** patent summarization, natural language processing, extractive summarization, abstractive summarization, machine learning

**Relevance Score:** 5

**TL;DR:** This paper presents a hybrid framework for automatic patent summarization combining extractive and abstractive methods using LexRank and a fine-tuned BART model.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing number of patents and their complex writing style necessitate efficient summarization methods to aid in patent analysis and comprehension.

**Method:** A hybrid framework using extractive and abstractive summarization methods, leveraging the LexRank algorithm for sentence retrieval and a fine-tuned BART model for generating summaries.

**Key Contributions:**

	1. Hybrid summarization framework combining extractive and abstractive methods
	2. Use of LexRank for important sentence retrieval
	3. Application of fine-tuned BART with LoRA for text summarization

**Result:** The proposed system efficiently generates abstractive summaries of patent documents, demonstrating improved efficacy in extracting pertinent information.

**Limitations:** The study may be limited by the specific domains of patents tested and the scalability of the model to all patent fields.

**Conclusion:** The study effectively combines advanced NLP techniques to address the challenge of summarizing complex patent documents, contributing to better patent analysis.

**Abstract:** Automatic patent summarization approaches that help in the patent analysis and comprehension procedure are in high demand due to the colossal growth of innovations. The development of natural language processing (NLP), text mining, and deep learning has notably amplified the efficacy of text summarization models for abundant types of documents. Summarizing patent text remains a pertinent challenge due to the labyrinthine writing style of these documents, which includes technical and legal intricacies. Additionally, these patent document contents are considerably lengthier than archetypal documents, which complicates the process of extracting pertinent information for summarization. Embodying extractive and abstractive text summarization methodologies into a hybrid framework, this study proposes a system for efficiently creating abstractive summaries of patent records. The procedure involves leveraging the LexRank graph-based algorithm to retrieve the important sentences from input parent texts, then utilizing a Bidirectional Auto-Regressive Transformer (BART) model that has been fine-tuned using Low-Ranking Adaptation (LoRA) for producing text summaries. This is accompanied by methodical testing and evaluation strategies. Furthermore, the author employed certain meta-learning techniques to achieve Domain Generalization (DG) of the abstractive component across multiple patent fields.

</details>


### [132] [Methods for Recognizing Nested Terms](https://arxiv.org/abs/2504.16007)

*Igor Rozhkov, Natalia Loukachevitch*

**Main category:** cs.CL

**Keywords:** nested terms, Binder model, term recognition, RuTermEval, computational linguistics

**Relevance Score:** 4

**TL;DR:** This paper presents the application of the Binder model for extracting nested terms, achieving the best results in the RuTermEval competition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this work stems from the need to effectively extract nested terms in computational linguistics, particularly from datasets which lack nested labeling.

**Method:** We utilized the Binder model, previously effective for nested named entities, to recognize and extract nested terms from both labeled and flat training data.

**Key Contributions:**

	1. Utilization of the Binder model for nested term extraction
	2. Best results in the RuTermEval competition
	3. Successful extraction of nested terms from flat annotated data

**Result:** Our approach yielded the best term recognition results across all tracks in the RuTermEval competition, indicating the effectiveness of our proposed methods.

**Limitations:** 

**Conclusion:** We concluded that several approaches we developed are effective for retrieving nested terms, even when trained on flat datasets without nested labels.

**Abstract:** In this paper, we describe our participation in the RuTermEval competition devoted to extracting nested terms. We apply the Binder model, which was previously successfully applied to the recognition of nested named entities, to extract nested terms. We obtained the best results of term recognition in all three tracks of the RuTermEval competition. In addition, we study the new task of recognition of nested terms from flat training data annotated with terms without nestedness. We can conclude that several approaches we proposed in this work are viable enough to retrieve nested terms effectively without nested labeling of them.

</details>
