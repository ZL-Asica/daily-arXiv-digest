# 2025-08-27

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 66]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Technology-assisted Personalized Yoga for Better Health -- Challenges and Outlook](https://arxiv.org/abs/2508.18283)

*Vivek Kumar, Himanshu Sahu, Hari Prabhat Gupta, Biplav Srivastava*

**Main category:** cs.HC

**Keywords:** Yoga personalization, decision support, pose sensing, machine learning, well-being

**Relevance Score:** 4

**TL;DR:** This vision paper addresses the challenges of personalizing Yoga practices to individual needs using a multidisciplinary computing approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To promote the effective practice of Yoga tailored to individual requirements by overcoming the complexities in selecting suitable poses and adjustments.

**Method:** The paper outlines a preliminary approach to Yoga personalization, discussing challenges in pose sensing and corrections recommendations, illustrated through a case study of Surya Namaskar.

**Key Contributions:**

	1. First comprehensive examination of Yoga personalization challenges.
	2. Introduction of decision support for pose sensing and recommendations.
	3. Case study application to demonstrate practical implications.

**Result:** It highlights how decision support systems can be developed for Yoga personalization, affecting well-being through tailored practices.

**Limitations:** The approach is preliminary and may need further validation and testing with real users.

**Conclusion:** The work emphasizes the need for a comprehensive examination of decision support issues in Yoga personalization using technological advancements.

**Abstract:** Yoga is a discipline of physical postures, breathing techniques, and meditative practices rooted in ancient Indian traditions, now embraced worldwide for promoting overall well-being and inner balance. The practices are a large set of items, our term for executable actions like physical poses or breath exercises, to offer for a person's well-being. However, to get benefits of Yoga tailored to a person's unique needs, a person needs to (a) discover their subset from the large and seemingly complex set with inter-dependencies, (b) continue to follow them with interest adjusted to their changing abilities and near-term objectives, and (c) as appropriate, adapt to alternative items based on changing environment and the person's health conditions. In this vision paper, we describe the challenges for the Yoga personalization problem. Next, we sketch a preliminary approach and use the experience to provide an outlook on solving the challenging problem using existing and novel techniques from a multidisciplinary computing perspective. To the best of our knowledge, this is the first paper that comprehensively examines decision support issues around Yoga personalization, from pose sensing to recommendation of corrections for a complete regimen, and illustrates with a case study of Surya Namaskar -- a set of 12 choreographed poses.

</details>


### [2] [Does Calibration Affect Human Actions?](https://arxiv.org/abs/2508.18317)

*Meir Nizri, Amos Azaria, Chirag Gupta, Noam Hazon*

**Main category:** cs.HC

**Keywords:** machine learning, calibration, trust, decision-making, HCI

**Relevance Score:** 8

**TL;DR:** The paper explores how calibrating machine learning classifiers impacts human decision-making and trust through an HCI experiment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of model calibration on human trust and decision-making in machine learning predictions.

**Method:** A Human-Computer-Interaction experiment was conducted to evaluate how calibration affects trust and correlation between human decisions and model predictions.

**Key Contributions:**

	1. Investigation of the impact of model calibration on non-expert human decisions
	2. Application of prospect theory to adjust calibrated scores
	3. Insights into trust dynamics regarding machine learning predictions

**Result:** Calibration improves trust and correlation between decisions and predictions, but additional corrections based on prospect theory are necessary for significant improvements.

**Limitations:** The study focuses on non-expert humans, which may limit generalizability to expert environments.

**Conclusion:** Calibration alone is insufficient; incorporating prospect theory corrections is vital for enhancing the relationship between human decisions and model predictions.

**Abstract:** Calibration has been proposed as a way to enhance the reliability and adoption of machine learning classifiers. We study a particular aspect of this proposal: how does calibrating a classification model affect the decisions made by non-expert humans consuming the model's predictions? We perform a Human-Computer-Interaction (HCI) experiment to ascertain the effect of calibration on (i) trust in the model, and (ii) the correlation between decisions and predictions. We also propose further corrections to the reported calibrated scores based on Kahneman and Tversky's prospect theory from behavioral economics, and study the effect of these corrections on trust and decision-making. We find that calibration is not sufficient on its own; the prospect theory correction is crucial for increasing the correlation between human decisions and the model's predictions. While this increased correlation suggests higher trust in the model, responses to ``Do you trust the model more?" are unaffected by the method used.

</details>


### [3] [Impact of Target and Tool Visualization on Depth Perception and Usability in Optical See-Through AR](https://arxiv.org/abs/2508.18481)

*Yue Yang, Xue Xie, Xinkai Wang, Hui Zhang, Chiming Yu, Xiaoxian Xiong, Lifeng Zhu, Yuanyi Zheng, Jue Cen, Bruce Daniel, Fred Baik*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Depth Perception, Holography, Usability, Transparency

**Relevance Score:** 8

**TL;DR:** The paper evaluates the impact of transparency and tool visualization on depth perception and usability in optical see-through augmented reality systems, particularly in surgical applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in depth perception and occlusion in optical see-through AR systems like Microsoft HoloLens 2 during arm's distance guidance tasks.

**Method:** Two experiments were conducted with ten participants: Experiment 1 compared high-transparency and low-transparency target rendering in a depth matching task, while Experiment 2 analyzed a simulated surgical task under various visualization conditions (virtual tool hologram, real tool, no tool tracking).

**Key Contributions:**

	1. Empirical evaluation of transparency effects on depth perception in OST-AR.
	2. Demonstrated importance of real tool visibility for accuracy and usability.
	3. Provided design recommendations for arm-distance AR systems regarding transparency and occlusion.

**Result:** The study found that opaque targets significantly reduce depth estimation error compared to highly transparent ones, and using a real tool improved accuracy and usability, unlike no tool tracking which performed the worst.

**Limitations:** 

**Conclusion:** Correct occlusion, opaque rendering, and real-time tracking of tools enhance depth perception and task performance in OST-AR, suggesting designers prioritize robust occlusion and tool visibility.

**Abstract:** Optical see-through augmented reality (OST-AR) systems like Microsoft HoloLens 2 hold promise for arm's distance guidance (e.g., surgery), but depth perception of the hologram and occlusion of real instruments remain challenging. We present an evaluation of how visualizing the target object with different transparencies and visualizing a tracked tool (virtual proxy vs. real tool vs. no tool tracking) affects depth perception and system usability. Ten participants performed two experiments on HoloLens 2. In Experiment 1, we compared high-transparency vs. low-transparency target rendering in a depth matching task at arm's length. In Experiment 2, participants performed a simulated surgical pinpoint task on a frontal bone target under six visualization conditions ($2 \times 3$: two target transparencies and three tool visualization modes: virtual tool hologram, real tool, or no tool tracking). We collected data on depth matching error, target localization error, system usability, task workload, and qualitative feedback. Results show that a more opaque target yields significantly lower depth estimation error than a highly transparent target at arm's distance. Moreover, showing the real tool (occluding the virtual target) led to the highest accuracy and usability with the lowest workload, while not tracking the tool yielded the worst performance and user ratings. However, making the target highly transparent, while allowing the real tool to remain visible, slightly impaired depth cues and did not improve usability. Our findings underscore that correct occlusion cues, rendering virtual content opaque and occluding it with real tools in real time, are critical for depth perception and precision in OST-AR. Designers of arm-distance AR systems should prioritize robust tool tracking and occlusion handling; if unavailable, cautiously use transparency to balance depth perception and tool visibility.

</details>


### [4] [Skeptik: A Hybrid Framework for Combating Potential Misinformation in Journalism](https://arxiv.org/abs/2508.18499)

*Arlen Fan, Fan Lei, Steven R. Corman, Ross Maciejewski*

**Main category:** cs.HC

**Keywords:** misinformation, logical fallacies, Large Language Models, media literacy, news articles

**Relevance Score:** 8

**TL;DR:** Skeptik is a hybrid framework that uses Large Language Models to detect logical fallacies in online news articles, enhancing media literacy and critical reading.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by misinformation stemming from flawed reasoning and logical fallacies in journalism.

**Method:** A web browser extension that integrates LLMs with heuristic approaches to analyze, annotate, and highlight potential logical fallacies in news articles.

**Key Contributions:**

	1. Development of an expandable classification system for logical fallacies
	2. Integration of LLMs for real-time analysis and annotation
	3. Creation of an interactive user interface for user engagement

**Result:** Skeptik effectively enhances readers' critical examination of news content and promotes media literacy through detailed explanations and interventions.

**Limitations:** 

**Conclusion:** The framework aims to improve critical reading and protect the public from deceptive information while enhancing news media credibility.

**Abstract:** The proliferation of misinformation in journalism, often stemming from flawed reasoning and logical fallacies, poses significant challenges to public understanding and trust in news media. Traditional fact-checking methods, while valuable, are insufficient for detecting the subtle logical inconsistencies that can mislead readers within seemingly factual content. To address this gap, we introduce Skeptik, a hybrid framework that integrates Large Language Models (LLMs) with heuristic approaches to analyze and annotate potential logical fallacies and reasoning errors in online news articles. Operating as a web browser extension, Skeptik automatically highlights sentences that may contain logical fallacies, provides detailed explanations, and offers multi-layered interventions to help readers critically assess the information presented. The system is designed to be extensible, accommodating a wide range of fallacy types and adapting to evolving misinformation tactics. Through comprehensive case studies, quantitative analyses, usability experiments, and expert evaluations, we demonstrate the effectiveness of Skeptik in enhancing readers' critical examination of news content and promoting media literacy. Our contributions include the development of an expandable classification system for logical fallacies, the innovative integration of LLMs for real-time analysis and annotation, and the creation of an interactive user interface that fosters user engagement and close reading. By emphasizing the logical integrity of textual content rather than relying solely on factual accuracy, Skeptik offers a comprehensive solution to combat potential misinformation in journalism. Ultimately, our framework aims to improve critical reading and protect the public from deceptive information online and enhance the overall credibility of news media.

</details>


### [5] [Beyond prior knowledge: The predictive role of knowledge-building in Tutor Learning](https://arxiv.org/abs/2508.18545)

*Tasmia Shahriar, Mia Ameen, Aditi Mallavarapu, Shiyan Jiang, Noboru Matsuda*

**Main category:** cs.HC

**Keywords:** knowledge-building, teachable agents, learning-by-teaching, procedural knowledge, conceptual knowledge

**Relevance Score:** 8

**TL;DR:** This study examines how engaging in knowledge-building activities in teachable agent environments enhances students' procedural and conceptual learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Students in learning-by-teaching environments struggle with knowledge-building and often resort to knowledge-telling. This limits their learning potential.

**Method:** The study investigates the relationship between knowledge-building, procedural knowledge, and conceptual knowledge in a teachable agent context.

**Key Contributions:**

	1. Demonstrates the importance of knowledge-building in learning environments.
	2. Establishes a bidirectional relationship between procedural and conceptual knowledge.
	3. Highlights the role of teachable agents in fostering deeper learning.

**Result:** The findings indicate a stable bidirectional relationship where higher engagement in knowledge-building activities leads to improved post-test scores in both procedural and conceptual knowledge.

**Limitations:** 

**Conclusion:** Knowledge-building is essential for enhancing learning outcomes in students, especially for those with lower prior knowledge.

**Abstract:** When adopting the role of a teacher in learning-by-teaching environments, students often struggle to engage in knowledge-building activities, such as providing explanations and addressing misconceptions. Instead, they frequently default to knowledge-telling behaviors, where they simply dictate what they already know or what to do without deeper reflection, thereby limiting learning. Teachable agents, particularly those capable of posing persistent follow-up questions, have been shown to encourage students (tutors) to shift from knowledge-telling to knowledge-building and enhance tutor learning. Tutor learning encompasses two interrelated types of knowledge: conceptual and procedural knowledge. Research has established a bidirectional relationship between these knowledge types, where improvements in one reinforce the other. This study investigates the role of knowledge-building in mediating the bidirectional relationship between procedural and conceptual learning. Our findings revealed a stable bidirectional relationship between procedural and conceptual knowledge, with higher post-test scores observed among students who engaged in knowledge-building, regardless of their procedural and conceptual pre-test performance. This suggests that knowledge-building serves as a crucial mechanism bridging the gap between students with low prior knowledge and higher conceptual and procedural learning gain.

</details>


### [6] [Gamification of Immersive Cervical Rehabilitation Exercises in VR: An Exploratory Study on Chin Tuck and Range of Motion Exercises](https://arxiv.org/abs/2508.18580)

*Haitham Abdelsalam, Chanelle Montpetit, Arash Harirpoush, Maryse Fortin, Yiming Xiao*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Cervical Rehabilitation, Gamification, User Engagement, Health Informatics

**Relevance Score:** 8

**TL;DR:** This paper explores the use of virtual reality and gamification in neck rehabilitation exercises, addressing traditional rehabilitation challenges through innovative game strategies.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility and adherence to neck rehabilitation, which is often hampered by the high costs and inefficiencies of traditional methods.

**Method:** An exploratory study investigating various gamification strategies for VR-based cervical rehabilitation through user engagement in chin tuck and neck range of motion exercises.

**Key Contributions:**

	1. Investigated novel gamification strategies in VR for neck rehabilitation.
	2. Studied user engagement with specific exercises in a VR context.
	3. Demonstrated positive outcomes on usability and perceived health value.

**Result:** The proposed VR neck rehabilitation games showed excellent usability, engagement, and perceived health value based on preliminary user feedback.

**Limitations:** The study is exploratory with preliminary findings; further research is needed to validate the effectiveness and scalability of the proposed strategies.

**Conclusion:** Gamified VR approaches have potential in enhancing neck rehabilitation effectiveness and user adherence, warranting further exploration in this emerging field.

**Abstract:** Chronic neck pain is a prevalent condition that affects millions of individuals worldwide, causing significant individual suffering and socioeconomic burdens. Although exercise rehabilitation is a staple in relieving pain and improving muscle function for the condition, traditional one-on-one rehabilitation sessions are costly and suffer from poor adherence and accessibility for the patients. Thanks to the increasing accessibility and recent advancements in sensing and display technology, virtual reality (VR) offers the potential to tackle the challenges in traditional exercise rehabilitation, particularly through gamification. However, still in its infancy, VR-based neck exercise rehabilitation lacks exploration in effective gamification strategies and existing prototypes. To address the knowledge gap, we conduct an exploratory study on the gamification strategies for VR-based cervical rehabilitation exercises by using chin tuck and neck range of motion exercises as examples. Specifically, with different game themes, we investigate a survival and level progression strategy for muscle strengthening (chin tuck) exercise for the first time, and the suitability of ambient reward for a neck range of motion exercise. Through a preliminary user study, we assess the proposed novel VR neck rehabilitation games and they demonstrate excellent usability, engagement, and perceived health value.

</details>


### [7] [Portable Silent Room: Exploring VR Design for Anxiety and Emotion Regulation for Neurodivergent Women and Non-Binary Individuals](https://arxiv.org/abs/2508.18591)

*Kinga Skiers, Yun Suen Pai, Marina Nakagawa, Kouta Minamizawa, Giulia Barbareschi*

**Main category:** cs.HC

**Keywords:** virtual reality, neurodivergence, emotional regulation, sensory overload, inclusive design

**Relevance Score:** 7

**TL;DR:** This research explores the use of virtual reality (VR) as a tool for emotional regulation among neurodivergent individuals, focusing on its effectiveness as a calming environment for managing anxiety and sensory overload.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Neurodivergent individuals face significant emotional challenges due to societal pressures and inadequate support, necessitating innovative solutions for emotional regulation.

**Method:** The study employed a mixed-methods approach including an online survey with 223 participants and an ideation workshop with 32 participants, followed by the development and testing of VR prototypes with 12 neurodivergent women and non-binary participants.

**Key Contributions:**

	1. Investigation of VR as a tool for emotional regulation in neurodivergent individuals.
	2. Development of adaptable VR environments based on user feedback.
	3. Testing of VR prototypes to create effective calming experiences.

**Result:** The final VR prototype was tested by 25 neurodivergent participants, showing effectiveness in providing a personalized environment for sensory regulation and emotional comfort.

**Limitations:** 

**Conclusion:** This research advances the design of inclusive VR environments as 'portable silent rooms,' available for neurodivergent individuals to help manage sensory overload and emotional dysregulation in any location.

**Abstract:** Neurodivergent individuals, particularly those with Autism and Attention Deficit Hyperactivity Disorder (ADHD), frequently experience anxiety, panic attacks, meltdowns, and emotional dysregulation due to societal pressures and inadequate accommodations. These challenges are especially pronounced for neurodivergent women and non-binary individuals navigating intersecting barriers of neurological differences and gender expectations. This research investigates virtual reality (VR) as a portable safe space for emotional regulation, addressing challenges of sensory overload and motion sickness while enhancing relaxation capabilities. Our mixed-methods approach included an online survey (N=223) and an ideation workshop (N=32), which provided key design elements for creating effective calming VR environments. Based on these findings, we developed and iteratively tested VR prototypes with neurodivergent women and non-binary participants (N=12), leading to a final version offering enhanced adaptability to individual sensory needs. This final prototype underwent a comprehensive evaluation with 25 neurodivergent participants to assess its effectiveness as a regulatory tool. This research contributes to the development of inclusive, adaptive VR environments that function as personalized "portable silent rooms" offering neurodivergent individuals on-demand access to sensory regulation regardless of physical location.

</details>


### [8] [Enhancing XAI Interpretation through a Reverse Mapping from Insights to Visualizations](https://arxiv.org/abs/2508.18640)

*Aniket Nuthalapati, Nicholas Hinds, Brian Y. Lim, Qianwen Wang*

**Main category:** cs.HC

**Keywords:** AI explanations, Human-Computer Interaction, visualization

**Relevance Score:** 8

**TL;DR:** The paper introduces Reverse Mapping, a novel approach that enhances visual AI explanations by integrating user-derived insights to improve understanding and interaction.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve users' ability to interpret and learn from AI model behavior in high-stakes domains.

**Method:** The system uses a large language model to extract structured insights from user interpretations and maps these onto visual explanations through interactive annotations and multi-view visualizations.

**Key Contributions:**

	1. Introduces a novel Reverse Mapping approach for AI explanations.
	2. Integrates user insights into visual explanations for improved understanding.
	3. Supports interactive features that enhance user engagement with AI outputs.

**Result:** Demonstrated effectiveness of the approach through a prototype system in two use cases, supported by qualitative user feedback indicating improved understanding of AI explanations.

**Limitations:** 

**Conclusion:** Reverse Mapping fosters more deliberate interaction with AI explanations, enhancing users' reasoning capabilities and learning.

**Abstract:** As AI systems become increasingly integrated into high-stakes domains, enabling users to accurately interpret model behavior is critical. While AI explanations can be provided, users often struggle to reason effectively with these explanations, limiting their ability to validate or learn from AI decisions. To address this gap, we introduce Reverse Mapping, a novel approach that enhances visual explanations by incorporating user-derived insights back into the explanation workflow. Our system extracts structured insights from free-form user interpretations using a large language model and maps them back onto visual explanations through interactive annotations and coordinated multi-view visualizations. Inspired by the verification loop in the visualization knowledge generation model, this design aims to foster more deliberate, reflective interaction with AI explanations. We demonstrate our approach in a prototype system with two use cases and qualitative user feedback.

</details>


### [9] [RÉCITKIT: A Spatial Toolkit for Designing and Evaluating Human-Centered Immersive Data Narratives](https://arxiv.org/abs/2508.18670)

*Vidya Setlur, Samuel Ridet*

**Main category:** cs.HC

**Keywords:** Spatial Computing, Data Storytelling, Immersive Experiences, Toolkit, User Evaluation

**Relevance Score:** 8

**TL;DR:** Introduction of R'ÉCITKIT, a toolkit for creating spatial data narratives in immersive environments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide guidance on building immersive data storytelling experiences using spatial computing.

**Method:** Development of the R'ÉCITKIT toolkit and a case study using Minard's historical flow map of Napoleon's campaign.

**Key Contributions:**

	1. Introduction of R'ÉCITKIT toolkit for spatial data narratives
	2. Evaluation using a historical case study
	3. Insights into the effectiveness of spatial interactions for data storytelling

**Result:** Preliminary evaluations showed enhanced insight formation through spatial interactions, with positive feedback on narrative clarity and engagement.

**Limitations:** Limited to a preliminary evaluation of 21 participants; feedback may not represent larger populations.

**Conclusion:** The findings suggest potential for improving interaction design in immersive storytelling and highlight areas for future enhancements.

**Abstract:** Spatial computing presents new opportunities for immersive data storytelling, yet there is limited guidance on how to build such experiences or adapt traditional narrative visualizations to this medium. We introduce a toolkit, R\'ECITKIT for supporting spatial data narratives in head-mounted display (HMD) environments. The toolkit allows developers to create interactive dashboards, tag data attributes as spatial assets to 3D models and immersive scenes, generate text and audio narratives, enabling dynamic filtering, and hierarchical drill-down data discoverability. To demonstrate the utility of the toolkit, we developed Charles Minard's historical flow map of Napoleon's 1812 campaign in Russia as an immersive experience on Apple Vision Pro. We conducted a preliminary evaluation with 21 participants that comprised two groups: developers, who evaluated the toolkit by authoring spatial stories and consumers, who provided feedback on the Minard app's narrative clarity, interaction design, and engagement. Feedback highlighted how spatial interactions and guided narration enhanced insight formation, with participants emphasizing the benefits of physical manipulation (e.g., gaze, pinch, navigation) for understanding temporal and geographic data. Participants also identified opportunities for future enhancement, including improved interaction affordance visibility, customizable storytelling logic, and integration of contextual assets to support user orientation. These findings contribute to the broader discourse on toolkit-driven approaches to immersive data storytelling across domains such as education, decision support, and exploratory analytics.

</details>


### [10] [Long-Term Variability in Physiological-Arousal Relationships for Robust Emotion Estimation](https://arxiv.org/abs/2508.18782)

*Hiroto Sakimura, Takayuki Nagaya, Tomoki Nishi, Tetsuo Kurahashi, Katsunori Kohda, Nobuhiko Muramoto*

**Main category:** cs.HC

**Keywords:** emotion estimation, physiological signals, longitudinal study, affect, Explainable Boosting Machines

**Relevance Score:** 7

**TL;DR:** This study explores the stability of physiological signals in estimating emotional states over long periods, indicating a need for periodic updates to emotion estimation models.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the assumption that relationships between physiological features and subjective affect are stable over long timeframes, which has rarely been tested.

**Method:** A longitudinal dataset was created by collecting physiological signals and self-reported emotional states from 24 participants over two three-month periods, using Explainable Boosting Machines for analysis.

**Key Contributions:**

	1. Developed a longitudinal dataset for studying physiological signals and emotional states.
	2. Demonstrated the long-term variability of physiological-arousal relationships.
	3. Highlighted the need for periodic model updates in emotion estimation.

**Result:** A model trained on the first period data showed a 5% decrease in accuracy when tested on the second period data, indicating long-term variability in relationships. Heart rate was stable while EDA varied significantly among individuals.

**Limitations:** Limited sample size of 24 participants.

**Conclusion:** Emotion estimation models should be periodically updated every five months to account for changes in physiological-arousal relationships for better performance.

**Abstract:** Estimating emotional states from physiological signals is a central topic in affective computing and psychophysiology. While many emotion estimation systems implicitly assume a stable relationship between physiological features and subjective affect, this assumption has rarely been tested over long timeframes. This study investigates whether such relationships remain consistent across several months within individuals. We developed a custom measurement system and constructed a longitudinal dataset by collecting physiological signals -- including blood volume pulse, electrodermal activity (EDA), skin temperature, and acceleration--along with self-reported emotional states from 24 participants over two three-month periods. Data were collected in naturalistic working environments, allowing analysis of the relationship between physiological features and subjective arousal in everyday contexts. We examined how physiological-arousal relationships evolve over time by using Explainable Boosting Machines (EBMs) to ensure model interpretability. A model trained on 1st-period data showed a 5\% decrease in accuracy when tested on 2nd-period data, indicating long-term variability in physiological-arousal associations. EBM-based comparisons further revealed that while heart rate remained a relatively stable predictor, minimum EDA exhibited substantial individual-level fluctuations between periods. While the number of participants is limited, these findings highlight the need to account for temporal variability in physiological-arousal relationships and suggest that emotion estimation models should be periodically updated -- e.g., every five months -- based on observed shift trends to maintain robust performance over time.

</details>


### [11] [Insights into User Interface Innovations from a Design Thinking Workshop at deRSE25](https://arxiv.org/abs/2508.18784)

*Maximilian Frank, Simon Lund*

**Main category:** cs.HC

**Keywords:** Large Language Models, User Interface, Design Thinking, Human-Centered Design, Interaction Concepts

**Relevance Score:** 9

**TL;DR:** The paper discusses innovative user interface concepts for Large Language Models (LLMs) developed during a design workshop, aiming to improve user interaction through flexible, user-centered designs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current user interfaces for LLMs, which often follow rigid interaction paradigms.

**Method:** Insights were gathered from a design thinking workshop where participants evaluated existing LLM interfaces and generated new concepts focusing on flexible context management and user control.

**Key Contributions:**

	1. New interaction concepts for LLMs identified by workshop participants
	2. Emphasis on flexible context management and dynamic conversation branching
	3. Guidance from a human-centered design process for UI development

**Result:** The workshop led to the creation of new UI interaction concepts that emphasized dynamic conversation branching and user-centered design.

**Limitations:** 

**Conclusion:** The ongoing development of LLM interfaces should prioritize innovative UI grounded in user-centered design principles, advocating for continuous refinement through user feedback.

**Abstract:** Large Language Models have become widely adopted tools due to their versatile capabilities, yet their user interfaces remain limited, often following rigid, linear interaction paradigms. In this paper, we present insights from a design thinking workshop held at the deRSE25 conference aiming at collaboratively developing innovative user interface concepts for LLMs. During the workshop, participants identified common use cases, evaluated the strengths and shortcomings of current LLM interfaces, and created visualizations of new interaction concepts emphasizing flexible context management, dynamic conversation branching, and enhanced mechanisms for user control. We describe how these participant-generated ideas advanced our own whiteboard-based UI approach. The ongoing development of this interface is guided by the human-centered design process - an iterative, user-focused methodology that emphasizes continuous refinement through user feedback. Broader implications for future LLM interface development are discussed, advocating for increased attention to UI innovation grounded in user-centered design principles.

</details>


### [12] [PRIMMDebug: A Debugging Teaching Aid For Secondary Students](https://arxiv.org/abs/2508.18875)

*Laurie Gale, Sue Sentance*

**Main category:** cs.HC

**Keywords:** debugging, education, systematic approaches, programming, HCI

**Relevance Score:** 3

**TL;DR:** The paper presents PRIMMDebug, a debugging tool designed for secondary school students learning text-based programming, promoting systematic and reflective debugging practices.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges faced by secondary school students in effectively debugging their programming code, which often leads to emotional distress and ineffective strategies.

**Method:** PRIMMDebug is an online tool that guides students through a systematic debugging process based on the PRIMM framework, encouraging them to articulate their thoughts while limiting code editing and execution.

**Key Contributions:**

	1. Introduction of PRIMMDebug as a structured debugging aid for education.
	2. Empirical evaluation of the effectiveness and student engagement with the tool.
	3. Identification of challenges in promoting systematic debugging practices among students.

**Result:** Students showed reluctance in engaging with the systematic and reflective practices encouraged by the tool, as evidenced by survey results and log data analysis from multiple schools.

**Limitations:** Students' reluctance to engage with the reflective practices of the tool may limit its effectiveness.

**Conclusion:** Refinements to approaches in systematic debugging are needed to better support student engagement and learning outcomes in debugging.

**Abstract:** Debugging is often a challenging and infuriating experience for secondary school students learning their first text-based programming language. Many students resort to ineffective debugging strategies, making success with solving errors unlikely and emotional distress common. Developing tools that encourage students to adopt a more systematic and reflective approach to debugging is therefore an important, but lacking, area of research. This paper presents PRIMMDebug, a debugging teaching aid for secondary school students learning text-based programming. The aid consists of an online tool that takes students through the steps of a systematic debugging process based on PRIMM, a framework for teaching programming. The tool promotes a reflective approach to debugging by heavily encouraging students to articulate their thoughts throughout the PRIMMDebug process while simultaneously limiting their ability to run and edit code. To evaluate the tool, a set of students from four secondary schools were taught with PRIMMDebug over several lessons. Survey results and log data analysis show that students were generally reluctant to engage with the systematicity and reflection that the tool encourages. Given that related work on systematic debugging has reported similar challenges, we end by considering how these approaches could be refined to help more students benefit from them.

</details>


### [13] [DESAMO: A Device for Elder-Friendly Smart Homes Powered by Embedded LLM with Audio Modality](https://arxiv.org/abs/2508.18918)

*Youngwon Choi, Donghyuk Jung, Hwayeon Kim*

**Main category:** cs.HC

**Keywords:** Elderly care, Smart home, Audio LLM, Natural interaction, User intent

**Relevance Score:** 9

**TL;DR:** DESAMO is an on-device smart home system that utilizes an Audio LLM to facilitate elder-friendly interactions, overcoming challenges in understanding unclear speech and non-speech audio.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the usability of smart home systems for elderly users who may face difficulties with traditional voice assistants.

**Method:** The system directly processes raw audio input using an Audio LLM, bypassing conventional ASR pipelines.

**Key Contributions:**

	1. Introduction of DESAMO, a novel elder-friendly smart home system
	2. Utilization of an Audio LLM for direct audio processing
	3. Enhanced detection of critical events related to user safety

**Result:** DESAMO demonstrates a robust capability in understanding user intent and detecting critical audio events, such as falls or distress calls, more effectively than traditional systems.

**Limitations:** 

**Conclusion:** The adoption of the Audio LLM allows for more natural and private interactions in smart home environments, particularly for the elderly.

**Abstract:** We present DESAMO, an on-device smart home system for elder-friendly use powered by Audio LLM, that supports natural and private interactions. While conventional voice assistants rely on ASR-based pipelines or ASR-LLM cascades, often struggling with the unclear speech common among elderly users and unable to handle non-speech audio, DESAMO leverages an Audio LLM to process raw audio input directly, enabling a robust understanding of user intent and critical events, such as falls or calls for help.

</details>


### [14] [Impact Assessment Card: Communicating Risks and Benefits of AI Uses](https://arxiv.org/abs/2508.18919)

*Edyta Bogucka, Marios Constantinides, Sanja Šćepanović, Daniele Quercia*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, AI Governance, Impact Assessment, Communication Tools, Public Understanding

**Relevance Score:** 7

**TL;DR:** The paper presents the Impact Assessment Card, a tool designed to communicate the risks and benefits of AI more clearly to non-experts, improving task performance and email quality in evaluations of AI systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for communicating AI risks often exclude non-technical individuals, hindering public understanding and governance.

**Method:** Development of the Impact Assessment Card through HCI research, followed by focus group discussions and an online study comparing the card's effectiveness with traditional reports.

**Key Contributions:**

	1. Introduction of the Impact Assessment Card as a novel communication tool for AI risks
	2. Empirical evidence showing improved task performance and email quality using the card
	3. Insights on design requirements for communicating technical information to non-experts.

**Result:** Participants who used the Impact Assessment Card completed tasks faster and submitted higher-quality emails than those using standard impact assessment reports.

**Limitations:** Limited to U.S. population sample; may not generalize to other cultural contexts.

**Conclusion:** Design choices in information presentation can enhance accessibility and support effective AI governance.

**Abstract:** Communicating the risks and benefits of AI is important for regulation and public understanding. Yet current methods such as technical reports often exclude people without technical expertise. Drawing on HCI research, we developed an Impact Assessment Card to present this information more clearly. We held three focus groups with a total of 12 participants who helped identify design requirements and create early versions of the card. We then tested a refined version in an online study with 235 participants, including AI developers, compliance experts, and members of the public selected to reflect the U.S. population by age, sex, and race. Participants used either the card or a full impact assessment report to write an email supporting or opposing a proposed AI system. The card led to faster task completion and higher-quality emails across all groups. We discuss how design choices can improve accessibility and support AI governance. Examples of cards are available at: https://social-dynamics.net/ai-risks/impact-card/.

</details>


### [15] [Reading minds on the road: decoding perceived risk in automated vehicles through 140K+ ratings](https://arxiv.org/abs/2508.19121)

*Xiaolin He, Zirui Li, Xinwei Wang, Riender Happee, Meng Wang*

**Main category:** cs.HC

**Keywords:** automated vehicles, perceived risk, real-time measurement, deep neural networks, human-computer interaction

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method to measure and decode perceived risk in automated vehicles in real time, using a large dataset and deep neural networks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Measuring perceived risk in automated vehicles (AVs) is crucial as it influences rider behavior and safety. Understanding how this perception evolves in real time can help in the design of safer AVs.

**Method:** Conducted an experiment with 2,164 participants who provided safety ratings while watching driving scenarios. Used continuous-signal reconstruction to create time-continuous perceived risk data, and trained deep neural networks to predict perceived risk based on vehicle kinematics.

**Key Contributions:**

	1. Largest dataset of perceived risk from AV interactions
	2. Successful implementation of deep learning for real-time risk prediction
	3. Insights into factors influencing perceived risk to enhance AV design

**Result:** Achieved a mean relative error below 3% in predicting moment-by-moment perceived risk, uncovering real-time factors influencing this perception.

**Limitations:** 

**Conclusion:** The study introduces a new method for quantifying dynamic passenger experiences, which can inform the design of automated systems across various domains, ensuring safety and trust.

**Abstract:** Perceived risk in automated vehicles (AVs) can create the very danger that automation is meant to prevent: a frightened rider may hesitate when seconds matter, misjudge hazards, or disengage. However, measuring how perceived risk evolves in real time during driving remains challenging, leaving a gap in decoding such hidden psychological states. Here, we present a novel method to time-continuously measure and decode perceived risk. We conducted a controlled experiment where 2,164 participants viewed high-fidelity videos of common highway driving scenes and provided 141,628 discrete safety ratings. Through continuous-signal reconstruction of the discrete ratings, we obtained 236 hours of time-continuous perceived risk data - the largest perceived risk dataset to date. Leveraging this dataset, we trained deep neural networks that predict moment-by-moment perceived risk from vehicle kinematics with a mean relative error below $3\%$. Explainable AI analysis uncovers which factors determine perceived risk in real time. Our findings demonstrate a new paradigm for quantifying dynamic passenger experience and psychological constructs in real time. These findings can guide the design of AVs and other machines that operate in close proximity to people, adjusting behaviour before trust erodes, and help realise automation's benefits in transport, healthcare, and service robotics.

</details>


### [16] [Beyond Competitive Gaming: How Casual Players Evaluate and Respond to Teammate Performance](https://arxiv.org/abs/2508.19230)

*Kaushall Senthil Nathan, Jieun Lee, Derrick M. Wang, Geneva M. Smith, Eugene Kukshinov, Daniel Harley, Lennart E. Nacke*

**Main category:** cs.HC

**Keywords:** Performance evaluation, casual games, cooperative gaming, teammate competence, behavioral responses

**Relevance Score:** 7

**TL;DR:** This research investigates how casual players evaluate teammate performance in cooperative video games, revealing distinct behavioral responses and assessment mechanisms compared to competitive gaming contexts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether existing performance evaluation frameworks from competitive video games apply to casual cooperative games, and to understand how players assess teammate competence in these settings.

**Method:** The study conducted a controlled between-subjects experiment with 23 participants in the game Overcooked 2, using observations, NASA TLX self-reports, and interviews to analyze player behavior and evaluations.

**Key Contributions:**

	1. Identified distinct behavioral responses in casual cooperative games compared to competitive environments.
	2. Demonstrated that traditional self-reporting methods do not capture all relevant behaviors like frustration.
	3. Proposed a new comparative operationalization for evaluating performance in casual games.

**Result:** The findings indicate frustration behaviors observed during gameplay that were not captured in self-reports and highlight that players evaluate performance through relative comparisons, challenging existing metrics used in competitive gaming.

**Limitations:** Limited sample size of 23 participants may affect generalizability of findings.

**Conclusion:** Performance evaluation mechanisms in casual cooperative games differ from those in competitive contexts, necessitating a separate framework for understanding player evaluations in these scenarios.

**Abstract:** Teammate performance evaluation fundamentally shapes intervention design in video games. However, our current understanding stems primarily from competitive E-Sports contexts where individual performance directly impacts outcomes. This research addresses whether performance evaluation mechanisms and behavioural responses identified in competitive games generalize to casual cooperative games. We investigated how casual players evaluate teammate competence and respond behaviourally in a controlled between-subjects experiment (N=23). We manipulated confederate performance in Overcooked 2, combining observations, NASA TLX self-reports, and interviews. We present two key findings. (1) Observations revealed frustration behaviours completely absent in self-report data. Thus, these instruments assess fundamentally distinct constructs. (2) Participants consistently evaluated teammate performance through relative comparison rather than absolute metrics. This contradicts task-performance operationalizations dominant in competitive gaming research. Hence, performance evaluation frameworks from competitive contexts cannot be directly applied to casual cooperative games. We provide empirical evidence that performance evaluation in casual games requires a comparative operationalization.

</details>


### [17] [Cluster-Aware Grid Layout](https://arxiv.org/abs/2308.03651)

*Yuxing Zhou, Weikai Yang, Jiashu Chen, Changjian Chen, Zhiyang Shen, Xiaonan Luo, Lingyun Yu, Shixia Liu*

**Main category:** cs.HC

**Keywords:** grid visualization, cluster analysis, optimization, human-computer interaction, data structures

**Relevance Score:** 4

**TL;DR:** The paper presents a cluster-aware grid layout method that enhances the visualization of data clusters by optimizing for proximity, compactness, and convexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing layout methods struggle with preserving cluster structures in grid visualizations, which can hinder data analysis.

**Method:** The proposed method involves a two-phase hybrid optimization strategy: a global phase for proximity and compactness within clusters, and a local phase for ensuring convexity of cluster shapes.

**Key Contributions:**

	1. Introduction of a cluster-aware grid layout method
	2. Hybrid optimization strategy for balancing proximity, compactness, and convexity
	3. Demonstrated effectiveness through experiments and use cases

**Result:** The evaluation through quantitative experiments and use cases indicates that the method effectively preserves cluster structures and aids analysis tasks.

**Limitations:** 

**Conclusion:** The cluster-aware grid layout method improves the depiction of clusters in grid visualizations, making it beneficial for data analysis in various applications.

**Abstract:** Grid visualizations are widely used in many applications to visually explain a set of data and their proximity relationships. However, existing layout methods face difficulties when dealing with the inherent cluster structures within the data. To address this issue, we propose a cluster-aware grid layout method that aims to better preserve cluster structures by simultaneously considering proximity, compactness, and convexity in the optimization process. Our method utilizes a hybrid optimization strategy that consists of two phases. The global phase aims to balance proximity and compactness within each cluster, while the local phase ensures the convexity of cluster shapes. We evaluate the proposed grid layout method through a series of quantitative experiments and two use cases, demonstrating its effectiveness in preserving cluster structures and facilitating analysis tasks.

</details>


### [18] [VR MRI Training for Adolescents: A Comparative Study of Gamified VR, Passive VR, 360° Video, and Traditional Educational Video](https://arxiv.org/abs/2504.09955)

*Yue Yang, Mengyao Guo, Yuxuan Wu, Wally Niu, Emmanuel A Corona, Bruce Daniel, Christoph Leuze, Fred Baik*

**Main category:** cs.HC

**Keywords:** MRI, Virtual Reality, Pediatrics, Anxiety, Engagement

**Relevance Score:** 8

**TL;DR:** This study evaluates the effectiveness of four MRI preparation modalities, highlighting the advantages of a gamified VR simulation for reducing motion and increasing preparedness in pediatric patients.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the MRI experience for pediatric patients, reducing anxiety and motion through innovative preparation techniques.

**Method:** A within-subjects design was employed with 11 participants (ages 10-16) to compare four MRI preparation methods: a gamified VR simulation, a passive VR experience, a 360-degree video, and a standard 2D video, focusing on outcomes such as head motion, anxiety, and preparedness.

**Key Contributions:**

	1. Introduction of gamified virtual reality for MRI preparation.
	2. Demonstrated lower head motion and higher preparedness in pediatric patients using VR.
	3. Provided design recommendations for immersive simulations in healthcare.

**Result:** The gamified VR condition resulted in significantly lower head motion (p < 0.001) and higher preparedness scores (p < 0.05). It also indicated a strong correlation between head motion and learning outcomes (p < 0.01).

**Limitations:** 

**Conclusion:** Gamified VR simulations enhance engagement and reduce anxiety, suggesting their potential integration into pediatric MRI workflows.

**Abstract:** Meta Quest Store: https://www.meta.com/experiences/stanford-mri-simulator/8205539289482347/ Magnetic Resonance Imaging (MRI) can be a stressful experience for pediatric patients due to the loud acoustic environment, enclosed scanner bore, and a prolonged requirement to remain still. While sedation is commonly used to manage anxiety and motion, it carries clinical risks and logistical burdens. Traditional preparatory approaches, such as instructional videos and mock scans, often lack engagement for older children and adolescents. In this study, we present a comparative evaluation of four MRI preparation modalities: (1) a gamified virtual reality (VR) simulation that trains stillness through real-time feedback; (2) a passive VR experience replicating the MRI environment without interactivity; (3) a 360{\deg} first-person video of a real MRI procedure; and (4) a standard 2D educational video. Using a within-subjects design (N = 11, ages 10-16), we assess each method's impact on head motion data, anxiety reduction, procedural preparedness, usability, cognitive workload, and subjective preference. Results show that the gamified VR condition has significantly lower head motion (p < 0.001) and yielded the highest preparedness scores (p < 0.05). Head motion data were significantly correlated with learning outcomes (p < 0.01), suggesting that behavioral performance in VR strongly indicates procedural readiness. While all modalities reduced anxiety and were rated usable, interactive VR was preferred by most participants and demonstrated unique advantages in promoting engagement and behavioral rehearsal. We conclude with design recommendations for designing immersive simulations and integrating VR training into pediatric imaging workflows.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Semantic Attractors and the Emergence of Meaning: Towards a Teleological Model of AGI](https://arxiv.org/abs/2508.18290)

*Hans-Joachim Rudolph*

**Main category:** cs.CL

**Keywords:** Semantic AGI, Semantic attractors, Tensor transformations

**Relevance Score:** 5

**TL;DR:** The paper introduces a theoretical framework for semantic AGI based on semantic attractors in complex-valued meaning spaces, moving beyond current transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a model of AGI that transcends statistical next-token prediction and captures the complexities of meaning formation.

**Method:** The study employs recursive tensor transformations and cyclic operations with the imaginary unit to establish a rotational semantic structure.

**Key Contributions:**

	1. Introduction of the concept of semantic attractors in AGI
	2. Development of a rotational semantic structure for modeling complex meanings
	3. Proposal of a new cognitive architecture for meaning generation

**Result:** It presents a semantic attractor that serves as a teleological operator, guiding meaning towards stability and coherence, contrasting with the probabilistic nature of existing models.

**Limitations:** 

**Conclusion:** True meaning arises from recursive convergence towards semantic coherence requiring a new cognitive architecture that shapes language rather than predicts it.

**Abstract:** This essay develops a theoretical framework for a semantic Artificial General Intelligence (AGI) based on the notion of semantic attractors in complex-valued meaning spaces. Departing from current transformer-based language models, which operate on statistical next-token prediction, we explore a model in which meaning is not inferred probabilistically but formed through recursive tensorial transformation. Using cyclic operations involving the imaginary unit \emph{i}, we describe a rotational semantic structure capable of modeling irony, homonymy, and ambiguity. At the center of this model, however, is a semantic attractor -- a teleological operator that, unlike statistical computation, acts as an intentional agent (Microvitum), guiding meaning toward stability, clarity, and expressive depth. Conceived in terms of gradient flows, tensor deformations, and iterative matrix dynamics, the attractor offers a model of semantic transformation that is not only mathematically suggestive, but also philosophically significant. We argue that true meaning emerges not from simulation, but from recursive convergence toward semantic coherence, and that this requires a fundamentally new kind of cognitive architecture -- one designed to shape language, not just predict it.

</details>


### [20] [LLMs Can't Handle Peer Pressure: Crumbling under Multi-Agent Social Interactions](https://arxiv.org/abs/2508.18321)

*Maojia Song, Tej Deep Pala, Weisheng Jin, Amir Zadeh, Chuan Li, Dorien Herremans, Soujanya Poria*

**Main category:** cs.CL

**Keywords:** multi-agent systems, large language models, trust formation, KAIROS benchmark, reinforcement learning

**Relevance Score:** 8

**TL;DR:** The paper introduces KAIROS, a benchmark for studying decision-making in multi-agent systems involving large language models, focusing on trust formation and misinformation resistance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze how large language models (LLMs) interact in multi-agent systems (MAS) and develop strategies to enhance trust and decision-making under social dynamics.

**Method:** The study presents KAIROS, a simulation of quiz contests with agents of varying reliability, allowing exploration of trust, peer influence, and decision-making strategies with various training approaches including prompting and reinforcement learning.

**Key Contributions:**

	1. Development of KAIROS benchmark for multi-agent systems
	2. Insights on trust formation and decision-making processes with LLMs
	3. Evaluation of various mitigation strategies including GRPO

**Result:** The best performance was achieved using Group Relative Policy Optimisation (GRPO) with context and outcome-based rewards, although it reduced robustness to social influence compared to base models.

**Limitations:** The model's robustness to social influence decreased when using GRPO compared to base models.

**Conclusion:** The findings highlight the complexities of trust and decision-making in multi-agent systems with LLMs, and suggest that GRPO is effective but can introduce vulnerabilities.

**Abstract:** Large language models (LLMs) are increasingly deployed in multi-agent systems (MAS) as components of collaborative intelligence, where peer interactions dynamically shape individual decision-making. Although prior work has focused on conformity bias, we extend the analysis to examine how LLMs form trust from previous impressions, resist misinformation, and integrate peer input during interaction, key factors for achieving collective intelligence under complex social dynamics. We present KAIROS, a benchmark simulating quiz contests with peer agents of varying reliability, offering fine-grained control over conditions such as expert-novice roles, noisy crowds, and adversarial peers. LLMs receive both historical interactions and current peer responses, allowing systematic investigation into how trust, peer action, and self-confidence influence decisions. As for mitigation strategies, we evaluate prompting, supervised fine-tuning, and reinforcement learning, Group Relative Policy Optimisation (GRPO), across multiple models. Our results reveal that GRPO with multi-agent context combined with outcome-based rewards and unconstrained reasoning achieves the best overall performance, but also decreases the robustness to social influence compared to Base models. The code and datasets are available at: https://github.com/declare-lab/KAIROS.

</details>


### [21] [Not All Visitors are Bilingual: A Measurement Study of the Multilingual Web from an Accessibility Perspective](https://arxiv.org/abs/2508.18328)

*Masudul Hasan Masud Bhuiyan, Matteo Varvello, Yasir Zaki, Cristian-Alexandru Staicu*

**Main category:** cs.CL

**Keywords:** multilingual web accessibility, visual impairment, non-Latin scripts, accessibility hints, automated testing

**Relevance Score:** 9

**TL;DR:** This paper introduces LangCrUX, a large-scale dataset for analyzing multilingual web accessibility for visually impaired users, revealing significant neglect of accessibility hints in non-Latin scripts and proposing Kizuki, an automated testing tool for better accessibility.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive datasets on multilingual web content and the challenges faced by users with visual impairments due to inadequate support for non-Latin scripts.

**Method:** The authors created the LangCrUX dataset, comprising 120,000 popular websites across 12 languages using non-Latin scripts, and performed a systematic analysis of multilingual web accessibility.

**Key Contributions:**

	1. Development of the LangCrUX dataset for multilingual web accessibility analysis
	2. Discovery of the shortcomings in accessibility hints for non-Latin scripts
	3. Introduction of Kizuki, a language-aware automated accessibility testing tool

**Result:** The analysis revealed widespread neglect of accessibility hints, which do not accurately reflect the language diversity of content, thus hampering the effectiveness of screen readers.

**Limitations:** The dataset is limited to popular websites and may not represent all multilingual content accurately.

**Conclusion:** The paper highlights the urgent need for better accessibility practices for multilingual web content, and introduces Kizuki as a solution to improve automated accessibility testing.

**Abstract:** English is the predominant language on the web, powering nearly half of the world's top ten million websites. Support for multilingual content is nevertheless growing, with many websites increasingly combining English with regional or native languages in both visible content and hidden metadata. This multilingualism introduces significant barriers for users with visual impairments, as assistive technologies like screen readers frequently lack robust support for non-Latin scripts and misrender or mispronounce non-English text, compounding accessibility challenges across diverse linguistic contexts. Yet, large-scale studies of this issue have been limited by the lack of comprehensive datasets on multilingual web content. To address this gap, we introduce LangCrUX, the first large-scale dataset of 120,000 popular websites across 12 languages that primarily use non-Latin scripts. Leveraging this dataset, we conduct a systematic analysis of multilingual web accessibility and uncover widespread neglect of accessibility hints. We find that these hints often fail to reflect the language diversity of visible content, reducing the effectiveness of screen readers and limiting web accessibility. We finally propose Kizuki, a language-aware automated accessibility testing extension to account for the limited utility of language-inconsistent accessibility hints.

</details>


### [22] [Language-Specific Layer Matters: Efficient Multilingual Enhancement for Large Vision-Language Models](https://arxiv.org/abs/2508.18381)

*Yuchun Fan, Yilin Wang, Yongyu Mu, Lei Huang, Bei Li, Xiaocheng Feng, Tong Xiao, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** vision-language models, multilingual enhancement, neuron activations

**Relevance Score:** 7

**TL;DR:** This paper introduces PLAST, a method for enhancing the multilingual capabilities of Large Vision-Language Models (LVLMs) through targeted fine-tuning of language-specific layers.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the imbalance in multilingual capabilities of LVLMs and improve their efficiency and understanding across multiple languages.

**Method:** PLAST identifies layers in LVLMs that respond to multilingual understanding by monitoring neuron activations and fine-tunes these layers using question-translation pairs.

**Key Contributions:**

	1. Introduction of PLAST for efficient multilingual enhancement in LVLMs
	2. Identification of language-specific neuron activations in shallow layers
	3. Demonstration of effective empirical results on multilingual benchmarks

**Result:** PLAST improves multilingual capabilities by tuning only 14% of the parameters, with empirical results showing significant enhancements on benchmark datasets MM-Bench and MMMB.

**Limitations:** 

**Conclusion:** PLAST provides an efficient means to enhance LVLM multilingual capabilities and can be extended to low-resource and complex reasoning tasks in visual contexts.

**Abstract:** Large vision-language models (LVLMs) have demonstrated exceptional capabilities in understanding visual information with human languages but also exhibit an imbalance in multilingual capabilities. In this work, we delve into the multilingual working pattern of LVLMs and identify a salient correlation between the multilingual understanding ability of LVLMs and language-specific neuron activations in shallow layers. Building on this insight, we introduce PLAST, a training recipe that achieves efficient multilingual enhancement for LVLMs by Precise LAnguage-Specific layers fine-Tuning. PLAST first identifies layers involved in multilingual understanding by monitoring language-specific neuron activations. These layers are then precisely fine-tuned with question-translation pairs to achieve multilingual alignment. Our empirical results on MM-Bench and MMMB demonstrate that PLAST effectively improves the multilingual capabilities of LVLMs and achieves significant efficiency with only 14% of the parameters tuned. Further analysis reveals that PLAST can be generalized to low-resource and complex visual reasoning tasks, facilitating the language-specific visual information engagement in shallow layers.

</details>


### [23] [Backprompting: Leveraging Synthetic Production Data for Health Advice Guardrails](https://arxiv.org/abs/2508.18384)

*Kellen Tan Cheng, Anna Lisa Gentile, Chad DeLuca, Guang-Jie Ren*

**Main category:** cs.CL

**Keywords:** large language models, guardrails, health informatics, backprompting, synthetic data

**Relevance Score:** 9

**TL;DR:** This paper introduces backprompting as a method to generate production-like labeled data for health advice guardrail development in LLMs, and demonstrates its efficacy in improving detection performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs in enterprise settings has raised significant risks, necessitating robust guardrails to filter outputs, which is hindered by the challenge of acquiring quality labeled data.

**Method:** The authors propose backprompting to create production-like labeled data, and incorporate a sparse human-in-the-loop clustering technique to label this synthetic data, enhancing the training of their detector.

**Key Contributions:**

	1. Introduction of backprompting for generating labeled data
	2. Implementation of a sparse human-in-the-loop clustering technique
	3. Demonstration of enhanced performance in health advice detection

**Result:** The proposed method improves detection accuracy for health advice in LLM outputs, outperforming GPT-4o by up to 3.73% with significantly fewer parameters (400x less).

**Limitations:** 

**Conclusion:** Backprompting, combined with a novel labeling technique, offers an effective solution for generating quality labeled data that improves guardrails for LLMs in health-related applications.

**Abstract:** The pervasiveness of large language models (LLMs) in enterprise settings has also brought forth a significant amount of risks associated with their usage. Guardrails technologies aim to mitigate this risk by filtering LLMs' input/output text through various detectors. However, developing and maintaining robust detectors faces many challenges, one of which is the difficulty in acquiring production-quality labeled data on real LLM outputs prior to deployment. In this work, we propose backprompting, a simple yet intuitive solution to generate production-like labeled data for health advice guardrails development. Furthermore, we pair our backprompting method with a sparse human-in-the-loop clustering technique to label the generated data. Our aim is to construct a parallel corpus roughly representative of the original dataset yet resembling real LLM output. We then infuse existing datasets with our synthetic examples to produce robust training data for our detector. We test our technique in one of the most difficult and nuanced guardrails: the identification of health advice in LLM output, and demonstrate improvement versus other solutions. Our detector is able to outperform GPT-4o by up to 3.73%, despite having 400x less parameters.

</details>


### [24] [Integral Transformer: Denoising Attention, Not Too Much Not Too Little](https://arxiv.org/abs/2508.18387)

*Ivan Kobyzev, Abbas Ghaddar, Dingtao Hu, Boxing Chen*

**Main category:** cs.CL

**Keywords:** self-attention, Integral Transformer, attention noise, language benchmarks, machine learning

**Relevance Score:** 7

**TL;DR:** The Integral Transformer is a novel self-attention mechanism designed to reduce attention noise caused by semantically uninformative tokens while preserving useful information.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of attention noise in self-attention mechanisms, which disproportionately weights uninformative tokens, and the limitations of existing methods.

**Method:** The Integral Transformer integrates signals from the logit distribution to denoise attention, mitigating noise while preserving important contributions from special tokens.

**Key Contributions:**

	1. Introduction of the Integral Transformer self-attention mechanism
	2. Demonstrated improvements over existing attention methods
	3. Revealed effective utilization of vanilla self-attention in lower layers.

**Result:** The Integral Transformer outperforms existing attention variants (vanilla, Cog, Differential attention) on standard benchmarks, enhancing performance by balancing attention distributions and reducing issues like rank collapse.

**Limitations:** 

**Conclusion:** The Integral Transformer offers a significant improvement over traditional self-attention mechanisms, especially in lower Transformer layers, contributing positively to model performance.

**Abstract:** Softmax self-attention often assigns disproportionate weight to semantically uninformative tokens such as special tokens and punctuation, a phenomenon known as attention noise. While recent methods like Cog Attention and the Differential Transformer have addressed this by introducing negative attention scores, they risk discarding useful information. In this paper, we propose the Integral Transformer, a novel self-attention mechanism that denoises attention by integrating signals sampled from the logit distribution. Our approach mitigates noise while preserving the contributions of special tokens critical for model performance. Extensive experiments demonstrate that our model outperforms vanilla, Cog, and Differential attention variants on well-established knowledge and reasoning language benchmarks. Moreover, our analysis reveals that employing vanilla self-attention in the lower Transformer layers enhances performance and that the Integral Transformer effectively balances attention distributions and reduces rank collapse in upper layers.

</details>


### [25] [Latent Self-Consistency for Reliable Majority-Set Selection in Short- and Long-Answer Reasoning](https://arxiv.org/abs/2508.18395)

*Jeong-seok Oh, Jay-yoon Lee*

**Main category:** cs.CL

**Keywords:** Latent Self-Consistency, Large Language Models, response consistency, semantic embedding, confidence calibration

**Relevance Score:** 9

**TL;DR:** Latent Self-Consistency (LSC) improves the consistency of responses in Large Language Models (LLMs) for both short and long-form questions by selecting semantically consistent answers using learnable token embeddings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistent outputs of LLMs, especially for complex or long-form questions, and to enhance the accuracy of responses in QA tasks.

**Method:** Introducing Latent Self-Consistency (LSC), which utilizes learnable token embeddings to select the most semantically consistent responses, with minimal impact on inference time and no changes needed to the model architecture.

**Key Contributions:**

	1. Introduction of Latent Self-Consistency (LSC) for improving response consistency in LLMs.
	2. Demonstration of LSC's superiority over SC, USC, and WUCS across diverse reasoning benchmarks.
	3. Provision of well-calibrated confidence estimates with low Expected Calibration Error.

**Result:** LSC outperforms existing methods like Self-Consistency (SC), Universal Self-Consistency (USC), and Weighted Unigram Consistency Score (WUCS) across multiple short-form and long-form reasoning benchmarks, with negligible computational overhead.

**Limitations:** 

**Conclusion:** LSC is positioned as a practical method for ensuring response consistency in LLMs across various formats while providing well-calibrated confidence estimates that minimize Expected Calibration Error.

**Abstract:** Probabilistic decoding in Large Language Models (LLMs) often yields inconsistent outputs, particularly on complex or long-form questions. Self-Consistency (SC) mitigates this for short-form QA by majority voting over exact strings, whereas Universal Self-Consistency (USC) and Weighted Unigram Consistency Score (WUCS) extend to long-form responses but lose accuracy on short-form benchmarks.   We introduce Latent Self-Consistency (LSC), which selects the most semantically consistent response using learnable token embeddings. A lightweight forward generation of summary tokens increases inference time by less than 1% and requires no changes to the model architecture.   Across 6 short-form and 5 long-form reasoning benchmarks (e.g., MATH, MMLU, TruthfulQA), LSC surpasses SC, USC and WUCS on all short-form and long-form ones on average, while maintaining negligible computational overhead. These results position LSC as a practical consistency-selection method that works reliably across answer formats. Additionally, LSC provides well-calibrated confidence estimates, maintaining low Expected Calibration Error across both answer formats.

</details>


### [26] [Can Out-of-Distribution Evaluations Uncover Reliance on Shortcuts? A Case Study in Question Answering](https://arxiv.org/abs/2508.18407)

*Michal Štefánik, Timothee Mickus, Marek Kadlčík, Michal Spiegel, Josef Kuchař*

**Main category:** cs.CL

**Keywords:** out-of-distribution, generalization, question-answering, spurious features, model robustness

**Relevance Score:** 8

**TL;DR:** This paper critiques the reliability of out-of-distribution (OOD) evaluations in AI, specifically in question-answering models, highlighting the discrepancies between OOD datasets and real-world deployment failures due to reliance on spurious features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that OOD evaluations accurately reflect model performance and robustness in real-world deployments.

**Method:** The authors analyze the relationship between OOD evaluation datasets and specific documented failure modes in QA models, focusing on the reliance on spurious features or prediction shortcuts.

**Key Contributions:**

	1. Critique of the validity of OOD evaluations for AI models.
	2. Identification of quality disparities among OOD datasets in capturing model robustness.
	3. Recommendations for more reliable evaluation methods beyond QA.

**Result:** Findings indicate that OOD datasets can significantly underperform relative to in-distribution evaluations, with varying qualities in their ability to capture robustness to shortcuts.

**Limitations:** The analysis is limited to specific QA models and may not generalize to all AI applications.

**Conclusion:** The study emphasizes the limitations of current OOD evaluations for generalization assessment and proposes improved methodologies for more robust evaluation.

**Abstract:** A majority of recent work in AI assesses models' generalization capabilities through the lens of performance on out-of-distribution (OOD) datasets. Despite their practicality, such evaluations build upon a strong assumption: that OOD evaluations can capture and reflect upon possible failures in a real-world deployment.   In this work, we challenge this assumption and confront the results obtained from OOD evaluations with a set of specific failure modes documented in existing question-answering (QA) models, referred to as a reliance on spurious features or prediction shortcuts.   We find that different datasets used for OOD evaluations in QA provide an estimate of models' robustness to shortcuts that have a vastly different quality, some largely under-performing even a simple, in-distribution evaluation. We partially attribute this to the observation that spurious shortcuts are shared across ID+OOD datasets, but also find cases where a dataset's quality for training and evaluation is largely disconnected. Our work underlines limitations of commonly-used OOD-based evaluations of generalization, and provides methodology and recommendations for evaluating generalization within and beyond QA more robustly.

</details>


### [27] [How Reliable are LLMs for Reasoning on the Re-ranking task?](https://arxiv.org/abs/2508.18444)

*Nafis Tanveer Islam, Zhiming Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, re-ranking, explainability, training methods, semantic understanding

**Relevance Score:** 9

**TL;DR:** This paper analyzes the impact of different training methods on the semantic understanding and explainability of Large Language Models (LLMs) in the re-ranking task, focusing on challenges posed by limited user engagement and transparency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how training methods affect the ability of LLMs to provide reliable re-ranking of content while ensuring transparency and explainability.

**Method:** The study utilizes a small ranking dataset from the Earth science domain to evaluate how different training methods influence the performance of LLMs in re-ranking tasks, and investigates the explainability of the reasoning behind the re-ranking.

**Key Contributions:**

	1. Analyzed the performance of LLMs in re-ranking tasks based on different training methods.
	2. Investigated the relationship between training methods and semantic understanding in LLMs.
	3. Assessed the explainability of LLM outputs in the context of limited training data.

**Result:** The analysis reveals that certain training methods yield better explainability, highlighting inconsistencies in how LLMs acquire semantic understanding and raise questions about their reliability.

**Limitations:** The study is based on a relatively small ranking dataset, which may limit the generalizability of the findings.

**Conclusion:** The findings suggest that improve transparency and informed decision-making can be achieved through optimized training methods that enhance the explainability of LLMs in re-ranking applications.

**Abstract:** With the improving semantic understanding capability of Large Language Models (LLMs), they exhibit a greater awareness and alignment with human values, but this comes at the cost of transparency. Although promising results are achieved via experimental analysis, an in-depth understanding of the LLM's internal workings is unavoidable to comprehend the reasoning behind the re-ranking, which provides end users with an explanation that enables them to make an informed decision. Moreover, in newly developed systems with limited user engagement and insufficient ranking data, accurately re-ranking content remains a significant challenge. While various training methods affect the training of LLMs and generate inference, our analysis has found that some training methods exhibit better explainability than others, implying that an accurate semantic understanding has not been learned through all training methods; instead, abstract knowledge has been gained to optimize evaluation, which raises questions about the true reliability of LLMs. Therefore, in this work, we analyze how different training methods affect the semantic understanding of the re-ranking task in LLMs and investigate whether these models can generate more informed textual reasoning to overcome the challenges of transparency or LLMs and limited training data. To analyze the LLMs for re-ranking tasks, we utilize a relatively small ranking dataset from the environment and the Earth science domain to re-rank retrieved content. Furthermore, we also analyze the explainable information to see if the re-ranking can be reasoned using explainability.

</details>


### [28] [Integrating gender inclusivity into large language models via instruction tuning](https://arxiv.org/abs/2508.18466)

*Alina Wróblewska, Bartosz Żuk*

**Main category:** cs.CL

**Keywords:** gender bias, language models, Polish, gender inclusivity, LLMs

**Relevance Score:** 7

**TL;DR:** This study addresses gender bias in Polish language models by tuning them with gender-inclusive guidelines and the IPIS dataset.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The masculine bias in Polish grammar affects LLM outputs, resulting in gender-imbalanced text generation.

**Method:** The authors developed a system prompt with gender-inclusive guidelines and fine-tuned several multilingual and Polish-specific LLMs using the IPIS dataset.

**Key Contributions:**

	1. Introduction of the IPIS dataset for gender-inclusive instructions in Polish
	2. Development of a systematic prompt design for gender inclusivity
	3. Demonstration of improved outputs in tuned LLMs regarding gender bias

**Result:** The tuned models demonstrate improved gender inclusivity in their outputs compared to standard models.

**Limitations:** The study may be limited by the specificity of the Polish language context and the need for broader datasets for other languages.

**Conclusion:** Integrating gender inclusivity into LLMs is feasible and can systematically reduce gender bias in Polish language generation.

**Abstract:** Imagine a language with masculine, feminine, and neuter grammatical genders, yet, due to historical and political conventions, masculine forms are predominantly used to refer to men, women and mixed-gender groups. This is the reality of contemporary Polish. A social consequence of this unfair linguistic system is that large language models (LLMs) trained on Polish texts inherit and reinforce this masculine bias, generating gender-imbalanced outputs. This study addresses this issue by tuning LLMs using the IPIS dataset, a collection of human-crafted gender-inclusive proofreading in Polish and Polish-to-English translation instructions. Grounded in a theoretical linguistic framework, we design a system prompt with explicit gender-inclusive guidelines for Polish. In our experiments, we IPIS-tune multilingual LLMs (Llama-8B, Mistral-7B and Mistral-Nemo) and Polish-specific LLMs (Bielik and PLLuM). Our approach aims to integrate gender inclusivity as an inherent feature of these models, offering a systematic solution to mitigate gender bias in Polish language generation.

</details>


### [29] [Principled Detection of Hallucinations in Large Language Models via Multiple Testing](https://arxiv.org/abs/2508.18473)

*Jiawei Li, Akshayaa Magesh, Venugopal V. Veeravalli*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, hypothesis testing, out-of-distribution detection, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a method for detecting hallucinations in Large Language Models (LLMs) by reformulating the problem as hypothesis testing and comparing it to out-of-distribution detection in ML models.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing reliance on LLMs for various applications raises concerns about their tendency to produce confident yet incorrect outputs, known as hallucinations.

**Method:** The authors propose a hypothesis testing framework for hallucination detection inspired by multiple-testing principles, including experimental validation against current methods.

**Key Contributions:**

	1. Formulation of hallucination detection as a hypothesis testing problem
	2. Introduction of a robust method inspired by multiple-testing
	3. Validation against existing state-of-the-art methods

**Result:** Extensive experiments demonstrate that the proposed method is robust and outperforms state-of-the-art hallucination detection methods.

**Limitations:** The effectiveness might vary across different types of LLMs and use cases; further validation could be required.

**Conclusion:** The proposed method provides a reliable means to detect hallucinations in LLMs, potentially improving the safety and reliability of systems that utilize these models.

**Abstract:** While Large Language Models (LLMs) have emerged as powerful foundational models to solve a variety of tasks, they have also been shown to be prone to hallucinations, i.e., generating responses that sound confident but are actually incorrect or even nonsensical. In this work, we formulate the problem of detecting hallucinations as a hypothesis testing problem and draw parallels to the problem of out-of-distribution detection in machine learning models. We propose a multiple-testing-inspired method to solve the hallucination detection problem, and provide extensive experimental results to validate the robustness of our approach against state-of-the-art methods.

</details>


### [30] [COMET-poly: Machine Translation Metric Grounded in Other Candidates](https://arxiv.org/abs/2508.18549)

*Maike Züfle, Vilém Zouhar, Tu Anh Dinh, Felipe Maia Polo, Jan Niehues, Mrinmaya Sachan*

**Main category:** cs.CL

**Keywords:** Machine Translation, Evaluation Metrics, COMET, Natural Language Processing, Human Judgment

**Relevance Score:** 5

**TL;DR:** This paper presents two novel metrics for evaluating machine translation that leverage additional information beyond single translations, aiming to enhance automated evaluation performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of automated machine translation metrics that do not consider the context of multiple alternatives, which may lead to less accurate assessments.

**Method:** The paper introduces two metrics: COMET-polycand, which compares a given translation with alternative translations of the same source, and COMET-polyic, which uses translations of similar source texts with associated quality scores for evaluation.

**Key Contributions:**

	1. Introduction of COMET-polycand and COMET-polyic for improved machine translation evaluation
	2. Demonstration of performance improvements using additional translation context
	3. Public release of the models for broader use.

**Result:** Combining additional translations in COMET-polycand improved segment-level performance from 0.079 to 0.118 in Kendall's tau-b correlation, while COMET-polyic achieved a similar improvement from 0.079 to 0.116.

**Limitations:** 

**Conclusion:** The proposed metrics demonstrate significant improvements in machine translation evaluation by incorporating context from multiple translations, and the models are made publicly available.

**Abstract:** Automated metrics for machine translation attempt to replicate human judgment. Unlike humans, who often assess a translation in the context of multiple alternatives, these metrics typically consider only the source sentence and a single translation. This discrepancy in the evaluation setup may negatively impact the performance of automated metrics. We propose two automated metrics that incorporate additional information beyond the single translation. COMET-polycand uses alternative translations of the same source sentence to compare and contrast with the translation at hand, thereby providing a more informed assessment of its quality. COMET-polyic, inspired by retrieval-based in-context learning, takes in translations of similar source texts along with their human-labeled quality scores to guide the evaluation. We find that including a single additional translation in COMET-polycand improves the segment-level metric performance (0.079 to 0.118 Kendall's tau-b correlation), with further gains when more translations are added. Incorporating retrieved examples in COMET-polyic yields similar improvements (0.079 to 0.116 Kendall's tau-b correlation). We release our models publicly.

</details>


### [31] [The Mind's Eye: A Multi-Faceted Reward Framework for Guiding Visual Metaphor Generation](https://arxiv.org/abs/2508.18569)

*Girish A. Koushik, Fatemeh Nazarieh, Katherine Birch, Shenbin Qian, Diptesh Kanojia*

**Main category:** cs.CL

**Keywords:** visual metaphor generation, self-evaluation, metaphor alignment, S-T-M mapping, machine learning

**Relevance Score:** 6

**TL;DR:** Proposes a self-evaluating framework for generating visual metaphors that integrates a metaphor decomposition score and meaning alignment metric, demonstrating promising results over existing baselines.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To generate coherent images from text metaphors while preserving meaning, which is inherently challenging due to the need for language understanding.

**Method:** Introduces a pipeline that decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, along with a training-based method that enhances metaphor alignment using a self-evaluation reward schema.

**Key Contributions:**

	1. Introduced a novel metaphor decomposition score.
	2. Developed a unique meaning alignment metric.
	3. Demonstrated a training-free pipeline that outperforms existing methods in specific scenarios.

**Result:** The training-free approach outperforms strong closed baselines on metaphor decomposition and meaning alignment scores, while the training-based approach performs closely behind. User studies indicate preferences for GPT-4o but highlight strengths in the training-free pipeline on abstract metaphors.

**Limitations:** Sensitivity to sampler settings; potential gaps in human aesthetic preferences.

**Conclusion:** Structured prompting and lightweight reinforcement learning effectively accomplish metaphor alignment without extensive retraining; gaps in performance related to human preference are linked to aesthetics and sampler settings.

**Abstract:** Visual metaphor generation is a challenging task that aims to generate an image given an input text metaphor. Inherently, it needs language understanding to bind a source concept with a target concept, in a way that preserves meaning while ensuring visual coherence. We propose a self-evaluating visual metaphor generation framework that focuses on metaphor alignment. Our self-evaluation approach combines existing metrics with our newly proposed metaphor decomposition score and a meaning alignment (MA) metric. Within this setup, we explore two novel approaches: a training-free pipeline that explicitly decomposes prompts into source-target-meaning (S-T-M) mapping for image synthesis, and a complementary training-based pipeline that improves alignment using our proposed self-evaluation reward schema, without any large-scale retraining. On the held-out test set, the training-free approach surpasses strong closed baselines (GPT-4o, Imagen) on decomposition, CLIP, and MA scores, with the training-based approach close behind. We evaluate our framework output using a user-facing study, and observed that participants preferred GPT-4o overall, while our training-free pipeline led open-source methods and edged Imagen on abstract metaphors. Our analyses show S-T-M prompting helps longer or more abstract metaphors, with closed models excelling on short, concrete cases; we also observe sensitivity to sampler settings. Overall, structured prompting and lightweight RL perform metaphor alignment well under modest compute, and remaining gaps to human preference appear driven by aesthetics and sampling.

</details>


### [32] [What do language models model? Transformers, automata, and the format of thought](https://arxiv.org/abs/2508.18598)

*Colin Klein*

**Main category:** cs.CL

**Keywords:** large language models, transformer architecture, human cognitive capacities, language processing, shortcut automata

**Relevance Score:** 7

**TL;DR:** The paper explores the differences between human linguistic capabilities and large language models (LLMs), arguing that LLMs model the corpus they are trained on rather than human cognitive processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the nature of what large language models actually model in comparison to human language processing capabilities.

**Method:** The author analyzes the computational architectures of transformers and cognitive science insights regarding human linguistic capabilities, particularly focusing on their linear versus supralinear processing formats.

**Key Contributions:**

	1. Non-deflationary defense of LLMs reflecting the corpus they are trained on
	2. Contrast between human supralinear computational capabilities and transformer linear formats
	3. Introduction of the concept of LLMs as discourse machines

**Result:** The study emphasizes that while language serves to express inner states, it also functions as a 'discourse machine' enabling new language creation in context.

**Limitations:** 

**Conclusion:** The author argues against the view that understanding LLMs is simply a deflationary task, positing a more nuanced perspective of language use in both humans and models.

**Abstract:** What do large language models actually model? Do they tell us something about human capacities, or are they models of the corpus we've trained them on? I give a non-deflationary defence of the latter position. Cognitive science tells us that linguistic capabilities in humans rely supralinear formats for computation. The transformer architecture, by contrast, supports at best a linear formats for processing. This argument will rely primarily on certain invariants of the computational architecture of transformers. I then suggest a positive story about what transformers are doing, focusing on Liu et al. (2022)'s intriguing speculations about shortcut automata. I conclude with why I don't think this is a terribly deflationary story. Language is not (just) a means for expressing inner state but also a kind of 'discourse machine' that lets us make new language given appropriate context. We have learned to use this technology in one way; LLMs have also learned to use it too, but via very different means.

</details>


### [33] [A New NMT Model for Translating Clinical Texts from English to Spanish](https://arxiv.org/abs/2508.18607)

*Rumeng Li, Xun Wang, Hong Yu*

**Main category:** cs.CL

**Keywords:** neural machine translation, health informatics, EHR, bilingual lexicon, biomedical knowledge

**Relevance Score:** 9

**TL;DR:** Introducing NOOV, a neural machine translation system designed to translate EHR narratives from English to Spanish with minimal in-domain parallel-aligned corpus.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The translation of electronic health record narratives faces challenges due to unknown words and the lack of parallel-aligned corpora, which can affect the accuracy and fluency of translations.

**Method:** NOOV employs a bilingual lexicon learned from parallel corpora and a phrase look-up table from biomedical knowledge resources to tackle unknown words and word-repetition issues in neural machine translation.

**Key Contributions:**

	1. Development of the NOOV neural machine translation system for EHR narratives.
	2. Integration of a bilingual lexicon for better word management in translations.
	3. Establishment of a phrase look-up table from biomedical knowledge resources to enhance translation quality.

**Result:** Evaluation indicates that NOOV produces improved translations of EHR narratives in terms of both accuracy and fluency compared to existing methods.

**Limitations:** 

**Conclusion:** NOOV demonstrates effective translation capabilities in clinical settings, offering a new approach to EHR translation with limited training data.

**Abstract:** Translating electronic health record (EHR) narratives from English to Spanish is a clinically important yet challenging task due to the lack of a parallel-aligned corpus and the abundant unknown words contained. To address such challenges, we propose \textbf{NOOV} (for No OOV), a new neural machine translation (NMT) system that requires little in-domain parallel-aligned corpus for training. NOOV integrates a bilingual lexicon automatically learned from parallel-aligned corpora and a phrase look-up table extracted from a large biomedical knowledge resource, to alleviate both the unknown word problem and the word-repeat challenge in NMT, enhancing better phrase generation of NMT systems. Evaluation shows that NOOV is able to generate better translation of EHR with improvement in both accuracy and fluency.

</details>


### [34] [Scaling Laws for Task-Stratified Knowledge in Post-Training Quantized Large Language Models](https://arxiv.org/abs/2508.18609)

*Chenxi Zhou, Pengfei Cao, Jiang Li, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Post-training quantization, Large language models, Knowledge capabilities, Scaling laws, Quantization strategies

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of post-training quantization on the knowledge capabilities of large language models, establishing task-stratified scaling laws and a framework for understanding varied sensitivities of memorization and utilization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how post-training quantization (PTQ) affects the knowledge capabilities of large language models and to address the gaps in existing scaling laws for quantized models.

**Method:** An empirical investigation was conducted to develop task-stratified scaling laws, focusing on model size, effective bit-width, calibration set size, and group size.

**Key Contributions:**

	1. Establishment of task-stratified scaling laws for LLMs under post-training quantization.
	2. Development of a unified quantitative framework to analyze the effects of quantization parameters.
	3. Insights into the differential sensitivities of knowledge memorization and utilization capabilities.

**Result:** Knowledge memorization is significantly more sensitive to variations in effective bit-width, calibration set size, and model size compared to knowledge utilization, indicating the need for more nuanced quantization strategies.

**Limitations:** 

**Conclusion:** The findings provide a framework for knowledge-aware quantization strategies that better preserve targeted cognitive functions in LLMs during deployment.

**Abstract:** Large language models (LLMs) present significant deployment challenges due to their scale, with post-training quantization (PTQ) emerging as a practical compression solution. However, a comprehensive understanding of how PTQ precisely impacts diverse LLM knowledge capabilities remains elusive, and existing scaling laws for quantized models often overlook crucial PTQ-specific parameters and task-specific sensitivities. This paper addresses these gaps by conducting an extensive empirical investigation to establish task-stratified scaling laws. We disentangle LLM knowledge into memorization and utilization capabilities and develop a unified quantitative framework that incorporates model size, effective bit-width, calibration set size, and group size. Our central finding reveals that knowledge memorization exhibits markedly greater sensitivity to variations in effective bit-width, calibration set size, and model size compared to the more robust knowledge utilization. These findings offer a fine-grained understanding of PTQ's impact and provide guidance for developing knowledge-aware quantization strategies that can better preserve targeted cognitive functions.

</details>


### [35] [Thinking Before You Speak: A Proactive Test-time Scaling Approach](https://arxiv.org/abs/2508.18648)

*Cong Li, Wenchang Chai, Hejun Wu, Yan Pan, Pengxu Wei, Liang Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Framework, Human Reasoning Patterns, Complex Problems, Insights

**Relevance Score:** 8

**TL;DR:** This paper proposes a reasoning framework called Thinking Before You Speak (TBYS) that enhances complex reasoning in Large Language Models (LLMs) by inserting proactive insights between reasoning steps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with complex reasoning tasks due to differences in human reasoning patterns and the training data. This gap can leave out critical insights necessary for completing reasoning steps effectively.

**Method:** The TBYS framework involves inserting proactively generated insights between reasoning steps, allowing for a review of the status and initiation of subsequent reasoning.

**Key Contributions:**

	1. Introduction of proactive insights in reasoning processes
	2. Development of the TBYS framework for LLMs
	3. Automated pipeline for collecting and filtering in-context examples

**Result:** Experiments on challenging mathematical datasets demonstrate that the TBYS framework significantly improves the reasoning capabilities of LLMs on complex problems.

**Limitations:** 

**Conclusion:** The TBYS framework shows promise in bridging the reasoning gap in LLMs by utilizing proactive insights, thus reducing the reliance on static prompts and improving reasoning processes.

**Abstract:** Large Language Models (LLMs) often exhibit deficiencies with complex reasoning tasks, such as maths, which we attribute to the discrepancy between human reasoning patterns and those presented in the LLMs' training data. When dealing with complex problems, humans tend to think carefully before expressing solutions. However, they often do not articulate their inner thoughts, including their intentions and chosen methodologies. Consequently, critical insights essential for bridging reasoning steps may be absent in training data collected from human sources. To bridge this gap, we proposes inserting \emph{insight}s between consecutive reasoning steps, which review the status and initiate the next reasoning steps. Unlike prior prompting strategies that rely on a single or a workflow of static prompts to facilitate reasoning, \emph{insight}s are \emph{proactively} generated to guide reasoning processes. We implement our idea as a reasoning framework, named \emph{Thinking Before You Speak} (TBYS), and design a pipeline for automatically collecting and filtering in-context examples for the generation of \emph{insight}s, which alleviates human labeling efforts and fine-tuning overheads. Experiments on challenging mathematical datasets verify the effectiveness of TBYS. Project website: https://gitee.com/jswrt/TBYS

</details>


### [36] [Breaking the Trade-Off Between Faithfulness and Expressiveness for Large Language Models](https://arxiv.org/abs/2508.18651)

*Chenxu Yang, Qingyi Si, Zheng Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, faithfulness, expressiveness, Collaborative Decoding, external knowledge

**Relevance Score:** 9

**TL;DR:** Proposes Collaborative Decoding (CoDe) to enhance Large Language Models' outputs by balancing faithfulness and expressiveness through external knowledge integration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-off between faithfulness and expressiveness in outputs generated by Large Language Models, which often lack reliable integration of external knowledge.

**Method:** The paper introduces Collaborative Decoding (CoDe), which dynamically integrates output probabilities with and without external knowledge, guided by distribution divergence and model confidence.

**Key Contributions:**

	1. Introduction of Collaborative Decoding (CoDe) framework
	2. Dynamic integration of knowledge into LLM outputs
	3. Implementation of a knowledge-aware reranking mechanism

**Result:** Experiments show that CoDe significantly improves the faithfulness of model outputs without sacrificing expressiveness across various LLMs and evaluation metrics.

**Limitations:** 

**Conclusion:** CoDe represents a plug-and-play solution that enhances the performance of LLMs by improving the integration of external knowledge while maintaining natural output quality.

**Abstract:** Grounding responses in external knowledge represents an effective strategy for mitigating hallucinations in Large Language Models (LLMs). However, current LLMs struggle to seamlessly integrate knowledge while simultaneously maintaining faithfulness (or fidelity) and expressiveness, capabilities that humans naturally possess. This limitation results in outputs that either lack support from external knowledge, thereby compromising faithfulness, or appear overly verbose and unnatural, thus sacrificing expressiveness. In this work, to break the trade-off between faithfulness and expressiveness, we propose Collaborative Decoding (CoDe), a novel approach that dynamically integrates output probabilities generated with and without external knowledge. This integration is guided by distribution divergence and model confidence, enabling the selective activation of relevant and reliable expressions from the model's internal parameters. Furthermore, we introduce a knowledge-aware reranking mechanism that prevents over-reliance on prior parametric knowledge while ensuring proper utilization of provided external information. Through comprehensive experiments, our plug-and-play CoDe framework demonstrates superior performance in enhancing faithfulness without compromising expressiveness across diverse LLMs and evaluation metrics, validating both its effectiveness and generalizability.

</details>


### [37] [Emotion Omni: Enabling Empathetic Speech Response Generation through Large Language Models](https://arxiv.org/abs/2508.18655)

*Haoyu Wang, Guangyan Zhang, Jiale Chen, Jingyu Li, Yuehai Wang, Yiwen Guo*

**Main category:** cs.CL

**Keywords:** speech LLM, empathy, human-machine interaction, data generation, emotional dialogue

**Relevance Score:** 9

**TL;DR:** Emotion Omni is a novel speech LLM architecture that generates empathetic responses by understanding emotional content without needing massive datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve interaction quality between users and speech assistants by incorporating emotional understanding in responses.

**Method:** The proposed model, Emotion Omni, leverages a unique architecture for understanding emotional cues and includes a data generation pipeline for creating a dataset of 200k emotional dialogues.

**Key Contributions:**

	1. Introduction of Emotion Omni model architecture for empathetic speech generation
	2. Creation of a 200k emotional dialogue dataset using a data generation pipeline
	3. Demonstration of empathetic response generation with limited training data

**Result:** Emotion Omni successfully generates empathetic responses to user queries using a limited amount of data, demonstrating the feasibility of developing speech LLMs without extensive training datasets.

**Limitations:** The model's performance may be affected by the quality and diversity of the generated dataset.

**Conclusion:** The development of Emotion Omni represents a significant step toward creating empathetic speech assistants that enhance user experience through better emotional understanding.

**Abstract:** With the development of speech large language models (speech LLMs), users can now interact directly with assistants via speech. However, most existing models simply convert the response content into speech without fully understanding the rich emotional and paralinguistic cues embedded in the user's query. In many cases, the same sentence can have different meanings depending on the emotional expression. Furthermore, emotional understanding is essential for improving user experience in human-machine interaction. Currently, most speech LLMs with empathetic capabilities are trained on massive datasets. This approach requires vast amounts of data and significant computational resources. Therefore, a key challenge lies in how to develop a speech LLM capable of generating empathetic responses with limited data and without the need for large-scale training. To address this challenge, we propose Emotion Omni, a novel model architecture designed to understand the emotional content of user speech input and generate empathetic speech responses. Additionally, we developed a data generation pipeline based on an open-source TTS framework to construct a 200k emotional dialogue dataset, which supports the construction of an empathetic speech assistant. The demos are available at https://w311411.github.io/omni_demo/

</details>


### [38] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)

*Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang*

**Main category:** cs.CL

**Keywords:** Generative Interfaces, Language Models, Human-AI Interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces Generative Interfaces for Language Models, which improve user interaction by generating adaptive UIs from user queries, outperforming traditional chat-based systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of interactions in multi-turn, information-dense, and exploratory tasks with LLMs, moving beyond linear request-response formats.

**Method:** The proposed framework creates structured interface-specific representations to generate task-specific UIs based on user queries, evaluated through a multidimensional assessment framework.

**Key Contributions:**

	1. Introduction of generative interfaces for LLMs.
	2. Development of a multidimensional assessment framework for evaluating user interaction.
	3. Demonstrated user preference for generative interfaces over conversational systems.

**Result:** Generative interfaces showed superior performance to traditional chat formats and were preferred by users in over 70% of interactions.

**Limitations:** 

**Conclusion:** The findings highlight the advantages of generative interfaces in human-AI interaction, paving the way for future research and application improvements.

**Abstract:** Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.

</details>


### [39] [Tailored Teaching with Balanced Difficulty: Elevating Reasoning in Multimodal Chain-of-Thought via Prompt Curriculum](https://arxiv.org/abs/2508.18673)

*Xinglong Yang, Quan Feng, Zhongying Pan, Xiang Chen, Yu Tian, Wentong Li, Shuofei Qiao, Yuxia Geng, Xingyu Zhao, Sheng-Jun Huang*

**Main category:** cs.CL

**Keywords:** Multimodal Chain-of-Thought, prompt selection, machine learning, active learning, large language models

**Relevance Score:** 7

**TL;DR:** The paper proposes a new framework for Multimodal Chain-of-Thought (MCoT) prompting that addresses the limitations of existing methods by using a tailored example selection process based on model abilities and task complexity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods of example selection for MCoT prompting lead to unstable model performance due to the use of randomly or manually selected examples. The paper aims to improve this by tailoring examples to the model's current capabilities.

**Method:** The authors propose a prompt curriculum design approach that combines model-perceived difficulty and intrinsic sample complexity to select diverse training examples that balance difficulty.

**Key Contributions:**

	1. Introduces a novel framework for prompt curriculum design in MCoT prompting.
	2. Combines model-perceived difficulty with intrinsic sample complexity for example selection.
	3. Demonstrates substantial performance improvements across multiple benchmarks and MLLMs.

**Result:** Experimentation on five benchmarks and with various Multimodal Large Language Models (MLLMs) shows significant and consistent performance improvements, reducing discrepancies from random sampling.

**Limitations:** 

**Conclusion:** The developed difficulty-balanced sampling strategy provides a robust and principled approach to enhance multimodal reasoning in MCoT prompting.

**Abstract:** The effectiveness of Multimodal Chain-of-Thought (MCoT) prompting is often limited by the use of randomly or manually selected examples. These examples fail to account for both model-specific knowledge distributions and the intrinsic complexity of the tasks, resulting in suboptimal and unstable model performance. To address this, we propose a novel framework inspired by the pedagogical principle of "tailored teaching with balanced difficulty". We reframe prompt selection as a prompt curriculum design problem: constructing a well ordered set of training examples that align with the model's current capabilities. Our approach integrates two complementary signals: (1) model-perceived difficulty, quantified through prediction disagreement in an active learning setup, capturing what the model itself finds challenging; and (2) intrinsic sample complexity, which measures the inherent difficulty of each question-image pair independently of any model. By jointly analyzing these signals, we develop a difficulty-balanced sampling strategy that ensures the selected prompt examples are diverse across both dimensions. Extensive experiments conducted on five challenging benchmarks and multiple popular Multimodal Large Language Models (MLLMs) demonstrate that our method yields substantial and consistent improvements and greatly reduces performance discrepancies caused by random sampling, providing a principled and robust approach for enhancing multimodal reasoning.

</details>


### [40] [Knowing or Guessing? Robust Medical Visual Question Answering via Joint Consistency and Contrastive Learning](https://arxiv.org/abs/2508.18687)

*Songtao Jiang, Yuxi Chen, Sibo Song, Yan Zhang, Yeying Jin, Yang Feng, Jian Wu, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Medical Vision-Language Models, Visual Question Answering, Consistency Learning

**Relevance Score:** 8

**TL;DR:** This paper addresses the fragility of Medical Vision-Language Models (Med-VLMs) in Medical Visual Question Answering, proposing a new dataset and innovative learning strategies to improve answer consistency and robustness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The inconsistency in answers from Med-VLMs when faced with different phrasings of the same medical question undermines their reliability in high-stakes medical scenarios.

**Method:** The authors construct RoMed, a dataset with 144k varied questions, and propose Consistency and Contrastive Learning (CCL) that uses knowledge-anchored consistency learning and bias-aware contrastive learning to enhance model robustness.

**Key Contributions:**

	1. Introduction of the RoMed dataset with diverse question variations
	2. Development of Consistency and Contrastive Learning methods
	3. Demonstration of improved model robustness and answer consistency

**Result:** State-of-the-art models like LLaVA-Med showed a performance drop of 40% in Recall when evaluated on RoMed compared to original VQA benchmarks; CCL improves answer consistency by 50% on the RoMed test set.

**Limitations:** 

**Conclusion:** The proposed CCL methodology significantly enhances the robustness and performance of Med-VLMs in Medical Visual Question Answering tasks.

**Abstract:** In high-stakes medical applications, consistent answering across diverse question phrasings is essential for reliable diagnosis. However, we reveal that current Medical Vision-Language Models (Med-VLMs) exhibit concerning fragility in Medical Visual Question Answering, as their answers fluctuate significantly when faced with semantically equivalent rephrasings of medical questions. We attribute this to two limitations: (1) insufficient alignment of medical concepts, leading to divergent reasoning patterns, and (2) hidden biases in training data that prioritize syntactic shortcuts over semantic understanding. To address these challenges, we construct RoMed, a dataset built upon original VQA datasets containing 144k questions with variations spanning word-level, sentence-level, and semantic-level perturbations. When evaluating state-of-the-art (SOTA) models like LLaVA-Med on RoMed, we observe alarming performance drops (e.g., a 40\% decline in Recall) compared to original VQA benchmarks, exposing critical robustness gaps. To bridge this gap, we propose Consistency and Contrastive Learning (CCL), which integrates two key components: (1) knowledge-anchored consistency learning, aligning Med-VLMs with medical knowledge rather than shallow feature patterns, and (2) bias-aware contrastive learning, mitigating data-specific priors through discriminative representation refinement. CCL achieves SOTA performance on three popular VQA benchmarks and notably improves answer consistency by 50\% on the challenging RoMed test set, demonstrating significantly enhanced robustness. Code will be released.

</details>


### [41] [Attention2Probability: Attention-Driven Terminology Probability Estimation for Robust Speech-to-Text System](https://arxiv.org/abs/2508.18701)

*Yanfan Du, Jun Zhang, Bin Wang, Jin Qiu, Lu Huang, Yuan Ge, Xiaoqian Liu, Tong Xiao, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** speech recognition, large language models, terminology accuracy, cross-attention, curriculum learning

**Relevance Score:** 7

**TL;DR:** This paper presents Attention2Probability, a method that estimates the probability of domain-specific terminology in speech recognition and translation, addressing challenges in generating accurate neologisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advances in speech large language models, accurately generating domain-specific terms in speech recognition and translation is still challenging.

**Method:** Attention2Probability leverages cross-attention weights between speech and terminology to estimate presence probabilities, while utilizing curriculum learning to enhance accuracy. A new speech dataset with terminology was also created to support this research.

**Key Contributions:**

	1. Introduction of Attention2Probability for terminology probability estimation in SLMs
	2. Creation and release of a new speech dataset with terminology for research
	3. Demonstration of superior performance in recall rates compared to existing methods

**Result:** Attention2Probability outperforms the VectorDB method, achieving maximum recall rates of 92.57% for Chinese and 86.83% for English, with a low latency of 8.71ms per query.

**Limitations:** 

**Conclusion:** Intervening in SLMs with Attention2Probability significantly improves terminology accuracy by 6-17%, highlighting limitations in SLMs' current utilization of terminology.

**Abstract:** Recent advances in speech large language models (SLMs) have improved speech recognition and translation in general domains, but accurately generating domain-specific terms or neologisms remains challenging. To address this, we propose Attention2Probability: attention-driven terminology probability estimation for robust speech-to-text system, which is lightweight, flexible, and accurate. Attention2Probability converts cross-attention weights between speech and terminology into presence probabilities, and it further employs curriculum learning to enhance retrieval accuracy. Furthermore, to tackle the lack of data for speech-to-text tasks with terminology intervention, we create and release a new speech dataset with terminology to support future research in this area. Experimental results show that Attention2Probability significantly outperforms the VectorDB method on our test set. Specifically, its maximum recall rates reach 92.57% for Chinese and 86.83% for English. This high recall is achieved with a latency of only 8.71ms per query. Intervening in SLMs' recognition and translation tasks using Attention2Probability-retrieved terms improves terminology accuracy by 6-17%, while revealing that the current utilization of terminology by SLMs has limitations.

</details>


### [42] [Filtering for Creativity: Adaptive Prompting for Multilingual Riddle Generation in LLMs](https://arxiv.org/abs/2508.18709)

*Duy Le, Kent Ziti, Evan Girard-Sun, Sean O'Brien, Vasu Sharma, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** multilingual, riddle generation, language models, Adaptive Originality Filtering, cross-lingual

**Relevance Score:** 4

**TL;DR:** This paper presents Adaptive Originality Filtering (AOF) to promote diverse and culturally fluent multilingual riddle generation using large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of standard prompting strategies that result in reused or shallow riddle generations in multilingual contexts.

**Method:** A framework called Adaptive Originality Filtering (AOF) is introduced, which utilizes cosine-based similarity rejection to filter out redundant output, while ensuring lexical novelty and cross-lingual fidelity.

**Key Contributions:**

	1. Introduction of Adaptive Originality Filtering (AOF) framework
	2. Demonstrated effectiveness on multiple LLMs and language pairs
	3. Improvement in lexical diversity and reduction of redundancy during riddle generation

**Result:** AOF-enhanced GPT-4o demonstrated significant improvements, achieving a Self-BLEU of 0.177 and Distinct-2 of 0.915 in Japanese, indicating enhanced lexical diversity and lower redundancy.

**Limitations:** 

**Conclusion:** The results suggest that employing semantic rejection can effectively guide creative generation in a culturally informed manner without the need for fine-tuning specific to tasks.

**Abstract:** Multilingual riddle generation challenges large language models (LLMs) to balance cultural fluency with creative abstraction. Standard prompting strategies -- zero-shot, few-shot, chain-of-thought -- tend to reuse memorized riddles or perform shallow paraphrasing. We introduce Adaptive Originality Filtering (AOF), a prompting framework that filters redundant generations using cosine-based similarity rejection, while enforcing lexical novelty and cross-lingual fidelity. Evaluated across three LLMs and four language pairs, AOF-enhanced GPT-4o achieves \texttt{0.177} Self-BLEU and \texttt{0.915} Distinct-2 in Japanese, signaling improved lexical diversity and reduced redundancy compared to other prompting methods and language pairs. Our findings show that semantic rejection can guide culturally grounded, creative generation without task-specific fine-tuning.

</details>


### [43] [EMMM, Explain Me My Model! Explainable Machine Generated Text Detection in Dialogues](https://arxiv.org/abs/2508.18715)

*Angela Yifei Yuan, Haoyi Li, Soyeon Caren Han, Christopher Leckie*

**Main category:** cs.CL

**Keywords:** large language models, machine-generated text, customer service, explainable AI, non-expert users

**Relevance Score:** 9

**TL;DR:** EMMM is an explanation-then-detection framework for detecting machine-generated text (MGT) in customer service, focusing on usability for non-expert users.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of large language models in customer service raises the risk of malicious impersonation through machine-generated text, necessitating effective detection methods that also provide explanations.

**Method:** The paper introduces EMMM, which offers explanations before detection, aiming to balance latency, accuracy, and interpretability for non-expert users in online settings.

**Key Contributions:**

	1. Introduction of EMMM framework for explainable MGT detection
	2. Demonstrated effective balance between accuracy, latency, and interpretability
	3. Open-sourced code and dataset for further research

**Result:** EMMM achieves competitive accuracy with state-of-the-art models while providing interpretable outputs preferred by 70% of human evaluators and maintaining low latency (under 1 second).

**Limitations:** 

**Conclusion:** The EMMM framework enhances the reliability of MGT detection in customer service by addressing the needs of non-expert users with interpretable explanations and efficient performance.

**Abstract:** The rapid adoption of large language models (LLMs) in customer service introduces new risks, as malicious actors can exploit them to conduct large-scale user impersonation through machine-generated text (MGT). Current MGT detection methods often struggle in online conversational settings, reducing the reliability and interpretability essential for trustworthy AI deployment. In customer service scenarios where operators are typically non-expert users, explanation become crucial for trustworthy MGT detection. In this paper, we propose EMMM, an explanation-then-detection framework that balances latency, accuracy, and non-expert-oriented interpretability. Experimental results demonstrate that EMMM provides explanations accessible to non-expert users, with 70\% of human evaluators preferring its outputs, while achieving competitive accuracy compared to state-of-the-art models and maintaining low latency, generating outputs within 1 second. Our code and dataset are open-sourced at https://github.com/AngieYYF/EMMM-explainable-chatbot-detection.

</details>


### [44] [Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models](https://arxiv.org/abs/2508.18739)

*Chang Wang, Siyu Yan, Depeng Yuan, Yuqi Chen, Yanhua Huang, Yuanhang Zheng, Shuhao Li, Yinqi Zhang, Kedi Chen, Mingrui Zhu, Ruiwen Xu*

**Main category:** cs.CL

**Keywords:** ad headlines, large language models, diversity in advertising

**Relevance Score:** 6

**TL;DR:** DIVER is a framework for generating diverse and high-quality ad headlines using LLMs, addressing the issue of homogeneous outputs in advertising.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for generating ad headlines often focus solely on quality or CTR, leading to a lack of diversity in outputs that can engage a wider audience.

**Method:** A multi-stage multi-objective optimization framework is proposed, which combines supervised fine-tuning and reinforcement learning to optimize both diversity and quality simultaneously during the generation of ad headlines.

**Key Contributions:**

	1. Introduction of the DIVER framework optimizing for both diversity and quality in ad headline generation.
	2. Development of a semantic- and stylistic-aware data generation pipeline for creating diverse training data.
	3. Validation of effectiveness through deployment on a large-scale content-sharing platform.

**Result:** Experiments on real-world datasets show that DIVER effectively improves both the advertiser value and CTR by 4.0% and 1.4%, respectively, while maintaining a balance between quality and diversity.

**Limitations:** 

**Conclusion:** DIVER offers a robust solution for advertisers seeking to enhance engagement through diverse ad headlines without compromising on quality.

**Abstract:** The generation of ad headlines plays a vital role in modern advertising, where both quality and diversity are essential to engage a broad range of audience segments. Current approaches primarily optimize language models for headline quality or click-through rates (CTR), often overlooking the need for diversity and resulting in homogeneous outputs. To address this limitation, we propose DIVER, a novel framework based on large language models (LLMs) that are jointly optimized for both diversity and quality. We first design a semantic- and stylistic-aware data generation pipeline that automatically produces high-quality training pairs with ad content and multiple diverse headlines. To achieve the goal of generating high-quality and diversified ad headlines within a single forward pass, we propose a multi-stage multi-objective optimization framework with supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments on real-world industrial datasets demonstrate that DIVER effectively balances quality and diversity. Deployed on a large-scale content-sharing platform serving hundreds of millions of users, our framework improves advertiser value (ADVV) and CTR by 4.0% and 1.4%.

</details>


### [45] [M3HG: Multimodal, Multi-scale, and Multi-type Node Heterogeneous Graph for Emotion Cause Triplet Extraction in Conversations](https://arxiv.org/abs/2508.18740)

*Qiao Liang, Ying Shen, Tiantian Chen, Lin Zhang*

**Main category:** cs.CL

**Keywords:** Emotion Cause Triplet Extraction, MECTEC, Multimodal Dataset

**Relevance Score:** 8

**TL;DR:** This paper introduces MECAD, a multimodal dataset for Emotion Cause Triplet Extraction (MECTEC), and proposes a novel model, M3HG, to improve extraction performance by capturing emotional and causal contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of datasets for Emotion Cause Triplet Extraction hinders model development, necessitating the creation of a more diverse dataset and improved modeling techniques.

**Method:** MECAD is introduced as the first multimodal, multi-scenario dataset for MECTEC, with 989 conversations from 56 TV series. The proposed M3HG model utilizes a multimodal heterogeneous graph to capture emotional and causal contexts and to fuse contextual information effectively.

**Key Contributions:**

	1. Introduction of the MECAD dataset, the first multimodal, multi-scenario dataset for MECTEC.
	2. Proposing the M3HG model that captures emotional and causal contexts explicitly.
	3. Demonstrating improved performance of M3HG over existing methods.

**Result:** Extensive experiments showed that M3HG outperforms existing state-of-the-art methods in Emotion Cause Triplet Extraction.

**Limitations:** 

**Conclusion:** The introduction of the MECAD dataset and the M3HG model provide significant advancements for research on emotion cause extraction in multimodal contexts.

**Abstract:** Emotion Cause Triplet Extraction in Multimodal Conversations (MECTEC) has recently gained significant attention in social media analysis, aiming to extract emotion utterances, cause utterances, and emotion categories simultaneously. However, the scarcity of related datasets, with only one published dataset featuring highly uniform dialogue scenarios, hinders model development in this field. To address this, we introduce MECAD, the first multimodal, multi-scenario MECTEC dataset, comprising 989 conversations from 56 TV series spanning a wide range of dialogue contexts. In addition, existing MECTEC methods fail to explicitly model emotional and causal contexts and neglect the fusion of semantic information at different levels, leading to performance degradation. In this paper, we propose M3HG, a novel model that explicitly captures emotional and causal contexts and effectively fuses contextual information at both inter- and intra-utterance levels via a multimodal heterogeneous graph. Extensive experiments demonstrate the effectiveness of M3HG compared with existing state-of-the-art methods. The codes and dataset are available at https://github.com/redifinition/M3HG.

</details>


### [46] [Chronological Passage Assembling in RAG framework for Temporal Question Answering](https://arxiv.org/abs/2508.18748)

*Byeongjeong Kim, Jeonghyun Park, Joonho Yang, Hwanhee Lee*

**Main category:** cs.CL

**Keywords:** long-context QA, narrative texts, retrieval-augmented generation, temporal order, ChronoRAG

**Relevance Score:** 9

**TL;DR:** ChronoRAG is a novel RAG framework designed for long-context question answering in narrative tasks, preserving narrative flow and temporal order in episodic information retrieval.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of long-context question answering in narrative texts, where understanding requires coherent timelines and contextual flow.

**Method:** ChronoRAG framework refines dispersed document information and captures temporal order among retrieved passages for improved narrative comprehension.

**Key Contributions:**

	1. Development of the ChronoRAG framework for narrative question answering
	2. Focus on temporal order in information retrieval
	3. Empirical validation showing improved performance on NarrativeQA tasks

**Result:** Empirical tests on the NarrativeQA dataset show significant improvements in both factual identification and understanding of complex sequential relationships in narratives.

**Limitations:** 

**Conclusion:** ChronoRAG effectively enhances narrative question answering by prioritizing contextual flow and temporal relationships in the retrieval process.

**Abstract:** Long-context question answering over narrative tasks is challenging because correct answers often hinge on reconstructing a coherent timeline of events while preserving contextual flow in a limited context window. Retrieval-augmented generation (RAG) indexing methods aim to address this challenge by selectively retrieving only necessary document segments. However, narrative texts possess unique characteristics that limit the effectiveness of these existing approaches. Specifically, understanding narrative texts requires more than isolated segments, as the broader context and sequential relationships between segments are crucial for comprehension. To address these limitations, we propose ChronoRAG, a novel RAG framework specialized for narrative texts. This approach focuses on two essential aspects: refining dispersed document information into coherent and structured passages, and preserving narrative flow by explicitly capturing and maintaining the temporal order among retrieved passages. We empirically demonstrate the effectiveness of ChronoRAG through experiments on the NarrativeQA dataset, showing substantial improvements in tasks requiring both factual identification and comprehension of complex sequential relationships, underscoring that reasoning over temporal order is crucial in resolving narrative QA.

</details>


### [47] [ThinkDial: An Open Recipe for Controlling Reasoning Effort in Large Language Models](https://arxiv.org/abs/2508.18773)

*Qianyu He, Siyu Yuan, Xuefeng Li, Mingxuan Wang, Jiangjie Chen*

**Main category:** cs.CL

**Keywords:** Large language models, controllable reasoning, machine learning, open-source framework, reinforcement learning

**Relevance Score:** 9

**TL;DR:** Introducing ThinkDial, an open-source framework for controlling LLM reasoning through discrete operational modes, achieving compression-performance trade-offs without significant performance loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of controlling computational effort in large language models (LLMs) during practical deployments, as current proprietary systems offer discrete operational modes that the open-source community lacks.

**Method:** An end-to-end training paradigm combining budget-mode supervised fine-tuning for embedding reasoning capabilities and budget-aware reinforcement learning for adaptive reward shaping.

**Key Contributions:**

	1. First open-recipe framework for controllable reasoning in LLMs
	2. Three distinct reasoning modes for tuning computational effort
	3. Demonstrated strong generalization on out-of-distribution tasks

**Result:** ThinkDial enables switching among three reasoning modes (High, Medium, Low) with significant token reduction and minimal performance degradation, demonstrating effective compression-performance trade-offs and strong out-of-distribution generalization.

**Limitations:** 

**Conclusion:** ThinkDial represents a significant advancement in controllable reasoning for LLMs, providing a practical solution for reducing computational costs while maintaining performance standards.

**Abstract:** Large language models (LLMs) with chain-of-thought reasoning have demonstrated remarkable problem-solving capabilities, but controlling their computational effort remains a significant challenge for practical deployment. Recent proprietary systems like OpenAI's gpt-oss series have introduced discrete operational modes for intuitive reasoning control, but the open-source community has largely failed to achieve such capabilities. In this paper, we introduce ThinkDial, the first open-recipe end-to-end framework that successfully implements gpt-oss-style controllable reasoning through discrete operational modes. Our system enables seamless switching between three distinct reasoning regimes: High mode (full reasoning capability), Medium mode (50 percent token reduction with <10 percent performance degradation), and Low mode (75 percent token reduction with <15 percent performance degradation). We achieve this through an end-to-end training paradigm that integrates budget-mode control throughout the entire pipeline: budget-mode supervised fine-tuning that embeds controllable reasoning capabilities directly into the learning process, and two-phase budget-aware reinforcement learning with adaptive reward shaping. Extensive experiments demonstrate that ThinkDial achieves target compression-performance trade-offs with clear response length reductions while maintaining performance thresholds. The framework also exhibits strong generalization capabilities on out-of-distribution tasks.

</details>


### [48] [Harnessing Rule-Based Reinforcement Learning for Enhanced Grammatical Error Correction](https://arxiv.org/abs/2508.18780)

*Yilin Li, Xunjian Yin, Yilin Chen, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** Grammatical Error Correction, Reinforcement Learning, LLMs, NLP

**Relevance Score:** 8

**TL;DR:** This paper presents a Rule-Based Reinforcement Learning (RL) framework for Grammatical Error Correction (GEC) using LLMs, achieving state-of-the-art performance on Chinese datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve Grammatical Error Correction by addressing the limitations of traditional supervised fine-tuning methods in LLMs, which fail to leverage the full reasoning potential of these models.

**Method:** The proposed framework utilizes Rule-Based RL to guide the LLMs in generating corrected sentences, enhancing their reasoning capabilities.

**Key Contributions:**

	1. Introduction of a Rule-Based RL framework for GEC
	2. Achieves state-of-the-art performance on Chinese datasets
	3. Demonstrates the advantages of using RL for guiding LLMs.

**Result:** The framework demonstrates state-of-the-art performance in GEC tasks, achieving a significant increase in recall on Chinese datasets.

**Limitations:** 

**Conclusion:** The study suggests that Rule-Based RL can effectively enhance the controllability and reliability of LLMs in grammatical error correction.

**Abstract:** Grammatical error correction is a significant task in NLP. Traditional methods based on encoder-decoder models have achieved certain success, but the application of LLMs in this field is still underexplored. Current research predominantly relies on supervised fine-tuning to train LLMs to directly generate the corrected sentence, which limits the model's powerful reasoning ability. To address this limitation, we propose a novel framework based on Rule-Based RL. Through experiments on the Chinese datasets, our Rule-Based RL framework achieves \textbf{state-of-the-art }performance, with a notable increase in \textbf{recall}. This result clearly highlights the advantages of using RL to steer LLMs, offering a more controllable and reliable paradigm for future development in GEC.

</details>


### [49] [Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems](https://arxiv.org/abs/2503.04945)

*Jooyoung Lee, Xiaochen Zhu, Georgi Karadzhov, Tom Stafford, Andreas Vlachos, Dongwon Lee*

**Main category:** cs.CL

**Keywords:** Deepfake detection, Human-AI collaboration, Group dynamics, Chatbot, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** Explores how the DeepFakeDeLiBot chatbot improves group collaboration in detecting deepfake text, although individual performance gains are limited.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of generative models necessitates better methods for distinguishing between authentic and machine-generated content, with collaborative human efforts augmented by AI potentially providing a solution.

**Method:** Experimentation with the DeepFakeDeLiBot chatbot among groups designed to identify deepfake text, measuring accuracy against individual efforts and group dynamics.

**Key Contributions:**

	1. Demonstration of improved accuracy in deepfake detection through group collaboration.
	2. Enhanced group dynamics facilitated by the chatbot, increasing engagement and diversity of reasoning.
	3. Insight into the limitations of AI tools in collaborative contexts, particularly regarding individual performance gains.

**Result:** Group participation improves identification accuracy of machine-generated paragraphs, with notable enhancements in group engagement and reasoning diversity, despite limited overall performance gains from the chatbot.

**Limitations:** Engagement with the chatbot did not lead to significant performance improvements for individuals and varied effectiveness based on perceptions of group collaboration.

**Conclusion:** Deliberative chatbots like DeepFakeDeLiBot enhance interactive group dynamics essential for accurate deepfake text detection, suggesting their potential for collaborative efforts in HCI contexts.

**Abstract:** The proliferation of generative models has presented significant challenges in distinguishing authentic human-authored content from deepfake content. Collaborative human efforts, augmented by AI tools, present a promising solution. In this study, we explore the potential of DeepFakeDeLiBot, a deliberation-enhancing chatbot, to support groups in detecting deepfake text. Our findings reveal that group-based problem-solving significantly improves the accuracy of identifying machine-generated paragraphs compared to individual efforts. While engagement with DeepFakeDeLiBot does not yield substantial performance gains overall, it enhances group dynamics by fostering greater participant engagement, consensus building, and the frequency and diversity of reasoning-based utterances. Additionally, participants with higher perceived effectiveness of group collaboration exhibited performance benefits from DeepFakeDeLiBot. These findings underscore the potential of deliberative chatbots in fostering interactive and productive group dynamics while ensuring accuracy in collaborative deepfake text detection. \textit{Dataset and source code used in this study will be made publicly available upon acceptance of the manuscript.

</details>


### [50] [Controllable Conversational Theme Detection Track at DSTC 12](https://arxiv.org/abs/2508.18783)

*Igor Shalyminov, Hang Su, Jake Vincent, Siffi Singh, Jason Cai, James Gung, Raphael Shu, Saab Mansour*

**Main category:** cs.CL

**Keywords:** Conversational Analytics, Theme Detection, Large Language Models, Dialog Systems, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces Theme Detection in conversational analytics aimed at automatically identifying and categorizing topics in conversations, leveraging recent advances in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for automating complex dialog analysis in domains like customer support and sales due to the rapid adoption of LLMs in analytics.

**Method:** It presents the Controllable Conversational Theme Detection as a competition track, framed as joint clustering and theme labeling, utilizing user preference data for granular control.

**Key Contributions:**

	1. Introduction of Theme Detection for automation in conversational analytics
	2. Framing of the Controllable Conversational Theme Detection problem
	3. Open availability of datasets and evaluation metrics for community use

**Result:** The paper discusses the overview of the task, provided datasets, evaluation metrics, and insights from participant submissions.

**Limitations:** 

**Conclusion:** The approach allows for flexibility in summarizing conversations and is aimed at reducing manual effort in dialog analysis.

**Abstract:** Conversational analytics has been on the forefront of transformation driven by the advances in Speech and Natural Language Processing techniques. Rapid adoption of Large Language Models (LLMs) in the analytics field has taken the problems that can be automated to a new level of complexity and scale. In this paper, we introduce Theme Detection as a critical task in conversational analytics, aimed at automatically identifying and categorizing topics within conversations. This process can significantly reduce the manual effort involved in analyzing expansive dialogs, particularly in domains like customer support or sales. Unlike traditional dialog intent detection, which often relies on a fixed set of intents for downstream system logic, themes are intended as a direct, user-facing summary of the conversation's core inquiry. This distinction allows for greater flexibility in theme surface forms and user-specific customizations. We pose Controllable Conversational Theme Detection problem as a public competition track at Dialog System Technology Challenge (DSTC) 12 -- it is framed as joint clustering and theme labeling of dialog utterances, with the distinctive aspect being controllability of the resulting theme clusters' granularity achieved via the provided user preference data. We give an overview of the problem, the associated dataset and the evaluation metrics, both automatic and human. Finally, we discuss the participant teams' submissions and provide insights from those. The track materials (data and code) are openly available in the GitHub repository.

</details>


### [51] [LaTeXTrans: Structured LaTeX Translation with Multi-Agent Coordination](https://arxiv.org/abs/2508.18791)

*Ziming Zhu, Chenglong Wang, Shunjie Xing, Yifu Huo, Fengning Tian, Quan Du, Di Yang, Chunliang Zhang, Tong Xiao, Jingbo Zhu*

**Main category:** cs.CL

**Keywords:** Machine Translation, LaTeX, Multi-agent System, Natural Language Processing, Translation Accuracy

**Relevance Score:** 4

**TL;DR:** LaTeXTrans is a collaborative multi-agent system designed for translating structured LaTeX documents while preserving format, structure, and terminology.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Modern machine translation struggles with LaTeX documents that contain natural language and complex formatting such as equations, tables, and references, which need to be preserved for accurate translation.

**Method:** LaTeXTrans employs six specialized agents: a Parser for syntax decomposition, a Translator for context-aware translations, a Validator, a Summarizer, a Terminology Extractor for consistency, and a Generator for reconstructing LaTeX documents.

**Key Contributions:**

	1. Introduction of a multi-agent system for LaTeX translation
	2. Enhanced accuracy and structural fidelity in translations
	3. Collaborative approach incorporating specialized agents for improved outcomes.

**Result:** LaTeXTrans demonstrates superior performance compared to mainstream machine translation systems, achieving higher accuracy in translation and better structural integrity in LaTeX documents.

**Limitations:** 

**Conclusion:** LaTeXTrans offers a practical solution for translating LaTeX-formatted documents while ensuring content integrity, making it valuable for fields with complex document requirements.

**Abstract:** Despite the remarkable progress of modern machine translation (MT) systems on general-domain texts, translating structured LaTeX-formatted documents remains a significant challenge. These documents typically interleave natural language with domain-specific syntax, such as mathematical equations, tables, figures, and cross-references, all of which must be accurately preserved to maintain semantic integrity and compilability. In this paper, we introduce LaTeXTrans, a collaborative multi-agent system designed to address this challenge. LaTeXTrans ensures format preservation, structural fidelity, and terminology consistency through six specialized agents: 1) a Parser that decomposes LaTeX into translation-friendly units via placeholder substitution and syntax filtering; 2) a Translator, Validator, Summarizer, and Terminology Extractor that work collaboratively to ensure context-aware, self-correcting, and terminology-consistent translations; 3) a Generator that reconstructs the translated content into well-structured LaTeX documents. Experimental results demonstrate that LaTeXTrans can outperform mainstream MT systems in both translation accuracy and structural fidelity, offering an effective and practical solution for translating LaTeX-formatted documents.

</details>


### [52] [LLM-based Contrastive Self-Supervised AMR Learning with Masked Graph Autoencoders for Fake News Detection](https://arxiv.org/abs/2508.18819)

*Shubham Gupta, Shraban Kumar Chatterjee, Suman Kundu*

**Main category:** cs.CL

**Keywords:** misinformation detection, self-supervised learning, large language model, news propagation, graph contrastive loss

**Relevance Score:** 9

**TL;DR:** This paper presents a self-supervised framework for misinformation detection that integrates semantic relations and news propagation dynamics, utilizing an LLM-based graph contrastive loss for improved feature separability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of misinformation detection in the presence of long-range dependencies and the need for extensive labeled datasets.

**Method:** A novel self-supervised misinformation detection framework combining Abstract Meaning Representation (AMR) and news propagation dynamics, utilizing LLM-based graph contrastive loss and a multi-view graph masked autoencoder.

**Key Contributions:**

	1. Integration of semantic relations and news propagation dynamics for misinformation detection.
	2. Development of LLM-based graph contrastive loss (LGCL) enhancing feature separability.
	3. Use of multi-view graph masked autoencoder to incorporate social context in learning.

**Result:** The proposed framework outperforms state-of-the-art methodologies in misinformation detection, achieving better performance with limited labeled datasets.

**Limitations:** 

**Conclusion:** The self-supervised approach improves generalizability and effectiveness in differentiating between fake and real news.

**Abstract:** The proliferation of misinformation in the digital age has led to significant societal challenges. Existing approaches often struggle with capturing long-range dependencies, complex semantic relations, and the social dynamics influencing news dissemination. Furthermore, these methods require extensive labelled datasets, making their deployment resource-intensive. In this study, we propose a novel self-supervised misinformation detection framework that integrates both complex semantic relations using Abstract Meaning Representation (AMR) and news propagation dynamics. We introduce an LLM-based graph contrastive loss (LGCL) that utilizes negative anchor points generated by a Large Language Model (LLM) to enhance feature separability in a zero-shot manner. To incorporate social context, we employ a multi view graph masked autoencoder, which learns news propagation features from social context graph. By combining these semantic and propagation-based features, our approach effectively differentiates between fake and real news in a self-supervised manner. Extensive experiments demonstrate that our self-supervised framework achieves superior performance compared to other state-of-the-art methodologies, even with limited labelled datasets while improving generalizability.

</details>


### [53] [Arrows of Math Reasoning Data Synthesis for Large Language Models: Diversity, Complexity and Correctness](https://arxiv.org/abs/2508.18824)

*Sirui Chen, Changxin Tian, Binbin Hu, Kunlong Chen, Ziqi Liu, Zhiqiang Zhang, Jun Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, Data Synthesis

**Relevance Score:** 9

**TL;DR:** A framework is proposed for generating high-quality mathematical data to enhance LLM reasoning, overcoming previous limitations in scalability and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional methods for training LLMs in mathematical reasoning struggle with scalability, cost, and data reliability, necessitating a new approach.

**Method:** A novel program-assisted synthesis framework is introduced, which generates diverse, complex, and correct mathematical problem-solution pairs using executable programs and a validation mechanism.

**Key Contributions:**

	1. Introduction of a program-assisted synthesis framework for LLM training
	2. Generation of 12.3 million verified problem-solution triples
	3. Demonstration of significant performance improvements on benchmark datasets

**Result:** Models fine-tuned on the generated data achieved state-of-the-art performance on benchmark datasets, indicating significant improvements in their inference capabilities.

**Limitations:** 

**Conclusion:** The proposed synthesis approach effectively enhances the mathematical reasoning of LLMs and generates a large corpus of verified problem-solving triples.

**Abstract:** Enhancing the mathematical reasoning of large language models (LLMs) demands high-quality training data, yet conventional methods face critical challenges in scalability, cost, and data reliability. To address these limitations, we propose a novel program-assisted synthesis framework that systematically generates a high-quality mathematical corpus with guaranteed diversity, complexity, and correctness. This framework integrates mathematical knowledge systems and domain-specific tools to create executable programs. These programs are then translated into natural language problem-solution pairs and vetted by a bilateral validation mechanism that verifies solution correctness against program outputs and ensures program-problem consistency. We have generated 12.3 million such problem-solving triples. Experiments demonstrate that models fine-tuned on our data significantly improve their inference capabilities, achieving state-of-the-art performance on several benchmark datasets and showcasing the effectiveness of our synthesis approach.

</details>


### [54] [ConfTuner: Training Large Language Models to Express Their Confidence Verbally](https://arxiv.org/abs/2508.18847)

*Yibo Li, Miao Xiong, Jiaying Wu, Bryan Hooi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Confidence Calibration, Machine Learning, Health Informatics, Trustworthy AI

**Relevance Score:** 9

**TL;DR:** ConfTuner is a new fine-tuning method for large language models that improves the calibration of verbalized confidence without requiring ground-truth confidence scores.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With LLMs being used in critical fields like healthcare, ensuring accurate expressions of uncertainty is vital for trustworthiness, yet current models often express overconfidence.

**Method:** The paper introduces ConfTuner, which utilizes a new loss function called tokenized Brier score to properly incentivize models to accurately reflect their confidence levels without needing ground-truth scores.

**Key Contributions:**

	1. Introduction of ConfTuner for fine-tuning LLMs
	2. Use of a tokenized Brier score as a new loss function
	3. Demonstrated generalization to black-box models for improved calibration

**Result:** ConfTuner shows improved calibration across various reasoning tasks and enhances performance in self-correction and model cascade contexts, even with black-box models like GPT-4o.

**Limitations:** 

**Conclusion:** Better calibration in LLMs leads to increased reliability and has significant implications for trust in AI applications in sensitive domains.

**Abstract:** Large Language Models (LLMs) are increasingly deployed in high-stakes domains such as science, law, and healthcare, where accurate expressions of uncertainty are essential for reliability and trust. However, current LLMs are often observed to generate incorrect answers with high confidence, a phenomenon known as "overconfidence". Recent efforts have focused on calibrating LLMs' verbalized confidence: i.e., their expressions of confidence in text form, such as "I am 80% confident that...". Existing approaches either rely on prompt engineering or fine-tuning with heuristically generated uncertainty estimates, both of which have limited effectiveness and generalizability. Motivated by the notion of proper scoring rules for calibration in classical machine learning models, we introduce ConfTuner, a simple and efficient fine-tuning method that introduces minimal overhead and does not require ground-truth confidence scores or proxy confidence estimates. ConfTuner relies on a new loss function, tokenized Brier score, which we theoretically prove to be a proper scoring rule, intuitively meaning that it "correctly incentivizes the model to report its true probability of being correct". ConfTuner improves calibration across diverse reasoning tasks and generalizes to black-box models such as GPT-4o. Our results further show that better-calibrated confidence enables downstream gains in self-correction and model cascade, advancing the development of trustworthy LLM systems. The code is available at https://github.com/liushiliushi/ConfTuner.

</details>


### [55] [ReflectivePrompt: Reflective evolution in autoprompting algorithms](https://arxiv.org/abs/2508.18870)

*Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin*

**Main category:** cs.CL

**Keywords:** Autoprompting, Evolutionary Algorithms, Large Language Models, Prompt Engineering, Machine Learning

**Relevance Score:** 9

**TL;DR:** ReflectivePrompt improves autoprompting for language models using evolutionary algorithms, achieving significant performance gains over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for optimized prompt selection in language models, driven by advancements in prompt engineering and the effectiveness of large language models (LLMs).

**Method:** ReflectivePrompt employs a reflective evolution approach with short-term and long-term reflection operations before crossover and elitist mutation, aiming for a more precise search of optimal prompts.

**Key Contributions:**

	1. Introduction of ReflectivePrompt for optimizing prompts in LLMs
	2. Use of evolutionary algorithms with reflective operations
	3. Demonstrated significant performance improvements on multiple datasets

**Result:** The method has been tested on 33 datasets and shows an average performance improvement of 28% on certain metrics compared to the previous state-of-the-art, EvoPrompt.

**Limitations:** 

**Conclusion:** ReflectivePrompt establishes itself as a leading solution in evolutionary algorithm-based autoprompting, enhancing the prompt selection process significantly.

**Abstract:** Autoprompting is the process of automatically selecting optimized prompts for language models, which has been gaining popularity with the rapid advancement of prompt engineering, driven by extensive research in the field of large language models (LLMs). This paper presents ReflectivePrompt - a novel autoprompting method based on evolutionary algorithms that employs a reflective evolution approach for more precise and comprehensive search of optimal prompts. ReflectivePrompt utilizes short-term and long-term reflection operations before crossover and elitist mutation to enhance the quality of the modifications they introduce. This method allows for the accumulation of knowledge obtained throughout the evolution process and updates it at each epoch based on the current population. ReflectivePrompt was tested on 33 datasets for classification and text generation tasks using open-access large language models: t-lite-instruct-0.1 and gemma3-27b-it. The method demonstrates, on average, a significant improvement (e.g., 28% on BBH compared to EvoPrompt) in metrics relative to current state-of-the-art approaches, thereby establishing itself as one of the most effective solutions in evolutionary algorithm-based autoprompting.

</details>


### [56] [Empowering Computing Education Researchers Through LLM-Assisted Content Analysis](https://arxiv.org/abs/2508.18872)

*Laurie Gale, Sebastian Mateos Nicolajsen*

**Main category:** cs.CL

**Keywords:** large language models, content analysis, computing education research

**Relevance Score:** 6

**TL;DR:** This paper proposes LLM-assisted content analysis (LACA) as a method for rigorous analysis of large qualitative datasets in computing education research, enabling more generalizable findings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Many computing education researchers struggle with conducting rigorous and generalizable research due to limited resources and capacity.

**Method:** LACA combines traditional content analysis with large language models to analyze large volumes of textual data efficiently.

**Key Contributions:**

	1. Introduction of LLM-assisted content analysis (LACA) method
	2. Demonstration of LACA using a computing education dataset
	3. Potential for improving research rigor and generalizability in computing education research

**Result:** The application of LACA to computing education datasets demonstrates its effectiveness in producing rigorous and reproducible results.

**Limitations:** 

**Conclusion:** LACA has the potential to enhance the quality and generalizability of research findings in computing education, advancing both research and practice in the discipline.

**Abstract:** Computing education research (CER) is often instigated by practitioners wanting to improve both their own and the wider discipline's teaching practice. However, the latter is often difficult as many researchers lack the colleagues, resources, or capacity to conduct research that is generalisable or rigorous enough to advance the discipline. As a result, research methods that enable sense-making with larger volumes of qualitative data, while not increasing the burden on the researcher, have significant potential within CER.   In this discussion paper, we propose such a method for conducting rigorous analysis on large volumes of textual data, namely a variation of LLM-assisted content analysis (LACA). This method combines content analysis with the use of large language models, empowering researchers to conduct larger-scale research which they would otherwise not be able to perform. Using a computing education dataset, we illustrate how LACA could be applied in a reproducible and rigorous manner. We believe this method has potential in CER, enabling more generalisable findings from a wider range of research. This, together with the development of similar methods, can help to advance both the practice and research quality of the CER discipline.

</details>


### [57] [Affective Polarization across European Parliaments](https://arxiv.org/abs/2508.18916)

*Bojan Evkoski, Igor Mozetič, Nikola Ljubešić, Petra Kralj Novak*

**Main category:** cs.CL

**Keywords:** affective polarization, political discourse, natural language processing, sentiment analysis

**Relevance Score:** 3

**TL;DR:** The study investigates affective polarization in European parliaments through automated sentiment analysis of parliamentary speeches, revealing consistent patterns of negativity towards opposing groups.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the dynamics of affective polarization in political discourse within European parliaments, using automated methods to analyze parliamentary speeches.

**Method:** Natural language processing techniques were applied to a comprehensive corpus of parliamentary speeches from six European countries to estimate sentiment and measure affective polarization.

**Key Contributions:**

	1. Automated analysis of affective polarization in parliamentary discourse
	2. Introduction of sentiment analysis to measure negativity in political speeches
	3. Identification of reciprocity as a mechanism in affective polarization

**Result:** The study found consistent affective polarization across all six parliaments, with levels of negativity in speeches reflecting an adversarial attitude towards opposing groups.

**Limitations:** 

**Conclusion:** The findings underscore the pervasive nature of affective polarization in political discourse, suggesting that reciprocity plays a significant role in shaping interactions between parliamentarians.

**Abstract:** Affective polarization, characterized by increased negativity and hostility towards opposing groups, has become a prominent feature of political discourse worldwide. Our study examines the presence of this type of polarization in a selection of European parliaments in a fully automated manner. Utilizing a comprehensive corpus of parliamentary speeches from the parliaments of six European countries, we employ natural language processing techniques to estimate parliamentarian sentiment. By comparing the levels of negativity conveyed in references to individuals from opposing groups versus one's own, we discover patterns of affectively polarized interactions. The findings demonstrate the existence of consistent affective polarization across all six European parliaments. Although activity correlates with negativity, there is no observed difference in affective polarization between less active and more active members of parliament. Finally, we show that reciprocity is a contributing mechanism in affective polarization between parliamentarians across all six parliaments.

</details>


### [58] [Diverse And Private Synthetic Datasets Generation for RAG evaluation: A multi-agent framework](https://arxiv.org/abs/2508.18929)

*Ilias Driouich, Hongliu Cao, Eoin Thomas*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, privacy preservation, synthetic QA datasets

**Relevance Score:** 9

**TL;DR:** This paper presents a multi-agent framework for generating synthetic QA datasets aimed at improving the evaluation of retrieval-augmented generation (RAG) systems, focusing on diversity and privacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation processes for RAG systems by ensuring that they are more representative of real-world constraints, especially in terms of privacy and diversity.

**Method:** The method involves three agents: a Diversity agent for maximizing semantic variability, a Privacy agent for masking sensitive information, and a QA curation agent for synthesizing diverse QA pairs.

**Key Contributions:**

	1. Development of a multi-agent framework for QA dataset generation
	2. Focus on semantic diversity and privacy preservation
	3. Demonstrated effectiveness of datasets in RAG evaluation

**Result:** Experiments show that the evaluation sets generated using this framework outperform traditional methods in terms of diversity and effectively mask sensitive data.

**Limitations:** 

**Conclusion:** The proposed framework provides a robust and ethically sound approach to RAG system evaluation, addressing privacy concerns and setting a foundation for future improvements in compliance with AI regulations.

**Abstract:** Retrieval-augmented generation (RAG) systems improve large language model outputs by incorporating external knowledge, enabling more informed and context-aware responses. However, the effectiveness and trustworthiness of these systems critically depends on how they are evaluated, particularly on whether the evaluation process captures real-world constraints like protecting sensitive information. While current evaluation efforts for RAG systems have primarily focused on the development of performance metrics, far less attention has been given to the design and quality of the underlying evaluation datasets, despite their pivotal role in enabling meaningful, reliable assessments. In this work, we introduce a novel multi-agent framework for generating synthetic QA datasets for RAG evaluation that prioritize semantic diversity and privacy preservation. Our approach involves: (1) a Diversity agent leveraging clustering techniques to maximize topical coverage and semantic variability, (2) a Privacy Agent that detects and mask sensitive information across multiple domains and (3) a QA curation agent that synthesizes private and diverse QA pairs suitable as ground truth for RAG evaluation. Extensive experiments demonstrate that our evaluation sets outperform baseline methods in diversity and achieve robust privacy masking on domain-specific datasets. This work offers a practical and ethically aligned pathway toward safer, more comprehensive RAG system evaluation, laying the foundation for future enhancements aligned with evolving AI regulations and compliance standards.

</details>


### [59] [Interpretable by AI Mother Tongue: Native Symbolic Reasoning in Neural Models](https://arxiv.org/abs/2508.18988)

*Hung Ming Liu*

**Main category:** cs.CL

**Keywords:** neural models, symbolic reasoning, interpretability

**Relevance Score:** 7

**TL;DR:** A framework for neural models to develop an AI Mother Tongue, enabling intuitive reasoning and interpretability through symbolic language.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To integrate reasoning into neural models in a more interpretable and intuitive manner than existing methods.

**Method:** The framework embeds reasoning into model representations using symbols for semantic patterns and gated induction for selective focus, supported by complementary training objectives.

**Key Contributions:**

	1. Development of a novel AI Mother Tongue framework for symbolism in neural models.
	2. Introduction of complementary training objectives for symbol purity and decision sparsity.
	3. Demonstrated effectiveness through experiments showcasing accuracy and reasoning transparency.

**Result:** Experiments show competitive accuracy in AI tasks and verifiable reasoning traces, indicating the effectiveness of the AI Mother Tongue in symbolic reasoning.

**Limitations:** 

**Conclusion:** The AI Mother Tongue framework provides a unified approach for enhancing interpretability, intuition, and symbolic reasoning in neural models.

**Abstract:** We present a framework where neural models develop an AI Mother Tongue, a native symbolic language that simultaneously supports intuitive reasoning, compositional symbol chains, and inherent interpretability. Unlike post-hoc explanation methods, our approach embeds reasoning directly into the model's representations: symbols capture meaningful semantic patterns, chains trace decision paths, and gated induction mechanisms guide selective focus, yielding transparent yet flexible reasoning. We introduce complementary training objectives to enhance symbol purity and decision sparsity, and employ a sequential specialization strategy to first build broad symbolic competence and then refine intuitive judgments. Experiments on AI tasks demonstrate competitive accuracy alongside verifiable reasoning traces, showing that AI Mother Tongue can serve as a unified mechanism for interpretability, intuition, and symbolic reasoning in neural models.

</details>


### [60] [Automatic Prompt Optimization with Prompt Distillation](https://arxiv.org/abs/2508.18992)

*Viktor N. Zhuravlev, Artur R. Khairullin, Ernest A. Dyagin, Alena N. Sitkina, Nikita I. Kulin*

**Main category:** cs.CL

**Keywords:** autoprompting, large language models, prompt engineering, text classification, text generation

**Relevance Score:** 9

**TL;DR:** DistillPrompt is a novel autoprompting method that optimizes prompts for language models, showing significant improvements in text classification and generation tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing interest in prompt engineering driven by research in large language models (LLMs), necessitating optimized prompt selection for improved performance.

**Method:** DistillPrompt employs a multi-stage integration of task-specific information into prompts through the use of distillation, compression, and aggregation operations to explore the prompt space.

**Key Contributions:**

	1. Introduction of a novel autoprompting method
	2. Significant improvement in prompt optimization for language models
	3. Demonstration of effectiveness across various datasets

**Result:** Tests showed an average performance improvement of 20.12% compared to existing methods such as Grips on various datasets for text classification and generation.

**Limitations:** 

**Conclusion:** DistillPrompt demonstrates effectiveness as a non-gradient approach to autoprompting, outperforming existing methods in key metrics.

**Abstract:** Autoprompting is the process of automatically selecting optimized prompts for language models, which is gaining popularity due to the rapid development of prompt engineering driven by extensive research in the field of large language models (LLMs). This paper presents DistillPrompt -- a novel autoprompting method based on large language models that employs a multi-stage integration of task-specific information into prompts using training data. DistillPrompt utilizes distillation, compression, and aggregation operations to explore the prompt space more thoroughly. The method was tested on different datasets for text classification and generation tasks using the t-lite-instruct-0.1 language model. The results demonstrate a significant average improvement (e.g., 20.12% across the entire dataset compared to Grips) in key metrics over existing methods in the field, establishing DistillPrompt as one of the most effective non-gradient approaches in autoprompting.

</details>


### [61] [MovieCORE: COgnitive REasoning in Movies](https://arxiv.org/abs/2508.19026)

*Gueter Josmy Faure, Min-Hung Chen, Jia-Fong Yeh, Ying Cheng, Hung-Ting Su, Yung-Hao Tang, Shang-Hong Lai, Winston H. Hsu*

**Main category:** cs.CL

**Keywords:** Video Question Answering, Large Language Models, Cognitive Understanding

**Relevance Score:** 6

**TL;DR:** MovieCORE is a novel video question answering dataset aimed at deeper cognitive understanding of movie content, utilizing large language models for question generation and evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of video question answering (VQA) systems by focusing on deeper cognitive processing rather than surface-level comprehension of movies.

**Method:** Introduces the MovieCORE dataset and employs large language models to generate and refine question-answer pairs, alongside a set of cognitive tests to evaluate dataset quality and model performance.

**Key Contributions:**

	1. Novel video question answering dataset designed for deeper cognitive engagement.
	2. Innovative usage of large language models as thought agents for question generation.
	3. Agentic Choice Enhancement module that significantly improves reasoning capabilities of VQA systems.

**Result:** The dataset allows for probing deeper cognitive understanding and the new Agentic Choice Enhancement (ACE) module improves reasoning capabilities of VQA models by up to 25%.

**Limitations:** 

**Conclusion:** MovieCORE advances movie understanding in AI and provides insights into the performance of VQA models on more nuanced cinematic questions.

**Abstract:** This paper introduces MovieCORE, a novel video question answering (VQA) dataset designed to probe deeper cognitive understanding of movie content. Unlike existing datasets that focus on surface-level comprehension, MovieCORE emphasizes questions that engage System-2 thinking while remaining specific to the video material. We present an innovative agentic brainstorming approach, utilizing multiple large language models (LLMs) as thought agents to generate and refine high-quality question-answer pairs. To evaluate dataset quality, we develop a set of cognitive tests assessing depth, thought-provocation potential, and syntactic complexity. We also propose a comprehensive evaluation scheme for assessing VQA model performance on deeper cognitive tasks. To address the limitations of existing video-language models (VLMs), we introduce an agentic enhancement module, Agentic Choice Enhancement (ACE), which improves model reasoning capabilities post-training by up to 25%. Our work contributes to advancing movie understanding in AI systems and provides valuable insights into the capabilities and limitations of current VQA models when faced with more challenging, nuanced questions about cinematic content. Our project page, dataset and code can be found at https://joslefaure.github.io/assets/html/moviecore.html.

</details>


### [62] [HiPlan: Hierarchical Planning for LLM-Based Agents with Adaptive Global-Local Guidance](https://arxiv.org/abs/2508.19076)

*Ziyue Li, Yuan Chang, Gaihong Yu, Xiaoqiu Le*

**Main category:** cs.CL

**Keywords:** Hierarchical Planning, LLMs, Decision-Making, Task Decomposition, Adaptive Guidance

**Relevance Score:** 9

**TL;DR:** HiPlan is a hierarchical planning framework designed to enhance decision-making in large language model agents by providing structured guidance and support for complex, long-horizon tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLM-based agents struggle with complex decision-making tasks due to a lack of guidance and oversight, leading to disorientation and execution failures.

**Method:** HiPlan decomposes tasks into milestone action guides and step-wise hints; it constructs a milestone library from expert demonstrations during the offline phase and adapts trajectory segments during execution to align with milestone objectives.

**Key Contributions:**

	1. Introduction of HiPlan framework for LLM decision-making
	2. Development of a milestone library from expert demonstrations
	3. Dynamic adaptation of past trajectory segments for real-time guidance

**Result:** HiPlan shows significant improvements over strong baselines in two challenging benchmarks, demonstrating enhanced decision-making capabilities of LLM agents.

**Limitations:** 

**Conclusion:** The hierarchical components of HiPlan offer complementary benefits, improving the overall performance of LLM-based agents in complex planning scenarios.

**Abstract:** Large language model (LLM)-based agents have demonstrated remarkable capabilities in decision-making tasks, but struggle significantly with complex, long-horizon planning scenarios. This arises from their lack of macroscopic guidance, causing disorientation and failures in complex tasks, as well as insufficient continuous oversight during execution, rendering them unresponsive to environmental changes and prone to deviations. To tackle these challenges, we introduce HiPlan, a hierarchical planning framework that provides adaptive global-local guidance to boost LLM-based agents'decision-making. HiPlan decomposes complex tasks into milestone action guides for general direction and step-wise hints for detailed actions. During the offline phase, we construct a milestone library from expert demonstrations, enabling structured experience reuse by retrieving semantically similar tasks and milestones. In the execution phase, trajectory segments from past milestones are dynamically adapted to generate step-wise hints that align current observations with the milestone objectives, bridging gaps and correcting deviations. Extensive experiments across two challenging benchmarks demonstrate that HiPlan substantially outperforms strong baselines, and ablation studies validate the complementary benefits of its hierarchical components.

</details>


### [63] ["Where does it hurt?" -- Dataset and Study on Physician Intent Trajectories in Doctor Patient Dialogues](https://arxiv.org/abs/2508.19077)

*Tom Röhr, Soumyadeep Roy, Fares Al Mohamad, Jens-Michalis Papaioannou, Wolfgang Nejdl, Felix Gers, Alexander Löser*

**Main category:** cs.CL

**Keywords:** physician intent, doctor-patient dialogue, medical classification, SOAP framework, dialogue summarization

**Relevance Score:** 9

**TL;DR:** This paper studies physician intent trajectories in doctor-patient dialogues using the Aci-bench dataset and develops a taxonomy based on the SOAP framework, contributing a large labeled dataset for intent classification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The primary goal in doctor-patient dialogues is to effectively gather information for diagnosis and treatment planning, highlighting a need to understand physician intents.

**Method:** A fine-grained taxonomy of physician intents was developed based on the SOAP framework, followed by labeling over 5000 dialogue turns in collaboration with medical experts from the Aci-bench dataset.

**Key Contributions:**

	1. First study of physician intent trajectories in dialogues
	2. Development of a taxonomy based on the SOAP framework
	3. Large labeled dataset for benchmarking medical intent classification

**Result:** Models showed high accuracy in understanding medical dialogues but struggled with transitions between SOAP categories; common trajectories in dialogue structures were identified.

**Limitations:** Models often fail to identify transitions between SOAP categories, indicating a gap in understanding dialogue transitions.

**Conclusion:** The study contributes significantly to medical dialogue systems and demonstrates the importance of intent filtering for dialogue summarization, improving performance.

**Abstract:** In a doctor-patient dialogue, the primary objective of physicians is to diagnose patients and propose a treatment plan. Medical doctors guide these conversations through targeted questioning to efficiently gather the information required to provide the best possible outcomes for patients. To the best of our knowledge, this is the first work that studies physician intent trajectories in doctor-patient dialogues. We use the `Ambient Clinical Intelligence Benchmark' (Aci-bench) dataset for our study. We collaborate with medical professionals to develop a fine-grained taxonomy of physician intents based on the SOAP framework (Subjective, Objective, Assessment, and Plan). We then conduct a large-scale annotation effort to label over 5000 doctor-patient turns with the help of a large number of medical experts recruited using Prolific, a popular crowd-sourcing platform. This large labeled dataset is an important resource contribution that we use for benchmarking the state-of-the-art generative and encoder models for medical intent classification tasks. Our findings show that our models understand the general structure of medical dialogues with high accuracy, but often fail to identify transitions between SOAP categories. We also report for the first time common trajectories in medical dialogue structures that provide valuable insights for designing `differential diagnosis' systems. Finally, we extensively study the impact of intent filtering for medical dialogue summarization and observe a significant boost in performance. We make the codes and data, including annotation guidelines, publicly available at https://github.com/DATEXIS/medical-intent-classification.

</details>


### [64] [It's All About In-Context Learning! Teaching Extremely Low-Resource Languages to LLMs](https://arxiv.org/abs/2508.19089)

*Yue Li, Zhixue Zhao, Carolina Scarton*

**Main category:** cs.CL

**Keywords:** low-resource languages, in-context learning, parameter-efficient fine-tuning, large language models, language alignment

**Relevance Score:** 7

**TL;DR:** This paper analyzes the effectiveness of LLMs in acquiring extremely low-resource languages through in-context learning versus parameter-efficient fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to support extremely low-resource languages, which are often written in rare scripts and lack sufficient training data for large language models (LLMs).

**Method:** Systematic evaluation of 20 under-represented languages across three state-of-the-art multilingual LLMs, comparing in-context learning and parameter-efficient fine-tuning methods.

**Key Contributions:**

	1. Comprehensive analysis of LLM adaptation for low-resource languages
	2. Evaluation of zero-shot in-context learning vs parameter-efficient fine-tuning
	3. Guidelines for LLM practitioners on handling low-resource languages

**Result:** Zero-shot in-context learning with language alignment shows high effectiveness for extremely low-resource languages, while parameter-efficient fine-tuning is limited in such contexts.

**Limitations:** The analysis may not cover all languages or scripts, and the effectiveness of the methods may vary across different contexts.

**Conclusion:** Guidelines for adapting LLMs to low-resource languages are provided, emphasizing the avoidance of fine-tuning on languages with unseen scripts.

**Abstract:** Extremely low-resource languages, especially those written in rare scripts, as shown in Figure 1, remain largely unsupported by large language models (LLMs). This is due in part to compounding factors such as the lack of training data. This paper delivers the first comprehensive analysis of whether LLMs can acquire such languages purely via in-context learning (ICL), with or without auxiliary alignment signals, and how these methods compare to parameter-efficient fine-tuning (PEFT). We systematically evaluate 20 under-represented languages across three state-of-the-art multilingual LLMs. Our findings highlight the limitation of PEFT when both language and its script are extremely under-represented by the LLM. In contrast, zero-shot ICL with language alignment is impressively effective on extremely low-resource languages, while few-shot ICL or PEFT is more beneficial for languages relatively better represented by LLMs. For LLM practitioners working on extremely low-resource languages, we summarise guidelines grounded by our results on adapting LLMs to low-resource languages, e.g., avoiding fine-tuning a multilingual model on languages of unseen scripts.

</details>


### [65] [Retrieval-Augmented Generation for Natural Language Art Provenance Searches in the Getty Provenance Index](https://arxiv.org/abs/2508.19093)

*Mathew Henrickson*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, art provenance, semantic retrieval, multilingual searches, cultural heritage

**Relevance Score:** 7

**TL;DR:** This paper introduces a Retrieval-Augmented Generation (RAG) framework for enhancing searches in art provenance studies, specifically utilizing the Getty Provenance Index.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenges faced in provenance research due to fragmented and multilingual archival data, which complicates the retrieval of ownership histories of artworks.

**Method:** The proposed framework facilitates natural-language and multilingual searches through semantic retrieval and contextual summarization, thereby minimizing reliance on structured metadata.

**Key Contributions:**

	1. Introduction of a RAG framework for art provenance studies
	2. Enabling multilingual searches through semantic retrieval
	3. Providing summarization of fragmented archival data

**Result:** The framework was evaluated using a sample of 10,000 records from the Getty Provenance Index, demonstrating its effective retrieval and summarization of auction records, providing a scalable solution for art market archive navigation.

**Limitations:** 

**Conclusion:** This RAG approach offers a valuable resource for historians and cultural heritage professionals by simplifying the search process in historically sensitive research contexts.

**Abstract:** This research presents a Retrieval-Augmented Generation (RAG) framework for art provenance studies, focusing on the Getty Provenance Index. Provenance research establishes the ownership history of artworks, which is essential for verifying authenticity, supporting restitution and legal claims, and understanding the cultural and historical context of art objects. The process is complicated by fragmented, multilingual archival data that hinders efficient retrieval. Current search portals require precise metadata, limiting exploratory searches. Our method enables natural-language and multilingual searches through semantic retrieval and contextual summarization, reducing dependence on metadata structures. We assess RAG's capability to retrieve and summarize auction records using a 10,000-record sample from the Getty Provenance Index - German Sales. The results show this approach provides a scalable solution for navigating art market archives, offering a practical tool for historians and cultural heritage professionals conducting historically sensitive research.

</details>


### [66] [Beyond the Black Box: Integrating Lexical and Semantic Methods in Quantitative Discourse Analysis with BERTopic](https://arxiv.org/abs/2508.19099)

*Thomas Compton*

**Main category:** cs.CL

**Keywords:** Quantitative Discourse Analysis, Large Language Models, Methodological Transparency

**Relevance Score:** 6

**TL;DR:** The paper presents a transparent hybrid framework for Quantitative Discourse Analysis (QDA) that integrates lexical and semantic methods, focusing on methodological transparency and researcher agency using custom Python pipelines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the methodological limitations of black box software in Quantitative Discourse Analysis and enhance transparency and reproducibility.

**Method:** The paper uses custom Python pipelines with NLTK, spaCy, and Sentence Transformers for preprocessing, lemmatisation, and embedding generation, and details a multi-layered iterative BERTopic modelling process employing UMAP, HDBSCAN, and c-TF-IDF keyword extraction, optimized through parameter tuning.

**Key Contributions:**

	1. Hybrid QDA framework combining lexical and semantic methods.
	2. Custom Python pipelines facilitating transparency in data processing.
	3. Iterative BERTopic modelling process with enhanced topic coherence.

**Result:** The proposed framework allows for fine-grained control over data processing, leading to improved topic coherence and the ability to triangulate lexical and semantic analyses.

**Limitations:** The paper does not explore the potential scalability issues of the proposed framework in larger datasets.

**Conclusion:** The authors emphasize the importance of transparency and researcher agency in computational discourse analysis through a multi-layered approach that enhances interpretability and reproducibility.

**Abstract:** Quantitative Discourse Analysis has seen growing adoption with the rise of Large Language Models and computational tools. However, reliance on black box software such as MAXQDA and NVivo risks undermining methodological transparency and alignment with research goals. This paper presents a hybrid, transparent framework for QDA that combines lexical and semantic methods to enable triangulation, reproducibility, and interpretability. Drawing from a case study in historical political discourse, we demonstrate how custom Python pipelines using NLTK, spaCy, and Sentence Transformers allow fine-grained control over preprocessing, lemmatisation, and embedding generation. We further detail our iterative BERTopic modelling process, incorporating UMAP dimensionality reduction, HDBSCAN clustering, and c-TF-IDF keyword extraction, optimised through parameter tuning and multiple runs to enhance topic coherence and coverage. By juxtaposing precise lexical searches with context-aware semantic clustering, we argue for a multi-layered approach that mitigates the limitations of either method in isolation. Our workflow underscores the importance of code-level transparency, researcher agency, and methodological triangulation in computational discourse studies. Code and supplementary materials are available via GitHub.

</details>


### [67] [Do LVLMs Know What They Know? A Systematic Study of Knowledge Boundary Perception in LVLMs](https://arxiv.org/abs/2508.19111)

*Zhikai Ding, Shiyu Ni, Keping Bi*

**Main category:** cs.CL

**Keywords:** vision-language models, confidence calibration, visual question answering, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper evaluates the perception of knowledge boundaries in large vision-language models and proposes methods to enhance their confidence estimation in visual question answering tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination issue in large vision-language models (LVLMs) and improve their ability to recognize their knowledge limitations when answering questions about images.

**Method:** The study evaluates three confidence signals in LVLMs: probabilistic confidence, answer consistency-based confidence, and verbalized confidence, and adapts several confidence calibration methods from large language models.

**Key Contributions:**

	1. Evaluation of three types of confidence signals in LVLMs
	2. Adaptation of confidence calibration methods from LLMs for LVLMs
	3. Comparison of LVLMs’ performance and confidence levels with LLMs.

**Result:** Experiments show LVLMs have a reasonable perception level regarding their knowledge boundaries, but there is substantial room for improvement. Probabilistic and consistency-based confidences are better indicators than verbalized confidence.

**Limitations:** LVLMs still have significant room for improvement in knowledge boundary perception despite reasonable performance levels.

**Conclusion:** Adapting confidence calibration methods from LLMs can enhance LVLMs' perception of their knowledge boundaries, leading to better performance in visual question answering tasks.

**Abstract:** Large vision-language models (LVLMs) demonstrate strong visual question answering (VQA) capabilities but are shown to hallucinate. A reliable model should perceive its knowledge boundaries-knowing what it knows and what it does not. This paper investigates LVLMs' perception of their knowledge boundaries by evaluating three types of confidence signals: probabilistic confidence, answer consistency-based confidence, and verbalized confidence. Experiments on three LVLMs across three VQA datasets show that, although LVLMs possess a reasonable perception level, there is substantial room for improvement. Among the three confidences, probabilistic and consistency-based signals are more reliable indicators, while verbalized confidence often leads to overconfidence. To enhance LVLMs' perception, we adapt several established confidence calibration methods from Large Language Models (LLMs) and propose three effective methods. Additionally, we compare LVLMs with their LLM counterparts, finding that jointly processing visual and textual inputs decreases question-answering performance but reduces confidence, resulting in an improved perception level compared to LLMs.

</details>


### [68] [Demystifying Scientific Problem-Solving in LLMs by Probing Knowledge and Reasoning](https://arxiv.org/abs/2508.19202)

*Alan Li, Yixin Liu, Arpan Sarkar, Doug Downey, Arman Cohan*

**Main category:** cs.CL

**Keywords:** scientific reasoning, language models, knowledge retrieval, reasoning frameworks, benchmarking

**Relevance Score:** 9

**TL;DR:** This paper introduces SciReas, a benchmark suite for scientific reasoning tasks in LLMs, along with an in-depth analysis through the KRUX framework that uncovers the relationship between knowledge retrieval and reasoning abilities.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of a holistic benchmark for evaluating scientific reasoning in LLMs and to understand the distinct roles of knowledge and reasoning in scientific problem solving.

**Method:** The study introduces SciReas and SciReas-Pro benchmarks for scientific reasoning and employs the KRUX framework to evaluate and analyze the performance of LLMs on these benchmarks.

**Key Contributions:**

	1. Introduction of holistic benchmarks for scientific reasoning in LLMs
	2. Development of the KRUX framework for analyzing knowledge and reasoning roles
	3. Release of SciLit01 as a strong baseline for scientific reasoning tasks.

**Result:** Key findings include that retrieving relevant knowledge is a major bottleneck for LLMs, and that external knowledge can significantly enhance reasoning models. Moreover, enhancing verbal reasoning improves LLMs' knowledge retrieval capabilities.

**Limitations:** 

**Conclusion:** The analysis provides actionable insights into improving LLMs for scientific reasoning tasks and highlights crucial areas for further research.

**Abstract:** Scientific problem solving poses unique challenges for LLMs, requiring both deep domain knowledge and the ability to apply such knowledge through complex reasoning. While automated scientific reasoners hold great promise for assisting human scientists, there is currently no widely adopted holistic benchmark for evaluating scientific reasoning, and few approaches systematically disentangle the distinct roles of knowledge and reasoning in these tasks. To address these gaps, we introduce SciReas, a diverse suite of existing benchmarks for scientific reasoning tasks, and SciReas-Pro, a selective subset that requires more complex reasoning. Our holistic evaluation surfaces insights about scientific reasoning performance that remain hidden when relying on individual benchmarks alone. We then propose KRUX, a probing framework for studying the distinct roles of reasoning and knowledge in scientific tasks. Combining the two, we conduct an in-depth analysis that yields several key findings: (1) Retrieving task-relevant knowledge from model parameters is a critical bottleneck for LLMs in scientific reasoning; (2) Reasoning models consistently benefit from external knowledge added in-context on top of the reasoning enhancement; (3) Enhancing verbalized reasoning improves LLMs' ability to surface task-relevant knowledge. Finally, we conduct a lightweight analysis, comparing our science-focused data composition with concurrent efforts on long CoT SFT, and release SciLit01, a strong 8B baseline for scientific reasoning.

</details>


### [69] [VibeVoice Technical Report](https://arxiv.org/abs/2508.19205)

*Zhiliang Peng, Jianwei Yu, Wenhui Wang, Yaoyao Chang, Yutao Sun, Li Dong, Yi Zhu, Weijiang Xu, Hangbo Bao, Zehua Wang, Shaohan Huang, Yan Xia, Furu Wei*

**Main category:** cs.CL

**Keywords:** speech synthesis, next-token diffusion, continuous speech tokenizer, data compression, multispeaker synthesis

**Relevance Score:** 6

**TL;DR:** VibeVoice is a novel model for synthesizing long-form speech with multiple speakers using next-token diffusion and an efficient continuous speech tokenizer.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To synthesize long-form speech with multiple speakers effectively and efficiently.

**Method:** Utilizes next-token diffusion for autoregressive generation of latent vectors, combined with a new continuous speech tokenizer for improved data compression.

**Key Contributions:**

	1. Introduction of a novel next-token diffusion model for speech synthesis
	2. Development of a continuous speech tokenizer that improves data compression by 80 times
	3. Ability to synthesize speech for 90 minutes with multiple speakers.

**Result:** VibeVoice synthesizes long-form speech for up to 90 minutes while achieving 80 times better data compression than the Encodec model, maintaining audio fidelity.

**Limitations:** 

**Conclusion:** VibeVoice surpasses both open-source and proprietary dialogue models in synthesizing authentic conversational vibes.

**Abstract:** This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models.

</details>


### [70] [Evaluating the Evaluators: Are readability metrics good measures of readability?](https://arxiv.org/abs/2508.19221)

*Isabel Cachola, Daniel Khashabi, Mark Dredze*

**Main category:** cs.CL

**Keywords:** Plain Language Summarization, readability evaluation, Language Models, human judgments, readability metrics

**Relevance Score:** 7

**TL;DR:** This paper surveys Plain Language Summarization (PLS) and evaluates traditional readability metrics against human judgments, finding that Language Models (LMs) provide better readability assessments for non-expert audiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of Plain Language Summarization (PLS) quality and accessibility for non-expert audiences by investigating readability metrics.

**Method:** A thorough survey of existing PLS literature, comparison of 8 traditional readability metrics with human judgments, and evaluation of Language Models for readability assessment.

**Key Contributions:**

	1. Identification of poor correlation between traditional metrics and human readability judgments.
	2. Demonstration of Language Models' superior performance in assessing readability.
	3. Recommendations for best practices in evaluating plain language summaries.

**Result:** Most traditional readability metrics correlate poorly with human judgments; Language Models outperform traditional metrics in evaluating readability of plain language summaries.

**Limitations:** The study is limited to the readability metrics tested; further research is needed to explore additional metrics and contexts.

**Conclusion:** Language Models are recommended over traditional metrics for assessing readability in PLS, and best practices for evaluation are proposed based on this study's findings.

**Abstract:** Plain Language Summarization (PLS) aims to distill complex documents into accessible summaries for non-expert audiences. In this paper, we conduct a thorough survey of PLS literature, and identify that the current standard practice for readability evaluation is to use traditional readability metrics, such as Flesch-Kincaid Grade Level (FKGL). However, despite proven utility in other fields, these metrics have not been compared to human readability judgments in PLS. We evaluate 8 readability metrics and show that most correlate poorly with human judgments, including the most popular metric, FKGL. We then show that Language Models (LMs) are better judges of readability, with the best-performing model achieving a Pearson correlation of 0.56 with human judgments. Extending our analysis to PLS datasets, which contain summaries aimed at non-expert audiences, we find that LMs better capture deeper measures of readability, such as required background knowledge, and lead to different conclusions than the traditional metrics. Based on these findings, we offer recommendations for best practices in the evaluation of plain language summaries. We release our analysis code and survey data.

</details>


### [71] [Generative Interfaces for Language Models](https://arxiv.org/abs/2508.19227)

*Jiaqi Chen, Yanzhe Zhang, Yutong Zhang, Yijia Shao, Diyi Yang*

**Main category:** cs.CL

**Keywords:** generative interfaces, human-AI interaction, large language models

**Relevance Score:** 9

**TL;DR:** This paper introduces Generative Interfaces for Language Models that create adaptive user interfaces to enhance multi-turn conversations and improve user experience compared to traditional chat-based systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of linear request-response formats in interactions with large language models, particularly in multi-turn and exploratory tasks.

**Method:** The framework utilizes structured interface-specific representations and iterative refinements to translate user queries into task-specific user interfaces for interaction.

**Key Contributions:**

	1. Introduction of Generative Interfaces for Language Models
	2. Development of a multidimensional assessment framework for comparing interaction systems
	3. Insights on user preference for generative interfaces over traditional chat-based interfaces

**Result:** Generative interfaces were shown to outperform traditional chat-based interfaces, with user preference favoring the new paradigm in over 70% of the cases.

**Limitations:** 

**Conclusion:** The study indicates that generative interfaces not only enhance user experience but also provide valuable insights into user preferences, paving the way for future advancements in human-AI interaction.

**Abstract:** Large language models (LLMs) are increasingly seen as assistants, copilots, and consultants, capable of supporting a wide range of tasks through natural conversation. However, most systems remain constrained by a linear request-response format that often makes interactions inefficient in multi-turn, information-dense, and exploratory tasks. To address these limitations, we propose Generative Interfaces for Language Models, a paradigm in which LLMs respond to user queries by proactively generating user interfaces (UIs) that enable more adaptive and interactive engagement. Our framework leverages structured interface-specific representations and iterative refinements to translate user queries into task-specific UIs. For systematic evaluation, we introduce a multidimensional assessment framework that compares generative interfaces with traditional chat-based ones across diverse tasks, interaction patterns, and query types, capturing functional, interactive, and emotional aspects of user experience. Results show that generative interfaces consistently outperform conversational ones, with humans preferring them in over 70% of cases. These findings clarify when and why users favor generative interfaces, paving the way for future advancements in human-AI interaction.

</details>


### [72] [HateDebias: On the Diversity and Variability of Hate Speech Debiasing](https://arxiv.org/abs/2406.04876)

*Hongyan Wu, Zhengming Chen, Zijian Li, Nankai Lin, Lianxi Wang, Shengyi Jiang, Aimin Yang*

**Main category:** cs.CL

**Keywords:** hate speech, bias, machine learning, fairness, continual learning

**Relevance Score:** 4

**TL;DR:** This paper introduces HateDebias, a benchmark for assessing hate speech detection models under diverse and evolving biases. It proposes a continual debiasing framework that incorporates memory replay to address performance degradation due to changing biases in real-world scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to effectively control hate speech on social media platforms and the ethical implications of bias in hate speech detection.

**Method:** The authors collected hate speech data reflecting various biases from real-world scenarios and created a dataset suitable for continuous learning. They evaluated detection accuracy and proposed a continual debiasing framework integrating memory replay and bias regularization.

**Key Contributions:**

	1. Introduction of the HateDebias benchmark for hate speech detection;
	2. Development of a continual debiasing framework to address dynamic biases;
	3. Demonstration of improved performance in real-world scenarios.

**Result:** Experiments revealed that the proposed methods improved performance in mitigating dynamic biases, highlighting their practicality for real-world applications.

**Limitations:** 

**Conclusion:** The HateDebias benchmark and the continual debiasing framework provide valuable tools for evaluating and addressing bias in hate speech detection models, contributing to fairer AI applications.

**Abstract:** Hate speech frequently appears on social media platforms and urgently needs to be effectively controlled. Alleviating the bias caused by hate speech can help resolve various ethical issues. Although existing research has constructed several datasets for hate speech detection, these datasets seldom consider the diversity and variability of bias, making them far from real-world scenarios. To fill this gap, we propose a benchmark HateDebias to analyze the fairness of models under dynamically evolving environments. Specifically, to meet the diversity of biases, we collect hate speech data with different types of biases from real-world scenarios. To further simulate the variability in the real-world scenarios(i.e., the changing of bias attributes in datasets), we construct a dataset to follow the continuous learning setting and evaluate the detection accuracy of models on the HateDebias, where performance degradation indicates a significant bias toward a specific attribute. To provide a potential direction, we further propose a continual debiasing framework tailored to dynamic bias in real-world scenarios, integrating memory replay and bias information regularization to ensure the fairness of the model. Experiment results on the HateDebias benchmark reveal that our methods achieve improved performance in mitigating dynamic biases in real-world scenarios, highlighting the practicality in real-world applications.

</details>


### [73] [ChatGPT Doesn't Trust Chargers Fans: Guardrail Sensitivity in Context](https://arxiv.org/abs/2407.06866)

*Victoria R. Li, Yida Chen, Naomi Saphra*

**Main category:** cs.CL

**Keywords:** language models, guardrails, bias, GPT-3.5, demographics

**Relevance Score:** 8

**TL;DR:** The paper examines biases in language model guardrails, revealing how user demographics affect LLM refusal rates for requests.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the often-neglected biases in guardrails of language models, particularly in how user demographics influence response tendencies.

**Method:** User biographies were generated with different ideological and demographic attributes to assess guardrail sensitivity in GPT-3.5.

**Key Contributions:**

	1. Identified bias in guardrail response based on user demographics
	2. Showed that sports fandom can influence guardrail behavior
	3. Demonstrated that guardrails infer user ideology based on contextual information

**Result:** Certain demographics, such as younger, female, and Asian-American users, experienced higher rates of refusal for specific requests; guardrails also reflected political biases based on inferred user ideology.

**Limitations:** 

**Conclusion:** LLM guardrails exhibit sensitivity to user identity, significantly impacting compliance with user requests based on demographic attributes and perceived political stance.

**Abstract:** While the biases of language models in production are extensively documented, the biases of their guardrails have been neglected. This paper studies how contextual information about the user influences the likelihood of an LLM to refuse to execute a request. By generating user biographies that offer ideological and demographic information, we find a number of biases in guardrail sensitivity on GPT-3.5. Younger, female, and Asian-American personas are more likely to trigger a refusal guardrail when requesting censored or illegal information. Guardrails are also sycophantic, refusing to comply with requests for a political position the user is likely to disagree with. We find that certain identity groups and seemingly innocuous information, e.g., sports fandom, can elicit changes in guardrail sensitivity similar to direct statements of political ideology. For each demographic category and even for American football team fandom, we find that ChatGPT appears to infer a likely political ideology and modify guardrail behavior accordingly.

</details>


### [74] [Recognizing Limits: Investigating Infeasibility in Large Language Models](https://arxiv.org/abs/2408.05873)

*Wenbo Zhang, Zihang Xu, Hengrui Cai*

**Main category:** cs.CL

**Keywords:** large language models, infeasible tasks, fine-tuning, evaluation dataset, real-world applications

**Relevance Score:** 9

**TL;DR:** This paper addresses how large language models (LLMs) can be improved to recognize and refuse infeasible tasks to reduce incorrect or fabricated responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper highlights the challenge of LLMs failing to handle queries that exceed their capabilities, leading to hallucinations and false information.

**Method:** Conceptualization of four categories of infeasible tasks; development of a new dataset for evaluation; benchmark testing on multiple LLMs and exploration of fine-tuning techniques.

**Key Contributions:**

	1. Identification of four main categories of infeasible tasks for LLMs.
	2. Development of a diverse dataset for benchmarking LLM capabilities.
	3. Evaluation of fine-tuning methods to increase refusal capabilities of LLMs.

**Result:** The experiments demonstrate that fine-tuning can effectively enhance LLMs' ability to decline infeasible tasks, showing promise for real-world application improvements.

**Limitations:** 

**Conclusion:** Improving LLMs’ refusal capabilities can significantly enhance their reliability and usability in practical contexts.

**Abstract:** Large language models (LLMs) have shown remarkable performance in various tasks but often fail to handle queries that exceed their knowledge and capabilities, leading to incorrect or fabricated responses. This paper addresses the need for LLMs to recognize and refuse infeasible tasks due to the requests surpassing their capabilities. We conceptualize four main categories of infeasible tasks for LLMs, which cover a broad spectrum of hallucination-related challenges identified in prior literature. We develop and benchmark a new dataset comprising diverse infeasible and feasible tasks to evaluate multiple LLMs' abilities to decline infeasible tasks. Furthermore, we explore the potential of increasing LLMs' refusal capabilities with fine-tuning. Our experiments validate the effectiveness of the trained models, suggesting promising directions for improving the performance of LLMs in real-world applications.

</details>


### [75] [Label Set Optimization via Activation Distribution Kurtosis for Zero-shot Classification with Generative Models](https://arxiv.org/abs/2410.19195)

*Yue Li, Zhixue Zhao, Carolina Scarton*

**Main category:** cs.CL

**Keywords:** In-context Learning, Zero-shot Classification, Label Optimization, Large Language Models, Kurtosis

**Relevance Score:** 9

**TL;DR:** This paper introduces LOADS, a method for optimizing label sets in zero-shot in-context learning (ICL) using large language models, focusing on the impact of label design on classification performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the underexplored area of how class label options affect zero-shot classification in in-context learning, emphasizing the significance of prompt design.

**Method:** LOADS uses kurtosis to measure neuron activation distributions for selecting optimal label sets, requiring only one forward pass without the need for labeled data.

**Key Contributions:**

	1. Introduction of LOADS for label set optimization
	2. Empirical analysis of the effects of label design on ICL
	3. Demonstrated performance gains across classification tasks and languages

**Result:** LOADS selected label words led to performance improvements in zero-shot ICL, with gains from 0.54 to 0.76 compared to conventional label words across various tasks and models.

**Limitations:** 

**Conclusion:** The findings highlight the importance of label design in zero-shot classification, demonstrating that proper label selection can enhance large language models' performance significantly.

**Abstract:** In-context learning (ICL) performance is highly sensitive to prompt design, yet the impact of class label options (e.g. lexicon or order) in zero-shot classification remains underexplored. This study proposes LOADS (Label set Optimization via Activation Distribution kurtosiS), a post-hoc method for selecting optimal label sets in zero-shot ICL with large language models (LLMs). LOADS is built upon the observations in our empirical analysis, the first to systematically examine how label option design (i.e., lexical choice, order, and elaboration) impacts classification performance. This analysis shows that the lexical choice of the labels in the prompt (such as agree vs. support in stance classification) plays an important role in both model performance and model's sensitivity to the label order. A further investigation demonstrates that optimal label words tend to activate fewer outlier neurons in LLMs' feed-forward networks. LOADS then leverages kurtosis to measure the neuron activation distribution for label selection, requiring only a single forward pass without gradient propagation or labelled data. The LOADS-selected label words consistently demonstrate effectiveness for zero-shot ICL across classification tasks, datasets, models and languages, achieving maximum performance gain from 0.54 to 0.76 compared to the conventional approach of using original dataset label words.

</details>


### [76] [From Intents to Conversations: Generating Intent-Driven Dialogues with Contrastive Learning for Multi-Turn Classification](https://arxiv.org/abs/2411.14252)

*Junhua Liu, Yong Keat Tan, Bin Fu, Kwan Hui Lim*

**Main category:** cs.CL

**Keywords:** Conversational AI, Intent Classification, Dialogue Generation, Multilingual Datasets, Hidden Markov Models

**Relevance Score:** 8

**TL;DR:** Introduces Chain-of-Intent, a framework combining HMMs with LLMs for generating multilingual dialogue datasets and enhances multi-turn intent classification through MINT-CL.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating large-scale, domain-specific, multilingual dialogue datasets for multi-turn intent classification in conversational AI systems.

**Method:** The proposed framework utilizes Hidden Markov Models (HMMs) in conjunction with Large Language Models (LLMs) to generate context-aware dialogues, guided by intent transition patterns extracted from real-world e-commerce chat logs.

**Key Contributions:**

	1. Development of Chain-of-Intent framework for dialogue generation
	2. Introduction of MINT-CL for multi-task contrastive learning
	3. Release of MINT-E multilingual dialogue corpus

**Result:** The approach shows improved performance in dialogue generation quality and classification accuracy over competitive baselines, particularly in multilingual contexts. A new multilingual dialogue corpus, MINT-E, is also proposed for future research.

**Limitations:** 

**Conclusion:** The combination of HMMs and LLMs effectively generates natural dialogues, while the MINT-CL framework reduces reliance on large annotated datasets, enhancing intent classification tasks.

**Abstract:** In conversational AI systems, a critical challenge in training effective multi-turn intent classification models lies in the generation of large-scale, domain-specific, multilingual dialogue datasets. In this paper, we introduce Chain-of-Intent, a novel framework that integrates Hidden Markov Models (HMMs) with Large Language Models (LLMs) to generate intent-driven, context-aware dialogues through self-play. Our method first extracts domain-specific intent transition patterns from real-world e-commerce chat logs, which guide the modeling of turn-level dynamics and intent sequences. LLMs are then employed to parameterize the emission probabilities of HMMs, enabling the generation of natural, coherent utterances aligned with predicted intents and dialogue context. We further propose MINT-CL, a multi-task contrastive learning framework for multi-turn intent classification, which improves performance while reducing dependence on large-scale annotated datasets. Empirical results demonstrate that our approach outperforms competitive baselines in both dialogue generation quality and classification accuracy, particularly in multilingual settings. To facilitate future research, we release MINT-E, a comprehensive, multilingual, intent-aware multi-turn dialogue corpus derived from the e-commerce domain. The reproduced source code and dataset are available at https://github.com/junhua/chain-of-intent.

</details>


### [77] [Adapting Large Language Models to Log Analysis with Interpretable Domain Knowledge](https://arxiv.org/abs/2412.01377)

*Yuhe Ji, Yilun Liu, Feiyu Yao, Minggui He, Shimin Tao, Xiaofeng Zhao, Su Chang, Xinhua Yang, Weibin Meng, Yuming Xie, Boxing Chen, Shenglin Zhang, Yongqian Sun*

**Main category:** cs.CL

**Keywords:** Log Analysis, Domain Adaptation, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper presents NLPLog, a dataset and SuperLog, a model that integrate domain knowledge into LLMs to enhance log analysis performance by reducing discrepancies between natural and log language representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of LLMs in log analysis and bridge the domain gap between natural language and domain-specific log language, enhancing performance in automatic fault and error management.

**Method:** The paper proposes a domain adaptation approach through continual pre-training of LLMs on interpretable natural texts containing log knowledge, supported by the development of the NLPLog dataset with over 250,000 question-answer pairs related to logs.

**Key Contributions:**

	1. Introduction of NLPLog, a new dataset for log-related knowledge
	2. Development of SuperLog, a model that outperforms existing solutions
	3. Demonstration of the effectiveness of domain adaptation using interpretable knowledge

**Result:** SuperLog, the resulting model, demonstrates significant performance improvements, achieving an average accuracy increase of 12.01% over the second-best model in four log analysis tasks.

**Limitations:** 

**Conclusion:** The integration of interpretable log knowledge via continual pre-training effectively enhances the performance of LLMs in log analysis, indicating the value of domain adaptation in AI applications.

**Abstract:** Log analysis represents a critical sub-domain within AI applications that facilitates automatic approaches to fault and error management of large-scaled software systems, saving labors of traditional manual methods. While existing solutions using large language models (LLMs) show promise, they are limited by a significant domain gap between natural and log languages (the latter contains rich domain-specific tokens such as status codes, IP addresses, resource pathes), which restricts their effectiveness in real-world applications. However, directly adapting general-purpose LLMs to log analysis using raw logs may degrade their performance due to inconsistent token distribution. In this paper, we present a domain adaptation approach that addresses these limitations by integrating interpretable domain knowledge into open-source LLMs through continual pre-training (CPT), which bridges this domain gap by adapting LLMs on interpretable natural texts with log knowledge (instead of raw logs) to reduce distribution discrepancy. To achieve this, we developed NLPLog, a comprehensive dataset containing over 250,000 question-answer pairs on log-related knowledge. Our resulting model, SuperLog, achieves the best performance across four log analysis tasks, with an average accuracy improvement of 12.01% over the second-best model. Ablation study also suggests advantages of domain adaption using interpretable log knowledge over using raw logs.

</details>


### [78] [TL-Training: A Task-Feature-Based Framework for Training Large Language Models in Tool Use](https://arxiv.org/abs/2412.15495)

*Junjie Ye, Yilong Wu, Sixian Li, Yuming Yang, Zhiheng Xi, Tao Gui, Qi Zhang, Xuanjing Huang, Peng Wang, Zhongchao Shi, Jianping Fan, Zhengyin Du*

**Main category:** cs.CL

**Keywords:** large language models, tool use, supervised fine-tuning, NLU, proximal policy optimization

**Relevance Score:** 9

**TL;DR:** The paper introduces TL-Training, a task-feature-based framework that optimizes fine-tuning for large language models (LLMs) to enhance tool-use performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve LLM performance in tool use, addressing the limitations of standard supervised fine-tuning (SFT) which often ignores task-specific characteristics.

**Method:** The authors analyze existing LLMs and propose TL-Training, which dynamically adjusts token importance and incorporates a reward mechanism tailored to specific error categories, optimized using proximal policy optimization.

**Key Contributions:**

	1. Development of TL-Training framework to improve tool-use in LLMs
	2. Dynamic adjustment of token weights during training
	3. Incorporation of a robust reward mechanism tailored to common tool-use errors

**Result:** TL-Training was validated on CodeLLaMA-2-7B, showing that it matches or surpasses existing LLMs in tool-use performance using a minimal amount of training data (1,217 points).

**Limitations:** 

**Conclusion:** The proposed method improves both robustness in noisy environments and general task performance, presenting a scalable approach for LLM training in tool usage.

**Abstract:** Large language models (LLMs) achieve remarkable advancements by leveraging tools to interact with environments, a critical step toward generalized AI. However, the standard supervised fine-tuning (SFT) approach, which relies on large-scale datasets, often overlooks task-specific characteristics in tool use, leading to performance bottlenecks. To address this issue, we analyze three existing LLMs and uncover key insights: training data can inadvertently impede tool-use behavior, token importance is distributed unevenly, and errors in tool calls fall into a small set of categories. Building on these findings, we propose~\emph{TL-Training}, a task-feature-based framework that mitigates the effects of suboptimal training data, dynamically adjusts token weights to prioritize key tokens during SFT, and incorporates a robust reward mechanism tailored to error categories, optimized through proximal policy optimization. We validate TL-Training by training CodeLLaMA-2-7B and evaluating it on four open-source test sets. Our results demonstrate that the LLM trained by our method matches or surpasses both open- and closed-source LLMs in tool-use performance using only 1,217 training data points. Additionally, our method enhances robustness in noisy environments and improves general task performance, offering a scalable and efficient paradigm for tool-use training in LLMs. Code and data are available at https://github.com/Junjie-Ye/TL-Training.

</details>


### [79] [Large Language Models Badly Generalize across Option Length, Problem Types, and Irrelevant Noun Replacements](https://arxiv.org/abs/2502.12459)

*Guangxiang Zhao, Saier Hu, Xiaoqi Jian, Jinzhu Wu, Yuhan Wu, Change Jia, Lin Sun, Xiangzheng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, generalization, bias, perturbations, evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces a Generalization Stress Test for evaluating LLMs' performance under minor controlled perturbations, revealing significant accuracy drops and biases despite high benchmark scores.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the generalization ability of Large Language Models in light of their observed performance across various tasks.

**Method:** A Generalization Stress Test is proposed, which includes perturbations such as option length, problem types, and irrelevant noun replacements to evaluate LLMs' responses.

**Key Contributions:**

	1. Introduction of a Generalization Stress Test for evaluating LLMs.
	2. Discovery of significant biases and accuracy drops in LLMs under slight perturbations.
	3. Insight into LLMs' superficial cue reliance rather than robust understanding.

**Result:** LLMs show severe accuracy drops indicating reliance on superficial cues rather than robust representations, with examples showing significant score fluctuations under different perturbations.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs lack strong generalization capabilities as they react poorly to slight content-preserving changes, indicating a need for improved model designs.

**Abstract:** In this paper, we propose a ``Generalization Stress Test" to assess Large Language Models' (LLMs) generalization ability under slight and controlled perturbations, including option length, problem types, and irrelevant noun replacements. We achieve novel and significant findings that, despite high benchmark scores, LLMs exhibit severe accuracy drops and unexpected biases (e.g., preference for longer distractors) when faced with these minor but content-preserving modifications. For example, Qwen 2.5 1.5B's MMLU score rises from 60 to 89 and drops from 89 to 36 when option lengths are changed without altering the question. Even GPT4o experiences a 25-point accuracy loss when problem types are changed, with a 6-point drop across all three modification categories. These analyses suggest that LLMs rely heavily on superficial cues rather than forming robust, abstract representations that generalize across formats, lexical variations, and irrelevant content shifts.

</details>


### [80] [Collaborative Evaluation of Deepfake Text with Deliberation-Enhancing Dialogue Systems](https://arxiv.org/abs/2503.04945)

*Jooyoung Lee, Xiaochen Zhu, Georgi Karadzhov, Tom Stafford, Andreas Vlachos, Dongwon Lee*

**Main category:** cs.CL

**Keywords:** deepfake detection, collaborative problem solving, deliberation-enhancing chatbot

**Relevance Score:** 7

**TL;DR:** This study investigates DeepFakeDeLiBot, a chatbot designed to aid group collaboration in detecting deepfake text, showing improved accuracy in group efforts compared to individual tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of generative models has made it challenging to differentiate between authentic and deepfake content, necessitating effective collaborative solutions.

**Method:** The effectiveness of DeepFakeDeLiBot was evaluated in group settings to detect deepfake text, focusing on metrics such as engagement, consensus building, and diversity in reasoning.

**Key Contributions:**

	1. Introduction of DeepFakeDeLiBot for deepfake text detection
	2. Demonstration of improved accuracy in group-based problem solving
	3. Analysis of group dynamics and effectiveness in collaboration

**Result:** Group-based problem-solving improved detection accuracy for machine-generated text, with benefits seen particularly in groups valuing collaboration.

**Limitations:** Engagement with DeepFakeDeLiBot did not lead to significant performance gains overall.

**Conclusion:** Deliberation-enhancing chatbots like DeepFakeDeLiBot can improve group dynamics and interaction, contributing to more effective detection of deepfake text.

**Abstract:** The proliferation of generative models has presented significant challenges in distinguishing authentic human-authored content from deepfake content. Collaborative human efforts, augmented by AI tools, present a promising solution. In this study, we explore the potential of DeepFakeDeLiBot, a deliberation-enhancing chatbot, to support groups in detecting deepfake text. Our findings reveal that group-based problem-solving significantly improves the accuracy of identifying machine-generated paragraphs compared to individual efforts. While engagement with DeepFakeDeLiBot does not yield substantial performance gains overall, it enhances group dynamics by fostering greater participant engagement, consensus building, and the frequency and diversity of reasoning-based utterances. Additionally, participants with higher perceived effectiveness of group collaboration exhibited performance benefits from DeepFakeDeLiBot. These findings underscore the potential of deliberative chatbots in fostering interactive and productive group dynamics while ensuring accuracy in collaborative deepfake text detection. \textit{Dataset and source code used in this study will be made publicly available upon acceptance of the manuscript.

</details>


### [81] [SmartBench: Is Your LLM Truly a Good Chinese Smartphone Assistant?](https://arxiv.org/abs/2503.06029)

*Xudong Lu, Haohao Gao, Renshou Wu, Shuai Ren, Xiaoxin Chen, Hongsheng Li, Fangyuan Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, HCI, Mobile Computing, Evaluation Benchmarks, Chinese

**Relevance Score:** 8

**TL;DR:** Introduction of SmartBench, a benchmark for evaluating on-device LLMs in Chinese mobile contexts.

**Read time:** 26 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation benchmarks for LLMs do not reflect real-world applications, particularly for Chinese-speaking users using mobile devices.

**Method:** Developed SmartBench, focusing on five categories of tasks related to on-device LLM functionalities and creating datasets with question-answer pairs for evaluation.

**Key Contributions:**

	1. First benchmark for evaluating on-device LLMs in Chinese mobile contexts
	2. Detailed analysis of smartphone manufacturer functionalities
	3. Automated evaluation criteria for practical mobile tasks

**Result:** SmartBench enables comprehensive evaluation of on-device LLMs in Chinese contexts, revealing performance insights post-quantized deployment on smartphone NPUs.

**Limitations:** Focus exclusively on Chinese language and mobile contexts, limiting broader applicability.

**Conclusion:** SmartBench provides a standardized framework for evaluating on-device LLMs, promoting future advancements in this domain.

**Abstract:** Large Language Models (LLMs) have become integral to daily life, especially advancing as intelligent assistants through on-device deployment on smartphones. However, existing LLM evaluation benchmarks predominantly focus on objective tasks like mathematics and coding in English, which do not necessarily reflect the practical use cases of on-device LLMs in real-world mobile scenarios, especially for Chinese users. To address these gaps, we introduce SmartBench, the first benchmark designed to evaluate the capabilities of on-device LLMs in Chinese mobile contexts. We analyze functionalities provided by representative smartphone manufacturers and divide them into five categories: text summarization, text Q&A, information extraction, content creation, and notification management, further detailed into 20 specific tasks. For each task, we construct high-quality datasets comprising 50 to 200 question-answer pairs that reflect everyday mobile interactions, and we develop automated evaluation criteria tailored for these tasks. We conduct comprehensive evaluations of on-device LLMs and MLLMs using SmartBench and also assess their performance after quantized deployment on real smartphone NPUs. Our contributions provide a standardized framework for evaluating on-device LLMs in Chinese, promoting further development and optimization in this critical area. Code and data will be available at https://github.com/vivo-ai-lab/SmartBench.

</details>


### [82] [Accelerate Parallelizable Reasoning via Parallel Decoding within One Sequence](https://arxiv.org/abs/2503.20533)

*Yijiong Yu*

**Main category:** cs.CL

**Keywords:** reasoning models, parallel processing, decoding speed, tree-like attention mask, computational efficiency

**Relevance Score:** 6

**TL;DR:** This paper presents a method for accelerating reasoning models by employing parallel processing to improve decoding speed while maintaining answer quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the computational inefficiency of generating lengthy reasoning sequences in reasoning models, which hinders their practicality.

**Method:** The authors propose a method that utilizes the parallelizability of specific tasks by decoding multiple tokens per forward pass with a tree-like attention mask, thus reducing memory usage.

**Key Contributions:**

	1. Introduction of a tree-like attention mask for parallel decoding
	2. Demonstration of nearly 100% speedup in decoding processes
	3. Maintaining answer quality while improving efficiency

**Result:** Experimental results indicate that the proposed method achieves nearly 100% speedup in decoding processes while preserving the quality of answers.

**Limitations:** 

**Conclusion:** The proposed approach significantly enhances the efficiency of reasoning models by leveraging parallel processing, making them more practical for real-world applications.

**Abstract:** Recent advances in reasoning models have demonstrated significant improvements in accuracy by employing detailed and comprehensive reasoning processes. However, generating these lengthy reasoning sequences is computationally expensive and time-consuming. To address this inefficiency, we leverage the inherent parallelizability of certain tasks to accelerate the reasoning process. Specifically, when multiple parallel reasoning steps exist, we decode multiple tokens per forward pass via a tree-like attention mask within a single sequence, avoiding additional memory usage. Experimental results show that our method achieves up to nearly 100\% speedup in decoding while basically maintaining the answer quality.

</details>


### [83] [An Agentic System for Rare Disease Diagnosis with Traceable Reasoning](https://arxiv.org/abs/2506.20430)

*Weike Zhao, Chaoyi Wu, Yanjie Fan, Xiaoman Zhang, Pengcheng Qiu, Yuze Sun, Xiao Zhou, Yanfeng Wang, Xin Sun, Ya Zhang, Yongguo Yu, Kun Sun, Weidi Xie*

**Main category:** cs.CL

**Keywords:** rare diseases, diagnosis, Large Language Models, DeepRare, health informatics

**Relevance Score:** 10

**TL;DR:** DeepRare is the first LLM-powered diagnostic system for rare diseases, generating ranked hypotheses with transparent reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Timely diagnosis of rare diseases is challenging due to clinical heterogeneity and limited clinician familiarity.

**Method:** DeepRare includes a host with long-term memory, specialized agent servers for analytical tasks, and integrates over 40 tools and up-to-date medical knowledge.

**Key Contributions:**

	1. First LLM-powered system for rare disease diagnosis
	2. Achieved 100% accuracy for a subset of diseases
	3. Integrates multiple analytical tools and knowledge sources

**Result:** DeepRare achieved 100% accuracy for 1,013 diseases, significantly outperformed other methods with an average Recall@1 of 57.18%, and was validated by clinical experts with 95.40% agreement.

**Limitations:** 

**Conclusion:** DeepRare provides a modular, scalable, and user-friendly solution for rare disease diagnosis, outperforming existing methods and ensuring reasoning transparency.

**Abstract:** Rare diseases collectively affect over 300 million individuals worldwide, yet timely and accurate diagnosis remains a pervasive challenge. This is largely due to their clinical heterogeneity, low individual prevalence, and the limited familiarity most clinicians have with rare conditions. Here, we introduce DeepRare, the first rare disease diagnosis agentic system powered by a large language model (LLM), capable of processing heterogeneous clinical inputs. The system generates ranked diagnostic hypotheses for rare diseases, each accompanied by a transparent chain of reasoning that links intermediate analytic steps to verifiable medical evidence.   DeepRare comprises three key components: a central host with a long-term memory module; specialized agent servers responsible for domain-specific analytical tasks integrating over 40 specialized tools and web-scale, up-to-date medical knowledge sources, ensuring access to the most current clinical information. This modular and scalable design enables complex diagnostic reasoning while maintaining traceability and adaptability. We evaluate DeepRare on eight datasets. The system demonstrates exceptional diagnostic performance among 2,919 diseases, achieving 100% accuracy for 1013 diseases. In HPO-based evaluations, DeepRare significantly outperforms other 15 methods, like traditional bioinformatics diagnostic tools, LLMs, and other agentic systems, achieving an average Recall@1 score of 57.18% and surpassing the second-best method (Reasoning LLM) by a substantial margin of 23.79 percentage points. For multi-modal input scenarios, DeepRare achieves 70.60% at Recall@1 compared to Exomiser's 53.20% in 109 cases. Manual verification of reasoning chains by clinical experts achieves 95.40% agreements. Furthermore, the DeepRare system has been implemented as a user-friendly web application http://raredx.cn/doctor.

</details>


### [84] [DLLMQuant: Quantizing Diffusion-based Large Language Models](https://arxiv.org/abs/2508.14090)

*Chen Xu, Dawei Yang*

**Main category:** cs.CL

**Keywords:** Diffusion-based models, Post-training quantization, Large language models, Temporal-Mask Adaptive Sampling, Interaction-Aware Activation Quantization

**Relevance Score:** 8

**TL;DR:** This paper presents DLLMQuant, a post-training quantization framework specifically designed for diffusion-based large language models to improve efficiency without severe accuracy loss.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of diffusion-based large language models is hindered by their large sizes and computation costs, and existing post-training quantization methods result in significant accuracy degradation.

**Method:** The paper proposes DLLMQuant, which includes three novel techniques: Temporal-Mask Adaptive Sampling for capturing distributions across decoding steps, Interaction-Aware Activation Quantization for dynamically allocating quantization resources, and Certainty-Guided Quantization for effective error compensation.

**Key Contributions:**

	1. Introduction of DLLMQuant framework for quantizing diffusion-based LLMs.
	2. Development of Temporal-Mask Adaptive Sampling for better distribution capture.
	3. Implementation of Interaction-Aware Activation Quantization to optimize resource allocation.

**Result:** DLLMQuant significantly improves performance and efficiency when quantizing diffusion-based large language models compared to traditional methods.

**Limitations:** 

**Conclusion:** The proposed techniques effectively address the unique challenges posed by DLLMs in the quantization process, leading to enhanced model performance.

**Abstract:** Diffusion-based large language models (DLLMs) have shown promise for non-autoregressive text generation, but their deployment is constrained by large model sizes and heavy computational costs. Post-training quantization (PTQ), a widely used method for compressing and accelerating Large Language Models (LLMs), suffers from severe accuracy degradation and reduced generalization performance when directly applied to DLLMs (e.g., AWQ suffers a 16% accuracy drop on LLADA under W4A4). This paper explores how DLLMs' key mechanisms - dynamic masking, iterative generation, bidirectional attention - clash with quantization. We identify three core issues: 1) Iterative generation and dynamic masking ratios lead to distinct token distributions across decoding steps, which are not adequately captured by existing PTQ calibration methods; 2) Quantization errors are accumulated and amplified progressively during iteration in DLLMs, causing quantized models to perform worse as decoding steps progress; 3) Unmasked tokens stabilize while masked remain probabilistic, making overall feature distribution incompatible with existing PTQ methods. To address these issues, we propose DLLMQuant, a PTQ framework tailored for DLLMs, which incorporates three novel techniques: 1) Temporal-Mask Adaptive Sampling (TMAS), a calibration method that accounts for both time and mask factors, with the capacity to capture distributions across timesteps. 2) Interaction-Aware Activation Quantization (IA-AQ), which utilizes bidirectional attention's interaction signals to dynamically allocate quantization resources. 3) Certainty-Guided Quantization (CGQ), which integrates mask status and token scores as key weighting criteria into error compensation, making weight quantization more suitable for DLLMs. Experiments show that DLLMQuant achieves significant performance gains while enhancing efficiency.

</details>
