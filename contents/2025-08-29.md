# 2025-08-29

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 9]

- [cs.CL](#cs.CL) [Total: 58]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Athena: Intermediate Representations for Iterative Scaffolded App Generation with an LLM](https://arxiv.org/abs/2508.20263)

*Jazbo Beason, Ruijia Cheng, Eldon Schoop, Jeffrey Nichols*

**Main category:** cs.HC

**Keywords:** Large Language Models, User Interface Generation, Prototype Development, Human-Computer Interaction, App Storyboard

**Relevance Score:** 9

**TL;DR:** Athena is a prototype application generation tool that helps developers create user interfaces using LLMs, employing intermediate representations to structure code generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generating complete user interfaces with LLMs is complex due to the intricacy of UI components and their interconnected files.

**Method:** The paper presents Athena, which uses shared intermediate representations like app storyboards, data models, and GUI skeletons to facilitate iterative UI development with LLMs.

**Key Contributions:**

	1. Introduction of shared intermediate representations for UI generation
	2. Demonstration of iterative development with LLMs
	3. User study validating Athena's effectiveness over traditional methods

**Result:** A user study showed that 75% of participants preferred Athena over a traditional chatbot-style approach for app prototyping.

**Limitations:** The study's sample size and scope may limit generalizability.

**Conclusion:** Athena can enhance the code generation process by producing organized and structured output while reducing errors.

**Abstract:** It is challenging to generate the code for a complete user interface using a Large Language Model (LLM). User interfaces are complex and their implementations often consist of multiple, inter-related files that together specify the contents of each screen, the navigation flows between the screens, and the data model used throughout the application. It is challenging to craft a single prompt for an LLM that contains enough detail to generate a complete user interface, and even then the result is frequently a single large and difficult to understand file that contains all of the generated screens. In this paper, we introduce Athena, a prototype application generation environment that demonstrates how the use of shared intermediate representations, including an app storyboard, data model, and GUI skeletons, can help a developer work with an LLM in an iterative fashion to craft a complete user interface. These intermediate representations also scaffold the LLM's code generation process, producing organized and structured code in multiple files while limiting errors. We evaluated Athena with a user study that found 75% of participants preferred our prototype over a typical chatbot-style baseline for prototyping apps.

</details>


### [2] [Identifying Framing Practices in Visualization Design Through Practitioner Reflections](https://arxiv.org/abs/2508.20383)

*Prakash Shukla, Paul Parsons*

**Main category:** cs.HC

**Keywords:** Framing, Visualization Design, Design Process, Strategic Judgment, Narrative Development

**Relevance Score:** 4

**TL;DR:** This study examines the role of framing in the design process of visualization, identifying it as a critical component that influences problem scoping, data interpretation, and narrative development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the underexamined role of framing in the design process of visualization, beyond its effects on audience perception.

**Method:** Analysis of podcasts and book chapters from over 80 professional visualization designers reflecting on their design practices.

**Key Contributions:**

	1. Identifies framing as a critical aspect of the design process in visualization.
	2. Explores conditions that trigger reframing in design.
	3. Outlines strategies used by practitioners to manage uncertainty.

**Result:** Framing is identified as a pervasive and iterative activity in visualization design, affecting various stages such as problem definition and stakeholder alignment.

**Limitations:** 

**Conclusion:** Framing is a core dimension of visualization practice that needs more focus in research and education to enhance practitioners' strategic judgment.

**Abstract:** Framing -- how designers define and reinterpret problems, shape narratives, and guide audience understanding -- is central to design practice. Yet in visualization research, framing has been examined mostly through its rhetorical and perceptual effects on audiences, leaving its role in the design process underexplored. This study addresses that gap by analyzing publicly available podcasts and book chapters in which over 80 professional visualization designers reflect on their work. We find that framing is a pervasive, iterative activity, evident in scoping problems, interpreting data, aligning with stakeholder goals, and shaping narrative direction. Our analysis identifies the conditions that trigger reframing and the strategies practitioners use to navigate uncertainty and guide design. These findings position framing as a core dimension of visualization practice and underscore the need for research and education to support the interpretive and strategic judgment that practitioners exercise throughout the design process.

</details>


### [3] [Human-Centered Design for Connected Automation: Predicting Pedestrian Crossing Intentions](https://arxiv.org/abs/2508.20464)

*Sanaz Motamedi, Viktoria Marcus, Griffin Pitts*

**Main category:** cs.HC

**Keywords:** automated driving systems, pedestrian behavior, human-machine interaction, traffic safety, Theory of Planned Behavior

**Relevance Score:** 8

**TL;DR:** This study explores pedestrian decision-making in road-crossing scenarios with level-5 automated driving systems (ADSs), highlighting the impact of external factors on their behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high fatality rates in road traffic due to human error and improve interactions between pedestrians and level-5 ADSs.

**Method:** The study extends the Theory of Planned Behavior (TPB) by adding four external factors (safety, trust, compatibility, understanding) and analyzes data from an online survey with 212 participants.

**Key Contributions:**

	1. Extended TPB to include external factors relevant to ADSs
	2. Identified key predictors of pedestrian crossing intentions
	3. Provided insights for designing eHMIs and V2X strategies

**Result:** Data analysis reveals that perceived behavioral control, attitude, and social information predict crossing intentions, with safety and understanding being significant influencers.

**Limitations:** 

**Conclusion:** The findings suggest the importance of designing effective human-machine interfaces and communication strategies for safer interactions between automated vehicles and pedestrians.

**Abstract:** Road traffic remains a leading cause of death worldwide, with pedestrians and other vulnerable road users accounting for over half of the 1.19 million annual fatalities, much of it due to human error. Level-5 automated driving systems (ADSs), capable of full self-driving without human oversight, have the potential to reduce these incidents. However, their effectiveness depends not only on automation performance but also on their ability to communicate intent and coordinate safely with pedestrians in the absence of traditional driver cues. Understanding how pedestrians interpret and respond to ADS behavior is therefore critical to the development of connected vehicle systems. This study extends the Theory of Planned Behavior (TPB) by incorporating four external factors (i.e. safety, trust, compatibility, and understanding) to model pedestrian decision-making in road-crossing scenarios involving level-5 ADSs. Using data from an online survey (n = 212), results show that perceived behavioral control, attitude, and social information significantly predict pedestrians' crossing intentions. External factors, particularly perceived safety and understanding, strongly influence these constructs. Findings provide actionable insights for designing external human-machine interfaces (eHMIs) and cooperative V2X communication strategies that support safe, transparent interactions between automated vehicles and pedestrians. This work contributes to the development of inclusive, human-centered connected mobility systems.

</details>


### [4] [What is "Spatial" about Spatial Computing?](https://arxiv.org/abs/2508.20477)

*Yibo Wang, Yuhan Luo, Janghee Cho, Junnan Yu*

**Main category:** cs.HC

**Keywords:** spatial computing, Human-Computer Interaction, mixed reality, geographic information systems, interdisciplinary integration

**Relevance Score:** 7

**TL;DR:** This paper explores the fragmented field of spatial computing, tracing its historical evolution and proposing a cohesive understanding that enhances identity and technological innovation in the interplay of physical and digital spaces.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the conceptual fragmentation in spatial computing and enhance interdisciplinary understanding and advancement.

**Method:** The authors trace the historical origins of spatial computing and categorize two perspectives on 'spatial' to unify understanding across disciplines.

**Key Contributions:**

	1. Identifies and categorizes the conceptual fragmentation in spatial computing.
	2. Proposes a unified conceptual model for understanding spatial computing across disciplines.
	3. Highlights implications for future technological innovations in HCI and spatial interactions.

**Result:** Identified two schools of thought on spatial computing: one focusing on spatial data guiding interaction in the physical world, and the other emphasizing embodied engagement through the integration of physical and digital environments.

**Limitations:** 

**Conclusion:** The synthesis of these perspectives establishes spatial computing as a transformative computational paradigm that redefines human interactions with environments.

**Abstract:** Recent advancements in geographic information systems and mixed reality technologies have positioned spatial computing as a transformative paradigm in computational science. However, the field remains conceptually fragmented, with diverse interpretations across disciplines like Human-Computer Interaction, Geographic Information Science, and Computer Science, which hinders a comprehensive understanding of spatial computing and poses challenges for its coherent advancement and interdisciplinary integration. In this paper, we trace the origins and historical evolution of spatial computing and examine how "spatial" is understood, identifying two schools of thought: "spatial" as the contextual understanding of space, where spatial data guides interaction in the physical world; and "spatial" as a mixed space for interaction, emphasizing the seamless integration of physical and digital environments to enable embodied engagement. By synthesizing these perspectives, we propose spatial computing as a computational paradigm that redefines the interplay between environment, computation, and human experience, offering a holistic lens to enhance its conceptual clarity and inspire future technological innovations that support meaningful interactions with and shaping of environments.

</details>


### [5] [VisiTrail: A Cognitive Visualization Tool for Time-Series Analysis of Eye Tracking Data from Attention Game](https://arxiv.org/abs/2508.20522)

*Abdul Rehman, Ilona Heldal, Jerry Chun-Wei Lin*

**Main category:** cs.HC

**Keywords:** Eye Tracking, Visual Attention, Cognitive Processes, Gaze Analysis, Interactive Environments

**Relevance Score:** 8

**TL;DR:** This paper presents a new analysis tool for eye tracking data that enhances understanding of visual attention and cognitive processes in interactive environments by integrating temporal pattern analysis and performance metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited insights from traditional gaze analyses in understanding visual attention dynamics and their impact on task performance in complex visual search scenarios.

**Method:** The proposed tool analyzes time series eye tracking data, incorporating gaze measures like fixations and saccades, and provides insights into temporal patterns and object-click sequences linked to user actions and performance metrics.

**Key Contributions:**

	1. Integration of time series analysis for gaze data
	2. Visualization techniques that interpret complex gaze-stimuli connections
	3. Linking gaze behavior directly to user actions with performance metrics

**Result:** The tool reveals how attention evolves during task performance and provides comprehensive visualizations that clarify complex relationships between gaze behavior and user actions.

**Limitations:** 

**Conclusion:** This eye tracking analysis tool offers a robust framework for understanding cognitive processes and enhancing performance in interactive visual tasks.

**Abstract:** Eye Tracking (ET) can help to understand visual attention and cognitive processes in interactive environments. In attention tasks, distinguishing between relevant target objects and distractors is crucial for effective performance, yet the underlying gaze patterns that drive successful task completion remain incompletely understood. Traditional gaze analyses lack comprehensive insights into the temporal dynamics of attention allocation and the relationship between gaze behavior and task performance. When applied to complex visual search scenarios, current gaze analysis methods face several limitations, including the isolation of measurements, visual stability, search efficiency, and the decision-making processes involved in these scenarios. This paper proposes an analysis tool that considers time series for eye tracking data from task performance and also gaze measures (fixations, saccades and smooth pursuit); temporal pattern analysis that reveals how attention evolves throughout task performance; object-click sequence tracking that directly links visual attention to user actions; and performance metrics that quantify both accuracy and efficiency. This tool provides comprehensive visualization techniques that make complex patterns of stimuli and gaze connections interpretable.

</details>


### [6] [Persode: Personalized Visual Journaling with Episodic Memory-Aware AI Agent](https://arxiv.org/abs/2508.20585)

*Seokho Jin, Manseo Kim, Sungho Byun, Hansol Kim, Jungmin Lee, Sujeong Baek, Semi Kim, Sanghum Park, Sung Park*

**Main category:** cs.HC

**Keywords:** Reflective journaling, Visual storytelling, Generation Alpha, Generation Z, Retrieval-Augmented Generation

**Relevance Score:** 8

**TL;DR:** Persode is a personalized journaling system designed for Generation Alpha and Z, integrating visual storytelling and memory-aware interactions to enhance user engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The traditional journaling methods lack personalization and do not engage the digital-native Generation Alpha and Z effectively, who prefer faster and more visually immersive experiences.

**Method:** Persode employs a tailored onboarding process to capture user demographics and stylistic preferences. It uses a Retrieval-Augmented Generation (RAG) framework to prioritize emotionally significant memories and generates visually engaging narratives through text-to-image models.

**Key Contributions:**

	1. Introduction of a personalized onboarding process for journaling
	2. Utilization of a RAG framework for prioritizing significant memories
	3. Dynamic generation of visual narratives using text-to-image models

**Result:** Persode successfully enhances emotional recall and engagement by providing personalized, context-rich interactions tailored to individual user preferences.

**Limitations:** 

**Conclusion:** By addressing personalization, visual engagement, and responsiveness, Persode effectively meets the evolving preferences of younger generations, bridging the gap between traditional and modern journaling.

**Abstract:** Reflective journaling often lacks personalization and fails to engage Generation Alpha and Z, who prefer visually immersive and fast-paced interactions over traditional text-heavy methods. Visual storytelling enhances emotional recall and offers an engaging way to process personal expe- riences. Designed with these digital-native generations in mind, this paper introduces Persode, a journaling system that integrates personalized onboarding, memory-aware conversational agents, and automated visual storytelling. Persode captures user demographics and stylistic preferences through a tailored onboarding process, ensuring outputs resonate with individual identities. Using a Retrieval-Augmented Generation (RAG) framework, it prioritizes emotionally significant memories to provide meaningful, context-rich interactions. Additionally, Persode dynamically transforms user experiences into visually engaging narratives by generating prompts for advanced text-to-image models, adapting characters, backgrounds, and styles to user preferences. By addressing the need for personalization, visual engagement, and responsiveness, Persode bridges the gap between traditional journaling and the evolving preferences of Gen Alpha and Z.

</details>


### [7] [Schema-Guided Response Generation using Multi-Frame Dialogue State for Motivational Interviewing Systems](https://arxiv.org/abs/2508.20635)

*Jie Zeng, Yukiko I. Nakano*

**Main category:** cs.HC

**Keywords:** Motivational Interviewing, Dialogue Systems, Large Language Models, Behavioral Change, User Study

**Relevance Score:** 8

**TL;DR:** This study proposes a dialogue system integrating Motivational Interviewing principles to guide LLMs in generating counselor responses that foster behavioral change.

**Read time:** 28 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to enhance client motivation for behavioral change through effective dialogue systems that follow Motivational Interviewing principles.

**Method:** A schema-guided approach was developed for updating multi-frame dialogue states, along with a decision mechanism for dynamically determining response focus based on MI principles.

**Key Contributions:**

	1. Development of a schema-guided approach for dialogue systems based on MI principles
	2. Evaluation of a dialogue system through user study
	3. Successful generation of MI-favorable responses in dialogue interactions

**Result:** The implemented dialogue system was evaluated through a user study, demonstrating success in generating MI-favorable responses and encouraging client deliberation through effective questioning.

**Limitations:** 

**Conclusion:** The study shows the feasibility of aligning LLM-generated responses with Motivational Interviewing to support clients in behavioral change.

**Abstract:** The primary goal of Motivational Interviewing (MI) is to help clients build their own motivation for behavioral change. To support this in dialogue systems, it is essential to guide large language models (LLMs) to generate counselor responses aligned with MI principles. By employing a schema-guided approach, this study proposes a method for updating multi-frame dialogue states and a strategy decision mechanism that dynamically determines the response focus in a manner grounded in MI principles. The proposed method was implemented in a dialogue system and evaluated through a user study. Results showed that the proposed system successfully generated MI-favorable responses and effectively encouraged the user's (client's) deliberation by asking eliciting questions.

</details>


### [8] [Understanding, Protecting, and Augmenting Human Cognition with Generative AI: A Synthesis of the CHI 2025 Tools for Thought Workshop](https://arxiv.org/abs/2508.21036)

*Lev Tankelevitch, Elena L. Glassman, Jessica He, Aniket Kittur, Mina Lee, Srishti Palani, Advait Sarkar, Gonzalo Ramos, Yvonne Rogers, Hari Subramonyam*

**Main category:** cs.HC

**Keywords:** Generative AI, Human Cognition, Design Practices, Metacognition, Critical Thinking

**Relevance Score:** 8

**TL;DR:** This paper summarizes a CHI 2025 workshop on how generative AI (GenAI) affects human cognition and explores opportunities for design practices that augment human thought.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the implications of generative AI on human cognition and identify tools and practices that can enhance critical thinking, memory, and creativity.

**Method:** The workshop brought together 56 researchers and designers to discuss the impact of GenAI on cognitive processes and design tools that can augment human thought.

**Key Contributions:**

	1. Identified key areas where generative AI can augment human cognition.
	2. Sourced diverse perspectives from academia and industry on GenAI's impact.
	3. Catalyzed discussions on design practices for developing tools that support cognitive processes.

**Result:** The workshop produced 34 papers and portfolios, leading to discussions that mapped research and design opportunities in the domain of GenAI and cognition.

**Limitations:** 

**Conclusion:** A synthesis of the workshop material serves to initiate a multidisciplinary community focused on integrating generative AI into cognitive enhancement practices.

**Abstract:** Generative AI (GenAI) radically expands the scope and capability of automation for work, education, and everyday tasks, a transformation posing both risks and opportunities for human cognition. How will human cognition change, and what opportunities are there for GenAI to augment it? Which theories, metrics, and other tools are needed to address these questions? The CHI 2025 workshop on Tools for Thought aimed to bridge an emerging science of how the use of GenAI affects human thought, from metacognition to critical thinking, memory, and creativity, with an emerging design practice for building GenAI tools that both protect and augment human thought. Fifty-six researchers, designers, and thinkers from across disciplines as well as industry and academia, along with 34 papers and portfolios, seeded a day of discussion, ideation, and community-building. We synthesize this material here to begin mapping the space of research and design opportunities and to catalyze a multidisciplinary community around this pressing area of research.

</details>


### [9] [OnGoal: Tracking and Visualizing Conversational Goals in Multi-Turn Dialogue with Large Language Models](https://arxiv.org/abs/2508.21061)

*Adam Coscia, Shunan Guo, Eunyee Koh, Alex Endert*

**Main category:** cs.HC

**Keywords:** goal management, LLM chat interface, user engagement

**Relevance Score:** 9

**TL;DR:** OnGoal is an LLM chat interface that facilitates better goal management in multi-turn dialogues, improving user engagement and task efficiency.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user evaluation and management of conversational goals in complex multi-turn dialogues with LLMs.

**Method:** A study with 20 participants on a writing task comparing OnGoal with a baseline chat interface without goal tracking.

**Key Contributions:**

	1. Introduction of OnGoal for goal management in LLM chats
	2. Demonstrated efficiency improvements in achieving conversational goals
	3. Provided design implications for future LLM chat interfaces

**Result:** Participants using OnGoal achieved their goals more efficiently while discovering new prompting strategies to improve communication during dialogues.

**Limitations:** 

**Conclusion:** Tracking and visualizing goals can enhance user engagement, reduce cognitive load, and improve feedback mechanisms in LLM dialogues.

**Abstract:** As multi-turn dialogues with large language models (LLMs) grow longer and more complex, how can users better evaluate and review progress on their conversational goals? We present OnGoal, an LLM chat interface that helps users better manage goal progress. OnGoal provides real-time feedback on goal alignment through LLM-assisted evaluation, explanations for evaluation results with examples, and overviews of goal progression over time, enabling users to navigate complex dialogues more effectively. Through a study with 20 participants on a writing task, we evaluate OnGoal against a baseline chat interface without goal tracking. Using OnGoal, participants spent less time and effort to achieve their goals while exploring new prompting strategies to overcome miscommunication, suggesting tracking and visualizing goals can enhance engagement and resilience in LLM dialogues. Our findings inspired design implications for future LLM chat interfaces that improve goal communication, reduce cognitive load, enhance interactivity, and enable feedback to improve LLM performance.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [10] [Social Bias in Multilingual Language Models: A Survey](https://arxiv.org/abs/2508.20201)

*Lance Calvin Lim Gamboa, Yue Feng, Mark Lee*

**Main category:** cs.CL

**Keywords:** multilingual models, bias evaluation, human-computer interaction, NLP advancement

**Relevance Score:** 7

**TL;DR:** This paper reviews research on bias in multilingual AI models, highlighting methodological gaps and proposing future directions for mitigating bias across languages and cultures.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the social biases present in pretrained multilingual models and enhance their evaluation and mitigation in non-English contexts.

**Method:** The study systematically reviews existing research on bias evaluation and mitigation in multilingual models, focusing on linguistic diversity and cultural considerations.

**Key Contributions:**

	1. Systematic analysis of bias evaluation in multilingual AI models.
	2. Identification of methodological gaps in current research.
	3. Proposals for future research directions to improve cross-cultural bias mitigation.

**Result:** The review identifies gaps in methodological choices, such as language preference and the lack of multilingual mitigation experiments, while also cataloging common issues and solutions in adapting bias benchmarks.

**Limitations:** The review may not encompass all existing literature and focuses more on certain languages.

**Conclusion:** The paper emphasizes the need for future research to enhance inclusivity and cultural appropriateness in multilingual bias literature and to align with advancements in NLP.

**Abstract:** Pretrained multilingual models exhibit the same social bias as models processing English texts. This systematic review analyzes emerging research that extends bias evaluation and mitigation approaches into multilingual and non-English contexts. We examine these studies with respect to linguistic diversity, cultural awareness, and their choice of evaluation metrics and mitigation techniques. Our survey illuminates gaps in the field's dominant methodological design choices (e.g., preference for certain languages, scarcity of multilingual mitigation experiments) while cataloging common issues encountered and solutions implemented in adapting bias benchmarks across languages and cultures. Drawing from the implications of our findings, we chart directions for future research that can reinforce the multilingual bias literature's inclusivity, cross-cultural appropriateness, and alignment with state-of-the-art NLP advancements.

</details>


### [11] [Prompting Strategies for Language Model-Based Item Generation in K-12 Education: Bridging the Gap Between Small and Large Language Models](https://arxiv.org/abs/2508.20217)

*Mohammad Amini, Babak Ahmadi, Xiaomeng Xiong, Yilin Zhang, Christopher Qiao*

**Main category:** cs.CL

**Keywords:** Automatic Generation, Language Models, Multiple Choice Questions, Morphological Assessment, Structured Prompting

**Relevance Score:** 7

**TL;DR:** The study investigates the use of language models for generating multiple choice questions to enhance morphological assessments, focusing on structured prompting and model efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and inconsistencies associated with manual development of assessment tests for morphology.

**Method:** The study utilized a two-fold approach comparing a fine-tuned medium model (Gemma) with a larger untuned model (GPT-3.5) and evaluated various structured prompting strategies.

**Key Contributions:**

	1. Demonstrated the effectiveness of structured prompting for AIG in assessments.
	2. Highlighted the benefits of fine-tuning on mid-sized models under limited data conditions.
	3. Proposed a scalable workflow for generating and validating language assessment items.

**Result:** Structured prompting significantly improved the outputs of the Gemma model, generating more construct-aligned items compared to the untuned GPT-3.5 in zero-shot configurations.

**Limitations:** The study focuses primarily on K-12 assessments and may not generalize to other educational levels or contexts.

**Conclusion:** Efficient fine-tuning and appropriate prompting strategies can enhance the performance of mid-sized language models for automated item generation in educational assessments.

**Abstract:** This study explores automatic generation (AIG) using language models to create multiple choice questions (MCQs) for morphological assessment, aiming to reduce the cost and inconsistency of manual test development. The study used a two-fold approach. First, we compared a fine-tuned medium model (Gemma, 2B) with a larger untuned one (GPT-3.5, 175B). Second, we evaluated seven structured prompting strategies, including zero-shot, few-shot, chain-of-thought, role-based, sequential, and combinations. Generated items were assessed using automated metrics and expert scoring across five dimensions. We also used GPT-4.1, trained on expert-rated samples, to simulate human scoring at scale. Results show that structured prompting, especially strategies combining chain-of-thought and sequential design, significantly improved Gemma's outputs. Gemma generally produced more construct-aligned and instructionally appropriate items than GPT-3.5's zero-shot responses, with prompt design playing a key role in mid-size model performance. This study demonstrates that structured prompting and efficient fine-tuning can enhance midsized models for AIG under limited data conditions. We highlight the value of combining automated metrics, expert judgment, and large-model simulation to ensure alignment with assessment goals. The proposed workflow offers a practical and scalable way to develop and validate language assessment items for K-12.

</details>


### [12] [Integrating SystemC TLM into FMI 3.0 Co-Simulations with an Open-Source Approach](https://arxiv.org/abs/2508.20223)

*Andrei Mihai Albu, Giovanni Pollo, Alessio Burrello, Daniele Jahier Pagliari, Cristian Tesconi, Alessandra Neri, Dario Soldi, Fabio Autieri, Sara Vinco*

**Main category:** cs.CL

**Keywords:** Cyber-physical systems, SystemC, FMI, Co-simulation, Automotive

**Relevance Score:** 2

**TL;DR:** This paper proposes an open-source methodology for integrating SystemC TLM models into FMI-based co-simulation, improving interoperability in automotive cyber-physical systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address integration challenges in modeling complex cyber-physical systems, particularly in automotive applications.

**Method:** The paper presents a methodology that encapsulates SystemC TLM components as FMI 3.0 Co Simulation FMUs, facilitating integration in heterogeneous simulation environments.

**Key Contributions:**

	1. Development of a methodology for encapsulating SystemC TLM components as FMI FMUs
	2. Introduction of an open-source toolchain for enhanced co-simulation
	3. Solving challenges related to time synchronization and data exchange

**Result:** The integration approach is shown to be feasible and effective, with demonstrated case studies addressing synchronization and data exchange.

**Limitations:** 

**Conclusion:** The proposed toolchain offers a standardized solution for co-simulation across different domains, thereby enhancing the modeling capabilities in automotive and other applications.

**Abstract:** The growing complexity of cyber-physical systems, particularly in automotive applications, has increased the demand for efficient modeling and cross-domain co-simulation techniques. While SystemC Transaction-Level Modeling (TLM) enables effective hardware/software co-design, its limited interoperability with models from other engineering domains poses integration challenges. This paper presents a fully open-source methodology for integrating SystemC TLM models into Functional Mock-up Interface (FMI)-based co-simulation workflows. By encapsulating SystemC TLM components as FMI 3.0 Co Simulation Functional Mock-up Units (FMUs), the proposed approach facilitates seamless, standardized integration across heterogeneous simulation environments. We introduce a lightweight open-source toolchain, address key technical challenges such as time synchronization and data exchange, and demonstrate the feasibility and effectiveness of the integration through representative case studies.

</details>


### [13] [Can Compact Language Models Search Like Agents? Distillation-Guided Policy Optimization for Preserving Agentic RAG Capabilities](https://arxiv.org/abs/2508.20324)

*Rikuto Kotoge, Mai Nishimura, Jiaxin Ma*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Language Models, Agentic Behaviors

**Relevance Score:** 8

**TL;DR:** Proposes Distillation-Guided Policy Optimization (DGPO) to enhance reasoning and search behavior in compact language models by using teacher demonstrations and guidance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning ability and performance of compact language models in reinforcement learning tasks, addressing issues of sparse rewards and unstable training.

**Method:** Introduces Distillation-Guided Policy Optimization (DGPO) which utilizes cold-start initialization from teacher demonstrations and continuous guidance during policy optimization.

**Key Contributions:**

	1. Introduction of Distillation-Guided Policy Optimization (DGPO) for compact language models.
	2. Development of Agentic RAG Capabilities (ARC) as a fine-grained evaluation metric.
	3. Demonstration of compact models achieving advanced behaviors through teacher-guided training.

**Result:** DGPO allows compact models to exhibit advanced agentic behaviors, sometimes outperforming larger models, demonstrating its effectiveness in resource-constrained environments.

**Limitations:** 

**Conclusion:** DGPO makes it feasible for compact language models to perform agentic RAG behaviors effectively, showing promising results in evaluation metrics and experiments.

**Abstract:** Reinforcement Learning has emerged as a post-training approach to elicit agentic RAG behaviors such as search and planning from language models. However, compact language models (e.g., 0.5B parameters) struggle due to poor reasoning ability, resulting in sparse rewards and unstable training. To overcome these difficulties, we propose Distillation-Guided Policy Optimization (DGPO), which addresses the challenges through cold-start initialization from teacher demonstrations and continuous teacher guidance during policy optimization. To systematically evaluate our approach, we introduce Agentic RAG Capabilities (ARC), a fine-grained metric analyzing reasoning, search coordination, and response synthesis. Comprehensive experiments demonstrate that DGPO enables compact models to achieve sophisticated agentic search behaviors, even outperforming the larger teacher model in some cases. DGPO makes agentic RAG feasible in computing resource-constrained environments.

</details>


### [14] [GUARD: Guideline Upholding Test through Adaptive Role-play and Jailbreak Diagnostics for LLMs](https://arxiv.org/abs/2508.20325)

*Haibo Jin, Ruoxi Chen, Peiyan Zhang, Andy Zhou, Yang Zhang, Haohan Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI Compliance, Ethics Guidelines, Jailbreak Diagnostics, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** This paper introduces GUARD, a testing method designed to ensure compliance of Large Language Models (LLMs) with government-issued ethical guidelines. It aims to operationalize vague guidelines into actionable questions to assess LLM adherence, using automated generation of guideline-violating questions and diagnostics to identify unethical responses.

**Read time:** 54 min

<details>
  <summary>Details</summary>

**Motivation:** Governments have issued high-level ethics guidelines for AI, but there is a lack of processes to translate these into specific, actionable test questions to verify compliance, raising concerns about harmful AI responses.

**Method:** GUARD operationalizes guidelines into specific, guideline-violating questions and employs jailbreak diagnostics (GUARD-JD) to provoke guideline violations in LLM responses, assessing their compliance.

**Key Contributions:**

	1. Introduced GUARD as a structured testing method for LLM compliance with ethical guidelines.
	2. Developed GUARD-JD for effective diagnostics of guideline-violating responses.
	3. Empirically validated against multiple LLMs and guidelines, showcasing transferability to other AI models.

**Result:** GUARD was empirically validated on seven LLMs, demonstrating its effectiveness in testing compliance with government-issued guidelines and identifying potential bypass scenarios for safety mechanisms.

**Limitations:** 

**Conclusion:** GUARD produces compliance reports detailing adherence levels and violations, proving applicable not only to LLMs but also potentially transferable to vision-language models.

**Abstract:** As Large Language Models become increasingly integral to various domains, their potential to generate harmful responses has prompted significant societal and regulatory concerns. In response, governments have issued ethics guidelines to promote the development of trustworthy AI. However, these guidelines are typically high-level demands for developers and testers, leaving a gap in translating them into actionable testing questions to verify LLM compliance.   To address this challenge, we introduce GUARD (\textbf{G}uideline \textbf{U}pholding Test through \textbf{A}daptive \textbf{R}ole-play and Jailbreak \textbf{D}iagnostics), a testing method designed to operationalize guidelines into specific guideline-violating questions that assess LLM adherence. To implement this, GUARD uses automated generation of guideline-violating questions based on government-issued guidelines, thereby testing whether responses comply with these guidelines. When responses directly violate guidelines, GUARD reports inconsistencies. Furthermore, for responses that do not directly violate guidelines, GUARD integrates the concept of ``jailbreaks'' to diagnostics, named GUARD-JD, which creates scenarios that provoke unethical or guideline-violating responses, effectively identifying potential scenarios that could bypass built-in safety mechanisms. Our method finally culminates in a compliance report, delineating the extent of adherence and highlighting any violations.   We have empirically validated the effectiveness of GUARD on seven LLMs, including Vicuna-13B, LongChat-7B, Llama2-7B, Llama-3-8B, GPT-3.5, GPT-4, GPT-4o, and Claude-3.7, by testing compliance under three government-issued guidelines and conducting jailbreak diagnostics. Additionally, GUARD-JD can transfer jailbreak diagnostics to vision-language models, demonstrating its usage in promoting reliable LLM-based applications.

</details>


### [15] [Joint Enhancement of Relational Reasoning for Long-Context LLMs](https://arxiv.org/abs/2508.20351)

*Zhirui Chen, Wei Shen, Jiashui Huang, Ling Shao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Graph-Based Reasoning, Long Contexts

**Relevance Score:** 9

**TL;DR:** The paper presents JERR, a framework to improve long-context comprehension in LLMs through graph-based reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) face challenges with long contexts and complex tasks due to memory limitations and lack of transparency.

**Method:** The JERR framework integrates synopsis extraction, graph construction, and relational reasoning, using chunking for text summarization, building a directed acyclic graph to resolve redundancy, and employing Monte Carlo Tree Search for complex reasoning.

**Key Contributions:**

	1. Introduction of the JERR framework for long-context comprehension
	2. Use of graph-based reasoning for improved logical consistency
	3. Enhancement of interpretability in LLM outputs through MCTS

**Result:** JERR outperforms all baselines on ROUGE and F1 metrics, achieving the highest scores on the LLM-Rater evaluation.

**Limitations:** 

**Conclusion:** JERR provides a robust solution for enhancing LLMs' ability to handle extended contexts and complex reasoning tasks reliably and transparently.

**Abstract:** Despite significant progress, large language models (LLMs) still struggle with long contexts due to memory limitations and their inability to tackle complex and long-context tasks. Additionally, LLMs often suffer from a lack of transparency and are prone to producing hallucinations. To address these challenges, we propose \textbf{JERR}, a novel framework designed to enhance long-context comprehension via graph-based reasoning in LLMs. JERR integrates three key components: synopsis extraction, graph construction, and relational reasoning. First, synopsis is extracted by chunking text strategically, allowing the model to summarize and understand information more efficiently. Second, we build a directed acyclic graph (DAG) to resolve redundancy, ensuring logical consistency and clarity. Finally, we incorporate Monte Carlo Tree Search (MCTS) to help the model navigate complex reasoning paths, ensuring more accurate and interpretable outputs. This framework provides a novel solution that enables LLMs to handle extended contexts and complex reasoning tasks with improved reliability and transparency. Experimental results show that JERR consistently outperforms all baselines on the ROUGE and F1 metrics, achieving the highest scores on the LLM-Rater evaluation.

</details>


### [16] [Graph-R1: Unleashing LLM Reasoning with NP-Hard Graph Problems](https://arxiv.org/abs/2508.20373)

*Yuyao Wang, Bowen Liu, Jianheng Tang, Nuo Chen, Yuhan Li, Qifan Zhang, Jia Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Graph Problems, Reinforcement Learning, Supervised Fine-Tuning, Long Chain-of-Thought

**Relevance Score:** 8

**TL;DR:** This paper introduces NP-hard graph problems as a synthetic training resource to enhance reasoning capabilities in large language models, proposing a two-stage post-training framework to improve reasoning depth and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for cost-effective alternatives to high-quality datasets for developing Long CoT behaviors in reasoning large language models.

**Method:** A two-stage post-training framework consisting of supervised fine-tuning on rejection-sampled NP-hard graph instances followed by reinforcement learning with a fine-grained reward design.

**Key Contributions:**

	1. Introduction of NP-hard graph problems as a synthetic training corpus for reasoning
	2. Development of a two-stage post-training framework combining supervised fine-tuning and reinforcement learning
	3. Demonstration of improved reasoning efficiency and accuracy in handling NP-hard problems.

**Result:** The model Graph-R1-7B achieves strong generalization across various domains and outperforms an existing large model on NP-hard graph problems in terms of accuracy and reasoning efficiency.

**Limitations:** 

**Conclusion:** NP-hard graph problems provide an effective and scalable resource for enhancing reasoning in language models, enabling further advancements in Long CoT reasoning.

**Abstract:** Reasoning Large Language Models (RLLMs) have recently achieved remarkable progress on complex reasoning tasks, largely enabled by their long chain-of-thought (Long CoT) capabilities. However, developing these Long CoT behaviors relies heavily on post-training with high-quality datasets, which are typically costly and human-curated (e.g., mathematics and code), leaving scalable alternatives unexplored. In this work, we introduce NP-hard (NPH) graph problems as a novel synthetic training corpus, as they inherently require deep reasoning, extensive exploration, and reflective strategies, which are core characteristics of Long CoT reasoning. Building on this insight, we develop a two-stage post-training framework: (i) Long CoT Supervised Fine-Tuning (SFT) on rejection-sampled NPH graph instances, which substantially enhances reasoning depth, and (ii) Reinforcement Learning (RL) with a fine-grained reward design, which sharpens reasoning efficiency. Our flagship model, Graph-R1-7B, demonstrates strong generalization across mathematics, coding, STEM, and logic, and surpasses QwQ-32B on NPH graph problems in both accuracy and reasoning efficiency. These results position NPH graph problems as an effective and scalable resource for advancing Long CoT reasoning in LLMs, opening a new frontier for LLM post-training. Our implementation is available at https://github.com/Graph-Reasoner/Graph-R1, with models and datasets hosted in our Hugging Face collection HKUST-DSAIL/Graph-R1.

</details>


### [17] [CAPE: Context-Aware Personality Evaluation Framework for Large Language Models](https://arxiv.org/abs/2508.20385)

*Jivnesh Sandhan, Fei Cheng, Tushar Sandhan, Yugo Murawaki*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personality Evaluation, Context-Aware, Response Consistency, Conversational History

**Relevance Score:** 9

**TL;DR:** This paper introduces the Context-Aware Personality Evaluation (CAPE) framework for assessing the personality traits of Large Language Models (LLMs), highlighting the impact of prior conversational context on response consistency and personality shifts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional psychometric assessments on LLMs by incorporating the contextual history of conversations, leading to a more realistic evaluation of their behavioral traits.

**Method:** The paper proposes the CAPE framework, introducing novel metrics to assess the consistency of LLM responses based on prior conversational interactions and conducting experiments on 7 different LLMs.

**Key Contributions:**

	1. Introduction of the CAPE framework for context-aware personality evaluation of LLMs.
	2. Development of novel metrics to quantify response consistency of LLMs based on conversational history.
	3. Demonstration of personality shifts in LLM responses influenced by prior interactions, with practical implications for Role Playing Agents.

**Result:** Experiments show that conversational history enhances response consistency but can induce personality shifts in models, with some models being more sensitive to context than others.

**Limitations:** The framework is contingent on the quality and relevance of prior conversations, which may not always represent real-world interactions comprehensively.

**Conclusion:** The CAPE framework improves the evaluation of LLMs by accounting for context, making their responses better aligned with human judgments, particularly in applications involving Role Playing Agents.

**Abstract:** Psychometric tests, traditionally used to assess humans, are now being applied to Large Language Models (LLMs) to evaluate their behavioral traits. However, existing studies follow a context-free approach, answering each question in isolation to avoid contextual influence. We term this the Disney World test, an artificial setting that ignores real-world applications, where conversational history shapes responses. To bridge this gap, we propose the first Context-Aware Personality Evaluation (CAPE) framework for LLMs, incorporating prior conversational interactions. To thoroughly analyze the influence of context, we introduce novel metrics to quantify the consistency of LLM responses, a fundamental trait in human behavior.   Our exhaustive experiments on 7 LLMs reveal that conversational history enhances response consistency via in-context learning but also induces personality shifts, with GPT-3.5-Turbo and GPT-4-Turbo exhibiting extreme deviations. While GPT models are robust to question ordering, Gemini-1.5-Flash and Llama-8B display significant sensitivity. Moreover, GPT models response stem from their intrinsic personality traits as well as prior interactions, whereas Gemini-1.5-Flash and Llama--8B heavily depend on prior interactions. Finally, applying our framework to Role Playing Agents (RPAs) shows context-dependent personality shifts improve response consistency and better align with human judgments. Our code and datasets are publicly available at: https://github.com/jivnesh/CAPE

</details>


### [18] [Measuring Reasoning Utility in LLMs via Conditional Entropy Reduction](https://arxiv.org/abs/2508.20395)

*Xu Guo*

**Main category:** cs.CL

**Keywords:** large language models, reasoning utility, conditional entropy, MATH dataset, answer correctness

**Relevance Score:** 9

**TL;DR:** This paper investigates the role of intermediate reasoning steps in large language models and their contribution to answer correctness, finding that decreasing conditional entropy during reasoning indicates higher accuracy.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how reasoning utility affects the correctness of answers generated by large language models, particularly in the context of autoregressive generation.

**Method:** An oracle study on the MATH dataset was conducted using Qwen2.5-32B and GPT-4o to generate reasoning chains. A separate model (Qwen3-8B) was employed to quantify the utility of these chains based on conditional entropy at each reasoning step.

**Key Contributions:**

	1. Demonstrates the relationship between reasoning utility and answer correctness
	2. Establishes that decreasing conditional entropy correlates with correct answers
	3. Provides insights into the efficiency of reasoning pipelines for LLMs

**Result:** The study found that decreasing conditional entropy during reasoning is associated with correct answers, while incorrect paths tend to be longer and have flat or increasing entropy.

**Limitations:** The study relies on specific models and datasets, which may limit generalizability.

**Conclusion:** These findings highlight the need for efficient reasoning pipelines that can identify and avoid unproductive reasoning early in the generation process.

**Abstract:** Recent advancements in large language models (LLMs) often rely on generating intermediate reasoning steps to enhance accuracy. However, little work has examined how reasoning utility contributes to the final answer's correctness. Due to the stochastic nature of autoregressive generation, generating more context does not guarantee increased confidence in the answer. If we could predict, during generation, whether a reasoning step will be useful, we could stop early or prune ineffective steps, avoiding distractions in the final decision.   We present an oracle study on MATH dataset, using Qwen2.5-32B and GPT-4o to generate reasoning chains, and then employing a separate model (Qwen3-8B) to quantify the utility of these chains for final accuracy. Specifically, we measure the model's uncertainty on the answer span Y at each reasoning step using conditional entropy (expected negative log-likelihood over the vocabulary) with context expanding step by step. Our results show a clear pattern: conditional entropy that decreases over steps is strongly associated with correct answers, whereas flat or increasing entropy often results in wrong answers. We also corroborate that incorrect reasoning paths tend to be longer than correct ones, suggesting that longer reasoning does not necessarily yield better outcomes. These findings serve as a foundation to inspire future work on designing efficient reasoning pipelines that detect and avoid unproductive reasoning early.

</details>


### [19] [UI-Bench: A Benchmark for Evaluating Design Capabilities of AI Text-to-App Tools](https://arxiv.org/abs/2508.20410)

*Sam Jung, Agustin Garcinuno, Spencer Mateega*

**Main category:** cs.CL

**Keywords:** AI, web design, benchmark, evaluation, TrueSkill

**Relevance Score:** 6

**TL;DR:** Introduction of UI-Bench, a benchmark for evaluating AI text-to-app tools for web design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously assess the visual excellence of AI text-to-app tools since no public benchmark currently exists.

**Method:** Evaluation through expert pairwise comparison, spanning 10 tools, 30 prompts, and 300 generated sites, using a TrueSkill-derived model.

**Key Contributions:**

	1. Introduction of the first large-scale benchmark for AI text-to-app tools
	2. Release of a complete prompt set and an open-source evaluation framework
	3. Establishment of a public leaderboard to track system performance.

**Result:** UI-Bench establishes a ranking of systems with calibrated confidence intervals and offers a public leaderboard.

**Limitations:** 

**Conclusion:** UI-Bench provides a reproducible standard for advancing AI-driven web design and promotes transparency in the performance of various tools.

**Abstract:** AI text-to-app tools promise high quality applications and websites in minutes, yet no public benchmark rigorously verifies those claims. We introduce UI-Bench, the first large-scale benchmark that evaluates visual excellence across competing AI text-to-app tools through expert pairwise comparison. Spanning 10 tools, 30 prompts, 300 generated sites, and \textit{4000+} expert judgments, UI-Bench ranks systems with a TrueSkill-derived model that yields calibrated confidence intervals. UI-Bench establishes a reproducible standard for advancing AI-driven web design. We release (i) the complete prompt set, (ii) an open-source evaluation framework, and (iii) a public leaderboard. The generated sites rated by participants will be released soon. View the UI-Bench leaderboard at https://uibench.ai/leaderboard.

</details>


### [20] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)

*Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan*

**Main category:** cs.CL

**Keywords:** proactive dialogue, large language models, evaluation framework, target planning, dialogue guidance

**Relevance Score:** 8

**TL;DR:** ProactiveEval is a unified framework for evaluating proactive dialogue abilities of large language models (LLMs), focusing on target planning and dialogue guidance across diverse domains.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluations of proactive dialogue in LLMs are fragmented and limited to domain-specific tasks, necessitating a more comprehensive approach.

**Method:** ProactiveEval decomposes proactive dialogue into target planning and dialogue guidance and establishes metrics for evaluation across six distinct domains, along with generating diverse evaluation data.

**Key Contributions:**

	1. Introduction of the ProactiveEval framework for evaluating proactive dialogue
	2. Creation of 328 evaluation environments across 6 domains
	3. Insights into the role of reasoning capabilities in proactive dialogue

**Result:** Experiments with 22 types of LLMs revealed that DeepSeek-R1 and Claude-3.7-Sonnet excelled in target planning and dialogue guidance, respectively.

**Limitations:** 

**Conclusion:** The study enhances understanding of how reasoning affects proactive dialogue behavior in LLMs and suggests directions for future research and model development.

**Abstract:** Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.

</details>


### [21] [DentalBench: Benchmarking and Advancing LLMs Capability for Bilingual Dentistry Understanding](https://arxiv.org/abs/2508.20416)

*Hengchuan Zhu, Yihuan Xu, Yichen Li, Zijie Meng, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** large language models, medical LLMs, dental benchmark, domain adaptation, question-answering

**Relevance Score:** 9

**TL;DR:** This paper presents DentalBench, a bilingual benchmark for evaluating LLMs in the dental domain, consisting of a QA benchmark and a large corpus for domain adaptation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in evaluating LLMs in specialized medical fields, particularly dentistry, due to limited resources.

**Method:** Introduction of DentalBench with two components: DentalQA for question-answering and DentalCorpus for domain-specific adaptations, followed by evaluation of 14 LLMs.

**Key Contributions:**

	1. Introduction of DentalBench, the first bilingual benchmark for the dental field.
	2. Development of a large-scale dental corpus for model adaptation.
	3. Evaluation of various LLMs revealing performance gaps and the impact of domain adaptation.

**Result:** Significant performance gaps were found across task types and languages, and domain adaptation notably improved model performance on knowledge-intensive tasks.

**Limitations:** Focused solely on dentistry, with potential applicability limited to generalization without additional domain-specific resources.

**Conclusion:** Domain-specific benchmarks are essential for developing effective LLMs in healthcare applications, confirming the potential for improved trustworthiness in medical AI.

**Abstract:** Recent advances in large language models (LLMs) and medical LLMs (Med-LLMs) have demonstrated strong performance on general medical benchmarks. However, their capabilities in specialized medical fields, such as dentistry which require deeper domain-specific knowledge, remain underexplored due to the lack of targeted evaluation resources. In this paper, we introduce DentalBench, the first comprehensive bilingual benchmark designed to evaluate and advance LLMs in the dental domain. DentalBench consists of two main components: DentalQA, an English-Chinese question-answering (QA) benchmark with 36,597 questions spanning 4 tasks and 16 dental subfields; and DentalCorpus, a large-scale, high-quality corpus with 337.35 million tokens curated for dental domain adaptation, supporting both supervised fine-tuning (SFT) and retrieval-augmented generation (RAG). We evaluate 14 LLMs, covering proprietary, open-source, and medical-specific models, and reveal significant performance gaps across task types and languages. Further experiments with Qwen-2.5-3B demonstrate that domain adaptation substantially improves model performance, particularly on knowledge-intensive and terminology-focused tasks, and highlight the importance of domain-specific benchmarks for developing trustworthy and effective LLMs tailored to healthcare applications.

</details>


### [22] [KG-CQR: Leveraging Structured Relation Representations in Knowledge Graphs for Contextual Query Retrieval](https://arxiv.org/abs/2508.20417)

*Chi Minh Bui, Ngoc Mai Thieu, Van Vinh Nguyen, Json J. Jung, Khac-Hoai Nam Bui*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Large Language Models, Retrieval-Augmented Generation, Contextual Query Retrieval, Multi-hop Question Answering

**Relevance Score:** 9

**TL;DR:** KG-CQR is a novel framework that enhances query retrieval in retrieval-augmented generation systems by using knowledge graphs to improve query contextualization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods focus on corpus-level context loss in RAG systems, but there is a need for better query enrichment to improve retrieval performance.

**Method:** KG-CQR employs subgraph extraction, completion, and contextual generation modules to enrich the representation of complex input queries using a knowledge graph, enabling it to work with different LLM sizes without requiring additional training.

**Key Contributions:**

	1. Introduction of KG-CQR for enriched query contextualization
	2. Model-agnostic pipeline for scalability across LLMs
	3. Demonstrated empirical improvements in retrieval tasks

**Result:** KG-CQR shows superior performance with a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 on RAGBench and MultiHop-RAG datasets compared to strong baselines.

**Limitations:** 

**Conclusion:** Incorporating KG-CQR into RAG tasks like multi-hop question answering consistently improves retrieval effectiveness over existing baselines.

**Abstract:** The integration of knowledge graphs (KGs) with large language models (LLMs) offers significant potential to improve the retrieval phase of retrieval-augmented generation (RAG) systems. In this study, we propose KG-CQR, a novel framework for Contextual Query Retrieval (CQR) that enhances the retrieval phase by enriching the contextual representation of complex input queries using a corpus-centric KG. Unlike existing methods that primarily address corpus-level context loss, KG-CQR focuses on query enrichment through structured relation representations, extracting and completing relevant KG subgraphs to generate semantically rich query contexts. Comprising subgraph extraction, completion, and contextual generation modules, KG-CQR operates as a model-agnostic pipeline, ensuring scalability across LLMs of varying sizes without additional training. Experimental results on RAGBench and MultiHop-RAG datasets demonstrate KG-CQR's superior performance, achieving a 4-6% improvement in mAP and a 2-3% improvement in Recall@25 over strong baseline models. Furthermore, evaluations on challenging RAG tasks such as multi-hop question answering show that, by incorporating KG-CQR, the performance consistently outperforms the existing baseline in terms of retrieval effectiveness

</details>


### [23] [CAMB: A comprehensive industrial LLM benchmark on civil aviation maintenance](https://arxiv.org/abs/2508.20420)

*Feng Zhang, Chengjie Pang, Yuehan Zhang, Chenyu Luo*

**Main category:** cs.CL

**Keywords:** civil aviation, maintenance, large language models, benchmark, RAG

**Relevance Score:** 4

**TL;DR:** Proposal of an industrial-grade benchmark for evaluating LLMs in civil aviation maintenance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of specialized evaluation tools for LLMs in civil aviation maintenance, which is critical for knowledge-intensive tasks and reasoning.

**Method:** Development of a benchmark specifically for civil aviation maintenance to measure LLM capabilities and identify knowledge gaps.

**Key Contributions:**

	1. Development of a specialized benchmark for LLMs in civil aviation maintenance.
	2. Identification of specific knowledge and reasoning gaps in existing LLMs.
	3. Open-sourcing the evaluation tool to promote further research.

**Result:** Demonstrated effectiveness of the benchmark in assessing model performance and provided an open-source tool to promote further research.

**Limitations:** 

**Conclusion:** The benchmark facilitates targeted improvements in LLM performance through specialized fine-tuning and optimization, addressing gaps in existing evaluation.

**Abstract:** Civil aviation maintenance is a domain characterized by stringent industry standards. Within this field, maintenance procedures and troubleshooting represent critical, knowledge-intensive tasks that require sophisticated reasoning. To address the lack of specialized evaluation tools for large language models (LLMs) in this vertical, we propose and develop an industrial-grade benchmark specifically designed for civil aviation maintenance. This benchmark serves a dual purpose: It provides a standardized tool to measure LLM capabilities within civil aviation maintenance, identifying specific gaps in domain knowledge and complex reasoning. By pinpointing these deficiencies, the benchmark establishes a foundation for targeted improvement efforts (e.g., domain-specific fine-tuning, RAG optimization, or specialized prompt engineering), ultimately facilitating progress toward more intelligent solutions within civil aviation maintenance. Our work addresses a significant gap in the current LLM evaluation, which primarily focuses on mathematical and coding reasoning tasks. In addition, given that Retrieval-Augmented Generation (RAG) systems are currently the dominant solutions in practical applications , we leverage this benchmark to evaluate existing well-known vector embedding models and LLMs for civil aviation maintenance scenarios. Through experimental exploration and analysis, we demonstrate the effectiveness of our benchmark in assessing model performance within this domain, and we open-source this evaluation benchmark and code to foster further research and development:https://github.com/CamBenchmark/cambenchmark

</details>


### [24] [Searching the Title of Practical Work of the Informatics Engineering Bachelor Program with the Case Base Reasoning Method](https://arxiv.org/abs/2508.20442)

*Agung Sukrisna Jaya, Osvari Arsalan, Danny Matthew Saputra*

**Main category:** cs.CL

**Keywords:** Case Based Reasoning, TF-IDF, Cosine Similarity, Title Searching, Practical Work

**Relevance Score:** 3

**TL;DR:** This paper discusses a Case Based Reasoning (CBR) system that uses TF-IDF and Cosine Similarity for searching practical work titles based on previous cases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of searching for practical work titles using past experiences with similar cases.

**Method:** The paper utilizes TF-IDF for vectorizing each practical work title word and employs Cosine Similarity to calculate similarity values.

**Key Contributions:**

	1. Development of a CBR system for title searching
	2. Application of TF-IDF and Cosine Similarity
	3. Empirical testing with practical work titles

**Result:** The system is capable of searching for practical work titles and keywords, achieving a consistent match score using 705 practical work titles.

**Limitations:** 

**Conclusion:** The two-phase testing showed that the title matching process is effective in returning relevant titles with high average match scores.

**Abstract:** Case Base Reasoning (CBR) is a case solving technique based on experience in cases that have occurred before with the highest similarity. CBR is used to search for practical work titles. TF-IDF is applied to process the vectorization of each practical work title word and Cosine Similarity for the calculation of similarity values. This system can search either in the form of titles or keywords. The output of the system is the title of practical work and the match value of each title. Based on the test results using 705 practical work titles, testing was carried out with five titles and carried out in two stages. The first stage searches with existing titles and the second stage randomizes the title from the first stage. And the results obtained in the second stage are the same number of titles found and the highest average match score.

</details>


### [25] [MCP-Bench: Benchmarking Tool-Using LLM Agents with Complex Real-World Tasks via MCP Servers](https://arxiv.org/abs/2508.20453)

*Zhenting Wang, Qi Chang, Hemani Patel, Shashank Biju, Cheng-En Wu, Quan Liu, Aolin Ding, Alireza Rezazadeh, Ankit Shah, Yujia Bao, Eugene Siow*

**Main category:** cs.CL

**Keywords:** MCP-Bench, large language models, benchmark, tool use, task evaluation

**Relevance Score:** 9

**TL;DR:** MCP-Bench is a benchmark for assessing LLMs on complex tasks that require tool use, coordination, and reasoning across domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a realistic benchmark to evaluate LLM performances on multi-step tasks involving various tools and planning capabilities.

**Method:** Development of MCP-Bench that connects LLMs to multiple MCP servers with a suite of complementary tools to perform complex, multi-step tasks.

**Key Contributions:**

	1. Introduction of a novel benchmark for LLMs addressing multi-step tasks
	2. Integration of multiple tools across domains for comprehensive task evaluation
	3. Establishment of a new evaluation framework focusing on planning and tool usage.

**Result:** Experiments on 20 advanced LLMs demonstrate ongoing challenges in utilizing the MCP-Bench effectively, highlighting performance issues across various tasks.

**Limitations:** MCP-Bench exposes persistent challenges in LLM performance that are not fully addressed through existing benchmarks.

**Conclusion:** MCP-Bench provides a more authentic assessment of LLM capabilities than previous benchmarks but reveals significant challenges in LLM task execution.

**Abstract:** We introduce MCP-Bench, a benchmark for evaluating large language models (LLMs) on realistic, multi-step tasks that demand tool use, cross-tool coordination, precise parameter control, and planning/reasoning for solving tasks. Built on the Model Context Protocol (MCP), MCP-Bench connects LLMs to 28 representative live MCP servers spanning 250 tools across domains such as finance, traveling, scientific computing, and academic search. Unlike prior API-based benchmarks, each MCP server provides a set of complementary tools designed to work together, enabling the construction of authentic, multi-step tasks with rich input-output coupling. Tasks in MCP-Bench test agents' ability to retrieve relevant tools from fuzzy instructions without explicit tool names, plan multi-hop execution trajectories for complex objectives, ground responses in intermediate tool outputs, and orchestrate cross-domain workflows - capabilities not adequately evaluated by existing benchmarks that rely on explicit tool specifications, shallow few-step workflows, and isolated domain operations. We propose a multi-faceted evaluation framework covering tool-level schema understanding and usage, trajectory-level planning, and task completion. Experiments on 20 advanced LLMs reveal persistent challenges in MCP-Bench. Code and data: https://github.com/Accenture/mcp-bench.

</details>


### [26] [Prediction of mortality and resource utilization in critical care: a deep learning approach using multimodal electronic health records with natural language processing techniques](https://arxiv.org/abs/2508.20460)

*Yucheng Ruan, Xiang Lan, Daniel J. Tan, Hairil Rizal Abdullah, Mengling Feng*

**Main category:** cs.CL

**Keywords:** mortality prediction, resource utilization, EHR, deep learning, natural language processing

**Relevance Score:** 9

**TL;DR:** A deep learning framework leveraging NLP techniques for predicting mortality and resource utilization from multimodal EHRs, showing improved performance against traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the prediction of mortality and resource utilization in ICUs by utilizing both structured and free-text clinical data from EHRs.

**Method:** Developed and evaluated a deep learning model on two EHR datasets, focusing on three clinical tasks and conducting an ablation study on key components of the model.

**Key Contributions:**

	1. Introduced a novel deep learning framework integrating multimodal EHR data.
	2. Demonstrated improved performance in mortality and resource utilization predictions.
	3. Highlighted the effectiveness of prompt learning with a transformer encoder on clinical tasks.

**Result:** The model improved performance metrics for mortality prediction by 1.6%/0.8% on BACC/AUROC, LOS prediction by 0.5%/2.2% on RMSE/MAE, and surgical duration estimation by 10.9%/11.0% on RMSE/MAE compared to existing best methods.

**Limitations:** 

**Conclusion:** The framework is effective and resilient in predicting outcomes from multimodal EHRs, demonstrating the advantages of integrating natural language processing techniques and prompt learning.

**Abstract:** Background Predicting mortality and resource utilization from electronic health records (EHRs) is challenging yet crucial for optimizing patient outcomes and managing costs in intensive care unit (ICU). Existing approaches predominantly focus on structured EHRs, often ignoring the valuable clinical insights in free-text notes. Additionally, the potential of textual information within structured data is not fully leveraged. This study aimed to introduce and assess a deep learning framework using natural language processing techniques that integrates multimodal EHRs to predict mortality and resource utilization in critical care settings. Methods Utilizing two real-world EHR datasets, we developed and evaluated our model on three clinical tasks with leading existing methods. We also performed an ablation study on three key components in our framework: medical prompts, free-texts, and pre-trained sentence encoder. Furthermore, we assessed the model's robustness against the corruption in structured EHRs. Results Our experiments on two real-world datasets across three clinical tasks showed that our proposed model improved performance metrics by 1.6\%/0.8\% on BACC/AUROC for mortality prediction, 0.5%/2.2% on RMSE/MAE for LOS prediction, 10.9%/11.0% on RMSE/MAE for surgical duration estimation compared to the best existing methods. It consistently demonstrated superior performance compared to other baselines across three tasks at different corruption rates. Conclusions The proposed framework is an effective and accurate deep learning approach for predicting mortality and resource utilization in critical care. The study also highlights the success of using prompt learning with a transformer encoder in analyzing multimodal EHRs. Importantly, the model showed strong resilience to data corruption within structured data, especially at high corruption levels.

</details>


### [27] [ConspirED: A Dataset for Cognitive Traits of Conspiracy Theories and Large Language Model Safety](https://arxiv.org/abs/2508.20468)

*Luke Bates, Max Glockner, Preslav Nakov, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** conspiracy theories, cognitive traits, LLM robustness, misinformation, dataset

**Relevance Score:** 7

**TL;DR:** This paper introduces ConspirED, a dataset for analyzing the cognitive traits of conspiracy theories and evaluates LLM performance against conspiratorial content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Conspiracy theories undermine trust in science, making it crucial to understand their structure and the impact of AI-generated misinformation.

**Method:** ConspirED is created by annotating excerpts from conspiracy articles using the CONSPIR cognitive framework to identify traits of conspiratorial ideation and test LLM robustness.

**Key Contributions:**

	1. Introduction of the ConspirED dataset for conspiratorial content
	2. Development of models to identify conspiratorial traits
	3. Evaluation of LLM robustness against conspiracy-related inputs

**Result:** The computational models identify key conspiratorial traits and reveal that LLMs reflect conspiratorial reasoning patterns in their outputs even when countering misinformation.

**Limitations:** The study may be limited by the dataset's representativeness of all conspiratorial content and the potential biases in LLM responses.

**Conclusion:** Understanding conspiratorial ideation through ConspirED and evaluating LLMs can help mitigate the spread of AI-generated misinformation.

**Abstract:** Conspiracy theories erode public trust in science and institutions while resisting debunking by evolving and absorbing counter-evidence. As AI-generated misinformation becomes increasingly sophisticated, understanding rhetorical patterns in conspiratorial content is important for developing interventions such as targeted prebunking and assessing AI vulnerabilities. We introduce ConspirED (CONSPIR Evaluation Dataset), which captures the cognitive traits of conspiratorial ideation in multi-sentence excerpts (80--120 words) from online conspiracy articles, annotated using the CONSPIR cognitive framework (Lewandowsky and Cook, 2020). ConspirED is the first dataset of conspiratorial content annotated for general cognitive traits. Using ConspirED, we (i) develop computational models that identify conspiratorial traits and determine dominant traits in text excerpts, and (ii) evaluate large language/reasoning model (LLM/LRM) robustness to conspiratorial inputs. We find that both are misaligned by conspiratorial content, producing output that mirrors input reasoning patterns, even when successfully deflecting comparable fact-checked misinformation.

</details>


### [28] [Languages Still Left Behind: Toward a Better Multilingual Machine Translation Benchmark](https://arxiv.org/abs/2508.20511)

*Chihiro Taguchi, Seng Mai, Keita Kurabe, Yusuke Sakai, Georgina Agyei, Soudabeh Eslami, David Chiang*

**Main category:** cs.CL

**Keywords:** multilingual machine translation, evaluation benchmarks, cultural bias, FLORES+, translation quality

**Relevance Score:** 7

**TL;DR:** This paper critiques the FLORES+ multilingual machine translation benchmark for its inadequacies in evaluation, particularly due to domain-specific and culturally biased source texts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of FLORES+ in evaluating multilingual machine translation systems across various languages and contexts.

**Method:** The authors conducted human assessments on translations in four languages, focusing on quality and cultural bias, and analyzed performance metrics using alternative evaluation sets.

**Key Contributions:**

	1. Critique of FLORES+ benchmark evaluation practices
	2. Demonstration of vulnerable scoring methodologies with heuristics
	3. Recommendation for culturally neutral evaluation standards

**Result:** Human evaluations showed many translations failed to meet the claimed quality standards, and high-quality models struggled with FLORES+, emphasizing the benchmark's flaws.

**Limitations:** The study is limited to four specific languages and may not generalize across all multilingual scenarios.

**Conclusion:** The study calls for improved multilingual MT benchmarks that are culturally neutral and less reliant on named entities to better represent real-world translation challenges.

**Abstract:** Multilingual machine translation (MT) benchmarks play a central role in evaluating the capabilities of modern MT systems. Among them, the FLORES+ benchmark is widely used, offering English-to-many translation data for over 200 languages, curated with strict quality control protocols. However, we study data in four languages (Asante Twi, Japanese, Jinghpaw, and South Azerbaijani) and uncover critical shortcomings in the benchmark's suitability for truly multilingual evaluation. Human assessments reveal that many translations fall below the claimed 90% quality standard, and the annotators report that source sentences are often too domain-specific and culturally biased toward the English-speaking world. We further demonstrate that simple heuristics, such as copying named entities, can yield non-trivial BLEU scores, suggesting vulnerabilities in the evaluation protocol. Notably, we show that MT models trained on high-quality, naturalistic data perform poorly on FLORES+ while achieving significant gains on our domain-relevant evaluation set. Based on these findings, we advocate for multilingual MT benchmarks that use domain-general and culturally neutral source texts rely less on named entities, in order to better reflect real-world translation challenges.

</details>


### [29] [SciTopic: Enhancing Topic Discovery in Scientific Literature through Advanced LLM](https://arxiv.org/abs/2508.20514)

*Pengjiang Li, Zaitian Wang, Xinhao Zhang, Ran Zhang, Lu Jiang, Pengfei Wang, Yuanchun Zhou*

**Main category:** cs.CL

**Keywords:** Topic Discovery, Large Language Models, Scientific Publications

**Relevance Score:** 9

**TL;DR:** The paper proposes a new method for topic discovery in scientific literature using large language models (LLMs) to improve the identification of research topics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide better tools for researchers to identify emerging trends and enhance scientific information retrieval through improved topic discovery methods.

**Method:** The proposed method, SciTopic, utilizes a textual encoder to analyze scientific publications, coupled with an entropy-based sampling module and triplet tasks guided by LLMs to optimize thematic relevance.

**Key Contributions:**

	1. Introduction of SciTopic, an LLM-enhanced topic discovery method
	2. Novel integration of entropy-based sampling and triplet tasks
	3. Demonstrated superior performance compared to current state-of-the-art methods

**Result:** Extensive experiments show that SciTopic significantly outperforms existing state-of-the-art methods in scientific topic discovery.

**Limitations:** 

**Conclusion:** The integration of LLMs in SciTopic enhances the precision of topic identification in scientific literature, facilitating faster and deeper insights for researchers.

**Abstract:** Topic discovery in scientific literature provides valuable insights for researchers to identify emerging trends and explore new avenues for investigation, facilitating easier scientific information retrieval. Many machine learning methods, particularly deep embedding techniques, have been applied to discover research topics. However, most existing topic discovery methods rely on word embedding to capture the semantics and lack a comprehensive understanding of scientific publications, struggling with complex, high-dimensional text relationships. Inspired by the exceptional comprehension of textual information by large language models (LLMs), we propose an advanced topic discovery method enhanced by LLMs to improve scientific topic identification, namely SciTopic. Specifically, we first build a textual encoder to capture the content from scientific publications, including metadata, title, and abstract. Next, we construct a space optimization module that integrates entropy-based sampling and triplet tasks guided by LLMs, enhancing the focus on thematic relevance and contextual intricacies between ambiguous instances. Then, we propose to fine-tune the textual encoder based on the guidance from the LLMs by optimizing the contrastive loss of the triplets, forcing the text encoder to better discriminate instances of different topics. Finally, extensive experiments conducted on three real-world datasets of scientific publications demonstrate that SciTopic outperforms the state-of-the-art (SOTA) scientific topic discovery methods, enabling researchers to gain deeper and faster insights.

</details>


### [30] [Overview of BioASQ 2024: The twelfth BioASQ challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20532)

*Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Salvador Lima-Lpez, Eullia Farr-Maduell, Martin Krallinger, Natalia Loukachevitch, Vera Davydova, Elena Tutubalina, Georgios Paliouras*

**Main category:** cs.CL

**Keywords:** BioASQ, biomedical semantic indexing, question answering, NER, clinical entity detection

**Relevance Score:** 6

**TL;DR:** Overview of the BioASQ challenge at CLEF 2024, highlighting competitions in biomedical indexing and question answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To promote advancements in biomedical semantic indexing and question answering through new and established tasks.

**Method:** Overview of the challenge structure, tasks, and participation, highlighting both established and new tasks in clinical entity detection and NER.

**Key Contributions:**

	1. Introduction of MultiCardioNER for clinical entity detection in cardiology
	2. Launch of BIONNE for nested NER in Russian and English
	3. Demonstrated competitive performance across multiple teams and tasks

**Result:** 37 teams participated with over 700 submissions, showing competitive performance across tasks, indicating advancements in the field.

**Limitations:** 

**Conclusion:** The BioASQ challenge continues to evolve, showcasing improvements in biomedical AI through international collaboration.

**Abstract:** This is an overview of the twelfth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2024. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks b and Synergy, and two new tasks: a) MultiCardioNER on the adaptation of clinical entity detection to the cardiology domain in a multilingual setting, and b) BIONNE on nested NER in Russian and English. In this edition of BioASQ, 37 competing teams participated with more than 700 distinct submissions in total for the four different shared tasks of the challenge. Similarly to previous editions, most of the participating systems achieved competitive performance, suggesting the continuous advancement of the state-of-the-art in the field.

</details>


### [31] [Overview of BioASQ 2025: The Thirteenth BioASQ Challenge on Large-Scale Biomedical Semantic Indexing and Question Answering](https://arxiv.org/abs/2508.20554)

*Anastasios Nentidis, Georgios Katsimpras, Anastasia Krithara, Martin Krallinger, Miguel Rodrguez-Ortega, Eduard Rodriguez-Lpez, Natalia Loukachevitch, Andrey Sakhovskiy, Elena Tutubalina, Dimitris Dimitriadis, Grigorios Tsoumakas, George Giannakoulas, Alexandra Bekiaridou, Athanasios Samaras, Giorgio Maria Di Nunzio, Nicola Ferro, Stefano Marchesin, Marco Martinelli, Gianmaria Silvello, Georgios Paliouras*

**Main category:** cs.CL

**Keywords:** BioASQ, biomedical semantic indexing, question answering, multilingual clinical summarization, information extraction

**Relevance Score:** 6

**TL;DR:** Overview of the thirteenth BioASQ challenge focusing on biomedical semantic indexing and question answering, highlighting new tasks and participation metrics.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To promote advances in large-scale biomedical semantic indexing and question answering through international challenges.

**Method:** The challenge included established tasks and introduced four new tasks focusing on multilingual summarization, nested named entity linking, clinical coding, and information extraction from gut-brain interplay, with 83 teams participating.

**Key Contributions:**

	1. Introduction of four new tasks related to multilingual clinical summarization and information extraction.
	2. High participation with 83 teams and over 1000 submissions.
	3. Demonstration of competitive performances by participating systems.

**Result:** More than 1000 distinct submissions were made across six challenges, with several systems achieving competitive performance.

**Limitations:** 

**Conclusion:** The results indicate continuous advancement in the state-of-the-art in biomedical question answering systems.

**Abstract:** This is an overview of the thirteenth edition of the BioASQ challenge in the context of the Conference and Labs of the Evaluation Forum (CLEF) 2025. BioASQ is a series of international challenges promoting advances in large-scale biomedical semantic indexing and question answering. This year, BioASQ consisted of new editions of the two established tasks, b and Synergy, and four new tasks: a) Task MultiClinSum on multilingual clinical summarization. b) Task BioNNE-L on nested named entity linking in Russian and English. c) Task ELCardioCC on clinical coding in cardiology. d) Task GutBrainIE on gut-brain interplay information extraction. In this edition of BioASQ, 83 competing teams participated with more than 1000 distinct submissions in total for the six different shared tasks of the challenge. Similar to previous editions, several participating systems achieved competitive performance, indicating the continuous advancement of the state-of-the-art in the field.

</details>


### [32] [Adaptive Federated Distillation for Multi-Domain Non-IID Textual Data](https://arxiv.org/abs/2508.20557)

*Jiahao Xiao, Jiangming Liu*

**Main category:** cs.CL

**Keywords:** federated learning, non-IID data, language models, benchmarking framework, privacy-preserving

**Relevance Score:** 8

**TL;DR:** This paper introduces a benchmarking framework for multi-domain non-IID scenarios in federated learning and proposes the Adaptive Federated Distillation (AdaFD) method to enhance model performance in diverse data environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of non-IID data in federated learning, addressing both label diversity and input diversity in natural language processing.

**Method:** A unified benchmarking framework is proposed to evaluate federated learning with a focus on multi-domain non-IID scenarios, complemented by the AdaFD framework for improving model performance.

**Key Contributions:**

	1. Introduction of a unified benchmarking framework for multi-domain non-IID scenarios in federated learning.
	2. Development of Adaptive Federated Distillation (AdaFD) to enhance performance with diverse data.
	3. Demonstration of improved model effectiveness in both homogeneous and heterogeneous settings.

**Result:** The AdaFD framework successfully addresses multi-domain non-IID challenges and outperforms existing methods by effectively capturing diversity among local clients.

**Limitations:** 

**Conclusion:** Adopting the proposed benchmarking framework and AdaFD method leads to improved performance and better evaluation of federated learning frameworks in real environments.

**Abstract:** The widespread success of pre-trained language models has established a new training paradigm, where a global PLM is fine-tuned using task-specific data from local clients. The local data are highly different from each other and can not capture the global distribution of the whole data in real world. To address the challenges of non-IID data in real environments, privacy-preserving federated distillation has been proposed and highly investigated. However, previous experimental non-IID scenarios are primarily identified with the label (output) diversity, without considering the diversity of language domains (input) that is crucial in natural language processing. In this paper, we introduce a comprehensive set of multi-domain non-IID scenarios and propose a unified benchmarking framework that includes diverse data. The benchmark can be used to evaluate the federated learning framework in a real environment. To this end, we propose an Adaptive Federated Distillation (AdaFD) framework designed to address multi-domain non-IID challenges in both homogeneous and heterogeneous settings. Experimental results demonstrate that our models capture the diversity of local clients and achieve better performance compared to the existing works. The code for this paper is available at: https://github.com/jiahaoxiao1228/AdaFD.

</details>


### [33] [Leveraging Generative Models for Real-Time Query-Driven Text Summarization in Large-Scale Web Search](https://arxiv.org/abs/2508.20559)

*Zeyu Xiong, Yixuan Nan, Li Gao, Hengzhu Tang, Shuaiqiang Wang, Junfeng Wang, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Query-Driven Text Summarization, Generative Models, Web Search

**Relevance Score:** 7

**TL;DR:** This study presents a novel framework for Query-Driven Text Summarization (QDTS) that leverages generative models to enhance user engagement in web search by generating concise summaries based on user queries.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve user engagement and facilitate rapid decision-making by generating informative summaries aligned with user queries, addressing the limitations of traditional extractive models.

**Method:** The proposed approach involves a framework that utilizes large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to create an efficient QDTS model with only 0.1B parameters.

**Key Contributions:**

	1. Introduction of a generative model framework for QDTS
	2. Demonstration of state-of-the-art performance against traditional models
	3. High deployment efficiency with minimal resource usage

**Result:** The framework outperforms existing production baselines on multiple industry-relevant metrics, achieving state-of-the-art performance while maintaining high deployment efficiency with low resource requirements.

**Limitations:** 

**Conclusion:** The study concludes that the generative model-based approach greatly enhances QDTS capabilities in industrial web search settings, overcoming key limitations of traditional models.

**Abstract:** In the dynamic landscape of large-scale web search, Query-Driven Text Summarization (QDTS) aims to generate concise and informative summaries from textual documents based on a given query, which is essential for improving user engagement and facilitating rapid decision-making. Traditional extractive summarization models, based primarily on ranking candidate summary segments, have been the dominant approach in industrial applications. However, these approaches suffer from two key limitations: 1) The multi-stage pipeline often introduces cumulative information loss and architectural bottlenecks due to its weakest component; 2) Traditional models lack sufficient semantic understanding of both user queries and documents, particularly when dealing with complex search intents. In this study, we propose a novel framework to pioneer the application of generative models to address real-time QDTS in industrial web search. Our approach integrates large model distillation, supervised fine-tuning, direct preference optimization, and lookahead decoding to transform a lightweight model with only 0.1B parameters into a domain-specialized QDTS expert. Evaluated on multiple industry-relevant metrics, our model outperforms the production baseline and achieves a new state of the art. Furthermore, it demonstrates excellent deployment efficiency, requiring only 334 NVIDIA L20 GPUs to handle \textasciitilde50,000 queries per second under 55~ms average latency per query.

</details>


### [34] [KCS: Diversify Multi-hop Question Generation with Knowledge Composition Sampling](https://arxiv.org/abs/2508.20567)

*Yangfan Wang, Jie Liu, Chen Tang, Lian Yan, Jingchi Jiang*

**Main category:** cs.CL

**Keywords:** multi-hop question answering, knowledge composition, probabilistic contrastive loss, data augmentation, stochastic decoding

**Relevance Score:** 7

**TL;DR:** This paper presents Knowledge Composition Sampling (KCS), a framework for enhancing multi-hop question answering by improving the diversity of generated questions through varied knowledge compositions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in multi-hop question answering caused by data sparsity and the tendency of language models to learn spurious patterns, aiming to improve question generation by integrating essential knowledge.

**Method:** KCS treats knowledge composition selection as a sentence-level conditional prediction task and employs a probabilistic contrastive loss function to enhance the relevance of generated knowledge pieces. It uses a stochastic decoding strategy during inference to maintain a balance between accuracy and diversity.

**Key Contributions:**

	1. Introduction of the Knowledge Composition Sampling framework for multi-hop question answering.
	2. Use of probabilistic contrastive loss for selecting relevant knowledge compositions.
	3. Implementation of a stochastic decoding strategy for balancing accuracy and diversity.

**Result:** KCS improves the accuracy of knowledge composition selection by 3.9% compared to competitive baselines and shows effective data augmentation results on HotpotQA and 2WikiMultihopQA datasets.

**Limitations:** 

**Conclusion:** The proposed KCS framework successfully enhances the generation of diverse multi-hop questions and demonstrates good performance in data augmentation for multi-hop question answering benchmarks.

**Abstract:** Multi-hop question answering faces substantial challenges due to data sparsity, which increases the likelihood of language models learning spurious patterns. To address this issue, prior research has focused on diversifying question generation through content planning and varied expression. However, these approaches often emphasize generating simple questions and neglect the integration of essential knowledge, such as relevant sentences within documents. This paper introduces the Knowledge Composition Sampling (KCS), an innovative framework designed to expand the diversity of generated multi-hop questions by sampling varied knowledge compositions within a given context. KCS models the knowledge composition selection as a sentence-level conditional prediction task and utilizes a probabilistic contrastive loss to predict the next most relevant piece of knowledge. During inference, we employ a stochastic decoding strategy to effectively balance accuracy and diversity. Compared to competitive baselines, our KCS improves the overall accuracy of knowledge composition selection by 3.9%, and its application for data augmentation yields improvements on HotpotQA and 2WikiMultihopQA datasets. Our code is available at: https://github.com/yangfanww/kcs.

</details>


### [35] [A Graph Talks, But Who's Listening? Rethinking Evaluations for Graph-Language Models](https://arxiv.org/abs/2508.20583)

*Soham Petkar, Hari Aakash K, Anirudh Vempati, Akshit Sinha, Ponnurangam Kumarauguru, Chirag Agarwal*

**Main category:** cs.CL

**Keywords:** Graph-Language Models, Graph Neural Networks, Multimodal Reasoning

**Relevance Score:** 8

**TL;DR:** The paper critiques current evaluation benchmarks for Graph-Language Models (GLMs) and introduces the CLEGR benchmark for multimodal reasoning assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacy of existing benchmarks in evaluating GLMs' ability to perform multimodal reasoning that combines graph structure and language semantics.

**Method:** Introduction of the CLEGR benchmark with synthetic graph generation and reasoning questions, followed by evaluations of various GLM architectures.

**Key Contributions:**

	1. Introduction of the CLEGR benchmark for multimodal reasoning
	2. Analysis of GLM architectures against soft-prompted LLM performance
	3. Identification of limitations in GLMs' graph reasoning capabilities

**Result:** GLM architectures do not significantly outperform soft-prompted LLMs, indicating that the graph structure may not be necessary for high performance in certain tasks.

**Limitations:** Evaluation primarily focused on specific architectures and may not encompass all types of GLMs.

**Conclusion:** Current GLM models struggle with structural reasoning and may not need integration of graph structures with LLMs, revealing potential limitations in graph-based reasoning capabilities.

**Abstract:** Developments in Graph-Language Models (GLMs) aim to integrate the structural reasoning capabilities of Graph Neural Networks (GNNs) with the semantic understanding of Large Language Models (LLMs). However, we demonstrate that current evaluation benchmarks for GLMs, which are primarily repurposed node-level classification datasets, are insufficient to assess multimodal reasoning. Our analysis reveals that strong performance on these benchmarks is achievable using unimodal information alone, suggesting that they do not necessitate graph-language integration. To address this evaluation gap, we introduce the CLEGR(Compositional Language-Graph Reasoning) benchmark, designed to evaluate multimodal reasoning at various complexity levels. Our benchmark employs a synthetic graph generation pipeline paired with questions that require joint reasoning over structure and textual semantics. We perform a thorough evaluation of representative GLM architectures and find that soft-prompted LLM baselines perform on par with GLMs that incorporate a full GNN backbone. This result calls into question the architectural necessity of incorporating graph structure into LLMs. We further show that GLMs exhibit significant performance degradation in tasks that require structural reasoning. These findings highlight limitations in the graph reasoning capabilities of current GLMs and provide a foundation for advancing the community toward explicit multimodal reasoning involving graph structure and language.

</details>


### [36] [Generative Annotation for ASR Named Entity Correction](https://arxiv.org/abs/2508.20700)

*Yuanchang Luo, Daimeng Wei, Shaojun Li, Hengchao Shang, Jiaxin Guo, Zongyao Li, Zhanglin Wu, Xiaoyu Chen, Zhiqiang Rao, Jinlong Yang, Hao Yang*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, named entity correction, speech features, generative method, entity accuracy

**Relevance Score:** 7

**TL;DR:** This paper presents a novel named entity correction (NEC) method leveraging speech sound features to improve transcription accuracy of domain-specific entities in automatic speech recognition systems.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** End-to-end automatic speech recognition systems often misinterpret domain-specific named entities, which leads to failures in subsequent tasks.

**Method:** The proposed NEC method utilizes speech sound features to identify candidate entities and employs a generative approach to annotate and replace incorrect entities in ASR transcripts.

**Key Contributions:**

	1. Introduction of a novel NEC method leveraging speech sound features
	2. Generative approach for annotating and correcting entity errors in ASR transcripts
	3. Open-sourcing of self-constructed test set and training data

**Result:** The results show that the new NEC method significantly improves entity accuracy compared to traditional models, especially in cases where the wrongly transcribed words differ greatly from the correct entities.

**Limitations:** 

**Conclusion:** This method represents a significant advancement in the correction of entity recognition in ASR systems, with open-source datasets to aid further research.

**Abstract:** End-to-end automatic speech recognition systems often fail to transcribe domain-specific named entities, causing catastrophic failures in downstream tasks. Numerous fast and lightweight named entity correction (NEC) models have been proposed in recent years. These models, mainly leveraging phonetic-level edit distance algorithms, have shown impressive performances. However, when the forms of the wrongly-transcribed words(s) and the ground-truth entity are significantly different, these methods often fail to locate the wrongly transcribed words in hypothesis, thus limiting their usage. We propose a novel NEC method that utilizes speech sound features to retrieve candidate entities. With speech sound features and candidate entities, we inovatively design a generative method to annotate entity errors in ASR transcripts and replace the text with correct entities. This method is effective in scenarios of word form difference. We test our method using open-source and self-constructed test sets. The results demonstrate that our NEC method can bring significant improvement to entity accuracy. We will open source our self-constructed test set and training data.

</details>


### [37] [Multi-Lingual Implicit Discourse Relation Recognition with Multi-Label Hierarchical Learning](https://arxiv.org/abs/2508.20712)

*Nelson Filipe Costa, Leila Kosseim*

**Main category:** cs.CL

**Keywords:** implicit discourse relations, multi-lingual classification, hierarchical dependencies, fine-tuning, discourse sense prediction

**Relevance Score:** 8

**TL;DR:** Introduction of HArch, a multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR), demonstrating superiority over existing LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for effective implicit discourse relation recognition across multiple languages and aims to improve the performance of discourse relation classification using a hierarchical approach.

**Method:** HArch leverages hierarchical dependencies between discourse senses to predict probability distributions in the PDTB 3.0 framework, evaluated on the DiscoGeM 2.0 corpus, compared to various encoder backbones and evaluated against LLMs.

**Key Contributions:**

	1. First multi-lingual and multi-label classification model for IDRR
	2. Hierarchical approach for improving discourse sense prediction
	3. Demonstrated superior performance compared to GPT-4o and Llama-4-Maverick

**Result:** RoBERTa-HArch performs best in English, while XLM-RoBERTa-HArch excels in the multi-lingual setting; the model consistently outperforms GPT-4o and Llama-4-Maverick in few-shot prompting.

**Limitations:** 

**Conclusion:** The hierarchical approach of HArch leads to state-of-the-art results on the DiscoGeM 1.0 corpus, showcasing the advantages of task-specific fine-tuning over general LLM prompting for IDRR.

**Abstract:** This paper introduces the first multi-lingual and multi-label classification model for implicit discourse relation recognition (IDRR). Our model, HArch, is evaluated on the recently released DiscoGeM 2.0 corpus and leverages hierarchical dependencies between discourse senses to predict probability distributions across all three sense levels in the PDTB 3.0 framework. We compare several pre-trained encoder backbones and find that RoBERTa-HArch achieves the best performance in English, while XLM-RoBERTa-HArch performs best in the multi-lingual setting. In addition, we compare our fine-tuned models against GPT-4o and Llama-4-Maverick using few-shot prompting across all language configurations. Our results show that our fine-tuned models consistently outperform these LLMs, highlighting the advantages of task-specific fine-tuning over prompting in IDRR. Finally, we report SOTA results on the DiscoGeM 1.0 corpus, further validating the effectiveness of our hierarchical approach.

</details>


### [38] [Addressing Tokenization Inconsistency in Steganography and Watermarking Based on Large Language Models](https://arxiv.org/abs/2508.20718)

*Ruiyi Yan, Yugo Murawaki*

**Main category:** cs.CL

**Keywords:** tokenization inconsistency, steganography, watermarking, text generation, large language models

**Relevance Score:** 7

**TL;DR:** The study addresses tokenization inconsistency in text-based steganography and watermarking, proposing solutions to improve robustness against misuse.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of text-based steganography and watermarking by addressing tokenization inconsistency (TI) which undermines robustness.

**Method:** Two tailored methods are proposed: a stepwise verification method for steganography and a post-hoc rollback method for watermarking to eliminate TI.

**Key Contributions:**

	1. Proposed solutions for eliminating tokenization inconsistency in steganography and watermarking.
	2. Demonstrated improvements in various metrics for both steganography and watermarking through experiments.
	3. Identified key characteristics of problematic tokens that lead to tokenization inconsistency.

**Result:** Experiments show that directly addressing TI improves fluency, imperceptibility, and anti-steganalysis capacity in steganography, and enhances detectability and robustness in watermarking.

**Limitations:** 

**Conclusion:** The findings highlight the critical nature of addressing tokenization inconsistency to strengthen steganography and watermarking techniques.

**Abstract:** Large language models have significantly enhanced the capacities and efficiency of text generation. On the one hand, they have improved the quality of text-based steganography. On the other hand, they have also underscored the importance of watermarking as a safeguard against malicious misuse. In this study, we focus on tokenization inconsistency (TI) between Alice and Bob in steganography and watermarking, where TI can undermine robustness. Our investigation reveals that the problematic tokens responsible for TI exhibit two key characteristics: infrequency and temporariness. Based on these findings, we propose two tailored solutions for TI elimination: a stepwise verification method for steganography and a post-hoc rollback method for watermarking. Experiments show that (1) compared to traditional disambiguation methods in steganography, directly addressing TI leads to improvements in fluency, imperceptibility, and anti-steganalysis capacity; (2) for watermarking, addressing TI enhances detectability and robustness against attacks.

</details>


### [39] [rStar2-Agent: Agentic Reasoning Technical Report](https://arxiv.org/abs/2508.20722)

*Ning Shang, Yifei Liu, Yi Zhu, Li Lyna Zhang, Weijiang Xu, Xinyu Guan, Buze Zhang, Bingcheng Dong, Xudong Zhou, Bowen Zhang, Ying Xin, Ziming Miao, Scarlett Li, Fan Yang, Mao Yang*

**Main category:** cs.CL

**Keywords:** agentic reinforcement learning, math reasoning, Python coding tools, cognitive abilities, machine learning

**Relevance Score:** 8

**TL;DR:** rStar2-Agent is a 14B math reasoning model that employs agentic reinforcement learning for enhanced cognitive performance in problem-solving and coding tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to achieve frontier-level performance in complex problem-solving and coding by leveraging agentic reinforcement learning.

**Method:** The model utilizes a new RL infrastructure with a reliable Python environment, the GRPO-RoC algorithm for effective reasoning in noisy coding environments, and a phased agent training approach.

**Key Contributions:**

	1. Introduced agentic RL infrastructure for coding tasks
	2. Developed GRPO-RoC for effective reasoning amid noisy environments
	3. Achieved state-of-the-art performance with efficient training methodology

**Result:** rStar2-Agent achieved state-of-the-art performance with an average pass@1 score of 80.6% on AIME24 and 69.8% on AIME25 in just 510 RL steps in one week.

**Limitations:** 

**Conclusion:** The model demonstrates strong generalization beyond mathematics to alignment and scientific reasoning tasks, with significant improvements over larger models.

**Abstract:** We introduce rStar2-Agent, a 14B math reasoning model trained with agentic reinforcement learning to achieve frontier-level performance. Beyond current long CoT, the model demonstrates advanced cognitive behaviors, such as thinking carefully before using Python coding tools and reflecting on code execution feedback to autonomously explore, verify, and refine intermediate steps in complex problem-solving. This capability is enabled through three key innovations that makes agentic RL effective at scale: (i) an efficient RL infrastructure with a reliable Python code environment that supports high-throughput execution and mitigates the high rollout costs, enabling training on limited GPU resources (64 MI300X GPUs); (ii) GRPO-RoC, an agentic RL algorithm with a Resample-on-Correct rollout strategy that addresses the inherent environment noises from coding tools, allowing the model to reason more effectively in a code environment; (iii) An efficient agent training recipe that starts with non-reasoning SFT and progresses through multi-RL stages, yielding advanced cognitive abilities with minimal compute cost. To this end, rStar2-Agent boosts a pre-trained 14B model to state of the art in only 510 RL steps within one week, achieving average pass@1 scores of 80.6% on AIME24 and 69.8% on AIME25, surpassing DeepSeek-R1 (671B) with significantly shorter responses. Beyond mathematics, rStar2-Agent-14B also demonstrates strong generalization to alignment, scientific reasoning, and agentic tool-use tasks. Code and training recipes are available at https://github.com/microsoft/rStar.

</details>


### [40] [Leveraging Semantic Triples for Private Document Generation with Local Differential Privacy Guarantees](https://arxiv.org/abs/2508.20736)

*Stephen Meisenbacher, Maulik Chevli, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Differential Privacy, Natural Language Processing, Semantic Triples, Document Generation, LLM Post-Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces DP-ST, a method for generating private documents under local Differential Privacy (DP) guarantees using semantic triples, balancing privacy and utility.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of transforming text under Differential Privacy while maintaining high coherence and utility, especially at lower  values.

**Method:** DP-ST leverages semantic triples for neighborhood-aware private document generation combined with LLM post-processing to ensure coherent text generation.

**Key Contributions:**

	1. Introduction of a novel method (DP-ST) for document generation under local DP
	2. Demonstration of coherent text generation at lower  values through semantic triples
	3. Evaluation of the divide-and-conquer paradigm in the context of privacy and utility.

**Result:** DP-ST demonstrates effective text generation that maintains coherence and balances privacy even at lower  values, highlighting the significance of neighborhood-aware strategies.

**Limitations:** The performance of DP-ST may still depend on the specific contexts or types of documents being processed.

**Conclusion:** The findings suggest that by focusing on privatization neighborhoods, it is possible to achieve reasonable levels of privacy without sacrificing the quality of generated text.

**Abstract:** Many works at the intersection of Differential Privacy (DP) in Natural Language Processing aim to protect privacy by transforming texts under DP guarantees. This can be performed in a variety of ways, from word perturbations to full document rewriting, and most often under local DP. Here, an input text must be made indistinguishable from any other potential text, within some bound governed by the privacy parameter $\varepsilon$. Such a guarantee is quite demanding, and recent works show that privatizing texts under local DP can only be done reasonably under very high $\varepsilon$ values. Addressing this challenge, we introduce DP-ST, which leverages semantic triples for neighborhood-aware private document generation under local DP guarantees. Through the evaluation of our method, we demonstrate the effectiveness of the divide-and-conquer paradigm, particularly when limiting the DP notion (and privacy guarantees) to that of a privatization neighborhood. When combined with LLM post-processing, our method allows for coherent text generation even at lower $\varepsilon$ values, while still balancing privacy and utility. These findings highlight the importance of coherence in achieving balanced privatization outputs at reasonable $\varepsilon$ levels.

</details>


### [41] [Specializing General-purpose LLM Embeddings for Implicit Hate Speech Detection across Datasets](https://arxiv.org/abs/2508.20750)

*Vassiliy Cheremetiev, Quang Long Ho Ngo, Chau Ying Kot, Alina Elena Baia, Andrea Cavallaro*

**Main category:** cs.CL

**Keywords:** implicit hate speech, large language models, embedding models, fine-tuning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper presents a method for detecting implicit hate speech by fine-tuning general-purpose embedding models based on large language models, achieving state-of-the-art results across multiple datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The detection of implicit hate speech is crucial due to its subtlety and the challenges posed by the use of indirect language, sarcasm, and coded terminology in conveying prejudice.

**Method:** The authors fine-tune recent general-purpose embedding models, namely Stella, Jasper, NV-Embed, and E5, focusing on the incorporation of emotional and contextual information to enhance detection performance.

**Key Contributions:**

	1. Fine-tuning of LLM-based embedding models for IHS detection
	2. Demonstrated improvement in detection metrics across multiple datasets
	3. Integration of contextual and emotional data into detection pipelines

**Result:** The method achieved up to a 1.10 percentage point improvement in F1-macro score on in-dataset evaluations, and up to a 20.35 percentage point improvement in cross-dataset evaluations.

**Limitations:** 

**Conclusion:** Fine-tuning LLMs for implicit hate speech detection significantly enhances performance and sets a new standard in this area.

**Abstract:** Implicit hate speech (IHS) is indirect language that conveys prejudice or hatred through subtle cues, sarcasm or coded terminology. IHS is challenging to detect as it does not include explicit derogatory or inflammatory words. To address this challenge, task-specific pipelines can be complemented with external knowledge or additional information such as context, emotions and sentiment data. In this paper, we show that, by solely fine-tuning recent general-purpose embedding models based on large language models (LLMs), such as Stella, Jasper, NV-Embed and E5, we achieve state-of-the-art performance. Experiments on multiple IHS datasets show up to 1.10 percentage points improvements for in-dataset, and up to 20.35 percentage points improvements in cross-dataset evaluation, in terms of F1-macro score.

</details>


### [42] [GUARD: Glocal Uncertainty-Aware Robust Decoding for Effective and Efficient Open-Ended Text Generation](https://arxiv.org/abs/2508.20757)

*Yuanhao Ding, Esteban Garces Arias, Meimingwei Li, Julian Rodemann, Matthias Aenmacher, Danlu Chen, Gaojuan Fan, Christian Heumann, Chongsheng Zhang*

**Main category:** cs.CL

**Keywords:** LLM, text generation, open-ended generation, coherence, diversity

**Relevance Score:** 9

**TL;DR:** GUARD is a self-adaptive decoding method that balances coherence and diversity in LLM outputs by using a novel uncertainty-driven framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of balancing coherence and diversity in open-ended text generation with LLMs, which is often hindered by hyperparameter dependence and high computational costs.

**Method:** GUARD utilizes a 'Glocal' uncertainty-driven approach, combining global entropy estimates with local entropy deviations to manage both long-term and short-term uncertainty signals effectively.

**Key Contributions:**

	1. Introduced a novel 'Glocal' uncertainty-driven framework for decoding methods in LLMs.
	2. Achieved improvements in generation speed without sacrificing text quality.
	3. Provided theoretical guarantees of unbiasedness and consistency for the proposed method.

**Result:** Experimental results show that GUARD achieves a balance between text diversity and coherence and improves generation speed, with validation from both human and LLM evaluators.

**Limitations:** 

**Conclusion:** GUARD provides a robust solution for enhancing LLM output quality by mitigating abrupt variations in uncertainty while maintaining computational efficiency.

**Abstract:** Open-ended text generation faces a critical challenge: balancing coherence with diversity in LLM outputs. While contrastive search-based decoding strategies have emerged to address this trade-off, their practical utility is often limited by hyperparameter dependence and high computational costs. We introduce GUARD, a self-adaptive decoding method that effectively balances these competing objectives through a novel "Glocal" uncertainty-driven framework. GUARD combines global entropy estimates with local entropy deviations to integrate both long-term and short-term uncertainty signals. We demonstrate that our proposed global entropy formulation effectively mitigates abrupt variations in uncertainty, such as sudden overconfidence or high entropy spikes, and provides theoretical guarantees of unbiasedness and consistency. To reduce computational overhead, we incorporate a simple yet effective token-count-based penalty into GUARD. Experimental results demonstrate that GUARD achieves a good balance between text diversity and coherence, while exhibiting substantial improvements in generation speed. In a more nuanced comparison study across different dimensions of text quality, both human and LLM evaluators validated its remarkable performance. Our code is available at https://github.com/YecanLee/GUARD.

</details>


### [43] [Feel the Difference? A Comparative Analysis of Emotional Arcs in Real and LLM-Generated CBT Sessions](https://arxiv.org/abs/2508.20764)

*Xiaoyi Wang, Jiwei Zhang, Guangtao Zhang, Honglei Guo*

**Main category:** cs.CL

**Keywords:** Cognitive Behavioral Therapy, Large Language Models, Emotional Dynamics, Mental Health, Synthetic Dialogues

**Relevance Score:** 9

**TL;DR:** This study compares emotional dynamics in real vs. LLM-generated therapy dialogues, finding key differences in emotional properties, leading to concerns about the fidelity of synthetic data.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the emotional dynamics in therapy dialogues generated by LLMs versus those from real sessions, assessing the implications for mental health applications.

**Method:** We adapt the Utterance Emotion Dynamics framework to compare emotional arcs in real CBT dialogues with LLM-generated dialogues from the CACTUS dataset.

**Key Contributions:**

	1. First comparative analysis of emotional arcs in real vs. synthetic therapy dialogues
	2. Introduction of RealCBT, a dataset of real CBT sessions for future research
	3. Insights into the emotional fidelity deficiencies of LLM-generated therapy dialogues

**Result:** Synthetic dialogues are fluent but lack the emotional variability and authenticity found in real sessions, particularly low emotional arc similarity for clients.

**Limitations:** The analysis is limited to Cognitive Behavioral Therapy dialogues and may not generalize to other therapeutic approaches.

**Conclusion:** The findings highlight significant limitations in LLM-generated therapy data and the need for authentic emotional representation in mental health applications.

**Abstract:** Synthetic therapy dialogues generated by large language models (LLMs) are increasingly used in mental health NLP to simulate counseling scenarios, train models, and supplement limited real-world data. However, it remains unclear whether these synthetic conversations capture the nuanced emotional dynamics of real therapy. In this work, we conduct the first comparative analysis of emotional arcs between real and LLM-generated Cognitive Behavioral Therapy dialogues. We adapt the Utterance Emotion Dynamics framework to analyze fine-grained affective trajectories across valence, arousal, and dominance dimensions. Our analysis spans both full dialogues and individual speaker roles (counselor and client), using real sessions transcribed from public videos and synthetic dialogues from the CACTUS dataset. We find that while synthetic dialogues are fluent and structurally coherent, they diverge from real conversations in key emotional properties: real sessions exhibit greater emotional variability,more emotion-laden language, and more authentic patterns of reactivity and regulation. Moreover, emotional arc similarity between real and synthetic speakers is low, especially for clients. These findings underscore the limitations of current LLM-generated therapy data and highlight the importance of emotional fidelity in mental health applications. We introduce RealCBT, a curated dataset of real CBT sessions, to support future research in this space.

</details>


### [44] [Turning the Spell Around: Lightweight Alignment Amplification via Rank-One Safety Injection](https://arxiv.org/abs/2508.20766)

*Harethah Abu Shairah, Hasan Abed Al Kader Hammoud, George Turkiyyah, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** Large Language Models, Safety Alignment, Rank-One Safety Injection, Harmful Requests, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper presents Rank-One Safety Injection (ROSI), a method for enhancing safety alignment in Large Language Models by modifying weight matrices to improve refusal rates of harmful requests without extensive fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing safety mechanisms in LLMs can be bypassed, necessitating new approaches to enhance safety alignment.

**Method:** ROSI involves a simple rank-one modification of write matrices in the model's residual stream, informed by harmful and harmless instruction pairs.

**Key Contributions:**

	1. Introduction of Rank-One Safety Injection (ROSI) technique for LLM safety enhancement
	2. Proven effectiveness of ROSI in increasing safety refusal rates
	3. Demonstration of ROSI's ability to realign uncensored models with latent safety directions

**Result:** ROSI significantly increases safety refusal rates, as shown by evaluations with Llama Guard 3, while maintaining performance on standard benchmarks like MMLU and HellaSwag.

**Limitations:** 

**Conclusion:** ROSI serves as an efficient last-mile safety procedure that effectively enhances LLM safety using interpretable weight steering without the need for extensive model fine-tuning.

**Abstract:** Safety alignment in Large Language Models (LLMs) often involves mediating internal representations to refuse harmful requests. Recent research has demonstrated that these safety mechanisms can be bypassed by ablating or removing specific representational directions within the model. In this paper, we propose the opposite approach: Rank-One Safety Injection (ROSI), a white-box method that amplifies a model's safety alignment by permanently steering its activations toward the refusal-mediating subspace. ROSI operates as a simple, fine-tuning-free rank-one weight modification applied to all residual stream write matrices. The required safety direction can be computed from a small set of harmful and harmless instruction pairs. We show that ROSI consistently increases safety refusal rates - as evaluated by Llama Guard 3 - while preserving the utility of the model on standard benchmarks such as MMLU, HellaSwag, and Arc. Furthermore, we show that ROSI can also re-align 'uncensored' models by amplifying their own latent safety directions, demonstrating its utility as an effective last-mile safety procedure. Our results suggest that targeted, interpretable weight steering is a cheap and potent mechanism to improve LLM safety, complementing more resource-intensive fine-tuning paradigms.

</details>


### [45] [Signs of Struggle: Spotting Cognitive Distortions across Language and Register](https://arxiv.org/abs/2508.20771)

*Abhishek Kuber, Enrico Liscio, Ruixuan Zhang, Caroline Figueroa, Pradeep K. Murukannaiah*

**Main category:** cs.CL

**Keywords:** mental health, cognitive distortions, cross-lingual, machine learning, domain adaptation

**Relevance Score:** 8

**TL;DR:** This paper studies the cross-lingual and cross-register detection of cognitive distortions in Dutch adolescents' forum posts, highlighting domain adaptation methods for improved model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses rising mental health issues among youth and the need for automated detection of psychological distress through text analysis.

**Method:** The research analyzes forum posts written by Dutch adolescents to evaluate the effectiveness of cognitive distortion detection models across different languages and writing styles.

**Key Contributions:**

	1. First study on cross-lingual and cross-register cognitive distortion detection in Dutch
	2. Analysis of the impact of language and style on detection accuracy
	3. Demonstration of domain adaptation methods' effectiveness

**Result:** The study finds that language changes and writing style variations significantly impact model performance, with domain adaptation methods providing promising results.

**Limitations:** Focus on a specific language (Dutch) and demographic (adolescents); may not generalize to other languages or age groups.

**Conclusion:** Early detection of cognitive distortions can enable timely interventions, but model performance is highly variable based on linguistic factors.

**Abstract:** Rising mental health issues among youth have increased interest in automated approaches for detecting early signs of psychological distress in digital text. One key focus is the identification of cognitive distortions, irrational thought patterns that have a role in aggravating mental distress. Early detection of these distortions may enable timely, low-cost interventions. While prior work has focused on English clinical data, we present the first in-depth study of cross-lingual and cross-register generalization of cognitive distortion detection, analyzing forum posts written by Dutch adolescents. Our findings show that while changes in language and writing style can significantly affect model performance, domain adaptation methods show the most promise.

</details>


### [46] [Exploring Machine Learning and Language Models for Multimodal Depression Detection](https://arxiv.org/abs/2508.20805)

*Javier Si Zhao Hong, Timothy Zoe Delaya, Sherwyn Chan Yin Kit, Pai Chet Ng, Xiaoxiao Miao*

**Main category:** cs.CL

**Keywords:** multimodal, depression detection, machine learning, deep learning, large language models

**Relevance Score:** 9

**TL;DR:** The paper presents a comparative study on multimodal depression detection using machine learning and deep learning models, focusing on audio, video, and text features.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance understanding of how different machine learning and deep learning models perform in the context of detecting depression through various media formats, in a mental health setting.

**Method:** The study evaluates the performance of XGBoost, transformer-based architectures, and large language models (LLMs) by analyzing their effectiveness on audio, video, and text data.

**Key Contributions:**

	1. Comparative analysis of various model architectures for depression detection across different modalities
	2. Insights into effective multimodal representation strategies for mental health
	3. Evaluation of LLMs in the context of mental health applications

**Result:** The findings showcase the relative strengths and weaknesses of XGBoost, transformers, and LLMs in capturing depression signals, providing valuable insights for developing effective multimodal prediction strategies.

**Limitations:** The study may be constrained by the specific datasets used, and results could vary with different data sources or modalities.

**Conclusion:** The research underscores the importance of exploring multimodal representations in mental health prediction, helping to inform future studies and applications.

**Abstract:** This paper presents our approach to the first Multimodal Personality-Aware Depression Detection Challenge, focusing on multimodal depression detection using machine learning and deep learning models. We explore and compare the performance of XGBoost, transformer-based architectures, and large language models (LLMs) on audio, video, and text features. Our results highlight the strengths and limitations of each type of model in capturing depression-related signals across modalities, offering insights into effective multimodal representation strategies for mental health prediction.

</details>


### [47] [GDLLM: A Global Distance-aware Modeling Approach Based on Large Language Models for Event Temporal Relation Extraction](https://arxiv.org/abs/2508.20828)

*Jie Zhao, Wanting Ning, Yuxiao Fei, Yubo Feng, Lishuang Li*

**Main category:** cs.CL

**Keywords:** Event Temporal Relation Extraction, Large Language Models, Graph Attention Network

**Relevance Score:** 9

**TL;DR:** GDLLM is a Global Distance-aware approach for Event Temporal Relation Extraction, improving the identification of temporal relations in Natural Language Processing by enhancing the capabilities of Large Language Models, particularly for minority relation classes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of Event Temporal Relation Extraction (ETRE) using Large Language Models (LLMs) by addressing limitations of minority class relations in imbalanced datasets and noise introduced by manual prompts.

**Method:** We propose GDLLM, which integrates a distance-aware graph structure with Graph Attention Network (GAT) and a soft inference-based temporal feature learning paradigm to enhance the learning of long-distance and short-distance dependencies.

**Key Contributions:**

	1. Introduction of the GDLLM framework
	2. Integration of distance-aware graph structure with LLMs
	3. State-of-the-art performance on temporal relation extraction tasks.

**Result:** GDLLM achieves state-of-the-art performance on the TB-Dense and MATRES datasets, particularly improving the identification of minority relation classes in ETRE.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances LLM's learning capabilities and overall performance in temporal relation extraction.

**Abstract:** In Natural Language Processing(NLP), Event Temporal Relation Extraction (ETRE) is to recognize the temporal relations of two events. Prior studies have noted the importance of language models for ETRE. However, the restricted pre-trained knowledge of Small Language Models(SLMs) limits their capability to handle minority class relations in imbalanced classification datasets. For Large Language Models(LLMs), researchers adopt manually designed prompts or instructions, which may introduce extra noise, leading to interference with the model's judgment of the long-distance dependencies between events. To address these issues, we propose GDLLM, a Global Distance-aware modeling approach based on LLMs. We first present a distance-aware graph structure utilizing Graph Attention Network(GAT) to assist the LLMs in capturing long-distance dependency features. Additionally, we design a temporal feature learning paradigm based on soft inference to augment the identification of relations with a short-distance proximity band, which supplements the probabilistic information generated by LLMs into the multi-head attention mechanism. Since the global feature can be captured effectively, our framework substantially enhances the performance of minority relation classes and improves the overall learning ability. Experiments on two publicly available datasets, TB-Dense and MATRES, demonstrate that our approach achieves state-of-the-art (SOTA) performance.

</details>


### [48] [MSRS: Evaluating Multi-Source Retrieval-Augmented Generation](https://arxiv.org/abs/2508.20867)

*Rohan Phanse, Yijie Zhou, Kejian Shi, Wencai Zhang, Yixin Liu, Yilun Zhao, Arman Cohan*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, multi-source retrieval, information synthesis

**Relevance Score:** 8

**TL;DR:** The paper presents a framework to evaluate retrieval-augmented generation (RAG) systems on multi-source information integration and long-form response generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluations of RAG systems are limited to single-source or short-form queries, failing to address real-world scenarios requiring information synthesis across multiple sources.

**Method:** A scalable framework for constructing evaluation benchmarks for RAG systems is introduced, along with two new benchmarks: MSRS-Story and MSRS-Meet, focused on narrative synthesis and summarization, respectively.

**Key Contributions:**

	1. Introduction of a scalable framework for evaluating RAG systems on multi-source tasks
	2. Development of two new benchmarks for narrative synthesis and summarization
	3. Demonstration that reasoning models are more effective than LLMs in multi-source synthesis scenarios.

**Result:** Experiments showed that generation quality is highly dependent on retrieval effectiveness, which varies by task, and reasoning models outperformed standard LLMs in multi-source synthesis tasks.

**Limitations:** The benchmarks do not cover all possible real-world complexities of multi-source information retrieval and synthesis.

**Conclusion:** The study emphasizes that effective retrieval is crucial for long-form response generation from diverse sources, and existing models need advancements to improve synthesis quality.

**Abstract:** Retrieval-augmented systems are typically evaluated in settings where information required to answer the query can be found within a single source or the answer is short-form or factoid-based. However, many real-world applications demand the ability to integrate and summarize information scattered across multiple sources, where no single source is sufficient to respond to the user's question. In such settings, the retrieval component of a RAG pipeline must recognize a variety of relevance signals, and the generation component must connect and synthesize information across multiple sources. We present a scalable framework for constructing evaluation benchmarks that challenge RAG systems to integrate information across distinct sources and generate long-form responses. Using our framework, we build two new benchmarks on Multi-Source Retrieval and Synthesis: MSRS-Story and MSRS-Meet, representing narrative synthesis and summarization tasks, respectively, that require retrieval from large collections. Our extensive experiments with various RAG pipelines -- including sparse and dense retrievers combined with frontier LLMs -- reveal that generation quality is highly dependent on retrieval effectiveness, which varies greatly by task. While multi-source synthesis proves challenging even in an oracle retrieval setting, we find that reasoning models significantly outperform standard LLMs at this distinct step.

</details>


### [49] [The Uneven Impact of Post-Training Quantization in Machine Translation](https://arxiv.org/abs/2508.20893)

*Benjamin Marie, Atsushi Fujita*

**Main category:** cs.CL

**Keywords:** Quantization, Machine Translation, Large Language Models, Low-resource Languages, Post-training Quantization

**Relevance Score:** 8

**TL;DR:** This paper evaluates post-training quantization for multilingual machine translation across 55 languages, analyzing its effects on translation quality, especially for low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the implications of quantization on deploying large language models (LLMs) for multilingual tasks, particularly in resource-constrained environments.

**Method:** Conducted a large-scale evaluation of post-training quantization (PTQ) using five LLMs with varying parameters, examining four quantization techniques across high- and low-resource languages.

**Key Contributions:**

	1. First large-scale evaluation of PTQ on machine translation across 55 languages.
	2. Identification of the impact of quantization techniques on translation quality for different language resources.
	3. Insights into model calibration and decoding hyperparameters under low-bit conditions.

**Result:** 4-bit quantization generally maintains translation quality for high-resource languages, but 2-bit quantization leads to significant quality loss in low-resource and diverse languages. GGUF variants performed consistently well, especially at 2-bit precision.

**Limitations:** Limited exploration of other language modeling tasks beyond machine translation; focuses mainly on specific quantization techniques.

**Conclusion:** The study provides insights into the deployment of multilingual LLMs in translation tasks under quantization constraints, stressing the importance of model size and quantization methods.

**Abstract:** Quantization is essential for deploying large language models (LLMs) on resource-constrained hardware, but its implications for multilingual tasks remain underexplored. We conduct the first large-scale evaluation of post-training quantization (PTQ) on machine translation across 55 languages using five LLMs ranging from 1.7B to 70B parameters. Our analysis reveals that while 4-bit quantization often preserves translation quality for high-resource languages and large models, significant degradation occurs for low-resource and typologically diverse languages, particularly in 2-bit settings. We compare four quantization techniques (AWQ, BitsAndBytes, GGUF, and AutoRound), showing that algorithm choice and model size jointly determine robustness. GGUF variants provide the most consistent performance, even at 2-bit precision. Additionally, we quantify the interactions between quantization, decoding hyperparameters, and calibration languages, finding that language-matched calibration offers benefits primarily in low-bit scenarios. Our findings offer actionable insights for deploying multilingual LLMs for machine translation under quantization constraints, especially in low-resource settings.

</details>


### [50] [SageLM: A Multi-aspect and Explainable Large Language Model for Speech Judgement](https://arxiv.org/abs/2508.20916)

*Yuan Ge, Junxiang Zhang, Xiaoqian Liu, Bei Li, Xiangnan Ma, Chenglong Wang, Kaiyang Ye, Yangfan Du, Linfeng Zhang, Yuxin Huang, Tong Xiao, Zhengtao Yu, JingBo Zhu*

**Main category:** cs.CL

**Keywords:** speech-to-speech, large language models, explainable AI, evaluation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** SageLM is a multi-aspect, explainable speech LLM designed for evaluating Speech-to-Speech systems by jointly assessing semantic and acoustic dimensions, achieving a high agreement rate with human evaluators.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fundamental challenge of evaluating Speech-to-Speech Large Language Models by creating a model that incorporates both semantic and acoustic evaluations.

**Method:** SageLM jointly assesses semantic and acoustic features, uses rationale-based supervision for explainability, and employs a two-stage training paradigm using a synthetic preference dataset called SpeechFeedback.

**Key Contributions:**

	1. Introduction of a multi-aspect evaluation model for S2S LLMs
	2. Implementing rationale-based supervision for enhanced explainability
	3. Creation of the SpeechFeedback synthetic preference dataset

**Result:** SageLM achieved an 82.79% agreement rate with human evaluators, outperforming existing methods by considerable margins in both semantic and acoustic evaluations.

**Limitations:** 

**Conclusion:** SageLM demonstrates a more comprehensive and effective approach to evaluating Speech-to-Speech LLMs by integrating multiple aspects of dialogue quality and providing improved explainability.

**Abstract:** Speech-to-Speech (S2S) Large Language Models (LLMs) are foundational to natural human-computer interaction, enabling end-to-end spoken dialogue systems. However, evaluating these models remains a fundamental challenge. We propose \texttt{SageLM}, an end-to-end, multi-aspect, and explainable speech LLM for comprehensive S2S LLMs evaluation. First, unlike cascaded approaches that disregard acoustic features, SageLM jointly assesses both semantic and acoustic dimensions. Second, it leverages rationale-based supervision to enhance explainability and guide model learning, achieving superior alignment with evaluation outcomes compared to rule-based reinforcement learning methods. Third, we introduce \textit{SpeechFeedback}, a synthetic preference dataset, and employ a two-stage training paradigm to mitigate the scarcity of speech preference data. Trained on both semantic and acoustic dimensions, SageLM achieves an 82.79\% agreement rate with human evaluators, outperforming cascaded and SLM-based baselines by at least 7.42\% and 26.20\%, respectively.

</details>


### [51] [How Can Input Reformulation Improve Tool Usage Accuracy in a Complex Dynamic Environment? A Study on $$-bench](https://arxiv.org/abs/2508.20931)

*Venkatesh Mishra, Amir Saeidi, Satyam Raj, Mutsumi Nakamura, Jayanth Srinivasa, Gaowen Liu, Ali Payani, Chitta Baral*

**Main category:** cs.CL

**Keywords:** large language models, tool use, multi-agent systems, input reformulation, conversational agents

**Relevance Score:** 9

**TL;DR:** This paper introduces the Input-Reformulation Multi-Agent (IRMA) framework to enhance large language models' performance in dynamic, multi-turn conversational environments by automatically reformulating user queries.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the failures of large language models as autonomous agents in maintaining consistent reasoning and adhering to domain-specific policies during tool-use in conversations.

**Method:** A manual analysis of common errors in conversation trajectories was conducted, followed by experiments on reformulating inputs to improve decision making in agents. The IRMA framework was proposed to automatically reformulate user queries with domain rules and tool suggestions.

**Key Contributions:**

	1. Introduction of the IRMA framework for query reformulation in LLMs
	2. Comprehensive analysis of common conversational errors in agent interactions
	3. Demonstrated significant performance improvements over existing methodologies

**Result:** IRMA significantly outperformed other methods (ReAct, Function Calling, and Self-Reflection) by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores.

**Limitations:** 

**Conclusion:** The findings demonstrate IRMA's superior reliability and consistency as a reformulation approach for tool-calling agents in dynamic environments.

**Abstract:** Recent advances in reasoning and planning capabilities of large language models (LLMs) have enabled their potential as autonomous agents capable of tool use in dynamic environments. However, in multi-turn conversational environments like $\tau$-bench, these agents often struggle with consistent reasoning, adherence to domain-specific policies, and extracting correct information over a long horizon of tool-calls and conversation. To capture and mitigate these failures, we conduct a comprehensive manual analysis of the common errors occurring in the conversation trajectories. We then experiment with reformulations of inputs to the tool-calling agent for improvement in agent decision making. Finally, we propose the Input-Reformulation Multi-Agent (IRMA) framework, which automatically reformulates user queries augmented with relevant domain rules and tool suggestions for the tool-calling agent to focus on. The results show that IRMA significantly outperforms ReAct, Function Calling, and Self-Reflection by 16.1%, 12.7%, and 19.1%, respectively, in overall pass^5 scores. These findings highlight the superior reliability and consistency of IRMA compared to other methods in dynamic environments.

</details>


### [52] [STARE at the Structure: Steering ICL Exemplar Selection with Structural Alignment](https://arxiv.org/abs/2508.20944)

*Jiaqian Li, Qisheng Hu, Jing Li, Wenya Wang*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Semantic Parsing, Exemplar Selection, BERT, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel two-stage exemplar selection strategy for improving In-Context Learning (ICL) in structured prediction tasks, focusing on semantic parsing. The proposed method enhances exemplar selection by ensuring structural alignment and semantic relevance, outperforming existing methods on multiple benchmarks.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of In-Context Learning (ICL) for structured prediction tasks by addressing the limitations in existing exemplar selection strategies, particularly the lack of structural alignment.

**Method:** A two-stage exemplar selection strategy that first fine-tunes a BERT-based retriever using structure-aware supervision to select exemplars. This is followed by enhancing the retriever with a plug-in module that amplifies syntactically meaningful information in hidden representations.

**Key Contributions:**

	1. Introduction of a two-stage exemplar selection strategy for ICL
	2. Utilization of structure-aware supervision for exemplar selection
	3. A plug-in module that enhances hidden representations for better semantic alignment

**Result:** The proposed method significantly outperforms existing approaches on four benchmarks across three semantic parsing tasks using various recent LLMs for inference.

**Limitations:** 

**Conclusion:** The new two-stage approach provides a strong balance between efficiency, generalizability, and performance, making it effective for semantic parsing tasks in ICL settings.

**Abstract:** In-Context Learning (ICL) has become a powerful paradigm that enables LLMs to perform a wide range of tasks without task-specific fine-tuning. However, the effectiveness of ICL heavily depends on the quality of exemplar selection. In particular, for structured prediction tasks such as semantic parsing, existing ICL selection strategies often overlook structural alignment, leading to suboptimal performance and poor generalization. To address this issue, we propose a novel two-stage exemplar selection strategy that achieves a strong balance between efficiency, generalizability, and performance. First, we fine-tune a BERT-based retriever using structure-aware supervision, guiding it to select exemplars that are both semantically relevant and structurally aligned. Then, we enhance the retriever with a plug-in module, which amplifies syntactically meaningful information in the hidden representations. This plug-in is model-agnostic, requires minimal overhead, and can be seamlessly integrated into existing pipelines. Experiments on four benchmarks spanning three semantic parsing tasks demonstrate that our method consistently outperforms existing baselines with multiple recent LLMs as inference-time models.

</details>


### [53] [ProactiveEval: A Unified Evaluation Framework for Proactive Dialogue Agents](https://arxiv.org/abs/2508.20973)

*Tianjian Liu, Fanqi Wan, Jiajian Guo, Xiaojun Quan*

**Main category:** cs.CL

**Keywords:** Proactive Dialogue, Large Language Models, Evaluation Framework, NLP, AI

**Relevance Score:** 8

**TL;DR:** This paper introduces ProactiveEval, a framework for evaluating proactive dialogue capabilities of large language models (LLMs) across various domains.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a comprehensive evaluation of proactive dialogue abilities in LLMs due to fragmented existing evaluations that focus on domain-specific or task-oriented scenarios.

**Method:** The framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics and generating diverse evaluation data across 328 environments in 6 domains.

**Key Contributions:**

	1. Introduction of ProactiveEval for unified evaluation of proactive dialogue in LLMs.
	2. Creation of 328 evaluation environments across different domains.
	3. Insights into how reasoning affects proactive dialogue capabilities.

**Result:** Experiments with 22 LLMs revealed that DeepSeek-R1 and Claude-3.7-Sonnet excelled in target planning and dialogue guidance tasks respectively.

**Limitations:** The framework may not cover all aspects of proactive dialogue and may require further domain-specific adaptation.

**Conclusion:** The study highlights the influence of reasoning capabilities on proactive dialogue behaviors and suggests directions for future model development.

**Abstract:** Proactive dialogue has emerged as a critical and challenging research problem in advancing large language models (LLMs). Existing works predominantly focus on domain-specific or task-oriented scenarios, which leads to fragmented evaluations and limits the comprehensive exploration of models' proactive conversation abilities. In this work, we propose ProactiveEval, a unified framework designed for evaluating proactive dialogue capabilities of LLMs. This framework decomposes proactive dialogue into target planning and dialogue guidance, establishing evaluation metrics across various domains. Moreover, it also enables the automatic generation of diverse and challenging evaluation data. Based on the proposed framework, we develop 328 evaluation environments spanning 6 distinct domains. Through experiments with 22 different types of LLMs, we show that DeepSeek-R1 and Claude-3.7-Sonnet exhibit exceptional performance on target planning and dialogue guidance tasks, respectively. Finally, we investigate how reasoning capabilities influence proactive behaviors and discuss their implications for future model development.

</details>


### [54] [Lethe: Purifying Backdoored Large Language Models with Knowledge Dilution](https://arxiv.org/abs/2508.21004)

*Chen Chen, Yuchen Sun, Jiaxin Gao, Xueluan Gong, Qian Wang, Ziyao Wang, Yongsen Zheng, Kwok-Yan Lam*

**Main category:** cs.CL

**Keywords:** large language models, backdoor attacks, defense mechanisms, knowledge dilution, natural language processing

**Relevance Score:** 9

**TL;DR:** LETHE is a novel method designed to eliminate backdoor behaviors in large language models (LLMs) through knowledge dilution, effectively neutralizing malicious triggers while maintaining model performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the vulnerability of LLMs to backdoor attacks, highlighting the need for comprehensive defenses against a range of attack types.

**Method:** LETHE combines an internal approach using a lightweight dataset to train a clean model, and an external mechanism that adds benign evidence to prompts, distracting the model from backdoor features.

**Key Contributions:**

	1. Introduction of LETHE for backdoor defense in LLMs
	2. Demonstrated superiority of LETHE over eight state-of-the-art defense methods
	3. Proved effectiveness across multiple widely used language models

**Result:** Experimental results show that LETHE significantly reduces the attack success rate of advanced backdoor attacks by up to 98% across five popular LLMs, while preserving model utility.

**Limitations:** 

**Conclusion:** LETHE is a cost-efficient and robust solution to enhance the security of LLMs against a variety of backdoor attacks.

**Abstract:** Large language models (LLMs) have seen significant advancements, achieving superior performance in various Natural Language Processing (NLP) tasks. However, they remain vulnerable to backdoor attacks, where models behave normally for standard queries but generate harmful responses or unintended output when specific triggers are activated. Existing backdoor defenses either lack comprehensiveness, focusing on narrow trigger settings, detection-only mechanisms, and limited domains, or fail to withstand advanced scenarios like model-editing-based, multi-trigger, and triggerless attacks. In this paper, we present LETHE, a novel method to eliminate backdoor behaviors from LLMs through knowledge dilution using both internal and external mechanisms. Internally, LETHE leverages a lightweight dataset to train a clean model, which is then merged with the backdoored model to neutralize malicious behaviors by diluting the backdoor impact within the model's parametric memory. Externally, LETHE incorporates benign and semantically relevant evidence into the prompt to distract LLM's attention from backdoor features. Experimental results on classification and generation domains across 5 widely used LLMs demonstrate that LETHE outperforms 8 state-of-the-art defense baselines against 8 backdoor attacks. LETHE reduces the attack success rate of advanced backdoor attacks by up to 98% while maintaining model utility. Furthermore, LETHE has proven to be cost-efficient and robust against adaptive backdoor attacks.

</details>


### [55] [An Agile Method for Implementing Retrieval Augmented Generation Tools in Industrial SMEs](https://arxiv.org/abs/2508.21024)

*Mathieu Bourdin, Anas Neumann, Thomas Paviot, Robert Pellerin, Samir Lamouri*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Human-Computer Interaction, Natural Language Processing, Small and Medium Enterprises, Method Engineering

**Relevance Score:** 9

**TL;DR:** The paper introduces EASI-RAG, a method to support the deployment of Retrieval-Augmented Generation (RAG) systems in Small and Medium Enterprises (SMEs), validated through a case study in an environmental testing laboratory.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of deploying RAG systems in SMEs, which often lack resources and NLP expertise.

**Method:** EASI-RAG, based on method engineering principles, involves structured roles, activities, and techniques for implementation.

**Key Contributions:**

	1. Introduction of EASI-RAG method for RAG deployment in SMEs
	2. Validation through a case study showing effective implementation
	3. Demonstrated improvements in user adoption and data reliability

**Result:** The deployment of a RAG tool in a real-world case study was completed in under a month, resulting in high user adoption and accurate query responses.

**Limitations:** Further research needed for generalization across diverse use cases and model integration.

**Conclusion:** EASI-RAG demonstrates the feasibility of RAG deployment in industrial contexts, with future work focused on generalizing the method and integrating it with fine-tuned models.

**Abstract:** Retrieval-Augmented Generation (RAG) has emerged as a powerful solution to mitigate the limitations of Large Language Models (LLMs), such as hallucinations and outdated knowledge. However, deploying RAG-based tools in Small and Medium Enterprises (SMEs) remains a challenge due to their limited resources and lack of expertise in natural language processing (NLP). This paper introduces EASI-RAG, Enterprise Application Support for Industrial RAG, a structured, agile method designed to facilitate the deployment of RAG systems in industrial SME contexts. EASI-RAG is based on method engineering principles and comprises well-defined roles, activities, and techniques. The method was validated through a real-world case study in an environmental testing laboratory, where a RAG tool was implemented to answer operators queries using data extracted from operational procedures. The system was deployed in under a month by a team with no prior RAG experience and was later iteratively improved based on user feedback. Results demonstrate that EASI-RAG supports fast implementation, high user adoption, delivers accurate answers, and enhances the reliability of underlying data. This work highlights the potential of RAG deployment in industrial SMEs. Future works include the need for generalization across diverse use cases and further integration with fine-tuned models.

</details>


### [56] [Re-Representation in Sentential Relation Extraction with Sequence Routing Algorithm](https://arxiv.org/abs/2508.21049)

*Ramazan Ali Bahrami, Ramin Yahyapour*

**Main category:** cs.CL

**Keywords:** sentential relation extraction, dynamic routing, neuroscience, re-representation, noise in labels

**Relevance Score:** 6

**TL;DR:** This paper presents an approach for sentential relation extraction using dynamic routing in capsules, achieving superior performance on certain datasets while revealing challenges like noise in larger datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sentential relation extraction in NLP and understand the factors influencing performance on different datasets.

**Method:** The approach utilizes dynamic routing in capsules to enhance the extraction of relations between sentences.

**Key Contributions:**

	1. Dynamic routing in capsules for sentential relation extraction
	2. Identification of noise in labels as a performance inhibitor
	3. Introduction of the concept of re-representation for better matching

**Result:** The proposed method outperforms state-of-the-art models on Tacred, Tacredrev, Retacred, and Conll04 datasets, but faces challenges with noise in the labels of the larger Wikidata dataset.

**Limitations:** The findings regarding noise in Wikidata labels highlight limitations in generalizability to larger datasets.

**Conclusion:** Re-representation is identified as a significant aspect of improving performance in sentential relation extraction, with implications for understanding the impact of label noise.

**Abstract:** Sentential relation extraction (RE) is an important task in natural language processing (NLP). In this paper we propose to do sentential RE with dynamic routing in capsules. We first show that the proposed approach outperform state of the art on common sentential relation extraction datasets Tacred, Tacredrev, Retacred, and Conll04. We then investigate potential reasons for its good performance on the mentioned datasets, and yet low performance on another similar, yet larger sentential RE dataset, Wikidata. As such, we identify noise in Wikidata labels as one of the reasons that can hinder performance. Additionally, we show associativity of better performance with better re-representation, a term from neuroscience referred to change of representation in human brain to improve the match at comparison time. As example, in the given analogous terms King:Queen::Man:Woman, at comparison time, and as a result of re-representation, the similarity between related head terms (King,Man), and tail terms (Queen,Woman) increases. As such, our observation show that our proposed model can do re-representation better than the vanilla model compared with. To that end, beside noise in the labels of the distantly supervised RE datasets, we propose re-representation as a challenge in sentential RE.

</details>


### [57] [Enabling Equitable Access to Trustworthy Financial Reasoning](https://arxiv.org/abs/2508.21051)

*William Jurayj, Nils Holzenberger, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** Tax Automation, Large Language Models, Symbolic Reasoning, Neuro-Symbolic Architectures, Cost Estimation

**Relevance Score:** 5

**TL;DR:** The paper proposes a neuro-symbolic approach to automate tax filing by integrating LLMs with symbolic solvers, aiming to enhance accuracy and auditability while reducing costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Tax filing involves complex reasoning and high stakes due to potential penalties for errors, necessitating accurate automated systems.

**Method:** The proposed system integrates large language models with a symbolic solver, enhancing performance through translation of tax rules into formal logic and using exemplars for case representations.

**Key Contributions:**

	1. Integration of LLMs with symbolic solvers for tax filing automation
	2. Method for estimating deployment costs based on penalties
	3. Improvement of performance via translation of text rules into formal logic

**Result:** Evaluations on the StAtutory Reasoning Assessment (SARA) dataset show improved performance and cost reductions compared to traditional methods.

**Limitations:** The dependency on the quality of the symbolic reasoning component may limit generalization to all tax scenarios.

**Conclusion:** Neuro-symbolic architectures can significantly enhance access to reliable tax assistance while maintaining economic feasibility.

**Abstract:** According to the United States Internal Revenue Service, ''the average American spends $\$270$ and 13 hours filing their taxes''. Even beyond the U.S., tax filing requires complex reasoning, combining application of overlapping rules with numerical calculations. Because errors can incur costly penalties, any automated system must deliver high accuracy and auditability, making modern large language models (LLMs) poorly suited for this task. We propose an approach that integrates LLMs with a symbolic solver to calculate tax obligations. We evaluate variants of this system on the challenging StAtutory Reasoning Assessment (SARA) dataset, and include a novel method for estimating the cost of deploying such a system based on real-world penalties for tax errors. We further show how combining up-front translation of plain-text rules into formal logic programs, combined with intelligently retrieved exemplars for formal case representations, can dramatically improve performance on this task and reduce costs to well below real-world averages. Our results demonstrate the promise and economic feasibility of neuro-symbolic architectures for increasing equitable access to reliable tax assistance.

</details>


### [58] [Probing Pre-Trained Language Models for Cross-Cultural Differences in Values](https://arxiv.org/abs/2203.13722)

*Arnav Arora, Lucie-Aime Kaffee, Isabelle Augenstein*

**Main category:** cs.CL

**Keywords:** Pre-Trained Language Models, cultural values, cross-cultural studies, bias in NLP, alignment of models

**Relevance Score:** 9

**TL;DR:** The paper studies the cultural values embedded in Pre-Trained Language Models (PTLMs) and their alignment with established value surveys.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically investigate how cultural values are represented in PTLMs and their implications in cross-cultural applications.

**Method:** The authors introduce probes to analyze and compare the values embedded in PTLMs against established cross-cultural value surveys.

**Key Contributions:**

	1. Introduced probes for analyzing cultural values in PTLMs
	2. Found weak alignment of PTLM values with cultural surveys
	3. Discussed implications and alignment strategies for PTLMs in cross-cultural settings

**Result:** The study finds that PTLMs capture cultural value differences, but these do not strongly align with established value surveys.

**Limitations:** The exploration of cultural values is still nascent and the methods may not capture all aspects of cultural nuances.

**Conclusion:** Misalignment of values in PTLMs may have implications for their use in diverse cultural contexts, suggesting a need for alignment methods.

**Abstract:** Language embeds information about social, cultural, and political values people hold. Prior work has explored social and potentially harmful biases encoded in Pre-Trained Language models (PTLMs). However, there has been no systematic study investigating how values embedded in these models vary across cultures. In this paper, we introduce probes to study which values across cultures are embedded in these models, and whether they align with existing theories and cross-cultural value surveys. We find that PTLMs capture differences in values across cultures, but those only weakly align with established value surveys. We discuss implications of using mis-aligned models in cross-cultural settings, as well as ways of aligning PTLMs with value surveys.

</details>


### [59] [ASVD: Activation-aware Singular Value Decomposition for Compressing Large Language Models](https://arxiv.org/abs/2312.05821)

*Zhihang Yuan, Yuzhang Shang, Yue Song, Dawei Yang, Qiang Wu, Yan Yan, Guangyu Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, Compression, Activation-aware Singular Value Decomposition, Low-rank Decomposition, KV Cache

**Relevance Score:** 9

**TL;DR:** This paper presents a new compression method for Large Language Models called Activation-aware Singular Value Decomposition (ASVD), which addresses challenges in weight decomposition and achieves significant reductions in memory usage and model size without compromising performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for wider adoption of Large Language Models (LLMs) due to their size and resource requirements motivates the exploration of new compression techniques.

**Method:** The paper introduces ASVD, a training-free approach that applies low-rank decomposition while managing activation outliers and optimizing layer-specific decomposition through an iterative calibration process.

**Key Contributions:**

	1. Introduction of Activation-aware Singular Value Decomposition (ASVD) for LLM compression
	2. Demonstration of layer-specific optimization in weight decomposition
	3. Significant KV cache memory reductions without performance loss

**Result:** ASVD can compress LLM networks by 10%-30% and achieve up to 50% reduction in KV cache memory requirements without any performance drop.

**Limitations:** 

**Conclusion:** The proposed ASVD method contributes to more efficient deployment of LLMs by significantly reducing size and resource demands while maintaining performance.

**Abstract:** In this paper, we introduce a new post-training compression paradigm for Large Language Models (LLMs) to facilitate their wider adoption. We delve into LLM weight low-rank decomposition, and find that the challenges of this task stem from (1) the distribution variance in the LLM activations and (2) the sensitivity difference among various kinds of layers. To address these issues, we propose a training-free approach called Activation-aware Singular Value Decomposition (ASVD). Specifically, ASVD manages activation outliers by transforming the weight matrix based on the activation distribution. This transformation allows the outliers in the activation matrix to be absorbed into the transformed weight matrix, thereby enhancing decomposition accuracy. Additionally, we propose an efficient iterative calibration process to optimize layer-specific decomposition by addressing the varying sensitivity of different LLM layers. In this way, ASVD can compress a network by 10%-30%. Based on the success of the low-rank decomposition of projection matrices in the self-attention module, we further introduce ASVD to compress the KV cache. By reducing the channel dimension of KV activations, memory requirements for KV cache can be largely reduced. ASVD can further achieve 50% KV cache reductions without performance drop in a training-free manner.

</details>


### [60] [LGDE: Local Graph-based Dictionary Expansion](https://arxiv.org/abs/2405.07764)

*Juni Schindler, Sneha Jha, Xixuan Zhang, Kilian Buehling, Annett Heft, Mauricio Barahona*

**Main category:** cs.CL

**Keywords:** graph-based methods, dictionary expansion, word embeddings, semantic similarity, information retrieval

**Relevance Score:** 7

**TL;DR:** The paper introduces Local Graph-based Dictionary Expansion (LGDE), a new method for discovering word similarities using graph diffusion and manifold learning, ultimately enhancing keyword expansion for information retrieval tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve keyword expansion in information retrieval by exploring semantic neighbourhoods of words beyond direct similarities.

**Method:** The LGDE method creates a word similarity graph using word embeddings and applies local community detection based on graph diffusion to capture complex word similarities.

**Key Contributions:**

	1. Introduction of a novel method for semantic neighbourhood discovery using local graph techniques.
	2. Validation of LGDE on real-world datasets and domains.
	3. Demonstration of improved keyword expansion for information retrieval tasks.

**Result:** Validation on user-generated corpora shows that LGDE enriches keyword lists significantly more than traditional methods based on direct similarities or co-occurrences.

**Limitations:** 

**Conclusion:** LGDE effectively expands the seed dictionary with more relevant keywords, as demonstrated in a real-world application in communication science.

**Abstract:** We present Local Graph-based Dictionary Expansion (LGDE), a method for data-driven discovery of the semantic neighbourhood of words using tools from manifold learning and network science. At the heart of LGDE lies the creation of a word similarity graph from the geometry of word embeddings followed by local community detection based on graph diffusion. The diffusion in the local graph manifold allows the exploration of the complex nonlinear geometry of word embeddings to capture word similarities based on paths of semantic association, over and above direct pairwise similarities. Exploiting such semantic neighbourhoods enables the expansion of dictionaries of pre-selected keywords, an important step for tasks in information retrieval, such as database queries and online data collection. We validate LGDE on two user-generated English-language corpora and show that LGDE enriches the list of keywords with improved performance relative to methods based on direct word similarities or co-occurrences. We further demonstrate our method through a real-world use case from communication science, where LGDE is evaluated quantitatively on the expansion of a conspiracy-related dictionary from online data collected and analysed by domain experts. Our empirical results and expert user assessment indicate that LGDE expands the seed dictionary with more useful keywords due to the manifold-learning-based similarity network.

</details>


### [61] [Bitune: Leveraging Bidirectional Attention to Improve Decoder-Only LLMs](https://arxiv.org/abs/2405.14862)

*Dawid J. Kopiczko, Tijmen Blankevoort, Yuki M. Asano*

**Main category:** cs.CL

**Keywords:** large language models, bidirectional attention, instruction tuning

**Relevance Score:** 8

**TL;DR:** Bitune enhances decoder-only LLMs with bidirectional attention, improving performance in various tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of masked causal attention in decoder-only large language models, which restricts information flow and expressiveness.

**Method:** Introducing Bitune, which incorporates bidirectional attention into the prompt processing of pretrained decoder-only LLMs.

**Key Contributions:**

	1. Proposed a novel method, Bitune, for incorporating bidirectional attention into decoder-only LLMs.
	2. Demonstrated significant improvements across various performance metrics in NLP tasks.
	3. Validated compatibility with parameter-efficient finetuning techniques.

**Result:** Significant performance improvements were observed in commonsense reasoning, arithmetic, and language understanding tasks during evaluation in instruction-tuning and question-answering settings.

**Limitations:** 

**Conclusion:** Bitune is effective and compatible with parameter-efficient finetuning techniques and full model finetuning, validated through ablation studies.

**Abstract:** Decoder-only large language models typically rely solely on masked causal attention, which limits their expressiveness by restricting information flow to one direction. We propose Bitune, a method that enhances pretrained decoder-only LLMs by incorporating bidirectional attention into prompt processing. We evaluate Bitune in instruction-tuning and question-answering settings, showing significant improvements in performance on commonsense reasoning, arithmetic, and language understanding tasks. Furthermore, extensive ablation studies validate the role of each component of the method, and demonstrate that Bitune is compatible with various parameter-efficient finetuning techniques and full model finetuning.

</details>


### [62] [SoAy: A Solution-based LLM API-using Methodology for Academic Information Seeking](https://arxiv.org/abs/2405.15165)

*Yuanchun Wang, Jifan Yu, Zijun Yao, Jing Zhang, Yuyang Xie, Shangqing Tu, Yiyang Fu, Youhe Feng, Jinkai Zhang, Jingyao Zhang, Bowen Huang, Yuanyao Li, Huihui Yuan, Lei Hou, Juanzi Li, Jie Tang*

**Main category:** cs.CL

**Keywords:** large language models, academic API, information seeking, complex API coupling, SoAy

**Relevance Score:** 9

**TL;DR:** This paper presents SoAy, a methodology leveraging large language models (LLMs) for improved academic API usage in information seeking, addressing complex API interactions through pre-constructed solution sequences.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To simplify the academic information seeking process and enhance the efficiency of using LLMs with complex API couplings.

**Method:** SoAy utilizes code with a pre-constructed API calling sequence as a reasoning method to better capture complex relationships between APIs.

**Key Contributions:**

	1. Development of SoAy methodology for LLM API usage in academic contexts
	2. Introduction of SoAyBench and SoAyEval for comprehensive evaluation
	3. Significant performance improvements validated through experiments

**Result:** Experimental results indicate a performance improvement of 34.58-75.99% over existing LLM API-based methods.

**Limitations:** 

**Conclusion:** SoAy demonstrates significant enhancements in reasoning efficiency for academic queries requiring complex API interactions, supported by an evaluation benchmark and publicly accessible resources.

**Abstract:** Applying large language models (LLMs) for academic API usage shows promise in reducing researchers' academic information seeking efforts. However, current LLM API-using methods struggle with complex API coupling commonly encountered in academic queries. To address this, we introduce SoAy, a solution-based LLM API-using methodology for academic information seeking. It uses code with a solution as the reasoning method, where a solution is a pre-constructed API calling sequence. The addition of the solution reduces the difficulty for the model to understand the complex relationships between APIs. Code improves the efficiency of reasoning.   To evaluate SoAy, we introduce SoAyBench, an evaluation benchmark accompanied by SoAyEval, built upon a cloned environment of APIs from AMiner. Experimental results demonstrate a 34.58-75.99\% performance improvement compared to state-of-the-art LLM API-based baselines. All datasets, codes, tuned models, and deployed online services are publicly accessible at https://github.com/RUCKBReasoning/SoAy.

</details>


### [63] [Query Optimization for Parametric Knowledge Refinement in Retrieval-Augmented Large Language Models](https://arxiv.org/abs/2411.07820)

*Youan Cong, Pritom Saha Akash, Cheng Wang, Kevin Chen-Chuan Chang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, query optimization

**Relevance Score:** 9

**TL;DR:** The ERRR framework optimizes queries for RAG systems by extracting knowledge from LLMs and refining queries to enhance information retrieval and response accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the pre-retrieval information gap in Retrieval-Augmented Generation systems and optimize query handling for better performance with LLMs.

**Method:** The ERRR framework employs a multi-step process starting with extraction of parametric knowledge from LLMs, followed by specialized query optimization and knowledge distillation to refine the queries using a smaller model.

**Key Contributions:**

	1. Introduction of the ERRR framework for query optimization in RAG systems
	2. Utilization of parametric knowledge extraction from LLMs
	3. Implementation of a trainable query optimizer through knowledge distillation

**Result:** ERRR outperforms existing query optimization techniques in RAG systems on multiple question-answering datasets, showcasing superior utility and accuracy while being cost-effective.

**Limitations:** 

**Conclusion:** The ERRR framework is a versatile and efficient solution for improving retrieval mechanisms in RAG systems, ultimately aiming for enhanced response accuracy in various applications.

**Abstract:** We introduce the Extract-Refine-Retrieve-Read (ERRR) framework, a novel approach designed to bridge the pre-retrieval information gap in Retrieval-Augmented Generation (RAG) systems through query optimization tailored to meet the specific knowledge requirements of Large Language Models (LLMs). Unlike conventional query optimization techniques used in RAG, the ERRR framework begins by extracting parametric knowledge from LLMs, followed by using a specialized query optimizer for refining these queries. This process ensures the retrieval of only the most pertinent information essential for generating accurate responses. Moreover, to enhance flexibility and reduce computational costs, we propose a trainable scheme for our pipeline that utilizes a smaller, tunable model as the query optimizer, which is refined through knowledge distillation from a larger teacher model. Our evaluations on various question-answering (QA) datasets and with different retrieval systems show that ERRR consistently outperforms existing baselines, proving to be a versatile and cost-effective module for improving the utility and accuracy of RAG systems.

</details>


### [64] [Improving the quality of Web-mined Parallel Corpora of Low-Resource Languages using Debiasing Heuristics](https://arxiv.org/abs/2502.19074)

*Aloka Fernando, Nisansa de Silva, Menan Velyuthan, Charitha Rathnayake, Surangika Ranathunga*

**Main category:** cs.CL

**Keywords:** multilingual language models, parallel data curation, neural machine translation

**Relevance Score:** 6

**TL;DR:** This paper examines how different pre-trained multilingual language models (multiPLMs) affect the ranking quality of sentence pairs in parallel data curation, aiming to improve neural machine translation (NMT) performance by filtering out noisy sentences using various heuristics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the impact of multiPLM choice on ranking quality in parallel data curation, which is crucial for enhancing NMT performance.

**Method:** The authors analyze multiPLMs (LASER3, XLM-R, LaBSE) using web-mined corpora (CCMatrix and CCAligned) across various language pairs and apply heuristics to reduce noise in top-ranked sentences.

**Key Contributions:**

	1. Identifies biases in multiPLMs that affect ranking quality in data curation.
	2. Proposes heuristics for noise reduction in top-ranked sentence pairs.
	3. Demonstrates improvements in NMT performance with filtered datasets.

**Result:** The research demonstrates that different multiPLMs show biases towards specific sentence types, leading to noise in the top-ranked samples. Heuristics employed significantly reduce this noise.

**Limitations:** 

**Conclusion:** By refining the ranking process with heuristics, the quality of data for NMT training can be improved, benefiting performance and consistency across different multiPLMs.

**Abstract:** Parallel Data Curation (PDC) techniques aim to filter out noisy parallel sentences from the web-mined corpora. Prior research has demonstrated that ranking sentence pairs using similarity scores on sentence embeddings derived from Pre-trained Multilingual Language Models (multiPLMs) and training the NMT systems with the top-ranked samples, produces superior NMT performance than when trained using the full dataset. However, previous research has shown that the choice of multiPLM significantly impacts the ranking quality. This paper investigates the reasons behind this disparity across multiPLMs. Using the web-mined corpora CCMatrix and CCAligned for En$\rightarrow$Si, En$\rightarrow$Ta and Si$\rightarrow$Ta, we show that different multiPLMs (LASER3, XLM-R, and LaBSE) are biased towards certain types of sentences, which allows noisy sentences to creep into the top-ranked samples. We show that by employing a series of heuristics, this noise can be removed to a certain extent. This results in improving the results of NMT systems trained with web-mined corpora and reduces the disparity across multiPLMs.

</details>


### [65] [Are formal and functional linguistic mechanisms dissociated in language models?](https://arxiv.org/abs/2503.11302)

*Michael Hanna, Yonatan Belinkov, Sandro Pezzelle*

**Main category:** cs.CL

**Keywords:** large language models, formal linguistic tasks, functional linguistic tasks

**Relevance Score:** 9

**TL;DR:** This paper investigates the localization of formal and functional linguistic mechanisms in large language models (LLMs) by analyzing computational circuits across various tasks.

**Read time:** 40 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the computational mechanisms behind LLMs' capabilities in formal and functional linguistic tasks can inform their design and application.

**Method:** The study compares 'circuits', or minimal computational subgraphs, responsible for various tasks in 5 LLMs across 10 distinct tasks.

**Key Contributions:**

	1. Identification of distinct circuits for formal and functional tasks in LLMs.
	2. Comparison of task circuits across multiple LLMs.
	3. Insights into the functional architecture of LLMs inspired by neuroscience.

**Result:** It was found that circuits for formal and functional tasks exhibit little overlap, as do circuits for different formal tasks, complicating the search for a unified formal linguistic network.

**Limitations:** The study does not conclusively identify a unified network and focuses on current model limitations.

**Conclusion:** The separation between formal and functional mechanisms suggests potential shared mechanisms for formal tasks, but a single unified network has not yet been identified.

**Abstract:** Although large language models (LLMs) are increasingly capable, these capabilities are unevenly distributed: they excel at formal linguistic tasks, such as producing fluent, grammatical text, but struggle more with functional linguistic tasks like reasoning and consistent fact retrieval. Inspired by neuroscience, recent work suggests that to succeed on both formal and functional linguistic tasks, LLMs should use different mechanisms for each; such localization could either be built-in or emerge spontaneously through training. In this paper, we ask: do current models, with fast-improving functional linguistic abilities, exhibit distinct localization of formal and functional linguistic mechanisms? We answer this by finding and comparing the "circuits", or minimal computational subgraphs, responsible for various formal and functional tasks. Comparing 5 LLMs across 10 distinct tasks, we find that while there is indeed little overlap between circuits for formal and functional tasks, there is also little overlap between formal linguistic tasks, as exists in the human brain. Thus, a single formal linguistic network, unified and distinct from functional task circuits, remains elusive. However, in terms of cross-task faithfulness - the ability of one circuit to solve another's task - we observe a separation between formal and functional mechanisms, suggesting that shared mechanisms between formal tasks may exist.

</details>


### [66] [SaRoHead: Detecting Satire in a Multi-Domain Romanian News Headline Dataset](https://arxiv.org/abs/2504.07612)

*Mihnea-Alexandru Vrlan, Rzvan-Alexandru Smdu, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel*

**Main category:** cs.CL

**Keywords:** satire detection, news headlines, machine learning, deep learning, Romanian language

**Relevance Score:** 4

**TL;DR:** The paper investigates the detection of satirical tone in Romanian news headlines using machine learning and deep learning models, finding that Bidirectional Transformers outperform other approaches.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of detecting satirical tones specifically in news headlines, independent of the main article.

**Method:** The study tests various models, including traditional machine learning algorithms and deep learning models, with a focus on Bidirectional Transformer models and the meta-learning Reptile approach.

**Key Contributions:**

	1. Demonstrated the feasibility of detecting satire in headlines without main article context
	2. Introduced the meta-learning Reptile approach in examining satirical headlines
	3. Showed superiority of Bidirectional Transformer models over traditional methods for this task

**Result:** The experiments reveal that Bidirectional Transformer models significantly surpass standard machine learning methods and LLMs in detecting satire in headlines.

**Limitations:** 

**Conclusion:** The findings suggest that focusing on headlines alone can be a viable method for identifying satirical content, with advanced models providing better accuracy.

**Abstract:** The primary goal of a news headline is to summarize an event in as few words as possible. Depending on the media outlet, a headline can serve as a means to objectively deliver a summary or improve its visibility. For the latter, specific publications may employ stylistic approaches that incorporate the use of sarcasm, irony, and exaggeration, key elements of a satirical approach. As such, even the headline must reflect the tone of the satirical main content. Current approaches for the Romanian language tend to detect the non-conventional tone (i.e., satire and clickbait) of the news content by combining both the main article and the headline. Because we consider a headline to be merely a brief summary of the main article, we investigate in this paper the presence of satirical tone in headlines alone, testing multiple baselines ranging from standard machine learning algorithms to deep learning models. Our experiments show that Bidirectional Transformer models outperform both standard machine-learning approaches and Large Language Models (LLMs), particularly when the meta-learning Reptile approach is employed.

</details>


### [67] [Multilingual Contextualization of Large Language Models for Document-Level Machine Translation](https://arxiv.org/abs/2504.12140)

*Miguel Moura Ramos, Patrick Fernandes, Sweta Agrawal, Andr F. T. Martins*

**Main category:** cs.CL

**Keywords:** Language Models, Document Translation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents a method for enhancing document-level translation using large language models (LLMs) by introducing a curated dataset called DocBlocks for targeted fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improving long-document translation, which remains a challenge due to long-range dependencies and discourse phenomena.

**Method:** Targeted fine-tuning on a curated dataset (DocBlocks) that supports multiple translation paradigms, including direct document-to-document and chunk-level translation.

**Key Contributions:**

	1. Introduction of DocBlocks for document-level translation training
	2. Demonstration of enhanced translation quality using multiple paradigms
	3. Improvement in inference speed over prompting and agent-based methods.

**Result:** Incorporating multiple translation paradigms significantly enhances both the quality and speed of document-level translation compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed approach effectively addresses the challenges in document-level translation by improving cross-sentence dependency capture while maintaining sentence-level performance.

**Abstract:** Large language models (LLMs) have demonstrated strong performance in sentence-level machine translation, but scaling to document-level translation remains challenging, particularly in modeling long-range dependencies and discourse phenomena across sentences and paragraphs. In this work, we propose a method to improve LLM-based long-document translation through targeted fine-tuning on high-quality document-level data, which we curate and introduce as DocBlocks. Our approach supports multiple translation paradigms, including direct document-to-document and chunk-level translation, by integrating instructions both with and without surrounding context. This enables models to better capture cross-sentence dependencies while maintaining strong sentence-level translation performance. Experimental results show that incorporating multiple translation paradigms improves document-level translation quality and inference speed compared to prompting and agent-based methods.

</details>
