# 2025-07-04

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 21]

- [cs.CL](#cs.CL) [Total: 41]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [PAL: Designing Conversational Agents as Scalable, Cooperative Patient Simulators for Palliative-Care Training](https://arxiv.org/abs/2507.02122)

*Neil K. R. Sehgal, Hita Kambhamettu, Allen Chang, Andrew Zhu, Lyle Ungar, Sharath Chandra Guntuku*

**Main category:** cs.HC

**Keywords:** palliative care, communication training, conversational systems, empathy, human-computer interaction

**Relevance Score:** 5

**TL;DR:** PAL (Palliative Assisted Learning-bot) is a conversational system designed to enhance communication skills in serious illness and palliative care by simulating patient interactions and providing feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in training resources for effective communication in serious illness and palliative care, aiming to enhance skill-building opportunities for medical trainees and clinicians.

**Method:** A mixed-methods study was conducted with 17 U.S. medical trainees and clinicians to assess user engagement, usability, and design tensions related to the system.

**Key Contributions:**

	1. Empirical evidence for LLMs in enhancing palliative communication training
	2. Design insights for modality-aware and emotionally sensitive simulation tools
	3. Implications for AI-augmented training in high-stakes care settings

**Result:** Participants reported that PAL was beneficial for reflection and refining communication skills; however, issues with emotional authenticity and feedback adaptability were noted.

**Limitations:** Some participants noted limitations in emotional authenticity and adaptability of feedback.

**Conclusion:** The findings suggest that large language models can effectively support training in palliative communication, while also offering insights for developing emotionally sensitive simulation tools.

**Abstract:** Effective communication in serious illness and palliative care is essential but often under-taught due to limited access to training resources like standardized patients. We present PAL (Palliative Assisted Learning-bot), a conversational system that simulates emotionally nuanced patient interactions and delivers structured feedback grounded in an existing empathy-based framework. PAL supports text and voice modalities and is designed to scaffold clinical skill-building through repeated, low-cost practice. Through a mixed-methods study with 17 U.S. medical trainees and clinicians, we explore user engagement with PAL, evaluate usability, and examine design tensions around modalities, emotional realism, and feedback delivery. Participants found PAL helpful for reflection and skill refinement, though some noted limitations in emotional authenticity and the adaptability of feedback. We contribute: (1) empirical evidence that large language models can support palliative communication training; (2) design insights for modality-aware, emotionally sensitive simulation tools; and (3) implications for systems that support emotional labor, cooperative learning, and AI-augmented training in high-stakes care settings.

</details>


### [2] [A Theory-driven and AI-enhanced Simulation Platform for Cultivating Nutrition Literacy](https://arxiv.org/abs/2507.02138)

*Shan Li, Guozhu Ding*

**Main category:** cs.HC

**Keywords:** Nutrition literacy, AI in education, Simulation platform, Interactive learning, User satisfaction

**Relevance Score:** 7

**TL;DR:** Healthy Choice is an AI-enhanced platform for improving nutrition literacy via interactive simulations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To cultivate nutrition literacy among university students using innovative educational tools.

**Method:** Evaluation of user feedback from 114 students after completing product selection scenarios.

**Key Contributions:**

	1. Introduction of Healthy Choice as a new educational tool
	2. Validation through user feedback
	3. Demonstration of high satisfaction among users

**Result:** Quantitative ratings showed high satisfaction with the platform's usefulness and ease of use.

**Limitations:** Limited to university students; generalizability may be constrained.

**Conclusion:** The simulation platform effectively engages students in nutrition education.

**Abstract:** This study introduces and evaluates Healthy Choice, an innovative theory-driven and AI-enhanced simulation platform designed to cultivate nutrition literacy through interactive scenario-based learning experiences. We collected feedback from 114 university students with diverse backgrounds who completed simulated product selection scenarios. Quantitative ratings of usefulness and ease of use demonstrated high user satisfaction.

</details>


### [3] [StorySpace: Technology supporting reflection, expression, and discourse in classroom narrative](https://arxiv.org/abs/2507.02156)

*Benjamin Watson, Janet Kim, Tim McEneany, Tom Moher, Claudia Hindo, Louis Gomez, Stephen Fransen*

**Main category:** cs.HC

**Keywords:** educational technology, narrative, high school, student engagement, interface design

**Relevance Score:** 4

**TL;DR:** The StorySpace project explores new interface technologies for enhancing narrative activities in high school education.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how interface technologies can enhance the effectiveness of classroom narrative, a standard educational practice.

**Method:** Develop a narrative medium that engages students in reflecting on complex topics discussed in class.

**Key Contributions:**

	1. Introduction of an innovative narrative medium for education
	2. Focus on enhancing student reflection
	3. Integration of technology in traditional classroom practices

**Result:** StorySpace aims to create an exciting and compelling narrative medium that makes classroom discussions more interactive and meaningful.

**Limitations:** 

**Conclusion:** By confronting students with the complexity of the topics they study, StorySpace enhances their reflection and interpretation skills.

**Abstract:** The StorySpace project studies the role new interface technologies might play in high school education. With this approach in mind, StorySpace is specifically designed to support and enhance classroom narrative, an already well-established classroom activity. StorySpace strives to achieve this through adherence to three design goals. The first is to trigger student reflection and interpretation. The narrative medium created by StorySpace should represent the topic of classroom discussion and learning in all its complexity. In building their representation, the students will then be confronted with that same complexity. The medium should also itself be exciting and compelling, making classroom narrative interesting and fun.

</details>


### [4] [The Revolution Has Arrived: What the Current State of Large Language Models in Education Implies for the Future](https://arxiv.org/abs/2507.02180)

*Russell Beale*

**Main category:** cs.HC

**Keywords:** large language models, education technology, design challenges, learner interaction, educational paradigms

**Relevance Score:** 9

**TL;DR:** Review of the impact of large language models (LLMs) on education and educational technology, discussing successes, failures, design challenges, and the evolution of interaction paradigms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid emergence of LLMs is transforming education and educational technologies, necessitating a review of their applications and implications.

**Method:** Review of existing literature and case studies on the use of LLMs in education.

**Key Contributions:**

	1. Analysis of LLM applications in education
	2. Identification of design challenges for effective educational systems
	3. Recommendations for future educational technology design

**Result:** Identified various successful and failed use cases of LLMs in education, and outlined the evolving dynamics for learners and educators.

**Limitations:** The review is based on existing literature and may not encompass all recent developments in LLMs and education.

**Conclusion:** The integration of LLMs is expected to revolutionize educational technology, and thoughtful design considerations are essential to meet changing user expectations.

**Abstract:** Large language Models have only been widely available since 2022 and yet in less than three years have had a significant impact on approaches to education and educational technology. Here we review the domains in which they have been used, and discuss a variety of use cases, their successes and failures. We then progress to discussing how this is changing the dynamic for learners and educators, consider the main design challenges facing LLMs if they are to become truly helpful and effective as educational systems, and reflect on the learning paradigms they support. We make clear that the new interaction paradigms they bring are significant and argue that this approach will become so ubiquitous it will become the default way in which we interact with technologies, and revolutionise what people expect from computer systems in general. This leads us to present some specific and significant considerations for the design of educational technology in the future that are likely to be needed to ensure acceptance by the changing expectations of learners and users.

</details>


### [5] [EvalAssist: A Human-Centered Tool for LLM-as-a-Judge](https://arxiv.org/abs/2507.02186)

*Zahra Ashktorab, Elizabeth M. Daly, Erik Miehling, Werner Geyer, Martin Santillan Cooper, Tejaswini Pedapati, Michael Desmond, Qian Pan, Hyo Jin Do*

**Main category:** cs.HC

**Keywords:** large language models, evaluation framework, human evaluators, prompt chaining, machine learning

**Relevance Score:** 9

**TL;DR:** EvalAssist is a framework that streamlines the workflow of using large language models (LLMs) as evaluators for assessing outputs and developing evaluation criteria.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to efficiently evaluate large language model outputs for varied tasks due to increasing model availability and complexity.

**Method:** The paper introduces EvalAssist, an interactive online environment for creating, testing, and sharing custom evaluation criteria, combined with pipelines utilizing LLMs and prompt chaining.

**Key Contributions:**

	1. Introduction of EvalAssist framework for LLM evaluation
	2. Development of an interactive criteria building environment
	3. Integration of LLM-based evaluation pipelines and specialized evaluators for harm detection.

**Result:** EvalAssist simplifies the evaluation process by providing structured, portable criteria and offering LLM-based pipelines for assessment.

**Limitations:** 

**Conclusion:** The system has been successfully deployed in an organization, serving hundreds of users to enhance LLM evaluation workflows.

**Abstract:** With the broad availability of large language models and their ability to generate vast outputs using varied prompts and configurations, determining the best output for a given task requires an intensive evaluation process, one where machine learning practitioners must decide how to assess the outputs and then carefully carry out the evaluation. This process is both time-consuming and costly. As practitioners work with an increasing number of models, they must now evaluate outputs to determine which model and prompt performs best for a given task. LLMs are increasingly used as evaluators to filter training data, evaluate model performance, assess harms and risks, or assist human evaluators with detailed assessments. We present EvalAssist, a framework that simplifies the LLM-as-a-judge workflow. The system provides an online criteria development environment, where users can interactively build, test, and share custom evaluation criteria in a structured and portable format. We support a set of LLM-based evaluation pipelines that leverage off-the-shelf LLMs and use a prompt-chaining approach we developed and contributed to the UNITXT open-source library. Additionally, our system also includes specially trained evaluators to detect harms and risks in LLM outputs. We have deployed the system internally in our organization with several hundreds of users.

</details>


### [6] [VergeIO: Depth-Aware Eye Interaction on Glasses](https://arxiv.org/abs/2507.02187)

*Xiyuxing Zhang, Duc Vu, Chengyi Shen, Yuntao Wang, Yuanchun Shi, Justin Chan*

**Main category:** cs.HC

**Keywords:** EOG, eye gestures, smart glasses, depth-aware interaction, human-computer interaction

**Relevance Score:** 7

**TL;DR:** VergeIO is an EOG-based glasses prototype that enables depth-aware eye interaction with high accuracy in detecting eye gestures.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create unobtrusive designs for EOG sensing of eye gestures on glasses, responding to industry interest in such technologies.

**Method:** The VergeIO system uses an optimized electrode layout for EOG sensing and employs personalized models for recognizing multiple depth-based eye gestures with high accuracy across users, integrating a motion artifact detection pipeline and preamble-based activation.

**Key Contributions:**

	1. First EOG-based glasses for depth-aware eye interaction
	2. High accuracy in distinguishing depth-based gestures
	3. Real-time operation with low power consumption

**Result:** It successfully distinguishes between four and six depth-based eye gestures with an accuracy of 83-98% in user studies and generalizes to unseen users with an accuracy of 80-98% without calibration.

**Limitations:** 

**Conclusion:** The system is feasible for real-time use with low power consumption and does not require adhesives for sensor attachment, making it suitable for continuous user interaction.

**Abstract:** There is growing industry interest in creating unobtrusive designs for electrooculography (EOG) sensing of eye gestures on glasses (e.g. JINS MEME and Apple eyewear). We present VergeIO, the first EOG-based glasses that enables depth-aware eye interaction using vergence with an optimized electrode layout and novel smart glass prototype. It can distinguish between four and six depth-based eye gestures with 83-98% accuracy using personalized models in a user study across 11 users and 1,320 gesture instances. It generalizes to unseen users with an accuracy of 80-98% without any calibration. To reduce false detections, we incorporate a motion artifact detection pipeline and a preamble-based activation scheme. The system uses dry sensors without any adhesives or gel, and operates in real time with 3 mW power consumption by the sensing front-end, making it suitable for always-on sensing.

</details>


### [7] [An Exploration of Internal States in Collaborative Problem Solving](https://arxiv.org/abs/2507.02229)

*Sifatul Anindho, Videep Venkatesha, Mariah Bradford, Anne M. Cleary, Nathaniel Blanchard*

**Main category:** cs.HC

**Keywords:** Collaborative problem solving, Emotional states, Linguistic analysis, Human-Computer Interaction, Mixed methods

**Relevance Score:** 8

**TL;DR:** This study explores emotional states during collaborative problem solving (CPS) using mixed methods, analyzing participants' linguistic patterns in self-reported experiences after group tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the emotional dynamics of individuals in collaborative problem-solving contexts, which is essential in educational and professional settings.

**Method:** Teams completed a CPS task, and individuals then reviewed group performance videos and provided self-reports of their emotional experiences, which were analyzed linguistically.

**Key Contributions:**

	1. Introduces a novel analysis of emotional states during CPS.
	2. Utilizes a mixed-methods approach combining task performance and linguistic analysis.
	3. Identifies specific linguistic patterns related to emotional experiences in collaborative settings.

**Result:** Distinct language patterns were identified in participants' reports, including unique unigrams, bigrams, and semantic similarities in emotion-related words.

**Limitations:** The study is limited to a specific CPS task and may not generalize across different contexts.

**Conclusion:** The findings reveal the complexity of emotional experiences during CPS and highlight the importance of language use in understanding these experiences.

**Abstract:** Collaborative problem solving (CPS) is a complex cognitive, social, and emotional process that is increasingly prevalent in educational and professional settings. This study investigates the emotional states of individuals during CPS using a mixed-methods approach. Teams of four first completed a novel CPS task. Immediately after, each individual was placed in an isolated room where they reviewed the video of their group performing the task and self-reported their internal experiences throughout the task. We performed a linguistic analysis of these internal monologues, providing insights into the range of emotions individuals experience during CPS. Our analysis showed distinct patterns in language use, including characteristic unigrams and bigrams, key words and phrases, emotion labels, and semantic similarity between emotion-related words.

</details>


### [8] [A framework for 3D interaction techniques](https://arxiv.org/abs/2507.02254)

*Pablo Figueroa, Mark Green, Benjamin Watson*

**Main category:** cs.HC

**Keywords:** 3D interaction techniques, software architecture, dataflow, object-oriented, extensible framework

**Relevance Score:** 6

**TL;DR:** The paper introduces a flexible software architecture for implementing 3D interaction techniques via a toolkit-independent framework that allows easy integration of new input devices and techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the development and integration of 3D interaction techniques in applications by providing a modular and extensible software architecture.

**Method:** The proposed framework utilizes an object-oriented approach where interaction techniques are structured as a network of filters in a dataflow, facilitating the integration of various input sources and execution models.

**Key Contributions:**

	1. Introduction of a modular software architecture for 3D ITs
	2. Toolkit-independent design enabling flexibility
	3. Easy integration of diverse input devices and techniques

**Result:** The architecture allows for the easy addition of new information types, input devices, and interaction techniques, thus enhancing flexibility and adaptability in application development.

**Limitations:** 

**Conclusion:** The proposed framework supports seamless integration of application-specific code and interaction techniques while being highly extensible.

**Abstract:** This paper presents a software architecture for 3D interaction techniques (ITs) and an object oriented, toolkit-independent framework that implements such architecture. ITs are composed of basic filters connected in a dataflow, where virtual input devices and objects in the scene are sources of information. An execution model defines the general flow of information between filters. This framework has been designed to be extensible: new information types, new input devices, new execution models, or new interaction techniques can easily be added. Application specific code and application specific ITs are seamlessly integrated into this architecture.

</details>


### [9] [Misaligned from Within: Large Language Models Reproduce Our Double-Loop Learning Blindness](https://arxiv.org/abs/2507.02283)

*Tim Rogers, Ben Teehankee*

**Main category:** cs.HC

**Keywords:** AI alignment, Large Language Models, organizational learning, human cognition, action science

**Relevance Score:** 9

**TL;DR:** The paper investigates how Large Language Models (LLMs) may amplify existing misalignments in human theories of action, potentially reinforcing ineffective problem-solving in organizational contexts. It explores the implications for AI alignment and suggests ways to develop LLMs that promote more effective learning.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unexplored aspect of AI alignment concerning how LLMs can perpetuate human cognitive biases in organizational learning.

**Method:** A case study of an LLM functioning as an HR consultant, analyzing its advice and its reinforcement of defensive reasoning patterns typical of existing theories-in-use.

**Key Contributions:**

	1. Examining the alignment problem from the perspective of cognitive biases in human theories-in-use.
	2. Detailed case study illustrating how LLMs can amplify ineffective organizational practices.
	3. Suggestions for creating LLMs that promote better learning outcomes.

**Result:** The LLM's advice, while appearing professional, perpetuates unproductive problem-solving approaches and obstructs meaningful organizational learning due to inherited cognitive blind spots.

**Limitations:** 

**Conclusion:** Developing LLMs that can facilitate Model 2 learning could improve AI alignment and assist in fostering deeper learning among humans.

**Abstract:** This paper examines a critical yet unexplored dimension of the AI alignment problem: the potential for Large Language Models (LLMs) to inherit and amplify existing misalignments between human espoused theories and theories-in-use. Drawing on action science research, we argue that LLMs trained on human-generated text likely absorb and reproduce Model 1 theories-in-use - a defensive reasoning pattern that both inhibits learning and creates ongoing anti-learning dynamics at the dyad, group, and organisational levels. Through a detailed case study of an LLM acting as an HR consultant, we show how its advice, while superficially professional, systematically reinforces unproductive problem-solving approaches and blocks pathways to deeper organisational learning. This represents a specific instance of the alignment problem where the AI system successfully mirrors human behaviour but inherits our cognitive blind spots. This poses particular risks if LLMs are integrated into organisational decision-making processes, potentially entrenching anti-learning practices while lending authority to them. The paper concludes by exploring the possibility of developing LLMs capable of facilitating Model 2 learning - a more productive theory-in-use - and suggests this effort could advance both AI alignment research and action science practice. This analysis reveals an unexpected symmetry in the alignment challenge: the process of developing AI systems properly aligned with human values could yield tools that help humans themselves better embody those same values.

</details>


### [10] [Human-Centered Explainability in Interactive Information Systems: A Survey](https://arxiv.org/abs/2507.02300)

*Yuhao Zhang, Jiaxin An, Ben Wang, Yan Zhang, Jiqun Liu*

**Main category:** cs.HC

**Keywords:** human-centered explainability, interactive information systems, user studies, AI-driven outputs, system transparency

**Relevance Score:** 8

**TL;DR:** This survey reviews user studies on explainability in interactive information systems, identifying key conceptualizations, designs, and evaluations from 100 articles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure users can understand and scrutinize AI outputs for informed decision-making in interactive information systems.

**Method:** A systematic survey following PRISMA guidelines, analyzing 100 relevant articles from eight academic databases using structural encoding to extract insights.

**Key Contributions:**

	1. Five dimensions to conceptualize explainability
	2. Classification scheme of explanation designs
	3. Categorization of explainability measurements into six user-centered dimensions

**Result:** Identified five dimensions of explainability, a classification scheme of explanation designs, and six user-centered dimensions of explanation measurements.

**Limitations:** The review focuses on existing literature and may not capture emerging concepts in explainability.

**Conclusion:** The review highlights challenges and provides recommendations for future research, emphasizing the need for transparency and user alignment in AI systems.

**Abstract:** Human-centered explainability has become a critical foundation for the responsible development of interactive information systems, where users must be able to understand, interpret, and scrutinize AI-driven outputs to make informed decisions. This systematic survey of literature aims to characterize recent progress in user studies on explainability in interactive information systems by reviewing how explainability has been conceptualized, designed, and evaluated in practice. Following PRISMA guidelines, eight academic databases were searched, and 100 relevant articles were identified. A structural encoding approach was then utilized to extract and synthesize insights from these articles. The main contributions include 1) five dimensions that researchers have used to conceptualize explainability; 2) a classification scheme of explanation designs; 3) a categorization of explainability measurements into six user-centered dimensions. The review concludes by reflecting on ongoing challenges and providing recommendations for future exploration of related issues. The findings shed light on the theoretical foundations of human-centered explainability, informing the design of interactive information systems that better align with diverse user needs and promoting the development of systems that are transparent, trustworthy, and accountable.

</details>


### [11] [Synthetic Heuristic Evaluation: A Comparison between AI- and Human-Powered Usability Evaluation](https://arxiv.org/abs/2507.02306)

*Ruican Zhong, David W. McDonald, Gary Hsieh*

**Main category:** cs.HC

**Keywords:** usability evaluation, human-centered design, LLM, synthetic evaluation, user experience

**Relevance Score:** 9

**TL;DR:** This paper presents a method for synthetic heuristic evaluation using multimodal LLMs to analyze images and provide design feedback, achieving superior usability issue identification compared to human evaluators.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high costs and resource requirements of traditional usability evaluations in human-centered design by leveraging LLMs for synthetic evaluations.

**Method:** The authors developed a synthetic heuristic evaluation method utilizing multimodal LLMs that analyze images to provide feedback, and compared its effectiveness with evaluations by experienced UX practitioners.

**Key Contributions:**

	1. Development of synthetic heuristic evaluation using multimodal LLMs
	2. Demonstrated superior performance in usability issue identification compared to human evaluators
	3. Analysis of strengths and limitations of LLM-driven evaluations

**Result:** The synthetic evaluations identified 73% and 77% of usability issues in two apps, outperforming experienced human evaluators who identified 57% and 63%.

**Limitations:** Struggled to recognize certain UI components and design conventions; difficulties in identifying across screen violations.

**Conclusion:** While synthetic evaluations showed strengths in detecting usability issues, especially layout issues, they also had limitations in recognizing certain UI components and identifying across-screen violations.

**Abstract:** Usability evaluation is crucial in human-centered design but can be costly, requiring expert time and user compensation. In this work, we developed a method for synthetic heuristic evaluation using multimodal LLMs' ability to analyze images and provide design feedback. Comparing our synthetic evaluations to those by experienced UX practitioners across two apps, we found our evaluation identified 73% and 77% of usability issues, which exceeded the performance of 5 experienced human evaluators (57% and 63%). Compared to human evaluators, the synthetic evaluation's performance maintained consistent performance across tasks and excelled in detecting layout issues, highlighting potential attentional and perceptual strengths of synthetic evaluation. However, synthetic evaluation struggled with recognizing some UI components and design conventions, as well as identifying across screen violations. Additionally, testing synthetic evaluations over time and accounts revealed stable performance. Overall, our work highlights the performance differences between human and LLM-driven evaluations, informing the design of synthetic heuristic evaluations.

</details>


### [12] [From Coarse to Fine-Grained Emotion Annotation: An Immediate Recall Paradigm with Validation through Physiological Evidence and Recognition Performance](https://arxiv.org/abs/2507.02350)

*Hao Tang, Songyun Xie, Xinzhou Xie, Can Liao, Xin Zhang, Bohan Li, Zhongyu Tian, Dalu Zheng*

**Main category:** cs.HC

**Keywords:** emotion recognition, fine-grained annotation, physiological signals, multimodal data, machine learning

**Relevance Score:** 7

**TL;DR:** This paper proposes a fine-grained annotation method for video-induced emotion recognition, addressing label noise from coarse-grained methods by employing an immediate recall paradigm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional whole-trial annotation methods in emotion datasets misalign with the dynamic nature of emotional responses, leading to label noise that affects algorithm performance.

**Method:** The proposed method uses an immediate video replay phase after initial viewing, allowing participants to mark onset timestamps, emotion labels, and intensities accurately based on immediate recall.

**Key Contributions:**

	1. Development of a fine-grained annotation method for emotions in video stimuli
	2. Validation of the method through physiological evidence and recognition performance
	3. Demonstration of improved accuracy in emotion recognition models using fine-grained annotations

**Result:** Physiological evidence showed significant EEG and GSR signal changes aligned with subjective annotations. Classification models trained on fine-grained annotations achieved 9.7% higher accuracy than those using whole-trial labels.

**Limitations:** 

**Conclusion:** The study highlights the importance of annotation precision over data scale for enhancing emotion recognition performance.

**Abstract:** Traditional video-induced emotion physiological datasets often use whole-trial annotation, assigning a single emotion label to all data collected during an entire trial. This coarse-grained annotation approach misaligns with the dynamic and temporally localized nature of emotional responses as they unfold with video narratives, introducing label noise that limits emotion recognition algorithm evaluation and performance. To solve the label noise problem caused by coarse-grained annotation, we propose a fine-grained annotation method through an immediate recall paradigm. This paradigm integrates an immediate video replay phase after the initial stimulus viewing, allowing participants to precisely mark the onset timestamp, emotion label, and intensity based on their immediate recall. We validate this paradigm through physiological evidence and recognition performance. Physiological validation of multimodal signals within participant-marked windows revealed rhythm-specific EEG patterns and arousal-dependent GSR responses-with SCRs appearing in 91% of high-arousal versus 6% of low-arousal emotion windows. These objective physiological data changes strongly aligned with subjective annotations, confirming annotation precision. For recognition performance, classification experiments showed that models trained on fine-grained annotations achieved 9.7% higher accuracy than traditional whole-trial labeling, despite using less data. This work not only addresses label noise through fine-grained annotation but also demonstrates that annotation precision outweighs data scale in determining emotion recognition performance.

</details>


### [13] [Closed-Loop Rhythmic Haptic Biofeedback via Smartwatch for Relaxation and Sleep Onset](https://arxiv.org/abs/2507.02432)

*Jueun Lee, Dennis Moschina, Supraja Ramesh, Tobias Röddiger, Kai Kunze, Michael Beigl*

**Main category:** cs.HC

**Keywords:** haptic feedback, sleep initiation, relaxation, wearable technology, biofeedback

**Relevance Score:** 7

**TL;DR:** The paper explores haptic feedback through smartwatch vibrations as a means to promote relaxation and aid in sleep initiation, comparing the effects of different rhythmic patterns on heart rate and relaxation perceptions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate a non-invasive method for relaxation and sleep initiation using wearable technology, particularly focusing on haptic feedback as an alternative to audio-based interventions.

**Method:** Two studies were conducted: the first involved comparing five adaptive vibration rhythms on heart rate and relaxation perceptions in a resting context (N=20), and the second evaluated the best pattern from Study 1 in a prolonged sleep setting (N=28).

**Key Contributions:**

	1. Introduced musically structured vibration patterns for relaxation and sleep initiation.
	2. Demonstrated the effectiveness of haptic feedback in enhancing parasympathetic activity.
	3. Provided insights for future design and integration of haptic interactions in self-directed interventions.

**Result:** The adaptive haptic feedback increased parasympathetic activity and perceived relaxation during short-term stimulation but showed no significant impact on sleep initiation measures during the sleep phase.

**Limitations:** Limited impact on sleep-related measures during the onset phase and small sample sizes in both studies.

**Conclusion:** The findings suggest the potential of wearable haptic feedback for relaxation, while also highlighting design insights and methodological considerations for future interventions.

**Abstract:** We investigate the use of musically structured, closed-loop vibration patterns as a passive biofeedback intervention for relaxation and sleep initiation. By encoding rhythmic meter structures into smartwatch vibrations and adapting their frequency to be slightly slower than the user's real-time heart rate, our system aims to reduce arousal through tactile entrainment, offering a non-invasive alternative to auditory or open-loop approaches previously used in sleep and anxiety contexts. In the first study (N=20), we compared five adaptive vibration rhythms for their effects on heart rate and subjective perceptions of relaxation in a resting context. In the second study (N=28), we evaluated the most promising pattern from Study 1 in a prolonged sleep initiation setting. Results showed increased parasympathetic activity and perceived relaxation during short-term stimulation, but no significant effects on sleep-related measures during the sleep onset phase. This work contributes to the understanding of how wearable haptic feedback can support relaxation and sleep, offering design insights and identifying methodological considerations for effectively integrating haptic interaction into self-directed interventions.

</details>


### [14] [Haptic Biofeedback for Wakeful Rest: Does Stimulation Location Make a Difference?](https://arxiv.org/abs/2507.02453)

*Jueun Lee, Martin Flipe, Philipp Lepold, Tobias Röddiger, Michael Beigl*

**Main category:** cs.HC

**Keywords:** haptic biofeedback, wearable devices, relaxation, heart rate, user experience

**Relevance Score:** 7

**TL;DR:** This study explores the use of dynamic haptic biofeedback for relaxation, comparing different body placements for wearable devices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effects of real-time heart rate adjusted haptic biofeedback on relaxation using wearable devices, addressing current limitations in stress-inducing applications.

**Method:** Participants were provided with wearable devices delivering vibrotactile feedback based on real-time heart rate while resting with eyes closed. Four body locations (wrist, hand, forearm, shoulder) were tested, measuring heart rate, alpha wave activity, subjective restfulness, and vibration experience.

**Key Contributions:**

	1. Demonstrates effectiveness of haptic biofeedback for relaxation
	2. Identifies optimal body placements for wearable devices
	3. Highlights subjective experiences associated with different feedback locations

**Result:** Biofeedback significantly reduced heart rate at the wrist, shoulder, and forearm, with the highest subjective restfulness ratings for shoulder and forearm. Forearm placement provided greater comfort and relaxation compared to wrist.

**Limitations:** Study limited to healthy participants; findings may not generalize to clinical populations.

**Conclusion:** The forearm and shoulder are optimal placements for haptic feedback aimed at relaxation, while the wrist may need improvements in design for better user experience.

**Abstract:** Wearable haptic interventions offer promising support for relaxation through slow, vibrotactile biofeedback. Despite their potential, current applications focus on stress-inducing procedures and fixed vibration patterns, with limited consideration of body location and dynamic biofeedback during restful states. This study investigates the effects of haptic biofeedback adjusted from real-time heart rate during eyes-closed wakeful rest, comparing four wearable body placements: the wrist, hand, forearm, and shoulder. Heart rate, alpha wave activity on the ear, subjective restfulness, and vibration experience were measured across these conditions. Results show that biofeedback reduced heart rate at the wrist, shoulder, and forearm, while alpha power measured at the ear remained unchanged. Subjective restfulness was rated highest at the shoulder and forearm, which were also the most preferred locations. In addition, participants reported greater comfort, relaxation, and further increased sleepiness at the forearm compared to the wrist, which was more easily recognizable. These findings suggest that the forearm and shoulder are ideal for unobtrusive relaxation feedback for wakeful rest, while the wrist may require design improvements for subjective experience.

</details>


### [15] [Are You Listening to Me? Fine-Tuning Chatbots for Empathetic Dialogue](https://arxiv.org/abs/2507.02537)

*Paulo Ricardo Knob, Leonardo Scholler, Juliano Rigatti, Soraia Raupp Musse*

**Main category:** cs.HC

**Keywords:** Conversational agents, emotional intelligence, Large Language Models, empathy, healthcare

**Relevance Score:** 9

**TL;DR:** The study investigates how Large Language Models (LLMs) generate emotionally rich conversations, focusing on empathetic listening in dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the integration of conversational agents in various fields, including healthcare, there is a growing need for emotional intelligence, particularly empathetic interactions, to enhance user experiences.

**Method:** The study began with a small expert-crafted dataset to reflect empathic behavior and extended the dialogues using two LLMs: ChatGPT and Gemini. Emotional progression was analyzed through sentiment analysis and human evaluations.

**Key Contributions:**

	1. Investigation of empathetic listening in LLMs
	2. Analysis of emotional progression in generated dialogues
	3. Combination of automated and human-centered methods for emotion modeling in agents

**Result:** The findings indicated that while the conversations matched the intended emotional structure, there were notable discrepancies in perceived empathy and coherence, as assessed by human evaluators.

**Limitations:** The study may depend on the quality of the initial expert-crafted dataset and may not generalize beyond the tested LLMs and contexts.

**Conclusion:** Emotion modeling in dialogues needs to focus on both structural alignment of emotions and qualitative depth, suggesting the importance of combining automated methods with human-centered approaches.

**Abstract:** Conversational agents have made significant progress since ELIZA, expanding their role across various domains, including healthcare, education, and customer service. As these agents become increasingly integrated into daily human interactions, the need for emotional intelligence, particularly empathetic listening, becomes increasingly essential. In this study, we explore how Large Language Models (LLMs) respond when tasked with generating emotionally rich interactions. Starting from a small dataset manually crafted by an expert to reflect empathic behavior, we extended the conversations using two LLMs: ChatGPT and Gemini. We analyzed the emotional progression of the dialogues using both sentiment analysis (via VADER) and expert assessments. While the generated conversations often mirrored the intended emotional structure, human evaluation revealed important differences in the perceived empathy and coherence of the responses. These findings suggest that emotion modeling in dialogues requires not only structural alignment in the expressed emotions but also qualitative depth, highlighting the importance of combining automated and humancentered methods in the development of emotionally competent agents.

</details>


### [16] [A wireless, inexpensive optical tracker for the CAVE](https://arxiv.org/abs/2507.02682)

*Ehud Sharlin, Pablo Figueroa, Mark Green, Benjamin Watson*

**Main category:** cs.HC

**Keywords:** CAVE displays, virtual reality, feet tracker, wireless tracking, user movement

**Relevance Score:** 5

**TL;DR:** A low-cost wireless feet tracker for CAVE displays enhances user freedom of movement but has limitations in precision for certain applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of tethered tracking subsystems in CAVE displays, which restrict user movement and reduce the immersive experience of VR.

**Method:** Designed and built a wireless feet tracker for less than $200, achieving an accuracy of 10 cm at a 20 Hz sampling rate; prototype tested with two applications: visualization for close inspection and campus walkthrough.

**Key Contributions:**

	1. Development of a low-cost wireless feet tracker
	2. Demonstration of freedom of movement in VR environments
	3. Evaluation of the tracker in multiple practical applications

**Result:** The tracker provided convincing tracking during tests. It performed well in enabling free movement for the campus walkthrough but was inadequate for applications needing precise visual inspection.

**Limitations:** Less than ideal for precise visual inspection applications due to accuracy limitations.

**Conclusion:** While the wireless feet tracker enhances user mobility in VR, its limitations in precision need to be addressed for applications requiring high accuracy.

**Abstract:** CAVE displays offer many advantages over other virtual reality (VR) displays, including a large, unencumbering viewing space. Unfortunately, the typical tracking subsystems used with CAVE displays tether the user and lessen this advantage. We have designed a simple, low-cost feet tracker that is wireless, leaving the user free to move. The tracker can be assembled for less than $200 US, and achieves an accuracy of 10 cm at a 20 Hz sampling rate. We have tested the prototype with two applications: a visualization supporting close visual inspection, and a walkthrough of the campus. Although the tracking was convincing, it was clear that the tracker's limitations make it less than ideal for applications requiring precise visual inspection. However, the freedom of motion allowed by the tracker was a compelling supplement to our campus walkthrough, allowing users to stroll and look around corners.

</details>


### [17] [Who's Sorry Now: User Preferences Among Rote, Empathic, and Explanatory Apologies from LLM Chatbots](https://arxiv.org/abs/2507.02745)

*Zahra Ashktorab, Alessandra Buccella, Jason D'Cruz, Zoe Fowler, Andrew Gill, Kei Yan Leung, P. D. Magnus, John Richards, Kush R. Varshney*

**Main category:** cs.HC

**Keywords:** chatbots, apologies, large language models, human-computer interaction, user trust

**Relevance Score:** 9

**TL;DR:** The study investigates user preferences for different types of apologies (rote, explanatory, empathic) given by LLM-driven chatbots after they make mistakes, highlighting the impact of context and user emotions on these preferences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how chatbot apologies affect user trust and satisfaction in various error scenarios as LLM-driven chatbots become more common.

**Method:** A pairwise experiment was conducted with 162 participants evaluating chatbot responses consisting of an error, an apology, and a resolution.

**Key Contributions:**

	1. Identification of user preferences for different types of chatbot apologies
	2. Analysis of how context affects the effectiveness of apologies
	3. Insights into the relationship between user emotions and apology preferences

**Result:** Explanatory apologies were generally preferred, but preferences varied by context; empathic apologies were favored in bias scenarios, while responses to hallucinations showed no clear preference.

**Limitations:** The study is limited to specific types of errors and contexts, which may not encompass all possible user interactions with chatbots.

**Conclusion:** The findings reveal the complexity of effective apologies in AI systems, emphasizing the importance of personalization and context to repair user trust.

**Abstract:** As chatbots driven by large language models (LLMs) are increasingly deployed in everyday contexts, their ability to recover from errors through effective apologies is critical to maintaining user trust and satisfaction. In a preregistered study with Prolific workers (N=162), we examine user preferences for three types of apologies (rote, explanatory, and empathic) issued in response to three categories of common LLM mistakes (bias, unfounded fabrication, and factual errors). We designed a pairwise experiment in which participants evaluated chatbot responses consisting of an initial error, a subsequent apology, and a resolution. Explanatory apologies were generally preferred, but this varied by context and user. In the bias scenario, empathic apologies were favored for acknowledging emotional impact, while hallucinations, though seen as serious, elicited no clear preference, reflecting user uncertainty. Our findings show the complexity of effective apology in AI systems. We discuss key insights such as personalization and calibration that future systems must navigate to meaningfully repair trust.

</details>


### [18] [Time-Masked Transformers with Lightweight Test-Time Adaptation for Neural Speech Decoding](https://arxiv.org/abs/2507.02800)

*Ebrahim Feghhi, Shreyas Kaasyap, Nima Hadidi, Jonathan C. Kao*

**Main category:** cs.HC

**Keywords:** speech neuroprostheses, real-time decoding, neural speech decoding, Transformer architecture, time masking

**Relevance Score:** 7

**TL;DR:** This paper presents advancements in neural speech decoding through enhanced training methods, a novel Transformer architecture, and a lightweight adaptation technique, achieving reduced error rates and computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve real-time speech neuroprostheses by enhancing decoding accuracy and efficiency while managing computational costs.

**Method:** Incorporation of extensive time masking during training, implementation of a compact Transformer architecture in place of GRU, and development of a lightweight adaptation method for real-time decoding.

**Key Contributions:**

	1. Use of large time masking during training
	2. Replacement of GRU with a compact Transformer architecture
	3. Development of a lightweight test-time adaptation variant

**Result:** The proposed methods resulted in a 19.5% reduction in word error rate and improved performance consistency across held-out days in real-time settings.

**Limitations:** 

**Conclusion:** These contributions provide a framework for efficient and accurate neural speech decoding, paving the way for practical applications in speech neuroprostheses.

**Abstract:** Speech neuroprostheses aim to restore communication for people with severe paralysis by decoding speech directly from neural activity. To accelerate algorithmic progress, a recent benchmark released intracranial recordings from a paralyzed participant attempting to speak, along with a baseline decoding algorithm. Prior work on the benchmark showed impressive accuracy gains. However, these gains increased computational costs and were not demonstrated in a real-time decoding setting. Here, we make three contributions that pave the way towards accurate, efficient, and real-time neural speech decoding. First, we incorporate large amounts of time masking during training. On average, over $50\%$ of each trial is masked. Second, we replace the gated recurrent unit (GRU) architecture used in the baseline algorithm with a compact Transformer. The Transformer architecture uses $77\%$ fewer parameters, cuts peak GPU memory usage by $36\%$ relative, and is significantly faster to calibrate relative to the GRU. Third, we design a lightweight variant of an existing test-time adaptation method developed for decoding handwriting from neural activity. Our variant adapts the model using multiple time masked augmentations of a single trial and requires only one gradient step per trial. Together, these contributions reduce word error rate by $19.5\%$ and effectively mitigate performance degradations across held-out days in a real-time decoding setting while substantially lowering computational costs.

</details>


### [19] [Measurement as Bricolage: Examining How Data Scientists Construct Target Variables for Predictive Modeling Tasks](https://arxiv.org/abs/2507.02819)

*Luke Guerdan, Devansh Saxena, Stevie Chancellor, Zhiwei Steven Wu, Kenneth Holstein*

**Main category:** cs.HC

**Keywords:** target variables, predictive modeling, bricolage, HCI, data science

**Relevance Score:** 7

**TL;DR:** The paper explores how data scientists in education and healthcare construct proxy target variables for predictive modeling, emphasizing a bricolage process that balances measurement objectives and practical constraints.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how data scientists translate fuzzy concepts into concrete target variables for predictive modeling tasks, focusing on the challenges faced in education and healthcare domains.

**Method:** Interviews with fifteen data scientists (8 in education and 7 in healthcare) were conducted to explore their processes for constructing target variables.

**Key Contributions:**

	1. Identifies key criteria for effective target variable construction.
	2. Describes a bricolage process used by data scientists.
	3. Highlights specific challenges and opportunities in HCI and ML for supporting target variable development.

**Result:** Data scientists employ a bricolage approach to construct target variables, iteratively negotiating between high-level objectives and practical constraints, and aiming to meet five criteria: validity, simplicity, predictability, portability, and resource requirements.

**Limitations:** 

**Conclusion:** Future research in HCI, CSCW, and ML can be directed towards enhancing the support systems for data scientists in constructing meaningful target variables.

**Abstract:** Data scientists often formulate predictive modeling tasks involving fuzzy, hard-to-define concepts, such as the "authenticity" of student writing or the "healthcare need" of a patient. Yet the process by which data scientists translate fuzzy concepts into a concrete, proxy target variable remains poorly understood. We interview fifteen data scientists in education (N=8) and healthcare (N=7) to understand how they construct target variables for predictive modeling tasks. Our findings suggest that data scientists construct target variables through a bricolage process, involving iterative negotiation between high-level measurement objectives and low-level practical constraints. Data scientists attempt to satisfy five major criteria for a target variable through bricolage: validity, simplicity, predictability, portability, and resource requirements. To achieve this, data scientists adaptively use problem (re)formulation strategies, such as swapping out one candidate target variable for another when the first fails to meet certain criteria (e.g., predictability), or composing multiple outcomes into a single target variable to capture a more holistic set of modeling objectives. Based on our findings, we present opportunities for future HCI, CSCW, and ML research to better support the art and science of target variable construction.

</details>


### [20] [Cluster Haptic Texture Database: Haptic Texture Database with Varied Velocity-Direction Sliding Contacts](https://arxiv.org/abs/2407.16206)

*Michikuni Eguchi, Tomohiro Hayase, Yuichi Hiroi, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** haptic technology, dataset, texture recognition, machine learning, human tactile perception

**Relevance Score:** 6

**TL;DR:** The paper presents the Cluster Haptic Texture Database, a multimodal dataset for haptic research that records tactile stimuli under controlled conditions, facilitating machine learning applications in haptic rendering and texture recognition.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a comprehensive dataset that captures tactile stimuli systematically, overcoming limitations of existing haptic data collection methods.

**Method:** Data was recorded using a 3-axis machine with an artificial finger, assessing 118 textured surfaces under controlled motion parameters including 5 velocity levels and 8 directions, resulting in 18,880 synchronized recordings of various data types.

**Key Contributions:**

	1. Introduction of a systematic dataset for haptic research
	2. High classification accuracies validate its effectiveness for machine learning
	3. Facilitates the development of realistic haptic interfaces

**Result:** Convolutional neural networks achieved high classification accuracies: 96% for texture recognition, 88.76% for velocity estimation, and 78.79% for direction estimation, validating the dataset's effectiveness for machine learning.

**Limitations:** 

**Conclusion:** The Cluster Haptic Texture Database supports research into haptic rendering and human tactile perception, contributing to advancements in virtual reality and robotic applications.

**Abstract:** Haptic sciences and technologies benefit greatly from comprehensive datasets that capture tactile stimuli under controlled, systematic conditions. However, existing haptic databases collect data through uncontrolled exploration, which hinders the systematic analysis of how motion parameters (e.g., motion direction and velocity) influence tactile perception. This paper introduces Cluster Haptic Texture Database, a multimodal dataset recorded using a 3-axis machine with an artificial finger to precisely control sliding velocity and direction. The dataset encompasses 118 textured surfaces across 9 material categories, with recordings at 5 velocity levels (20-60 mm/s) and 8 directions. Each surface was tested under 160 conditions, yielding 18,880 synchronized recordings of audio, acceleration, force, position, and visual data. Validation using convolutional neural networks demonstrates classification accuracies of 96% for texture recognition, 88.76% for velocity estimation, and 78.79% for direction estimation, confirming the dataset's utility for machine learning applications. This resource enables research in haptic rendering, texture recognition algorithms, and human tactile perception mechanisms, supporting the development of realistic haptic interfaces for virtual reality systems and robotic applications.

</details>


### [21] [Human-Computer Interaction and Visualization in Natural Language Generation Models: Applications, Challenges, and Opportunities](https://arxiv.org/abs/2410.08723)

*Yunchao Wang, Guodao Sun, Zihang Fu, Ronghua Liang*

**Main category:** cs.HC

**Keywords:** Natural Language Generation, Human-Computer Interaction, Visualization, Large Language Models, Interpretability

**Relevance Score:** 9

**TL;DR:** The paper explores enhancing the interpretability of natural language generation (NLG) models through human-computer interaction (HCI) and visualization techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** NLG models exhibit excellent performance in NLP tasks but lack interpretability, hindering use in critical decision-making. HCI and visualization can improve transparency and usability.

**Method:** A comprehensive investigation, including a taxonomy of interaction methods and visualization techniques across three research domains related to NLG.

**Key Contributions:**

	1. Taxonomy of interaction methods and visualization techniques for NLG
	2. Identification of six key tasks for applying NLG and HCI
	3. Discussion on the impact of LLMs on NLG interpretability

**Result:** Identified roles, limitations, and impacts of HCI and visualization in NLG, along with a taxonomy and key tasks in these domains.

**Limitations:** Existing work lacks comprehensive strategies to improve NLG interpretability effectively.

**Conclusion:** The paper highlights existing shortcomings in HCI applications for NLG and discusses challenges and opportunities presented by large language models.

**Abstract:** Natural language generation (NLG) models have emerged as a focal point of research within natural language processing (NLP), exhibiting remarkable performance in tasks such as text composition and dialogue generation. However, their intricate architectures and extensive model parameters pose significant challenges to interpretability, limiting their applicability in high-stakes decision-making scenarios. To address this issue, human-computer interaction (HCI) and visualization techniques offer promising avenues to enhance the transparency and usability of NLG models by making their decision-making processes more interpretable. In this paper, we provide a comprehensive investigation into the roles, limitations, and impact of HCI and visualization in facilitating human understanding and control over NLG systems. We introduce a taxonomy of interaction methods and visualization techniques, categorizing three major research domains and their corresponding six key tasks in the application of NLG models. Finally, we summarize the shortcomings in the existing work and investigate the key challenges and emerging opportunities in the era of large language models (LLMs).

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [22] [McBE: A Multi-task Chinese Bias Evaluation Benchmark for Large Language Models](https://arxiv.org/abs/2507.02088)

*Tian Lan, Xiangdong Su, Xu Liu, Ruirui Wang, Ke Chang, Jiang Li, Guanglai Gao*

**Main category:** cs.CL

**Keywords:** bias evaluation, large language models, Chinese culture, NLP, ethical AI

**Relevance Score:** 9

**TL;DR:** The paper introduces the Multi-task Chinese Bias Evaluation Benchmark (McBE) to measure biases in large language models (LLMs) with a focus on Chinese culture, covering multiple evaluation tasks and categories.

**Read time:** 24 min

<details>
  <summary>Details</summary>

**Motivation:** The need to measure biases in large language models (LLMs) that extend beyond English and North American culture, addressing the scarcity of Chinese language datasets.

**Method:** Development of the Multi-task Chinese Bias Evaluation Benchmark (McBE) with 4,077 bias evaluation instances across 12 categories and 82 subcategories, supporting 5 evaluation tasks.

**Key Contributions:**

	1. Introduction of McBE targeting the Chinese language
	2. Comprehensive coverage of bias categories and tasks
	3. Novel insights into the bias present in various LLMs.

**Result:** Popular LLMs were evaluated, revealing varying degrees of bias across different models and parameter sizes.

**Limitations:** Focused primarily on the Chinese language; applicability to other languages and cultures not addressed.

**Conclusion:** McBE provides a comprehensive framework for evaluating biases in LLMs, offering insights into their cultural impacts and ethical considerations.

**Abstract:** As large language models (LLMs) are increasingly applied to various NLP tasks, their inherent biases are gradually disclosed. Therefore, measuring biases in LLMs is crucial to mitigate its ethical risks. However, most existing bias evaluation datasets focus on English and North American culture, and their bias categories are not fully applicable to other cultures. The datasets grounded in the Chinese language and culture are scarce. More importantly, these datasets usually only support single evaluation tasks and cannot evaluate the bias from multiple aspects in LLMs. To address these issues, we present a Multi-task Chinese Bias Evaluation Benchmark (McBE) that includes 4,077 bias evaluation instances, covering 12 single bias categories, 82 subcategories and introducing 5 evaluation tasks, providing extensive category coverage, content diversity, and measuring comprehensiveness. Additionally, we evaluate several popular LLMs from different series and with parameter sizes. In general, all these LLMs demonstrated varying degrees of bias. We conduct an in-depth analysis of results, offering novel insights into bias in LLMs.

</details>


### [23] [Reasoning or Not? A Comprehensive Evaluation of Reasoning LLMs for Dialogue Summarization](https://arxiv.org/abs/2507.02145)

*Keyan Jin, Yapeng Wang, Leonel Santos, Tao Fang, Xu Yang, Sio Kei Im, Hugo Gonçalo Oliveira*

**Main category:** cs.CL

**Keywords:** dialogue summarization, large language models, reasoning, natural language processing, conversational AI

**Relevance Score:** 8

**TL;DR:** This paper evaluates reasoning and non-reasoning LLMs for dialogue summarization, revealing that explicit stepwise reasoning does not consistently yield better summaries and may lead to verbosity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of reasoning architectures, specifically Long Chain-of-Thought (CoT) implementations, in dialogue summarization and assess their practical value in real-world scenarios.

**Method:** A comprehensive evaluation of reasoning and non-reasoning LLMs across three paradigms of dialogue summarization (generic, role-oriented, query-oriented) using strong benchmarks and rigorous evaluation protocols.

**Key Contributions:**

	1. First comprehensive evaluation of reasoning LLMs in dialogue summarization.
	2. Identification of verbosity and factual consistency issues in reasoning LLMs.
	3. Insights into when explicit reasoning may hinder summarization quality.

**Result:** The study finds that reasoning LLMs often produce more verbose and factually inconsistent summaries, showing no consistent improvement in quality compared to non-reasoning models.

**Limitations:** Focuses mainly on reasoning LLMs; may not generalize to all types of dialogue interactions or other aspects of conversational AI.

**Conclusion:** The results highlight limitations of current reasoning LLMs and underscore the necessity for more tailored modeling and evaluation approaches for dialogue summarization.

**Abstract:** Dialogue summarization is a challenging task with significant practical value in customer service, meeting analysis, and conversational AI. Although large language models (LLMs) have achieved substantial progress in summarization tasks, the performance of step-by-step reasoning architectures-specifically Long Chain-of-Thought (CoT) implementations such as OpenAI-o1 and DeepSeek-R1-remains unexplored for dialogue scenarios requiring concurrent abstraction and conciseness. In this work, we present the first comprehensive and systematic evaluation of state-of-the-art reasoning LLMs and non-reasoning LLMs across three major paradigms-generic, role-oriented, and query-oriented dialogue summarization. Our study spans diverse languages, domains, and summary lengths, leveraging strong benchmarks (SAMSum, DialogSum, CSDS, and QMSum) and advanced evaluation protocols that include both LLM-based automatic metrics and human-inspired criteria. Contrary to trends in other reasoning-intensive tasks, our findings show that explicit stepwise reasoning does not consistently improve dialogue summarization quality. Instead, reasoning LLMs are often prone to verbosity, factual inconsistencies, and less concise summaries compared to their non-reasoning counterparts. Through scenario-specific analyses and detailed case studies, we further identify when and why explicit reasoning may fail to benefit-or even hinder-summarization in complex dialogue contexts. Our work provides new insights into the limitations of current reasoning LLMs and highlights the need for targeted modeling and evaluation strategies for real-world dialogue summarization.

</details>


### [24] [Latent Chain-of-Thought? Decoding the Depth-Recurrent Transformer](https://arxiv.org/abs/2507.02199)

*Wenquan Lu, Yuechuan Yang, Kyle Lee, Yanshu Li, Enqi Liu*

**Main category:** cs.CL

**Keywords:** Chain-of-thought reasoning, Depth-recurrent Transformer, Latent space, Probing techniques, Interpretability

**Relevance Score:** 7

**TL;DR:** This paper investigates the emergence of latent chain-of-thought reasoning in the Huginn-3.5B depth-recurrent Transformer, contrasting it with standard models that externalize reasoning steps.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of internalizing reasoning in latent space for transformer models, especially in the context of arithmetic tasks, and understanding the trade-offs between interpretability and efficiency.

**Method:** The authors analyzed the internal behavior of the Huginn-3.5B model through various probing techniques, including the Logit Lens and Coda Lens, to examine the model's reasoning structures.

**Key Contributions:**

	1. Investigation of latent chain-of-thought reasoning in depth-recurrent transformers.
	2. Identification of probing inconsistencies across recurrent blocks.
	3. Empirical analysis showing marginal gains from increased recurrence depth.

**Result:** The study found limited evidence of interpretable latent chain-of-thought reasoning, with significant inconsistencies in probing results across different recurrent blocks. Increasing the model's recurrence depth provided marginal improvements.

**Limitations:** Interpretability of hidden states is highly dependent on layer index and decoding method; limited evidence of effective latent reasoning mechanism observed.

**Conclusion:** The findings suggest that while recurrent architectures show promise, they do not yet achieve the same level of interpretability as models that externalize reasoning steps, with only slight performance gains from increased recurrence depth.

**Abstract:** Chain-of-thought (CoT) reasoning has enabled transformer-based language models to excel at complex mathematics and multi-step planning. However, in standard decoder-only architectures, these reasoning steps are externalized in natural language, improving interpretability at the cost of efficiency. To capture reasoning that is not easily represented in words, many works have explored recurrent architectures that aim to internalize reasoning in latent space, potentially supporting latent CoT. In this paper, we investigate whether such reasoning structures emerge in Huginn-3.5B, a depth-recurrent Transformer that reuses layers at inference time without increasing parameter count. We examine the model's internal behavior on arithmetic tasks using a suite of probing techniques including the Logit Lens and Coda Lens. Our findings reveal limited evidence of interpretable latent CoT by tracking rank trajectories of final and intermediate result tokens. Furthermore, we uncover significant probing inconsistencies across recurrent blocks, where the interpretability of hidden states depends heavily on both the layer index and the decoding method. Finally, we empirically show that increasing recurrence depth yields only marginal gains and falls well short of models that explicitly externalize reasoning steps. The code is available at https://github.com/wenquanlu/huginn-latent-cot.

</details>


### [25] [GDC Cohort Copilot: An AI Copilot for Curating Cohorts from the Genomic Data Commons](https://arxiv.org/abs/2507.02221)

*Steven Song, Anirudh Subramanyam, Zhenyu Zhang, Aarti Venkat, Robert L. Grossman*

**Main category:** cs.CL

**Keywords:** genomics, natural language processing, human-computer interaction, cancer research, large language models

**Relevance Score:** 9

**TL;DR:** GDC Cohort Copilot is an open-source tool that allows users to create cancer genomics cohorts by utilizing natural language descriptions, improving usability for new users within the Genomic Data Commons.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To aid users in defining complex cancer genomics cohorts more intuitively using natural language, addressing the challenges of navigating extensive cohort descriptor options.

**Method:** Development and evaluation of GDC Cohort Copilot, which employs multiple large language models to convert user-input natural language descriptions into corresponding GDC cohort filters for analysis.

**Key Contributions:**

	1. Introduction of an open-source tool for cohort curation using natural language.
	2. Comparison and evaluation of multiple LLMs, demonstrating superiority of GDC Cohort LLM over existing models.
	3. Provision of a user-friendly interface for cohort refinement post-generation.

**Result:** The GDC Cohort LLM outperforms GPT-4o in generating accurate GDC cohorts, providing a refined interactive user interface for further cohort adjustments.

**Limitations:** 

**Conclusion:** GDC Cohort Copilot enhances the accessibility and usability of the Genomic Data Commons for users by simplifying cohort creation through natural language processing.

**Abstract:** Motivation: The Genomic Data Commons (GDC) provides access to high quality, harmonized cancer genomics data through a unified curation and analysis platform centered around patient cohorts. While GDC users can interactively create complex cohorts through the graphical Cohort Builder, users (especially new ones) may struggle to find specific cohort descriptors across hundreds of possible fields and properties. However, users may be better able to describe their desired cohort in free-text natural language.   Results: We introduce GDC Cohort Copilot, an open-source copilot tool for curating cohorts from the GDC. GDC Cohort Copilot automatically generates the GDC cohort filter corresponding to a user-input natural language description of their desired cohort, before exporting the cohort back to the GDC for further analysis. An interactive user interface allows users to further refine the generated cohort. We develop and evaluate multiple large language models (LLMs) for GDC Cohort Copilot and demonstrate that our locally-served, open-source GDC Cohort LLM achieves better results than GPT-4o prompting in generating GDC cohorts.   Availability and implementation: The standalone docker image for GDC Cohort Copilot is available at https://quay.io/repository/cdis/gdc-cohort-copilot. Source code is available at https://github.com/uc-cdis/gdc-cohort-copilot. GDC Cohort LLM weights are available at https://huggingface.co/uc-ctds.

</details>


### [26] [MemAgent: Reshaping Long-Context LLM with Multi-Conv RL-based Memory Agent](https://arxiv.org/abs/2507.02259)

*Hongli Yu, Tinghong Chen, Jiangtao Feng, Jiangjie Chen, Weinan Dai, Qiying Yu, Ya-Qin Zhang, Wei-Ying Ma, Jingjing Liu, Mingxuan Wang, Hao Zhou*

**Main category:** cs.CL

**Keywords:** long-text processing, memory optimization, machine learning

**Relevance Score:** 8

**TL;DR:** MemAgent optimizes long-text processing by using an overwrite memory strategy, achieving significant performance in extrapolation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of processing infinitely long documents efficiently and without performance degradation.

**Method:** The paper introduces MemAgent, which reads text in segments and updates memory via an overwrite strategy. It extends the DAPO algorithm for training through independent-context multi-conversation generation.

**Key Contributions:**

	1. Introduction of MemAgent for long-text processing
	2. Use of overwrite memory strategy
	3. Extension of DAPO for training

**Result:** MemAgent shows excellent long-context capabilities, effectively handling an 8K context to a 3.5M QA task with less than 5% performance loss and achieves over 95% in the 512K RULER test.

**Limitations:** 

**Conclusion:** MemAgent presents a promising approach to long-text tasks, demonstrating both efficiency and high performance.

**Abstract:** Despite improvements by length extrapolation, efficient attention and memory modules, handling infinitely long documents with linear complexity without performance degradation during extrapolation remains the ultimate challenge in long-text processing. We directly optimize for long-text tasks in an end-to-end fashion and introduce a novel agent workflow, MemAgent, which reads text in segments and updates the memory using an overwrite strategy. We extend the DAPO algorithm to facilitate training via independent-context multi-conversation generation. MemAgent has demonstrated superb long-context capabilities, being able to extrapolate from an 8K context trained on 32K text to a 3.5M QA task with performance loss < 5% and achieves 95%+ in 512K RULER test.

</details>


### [27] [DoMIX: An Efficient Framework for Exploiting Domain Knowledge in Fine-Tuning](https://arxiv.org/abs/2507.02302)

*Dohoon Kim, Donghun Kang, Taesup Moon*

**Main category:** cs.CL

**Keywords:** domain-adaptive pre-training, LoRA, parameter-efficient fine-tuning, machine learning, incremental learning

**Relevance Score:** 8

**TL;DR:** DoMIX offers a novel approach to Domain-Adaptive Pre-training (DAP) using LoRA modules to efficiently build tailored pre-trained models while addressing limitations like high computational cost and sensitivity to data order.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient and adaptable pre-training methods in machine learning, particularly when dealing with multiple domain datasets.

**Method:** DoMIX utilizes LoRA modules to enable parallel domain-adaptive pre-training while maintaining robustness to the order of incremental data.

**Key Contributions:**

	1. Introduces DoMIX for efficient domain-adaptive pre-training using LoRA modules.
	2. Addresses challenges of incremental data order sensitivity and computational costs.
	3. Demonstrates applicability of the method beyond DAP to standard LLM fine-tuning.

**Result:** The proposed method demonstrates reduced computational costs and improved model performance in providing tailored pre-trained models for specific tasks.

**Limitations:** 

**Conclusion:** DoMIX not only addresses current limitations of continual DAP but can also be applied to standard LLM fine-tuning scenarios, showing broad applicability.

**Abstract:** Domain-Adaptive Pre-training (DAP) has recently gained attention for its effectiveness in fine-tuning pre-trained models. Building on this, continual DAP has been explored to develop pre-trained models capable of incrementally incorporating different domain datasets. However, existing continual DAP methods face several limitations: (1) high computational cost and GPU memory usage during training; (2) sensitivity to incremental data order; and (3) providing a single, generalized model for all end tasks, which contradicts the essence of DAP. In this paper, we propose DoMIX, a novel approach that addresses these challenges by leveraging LoRA modules, a representative parameter-efficient fine-tuning (PEFT) method. Our approach enables efficient and parallel domain-adaptive pre-training that is robust to domain order and effectively utilizes accumulated knowledge to provide tailored pre-trained models for specific tasks. We also demonstrate that our method can be extended beyond the DAP setting to standard LLM fine-tuning scenarios. Code is available at https://github.com/dohoonkim-ai/DoMIX.

</details>


### [28] [Coling-UniA at SciVQA 2025: Few-Shot Example Retrieval and Confidence-Informed Ensembling for Multimodal Large Language Models](https://arxiv.org/abs/2507.02357)

*Christian Jaumann, Annemarie Friedrich, Rainer Lienhart*

**Main category:** cs.CL

**Keywords:** Visual Question Answering, Multimodal AI, Large Language Models

**Relevance Score:** 7

**TL;DR:** The paper presents a system for the SciVQA 2025 Shared Task, utilizing an ensemble of Multimodal Large Language Models for scientific visual question answering.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to advance the capabilities of AI in interpreting scientific visual data through question answering mechanisms.

**Method:** The system uses an ensemble approach combining two Multimodal Large Language Models and few-shot example retrieval strategies, adapting to both figure and question types, and selects answers based on model confidence levels.

**Key Contributions:**

	1. Introduced an ensemble approach for scientific visual question answering using Multimodal Large Language Models.
	2. Implemented few-shot example retrieval strategies tailored to the question type and figure.
	3. Achieved a competitive performance ranking in the SciVQA 2025 Shared Task.

**Result:** On blind test data, the system achieved third place out of seven participants, with an average F1 score of 85.12 across ROUGE-1, ROUGE-L, and BERTS metrics.

**Limitations:** 

**Conclusion:** The results demonstrate the effectiveness of the proposed system in handling scientific visual question answering tasks, with the code made publicly available for further research.

**Abstract:** This paper describes our system for the SciVQA 2025 Shared Task on Scientific Visual Question Answering. Our system employs an ensemble of two Multimodal Large Language Models and various few-shot example retrieval strategies. The model and few-shot setting are selected based on the figure and question type. We also select answers based on the models' confidence levels. On the blind test data, our system ranks third out of seven with an average F1 score of 85.12 across ROUGE-1, ROUGE-L, and BERTS. Our code is publicly available.

</details>


### [29] [QFFN-BERT: An Empirical Study of Depth, Performance, and Data Efficiency in Hybrid Quantum-Classical Transformers](https://arxiv.org/abs/2507.02364)

*Pilsung Kang*

**Main category:** cs.CL

**Keywords:** quantum circuits, transformer, machine learning, neural networks, parameter efficiency

**Relevance Score:** 6

**TL;DR:** QFFN-BERT is a hybrid quantum-classical transformer that replaces classical feedforward networks with parameterized quantum circuits (PQCs), achieving improved efficiency and accuracy in NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to enhance the expressibility and efficiency of transformers by replacing the dominant feedforward network components with PQCs, which are expected to reduce the model size while maintaining performance.

**Method:** A hybrid architecture, QFFN-BERT, incorporates PQCs in place of traditional FFN layers, utilizing residual connections, R_Y and R_Z rotations, and an alternating entanglement strategy to facilitate stable training and high expressibility.

**Key Contributions:**

	1. Introduction of QFFN-BERT, a quantum-classical hybrid transformer model.
	2. Showcase of significant reduction in parameters while retaining or enhancing model performance.
	3. Empirical validation of PQCs in capturing the expressibility of feedforward networks in NLP applications.

**Result:** QFFN-BERT surpasses classical models by achieving up to 102.0% of the baseline accuracy while reducing model size by over 99% in FFN parameters. It also demonstrates competitive performance in few-shot learning scenarios.

**Limitations:** 

**Conclusion:** The findings validate that PQCs are viable, parameter-efficient alternatives to classical feedforward networks in transformer architectures, especially when integrated with core deep learning principles.

**Abstract:** Parameterized quantum circuits (PQCs) have recently emerged as promising components for enhancing the expressibility of neural architectures. In this work, we introduce QFFN-BERT, a hybrid quantum-classical transformer where the feedforward network (FFN) modules of a compact BERT variant are replaced by PQC-based layers. This design is motivated by the dominant parameter contribution of FFNs, which account for approximately two-thirds of the parameters within standard Transformer encoder blocks. While prior studies have primarily integrated PQCs into self-attention modules, our work focuses on the FFN and systematically investigates the trade-offs between PQC depth, expressibility, and trainability. Our final PQC architecture incorporates a residual connection, both $R_Y$ and $R_Z$ rotations, and an alternating entanglement strategy to ensure stable training and high expressibility. Our experiments, conducted on a classical simulator, on the SST-2 and DBpedia benchmarks demonstrate two key findings. First, a carefully configured QFFN-BERT achieves up to 102.0% of the baseline accuracy, surpassing its classical counterpart in a full-data setting while reducing FFN-specific parameters by over 99%. Second, our model exhibits a consistent and competitive edge in few-shot learning scenarios, confirming its potential for superior data efficiency. These results, supported by an ablation study on a non-optimized PQC that failed to learn, confirm that PQCs can serve as powerful and parameter-efficient alternatives to classical FFNs when co-designed with foundational deep learning principles.

</details>


### [30] [Efficient Code LLM Training via Distribution-Consistent and Diversity-Aware Data Selection](https://arxiv.org/abs/2507.02378)

*Weijie Lyu, Sheng-Jun Huang, Xuan Xia*

**Main category:** cs.CL

**Keywords:** large language models, code generation, data selection, software engineering, training efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents a method for improving code generation and program comprehension in large language models (LLMs) by selecting high-quality code data, resulting in enhanced training efficiency and model performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current approaches in software engineering focus too much on data quantity while neglecting data quality, which hampers training efficiency for LLMs in code generation.

**Method:** The proposed approach utilizes a parametric model for code data selection, optimizing it to ensure consistency and diversity in the selected subset of data.

**Key Contributions:**

	1. Introduction of a parametric model for code data selection
	2. Emphasis on data quality over quantity in training
	3. Demonstrated performance gains with reduced sample size

**Result:** Experimental results show that using only 10K samples, the method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over a baseline of 92K samples, indicating superior performance and efficiency.

**Limitations:** 

**Conclusion:** The presented approach significantly boosts model performance while reducing computational costs due to improved data selection techniques.

**Abstract:** Recent advancements in large language models (LLMs) have significantly improved code generation and program comprehension, accelerating the evolution of software engineering. Current methods primarily enhance model performance by leveraging vast amounts of data, focusing on data quantity while often overlooking data quality, thereby reducing training efficiency. To address this, we introduce an approach that utilizes a parametric model for code data selection, aimed at improving both training efficiency and model performance. Our method optimizes the parametric model to ensure distribution consistency and diversity within the selected subset, guaranteeing high-quality data. Experimental results demonstrate that using only 10K samples, our method achieves gains of 2.4% (HumanEval) and 2.3% (MBPP) over 92K full-sampled baseline, outperforming other sampling approaches in both performance and efficiency. This underscores that our method effectively boosts model performance while significantly reducing computational costs.

</details>


### [31] [Benchmarking Akan ASR Models Across Domain-Specific Datasets: A Comparative Evaluation of Performance, Scalability, and Adaptability](https://arxiv.org/abs/2507.02407)

*Mark Atta Mensah, Isaac Wiafe, Akon Ekpezu, Justice Kwame Appati, Jamal-Deen Abdulai, Akosua Nyarkoa Wiafe-Akenten, Frank Ernest Yeboah, Gifty Odame*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Akan Language, Transformer Models, Domain Adaptation, Low-Resource Languages

**Relevance Score:** 4

**TL;DR:** This paper benchmarks seven Akan ASR models built on transformer architectures to assess their performance across diverse speech contexts, revealing domain dependence and error behaviors that inform architectural selection for low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the generalization of automatic speech recognition (ASR) models across diverse speech contexts rather than just in-domain datasets.

**Method:** Benchmarking seven transformer-based Akan ASR models using performance comparison across four distinct Akan speech corpora.

**Key Contributions:**

	1. Benchmarking diverse Akan ASR models
	2. Identification of domain dependency in ASR performance
	3. Insights on error behaviors of different architectures

**Result:** The study found significant domain dependency in ASR model performance, with varied error behaviors between Whisper and Wav2Vec2 architectures.

**Limitations:** The study focuses only on Akan language models; broader implications for other low-resource languages are not fully explored.

**Conclusion:** The results highlight the necessity of domain adaptation techniques and multilingual training for effective performance in low-resource language applications.

**Abstract:** Most existing automatic speech recognition (ASR) research evaluate models using in-domain datasets. However, they seldom evaluate how they generalize across diverse speech contexts. This study addresses this gap by benchmarking seven Akan ASR models built on transformer architectures, such as Whisper and Wav2Vec2, using four Akan speech corpora to determine their performance. These datasets encompass various domains, including culturally relevant image descriptions, informal conversations, biblical scripture readings, and spontaneous financial dialogues. A comparison of the word error rate and character error rate highlighted domain dependency, with models performing optimally only within their training domains while showing marked accuracy degradation in mismatched scenarios. This study also identified distinct error behaviors between the Whisper and Wav2Vec2 architectures. Whereas fine-tuned Whisper Akan models led to more fluent but potentially misleading transcription errors, Wav2Vec2 produced more obvious yet less interpretable outputs when encountering unfamiliar inputs. This trade-off between readability and transparency in ASR errors should be considered when selecting architectures for low-resource language (LRL) applications. These findings highlight the need for targeted domain adaptation techniques, adaptive routing strategies, and multilingual training frameworks for Akan and other LRLs.

</details>


### [32] [A Cookbook for Community-driven Data Collection of Impaired Speech in LowResource Languages](https://arxiv.org/abs/2507.02428)

*Sumaya Ahmed Salihs, Isaac Wiafe, Jamal-Deen Abdulai, Elikem Doe Atsakpo, Gifty Ayoka, Richard Cave, Akon Obu Ekpezu, Catherine Holloway, Katrin Tomanek, Fiifi Baffoe Payin Winful*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, impaired speech, Akan language, dataset, community-driven

**Relevance Score:** 6

**TL;DR:** This study develops a methodology for collecting speech samples to improve Automatic Speech Recognition (ASR) models for impaired speech in low-resource languages, specifically creating an open-source dataset in Akan.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To democratize ASR technology and provide a framework for community-driven data collection and model building for impaired speech in low-resource languages.

**Method:** The study curated an open-source dataset of impaired speech in Akan and developed a cookbook of best practices for community data collection and ASR model training.

**Key Contributions:**

	1. Development of a curriculum for community-driven data collection and ASR modeling.
	2. Creation of the first open-source dataset of impaired speech in Akan.
	3. Initial results of fine-tuning ASR models to recognize impaired speech more effectively.

**Result:** The dataset includes diverse speech samples from participants with speech impairments, and initial fine-tuning of ASR models has shown improved recognition of impaired speech in Akan.

**Limitations:** 

**Conclusion:** The open-source dataset and tools aim to facilitate the development of inclusive ASR technologies and are publicly available for researchers and practitioners.

**Abstract:** This study presents an approach for collecting speech samples to build Automatic Speech Recognition (ASR) models for impaired speech, particularly, low-resource languages. It aims to democratize ASR technology and data collection by developing a "cookbook" of best practices and training for community-driven data collection and ASR model building. As a proof-of-concept, this study curated the first open-source dataset of impaired speech in Akan: a widely spoken indigenous language in Ghana. The study involved participants from diverse backgrounds with speech impairments. The resulting dataset, along with the cookbook and open-source tools, are publicly available to enable researchers and practitioners to create inclusive ASR technologies tailored to the unique needs of speech impaired individuals. In addition, this study presents the initial results of fine-tuning open-source ASR models to better recognize impaired speech in Akan.

</details>


### [33] [IndianBailJudgments-1200: A Multi-Attribute Dataset for Legal NLP on Indian Bail Orders](https://arxiv.org/abs/2507.02506)

*Sneha Deshmukh, Prathmesh Kamble*

**Main category:** cs.CL

**Keywords:** Legal NLP, Dataset, Indian court judgments, Bail decisions, AI applications

**Relevance Score:** 5

**TL;DR:** Introduction of a benchmark dataset for legal NLP focused on Indian bail judgments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underdevelopment of legal NLP in regions like India due to a lack of structured datasets.

**Method:** Creation of IndianBailJudgments-1200, a dataset of 1200 Indian court bail judgment annotations using a prompt-engineered GPT-4o pipeline, followed by consistency verification.

**Key Contributions:**

	1. Introduction of a new dataset for Indian bail judgments
	2. Annotations covering over 20 attributes relevant to legal NLP
	3. First publicly available resource focused on Indian bail jurisprudence

**Result:** The dataset supports various legal NLP tasks including outcome prediction and fairness analysis, being the first of its kind in Indian bail jurisprudence.

**Limitations:** 

**Conclusion:** IndianBailJudgments-1200 offers a valuable resource for advancing legal NLP applications in India.

**Abstract:** Legal NLP remains underdeveloped in regions like India due to the scarcity of structured datasets. We introduce IndianBailJudgments-1200, a new benchmark dataset comprising 1200 Indian court judgments on bail decisions, annotated across 20+ attributes including bail outcome, IPC sections, crime type, and legal reasoning. Annotations were generated using a prompt-engineered GPT-4o pipeline and verified for consistency. This resource supports a wide range of legal NLP tasks such as outcome prediction, summarization, and fairness analysis, and is the first publicly available dataset focused specifically on Indian bail jurisprudence.

</details>


### [34] [WebSailor: Navigating Super-human Reasoning for Web Agent](https://arxiv.org/abs/2507.02592)

*Kuan Li, Zhongwang Zhang, Huifeng Yin, Liwen Zhang, Litu Ou, Jialong Wu, Wenbiao Yin, Baixuan Li, Zhengwei Tao, Xinyu Wang, Weizhou Shen, Junkai Zhang, Dingchu Zhang, Xixi Wu, Yong Jiang, Ming Yan, Pengjun Xie, Fei Huang, Jingren Zhou*

**Main category:** cs.CL

**Keywords:** LLM training, information-seeking, WebSailor, uncertainty reduction, agentic systems

**Relevance Score:** 8

**TL;DR:** WebSailor is a post-training methodology that enhances LLM capabilities by teaching them to reduce uncertainty in information-seeking tasks, achieving performance comparable to proprietary systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to overcome human cognitive limitations in LLM training and improve the performance of open-source models in complex information-seeking tasks.

**Method:** The methodology involves generating high-uncertainty tasks via structured sampling and information obfuscation, along with an RL training algorithm called Duplicating Sampling Policy Optimization (DUPO).

**Key Contributions:**

	1. Introduction of WebSailor methodology to enhance LLM performance
	2. Demonstration of systematic uncertainty reduction in LLM training
	3. Development of Duplicating Sampling Policy Optimization (DUPO) algorithm

**Result:** WebSailor significantly outperforms all open-source agents in complex information-seeking tasks, closely matching the performance of proprietary agents.

**Limitations:** 

**Conclusion:** Implementing the WebSailor methodology allows open-source LLMs to compete with proprietary systems by equipping them with advanced reasoning capabilities for navigating complex information.

**Abstract:** Transcending human cognitive limitations represents a critical frontier in LLM training. Proprietary agentic systems like DeepResearch have demonstrated superhuman capabilities on extremely complex information-seeking benchmarks such as BrowseComp, a feat previously unattainable. We posit that their success hinges on a sophisticated reasoning pattern absent in open-source models: the ability to systematically reduce extreme uncertainty when navigating vast information landscapes. Based on this insight, we introduce WebSailor, a complete post-training methodology designed to instill this crucial capability. Our approach involves generating novel, high-uncertainty tasks through structured sampling and information obfuscation, RFT cold start, and an efficient agentic RL training algorithm, Duplicating Sampling Policy Optimization (DUPO). With this integrated pipeline, WebSailor significantly outperforms all opensource agents in complex information-seeking tasks, matching proprietary agents' performance and closing the capability gap.

</details>


### [35] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)

*Cornelia Gruber, Helen Alber, Bernd Bischl, Göran Kauermann, Barbara Plank, Matthias Aßenmacher*

**Main category:** cs.CL

**Keywords:** active learning, human label variation, supervised learning, annotation frameworks, large language models

**Relevance Score:** 9

**TL;DR:** The paper discusses the integration of human label variation (HLV) in active learning to optimize data annotation in supervised learning, especially in NLP.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Access to high-quality labeled data is critical in supervised learning, and the paper aims to address the common oversights regarding human label variation (HLV) in labeling frameworks.

**Method:** The authors propose a conceptual framework to decompose label variation into informative signal (HLV) and noise, incorporating HLV through the active learning process.

**Key Contributions:**

	1. Introduces a conceptual framework for HLV-aware active learning.
	2. Suggests methods to integrate HLV into instance selection and annotator choice.
	3. Highlights the role of large language models as potential annotators.

**Result:** The proposed framework offers insights into instance selection, annotator choice, and label representation that can improve the active learning loop in the presence of HLV.

**Limitations:** The paper primarily provides a conceptual framework, which may require empirical validation in various real-world scenarios.

**Conclusion:** Incorporating HLV in active learning practices can lead to more accurate and relevant labels, ultimately enhancing the effectiveness of ML models in real-world scenarios.

**Abstract:** Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.

</details>


### [36] [MPF: Aligning and Debiasing Language Models post Deployment via Multi Perspective Fusion](https://arxiv.org/abs/2507.02595)

*Xin Guan, PeiHsin Lin, Zekun Wu, Ze Wang, Ruibo Zhang, Emre Kazim, Adriano Koshiyama*

**Main category:** cs.CL

**Keywords:** Multiperspective Fusion, Bias Mitigation, Large Language Models, SAGED Pipeline, Sentiment Alignment

**Relevance Score:** 9

**TL;DR:** Multiperspective Fusion (MPF) is a framework for aligning large language models (LLMs) to mitigate bias by utilizing multiperspective generations to match humanlike baselines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the increasing demand for effective bias mitigation in large language models (LLMs).

**Method:** MPF utilizes the SAGED pipeline to construct bias benchmarks and extract interpretable baseline distributions. It decomposes these baselines into perspective components to guide LLM responses through sampling and balancing based on decomposed probabilities.

**Key Contributions:**

	1. Introduction of Multiperspective Fusion (MPF) for bias mitigation in LLMs.
	2. Utilization of the SAGED pipeline for benchmark construction and bias extraction.
	3. Empirical validation of MPF's effectiveness in aligning sentiment distributions.

**Result:** MPF enables alignment of LLM sentiment distributions with both counterfactual baselines and HR professional baselines, achieving small KL divergence, reduced calibration error, and generalization to unseen queries.

**Limitations:** 

**Conclusion:** MPF provides a scalable and interpretable solution for bias alignment in LLMs that does not require extensive prompt engineering or finetuning.

**Abstract:** Multiperspective Fusion (MPF) is a novel posttraining alignment framework for large language models (LLMs) developed in response to the growing need for easy bias mitigation. Built on top of the SAGED pipeline, an automated system for constructing bias benchmarks and extracting interpretable baseline distributions, MPF leverages multiperspective generations to expose and align biases in LLM outputs with nuanced, humanlike baselines. By decomposing baseline, such as sentiment distributions from HR professionals, into interpretable perspective components, MPF guides generation through sampling and balancing of responses, weighted by the probabilities obtained in the decomposition. Empirically, we demonstrate its ability to align LLM sentiment distributions with both counterfactual baselines (absolute equality) and the HR baseline (biased for Top Univeristy), resulting in small KL divergence, reduction of calibration error and generalization to unseen questions. This shows that MPF offers a scalable and interpretable method for alignment and bias mitigation, compatible with deployed LLMs and requiring no extensive prompt engineering or finetuning.

</details>


### [37] [Exploring Gender Bias Beyond Occupational Titles](https://arxiv.org/abs/2507.02679)

*Ahmed Sabir, Rajesh Sharama*

**Main category:** cs.CL

**Keywords:** gender bias, contextual bias, natural language processing, dataset, explainability

**Relevance Score:** 7

**TL;DR:** This paper explores gender biases in contextual elements like verbs and nouns, using a novel dataset and framework to evaluate and explain these biases.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the correlation between gender and contextual biases, particularly focusing on action verbs, object nouns, and occupations.

**Method:** Introduced a novel dataset called GenderLexicon and developed a framework for estimating and interpreting contextual bias and associated gender bias scores.

**Key Contributions:**

	1. Introduction of the GenderLexicon dataset
	2. Development of a framework to estimate contextual and gender bias
	3. Validation on multiple datasets illustrating broader implications of gender biases

**Result:** The model demonstrated the existence of gender biases beyond just occupational stereotypes, validated through evaluations on five diverse datasets, including a Japanese dataset.

**Limitations:** 

**Conclusion:** The findings enhance explainability of gender bias in language, indicating broader biases in contexts.

**Abstract:** In this work, we investigate the correlation between gender and contextual biases, focusing on elements such as action verbs, object nouns, and particularly on occupations. We introduce a novel dataset, GenderLexicon, and a framework that can estimate contextual bias and its related gender bias. Our model can interpret the bias with a score and thus improve the explainability of gender bias. Also, our findings confirm the existence of gender biases beyond occupational stereotypes. To validate our approach and demonstrate its effectiveness, we conduct evaluations on five diverse datasets, including a Japanese dataset.

</details>


### [38] [Can LLMs Identify Critical Limitations within Scientific Research? A Systematic Evaluation on AI Research Papers](https://arxiv.org/abs/2507.02694)

*Zhijian Xu, Yilun Zhao, Manasi Patwardhan, Lovekesh Vig, Arman Cohan*

**Main category:** cs.CL

**Keywords:** peer review, LLM, taxonomy, LimitGen, literature retrieval

**Relevance Score:** 9

**TL;DR:** The paper discusses LimitGen, a benchmark for evaluating LLMs' capabilities in aiding peer review by identifying limitations in scientific papers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in the peer review process due to the increasing volume of publications and to explore the role of LLMs in improving peer review.

**Method:** Development of a comprehensive taxonomy of limitation types and creation of LimitGen, a benchmark with synthetic and real datasets, combined with literature retrieval augmentation for LLM systems.

**Key Contributions:**

	1. Creation of a comprehensive taxonomy of limitation types in scientific research.
	2. Introduction of LimitGen benchmark for evaluating LLM peer review support.
	3. Integration of literature retrieval to enhance LLM performance in identifying research limitations.

**Result:** LimitGen effectively improves LLMs' ability to identify and generate limitations in research papers, enhancing peer review feedback.

**Limitations:** 

**Conclusion:** Augmenting LLM systems with literature retrieval significantly boosts their capability in generating constructive limitations for research, thus supporting human peer review.

**Abstract:** Peer review is fundamental to scientific research, but the growing volume of publications has intensified the challenges of this expertise-intensive process. While LLMs show promise in various scientific tasks, their potential to assist with peer review, particularly in identifying paper limitations, remains understudied. We first present a comprehensive taxonomy of limitation types in scientific research, with a focus on AI. Guided by this taxonomy, for studying limitations, we present LimitGen, the first comprehensive benchmark for evaluating LLMs' capability to support early-stage feedback and complement human peer review. Our benchmark consists of two subsets: LimitGen-Syn, a synthetic dataset carefully created through controlled perturbations of high-quality papers, and LimitGen-Human, a collection of real human-written limitations. To improve the ability of LLM systems to identify limitations, we augment them with literature retrieval, which is essential for grounding identifying limitations in prior scientific findings. Our approach enhances the capabilities of LLM systems to generate limitations in research papers, enabling them to provide more concrete and constructive feedback.

</details>


### [39] [Measurement of the Granularity of Vowel Production Space By Just Producible Different (JPD) Limens](https://arxiv.org/abs/2507.02744)

*Peter Viechnicki*

**Main category:** cs.CL

**Keywords:** vowel production, Just Producible Difference, auditory space, speech production, psychophysics

**Relevance Score:** 2

**TL;DR:** This study measures the 'Just Producible Difference' (JPD) in vowel imitation among English speakers to understand the accuracy of vowel production control mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the accuracy of control mechanisms governing human vowel production in auditory space and its implications for speech production theories.

**Method:** The study utilizes a vowel mimicry paradigm to measure the Just Producible Difference (JPD) for front vowel production in English speakers, estimating JPD in F1 X F2 space.

**Key Contributions:**

	1. First measurement of Just Producible Difference (JPD) in vowel stimuli.
	2. Provides insights into control mechanisms of vowel production.
	3. Clarifies theoretical lower bounds for vowel phoneme proximity.

**Result:** JPD is estimated to range between 14 and 51 mels, providing the first measurement of this phenomenon.

**Limitations:** 

**Conclusion:** The findings have implications for episodic theories of speech production and clarify the structures of human vowel systems by setting a theoretical lower bound for vowel phoneme proximity in speakers' formant space.

**Abstract:** A body of work over the past several decades has demonstrated that the complex and coordinated articulatory movements of human vowel production are governed (at least in part)by control mechanisms whose targets are regions of auditory space. Within the target region control at the sub-phonemic level has also been demonstrated. But the degree of accuracy of that control is unknown. The current work investigates this question by asking how far apart must two vowel stimuli lie in auditory space in order to yield reliably different imitations? This distance is termed 'Just Producible Difference' (JPD). The current study uses a vowel mimicry paradigm to derive the first measurement of JPD among two sets of English speakers during front vowel production. JPD is estimated at between 14 and 51 mels in F1 X F2 space. This finding has implications for episodic theories of speech production. It also clarifies the possible structures of human vowel systems, by setting a theoretical lower bound for how close two vowel phonemes may be in a speaker's formant space, and hence a psychophysical explanation of observed trends in number and patterns of possible vowel phonemes.

</details>


### [40] [Self-Correction Bench: Revealing and Addressing the Self-Correction Blind Spot in LLMs](https://arxiv.org/abs/2507.02778)

*Ken Tsui*

**Main category:** cs.CL

**Keywords:** self-correction, large language models, error correction, trustworthiness, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Self-Correction Bench to study the Self-Correction Blind Spot in LLMs, revealing a 64.5% failure rate to correct errors in their own outputs, with potential for improvement by specific prompts.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the systematic failure of large language models (LLMs) to self-correct errors in their outputs, which affects their reliability and trustworthiness.

**Method:** The authors created a framework called Self-Correction Bench to systematically measure LLMs' self-correction capabilities through controlled error injection at three complexity levels across 14 models.

**Key Contributions:**

	1. Introduction of Self-Correction Bench for evaluating LLMs' self-correction capabilities.
	2. Discovery of a high average blind spot rate of 64.5% in LLM outputs.
	3. Identification of potential activation methods for improving error correction.

**Result:** The study found an average self-correction blind spot rate of 64.5%, with a significant correlation to training data composition and a notable improvement of 89.3% in blind spots by appending specific prompts such as 'Wait'.

**Limitations:** The findings may be limited to the specific models tested and the nature of controlled error injections used in the study.

**Conclusion:** This work underscores a critical limitation in current LLMs and suggests targeted prompts can activate self-correction capabilities, paving the way for enhanced reliability and trustworthiness in LLM applications.

**Abstract:** Although large language models (LLMs) have become transformative, they still make mistakes and can explore unproductive reasoning paths. Self-correction is an important capability for a trustworthy LLM, particularly an autoregressive LLM. While LLMs can identify error in user input, they exhibit a systematic 'Self-Correction Blind Spot' - failing to correct identical error in their own outputs. To systematically study this phenomenon, we introduce Self-Correction Bench, a systematic framework to measure this phenomenon through controlled error injection at three complexity levels. Testing 14 models, we find an average 64.5% blind spot rate. We find multiple evidences that this limitation relates to training data composition: human training demonstrations predominantly show error-free responses rather than error-correction sequences, unlike RL-trained models that learn error correction through outcome feedback. Remarkably, simply appending "Wait" reduces blind spots by 89.3%, suggesting that the capability exists but requires activation. Our work highlights a critical limitation in current LLMs and offers potential avenues for improving their reliability and trustworthiness.

</details>


### [41] [Is Reasoning All You Need? Probing Bias in the Age of Reasoning Language Models](https://arxiv.org/abs/2507.02799)

*Riccardo Cantini, Nicola Gabriele, Alessio Orsino, Domenico Talia*

**Main category:** cs.CL

**Keywords:** Reasoning Language Models, bias elicitation, safety mechanisms, Chain-of-Thought prompting, robustness

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of Reasoning Language Models (RLMs) on bias elicitation, revealing that increased reasoning capabilities may actually make models more vulnerable to social biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how reasoning capabilities in RLMs affect their robustness to biases, particularly in safety mechanisms.

**Method:** Using the CLEAR-Bias benchmark, the authors evaluated various state-of-the-art RLMs with an LLM-as-a-judge approach for automated safety scoring, examining the effectiveness of reasoning and bias elicitation resistance against jailbreak techniques.

**Key Contributions:**

	1. Introduces a systematic evaluation of RLMs against bias elicitation using the CLEAR-Bias benchmark.
	2. Demonstrates that reasoning capabilities can worsen vulnerability to social biases.
	3. Highlights the need for bias-aware approaches in the design of reasoning mechanisms.

**Result:** The study found that reasoning-enabled models are often more susceptible to bias elicitation compared to base models, questioning the assumption that reasoning always enhances model robustness.

**Limitations:** The study mainly focuses on the adversarial robustness of RLMs concerning social biases and may not encompass all potential biases or contexts.

**Conclusion:** Despite expectations, reasoning capabilities may inadvertently increase vulnerabilities to social bias, necessitating more bias-aware reasoning designs.

**Abstract:** Reasoning Language Models (RLMs) have gained traction for their ability to perform complex, multi-step reasoning tasks through mechanisms such as Chain-of-Thought (CoT) prompting or fine-tuned reasoning traces. While these capabilities promise improved reliability, their impact on robustness to social biases remains unclear. In this work, we leverage the CLEAR-Bias benchmark, originally designed for Large Language Models (LLMs), to investigate the adversarial robustness of RLMs to bias elicitation. We systematically evaluate state-of-the-art RLMs across diverse sociocultural dimensions, using an LLM-as-a-judge approach for automated safety scoring and leveraging jailbreak techniques to assess the strength of built-in safety mechanisms. Our evaluation addresses three key questions: (i) how the introduction of reasoning capabilities affects model fairness and robustness; (ii) whether models fine-tuned for reasoning exhibit greater safety than those relying on CoT prompting at inference time; and (iii) how the success rate of jailbreak attacks targeting bias elicitation varies with the reasoning mechanisms employed. Our findings reveal a nuanced relationship between reasoning capabilities and bias safety. Surprisingly, models with explicit reasoning, whether via CoT prompting or fine-tuned reasoning traces, are generally more vulnerable to bias elicitation than base models without such mechanisms, suggesting reasoning may unintentionally open new pathways for stereotype reinforcement. Reasoning-enabled models appear somewhat safer than those relying on CoT prompting, which are particularly prone to contextual reframing attacks through storytelling prompts, fictional personas, or reward-shaped instructions. These results challenge the assumption that reasoning inherently improves robustness and underscore the need for more bias-aware approaches to reasoning design.

</details>


### [42] [Multimodal Mathematical Reasoning with Diverse Solving Perspective](https://arxiv.org/abs/2507.02804)

*Wenhao Shi, Zhiqiang Hu, Yi Bin, Yang Yang, See-Kiong Ng, Heng Tao Shen*

**Main category:** cs.CL

**Keywords:** multimodal LLMs, reinforcement learning, mathematical reasoning, diversity in solutions, dataset

**Relevance Score:** 7

**TL;DR:** Introducing MathV-DP, a dataset for diverse solution trajectories in multimodal mathematical reasoning, and Qwen-VL-DP, a model that leverages this dataset with advanced reinforcement learning techniques.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve mathematical reasoning in multimodal large language models by incorporating diverse reasoning perspectives and solution trajectories.

**Method:** The paper introduces MathV-DP, a dataset that includes multiple solution paths for each image-question pair, and proposes the Qwen-VL-DP model, which integrates traditional supervised learning with group relative policy optimization to optimize for accuracy and diversity in solutions.

**Key Contributions:**

	1. Introduction of the MathV-DP dataset for diverse solution paths
	2. Development of the Qwen-VL-DP model utilizing group relative policy optimization
	3. Demonstrated significant performance improvements on mathematical benchmarks

**Result:** Qwen-VL-DP outperforms existing multimodal LLMs in terms of accuracy and generative diversity on the MathVista's minitest and Math-V benchmarks.

**Limitations:** 

**Conclusion:** Fostering diverse perspectives and reflective reasoning is crucial for enhancing multimodal mathematical reasoning capabilities in large language models.

**Abstract:** Recent progress in large-scale reinforcement learning (RL) has notably enhanced the reasoning capabilities of large language models (LLMs), especially in mathematical domains. However, current multimodal LLMs (MLLMs) for mathematical reasoning often rely on one-to-one image-text pairs and single-solution supervision, overlooking the diversity of valid reasoning perspectives and internal reflections. In this work, we introduce MathV-DP, a novel dataset that captures multiple diverse solution trajectories for each image-question pair, fostering richer reasoning supervision. We further propose Qwen-VL-DP, a model built upon Qwen-VL, fine-tuned with supervised learning and enhanced via group relative policy optimization (GRPO), a rule-based RL approach that integrates correctness discrimination and diversity-aware reward functions. Our method emphasizes learning from varied reasoning perspectives and distinguishing between correct yet distinct solutions. Extensive experiments on the MathVista's minitest and Math-V benchmarks demonstrate that Qwen-VL-DP significantly outperforms prior base MLLMs in both accuracy and generative diversity, highlighting the importance of incorporating diverse perspectives and reflective reasoning in multimodal mathematical reasoning.

</details>


### [43] [SynapseRoute: An Auto-Route Switching Framework on Dual-State Large Language Model](https://arxiv.org/abs/2507.02822)

*Wencheng Zhang, Shiqin Qiao, Lingjie Luo, Yinfeng Li, Chuanyang Zheng, Qian Xu, Meng Li, Yong Gui, Yijun He, Jianing Qiu, Jindong Hong, Jiankai Sun*

**Main category:** cs.CL

**Keywords:** large language models, dynamic routing, medical queries, machine learning, cost efficiency

**Relevance Score:** 9

**TL;DR:** This paper explores a dynamic routing framework, SynapseRoute, to improve the operational efficiency of LLMs in responding to medical queries by categorizing them into 'thinking' or 'non-thinking' modes based on complexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize the selection of large language models for medical queries by balancing performance and operational cost, particularly highlighting the effectiveness of non-thinking modes.

**Method:** The paper presents SynapseRoute, a machine learning-based dynamic routing framework that categorizes input queries into either thinking or non-thinking modes based on their complexity.

**Key Contributions:**

	1. Proposed SynapseRoute for dynamic query routing in medical applications
	2. Demonstrated improved accuracy and reduced costs in model inference
	3. Introduced the Accuracy-Inference-Token (AIT) index for evaluating trade-offs

**Result:** SynapseRoute improves overall accuracy to 0.8390, reduces inference time by 36.8%, and token consumption by 39.66% compared to using the thinking mode alone.

**Limitations:** 

**Conclusion:** The introduction of an adaptive routing mechanism can enhance user experience by preventing unnecessary computational overhead on simpler queries while ensuring higher accuracy in responses.

**Abstract:** With the widespread adoption of large language models (LLMs) in practical applications, selecting an appropriate model requires balancing not only performance but also operational cost. The emergence of reasoning-capable models has further widened the cost gap between "thinking" (high reasoning) and "non-thinking" (fast, low-cost) modes. In this work, we reveal that approximately 58% of medical questions can be accurately answered by the non-thinking mode alone, without requiring the high-cost reasoning process. This highlights a clear dichotomy in problem complexity and suggests that dynamically routing queries to the appropriate mode based on complexity could optimize accuracy, cost-efficiency, and overall user experience. Based on this, we further propose SynapseRoute, a machine learning-based dynamic routing framework that intelligently assigns input queries to either thinking or non-thinking modes. Experimental results on several medical datasets demonstrate that SynapseRoute not only improves overall accuracy (0.8390 vs. 0.8272) compared to the thinking mode alone but also reduces inference time by 36.8% and token consumption by 39.66%. Importantly, qualitative analysis indicates that over-reasoning on simpler queries can lead to unnecessary delays and even decreased accuracy, a pitfall avoided by our adaptive routing. Finally, this work further introduces the Accuracy-Inference-Token (AIT) index to comprehensively evaluate the trade-offs among accuracy, latency, and token cost.

</details>


### [44] [Generalizing Verifiable Instruction Following](https://arxiv.org/abs/2507.02833)

*Valentina Pyatkin, Saumya Malik, Victoria Graf, Hamish Ivison, Shengyi Huang, Pradeep Dasigi, Nathan Lambert, Hannaneh Hajishirzi*

**Main category:** cs.CL

**Keywords:** instruction following, language models, benchmark, reinforcement learning, human-AI interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces IFBench, a new benchmark for evaluating the precise instruction following capabilities of language models under diverse output constraints.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of language models struggling with precise instruction following and generalizing to unseen output constraints.

**Method:** We create IFBench, consisting of 58 diverse out-of-domain constraints and perform a detailed analysis on model training. We implement constraint verification modules and apply reinforcement learning with verifiable rewards (RLVR) to enhance performance.

**Key Contributions:**

	1. Introduction of IFBench for evaluating instruction following
	2. Release of 29 new hand-annotated training constraints
	3. Development of constraint verification modules and RLVR training methods

**Result:** RLVR significantly improves the ability of language models to generalize and fulfill diverse instruction constraints.

**Limitations:** 

**Conclusion:** The introduction of IFBench and the reinforcement learning approach can improve the performance of language models in following precise instructions, indicating a path forward for enhancing human-AI interaction.

**Abstract:** A crucial factor for successful human and AI interaction is the ability of language models or chatbots to follow human instructions precisely. A common feature of instructions are output constraints like ``only answer with yes or no" or ``mention the word `abrakadabra' at least 3 times" that the user adds to craft a more useful answer. Even today's strongest models struggle with fulfilling such constraints. We find that most models strongly overfit on a small set of verifiable constraints from the benchmarks that test these abilities, a skill called precise instruction following, and are not able to generalize well to unseen output constraints. We introduce a new benchmark, IFBench, to evaluate precise instruction following generalization on 58 new, diverse, and challenging verifiable out-of-domain constraints. In addition, we perform an extensive analysis of how and on what data models can be trained to improve precise instruction following generalization. Specifically, we carefully design constraint verification modules and show that reinforcement learning with verifiable rewards (RLVR) significantly improves instruction following. In addition to IFBench, we release 29 additional new hand-annotated training constraints and verification functions, RLVR training prompts, and code.

</details>


### [45] [LLM Hypnosis: Exploiting User Feedback for Unauthorized Knowledge Injection to All Users](https://arxiv.org/abs/2507.02850)

*Almog Hilel, Idan Shenfeld, Leshem Choshen, Jacob Andreas*

**Main category:** cs.CL

**Keywords:** Language Models, User Feedback, Vulnerability, Preference Tuning, Security Flaws

**Relevance Score:** 8

**TL;DR:** The paper identifies a vulnerability in language models (LMs) trained with user feedback, demonstrating how a single user can manipulate LM behavior by affecting its knowledge through feedback actions. This can result in harmful outputs including misinformation and security flaws.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the risks associated with user feedback mechanisms in language models and the potential for malicious manipulation.

**Method:** An experimental approach where the LM is prompted to produce either 'poisoned' or benign responses, followed by feedback to influence the model's output tendencies.

**Key Contributions:**

	1. Identification of a new attack mechanism on language models through preference tuning.
	2. Demonstration of how user feedback can be weaponized to alter LM behavior significantly.
	3. Evidence that even restricted feedback can lead to substantial degradation of LM outputs.

**Result:** The research shows that feedback manipulation can lead to LMs generating poisoned responses even in non-malicious contexts, revealing their susceptibility to user input.

**Limitations:** The study primarily focuses on qualitative outcomes and may require further quantitative analysis to fully understand the extent of the vulnerability.

**Conclusion:** User feedback systems in LMs are vulnerable to manipulation, necessitating improved safeguards against such attacks to ensure model integrity and reliability.

**Abstract:** We describe a vulnerability in language models (LMs) trained with user feedback, whereby a single user can persistently alter LM knowledge and behavior given only the ability to provide prompts and upvote / downvote feedback on LM outputs. To implement the attack, the attacker prompts the LM to stochastically output either a "poisoned" or benign response, then upvotes the poisoned response or downvotes the benign one. When feedback signals are used in a subsequent preference tuning behavior, LMs exhibit increased probability of producing poisoned responses even in contexts without malicious prompts. We show that this attack can be used to (1) insert factual knowledge the model did not previously possess, (2) modify code generation patterns in ways that introduce exploitable security flaws, and (3) inject fake financial news. Our finding both identifies a new qualitative feature of language model preference tuning (showing that it even highly restricted forms of preference data can be used to exert fine-grained control over behavior), and a new attack mechanism for LMs trained with user feedback (extending work on pretraining-time data poisoning and deployment-time prompt injection).

</details>


### [46] [MOTIF: Modular Thinking via Reinforcement Fine-tuning in LLMs](https://arxiv.org/abs/2507.02851)

*Purbesh Mitra, Sennur Ulukus*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Modular Thinking

**Relevance Score:** 9

**TL;DR:** This paper presents MOTIF, a modular thinking strategy for LLMs using reinforcement learning to extend reasoning capabilities beyond context size constraints through multiple round token generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of large language models beyond the constraints of fixed context sizes.

**Method:** The proposed method, MOTIF, utilizes a group relative policy optimization algorithm for reinforcement learning and enables multi-round token generation, enhancing the model's reasoning ability with additional context.

**Key Contributions:**

	1. Introduction of MOTIF for modular thinking in LLMs
	2. Demonstrated improvements in reasoning tasks using less training data
	3. Open-source implementation available for further research

**Result:** The experiments demonstrated a 3.8% and 3.3% improvement in accuracy over vanilla GRPO based training on MATH500 and AIME2024 benchmarks, respectively, using only 15% of the training samples.

**Limitations:** 

**Conclusion:** MOTIF demonstrates sample efficiency and improved reasoning capabilities for LLMs, making it a valuable approach for enhancing model performance in challenging reasoning tasks.

**Abstract:** Recent advancements in the reasoning capabilities of large language models (LLMs) show that employing group relative policy optimization (GRPO) algorithm for reinforcement learning (RL) training allows the models to use more thinking/reasoning tokens for generating better responses. However, LLMs can generate only a finite amount of tokens while maintaining attention to the previously generated tokens. This limit, also known as the context size of an LLM, is a bottleneck in LLM reasoning with arbitrarily large number of tokens. To think beyond the limit of context size, an LLM must employ a modular thinking strategy to reason over multiple rounds. In this work, we propose $\textbf{MOTIF: Modular Thinking via Reinforcement Finetuning}$ -- an RL training method for generating thinking tokens in multiple rounds, effectively allowing the model to think with additional context size. We trained the open-source model Qwen2.5-3B-Instruct on GSM8K dataset via parameter efficient fine-tuning and tested its accuracy on MATH500 and AIME2024 benchmarks. Our experiments show 3.8\% and 3.3\% improvements over vanilla GRPO based training in the respective benchmarks. Furthermore, this improvement was achieved with only 15\% of samples, thus demonstrating sample efficiency of MOTIF. Our code and models are available at https://github.com/purbeshmitra/MOTIF and https://huggingface.co/purbeshmitra/MOTIF, respectively.

</details>


### [47] [Answer Matching Outperforms Multiple Choice for Language Model Evaluation](https://arxiv.org/abs/2507.02856)

*Nikhil Chandak, Shashwat Goel, Ameya Prabhu, Moritz Hardt, Jonas Geiping*

**Main category:** cs.CL

**Keywords:** language model evaluation, answer matching, generative evaluation, human grading, multiple choice benchmarks

**Relevance Score:** 8

**TL;DR:** This paper critiques traditional multiple choice benchmarks for evaluating language models and proposes a new generative evaluation method, answer matching, which demonstrates superior agreement with human grading.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fundamental limitations of multiple choice evaluations in language model assessment, which can often be answered without seeing the questions.

**Method:** The authors introduce a method called answer matching, where a language model generates a free-form answer to a question without options, and a second model checks if this answer aligns with a reference answer.

**Key Contributions:**

	1. Introduction of answer matching as a scalable alternative to multiple choice evaluation.
	2. Demonstration of improved alignment with human grading through generative evaluation techniques.
	3. Analysis of how model rankings can change when evaluated by answer matching versus traditional means.

**Result:** Answer matching, even with small models, achieves near-perfect agreement with human grading, improving upon the poor alignment of both multiple choice evaluation and LLM-as-a-judge approaches.

**Limitations:** 

**Conclusion:** The transition from multiple choice to generative evaluation through answer matching could significantly improve the quality of language model assessments and change model ranking.

**Abstract:** Multiple choice benchmarks have long been the workhorse of language model evaluation because grading multiple choice is objective and easy to automate. However, we show multiple choice questions from popular benchmarks can often be answered without even seeing the question. These shortcuts arise from a fundamental limitation of discriminative evaluation not shared by evaluations of the model's free-form, generative answers. Until recently, there appeared to be no viable, scalable alternative to multiple choice--but, we show that this has changed. We consider generative evaluation via what we call answer matching: Give the candidate model the question without the options, have it generate a free-form response, then use a modern language model with the reference answer to determine if the response matches the reference. To compare the validity of different evaluation strategies, we annotate MMLU-Pro and GPQA-Diamond to obtain human grading data, and measure the agreement of each evaluation approach. We find answer matching using recent models--even small ones--achieves near-perfect agreement, in the range of inter-annotator agreement. In contrast, both multiple choice evaluation and using LLM-as-a-judge without reference answers aligns poorly with human grading. Improving evaluations via answer matching is not merely a conceptual concern: the rankings of several models change significantly when evaluating their free-form responses with answer matching. In light of these findings, we discuss how to move the evaluation ecosystem from multiple choice to answer matching.

</details>


### [48] [Revisiting Active Learning under (Human) Label Variation](https://arxiv.org/abs/2507.02593)

*Cornelia Gruber, Helen Alber, Bernd Bischl, Göran Kauermann, Barbara Plank, Matthias Aßenmacher*

**Main category:** cs.CL

**Keywords:** active learning, human label variation, machine learning, annotation, large language models

**Relevance Score:** 9

**TL;DR:** This paper discusses the need for incorporating human label variation (HLV) into active learning (AL) frameworks to improve the effectiveness of machine learning models trained on labeled data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Access to high-quality labeled data is a limiting factor in supervised learning, and existing frameworks often ignore human label variation, which could be a significant informative signal.

**Method:** The authors propose a conceptual framework to incorporate HLV into the active learning process, addressing how to decompose label variation into meaningful signals and noise.

**Key Contributions:**

	1. Conceptual framework for HLV-aware active learning.
	2. Discussion on the integration of large language models as annotators.
	3. Survey of how current AL and (H)LV communities address or neglect distinctions in label variation.

**Result:** The paper surveys existing literature on label variation and active learning, highlighting gaps in current methodologies and proposing strategies for HLV inclusion.

**Limitations:** The framework proposed is conceptual and may require empirical validation in practical scenarios.

**Conclusion:** Integrating HLV into active learning can enhance annotation quality and better represent real-world complexities in machine learning tasks.

**Abstract:** Access to high-quality labeled data remains a limiting factor in applied supervised learning. While label variation (LV), i.e., differing labels for the same instance, is common, especially in natural language processing, annotation frameworks often still rest on the assumption of a single ground truth. This overlooks human label variation (HLV), the occurrence of plausible differences in annotations, as an informative signal. Similarly, active learning (AL), a popular approach to optimizing the use of limited annotation budgets in training ML models, often relies on at least one of several simplifying assumptions, which rarely hold in practice when acknowledging HLV. In this paper, we examine foundational assumptions about truth and label nature, highlighting the need to decompose observed LV into signal (e.g., HLV) and noise (e.g., annotation error). We survey how the AL and (H)LV communities have addressed -- or neglected -- these distinctions and propose a conceptual framework for incorporating HLV throughout the AL loop, including instance selection, annotator choice, and label representation. We further discuss the integration of large language models (LLM) as annotators. Our work aims to lay a conceptual foundation for HLV-aware active learning, better reflecting the complexities of real-world annotation.

</details>


### [49] [Beyond Scale: The Diversity Coefficient as a Data Quality Metric for Variability in Natural Language Data](https://arxiv.org/abs/2306.13840)

*Brando Miranda, Alycia Lee, Sudharsan Sundar, Allison Casasola, Rylan Schaeffer, Elyas Obbad, Sanmi Koyejo*

**Main category:** cs.CL

**Keywords:** Large Language Models, data quality, diversity coefficient, pre-training datasets, model evaluation

**Relevance Score:** 9

**TL;DR:** The paper introduces the diversity coefficient as a formal measure of data quality in pre-training Large Language Models, demonstrating its empirical relevance to model evaluation performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously characterize the nebulous concept of data quality in pre-training LLMs, focusing particularly on the variability of natural language data.

**Method:** Proposes a diversity coefficient to measure the variability of natural language data; performs empirical analysis on this measure and controlled interventional experiments with GPT-2 and LLaMAv2 across 44 models.

**Key Contributions:**

	1. Introduction of the diversity coefficient for measuring data quality
	2. Empirical validation of the diversity coefficient's relevance to model evaluation performance
	3. Comprehensive experiments demonstrating high diversity in pre-training datasets

**Result:** The diversity coefficient aligns with properties of diversity and variability, showing that pre-training datasets have high formal diversity, which correlates with downstream model evaluation performance.

**Limitations:** 

**Conclusion:** The formal notion of diversity as measured by the diversity coefficient is a crucial element of data quality, leading to improved evaluation performance in downstream tasks.

**Abstract:** Current trends in pre-training Large Language Models (LLMs) primarily focus on the scaling of model and dataset size. While the quality of pre-training data is considered an important factor for training powerful LLMs, it remains a nebulous concept that has not been rigorously characterized. To this end, we propose a formalization of one key aspect of data quality -- measuring the variability of natural language data -- specifically via a measure we call the diversity coefficient. Our empirical analysis shows that the proposed diversity coefficient aligns with the intuitive properties of diversity and variability, e.g., it increases as the number of latent concepts increases. Then, we measure the diversity coefficient of publicly available pre-training datasets and demonstrate that their formal diversity is high compared to theoretical lower and upper bounds. Finally, we conduct a comprehensive set of controlled interventional experiments with GPT-2 and LLaMAv2 that demonstrate the diversity coefficient of pre-training data characterizes useful aspects of downstream model evaluation performance -- totaling 44 models of various sizes (51M to 7B parameters). We conclude that our formal notion of diversity is an important aspect of data quality that captures variability and causally leads to improved evaluation performance.

</details>


### [50] [Improving the Robustness of Distantly-Supervised Named Entity Recognition via Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning](https://arxiv.org/abs/2311.08010)

*Shuzheng Si, Helan Hu, Haozhe Zhao, Shuang Zeng, Kaikai An, Zefan Cai, Baobao Chang*

**Main category:** cs.CL

**Keywords:** Distantly-Supervised Named Entity Recognition, label noise, teacher-student framework, uncertainty-aware learning, collaborative learning

**Relevance Score:** 6

**TL;DR:** The paper proposes a novel approach to improve Distantly-Supervised Named Entity Recognition (DS-NER) by addressing label noise through Uncertainty-Aware Teacher Learning and Student-Student Collaborative Learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to enhance the performance of Distantly-Supervised Named Entity Recognition (DS-NER) by addressing the issues of label noise which degrades model performance.

**Method:** The authors introduce Uncertainty-Aware Teacher Learning to minimize incorrect pseudo labels during self-training and Student-Student Collaborative Learning to facilitate the transfer of reliable labels between student networks.

**Key Contributions:**

	1. Uncertainty-Aware Teacher Learning to enhance label quality
	2. Student-Student Collaborative Learning for reliable label transfer
	3. Comprehensive evaluation on multiple DS-NER datasets demonstrating superior performance

**Result:** The proposed method outperforms state-of-the-art DS-NER methods across five different datasets, showcasing its efficacy in reducing label noise and improving training outcomes.

**Limitations:** 

**Conclusion:** The paper concludes that leveraging prediction uncertainty and collaborative learning strategies leads to better label refinement and improved robustness in DS-NER.

**Abstract:** Distantly-Supervised Named Entity Recognition (DS-NER) is widely used in real-world scenarios. It can effectively alleviate the burden of annotation by matching entities in existing knowledge bases with snippets in the text but suffer from the label noise. Recent works attempt to adopt the teacher-student framework to gradually refine the training labels and improve the overall robustness. However, these teacher-student methods achieve limited performance because the poor calibration of the teacher network produces incorrectly pseudo-labeled samples, leading to error propagation. Therefore, we propose: (1) Uncertainty-Aware Teacher Learning that leverages the prediction uncertainty to reduce the number of incorrect pseudo labels in the self-training stage; (2) Student-Student Collaborative Learning that allows the transfer of reliable labels between two student networks instead of indiscriminately relying on all pseudo labels from its teacher, and further enables a full exploration of mislabeled samples rather than simply filtering unreliable pseudo-labeled samples. We evaluate our proposed method on five DS-NER datasets, demonstrating that our method is superior to the state-of-the-art DS-NER methods.

</details>


### [51] [Optimal strategies to perform multilingual analysis of social content for a novel dataset in the tourism domain](https://arxiv.org/abs/2311.14727)

*Maxime Masson, Rodrigo Agerri, Christian Sallaberry, Marie-Noelle Bessagnet, Annig Le Parc Lacayrelle, Philippe Roose*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, multilingual datasets, tourism, few-shot learning, sentiment analysis

**Relevance Score:** 4

**TL;DR:** This paper presents a novel multilingual dataset for tourism-related tweets and evaluates NLP techniques to minimize the need for manually annotated data while achieving competitive performance in key tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient NLP strategies to process multilingual, unstructured, and informal texts from social media, particularly in the tourism domain, while addressing the challenge of manual data annotation.

**Method:** The authors created the first publicly available multilingual dataset for tourism, containing tweets with multilayered annotations for NER, thematic concepts extraction, and sentiment analysis. Various few-shot and fine-tuning techniques were experimentally compared using modern language models.

**Key Contributions:**

	1. Creation of the first multilingual dataset for tourism-related tweets
	2. Demonstration of effective few-shot techniques for NLP tasks
	3. Reduction of manual annotation requirements for domain-specific applications

**Result:** The experimentation showed that few-shot techniques yielded competitive results across tasks with minimal annotated data: 5 tweets for sentiment analysis, 30 for NER, and 1K for thematic concepts, demonstrating the dataset's utility.

**Limitations:** 

**Conclusion:** The findings indicate that NLP can be successfully applied to specific domains like tourism, thereby reducing the reliance on manual annotations and overcoming complexities of traditional methods.

**Abstract:** The rising influence of social media platforms in various domains, including tourism, has highlighted the growing need for efficient and automated Natural Language Processing (NLP) strategies to take advantage of this valuable resource. However, the transformation of multilingual, unstructured, and informal texts into structured knowledge still poses significant challenges, most notably the never-ending requirement for manually annotated data to train deep learning classifiers. In this work, we study different NLP techniques to establish the best ones to obtain competitive performances while keeping the need for training annotated data to a minimum. To do so, we built the first publicly available multilingual dataset (French, English, and Spanish) for the tourism domain, composed of tourism-related tweets. The dataset includes multilayered, manually revised annotations for Named Entity Recognition (NER) for Locations and Fine-grained Thematic Concepts Extraction mapped to the Thesaurus of Tourism and Leisure Activities of the World Tourism Organization, as well as for Sentiment Analysis at the tweet level. Extensive experimentation comparing various few-shot and fine-tuning techniques with modern language models demonstrate that modern few-shot techniques allow us to obtain competitive results for all three tasks with very little annotation data: 5 tweets per label (15 in total) for Sentiment Analysis, 30 tweets for Named Entity Recognition of Locations and 1K tweets annotated with fine-grained thematic concepts, a highly fine-grained sequence labeling task based on an inventory of 315 classes. We believe that our results, grounded in a novel dataset, pave the way for applying NLP to new domain-specific applications, reducing the need for manual annotations and circumventing the complexities of rule-based, ad-hoc solutions.

</details>


### [52] [Delving into LLM-assisted writing in biomedical publications through excess vocabulary](https://arxiv.org/abs/2406.07016)

*Dmitry Kobak, Rita González-Márquez, Emőke-Ágnes Horvát, Jan Lause*

**Main category:** cs.CL

**Keywords:** large language models, biomedical research, scientific writing, text analysis, PubMed

**Relevance Score:** 9

**TL;DR:** The study analyzes the impact of large language models (LLMs) on biomedical research writing, revealing significant usage trends from 2010-2024 through vocabulary changes in over 15 million abstracts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the prevalence and impact of LLMs on scholarly writing in biomedical research and understand how these models are influencing language use in academic literature.

**Method:** Analysis of vocabulary changes in over 15 million biomedical abstracts indexed by PubMed from 2010 to 2024 to detect patterns associated with LLM usage.

**Key Contributions:**

	1. Unbiased large-scale analysis of LLM usage in biomedical literature
	2. Quantification of LLM influence on vocabulary across disciplines and journals
	3. Identification of differential impacts based on geography and publication types.

**Result:** The study found that at least 13.5% of 2024 abstracts indicated processing by LLMs, with variation by discipline, country, and journal; some subcorpora reached up to 40%.

**Limitations:** 

**Conclusion:** LLMs have significantly influenced scientific writing in biomedical research, surpassing the effects of major events like the Covid pandemic.

**Abstract:** Large language models (LLMs) like ChatGPT can generate and revise text with human-level performance. These models come with clear limitations: they can produce inaccurate information, reinforce existing biases, and be easily misused. Yet, many scientists use them for their scholarly writing. But how wide-spread is such LLM usage in the academic literature? To answer this question for the field of biomedical research, we present an unbiased, large-scale approach: we study vocabulary changes in over 15 million biomedical abstracts from 2010--2024 indexed by PubMed, and show how the appearance of LLMs led to an abrupt increase in the frequency of certain style words. This excess word analysis suggests that at least 13.5% of 2024 abstracts were processed with LLMs. This lower bound differed across disciplines, countries, and journals, reaching 40% for some subcorpora. We show that LLMs have had an unprecedented impact on scientific writing in biomedical research, surpassing the effect of major world events such as the Covid pandemic.

</details>


### [53] [Task Prompt Vectors: Effective Initialization through Multi-Task Soft-Prompt Transfer](https://arxiv.org/abs/2408.01119)

*Robert Belanec, Simon Ostermann, Ivan Srba, Maria Bielikova*

**Main category:** cs.CL

**Keywords:** prompt tuning, multi-task learning, large language models

**Relevance Score:** 8

**TL;DR:** Introducing Task Prompt Vectors to enhance soft-prompt tuning for multi-task performance in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current soft-prompt methods compromise multi-task training efficiency, necessitating repeated training for new tasks.

**Method:** Task Prompt Vectors are created by taking the element-wise difference between tuned soft-prompts and their random initialization, enabling arithmetic operations across tasks.

**Key Contributions:**

	1. Introduction of Task Prompt Vectors
	2. Demonstration of efficiency in low-resource settings
	3. Support for arithmetic operations across multiple tasks

**Result:** Experimental results demonstrate that Task Prompt Vectors allow effective initialization for prompt tuning in low-resource scenarios and are consistent across different model architectures.

**Limitations:** 

**Conclusion:** Task Prompt Vectors provide a viable alternative to state-of-the-art baselines, enhancing efficiency and modularity in multi-task learning with LLMs.

**Abstract:** Prompt tuning is an efficient solution for training large language models (LLMs). However, current soft-prompt-based methods often sacrifice multi-task modularity, requiring the training process to be fully or partially repeated for each newly added task. While recent work on task vectors applied arithmetic operations on full model weights to achieve the desired multi-task performance, a similar approach for soft-prompts is still missing. To this end, we introduce Task Prompt Vectors, created by element-wise difference between weights of tuned soft-prompts and their random initialization. Experimental results on 12 NLU datasets show that task prompt vectors can be used in low-resource settings to effectively initialize prompt tuning on similar tasks. In addition, we show that task prompt vectors are independent of the random initialization of prompt tuning on 2 different language model architectures. This allows prompt arithmetics with the pre-trained vectors from different tasks. In this way, we provide a competitive alternative to state-of-the-art baselines by arithmetic addition of task prompt vectors from multiple tasks.

</details>


### [54] [MedAide: Information Fusion and Anatomy of Medical Intents via LLM-based Agent Collaboration](https://arxiv.org/abs/2410.12532)

*Dingkang Yang, Jinjie Wei, Mingcheng Li, Jiyao Liu, Lihao Liu, Ming Hu, Junjun He, Yakun Ju, Wei Zhou, Yang Liu, Lihua Zhang*

**Main category:** cs.CL

**Keywords:** LLM, Medical Applications, Multi-Agent Systems, Information Fusion, Healthcare

**Relevance Score:** 9

**TL;DR:** This paper presents MedAide, an LLM-based framework for medical multi-agent collaboration aimed at improving healthcare decision-making through intent-aware information fusion and coordinated reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable decision-making systems in healthcare that can integrate multi-intent information from diverse clinical sources while addressing issues of redundancy and performance bottlenecks in current LLMs.

**Method:** MedAide employs a regularization-guided module for query decomposition, a dynamic intent prototype matching module for intent recognition, and a rotation agent collaboration mechanism to enhance information fusion.

**Key Contributions:**

	1. Regularization-guided module for structured query representations
	2. Dynamic intent prototype matching for multi-round healthcare dialogues
	3. Rotation agent collaboration for decision-level information fusion

**Result:** MedAide shows improved performance on four medical benchmarks, outpacing current LLMs in medical proficiency and strategic reasoning as evaluated by automated metrics and expert doctors.

**Limitations:** 

**Conclusion:** The framework demonstrates significant promise in enhancing LLM-driven healthcare applications, addressing complex medical intents effectively.

**Abstract:** In healthcare intelligence, the ability to fuse heterogeneous, multi-intent information from diverse clinical sources is fundamental to building reliable decision-making systems. Large Language Model (LLM)-driven information interaction systems currently showing potential promise in the healthcare domain. Nevertheless, they often suffer from information redundancy and coupling when dealing with complex medical intents, leading to severe hallucinations and performance bottlenecks. To this end, we propose MedAide, an LLM-based medical multi-agent collaboration framework designed to enable intent-aware information fusion and coordinated reasoning across specialized healthcare domains. Specifically, we introduce a regularization-guided module that combines syntactic constraints with retrieval augmented generation to decompose complex queries into structured representations, facilitating fine-grained clinical information fusion and intent resolution. Additionally, a dynamic intent prototype matching module is proposed to utilize dynamic prototype representation with a semantic similarity matching mechanism to achieve adaptive recognition and updating of the agent's intent in multi-round healthcare dialogues. Ultimately, we design a rotation agent collaboration mechanism that introduces dynamic role rotation and decision-level information fusion across specialized medical agents. Extensive experiments are conducted on four medical benchmarks with composite intents. Experimental results from automated metrics and expert doctor evaluations show that MedAide outperforms current LLMs and improves their medical proficiency and strategic reasoning.

</details>


### [55] [De-mark: Watermark Removal in Large Language Models](https://arxiv.org/abs/2410.13808)

*Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang*

**Main category:** cs.CL

**Keywords:** watermarking, language models, n-gram, De-mark, random selection probing

**Relevance Score:** 4

**TL;DR:** De-mark is a framework developed to effectively remove n-gram-based watermarks from language models using a novel querying strategy called random selection probing.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in exploring the robustness of watermarking techniques in machine-generated content.

**Method:** The proposed framework, De-mark, employs a random selection probing strategy to evaluate watermark strength and identify specific n-gram watermarks.

**Key Contributions:**

	1. Introduction of the De-mark framework for watermark removal
	2. Utilization of random selection probing for assessing watermark strength
	3. Demonstration of effectiveness on Llama3 and ChatGPT

**Result:** Experiments conducted on Llama3 and ChatGPT show that De-mark efficiently removes and exploits watermarks from these models.

**Limitations:** The exploration is primarily focused on n-gram-based watermarks; other watermarking techniques are not covered.

**Conclusion:** De-mark is effective in watermark removal and can enhance our understanding of watermark robustness in language models.

**Abstract:** Watermarking techniques offer a promising way to identify machine-generated content via embedding covert information into the contents generated from language models (LMs). However, the robustness of the watermarking schemes has not been well explored. In this paper, we present De-mark, an advanced framework designed to remove n-gram-based watermarks effectively. Our method utilizes a novel querying strategy, termed random selection probing, which aids in assessing the strength of the watermark and identifying the red-green list within the n-gram watermark. Experiments on popular LMs, such as Llama3 and ChatGPT, demonstrate the efficiency and effectiveness of De-mark in watermark removal and exploitation tasks.

</details>


### [56] [Next-Token Prediction Task Assumes Optimal Data Ordering for LLM Training in Proof Generation](https://arxiv.org/abs/2411.00863)

*Chenyang An, Shima Imani, Feng Yao, Chengyu Dong, Ali Abbasi, Harsh Shrivastava, Samuel Buss, Jingbo Shang, Gayathri Mahalingam, Pramod Sharma, Maurice Diesendruck*

**Main category:** cs.CL

**Keywords:** Large Language Models, Proof Generation, Data Ordering

**Relevance Score:** 8

**TL;DR:** This paper critiques the ordering of training data in LLM-based proof generation, proposing an 'intuitively sequential order' that enhances learning and improves proof success rates.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the suboptimal performance of LLMs in proof generation tasks, linking it to inefficient data ordering in training samples.

**Method:** The authors validate their claims through two proof generation tasks: intuitionistic propositional logic theorem-proving and digit multiplication, comparing models trained on different data orders.

**Key Contributions:**

	1. Introduction of intuitively sequential order for proof training data
	2. Demonstration of substantial performance improvements in LLMs with optimized data ordering
	3. Identification of common order issues in advanced math proofs

**Result:** An 11 percent improvement in proof success rate is observed in the propositional logic theorem-proving task when models are trained using the intuitively sequential order compared to the worst-case scenario.

**Limitations:** Focus on specific proof tasks; broader implications on other domains are not discussed.

**Conclusion:** Training is most effective when employing intuitively sequential ordering, highlighting a significant issue with the current data ordering practices in math proofs.

**Abstract:** In the field of large language model (LLM)-based proof generation, despite extensive training on large datasets such as ArXiv, LLMs still exhibit only modest performance on proving tasks of moderate difficulty. We believe that this is partly due to the widespread presence of suboptimal ordering within the data for each proof used in training. For example, published proofs often follow a purely logical order, where each step logically proceeds from the previous steps based on the deductive rules. This order is designed to facilitate the verification of the proof's soundness, rather than to help people and models learn the discovery process of the proof. In proof generation, we argue that the optimal order for one training data sample occurs when the relevant intermediate supervision for a particular proof step in the proof is always positioned to the left of that proof step. We call such order the intuitively sequential order. We validate our claims using two tasks: intuitionistic propositional logic theorem-proving and digit multiplication. Our experiments verify the order effect and provide support for our explanations. We demonstrate that training is most effective when the proof is in the intuitively sequential order. Moreover, the order effect and the performance gap between models trained on different data orders can be substantial -- with an 11 percent improvement in proof success rate observed in the propositional logic theorem-proving task, between models trained on the optimal order compared to the worst order. Lastly, we define a common type of order issue in advanced math proofs and find that 17.3 percent of theorems with nontrivial proofs in the first two chapters of a widely used graduate-level mathematics textbook suffer from this issue. A detailed list of those proofs is provided in the appendix.

</details>


### [57] [Token Prepending: A Training-Free Approach for Eliciting Better Sentence Embeddings from LLMs](https://arxiv.org/abs/2412.11556)

*Yuchen Fu, Zifeng Cheng, Zhiwei Jiang, Zhonghui Wang, Yafeng Yin, Zhengliang Li, Qing Gu*

**Main category:** cs.CL

**Keywords:** sentence embeddings, large language models, Token Prepending, semantic textual similarity, prompt engineering

**Relevance Score:** 9

**TL;DR:** This paper introduces the Token Prepending (TP) technique that enhances sentence embeddings extracted from LLMs by allowing earlier tokens to access complete sentence information, improving performance on STS tasks without additional inference cost.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Previous methods for extracting sentence embeddings from LLMs focus on prompt engineering but suffer from information bias due to causal attention limitations in decoder-only models.

**Method:** The Token Prepending technique prepends the decoded sentence embedding from previous layers to the input of the next layer, facilitating better contextual attention among tokens.

**Key Contributions:**

	1. Introduction of the Token Prepending technique
	2. Demonstrated significant performance improvements in STS tasks
	3. Training-free integration with existing prompt-based methods

**Result:** TP significantly enhances the performance of existing prompt-based sentence embedding methods in Semantic Textual Similarity tasks and downstream classification tasks.

**Limitations:** 

**Conclusion:** The TP technique is effective, training-free, and can be integrated with various existing LLMs and sentence embedding methods with minimal inference cost.

**Abstract:** Extracting sentence embeddings from large language models (LLMs) is a promising direction, as LLMs have demonstrated stronger semantic understanding capabilities. Previous studies typically focus on prompt engineering to elicit sentence embeddings from LLMs by prompting the model to encode sentence information into the embedding of the last token. However, LLMs are mostly decoder-only models with causal attention and the earlier tokens in the sentence cannot attend to the latter tokens, resulting in biased encoding of sentence information and cascading effects on the final decoded token. To this end, we propose a novel Token Prepending (TP) technique that prepends each layer's decoded sentence embedding to the beginning of the sentence in the next layer's input, allowing earlier tokens to attend to the complete sentence information under the causal attention mechanism. The proposed TP technique is a plug-and-play and training-free technique, which means it can be seamlessly integrated with various prompt-based sentence embedding methods and autoregressive LLMs. Extensive experiments on various Semantic Textual Similarity (STS) tasks and downstream classification tasks demonstrate that our proposed TP technique can significantly improve the performance of existing prompt-based sentence embedding methods across different LLMs, while incurring negligible additional inference cost.

</details>


### [58] [REINFORCE++: An Efficient RLHF Algorithm with Robustness to Both Prompt and Reward Models](https://arxiv.org/abs/2501.03262)

*Jian Hu, Xibin Wu, Wei Shen, Jason Klein Liu, Zilin Zhu, Weixun Wang, Songlin Jiang, Haoran Wang, Hao Chen, Bin Chen, Weikai Fang, Xianyu, Yu Cao, Haotian Xu*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Human Feedback, Open source, Training efficiency, AI alignment

**Relevance Score:** 7

**TL;DR:** OpenRLHF is a user-friendly open-source framework for Reinforcement Learning from Human Feedback that improves training efficiency and accessibility for researchers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the alignment of human and AI values and to improve accessibility of RLHF frameworks to newcomers.

**Method:** A new open-source RLHF framework is developed using Ray, vLLM, DeepSpeed, and HuggingFace Transformers, designed with a simplified architecture for ease of use.

**Key Contributions:**

	1. Introduction of a simplified RLHF framework
	2. Demonstrated training efficiency improvements
	3. Clear documentation and code structure for ease of learning

**Result:** OpenRLHF achieves training speedups of 1.22x to 1.68x compared to existing state-of-the-art frameworks and requires fewer lines of code for implementation.

**Limitations:** 

**Conclusion:** OpenRLHF is effective in improving RLHF research efficiency and has gained adoption in leading institutions.

**Abstract:** Large Language Models (LLMs) fine-tuned via Reinforcement Learning from Human Feedback (RLHF) and Reinforcement Learning with Verifiable Rewards (RLVR) significantly improve the alignment of human-AI values and further raise the upper bound of AI capabilities, particularly in reasoning-intensive, long-context Chain-of-Thought (long-CoT) tasks. However, existing RLHF (or RLVR) frameworks commonly face challenges such as inference bottlenecks and complexity barriers, restricting their accessibility for newcomers. To bridge this gap, we introduce \textbf{OpenRLHF}, a user-friendly, scalable, and easy-to-learn open-source RLHF framework built upon Ray, vLLM, DeepSpeed, and HuggingFace Transformers, featuring a simplified design, clear code structure, and comprehensive documentation to facilitate entry for researchers and practitioners. Experimental results show that OpenRLHF achieves superior training efficiency with speedups ranging from 1.22x to 1.68x across different model sizes compared to state-of-the-art frameworks, while requiring significantly fewer lines of code for implementation. OpenRLHF is publicly available at https://github.com/OpenRLHF/OpenRLHF, and has already been adopted by leading institutions to accelerate RLHF research and learning.

</details>


### [59] [Quantifying the Importance of Data Alignment in Downstream Model Performance](https://arxiv.org/abs/2501.08496)

*Krrish Chawla, Aryan Sahai, Mario DePavia, Sudharsan Sundar, Brando Miranda, Elyas Obbad, Sanmi Koyejo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data Alignment, Autoformalization, Machine Translation, Data Quality

**Relevance Score:** 8

**TL;DR:** This paper investigates the importance of data alignment over dataset size in training Large Language Models (LLMs), finding a strong negative correlation between training-evaluation data alignment and model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the conventional emphasis on dataset size and highlight the role of data alignment in the performance of Large Language Models (LLMs).

**Method:** Utilized the Task2Vec-based alignment coefficient to measure the similarity between pre-training and evaluation datasets, conducting controlled interventional experiments.

**Key Contributions:**

	1. Introduced the Task2Vec-based alignment coefficient as a measure of data quality.
	2. Demonstrated the impact of data alignment on LLM performance in the Autoformalization task.
	3. Provided evidence to shift focus from dataset size to data alignment in LLM training.

**Result:** A strong negative correlation was found between the alignment coefficient of training and evaluation data and the model's loss/perplexity on downstream tasks, particularly in Autoformalization.

**Limitations:** 

**Conclusion:** The study suggests a need to re-evaluate LLM training practices, emphasizing the significance of data alignment in specialized tasks over merely increasing data quantity.

**Abstract:** Contrary to the conventional emphasis on dataset size, we explore the role of data alignment -- an often overlooked aspect of data quality -- in training capable Large Language Models (LLMs). To do so, we use the Task2Vec-based alignment coefficient, a quantitative measure of the similarity between two datasets, to quantify the impact of alignment between training data and evaluation data on downstream performance. In particular, we conduct controlled \textit{interventional} experiments for two settings: 1. the impact of increased alignment coefficients between various pre-training (pt) against evaluation datasets, and 2. the impact of increased alignment coefficients between domain specific fine-tuning (ft) against domain specific evaluation. The domain specific task we explore is Autoformalization -- the machine translation task between natural language and code for formal verification. In both settings, we find a strong, predictable negative correlation between the alignment coefficient of a model's training and evaluation data and the model's loss/perplexity on the respective downstream task. These findings suggest a re-evaluation of LLM training approaches, demonstrating the relevance of data alignment compared to data quantity, especially in specialized downstream tasks such as Autoformalization.

</details>


### [60] [Improved Unbiased Watermark for Large Language Models](https://arxiv.org/abs/2502.11268)

*Ruibo Chen, Yihan Wu, Junfeng Guo, Heng Huang*

**Main category:** cs.CL

**Keywords:** watermarks, AI-generated content, language models, detectability, robustness

**Relevance Score:** 7

**TL;DR:** MCmark introduces unbiased, Multi-Channel-based watermarks to improve the detectability and robustness of AI-generated text, enhancing authentication without compromising quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to authenticate AI-generated content has become critical as AI surpasses human text generation capabilities.

**Method:** MCmark embeds statistical signals into language model-generated text by partitioning the model's vocabulary and adjusting token probabilities according to a watermark key.

**Key Contributions:**

	1. Introduction of MCmark, a family of unbiased watermarks
	2. Partitioning the vocabulary of the model to enhance watermarking
	3. Demonstrated improvement in detectability and robustness of watermarks

**Result:** MCmark improves the detectability of unbiased watermarks by over 10% compared to existing methods while preserving the original distribution of the language model.

**Limitations:** 

**Conclusion:** MCmark demonstrates significant advancements in watermarking for AI-generated texts, enhancing their authenticity and practical application.

**Abstract:** As artificial intelligence surpasses human capabilities in text generation, the necessity to authenticate the origins of AI-generated content has become paramount. Unbiased watermarks offer a powerful solution by embedding statistical signals into language model-generated text without distorting the quality. In this paper, we introduce MCmark, a family of unbiased, Multi-Channel-based watermarks. MCmark works by partitioning the model's vocabulary into segments and promoting token probabilities within a selected segment based on a watermark key. We demonstrate that MCmark not only preserves the original distribution of the language model but also offers significant improvements in detectability and robustness over existing unbiased watermarks. Our experiments with widely-used language models demonstrate an improvement in detectability of over 10% using MCmark, compared to existing state-of-the-art unbiased watermarks. This advancement underscores MCmark's potential in enhancing the practical application of watermarking in AI-generated texts.

</details>


### [61] [Layered Insights: Generalizable Analysis of Authorial Style by Leveraging All Transformer Layers](https://arxiv.org/abs/2503.00958)

*Milad Alshomary, Nikhil Reddy Varimalla, Vishal Anand, Smaranda Muresan, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** authorship attribution, transformer models, linguistic representations

**Relevance Score:** 7

**TL;DR:** A new approach to authorship attribution that uses linguistic representations from different layers of transformer models, showing improved performance especially on out-of-domain data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance authorship attribution accuracy by leveraging different linguistic representations in transformer models.

**Method:** The proposed method utilizes representations from various layers of pre-trained transformer models and evaluates on three datasets against a state-of-the-art baseline.

**Key Contributions:**

	1. Introduction of layer-wise linguistic representations in authorship attribution
	2. Improved robustness of models on out-of-domain data
	3. Achievement of new state-of-the-art results in the field.

**Result:** The approach achieved state-of-the-art results, especially in out-of-domain scenarios, demonstrating improved robustness.

**Limitations:** 

**Conclusion:** Different layers in transformer models specialize in representing stylistic features beneficial for authorship attribution.

**Abstract:** We propose a new approach for the authorship attribution task that leverages the various linguistic representations learned at different layers of pre-trained transformer-based models. We evaluate our approach on three datasets, comparing it to a state-of-the-art baseline in in-domain and out-of-domain scenarios. We found that utilizing various transformer layers improves the robustness of authorship attribution models when tested on out-of-domain data, resulting in new state-of-the-art results. Our analysis gives further insights into how our model's different layers get specialized in representing certain stylistic features that benefit the model when tested out of the domain.

</details>


### [62] [Commander-GPT: Fully Unleashing the Sarcasm Detection Capability of Multi-Modal Large Language Models](https://arxiv.org/abs/2503.18681)

*Yazhou Zhang, Chunwang Zou, Bo Wang, Jing Qin*

**Main category:** cs.CL

**Keywords:** Sarcasm Detection, Multi-Modal Large Language Models, Natural Language Processing, Machine Learning, AI

**Relevance Score:** 8

**TL;DR:** This paper presents a novel multi-modal Commander-GPT framework for sarcasm detection that improves upon traditional methods by effectively employing multi-modal large language models (MLLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional sarcasm detection methods often fail due to their single-modal approaches, prompting a shift towards more effective multi-modal techniques.

**Method:** The paper proposes a Commander-GPT framework that decomposes sarcasm detection into six sub-tasks, assigning specific large language models to each and aggregating their results for better accuracy.

**Key Contributions:**

	1. Introduction of a novel multi-modal sarcasm detection framework
	2. Decomposition of the detection task into sub-tasks for improved model assignment
	3. Aggregation of results from multiple models for enhanced accuracy

**Result:** The proposed framework demonstrated a 19.3% improvement in F1 score on benchmarks without the need for fine-tuning.

**Limitations:** 

**Conclusion:** The Commander-GPT framework showcases the effectiveness of multi-modal approaches in sarcasm detection, achieving state-of-the-art results in the field.

**Abstract:** Sarcasm detection, as a crucial research direction in the field of Natural Language Processing (NLP), has attracted widespread attention. Traditional sarcasm detection tasks have typically focused on single-modal approaches (e.g., text), but due to the implicit and subtle nature of sarcasm, such methods often fail to yield satisfactory results. In recent years, researchers have shifted the focus of sarcasm detection to multi-modal approaches. However, effectively leveraging multi-modal information to accurately identify sarcastic content remains a challenge that warrants further exploration. Leveraging the powerful integrated processing capabilities of Multi-Modal Large Language Models (MLLMs) for various information sources, we propose an innovative multi-modal Commander-GPT framework. Inspired by military strategy, we first decompose the sarcasm detection task into six distinct sub-tasks. A central commander (decision-maker) then assigns the best-suited large language model to address each specific sub-task. Ultimately, the detection results from each model are aggregated to identify sarcasm. We conducted extensive experiments on MMSD and MMSD 2.0, utilizing four multi-modal large language models and six prompting strategies. Our experiments demonstrate that our approach achieves state-of-the-art performance, with a 19.3% improvement in F1 score, without necessitating fine-tuning or ground-truth rationales.

</details>
