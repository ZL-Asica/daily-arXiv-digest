# 2025-04-29

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 35]

- [cs.CL](#cs.CL) [Total: 102]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [From Prompts to Propositions: A Logic-Based Lens on Student-LLM Interactions](https://arxiv.org/abs/2504.18691)

*Ali Alfageeh, Sadegh AlMahdi Kazemi Zarkouei, Daye Nam, Daniel Prol, Matin Amoozadeh, Souti Chattopadhyay, James Prather, Paul Denny, Juho Leinonen, Michael Hilton, Sruti Srinivasa Ragavan, Mohammad Amin Alipour*

**Main category:** cs.HC

**Keywords:** large language models, prompt analysis, computing education, natural language programming, student interventions

**Relevance Score:** 9

**TL;DR:** This paper investigates the prompting behavior of students using LLMs in computing education through a novel method called Prompt2Constraints, which analyzes prompts as logical constraints to identify patterns and support struggling students.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how students use large language models (LLMs) in computational tasks is essential for improving educational outcomes in computing education.

**Method:** The authors introduce a method called Prompt2Constraints that translates student prompts into logical constraints, allowing for systematic analysis of prompting behavior.

**Key Contributions:** Introduction of Prompt2Constraints method for systematic analysis of student prompts., Identification of patterns in prompt evolution and strategies used by students., Implications for providing interventions to support students in real-time.

**Result:** Analysis of 1,872 prompts from 203 students revealed that while successful and unsuccessful prompts had a similar number of constraints, unsuccessful prompts often involved greater modifications, indicating a shift in problem-solving strategies.

**Limitations:** 

**Future Work:** Future research could explore the application of this method to more complex computational tasks and further integration into educational programming tools.

**Conclusion:** The findings suggest that Prompt2Constraints enables the identification of students who struggle with natural language programming tasks, offering potential for real-time support in programming tools.

**Abstract:** Background and Context. The increasing integration of large language models (LLMs) in computing education presents an emerging challenge in understanding how students use LLMs and craft prompts to solve computational tasks. Prior research has used both qualitative and quantitative methods to analyze prompting behavior, but these approaches lack scalability or fail to effectively capture the semantic evolution of prompts. Objective. In this paper, we investigate whether students prompts can be systematically analyzed using propositional logic constraints. We examine whether this approach can identify patterns in prompt evolution, detect struggling students, and provide insights into effective and ineffective strategies. Method. We introduce Prompt2Constraints, a novel method that translates students prompts into logical constraints. The constraints are able to represent the intent of the prompts in succinct and quantifiable ways. We used this approach to analyze a dataset of 1,872 prompts from 203 students solving introductory programming tasks. Findings. We find that while successful and unsuccessful attempts tend to use a similar number of constraints overall, when students fail, they often modify their prompts more significantly, shifting problem-solving strategies midway. We also identify points where specific interventions could be most helpful to students for refining their prompts. Implications. This work offers a new and scalable way to detect students who struggle in solving natural language programming tasks. This work could be extended to investigate more complex tasks and integrated into programming tools to provide real-time support.

</details>


### [2] [Beyond Isolation: Towards an Interactionist Perspective on Human Cognitive Bias and AI Bias](https://arxiv.org/abs/2504.18759)

*Nick von Felten*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, Cognitive bias, AI bias, Mitigation strategies, Human cognition

**Relevance Score:** 8

**TL;DR:** This position paper advocates for an integrated approach to understanding the interaction between human and AI biases, proposing a framework to map these biases to mitigation strategies to protect human cognition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to explore the dynamic interplay between human and AI biases, which is often overlooked in discussions about AI. Many breakthroughs in science arise from understanding interactions rather than isolated perspectives.

**Method:** The paper proposes a comprehensive framework that identifies and maps compound human-AI biases to specific mitigation strategies, calling for an evaluation of how these biases interact.

**Key Contributions:** Advocates for a comprehensive framework addressing human-AI bias interactions., Highlights the potential of calibrated systems in mitigating cognitive biases., Outlines concrete steps for future development of bias mitigation strategies.

**Result:** The author highlights that biased AI can amplify human cognitive biases, while well-calibrated AI can help in mitigating these biases, emphasizing the importance of examining their interactions.

**Limitations:** 

**Future Work:** Further research is needed to refine the framework and empirically validate the proposed mapping of biases to mitigation strategies.

**Conclusion:** A thorough understanding of the interaction between human and AI biases is crucial for the protection of human cognition, and necessary steps for developing a mapping framework are outlined.

**Abstract:** Isolated perspectives have often paved the way for great scientific discoveries. However, many breakthroughs only emerged when moving away from singular views towards interactions. Discussions on Artificial Intelligence (AI) typically treat human and AI bias as distinct challenges, leaving their dynamic interplay and compounding potential largely unexplored. Recent research suggests that biased AI can amplify human cognitive biases, while well-calibrated systems might help mitigate them. In this position paper, I advocate for transcending beyond separate treatment of human and AI biases and instead focus on their interaction effects. I argue that a comprehensive framework, one that maps (compound human-AI) biases to mitigation strategies, is essential for understanding and protecting human cognition, and I outline concrete steps for its development.

</details>


### [3] [Clones in the Machine: A Feminist Critique of Agency in Digital Cloning](https://arxiv.org/abs/2504.18807)

*Siân Brooke*

**Main category:** cs.HC

**Keywords:** digital cloning, AI ethics, decentralized data, dynamic consent, feminist theories

**Relevance Score:** 7

**TL;DR:** The paper critiques digital cloning in academic research, emphasizing ethical concerns surrounding consent and representation, while proposing decentralized data repositories as a solution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the ethical implications of digital cloning in research, highlighting issues of consent, agency, and representation that are often overlooked.

**Method:** The critique is based on feminist theories of agency and provides a framework for understanding the impact of digital cloning.

**Key Contributions:** Critique of AI solutionism in digital cloning, Integration of feminist theories to address ethical concerns, Proposals for decentralized data and dynamic consent models

**Result:** The findings reveal that digital cloning can oversimplify human complexity and may reinforce systemic biases due to its reductionist nature.

**Limitations:** The paper does not provide empirical data to support its arguments or proposed solutions.

**Future Work:** Future research could explore the practical implementation of the proposed decentralized data repositories and dynamic consent models.

**Conclusion:** The paper concludes that implementing decentralized data repositories and dynamic consent models can promote more ethical AI practices.

**Abstract:** This paper critiques digital cloning in academic research, highlighting how it exemplifies AI solutionism. Digital clones, which replicate user data to simulate behavior, are often seen as scalable tools for behavioral insights. However, this framing obscures ethical concerns around consent, agency, and representation. Drawing on feminist theories of agency, the paper argues that digital cloning oversimplifies human complexity and risks perpetuating systemic biases. To address these issues, it proposes decentralized data repositories and dynamic consent models, promoting ethical, context-aware AI practices that challenge the reductionist logic of AI solutionism

</details>


### [4] [Understanding Decentralized Social Feed Curation on Mastodon](https://arxiv.org/abs/2504.18817)

*Yuhan Liu, Emmy Song, Owen Xingjian Zhang, Jewel Merriman, Lei Zhang, Andrés Monroy-Hernández*

**Main category:** cs.HC

**Keywords:** Mastodon, decentralized social media, feed curation, user experience, algorithmic curation

**Relevance Score:** 6

**TL;DR:** This paper explores user interactions with decentralized social media platform Mastodon through qualitative interviews and presents a prototype for personalized feed curation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand user perceptions and interactions on decentralized social media platforms like Mastodon and empower personalization of feeds.

**Method:** A two-part interview study with 21 Mastodon users followed by the creation of a web-based prototype, Braids, for feed curation.

**Key Contributions:** Identified user perceptions and curation strategies on Mastodon, Developed a web-based prototype, Braids, for feed personalization, Discussed implications of algorithmic vs rule-based curation in decentralized platforms

**Result:** The findings reveal user interactions with feeds and highlight challenges and opportunities for using design to improve acceptance of algorithmic curation.

**Limitations:** The study is limited to a small sample size of 21 users and focuses solely on Mastodon.

**Future Work:** Future research could explore broader applications of seamful design in decentralized social media and user acceptance of curation algorithms.

**Conclusion:** User experiences can be optimized by addressing the balance between app development and add-ons in decentralized social media.

**Abstract:** As centralized social media platforms face growing concerns, more users are seeking greater control over their social feeds and turning to decentralized alternatives such as Mastodon. The decentralized nature of Mastodon creates unique opportunities for customizing feeds, yet user perceptions and curation strategies on these platforms remain unknown. This paper presents findings from a two-part interview study with 21 Mastodon users, exploring how they perceive, interact with, and manage their current feeds, and how we can better empower users to personalize their feeds on Mastodon. We use the qualitative findings of the first part of the study to guide the creation of Braids, a web-based prototype for feed curation. Results from the second part of our study, using Braids, highlighted opportunities and challenges for future research, particularly in using seamful design to enhance people's acceptance of algorithmic curation and nuanced trade-offs between machine learning-based and rule-based curation algorithms. To optimize user experience, we also discuss the tension between creating new apps and building add-ons in the decentralized social media realm.

</details>


### [5] [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)

*Andrew M. Bean, Rebecca Payne, Guy Parsons, Hannah Rose Kirk, Juan Ciro, Rafael Mosquera, Sara Hincapié Monsalve, Aruna S. Ekanayaka, Lionel Tarassenko, Luc Rocher, Adam Mahdi*

**Main category:** cs.HC

**Keywords:** Large language models, Healthcare, Medical advice, User interaction, Systematic testing

**Relevance Score:** 9

**TL;DR:** A study tests the effectiveness of large language models (LLMs) in assisting the public with medical decision-making, revealing significant gaps in real-world performance despite high accuracy in controlled assessments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the potential of LLMs in providing medical advice and support to the public amidst concerns about their real-world application versus clinical testing outcomes.

**Method:** In a controlled study with 1,298 participants, individuals received assistance from either LLMs (GPT-4o, Llama 3, Command R+) or a source of their choice (control) in ten medical scenarios.

**Key Contributions:** Demonstrated the gap between LLMs' performance in controlled tests versus real-world applicability., Highlighted the importance of user interaction in the effectiveness of LLMs in healthcare advice., Recommended systematic human user testing to ensure the readiness of LLMs for public healthcare use.

**Result:** LLMs demonstrated high accuracy in identifying conditions (94.9%) and dispositions (56.3%) when evaluated alone, but users aided by LLMs only identified conditions in 34.5% and dispositions in 44.2% of cases, similar to the control group.

**Limitations:** Study limited to ten medical scenarios and may not represent all potential healthcare advice situations; controlled environment may not reflect real-world dynamics.

**Future Work:** Encourage further research on human user testing for assessing interaction capabilities of LLMs in healthcare before deployment.

**Conclusion:** The user interaction with LLMs poses challenges to their effectiveness in medical contexts, indicating a need for rigorous user testing before public deployment.

**Abstract:** Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.

</details>


### [6] [AI Chatbots for Mental Health: Values and Harms from Lived Experiences of Depression](https://arxiv.org/abs/2504.18932)

*Dong Whi Yoo, Jiayue Melissa Shi, Violeta J. Rodriguez, Koustuv Saha*

**Main category:** cs.HC

**Keywords:** mental health, AI chatbots, user experience, depression, ethical design

**Relevance Score:** 9

**TL;DR:** This study examines the development and user insights of a chatbot called Zenny for depression self-management, focusing on values related to harms in mental health AI.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the values of individuals with lived experiences of depression relate to potential harms caused by AI mental health chatbots.

**Method:** A technology probe in the form of a GPT-4o based chatbot was developed and used to interview 17 individuals with lived experiences of depression; thematic analysis was conducted on the responses.

**Key Contributions:** Development of Zenny, a chatbot for depression self-management, Identification of key values important for mental health AI design, Recommendations for minimizing risks in AI chatbots for mental health

**Result:** Key values identified include informational support, emotional support, personalization, privacy, and crisis management, which are crucial for improving chatbot design.

**Limitations:** The study is limited by its small sample size (17 participants) and could benefit from a broader demographic.

**Future Work:** Future research should explore the implementation of these values in wider contexts and with diverse populations.

**Conclusion:** The study provides insights into lived experience values that can inform the design of more effective and safer AI chatbots for mental health support.

**Abstract:** Recent advancements in LLMs enable chatbots to interact with individuals on a range of queries, including sensitive mental health contexts. Despite uncertainties about their effectiveness and reliability, the development of LLMs in these areas is growing, potentially leading to harms. To better identify and mitigate these harms, it is critical to understand how the values of people with lived experiences relate to the harms. In this study, we developed a technology probe, a GPT-4o based chatbot called Zenny, enabling participants to engage with depression self-management scenarios informed by previous research. We used Zenny to interview 17 individuals with lived experiences of depression. Our thematic analysis revealed key values: informational support, emotional support, personalization, privacy, and crisis management. This work explores the relationship between lived experience values, potential harms, and design recommendations for mental health AI chatbots, aiming to enhance self-management support while minimizing risks.

</details>


### [7] [Advancing Face-to-Face Emotion Communication: A Multimodal Dataset (AFFEC)](https://arxiv.org/abs/2504.18969)

*Meisam J. Sekiavandi, Laurits Dixen, Jostein Fimland, Sree Keerthi Desu, Antonia-Bianca Zserai, Ye Sul Lee, Maria Barrett, Paolo Burre*

**Main category:** cs.HC

**Keywords:** emotion recognition, multimodal dataset, human-computer interaction, affect, EEG

**Relevance Score:** 9

**TL;DR:** The paper presents the AFFEC dataset designed for emotion recognition in human-computer interaction, featuring multimodal data from 73 participants in simulated emotional dialogues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve emotion recognition systems by capturing the complexity of human emotions in real-world interactions, addressing the limitations of current datasets.

**Method:** The study introduces the AFFEC dataset, which includes various modalities such as EEG, eye-tracking, GSR, and facial videos, recorded during 84 emotional dialogues involving six emotions. Baseline analyses were conducted to test classification performance.

**Key Contributions:** Introduction of the AFFEC dataset with extensive multimodal data, Distinction between felt and perceived emotions, Demonstration of significant classification improvements with minimal processing

**Result:** Initial analyses show that even simple processing methods can significantly outperform chance-level classification, particularly in identifying arousal, with added personality traits enhancing predictions of felt emotions.

**Limitations:** The dataset is based on simulated interactions, which may not fully replicate the complexity of genuine emotional exchanges in natural settings.

**Future Work:** Further exploration of adaptive emotion recognition systems that leverage the AFFEC dataset in more diverse and uncontrolled environments.

**Conclusion:** The AFFEC dataset enables the development of more nuanced emotion recognition models, taking into account individual differences and contextual factors in emotional communication.

**Abstract:** Emotion recognition has the potential to play a pivotal role in enhancing human-computer interaction by enabling systems to accurately interpret and respond to human affect. Yet, capturing emotions in face-to-face contexts remains challenging due to subtle nonverbal cues, variations in personal traits, and the real-time dynamics of genuine interactions. Existing emotion recognition datasets often rely on limited modalities or controlled conditions, thereby missing the richness and variability found in real-world scenarios.   In this work, we introduce Advancing Face-to-Face Emotion Communication (AFFEC), a multimodal dataset designed to address these gaps. AFFEC encompasses 84 simulated emotional dialogues across six distinct emotions, recorded from 73 participants over more than 5,000 trials and annotated with more than 20,000 labels. It integrates electroencephalography (EEG), eye-tracking, galvanic skin response (GSR), facial videos, and Big Five personality assessments. Crucially, AFFEC explicitly distinguishes between felt emotions (the participant's internal affect) and perceived emotions (the observer's interpretation of the stimulus).   Baseline analyses spanning unimodal features and straightforward multimodal fusion demonstrate that even minimal processing yields classification performance significantly above chance, especially for arousal. Incorporating personality traits further improves predictions of felt emotions, highlighting the importance of individual differences. By bridging controlled experimentation with more realistic face-to-face stimuli, AFFEC offers a unique resource for researchers aiming to develop context-sensitive, adaptive, and personalized emotion recognition models.

</details>


### [8] [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)

*Saramsh Gautam, Mahmood Jasim*

**Main category:** cs.HC

**Keywords:** multilingual communication, collaborative research, language barriers, ESL researchers, multimodal systems

**Relevance Score:** 8

**TL;DR:** The paper presents LINC, a multimodal system designed to enhance collaboration among ESL researchers in multilingual teams, addressing communication challenges through real-time support and post-meeting analysis.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the difficulties faced by ESL researchers in communicating and contributing during multilingual collaborative meetings, highlighting the need for effective support systems.

**Method:** The authors conducted a survey of 64 ESL researchers to identify design goals and subsequently developed the LINC system, which includes a real-time communication module and a post-meeting dashboard. A two-phased evaluation study was performed with six multilingual team triads to assess user experience and communication effectiveness.

**Key Contributions:** Development of a multimodal collaboration system (LINC) for ESL researchers., Real-time communication and post-meeting analysis features that support multilingual discussions., Identification of factors impacting multilingual meeting participation beyond language.

**Result:** Participants using LINC reported improved communication in their preferred language, enhanced recall of meeting insights, and better preparedness for future meetings, demonstrating the system's effectiveness in addressing language barriers.

**Limitations:** The study involved a small sample size and was conducted in specific multilingual contexts, which may limit generalizability.

**Future Work:** Future research should explore further integrations of multimodal features and their impact on diverse collaborative contexts beyond research environments.

**Conclusion:** LINC supports effective collaboration in multilingual settings, suggesting that factors beyond language preferences also affect participation, emphasizing the relevance of multimodal systems in such collaborative environments.

**Abstract:** Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.

</details>


### [9] [Investigating the Prominence and Severity of Bugs and Glitches Within Games and Their Effects on Player Experience](https://arxiv.org/abs/2504.19010)

*Jessica Backus*

**Main category:** cs.HC

**Keywords:** video games, glitches, player experience, thematic analysis, game design

**Relevance Score:** 3

**TL;DR:** This research explores the impact of video game glitches and bugs on player experience through literature review and thematic analysis of observations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the implications of glitches and bugs in video games on player experience, filling a gap in existing literature that mainly focuses on the mechanics of bugs rather than their effects on players.

**Method:** The methodology involved reviewing relevant literature and conducting observations of different glitches and bugs in games, analyzed through thematic analysis.

**Key Contributions:** Identifies a gap in literature regarding player experience in relation to game glitches., Utilizes thematic analysis to connect observed data with theoretical implications., Highlights behavioral patterns emerging from player interactions with glitches.

**Result:** The study generated themes that connect the observed glitches to existing literature, revealing a lack of focus on the player's overall experience and behaviors influenced by these glitches.

**Limitations:** The study is exploratory and based on observational data, which may not capture all player experiences or types of glitches comprehensively.

**Future Work:** Future research could explore specific player reactions to different types of glitches and how these insights could influence game development.

**Conclusion:** The research suggests that understanding player experience in the context of game glitches and bugs is crucial, as it could inform game design and improve player satisfaction.

**Abstract:** Different errors that occur in video games are often referred to as glitches or bugs. The goal of this exploratory research is to understand how these glitches and bugs within video games affect a players experience. To do this, I reviewed relevant literature and performed observations of these different errors in different games via Twitch livestreams. I then performed thematic analysis with the observation data and generated themes that tie back into to the relevant literature. Most of the current literature focuses on the what and how behind bugs in games, but very little on the implications of these bugs on the overall experience for the players, and what patterns of behavior may emerge because of them.

</details>


### [10] [Generative AI Literacy: A Comprehensive Framework for Literacy and Responsible Use](https://arxiv.org/abs/2504.19038)

*Chengzhi Zhang, Brian Magerko*

**Main category:** cs.HC

**Keywords:** Generative AI, Literacy, Guidelines, Ethical Use, AI Tools

**Relevance Score:** 8

**TL;DR:** This paper proposes 12 guidelines for generative AI literacy to help individuals evaluate and use generative AI tools responsibly.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid rise of generative AI tools has outpaced existing AI literacy frameworks, resulting in a critical gap in users' ability to evaluate and use these technologies effectively.

**Method:** The authors developed a set of guidelines organized into four key aspects related to the selection, interaction, and understanding of generative AI.

**Key Contributions:** Introduction of 12 specific guidelines for generative AI literacy., Categorization of guidelines into four key aspects for clarity., Focus on ethical and informed utilization of generative AI tools.

**Result:** The proposed guidelines aim to empower users in schools, companies, and organizations to engage with generative AI in an informed and ethical manner.

**Limitations:** The effectiveness of the guidelines in diverse contexts remains to be empirically tested.

**Future Work:** Future research should explore the practical implementation of these guidelines and their impact on user competency.

**Conclusion:** The guidelines provide a structured approach to enhance generative AI literacy, fostering responsible usage among diverse user groups.

**Abstract:** After the release of several AI literacy guidelines, the rapid rise and widespread adoption of generative AI, such as ChatGPT, Dall E, and Deepseek, have transformed our lives. Unlike traditional AI algorithms (e.g., convolutional neural networks, semantic networks, classifiers) captured in existing AI literacy frameworks, generative AI exhibits distinct and more nuanced characteristics. However, a lack of robust generative AI literacy is hindering individuals ability to evaluate critically and use these models effectively and responsibly. To address this gap, we propose a set of guidelines with 12 items for generative AI literacy, organized into four key aspects: (1) Guidelines for Generative AI Tool Selection and Prompting, (2) Guidelines for Understanding Interaction with Generative AI, (3) Guidelines for Understanding Interaction with Generative AI, and (4) Guidelines for High Level Understanding of Generative AI. These guidelines aim to support schools, companies, educators, and organizations in developing frameworks that empower their members, such as students, employees, and stakeholders, to use generative AI in an efficient, ethical, and informed way.

</details>


### [11] [Beyond Levels of Driving Automation: A Triadic Framework of Human-AI Collaboration in On-Road Mobility](https://arxiv.org/abs/2504.19120)

*Gaojian Huang, Yantong Jin, Wei-Hsiang Lo*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Automated vehicles, Dynamic roles

**Relevance Score:** 4

**TL;DR:** This study proposes a triadic human-AI collaboration framework for automated vehicles, focusing on dynamic roles in real-time collaboration.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of clarity in how human users and AI should collaborate in dynamic driving contexts, where control roles shift frequently.

**Method:** The study introduces a framework comprising three AI roles (Advisor, Co-Pilot, and Guardian) that adapt to the needs of human users in real-time.

**Key Contributions:** Introduction of a triadic human-AI collaboration framework, Definition of three adaptive AI roles, Foundation for role-based collaboration strategies in automated vehicles

**Result:** The framework provides a basis for developing adaptive, role-based collaboration strategies in automated vehicles.

**Limitations:** 

**Future Work:** Future research could explore practical applications of the framework in real-world driving scenarios and investigate user acceptance of the adaptive roles.

**Conclusion:** The proposed framework aims to improve human-AI collaboration in dynamic driving scenarios by defining flexible AI roles.

**Abstract:** The goal of the current study is to introduce a triadic human-AI collaboration framework for the automated vehicle domain. Previous classifications (e.g., SAE Levels of Automation) focus on defining automation levels based on who controls the vehicle. However, it remains unclear how human users and AI should collaborate in real-time, especially in dynamic driving contexts, where roles can shift frequently. To fill the gap, this study proposes a triadic human-AI collaboration framework with three AI roles (i.e., Advisor, Co-Pilot, and Guardian) that dynamically adapt to human needs. Overall, the study lays a foundation for developing adaptive, role-based human-AI collaboration strategies in automated vehicles.

</details>


### [12] [SnuggleSense: Empowering Online Harm Survivors Through a Structured Sensemaking Process](https://arxiv.org/abs/2504.19158)

*Sijia Xiao, Haodi Zou, Amy Mathews, Jingshu Rui, Coye Cheshire, Niloufar Salehi*

**Main category:** cs.HC

**Keywords:** SnuggleSense, cyberbullying, restorative justice, online harm, support networks

**Relevance Score:** 7

**TL;DR:** SnuggleSense is a system designed to empower survivors of online interpersonal harm through structured sensemaking, enhancing their agency and promoting restorative justice.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of traditional content moderation approaches in supporting survivors of online harm such as cyberbullying and sexual harassment.

**Method:** A controlled experiment was conducted to compare the effectiveness of SnuggleSense in enhancing survivors' sensemaking to an unstructured approach.

**Key Contributions:** Introduction of SnuggleSense for structured sensemaking of interpersonal harm., Empowerment of survivors through personalized recommendations and community support., Insights into design principles for supporting survivors' agency during recovery.

**Result:** SnuggleSense significantly enhances sensemaking, fostering community awareness and supportive networks while promoting a restorative justice ethos.

**Limitations:** 

**Future Work:** 

**Conclusion:** The system provides valuable design insights for tailoring support to survivors while maintaining their agency, emphasizing the importance of community and restorative practices.

**Abstract:** Online interpersonal harm, such as cyberbullying and sexual harassment, remains a pervasive issue on social media platforms. Traditional approaches, primarily content moderation, often overlook survivors' needs and agency. We introduce SnuggleSense, a system that empowers survivors through structured sensemaking. Inspired by restorative justice practices, SnuggleSense guides survivors through reflective questions, offers personalized recommendations from similar survivors, and visualizes plans using interactive sticky notes. A controlled experiment demonstrates that SnuggleSense significantly enhances sensemaking compared to an unstructured process of making sense of the harm. We argue that SnuggleSense fosters community awareness, cultivates a supportive survivor network, and promotes a restorative justice-oriented approach toward restoration and healing. We also discuss design insights, such as tailoring informational support and providing guidance while preserving survivors' agency.

</details>


### [13] [Beyond Physical Reach: Comparing Head- and Cane-Mounted Cameras for Last-Mile Navigation by Blind Users](https://arxiv.org/abs/2504.19345)

*Apurv Varshney, Lucas Nadolskis, Tobias Höllerer, Michael Beyeler*

**Main category:** cs.HC

**Keywords:** Blind navigation, Wearable cameras, Assistive technology, Human-computer interaction, Hybrid navigation aids

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of wearable cameras for blind navigation by comparing head- and cane-mounted perspectives through user surveys and controlled experiments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Blind individuals experience significant challenges in last-mile navigation, necessitating the development of effective assistive navigation tools.

**Method:** The study involved surveying ten experienced blind cane users to understand their navigation strategies and technology preferences, followed by controlled data collection with cameras mounted on the head and cane of a blind participant navigating various environments.

**Key Contributions:** Systematic comparison of head-mounted vs. cane-mounted cameras for navigation, Insights from user surveys on assistive technology preferences, Evaluation of SLAM and 3D reconstruction for navigation aid design

**Result:** Findings revealed that head-mounted cameras provide better localization accuracy, while cane-mounted setups offer superior ground-level coverage. A combined configuration of both cameras yielded the best performance.

**Limitations:** The study is limited to a small number of participants and specific environments, which may not generalize across all navigation challenges for the blind.

**Future Work:** Future research should explore additional environments and further refine sensor integration techniques for improved navigation aids.

**Conclusion:** The results emphasize the importance of hybrid navigation aids that leverage the strengths of multiple sensor placements for better navigation assistance.

**Abstract:** Blind individuals face persistent challenges in last-mile navigation, including locating entrances, identifying obstacles, and navigating complex or cluttered spaces. Although wearable cameras are increasingly used in assistive systems, there has been no systematic, vantage-focused comparison to guide their design. This paper addresses that gap through a two-part investigation. First, we surveyed ten experienced blind cane users, uncovering navigation strategies, pain points, and technology preferences. Participants stressed the importance of multi-sensory integration, destination-focused travel, and assistive tools that complement (rather than replace) the cane's tactile utility. Second, we conducted controlled data collection with a blind participant navigating five real-world environments using synchronized head- and cane-mounted cameras, isolating vantage placement as the primary variable. To assess how each vantage supports spatial perception, we evaluated SLAM performance (for localization and mapping) and NeRF-based 3D reconstruction (for downstream scene understanding). Head-mounted sensors delivered superior localization accuracy, while cane-mounted views offered broader ground-level coverage and richer environmental reconstructions. A combined (head+cane) configuration consistently outperformed both. These results highlight the complementary strengths of different sensor placements and offer actionable guidance for developing hybrid navigation aids that are perceptive, robust, and user-aligned.

</details>


### [14] [MER 2025: When Affective Computing Meets Large Language Models](https://arxiv.org/abs/2504.19423)

*Zheng Lian, Rui Liu, Kele Xu, Bin Liu, Xuefei Liu, Yazhou Zhang, Xin Liu, Yong Li, Zebang Cheng, Haolin Zuo, Ziyang Ma, Xiaojiang Peng, Xie Chen, Ya Li, Erik Cambria, Guoying Zhao, Björn W. Schuller, Jianhua Tao*

**Main category:** cs.HC

**Keywords:** Affective Computing, Large Language Models, Emotion Recognition, Multimodal Cues, Fine-grained Emotions

**Relevance Score:** 8

**TL;DR:** MER2025 challenge focuses on the intersection of affective computing and large language models, addressing innovative approaches for emotion recognition.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore emerging trends in affective computing, particularly in relation to large language models (LLMs).

**Method:** The challenge includes four tracks: MER-SEMI for semi-supervised learning of fixed categorical emotions, MER-FG for fine-grained emotion recognition, MER-DES for incorporating multimodal cues, and MER-PR for exploring emotion prediction's impact on personality recognition.

**Key Contributions:** Focus on LLMs in emotion recognition, Introduction of multimodal cues, Exploration of fine-grained emotional states

**Result:** Participants will work with baseline code and datasets provided for each challenge track, facilitating research in emotion understanding.

**Limitations:** 

**Future Work:** Potential future research could extend LLM applications in emotion recognition and investigate additional multimodal approaches.

**Conclusion:** MER2025 represents a shift from traditional emotion taxonomies to LLM-driven generative methods for emotion recognition.

**Abstract:** MER2025 is the third year of our MER series of challenges, aiming to bring together researchers in the affective computing community to explore emerging trends and future directions in the field. Previously, MER2023 focused on multi-label learning, noise robustness, and semi-supervised learning, while MER2024 introduced a new track dedicated to open-vocabulary emotion recognition. This year, MER2025 centers on the theme "When Affective Computing Meets Large Language Models (LLMs)".We aim to shift the paradigm from traditional categorical frameworks reliant on predefined emotion taxonomies to LLM-driven generative methods, offering innovative solutions for more accurate and reliable emotion understanding. The challenge features four tracks: MER-SEMI focuses on fixed categorical emotion recognition enhanced by semi-supervised learning; MER-FG explores fine-grained emotions, expanding recognition from basic to nuanced emotional states; MER-DES incorporates multimodal cues (beyond emotion words) into predictions to enhance model interpretability; MER-PR investigates whether emotion prediction results can improve personality recognition performance. For the first three tracks, baseline code is available at MERTools, and datasets can be accessed via Hugging Face. For the last track, the dataset and baseline code are available on GitHub.

</details>


### [15] [A Real-Time Gesture-Based Control Framework](https://arxiv.org/abs/2504.19460)

*Mahya Khazaei, Ali Bahrani, George Tzanetakis*

**Main category:** cs.HC

**Keywords:** Gesture Control, Human-Computer Interaction, Machine Learning, Audio Manipulation, Interactive Installations

**Relevance Score:** 7

**TL;DR:** A gesture control framework enhances live audio interactions through human movement analysis using video input.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create an immersive real-time audio manipulation experience for performers by linking visual and auditory stimuli.

**Method:** The framework utilizes computer vision and machine learning to track movements and interpret gestures, enabling audio control based on user actions.

**Key Contributions:** Real-time gesture control for audio manipulation, Integration of computer vision with machine learning techniques, User-independent functionality requiring minimal gesture samples

**Result:** Users can influence music elements like tempo and pitch with simple gestures, requiring minimal gesture samples for effective training.

**Limitations:** 

**Future Work:** Further exploration of gesture diversity and enhanced machine learning models for broader applications.

**Conclusion:** The developed system showcases the potential for intuitive human-machine interactions in live performance and installation settings.

**Abstract:** We introduce a real-time, human-in-the-loop gesture control framework that can dynamically adapt audio and music based on human movement by analyzing live video input. By creating a responsive connection between visual and auditory stimuli, this system enables dancers and performers to not only respond to music but also influence it through their movements. Designed for live performances, interactive installations, and personal use, it offers an immersive experience where users can shape the music in real time.   The framework integrates computer vision and machine learning techniques to track and interpret motion, allowing users to manipulate audio elements such as tempo, pitch, effects, and playback sequence. With ongoing training, it achieves user-independent functionality, requiring as few as 50 to 80 samples to label simple gestures. This framework combines gesture training, cue mapping, and audio manipulation to create a dynamic, interactive experience. Gestures are interpreted as input signals, mapped to sound control commands, and used to naturally adjust music elements, showcasing the seamless interplay between human interaction and machine response.

</details>


### [16] [Scene2Hap: Combining LLMs and Physical Modeling for Automatically Generating Vibrotactile Signals for Full VR Scenes](https://arxiv.org/abs/2504.19611)

*Arata Jingu, Easa AliAbbasi, Paul Strohmeier, Jürgen Steimle*

**Main category:** cs.HC

**Keywords:** Haptic feedback, Virtual reality, Large language model, Multimodal, Vibrotactile signals

**Relevance Score:** 9

**TL;DR:** Scene2Hap is an LLM-centered system that automates the design of vibrotactile feedback for VR scenes based on object semantics and physical context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Creating realistic haptic feedback for all objects in a VR environment is labor-intensive and time-consuming, necessitating an automated solution.

**Method:** Scene2Hap utilizes a multimodal large language model to analyze and generate haptic feedback by estimating semantic attributes and physical context of objects in VR scenes.

**Key Contributions:** Development of Scene2Hap for automated haptic feedback design in VR, Integration of multimodal inputs for hardware and software synergy in haptic feedback, Demonstrated improved user experience through empirical testing.

**Result:** User studies show that Scene2Hap effectively estimates semantics and physical context while improving usability, perceived materiality, and spatial awareness through effective vibration modeling.

**Limitations:** The effectiveness may vary with more complex scenarios and diverse object types in VR setups.

**Future Work:** Further exploration of advanced models for diverse object interactions and integration with more VR platforms is needed.

**Conclusion:** The approach significantly enhances the immersive experience of VR by providing tailored haptic feedback based on an object's characteristics.

**Abstract:** Haptic feedback contributes to immersive virtual reality (VR) experiences. Designing such feedback at scale, for all objects within a VR scene and their respective arrangements, remains a time-consuming task. We present Scene2Hap, an LLM-centered system that automatically designs object-level vibrotactile feedback for entire VR scenes based on the objects' semantic attributes and physical context. Scene2Hap employs a multimodal large language model to estimate the semantics and physical context of each object, including its material properties and vibration behavior, from the multimodal information present in the VR scene. This semantic and physical context is then used to create plausible vibrotactile signals by generating or retrieving audio signals and converting them to vibrotactile signals. For the more realistic spatial rendering of haptics in VR, Scene2Hap estimates the propagation and attenuation of vibration signals from their source across objects in the scene, considering the estimated material properties and physical context, such as the distance and contact between virtual objects. Results from two user studies confirm that Scene2Hap successfully estimates the semantics and physical context of VR scenes, and the physical modeling of vibration propagation improves usability, perceived materiality, and spatial awareness.

</details>


### [17] [Interactive Discovery and Exploration of Visual Bias in Generative Text-to-Image Models](https://arxiv.org/abs/2504.19703)

*Johannes Eschner, Roberto Labadie-Tamayo, Matthias Zeppelzauer, Manuela Waldner*

**Main category:** cs.HC

**Keywords:** Visual Bias Explorer, Text-to-Image, Bias analysis, Generative models, CLIP

**Relevance Score:** 8

**TL;DR:** Introduction of Visual Bias Explorer (ViBEx) to analyze visual bias in T2I models.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically analyze and uncover biases present in generative Text-to-Image models.

**Method:** ViBEx utilizes a flexible prompting tree interface and zero-shot bias probing with CLIP for bias exploration and supports confirmatory analysis through visual inspection of biases.

**Key Contributions:** Development of ViBEx for interactive bias exploration, Implementation of a novel flexible prompting tree interface, Support for confirmatory bias analysis using visual inspection

**Result:** Experts in AI and ethics identified previously undocumented visual biases in T2I outputs using ViBEx.

**Limitations:** 

**Future Work:** Further development and application of ViBEx to explore additional biases and refine bias discovery techniques.

**Conclusion:** ViBEx provides a model-agnostic tool for exploring and analyzing visual bias in generative T2I models.

**Abstract:** Bias in generative Text-to-Image (T2I) models is a known issue, yet systematically analyzing such models' outputs to uncover it remains challenging. We introduce the Visual Bias Explorer (ViBEx) to interactively explore the output space of T2I models to support the discovery of visual bias. ViBEx introduces a novel flexible prompting tree interface in combination with zero-shot bias probing using CLIP for quick and approximate bias exploration. It additionally supports in-depth confirmatory bias analysis through visual inspection of forward, intersectional, and inverse bias queries. ViBEx is model-agnostic and publicly available. In four case study interviews, experts in AI and ethics were able to discover visual biases that have so far not been described in literature.

</details>


### [18] [Crafting a Personal Journaling Practice: Negotiating Ecosystems of Materials, Personal Context, and Community in Analog Journaling](https://arxiv.org/abs/2504.19767)

*Katherine Lin, Juna Kawai-Yue, Adira Sklar, Lucy Hecht, Sarah Sterman, Tiffany Tseng*

**Main category:** cs.HC

**Keywords:** Analog journaling, Mental health, Design opportunities, Qualitative analysis, User practices

**Relevance Score:** 3

**TL;DR:** This paper explores analog journaling practices, their personal motivations, and the ecosystems that shape them, based on qualitative analysis of social media content and interviews.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how analog journaling practices shape mental health and identity development.

**Method:** Qualitative analysis of public journaling content on YouTube and Instagram, along with interviews with 11 journalers.

**Key Contributions:** Identification of the journaling ecosystem affecting practices and identities., Insights into design opportunities for future journaling tools., Analysis of qualitative data from social media and interviews.

**Result:** Identified a journaling ecosystem that influences practices and identities, showcasing how materials, context, and communities shape journaling.

**Limitations:** 

**Future Work:** Exploration of how future tools can support diverse journaling practices.

**Conclusion:** Insights can inform design opportunities for future journaling tools that better reflect user practices and goals.

**Abstract:** Analog journaling has grown in popularity, with journaling on paper encompassing a range of motivations, styles, and practices including planning, habit-tracking, and reflecting. Journalers develop strong personal preferences around the tools they use, the ideas they capture, and the layout in which they represent their ideas and memories. Understanding how analog journaling practices are individually shaped and crafted over time is critical to supporting the varied benefits associated with journaling, including improved mental health and positive support for identity development. To understand this development, we qualitatively analyzed publicly-shared journaling content from YouTube and Instagram and interviewed 11 journalers. We report on our identification of the journaling ecosystem in which journaling practices are shaped by materials, personal context, and communities, sharing how this ecosystem plays a role in the practices and identities of journalers as they customize their journaling routine to best suit their personal goals. Using these insights, we discuss design opportunities for how future tools can better align with and reflect the rich affordances and practices of journaling on paper.

</details>


### [19] [Memento: Augmenting Personalized Memory via Practical Multimodal Wearable Sensing in Visual Search and Wayfinding Navigation](https://arxiv.org/abs/2504.19772)

*Indrajeet Ghosh, Kasthuri Jayarajah, Nicholas Waytowich, Nirmalya Roy*

**Main category:** cs.HC

**Keywords:** Working Memory, Wearable Technology, Cognitive Load, Human-Computer Interaction, Intelligent Cues

**Relevance Score:** 8

**TL;DR:** Memento is a framework that enhances short-term memory recall using multimodal wearable sensor data, showing significant improvements in recall performance and cognitive load reduction through user studies.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in using technology to support short-term recall in daily activities, as most prior research focuses on enhancing long-term memory.

**Method:** Memento utilizes multimodal wearable sensor data to detect changes in cognitive state and provide intelligent cues to assist recall during tasks.

**Key Contributions:** Introduction of Memento framework for short-term memory support, Significant empirical results showing improvements in recall and cognitive load, Comparison of effectiveness against computer vision-based approaches

**Result:** User studies showed that participants receiving cues from Memento improved recall by 20-23% and reduced cognitive load and review time by 46%.

**Limitations:** The study is limited to specific tasks and participant demographics.

**Future Work:** Exploration of additional cognitive tasks and broader user demographics for further validation of the framework.

**Conclusion:** Memento demonstrates effective enhancement of short-term recall through intelligent cueing, significantly outperforming existing approaches.

**Abstract:** Working memory involves the temporary retention of information over short periods. It is a critical cognitive function that enables humans to perform various online processing tasks, such as dialing a phone number, recalling misplaced items' locations, or navigating through a store. However, inherent limitations in an individual's capacity to retain information often result in forgetting important details during such tasks. Although previous research has successfully utilized wearable and assistive technologies to enhance long-term memory functions (e.g., episodic memory), their application to supporting short-term recall in daily activities remains underexplored. To address this gap, we present Memento, a framework that uses multimodal wearable sensor data to detect significant changes in cognitive state and provide intelligent in situ cues to enhance recall. Through two user studies involving 15 and 25 participants in visual search navigation tasks, we demonstrate that participants receiving visual cues from Memento achieved significantly better route recall, improving approximately 20-23% compared to free recall. Furthermore, Memento reduced cognitive load and review time by 46% while also substantially reducing computation time (3.86 seconds vs. 15.35 seconds), offering an average of 75% effectiveness compared to computer vision-based cue selection approaches.

</details>


### [20] [LLM-Powered GUI Agents in Phone Automation: Surveying Progress and Prospects](https://arxiv.org/abs/2504.19838)

*Guangyi Liu, Pengxiang Zhao, Liang Liu, Yaxuan Guo, Han Xiao, Weifeng Lin, Yuxiang Chai, Yue Han, Shuai Ren, Hao Wang, Xiaoyu Liang, Wenhao Wang, Tianze Wu, Linghao Li, Hao Wang, Guanjing Xiong, Yong Liu, Hongsheng Li*

**Main category:** cs.HC

**Keywords:** Large Language Models, Phone Automation, GUI Agents, User Intent, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper reviews the evolution and challenges of LLM-driven phone GUI agents, proposing a taxonomy and addressing open challenges in the field.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models, phone automation has changed significantly, necessitating a comprehensive understanding and taxonomy of LLM-driven GUI agents.

**Method:** The paper reviews existing literature, identifies key challenges, proposes a taxonomy, and discusses various methodologies including supervised fine-tuning and reinforcement learning for enhancing phone GUI agents.

**Key Contributions:** Provides a taxonomy of LLM-driven phone GUI agent frameworks and methodologies., Identifies key challenges and how LLMs can overcome them., Offers insights into future research directions and open challenges in the field.

**Result:** It identifies three main challenges in phone GUI automation and how LLMs can address these through advanced language understanding and multimodal perception, paving the way for adaptive and intelligent phone agents.

**Limitations:** The paper may not cover all emerging technologies and approaches as the field is rapidly evolving.

**Future Work:** The authors suggest exploring dataset diversity, deployment efficiency, user-centric adaptation, and addressing security concerns as future research areas.

**Conclusion:** The study serves as a reference for researchers, highlighting research gaps and future directions in the use of LLMs in phone GUI automation.

**Abstract:** With the rapid rise of large language models (LLMs), phone automation has undergone transformative changes. This paper systematically reviews LLM-driven phone GUI agents, highlighting their evolution from script-based automation to intelligent, adaptive systems. We first contextualize key challenges, (i) limited generality, (ii) high maintenance overhead, and (iii) weak intent comprehension, and show how LLMs address these issues through advanced language understanding, multimodal perception, and robust decision-making. We then propose a taxonomy covering fundamental agent frameworks (single-agent, multi-agent, plan-then-act), modeling approaches (prompt engineering, training-based), and essential datasets and benchmarks. Furthermore, we detail task-specific architectures, supervised fine-tuning, and reinforcement learning strategies that bridge user intent and GUI operations. Finally, we discuss open challenges such as dataset diversity, on-device deployment efficiency, user-centric adaptation, and security concerns, offering forward-looking insights into this rapidly evolving field. By providing a structured overview and identifying pressing research gaps, this paper serves as a definitive reference for researchers and practitioners seeking to harness LLMs in designing scalable, user-friendly phone GUI agents.

</details>


### [21] [Applying LLM-Powered Virtual Humans to Child Interviews in Child-Centered Design](https://arxiv.org/abs/2504.20016)

*Linshi Li, Hanlin Cai*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, Child-centered design, Virtual humans

**Relevance Score:** 8

**TL;DR:** This study explores the use of AI-powered virtual humans in child-centered design to improve engagement in interviews with children.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Engaging children directly in research is essential for understanding their experiences, but traditional methods often prioritize adult perspectives and face unique challenges.

**Method:** The study established design guidelines for LLM-powered virtual humans tailored to interviewing children, developed three Human-AI workflows, and conducted a user study with 15 children aged 6 to 12.

**Key Contributions:** Key design guidelines for child interviews using AI-powered virtual humans, Development of three human-AI interaction workflows (LLM-Auto, LLM-Interview, LLM-Analyze), Empirical results demonstrating improved child engagement with LLM-Analyze workflow

**Result:** The LLM-Analyze workflow significantly outperformed others by eliciting longer responses, achieving higher user experience ratings, and promoting better engagement with children.

**Limitations:** The study is limited to a specific age group (6 to 12 years) and involves only a small sample size (15 children).

**Future Work:** Further research could explore the adaptability of these workflows for different age groups and contexts.

**Conclusion:** AI-powered virtual humans can enhance child-centered research by providing a structured and engaging interaction method for interviews.

**Abstract:** In child-centered design, directly engaging children is crucial for deeply understanding their experiences. However, current research often prioritizes adult perspectives, as interviewing children involves unique challenges such as environmental sensitivities and the need for trust-building. AI-powered virtual humans (VHs) offer a promising approach to facilitate engaging and multimodal interactions with children. This study establishes key design guidelines for LLM-powered virtual humans tailored to child interviews, standardizing multimodal elements including color schemes, voice characteristics, facial features, expressions, head movements, and gestures. Using ChatGPT-based prompt engineering, we developed three distinct Human-AI workflows (LLM-Auto, LLM-Interview, and LLM-Analyze) and conducted a user study involving 15 children aged 6 to 12. The results indicated that the LLM-Analyze workflow outperformed the others by eliciting longer responses, achieving higher user experience ratings, and promoting more effective child engagement.

</details>


### [22] [Cam-2-Cam: Exploring the Design Space of Dual-Camera Interactions for Smartphone-based Augmented Reality](https://arxiv.org/abs/2504.20035)

*Brandon Woodard, Melvin He, Mose Sakashita, Jing Qian, Zainab Iftikhar, Joseph LaViola Jr*

**Main category:** cs.HC

**Keywords:** Augmented Reality, User Interaction, Dual-Camera Systems, Smartphone AR, User Experience

**Relevance Score:** 7

**TL;DR:** The paper introduces Cam-2-Cam, a concept for smartphone AR that utilizes both front and rear cameras for enhanced interaction. It discusses key design lessons derived from participant studies, focusing on contextual relevance and user orientation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the practicality of smartphone-based AR applications by overcoming limitations of single-camera systems and improving user interaction experiences.

**Method:** Implemented the Cam-2-Cam interaction concept in three AR applications; conducted qualitative analysis with 30 participants to gather insights on user interactions.

**Key Contributions:** Introduction of the Cam-2-Cam interaction concept for smartphone AR., Identification of design principles that balance user experience with technology limitations., Insights into preventing user disorientation during AR interactions.

**Result:** Two main design lessons were identified: balancing contextual relevance with feedback quality, and preventing user disorientation through simultaneous capture and alternating camera usage.

**Limitations:** Findings are based on qualitative analysis with a limited sample size of participants.

**Future Work:** Further exploration of dual-camera setups and their impact on user experiences in smartphone AR.

**Conclusion:** The design lessons aim to expand the interaction space of AR and encourage the adoption of dual-camera setups in future smartphone AR applications.

**Abstract:** Off-the-shelf smartphone-based AR systems typically use a single front-facing or rear-facing camera, which restricts user interactions to a narrow field of view and small screen size, thus reducing their practicality. We present \textit{Cam-2-Cam}, an interaction concept implemented in three smartphone-based AR applications with interactions that span both cameras. Results from our qualitative analysis conducted on 30 participants presented two major design lessons that explore the interaction space of smartphone AR while maintaining critical AR interface attributes like embodiment and immersion: (1) \textit{Balancing Contextual Relevance and Feedback Quality} serves to outline a delicate balance between implementing familiar interactions people do in the real world and the quality of multimodal AR responses and (2) \textit{Preventing Disorientation using Simultaneous Capture and Alternating Cameras} which details how to prevent disorientation during AR interactions using the two distinct camera techniques we implemented in the paper. Additionally, we consider observed user assumptions or natural tendencies to inform future implementations of dual-camera setups for smartphone-based AR. We envision our design lessons as an initial pioneering step toward expanding the interaction space of smartphone-based AR, potentially driving broader adoption and overcoming limitations of single-camera AR.

</details>


### [23] [Generative AI has lowered the barriers to computational social sciences](https://arxiv.org/abs/2311.10833)

*Yongjun Zhang*

**Main category:** cs.HC

**Keywords:** Generative AI, Computational Social Science, Data Analysis, Prompt Engineering, Educational Tools

**Relevance Score:** 5

**TL;DR:** This paper explores how generative AI enhances computational social science by improving productivity, data analysis, and educational accessibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for enhancing productivity, data analysis capabilities, and educational accessibility in computational social science, especially for those with limited programming skills.

**Method:** The paper discusses the applications of generative AI for automating code generation, annotation, debugging, and advanced data analysis through prompt engineering.

**Key Contributions:** Enhancement of productivity in social science research through automation., Introduction of prompt engineering for sophisticated data analysis., Improvement in educational tools for computational social science.

**Result:** Generative AI significantly boosts the productivity of social scientists, allows for advanced data analysis, and improves educational tools for learning coding and data interaction.

**Limitations:** 

**Future Work:** Further research on integrating generative AI tools into existing social science methodologies.

**Conclusion:** Generative AI holds transformative potential for computational social science, making it more accessible and efficient for researchers and students.

**Abstract:** Generative artificial intelligence (AI) has revolutionized the field of computational social science (CSS), unleashing new possibilities for collecting and analyzing multimodal data, especially for scholars who may not have extensive programming expertise. This breakthrough carries profound implications for social scientists. First, generative AI can significantly enhance the productivity of social scientists by automating the generation, annotation, and debugging of code. Second, it empowers researchers to delve into sophisticated data analysis through the innovative use of prompt engineering. Last, the educational sphere of CSS stands to benefit immensely from these tools, given their exceptional ability to annotate and elucidate complex codes for learners, thereby simplifying the learning process and making the technology more accessible.

</details>


### [24] [Predicting and Understanding Turn-Taking Behavior in Open-Ended Group Activities in Virtual Reality](https://arxiv.org/abs/2407.02896)

*Portia Wang, Eugy Han, Anna C. M. Queiroz, Cyan DeVeaux, Jeremy N. Bailenson*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Turn-taking, Social Dynamics, Machine Learning, Adaptative Assistive Technologies

**Relevance Score:** 7

**TL;DR:** This study predicts turn-taking behaviors in virtual reality using social dynamics features, analyzing 77 VR classroom sessions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage user behaviors and social dynamics in networked VR to enhance understanding and prediction of turn-taking and to provide personalized assistance in social interactions.

**Method:** The study analyzed a large-scale VR classroom dataset and employed gradient boosting classifiers to predict turn-taking behaviors across three tasks relevant to social interactions.

**Key Contributions:** Prediction of turn-taking behaviors in VR environments using social dynamics features, Identification of salient factors influencing turn-taking predictions, Demonstration of robust model performance across novel groups and activities

**Result:** The classifiers achieved accuracies between 0.71 and 0.78 AUC, identifying key features influencing predictions such as group size and listener behavior.

**Limitations:** The approach is limited to social interactions captured in the dataset and may not generalize to all types of interactions outside of the VR classroom context.

**Future Work:** Investigating the application of these predictive models in varied VR settings and exploring further personalization of virtual assistant behaviors for users with sensory differences.

**Conclusion:** The findings indicate that specific social dynamics features are reliable indicators of turn-taking in VR, applicable across diverse social settings.

**Abstract:** In networked virtual reality (VR), user behaviors, individual differences, and group dynamics can serve as important signals into future speech behaviors, such as who the next speaker will be and the timing of turn-taking behaviors. The ability to predict and understand these behaviors offers opportunities to provide adaptive and personalized assistance, for example helping users with varying sensory abilities navigate complex social scenes and instantiating virtual moderators with natural behaviors. In this work, we predict turn-taking behaviors using features extracted based on social dynamics literature. We discuss results from a large-scale VR classroom dataset consisting of 77 sessions and 1660 minutes of small-group social interactions collected over four weeks. In our evaluation, gradient boosting classifiers achieved the best performance, with accuracies of 0.71--0.78 AUC (area under the ROC curve) across three tasks concerning the "what", "who", and "when" of turn-taking behaviors. In interpreting these models, we found that group size, listener personality, speech-related behavior (e.g., time elapsed since the listener's last speech event), group gaze (e.g., how much the group looks at the speaker), as well as the listener's and previous speaker's head pitch, head y-axis position, and left hand y-axis position more saliently influenced predictions. Results suggested that these features remain reliable indicators in novel social VR settings, as prediction performance is robust over time and with groups and activities not used in the training dataset. We discuss theoretical and practical implications of the work.

</details>


### [25] [What Should We Engineer in Prompts? Training Humans in Requirement-Driven LLM Use](https://arxiv.org/abs/2409.08775)

*Qianou Ma, Weirui Peng, Chenyang Yang, Hua Shen, Kenneth Koedinger, Tongshuang Wu*

**Main category:** cs.HC

**Keywords:** prompt engineering, requirement articulation, LLM applications

**Relevance Score:** 9

**TL;DR:** The paper introduces Requirement-Oriented Prompt Engineering (ROPE), designed to improve requirement articulation for better LLM performance in complex tasks, showing significant gains over conventional techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing prompt engineering techniques often neglect the importance of clearly articulating customized requirements, which is crucial for effective LLM application.

**Method:** ROPE is implemented through a training suite that allows users to practice generating requirements with feedback from LLMs. It was tested in a controlled experiment involving 30 novices.

**Key Contributions:** Introduction of the ROPE paradigm for prompt engineering., Development of a training suite for practicing requirement articulation., Empirical evidence showing significant performance improvements over traditional methods.

**Result:** Participants trained with ROPE achieved a 20% improvement in task performance compared to a 1% improvement with conventional methods, demonstrating the effectiveness of focusing on requirement quality.

**Limitations:** 

**Future Work:** Exploration of other potential areas where ROPE can be applied and how it can further improve interaction with LLMs.

**Conclusion:** ROPE empowers end-users to create better LLM applications by enhancing their capability to articulate requirements clearly, indicating a strong link between input quality and output success.

**Abstract:** Prompting LLMs for complex tasks (e.g., building a trip advisor chatbot) needs humans to clearly articulate customized requirements (e.g., "start the response with a tl;dr"). However, existing prompt engineering instructions often lack focused training on requirement articulation and instead tend to emphasize increasingly automatable strategies (e.g., tricks like adding role-plays and "think step-by-step"). To address the gap, we introduce Requirement-Oriented Prompt Engineering (ROPE), a paradigm that focuses human attention on generating clear, complete requirements during prompting. We implement ROPE through an assessment and training suite that provides deliberate practice with LLM-generated feedback. In a randomized controlled experiment with 30 novices, ROPE significantly outperforms conventional prompt engineering training (20% vs. 1% gains), a gap that automatic prompt optimization cannot close. Furthermore, we demonstrate a direct correlation between the quality of input requirements and LLM outputs. Our work paves the way to empower more end-users to build complex LLM applications.

</details>


### [26] [Interruption Handling for Conversational Robots](https://arxiv.org/abs/2501.01568)

*Shiye Cao, Jiwon Moon, Amama Mahmood, Victor Nikhil Antony, Ziang Xiao, Anqi Liu, Chien-Ming Huang*

**Main category:** cs.HC

**Keywords:** human-robot interaction, interruptions, LLM-powered systems, real-time management, conversational robots

**Relevance Score:** 8

**TL;DR:** The paper presents a system for real-time detection and management of user-initiated interruptions in robotic conversations, improving the robot's conversational efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the management of user-initiated interruptions in robotic systems, which are often not handled effectively despite advancements in technology.

**Method:** The system detects user-initiated interruptions based on the interrupter's intent and manages these interruptions in real-time, utilizing interaction patterns from human-human interactions.

**Key Contributions:** Introduced a real-time interruption detection and management system for robotic conversations., Integrated the system into an LLM-powered social robot and validated its performance in two tasks., Provided insights for designing better interruption-handling behaviors in conversational robots.

**Result:** The system handled 93.69% of user-initiated interruptions effectively during user tests, demonstrating its potential for enhancing robotic conversations.

**Limitations:** The study's findings may be limited by the specific scenarios tested and the sample size of 21 participants.

**Future Work:** Explore the application of the developed system in broader contexts and its adaptation to other interaction patterns beyond the tested scenarios.

**Conclusion:** Designing conversational robots that manage interruptions effectively can lead to more dynamic and effective human-robot interactions.

**Abstract:** Interruptions, a fundamental component of human communication, can enhance the dynamism and effectiveness of conversations, but only when effectively managed by all parties involved. Despite advancements in robotic systems, state-of-the-art systems still have limited capabilities in handling user-initiated interruptions in real-time. Prior research has primarily focused on post hoc analysis of interruptions. To address this gap, we present a system that detects user-initiated interruptions and manages them in real-time based on the interrupter's intent (i.e., cooperative agreement, cooperative assistance, cooperative clarification, or disruptive interruption). The system was designed based on interaction patterns identified from human-human interaction data. We integrated our system into an LLM-powered social robot and validated its effectiveness through a timed decision-making task and a contentious discussion task with 21 participants. Our system successfully handled 93.69% (n=104/111) of user-initiated interruptions. We discuss our learnings and their implications for designing interruption-handling behaviors in conversational robots.

</details>


### [27] [Media Bias Detector: Designing and Implementing a Tool for Real-Time Selection and Framing Bias Analysis in News Coverage](https://arxiv.org/abs/2502.06009)

*Jenny S Wang, Samar Haider, Amir Tohidi, Anushkaa Gupta, Yuxuan Zhang, Chris Callison-Burch, David Rothschild, Duncan J Watts*

**Main category:** cs.HC

**Keywords:** Media Bias, AI Tools, Large Language Models, Journalism, User Engagement

**Relevance Score:** 7

**TL;DR:** The Media Bias Detector leverages large language models to provide insights into the editorial choices and bias in news articles, helping users critically engage with media content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To expose media bias caused by editorial choices in mainstream media, which can mislead readers without using explicit falsehoods.

**Method:** The tool integrates large language models to analyze the topics, tone, political lean, and facts of news articles aggregated at the publisher level. Usability and functionality were assessed through expert interviews and a survey of news consumers.

**Key Contributions:** Introduction of the Media Bias Detector tool, Integration of large language models for media analysis, Insights from expert interviews and consumer surveys on usability and impact

**Result:** The tool provides near real-time, granular insights into media bias, demonstrating its applicability and potential impact on user engagement with news content.

**Limitations:** The study may have limitations in the generalizability of the findings from expert interviews and surveys.

**Future Work:** Further research on enhancing the tool's capabilities and expanding its impact across diverse user groups.

**Conclusion:** AI-driven tools like the Media Bias Detector can empower users to critically engage with media content, especially in politically charged contexts.

**Abstract:** Mainstream media, through their decisions on what to cover and how to frame the stories they cover, can mislead readers without using outright falsehoods. Therefore, it is crucial to have tools that expose these editorial choices underlying media bias. In this paper, we introduce the Media Bias Detector, a tool for researchers, journalists, and news consumers. By integrating large language models, we provide near real-time granular insights into the topics, tone, political lean, and facts of news articles aggregated to the publisher level. We assessed the tool's impact by interviewing 13 experts from journalism, communications, and political science, revealing key insights into usability and functionality, practical applications, and AI's role in powering media bias tools. We explored this in more depth with a follow-up survey of 150 news consumers. This work highlights opportunities for AI-driven tools that empower users to critically engage with media content, particularly in politically charged environments.

</details>


### [28] [Grid Labeling: Crowdsourcing Task-Specific Importance from Visualizations](https://arxiv.org/abs/2502.13902)

*Minsuk Chang, Yao Wang, Huichen Will Wang, Andreas Bulling, Cindy Xiong Bearfield*

**Main category:** cs.HC

**Keywords:** visualization, task-specific importance, saliency models, Grid Labeling, human subject study

**Relevance Score:** 7

**TL;DR:** This paper introduces Grid Labeling, a method for collecting task-specific importance data for visualizations to improve saliency prediction models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches to visual attention analysis are mostly based on free-viewing saliency models, which do not account for task-dependent factors. The authors aim to enhance saliency prediction models by addressing the challenges in collecting task-relevant importance data.

**Method:** Grid Labeling segments visualizations into Adaptive Grids for efficient annotation, allowing users to provide task-specific importance data with lower cognitive and physical effort compared to other methods.

**Key Contributions:** Introduction of Grid Labeling for efficient data collection, Demonstration of Adaptive Grids for annotation, Comparison showing superior performance of Grid Labeling over existing methods

**Result:** The human subject study demonstrated that Grid Labeling yields the least noisy data and highest inter-participant agreement, with fewer participants and reduced cognitive effort needed.

**Limitations:** The study may be limited by the specific tasks tested and may not generalize to all types of visualizations or user groups.

**Future Work:** Further research could explore additional tasks and different types of visualizations to broaden the application of Grid Labeling.

**Conclusion:** Grid Labeling presents a novel solution for collecting more relevant and efficient importance data that can enhance saliency models in task-dependent scenarios.

**Abstract:** Knowing where people look in visualizations is key to effective design. Yet, existing research primarily focuses on free-viewing-based saliency models - although visual attention is inherently task-dependent. Collecting task-relevant importance data remains a resource-intensive challenge. To address this, we introduce Grid Labeling - a novel annotation method for collecting task-specific importance data to enhance saliency prediction models. Grid Labeling dynamically segments visualizations into Adaptive Grids, enabling efficient, low-effort annotation while adapting to visualization structure. We conducted a human subject study comparing Grid Labeling with existing annotation methods, ImportAnnots, and BubbleView across multiple metrics. Results show that Grid Labeling produces the least noisy data and the highest inter-participant agreement with fewer participants while requiring less physical (e.g., clicks/mouse movements) and cognitive effort. An interactive demo is available at https://jangsus1.github.io/Grid-Labeling.

</details>


### [29] [Desirable Unfamiliarity: Insights from Eye Movements on Engagement and Readability of Dictation Interfaces](https://arxiv.org/abs/2503.08539)

*Zhaohui Liang, Yonglin Chen, Naser Al Madi, Can Liu*

**Main category:** cs.HC

**Keywords:** Dictation Interfaces, Eye-Tracking, Human-Computer Interaction, User Experience, Text Input

**Relevance Score:** 8

**TL;DR:** The study evaluates five dictation interfaces using eye-tracking to understand how users read and review dictated text, revealing preferences for summaries and highlighting methods that improve readability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate user behavior regarding reading and reviewing text from dictation interfaces, and to enhance the usability of such systems.

**Method:** Conducted a controlled eye-tracking experiment with 20 participants comparing five dictation interfaces: PLAIN, AOC, RAKE, GP-TSM, and SUMMARY.

**Key Contributions:** Comparison of five dictation interfaces using eye-tracking, Insights into user reading behavior during dictation and reviewing, Recommendations for improving dictation interface design with summary features

**Result:** Participants spent only 7--11% of their time actively reading during composition and preferred SUMMARY for its readability, despite its use of unfamiliar words.

**Limitations:** Limited sample size of 20 participants may affect generalizability of results.

**Future Work:** Further exploration of different dictation methods and user adaptations to varying interface feedback types.

**Conclusion:** Dictation interfaces should incorporate summarization and key information display to improve user experience and recall.

**Abstract:** Dictation interfaces support efficient text input, but the transcribed text can be hard to read. To understand how users read and review dictated text, we conducted a controlled eye-tracking experiment with 20 participants to compare five dictation interfaces: PLAIN (real-time transcription), AOC (periodic corrections), RAKE (keyword highlights), GP-TSM (grammar-preserving highlights), and SUMMARY (LLM-generated abstraction summary). The study analyzed participants' gaze patterns during their speech composition and reviewing processes. The findings show that during composition, participants spent only 7--11% of their time actively reading, and they favored real-time feedback and avoided distracting interface changes. During reviewing, although SUMMARY introduced unfamiliar words (requiring longer and more frequent fixation), they were easier to read (requiring fewer regressions). Participants preferred SUMMARY for the polished text that preserved fidelity to original meanings. RAKE guided the reading of self-produced text better than GP-TSM. RAKE guides the reading of self-produced text better than GP-TSM. These surprising findings suggest that dictation interfaces could consider showing summaries or key information to support recall instead of raw transcripts.

</details>


### [30] [A Case Study of Scalable Content Annotation Using Multi-LLM Consensus and Human Review](https://arxiv.org/abs/2503.17620)

*Mingyue Yuan, Jieshan Chen, Zhenchang Xing, Gelareh Mohammadi, Aaron Quigley*

**Main category:** cs.HC

**Keywords:** annotation, LLM, human review, automation, content analysis

**Relevance Score:** 8

**TL;DR:** The paper presents MCHR, a semi-automated framework enhancing content annotation efficiency by integrating multiple LLMs with human review to reduce annotation time while maintaining high accuracy.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Content annotation at scale is challenging due to the need for substantial human expertise and effort.

**Method:** MCHR employs a structured consensus-building mechanism among multiple LLMs, coupled with an adaptive review protocol that involves targeted human review.

**Key Contributions:** Introduction of a novel semi-automated framework (MCHR) for content annotation, Development of a structured consensus-building mechanism among LLMs, Implementation of an adaptive review protocol to strategically engage human expertise

**Result:** MCHR demonstrates a reduction in annotation time by 32% to 100% compared to manual methods, maintaining accuracy between 85.5% and 98% across various tasks.

**Limitations:** 

**Future Work:** 

**Conclusion:** The systematic integration of LLMs and human review via MCHR significantly improves annotation efficiency while preserving accuracy.

**Abstract:** Content annotation at scale remains challenging, requiring substantial human expertise and effort. This paper presents a case study in code documentation analysis, where we explore the balance between automation efficiency and annotation accuracy. We present MCHR (Multi-LLM Consensus with Human Review), a novel semi-automated framework that enhances annotation scalability through the systematic integration of multiple LLMs and targeted human review. Our framework introduces a structured consensus-building mechanism among LLMs and an adaptive review protocol that strategically engages human expertise. Through our case study, we demonstrate that MCHR reduces annotation time by 32% to 100% compared to manual annotation while maintaining high accuracy (85.5% to 98%) across different difficulty levels, from basic binary classification to challenging open-set scenarios.

</details>


### [31] [Learning from Elders: Making an LLM-powered Chatbot for Retirement Communities more Accessible through User-centered Design](https://arxiv.org/abs/2504.08985)

*Luna Xingyu Li, Ray-yuan Chung, Feng Chen, Wenyu Zeng, Yein Jeon, Oleg Zaslavsky*

**Main category:** cs.HC

**Keywords:** LLM, chatbot, eHealth literacy, older adults, human-centered design

**Relevance Score:** 8

**TL;DR:** The paper presents an LLM-powered chatbot prototype aimed at improving digital engagement and eHealth literacy among older adults in retirement communities through a human-centered design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address low technology and eHealth literacy among older adults to enhance their engagement with digital tools.

**Method:** Designed and refined a chatbot using interviews, persona development, and tailored prompt engineering, followed by a pilot trial for feedback.

**Key Contributions:** Human-centered design approach for chatbot development., Integration of accessibility features like adjustable font size and personalized responses., Demonstrated effectiveness through pilot testing with real users.

**Result:** The pilot trial showed high satisfaction and ease of use among residents, indicating the chatbot's potential to empower users while identifying areas for improvement.

**Limitations:** Further improvements needed based on pilot feedback; future functionalities are yet to be implemented.

**Future Work:** Implement voice-to-text functions and conduct longitudinal intervention studies.

**Conclusion:** LLM-driven chatbots can enhance accessibility and personalization for older adults, helping bridge literacy gaps in retirement communities.

**Abstract:** Low technology and eHealth literacy among older adults in retirement communities hinder engagement with digital tools. To address this, we designed an LLM-powered chatbot prototype using a human-centered approach for a local retirement community. Through interviews and persona development, we prioritized accessibility and dual functionality: simplifying internal information retrieval and improving technology and eHealth literacy. A pilot trial with residents demonstrated high satisfaction and ease of use, but also identified areas for further improvement. Based on the feedback, we refined the chatbot using GPT-3.5 Turbo and Streamlit. The chatbot employs tailored prompt engineering to deliver concise responses. Accessible features like adjustable font size, interface theme and personalized follow-up responses were implemented. Future steps include enabling voice-to-text function and longitudinal intervention studies. Together, our results highlight the potential of LLM-driven chatbots to empower older adults through accessible, personalized interactions, bridging literacy gaps in retirement communities.

</details>


### [32] [A Multi-Layered Research Framework for Human-Centered AI: Defining the Path to Explainability and Trust](https://arxiv.org/abs/2504.13926)

*Chameera De Silva, Thilina Halloluwa, Dhaval Vyas*

**Main category:** cs.HC

**Keywords:** Human-Centered AI, Explainable AI, Healthcare, AI Transparency, Decision-Making

**Relevance Score:** 9

**TL;DR:** This paper introduces a three-layered framework merging Human-Centered AI and Explainable AI to improve transparency and trust in high-stakes decision-making contexts such as healthcare and finance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI in critical fields is limited by transparency and trust issues, necessitating a unified approach to enhance decision-making effectiveness.

**Method:** A three-layered framework is proposed, including an AI model with built-in explainability, a user-centric explanation layer, and a feedback loop for real-time adaptation.

**Key Contributions:** Novel three-layered framework for explainability, Integration of human-centered design with AI, Evaluation across multiple high-stakes domains

**Result:** The framework was evaluated in healthcare, finance, and software development, showing improved decision-making, regulatory compliance, and enhanced public trust.

**Limitations:** Current version requires significant revisions and restructuring to reflect intended contributions more accurately.

**Future Work:** Plans for revision and potential resubmission to improve the work.

**Conclusion:** The findings contribute to the field of Human-Centered Explainable AI, illustrating the potential for creating transparent, adaptable, and ethically aligned AI systems.

**Abstract:** The integration of Artificial Intelligence (AI) into high-stakes domains such as healthcare, finance, and autonomous systems is often constrained by concerns over transparency, interpretability, and trust. While Human-Centered AI (HCAI) emphasizes alignment with human values, Explainable AI (XAI) enhances transparency by making AI decisions more understandable. However, the lack of a unified approach limits AI's effectiveness in critical decision-making scenarios. This paper presents a novel three-layered framework that bridges HCAI and XAI to establish a structured explainability paradigm. The framework comprises (1) a foundational AI model with built-in explainability mechanisms, (2) a human-centered explanation layer that tailors explanations based on cognitive load and user expertise, and (3) a dynamic feedback loop that refines explanations through real-time user interaction. The framework is evaluated across healthcare, finance, and software development, demonstrating its potential to enhance decision-making, regulatory compliance, and public trust. Our findings advance Human-Centered Explainable AI (HCXAI), fostering AI systems that are transparent, adaptable, and ethically aligned.

</details>


### [33] [Promoting Real-Time Reflection in Synchronous Communication with Generative AI](https://arxiv.org/abs/2504.15647)

*Yi Wen, Meng Xia*

**Main category:** cs.HC

**Keywords:** real-time reflection, generative AI, synchronous communication, human-AI interaction, design implications

**Relevance Score:** 7

**TL;DR:** This paper discusses the importance of real-time reflection in communication, exploring how generative AI can enhance this process while addressing challenges in interaction design and information presentation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve effectiveness in synchronous communication through real-time reflection.

**Method:** Review existing research on systems for real-time reflection in synchronous communication and discuss design implications.

**Key Contributions:** Review of existing systems for reflection in synchronous communication, Discussion of design implications for human-AI interaction, Insights into integrating generative AI for better reflection support

**Result:** Identified key challenges and potential design solutions for integrating generative AI to support real-time reflection in communication.

**Limitations:** 

**Future Work:** Further exploration of interaction design strategies for real-time AI-supported reflection.

**Conclusion:** Effective human-AI interaction design can enhance real-time reflection, leading to improved communication outcomes.

**Abstract:** Real-time reflection plays a vital role in synchronous communication. It enables users to adjust their communication strategies dynamically, thereby improving the effectiveness of their communication. Generative AI holds significant potential to enhance real-time reflection due to its ability to comprehensively understand the current context and generate personalized and nuanced content. However, it is challenging to design the way of interaction and information presentation to support the real-time workflow rather than disrupt it. In this position paper, we present a review of existing research on systems designed for reflection in different synchronous communication scenarios. Based on that, we discuss design implications on how to design human-AI interaction to support reflection in real time.

</details>


### [34] [Clinical knowledge in LLMs does not translate to human interactions](https://arxiv.org/abs/2504.18919)

*Andrew M. Bean, Rebecca Payne, Guy Parsons, Hannah Rose Kirk, Juan Ciro, Rafael Mosquera, Sara Hincapié Monsalve, Aruna S. Ekanayaka, Lionel Tarassenko, Luc Rocher, Adam Mahdi*

**Main category:** cs.HC

**Keywords:** large language models, medical advice, user interaction, human-computer interaction, healthcare applications

**Relevance Score:** 9

**TL;DR:** This study evaluates the effectiveness of large language models (LLMs) in assisting the public with medical advice, revealing significant gaps between LLM performance and user interaction outcomes.

**Read time:** 52 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of large language models in providing accurate medical advice to the public, considering the gap between exam performance and real-world application.

**Method:** A controlled study with 1,298 participants tested LLMs against a control group in identifying medical conditions and recommended actions across ten scenarios.

**Key Contributions:** Demonstrated that LLMs perform well in controlled environments but not in user interactions., Identified significant accuracy gaps in user interaction with LLMs compared to LLMs operating independently., Recommended a shift towards human user testing to better evaluate LLMs' interactive capabilities.

**Result:** LLMs accurately identified conditions in 94.9% of cases and disposition in 56.3% when tested alone; however, user interaction resulted in less than 34.5% accuracy for conditions and 44.2% for dispositions.

**Limitations:** The findings suggest limitations in how LLMs interact with users, which were not predicted by standard benchmarks for medical knowledge.

**Future Work:** Future research should focus on systematic human user testing and improving LLM interfaces for better user interactions in healthcare settings.

**Conclusion:** The study highlights the challenge of user interactions in deploying LLMs for medical advice and stresses the need for systematic human user testing before public deployment.

**Abstract:** Global healthcare providers are exploring use of large language models (LLMs) to provide medical advice to the public. LLMs now achieve nearly perfect scores on medical licensing exams, but this does not necessarily translate to accurate performance in real-world settings. We tested if LLMs can assist members of the public in identifying underlying conditions and choosing a course of action (disposition) in ten medical scenarios in a controlled study with 1,298 participants. Participants were randomly assigned to receive assistance from an LLM (GPT-4o, Llama 3, Command R+) or a source of their choice (control). Tested alone, LLMs complete the scenarios accurately, correctly identifying conditions in 94.9% of cases and disposition in 56.3% on average. However, participants using the same LLMs identified relevant conditions in less than 34.5% of cases and disposition in less than 44.2%, both no better than the control group. We identify user interactions as a challenge to the deployment of LLMs for medical advice. Standard benchmarks for medical knowledge and simulated patient interactions do not predict the failures we find with human participants. Moving forward, we recommend systematic human user testing to evaluate interactive capabilities prior to public deployments in healthcare.

</details>


### [35] [LINC: Supporting Language Independent Communication and Comprehension to Enhance Contribution in Multilingual Collaborative Meetings](https://arxiv.org/abs/2504.18988)

*Saramsh Gautam, Mahmood Jasim*

**Main category:** cs.HC

**Keywords:** multilingual collaboration, LINC, ESL researchers

**Relevance Score:** 8

**TL;DR:** The paper presents LINC, a multimodal system designed to facilitate collaboration among ESL researchers by improving communication and comprehension in multilingual meetings.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address communication barriers faced by ESL researchers in collaborative environments due to language differences, which limit their participation and contributions.

**Method:** A survey was conducted with 64 ESL researchers to identify design goals, leading to the development of the LINC system, which includes a real-time multilingual communication module and a post-meeting dashboard. The system's effectiveness was evaluated with a study involving six multilingual teams.

**Key Contributions:** Identification of key design goals for multilingual collaboration, Development and evaluation of the LINC system, Insights into external factors affecting participation in multilingual meetings

**Result:** Participants using LINC were able to communicate in their preferred languages, recall actionable insights from discussions, and prepare more effectively for meetings, indicating improved participation and comprehension.

**Limitations:** The study may be limited by the small sample size and specific context of ESL researchers, which may not generalize to all collaborative settings.

**Future Work:** Investigating additional multimodal tools and methods to further improve participation and outcomes in multilingual collaborative environments.

**Conclusion:** The use of multimodal systems like LINC can significantly enhance the collaboration experience for multilingual teams in research contexts, addressing language barriers and promoting participation.

**Abstract:** Collaborative research often includes contributors with varied perspectives from diverse linguistic backgrounds. However, English as a Second Language (ESL) researchers often struggle to communicate during meetings in English and comprehend discussions, leading to limited contribution. To investigate these challenges, we surveyed 64 ESL researchers who frequently collaborate in multilingual teams and identified four key design goals around participation, comprehension, documentation, and feedback. Guided by these design goals, we developed LINC, a multimodal Language INdependent Collaboration system with two components: a real-time module for multilingual communication during meetings and a post-meeting dashboard for discussion analysis. We evaluated the system through a two-phased study with six triads of multilingual teams. We found that using LINC, participants benefited from communicating in their preferred language, recalled and reviewed actionable insights, and prepared for upcoming meetings effectively. We discuss external factors that impact multilingual meeting participation beyond language preferences and the implications of multimodal systems in facilitating meetings in hybrid multilingual collaborative settings beyond research.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [36] [Mind the Language Gap: Automated and Augmented Evaluation of Bias in LLMs for High- and Low-Resource Languages](https://arxiv.org/abs/2504.18560)

*Alessio Buscemi, Cédric Lothritz, Sergio Morales, Marcos Gomez-Vazquez, Robert Clarisó, Jordi Cabot, German Castignani*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias Testing, Multilingual, Natural Language Processing, Social Biases

**Relevance Score:** 8

**TL;DR:** Introducing MLA-BiTe, a framework for multilingual bias testing in LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate and mitigate social biases in large language models due to their training data.

**Method:** MLA-BiTe uses automated translation and paraphrasing to conduct comprehensive multilingual bias assessments.

**Key Contributions:** Development of the MLA-BiTe framework for multilingual bias testing, Application of automated translation and paraphrasing for bias assessment, Evaluation of LLMs across multiple languages, including low-resource options.

**Result:** The effectiveness of MLA-BiTe was demonstrated by testing four state-of-the-art LLMs in six languages, focusing on multiple sensitive discrimination categories.

**Limitations:** The performance of translations and the representation of certain languages may affect bias assessment accuracy.

**Future Work:** Further exploration of bias assessment across additional languages and improving translation techniques for better accuracy.

**Conclusion:** MLA-BiTe provides a significant advancement in bias evaluation methods by allowing for a broader exploration of biases across languages.

**Abstract:** Large Language Models (LLMs) have exhibited impressive natural language processing capabilities but often perpetuate social biases inherent in their training data. To address this, we introduce MultiLingual Augmented Bias Testing (MLA-BiTe), a framework that improves prior bias evaluation methods by enabling systematic multilingual bias testing. MLA-BiTe leverages automated translation and paraphrasing techniques to support comprehensive assessments across diverse linguistic settings. In this study, we evaluate the effectiveness of MLA-BiTe by testing four state-of-the-art LLMs in six languages -- including two low-resource languages -- focusing on seven sensitive categories of discrimination.

</details>


### [37] [Span-Level Hallucination Detection for LLM-Generated Answers](https://arxiv.org/abs/2504.18639)

*Passant Elchafei, Mervet Abu-Elkheir*

**Main category:** cs.CL

**Keywords:** hallucination detection, Semantic Role Labeling, LLM-generated texts

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for detecting hallucinations in LLM-generated answers by integrating Semantic Role Labeling with a DeBERTa-based textual entailment model, demonstrating competitive detection performance in experiments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Improving factual consistency in LLM-generated answers necessitates effective methods for detecting hallucinations.

**Method:** A span-level hallucination detection framework that utilizes Semantic Role Labeling to decompose answers and a DeBERTa-based textual entailment model to evaluate semantic alignment against a reference context.

**Key Contributions:** Development of a span-level hallucination detection framework for LLM-generated texts., Integration of Semantic Role Labeling to enhance contextual evaluation., Validation of hallucination detection through advanced fact-checking methods.

**Result:** Experiments on the Mu-SHROOM dataset show competitive performance in detecting hallucinated spans, verified through fact-checking with GPT-4 and LLaMA.

**Limitations:** The framework currently focuses only on English and Arabic texts and may require adaptation for other languages.

**Future Work:** Exploring the application of the framework to additional languages and contexts in LLM-generated content.

**Conclusion:** The proposed framework contributes significantly to the task of hallucination detection in LLM-generated responses.

**Abstract:** Detecting spans of hallucination in LLM-generated answers is crucial for improving factual consistency. This paper presents a span-level hallucination detection framework for the SemEval-2025 Shared Task, focusing on English and Arabic texts. Our approach integrates Semantic Role Labeling (SRL) to decompose the answer into atomic roles, which are then compared with a retrieved reference context obtained via question-based LLM prompting. Using a DeBERTa-based textual entailment model, we evaluate each role semantic alignment with the retrieved context. The entailment scores are further refined through token-level confidence measures derived from output logits, and the combined scores are used to detect hallucinated spans. Experiments on the Mu-SHROOM dataset demonstrate competitive performance. Additionally, hallucinated spans have been verified through fact-checking by prompting GPT-4 and LLaMA. Our findings contribute to improving hallucination detection in LLM-generated responses.

</details>


### [38] [Can Third-parties Read Our Emotions?](https://arxiv.org/abs/2504.18673)

*Jiayi Li, Yingfan Zhou, Pranav Narayanan Venkit, Halima Binte Islam, Sneha Arya, Shomir Wilson, Sarah Rajtmajer*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, emotion recognition, third-party annotations, large language models, annotation practices

**Relevance Score:** 8

**TL;DR:** The study evaluates the effectiveness of third-party annotations in emotion recognition tasks, finding that LLMs generally outperform human annotators, with improvements possible through demographic similarity and refined annotation practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reliability of third-party annotations in capturing authors' private emotional states in Natural Language Processing tasks.

**Method:** Human subjects experiments were conducted to compare third-party annotations with first-party emotion labels in emotion recognition tasks.

**Key Contributions:** Demonstrated limitations of third-party annotations in capturing authors' emotions, Showed that LLMs outperform human annotators in emotion recognition tasks, Introduced a framework for evaluating annotation practices.

**Result:** Findings indicate significant limitations in third-party annotations, with LLMs performing better than human annotators. Improvements in performance were noted when incorporating demographic similarities.

**Limitations:** The study primarily focuses on emotion recognition tasks and may not generalize across other NLP applications.

**Future Work:** Recommendations for future research include refining annotation processes and further exploring the role of demographic factors in annotation quality.

**Conclusion:** The study calls for improved annotation practices and a framework for evaluating third-party annotations to better model authors' private states.

**Abstract:** Natural Language Processing tasks that aim to infer an author's private states, e.g., emotions and opinions, from their written text, typically rely on datasets annotated by third-party annotators. However, the assumption that third-party annotators can accurately capture authors' private states remains largely unexamined. In this study, we present human subjects experiments on emotion recognition tasks that directly compare third-party annotations with first-party (author-provided) emotion labels. Our findings reveal significant limitations in third-party annotations-whether provided by human annotators or large language models (LLMs)-in faithfully representing authors' private states. However, LLMs outperform human annotators nearly across the board. We further explore methods to improve third-party annotation quality. We find that demographic similarity between first-party authors and third-party human annotators enhances annotation performance. While incorporating first-party demographic information into prompts leads to a marginal but statistically significant improvement in LLMs' performance. We introduce a framework for evaluating the limitations of third-party annotations and call for refined annotation practices to accurately represent and model authors' private states.

</details>


### [39] [Spatial Speech Translation: Translating Across Space With Binaural Hearables](https://arxiv.org/abs/2504.18715)

*Tuochao Chen, Qirui Wang, Runlin He, Shyam Gollakota*

**Main category:** cs.CL

**Keywords:** Spatial Speech Translation, Hearables, Binaural Rendering, Real-time Translation, Speech Localization

**Relevance Score:** 9

**TL;DR:** The paper presents a novel concept of spatial speech translation for hearables that translates speech in the wearer’s environment while preserving spatial cues and speaker characteristics, achieving effective results in real-world conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable users in crowded multilingual environments to understand speech by translating it into their native language while maintaining the spatial characteristics of each speaker.

**Method:** The methodology involves addressing technical challenges in blind source separation, localization, real-time translation, and binaural rendering for accurate spatial audio output on Apple M2 silicon.

**Key Contributions:** Introduction of spatial speech translation for hearables, Real-time translation maintaining spatial cues and speaker characteristics, Proof-of-concept evaluations demonstrating effectiveness in real-world settings

**Result:** The system achieves a BLEU score of up to 22.01 in translating speech even in noisy environments, significantly outperforming existing models.

**Limitations:** The current prototype is a proof-of-concept, and further development is needed for commercial applications and broader language support.

**Future Work:** Explore enhancements for broader language pairs, integration with more sophisticated processing algorithms, and improvements in user experience.

**Conclusion:** This research marks a significant step towards the integration of spatial perception in speech translation, paving the way for more natural interactions between users and multilingual environments.

**Abstract:** Imagine being in a crowded space where people speak a different language and having hearables that transform the auditory space into your native language, while preserving the spatial cues for all speakers. We introduce spatial speech translation, a novel concept for hearables that translate speakers in the wearer's environment, while maintaining the direction and unique voice characteristics of each speaker in the binaural output. To achieve this, we tackle several technical challenges spanning blind source separation, localization, real-time expressive translation, and binaural rendering to preserve the speaker directions in the translated audio, while achieving real-time inference on the Apple M2 silicon. Our proof-of-concept evaluation with a prototype binaural headset shows that, unlike existing models, which fail in the presence of interference, we achieve a BLEU score of up to 22.01 when translating between languages, despite strong interference from other speakers in the environment. User studies further confirm the system's effectiveness in spatially rendering the translated speech in previously unseen real-world reverberant environments. Taking a step back, this work marks the first step towards integrating spatial perception into speech translation.

</details>


### [40] [Building UD Cairo for Old English in the Classroom](https://arxiv.org/abs/2504.18718)

*Lauren Levine, Junghyun Min, Amir Zeldes*

**Main category:** cs.CL

**Keywords:** Old English, treebank, linguistic annotation, LLM, historical linguistics

**Relevance Score:** 2

**TL;DR:** This paper presents a treebank for Old English based on classroom data collection using LLM prompting and authentic texts, highlighting challenges and suggesting improvements in annotation and parsing performance.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding and teaching of Old English through the creation of a treebank that utilizes both LLMs and authentic data.

**Method:** The authors collected and annotated a sample of 20 sentences in Old English, involving multiple beginner annotators and comparing their results, while also conducting parsing experiments.

**Key Contributions:** Creation of a treebank for Old English using classroom data., Utilization of LLM prompting alongside authentic data collections., Insights into the effectiveness of beginner annotators in producing reliable linguistic data.

**Result:** The study found LLM outputs in Old English to be inaccurate but correctable with post-editing, and showed that beginner annotations, while imperfect, can yield useful results when aggregated.

**Limitations:** The performance of LLMs in generating authentic Old English syntax remains inadequate without significant post-editing.

**Future Work:** Further research is needed to improve LLM performance on historical languages and to explore more systematic approaches to training beginner annotators.

**Conclusion:** While current LLMs struggle with Old English syntax, collaborative annotation and incorporating lessons from authentic data can enhance both learning and data quality.

**Abstract:** In this paper we present a sample treebank for Old English based on the UD Cairo sentences, collected and annotated as part of a classroom curriculum in Historical Linguistics. To collect the data, a sample of 20 sentences illustrating a range of syntactic constructions in the world's languages, we employ a combination of LLM prompting and searches in authentic Old English data. For annotation we assigned sentences to multiple students with limited prior exposure to UD, whose annotations we compare and adjudicate. Our results suggest that while current LLM outputs in Old English do not reflect authentic syntax, this can be mitigated by post-editing, and that although beginner annotators do not possess enough background to complete the task perfectly, taken together they can produce good results and learn from the experience. We also conduct preliminary parsing experiments using Modern English training data, and find that although performance on Old English is poor, parsing on annotated features (lemma, hyperlemma, gloss) leads to improved performance.

</details>


### [41] [EvidenceBench: A Benchmark for Extracting Evidence from Biomedical Papers](https://arxiv.org/abs/2504.18736)

*Jianyou Wang, Weili Cao, Kaicheng Wang, Xiaoyue Wang, Ashish Dalvi, Gino Prasad, Qishan Liang, Hsuan-lin Her, Ming Wang, Qin Yang, Gene W. Yeo, David E. Neal, Maxim Khan, Christopher D. Rosin, Ramamohan Paturi, Leon Bergen*

**Main category:** cs.CL

**Keywords:** biomedical, evidence retrieval, language models, machine learning, hypothesis generation

**Relevance Score:** 8

**TL;DR:** This paper introduces EvidenceBench, a benchmark for evaluating models that find evidence relevant to hypotheses in biomedical papers, demonstrating its validity with expert annotations and highlighting the performance gap between models and experts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Finding relevant evidence in biomedical literature is crucial for researchers testing scientific hypotheses.

**Method:** The authors developed a novel pipeline involving hypothesis generation and sentence-by-sentence annotation guided by expert judgment to create EvidenceBench and EvidenceBench-100k datasets.

**Key Contributions:** Introduction of EvidenceBench and EvidenceBench-100k for evaluating evidence retrieval models., Demonstration of significant performance gaps between current models and human experts., Creation of a scalable annotation pipeline based on expert judgment.

**Result:** Evaluation revealed that various language models and retrieval systems underperformed compared to expert annotators in finding relevant evidence.

**Limitations:** Current models still do not reach expert-level performance, indicating room for improvement.

**Future Work:** Future research should focus on enhancing models to improve their performance in evidence retrieval tasks.

**Conclusion:** The EvidenceBench benchmarks highlight the need for improved models in identifying relevant evidence, with a larger dataset available for advancing research in this area.

**Abstract:** We study the task of automatically finding evidence relevant to hypotheses in biomedical papers. Finding relevant evidence is an important step when researchers investigate scientific hypotheses. We introduce EvidenceBench to measure models performance on this task, which is created by a novel pipeline that consists of hypothesis generation and sentence-by-sentence annotation of biomedical papers for relevant evidence, completely guided by and faithfully following existing human experts judgment. We demonstrate the pipeline's validity and accuracy with multiple sets of human-expert annotations. We evaluated a diverse set of language models and retrieval systems on the benchmark and found that model performances still fall significantly short of the expert level on this task. To show the scalability of our proposed pipeline, we create a larger EvidenceBench-100k with 107,461 fully annotated papers with hypotheses to facilitate model training and development. Both datasets are available at https://github.com/EvidenceBench/EvidenceBench

</details>


### [42] [SynLexLM: Scaling Legal LLMs with Synthetic Data and Curriculum Learning](https://arxiv.org/abs/2504.18762)

*Ojasw Upadhyay, Abishek Saravankumar, Ayman Ismail*

**Main category:** cs.CL

**Keywords:** Large Language Models, legal AI, curriculum learning, synthetic data augmentation, legal benchmarks

**Relevance Score:** 7

**TL;DR:** Introduction of SynLexLM, a novel legal LLM training method utilizing curriculum learning and synthetic data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of fine-tuning LLMs for specialized domains, particularly law, where data scarcity and legal nuances are prevalent.

**Method:** The authors propose a curriculum learning approach that starts from simple legal texts and progressively advances to complex materials, supplemented with synthetic data generation functionalities.

**Key Contributions:** Introduction of SynLexLM for legal LLM pre-training, Utilization of curriculum learning for progressive training, Implementation of synthetic data augmentation for data scarcity mitigation.

**Result:** Preliminary results indicate that SynLexLM outperforms traditional models on specific legal benchmarks.

**Limitations:** The current work is preliminary and further validation on a broader set of legal tasks is required.

**Future Work:** Exploration of additional legal domains and larger-scale benchmarks for comprehensive evaluation.

**Conclusion:** This approach could enhance legal document analysis and research tools, democratizing access to advanced legal AI.

**Abstract:** Large Language Models (LLMs) are powerful but often require extensive fine-tuning and large datasets for specialized domains like law. General-purpose pre-training may not capture legal nuances, and acquiring sufficient legal data is challenging. We introduce SynLexLM, a novel approach to efficiently pre-train a legal LLM. Our method employs curriculum learning, progressing from simple to complex legal texts and queries, combined with synthetic data augmentation using models like Gemini Pro to address data scarcity. We aim to achieve improved performance on legal benchmarks (BigLaw-Bench, EUR-Lex-Sum) compared to traditional models and fine-tuned versions. Preliminary work involves generating synthetic QA pairs reflecting legal reasoning. This work aims to enhance legal document analysis and research tools, potentially democratizing access to advanced legal AI.

</details>


### [43] [Stealing Creator's Workflow: A Creator-Inspired Agentic Framework with Iterative Feedback Loop for Improved Scientific Short-form Generation](https://arxiv.org/abs/2504.18805)

*Jong Inn Park, Maanas Taneja, Qianwen Wang, Dongyeop Kang*

**Main category:** cs.CL

**Keywords:** Video Generation, Multi-LLM Framework, Scientific Dissemination

**Relevance Score:** 8

**TL;DR:** The paper presents SciTalk, a multi-LLM framework for generating accurate and engaging short-form videos from scientific papers, addressing challenges in content complexity and feedback loops in video creation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the generation of engaging and accurate short-form videos from complex scientific content, addressing issues such as factual inaccuracies and visual artifacts in existing methods.

**Method:** SciTalk employs a multi-LLM agentic framework that utilizes specialized agents for content summarization, visual scene planning, and editing, incorporating an iterative feedback system where video agents simulate user roles to refine video generation.

**Key Contributions:** Introduction of a novel multi-LLM framework for video generation from scientific papers., Implementation of an iterative feedback mechanism involving simulated user roles to improve video content., Demonstration of improved accuracy and engagement in video generation compared to existing methods.

**Result:** Experimental evaluations reveal that SciTalk significantly outperforms traditional prompting methods in creating scientifically accurate and engaging videos through its iterative refinement process.

**Limitations:** Preliminary results do not match the quality of videos created by human content creators.

**Future Work:** Future directions include improving the quality of generated videos further and exploring additional sources and agents to enhance the framework's performance.

**Conclusion:** While current results do not yet reach the quality of human creators, SciTalk offers insights into the process and challenges of feedback-driven video generation.

**Abstract:** Generating engaging, accurate short-form videos from scientific papers is challenging due to content complexity and the gap between expert authors and readers. Existing end-to-end methods often suffer from factual inaccuracies and visual artifacts, limiting their utility for scientific dissemination. To address these issues, we propose SciTalk, a novel multi-LLM agentic framework, grounding videos in various sources, such as text, figures, visual styles, and avatars. Inspired by content creators' workflows, SciTalk uses specialized agents for content summarization, visual scene planning, and text and layout editing, and incorporates an iterative feedback mechanism where video agents simulate user roles to give feedback on generated videos from previous iterations and refine generation prompts. Experimental evaluations show that SciTalk outperforms simple prompting methods in generating scientifically accurate and engaging content over the refined loop of video generation. Although preliminary results are still not yet matching human creators' quality, our framework provides valuable insights into the challenges and benefits of feedback-driven video generation. Our code, data, and generated videos will be publicly available.

</details>


### [44] [Toward Generalizable Evaluation in the LLM Era: A Survey Beyond Benchmarks](https://arxiv.org/abs/2504.18838)

*Yixin Cao, Shibo Hong, Xinze Li, Jiahao Ying, Yubo Ma, Haiyuan Liang, Yantao Liu, Zijun Yao, Xiaozhi Wang, Dan Huang, Wenxuan Zhang, Lifu Huang, Muhao Chen, Lei Hou, Qianru Sun, Xingjun Ma, Zuxuan Wu, Min-Yen Kan, David Lo, Qi Zhang, Heng Ji, Jing Jiang, Juanzi Li, Aixin Sun, Xuanjing Huang, Tat-Seng Chua, Yu-Gang Jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evaluation, Machine Learning, Automated Evaluation, Capability-based Evaluation

**Relevance Score:** 8

**TL;DR:** This survey addresses the evaluation challenges posed by the rapid evolution of Large Language Models (LLMs), focusing on the shift from task-specific to capability-based evaluation and from manual to automated evaluation, while highlighting the generalization issue in evaluation.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of LLMs presents significant challenges for evaluation methods, necessitating a survey to address these emerging issues.

**Method:** The survey analyzes transitions in evaluation approaches, focusing on capability-based benchmarks and automated evaluation methods, and discusses the generalization issue with respect to evaluation methods, datasets, evaluators, and metrics.

**Key Contributions:** Identifies key transitions in evaluation methodology for LLMs, Analyzes the generalization issue in model evaluation, Provides a living repository for ongoing updates and contributions.

**Result:** The survey identifies two major transitions in evaluation methodology and outlines the persistent evaluation generalization issue that limits the scalability of test sets.

**Limitations:** The persistent evaluation generalization issue hampers the effectiveness of current evaluation practices as LLM capabilities grow.

**Future Work:** Future work involves ongoing contributions to the repository and further exploration of scalable evaluation methods.

**Conclusion:** New evaluation methods must evolve alongside LLMs, addressing capability-based evaluations and automating the evaluation process, while the generalization issue remains a core challenge.

**Abstract:** Large Language Models (LLMs) are advancing at an amazing speed and have become indispensable across academia, industry, and daily applications. To keep pace with the status quo, this survey probes the core challenges that the rise of LLMs poses for evaluation. We identify and analyze two pivotal transitions: (i) from task-specific to capability-based evaluation, which reorganizes benchmarks around core competencies such as knowledge, reasoning, instruction following, multi-modal understanding, and safety; and (ii) from manual to automated evaluation, encompassing dynamic dataset curation and "LLM-as-a-judge" scoring.   Yet, even with these transitions, a crucial obstacle persists: the evaluation generalization issue. Bounded test sets cannot scale alongside models whose abilities grow seemingly without limit. We will dissect this issue, along with the core challenges of the above two transitions, from the perspectives of methods, datasets, evaluators, and metrics. Due to the fast evolving of this field, we will maintain a living GitHub repository (links are in each section) to crowd-source updates and corrections, and warmly invite contributors and collaborators.

</details>


### [45] [Towards Robust Dialogue Breakdown Detection: Addressing Disruptors in Large Language Models with Self-Guided Reasoning](https://arxiv.org/abs/2504.18839)

*Abdellah Ghassel, Xianzhi Li, Xiaodan Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conversational AI, Dialogue Breakdown Detection, Fine-tuning Strategies, Real-time Deployment

**Relevance Score:** 9

**TL;DR:** This paper addresses the detection and mitigation of dialogue breakdowns in LLM-driven conversational systems by proposing a specialized fine-tuning approach combined with advanced prompting strategies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of incoherent or contradictory responses from large language models (LLMs) that undermine user trust in conversational systems.

**Method:** Combining specialized fine-tuning with advanced prompting strategies including few-shot learning, chain-of-thought reasoning, and analogical prompting, while also implementing a real-time deployment architecture.

**Key Contributions:** Introduction of advanced prompting strategies for dialogue systems, Real-time deployment architecture for cost-effective operation, Significant improvement in model performance and breakdown detection

**Result:** Achieved a 7% accuracy improvement over the base model on the BETOLD dataset and showed significant operational cost reductions by selectively using more resource-intensive models only when breakdowns are detected.

**Limitations:** 

**Future Work:** Further exploration of generalization capabilities and application to other languages or dialogue scenarios.

**Conclusion:** The proposed approach offers a scalable solution for robust conversational AI, enhancing efficiency, interpretability, and reliability in high-impact domains.

**Abstract:** Large language models (LLMs) are rapidly changing various domains. However, their capabilities in handling conversational breakdowns still require an in-depth exploration. This paper addresses the challenge of detecting and mitigating dialogue breakdowns within LLM-driven conversational systems. While powerful models from OpenAI and Anthropic excel in many dialogue tasks, they can still produce incoherent or contradictory responses, commonly referred to as breakdowns, which undermine user trust. To tackle this, we propose an approach that combines specialized fine-tuning with advanced prompting strategies, including few-shot learning, chain-of-thought reasoning, and analogical prompting. In particular, we fine-tune a small 8B model and demonstrate its robust classification and calibration capabilities in English and Japanese dialogue. We also validate its generalization on the BETOLD dataset, achieving a 7\% accuracy improvement over its base model. Furthermore, we introduce a real-time deployment architecture that selectively escalates suspicious responses to more resource-intensive frontier models only when breakdowns are detected, significantly cutting operational expenses and energy consumption. Experimental results show our method surpasses prior state-of-the-art specialized classifiers while also narrowing performance gaps between smaller open-source models and large proprietary ones. Our approach offers a scalable solution for robust conversational AI in high-impact domains by combining efficiency, interpretability, and reliability.

</details>


### [46] [When2Call: When (not) to Call Tools](https://arxiv.org/abs/2504.18851)

*Hayley Ross, Ameya Sunil Mahabaleshwarkar, Yoshi Suhara*

**Main category:** cs.CL

**Keywords:** Language Models, tool calling, benchmark, HCI, preference optimization

**Relevance Score:** 8

**TL;DR:** This paper presents the When2Call benchmark for evaluating decision-making in tool-calling for Language Models, highlighting a significant area for improvement in existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to assess when Language Models should call external tools, go for follow-up questions, or admit inability to answer, beyond just tool accuracy.

**Method:** Development of the When2Call benchmark and a training set that includes a preference optimization training regime for enhanced learning.

**Key Contributions:** Introduction of the When2Call benchmark for evaluating tool-calling decisions., Development of a preference optimization training regime., Release of benchmark data and evaluation scripts.

**Result:** State-of-the-art tool-calling LMs show substantial room for improvement on the When2Call benchmark, proving its significance for future assessments.

**Limitations:** 

**Future Work:** Further research could focus on improving model decision-making processes and exploring additional tool-calling scenarios.

**Conclusion:** The creation of When2Call emphasizes the need for refined benchmarks and training methods in tool-calling decision-making for LMs.

**Abstract:** Leveraging external tools is a key feature for modern Language Models (LMs) to expand their capabilities and integrate them into existing systems. However, existing benchmarks primarily focus on the accuracy of tool calling -- whether the correct tool is called with the correct parameters -- and less on evaluating when LMs should (not) call tools. We develop a new benchmark, When2Call, which evaluates tool-calling decision-making: when to generate a tool call, when to ask follow-up questions and when to admit the question can't be answered with the tools provided. We find that state-of-the-art tool-calling LMs show significant room for improvement on When2Call, indicating the importance of this benchmark. We also develop a training set for When2Call and leverage the multiple-choice nature of the benchmark to develop a preference optimization training regime, which shows considerably more improvement than traditional fine-tuning. We release the benchmark and training data as well as evaluation scripts at https://github.com/NVIDIA/When2Call.

</details>


### [47] [Effective Length Extrapolation via Dimension-Wise Positional Embeddings Manipulation](https://arxiv.org/abs/2504.18857)

*Yi Lu, Wanxu Zhao, Xin Zhou, Chenxin An, Chenglong Wang, Shuo Li, Yuming Yang, Jun Zhao, Tao Ji, Tao Gui, Qi Zhang, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, context window, Dimension-Wise Positional Embeddings, extrapolation, training-free

**Relevance Score:** 9

**TL;DR:** This paper introduces Dimension-Wise Positional Embeddings Manipulation (DPE), a framework for extending the context window of Large Language Models (LLMs) without retraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of LLMs in processing long input contexts without incurring the high costs of retraining large-scale models.

**Method:** DPE manipulates the position indices of key dimensions selectively, optimizing each dimension's effective length for improved context extension.

**Key Contributions:** Introduces a novel framework, DPE, for context extension in LLMs without retraining., Enhances model performance on long-context benchmarks., Demonstrates superior capabilities of Llama3-8k 8B with DPE compared to other models.

**Result:** DPE allows Llama3-8k 8B to handle context windows of 128k tokens, outperforming baseline methods such as YaRN and Self-Extend, while also enhancing performance on long-context benchmarks by over 18 points.

**Limitations:** 

**Future Work:** Further exploration of DPE's applicability to other LLM architectures and potential optimizations.

**Conclusion:** DPE significantly improves LLMs' context handling capabilities without the need for continual training, achieving performance comparable to commercial models like GPT-4-128K.

**Abstract:** Large Language Models (LLMs) often struggle to process and generate coherent context when the number of input tokens exceeds the pre-trained length. Recent advancements in long-context extension have significantly expanded the context window of LLMs but require expensive overhead to train the large-scale models with longer context. In this work, we propose Dimension-Wise Positional Embeddings Manipulation (DPE), a training-free framework to extrapolate the context window of LLMs by diving into RoPE's different hidden dimensions. Instead of manipulating all dimensions equally, DPE detects the effective length for every dimension and finds the key dimensions for context extension. We reuse the original position indices with their embeddings from the pre-trained model and manipulate the key dimensions' position indices to their most effective lengths. In this way, DPE adjusts the pre-trained models with minimal modifications while ensuring that each dimension reaches its optimal state for extrapolation. DPE significantly surpasses well-known baselines such as YaRN and Self-Extend. DPE enables Llama3-8k 8B to support context windows of 128k tokens without continual training and integrates seamlessly with Flash Attention 2. In addition to its impressive extrapolation capability, DPE also dramatically improves the models' performance within training length, such as Llama3.1 70B, by over 18 points on popular long-context benchmarks RULER. When compared with commercial models, Llama 3.1 70B with DPE even achieves better performance than GPT-4-128K.

</details>


### [48] [Latent Adversarial Training Improves the Representation of Refusal](https://arxiv.org/abs/2504.18872)

*Alexandra Abbas, Nora Petrova, Helios Ael Lyons, Natalia Perez-Campanero*

**Main category:** cs.CL

**Keywords:** Latent Adversarial Training, Refusal Behavior, Language Models

**Relevance Score:** 8

**TL;DR:** This paper analyzes how Latent Adversarial Training (LAT) influences the representation of refusal behavior in language models, revealing that LAT creates a concentrated representation in latent space that enhances vulnerability to certain attacks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness and limitations of Latent Adversarial Training (LAT) in enhancing the robustness of language models against harmful instructions.

**Method:** The study examines the Llama 2 7B model by calculating activation differences between harmful and harmless instruction pairs and applies Singular Value Decomposition (SVD) to analyze the representation of refusal behavior under LAT compared to traditional methods.

**Key Contributions:** Demonstrates LAT's impact on refusal behavior representation in language models., Identifies vulnerabilities associated with improved refusal vectors generated by LAT., Highlights the trade-offs between robustness and vulnerability in model training approaches.

**Result:** LAT significantly alters the refusal representation, concentrating it in the first two SVD components, which account for about 75% of activation variance, leading to improved robustness against attacks from reference models but increased vulnerability to self-generated vectors.

**Limitations:** The study primarily focuses on the Llama 2 7B model and may not generalize to all language models.

**Future Work:** Further research is needed to explore LAT's effectiveness across different architectures and its interaction with various training methods.

**Conclusion:** LAT enhances the representation of refusal behavior but introduces new vulnerabilities, suggesting both potential for improving model safety and the need for caution in its application.

**Abstract:** Recent work has shown that language models' refusal behavior is primarily encoded in a single direction in their latent space, making it vulnerable to targeted attacks. Although Latent Adversarial Training (LAT) attempts to improve robustness by introducing noise during training, a key question remains: How does this noise-based training affect the underlying representation of refusal behavior? Understanding this encoding is crucial for evaluating LAT's effectiveness and limitations, just as the discovery of linear refusal directions revealed vulnerabilities in traditional supervised safety fine-tuning (SSFT).   Through the analysis of Llama 2 7B, we examine how LAT reorganizes the refusal behavior in the model's latent space compared to SSFT and embedding space adversarial training (AT). By computing activation differences between harmful and harmless instruction pairs and applying Singular Value Decomposition (SVD), we find that LAT significantly alters the refusal representation, concentrating it in the first two SVD components which explain approximately 75 percent of the activation differences variance - significantly higher than in reference models. This concentrated representation leads to more effective and transferable refusal vectors for ablation attacks: LAT models show improved robustness when attacked with vectors from reference models but become more vulnerable to self-generated vectors compared to SSFT and AT. Our findings suggest that LAT's training perturbations enable a more comprehensive representation of refusal behavior, highlighting both its potential strengths and vulnerabilities for improving model safety.

</details>


### [49] [A Simple Ensemble Strategy for LLM Inference: Towards More Stable Text Classification](https://arxiv.org/abs/2504.18884)

*Junichiro Niimi*

**Main category:** cs.CL

**Keywords:** large language models, sentiment analysis, ensemble strategy, reproducibility, majority voting

**Relevance Score:** 9

**TL;DR:** This study presents a new ensemble strategy for sentiment analysis using LLMs, showing improved accuracy over single attempts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the variability and reproducibility issues in the results of large language models (LLMs) for sentiment analysis, leveraging a majority voting method commonly used in human annotation.

**Method:** An ensemble strategy was implemented by integrating multiple inferences from medium-sized LLMs, compared against results from a single inference from a large LLM.

**Key Contributions:** Introduction of an ensemble strategy for LLMs in sentiment analysis, Demonstrated improvement in accuracy and robustness, Reduction in RMSE by 18.6% compared to single model inference

**Result:** The ensemble approach demonstrated an 18.6% reduction in RMSE, leading to more robust and accurate sentiment analysis results.

**Limitations:** 

**Future Work:** Further exploration of ensemble methods with different configurations and applications in various NLP tasks.

**Conclusion:** Employing an ensemble strategy significantly enhances the performance of LLMs in sentiment analysis, making it a viable alternative to using a single large model.

**Abstract:** With the advance of large language models (LLMs), LLMs have been utilized for the various tasks. However, the issues of variability and reproducibility of results from each trial of LLMs have been largely overlooked in existing literature while actual human annotation uses majority voting to resolve disagreements among annotators. Therefore, this study introduces the straightforward ensemble strategy to a sentiment analysis using LLMs. As the results, we demonstrate that the ensemble of multiple inference using medium-sized LLMs produces more robust and accurate results than using a large model with a single attempt with reducing RMSE by 18.6%.

</details>


### [50] [MTCSC: Retrieval-Augmented Iterative Refinement for Chinese Spelling Correction](https://arxiv.org/abs/2504.18938)

*Junhong Liang, Yu Zhou*

**Main category:** cs.CL

**Keywords:** Chinese Spelling Correction, Large Language Models, Domain Adaptation, Error Correction, Multi-Turn CSC

**Relevance Score:** 8

**TL;DR:** This paper introduces the MTCSC framework for variable-length Chinese Spelling Correction, enhancing error correction with domain adaptation and length consistency.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional Chinese Spelling Correction methods that require fixed input-output length parity and struggle with domain-specific corrections.

**Method:** The proposed MTCSC framework utilizes a retrieval-augmented generation mechanism enhanced with a length reflection mechanism for multi-turn correction.

**Key Contributions:** Introduction of MTCSC for variable-length correction scenarios, Use of a length reflection mechanism for output fidelity, Implementation of a multi-source combination strategy for enhanced domain adaptation

**Result:** The proposed method shows significant improvement in correction quality across diverse domain-specific datasets, particularly for variable-length errors.

**Limitations:** The performance may still depend on the quality of domain-specific training data and dictionaries utilized in the retrieval database.

**Future Work:** Further exploration of additional error types and refinement of the retrieval-enhanced correction methods.

**Conclusion:** MTCSC effectively extends the capabilities of traditional CSC approaches, allowing for flexible error correction suitable for various domains.

**Abstract:** Chinese Spelling Correction (CSC) aims to detect and correct erroneous tokens in sentences. While Large Language Models (LLMs) have shown remarkable success in identifying and rectifying potential errors, they often struggle with maintaining consistent output lengths and adapting to domain-specific corrections. Furthermore, existing CSC task impose rigid constraints requiring input and output lengths to be identical, limiting their applicability. In this work, we extend traditional CSC to variable-length correction scenarios, including Chinese Splitting Error Correction (CSEC) and ASR N-best Error Correction. To address domain adaptation and length consistency, we propose MTCSC (Multi-Turn CSC) framework based on RAG enhanced with a length reflection mechanism. Our approach constructs a retrieval database from domain-specific training data and dictionaries, fine-tuning retrievers to optimize performance for error-containing inputs. Additionally, we introduce a multi-source combination strategy with iterative length reflection to ensure output length fidelity. Experiments across diverse domain datasets demonstrate that our method significantly outperforms current approaches in correction quality, particularly in handling domain-specific and variable-length error correction tasks.

</details>


### [51] [LawFlow : Collecting and Simulating Lawyers' Thought Processes](https://arxiv.org/abs/2504.18942)

*Debarati Das, Khanh Chi Le, Ritik Sachin Parkar, Karin De Langis, Brendan Madson, Chad M. Berryman, Robin M. Willis, Daniel H. Moses, Brett McDonnell, Daniel Schwarcz, Dongyeop Kang*

**Main category:** cs.CL

**Keywords:** Legal workflows, AI in law, Human-computer interaction, LLM, LawFlow

**Relevance Score:** 5

**TL;DR:** The paper introduces LawFlow, a dataset of complete legal workflows to improve AI's support for legal practitioners, comparing human and LLM-generated workflows, and suggesting design improvements for AI tools in legal practice.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in AI support for complex, end-to-end legal decision-making by providing a dataset that reflects real-world legal workflows.

**Method:** Introduction of LawFlow dataset, comparison of human and LLM-generated workflows, analysis of structure and reasoning flexibility.

**Key Contributions:** Introduction of the LawFlow dataset, Comparison of human and LLM workflow characteristics, Design suggestions for improving AI in legal assistance

**Result:** Human workflows are modular and adaptive, while LLM workflows are sequential and less sensitive to context; findings suggest legal professionals prefer AI in supportive roles rather than full execution of complex tasks.

**Limitations:** The study primarily focuses on specific legal scenarios and may not generalize to all legal practices.

**Future Work:** Explore further improvements in LLM capabilities for more collaborative and reasoning-aware legal AI systems.

**Conclusion:** Current LLMs have limitations in supporting complex legal workflows; better design suggestions for collaborative AI systems in legal contexts are needed.

**Abstract:** Legal practitioners, particularly those early in their careers, face complex, high-stakes tasks that require adaptive, context-sensitive reasoning. While AI holds promise in supporting legal work, current datasets and models are narrowly focused on isolated subtasks and fail to capture the end-to-end decision-making required in real-world practice. To address this gap, we introduce LawFlow, a dataset of complete end-to-end legal workflows collected from trained law students, grounded in real-world business entity formation scenarios. Unlike prior datasets focused on input-output pairs or linear chains of thought, LawFlow captures dynamic, modular, and iterative reasoning processes that reflect the ambiguity, revision, and client-adaptive strategies of legal practice. Using LawFlow, we compare human and LLM-generated workflows, revealing systematic differences in structure, reasoning flexibility, and plan execution. Human workflows tend to be modular and adaptive, while LLM workflows are more sequential, exhaustive, and less sensitive to downstream implications. Our findings also suggest that legal professionals prefer AI to carry out supportive roles, such as brainstorming, identifying blind spots, and surfacing alternatives, rather than executing complex workflows end-to-end. Building on these findings, we propose a set of design suggestions, rooted in empirical observations, that align AI assistance with human goals of clarity, completeness, creativity, and efficiency, through hybrid planning, adaptive execution, and decision-point support. Our results highlight both the current limitations of LLMs in supporting complex legal workflows and opportunities for developing more collaborative, reasoning-aware legal AI systems. All data and code are available on our project page (https://minnesotanlp.github.io/LawFlow-website/).

</details>


### [52] [Dynamic Fisher-weighted Model Merging via Bayesian Optimization](https://arxiv.org/abs/2504.18992)

*Sanwoo Lee, Jiahao Liu, Qifan Wang, Jingang Wang, Xunliang Cai, Yunfang Wu*

**Main category:** cs.CL

**Keywords:** Dynamic Merging, Language Models, Bayesian Optimization, Multi-task Learning, Parameter Scaling

**Relevance Score:** 9

**TL;DR:** The paper introduces Dynamic Fisher-weighted Merging (DF-Merge), a novel framework for merging task-specific language models, improving performance significantly over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for efficient multi-task language models, but existing parameter merging methods lead to performance gaps compared to multi-task fine-tuning. This paper aims to improve model merging to address this issue.

**Method:** The authors propose DF-Merge, where candidate models are associated with coefficients that scale their parameters. Bayesian optimization adjusts these coefficients dynamically to enhance performance, while integrating parameter importance using Fisher information.

**Key Contributions:** Introduction of a unified framework that merges existing merging strategies., Development of DF-Merge, which utilizes Bayesian optimization for dynamic parameter scaling., Demonstration of enhanced performance in multi-task settings with minimal additional training data.

**Result:** DF-Merge demonstrates superior performance against strong baselines across different model sizes and tasks, achieving near-optimal results in few iterations even with limited validation data.

**Limitations:** 

**Future Work:** Further exploration of the framework's applicability across additional tasks and evaluation of its performance in real-world scenarios.

**Conclusion:** The unified merging framework of DF-Merge allows for efficient and effective multi-task model creation, revealing the potential for improvements in model merging strategies.

**Abstract:** The fine-tuning of pre-trained language models has resulted in the widespread availability of task-specific models. Model merging offers an efficient way to create multi-task models by combining these fine-tuned models at the parameter level, without the need for training data or joint training on multiple datasets. Existing merging approaches typically involve scaling the parameters model-wise or integrating parameter importance parameter-wise. Both approaches exhibit their own weaknesses, leading to a notable performance gap compared to multi-task fine-tuning. In this paper, we unify these seemingly distinct strategies into a more general merging framework, and introduce Dynamic Fisher-weighted Merging (DF-Merge). Specifically, candidate models are associated with a set of coefficients that linearly scale their fine-tuned parameters. Bayesian optimization is applied to dynamically adjust these coefficients, aiming to maximize overall performance on validation sets. Each iteration of this process integrates parameter importance based on the Fisher information conditioned by the coefficients. Experimental results show that DF-Merge outperforms strong baselines across models of different sizes and a variety of tasks. Our analysis shows that the effectiveness of DF-Merge arises from the unified view of merging and that near-optimal performance is achievable in a few iterations, even with minimal validation data.

</details>


### [53] [Graph of Attacks: Improved Black-Box and Interpretable Jailbreaks for LLMs](https://arxiv.org/abs/2504.19019)

*Mohammad Akbar-Tajari, Mohammad Taher Pilehvar, Mohammad Mahmoody*

**Main category:** cs.CL

**Keywords:** Large Language Models, adversarial attacks, robustness testing, Graph of Thoughts, black-box attacks

**Relevance Score:** 9

**TL;DR:** The paper introduces GoAT, a method to generate effective adversarial prompts to test the robustness of Large Language Models (LLMs), achieving high success rates against existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address vulnerabilities in Large Language Models (LLMs) that allow adversarial jailbreaks, improving model safety and alignment with societal standards.

**Method:** GoAT uses a graph structure to dynamically generate and refine adversarial prompts through collaborative exploration of multiple attack paths, enhancing the effectiveness of black-box attacks.

**Key Contributions:** Introduction of the GoAT framework for generating adversarial prompts., Better jailbreak success rates than current state-of-the-art methods., Dynamic integration of multiple reasoning paths to enhance attack effectiveness.

**Result:** GoAT achieves up to five times better jailbreak success rates compared to state-of-the-art methods, without needing access to model parameters, producing human-readable prompts.

**Limitations:** The method currently focuses on LLMs and may not be universally applicable to all AI model types.

**Future Work:** Further exploration of the collaborative reasoning framework and its applications in various AI models, as well as enhancing the attack strategies.

**Conclusion:** GoAT offers a significant advancement in testing LLM robustness, allowing for deeper exploration of adversarial vulnerabilities while maintaining model safety.

**Abstract:** The challenge of ensuring Large Language Models (LLMs) align with societal standards is of increasing interest, as these models are still prone to adversarial jailbreaks that bypass their safety mechanisms. Identifying these vulnerabilities is crucial for enhancing the robustness of LLMs against such exploits. We propose Graph of ATtacks (GoAT), a method for generating adversarial prompts to test the robustness of LLM alignment using the Graph of Thoughts framework [Besta et al., 2024]. GoAT excels at generating highly effective jailbreak prompts with fewer queries to the victim model than state-of-the-art attacks, achieving up to five times better jailbreak success rate against robust models like Llama. Notably, GoAT creates high-quality, human-readable prompts without requiring access to the targeted model's parameters, making it a black-box attack. Unlike approaches constrained by tree-based reasoning, GoAT's reasoning is based on a more intricate graph structure. By making simultaneous attack paths aware of each other's progress, this dynamic framework allows a deeper integration and refinement of reasoning paths, significantly enhancing the collaborative exploration of adversarial vulnerabilities in LLMs. At a technical level, GoAT starts with a graph structure and iteratively refines it by combining and improving thoughts, enabling synergy between different thought paths. The code for our implementation can be found at: https://github.com/GoAT-pydev/Graph_of_Attacks.

</details>


### [54] [Advancing Scientific Text Classification: Fine-Tuned Models with Dataset Expansion and Hard-Voting](https://arxiv.org/abs/2504.19021)

*Zhyar Rzgar K Rostam, Gábor Kertész*

**Main category:** cs.CL

**Keywords:** text classification, pre-trained language models, dataset augmentation, hard-voting strategy, academic publications

**Relevance Score:** 8

**TL;DR:** This study investigates the effectiveness of fine-tuning pre-trained language models for academic text classification using an augmented dataset from the Web of Science.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of academic publications necessitates efficient text classification techniques.

**Method:** The study fine-tunes various pre-trained language models (BERT, SciBERT, BioBERT, BlueBERT) on an augmented dataset sourced from targeted queries in the WoS database, applying a hard-voting strategy for improved accuracy.

**Key Contributions:** Evaluation of PLMs for scientific text classification, Contribution of dataset augmentation to model performance, Comparison of domain-specific and general-purpose models

**Result:** Fine-tuning with dynamic learning rates and early stopping leads to significant accuracy improvements, particularly in specialized domains, with domain-specific models outperforming general-purpose ones.

**Limitations:** The study may be limited by the focused domain and the specific dataset used for augmentation, potentially affecting generalizability to other fields.

**Future Work:** Further research could explore additional datasets and model architectures to enhance generalization across diverse academic fields.

**Conclusion:** The research demonstrates that dataset augmentation, label prediction strategies, and focused fine-tuning can greatly enhance performance in academic text classification.

**Abstract:** Efficient text classification is essential for handling the increasing volume of academic publications. This study explores the use of pre-trained language models (PLMs), including BERT, SciBERT, BioBERT, and BlueBERT, fine-tuned on the Web of Science (WoS-46985) dataset for scientific text classification. To enhance performance, we augment the dataset by executing seven targeted queries in the WoS database, retrieving 1,000 articles per category aligned with WoS-46985's main classes. PLMs predict labels for this unlabeled data, and a hard-voting strategy combines predictions for improved accuracy and confidence. Fine-tuning on the expanded dataset with dynamic learning rates and early stopping significantly boosts classification accuracy, especially in specialized domains. Domain-specific models like SciBERT and BioBERT consistently outperform general-purpose models such as BERT. These findings underscore the efficacy of dataset augmentation, inference-driven label prediction, hard-voting, and fine-tuning techniques in creating robust and scalable solutions for automated academic text classification.

</details>


### [55] [KETCHUP: K-Step Return Estimation for Sequential Knowledge Distillation](https://arxiv.org/abs/2504.19024)

*Jiabin Fan, Guoqing Luo, Michael Bowling, Lili Mou*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Knowledge Distillation, Text Generation, K-step Return, Large Language Models

**Relevance Score:** 8

**TL;DR:** The paper introduces KETCHUP, a k-step return estimation method for RL-based knowledge distillation in text generation, which reduces gradient variance and enhances performance on LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved reinforcement learning optimization in knowledge distillation for large language models.

**Method:** The paper uses a k-step return induction based on the Bellman Optimality Equation to estimate multiple steps in reinforcement learning.

**Key Contributions:** Introduction of KETCHUP, a novel k-step return estimation method for RL-KD., Theoretical analysis showing reduced gradient variance in RL optimization., Empirical results demonstrating superior performance in text generation tasks.

**Result:** Empirical evaluations across three text generation tasks show that KETCHUP outperforms existing methods in terms of standard metrics and LLM evaluations.

**Limitations:** 

**Future Work:** Further exploration of KETCHUP's application in different RL paradigms and other text generation tasks.

**Conclusion:** KETCHUP effectively improves the optimization process in RL-based knowledge distillation, particularly for larger student models.

**Abstract:** We propose a novel k-step return estimation method (called KETCHUP) for Reinforcement Learning(RL)-based knowledge distillation (KD) in text generation tasks. Our idea is to induce a K-step return by using the Bellman Optimality Equation for multiple steps. Theoretical analysis shows that this K-step formulation reduces the variance of the gradient estimates, thus leading to improved RL optimization especially when the student model size is large. Empirical evaluation on three text generation tasks demonstrates that our approach yields superior performance in both standard task metrics and large language model (LLM)-based evaluation. These results suggest that our K-step return induction offers a promising direction for enhancing RL-based KD in LLM research.

</details>


### [56] [Calibrating Translation Decoding with Quality Estimation on LLMs](https://arxiv.org/abs/2504.19044)

*Di Wu, Yibin Lei, Christof Monz*

**Main category:** cs.CL

**Keywords:** Neural Machine Translation, Calibration, Translation Quality, Large Language Models, Hypothesis Likelihoods

**Relevance Score:** 8

**TL;DR:** This paper proposes a method to calibrate hypothesis likelihoods in neural machine translation (NMT) systems, aiming to align the decoding process with real-world translation quality by optimizing their Pearson correlation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional maximum a posteriori (MAP) decoding in NMT often results in low-quality translations, motivating a need for better alignment between decoding objectives and real-world translation quality.

**Method:** The proposed method optimizes the Pearson correlation between hypothesis likelihoods and translation quality, using a distribution view for calibration, thus enhancing translation decoding effectiveness.

**Key Contributions:** Introduces a novel calibration method aligning translation likelihoods with quality, Demonstrates substantial translation quality gains using limited training, Outperforms existing translation quality estimation models in human evaluations.

**Result:** Implementing this approach leads to substantial improvements in translations from large language models (LLMs) with limited training, outperforming previous methods across various metrics and human evaluations.

**Limitations:** The study focuses primarily on calibration without delving into comprehensive algorithmic design of LLMs or other underlying techniques.

**Future Work:** Further exploration into the integration of this calibration method with other advanced model architectures and more extensive training datasets is suggested.

**Conclusion:** Calibrating translation likelihoods enhances MAP decoding efficiency and aligns predictions with actual translation quality, with the created state-of-the-art model available to the community.

**Abstract:** Neural machine translation (NMT) systems typically employ maximum a posteriori (MAP) decoding to select the highest-scoring translation from the distribution mass. However, recent evidence highlights the inadequacy of MAP decoding, often resulting in low-quality or even pathological hypotheses -- the decoding objective is not aligned with real-world translation quality. This paper proposes calibrating hypothesis likelihoods with translation quality from a distribution view by directly optimizing their Pearson correlation -- thereby enhancing the effectiveness of translation decoding. With our method, translation on large language models (LLMs) improves substantially after limited training (2K instances per direction). This improvement is orthogonal to those achieved through supervised fine-tuning, leading to substantial gains across a broad range of metrics and human evaluations -- even when applied to top-performing translation-specialized LLMs fine-tuned on high-quality translation data, such as Tower, or when compared to recent preference optimization methods, like CPO. Moreover, the calibrated translation likelihood can directly serve as a strong proxy for translation quality, closely approximating or even surpassing some state-of-the-art translation quality estimation models, like CometKiwi. Lastly, our in-depth analysis demonstrates that calibration enhances the effectiveness of MAP decoding, thereby enabling greater efficiency in real-world deployment. The resulting state-of-the-art translation model, which covers 10 languages, along with the accompanying code and human evaluation data, has been released to the community: https://github.com/moore3930/calibrating-llm-mt.

</details>


### [57] [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)

*Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib*

**Main category:** cs.CL

**Keywords:** Clinical Summarization, Large Language Models, Discharge Reports, Hallucination Detection, Patient Care

**Relevance Score:** 9

**TL;DR:** This paper evaluates open-source large language models for summarizing clinical discharge reports, focusing on key event extraction and hallucination prevalence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve the accuracy and reliability of clinical summarization using LLMs, essential for patient understanding and care management.

**Method:** The study involves numerical simulations to assess the performance of open-source LLMs in extracting key events from discharge reports.

**Key Contributions:** Evaluation of open-source LLMs for clinical discharge summaries, Analysis of key event extraction effectiveness, Assessment of hallucination prevalence in LLM output

**Result:** The evaluation reveals insights into the accuracy of extracted content and identifies the types of hallucinations present in the model-generated summaries.

**Limitations:** The study may be limited by the specific types of discharge reports used and the models evaluated.

**Future Work:** Future research should explore additional clinical texts and improve LLM methodologies to reduce hallucinations and enhance summarization quality.

**Conclusion:** Findings highlight the potential of LLMs in clinical settings while emphasizing the need for careful evaluation of hallucinations for maintaining reliability in patient care.

**Abstract:** Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization.

</details>


### [58] [Hallucinations and Key Information Extraction in Medical Texts: A Comprehensive Assessment of Open-Source Large Language Models](https://arxiv.org/abs/2504.19061)

*Anindya Bijoy Das, Shibbir Ahmed, Shahnewaz Karim Sakib*

**Main category:** cs.CL

**Keywords:** Clinical summarization, Large language models, Hallucinations, Discharge reports, Patient care

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of open-source large language models in summarizing clinical discharge reports and examines the prevalence of hallucinations in the generated summaries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Clinical summarization is essential for improving patient understanding and care management by simplifying complex medical data.

**Method:** The study investigates the use of open-source LLMs to extract key events from discharge reports and assesses hallucinations in the generated summaries through numerical simulations.

**Key Contributions:** Evaluation of open-source LLMs in clinical summarization tasks, Analysis of hallucinations in model-generated summaries, Comprehensive numerical simulations to assess model performance

**Result:** The performance of open-source LLMs in extracting key clinical information and the prevalence of hallucinations in the summaries are rigorously evaluated and reported.

**Limitations:** Focuses on specific types of clinical texts (discharge reports), which may limit generalizability.

**Future Work:** Further research could expand to other clinical documentation types and investigate methods to minimize hallucinations.

**Conclusion:** The findings highlight the potential of LLMs in clinical summarization while raising concerns about the reliability and accuracy of the generated information due to hallucinations.

**Abstract:** Clinical summarization is crucial in healthcare as it distills complex medical data into digestible information, enhancing patient understanding and care management. Large language models (LLMs) have shown significant potential in automating and improving the accuracy of such summarizations due to their advanced natural language understanding capabilities. These models are particularly applicable in the context of summarizing medical/clinical texts, where precise and concise information transfer is essential. In this paper, we investigate the effectiveness of open-source LLMs in extracting key events from discharge reports, such as reasons for hospital admission, significant in-hospital events, and critical follow-up actions. In addition, we also assess the prevalence of various types of hallucinations in the summaries produced by these models. Detecting hallucinations is vital as it directly influences the reliability of the information, potentially affecting patient care and treatment outcomes. We conduct comprehensive numerical simulations to rigorously evaluate the performance of these models, further probing the accuracy and fidelity of the extracted content in clinical summarization.

</details>


### [59] [ClimaEmpact: Domain-Aligned Small Language Models and Datasets for Extreme Weather Analytics](https://arxiv.org/abs/2504.19066)

*Deeksha Varshney, Keane Ong, Rui Mao, Erik Cambria, Gianmarco Mengaldo*

**Main category:** cs.CL

**Keywords:** Extreme Weather, Language Models, Data Alignment, Natural Language Processing, Analytics

**Relevance Score:** 6

**TL;DR:** This paper introduces EWRA, an innovative method that enhances small language models for analyzing extreme weather events by integrating structured reasoning paths and a large dataset of related news articles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate assessments of extreme weather events are vital for research and policy, but a lack of localized data hinders effective decision-making.

**Method:** The paper proposes Extreme Weather Reasoning-Aware Alignment (EWRA), which incorporates structured reasoning paths derived from large language models into small language models, utilizing the ExtremeWeatherNews dataset.

**Key Contributions:** Proposal of EWRA for aligning small language models with structured reasoning, Creation of the ExtremeWeatherNews dataset to support extreme weather analysis, Demonstration of improved performance of small language models over task-specific models in extreme weather tasks.

**Result:** The approach improves small language models' abilities to generate well-grounded, domain-specific responses for extreme weather analytics, outperforming task-specific models.

**Limitations:** 

**Future Work:** Further exploration of the framework's applicability to other domains and refining the reasoning alignment processes.

**Conclusion:** The EWRA method and the ClimaEmpact framework enhance the performance and real-world applicability of small language models in extreme weather analytics.

**Abstract:** Accurate assessments of extreme weather events are vital for research and policy, yet localized and granular data remain scarce in many parts of the world. This data gap limits our ability to analyze potential outcomes and implications of extreme weather events, hindering effective decision-making. Large Language Models (LLMs) can process vast amounts of unstructured text data, extract meaningful insights, and generate detailed assessments by synthesizing information from multiple sources. Furthermore, LLMs can seamlessly transfer their general language understanding to smaller models, enabling these models to retain key knowledge while being fine-tuned for specific tasks. In this paper, we propose Extreme Weather Reasoning-Aware Alignment (EWRA), a method that enhances small language models (SLMs) by incorporating structured reasoning paths derived from LLMs, and ExtremeWeatherNews, a large dataset of extreme weather event-related news articles. EWRA and ExtremeWeatherNews together form the overall framework, ClimaEmpact, that focuses on addressing three critical extreme-weather tasks: categorization of tangible vulnerabilities/impacts, topic labeling, and emotion analysis. By aligning SLMs with advanced reasoning strategies on ExtremeWeatherNews (and its derived dataset ExtremeAlign used specifically for SLM alignment), EWRA improves the SLMs' ability to generate well-grounded and domain-specific responses for extreme weather analytics. Our results show that the approach proposed guides SLMs to output domain-aligned responses, surpassing the performance of task-specific models and offering enhanced real-world applicability for extreme weather analytics.

</details>


### [60] [Sample-Efficient Language Model for Hinglish Conversational AI](https://arxiv.org/abs/2504.19070)

*Sakshi Singh, Abhinav Prakash, Aakriti Shah, Chaitanya Sachdeva, Sanjana Dumpala*

**Main category:** cs.CL

**Keywords:** Hinglish, conversational AI, language models, fine-tuning, cross-lingual

**Relevance Score:** 8

**TL;DR:** Development of a sample-efficient Hinglish chatbot using cross-lingual language models and fine-tuning techniques to overcome unique computational challenges.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to address the challenges posed by Hinglish, a code-mixed language, in generating conversational dialogue efficiently due to data scarcity and lack of standardization.

**Method:** The paper evaluates multiple pre-trained cross-lingual models and employs fine-tuning techniques alongside synthetic dialogue generation to enhance performance on Hinglish tasks.

**Key Contributions:** Evaluation of pre-trained cross-lingual models for Hinglish, Integration of synthetic dialogues with existing datasets, Demonstration of competitive performance with fewer parameters

**Result:** Models with fewer parameters, when fine-tuned on high-quality, code-mixed datasets, demonstrated competitive performance in conversational generation tasks.

**Limitations:** The reliance on synthetic data may impact the realism of conversational quality.

**Future Work:** Exploration of additional fine-tuning strategies and outside data sources to further improve model performance on Hinglish conversational tasks.

**Conclusion:** Efficient fine-tuning methods on limited data can yield effective results for developing conversational AI in code-mixed languages like Hinglish.

**Abstract:** This paper presents our process for developing a sample-efficient language model for a conversational Hinglish chatbot. Hinglish, a code-mixed language that combines Hindi and English, presents a unique computational challenge due to inconsistent spelling, lack of standardization, and limited quality of conversational data. This work evaluates multiple pre-trained cross-lingual language models, including Gemma3-4B and Qwen2.5-7B, and employs fine-tuning techniques to improve performance on Hinglish conversational tasks. The proposed approach integrates synthetically generated dialogues with insights from existing Hinglish datasets to address data scarcity. Experimental results demonstrate that models with fewer parameters, when appropriately fine-tuned on high-quality code-mixed data, can achieve competitive performance for Hinglish conversation generation while maintaining computational efficiency.

</details>


### [61] [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)

*Kristen Sussman, Daniel Carter*

**Main category:** cs.CL

**Keywords:** AI-mediated communication, social media, sentiment analysis, linguistic patterns, large language models

**Relevance Score:** 7

**TL;DR:** This study analyzes the impact of AI-mediated communication on linguistic patterns by comparing tweets from 2020 and 2024, revealing shifts in sentiment and text complexity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of AI-mediated communication on language use and sentiment in social media over time.

**Method:** The study compares a dataset of 970,919 tweets from 2020 and 20,000 tweets from 2024 using Flesch-Kincaid readability and polarity scores for analysis.

**Key Contributions:** Demonstrates the impact of AI on language use in social media over time., Provides statistical evidence of sentiment shifts in social media discourse., Highlights the changing nature of emotional expression in the context of AI-mediated communication.

**Result:** A significant increase in mean sentiment polarity was found from 0.04 in 2020 to 0.12 in 2024, along with a shift from 54.8% neutral content in 2020 to 39.8% in 2024 and an increase in positive sentiment from 28.6% to 45.9%.

**Limitations:** The analysis is limited to tweets that mention Donald Trump, which may not be representative of broader social media trends.

**Future Work:** Future research could explore the effects of AI communication in other contexts and with more diverse datasets.

**Conclusion:** The results indicate an increasing influence of AI on social media communication, affecting language and emotional expression.

**Abstract:** Given the subtle human-like effects of large language models on linguistic patterns, this study examines shifts in language over time to detect the impact of AI-mediated communication (AI- MC) on social media. We compare a replicated dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the same period in 2024, all of which mention Donald Trump during election periods. Using a combination of Flesch-Kincaid readability and polarity scores, we analyze changes in text complexity and sentiment. Our findings reveal a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%). These findings suggest not only an increasing presence of AI in social media communication but also its impact on language and emotional expression patterns.

</details>


### [62] [Efficient Reasoning for LLMs through Speculative Chain-of-Thought](https://arxiv.org/abs/2504.19095)

*Jikai Wang, Juntao Li, Lijun Wu, Min Zhang*

**Main category:** cs.CL

**Keywords:** Reasoning Language Models, Speculative Chain-of-Thought, Model Collaboration

**Relevance Score:** 8

**TL;DR:** Introduction of Speculative Chain-of-Thought (SCoT) to reduce reasoning latency using model collaboration.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high reasoning costs and latency in large reasoning language models while maintaining accuracy.

**Method:** The method involves drafting thoughts with a lightweight model and correcting mistakes with a larger target model, enhancing drafting efficiency and prediction accuracy.

**Key Contributions:** Introduction of SCoT for efficient reasoning in large models., Demonstration of reduced reasoning latency with maintained accuracy., Application of lightweight and heavyweight model collaboration.

**Result:** Experimental results demonstrate that SCoT can reduce reasoning latency by 48% to 66% while achieving performance near the target model on various datasets.

**Limitations:** The paper does not address the trade-offs between model sizes in practical applications or robustness of the proposed approach in varied contexts.

**Future Work:** Exploration of further enhancing model collaborations and testing across additional domains or datasets.

**Conclusion:** SCoT presents a novel approach to improve reasoning efficiency without sacrificing accuracy, which could influence future model implementations.

**Abstract:** Large reasoning language models such as OpenAI-o1 and Deepseek-R1 have recently attracted widespread attention due to their impressive task-solving abilities. However, the enormous model size and the generation of lengthy thought chains introduce significant reasoning costs and response latency. Existing methods for efficient reasoning mainly focus on reducing the number of model parameters or shortening the chain-of-thought length. In this paper, we introduce Speculative Chain-of-Thought (SCoT), which reduces reasoning latency from another perspective by accelerated average reasoning speed through large and small model collaboration. SCoT conducts thought-level drafting using a lightweight draft model. Then it selects the best CoT draft and corrects the error cases with the target model. The proposed thinking behavior alignment improves the efficiency of drafting and the draft selection strategy maintains the prediction accuracy for complex problems. Experimental results on GSM8K, MATH, GaoKao, CollegeMath and Olympiad datasets show that SCoT reduces reasoning latency by 48\%$\sim$66\% for Deepseek-R1-Distill-Qwen-32B while achieving near-target-model-level performance. Our code is available at https://github.com/Jikai0Wang/Speculative_CoT.

</details>


### [63] [Privacy-Preserving Federated Embedding Learning for Localized Retrieval-Augmented Generation](https://arxiv.org/abs/2504.19101)

*Qianren Mao, Qili Zhang, Hanwen Hao, Zhentao Han, Runhua Xu, Weifeng Jiang, Qi Hu, Zhijun Chen, Tyler Zhou, Bo Li, Yangqiu Song, Jin Dong, Jianxin Li, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Federated Learning, Data Privacy, Knowledge Distillation, Homomorphic Encryption

**Relevance Score:** 9

**TL;DR:** This paper presents a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG) that addresses data privacy challenges in private Retrieval-Augmented Generation systems by leveraging federated learning and knowledge distillation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the accuracy and credibility of Large Language Models in Question & Answer tasks while addressing the challenges of data scarcity and privacy in private RAG systems.

**Method:** The paper proposes the FedE4RAG framework, which uses federated learning for collaborative training of RAG retrieval models, securing data privacy through aggregation of model parameters and employing knowledge distillation between server and clients.

**Key Contributions:** Introduction of the FedE4RAG framework for federated learning in RAG systems, Application of knowledge distillation to enhance client-side model training, Integration of homomorphic encryption for safeguarding model parameters

**Result:** Extensive experiments show that FedE4RAG significantly improves the performance of private RAG systems while ensuring robust data privacy protection, effectively balancing data security with availability.

**Limitations:** 

**Future Work:** Exploration of further optimizations in federated learning for RAG systems and broader applications in different domains.

**Conclusion:** The proposed FedE4RAG framework can facilitate better deployment of privacy-preserving RAG systems, making them more effective in real-world applications.

**Abstract:** Retrieval-Augmented Generation (RAG) has recently emerged as a promising solution for enhancing the accuracy and credibility of Large Language Models (LLMs), particularly in Question & Answer tasks. This is achieved by incorporating proprietary and private data from integrated databases. However, private RAG systems face significant challenges due to the scarcity of private domain data and critical data privacy issues. These obstacles impede the deployment of private RAG systems, as developing privacy-preserving RAG systems requires a delicate balance between data security and data availability. To address these challenges, we regard federated learning (FL) as a highly promising technology for privacy-preserving RAG services. We propose a novel framework called Federated Retrieval-Augmented Generation (FedE4RAG). This framework facilitates collaborative training of client-side RAG retrieval models. The parameters of these models are aggregated and distributed on a central-server, ensuring data privacy without direct sharing of raw data. In FedE4RAG, knowledge distillation is employed for communication between the server and client models. This technique improves the generalization of local RAG retrievers during the federated learning process. Additionally, we apply homomorphic encryption within federated learning to safeguard model parameters and mitigate concerns related to data leakage. Extensive experiments conducted on the real-world dataset have validated the effectiveness of FedE4RAG. The results demonstrate that our proposed framework can markedly enhance the performance of private RAG systems while maintaining robust data privacy protection.

</details>


### [64] [APE-Bench I: Towards File-level Automated Proof Engineering of Formal Math Libraries](https://arxiv.org/abs/2504.19110)

*Huajian Xin, Luming Li, Xiaoran Jin, Jacques Fleuriot, Wenda Li*

**Main category:** cs.CL

**Keywords:** Automated Proof Engineering, Large Language Models, Formal Theorem Proving

**Relevance Score:** 5

**TL;DR:** This paper introduces Automated Proof Engineering (APE) to enhance automated proof tasks using large language models (LLMs) and presents APE-Bench I, a benchmark from real-world proof libraries.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing benchmarks in formal theorem proving that do not reflect real-world, iterative workflows in formal mathematics.

**Method:** Development of APE-Bench I, a benchmark using commit histories from Mathlib4 with tasks described in natural language and verified through a hybrid method using the Lean compiler and LLM-as-a-Judge.

**Key Contributions:** Introduction of Automated Proof Engineering (APE) concept., Creation of the APE-Bench I benchmark from real-world data., Development of Eleanstic, a scalable verification infrastructure.

**Result:** Empirical testing shows strong performance for localized edits but significant challenges in complex proof engineering tasks.

**Limitations:** Performance degradation on complex proof engineering tasks was noted, indicating existing limitations in current LLM approaches.

**Future Work:** Future benchmarks will target improvements in multi-file coordination, project-scale verification, and the creation of autonomous agents for formal libraries.

**Conclusion:** This research establishes a foundational framework for future automated proof workflows and suggests directions for more complex multi-file verification benchmarks.

**Abstract:** Recent progress in large language models (LLMs) has shown promise in formal theorem proving, yet existing benchmarks remain limited to isolated, static proof tasks, failing to capture the iterative, engineering-intensive workflows of real-world formal mathematics libraries. Motivated by analogous advances in software engineering, we introduce the paradigm of Automated Proof Engineering (APE), which aims to automate proof engineering tasks such as feature addition, proof refactoring, and bug fixing using LLMs. To facilitate research in this direction, we present APE-Bench I, the first realistic benchmark built from real-world commit histories of Mathlib4, featuring diverse file-level tasks described in natural language and verified via a hybrid approach combining the Lean compiler and LLM-as-a-Judge. We further develop Eleanstic, a scalable parallel verification infrastructure optimized for proof checking across multiple versions of Mathlib. Empirical results on state-of-the-art LLMs demonstrate strong performance on localized edits but substantial degradation on handling complex proof engineering. This work lays the foundation for developing agentic workflows in proof engineering, with future benchmarks targeting multi-file coordination, project-scale verification, and autonomous agents capable of planning, editing, and repairing formal libraries.

</details>


### [65] [SPC: Evolving Self-Play Critic via Adversarial Games for LLM Reasoning](https://arxiv.org/abs/2504.19162)

*Jiaqi Chen, Bang Zhang, Ruotian Ma, Peisong Wang, Xiaodan Liang, Zhaopeng Tu, Xiaolong Li, Kwan-Yee K. Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Play Critic, Reasoning Evaluation, Reinforcement Learning, Error Detection

**Relevance Score:** 8

**TL;DR:** Introducing Self-Play Critic (SPC), a method that assesses reasoning steps in large language models through adversarial self-play, leading to improved error detection without manual annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of evaluating LLM reasoning reliability due to the high cost and difficulty of obtaining precise step-level supervision.

**Method:** SPC utilizes two copies of a base model, a 'sneaky generator' that creates difficult-to-detect errors and a 'critic' that analyzes these steps, training them through adversarial self-play games using reinforcement learning.

**Key Contributions:** Introduction of a novel adversarial self-play methodology for model training, Improvement of error detection capabilities in LLMs without manual annotation, Significant enhancements in mathematical reasoning performance on benchmark datasets

**Result:** SPC enhances error detection capabilities, improving accuracy from 70.8% to 77.7% on the ProcessBench benchmark and outperforming strong baseline models.

**Limitations:** 

**Future Work:** Exploration of further applications of SPC in various reasoning tasks and enhancement of critic model capabilities.

**Conclusion:** SPC effectively trains LLMs to improve their reasoning performance, specifically in mathematical tasks, and demonstrates the ability to reduce reliance on manual supervision.

**Abstract:** Evaluating the step-by-step reliability of large language model (LLM) reasoning, such as Chain-of-Thought, remains challenging due to the difficulty and cost of obtaining high-quality step-level supervision. In this paper, we introduce Self-Play Critic (SPC), a novel approach where a critic model evolves its ability to assess reasoning steps through adversarial self-play games, eliminating the need for manual step-level annotation. SPC involves fine-tuning two copies of a base model to play two roles, namely a "sneaky generator" that deliberately produces erroneous steps designed to be difficult to detect, and a "critic" that analyzes the correctness of reasoning steps. These two models engage in an adversarial game in which the generator aims to fool the critic, while the critic model seeks to identify the generator's errors. Using reinforcement learning based on the game outcomes, the models iteratively improve; the winner of each confrontation receives a positive reward and the loser receives a negative reward, driving continuous self-evolution. Experiments on three reasoning process benchmarks (ProcessBench, PRM800K, DeltaBench) demonstrate that our SPC progressively enhances its error detection capabilities (e.g., accuracy increases from 70.8% to 77.7% on ProcessBench) and surpasses strong baselines, including distilled R1 model. Furthermore, applying SPC to guide the test-time search of diverse LLMs significantly improves their mathematical reasoning performance on MATH500 and AIME2024, outperforming state-of-the-art process reward models.

</details>


### [66] [WuNeng: Hybrid State with Attention](https://arxiv.org/abs/2504.19191)

*Liu Xiao, Li Zhiyuan, Lin Yueyu*

**Main category:** cs.CL

**Keywords:** WuNeng, large language models, RWKV-7, attention mechanisms, neural architectures

**Relevance Score:** 6

**TL;DR:** The WuNeng architecture enhances large language models by integrating RNN-based RWKV-7 with improved attention mechanisms for better contextual coherence and efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the expressivity and computational efficiency of large language models while maintaining contextual coherence.

**Method:** Integration of RNN-based RWKV-7 with advanced attention mechanisms, using a hybrid-head concept to augment standard multi-head attention and a cross-head interaction technique for dynamic synergy among heads.

**Key Contributions:** Introduction of a novel architecture integrating RWKV-7 with advanced attention mechanisms., Use of hybrid-head concept to enhance multi-head attention., Implementation of multi-token state processing for capturing sequence-wide dependencies.

**Result:** The architecture significantly boosts the expressivity of the model with minimal additional parameters, excelling in complex reasoning and sequence generation tasks.

**Limitations:** 

**Future Work:** 

**Conclusion:** WuNeng establishes a new standard for balancing expressivity and computational efficiency in neural architectures.

**Abstract:** The WuNeng architecture introduces a novel approach to enhancing the expressivity and power of large language models by integrating recurrent neural network (RNN)-based RWKV-7 with advanced attention mechanisms, prioritizing heightened contextual coherence over reducing KV cache size. Building upon the hybrid-head concept from Hymba, WuNeng augments standard multi-head attention with additional RWKV-7 state-driven heads, rather than replacing existing heads, to enrich the model's representational capacity. A cross-head interaction technique fosters dynamic synergy among standard, state-driven, and newly introduced middle heads, leveraging concatenation, additive modulation, and gated fusion for robust information integration. Furthermore, a multi-token state processing mechanism harnesses the continuous RWKV-7 state to capture intricate, sequence-wide dependencies, significantly boosting expressivity. Remarkably, these enhancements are achieved with minimal additional parameters, ensuring efficiency while empowering the model to excel in complex reasoning and sequence generation tasks. WuNeng sets a new standard for balancing expressivity and computational efficiency in modern neural architectures.

</details>


### [67] [Dynamic Embedded Topic Models: properties and recommendations based on diverse corpora](https://arxiv.org/abs/2504.19209)

*Elisabeth Fittschen, Bella Xia, Leib Celnik, Paul Dilley, Tom Lippincott*

**Main category:** cs.CL

**Keywords:** Dynamic Embedded Topic Model, diachronic corpora, implementation choices

**Relevance Score:** 4

**TL;DR:** This paper evaluates dynamic embedded topic model implementations across various historical corpora to enhance applied scholarship utility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the Dynamic Embedded Topic Model to better handle diachronic corpora and identify key implementation decisions that enhance its applicability in scholarship.

**Method:** The authors apply the Dynamic Embedded Topic Model to five different diachronic corpora, analyzing various implementation choices and their effects.

**Key Contributions:** Identification of implementation choices that affect the utility of the model, Recommending practical approaches for vocabulary size scalability, Highlighting performance aspects that do not significantly hinder application

**Result:** Key findings indicate that model performance can be improved through better vocabulary scalability and flexible interval modeling, while certain limitations do not significantly affect performance.

**Limitations:** The model's application is still impacted by certain decisions, but many hypothesized limiting factors were found not to affect performance significantly.

**Future Work:** Further exploration of additional implementation choices and their impact on performance across different types of corpora.

**Conclusion:** Enhancing the scalability of vocabulary size and modeling flexibility can maximize the effectiveness of the Dynamic Embedded Topic Model.

**Abstract:** We measure the effects of several implementation choices for the Dynamic Embedded Topic Model, as applied to five distinct diachronic corpora, with the goal of isolating important decisions for its use and further development. We identify priorities that will maximize utility in applied scholarship, including the practical scalability of vocabulary size to best exploit the strengths of embedded representations, and more flexible modeling of intervals to accommodate the uneven temporal distributions of historical writing. Of similar importance, we find performance is not significantly or consistently affected by several aspects that otherwise limit the model's application or might consume the resources of a grid search.

</details>


### [68] [Uncertainty Quantification for Language Models: A Suite of Black-Box, White-Box, LLM Judge, and Ensemble Scorers](https://arxiv.org/abs/2504.19254)

*Dylan Bouchard, Mohit Singh Chauhan*

**Main category:** cs.CL

**Keywords:** hallucination detection, Large Language Models, uncertainty quantification, healthcare, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a framework for zero-resource hallucination detection in Large Language Models using uncertainty quantification techniques, showing improved performance in high-stakes applications like healthcare.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is the persistent issue of hallucinations in Large Language Models, especially as they are increasingly used in high-stakes domains such as healthcare and finance.

**Method:** The authors adapt various uncertainty quantification techniques into a standardized framework, creating response-level confidence scores and a tunable ensemble approach for improved detection of hallucinations.

**Key Contributions:** Versatile framework for zero-resource hallucination detection., Introduction of a tunable ensemble approach for optimizing confidence scores., Provision of a companion Python toolkit, UQLM, for implementation.

**Result:** The tunable ensemble approach typically outperforms both its individual components and existing methods, demonstrating enhanced accuracy and reliability for LLM applications.

**Limitations:** 

**Future Work:** Future work could explore enhancements in uncertainty quantification techniques and expand the toolkit's applications in different high-stakes domains.

**Conclusion:** The results highlight the importance of customized hallucination detection strategies to bolster the performance of LLMs in critical applications.

**Abstract:** Hallucinations are a persistent problem with Large Language Models (LLMs). As these models become increasingly used in high-stakes domains, such as healthcare and finance, the need for effective hallucination detection is crucial. To this end, we propose a versatile framework for zero-resource hallucination detection that practitioners can apply to real-world use cases. To achieve this, we adapt a variety of existing uncertainty quantification (UQ) techniques, including black-box UQ, white-box UQ, and LLM-as-a-Judge, transforming them as necessary into standardized response-level confidence scores ranging from 0 to 1. To enhance flexibility, we introduce a tunable ensemble approach that incorporates any combination of the individual confidence scores. This approach enables practitioners to optimize the ensemble for a specific use case for improved performance. To streamline implementation, the full suite of scorers is offered in this paper's companion Python toolkit, UQLM. To evaluate the performance of the various scorers, we conduct an extensive set of experiments using several LLM question-answering benchmarks. We find that our tunable ensemble typically surpasses its individual components and outperforms existing hallucination detection methods. Our results demonstrate the benefits of customized hallucination detection strategies for improving the accuracy and reliability of LLMs.

</details>


### [69] [VIST-GPT: Ushering in the Era of Visual Storytelling with LLMs?](https://arxiv.org/abs/2504.19267)

*Mohamed Gado, Towhid Taliee, Muhammad Memon, Dmitry Ignatov, Radu Timofte*

**Main category:** cs.CL

**Keywords:** Visual Storytelling, Multimodal Models, Narrative Generation

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel multimodal model for visual storytelling that produces narratives from images, using advanced evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to enhance the task of visual storytelling by leveraging recent advancements in multimodal models and addressing the limitations of traditional evaluation metrics.

**Method:** The authors present the VIST-GPT model, which employs transformer-based architectures and utilizes the Visual Storytelling (VIST) dataset to generate narratives.

**Key Contributions:** Introduction of VIST-GPT model for visual storytelling using transformer-based architecture, Development of RoViST and GROOVIST metrics for narrative evaluation, Demonstration of improved narrative generation using multimodal models

**Result:** The proposed model produces visually grounded and contextually appropriate narratives, outperforming traditional models.

**Limitations:** Evaluation metrics may still require validation against human judgments across diverse storytelling contexts.

**Future Work:** Future research could explore further refinements in narrative generation techniques and enhanced evaluation metrics.

**Conclusion:** Novel reference-free metrics RoViST and GROOVIST are introduced to better evaluate the quality of visual narratives, focusing on aspects like grounding and coherence.

**Abstract:** Visual storytelling is an interdisciplinary field combining computer vision and natural language processing to generate cohesive narratives from sequences of images. This paper presents a novel approach that leverages recent advancements in multimodal models, specifically adapting transformer-based architectures and large multimodal models, for the visual storytelling task. Leveraging the large-scale Visual Storytelling (VIST) dataset, our VIST-GPT model produces visually grounded, contextually appropriate narratives. We address the limitations of traditional evaluation metrics, such as BLEU, METEOR, ROUGE, and CIDEr, which are not suitable for this task. Instead, we utilize RoViST and GROOVIST, novel reference-free metrics designed to assess visual storytelling, focusing on visual grounding, coherence, and non-redundancy. These metrics provide a more nuanced evaluation of narrative quality, aligning closely with human judgment.

</details>


### [70] [AndroidGen: Building an Android Language Agent under Data Scarcity](https://arxiv.org/abs/2504.19298)

*Hanyu Lai, Junjie Gao, Xiao Liu, Yifan Xu, Shudan Zhang, Yuxiao Dong, Jie Tang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mobile Agents, Data Scarcity, Human Tasks, Open-source

**Relevance Score:** 8

**TL;DR:** The paper introduces AndroidGen, a framework to enhance LLM-based agents on mobile devices by addressing data scarcity through trajectory collection and open-source model training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the gap in using LLMs as agents on mobile devices due to high demands for quality data and challenges in human annotation.

**Method:** The authors develop AndroidGen to enhance LLM capabilities by collecting task trajectories and training open-source LLMs without the need for manually labeled data.

**Key Contributions:** Introduction of AndroidGen framework for LLM-based mobile agents, Development of open-source mobile agent training without manual annotation, Extensive evaluation showcasing improvements over existing systems

**Result:** AndroidGen was extensively evaluated alongside AndroidWorld, AitW, and various applications, showing significant improvements in performance.

**Limitations:** The study focuses on specific applications and may not generalize to all mobile environments or tasks.

**Future Work:** Further improvements to the trajectory collection process and enhancements to the LLM training methodology are proposed as future exploration.

**Conclusion:** The research reveals the potential of AndroidGen in improving LLM-based mobile agents and highlights areas for future exploration and enhancement.

**Abstract:** Large language models have opened up a world of possibilities for various NLP tasks, sparking optimism for the future. Despite their potential, LLMs have yet to be widely used as agents on real mobile devices. The main challenge is the need for high-quality data sources. Time constraints and labor intensity often hinder human annotation. On the other hand, existing LLMs exhibit inadequate completion rates and need a robust data filtration strategy. Given these challenges, we develop a framework called AndroidGen to enhance the capabilities of LLM-based agents under data scarcity. In addition, we leverage AndroidGen to collect trajectories given human tasks and train open-source LLMs on these trajectories to develop an open-source mobile agent without manually labeled trajectories. We extensively evaluate AndroidGen with AndroidWorld, AitW, and various popular applications, demonstrating its improvements and revealing potential areas for future improvement. Code, model, and data are available at https://github.com/THUDM/AndroidGen.

</details>


### [71] [BrowseComp-ZH: Benchmarking Web Browsing Ability of Large Language Models in Chinese](https://arxiv.org/abs/2504.19314)

*Peilin Zhou, Bruce Leon, Xiang Ying, Can Zhang, Yifan Shao, Qichen Ye, Dading Chong, Zhiling Jin, Chenxuan Xie, Meng Cao, Yuxin Gu, Sixin Hong, Jing Ren, Jian Chen, Chao Liu, Yining Hua*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarking, Human-Computer Interaction, Information Retrieval

**Relevance Score:** 9

**TL;DR:** BrowseComp-ZH is a benchmark for evaluating large language models on the Chinese web, highlighting their current limitations in reasoning and retrieval tasks.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate large language models (LLMs) in real-time web browsing, especially on the Chinese web, where existing benchmarks are lacking.

**Method:** The benchmark consists of 289 multi-hop questions across 11 domains, with a two-stage quality control process to ensure question difficulty and answer uniqueness.

**Key Contributions:** Introduction of BrowseComp-ZH for benchmarking LLMs on the Chinese web, High-difficulty multi-hop questions covering diverse domains, Public release of dataset and construction guidelines

**Result:** Most evaluated models, including OpenAI's DeepResearch, perform poorly, with accuracy rates below 10% for many, and only a few exceeding 20%.

**Limitations:** The benchmark primarily focuses on the Chinese web and may not address other languages and information systems.

**Future Work:** Extending the evaluation framework to other languages and improving model capabilities in reasoning and retrieval.

**Conclusion:** The findings illustrate the struggle of current models with complex retrieval and reasoning tasks that are necessary for browsing effectively.

**Abstract:** As large language models (LLMs) evolve into tool-using agents, the ability to browse the web in real-time has become a critical yardstick for measuring their reasoning and retrieval competence. Existing benchmarks such as BrowseComp concentrate on English and overlook the linguistic, infrastructural, and censorship-related complexities of other major information ecosystems -- most notably Chinese. To address this gap, we introduce BrowseComp-ZH, a high-difficulty benchmark purpose-built to comprehensively evaluate LLM agents on the Chinese web. BrowseComp-ZH consists of 289 multi-hop questions spanning 11 diverse domains. Each question is reverse-engineered from a short, objective, and easily verifiable answer (e.g., a date, number, or proper noun). A two-stage quality control protocol is applied to strive for high question difficulty and answer uniqueness. We benchmark over 20 state-of-the-art language models and agentic search systems on our proposed BrowseComp-ZH. Despite their strong conversational and retrieval capabilities, most models struggle severely: a large number achieve accuracy rates below 10%, and only a handful exceed 20%. Even the best-performing system, OpenAI's DeepResearch, reaches just 42.9%. These results demonstrate the considerable difficulty of BrowseComp-ZH, where success demands not only effective retrieval strategies, but also sophisticated reasoning and information reconciliation -- capabilities that current models still struggle to master. Our dataset, construction guidelines, and benchmark results have been publicly released at https://github.com/PALIN2018/BrowseComp-ZH.

</details>


### [72] [Unified Multi-Task Learning & Model Fusion for Efficient Language Model Guardrailing](https://arxiv.org/abs/2504.19333)

*James O' Neill, Santhosh Subramanian, Eric Lin, Vaikkunth Mugunthan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Task-Specific Classifiers, Guardrail Models

**Relevance Score:** 9

**TL;DR:** This paper presents a method for generating task-specific classifiers that outperform state-of-the-art models in safety detection while being significantly smaller and more efficient.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing reliance on large language models for safety measures leads to challenges, such as latency and high resource consumption, which this work aims to address.

**Method:** The authors propose task-specific data generation to create fine-tuned classifiers and introduce a model, MultiTaskGuard, pretrained on a synthetically generated dataset. They also present a search-based model merging approach to optimize the combination of policies.

**Key Contributions:** Introduced task-specific data generation for classifier tuning, Developed MultiTaskGuard model for better generalization, Proposed a search-based model merging technique for optimal parameter combination

**Result:** On 7 public datasets, the proposed classifiers demonstrated an average F1 score improvement of 29.92 points over Aegis-LlamaGuard and 21.62 over gpt-4o, outperforming existing LLMs and guardrail APIs.

**Limitations:** The study may be limited by the dependency on synthetic data and the potential for overfitting in specific task contexts.

**Future Work:** Exploration of more diverse synthetic datasets and further refinement of the merging approach for improved model performance.

**Conclusion:** The results indicate that efficient guardrail classifiers can be both compact and highly effective in detecting safe and unsafe behaviors, presenting a viable alternative to current LLMs.

**Abstract:** The trend towards large language models (LLMs) for guardrailing against undesired behaviors is increasing and has shown promise for censoring user inputs. However, increased latency, memory consumption, hosting expenses and non-structured outputs can make their use prohibitive.   In this work, we show that task-specific data generation can lead to fine-tuned classifiers that significantly outperform current state of the art (SoTA) while being orders of magnitude smaller. Secondly, we show that using a single model, \texttt{MultiTaskGuard}, that is pretrained on a large synthetically generated dataset with unique task instructions further improves generalization. Thirdly, our most performant models, \texttt{UniGuard}, are found using our proposed search-based model merging approach that finds an optimal set of parameters to combine single-policy models and multi-policy guardrail models. % On 7 public datasets and 4 guardrail benchmarks we created, our efficient guardrail classifiers improve over the best performing SoTA publicly available LLMs and 3$^{\text{rd}}$ party guardrail APIs in detecting unsafe and safe behaviors by an average F1 score improvement of \textbf{29.92} points over Aegis-LlamaGuard and \textbf{21.62} over \texttt{gpt-4o}, respectively. Lastly, our guardrail synthetic data generation process that uses custom task-specific guardrail poli

</details>


### [73] [Explanatory Summarization with Discourse-Driven Planning](https://arxiv.org/abs/2504.19339)

*Dongqi Liu, Xi Yu, Vera Demberg, Mirella Lapata*

**Main category:** cs.CL

**Keywords:** automatic summarization, discourse frameworks, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper presents a plan-based approach to enhance automatic summarization of scientific documents by integrating discourse frameworks to improve clarity in lay summaries.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current automatic summarization methods lack explicit modeling of explanations, making it hard to align with human-written summaries.

**Method:** The authors propose two discourse-driven planning strategies that condition a discourse plan either as part of the input or as part of the output prefix for generating summaries.

**Key Contributions:** Introduction of discourse-driven planning strategies for summary generation., Demonstrated improvements in summary quality over existing methods., Enhanced robustness and controllability in automatic summarization.

**Result:** Empirical experiments on three lay summarization datasets demonstrate that the proposed approach significantly outperforms existing state-of-the-art methods in summary quality, model robustness, and controllability.

**Limitations:** 

**Future Work:** Explore further integration of discourse frameworks in various summarization tasks and develop user-centered evaluation methods.

**Conclusion:** The plan-based approach effectively improves the quality of lay summaries by better aligning explanatory content with human expectations.

**Abstract:** Lay summaries for scientific documents typically include explanations to help readers grasp sophisticated concepts or arguments. However, current automatic summarization methods do not explicitly model explanations, which makes it difficult to align the proportion of explanatory content with human-written summaries. In this paper, we present a plan-based approach that leverages discourse frameworks to organize summary generation and guide explanatory sentences by prompting responses to the plan. Specifically, we propose two discourse-driven planning strategies, where the plan is conditioned as part of the input or part of the output prefix, respectively. Empirical experiments on three lay summarization datasets show that our approach outperforms existing state-of-the-art methods in terms of summary quality, and it enhances model robustness, controllability, and mitigates hallucination.

</details>


### [74] [ICL CIPHERS: Quantifying "Learning'' in In-Context Learning via Substitution Ciphers](https://arxiv.org/abs/2504.19395)

*Zhouxiang Fang, Aayush Mishra, Muhan Gao, Anqi Liu, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Large Language Models, Substitution Ciphers, Task Retrieval, Task Learning

**Relevance Score:** 7

**TL;DR:** This paper introduces ICL CIPHERS, a novel approach to study In-Context Learning (ICL) in LLMs that utilizes reversible substitution ciphers to distinguish between task retrieval and task learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand the dual modes of In-Context Learning (ICL) in large language models (LLMs) by using a structured approach with cryptography-inspired reformulations.

**Method:** The authors developed a class of task reformulations called ICL CIPHERS, which involve substituting tokens in input sentences with irrelevant ones based on a fixed reversible pattern. This allows for the assessment of LLMs' capabilities to decipher the underlying tasks while maintaining abstract task structures.

**Key Contributions:** Introduction of ICL CIPHERS to study In-Context Learning in LLMs., Demonstration that LLMs can better solve bijective ciphers than non-bijective ones, highlighting their learning capabilities., Evidence of LLMs' internal representations aiding in deciphering inputs.

**Result:** Experiments show that LLMs perform better in decoding ICL CIPHERS that employ bijective mappings compared to non-bijective (irreversible) mappings, indicating a novel way to quantify learning in ICL.

**Limitations:** The performance gap is small, suggesting that while there is some distinction in capabilities, it may not be significant across all scenarios.

**Future Work:** Further exploration of different cipher constructions and their impacts on LLM performance; potential applications in enhancing understanding of learning paradigms in AI.

**Conclusion:** The results suggest that the nature of the mapping in ICL can affect performance, with consistent results across various datasets and models. Insights into LLMs' internal representations were also examined through this approach.

**Abstract:** Recent works have suggested that In-Context Learning (ICL) operates in dual modes, i.e. task retrieval (remember learned patterns from pre-training) and task learning (inference-time ``learning'' from demonstrations). However, disentangling these the two modes remains a challenging goal. We introduce ICL CIPHERS, a class of task reformulations based on substitution ciphers borrowed from classic cryptography. In this approach, a subset of tokens in the in-context inputs are substituted with other (irrelevant) tokens, rendering English sentences less comprehensible to human eye. However, by design, there is a latent, fixed pattern to this substitution, making it reversible. This bijective (reversible) cipher ensures that the task remains a well-defined task in some abstract sense, despite the transformations. It is a curious question if LLMs can solve ICL CIPHERS with a BIJECTIVE mapping, which requires deciphering the latent cipher. We show that LLMs are better at solving ICL CIPHERS with BIJECTIVE mappings than the NON-BIJECTIVE (irreversible) baseline, providing a novel approach to quantify ``learning'' in ICL. While this gap is small, it is consistent across the board on four datasets and six models. Finally, we examine LLMs' internal representations and identify evidence in their ability to decode the ciphered inputs.

</details>


### [75] [Context Selection and Rewriting for Video-based EducationalQuestion Generation](https://arxiv.org/abs/2504.19406)

*Mengxia Yu, Bang Nguyen, Olivia Zino, Meng Jiang*

**Main category:** cs.CL

**Keywords:** Educational Question Generation, Large Language Models, Real-world Data, Context Selection, Video Lectures

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework for educational question generation (EQG) from real-world classroom lectures, utilizing large language models to dynamically select and rewrite context from transcripts and videos.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve educational question generation from real-world classroom content, addressing the limitations of existing EQG datasets that rely on predefined texts.

**Method:** A framework is introduced that selects contexts from lecture transcripts and video keyframes based on answer relevance and temporal proximity, integrating them into answer-containing knowledge statements.

**Key Contributions:** Dataset of educational questions generated from real-world classroom lectures, Novel framework for dynamic context selection and rewriting using large language models, Improvements in question generation quality and relevance

**Result:** The approach significantly enhances the quality and relevance of generated questions compared to existing methods.

**Limitations:** Current methods still struggle with contextual selection from extensive transcripts.

**Future Work:** Further research could explore more efficient ways to process larger datasets and integrate additional contextual data sources.

**Conclusion:** The proposed framework overcomes challenges in EQG, leading to better alignment of questions with target answers and timestamps.

**Abstract:** Educational question generation (EQG) is a crucial component of intelligent educational systems, significantly aiding self-assessment, active learning, and personalized education. While EQG systems have emerged, existing datasets typically rely on predefined, carefully edited texts, failing to represent real-world classroom content, including lecture speech with a set of complementary slides. To bridge this gap, we collect a dataset of educational questions based on lectures from real-world classrooms. On this realistic dataset, we find that current methods for EQG struggle with accurately generating questions from educational videos, particularly in aligning with specific timestamps and target answers. Common challenges include selecting informative contexts from extensive transcripts and ensuring generated questions meaningfully incorporate the target answer. To address the challenges, we introduce a novel framework utilizing large language models for dynamically selecting and rewriting contexts based on target timestamps and answers. First, our framework selects contexts from both lecture transcripts and video keyframes based on answer relevance and temporal proximity. Then, we integrate the contexts selected from both modalities and rewrite them into answer-containing knowledge statements, to enhance the logical connection between the contexts and the desired answer. This approach significantly improves the quality and relevance of the generated questions. Our dataset and code are released in https://github.com/mengxiayu/COSER.

</details>


### [76] [Mem0: Building Production-Ready AI Agents with Scalable Long-Term Memory](https://arxiv.org/abs/2504.19413)

*Prateek Chhikara, Dev Khant, Saket Aryan, Taranjeet Singh, Deshraj Yadav*

**Main category:** cs.CL

**Keywords:** Large Language Models, Memory Management, Conversational AI, Graph-based Memory, Human-Computer Interaction

**Relevance Score:** 9

**TL;DR:** Mem0 is a memory-centric architecture designed to enhance long-term coherence in dialogues by dynamically managing memory in large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges faced by LLMs in maintaining consistency over prolonged multi-session dialogues due to fixed context windows.

**Method:** Mem0 employs a scalable memory architecture that extracts, consolidates, and retrieves salient information from ongoing conversations, with a variant using graph-based representations to manage relational structures among conversational elements.

**Key Contributions:** Introduction of Mem0 architecture for dynamic memory management in LLMs, Utilization of graph-based memory representations for complex relational conversation aspects, Significant performance improvements in memory systems across various query types

**Result:** Empirical evaluations on the LOCOMO benchmark demonstrate Mem0's superiority over existing memory-augmented systems and other baseline models, achieving 26% improvement in LLM-as-a-Judge metric and a 91% reduction in latency and token costs.

**Limitations:** 

**Future Work:** Explore further improvements in memory representation techniques and their impacts on conversational AI performance.

**Conclusion:** Mem0 enhances conversational coherence and efficiency in LLMs through structured memory mechanisms, suggesting a path forward for more effective AI agents.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable prowess in generating contextually coherent responses, yet their fixed context windows pose fundamental challenges for maintaining consistency over prolonged multi-session dialogues. We introduce Mem0, a scalable memory-centric architecture that addresses this issue by dynamically extracting, consolidating, and retrieving salient information from ongoing conversations. Building on this foundation, we further propose an enhanced variant that leverages graph-based memory representations to capture complex relational structures among conversational elements. Through comprehensive evaluations on LOCOMO benchmark, we systematically compare our approaches against six baseline categories: (i) established memory-augmented systems, (ii) retrieval-augmented generation (RAG) with varying chunk sizes and k-values, (iii) a full-context approach that processes the entire conversation history, (iv) an open-source memory solution, (v) a proprietary model system, and (vi) a dedicated memory management platform. Empirical results show that our methods consistently outperform all existing memory systems across four question categories: single-hop, temporal, multi-hop, and open-domain. Notably, Mem0 achieves 26% relative improvements in the LLM-as-a-Judge metric over OpenAI, while Mem0 with graph memory achieves around 2% higher overall score than the base configuration. Beyond accuracy gains, we also markedly reduce computational overhead compared to full-context method. In particular, Mem0 attains a 91% lower p95 latency and saves more than 90% token cost, offering a compelling balance between advanced reasoning capabilities and practical deployment constraints. Our findings highlight critical role of structured, persistent memory mechanisms for long-term conversational coherence, paving the way for more reliable and efficient LLM-driven AI agents.

</details>


### [77] [Context-Guided Dynamic Retrieval for Improving Generation Quality in RAG Models](https://arxiv.org/abs/2504.19436)

*Jacky He, Guiran Liu, Binrong Zhu, Hanlu Zhang, Hongye Zheng, Xiaokai Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Natural Questions dataset, knowledge retrieval

**Relevance Score:** 9

**TL;DR:** The paper presents a dynamic optimization method for Retrieval-Augmented Generation (RAG) that enhances semantic understanding and knowledge access in language models for open-domain question answering and generation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address limitations in static RAG structures concerning context adaptation and knowledge access.

**Method:** Introduces a state-aware dynamic knowledge retrieval mechanism using a multi-level perceptive retrieval vector and a differentiable document matching path, allowing end-to-end joint training of retrieval and generation modules.

**Key Contributions:** State-aware dynamic knowledge retrieval mechanism, Multi-level perceptive retrieval vector construction, End-to-end joint training of retrieval and generation modules

**Result:** Significant improvements in BLEU and ROUGE-L scores, and enhanced robustness and generation consistency in tasks with semantic ambiguity and multi-document fusion.

**Limitations:** 

**Future Work:** 

**Conclusion:** The proposed method broadens the application potential of RAG architectures in high-quality language generation systems.

**Abstract:** This paper focuses on the dynamic optimization of the Retrieval-Augmented Generation (RAG) architecture. It proposes a state-aware dynamic knowledge retrieval mechanism to enhance semantic understanding and knowledge scheduling efficiency in large language models for open-domain question answering and complex generation tasks. The method introduces a multi-level perceptive retrieval vector construction strategy and a differentiable document matching path. These components enable end-to-end joint training and collaborative optimization of the retrieval and generation modules. This effectively addresses the limitations of static RAG structures in context adaptation and knowledge access. Experiments are conducted on the Natural Questions dataset. The proposed structure is thoroughly evaluated across different large models, including GPT-4, GPT-4o, and DeepSeek. Comparative and ablation experiments from multiple perspectives confirm the significant improvements in BLEU and ROUGE-L scores. The approach also demonstrates stronger robustness and generation consistency in tasks involving semantic ambiguity and multi-document fusion. These results highlight its broad application potential and practical value in building high-quality language generation systems.

</details>


### [78] [Systematic Bias in Large Language Models: Discrepant Response Patterns in Binary vs. Continuous Judgment Tasks](https://arxiv.org/abs/2504.19445)

*Yi-Long Lu, Chunhui Zhang, Wei Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Response Format, Bias, Decision-Making, Sentiment Analysis

**Relevance Score:** 9

**TL;DR:** This study explores the influence of response format on the judgment biases of large language models in psychological text analyses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the reliability of large language models (LLMs) in decision-making tasks and understand how response formats can introduce biases.

**Method:** The study examined LLM responses in two tasks—a value statement judgment task and a text sentiment analysis task—utilizing binary and continuous response formats across various models.

**Key Contributions:** Demonstrated the effect of response format on LLM judgment biases., Conducted evaluations across multiple models, both open-source and commercial., Provided insights for designing LLM application tasks to minimize biases.

**Result:** The findings showed a consistent negative bias, with LLMs delivering more 'negative' judgments in binary formats than in continuous ones, confirmed by control experiments.

**Limitations:** Results may vary with different LLM architectures or other untested formats.

**Future Work:** Further exploration of response formats and their implications on other decision-making tasks involving LLMs.

**Conclusion:** Small changes in task design, like response format, can significantly impact LLM outputs and introduce systematic biases in decision-making.

**Abstract:** Large Language Models (LLMs) are increasingly used in tasks such as psychological text analysis and decision-making in automated workflows. However, their reliability remains a concern due to potential biases inherited from their training process. In this study, we examine how different response format: binary versus continuous, may systematically influence LLMs' judgments. In a value statement judgments task and a text sentiment analysis task, we prompted LLMs to simulate human responses and tested both formats across several models, including both open-source and commercial models. Our findings revealed a consistent negative bias: LLMs were more likely to deliver "negative" judgments in binary formats compared to continuous ones. Control experiments further revealed that this pattern holds across both tasks. Our results highlight the importance of considering response format when applying LLMs to decision tasks, as small changes in task design can introduce systematic biases.

</details>


### [79] [Towards Long Context Hallucination Detection](https://arxiv.org/abs/2504.19457)

*Siyi Liu, Kishaloy Halder, Zheng Qi, Wei Xiao, Nikolaos Pappas, Phu Mon Htut, Neha Anna John, Yassine Benajiba, Dan Roth*

**Main category:** cs.CL

**Keywords:** Large Language Models, Contextual Hallucination, Long-context Inputs, BERT, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper addresses the issue of contextual hallucination in large language models (LLMs) specifically for long-context inputs by proposing a new dataset and a novel architecture for improved detection.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address contextual hallucinations in LLMs, particularly when dealing with long-context inputs, which remains an open problem.

**Method:** The authors constructed a dataset aimed at long-context hallucination detection and proposed a novel architecture that employs a decomposition and aggregation mechanism for pre-trained encoder models like BERT to enhance their capability in processing long contexts.

**Key Contributions:** New dataset for long-context hallucination detection, Novel architecture for processing long contexts and detecting hallucinations, Performance improvement over existing models in terms of speed and accuracy

**Result:** The proposed architecture significantly outperforms previous models of similar size and LLM-based models across various metrics, and enables faster inference.

**Limitations:** 

**Future Work:** Future research could further refine the architecture and explore additional methods for reducing contextual hallucination in various model types.

**Conclusion:** This work lays the groundwork for improved contextual hallucination detection in LLMs using long-context inputs, demonstrating the potential for enhanced performance with the proposed methods.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable performance across various tasks. However, they are prone to contextual hallucination, generating information that is either unsubstantiated or contradictory to the given context. Although many studies have investigated contextual hallucinations in LLMs, addressing them in long-context inputs remains an open problem. In this work, we take an initial step toward solving this problem by constructing a dataset specifically designed for long-context hallucination detection. Furthermore, we propose a novel architecture that enables pre-trained encoder models, such as BERT, to process long contexts and effectively detect contextual hallucinations through a decomposition and aggregation mechanism. Our experimental results show that the proposed architecture significantly outperforms previous models of similar size as well as LLM-based models across various metrics, while providing substantially faster inference.

</details>


### [80] [BRIDGE: Benchmarking Large Language Models for Understanding Real-world Clinical Practice Text](https://arxiv.org/abs/2504.19467)

*Jiageng Wu, Bowen Gu, Ren Zhou, Kevin Xie, Doug Snyder, Yixing Jiang, Valentina Carducci, Richard Wyss, Rishi J Desai, Emily Alsentzer, Leo Anthony Celi, Adam Rodman, Sebastian Schneeweiss, Jonathan H. Chen, Santiago Romero-Brufau, Kueiyu Joshua Lin, Jie Yang*

**Main category:** cs.CL

**Keywords:** large language models, benchmarking, healthcare, electronic health records, NLP

**Relevance Score:** 9

**TL;DR:** BRIDGE is a comprehensive multilingual benchmark for evaluating large language models (LLMs) in clinical contexts, comprising 87 tasks from real-world electronic health records across nine languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive evaluations of LLMs in clinical contexts, with existing benchmarks limited in scope and generalizability.

**Method:** We created BRIDGE, a multilingual benchmark featuring 87 tasks sourced from real-world clinical data, and evaluated 52 state-of-the-art LLMs through 13,572 experiments across various inference strategies.

**Key Contributions:** Introduction of the BRIDGE benchmark for multilingual clinical evaluations, Comprehensive assessment of 52 LLMs across various healthcare applications, Insights into performance discrepancies in LLMs based on architecture and tuning

**Result:** Our evaluation revealed significant performance discrepancies among model sizes and clinical specialties, with open-source LLMs performing comparably to proprietary models, while older medically fine-tuned LLMs often underperformed.

**Limitations:** The benchmark may not capture all nuances of EHR data and is limited to the tasks included.

**Future Work:** Future research could expand the benchmark to include additional tasks and assess the performance of emerging LLMs in different clinical scenarios.

**Conclusion:** BRIDGE serves as a foundational resource for the evaluation and development of LLMs in understanding real-world clinical text, providing a leaderboard for comparative analysis.

**Abstract:** Large language models (LLMs) hold great promise for medical applications and are evolving rapidly, with new models being released at an accelerated pace. However, current evaluations of LLMs in clinical contexts remain limited. Most existing benchmarks rely on medical exam-style questions or PubMed-derived text, failing to capture the complexity of real-world electronic health record (EHR) data. Others focus narrowly on specific application scenarios, limiting their generalizability across broader clinical use. To address this gap, we present BRIDGE, a comprehensive multilingual benchmark comprising 87 tasks sourced from real-world clinical data sources across nine languages. We systematically evaluated 52 state-of-the-art LLMs (including DeepSeek-R1, GPT-4o, Gemini, and Llama 4) under various inference strategies. With a total of 13,572 experiments, our results reveal substantial performance variation across model sizes, languages, natural language processing tasks, and clinical specialties. Notably, we demonstrate that open-source LLMs can achieve performance comparable to proprietary models, while medically fine-tuned LLMs based on older architectures often underperform versus updated general-purpose models. The BRIDGE and its corresponding leaderboard serve as a foundational resource and a unique reference for the development and evaluation of new LLMs in real-world clinical text understanding.

</details>


### [81] [Conflicts in Texts: Data, Implications and Challenges](https://arxiv.org/abs/2504.19472)

*Siyi Liu, Dan Roth*

**Main category:** cs.CL

**Keywords:** NLP, conflicting information, reliability, trustworthiness, mitigation strategies

**Relevance Score:** 9

**TL;DR:** This survey discusses the issue of conflicting information in NLP models, categorizes these conflicts, and proposes strategies for developing more reliable systems.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of NLP models into real-world applications necessitates addressing the generation and reliance on conflicting information to enhance model reliability and trustworthiness.

**Method:** The paper categorizes conflicts into three areas: natural texts on the web, human-annotated data, and model interactions, analyzing each type and discussing their implications.

**Key Contributions:** Unified categorization of conflicts in NLP models, Analysis of implications of conflicting information, Proposed strategies for conflict-aware NLP systems

**Result:** The survey identifies key challenges in managing conflicting information and proposes mitigation strategies to develop conflict-aware NLP systems.

**Limitations:** The survey does not provide empirical results or case studies to validate the proposed strategies.

**Future Work:** Future research should explore more effective methods to reason over and reconcile conflicting information in NLP systems.

**Conclusion:** Addressing conflicting information is crucial for the reliability of NLP models, and future research should focus on unifying mitigation strategies.

**Abstract:** As NLP models become increasingly integrated into real-world applications, it becomes clear that there is a need to address the fact that models often rely on and generate conflicting information. Conflicts could reflect the complexity of situations, changes that need to be explained and dealt with, difficulties in data annotation, and mistakes in generated outputs. In all cases, disregarding the conflicts in data could result in undesired behaviors of models and undermine NLP models' reliability and trustworthiness. This survey categorizes these conflicts into three key areas: (1) natural texts on the web, where factual inconsistencies, subjective biases, and multiple perspectives introduce contradictions; (2) human-annotated data, where annotator disagreements, mistakes, and societal biases impact model training; and (3) model interactions, where hallucinations and knowledge conflicts emerge during deployment. While prior work has addressed some of these conflicts in isolation, we unify them under the broader concept of conflicting information, analyze their implications, and discuss mitigation strategies. We highlight key challenges and future directions for developing conflict-aware NLP systems that can reason over and reconcile conflicting information more effectively.

</details>


### [82] [Detecting Effects of AI-Mediated Communication on Language Complexity and Sentiment](https://arxiv.org/abs/2504.19556)

*Kristen Sussman, Daniel Carter*

**Main category:** cs.CL

**Keywords:** AI-mediated communication, social media analysis, sentiment analysis

**Relevance Score:** 8

**TL;DR:** This study analyzes the impact of AI-mediated communication on language patterns in tweets mentioning Donald Trump from 2020 and 2024, revealing increased sentiment positivity and changes in text complexity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI influences linguistic patterns in social media communication over time.

**Method:** The study compares 970,919 tweets from 2020 with 20,000 tweets from 2024, analyzing changes in text complexity and sentiment using Flesch-Kincaid readability and polarity scores.

**Key Contributions:** Demonstrated the influence of AI on linguistic changes on social media., Provided quantifiable data on sentiment shifts before and after AI emergence in communication., Highlighted the changing dynamics of emotional expression in political discourse on social media.

**Result:** The analysis shows a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%).

**Limitations:** 

**Future Work:** 

**Conclusion:** The findings indicate a greater presence of AI in social media communication and its effect on language and emotional expression.

**Abstract:** Given the subtle human-like effects of large language models on linguistic patterns, this study examines shifts in language over time to detect the impact of AI-mediated communication (AI- MC) on social media. We compare a replicated dataset of 970,919 tweets from 2020 (pre-ChatGPT) with 20,000 tweets from the same period in 2024, all of which mention Donald Trump during election periods. Using a combination of Flesch-Kincaid readability and polarity scores, we analyze changes in text complexity and sentiment. Our findings reveal a significant increase in mean sentiment polarity (0.12 vs. 0.04) and a shift from predominantly neutral content (54.8% in 2020 to 39.8% in 2024) to more positive expressions (28.6% to 45.9%). These findings suggest not only an increasing presence of AI in social media communication but also its impact on language and emotional expression patterns.

</details>


### [83] [m-KAILIN: Knowledge-Driven Agentic Scientific Corpus Distillation Framework for Biomedical Large Language Models Training](https://arxiv.org/abs/2504.19565)

*Meng Xiao, Xunxin Cai, Chengrui Wang, Yuanchun Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Model, Agentic AI, Dataset Distillation, Multi-agent Collaboration

**Relevance Score:** 9

**TL;DR:** The paper proposes a multi-agent framework for distilling biomedical corpora tailored for LLMs, enhancing their performance in biomedical question-answering tasks.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** Existing annotated scientific corpora are inadequate for training LLMs in biomedical research due to quality and quantity limitations.

**Method:** A collaborative multi-agent architecture where specialized agents, guided by the MeSH hierarchy, autonomously extract and synthesize data from scientific literature to generate question-answer pairs.

**Key Contributions:** Introduction of a multi-agent framework for biomedical corpus distillation, Development of an AI-Ready dataset that enhances LLM training outcomes, Validation of the framework's effectiveness through comprehensive experimental results

**Result:** Language models trained on the proposed distilled datasets significantly outperform existing LLM baselines in biomedical question-answering tasks.

**Limitations:** 

**Future Work:** 

**Conclusion:** The study demonstrates the effectiveness of multi-agent collaboration in improving biomedical LLM training results.

**Abstract:** The rapid progress of large language models (LLMs) in biomedical research has underscored the limitations of existing open-source annotated scientific corpora, which are often insufficient in quantity and quality. Addressing the challenge posed by the complex hierarchy of biomedical knowledge, we propose a knowledge-driven, multi-agent framework for scientific corpus distillation tailored for LLM training in the biomedical domain. Central to our approach is a collaborative multi-agent architecture, where specialized agents, each guided by the Medical Subject Headings (MeSH) hierarchy, work in concert to autonomously extract, synthesize, and self-evaluate high-quality textual data from vast scientific literature. These agents collectively generate and refine domain-specific question-answer pairs, ensuring comprehensive coverage and consistency with biomedical ontologies while minimizing manual involvement. Extensive experimental results show that language models trained on our multi-agent distilled datasets achieve notable improvements in biomedical question-answering tasks, outperforming both strong life sciences LLM baselines and advanced proprietary models. Notably, our AI-Ready dataset enables Llama3-70B to surpass GPT-4 with MedPrompt and Med-PaLM-2, despite their larger scale. Detailed ablation studies and case analyses further validate the effectiveness and synergy of each agent within the framework, highlighting the potential of multi-agent collaboration in biomedical LLM training.

</details>


### [84] [Arabic Metaphor Sentiment Classification Using Semantic Information](https://arxiv.org/abs/2504.19590)

*Israa Alsiyat*

**Main category:** cs.CL

**Keywords:** Sentiment classification, Arabic Metaphor Corpus, Semantic tags

**Relevance Score:** 4

**TL;DR:** This paper presents a new approach for sentiment classification of the Arabic Metaphor Corpus using semantic tags.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of Arabic online metaphors on sentiment classification and to introduce a novel tool for this purpose.

**Method:** The approach involves the development of an automatic tool that applies semantic emotional tags for sentiment classification, evaluated with standard metrics like F-score, recall, and precision.

**Key Contributions:** Introduction of a sentiment classification tool for Arabic metaphors, Application of semantic emotional tags, Evaluation using standard classification metrics

**Result:** The evaluation demonstrated the effectiveness of the tool in classifying sentiment related to Arabic metaphors, providing a new perspective on metaphor analysis in sentiment.

**Limitations:** Limited scope as it focuses solely on Arabic metaphors and may not generalize to other languages or metaphor types.

**Future Work:** Further exploration of sentiment classification methods for different languages and metaphor types, as well as enhancement of the tool's capabilities.

**Conclusion:** This approach represents the first sentiment classification for Arabic metaphors using semantic tagging, showing potential for future research in the area.

**Abstract:** In this paper, I discuss the testing of the Arabic Metaphor Corpus (AMC) [1] using newly designed automatic tools for sentiment classification for AMC based on semantic tags. The tool incorporates semantic emotional tags for sentiment classification. I evaluate the tool using standard methods, which are F-score, recall, and precision. The method is to show the impact of Arabic online metaphors on sentiment through the newly designed tools. To the best of our knowledge, this is the first approach to conduct sentiment classification for Arabic metaphors using semantic tags to find the impact of the metaphor.

</details>


### [85] [Coreference Resolution for Vietnamese Narrative Texts](https://arxiv.org/abs/2504.19606)

*Hieu-Dai Tran, Duc-Vu Nguyen, Ngan Luu-Thuy Nguyen*

**Main category:** cs.CL

**Keywords:** coreference resolution, Vietnamese, large language models, NLP, annotated dataset

**Relevance Score:** 8

**TL;DR:** This paper addresses coreference resolution in Vietnamese using a newly developed dataset and evaluates LLM performance for this task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Coreference resolution is crucial in NLP, especially for low-resource languages like Vietnamese where annotated datasets are scarce.

**Method:** Creation of a comprehensive annotated dataset from narrative texts of VnExpress; detailed annotation guidelines were established. Performance of LLMs (GPT-3.5-Turbo and GPT-4) evaluated on this dataset.

**Key Contributions:** Development of an annotated dataset for coreference resolution in Vietnamese., Evaluation of LLM performance on the dataset with findings favoring GPT-4., Establishment of annotation guidelines for entity consistency and accuracy.

**Result:** GPT-4 significantly outperformed GPT-3.5-Turbo in accuracy and response consistency for coreference resolution in Vietnamese.

**Limitations:** Limited to one language (Vietnamese) and its specific context, potential generalizability issues to other languages or datasets.

**Future Work:** Further exploration of coreference resolution in other low-resource languages and improvement of dataset diversity.

**Conclusion:** GPT-4 is a more reliable tool for coreference resolution in Vietnamese compared to GPT-3.5-Turbo.

**Abstract:** Coreference resolution is a vital task in natural language processing (NLP) that involves identifying and linking different expressions in a text that refer to the same entity. This task is particularly challenging for Vietnamese, a low-resource language with limited annotated datasets. To address these challenges, we developed a comprehensive annotated dataset using narrative texts from VnExpress, a widely-read Vietnamese online news platform. We established detailed guidelines for annotating entities, focusing on ensuring consistency and accuracy. Additionally, we evaluated the performance of large language models (LLMs), specifically GPT-3.5-Turbo and GPT-4, on this dataset. Our results demonstrate that GPT-4 significantly outperforms GPT-3.5-Turbo in terms of both accuracy and response consistency, making it a more reliable tool for coreference resolution in Vietnamese.

</details>


### [86] [VCM: Vision Concept Modeling Based on Implicit Contrastive Learning with Vision-Language Instruction Fine-Tuning](https://arxiv.org/abs/2504.19627)

*Run Luo, Renke Shan, Longze Chen, Ziqiang Liu, Lu Wang, Min Yang, Xiaobo Xia*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, self-supervised learning, visual concept modeling

**Relevance Score:** 8

**TL;DR:** VCM is a self-supervised framework for visual concept modeling that improves efficiency in LVLMs by reducing computational costs while maintaining performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LVLMs are inefficient as they process full images at the token level, unlike humans who analyze conceptual information. Improving this efficiency is crucial for real-world applications.

**Method:** VCM employs an end-to-end self-supervised learning approach, utilizing implicit contrastive learning and vision-language fine-tuning to build a visual concept model without costly annotations.

**Key Contributions:** Introduction of VCM as a novel visual concept modeling framework, Demonstrated 85% reduction in computational costs for LLaVA-1.5-7B, Enhanced performance in classic visual concept perception tasks

**Result:** VCM demonstrates a significant reduction in computational costs (85% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance in various image understanding tasks.

**Limitations:** 

**Future Work:** Further explorations on improving visual concept extraction and applying VCM to more real-world tasks.

**Conclusion:** The proposed VCM framework enhances the capabilities of visual encoders in perception tasks and validates its effectiveness through extensive experiments.

**Abstract:** Large Vision-Language Models (LVLMs) are pivotal for real-world AI tasks like embodied intelligence due to their strong vision-language reasoning abilities. However, current LVLMs process entire images at the token level, which is inefficient compared to humans who analyze information and generate content at the conceptual level, extracting relevant visual concepts with minimal effort. This inefficiency, stemming from the lack of a visual concept model, limits LVLMs' usability in real-world applications. To address this, we propose VCM, an end-to-end self-supervised visual concept modeling framework. VCM leverages implicit contrastive learning across multiple sampled instances and vision-language fine-tuning to construct a visual concept model without requiring costly concept-level annotations. Our results show that VCM significantly reduces computational costs (e.g., 85\% fewer FLOPs for LLaVA-1.5-7B) while maintaining strong performance across diverse image understanding tasks. Moreover, VCM enhances visual encoders' capabilities in classic visual concept perception tasks. Extensive quantitative and qualitative experiments validate the effectiveness and efficiency of VCM.

</details>


### [87] [A Comprehensive Part-of-Speech Tagging to Standardize Central-Kurdish Language: A Research Guide for Kurdish Natural Language Processing Tasks](https://arxiv.org/abs/2504.19645)

*Shadan Shukr Sabr, Nazira Sabr Mustafa, Talar Sabah Omar, Salah Hwayyiz Rasool, Nawzad Anwer Omer, Darya Sabir Hamad, Hemin Abdulhameed Shams, Omer Mahmood Kareem, Rozhan Noori Abdullah, Khabat Atar Abdullah, Mahabad Azad Mohammad, Haneen Al-Raghefy, Safar M. Asaad, Sara Jamal Mohammed, Twana Saeed Ali, Fazil Shawrow, Halgurd S. Maghdid*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Part-of-Speech tagging, Central-Kurdish language, Standardization, NLP resources

**Relevance Score:** 4

**TL;DR:** This paper proposes a comprehensive and standardized Part-of-Speech (POS) tagset for the low-resourced Central-Kurdish language (CKL) to enhance NLP tasks performance.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of resources and standardized POS tagging for low-resourced languages like Central-Kurdish, which hinders the development of NLP applications.

**Method:** The study involved gathering various POS tags from existing literature and Kurdish linguistic experts to create a comprehensive POS tagset specifically for CKL. It also included an initial comparison with the Universal Dependencies framework.

**Key Contributions:** Development of a comprehensive POS tagset for Central-Kurdish, Standardization of part-of-speech tags from expert consultations, Initial performance evaluation against the Universal Dependencies framework

**Result:** The proposed POS tagset improved the accuracy of CKL NLP tasks and provided a standardized way of annotating a large CKL corpus which can benefit machine translation and text recommendation.

**Limitations:** The paper primarily focuses on POS tagging and does not address other NLP aspects such as machine translation or speech recognition for CKL.

**Future Work:** Further studies may enhance NLP applications using the developed POS tagset and explore additional resources for other low-resourced languages.

**Conclusion:** A standardized POS tagset for CKL can significantly enhance the performance of various NLP applications and facilitate further development in the field for this language.

**Abstract:** - The field of natural language processing (NLP) has dramatically expanded within the last decade. Many human-being applications are conducted daily via NLP tasks, starting from machine translation, speech recognition, text generation and recommendations, Part-of-Speech tagging (POS), and Named-Entity Recognition (NER). However, low-resourced languages, such as the Central-Kurdish language (CKL), mainly remain unexamined due to shortage of necessary resources to support their development. The POS tagging task is the base of other NLP tasks; for example, the POS tag set has been used to standardized languages to provide the relationship between words among the sentences, followed by machine translation and text recommendation. Specifically, for the CKL, most of the utilized or provided POS tagsets are neither standardized nor comprehensive. To this end, this study presented an accurate and comprehensive POS tagset for the CKL to provide better performance of the Kurdish NLP tasks. The article also collected most of the POS tags from different studies as well as from Kurdish linguistic experts to standardized part-of-speech tags. The proposed POS tagset is designed to annotate a large CKL corpus and support Kurdish NLP tasks. The initial investigations of this study via comparison with the Universal Dependencies framework for standard languages, show that the proposed POS tagset can streamline or correct sentences more accurately for Kurdish NLP tasks.

</details>


### [88] [Multimodal Conditioned Diffusive Time Series Forecasting](https://arxiv.org/abs/2504.19669)

*Chen Su, Yuanhe Tian, Yan Song*

**Main category:** cs.CL

**Keywords:** multimodal, time series forecasting, diffusion model, MCD-TSF, machine learning

**Relevance Score:** 4

**TL;DR:** Proposes a multimodal conditioned diffusion model for time series forecasting, leveraging timestamps and text for enhanced predictive accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing diffusion-based approaches in time series forecasting by utilizing rich multimodal information.

**Method:** Introduces a multimodal conditioned diffusion model (MCD-TSF) that combines timestamps and texts as guidance for time series modeling.

**Key Contributions:** Introduces the MCD-TSF model for time series forecasting using multimodal data., Demonstrates superior performance on benchmark datasets compared to existing approaches., Establishes temporal and semantic correlations to enhance model predictions.

**Result:** The MCD-TSF model outperforms existing methods on real-world benchmark datasets across eight domains, achieving state-of-the-art performance.

**Limitations:** 

**Future Work:** Future research can explore further multimodal integrations and applications of the MCD-TSF model.

**Conclusion:** Leveraging multimodal information significantly improves time series forecasting accuracy and opens new avenues for research in this area.

**Abstract:** Diffusion models achieve remarkable success in processing images and text, and have been extended to special domains such as time series forecasting (TSF). Existing diffusion-based approaches for TSF primarily focus on modeling single-modality numerical sequences, overlooking the rich multimodal information in time series data. To effectively leverage such information for prediction, we propose a multimodal conditioned diffusion model for TSF, namely, MCD-TSF, to jointly utilize timestamps and texts as extra guidance for time series modeling, especially for forecasting. Specifically, Timestamps are combined with time series to establish temporal and semantic correlations among different data points when aggregating information along the temporal dimension. Texts serve as supplementary descriptions of time series' history, and adaptively aligned with data points as well as dynamically controlled in a classifier-free manner. Extensive experiments on real-world benchmark datasets across eight domains demonstrate that the proposed MCD-TSF model achieves state-of-the-art performance.

</details>


### [89] [Annif at SemEval-2025 Task 5: Traditional XMTC augmented by LLMs](https://arxiv.org/abs/2504.19675)

*Osma Suominen, Juho Inkinen, Mona Lehtinen*

**Main category:** cs.CL

**Keywords:** Subject Indexing, Large Language Models, Natural Language Processing, Machine Learning, Multilingual Contexts

**Relevance Score:** 9

**TL;DR:** The paper presents the Annif system for subject indexing using large language models in SemEval-2025, achieving top rankings in multiple evaluation categories.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve subject indexing for multilingual bibliographic records using advanced techniques in natural language processing and machine learning.

**Method:** The approach utilizes the Annif toolkit, integrating traditional NLP and ML techniques with LLM-based methods for translation, synthetic data generation, and merging predictions from monolingual models.

**Key Contributions:** Introduction of the Annif system leveraging LLMs for subject indexing., Top rankings in subject prediction categories showcase its effectiveness., Demonstration of improved accuracy in multilingual subject indexing.

**Result:** The Annif system ranked first in the all-subjects category, second in the tib-core-subjects category during quantitative evaluation, and fourth in qualitative evaluations.

**Limitations:** 

**Future Work:** Exploration of further enhancements in LLM integration for indexing tasks.

**Conclusion:** Combining traditional XMTC algorithms with modern LLM techniques can significantly enhance the accuracy and efficiency of subject indexing in multilingual contexts.

**Abstract:** This paper presents the Annif system in SemEval-2025 Task 5 (LLMs4Subjects), which focussed on subject indexing using large language models (LLMs). The task required creating subject predictions for bibliographic records from the bilingual TIBKAT database using the GND subject vocabulary. Our approach combines traditional natural language processing and machine learning techniques implemented in the Annif toolkit with innovative LLM-based methods for translation and synthetic data generation, and merging predictions from monolingual models. The system ranked first in the all-subjects category and second in the tib-core-subjects category in the quantitative evaluation, and fourth in qualitative evaluations. These findings demonstrate the potential of combining traditional XMTC algorithms with modern LLM techniques to improve the accuracy and efficiency of subject indexing in multilingual contexts.

</details>


### [90] [Taming the Titans: A Survey of Efficient LLM Inference Serving](https://arxiv.org/abs/2504.19720)

*Ranran Zhen, Juntao Li, Yixin Ji, Zhenlin Yang, Tong Liu, Qingrong Xia, Xinyu Duan, Zhefeng Wang, Baoxing Huai, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Generative AI, Inference optimization, Memory overhead, Computational demands

**Relevance Score:** 9

**TL;DR:** This paper surveys advancements in optimizing Large Language Model (LLM) inference services, addressing memory overhead and computational challenges, including instance-level and cluster-level strategies.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the significant memory and computational challenges posed by LLMs for Generative AI, ensuring low latency and high throughput for inference services.

**Method:** The paper reviews instance-level approaches like model placement and request scheduling, and cluster-level strategies such as deployment and load balancing, along with emerging scenarios and critical areas in LLM inference.

**Key Contributions:** Comprehensive review of instance-level and cluster-level optimization strategies for LLM inference., Identification of emerging scenarios and critical areas relevant to LLM serving., Outlining future research directions to enhance LLM inference performance.

**Result:** The survey highlights various optimization strategies and frameworks, providing a comprehensive overview of both established and emerging methods in LLM inference.

**Limitations:** The paper is a work in progress and may not cover all recent advancements comprehensively.

**Future Work:** Further exploration of specific tasks, modules, and auxiliary methods in LLM inference to refine and enhance services.

**Conclusion:** The insights gathered in this survey advance the understanding and improvement of LLM inference serving, with suggested future research directions to address existing limitations.

**Abstract:** Large Language Models (LLMs) for Generative AI have achieved remarkable progress, evolving into sophisticated and versatile tools widely adopted across various domains and applications. However, the substantial memory overhead caused by their vast number of parameters, combined with the high computational demands of the attention mechanism, poses significant challenges in achieving low latency and high throughput for LLM inference services. Recent advancements, driven by groundbreaking research, have significantly accelerated progress in this field. This paper provides a comprehensive survey of these methods, covering fundamental instance-level approaches, in-depth cluster-level strategies, emerging scenario directions, and other miscellaneous but important areas. At the instance level, we review model placement, request scheduling, decoding length prediction, storage management, and the disaggregation paradigm. At the cluster level, we explore GPU cluster deployment, multi-instance load balancing, and cloud service solutions. For emerging scenarios, we organize the discussion around specific tasks, modules, and auxiliary methods. To ensure a holistic overview, we also highlight several niche yet critical areas. Finally, we outline potential research directions to further advance the field of LLM inference serving.

</details>


### [91] [LLM-Assisted Automated Deductive Coding of Dialogue Data: Leveraging Dialogue-Specific Characteristics to Enhance Contextual Understanding](https://arxiv.org/abs/2504.19734)

*Ying Na, Shihui Feng*

**Main category:** cs.CL

**Keywords:** Large Language Models, automated coding, dialogue analysis, contextual challenges, education research

**Relevance Score:** 8

**TL;DR:** This study develops a novel LLM-assisted automated coding framework for dialogue data, addressing contextual challenges to improve accuracy in coding and analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of learning processes through dialogue data and enhance the accuracy of automated coding using LLMs.

**Method:** Developed a framework that predicts codes for utterances based on dialogue-specific characteristics, utilizing multiple LLMs for collaborative code prediction and implementing consistency checking for contextual accuracy.

**Key Contributions:** Novel LLM-assisted automated coding approach for dialogue data, Use of multiple LLMs for collaborative code prediction, Implementation of consistency checking using GPT-4o to improve accuracy

**Result:** Substantial accuracy improvement in coding predictions, with findings indicating that act predictions are more accurate than event predictions.

**Limitations:** Inherent challenges of understanding complex contextual information remain persistent.

**Future Work:** Further exploration of LLM capabilities for dialogue analysis and potential applications in other fields.

**Conclusion:** The proposed framework enhances the precision of automated coding of dialogue data and presents a scalable solution for tackling contextual challenges in dialogue analysis.

**Abstract:** Dialogue data has been a key source for understanding learning processes, offering critical insights into how students engage in collaborative discussions and how these interactions shape their knowledge construction. The advent of Large Language Models (LLMs) has introduced promising opportunities for advancing qualitative research, particularly in the automated coding of dialogue data. However, the inherent contextual complexity of dialogue presents unique challenges for these models, especially in understanding and interpreting complex contextual information. This study addresses these challenges by developing a novel LLM-assisted automated coding approach for dialogue data. The novelty of our proposed framework is threefold: 1) We predict the code for an utterance based on dialogue-specific characteristics -- communicative acts and communicative events -- using separate prompts following the role prompts and chain-of-thoughts methods; 2) We engaged multiple LLMs including GPT-4-turbo, GPT-4o, DeepSeek in collaborative code prediction; 3) We leveraged the interrelation between events and acts to implement consistency checking using GPT-4o. In particular, our contextual consistency checking provided a substantial accuracy improvement. We also found the accuracy of act predictions was consistently higher than that of event predictions. This study contributes a new methodological framework for enhancing the precision of automated coding of dialogue data as well as offers a scalable solution for addressing the contextual challenges inherent in dialogue analysis.

</details>


### [92] [Moral Reasoning Across Languages: The Critical Role of Low-Resource Languages in LLMs](https://arxiv.org/abs/2504.19759)

*Huichi Zhou, Zehao Xu, Munan Zhao, Kaihong Li, Yiqiang Li, Hongtao Wang*

**Main category:** cs.CL

**Keywords:** moral reasoning, multilingual NLP, large language models, low-resource languages, benchmark

**Relevance Score:** 8

**TL;DR:** This paper presents the Multilingual Moral Reasoning Benchmark (MMRB) to assess moral reasoning in large language models across various languages and contextual complexities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and understand the moral reasoning capabilities of LLMs in different languages and complexity levels, especially focusing on low-resource languages.

**Method:** The paper introduces the MMRB and tests LLMs on moral reasoning tasks across three context levels (sentence, paragraph, document) in five languages. The LLaMA-3-8B model is fine-tuned with curated monolingual data.

**Key Contributions:** Development of the Multilingual Moral Reasoning Benchmark (MMRB)., Insights on the decline of moral reasoning performance with increasing contextual complexity., Evidence of critical roles played by low-resource languages in multilingual NLP.

**Result:** Results indicate that moral reasoning performance declines with increasing context complexity, with low-resource languages like Vietnamese demonstrating significant challenges.

**Limitations:** The focus on specific languages and the reliance on multilingual datasets may not represent all possible scenarios in moral reasoning tasks.

**Future Work:** Future research could explore more diverse languages, improve the robustness of LLMs in low-resource contexts, and enhance the benchmarking methodologies.

**Conclusion:** The findings emphasize the greater impact that low-resource languages have on multilingual reasoning and underline their vital importance in the field of multilingual NLP.

**Abstract:** In this paper, we introduce the Multilingual Moral Reasoning Benchmark (MMRB) to evaluate the moral reasoning abilities of large language models (LLMs) across five typologically diverse languages and three levels of contextual complexity: sentence, paragraph, and document. Our results show moral reasoning performance degrades with increasing context complexity, particularly for low-resource languages such as Vietnamese. We further fine-tune the open-source LLaMA-3-8B model using curated monolingual data for alignment and poisoning. Surprisingly, low-resource languages have a stronger impact on multilingual reasoning than high-resource ones, highlighting their critical role in multilingual NLP.

</details>


### [93] [Can a Crow Hatch a Falcon? Lineage Matters in Predicting Large Language Model Performance](https://arxiv.org/abs/2504.19811)

*Takuya Tamura, Taro Yano, Masafumi Enomoto, Masafumi Oyamada*

**Main category:** cs.CL

**Keywords:** Large Language Models, performance prediction, lineage relationships, matrix factorization, cold-start problem

**Relevance Score:** 8

**TL;DR:** The paper introduces a Lineage-Regularized Matrix Factorization (LRMF) framework for forecasting LLM performance by incorporating ancestral ties among models, outperforming previous methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to accurately predict LLM performance to reduce computational costs and development time, particularly considering lineage relationships among models.

**Method:** A novel LRMF framework using a graph Laplacian regularizer that encodes ancestral ties among LLMs and leverages multi-hop parent-child connections.

**Key Contributions:** Introduction of the LRMF framework, Use of a graph Laplacian regularizer to encode ancestral ties, Improved performance prediction for cold-start models

**Result:** LRMF demonstrates consistent performance improvements, achieving 7-10 percentage points higher correlation with actual performance compared to conventional methods in a study of 2,934 models and 21,000+ instances.

**Limitations:** 

**Future Work:** Exploration of additional models and benchmarks to further validate the lineage-guided prediction approach.

**Conclusion:** Lineage constraints effectively improve performance predictions and address cold-start problems, offering a resource-efficient strategy for LLM development.

**Abstract:** Accurately forecasting the performance of Large Language Models (LLMs) before extensive fine-tuning or merging can substantially reduce both computational expense and development time. Although prior approaches like scaling laws account for global factors such as parameter size or training tokens, they often overlook explicit lineage relationships - i.e., which models are derived or merged from which parents. In this work, we propose a novel Lineage-Regularized Matrix Factorization (LRMF) framework that encodes ancestral ties among LLMs via a graph Laplacian regularizer. By leveraging multi-hop parent-child connections, LRMF consistently outperforms conventional matrix factorization and collaborative filtering methods in both instance-level and benchmark-level performance prediction. Our large-scale study includes 2,934 publicly available Hugging Face models and 21,000+ instances across 6 major benchmarks, showing that lineage constraints yield up to 7-10 percentage points higher correlation with actual performance compared to baselines. Moreover, LRMF effectively addresses the cold-start problem, providing accurate estimates for newly derived or merged models even with minimal data. This lineage-guided strategy thus offers a resource-efficient way to inform hyperparameter tuning, data selection, and model combination in modern LLM development.

</details>


### [94] [To MT or not to MT: An eye-tracking study on the reception by Dutch readers of different translation and creativity levels](https://arxiv.org/abs/2504.19850)

*Kyo Gerrits, Ana Guerberof-Arenas*

**Main category:** cs.CL

**Keywords:** translation modalities, cognitive load, creativity, machine translation, reader immersion

**Relevance Score:** 4

**TL;DR:** This pilot study examines how translation modality affects cognitive load in readers, focusing on creativity and errors in different translation types.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of creativity and errors in translation modalities on reader cognitive load.

**Method:** A pilot study involving eight participants who filled in a questionnaire, read a short story using an eye-tracker, and conducted retrospective think-aloud interviews under four translation conditions: machine translation, post-editing, human translation, and original source text.

**Key Contributions:** Introduction of units of creative potential (UCP) in translation studies., Novel findings on the relationship between translation creativity and cognitive load., Availability of study code and data for further research.

**Result:** Higher units of creative potential increase cognitive load, with human translation showing the highest cognitive load and machine translation the lowest; no effect of error was observed on cognitive load.

**Limitations:** Small sample size of eight participants may limit the generalizability of the results.

**Future Work:** Further research into translation creativity effects on other reader experiences and potential applications in translation practices.

**Conclusion:** Translation creativity impacts cognitive load, potentially linking higher cognitive load with increased reader enjoyment and immersion.

**Abstract:** This article presents the results of a pilot study involving the reception of a fictional short story translated from English into Dutch under four conditions: machine translation (MT), post-editing (PE), human translation (HT) and original source text (ST). The aim is to understand how creativity and errors in different translation modalities affect readers, specifically regarding cognitive load. Eight participants filled in a questionnaire, read a story using an eye-tracker, and conducted a retrospective think-aloud (RTA) interview. The results show that units of creative potential (UCP) increase cognitive load and that this effect is highest for HT and lowest for MT; no effect of error was observed. Triangulating the data with RTAs leads us to hypothesize that the higher cognitive load in UCPs is linked to increases in reader enjoyment and immersion. The effect of translation creativity on cognitive load in different translation modalities at word-level is novel and opens up new avenues for further research. All the code and data are available at https://github.com/INCREC/Pilot_to_MT_or_not_to_MT

</details>


### [95] [Efficient Domain-adaptive Continual Pretraining for the Process Industry in the German Language](https://arxiv.org/abs/2504.19856)

*Anastasia Zhukova, Christian E. Matt, Terry Ruas, Bela Gipp*

**Main category:** cs.CL

**Keywords:** domain-adaptive continual pretraining, in-context learning, k-nearest neighbors, NLP, low-resource industries

**Relevance Score:** 6

**TL;DR:** Introducing ICL-APT, a method leveraging in-context learning and kNN to enhance domain-adaptive continual pretraining while reducing GPU time and maintaining model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of obtaining domain-related data for DAPT in languages other than English, specifically in the German language for the process industry.

**Method:** ICL-APT utilizes in-context learning and k-nearest neighbors to augment target data with domain-specific texts, improving training efficiency while reducing computational demands.

**Key Contributions:** Development of ICL-APT technique, Demonstration of significant performance improvement over traditional DAPT, Reduction in computational time for training

**Result:** The ICL-APT approach outperforms traditional DAPT by 3.5 average IR metrics (mAP, MRR, and nDCG) and requires nearly 4 times less computing time.

**Limitations:** 

**Future Work:** Exploration of ICL-APT's applicability to other low-resource industries beyond the process industry.

**Conclusion:** ICL-APT provides a cost-effective solution for low-resource industries, making NLP-based solutions more accessible and feasible.

**Abstract:** Domain-adaptive continual pretraining (DAPT) is a state-of-the-art technique that further trains a language model (LM) on its pretraining task, e.g., language masking. Although popular, it requires a significant corpus of domain-related data, which is difficult to obtain for specific domains in languages other than English, such as the process industry in the German language. This paper introduces an efficient approach called ICL-augmented pretraining or ICL-APT that leverages in-context learning (ICL) and k-nearest neighbors (kNN) to augment target data with domain-related and in-domain texts, significantly reducing GPU time while maintaining strong model performance. Our results show that this approach performs better than traditional DAPT by 3.5 of the average IR metrics (e.g., mAP, MRR, and nDCG) and requires almost 4 times less computing time, providing a cost-effective solution for industries with limited computational capacity. The findings highlight the broader applicability of this framework to other low-resource industries, making NLP-based solutions more accessible and feasible in production environments.

</details>


### [96] [semi-PD: Towards Efficient LLM Serving via Phase-Wise Disaggregated Computation and Unified Storage](https://arxiv.org/abs/2504.19867)

*Ke Hong, Lufang Chen, Zhong Wang, Xiuhong Li, Qiuli Mao, Jianping Ma, Chao Xiong, Guanyu Wu, Buhe Han, Guohao Dai, Yun Liang, Yu Wang*

**Main category:** cs.CL

**Keywords:** large language models, disaggregated computation, unified storage, resource management, latency optimization

**Relevance Score:** 8

**TL;DR:** This paper introduces semi-PD, a novel LLM serving system that utilizes disaggregated computation with unified storage to improve serving performance, particularly under high request rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLM serving systems suffer from storage inefficiencies and latency issues, especially in disaggregated systems.

**Method:** The paper proposes a new architecture, semi-PD, which employs a computation resource controller and a unified memory manager for better resource management and performance optimization.

**Key Contributions:** Introduction of semi-PD system characterized by disaggregated computation and unified storage, Development of a low-overhead resource adjustment mechanism, Implementation of an SLO-aware dynamic partitioning algorithm

**Result:** semi-PD reduces average end-to-end latency per request by 1.27-2.58 times on DeepSeek models, and increases request handling capacity by 1.55-1.72 times on Llama models while maintaining latency constraints.

**Limitations:** 

**Future Work:** 

**Conclusion:** The semi-PD system significantly enhances performance for LLM serving, mitigating the common storage and latency issues associated with existing approaches.

**Abstract:** Existing large language model (LLM) serving systems fall into two categories: 1) a unified system where prefill phase and decode phase are co-located on the same GPU, sharing the unified computational resource and storage, and 2) a disaggregated system where the two phases are disaggregated to different GPUs. The design of the disaggregated system addresses the latency interference and sophisticated scheduling issues in the unified system but leads to storage challenges including 1) replicated weights for both phases that prevent flexible deployment, 2) KV cache transfer overhead between the two phases, 3) storage imbalance that causes substantial wasted space of the GPU capacity, and 4) suboptimal resource adjustment arising from the difficulties in migrating KV cache. Such storage inefficiency delivers poor serving performance under high request rates.   In this paper, we identify that the advantage of the disaggregated system lies in the disaggregated computation, i.e., partitioning the computational resource to enable the asynchronous computation of two phases. Thus, we propose a novel LLM serving system, semi-PD, characterized by disaggregated computation and unified storage. In semi-PD, we introduce a computation resource controller to achieve disaggregated computation at the streaming multi-processor (SM) level, and a unified memory manager to manage the asynchronous memory access from both phases. semi-PD has a low-overhead resource adjustment mechanism between the two phases, and a service-level objective (SLO) aware dynamic partitioning algorithm to optimize the SLO attainment. Compared to state-of-the-art systems, semi-PD maintains lower latency at higher request rates, reducing the average end-to-end latency per request by 1.27-2.58x on DeepSeek series models, and serves 1.55-1.72x more requests adhering to latency constraints on Llama series models.

</details>


### [97] [GenCLS++: Pushing the Boundaries of Generative Classification in LLMs Through Comprehensive SFT and RL Studies Across Diverse Datasets](https://arxiv.org/abs/2504.19898)

*Mingqian He, Fei Zhao, Chonggang Lu, Ziyan Liu, Yue Wang, Haofu Qian*

**Main category:** cs.CL

**Keywords:** text classification, generative models, reinforcement learning, large language models, supervised fine-tuning

**Relevance Score:** 9

**TL;DR:** GenCLS++ is a framework for generative text classification that optimizes supervised fine-tuning (SFT) and reinforcement learning (RL) while exploring various strategies, leading to significant accuracy improvements over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capabilities of Large Language Models (LLMs) in text classification by addressing shortcomings in traditional discriminative methods and leveraging the strengths of generative approaches.

**Method:** The authors propose GenCLS++, a framework that jointly optimizes SFT and RL while exploring five high-level strategy dimensions during both training and inference stages.

**Key Contributions:** Introduction of GenCLS++, a unified framework for generative text classification combining SFT and RL., Systematic exploration of five high-level strategy dimensions for improved classification performance., Empirical evidence showing that classification tasks perform better without reasoning steps, contrasting with reasoning-intensive tasks.

**Result:** GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline across seven datasets, with an increase to 4.00% on public datasets.

**Limitations:** Further exploration into other potential dimensions and strategies in RL and generative classification is needed.

**Future Work:** Investigation into alternative training methodologies and their effects on generative classification tasks.

**Conclusion:** The findings indicate that classification tasks do not benefit from explicit reasoning processes, offering new insights for future LLM applications.

**Abstract:** As a fundamental task in machine learning, text classification plays a crucial role in many areas. With the rapid scaling of Large Language Models (LLMs), particularly through reinforcement learning (RL), there is a growing need for more capable discriminators. Consequently, advances in classification are becoming increasingly vital for enhancing the overall capabilities of LLMs. Traditional discriminative methods map text to labels but overlook LLMs' intrinsic generative strengths. Generative classification addresses this by prompting the model to directly output labels. However, existing studies still rely on simple SFT alone, seldom probing the interplay between training and inference prompts, and no work has systematically leveraged RL for generative text classifiers and unified SFT, RL, and inference-time prompting in one framework. We bridge this gap with GenCLS++, a framework that jointly optimizes SFT and RL while systematically exploring five high-level strategy dimensions-in-context learning variants, category definitions, explicit uncertainty labels, semantically irrelevant numeric labels, and perplexity-based decoding-during both training and inference. After an SFT "policy warm-up," we apply RL with a simple rule-based reward, yielding sizable extra gains. Across seven datasets, GenCLS++ achieves an average accuracy improvement of 3.46% relative to the naive SFT baseline; on public datasets, this improvement rises to 4.00%. Notably, unlike reasoning-intensive tasks that benefit from explicit thinking processes, we find that classification tasks perform better without such reasoning steps. These insights into the role of explicit reasoning provide valuable guidance for future LLM applications.

</details>


### [98] [Assessing the Potential of Generative Agents in Crowdsourced Fact-Checking](https://arxiv.org/abs/2504.19940)

*Luigia Costabile, Gian Marco Orlando, Valerio La Gatta, Vincenzo Moscato*

**Main category:** cs.CL

**Keywords:** fact-checking, crowdsourcing, generative agents, machine learning, bias reduction

**Relevance Score:** 8

**TL;DR:** This paper explores the use of LLM-powered generative agents in crowdsourced fact-checking, demonstrating their superiority over human crowds in truthfulness classification and bias reduction.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in online misinformation necessitates effective, scalable fact-checking solutions. Crowdsourced methods are gaining popularity despite concerns about quality and bias.

**Method:** The study simulates crowds composed of generative agents with varied demographic and ideological backgrounds, which evaluate claims and retrieve evidence for fact-checking tasks.

**Key Contributions:** Demonstration of generative agents enhancing fact-checking tasks., Evidence of superior performance of agents over humans in veracity assessments., Identification of structured decision-making processes in agents.

**Result:** Generative agents outperform human crowds in truthfulness classification, show higher consistency, and are less biased, using more structured criteria such as Accuracy and Precision.

**Limitations:** The study mainly focuses on simulations rather than real-world applications, limiting its generalizability.

**Future Work:** Explore the real-world implementation of LLM-powered agents in fact-checking processes and their integration with existing systems.

**Conclusion:** Generative agents can be valuable contributors to fact-checking, providing scalability and reduced bias in crowd-based systems.

**Abstract:** The growing spread of online misinformation has created an urgent need for scalable, reliable fact-checking solutions. Crowdsourced fact-checking - where non-experts evaluate claim veracity - offers a cost-effective alternative to expert verification, despite concerns about variability in quality and bias. Encouraged by promising results in certain contexts, major platforms such as X (formerly Twitter), Facebook, and Instagram have begun shifting from centralized moderation to decentralized, crowd-based approaches.   In parallel, advances in Large Language Models (LLMs) have shown strong performance across core fact-checking tasks, including claim detection and evidence evaluation. However, their potential role in crowdsourced workflows remains unexplored. This paper investigates whether LLM-powered generative agents - autonomous entities that emulate human behavior and decision-making - can meaningfully contribute to fact-checking tasks traditionally reserved for human crowds. Using the protocol of La Barbera et al. (2024), we simulate crowds of generative agents with diverse demographic and ideological profiles. Agents retrieve evidence, assess claims along multiple quality dimensions, and issue final veracity judgments.   Our results show that agent crowds outperform human crowds in truthfulness classification, exhibit higher internal consistency, and show reduced susceptibility to social and cognitive biases. Compared to humans, agents rely more systematically on informative criteria such as Accuracy, Precision, and Informativeness, suggesting a more structured decision-making process. Overall, our findings highlight the potential of generative agents as scalable, consistent, and less biased contributors to crowd-based fact-checking systems.

</details>


### [99] [TD-EVAL: Revisiting Task-Oriented Dialogue Evaluation by Combining Turn-Level Precision with Dialogue-Level Comparisons](https://arxiv.org/abs/2504.19982)

*Emre Can Acikgoz, Carl Guo, Suvodip Dey, Akul Datta, Takyoung Kim, Gokhan Tur, Dilek Hakkani-Tür*

**Main category:** cs.CL

**Keywords:** Task-oriented dialogue, Evaluation framework, Large Language Models, Dialogue systems, Human judgments

**Relevance Score:** 8

**TL;DR:** This paper introduces TD-EVAL, a two-step evaluation framework for task-oriented dialogue systems that combines turn-level analysis and dialogue-level comparisons for improved evaluation accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is the need for better evaluation methodologies for task-oriented dialogue systems as they evolve with the increasing capabilities of Large Language Models, which traditional metrics do not adequately address.

**Method:** The paper proposes TD-EVAL, which evaluates dialogue interactions in two steps: first through fine-grained analysis of individual responses using three dimensions, and second through holistic comparisons of dialogues using pairwise assessments.

**Key Contributions:** Introduction of TD-EVAL framework for dialogue evaluation, Fine-grained turn-level analysis alongside holistic assessments, Demonstrated alignment with human judgment compared to existing metrics

**Result:** Experiments on MultiWOZ 2.4 and τ-Bench show that TD-EVAL identifies critical conversational errors missed by conventional metrics and aligns better with human judgments.

**Limitations:** 

**Future Work:** Future research can explore applying TD-EVAL in different contexts and integrating it with varying dialogue systems to further validate its applicability and robustness.

**Conclusion:** TD-EVAL presents a new approach for evaluating task-oriented dialogue systems, balancing both the granularity of turn analysis and the holistic view of dialogue quality, offering a versatile framework for future research.

**Abstract:** Task-oriented dialogue (TOD) systems are experiencing a revolution driven by Large Language Models (LLMs), yet the evaluation methodologies for these systems remain insufficient for their growing sophistication. While traditional automatic metrics effectively assessed earlier modular systems, they focus solely on the dialogue level and cannot detect critical intermediate errors that can arise during user-agent interactions. In this paper, we introduce TD-EVAL (Turn and Dialogue-level Evaluation), a two-step evaluation framework that unifies fine-grained turn-level analysis with holistic dialogue-level comparisons. At turn level, we evaluate each response along three TOD-specific dimensions: conversation cohesion, backend knowledge consistency, and policy compliance. Meanwhile, we design TOD Agent Arena that uses pairwise comparisons to provide a measure of dialogue-level quality. Through experiments on MultiWOZ 2.4 and {\tau}-Bench, we demonstrate that TD-EVAL effectively identifies the conversational errors that conventional metrics miss. Furthermore, TD-EVAL exhibits better alignment with human judgments than traditional and LLM-based metrics. These findings demonstrate that TD-EVAL introduces a new paradigm for TOD system evaluation, efficiently assessing both turn and system levels with a plug-and-play framework for future research.

</details>


### [100] [Knowledge Distillation of Domain-adapted LLMs for Question-Answering in Telecom](https://arxiv.org/abs/2504.20000)

*Rishika Sen, Sujoy Roychowdhury, Sumit Soman, H. G. Ranjani, Srikhetra Mohanty*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Large Language Models, Supervised Fine-tuning, Question-Answering, Telecom

**Relevance Score:** 8

**TL;DR:** This paper investigates knowledge distillation (KD) in telecom domain QA tasks, focusing on the impact of fine-tuning teacher and student LLMs and vocabulary on model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the effects of domain adaptation through Knowledge Distillation in large language models, particularly in the telecom sector's Question-Answering applications.

**Method:** The study conducts experiments comparing Supervised Fine-tuning (SFT) of teacher models, student models, and both, while assessing different vocabularies and KD algorithms (vanilla KD and Dual Space KD).

**Key Contributions:** Systematic exploration of fine-tuning strategies for KD in QA tasks., Multi-metric evaluation of model performance post-distillation., Insights on the influence of vocabulary on the effectiveness of KD.

**Result:** The results indicate that fine-tuning the teacher model enhances the performance of the distilled model with the same vocabulary; however, using both teacher and student models for fine-tuning consistently yields better outcomes across all evaluation metrics.

**Limitations:** The study is limited to telecom domain applications, and results might vary with different domains or tasks beyond QA.

**Future Work:** Future research can explore additional domains and tasks, as well as variations in KD algorithms and their interactions with different model architectures.

**Conclusion:** SFT of both teacher and student models improves performance overall, with the statistical significance influenced by the vocabulary used in the teacher models.

**Abstract:** Knowledge Distillation (KD) is one of the approaches to reduce the size of Large Language Models (LLMs). A LLM with smaller number of model parameters (student) is trained to mimic the performance of a LLM of a larger size (teacher model) on a specific task. For domain-specific tasks, it is not clear if teacher or student model, or both, must be considered for domain adaptation. In this work, we study this problem from perspective of telecom domain Question-Answering (QA) task. We systematically experiment with Supervised Fine-tuning (SFT) of teacher only, SFT of student only and SFT of both prior to KD. We design experiments to study the impact of vocabulary (same and different) and KD algorithms (vanilla KD and Dual Space KD, DSKD) on the distilled model. Multi-faceted evaluation of the distillation using 14 different metrics (N-gram, embedding and LLM-based metrics) is considered. Experimental results show that SFT of teacher improves performance of distilled model when both models have same vocabulary, irrespective of algorithm and metrics. Overall, SFT of both teacher and student results in better performance across all metrics, although the statistical significance of the same depends on the vocabulary of the teacher models.

</details>


### [101] [LLM-Generated Fake News Induces Truth Decay in News Ecosystem: A Case Study on Neural News Recommendation](https://arxiv.org/abs/2504.20013)

*Beizhe Hu, Qiang Sheng, Juan Cao, Yang Li, Danding Wang*

**Main category:** cs.CL

**Keywords:** fake news, large language models, news recommendation, truth decay, news ecosystem

**Relevance Score:** 7

**TL;DR:** The study explores the impact of LLM-generated fake news on neural news recommendation systems, revealing a phenomenon termed 'truth decay.'

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the challenges posed by LLM-generated fake news on the news ecosystem and its moderation.

**Method:** Developed a simulation pipeline and a dataset containing approximately 56,000 generated news articles of various types.

**Key Contributions:** Development of a simulation pipeline to analyze LLM-specific impacts on news ecosystems., Identification of 'truth decay' in news rankings caused by LLM-generated news., Correlation findings between news perplexity and its ranking in recommendation systems.

**Result:** Real news rankings deteriorate ('truth decay') as LLM-generated news infiltrates news recommendation systems, showing a correlation between perplexity and news ranking.

**Limitations:** The study primarily focuses on simulated environments; real-world implications may vary.

**Future Work:** Investigation of additional countermeasures against LLM-generated fake news and their efficacy in real-world applications.

**Conclusion:** LLM-generated fake news poses significant threats to the integrity of news ecosystems, necessitating proactive measures from stakeholders.

**Abstract:** Online fake news moderation now faces a new challenge brought by the malicious use of large language models (LLMs) in fake news production. Though existing works have shown LLM-generated fake news is hard to detect from an individual aspect, it remains underexplored how its large-scale release will impact the news ecosystem. In this study, we develop a simulation pipeline and a dataset with ~56k generated news of diverse types to investigate the effects of LLM-generated fake news within neural news recommendation systems. Our findings expose a truth decay phenomenon, where real news is gradually losing its advantageous position in news ranking against fake news as LLM-generated news is involved in news recommendation. We further provide an explanation about why truth decay occurs from a familiarity perspective and show the positive correlation between perplexity and news ranking. Finally, we discuss the threats of LLM-generated fake news and provide possible countermeasures. We urge stakeholders to address this emerging challenge to preserve the integrity of news ecosystems.

</details>


### [102] [Better To Ask in English? Evaluating Factual Accuracy of Multilingual LLMs in English and Low-Resource Languages](https://arxiv.org/abs/2504.20022)

*Pritika Rohera, Chaitrali Ginimav, Gayatri Sawant, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Factual accuracy, Indic languages, Hallucination, Language models

**Relevance Score:** 8

**TL;DR:** The study assesses the factual accuracy of multilingual LLMs in English and 19 Indic languages using the IndicQuest dataset, finding better performance in English and increased hallucination in Indic responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the factual accuracy of LLMs in low-resource Indic languages compared to high-resource English, as existing studies have primarily focused on the latter.

**Method:** The paper compares the performance of various LLMs (GPT-4o, Gemma-2-9B, Gemma-2-2B, Llama-3.1-8B) on the IndicQuest dataset, which contains question-answer pairs in both English and Indic languages, analyzing responses to the same questions in both language contexts.

**Key Contributions:** Assessment of factual accuracy for LLMs across multiple languages including low-resource Indic languages, Identification of a performance gap in LLMs between English and Indic contexts, Highlighting the problem of hallucination in language model responses for low-resource languages

**Result:** LLMs generally perform better in English than in Indic languages and show a higher tendency for hallucination in responses generated in low-resource languages.

**Limitations:** The study is limited to only a subset of LLMs and does not explore more extensive datasets or a wider range of low-resource languages.

**Future Work:** Future research could expand the dataset to include more low-resource languages and explore improvements in LLM training for better multilingual support.

**Conclusion:** The findings indicate that current LLMs struggle with factual accuracy in low-resource languages, presenting challenges in multilingual understanding and contextual accuracy.

**Abstract:** Multilingual Large Language Models (LLMs) have demonstrated significant effectiveness across various languages, particularly in high-resource languages such as English. However, their performance in terms of factual accuracy across other low-resource languages, especially Indic languages, remains an area of investigation. In this study, we assess the factual accuracy of LLMs - GPT-4o, Gemma-2-9B, Gemma-2-2B, and Llama-3.1-8B - by comparing their performance in English and Indic languages using the IndicQuest dataset, which contains question-answer pairs in English and 19 Indic languages. By asking the same questions in English and their respective Indic translations, we analyze whether the models are more reliable for regional context questions in Indic languages or when operating in English. Our findings reveal that LLMs often perform better in English, even for questions rooted in Indic contexts. Notably, we observe a higher tendency for hallucination in responses generated in low-resource Indic languages, highlighting challenges in the multilingual understanding capabilities of current LLMs.

</details>


### [103] [AutoJudge: Judge Decoding Without Manual Annotation](https://arxiv.org/abs/2504.20039)

*Roman Garipov, Fedor Velikonivtsev, Ruslan Svirschevski, Vage Egiazarian, Max Ryabinin*

**Main category:** cs.CL

**Keywords:** AutoJudge, speculative decoding, LLM inference, accuracy, token generation

**Relevance Score:** 9

**TL;DR:** AutoJudge is a framework for accelerating LLM inference by using task-specific lossy speculative decoding, allowing for faster generation of 'unimportant' tokens while maintaining quality.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the speed of large language model inference by differentiating between important and unimportant tokens in the output.

**Method:** The framework utilizes a semi-greedy search algorithm and a lightweight classifier to predict which tokens can be safely accepted or skipped during inference.

**Key Contributions:** Introduction of task-specific lossy speculative decoding for LLMs., Development of a semi-greedy search algorithm for token acceptance., Creation of a lightweight classifier based on LLM embeddings for inference optimization.

**Result:** AutoJudge achieved up to 1.5x more accepted tokens per verification cycle in zero-shot GSM8K reasoning with under 1% accuracy degradation, and over 2x speedup with small accuracy loss in programming tasks on the LiveCodeBench benchmark.

**Limitations:** The performance may vary with different model architectures and tasks; further validation is needed.

**Future Work:** Exploration of broader applications of the AutoJudge framework and improvements on other model architectures.

**Conclusion:** The approach demonstrates significant performance improvements in LLM inference speed while maintaining high accuracy and generalizing well across different tasks.

**Abstract:** We introduce AutoJudge, a framework that accelerates large language model (LLM) inference with task-specific lossy speculative decoding. Instead of matching the original model output distribution token-by-token, we identify which of the generated tokens affect the downstream quality of the generated response, relaxing the guarantee so that the "unimportant" tokens can be generated faster. Our approach relies on a semi-greedy search algorithm to test which of the mismatches between target and draft model should be corrected to preserve quality, and which ones may be skipped. We then train a lightweight classifier based on existing LLM embeddings to predict, at inference time, which mismatching tokens can be safely accepted without compromising the final answer quality. We test our approach with Llama 3.2 1B (draft) and Llama 3.1 8B (target) models on zero-shot GSM8K reasoning, where it achieves up to 1.5x more accepted tokens per verification cycle with under 1% degradation in answer accuracy compared to standard speculative decoding and over 2x with small loss in accuracy. When applied to the LiveCodeBench benchmark, our approach automatically detects other, programming-specific important tokens and shows similar speedups, demonstrating its ability to generalize across tasks.

</details>


### [104] [A Bayesian approach to modeling topic-metadata relationships](https://arxiv.org/abs/2104.02496)

*P. Schulze, S. Wiegrebe, P. W. Thurner, C. Heumann, M. Aßenmacher*

**Main category:** cs.CL

**Keywords:** topic modeling, Bayesian statistics, social media analysis

**Relevance Score:** 4

**TL;DR:** This paper improves topic modeling by refining the method of composition and adopting a fully Bayesian approach for estimating relationships between topics and metadata.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the estimation of relationships between latent topic structures and relevant metadata in topic modeling.

**Method:** The paper refines the method of composition by replacing linear regression with Beta regression and adopts a fully Bayesian framework for stability and uncertainty quantification.

**Key Contributions:** Refinement of the method of composition by using Beta regression instead of linear regression., Full Bayesian approach for improved estimation and uncertainty quantification in topic modeling.

**Result:** The proposed methodology demonstrates improved estimation of topic proportions in relation to metadata, specifically applied to Twitter posts by German parliamentarians.

**Limitations:** 

**Future Work:** 

**Conclusion:** The enhancements lead to better interpretation of relationships in topic modeling, providing a clearer understanding of how topics relate to covariates.

**Abstract:** The objective of advanced topic modeling is not only to explore latent topical structures, but also to estimate relationships between the discovered topics and theoretically relevant metadata. Methods used to estimate such relationships must take into account that the topical structure is not directly observed, but instead being estimated itself in an unsupervised fashion, usually by common topic models. A frequently used procedure to achieve this is the method of composition, a Monte Carlo sampling technique performing multiple repeated linear regressions of sampled topic proportions on metadata covariates. In this paper, we propose two modifications of this approach: First, we substantially refine the existing implementation of the method of composition from the R package stm by replacing linear regression with the more appropriate Beta regression. Second, we provide a fundamental enhancement of the entire estimation framework by substituting the current blending of frequentist and Bayesian methods with a fully Bayesian approach. This allows for a more appropriate quantification of uncertainty. We illustrate our improved methodology by investigating relationships between Twitter posts by German parliamentarians and different metadata covariates related to their electoral districts, using the Structural Topic Model to estimate topic proportions.

</details>


### [105] [Understanding Dataset Difficulty with $\mathcal{V}$-Usable Information](https://arxiv.org/abs/2110.08420)

*Kawin Ethayarajh, Yejin Choi, Swabha Swayamdipta*

**Main category:** cs.CL

**Keywords:** Dataset Difficulty, Usable Information, Pointwise Information, NLP, Interpretability

**Relevance Score:** 7

**TL;DR:** This paper introduces a framework to estimate dataset difficulty by analyzing model-specific usable information and individual instance difficulty, allowing for better understanding and interpretability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a deeper understanding of dataset difficulty beyond state-of-the-art comparisons, focusing on instance-level attributes for a given model.

**Method:** The authors frame dataset difficulty in terms of $	extit{usable information}$ specific to a model, introducing pointwise $	extit{V}$-information (PVI) for assessing individual instances.

**Key Contributions:** Introduction of $	extit{usable information}$ for model-specific dataset difficulty estimation, Development of pointwise $	extit{V}$-information (PVI) for individual instance analysis, Framework allows for interpretability of attributes and insights into dataset characteristics.

**Result:** The framework enables the comparison of different datasets and instances for a given model, revealing insights into their difficulty and allowing for the interpretation of input attributes.

**Limitations:** The applicability of the framework may vary across different models and datasets not examined in this study.

**Future Work:** Exploration of broader applications of the framework to other domains and model comparisons.

**Conclusion:** The proposed methods enhance our understanding of dataset difficulties in NLP and enable discovery of potential annotation artefacts in existing benchmarks.

**Abstract:** Estimating the difficulty of a dataset typically involves comparing state-of-the-art models to humans; the bigger the performance gap, the harder the dataset is said to be. However, this comparison provides little understanding of how difficult each instance in a given distribution is, or what attributes make the dataset difficult for a given model. To address these questions, we frame dataset difficulty -- w.r.t. a model $\mathcal{V}$ -- as the lack of $\mathcal{V}$-$\textit{usable information}$ (Xu et al., 2019), where a lower value indicates a more difficult dataset for $\mathcal{V}$. We further introduce $\textit{pointwise $\mathcal{V}$-information}$ (PVI) for measuring the difficulty of individual instances w.r.t. a given distribution. While standard evaluation metrics typically only compare different models for the same dataset, $\mathcal{V}$-$\textit{usable information}$ and PVI also permit the converse: for a given model $\mathcal{V}$, we can compare different datasets, as well as different instances/slices of the same dataset. Furthermore, our framework allows for the interpretability of different input attributes via transformations of the input, which we use to discover annotation artefacts in widely-used NLP benchmarks.

</details>


### [106] [Cognitive and Cultural Topology of Linguistic Categories:A Semantic-Pragmatic Metric Approach](https://arxiv.org/abs/2112.06876)

*Eugene Yu Ji*

**Main category:** cs.CL

**Keywords:** NLP, Semantic Typicality, Pragmatic Salience

**Relevance Score:** 6

**TL;DR:** This paper presents a novel geometric metric for analyzing semantic and pragmatic features in NLP using word co-occurrence patterns, demonstrating significant improvements in mapping basic-level categories pertinent to both cognitive and socio-cultural dimensions.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in modeling semantic and pragmatic dimensions in NLP, particularly the interaction between these features and the lack of interdisciplinary insights.

**Method:** The study introduces a geometric metric that utilizes word co-occurrence patterns to map semantic typicality and pragmatic salience in a two-dimensional hyperbolic space.

**Key Contributions:** Novel geometric metric for semantic and pragmatic analysis, Implementation of interpretable language embedding models, Demonstration of cognitive-cultural interplay in linguistic categories

**Result:** The semantic-pragmatic metric developed produces superior mappings for basic-level categories, achieving better performance than traditional cognitive semantics benchmarks and showing strong socio-cultural relevance.

**Limitations:** 

**Future Work:** 

**Conclusion:** The findings suggest that basic-level categories should be analyzed through both semantic and pragmatic lenses, emphasizing their dual role in cognitive and cultural contexts, leading to the development of interpretable, human-centric language embedding models.

**Abstract:** In recent years, the field of NLP has seen growing interest in modeling both semantic and pragmatic dimensions. Despite this progress, two key challenges persist: firstly, the complex task of mapping and analyzing the interactions between semantic and pragmatic features; secondly, the insufficient incorporation of relevant insights from related disciplines outside NLP. Addressing these issues, this study introduces a novel geometric metric that utilizes word co-occurrence patterns. This metric maps two fundamental properties - semantic typicality (cognitive) and pragmatic salience (socio-cultural) - for basic-level categories within a two-dimensional hyperbolic space. Our evaluations reveal that this semantic-pragmatic metric produces mappings for basic-level categories that not only surpass traditional cognitive semantics benchmarks but also demonstrate significant socio-cultural relevance. This finding proposes that basic-level categories, traditionally viewed as semantics-driven cognitive constructs, should be examined through the lens of both semantic and pragmatic dimensions, highlighting their role as a cognitive-cultural interface. The broad contribution of this paper lies in the development of medium-sized, interpretable, and human-centric language embedding models, which can effectively blend semantic and pragmatic dimensions to elucidate both the cognitive and socio-cultural significance of linguistic categories.

</details>


### [107] [Generative Meta-Learning for Zero-Shot Relation Triplet Extraction](https://arxiv.org/abs/2305.01920)

*Wanli Li, Tieyun Qian, Yi Song, Zeyu Zhang, Jiawei Li, Zhuang Chen, Lixin Zou*

**Main category:** cs.CL

**Keywords:** Zero-shot Relation Extraction, bi-level optimization, meta-learning, generalization, pre-trained language models

**Relevance Score:** 6

**TL;DR:** This paper proposes a generative meta-learning framework for Zero-shot Relation Triplet Extraction (ZeroRTE) that enhances generalization capabilities using bi-level optimization and pre-trained language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the generalization performance of models in ZeroRTE, which aims to extract relation triplets from texts with unseen relation types, leveraging the challenges posed by limited generalization capabilities in existing methods.

**Method:** The paper introduces a bi-level optimization (BLO) approach combined with pre-trained language models, employing a generative meta-learning framework that focuses on two loss levels: an upper-level loss for generalization and a lower-level loss for data fitting.

**Key Contributions:** Integration of bi-level optimization with pre-trained language models for improved generalization in ZeroRTE., Development of three generative meta-learning methods tailored to different categories of meta-learning., Comprehensive experimental results showcasing the framework's effectiveness in targeted tasks.

**Result:** Extensive experimental results show that the proposed framework significantly improves performance on the ZeroRTE task compared to existing methods.

**Limitations:** 

**Future Work:** Future research directions include exploring further enhancements in generalization techniques and applying the framework to additional natural language processing tasks.

**Conclusion:** The proposed generative meta-learning framework effectively boosts the generalization capabilities of models in ZeroRTE, demonstrating better performance and providing a new approach for relation extraction tasks.

**Abstract:** Zero-shot Relation Triplet Extraction (ZeroRTE) aims to extract relation triplets from texts containing unseen relation types. This capability benefits various downstream information retrieval (IR) tasks. The primary challenge lies in enabling models to generalize effectively to unseen relation categories. Existing approaches typically leverage the knowledge embedded in pre-trained language models to accomplish the generalization process. However, these methods focus solely on fitting the training data during training, without specifically improving the model's generalization performance, resulting in limited generalization capability. For this reason, we explore the integration of bi-level optimization (BLO) with pre-trained language models for learning generalized knowledge directly from the training data, and propose a generative meta-learning framework which exploits the `learning-to-learn' ability of meta-learning to boost the generalization capability of generative models.   Specifically, we introduce a BLO approach that simultaneously addresses data fitting and generalization. This is achieved by constructing an upper-level loss to focus on generalization and a lower-level loss to ensure accurate data fitting. Building on this, we subsequently develop three generative meta-learning methods, each tailored to a distinct category of meta-learning. Extensive experimental results demonstrate that our framework performs well on the ZeroRTE task. Our code is available at https://github.com/leeworry/TGM-MetaLearning.

</details>


### [108] [Benchmarking large language models for biomedical natural language processing applications and recommendations](https://arxiv.org/abs/2305.16326)

*Qingyu Chen, Yan Hu, Xueqing Peng, Qianqian Xie, Qiao Jin, Aidan Gilson, Maxwell B. Singer, Xuguang Ai, Po-Ting Lai, Zhizheng Wang, Vipina Kuttichi Keloth, Kalpana Raja, Jiming Huang, Huan He, Fongci Lin, Jingcheng Du, Rui Zhang, W. Jim Zheng, Ron A. Adelman, Zhiyong Lu, Hua Xu*

**Main category:** cs.CL

**Keywords:** Biomedical Natural Language Processing, Large Language Models, Knowledge Curation

**Relevance Score:** 9

**TL;DR:** This paper evaluates the performance of four large language models (LLMs) in biomedical natural language processing (BioNLP) tasks and compares them to traditional fine-tuning approaches.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid growth of biomedical literature requires effective automation for knowledge curation and synthesis, where BioNLP plays a critical role. Understanding the effectiveness of LLMs in this domain is essential for future applications.

**Method:** A systematic evaluation of four LLMs (GPT and LLaMA), tested on 12 BioNLP benchmarks across six applications, focusing on zero-shot, few-shot, and fine-tuning performance compared to traditional models like BERT and BART.

**Key Contributions:** Systematic evaluation of LLMs in BioNLP tasks, Comparison of LLM performance against traditional models, Identification of issues like hallucinations in LLM outputs

**Result:** Traditional fine-tuning methods outperform zero or few-shot approaches of LLMs in most tasks, but closed-source models like GPT-4 show superior performance in reasoning-related tasks. Open-source models need fine-tuning to perform comparably.

**Limitations:** The evaluation is limited to specific LLMs and benchmarks, and results may not generalize across all BioNLP tasks or datasets.

**Future Work:** Further investigations into improving open-source LLMs and addressing their limitations in BioNLP applications.

**Conclusion:** While traditional models generally outperform LLMs in BioNLP tasks, LLMs like GPT-4 can excel in specific areas, highlighting the need for careful application and further improvement of open-source models.

**Abstract:** The rapid growth of biomedical literature poses challenges for manual knowledge curation and synthesis. Biomedical Natural Language Processing (BioNLP) automates the process. While Large Language Models (LLMs) have shown promise in general domains, their effectiveness in BioNLP tasks remains unclear due to limited benchmarks and practical guidelines.   We perform a systematic evaluation of four LLMs, GPT and LLaMA representatives on 12 BioNLP benchmarks across six applications. We compare their zero-shot, few-shot, and fine-tuning performance with traditional fine-tuning of BERT or BART models. We examine inconsistencies, missing information, hallucinations, and perform cost analysis. Here we show that traditional fine-tuning outperforms zero or few shot LLMs in most tasks. However, closed-source LLMs like GPT-4 excel in reasoning-related tasks such as medical question answering. Open source LLMs still require fine-tuning to close performance gaps. We find issues like missing information and hallucinations in LLM outputs. These results offer practical insights for applying LLMs in BioNLP.

</details>


### [109] [Ragas: Automated Evaluation of Retrieval Augmented Generation](https://arxiv.org/abs/2309.15217)

*Shahul Es, Jithin James, Luis Espinosa-Anke, Steven Schockaert*

**Main category:** cs.CL

**Keywords:** Retrieval Augmented Generation, Evaluation Framework, LLM, Reference-free Metrics, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** Ragas is a framework for evaluating Retrieval Augmented Generation (RAG) systems without relying on human annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in evaluating RAG architectures which combine retrieval systems and LLMs, crucial for reducing hallucinations in generated content.

**Method:** The authors propose a set of metrics to evaluate retrieval effectiveness, LLM reliance on retrieved passages, and the quality of the generated text, independent of human annotations.

**Key Contributions:** Introduction of a reference-free evaluation framework for RAG, Development of a suite of metrics for various evaluation dimensions, Facilitation of faster evaluation cycles for RAG systems.

**Result:** Ragas offers a suite of reference-free evaluation metrics, enhancing the speed of evaluation cycles for RAG architectures.

**Limitations:** The framework's effectiveness in diverse real-world applications still needs further investigation.

**Future Work:** Further refinement of the proposed metrics and exploration of their application across different types of RAG systems.

**Conclusion:** Implementing Ragas can significantly accelerate the evaluation of RAG architectures, addressing the need for faster assessments as LLM usage grows.

**Abstract:** We introduce Ragas (Retrieval Augmented Generation Assessment), a framework for reference-free evaluation of Retrieval Augmented Generation (RAG) pipelines. RAG systems are composed of a retrieval and an LLM based generation module, and provide LLMs with knowledge from a reference textual database, which enables them to act as a natural language layer between a user and textual databases, reducing the risk of hallucinations. Evaluating RAG architectures is, however, challenging because there are several dimensions to consider: the ability of the retrieval system to identify relevant and focused context passages, the ability of the LLM to exploit such passages in a faithful way, or the quality of the generation itself. With Ragas, we put forward a suite of metrics which can be used to evaluate these different dimensions \textit{without having to rely on ground truth human annotations}. We posit that such a framework can crucially contribute to faster evaluation cycles of RAG architectures, which is especially important given the fast adoption of LLMs.

</details>


### [110] [Large language models for newspaper sentiment analysis during COVID-19: The Guardian](https://arxiv.org/abs/2405.13056)

*Rohitash Chandra, Baicheng Zhu, Qingying Fang, Eka Shinjikashvili*

**Main category:** cs.CL

**Keywords:** COVID-19, Sentiment Analysis, Large Language Models, Media Coverage, Public Sentiment

**Relevance Score:** 7

**TL;DR:** This study conducts a sentiment analysis of The Guardian newspaper during COVID-19, revealing a predominance of negative sentiments and a focus shift from crisis response to health and economic impacts over time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how media coverage during COVID-19 reflected public sentiment and emotional trends, comparing it with social media sentiment analyses.

**Method:** Sentiment analysis of articles from The Guardian newspaper during various COVID-19 stages, utilizing large language models refined with expert-labelled sentiment data.

**Key Contributions:** Conducted an in-depth sentiment analysis of a major newspaper during COVID-19., Utilized large language models for sentiment analysis, reflecting nuanced emotional trends., Provided a comparison of newspaper sentiment with social media, highlighting differences in emotional representation.

**Result:** The analysis showed a shift in public sentiment from urgent crisis response to concerns about health and the economy, with The Guardian exhibiting a predominance of negative sentiments.

**Limitations:** Focused solely on The Guardian newspaper and selected countries, which may limit generalizability.

**Future Work:** Expanding sentiment analysis to include more diverse news sources and countries for broader insights into media coverage during health crises.

**Conclusion:** The study suggests that media narratives during the pandemic often conveyed a grim outlook, contrasting with more diversified sentiments observed in social media analyses.

**Abstract:** During the COVID-19 pandemic, the news media coverage encompassed a wide range of topics that includes viral transmission, allocation of medical resources, and government response measures. There have been studies on sentiment analysis of social media platforms during COVID-19 to understand the public response given the rise of cases and government strategies implemented to control the spread of the virus. Sentiment analysis can provide a better understanding of changes in societal opinions and emotional trends during the pandemic. Apart from social media, newspapers have played a vital role in the dissemination of information, including information from the government, experts, and also the public about various topics. A study of sentiment analysis of newspaper sources during COVID-19 for selected countries can give an overview of how the media covered the pandemic. In this study, we select The Guardian newspaper and provide a sentiment analysis during various stages of COVID-19 that includes initial transmission, lockdowns and vaccination. We employ novel large language models (LLMs) and refine them with expert-labelled sentiment analysis data. We also provide an analysis of sentiments experienced pre-pandemic for comparison. The results indicate that during the early pandemic stages, public sentiment prioritised urgent crisis response, later shifting focus to addressing the impact on health and the economy. In comparison with related studies about social media sentiment analyses, we found a discrepancy between The Guardian with dominance of negative sentiments (sad, annoyed, anxious and denial), suggesting that social media offers a more diversified emotional reflection. We found a grim narrative in The Guardian with overall dominance of negative sentiments, pre and during COVID-19 across news sections including Australia, UK, World News, and Opinion

</details>


### [111] [Fake News Detection: It's All in the Data!](https://arxiv.org/abs/2407.02122)

*Soveatin Kuntur, Anna Wróblewska, Marcin Paprzycki, Maria Ganzha*

**Main category:** cs.CL

**Keywords:** fake news detection, dataset quality, ethical issues, GitHub repository, research support

**Relevance Score:** 4

**TL;DR:** A comprehensive survey focusing on fake news detection, emphasizing dataset quality, diversity, and ethical considerations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a resource for researchers in fake news detection, highlighting the importance of dataset quality and diversity.

**Method:** Survey of dataset features, labeling systems, biases, and ethical issues related to fake news detection.

**Key Contributions:** Emphasizes dataset quality and diversity in fake news detection, Documents various labeling systems and biases in datasets, Provides a GitHub repository of publicly accessible datasets for research.

**Result:** Identified key factors affecting the robustness of detection models and presented a consolidated GitHub repository for datasets.

**Limitations:** 

**Future Work:** Future research could explore innovative labeling systems and further development of detection models using high-quality datasets.

**Conclusion:** The paper underscores the need for high-quality datasets to improve fake news detection efforts and offers a consolidated repository for researchers.

**Abstract:** This comprehensive survey serves as an indispensable resource for researchers embarking on the journey of fake news detection. By highlighting the pivotal role of dataset quality and diversity, it underscores the significance of these elements in the effectiveness and robustness of detection models. The survey meticulously outlines the key features of datasets, various labeling systems employed, and prevalent biases that can impact model performance. Additionally, it addresses critical ethical issues and best practices, offering a thorough overview of the current state of available datasets. Our contribution to this field is further enriched by the provision of GitHub repository, which consolidates publicly accessible datasets into a single, user-friendly portal. This repository is designed to facilitate and stimulate further research and development efforts aimed at combating the pervasive issue of fake news.

</details>


### [112] [Pula: Training Large Language Models for Setswana](https://arxiv.org/abs/2408.02239)

*Nathan Brown, Vukosi Marivate*

**Main category:** cs.CL

**Keywords:** bilingual language models, Setswana, language translation, instruction-tuning, machine learning

**Relevance Score:** 8

**TL;DR:** Pula is a suite of bilingual language models for Setswana and English, achieving state-of-the-art performance on particular tasks and accompanied by the largest Setswana text corpus and various resources for further research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for advanced bilingual models that can effectively handle Setswana and English to improve translation and reasoning tasks.

**Method:** Development of bilingual language models Pula 1B, 3B, 8B, and 14B using efficient fine-tuning on a large Setswana text corpus.

**Key Contributions:** Introduction of Pula models for Setswana and English., Release of the Marothodi corpus, the largest Setswana text corpus., Creation of the Medupi dataset for instruction tuning in Setswana.

**Result:** Pula models outperform existing benchmarks on English-Setswana translation and state-of-the-art performance on Setswana reasoning tasks.

**Limitations:** Mainly focused on bilingual capabilities, which may limit generalizability to monolingual contexts.

**Future Work:** Future research may explore further enhancements in language understanding and reasoning in bilingual contexts.

**Conclusion:** The work contributes significant resources for future research in bilingual AI applications and enhances the understanding of Setswana language processing.

**Abstract:** In this work we present Pula, a suite of bilingual language models proficient in both Setswana and English. Leveraging recent advancements in data availability and efficient fine-tuning, Pula 8B and Pula 14B outperform GPT-4o and Gemini 1.5 Pro on English-Setswana translation tasks and achieve state-of-the-art performance on Setswana reasoning tasks for their size. We release the weights for Pula 1B, 3B, 8B, and 14B as well as training logs and training and evaluation code. Alongside Pula, we release the largest-ever Setswana text corpus, Marothodi, and the first comprehensive Setswana instruction-tuning dataset, Medupi, consisting of reformatted datasets, translated corpora, and synthetic LLM-generated text. To accompany this data, we release the code used for dataset construction, formatting, filtering, and scraping. Last, we release two Setswana LLM-translated benchmarks, MMLU-tsn and GSM8K-tsn, to measure Setswana knowledge and reasoning capabilities.

</details>


### [113] [AdTEC: A Unified Benchmark for Evaluating Text Quality in Search Engine Advertising](https://arxiv.org/abs/2408.05906)

*Peinan Zhang, Yusuke Sakai, Masato Mita, Hiroki Ouchi, Taro Watanabe*

**Main category:** cs.CL

**Keywords:** Natural Language Generation, Ad Text Evaluation, Pre-trained Language Models, Japanese Dataset, Advertising

**Relevance Score:** 5

**TL;DR:** The paper presents AdTEC, a public benchmark for evaluating ad texts created by natural language generation, highlighting quality assessment tasks and performance comparisons between PLMs and human evaluators.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing need for quality evaluation of automatically generated ad texts in real-world advertising contexts.

**Method:** The authors define five tasks for evaluating ad text quality and create a Japanese dataset influenced by practical experiences from advertising agencies.

**Key Contributions:** Definition of five evaluation tasks for ad texts, Creation of a practical Japanese dataset for evaluation, Performance analysis of PLMs vs human evaluators

**Result:** The evaluation shows that while pre-trained language models (PLMs) are effective in certain tasks, human evaluators still outperform them in specific areas, indicating potential improvements.

**Limitations:** The dataset is focused on the Japanese language and may not generalize to other languages or contexts.

**Future Work:** Future research could explore adaptation of the benchmark for other languages and improvement strategies for PLMs.

**Conclusion:** The benchmark lays the groundwork for further advancements in evaluating the quality of automatically generated ad texts.

**Abstract:** With the increase in the fluency of ad texts automatically created by natural language generation technology, there is high demand to verify the quality of these creatives in a real-world setting. We propose AdTEC (Ad Text Evaluation Benchmark by CyberAgent), the first public benchmark to evaluate ad texts from multiple perspectives within practical advertising operations. Our contributions are as follows: (i) Defining five tasks for evaluating the quality of ad texts, as well as building a Japanese dataset based on the practical operational experiences of building a Japanese dataset based on the practical operational experiences of advertising agencies, which are typically kept in-house. (ii) Validating the performance of existing pre-trained language models (PLMs) and human evaluators on the dataset. (iii) Analyzing the characteristics and providing challenges of the benchmark. The results show that while PLMs have already reached practical usage level in several tasks, humans still outperform in certain domains, implying that there is significant room for improvement in this area.

</details>


### [114] [W-RAG: Weakly Supervised Dense Retrieval in RAG for Open-domain Question Answering](https://arxiv.org/abs/2408.08444)

*Jinming Nian, Zhiyuan Peng, Qifan Wang, Yi Fang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Open-domain Question Answering, Large Language Models, Dense Retrieval, Fine-tuning

**Relevance Score:** 10

**TL;DR:** W-RAG enhances LLMs for open-domain question answering by fine-tuning the retrieval process based on task-specific signals, improving retrieval and answer generation performance with minimal human annotation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve open-domain question answering (OpenQA) performance of LLMs by addressing their reliance on internal knowledge and enhancing retrieval with weak supervision from task data.

**Method:** W-RAG uses weak training signals from the OpenQA task to fine-tune the retriever, reranking top passages based on their likelihood of enabling correct answer generation.

**Key Contributions:** Introduction of W-RAG for enhancing retrieval in OpenQA tasks, Leveraging weak training signals from downstream tasks, Demonstrating effectiveness across multiple datasets without heavy reliance on human annotation

**Result:** W-RAG achieves improved retrieval and OpenQA performance on four datasets, reaching results akin to human-labeled data fine-tuning while mitigating the need for expensive annotations.

**Limitations:** The reliance on the quality of passage retrieval and the potential need for further evaluation on diverse datasets or tasks beyond OpenQA.

**Future Work:** Exploration of additional task-specific signals and further refinement of the retrieval approach in broader contexts.

**Conclusion:** The proposed method effectively utilizes the link between question answering tasks and retrieval processes, showcasing that weak signals can enhance model performance significantly without extensive labeling efforts.

**Abstract:** In knowledge-intensive tasks such as open-domain question answering (OpenQA), large language models (LLMs) often struggle to generate factual answers, relying solely on their internal (parametric) knowledge. To address this limitation, Retrieval-Augmented Generation (RAG) systems enhance LLMs by retrieving relevant information from external sources, thereby positioning the retriever as a pivotal component. Although dense retrieval demonstrates state-of-the-art performance, its training poses challenges due to the scarcity of ground-truth evidence, largely attributed to the high costs of human annotation. In this paper, we propose W-RAG, a method that draws weak training signals from the downstream task (such as OpenQA) of an LLM, and fine-tunes the retriever to prioritize passages that most benefit the task. Specifically, we rerank the top-$k$ passages retrieved via BM25 by assessing the probability that the LLM will generate the correct answer for a question given each passage. The highest-ranking passages are then used as positive fine-tuning examples for dense retrieval. We conduct comprehensive experiments across four publicly available OpenQA datasets to demonstrate that our approach enhances both retrieval and OpenQA performance compared to baseline models, achieving results comparable to models fine-tuned with human-labeled data.

</details>


### [115] [Data Processing for the OpenGPT-X Model Family](https://arxiv.org/abs/2410.08800)

*Nicolo' Brandizzi, Hammam Abdelwahab, Anirban Bhowmick, Lennard Helmer, Benny Jörg Stein, Pavel Denisov, Qasid Saleem, Michael Fromm, Mehdi Ali, Richard Rutmann, Farzad Naderi, Mohamad Saif Agy, Alexander Schwirjow, Fabian Küch, Luzian Hahn, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Dennis Wegener, Nicolas Flores-Herr, Joachim Köhler, Johannes Leveling*

**Main category:** cs.CL

**Keywords:** data preparation, multilingual LLMs, European languages, data processing pipeline, OpenGPT-X

**Relevance Score:** 7

**TL;DR:** Overview of the data preparation pipeline for the OpenGPT-X project focused on multilingual LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create high-performance multilingual LLMs that serve real-world applications in major European languages, adhering to data regulations.

**Method:** The paper outlines the data selection, requirement definition, and the distinct pipelines for curated and web data, detailing the filtering and deduplication processes.

**Key Contributions:** Comprehensive methodologies for handling and processing multilingual data., Analysis of datasets aligned with European regulations., Recommendations for data preparation in LLM projects.

**Result:** An in-depth analysis of datasets highlighting transparency and alignment with European data regulations, along with specialized algorithmic solutions for both curated and web data pipelines.

**Limitations:** The paper does not address specific algorithmic design aspects or the performance metrics of the LLMs derived from the datasets.

**Future Work:** Further studies on enhancing data preparation techniques and addressing challenges in multilingual data contexts.

**Conclusion:** The findings provide insights into the challenges of large-scale multilingual dataset preparation and suggest future directions for similar projects.

**Abstract:** This paper presents a comprehensive overview of the data preparation pipeline developed for the OpenGPT-X project, a large-scale initiative aimed at creating open and high-performance multilingual large language models (LLMs). The project goal is to deliver models that cover all major European languages, with a particular focus on real-world applications within the European Union. We explain all data processing steps, starting with the data selection and requirement definition to the preparation of the final datasets for model training. We distinguish between curated data and web data, as each of these categories is handled by distinct pipelines, with curated data undergoing minimal filtering and web data requiring extensive filtering and deduplication. This distinction guided the development of specialized algorithmic solutions for both pipelines. In addition to describing the processing methodologies, we provide an in-depth analysis of the datasets, increasing transparency and alignment with European data regulations. Finally, we share key insights and challenges faced during the project, offering recommendations for future endeavors in large-scale multilingual data preparation for LLMs.

</details>


### [116] [Speculative Knowledge Distillation: Bridging the Teacher-Student Gap Through Interleaved Sampling](https://arxiv.org/abs/2410.11325)

*Wenda Xu, Rujun Han, Zifeng Wang, Long T. Le, Dhruv Madeka, Lei Li, William Yang Wang, Rishabh Agarwal, Chen-Yu Lee, Tomas Pfister*

**Main category:** cs.CL

**Keywords:** Knowledge Distillation, Speculative Knowledge Distillation, Text Generation, Machine Learning, AI Applications

**Relevance Score:** 6

**TL;DR:** Introduction of Speculative Knowledge Distillation (SKD) to improve knowledge transfer between student and teacher models in knowledge distillation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current knowledge distillation methods, such as distribution mismatches and low-quality training examples.

**Method:** Speculative Knowledge Distillation (SKD) enables student models to generate tokens, which are then refined by the teacher model to produce high-quality training data that aligns with the student's inference-time distribution.

**Key Contributions:** Introduction of Speculative Knowledge Distillation (SKD) methodology., Demonstration of superior performance of SKD over existing KD methods., Application of SKD in multiple text generation tasks validates its efficiency.

**Result:** SKD outperforms existing knowledge distillation methods across various text generation tasks, demonstrating improved performance in translation, summarization, math, and instruction following.

**Limitations:** 

**Future Work:** Exploration of additional domains and improvements in the adaptability of the model cooperation in SKD.

**Conclusion:** SKD represents a significant advancement in knowledge distillation techniques, enhancing the cooperation between student and teacher models for better knowledge transfer.

**Abstract:** Recent advances in knowledge distillation (KD) have enabled smaller student models to approach the performance of larger teacher models. However, popular methods such as supervised KD and on-policy KD, are adversely impacted by the knowledge gaps between teacher-student in practical scenarios. Supervised KD suffers from a distribution mismatch between training with a static dataset and inference over final student-generated outputs. Conversely, on-policy KD, which uses student-generated samples for training, can suffer from low-quality training examples with which teacher models are not familiar, resulting in inaccurate teacher feedback. To address these limitations, we introduce Speculative Knowledge Distillation (SKD), a novel approach that leverages cooperation between student and teacher models to generate high-quality training data on-the-fly while aligning with the student's inference-time distribution. In SKD, the student proposes tokens, and the teacher replaces poorly ranked ones based on its own distribution, transferring high-quality knowledge adaptively. We evaluate SKD on various text generation tasks, including translation, summarization, math, and instruction following, and show that SKD consistently outperforms existing KD methods across different domains, data sizes, and model initialization strategies.

</details>


### [117] [Open Domain Question Answering with Conflicting Contexts](https://arxiv.org/abs/2410.12311)

*Siyi Liu, Qiang Ning, Kishaloy Halder, Wei Xiao, Zheng Qi, Phu Mon Htut, Yi Zhang, Neha Anna John, Bonan Min, Yassine Benajiba, Dan Roth*

**Main category:** cs.CL

**Keywords:** Open Domain Question Answering, Conflicting Information, Large Language Models, Human Reasoning, Dataset QACC

**Relevance Score:** 9

**TL;DR:** The paper addresses the issue of conflicting information in open domain question answering systems, demonstrating the limitations of LLMs when confronted with such contexts, and explores the potential of finetuning these models to improve reasoning through conflict.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Open domain question answering systems depend on vast text collections for information, which can contain conflicting data leading to inaccurate answers.

**Method:** A human-annotated dataset, QACC, was collected to evaluate how often unambiguous questions lead to conflicting answers from Google Search. The study benchmarks three LLMs on this dataset and examines human reasoning through conflicting contexts by analyzing annotator explanations.

**Key Contributions:** Introduction of the QACC dataset for benchmarking conflicting contexts in question answering., Evaluation of LLMs' limitations in handling conflicting information., Proposal of finetuning LLMs for better reasoning with conflicting contexts.

**Result:** The findings reveal that up to 25% of open domain questions can lead to conflicting contexts, and current LLMs struggle to effectively respond in these situations. However, finetuning LLMs to explain their reasoning improves their capability to manage conflicting information.

**Limitations:** The study is limited to the capability of current LLMs and may not fully address all aspects of conflicting information in open domain QA systems.

**Future Work:** Further research is suggested to improve LLMs through alternative training methods and exploring other approaches in resolving information conflicts.

**Conclusion:** Enhancing LLMs with explanation-based finetuning offers a promising direction for developing more reliable question answering systems capable of resolving information conflicts.

**Abstract:** Open domain question answering systems frequently rely on information retrieved from large collections of text (such as the Web) to answer questions. However, such collections of text often contain conflicting information, and indiscriminately depending on this information may result in untruthful and inaccurate answers. To understand the gravity of this problem, we collect a human-annotated dataset, Question Answering with Conflicting Contexts (QACC), and find that as much as 25% of unambiguous, open domain questions can lead to conflicting contexts when retrieved using Google Search. We evaluate and benchmark three powerful Large Language Models (LLMs) with our dataset QACC and demonstrate their limitations in effectively addressing questions with conflicting information. To explore how humans reason through conflicting contexts, we request our annotators to provide explanations for their selections of correct answers. We demonstrate that by finetuning LLMs to explain their answers, we can introduce richer information into their training that guide them through the process of reasoning with conflicting contexts.

</details>


### [118] [From Single to Multi: How LLMs Hallucinate in Multi-Document Summarization](https://arxiv.org/abs/2410.13961)

*Catarina G. Belem, Pouya Pezeshkpour, Hayate Iso, Seiji Maekawa, Nikita Bhutani, Estevam Hruschka*

**Main category:** cs.CL

**Keywords:** hallucinations, multi-document summarization, large language models, datasets, evaluation benchmarks

**Relevance Score:** 9

**TL;DR:** This paper investigates hallucinations in large language models during multi-document summarization and introduces new benchmarks to evaluate this issue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the under-researched area of hallucinations in LLMs for multi-document summarization tasks, as existing research mainly focuses on single-document tasks.

**Method:** The authors created two novel multi-document benchmarks from existing news and conversation datasets, annotated with topic-specific insights, and evaluated five LLMs on these benchmarks.

**Key Contributions:** Introduction of two novel benchmarks for MDS hallucination evaluation, Analysis of hallucination characteristics and common errors in LLM-generated summaries, Empirical evaluation revealing high rates of hallucinations in popular LLMs

**Result:** Findings reveal that, on average, 75% of the content generated in LLM summaries is hallucinated, particularly more frequently towards summary ends, with notable fabrication of non-existent topic-related information.

**Limitations:** The post-hoc methods tested were only moderately effective, indicating that more sophisticated mitigation strategies are necessary.

**Future Work:** Future research should focus on developing more effective techniques to systematically reduce hallucinations observed in multi-document summarization.

**Conclusion:** There is a significant need for more effective approaches to mitigate hallucinations in multi-document summarization, as existing simple post-hoc methods were found only moderately effective.

**Abstract:** Although many studies have investigated and reduced hallucinations in large language models (LLMs) for single-document tasks, research on hallucination in multi-document summarization (MDS) tasks remains largely unexplored. Specifically, it is unclear how the challenges arising from handling multiple documents (e.g., repetition and diversity of information) affect models outputs. In this work, we investigate how hallucinations manifest in LLMs when summarizing topic-specific information from multiple documents. Since no benchmarks exist for investigating hallucinations in MDS, we use existing news and conversation datasets, annotated with topic-specific insights, to create two novel multi-document benchmarks. When evaluating 5 LLMs on our benchmarks, we observe that on average, up to 75% of the content in LLM-generated summary is hallucinated, with hallucinations more likely to occur towards the end of the summaries. Moreover, when summarizing non-existent topic-related information, gpt-3.5-turbo and GPT-4o still generate summaries about 79.35% and 44% of the time, raising concerns about their tendency to fabricate content. To understand the characteristics of these hallucinations, we manually evaluate 700+ insights and find that most errors stem from either failing to follow instructions or producing overly generic insights. Motivated by these observations, we investigate the efficacy of simple post-hoc baselines in mitigating hallucinations but find them only moderately effective. Our results underscore the need for more effective approaches to systematically mitigate hallucinations in MDS. We release our dataset and code at github.com/megagonlabs/Hallucination_MDS.

</details>


### [119] [WikiNER-fr-gold: A Gold-Standard NER Corpus](https://arxiv.org/abs/2411.00030)

*Danrun Cao, Nicolas Béchet, Pierre-François Marteau*

**Main category:** cs.CL

**Keywords:** WikiNER, Named Entity Recognition, Corpus Quality, Annotation Guidelines, Silver-Standard

**Relevance Score:** 4

**TL;DR:** This paper revises the WikiNER corpus for French Named Entity Recognition, creating a gold-standard version to enhance quality and consistency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of the WikiNER corpus, which was generated in a semi-supervised manner and lacks manual verification.

**Method:** The authors sampled 20% of the original French sub-corpus to create WikiNER-fr-gold, defined annotation guidelines, and revised the corpus based on these guidelines.

**Key Contributions:** Creation of WikiNER-fr-gold to enhance data quality, Establishment of annotation guidelines for Named Entity Recognition, Analysis of errors in the original corpus

**Result:** The revised corpus contains 26,818 sentences and provides a more reliable dataset for Named Entity Recognition tasks.

**Limitations:** The study focuses solely on the French sub-corpus of WikiNER and may not address issues in other language sub-corpora.

**Future Work:** Exploration of additional languages and further refinements in annotation processes.

**Conclusion:** The study highlights inconsistencies in the original WikiNER-fr corpus and sets the groundwork for improving Named Entity Recognition datasets.

**Abstract:** We address in this article the the quality of the WikiNER corpus, a multilingual Named Entity Recognition corpus, and provide a consolidated version of it. The annotation of WikiNER was produced in a semi-supervised manner i.e. no manual verification has been carried out a posteriori. Such corpus is called silver-standard. In this paper we propose WikiNER-fr-gold which is a revised version of the French proportion of WikiNER. Our corpus consists of randomly sampled 20% of the original French sub-corpus (26,818 sentences with 700k tokens). We start by summarizing the entity types included in each category in order to define an annotation guideline, and then we proceed to revise the corpus. Finally we present an analysis of errors and inconsistency observed in the WikiNER-fr corpus, and we discuss potential future work directions.

</details>


### [120] [An Attempt to Develop a Neural Parser based on Simplified Head-Driven Phrase Structure Grammar on Vietnamese](https://arxiv.org/abs/2411.17270)

*Duc-Vu Nguyen, Thang Chau Phan, Quoc-Nam Nguyen, Kiet Van Nguyen, Ngan Luu-Thuy Nguyen*

**Main category:** cs.CL

**Keywords:** Vietnamese NLP, Neural Parser, HPSG, Constituency Parsing, Dependency Parsing

**Relevance Score:** 4

**TL;DR:** Development of a neural parser for Vietnamese using simplified Head-Driven Phrase Structure Grammar, achieving new state-of-the-art results in constituency parsing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues with existing Vietnamese corpora not adhering to simplified HPSG rules and to improve natural language processing for Vietnamese.

**Method:** Modified an existing neural parser by using PhoBERT and XLM-RoBERTa models on compliant training samples to achieve better parsing performance.

**Key Contributions:** New state-of-the-art F-score for constituency parsing in Vietnamese, Innovative use of modified models (PhoBERT and XLM-RoBERTa), Highlights the importance of expert consultation in linguistic research.

**Result:** Achieved a new state-of-the-art F-score of 82% for constituency parsing and outperformed previous studies in dependency parsing, though it had lower Labeled Attachment Score.

**Limitations:** Lower Labeled Attachment Score likely due to neglecting expert consultation during development.

**Future Work:** Encouragement of expert involvement in future treebank development for improved accuracy and linguistic adherence.

**Conclusion:** Emphasizes the need for collaboration with linguistic experts to enhance treebank development for Vietnamese NLP.

**Abstract:** In this paper, we aimed to develop a neural parser for Vietnamese based on simplified Head-Driven Phrase Structure Grammar (HPSG). The existing corpora, VietTreebank and VnDT, had around 15% of constituency and dependency tree pairs that did not adhere to simplified HPSG rules. To attempt to address the issue of the corpora not adhering to simplified HPSG rules, we randomly permuted samples from the training and development sets to make them compliant with simplified HPSG. We then modified the first simplified HPSG Neural Parser for the Penn Treebank by replacing it with the PhoBERT or XLM-RoBERTa models, which can encode Vietnamese texts. We conducted experiments on our modified VietTreebank and VnDT corpora. Our extensive experiments showed that the simplified HPSG Neural Parser achieved a new state-of-the-art F-score of 82% for constituency parsing when using the same predicted part-of-speech (POS) tags as the self-attentive constituency parser. Additionally, it outperformed previous studies in dependency parsing with a higher Unlabeled Attachment Score (UAS). However, our parser obtained lower Labeled Attachment Score (LAS) scores likely due to our focus on arc permutation without changing the original labels, as we did not consult with a linguistic expert. Lastly, the research findings of this paper suggest that simplified HPSG should be given more attention to linguistic expert when developing treebanks for Vietnamese natural language processing.

</details>


### [121] [Investigating Length Issues in Document-level Machine Translation](https://arxiv.org/abs/2412.17592)

*Ziqian Peng, Rachel Bawden, François Yvon*

**Main category:** cs.CL

**Keywords:** Machine Translation, Transformer, Document-Level Translation

**Relevance Score:** 6

**TL;DR:** This study examines the impact of input text length on machine translation performance using transformer architectures, revealing that longer documents result in decreased translation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the limitations of machine translation systems in handling very long texts and understand how document length affects translation quality.

**Method:** The authors designed experiments with two machine translation architectures to measure the effect of incremental input text lengths on translation outputs.

**Key Contributions:** Demonstrated the negative impact of input length on translation quality., Investigated the relevance of sentence positioning within documents for MT quality., Provided empirical evidence on the limits of document-level MT compared to sentence-based MT.

**Result:** The results indicate that translation performance declines as text length increases, and that earlier sentences within documents are translated with higher quality, with some attempts to manipulate input length and positional embeddings showing minimal improvement.

**Limitations:** The manipulations of document lengths and positional embeddings only marginally improve translation quality.

**Future Work:** Further research is needed to develop more effective techniques to enhance translation performance for longer texts.

**Conclusion:** Document-level machine translation is feasible but does not yet achieve the same performance as sentence-based translation methods.

**Abstract:** Transformer architectures are increasingly effective at processing and generating very long chunks of texts, opening new perspectives for document-level machine translation (MT). In this work, we challenge the ability of MT systems to handle texts comprising up to several thousands of tokens. We design and implement a new approach designed to precisely measure the effect of length increments on MT outputs. Our experiments with two representative architectures unambiguously show that (a)~translation performance decreases with the length of the input text; (b)~the position of sentences within the document matters, and translation quality is higher for sentences occurring earlier in a document. We further show that manipulating the distribution of document lengths and of positional embeddings only marginally mitigates such problems. Our results suggest that even though document-level MT is computationally feasible, it does not yet match the performance of sentence-based MT.

</details>


### [122] [LiveIdeaBench: Evaluating LLMs' Divergent Thinking for Scientific Idea Generation with Minimal Context](https://arxiv.org/abs/2412.17596)

*Kai Ruan, Xuan Wang, Jixiang Hong, Peng Wang, Yang Liu, Hao Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, scientific idea generation, benchmark, creativity, evaluation

**Relevance Score:** 9

**TL;DR:** LiveIdeaBench is a benchmark for evaluating LLMs' scientific idea generation capabilities based on single-keyword prompts, revealing gaps between creativity and general intelligence metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess LLMs' abilities in scientific idea generation beyond traditional evaluation metrics focused on general intelligence.

**Method:** The benchmark evaluates LLMs' creativity across five dimensions: originality, feasibility, fluency, flexibility, and clarity using a dynamic panel of state-of-the-art models and single-keyword prompts.

**Key Contributions:** Introduction of the LiveIdeaBench benchmark for evaluating scientific idea generation in LLMs., Findings that highlight the disconnect between general intelligence scores and creative performance., Recommendations for alternative training strategies to improve LLMs' scientific idea generation capabilities.

**Result:** Experiments with over 40 models show that creativity in scientific idea generation is poorly predicted by general intelligence metrics, with models like QwQ-32B-preview performing comparably to top models despite lower general scores.

**Limitations:** The benchmark only assesses performance using single-keyword prompts, which may limit the scope of evaluation.

**Future Work:** Investigate training strategies tailored specifically for improving idea generation capabilities and expand the benchmark to include more complex prompts.

**Conclusion:** There is a need for specialized benchmarks in scientific idea generation, which may require distinct training strategies for LLMs to enhance their capabilities in this area.

**Abstract:** While Large Language Models (LLMs) demonstrate remarkable capabilities in scientific tasks such as literature analysis and experimental design (e.g., accurately extracting key findings from papers or generating coherent experimental procedures), existing evaluation benchmarks primarily assess performance using rich contextual inputs. We introduce LiveIdeaBench, a comprehensive benchmark evaluating LLMs' scientific idea generation by assessing divergent thinking capabilities using single-keyword prompts. Drawing from Guilford's creativity theory, our benchmark employs a dynamic panel of state-of-the-art LLMs to assess generated ideas across five key dimensions: originality, feasibility, fluency, flexibility, and clarity. Through extensive experimentation with over 40 leading models across 1,180 keywords spanning 22 scientific domains, we reveal that the scientific idea generation capabilities measured by our benchmark, are poorly predicted by standard metrics of general intelligence. Our results demonstrate that models like QwQ-32B-preview achieve creative performance comparable to top-tier models such as claude-3.7-sonnet:thinking, despite significant gaps in their general intelligence scores. These findings highlight the need for specialized evaluation benchmarks for scientific idea generation and suggest that enhancing these idea generation capabilities in LLMs may require different training strategies than those used for improving general problem-solving abilities, potentially enabling a wider range of AI tools tailored for different stages of the scientific process.

</details>


### [123] [Disambiguating Numeral Sequences to Decipher Ancient Accounting Corpora](https://arxiv.org/abs/2502.00090)

*Logan Born, M. Willis Monroe, Kathryn Kelley, Anoop Sarkar*

**Main category:** cs.CL

**Keywords:** proto-Elamite script, numerical disambiguation, bootstrapping algorithm, classifier techniques, ancient accounting

**Relevance Score:** 0

**TL;DR:** This paper focuses on disambiguating numeral readings in the proto-Elamite script using algorithmic techniques and classifiers to enhance understanding of numeric quantification in ancient records.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The ancient proto-Elamite script has ambiguous numeral representations, complicating efforts to accurately interpret numeric data; understanding these values is vital for historical and accounting analysis.

**Method:** The paper presents two disambiguation techniques that utilize structural properties of original documents and employ classifiers trained through a bootstrapping algorithm, along with the development of a test set for evaluating these techniques.

**Key Contributions:** Development of two disambiguation techniques for proto-Elamite numerals, Creation of a test set for evaluating disambiguation methods, Novel cautious rule selection approach for bootstrapped classifiers

**Result:** The analysis validates previous assumptions about the proto-Elamite script and reveals unknown correlations between the content of tablets and the magnitude of numerals, utilizing improved baseline model performance after correcting prior errors in numeral values.

**Limitations:** The study focuses exclusively on numeral disambiguation, which may limit its applicability to other aspects of the proto-Elamite script and does not address full script decipherment.

**Future Work:** Future research could expand on additional applications of the disambiguation techniques to other historical scripts and explore deeper insights into scripts with highly ambiguous representations.

**Conclusion:** This work aids significantly in the understanding and deciphering of proto-Elamite numerals, which are heavily featured in accounting-related tablets, thus enhancing the study of ancient numerical systems.

**Abstract:** A numeration system encodes abstract numeric quantities as concrete strings of written characters. The numeration systems used by modern scripts tend to be precise and unambiguous, but this was not so for the ancient and partially-deciphered proto-Elamite (PE) script, where written numerals can have up to four distinct readings depending on the system that is used to read them. We consider the task of disambiguating between these readings in order to determine the values of the numeric quantities recorded in this corpus. We algorithmically extract a list of possible readings for each PE numeral notation, and contribute two disambiguation techniques based on structural properties of the original documents and classifiers learned with the bootstrapping algorithm. We also contribute a test set for evaluating disambiguation techniques, as well as a novel approach to cautious rule selection for bootstrapped classifiers. Our analysis confirms existing intuitions about this script and reveals previously-unknown correlations between tablet content and numeral magnitude. This work is crucial to understanding and deciphering PE, as the corpus is heavily accounting-focused and contains many more numeric tokens than tokens of text.

</details>


### [124] [InfoQuest: Evaluating Multi-Turn Dialogue Agents for Open-Ended Conversations with Hidden Context](https://arxiv.org/abs/2502.12257)

*Bryan L. M. de Oliveira, Luana G. B. Martins, Bruno Brandão, Luckeciano C. Melo*

**Main category:** cs.CL

**Keywords:** Dialogue agents, Ambiguous requests, Information-seeking dialogue

**Relevance Score:** 8

**TL;DR:** A benchmark called InfoQuest is introduced to assess how dialogue agents manage ambiguous user requests through multi-turn conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of dialogue agents in handling ambiguous or incomplete user requests and to improve their interaction capabilities.

**Method:** The paper introduces InfoQuest, a chat benchmark with intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions.

**Key Contributions:** Introduction of the InfoQuest benchmark, Systematic methodology for generating and evaluating scenarios, Insights into limitations of current language models in multi-turn dialogue

**Result:** Evaluation shows that while proprietary models perform better, all dialogue assistants struggle to effectively gather necessary information, often defaulting to generic responses.

**Limitations:** Current models often need multiple turns to infer intent and provide generic responses without clarification.

**Future Work:** Further investigation into self-improvement methodologies and development of models that can better handle ambiguity in user requests.

**Conclusion:** The need for improved models that can initiate clarifying questions and better understand user intent in multi-turn interactions is highlighted.

**Abstract:** Large language models excel at following explicit instructions, but they often struggle with ambiguous or incomplete user requests, defaulting to verbose, generic responses instead of seeking clarification. We introduce InfoQuest, a multi-turn chat benchmark designed to evaluate how dialogue agents handle hidden context in open-ended user requests. This benchmark presents intentionally ambiguous scenarios that require models to engage in information-seeking dialogue by asking clarifying questions before providing appropriate responses. Our evaluation of both open and closed models reveals that, while proprietary models generally perform better, all current assistants struggle to gather critical information effectively. They often require multiple turns to infer user intent and frequently default to generic responses without proper clarification. We provide a systematic methodology for generating diverse scenarios and evaluating models' information-seeking capabilities, which can be leveraged to automatically generate data for self-improvement. We also offer insights into the current limitations of language models in handling ambiguous requests through multi-turn interactions.

</details>


### [125] [Oreo: A Plug-in Context Reconstructor to Enhance Retrieval-Augmented Generation](https://arxiv.org/abs/2502.13019)

*Sha Li, Naren Ramakrishnan*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel module for refining retrieved information in Retrieval-Augmented Generation (RAG) to enhance the accuracy and contextual relevance of outputs from Large Language Models (LLMs).

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issues of erroneous and distracting information retrieved from external documents during RAG, which affects downstream task effectiveness.

**Method:** The authors propose a compact and efficient module that refines retrieved chunks through a three-stage training process involving supervised fine-tuning, contrastive multi-task learning, and reinforcement learning-based alignment.

**Key Contributions:** Introduction of a new module for refining retrieved information in RAG, Three-stage training paradigm for optimizing knowledge extraction and alignment, Improved performance of LLMs in generating contextually appropriate outputs

**Result:** The implementation of the module leads to LLMs producing outputs that are more accurate, reliable, and contextually appropriate, showing improved performance in downstream tasks.

**Limitations:** The paper does not detail the computational resources required for implementing the proposed module, which may impact scalability.

**Future Work:** Exploration of additional learning paradigms and integration with other types of models or domains to further enhance information retrieval and generation.

**Conclusion:** The proposed module enhances the effectiveness of RAG by extracting and reorganizing relevant information in a query-specific format, aligning better with LLM generation preferences.

**Abstract:** Retrieval-Augmented Generation (RAG) aims to augment the capabilities of Large Language Models (LLMs) by retrieving and incorporate external documents or chunks prior to generation. However, even improved retriever relevance can brings erroneous or contextually distracting information, undermining the effectiveness of RAG in downstream tasks. We introduce a compact, efficient, and pluggable module designed to refine retrieved chunks before using them for generation. The module aims to extract and reorganize the most relevant and supportive information into a concise, query-specific format. Through a three-stage training paradigm - comprising supervised fine - tuning, contrastive multi-task learning, and reinforcement learning-based alignment - it prioritizes critical knowledge and aligns it with the generator's preferences. This approach enables LLMs to produce outputs that are more accurate, reliable, and contextually appropriate.

</details>


### [126] [LIFT: Improving Long Context Understanding of Large Language Models through Long Input Fine-Tuning](https://arxiv.org/abs/2502.14644)

*Yansheng Mao, Yufei Xu, Jiaqi Li, Fanxu Meng, Haotong Yang, Zilong Zheng, Xiyuan Wang, Muhan Zhang*

**Main category:** cs.CL

**Keywords:** Long Input Fine-Tuning, Gated Memory, Long-context modeling, LLMs, In-context learning

**Relevance Score:** 8

**TL;DR:** The paper introduces Long Input Fine-Tuning (LIFT), enhancing long-context performance of short-context LLMs by adapting model parameters based on long inputs and utilizing Gated Memory for improved performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenge of long context understanding in large language models due to limited context windows.

**Method:** LIFT dynamically adapts model parameters for long inputs instead of extending context window sizes, while Gated Memory balances memorization and in-context learning.

**Key Contributions:** Introduction of Long Input Fine-Tuning (LIFT) for long-context modeling., Development of Gated Memory to enhance ICL alongside long input processing., Comprehensive analysis of LIFT's performance and its future research directions.

**Result:** LIFT allows short-context LLMs to answer questions relying on long input information not included in the context, showing improved long-context capabilities.

**Limitations:** The paper discusses strengths and limitations regarding LIFT's implementation in long context understanding.

**Future Work:** Exploring further enhancements to LIFT and its applications in various scenarios.

**Conclusion:** LIFT demonstrates effective long-context modeling with short-context LLMs, though it has strengths and limitations that need further exploration.

**Abstract:** Long context understanding remains challenging for large language models due to their limited context windows. This paper presents Long Input Fine-Tuning (LIFT), a novel framework for long-context modeling that can improve the long-context performance of arbitrary (short-context) LLMs by dynamically adapting model parameters based on the long input. Importantly, LIFT, rather than endlessly extending the context window size to accommodate increasingly longer inputs in context, chooses to store and absorb the long input in parameter. By fine-tuning the long input into model parameters, LIFT allows short-context LLMs to answer questions even when the required information is not provided in the context during inference. Furthermore, to enhance LIFT performance while maintaining the original in-context learning (ICL) capabilities, we introduce Gated Memory, a specialized attention adapter that automatically balances long input memorization and ICL. We provide a comprehensive analysis of the strengths and limitations of LIFT on long context understanding, offering valuable directions for future research.

</details>


### [127] [Latent Factor Models Meets Instructions: Goal-conditioned Latent Factor Discovery without Task Supervision](https://arxiv.org/abs/2502.15147)

*Zhouhang Xie, Tushar Khot, Bhavana Dalvi Mishra, Harshit Surana, Julian McAuley, Peter Clark, Bodhisattwa Prasad Majumder*

**Main category:** cs.CL

**Keywords:** latent factor discovery, instruction-following LLM, noisy datasets, statistical models, concept discovery

**Relevance Score:** 8

**TL;DR:** Instruct-LF is a novel system that leverages instruction-following LLMs and statistical models to discover meaningful latent factors from large, noisy datasets, significantly improving task performance in various applications.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of discovered concepts from unstructured documents, particularly when faced with noisy data or limitations in LLM reasoning capabilities.

**Method:** Instruct-LF integrates LLMs with statistical models to propose goal-related properties, estimates their presence across datasets, and employs gradient-based optimization to reveal hidden factors represented by clusters of co-occurring properties.

**Key Contributions:** Introduction of Instruct-LF for latent factor discovery, Demonstrated 5-52% performance improvements in various tasks, Human evaluations show a preference for Instruct-LF's outputs over baselines

**Result:** Instruct-LF enhances performance in tasks such as movie recommendation, text-world navigation, and legal document categorization by 5-52% compared to best baselines and received a higher preference rate in human evaluations.

**Limitations:** Quality of discovered concepts is still dependent on the underlying data quality and LLM reasoning.

**Future Work:** Exploring further enhancements to the model's robustness and effectiveness with varying data characteristics.

**Conclusion:** The integration of LLMs with statistical techniques enables more reliable concept discovery in challenging datasets, demonstrating practical applications across multiple domains.

**Abstract:** Instruction-following LLMs have recently allowed systems to discover hidden concepts from a collection of unstructured documents based on a natural language description of the purpose of the discovery (i.e., goal). Still, the quality of the discovered concepts remains mixed, as it depends heavily on LLM's reasoning ability and drops when the data is noisy or beyond LLM's knowledge. We present Instruct-LF, a goal-oriented latent factor discovery system that integrates LLM's instruction-following ability with statistical models to handle large, noisy datasets where LLM reasoning alone falls short.   Instruct-LF uses LLMs to propose fine-grained, goal-related properties from documents, estimates their presence across the dataset, and applies gradient-based optimization to uncover hidden factors, where each factor is represented by a cluster of co-occurring properties. We evaluate latent factors produced by Instruct-LF on movie recommendation, text-world navigation, and legal document categorization tasks. These interpretable representations improve downstream task performance by 5-52% than the best baselines and were preferred 1.8 times as often as the best alternative, on average, in human evaluation.

</details>


### [128] [Protecting multimodal large language models against misleading visualizations](https://arxiv.org/abs/2502.20503)

*Jonathan Tonglet, Tinne Tuytelaars, Marie-Francine Moens, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Automated Chart Understanding, Misleading Visualizations

**Relevance Score:** 8

**TL;DR:** This paper identifies vulnerabilities in multimodal large language models (MLLMs) when interpreting misleading visualizations and introduces novel methods to improve their accuracy while maintaining performance on accurate charts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the reliability of MLLMs in the presence of misleading visualizations that can lead to disinformation.

**Method:** The authors introduce inference-time methods, including one that extracts the underlying data from visualizations to enhance question-answering accuracy using a text-only LLM.

**Key Contributions:** Identification of a critical vulnerability in MLLMs regarding misleading visualizations., Introduction of the first inference-time methods to enhance accuracy on misleading charts., Establishment of benchmark results to guide future research efforts.

**Result:** The study finds a significant drop in question-answering accuracy for MLLMs on misleading visualizations, equating to random baseline performance, while presenting new methods that effectively counter this issue.

**Limitations:** The proposed methods may not generalize to all types of misleading visualizations and require further validation in diverse contexts.

**Future Work:** Further exploration of more robust models and additional techniques to handle varying forms of misleading data visualizations.

**Conclusion:** The paper reveals a major gap in the robustness of current MLLMs and sets benchmark results for future research on reliable automated chart understanding.

**Abstract:** Visualizations play a pivotal role in daily communication in an increasingly data-driven world. Research on multimodal large language models (MLLMs) for automated chart understanding has accelerated massively, with steady improvements on standard benchmarks. However, for MLLMs to be reliable, they must be robust to misleading visualizations, charts that distort the underlying data, leading readers to draw inaccurate conclusions that may support disinformation. Here, we uncover an important vulnerability: MLLM question-answering accuracy on misleading visualizations drops on average to the level of a random baseline. To address this, we introduce the first inference-time methods to improve performance on misleading visualizations, without compromising accuracy on non-misleading ones. The most effective method extracts the underlying data table and uses a text-only LLM to answer the question based on the table. Our findings expose a critical blind spot in current research and establish benchmark results to guide future efforts in reliable MLLMs.

</details>


### [129] [Compositional Subspace Representation Fine-tuning for Adaptive Large Language Models](https://arxiv.org/abs/2503.10617)

*Andy Zhou*

**Main category:** cs.CL

**Keywords:** large language models, cross-skill interference, multi-task learning, subspace representation, instruction following

**Relevance Score:** 8

**TL;DR:** Proposes CS-ReFT, a method to reduce cross-skill interference in large language models by learning orthonormal subspace transformations for distinct skills, enhancing multi-task performance with minimal parameter overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address cross-skill interference in large language models, where improvements in one skill can degrade performance in another, especially in hidden-state representations.

**Method:** CS-ReFT learns multiple orthonormal subspace transformations and composes them using a lightweight router to isolate subspace edits in the hidden state.

**Key Contributions:** Introduction of Compositional Subspace Representation Fine-tuning (CS-ReFT) to mitigate cross-task conflicts., Demonstrated effectiveness on the AlpacaEval benchmark with significant performance improvement and minimal parameter overhead., Novel approach focusing on hidden-state representation rather than weight matrices for multi-task learning.

**Result:** CS-ReFT applied to Llama-2-7B achieved a 93.94% win rate on the AlpacaEval benchmark, outperforming GPT-3.5 Turbo (86.30%) with only 0.0098% of model parameters used.

**Limitations:** 

**Future Work:** Further exploration of subspace representation techniques and their application across various language models and tasks.

**Conclusion:** The method demonstrates that specialized representation edits can effectively enhance multi-task instruction following in large language models with minimal additional overhead.

**Abstract:** Adapting large language models to multiple tasks can cause cross-skill interference, where improvements for one skill degrade another. While methods such as LoRA impose orthogonality constraints at the weight level, they do not fully address interference in hidden-state representations. We propose Compositional Subspace Representation Fine-tuning (CS-ReFT), a novel representation-based approach that learns multiple orthonormal subspace transformations, each specializing in a distinct skill, and composes them via a lightweight router. By isolating these subspace edits in the hidden state, rather than weight matrices, CS-ReFT prevents cross-task conflicts more effectively. On the AlpacaEval benchmark, applying CS-ReFT to Llama-2-7B achieves a 93.94% win rate, surpassing GPT-3.5 Turbo (86.30%) while requiring only 0.0098% of model parameters. These findings show that specialized representation edits, composed via a simple router, significantly enhance multi-task instruction following with minimal overhead.

</details>


### [130] [Persona Dynamics: Unveiling the Impact of Personality Traits on Agents in Text-Based Games](https://arxiv.org/abs/2504.06868)

*Seungwon Lim, Seungbeen Lee, Dongjun Min, Youngjae Yu*

**Main category:** cs.CL

**Keywords:** Personality Adaptation, AI Agents, Decision Making, Human-Centric AI, Interactive Environments

**Relevance Score:** 8

**TL;DR:** This paper introduces PANDA, a method for integrating human-like personality traits into AI agents to enhance their decision-making in text-based interactive environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning artificial agent behaviors with human values is a key challenge, particularly in complex decision-making tasks.

**Method:** The method involves training a personality classifier to identify agent personality types and integrating these profiles into the agent's policy-learning process.

**Key Contributions:** Introduction of a novel personality-adapted decision-making framework (PANDA), Demonstration of significant performance advantages for agents with certain personality traits, Insights into the interaction between personality traits and agent actions in gaming environments.

**Result:** Agents with distinct personality types were deployed in 25 text-based games, revealing that personality adaptation influences decision-making and performance positively, especially for traits like Openness.

**Limitations:** The study is limited to text-based games, so its applicability to other domains may vary.

**Future Work:** Future research could explore broader domains for personality adaptation and investigate the impact of other personality traits.

**Conclusion:** Personality-adapted agents can improve alignment and effectiveness in decision-making, promoting human-centric interactions in AI applications.

**Abstract:** Artificial agents are increasingly central to complex interactions and decision-making tasks, yet aligning their behaviors with desired human values remains an open challenge. In this work, we investigate how human-like personality traits influence agent behavior and performance within text-based interactive environments. We introduce PANDA: Personality Adapted Neural Decision Agents, a novel method for projecting human personality traits onto agents to guide their behavior. To induce personality in a text-based game agent, (i) we train a personality classifier to identify what personality type the agent's actions exhibit, and (ii) we integrate the personality profiles directly into the agent's policy-learning pipeline. By deploying agents embodying 16 distinct personality types across 25 text-based games and analyzing their trajectories, we demonstrate that an agent's action decisions can be guided toward specific personality profiles. Moreover, certain personality types, such as those characterized by higher levels of Openness, display marked advantages in performance. These findings underscore the promise of personality-adapted agents for fostering more aligned, effective, and human-centric decision-making in interactive environments.

</details>


### [131] [Evaluating the Quality of Benchmark Datasets for Low-Resource Languages: A Case Study on Turkish](https://arxiv.org/abs/2504.09714)

*Ayşe Aysu Cengiz, Ahmet Kaan Sever, Elif Ecem Ümütlü, Naime Şeyma Erdem, Burak Aytan, Büşra Tufan, Abdullah Topraksoy, Esra Darıcı, Cagri Toraman*

**Main category:** cs.CL

**Keywords:** dataset evaluation, linguistic quality, cultural appropriateness, benchmark datasets, low-resource languages

**Relevance Score:** 7

**TL;DR:** This study evaluates the quality of 17 Turkish benchmark datasets and identifies significant shortcomings in their linguistic and cultural suitability, with 70% failing to meet quality standards.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenges posed by translated datasets from English or multilingual resources that may lack cultural appropriateness and quality.

**Method:** A comprehensive framework assessing six criteria is used to evaluate the datasets, incorporating assessments from both human and LLM judges.

**Key Contributions:** Evaluation framework for benchmark datasets in low-resource languages, Comparison of human and LLM performance in dataset evaluation, Highlighting the need for improved dataset quality control

**Result:** 70% of the examined benchmark datasets do not meet the heuristic quality standards; human annotators outperform LLMs in cultural common sense knowledge.

**Limitations:** The study only examines Turkish datasets and may not generalize to other languages.

**Future Work:** Further research is needed to develop quality control mechanisms for datasets and to explore improved methodologies for evaluating linguistic and cultural suitability.

**Conclusion:** There is a critical need for more rigorous quality control in the creation of datasets for low-resource languages to ensure linguistic and cultural relevance.

**Abstract:** The reliance on translated or adapted datasets from English or multilingual resources introduces challenges regarding linguistic and cultural suitability. This study addresses the need for robust and culturally appropriate benchmarks by evaluating the quality of 17 commonly used Turkish benchmark datasets. Using a comprehensive framework that assesses six criteria, both human and LLM-judge annotators provide detailed evaluations to identify dataset strengths and shortcomings.   Our results reveal that 70% of the benchmark datasets fail to meet our heuristic quality standards. The correctness of the usage of technical terms is the strongest criterion, but 85% of the criteria are not satisfied in the examined datasets. Although LLM judges demonstrate potential, they are less effective than human annotators, particularly in understanding cultural common sense knowledge and interpreting fluent, unambiguous text. GPT-4o has stronger labeling capabilities for grammatical and technical tasks, while Llama3.3-70B excels at correctness and cultural knowledge evaluation. Our findings emphasize the urgent need for more rigorous quality control in creating and adapting datasets for low-resource languages.

</details>


### [132] [Exploring the Role of Knowledge Graph-Based RAG in Japanese Medical Question Answering with Small-Scale LLMs](https://arxiv.org/abs/2504.10982)

*Yingjian Chen, Feiyang Li, Xingyu Song, Tianxiao Li, Zixin Xu, Xiujie Chen, Issey Sukeda, Irene Li*

**Main category:** cs.CL

**Keywords:** Japanese medical QA, knowledge graph, retrieval-augmented generation, open-source LLMs, low-resource languages

**Relevance Score:** 8

**TL;DR:** This paper explores a KG-based RAG framework for Japanese medical QA using small open-source LLMs, revealing limited effectiveness and challenges related to retrieved content quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in Japanese medical QA, particularly due to privacy constraints, and to explore the combination of instruction-tuned open-source LLMs with retrieval-augmented generation (RAG).

**Method:** The authors developed a knowledge graph-based RAG framework tailored for small-scale open-source LLMs and conducted experiments to assess its impact on Japanese medical question answering.

**Key Contributions:** First exploration of KG-based RAG in Japanese medical QA., Insights into the challenges of applying RAG in low-resource languages., Empirical data showing the impact of content quality on RAG performance.

**Result:** Experimental results indicate that the KG-based RAG has a limited impact on the performance of Japanese medical QA with small-scale open-source LLMs, highly influenced by the quality of external content.

**Limitations:** The effectiveness of the RAG framework is constrained by the quality and relevance of the external retrieved content.

**Future Work:** Future research could focus on enhancing the retrieval processes and evaluating the framework with different language models and medical domains.

**Conclusion:** The findings highlight the sensitivity of the KG-based RAG framework to the relevance of retrieved information and provide insights for optimizing RAG in low-resource language contexts.

**Abstract:** Large language models (LLMs) perform well in medical QA, but their effectiveness in Japanese contexts is limited due to privacy constraints that prevent the use of commercial models like GPT-4 in clinical settings. As a result, recent efforts focus on instruction-tuning open-source LLMs, though the potential of combining them with retrieval-augmented generation (RAG) remains underexplored. To bridge this gap, we are the first to explore a knowledge graph-based (KG) RAG framework for Japanese medical QA small-scale open-source LLMs. Experimental results show that KG-based RAG has only a limited impact on Japanese medical QA using small-scale open-source LLMs. Further case studies reveal that the effectiveness of the RAG is sensitive to the quality and relevance of the external retrieved content. These findings offer valuable insights into the challenges and potential of applying RAG in Japanese medical QA, while also serving as a reference for other low-resource languages.

</details>


### [133] [SemEval-2025 Task 3: Mu-SHROOM, the Multilingual Shared Task on Hallucinations and Related Observable Overgeneration Mistakes](https://arxiv.org/abs/2504.11975)

*Raúl Vázquez, Timothee Mickus, Elaine Zosa, Teemu Vahtola, Jörg Tiedemann, Aman Sinha, Vincent Segonne, Fernando Sánchez-Vega, Alessandro Raganato, Jindřich Libovický, Jussi Karlgren, Shaoxiong Ji, Jindřich Helcl, Liane Guillou, Ona de Gibert, Jaione Bengoetxea, Joseph Attieh, Marianna Apidianaki*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, span-labeling, SemEval-2025

**Relevance Score:** 8

**TL;DR:** This paper presents the Mu-SHROOM shared task on detecting hallucinations in outputs from instruction-tuned large language models across 14 languages, analyzing over 2,600 submissions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of detecting hallucinations and overgeneration mistakes in outputs from large language models, emphasizing the significance of this issue in AI applications.

**Method:** The hallucination detection problem is framed as a span-labeling task, supported by empirical analysis of submissions from participating teams.

**Key Contributions:** Introduction of the Mu-SHROOM shared task for detecting hallucinations in LLM outputs., Analysis of 2,618 submissions to identify factors influencing task performance., Insights into language-related challenges in hallucination detection.

**Result:** The results include various methodologies from 43 teams, highlighting factors that contribute to strong performance and the variability of hallucinations in different languages.

**Limitations:** Variability of hallucinations across languages and high annotator disagreement when labeling.

**Future Work:** Further research on improving detection methods and reducing annotator disagreement.

**Conclusion:** The study emphasizes the importance of addressing varying hallucination degrees and annotator disagreements in labeling tasks, suggesting areas for future research.

**Abstract:** We present the Mu-SHROOM shared task which is focused on detecting hallucinations and other overgeneration mistakes in the output of instruction-tuned large language models (LLMs). Mu-SHROOM addresses general-purpose LLMs in 14 languages, and frames the hallucination detection problem as a span-labeling task. We received 2,618 submissions from 43 participating teams employing diverse methodologies. The large number of submissions underscores the interest of the community in hallucination detection. We present the results of the participating systems and conduct an empirical analysis to identify key factors contributing to strong performance in this task. We also emphasize relevant current challenges, notably the varying degree of hallucinations across languages and the high annotator disagreement when labeling hallucination spans.

</details>


### [134] [Gauging Overprecision in LLMs: An Empirical Study](https://arxiv.org/abs/2504.12098)

*Adil Bahaj, Hamed Rahimi, Mohamed Chetouani, Mounir Ghogho*

**Main category:** cs.CL

**Keywords:** Large Language Models, Overconfidence, Evaluation Techniques, Cognitive Science, Numerical Precision

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework to study overconfidence in large language models (LLMs), focusing on the concept of overprecision from cognitive science, and evaluates its effects on numerical tasks.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to quantify the trustworthiness of LLM outputs by addressing the issue of overconfidence, particularly in numerical generation tasks and its implications for understanding LLM reliability.

**Method:** The framework consists of three phases: generation (where LLMs produce answers with specified confidence intervals), refinement (where answers are improved), and evaluation (where the internal workings of LLMs are analyzed).

**Key Contributions:** Introduces a novel framework for studying overconfidence in LLMs., Findings on the lack of calibration of LLMs for numerical tasks., Identifies key differences in LLM performance based on task and prompting.

**Result:** The study identifies that LLMs exhibit high levels of uncalibrated responses in numerical tasks, show no correlation between interval length and confidence levels, and demonstrate varying numerical precision based on task and prompting techniques.

**Limitations:** The study primarily focuses on numerical tasks and may not generalize to other types of tasks involving LLMs.

**Future Work:** Future research could explore overconfidence in different LLM architectures and tasks, and investigate strategies to improve calibration in LLM outputs.

**Conclusion:** This research provides new insights into LLM overconfidence, highlighting areas that require attention and setting a baseline for future studies on overprecision in LLMs.

**Abstract:** Recently, overconfidence in large language models (LLMs) has garnered considerable attention due to its fundamental importance in quantifying the trustworthiness of LLM generation. However, existing approaches prompt the \textit{black box LLMs} to produce their confidence (\textit{verbalized confidence}), which can be subject to many biases and hallucinations. Inspired by a different aspect of overconfidence in cognitive science called \textit{overprecision}, we designed a framework for its study in black box LLMs. This framework contains three main phases: 1) generation, 2) refinement and 3) evaluation. In the generation phase we prompt the LLM to generate answers to numerical questions in the form of intervals with a certain level of confidence. This confidence level is imposed in the prompt and not required for the LLM to generate as in previous approaches. We use various prompting techniques and use the same prompt multiple times to gauge the effects of randomness in the generation process. In the refinement phase, answers from the previous phase are refined to generate better answers. The LLM answers are evaluated and studied in the evaluation phase to understand its internal workings. This study allowed us to gain various insights into LLM overprecision: 1) LLMs are highly uncalibrated for numerical tasks 2) there is no correlation between the length of the interval and the imposed confidence level, which can be symptomatic of a a) lack of understanding of the concept of confidence or b) inability to adjust self-confidence by following instructions, {3) LLM numerical precision differs depending on the task, scale of answer and prompting technique 4) Refinement of answers doesn't improve precision in most cases. We believe this study offers new perspectives on LLM overconfidence and serves as a strong baseline for overprecision in LLMs.

</details>


### [135] [Generative AI Act II: Test Time Scaling Drives Cognition Engineering](https://arxiv.org/abs/2504.13828)

*Shijie Xia, Yiwei Qin, Xuefeng Li, Yan Ma, Run-Ze Fan, Steffi Chern, Haoyang Zou, Fan Zhou, Xiangkun Hu, Jiahe Jin, Yanheng He, Yixin Ye, Yixiu Liu, Pengfei Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, cognition engineering, test-time scaling

**Relevance Score:** 8

**TL;DR:** The paper discusses the evolution from traditional Large Language Models (LLMs) to advanced cognition engineering in AI, outlining the shift towards thought-construction engines and the implications for user interaction with AI.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of first-generation LLMs and explore the potential of cognition engineering in improving human-AI interaction.

**Method:** The paper presents comprehensive tutorials and implementations for understanding cognition engineering and its application in generative AI.

**Key Contributions:** Clarification of cognition engineering concepts, Tutorials for implementing advanced AI techniques, A repository of papers on test-time scaling

**Result:** The introduction of test-time scaling techniques is highlighted as pivotal in transforming LLMs from knowledge-retrieval to thought-construction engines, fostering deeper cognitive connections with AI.

**Limitations:** Potentially limited access to computational resources for practical application of discussed techniques.

**Future Work:** Encouragement for ongoing contributions to the field and continuous updates to the shared repository.

**Conclusion:** The paper emphasizes the critical need for practitioners to engage with cognition engineering to fully leverage the capabilities of advanced AI systems.

**Abstract:** The first generation of Large Language Models - what might be called "Act I" of generative AI (2020-2023) - achieved remarkable success through massive parameter and data scaling, yet exhibited fundamental limitations such as knowledge latency, shallow reasoning, and constrained cognitive processes. During this era, prompt engineering emerged as our primary interface with AI, enabling dialogue-level communication through natural language. We now witness the emergence of "Act II" (2024-present), where models are transitioning from knowledge-retrieval systems (in latent space) to thought-construction engines through test-time scaling techniques. This new paradigm establishes a mind-level connection with AI through language-based thoughts. In this paper, we clarify the conceptual foundations of cognition engineering and explain why this moment is critical for its development. We systematically break down these advanced approaches through comprehensive tutorials and optimized implementations, democratizing access to cognition engineering and enabling every practitioner to participate in AI's second act. We provide a regularly updated collection of papers on test-time scaling in the GitHub Repository: https://github.com/GAIR-NLP/cognition-engineering

</details>


### [136] [Bigram Subnetworks: Mapping to Next Tokens in Transformer Language Models](https://arxiv.org/abs/2504.15471)

*Tyler A. Chang, Benjamin K. Bergen*

**Main category:** cs.CL

**Keywords:** Transformer models, bigram predictions, subnetwork, language models, activation transformation

**Relevance Score:** 7

**TL;DR:** This paper identifies minimal subnetwork structures within Transformer language models that are responsible for bigram predictions and their necessity for model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the minimal transformation processes in language models that are crucial for generating next token predictions.

**Method:** The study involved isolating language model subnetworks that focus on bigram predictions, examining their effectiveness within fully trained models with up to 1 billion parameters.

**Key Contributions:** Identification of bigram subnetworks in language models up to 1B parameters., Demonstration of their critical role in model performance despite their small size., Insight into the structural organization of these subnetworks relating to predictions.

**Result:** It was found that bigram subnetworks are crucial for performance, making proper predictions with less than 0.2% of model parameters, primarily located in the first MLP layer of the Transformer model.

**Limitations:** 

**Future Work:** Further exploration into how these minimal subnetworks can serve as a basis for understanding more complex language model functionalities.

**Conclusion:** Bigram subnetworks represent a necessary and sufficient component for basic next token predictions, serving as a foundation for future research on more complex language model circuits.

**Abstract:** In Transformer language models, activation vectors transform from current token embeddings to next token predictions as they pass through the model. To isolate a minimal form of this transformation, we identify language model subnetworks that make bigram predictions, naive next token predictions based only on the current token. We find that bigram subnetworks can be found in fully trained language models up to 1B parameters, and these subnetworks are critical for model performance even when they consist of less than 0.2% of model parameters. Bigram subnetworks are concentrated in the first Transformer MLP layer, and they overlap significantly with subnetworks trained to optimally prune a given model. Mechanistically, the bigram subnetworks often recreate a pattern from the full models where the first layer induces a sharp change that aligns activations with next token predictions rather than current token representations. Our results demonstrate that bigram subnetworks comprise a minimal subset of parameters that are both necessary and sufficient for basic next token predictions in language models, and they help drive the transformation from current to next token activations in the residual stream. These subnetworks can lay a foundation for studying more complex language model circuits by building up from a minimal circuit.

</details>


### [137] [SARI: Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning](https://arxiv.org/abs/2504.15900)

*Cheng Wen, Tingwei Guo, Shuaijiang Zhao, Wei Zou, Xiangang Li*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Audio-Language Models, Curriculum Learning, Structured Reasoning, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper extends the GRPO framework to improve audio-language models through structured reasoning and curriculum learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how reinforcement learning can enhance audio-language reasoning capabilities in large language models, building upon prior successes in text-based models.

**Method:** The authors applied a two-stage regimen involving supervised fine-tuning on structured and unstructured reasoning, followed by curriculum-guided Group-Relative Policy Optimization (GRPO), to a Large Audio-Language Model (LALM).

**Key Contributions:** Development of a structured audio reasoning model (SARI), Demonstration of significant accuracy improvement over baseline models, Insights into the importance of structured reasoning and curriculum learning in RL for audio tasks.

**Result:** The structured audio reasoning model, SARI, achieved a 16.35% improvement in average accuracy over the baseline Qwen2-Audio-7B-Instruct and reached state-of-the-art performance of 67.08% on the MMAU test-mini benchmark.

**Limitations:** 

**Future Work:** Exploration of additional architectures and deeper implications of curriculum learning in various audio-language tasks.

**Conclusion:** Explicit and structured reasoning with curriculum learning significantly enhances the understanding of audio-language tasks in large language models.

**Abstract:** Recent work shows that reinforcement learning(RL) can markedly sharpen the reasoning ability of large language models (LLMs) by prompting them to "think before answering." Yet whether and how these gains transfer to audio-language reasoning remains largely unexplored. We extend the Group-Relative Policy Optimization (GRPO) framework from DeepSeek-R1 to a Large Audio-Language Model (LALM), and construct a 32k sample multiple-choice corpus. Using a two-stage regimen supervised fine-tuning on structured and unstructured chains-of-thought, followed by curriculum-guided GRPO, we systematically compare implicit vs. explicit, and structured vs. free form reasoning under identical architectures. Our structured audio reasoning model, SARI (Structured Audio Reasoning via Curriculum-Guided Reinforcement Learning), achieves a 16.35% improvement in average accuracy over the base model Qwen2-Audio-7B-Instruct. Furthermore, the variant built upon Qwen2.5-Omni reaches state-of-the-art performance of 67.08% on the MMAU test-mini benchmark. Ablation experiments show that on the base model we use: (i) SFT warm-up is important for stable RL training, (ii) structured chains yield more robust generalization than unstructured ones, and (iii) easy-to-hard curricula accelerate convergence and improve final performance. These findings demonstrate that explicit, structured reasoning and curriculum learning substantially enhances audio-language understanding.

</details>
