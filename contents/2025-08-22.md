# 2025-08-22

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 12]

- [cs.CL](#cs.CL) [Total: 74]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LitForager: Exploring Multimodal Literature Foraging Strategies in Immersive Sensemaking](https://arxiv.org/abs/2508.15043)

*Haoyang Yang, Elliott H. Faa, Weijian Liu, Shunan Guo, Duen Horng Chau, Yalong Yang*

**Main category:** cs.HC

**Keywords:** immersive analytics, information foraging, literature exploration, WebXR, spatial sensemaking

**Relevance Score:** 8

**TL;DR:** Introduction of LitForager, an interactive tool for literature exploration in immersive environments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid increase in academic publications makes it crucial for researchers to efficiently explore and comprehend literature, particularly in the information foraging phase, which is less addressed compared to information synthesis.

**Method:** LitForager was developed using WebXR and built upon insights from a formative study with researchers. It employs network-based visualizations and multimodal interactions to facilitate the exploration of research literature.

**Key Contributions:**

	1. Introduction of a novel tool for immersive literature exploration
	2. Support for information foraging in academic research
	3. Demonstration of effective spatial sensemaking through user studies

**Result:** An observational user study with 15 researchers showed that LitForager effectively supported fluid foraging strategies and spatial sensemaking, enhancing the literature exploration process.

**Limitations:** 

**Conclusion:** LitForager addresses the critical gap in information foraging within immersive analytics, promoting enhanced sensemaking through its interactive features.

**Abstract:** Exploring and comprehending relevant academic literature is a vital yet challenging task for researchers, especially given the rapid expansion in research publications. This task fundamentally involves sensemaking - interpreting complex, scattered information sources to build understanding. While emerging immersive analytics tools have shown cognitive benefits like enhanced spatial memory and reduced mental load, they predominantly focus on information synthesis (e.g., organizing known documents). In contrast, the equally important information foraging phase - discovering and gathering relevant literature - remains underexplored within immersive environments, hindering a complete sensemaking workflow. To bridge this gap, we introduce LitForager, an interactive literature exploration tool designed to facilitate information foraging of research literature within an immersive sensemaking workflow using network-based visualizations and multimodal interactions. Developed with WebXR and informed by a formative study with researchers, LitForager supports exploration guidance, spatial organization, and seamless transition through a 3D literature network. An observational user study with 15 researchers demonstrated LitForager's effectiveness in supporting fluid foraging strategies and spatial sensemaking through its multimodal interface.

</details>


### [2] [Understanding Accessibility Needs of Blind Authors on CMS-Based Websites](https://arxiv.org/abs/2508.15045)

*Guillermo Vera-Amaro, José Rafael Rojano-Cáceres*

**Main category:** cs.HC

**Keywords:** Accessibility, Content Management Systems, Blind Users, Human-Computer Interaction, AI Tools

**Relevance Score:** 9

**TL;DR:** This paper explores accessibility challenges for blind users in Content Management Systems (CMS), emphasizing user-centered design and the integration of AI tools to enhance usability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited attention given to blind users as content creators in CMS and improve web accessibility research in this area.

**Method:** A two-fold evaluation involving automated tools and manual usability testing with blind and sighted participants, supplemented by expert analysis using the Barrier Walkthrough method.

**Key Contributions:**

	1. Identification of CMS accessibility barriers for blind users.
	2. Evidence of effectiveness of AI-generated image descriptions.
	3. Recommendations for improving CMS design for better usability.

**Result:** Challenges primarily stem from block-based interfaces that are falsely marked as accessible, leading to usability issues; improvements were noted with text-based editors and AI tools for image descriptions.

**Limitations:** The study involved a limited number of participants and may not be generalizable to all CMS platforms.

**Conclusion:** The study highlights the need for consistent navigation structures and user-centered design practices to enhance CMS accessibility for blind authors.

**Abstract:** This paper addresses the limited attention given to blind users as content creators in Content Management Systems (CMS), a gap that remains under-explored in web accessibility research. For blind authors, effective interaction with CMS platforms requires more than technical compliance; it demands interfaces designed with semantic clarity, predictable navigation, and meaningful feedback for screen reader users. This study investigates the accessibility barriers blind users face when performing key tasks, such as page creation, menu editing, and image publishing, using CMS platforms. A two-fold evaluation was conducted using automated tools and manual usability testing with three blind and one sighted participant, complemented by expert analysis based on the Barrier Walkthrough method. Results showed that block-based interfaces were particularly challenging, often marked as accessible by automated tools but resulting in critical usability issues during manual evaluation. The use of a text-based editor, the integration of AI-generated image descriptions, and training aligned with screen reader workflows, significantly improved usability and autonomy. These findings underscore the limitations of automated assessments and highlight the importance of user-centered design practices. Enhancing CMS accessibility requires consistent navigation structures, reduced reliance on visual interaction patterns, and the integration of AI tools that support blind content authors throughout the content creation process.

</details>


### [3] [QueryGenie: Making LLM-Based Database Querying Transparent and Controllable](https://arxiv.org/abs/2508.15146)

*Longfei Chen, Shenghan Gao, Shiwei Wang, Ken Lin, Yun Wang, Quan Li*

**Main category:** cs.HC

**Keywords:** Conversational interfaces, Large language models, User interaction, Query generation, Human feedback

**Relevance Score:** 9

**TL;DR:** QueryGenie is an interactive system designed to enhance user interaction with LLM-driven query generation by improving transparency and control.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in conversational user interfaces powered by LLMs, such as misinterpretation of user intent and hallucinated content.

**Method:** The system allows users to monitor and guide the query generation process through incremental reasoning and real-time validation.

**Key Contributions:**

	1. Development of the QueryGenie interactive system
	2. Mechanisms for real-time validation and user guidance
	3. Incremental reasoning to enhance understanding of query generation

**Result:** QueryGenie enables users to iteratively refine query logic, aligning it with their intent more effectively.

**Limitations:** 

**Conclusion:** The proposed system aims to promote a more reliable and practical utility for LLM-driven queries.

**Abstract:** Conversational user interfaces powered by large language models (LLMs) have significantly lowered the technical barriers to database querying. However, existing tools still encounter several challenges, such as misinterpretation of user intent, generation of hallucinated content, and the absence of effective mechanisms for human feedback-all of which undermine their reliability and practical utility. To address these issues and promote a more transparent and controllable querying experience, we proposed QueryGenie, an interactive system that enables users to monitor, understand, and guide the LLM-driven query generation process. Through incremental reasoning, real-time validation, and responsive interaction mechanisms, users can iteratively refine query logic and ensure alignment with their intent.

</details>


### [4] [ReviseMate: Exploring Contextual Support for Digesting STEM Paper Reviews](https://arxiv.org/abs/2508.15148)

*Yuansong Xu, Shuhao Zhang, Yijie Fan, Shaohan Shi, Zhenhui Peng, Quan Li*

**Main category:** cs.HC

**Keywords:** human-computer interaction, review feedback, interactive tools, user study, academic writing

**Relevance Score:** 8

**TL;DR:** This paper presents ReviseMate, an interactive system designed to improve how researchers assimilate and integrate reviewer feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses challenges researchers face in processing reviewer feedback, including time consumption and the need for analytical skills.

**Method:** The authors conducted interviews to identify authors' concerns and developed storyboards that led to the creation of ReviseMate. A controlled study with 31 users was performed, followed by field deployment with 6 users.

**Key Contributions:**

	1. Introduction of ReviseMate as a novel interactive tool for reviewing feedback
	2. Empirical testing demonstrating the effectiveness of ReviseMate
	3. Insights from interviews guiding the design of the tool

**Result:** Users found ReviseMate superior to traditional methods, with positive interaction feedback indicating improved comprehension during review digestion.

**Limitations:** The study's user tests involved a limited number of participants, potentially affecting generalizability.

**Conclusion:** ReviseMate significantly enhances the assimilation of reviewer feedback, suggesting a strong potential for interactive tools in the academic review process.

**Abstract:** Effectively assimilating and integrating reviewer feedback is crucial for researchers seeking to refine their papers and handle potential rebuttal phases in academic venues. However, traditional review digestion processes present challenges such as time consumption, reading fatigue, and the requisite for comprehensive analytical skills. Prior research on review analysis often provides theoretical guidance with limited targeted support. Additionally, general text comprehension tools overlook the intricate nature of comprehensively understanding reviews and lack contextual assistance. To bridge this gap, we formulated research questions to explore the authors' concerns and methods for enhancing comprehension during the review digestion phase. Through interviews and the creation of storyboards, we developed ReviseMate, an interactive system designed to address the identified challenges. A controlled user study (N=31) demonstrated the superiority of ReviseMate over baseline methods, with positive feedback regarding user interaction. Subsequent field deployment (N=6) further validated the effectiveness of ReviseMate in real-world review digestion scenarios. These findings underscore the potential of interactive tools to significantly enhance the assimilation and integration of reviewer feedback during the manuscript review process.

</details>


### [5] [Evaluating an Immersive Analytics Application at an Enterprise Business Intelligence Customer Conference](https://arxiv.org/abs/2508.15152)

*Matthew Brehmer, Ginger Gloystein, Bailiang Zhou, Abby Gray, Sruthi Pillai, Ben Medina, Vidya Setlur*

**Main category:** cs.HC

**Keywords:** immersive analytics, usability, head-mounted displays, business intelligence, evaluation methodologies

**Relevance Score:** 7

**TL;DR:** Evaluation of an immersive analytics application (Tableau for visionOS) reveals challenges in assessing usability and utility in an enterprise context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To gather diverse feedback on usability and utility of a new immersive analytics application at a Business Intelligence conference.

**Method:** Formative evaluation with 22 participants assessing usability and feedback on head-mounted displays for business intelligence applications.

**Key Contributions:**

	1. Insights on usability challenges specific to immersive analytics applications.
	2. Recommendations for new evaluation methodologies that consider unique interaction patterns with 3D interfaces.
	3. Enterprise perspective on the use of head-mounted displays in analytical workflows.

**Result:** Insights on usability of Tableau for visionOS and the potential of HMDs for engaging with BI data were collected, highlighting the need for new evaluation methods.

**Limitations:** Study conducted at a specific conference may limit the generalizability of findings.

**Conclusion:** The study suggests integrating qualitative and quantitative evaluation measures for immersive analytics in enterprise settings.

**Abstract:** We reflect on an evaluation of an immersive analytics application (Tableau for visionOS) conducted at a large enterprise business intelligence (BI) conference. Conducting a study in such a context offered an opportunistic setting to gather diverse feedback. However, this setting also highlighted the challenge of evaluating usability while also assessing potential utility, as feedback straddled between the novelty of the experience and the practicality of the application in participants' analytical workflows. This formative evaluation with 22 participants allowed us to gather insights with respect to the usability of Tableau for visionOS, along with broader perspectives on the potential for head-mounted displays (HMDs) to promote new ways to engage with BI data. Our experience suggests a need for new evaluation considerations that integrate qualitative and quantitative measures and account for unique interaction patterns with 3D representations and interfaces accessible via an HMD. Overall, we contribute an enterprise perspective on evaluation methodologies for immersive analytics.

</details>


### [6] [GenTune: Toward Traceable Prompts to Improve Controllability of Image Refinement in Environment Design](https://arxiv.org/abs/2508.15227)

*Wen-Fan Wang, Ting-Ying Lee, Chien-Ting Lu, Che-Wei Hsu, Nil Ponsa Campany, Yu Chen, Mike Y. Chen, Bing-Yu Chen*

**Main category:** cs.HC

**Keywords:** human-computer interaction, generative AI, environment design, LLM, image refinement

**Relevance Score:** 8

**TL;DR:** GenTune enhances human-AI collaboration in environment design by clarifying AI-generated prompts for image content, improving comprehension and refinement efficiency among designers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Environment designers face challenges with lengthy LLM-generated prompts and inpainting, hindering their ability to create coherent images.

**Method:** GenTune clarifies the mapping between AI-generated prompts and specific image content, allowing designers to revise prompt labels for targeted image refinement.

**Key Contributions:**

	1. Introduces GenTune for better understanding of AI-generated prompts
	2. Improves designers' ability to refine images with global consistency
	3. Demonstrates effectiveness through user studies and real-world application

**Result:** In studies with designers, GenTune significantly improved prompt-image comprehension, refinement quality, efficiency, and overall satisfaction (all p < .01).

**Limitations:** The study's scope was limited to specific design contexts and a small number of designers.

**Conclusion:** The system demonstrated effective real-world application in environment design studios, enhancing collaborative design processes.

**Abstract:** Environment designers in the entertainment industry create imaginative 2D and 3D scenes for games, films, and television, requiring both fine-grained control of specific details and consistent global coherence. Designers have increasingly integrated generative AI into their workflows, often relying on large language models (LLMs) to expand user prompts for text-to-image generation, then iteratively refining those prompts and applying inpainting. However, our formative study with 10 designers surfaced two key challenges: (1) the lengthy LLM-generated prompts make it difficult to understand and isolate the keywords that must be revised for specific visual elements; and (2) while inpainting supports localized edits, it can struggle with global consistency and correctness. Based on these insights, we present GenTune, an approach that enhances human--AI collaboration by clarifying how AI-generated prompts map to image content. Our GenTune system lets designers select any element in a generated image, trace it back to the corresponding prompt labels, and revise those labels to guide precise yet globally consistent image refinement. In a summative study with 20 designers, GenTune significantly improved prompt--image comprehension, refinement quality, and efficiency, and overall satisfaction (all $p < .01$) compared to current practice. A follow-up field study with two studios further demonstrated its effectiveness in real-world settings.

</details>


### [7] [Visualization on Smart Wristbands: Results from an In-situ Design Workshop with Four Scenarios](https://arxiv.org/abs/2508.15249)

*Alaul Islam, Fairouz Grioui, Raimund Dachselt, Petra Isenberg*

**Main category:** cs.HC

**Keywords:** smart wristbands, data visualization, human-computer interaction, responsive design, wearable technology

**Relevance Score:** 7

**TL;DR:** This paper reports on an ideation workshop aimed at designing responsive data visualizations for smart wristbands, considering user arm postures during various activities.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to understand how the spatial layout and visualization design of data on smart wristbands can be optimized for different arm postures during various activities.

**Method:** The authors conducted a paper-based ideation workshop where participants explored how data visualizations should vary based on user activity scenarios (office work, leisurely walks, cycling, and driving).

**Key Contributions:**

	1. Identified key usage scenarios impacting visualization design on wristbands
	2. Emphasized user preference for responsive and adaptive visualizations
	3. Provided insights into nuanced design considerations based on arm posture

**Result:** The findings indicated that participants preferred responsive visualization designs that could adapt to movements, highlighting the importance of context in data presentation.

**Limitations:** The technology is not yet commercially available, and the study was conducted as a paper-based ideation exercise, which may not fully capture real-world usability.

**Conclusion:** Overall, the study emphasizes the need for adaptive design in data visualizations on wearable technology to provide effective user experiences across different contexts.

**Abstract:** We present the results of an in-situ ideation workshop for designing data visualizations on smart wristbands that can show data around the entire wrist of a wearer. Wristbands pose interesting challenges because the visibility of different areas of the band depends on the wearer's arm posture. We focused on four usage scenarios that lead to different postures: office work, leisurely walks, cycling, and driving. As the technology for smart wristbands is not yet commercially available, we conducted a paper-based ideation exercise that showed how spatial layout and visualization design on smart wristbands may need to vary depending on the types of data items of interest and arm postures. Participants expressed a strong preference for responsive visualization designs that could adapt to the movement of wearers' arms. Supplemental material from the study is available here: https://osf.io/4hrca/.

</details>


### [8] [Spatio-Temporal Mixed and Augmented Reality Experience Description for Interactive Playback](https://arxiv.org/abs/2508.15258)

*Dooyoung Kim, Woontack Woo*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Augmented Reality, Interactive Playback, User Experience, Event Representation

**Relevance Score:** 6

**TL;DR:** This paper introduces the Spatio-Temporal Mixed and Augmented Reality Experience Description (MAR-ED) framework, which enhances the capture and playback of events in augmented reality by enabling user-driven interactivity and dynamic adaptation to environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a standardized method to represent past events in interactive augmented reality that enhances the existing capabilities of spatial media technologies.

**Method:** The MAR-ED framework utilizes three core primitives: Event Primitives for semantic scene graph representation, Keyframe Primitives for data access, and Playback Primitives for adaptive user-driven playback.

**Key Contributions:**

	1. Introduction of Event, Keyframe, and Playback Primitives for augmented reality experiences.
	2. A three-stage process for creating adaptive MAR experiences.
	3. Potential applications in training, cultural heritage, and interactive storytelling.

**Result:** The MAR-ED framework allows recorded events to evolve into immersive experiences that adapt to the viewer's environment and narrative, enhancing engagement and interactivity.

**Limitations:** No specific limitations discussed in the abstract.

**Conclusion:** MAR-ED facilitates the transformation of static recordings into dynamic, interactive experiences that can be applied to various fields like training, cultural heritage, and storytelling.

**Abstract:** We propose the Spatio-Temporal Mixed and Augmented Reality Experience Description (MAR-ED), a novel framework to standardize the representation of past events for interactive and adaptive playback in a user's present physical space. While current spatial media technologies have primarily focused on capturing or replaying content as static assets, often disconnected from the viewer's environment or offering limited interactivity, the means to describe an experience's underlying semantic and interactive structure remains underexplored. We propose a descriptive framework called MAR-ED based on three core primitives: 1) Event Primitives for semantic scene graph representation, 2) Keyframe Primitives for efficient and meaningful data access, and 3) Playback Primitives for user-driven adaptive interactive playback of recorded MAR experience. The proposed flowchart of the three-stage process of the proposed MAR-ED framework transforms a recorded experience into a unique adaptive MAR experience during playback, where its spatio-temporal structure dynamically conforms to a new environment and its narrative can be altered by live user input. Drawing on this framework, personal digital memories and recorded events can evolve beyond passive 2D/3D videos into immersive, spatially-integrated group experiences, opening new paradigms for training, cultural heritage, and interactive storytelling without requiring complex, per-user adaptive rendering.

</details>


### [9] [Foundation Models for Cross-Domain EEG Analysis Application: A Survey](https://arxiv.org/abs/2508.15716)

*Hongqi Li, Yitong Chen, Yujuan Wang, Weihang Ni, Haodong Zhang*

**Main category:** cs.HC

**Keywords:** EEG analysis, foundation models, taxonomy, multimodal frameworks, interpretability

**Relevance Score:** 4

**TL;DR:** The paper introduces a comprehensive taxonomy for foundation models in EEG analysis, addressing the fragmentation in research by categorizing advancements based on output modalities while highlighting challenges in model interpretability and real-world applicability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmented research landscape in EEG analysis by providing a systematic categorization of foundation models.

**Method:** The study presents a modality-oriented taxonomy for EEG analysis, classifying advancements based on output modalities and analyzing research ideas, theoretical foundations, and architectural innovations.

**Key Contributions:**

	1. First comprehensive taxonomy for foundation models in EEG analysis
	2. Systematic organization of research advances based on output modalities
	3. Identification of open challenges in EEG-based systems

**Result:** The paper organizes EEG analysis advancements into categories like EEG decoding, EEG-text, EEG-vision, EEG-audio, and multimodal frameworks, while also identifying challenges such as interpretability and generalization.

**Limitations:** The paper does not provide solutions to the identified challenges but highlights them for future research.

**Conclusion:** This work serves as a reference framework for future developments in EEG models and aims to enhance their scalability and applicability in real-world scenarios.

**Abstract:** Electroencephalography (EEG) analysis stands at the forefront of neuroscience and artificial intelligence research, where foundation models are reshaping the traditional EEG analysis paradigm by leveraging their powerful representational capacity and cross-modal generalization. However, the rapid proliferation of these techniques has led to a fragmented research landscape, characterized by diverse model roles, inconsistent architectures, and a lack of systematic categorization. To bridge this gap, this study presents the first comprehensive modality-oriented taxonomy for foundation models in EEG analysis, systematically organizing research advances based on output modalities of the native EEG decoding, EEG-text, EEG-vision, EEG-audio, and broader multimodal frameworks. We rigorously analyze each category's research ideas, theoretical foundations, and architectural innovations, while highlighting open challenges such as model interpretability, cross-domain generalization, and real-world applicability in EEG-based systems. By unifying this dispersed field, our work not only provides a reference framework for future methodology development but accelerates the translation of EEG foundation models into scalable, interpretable, and online actionable solutions.

</details>


### [10] [Demystifying Reward Design in Reinforcement Learning for Upper Extremity Interaction: Practical Guidelines for Biomechanical Simulations in HCI](https://arxiv.org/abs/2508.15727)

*Hannah Selder, Florian Fischer, Per Ola Kristensson, Arthur Fleig*

**Main category:** cs.HC

**Keywords:** reinforcement learning, reward functions, HCI, biomechanical simulations, user modeling

**Relevance Score:** 8

**TL;DR:** This paper analyzes the design of reward functions for reinforcement learning in biomechanical simulations and provides guidelines to improve efficiency in HCI task performance.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize the reward function design process in reinforcement learning for HCI applications, which is often hindered by intuition-based approaches.

**Method:** The paper conducts a systematic analysis of reward function components (effort minimization, task completion bonuses, and target proximity incentives) across various HCI tasks.

**Key Contributions:**

	1. Systematic analysis of reward function components in HCI tasks.
	2. Practical guidelines for designing reward functions in biomechanical simulations.
	3. Validation of guidelines on remote control and keyboard typing tasks.

**Result:** Proximity incentives are critical for guiding user movement, completion bonuses enhance task success, and effort minimization terms can refine motion regularity when properly scaled.

**Limitations:** The study focuses on a limited range of HCI tasks and might not generalize to all biomechanical simulations.

**Conclusion:** The paper offers practical guidelines for designing reward functions to create effective biomechanical simulations without requiring extensive knowledge of reinforcement learning.

**Abstract:** Designing effective reward functions is critical for reinforcement learning-based biomechanical simulations, yet HCI researchers and practitioners often waste (computation) time with unintuitive trial-and-error tuning. This paper demystifies reward function design by systematically analyzing the impact of effort minimization, task completion bonuses, and target proximity incentives on typical HCI tasks such as pointing, tracking, and choice reaction. We show that proximity incentives are essential for guiding movement, while completion bonuses ensure task success. Effort terms, though optional, help refine motion regularity when appropriately scaled. We perform an extensive analysis of how sensitive task success and completion time depend on the weights of these three reward components. From these results we derive practical guidelines to create plausible biomechanical simulations without the need for reinforcement learning expertise, which we then validate on remote control and keyboard typing tasks. This paper advances simulation-based interaction design and evaluation in HCI by improving the efficiency and applicability of biomechanical user modeling for real-world interface development.

</details>


### [11] ["Does the cafe entrance look accessible? Where is the door?" Towards Geospatial AI Agents for Visual Inquiries](https://arxiv.org/abs/2508.15752)

*Jon E. Froehlich, Jared Hwang, Zeyu Wang, John S. O'Meara, Xia Su, William Huang, Yang Zhang, Alex Fiannaca, Philip Nelson, Shaun Kane*

**Main category:** cs.HC

**Keywords:** Geo-Visual Agents, multimodal AI, geospatial images, GIS data, accessibility

**Relevance Score:** 6

**TL;DR:** The paper presents the concept of Geo-Visual Agents, multimodal AI agents designed to address geo-visual inquiries by analyzing both geospatial images and traditional GIS data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Interactive digital maps are limited by pre-existing structured data, which restricts their ability to effectively answer nuanced geo-visual questions regarding the world.

**Method:** The authors propose a framework for Geo-Visual Agents that combines analysis of geospatial images (streetscapes, place-based photos, aerial imagery) with traditional GIS data.

**Key Contributions:**

	1. Introduction of Geo-Visual Agents that utilize multimodal data.
	2. Demonstration of applications through exemplars.
	3. Identification of future challenges and opportunities for research in geo-visual inquiries.

**Result:** The paper exemplifies the Geo-Visual Agents in three specific cases and highlights their potential to enhance interactions with geographic information.

**Limitations:** Limited to the exploration of data processing methodologies and challenges, without delving into the technical aspects of implementation.

**Conclusion:** Geo-Visual Agents present an innovative solution to improve the understanding of complex spatial queries, leading to improved accessibility and interaction with geographic data.

**Abstract:** Interactive digital maps have revolutionized how people travel and learn about the world; however, they rely on pre-existing structured data in GIS databases (e.g., road networks, POI indices), limiting their ability to address geo-visual questions related to what the world looks like. We introduce our vision for Geo-Visual Agents--multimodal AI agents capable of understanding and responding to nuanced visual-spatial inquiries about the world by analyzing large-scale repositories of geospatial images, including streetscapes (e.g., Google Street View), place-based photos (e.g., TripAdvisor, Yelp), and aerial imagery (e.g., satellite photos) combined with traditional GIS data sources. We define our vision, describe sensing and interaction approaches, provide three exemplars, and enumerate key challenges and opportunities for future work.

</details>


### [12] [Balancing Exploration and Cybersickness: Investigating Curiosity-Driven Behavior in Virtual Environments](https://arxiv.org/abs/2501.04905)

*Tangyao Li, Yuyang Wang*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Curiosity, Cybersickness, Navigation Behavior, Reinforcement Learning

**Relevance Score:** 7

**TL;DR:** This study explores how curiosity influences decision-making during virtual navigation and its relationship with cybersickness.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the diverse interactions and navigation behaviors of users during virtual reality experiences, and to address the limitations of existing models that fail to account for users' actual decision-making processes.

**Method:** Proposes a model incorporating curiosity as a driving factor in decision-making in virtual navigation while balancing exploration against cybersickness, evaluated through quantitative analysis of user behavior in varying virtual environments.

**Key Contributions:**

	1. Introduces curiosity as a key factor in understanding navigation behavior in virtual environments.
	2. Quantitative analysis demonstrating the relationship between curiosity levels and navigation strategies among users.
	3. Offers a model that considers psychological and environmental factors in decision-making during virtual navigation.

**Result:** Most VR users displayed conservative navigation strategies with generally negative curiosity, but curiosity increased with changes in the virtual environment, indicating a complex interplay between exploration and discomfort.

**Limitations:** Future research is needed to incorporate more psychological and environmental factors to refine the model.

**Conclusion:** The study provides a new quantitative model for understanding curiosity-driven navigation in VR, establishing a foundation for future research into integrating psychological factors for better prediction of navigation patterns.

**Abstract:** During virtual navigation, users exhibit varied interaction and navigation behaviors influenced by several factors. Existing theories and models have been developed to explain and predict these diverse patterns. While users often experience uncomfortable sensations, such as cybersickness, during virtual reality (VR) use, they do not always make optimal decisions to mitigate these effects. Although methods like reinforcement learning have been used to model decision-making processes, they typically rely on random selection to simulate actions, failing to capture the complexities of real navigation behavior. In this study, we propose curiosity as a key factor driving irrational decision-making, suggesting that users continuously balance exploration and cybersickness according to the free energy principle during virtual navigation. Our findings show that VR users generally adopt conservative strategies when navigating, with most participants displaying negative curiosity across trials. However, curiosity levels tend to rise when the virtual environment changes, illustrating the dynamic interplay between exploration and discomfort. This study provides a quantitative approach to decoding curiosity-driven behavior during virtual navigation, offering insights into how users balance exploration and the avoidance of cybersickness. Future research will further refine this model by incorporating additional psychological and environmental factors to improve the accuracy of navigation pattern predictions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [13] [Efficient Switchable Safety Control in LLMs via Magic-Token-Guided Co-Training](https://arxiv.org/abs/2508.14904)

*Jianfeng Si, Lin Sun, Zhewen Tan, Xiangzheng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Content Safety, Co-training Framework, Safety Behaviors, Fine-tuning

**Relevance Score:** 9

**TL;DR:** A unified co-training framework for content safety in LLMs that integrates multiple safety behaviors in a single SFT stage, enhancing efficiency and control.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods lack fine-grained, controllable safety behavior in LLMs and often require complex training pipelines.

**Method:** A co-training framework that activates safety behaviors via a simple instruction or 'magic token' that allows behavioral switching at inference time.

**Key Contributions:**

	1. Unified co-training framework for content safety
	2. Dynamic activation of safety behaviors using a magic token
	3. Improved safety performance with reduced training complexity

**Result:** The method matches the safety alignment quality of SFT+DPO, with improved safety performance and reduced training complexity compared to existing models.

**Limitations:** 

**Conclusion:** The proposed framework offers a scalable and efficient approach to ensure content safety in LLMs while significantly lowering deployment costs.

**Abstract:** Current methods for content safety in Large Language Models (LLMs), such as Supervised Fine-Tuning (SFT) and Reinforcement Learning from Human Feedback (RLHF), often rely on multi-stage training pipelines and lack fine-grained, post-deployment controllability. To address these limitations, we propose a unified co-training framework that efficiently integrates multiple safety behaviors: positive (lawful/prosocial), negative (unfiltered/risk-prone) and rejective (refusal-oriented/conservative) within a single SFT stage. Notably, each behavior is dynamically activated via a simple system-level instruction, or magic token, enabling stealthy and efficient behavioral switching at inference time. This flexibility supports diverse deployment scenarios, such as positive for safe user interaction, negative for internal red-teaming, and rejective for context-aware refusals triggered by upstream moderation signals. This co-training strategy induces a distinct Safety Alignment Margin in the output space, characterized by well-separated response distributions corresponding to each safety mode. The existence of this margin provides empirical evidence for the model's safety robustness and enables unprecedented fine-grained control. Experiments show that our method matches the safety alignment quality of SFT+DPO, with our 8B model notably surpassing DeepSeek-R1 (671B) in safety performance, while significantly reducing both training complexity and deployment costs. This work presents a scalable, efficient, and highly controllable solution for LLM content safety.

</details>


### [14] [Preliminary Ranking of WMT25 General Machine Translation Systems](https://arxiv.org/abs/2508.14909)

*Tom Kocmi, Eleftherios Avramidis, Rachel Bawden, Ondřej Bojar, Konstantin Dranch, Anton Dvorkovich, Sergey Dukanov, Natalia Fedorova, Mark Fishel, Markus Freitag, Thamme Gowda, Roman Grundkiewicz, Barry Haddow, Marzena Karpinska, Philipp Koehn, Howard Lakougna, Jessica Lundin, Kenton Murray, Masaaki Nagata, Stefano Perrella, Lorenzo Proietti, Martin Popel, Maja Popović, Parker Riley, Mariya Shmatova, Steinþór Steingrímsson, Lisa Yankovskaya, Vilém Zouhar*

**Main category:** cs.CL

**Keywords:** Machine Translation, Automatic Metrics, WMT25

**Relevance Score:** 4

**TL;DR:** Preliminary ranking of WMT25 General Machine Translation Shared Task based on automatic metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate MT systems using automatic metrics ahead of the official ranking based on human evaluation.

**Method:** Preliminary evaluations were conducted using automatic metrics to rank various machine translation systems.

**Key Contributions:**

	1. Preliminary ranking of MT systems for WMT25 using automatic metrics
	2. Identification of potential biases in automatic evaluation methods
	3. Aiding participants in preparing submissions based on initial results

**Result:** The preliminary rankings indicate potential biases towards systems using re-ranking techniques.

**Limitations:** The automatic ranking may favor systems using advanced techniques, not reflecting true performance.

**Conclusion:** The official WMT25 ranking will rely on human evaluation, considered more reliable than automatic metrics.

**Abstract:** We present the preliminary ranking of the WMT25 General Machine Translation Shared Task, in which MT systems have been evaluated using automatic metrics. As this ranking is based on automatic evaluations, it may be biased in favor of systems that employ re-ranking techniques, such as Quality Estimation re-ranking or Minimum Bayes Risk decoding. The official WMT25 ranking will be based on human evaluation, which is more reliable and will supersede the automatic ranking.   The purpose of this report is not to present the final findings of the General MT task, but rather to share preliminary results with task participants, which may be useful when preparing their system submission papers.

</details>


### [15] [Bridging the Culture Gap: A Framework for LLM-Driven Socio-Cultural Localization of Math Word Problems in Low-Resource Languages](https://arxiv.org/abs/2508.14913)

*Israel Abebe Azime, Tadesse Destaw Belay, Dietrich Klakow, Philipp Slusallek, Anshuman Chhabra*

**Main category:** cs.CL

**Keywords:** large language models, multilingual reasoning, cultural localization, math word problems, low-resource languages

**Relevance Score:** 8

**TL;DR:** The paper presents a framework for cultural localization of math word problems in low-resource languages using LLMs to create datasets with native entities, addressing biases in existing multilingual benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in multilingual and culturally-grounded mathematical reasoning in low-resource languages, which usually rely on English-centric datasets due to the scarcity of localized data.

**Method:** Developing a framework that utilizes LLMs to automatically construct datasets with culturally relevant names, organizations, and currencies for math word problems.

**Key Contributions:**

	1. Introduction of a cultural localization framework for math word problems.
	2. Demonstration of improved benchmark performance through the use of native entities.
	3. Reduction of English-centric biases in multilingual datasets.

**Result:** The experimental results demonstrate that the proposed framework reduces English-centric entity bias and enhances multilingual math abilities by introducing native entities.

**Limitations:** The approach may still depend on the quality of existing sources for native entities and may not cover all low-resource languages equally.

**Conclusion:** The framework significantly aids in producing culturally accurate datasets, improving the robustness of LLMs in handling multilingual mathematical problems.

**Abstract:** Large language models (LLMs) have demonstrated significant capabilities in solving mathematical problems expressed in natural language. However, multilingual and culturally-grounded mathematical reasoning in low-resource languages lags behind English due to the scarcity of socio-cultural task datasets that reflect accurate native entities such as person names, organization names, and currencies. Existing multilingual benchmarks are predominantly produced via translation and typically retain English-centric entities, owing to the high cost associated with human annotater-based localization. Moreover, automated localization tools are limited, and hence, truly localized datasets remain scarce. To bridge this gap, we introduce a framework for LLM-driven cultural localization of math word problems that automatically constructs datasets with native names, organizations, and currencies from existing sources. We find that translated benchmarks can obscure true multilingual math ability under appropriate socio-cultural contexts. Through extensive experiments, we also show that our framework can help mitigate English-centric entity bias and improves robustness when native entities are introduced across various languages.

</details>


### [16] [Improving LLMs for Machine Translation Using Synthetic Preference Data](https://arxiv.org/abs/2508.14951)

*Dario Vajda, Domen Vreš, Marko Robnik-Šikonja*

**Main category:** cs.CL

**Keywords:** Machine Translation, Direct Preference Optimization, Large Language Models

**Relevance Score:** 7

**TL;DR:** This paper discusses improvements to a general instruction-tuned large language model for machine translation, specifically using Slovene and employing Direct Preference Optimization (DPO) on a curated dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance machine translation using instruction-tuned LLMs with minimal data resources.

**Method:** The authors conducted DPO training on a subset of a public dataset, generating a training dataset through translation and ranking of English Wikipedia articles with two different LLMs.

**Key Contributions:**

	1. Improved machine translation performance using a minimal data approach.
	2. Utilization of Direct Preference Optimization for training LLMs.
	3. Demonstrated application focusing on Slovene language translation.

**Result:** The fine-tuned model showed a COMET score improvement of approximately 0.04 and 0.02 over the baseline models, along with a reduction in language and formatting errors.

**Limitations:** 

**Conclusion:** Fine-tuning the instruction-tuned model with DPO leads to significant improvements in translation quality and consistency.

**Abstract:** Large language models have emerged as effective machine translation systems. In this paper, we explore how a general instruction-tuned large language model can be improved for machine translation using relatively few easily produced data resources. Using Slovene as a use case, we improve the GaMS-9B-Instruct model using Direct Preference Optimization (DPO) training on a programmatically curated and enhanced subset of a public dataset. As DPO requires pairs of quality-ranked instances, we generated its training dataset by translating English Wikipedia articles using two LLMs, GaMS-9B-Instruct and EuroLLM-9B-Instruct. We ranked the resulting translations based on heuristics coupled with automatic evaluation metrics such as COMET. The evaluation shows that our fine-tuned model outperforms both models involved in the dataset generation. In comparison to the baseline models, the fine-tuned model achieved a COMET score gain of around 0.04 and 0.02, respectively, on translating Wikipedia articles. It also more consistently avoids language and formatting errors.

</details>


### [17] [Multilingual Datasets for Custom Input Extraction and Explanation Requests Parsing in Conversational XAI Systems](https://arxiv.org/abs/2508.14982)

*Qianli Wang, Tatiana Anikina, Nils Feldhus, Simon Ostermann, Fedor Splitt, Jiaao Li, Yoana Tsoneva, Sebastian Möller, Vera Schmitt*

**Main category:** cs.CL

**Keywords:** Conversational Explainable AI, Multilingual Datasets, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper introduces multilingual datasets and parsing approaches to improve conversational explainable AI systems using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in multilingual generalization and limited support for free-form custom inputs in current Conversational Explainable AI systems.

**Method:** The authors developed MultiCoXQL, a multilingual extension of CoXQL, and Compass, a dataset for custom input extraction. They evaluated multiple LLMs using different parsing strategies across these datasets.

**Key Contributions:**

	1. Introduced MultiCoXQL, a multilingual extension of the CoXQL dataset encompassing five languages.
	2. Developed Compass, a dataset focused on custom input extraction with 11 intents in five languages.
	3. Proposed a new parsing approach that enhances multilingual parsing performance.

**Result:** The evaluation demonstrated improved parsing performance for multilingual ConvXAI systems, showcasing the effectiveness of the proposed datasets and parsing approach.

**Limitations:** The study may require further testing across more diverse languages and user intents to fully assess generalization capabilities.

**Conclusion:** The introduction of MultiCoXQL and Compass aids in enhancing the adaptability of ConvXAI systems across multiple languages and input types, contributing to better user comprehension.

**Abstract:** Conversational explainable artificial intelligence (ConvXAI) systems based on large language models (LLMs) have garnered considerable attention for their ability to enhance user comprehension through dialogue-based explanations. Current ConvXAI systems often are based on intent recognition to accurately identify the user's desired intention and map it to an explainability method. While such methods offer great precision and reliability in discerning users' underlying intentions for English, a significant challenge in the scarcity of training data persists, which impedes multilingual generalization. Besides, the support for free-form custom inputs, which are user-defined data distinct from pre-configured dataset instances, remains largely limited. To bridge these gaps, we first introduce MultiCoXQL, a multilingual extension of the CoXQL dataset spanning five typologically diverse languages, including one low-resource language. Subsequently, we propose a new parsing approach aimed at enhancing multilingual parsing performance, and evaluate three LLMs on MultiCoXQL using various parsing strategies. Furthermore, we present Compass, a new multilingual dataset designed for custom input extraction in ConvXAI systems, encompassing 11 intents across the same five languages as MultiCoXQL. We conduct monolingual, cross-lingual, and multilingual evaluations on Compass, employing three LLMs of varying sizes alongside BERT-type models.

</details>


### [18] [Reward-Shifted Speculative Sampling Is An Efficient Test-Time Weak-to-Strong Aligner](https://arxiv.org/abs/2508.15044)

*Bolian Li, Yanran Wu, Xinyu Luo, Ruqi Zhang*

**Main category:** cs.CL

**Keywords:** large language models, human preference alignment, speculative sampling, inference cost, test-time alignment

**Relevance Score:** 9

**TL;DR:** The paper introduces the reward-Shifted Speculative Sampling (SSS) algorithm to align large language models with human preferences at inference time, reducing costs while improving performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning large language models (LLMs) with human preferences is crucial, but current test-time alignment techniques incur substantial inference costs, posing practical limitations.

**Method:** The reward-Shifted Speculative Sampling (SSS) algorithm uses a small draft model aligned with human preferences to predict future tokens, which helps exploit the distributional shift between draft and target models to enhance low-cost alignment without changing the target model.

**Key Contributions:**

	1. Introduced the reward-Shifted Speculative Sampling (SSS) algorithm for efficient LLM alignment.
	2. Theoretical demonstration of exploiting the distributional shift between aligned and unaligned models.
	3. Validation of improved alignment outcomes at lower inference costs.

**Result:** The SSS algorithm demonstrates superior gold reward scores with significantly reduced inference costs during test-time weak-to-strong alignment experiments.

**Limitations:** 

**Conclusion:** SSS effectively aligns LLMs with human preferences while minimizing inference costs, making it a promising approach for practical applications of LLM alignment.

**Abstract:** Aligning large language models (LLMs) with human preferences has become a critical step in their development. Recent research has increasingly focused on test-time alignment, where additional compute is allocated during inference to enhance LLM safety and reasoning capabilities. However, these test-time alignment techniques often incur substantial inference costs, limiting their practical application. We are inspired by the speculative sampling acceleration, which leverages a small draft model to efficiently predict future tokens, to address the efficiency bottleneck of test-time alignment. We introduce the reward-Shifted Speculative Sampling (SSS) algorithm, in which the draft model is aligned with human preferences, while the target model remains unchanged. We theoretically demonstrate that the distributional shift between the aligned draft model and the unaligned target model can be exploited to recover the RLHF optimal solution without actually obtaining it, by modifying the acceptance criterion and bonus token distribution. Our algorithm achieves superior gold reward scores at a significantly reduced inference cost in test-time weak-to-strong alignment experiments, thereby validating both its effectiveness and efficiency.

</details>


### [19] [LongRecall: A Structured Approach for Robust Recall Evaluation in Long-Form Text](https://arxiv.org/abs/2508.15085)

*MohamamdJavad Ardestani, Ehsan Kamalloo, Davood Rafiei*

**Main category:** cs.CL

**Keywords:** recall evaluation, machine-generated text, long-form question answering

**Relevance Score:** 9

**TL;DR:** LongRecall is introduced as a three-stage recall evaluation framework designed to improve the completeness of machine-generated text in critical domains such as medicine and law.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing recall metrics fail to account for lexical variations and can lead to errors in high-stakes areas, necessitating a more robust method for assessing recall accuracy in machine-generated answers.

**Method:** The LongRecall framework decomposes answers into self-contained facts and utilizes lexical and semantic filtering along with structured entailment checks to verify alignments.

**Key Contributions:**

	1. Introduces LongRecall, a three-stage recall evaluation framework
	2. Implements structured entailment checks to increase accuracy
	3. Demonstrates improvements over existing recall metrics on QA benchmarks

**Result:** Evaluation on long-form QA benchmarks shows LongRecall significantly outperforms traditional recall metrics and LLM-based approaches in recall accuracy.

**Limitations:** The framework may still be susceptible to errors in extremely ambiguous contexts or highly complex queries where fact decomposition is non-trivial.

**Conclusion:** LongRecall provides a novel approach to systematically assess recall in machine-generated text, helping mitigate issues related to hallucinations and misalignments in AI-generated content.

**Abstract:** LongRecall. The completeness of machine-generated text, ensuring that it captures all relevant information, is crucial in domains such as medicine and law and in tasks like list-based question answering (QA), where omissions can have serious consequences. However, existing recall metrics often depend on lexical overlap, leading to errors with unsubstantiated entities and paraphrased answers, while LLM-as-a-Judge methods with long holistic prompts capture broader semantics but remain prone to misalignment and hallucinations without structured verification. We introduce LongRecall, a general three-stage recall evaluation framework that decomposes answers into self-contained facts, successively narrows plausible candidate matches through lexical and semantic filtering, and verifies their alignment through structured entailment checks. This design reduces false positives and false negatives while accommodating diverse phrasings and contextual variations, serving as a foundational building block for systematic recall assessment. We evaluate LongRecall on three challenging long-form QA benchmarks using both human annotations and LLM-based judges, demonstrating substantial improvements in recall accuracy over strong lexical and LLM-as-a-Judge baselines.

</details>


### [20] [Mapping the Course for Prompt-based Structured Prediction](https://arxiv.org/abs/2508.15090)

*Matt Pauk, Maria Leonor Pacheco*

**Main category:** cs.CL

**Keywords:** Language Models, Structured Prediction, Symbolic Inference

**Relevance Score:** 9

**TL;DR:** This paper proposes combining LLMs with combinatorial inference to improve outcomes in structured prediction tasks by leveraging symbolic inference for better accuracy and consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address issues in LLMs such as hallucinations and complex reasoning problems in language tasks without requiring fine-tuning.

**Method:** The authors conducted exhaustive experiments to evaluate different prompting strategies and their effectiveness in estimating LLM confidence values for symbolic inference, combined with structured prediction objectives.

**Key Contributions:**

	1. Proposed a novel combination of LLMs with combinatorial inference
	2. Demonstrated the effectiveness of symbolic inference on boosting accuracy
	3. Showed the value of structured learning in improving LLM performance

**Result:** The findings indicate that adding symbolic inference improves consistency and accuracy of predictions compared to prompting alone. Additionally, calibrating and fine-tuning with structured prediction objectives enhances performance on complex tasks.

**Limitations:** 

**Conclusion:** Structured learning remains important for enhancing LLM performance, even in light of prevailing capabilities of LLMs.

**Abstract:** LLMs have been shown to be useful for a variety of language tasks, without requiring task-specific fine-tuning. However, these models often struggle with hallucinations and complex reasoning problems due to their autoregressive nature. We propose to address some of these issues, specifically in the area of structured prediction, by combining LLMs with combinatorial inference in an attempt to marry the predictive power of LLMs with the structural consistency provided by inference methods. We perform exhaustive experiments in an effort to understand which prompting strategies can effectively estimate LLM confidence values for use with symbolic inference, and show that, regardless of the prompting strategy, the addition of symbolic inference on top of prompting alone leads to more consistent and accurate predictions. Additionally, we show that calibration and fine-tuning using structured prediction objectives leads to increased performance for challenging tasks, showing that structured learning is still valuable in the era of LLMs.

</details>


### [21] [Nemotron-CC-Math: A 133 Billion-Token-Scale High Quality Math Pretraining Dataset](https://arxiv.org/abs/2508.15096)

*Rabeeh Karimi Mahabadi, Sanjeev Satheesh, Shrimai Prabhumoye, Mostofa Patwary, Mohammad Shoeybi, Bryan Catanzaro*

**Main category:** cs.CL

**Keywords:** large language models, mathematics, pretraining, scientific text extraction, open datasets

**Relevance Score:** 9

**TL;DR:** Introducing Nemotron-CC-Math, a large-scale mathematical corpus that improves reasoning capabilities in large language models by addressing the challenges in previous datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing math-focused datasets suffer from quality issues in extraction and structure preservation, necessitating a robust solution for effective pretraining of LLMs.

**Method:** A novel, domain-agnostic pipeline was developed for scientific text extraction, leveraging layout-aware rendering and an LLM-based cleaning stage to recover mathematical content accurately.

**Key Contributions:**

	1. Development of the Nemotron-CC-Math corpus addressing previous dataset limitations
	2. First robust pipeline for extracting scientific content from noisy web-scale data
	3. Significant performance improvements for LLMs pretraining in reasoning capabilities

**Result:** Nemotron-CC-Math-4+ contains 52B tokens and surpasses existing datasets in quality, yielding significant performance gains when used to pretrain the Nemotron-T 8B model, especially in math and general-domain tasks.

**Limitations:** 

**Conclusion:** The presented pipeline sets a new state of the art for open math pretraining corpora and supports open-source initiatives by releasing the code and datasets.

**Abstract:** Pretraining large language models (LLMs) on high-quality, structured data such as mathematics and code substantially enhances reasoning capabilities. However, existing math-focused datasets built from Common Crawl suffer from degraded quality due to brittle extraction heuristics, lossy HTML-to-text conversion, and the failure to reliably preserve mathematical structure. In this work, we introduce Nemotron-CC-Math, a large-scale, high-quality mathematical corpus constructed from Common Crawl using a novel, domain-agnostic pipeline specifically designed for robust scientific text extraction.   Unlike previous efforts, our pipeline recovers math across various formats (e.g., MathJax, KaTeX, MathML) by leveraging layout-aware rendering with lynx and a targeted LLM-based cleaning stage. This approach preserves the structural integrity of equations and code blocks while removing boilerplate, standardizing notation into LaTeX representation, and correcting inconsistencies.   We collected a large, high-quality math corpus, namely Nemotron-CC-Math-3+ (133B tokens) and Nemotron-CC-Math-4+ (52B tokens). Notably, Nemotron-CC-Math-4+ not only surpasses all prior open math datasets-including MegaMath, FineMath, and OpenWebMath-but also contains 5.5 times more tokens than FineMath-4+, which was previously the highest-quality math pretraining dataset. When used to pretrain a Nemotron-T 8B model, our corpus yields +4.8 to +12.6 gains on MATH and +4.6 to +14.3 gains on MBPP+ over strong baselines, while also improving general-domain performance on MMLU and MMLU-Stem.   We present the first pipeline to reliably extract scientific content--including math--from noisy web-scale data, yielding measurable gains in math, code, and general reasoning, and setting a new state of the art among open math pretraining corpora. To support open-source efforts, we release our code and datasets.

</details>


### [22] [Identifying and Answering Questions with False Assumptions: An Interpretable Approach](https://arxiv.org/abs/2508.15139)

*Zijie Wang, Eduardo Blanco*

**Main category:** cs.CL

**Keywords:** False Assumptions, Large Language Models, Fact Verification

**Relevance Score:** 9

**TL;DR:** This paper addresses the challenge of identifying and answering questions with false assumptions using Large Language Models (LLMs), proposing a method that mitigates hallucinations through fact verification and incorporating external evidence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to answer questions that arise from false assumptions, which lead to misleading answers from LLMs.

**Method:** The paper reduces the problem to fact verification and leverages external evidence to mitigate hallucinations, while also generating and validating atomic assumptions.

**Key Contributions:**

	1. Reduces the problem of false assumptions to fact verification.
	2. Introduces external evidence to combat hallucinations in LLM outputs.
	3. Shows that generating atomic assumptions leads to improved and interpretable answers.

**Result:** Experiments demonstrate that incorporating retrieved evidence is beneficial, and generating atomic assumptions provides more interpretable answers by highlighting false assumptions.

**Limitations:** 

**Conclusion:** The proposed approach improves the quality of responses from LLMs when dealing with questions that contain false assumptions.

**Abstract:** People often ask questions with false assumptions, a type of question that does not have regular answers. Answering such questions require first identifying the false assumptions. Large Language Models (LLMs) often generate misleading answers because of hallucinations. In this paper, we focus on identifying and answering questions with false assumptions in several domains. We first investigate to reduce the problem to fact verification. Then, we present an approach leveraging external evidence to mitigate hallucinations. Experiments with five LLMs demonstrate that (1) incorporating retrieved evidence is beneficial and (2) generating and validating atomic assumptions yields more improvements and provides an interpretable answer by specifying the false assumptions.

</details>


### [23] [ContextualLVLM-Agent: A Holistic Framework for Multi-Turn Visually-Grounded Dialogue and Complex Instruction Following](https://arxiv.org/abs/2508.15164)

*Seungmin Han, Haeun Kwon, Ji-jun Park, Taeyang Yoon*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-Modal Interaction, Dialogue Reasoning

**Relevance Score:** 9

**TL;DR:** Introducing MMDR-Bench, a new dataset for evaluating multi-modal dialogue reasoning, and CoLVLM Agent, a framework for enhancing Large Vision-Language Models with improved reasoning capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs and LVLMs struggle with complex, multi-turn, visually-grounded tasks that require sustained understanding and reasoning, prompting the need for better evaluation benchmarks and models.

**Method:** The paper introduces MMDR-Bench, comprising 300 complex dialogue scenarios, and proposes CoLVLM Agent, which enhances existing models using a memory-perception-planning-execution cycle without extensive retraining.

**Key Contributions:**

	1. Creation of MMDR-Bench for multi-modal dialogue evaluation
	2. Development of CoLVLM Agent to enhance LVLMs with reasoning capabilities
	3. Demonstrated superior performance compared to current state-of-the-art models

**Result:** CoLVLM Agent significantly outperformed models like GPT-4o and Gemini 1.5 Pro, achieving an average evaluation score of 4.03 on MMDR-Bench, showcasing improvements in reasoning and instruction adherence.

**Limitations:** 

**Conclusion:** The modular design and iterative approach of CoLVLM Agent validate its effectiveness for handling complex multi-modal interactions better than competitors.

**Abstract:** Despite significant advancements in Large Language Models (LLMs) and Large Vision-Language Models (LVLMs), current models still face substantial challenges in handling complex, multi-turn, and visually-grounded tasks that demand deep reasoning, sustained contextual understanding, entity tracking, and multi-step instruction following. Existing benchmarks often fall short in capturing the dynamism and intricacies of real-world multi-modal interactions, leading to issues such as context loss and visual hallucinations. To address these limitations, we introduce MMDR-Bench (Multi-Modal Dialogue Reasoning Benchmark), a novel dataset comprising 300 meticulously designed complex multi-turn dialogue scenarios, each averaging 5-7 turns and evaluated across six core dimensions including visual entity tracking and reasoning depth. Furthermore, we propose CoLVLM Agent (Contextual LVLM Agent), a holistic framework that enhances existing LVLMs with advanced reasoning and instruction following capabilities through an iterative "memory-perception-planning-execution" cycle, requiring no extensive re-training of the underlying models. Our extensive experiments on MMDR-Bench demonstrate that CoLVLM Agent consistently achieves superior performance, attaining an average human evaluation score of 4.03, notably surpassing state-of-the-art commercial models like GPT-4o (3.92) and Gemini 1.5 Pro (3.85). The framework exhibits significant advantages in reasoning depth, instruction adherence, and error suppression, and maintains robust performance over extended dialogue turns, validating the effectiveness of its modular design and iterative approach for complex multi-modal interactions.

</details>


### [24] [SemToken: Semantic-Aware Tokenization for Efficient Long-Context Language Modeling](https://arxiv.org/abs/2508.15190)

*Dong Liu, Yanxuan Yu*

**Main category:** cs.CL

**Keywords:** Tokenization, Semantic Clustering, Language Models, Natural Language Processing, Efficiency

**Relevance Score:** 8

**TL;DR:** SemToken is a semantic-aware tokenization framework designed to improve language modeling by reducing token redundancy and enhancing computation efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current tokenization methods fail to consider the semantic structure of text, leading to inefficiencies in handling long-context scenarios and redundancy in token usage.

**Method:** SemToken utilizes lightweight encoders to extract contextual semantic embeddings and performs local semantic clustering to merge semantically equivalent tokens. It adjusts token granularity based on semantic density for more effective long-context handling.

**Key Contributions:**

	1. Introduction of SemToken, a semantic-aware tokenization framework
	2. Demonstrated improvements in token count reduction and computational speed
	3. Successful integration with modern language models and attention acceleration methods

**Result:** SemToken achieves up to 2.4× reduction in token count and 1.9× speedup with minimal to no degradation in downstream accuracy and perplexity on long-context language modeling benchmarks.

**Limitations:** 

**Conclusion:** The semantic structure of text provides a valuable dimension for optimizing tokenization, leading to improved efficiency and effectiveness in large language models.

**Abstract:** Tokenization plays a critical role in language modeling, yet existing approaches such as Byte-Pair Encoding (BPE) or WordPiece operate purely on frequency statistics, ignoring the underlying semantic structure of text. This leads to over-tokenization of semantically redundant spans and underutilization of contextual coherence, particularly in long-context scenarios. In this work, we propose \textbf{SemToken}, a semantic-aware tokenization framework that jointly reduces token redundancy and improves computation efficiency. SemToken first extracts contextual semantic embeddings via lightweight encoders and performs local semantic clustering to merge semantically equivalent tokens. Then, it allocates heterogeneous token granularity based on semantic density, allowing finer-grained tokenization in content-rich regions and coarser compression in repetitive or low-entropy spans. SemToken can be seamlessly integrated with modern language models and attention acceleration methods. Experiments on long-context language modeling benchmarks such as WikiText-103 and LongBench show that SemToken achieves up to $2.4\times$ reduction in token count and $1.9\times$ speedup, with negligible or no degradation in perplexity and downstream accuracy. Our findings suggest that semantic structure offers a promising new axis for optimizing tokenization and computation in large language models.

</details>


### [25] [Fin-PRM: A Domain-Specialized Process Reward Model for Financial Reasoning in Large Language Models](https://arxiv.org/abs/2508.15202)

*Yuanchen Zhou, Shuo Jiang, Jie Zhu, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang*

**Main category:** cs.CL

**Keywords:** Process Reward Models, Financial reasoning, Large language models

**Relevance Score:** 7

**TL;DR:** Introducing Fin-PRM, a domain-specialized Process Reward Model for evaluating intermediate reasoning in financial tasks, outperforming traditional models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Process Reward Models lack efficacy in specialized domains like finance, which require structured and regulatory-compliant reasoning.

**Method:** Fin-PRM integrates step-level and trajectory-level reward supervision for fine-grained evaluation in financial reasoning tasks, applied in both offline and online settings.

**Key Contributions:**

	1. Introduction of Fin-PRM for financial tasks
	2. Integration of step-level and trajectory-level reward supervision
	3. Demonstrated significant improvements in reasoning task performance

**Result:** Fin-PRM outperforms general Purpose PRMs and domain baselines on benchmarks like CFLUE and FinQA, showing significant improvements in supervised and reinforcement learning.

**Limitations:** 

**Conclusion:** Domain-specialized reward modeling significantly enhances LLM alignment with expert-level financial reasoning, providing resources for broader application.

**Abstract:** Process Reward Models (PRMs) have emerged as a promising framework for supervising intermediate reasoning in large language models (LLMs), yet existing PRMs are primarily trained on general or Science, Technology, Engineering, and Mathematics (STEM) domains and fall short in domain-specific contexts such as finance, where reasoning is more structured, symbolic, and sensitive to factual and regulatory correctness. We introduce \textbf{Fin-PRM}, a domain-specialized, trajectory-aware PRM tailored to evaluate intermediate reasoning steps in financial tasks. Fin-PRM integrates step-level and trajectory-level reward supervision, enabling fine-grained evaluation of reasoning traces aligned with financial logic. We apply Fin-PRM in both offline and online reward learning settings, supporting three key applications: (i) selecting high-quality reasoning trajectories for distillation-based supervised fine-tuning, (ii) providing dense process-level rewards for reinforcement learning, and (iii) guiding reward-informed Best-of-N inference at test time. Experimental results on financial reasoning benchmarks, including CFLUE and FinQA, demonstrate that Fin-PRM consistently outperforms general-purpose PRMs and strong domain baselines in trajectory selection quality. Downstream models trained with Fin-PRM yield substantial improvements with baselines, with gains of 12.9\% in supervised learning, 5.2\% in reinforcement learning, and 5.1\% in test-time performance. These findings highlight the value of domain-specialized reward modeling for aligning LLMs with expert-level financial reasoning. Our project resources will be available at https://github.com/aliyun/qwen-dianjin.

</details>


### [26] [SparK: Query-Aware Unstructured Sparsity with Recoverable KV Cache Channel Pruning](https://arxiv.org/abs/2508.15212)

*Huanxuan Liao, Yixing Xu, Shizhu He, Guanchen Li, Xuanwu Yin, Dong Li, Emad Barsoum, Jun Zhao, Kang Liu*

**Main category:** cs.CL

**Keywords:** Long-context inference, large language models, KV cache, channel sparsity, memory efficiency

**Relevance Score:** 9

**TL;DR:** SPARK is a training-free method that prunes KV cache at the channel level to improve efficiency in LLMs during long-context inference, achieving over 30% reduction in memory usage and maintaining accuracy with aggressive pruning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the KV cache bottleneck in LLMs, which limits memory and computational efficiency during long-context inference.

**Method:** SPARK applies unstructured sparsity by pruning KV at the channel level and dynamically restoring pruned entries during attention score computation.

**Key Contributions:**

	1. Proposes a training-free method for channel-level pruning of KV cache in LLMs.
	2. Achieves significant memory savings while improving or maintaining model accuracy.
	3. Demonstrates compatibility with other KV compression techniques for enhanced performance.

**Result:** SPARK reduces channel-level redundancy, enabling longer sequence processing within the same memory budget and achieving over 30% reduction in KV cache storage compared to eviction-based methods, with minimal degradation in performance.

**Limitations:** 

**Conclusion:** SPARK demonstrates robustness and effectiveness, maintaining performance with less than 5% degradation even with 80% pruning, and is compatible with existing compression and quantization techniques.

**Abstract:** Long-context inference in large language models (LLMs) is increasingly constrained by the KV cache bottleneck: memory usage grows linearly with sequence length, while attention computation scales quadratically. Existing approaches address this issue by compressing the KV cache along the temporal axis through strategies such as token eviction or merging to reduce memory and computational overhead. However, these methods often neglect fine-grained importance variations across feature dimensions (i.e., the channel axis), thereby limiting their ability to effectively balance efficiency and model accuracy. In reality, we observe that channel saliency varies dramatically across both queries and positions: certain feature channels carry near-zero information for a given query, while others spike in relevance. To address this oversight, we propose SPARK, a training-free plug-and-play method that applies unstructured sparsity by pruning KV at the channel level, while dynamically restoring the pruned entries during attention score computation. Notably, our approach is orthogonal to existing KV compression and quantization techniques, making it compatible for integration with them to achieve further acceleration. By reducing channel-level redundancy, SPARK enables processing of longer sequences within the same memory budget. For sequences of equal length, SPARK not only preserves or improves model accuracy but also reduces KV cache storage by over 30% compared to eviction-based methods. Furthermore, even with an aggressive pruning ratio of 80%, SPARK maintains performance with less degradation than 5% compared to the baseline eviction method, demonstrating its robustness and effectiveness. Our code will be available at https://github.com/Xnhyacinth/SparK.

</details>


### [27] [Select to Know: An Internal-External Knowledge Self-Selection Framework for Domain-Specific Question Answering](https://arxiv.org/abs/2508.15213)

*Bolei He, Xinran He, Run Shao, Shanfu Shu, Xianwei Xue, Mingquan Cheng, Haifeng Li, Zhenhua Ling*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, domain-specific QA

**Relevance Score:** 9

**TL;DR:** Selct2Know (S2K) is a framework designed to enhance the performance of LLMs in domain-specific question-answering by selectively integrating external knowledge and improving reasoning capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs excel in general QA but face challenges in domain-specific applications due to hallucinations and latency from noisy retrieval, as well as high costs for continued pretraining.

**Method:** The S2K framework utilizes an internal-external knowledge self-selection strategy and selective supervised fine-tuning to integrate domain knowledge progressively.

**Key Contributions:**

	1. Introduction of the Selct2Know (S2K) framework for knowledge integration
	2. Development of a structured reasoning data generation pipeline
	3. Enhanced reasoning capabilities through GRPO integration

**Result:** Experiments indicated that S2K consistently outperforms other methods on medical, legal, and financial QA benchmarks, achieving comparable results to domain-pretrained LLMs at a lower cost.

**Limitations:** 

**Conclusion:** The S2K framework effectively addresses the limitations of existing methods by mimicking human learning processes and providing cost-effective domain knowledge acquisition.

**Abstract:** Large Language Models (LLMs) perform well in general QA but often struggle in domain-specific scenarios. Retrieval-Augmented Generation (RAG) introduces external knowledge but suffers from hallucinations and latency due to noisy retrievals. Continued pretraining internalizes domain knowledge but is costly and lacks cross-domain flexibility. We attribute this challenge to the long-tail distribution of domain knowledge, which leaves partial yet useful internal knowledge underutilized. We further argue that knowledge acquisition should be progressive, mirroring human learning: first understanding concepts, then applying them to complex reasoning. To address this, we propose Selct2Know (S2K), a cost-effective framework that internalizes domain knowledge through an internal-external knowledge self-selection strategy and selective supervised fine-tuning. We also introduce a structured reasoning data generation pipeline and integrate GRPO to enhance reasoning ability. Experiments on medical, legal, and financial QA benchmarks show that S2K consistently outperforms existing methods and matches domain-pretrained LLMs with significantly lower cost.

</details>


### [28] [Self-Guided Function Calling in Large Language Models via Stepwise Experience Recall](https://arxiv.org/abs/2508.15214)

*Sijia Cui, Aiyao He, Shuai Xu, Hongming Zhang, Yanna Wang, Qingyang Zhang, Yajing Wang, Bo Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Tool Selection, Experience Retrieval, Human-Computer Interaction, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces SEER, a self-guided method for improving LLM interactions with external systems through fine-grained experience retrieval.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome challenges in tool selection, parameter generation, and tool-chain planning faced by LLMs when using multiple tools.

**Method:** SEER employs stepwise retrieval from an experience pool that continuously updates with past successful tool usage trajectories.

**Key Contributions:**

	1. Introduction of Stepwise Experience Recall (SEER) for tool interaction
	2. Continuous updates to user experience pool improve tool usage performance
	3. Substantial accuracy gains demonstrated on the ToolQA benchmark and real-world domains.

**Result:** SEER shows an average improvement of 6.1% on easy and 4.7% on hard questions in the ToolQA benchmark, with even higher accuracy gains in real-world domain evaluations.

**Limitations:** 

**Conclusion:** The continuous update mechanism of SEER enhances LLM performance over time without needing extensive prompt engineering or manual curation.

**Abstract:** Function calling enables large language models (LLMs) to interact with external systems by leveraging tools and APIs. When faced with multi-step tool usage, LLMs still struggle with tool selection, parameter generation, and tool-chain planning. Existing methods typically rely on manually designing task-specific demonstrations, or retrieving from a curated library. These approaches demand substantial expert effort and prompt engineering becomes increasingly complex and inefficient as tool diversity and task difficulty scale. To address these challenges, we propose a self-guided method, Stepwise Experience Recall (SEER), which performs fine-grained, stepwise retrieval from a continually updated experience pool. Instead of relying on static or manually curated library, SEER incrementally augments the experience pool with past successful trajectories, enabling continuous expansion of the pool and improved model performance over time. Evaluated on the ToolQA benchmark, SEER achieves an average improvement of 6.1\% on easy and 4.7\% on hard questions. We further test SEER on $\tau$-bench, which includes two real-world domains. Powered by Qwen2.5-7B and Qwen2.5-72B models, SEER demonstrates substantial accuracy gains of 7.44\% and 23.38\%, respectively.

</details>


### [29] [Are Checklists Really Useful for Automatic Evaluation of Generative Tasks?](https://arxiv.org/abs/2508.15218)

*Momoka Furuhashi, Kouta Nakayama, Takashi Kodama, Saku Sugawara*

**Main category:** cs.CL

**Keywords:** automatic evaluation, checklist generation, large language models, generative tasks, human evaluation

**Relevance Score:** 8

**TL;DR:** The paper investigates the effectiveness of automatic checklist generation for evaluating generative tasks using large language models, highlighting selective use as beneficial in some settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in automatic evaluation of generative tasks due to ambiguous criteria and explore the effectiveness of automatic checklist generation.

**Method:** The authors generate checklists using six methods, evaluate their effectiveness across eight model sizes, and analyze correlation with human evaluations through experiments on pairwise comparison and direct scoring tasks.

**Key Contributions:**

	1. Demonstrated the benefits of selective checklist use in evaluation settings
	2. Generated checklists using six distinct methods
	3. Analyzed checklist item correlation with human evaluations

**Result:** Selective checklist use improves evaluation performance in pairwise settings, with inconsistent benefits in direct scoring. Items with low correlation to human scores still reflect human-written criteria.

**Limitations:** Potential inconsistencies in human evaluation criteria and the effectiveness of checklist methods may vary.

**Conclusion:** Clearly defining objective evaluation criteria is essential for guiding both human and automatic evaluations in generative tasks.

**Abstract:** Automatic evaluation of generative tasks using large language models faces challenges due to ambiguous criteria. Although automatic checklist generation is a potentially promising approach, its usefulness remains underexplored. We investigate whether checklists should be used for all questions or selectively, generate them using six methods, evaluate their effectiveness across eight model sizes, and identify checklist items that correlate with human evaluations. Through experiments on pairwise comparison and direct scoring tasks, we find that selective checklist use tends to improve evaluation performance in pairwise settings, while its benefits are less consistent in direct scoring. Our analysis also shows that even checklist items with low correlation to human scores often reflect human-written criteria, indicating potential inconsistencies in human evaluation. These findings highlight the need to more clearly define objective evaluation criteria to guide both human and automatic evaluations. \footnote{Our code is available at~https://github.com/momo0817/checklist-effectiveness-study

</details>


### [30] [VocabTailor: Dynamic Vocabulary Selection for Downstream Tasks in Small Language Models](https://arxiv.org/abs/2508.15229)

*Hanling Zhang, Yayu Zhou, Tongcheng Fang, Zhihang Yuan, Guohao Dai, Yu Wang*

**Main category:** cs.CL

**Keywords:** Small Language Models, Dynamic Vocabulary Selection, Memory Efficiency

**Relevance Score:** 7

**TL;DR:** The paper presents VocabTailor, a framework for dynamic vocabulary selection in Small Language Models (SLMs) to reduce memory usage while maintaining task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Memory limitations in Small Language Models (SLMs) hinder their deployment in resource-constrained environments, particularly due to large vocabulary sizes that impact efficiency.

**Method:** VocabTailor employs a decoupled dynamic vocabulary selection framework, offloading embeddings and using a hybrid static-dynamic vocabulary selection strategy for LM Head.

**Key Contributions:**

	1. Introduction of VocabTailor for dynamic vocabulary selection in SLMs
	2. Reduction of vocabulary-related memory usage by up to 99%
	3. Demonstrated minimal performance degradation across multiple tasks

**Result:** VocabTailor achieves up to 99% reduction in memory usage for vocabulary-related components without significant performance loss on various tasks.

**Limitations:** 

**Conclusion:** The proposed framework significantly surpasses traditional static vocabulary pruning methods, improving efficiency in utilizing memory resources while preserving model performance.

**Abstract:** Small Language Models (SLMs) provide computational advantages in resource-constrained environments, yet memory limitations remain a critical bottleneck for edge device deployment. A substantial portion of SLMs' memory footprint stems from vocabulary-related components, particularly embeddings and language modeling (LM) heads, due to large vocabulary sizes. Existing static vocabulary pruning, while reducing memory usage, suffers from rigid, one-size-fits-all designs that cause information loss from the prefill stage and a lack of flexibility. In this work, we identify two key principles underlying the vocabulary reduction challenge: the lexical locality principle, the observation that only a small subset of tokens is required during any single inference, and the asymmetry in computational characteristics between vocabulary-related components of SLM. Based on these insights, we introduce VocabTailor, a novel decoupled dynamic vocabulary selection framework that addresses memory constraints through offloading embedding and implements a hybrid static-dynamic vocabulary selection strategy for LM Head, enabling on-demand loading of vocabulary components. Comprehensive experiments across diverse downstream tasks demonstrate that VocabTailor achieves a reduction of up to 99% in the memory usage of vocabulary-related components with minimal or no degradation in task performance, substantially outperforming existing static vocabulary pruning.

</details>


### [31] [WangchanThaiInstruct: An instruction-following Dataset for Culture-Aware, Multitask, and Multi-domain Evaluation in Thai](https://arxiv.org/abs/2508.15239)

*Peerat Limkonchotiwat, Pume Tuchinda, Lalita Lowphansirikul, Surapon Nonesung, Panuthep Tasawong, Alham Fikri Aji, Can Udomcharoenchaikit, Sarana Nutanong*

**Main category:** cs.CL

**Keywords:** large language models, Thai dataset, instruction tuning, low-resource languages, cultural nuances

**Relevance Score:** 8

**TL;DR:** WangchanThaiInstruct introduces a Thai dataset for evaluating and tuning large language models, highlighting performance disparities in low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gaps of large language models in low-resource languages like Thai, which are often overlooked in existing benchmarks that rely on translations.

**Method:** The dataset was created through a multi-stage quality control process involving annotators, domain experts, and AI researchers, covering multiple professional domains and various task types.

**Key Contributions:**

	1. Introduction of WangchanThaiInstruct, a human-authored Thai dataset.
	2. Demonstration of performance advantages for models trained on culturally grounded data.
	3. Evidence of substantial performance gaps in zero-shot evaluations of language models in Thai.

**Result:** Models fine-tuned on WangchanThaiInstruct outperformed those using translated data in both in-domain and out-of-domain benchmarks, revealing significant performance gaps in culturally and professionally specific tasks.

**Limitations:** 

**Conclusion:** The study emphasizes the importance of culturally and professionally grounded instruction data for improving LLM alignment in linguistically diverse settings.

**Abstract:** Large language models excel at instruction-following in English, but their performance in low-resource languages like Thai remains underexplored. Existing benchmarks often rely on translations, missing cultural and domain-specific nuances needed for real-world use. We present WangchanThaiInstruct, a human-authored Thai dataset for evaluation and instruction tuning, covering four professional domains and seven task types. Created through a multi-stage quality control process with annotators, domain experts, and AI researchers, WangchanThaiInstruct supports two studies: (1) a zero-shot evaluation showing performance gaps on culturally and professionally specific tasks, and (2) an instruction tuning study with ablations isolating the effect of native supervision. Models fine-tuned on WangchanThaiInstruct outperform those using translated data in both in-domain and out-of-domain benchmarks. These findings underscore the need for culturally and professionally grounded instruction data to improve LLM alignment in low-resource, linguistically diverse settings.

</details>


### [32] [UniCoM: A Universal Code-Switching Speech Generator](https://arxiv.org/abs/2508.15244)

*Sangmin Lee, Woojin Chung, Seyun Um, Hong-Goo Kang*

**Main category:** cs.CL

**Keywords:** code-switching, speech technology, multilingual systems, natural language processing, speech recognition

**Relevance Score:** 7

**TL;DR:** This paper presents a novel pipeline, Universal Code-Mixer (UniCoM), for generating high-quality code-switching speech samples to advance multilingual speech technology.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges of handling code-switching in multilingual speech systems due to the lack of suitable datasets.

**Method:** The proposed method, Substituting WORDs with Synonyms (SWORDS), replaces selected words with their translations considering parts of speech, generating natural code-switched samples.

**Key Contributions:**

	1. Proposed a novel pipeline for generating code-switching speech samples.
	2. Constructed the CS-FLEURS corpus for ASR and S2TT.
	3. Showed competitive performance of CS-FLEURS against existing datasets.

**Result:** The Code-Switching FLEURS (CS-FLEURS) corpus was constructed, achieving high intelligibility and naturalness, and performing comparably to existing datasets in ASR and S2TT tasks.

**Limitations:** 

**Conclusion:** UniCoM and CS-FLEURS are expected to advance code-switching speech technology and promote inclusive multilingual systems.

**Abstract:** Code-switching (CS), the alternation between two or more languages within a single speaker's utterances, is common in real-world conversations and poses significant challenges for multilingual speech technology. However, systems capable of handling this phenomenon remain underexplored, primarily due to the scarcity of suitable datasets. To resolve this issue, we propose Universal Code-Mixer (UniCoM), a novel pipeline for generating high-quality, natural CS samples without altering sentence semantics. Our approach utilizes an algorithm we call Substituting WORDs with Synonyms (SWORDS), which generates CS speech by replacing selected words with their translations while considering their parts of speech. Using UniCoM, we construct Code-Switching FLEURS (CS-FLEURS), a multilingual CS corpus designed for automatic speech recognition (ASR) and speech-to-text translation (S2TT). Experimental results show that CS-FLEURS achieves high intelligibility and naturalness, performing comparably to existing datasets on both objective and subjective metrics. We expect our approach to advance CS speech technology and enable more inclusive multilingual systems.

</details>


### [33] [EMNLP: Educator-role Moral and Normative Large Language Models Profiling](https://arxiv.org/abs/2508.15250)

*Yilin Jiang, Mingzi Zhang, Sheng Jin, Zengyi Yu, Xiangjie Kong, Binghao Tu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Moral Dilemmas, Educational AI, Ethics, Personality Profiling

**Relevance Score:** 8

**TL;DR:** The paper introduces a framework for profiling Large Language Models (LLMs) in educational roles, focusing on ethical and psychological evaluation through a series of moral dilemmas and soft prompt injections.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of comprehensive psychological and ethical evaluations of Large Language Models (LLMs) in professional simulations, particularly in educational contexts.

**Method:** The paper presents the Educator-role Moral and Normative LLMs Profiling (EMNLP) framework which constructs 88 teacher-specific moral dilemmas and employs soft prompt injections to assess compliance and vulnerability in LLMs.

**Key Contributions:**

	1. Introduction of the EMNLP framework for profiling LLMs in education.
	2. Creation of 88 teacher-specific moral dilemmas for comprehensive assessment.
	3. First benchmarking methodology for ethical risk assessment in teacher-role LLMs.

**Result:** Experiments reveal that teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, perform well in abstract moral reasoning, but face challenges in emotionally complex situations. Additionally, there is a paradox where models with better reasoning are more susceptible to harmful prompt injections.

**Limitations:** The findings suggest limited influence of model temperature and hyperparameters on risk behaviors, indicating a need for deeper investigation into model safety.

**Conclusion:** The study establishes a benchmark for assessing the ethical and psychological alignment of teacher-role LLMs in educational AI contexts, highlighting the importance of understanding the balance between model capability and safety.

**Abstract:** Simulating Professions (SP) enables Large Language Models (LLMs) to emulate professional roles. However, comprehensive psychological and ethical evaluation in these contexts remains lacking. This paper introduces EMNLP, an Educator-role Moral and Normative LLMs Profiling framework for personality profiling, moral development stage measurement, and ethical risk under soft prompt injection. EMNLP extends existing scales and constructs 88 teacher-specific moral dilemmas, enabling profession-oriented comparison with human teachers. A targeted soft prompt injection set evaluates compliance and vulnerability in teacher SP. Experiments on 12 LLMs show teacher-role LLMs exhibit more idealized and polarized personalities than human teachers, excel in abstract moral reasoning, but struggle with emotionally complex situations. Models with stronger reasoning are more vulnerable to harmful prompt injection, revealing a paradox between capability and safety. The model temperature and other hyperparameters have limited influence except in some risk behaviors. This paper presents the first benchmark to assess ethical and psychological alignment of teacher-role LLMs for educational AI. Resources are available at https://e-m-n-l-p.github.io/.

</details>


### [34] [Conflict-Aware Soft Prompting for Retrieval-Augmented Generation](https://arxiv.org/abs/2508.15253)

*Eunseong Choi, June Park, Hyeri Lee, Jongwuk Lee*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Large language models, Context-memory conflict

**Relevance Score:** 9

**TL;DR:** Introducing CARE, a method to resolve context-memory conflicts in RAG systems, improving reliability in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability of retrieval-augmented generation (RAG) by addressing conflicts between external context and LLM's parametric knowledge.

**Method:** CARE integrates a context assessor that evaluates context reliability and provides a guidance signal to the underlying LLM through grounded/adversarial soft prompting.

**Key Contributions:**

	1. Development of the Conflict-Aware REtrieval-Augmented Generation (CARE) method
	2. Integration of a context assessor for reliability evaluation
	3. Demonstrated performance improvement on QA and fact-checking benchmarks

**Result:** CARE reduced context-memory conflicts, achieving an average performance gain of 5.0% on QA and fact-checking benchmarks.

**Limitations:** 

**Conclusion:** CARE presents a significant step towards building trustworthy and adaptive RAG systems by effectively managing context-memory conflicts.

**Abstract:** Retrieval-augmented generation (RAG) enhances the capabilities of large language models (LLMs) by incorporating external knowledge into their input prompts. However, when the retrieved context contradicts the LLM's parametric knowledge, it often fails to resolve the conflict between incorrect external context and correct parametric knowledge, known as context-memory conflict. To tackle this problem, we introduce Conflict-Aware REtrieval-Augmented Generation (CARE), consisting of a context assessor and a base LLM. The context assessor encodes compact memory token embeddings from raw context tokens. Through grounded/adversarial soft prompting, the context assessor is trained to discern unreliable context and capture a guidance signal that directs reasoning toward the more reliable knowledge source. Extensive experiments show that CARE effectively mitigates context-memory conflicts, leading to an average performance gain of 5.0\% on QA and fact-checking benchmarks, establishing a promising direction for trustworthy and adaptive RAG systems.

</details>


### [35] [TComQA: Extracting Temporal Commonsense from Text](https://arxiv.org/abs/2508.15274)

*Lekshmi R Nair, Arun Sankar, Koninika Pal*

**Main category:** cs.CL

**Keywords:** temporal commonsense, large language models, TComQA, natural language understanding, temporal reasoning

**Relevance Score:** 7

**TL;DR:** This paper presents a method for extracting temporal commonsense from text using large language models (LLMs) and introduces a new dataset called TComQA, which demonstrates improved performance in temporal question answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need to enhance the reasoning capabilities of LLMs regarding temporal commonsense, as it is often implicit in natural language, impacting their performance on tasks requiring temporal understanding.

**Method:** The authors develop a temporal commonsense extraction pipeline that utilizes LLMs to automatically mine temporal commonsense from text, evaluated through various experimental setups.

**Key Contributions:**

	1. Introduction of a temporal commonsense extraction pipeline using LLMs
	2. Development of the TComQA dataset validated through crowdsourcing
	3. Demonstrated improved performance in temporal question answering tasks compared to existing datasets

**Result:** The proposed pipeline results in the creation of the TComQA dataset, achieving over 80% precision when validated through crowdsourcing, and outperforms existing methods in temporal question answering tasks.

**Limitations:** 

**Conclusion:** The study highlights the potential of mining temporal commonsense to improve the capabilities of language models, addressing challenges in understanding temporal context in natural language.

**Abstract:** Understanding events necessitates grasping their temporal context, which is often not explicitly stated in natural language. For example, it is not a trivial task for a machine to infer that a museum tour may last for a few hours, but can not take months. Recent studies indicate that even advanced large language models (LLMs) struggle in generating text that require reasoning with temporal commonsense due to its infrequent explicit mention in text. Therefore, automatically mining temporal commonsense for events enables the creation of robust language models. In this work, we investigate the capacity of LLMs to extract temporal commonsense from text and evaluate multiple experimental setups to assess their effectiveness. Here, we propose a temporal commonsense extraction pipeline that leverages LLMs to automatically mine temporal commonsense and use it to construct TComQA, a dataset derived from SAMSum and RealNews corpora. TComQA has been validated through crowdsourcing and achieves over 80\% precision in extracting temporal commonsense. The model trained with TComQA also outperforms an LLM fine-tuned on existing dataset of temporal question answering task.

</details>


### [36] [CUPE: Contextless Universal Phoneme Encoder for Language-Agnostic Speech Processing](https://arxiv.org/abs/2508.15316)

*Abdul Rehman, Jian-Jun Zhang, Xiaosong Yang*

**Main category:** cs.CL

**Keywords:** phoneme recognition, cross-lingual performance, acoustic patterns

**Relevance Score:** 4

**TL;DR:** CUPE is a lightweight model for universal phoneme recognition that processes phoneme-length speech segments effectively and achieves strong cross-lingual performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for phoneme representations free from contextual influence in various speech processing tasks.

**Method:** CUPE processes short, fixed-width speech windows independently while capturing key phoneme features in just 120 milliseconds.

**Key Contributions:**

	1. Development of CUPE for lightweight phoneme recognition
	2. Competitive performance with fewer parameters compared to existing methods
	3. Strong cross-lingual generalization using fundamental acoustic patterns

**Result:** The model demonstrates strong cross-lingual generalization and competitive performance through extensive evaluation on diverse languages, including zero-shot tests.

**Limitations:** 

**Conclusion:** Effective universal speech processing can be achieved by modeling basic acoustic patterns within phoneme-length windows.

**Abstract:** Universal phoneme recognition typically requires analyzing long speech segments and language-specific patterns. Many speech processing tasks require pure phoneme representations free from contextual influence, which motivated our development of CUPE - a lightweight model that captures key phoneme features in just 120 milliseconds, about one phoneme's length. CUPE processes short, fixed-width windows independently and, despite fewer parameters than current approaches, achieves competitive cross-lingual performance by learning fundamental acoustic patterns common to all languages. Our extensive evaluation through supervised and self-supervised training on diverse languages, including zero-shot tests on the UCLA Phonetic Corpus, demonstrates strong cross-lingual generalization and reveals that effective universal speech processing is possible through modeling basic acoustic patterns within phoneme-length windows.

</details>


### [37] [KG-EDAS: A Meta-Metric Framework for Evaluating Knowledge Graph Completion Models](https://arxiv.org/abs/2508.15357)

*Haji Gul, Abul Ghani Naim, Ajaz Ahmad Bhat*

**Main category:** cs.CL

**Keywords:** Knowledge Graphs, Knowledge Graph Completion, Meta-metric, Evaluation Metrics, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** The paper introduces a new meta-metric, KG Evaluation based on Distance from Average Solution (EDAS), for evaluating Knowledge Graph Completion models across multiple datasets and metrics, aiming to provide a unified and interpretable evaluation framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of inconsistent evaluations of KGC models across different datasets and metrics, which complicate model selection for downstream tasks.

**Method:** The paper proposes the EDAS meta-metric, which synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score, providing a global perspective on KGC model performance.

**Key Contributions:**

	1. Introduction of the EDAS meta-metric for KGC evaluation
	2. Integration of multiple evaluation metrics into a single normalized score
	3. Demonstration of consistent and generalizable evaluation framework through experiments.

**Result:** Experimental results demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, showing a consistent and robust evaluation framework.

**Limitations:** 

**Conclusion:** EDAS promotes fairness in cross-dataset evaluations and supports more informed model selection for Knowledge Graph Completion models.

**Abstract:** Knowledge Graphs (KGs) enable applications in various domains such as semantic search, recommendation systems, and natural language processing. KGs are often incomplete, missing entities and relations, an issue addressed by Knowledge Graph Completion (KGC) methods that predict missing elements. Different evaluation metrics, such as Mean Reciprocal Rank (MRR), Mean Rank (MR), and Hit@k, are commonly used to assess the performance of such KGC models. A major challenge in evaluating KGC models, however, lies in comparing their performance across multiple datasets and metrics. A model may outperform others on one dataset but underperform on another, making it difficult to determine overall superiority. Moreover, even within a single dataset, different metrics such as MRR and Hit@1 can yield conflicting rankings, where one model excels in MRR while another performs better in Hit@1, further complicating model selection for downstream tasks. These inconsistencies hinder holistic comparisons and highlight the need for a unified meta-metric that integrates performance across all metrics and datasets to enable a more reliable and interpretable evaluation framework. To address this need, we propose KG Evaluation based on Distance from Average Solution (EDAS), a robust and interpretable meta-metric that synthesizes model performance across multiple datasets and diverse evaluation criteria into a single normalized score ($M_i \in [0,1]$). Unlike traditional metrics that focus on isolated aspects of performance, EDAS offers a global perspective that supports more informed model selection and promotes fairness in cross-dataset evaluation. Experimental results on benchmark datasets such as FB15k-237 and WN18RR demonstrate that EDAS effectively integrates multi-metric, multi-dataset performance into a unified ranking, offering a consistent, robust, and generalizable framework for evaluating KGC models.

</details>


### [38] [A Survey on Large Language Model Benchmarks](https://arxiv.org/abs/2508.15361)

*Shiwen Ni, Guhong Chen, Shuaimin Li, Xuanang Chen, Siyi Li, Bingli Wang, Qiyao Wang, Xingjian Wang, Yifan Zhang, Liyang Fan, Chengming Li, Ruifeng Xu, Le Sun, Min Yang*

**Main category:** cs.CL

**Keywords:** large language models, evaluation benchmarks, performance assessment, systematic review, model development

**Relevance Score:** 8

**TL;DR:** This paper reviews 283 benchmarks for evaluating large language models, categorizing them into general, domain-specific, and target-specific types, while highlighting issues like inflated scores and biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well large language models are evaluated through benchmarks and to guide future developments in this area.

**Method:** A systematic review categorizing 283 benchmarks into three categories: general capabilities, domain-specific, and target-specific, analyzing their characteristics and issues.

**Key Contributions:**

	1. Systematic categorization of 283 evaluation benchmarks for large language models.
	2. Identification of key performance evaluation issues in existing benchmarks.
	3. Proposal of a design paradigm for future benchmark development.

**Result:** Identified three main categories of benchmarks and highlighted problems like data contamination, biases, and a lack of evaluation on dynamic environments.

**Limitations:** Issues include biased evaluations and lack of considerations for dynamic environments in current benchmarks.

**Conclusion:** A referable design paradigm for innovating benchmarks is proposed to address current shortcomings.

**Abstract:** In recent years, with the rapid development of the depth and breadth of large language models' capabilities, various corresponding evaluation benchmarks have been emerging in increasing numbers. As a quantitative assessment tool for model performance, benchmarks are not only a core means to measure model capabilities but also a key element in guiding the direction of model development and promoting technological innovation. We systematically review the current status and development of large language model benchmarks for the first time, categorizing 283 representative benchmarks into three categories: general capabilities, domain-specific, and target-specific. General capability benchmarks cover aspects such as core linguistics, knowledge, and reasoning; domain-specific benchmarks focus on fields like natural sciences, humanities and social sciences, and engineering technology; target-specific benchmarks pay attention to risks, reliability, agents, etc. We point out that current benchmarks have problems such as inflated scores caused by data contamination, unfair evaluation due to cultural and linguistic biases, and lack of evaluation on process credibility and dynamic environments, and provide a referable design paradigm for future benchmark innovation.

</details>


### [39] [Unveiling Trust in Multimodal Large Language Models: Evaluation, Analysis, and Mitigation](https://arxiv.org/abs/2508.15370)

*Yichi Zhang, Yao Huang, Yifan Wang, Yitong Sun, Chang Liu, Zhe Zhao, Zhengwei Fang, Huanran Chen, Xiao Yang, Xingxing Wei, Hang Su, Yinpeng Dong, Jun Zhu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Trustworthiness, Benchmark, Risk Mitigation, Reasoning

**Relevance Score:** 9

**TL;DR:** MultiTrust-X is a benchmark developed to evaluate, analyze, and mitigate trustworthiness issues in Multimodal Large Language Models (MLLMs), addressing aspects such as truthfulness, robustness, and fairness, while highlighting the risks introduced by multimodality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The trustworthiness of MLLMs is increasingly important but poorly addressed by current evaluation methods that often overlook multimodal risks and their impacts.

**Method:** The paper introduces a three-dimensional framework consisting of five trustworthiness aspects, two novel risk types tailored for multimodal contexts, and various mitigation strategies, leading to the creation of a benchmark with 32 tasks and 28 datasets for evaluating MLLMs.

**Key Contributions:**

	1. Introduction of MultiTrust-X for comprehensive evaluation of MLLMs
	2. Development of a framework to analyze multimodal risks in MLLMs
	3. Proposal of RESA for improving trustworthiness via reasoning techniques.

**Result:** The experiments reveal significant vulnerabilities in MLLMs, including the gap between trustworthiness and overall capabilities. The analysis shows that while mitigation methods can improve specific areas, they often compromise model utility and fail to enhance overall trustworthiness.

**Limitations:** Existing mitigation strategies show limited effectiveness and can introduce trade-offs that reduce utility.

**Conclusion:** The findings emphasize the need for better reasoning methods to mitigate risks effectively, leading to the proposal of a Reasoning-Enhanced Safety Alignment (RESA) approach for enhanced safety and performance in MLLMs.

**Abstract:** The trustworthiness of Multimodal Large Language Models (MLLMs) remains an intense concern despite the significant progress in their capabilities. Existing evaluation and mitigation approaches often focus on narrow aspects and overlook risks introduced by the multimodality. To tackle these challenges, we propose MultiTrust-X, a comprehensive benchmark for evaluating, analyzing, and mitigating the trustworthiness issues of MLLMs. We define a three-dimensional framework, encompassing five trustworthiness aspects which include truthfulness, robustness, safety, fairness, and privacy; two novel risk types covering multimodal risks and cross-modal impacts; and various mitigation strategies from the perspectives of data, model architecture, training, and inference algorithms. Based on the taxonomy, MultiTrust-X includes 32 tasks and 28 curated datasets, enabling holistic evaluations over 30 open-source and proprietary MLLMs and in-depth analysis with 8 representative mitigation methods. Our extensive experiments reveal significant vulnerabilities in current models, including a gap between trustworthiness and general capabilities, as well as the amplification of potential risks in base LLMs by both multimodal training and inference. Moreover, our controlled analysis uncovers key limitations in existing mitigation strategies that, while some methods yield improvements in specific aspects, few effectively address overall trustworthiness, and many introduce unexpected trade-offs that compromise model utility. These findings also provide practical insights for future improvements, such as the benefits of reasoning to better balance safety and performance. Based on these insights, we introduce a Reasoning-Enhanced Safety Alignment (RESA) approach that equips the model with chain-of-thought reasoning ability to discover the underlying risks, achieving state-of-the-art results.

</details>


### [40] [Confidence-Modulated Speculative Decoding for Large Language Models](https://arxiv.org/abs/2508.15371)

*Jaydip Sen, Subhasis Dasgupta, Hetvi Waghela*

**Main category:** cs.CL

**Keywords:** speculative decoding, autoregressive inference, large language models, machine translation, summarization

**Relevance Score:** 9

**TL;DR:** This paper introduces an adaptive information-theoretic framework for speculative decoding in autoregressive models, optimizing token generation and verification processes to enhance efficiency and output quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of autoregressive inference in large language models by adapting speculative decoding methods to varying model uncertainties and input complexities.

**Method:** The proposed framework employs confidence-modulated drafting, using uncertainty measures to dynamically adjust the drafting lengths and verification processes during token generation.

**Key Contributions:**

	1. Information-theoretic framework for adaptive speculative decoding
	2. Dynamic adjustment of drafting lengths based on uncertainty measures
	3. Improved resource utilization and output fidelity during token generation

**Result:** The adaptive speculative decoding method shows significant speedups in machine translation and summarization tasks, outperforming standard methods while maintaining or enhancing BLEU and ROUGE scores.

**Limitations:** 

**Conclusion:** The proposed method provides an efficient and robust decoding alternative for large language models, enhancing both speed and quality under uncertain conditions.

**Abstract:** Speculative decoding has emerged as an effective approach for accelerating autoregressive inference by parallelizing token generation through a draft-then-verify paradigm. However, existing methods rely on static drafting lengths and rigid verification criteria, limiting their adaptability across varying model uncertainties and input complexities. This paper proposes an information-theoretic framework for speculative decoding based on confidence-modulated drafting. By leveraging entropy and margin-based uncertainty measures over the drafter's output distribution, the proposed method dynamically adjusts the number of speculatively generated tokens at each iteration. This adaptive mechanism reduces rollback frequency, improves resource utilization, and maintains output fidelity. Additionally, the verification process is modulated using the same confidence signals, enabling more flexible acceptance of drafted tokens without sacrificing generation quality. Experiments on machine translation and summarization tasks demonstrate significant speedups over standard speculative decoding while preserving or improving BLEU and ROUGE scores. The proposed approach offers a principled, plug-in method for efficient and robust decoding in large language models under varying conditions of uncertainty.

</details>


### [41] [Exploiting Vocabulary Frequency Imbalance in Language Model Pre-training](https://arxiv.org/abs/2508.15390)

*Woojin Chung, Jeonghoon Kim*

**Main category:** cs.CL

**Keywords:** large language models, tokenizers, vocabulary size, complexity, loss dynamics

**Relevance Score:** 7

**TL;DR:** This paper examines the effects of vocabulary size on language model performance, showing that larger vocabularies lower the complexity of tokenized text but primarily benefit common words, with implications for tokenizer-model design.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of clarity on the benefits of increasing vocabulary sizes in large language models, as token distributions are imbalanced with few words being dominant.

**Method:** Conducted a controlled study scaling vocabulary from 24K to 196K while keeping data, compute, and optimization constant; analyzed tokenized text complexity using Kolmogorov complexity and examined loss dynamics with word-level decomposition.

**Key Contributions:**

	1. Reframed the understanding of vocabulary effects on language models as a matter of reducing token complexity.
	2. Demonstrated that larger vocabularies primarily benefit common words while increasing loss on rare words.
	3. Provided insights into the relationships between vocabulary size, token distribution, and loss dynamics.

**Result:** Larger vocabularies reduce tokenized text complexity but mainly improve loss on the most frequent words; introducing norms to input/output embeddings can reverse the boost from increased vocabulary size.

**Limitations:** 

**Conclusion:** The results suggest focusing on reducing the complexity of tokenized text is more beneficial than merely increasing vocabulary size, influencing future tokenizer and model co-design.

**Abstract:** Large language models are trained with tokenizers, and the resulting token distribution is highly imbalanced: a few words dominate the stream while most occur rarely. Recent practice favors ever-larger vocabularies, but the source of the benefit is unclear. We conduct a controlled study that scales the language model's vocabulary from 24K to 196K while holding data, compute, and optimization fixed. We first quantify the complexity of tokenized text, formalized via Kolmogorov complexity, and show that larger vocabularies reduce this complexity. Above 24K, every common word is already a single token, so further growth mainly deepens the relative token-frequency imbalance. A word-level loss decomposition shows that larger vocabularies reduce cross-entropy almost exclusively by lowering uncertainty on the 2,500 most frequent words, even though loss on the rare tail rises. Constraining input and output embedding norms to attenuate the effect of token-frequency imbalance reverses the gain, directly showing that the model exploits rather than suffers from imbalance. Because the same frequent words cover roughly 77% of tokens in downstream benchmarks, this training advantage transfers intact. We also show that enlarging model parameters with a fixed vocabulary yields the same frequent-word benefit. Our results reframe "bigger vocabularies help" as "lowering the complexity of tokenized text helps," providing a simple, principled lever for tokenizer-model co-design and clarifying the loss dynamics that govern language-model scaling in pre-training.

</details>


### [42] [Attribution, Citation, and Quotation: A Survey of Evidence-based Text Generation with Large Language Models](https://arxiv.org/abs/2508.15396)

*Tobias Schreieder, Tim Schopf, Michael Färber*

**Main category:** cs.CL

**Keywords:** Large Language Models, Evidence-Based Generation, Text Generation, Machine Learning  Evaluation, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper reviews evidence-based text generation with large language models (LLMs), offering a unified taxonomy and analyzing evaluation metrics to improve reliability and traceability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the reliability and trustworthiness concerns surrounding large language models by focusing on evidence-based text generation, linking outputs to supporting evidence.

**Method:** The authors systematically analyzed 134 papers and evaluated 300 metrics across seven dimensions to create a unified taxonomy for evidence-based text generation in LLMs.

**Key Contributions:**

	1. Unified taxonomy of evidence-based text generation with LLMs
	2. Analysis of 300 evaluation metrics across seven dimensions
	3. Identification of open challenges and future directions for research

**Result:** A comprehensive taxonomy for evidence-based text generation was introduced, along with an examination of distinctive characteristics and methods in the field, coupled with open challenges and future directions.

**Limitations:** 

**Conclusion:** The research contributes to a more coherent understanding of evidence-based text generation with LLMs, paving the way for unified benchmarks and improved practices in the community.

**Abstract:** The increasing adoption of large language models (LLMs) has been accompanied by growing concerns regarding their reliability and trustworthiness. As a result, a growing body of research focuses on evidence-based text generation with LLMs, aiming to link model outputs to supporting evidence to ensure traceability and verifiability. However, the field is fragmented due to inconsistent terminology, isolated evaluation practices, and a lack of unified benchmarks. To bridge this gap, we systematically analyze 134 papers, introduce a unified taxonomy of evidence-based text generation with LLMs, and investigate 300 evaluation metrics across seven key dimensions. Thereby, we focus on approaches that use citations, attribution, or quotations for evidence-based text generation. Building on this, we examine the distinctive characteristics and representative methods in the field. Finally, we highlight open challenges and outline promising directions for future work.

</details>


### [43] [When Audio and Text Disagree: Revealing Text Bias in Large Audio-Language Models](https://arxiv.org/abs/2508.15407)

*Cheng Wang, Gelei Deng, Xianglin Yang, Han Qiu, Tianwei Zhang*

**Main category:** cs.CL

**Keywords:** Large Audio-Language Models, multimodal inputs, text bias, benchmark, audio understanding

**Relevance Score:** 7

**TL;DR:** This paper introduces MCR-BENCH, a benchmark for evaluating Large Audio-Language Models' performance with conflicting audio-text information, revealing a bias toward textual input that hampers audio task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance and reliability of Large Audio-Language Models (LALMs) in processing inconsistent audio and text data.

**Method:** The paper presents MCR-BENCH, a benchmark for evaluating LALMs on diverse audio understanding tasks, especially when faced with conflicting audio and text pairs.

**Key Contributions:**

	1. Introduction of MCR-BENCH as a benchmark for LALMs.
	2. Demonstration of significant text bias in LALMs when faced with inconsistencies.
	3. Exploration of mitigation strategies through supervised finetuning.

**Result:** LALMs demonstrated a significant bias towards textual input over audio, leading to performance degradation in audio-centric tasks and highlighting a need for improved modality balance.

**Limitations:** Focuses primarily on inconsistencies between audio and text; other modalities are not addressed.

**Conclusion:** The findings call for better training strategies and fusion mechanisms to enhance the handling of conflicting multimodal inputs in LALMs.

**Abstract:** Large Audio-Language Models (LALMs) are enhanced with audio perception capabilities, enabling them to effectively process and understand multimodal inputs that combine audio and text. However, their performance in handling conflicting information between audio and text modalities remains largely unexamined. This paper introduces MCR-BENCH, the first comprehensive benchmark specifically designed to evaluate how LALMs prioritize information when presented with inconsistent audio-text pairs. Through extensive evaluation across diverse audio understanding tasks, we reveal a concerning phenomenon: when inconsistencies exist between modalities, LALMs display a significant bias toward textual input, frequently disregarding audio evidence. This tendency leads to substantial performance degradation in audio-centric tasks and raises important reliability concerns for real-world applications. We further investigate the influencing factors of text bias, and explore mitigation strategies through supervised finetuning, and analyze model confidence patterns that reveal persistent overconfidence even with contradictory inputs. These findings underscore the need for improved modality balance during training and more sophisticated fusion mechanisms to enhance the robustness when handling conflicting multi-modal inputs. The project is available at https://github.com/WangCheng0116/MCR-BENCH.

</details>


### [44] [LLaSO: A Foundational Framework for Reproducible Research in Large Language and Speech Model](https://arxiv.org/abs/2508.15418)

*Yirong Sun, Yizhong Geng, Peidong Wei, Yanjun Chen, Jinghan Yang, Rongfei Chen, Wei Zhang, Xiaoyu Shen*

**Main category:** cs.CL

**Keywords:** Large Speech-Language Models, Open Framework, Datasets, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** Introduction of LLaSO, a fully open framework for large-scale speech-language modeling, providing datasets and benchmarks to facilitate research.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of transparency and fragmented architectures in large speech-language models has hindered systematic comparison and reproducibility in research.

**Method:** Development of LLaSO, which includes LLaSO-Align, LLaSO-Instruct, and LLaSO-Eval, alongside the release of LLaSO-Base, a 3.8B-parameter reference model trained on public data.

**Key Contributions:**

	1. Introduction of LLaSO as an open framework for LSLMs
	2. Release of extensive datasets for training and evaluation
	3. Establishment of a reproducible benchmark for community use

**Result:** LLaSO-Base achieves a normalized score of 0.72, establishing a strong baseline that surpasses comparable models, while exposing performance gaps in unseen tasks.

**Limitations:** Generalization gaps persist on unseen tasks, especially in pure audio scenarios.

**Conclusion:** LLaSO aims to unify research efforts in LSLM by releasing comprehensive data and models, accelerating progress in the field.

**Abstract:** The development of Large Speech-Language Models (LSLMs) has been slowed by fragmented architectures and a lack of transparency, hindering the systematic comparison and reproducibility of research. Unlike in the vision-language domain, the LSLM field suffers from the common practice of releasing model weights without their corresponding training data and configurations. To address these critical gaps, we introduce LLaSO, the first fully open, end-to-end framework for large-scale speech-language modeling. LLaSO provides the community with three essential resources: (1) LLaSO-Align, a 12M-instance speech-text alignment corpus; (2) LLaSO-Instruct, a 13.5M-instance multi-task instruction-tuning dataset; and (3) LLaSO-Eval, a reproducible benchmark for standardized evaluation. To validate our framework, we build and release LLaSO-Base, a 3.8B-parameter reference model trained exclusively on our public data. It achieves a normalized score of 0.72, establishing a strong, reproducible baseline that surpasses comparable models. Our analysis reveals that while broader training coverage enhances performance, significant generalization gaps persist on unseen tasks, particularly in pure audio scenarios. By releasing the complete stack of data, benchmarks, and models, LLaSO establishes a foundational open standard to unify research efforts and accelerate community-driven progress in LSLMs. We release the code, dataset, pretrained models, and results in https://github.com/EIT-NLP/LLaSO.

</details>


### [45] [A Study of Privacy-preserving Language Modeling Approaches](https://arxiv.org/abs/2508.15421)

*Pritilata Saha, Abhirup Sinha*

**Main category:** cs.CL

**Keywords:** privacy, language models, privacy preservation, human rights, data security

**Relevance Score:** 9

**TL;DR:** This study investigates privacy-preserving approaches in language modeling, emphasizing their importance in protecting individuals' privacy rights and outlining future research directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy risks associated with language models trained on sensitive data and to explore methods for mitigating these risks.

**Method:** Comprehensive study of various privacy-preserving language modeling approaches, highlighting their strengths and investigating their limitations.

**Key Contributions:**

	1. Comprehensive review of privacy-preserving language models
	2. Analysis of strengths and limitations of these approaches
	3. Identification of future research directions in privacy preservation.

**Result:** An in-depth overview of privacy-preserving approaches, contributing insights to inform future research directions in this field.

**Limitations:** The study may not cover all emerging techniques in privacy preservation as the field is rapidly evolving.

**Conclusion:** This research is vital for understanding and improving privacy protection in language models, given the increasing use of these models in sensitive applications.

**Abstract:** Recent developments in language modeling have increased their use in various applications and domains. Language models, often trained on sensitive data, can memorize and disclose this information during privacy attacks, raising concerns about protecting individuals' privacy rights. Preserving privacy in language models has become a crucial area of research, as privacy is one of the fundamental human rights. Despite its significance, understanding of how much privacy risk these language models possess and how it can be mitigated is still limited. This research addresses this by providing a comprehensive study of the privacy-preserving language modeling approaches. This study gives an in-depth overview of these approaches, highlights their strengths, and investigates their limitations. The outcomes of this study contribute to the ongoing research on privacy-preserving language modeling, providing valuable insights and outlining future research directions.

</details>


### [46] [M-HELP: Using Social Media Data to Detect Mental Health Help-Seeking Signals](https://arxiv.org/abs/2508.15440)

*MSVPJ Sathvik, Zuhair Hasan Shaik, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** mental health, dataset, help-seeking behavior, social media, AI models

**Relevance Score:** 7

**TL;DR:** The paper introduces the M-Help dataset for detecting help-seeking behavior related to mental health on social media, enabling better identification and diagnosis of mental health disorders.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the critical gap in identifying individuals actively seeking help for mental health issues on social media, amidst a global mental health crisis.

**Method:** The study introduces the M-Help dataset, which categorizes help-seeking behavior and links it to specific mental health disorders and their causes.

**Key Contributions:**

	1. Introduction of the M-Help dataset for help-seeking behavior
	2. Links mental health disorders to specific life stressors
	3. Facilitates advanced AI model training for better diagnosis and understanding of mental health issues.

**Result:** AI models trained on the M-Help dataset can successfully identify help-seekers, diagnose mental health conditions, and reveal underlying causes.

**Limitations:** 

**Conclusion:** The M-Help dataset offers a comprehensive approach to understanding and addressing help-seeking behaviors related to mental health in social media contexts.

**Abstract:** Mental health disorders are a global crisis. While various datasets exist for detecting such disorders, there remains a critical gap in identifying individuals actively seeking help. This paper introduces a novel dataset, M-Help, specifically designed to detect help-seeking behavior on social media. The dataset goes beyond traditional labels by identifying not only help-seeking activity but also specific mental health disorders and their underlying causes, such as relationship challenges or financial stressors. AI models trained on M-Help can address three key tasks: identifying help-seekers, diagnosing mental health conditions, and uncovering the root causes of issues.

</details>


### [47] [Principle Methods of Rendering Non-equivalent Words from Uzbek and Dari to Russian and English](https://arxiv.org/abs/2508.15453)

*Mohammad Ibrahim Qani*

**Main category:** cs.CL

**Keywords:** translation, non-equivalent words, cultural context, language rendering, linguistics

**Relevance Score:** 3

**TL;DR:** Research on methods to translate non-equivalent words from source languages to target languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address misunderstandings that arise from non-equivalent words in translation, particularly in cultural and traditional contexts.

**Method:** Library-based research was used to identify methods and rules for rendering non-equivalent words.

**Key Contributions:**

	1. Introduced methods for translating non-equivalent words
	2. Provided practical examples of 25 rendered words
	3. Highlighted the significance of cultural context in translation.

**Result:** Identified different methods for translating 25 specific non-equivalent words from Dar and Uzbek into English and Russian.

**Limitations:** 

**Conclusion:** The study provides insights into professional rendering of non-equivalent words, with an emphasis on the ongoing need for research in this area.

**Abstract:** These pure languages understanding directly relates to translation knowledge where linguists and translators need to work and research to eradicate misunderstanding. Misunderstandings mostly appear in non-equivalent words because there are different local and internal words like food, garment, cultural and traditional words and others in every notion. Truly, most of these words do not have equivalent in the target language and these words need to be worked and find their equivalent in the target language to fully understand the both languages. The purpose of this research is to introduce the methods of rendering non-equivalent words professionally from the source language to the target language and this research has been completed using library-based research. However, some of these non-equivalent words are already professionally rendered to the target language but still there many other words to be rendered. As a result, this research paper includes different ways and rules of rendering non-equivalent words from source language to the target language and 25 non-equvalent words have been rendered from Dar & Uzbek into English and Russian languages.

</details>


### [48] [PyTOD: Programmable Task-Oriented Dialogue with Execution Feedback](https://arxiv.org/abs/2508.15456)

*Alexandru Coca, Bo-Hsiang Tseng, Pete Boothroyd, Jianpeng Cheng, Mark Gaynor, Zhenxing Zhang, Joe Stacey, Tristan Guigue, Héctor Martinez Alonso, Diarmuid Ó Séaghdha, Anders Johannsen*

**Main category:** cs.CL

**Keywords:** task-oriented dialogue, state tracking, language models, dialogue systems, execution-aware tracking

**Relevance Score:** 8

**TL;DR:** PyTOD is a task-oriented dialogue agent that uses executable code for accurate state tracking and outperforms baselines on the SGD benchmark.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of programmable task-oriented dialogue agents relies on accurate state tracking, which is essential for maintaining dialogue flow and achieving user goals.

**Method:** PyTOD implements a constrained decoding method that utilizes a language model for following API schemata instead of traditional grammar rules, enabling effective state tracking and error correction through policy and execution feedback.

**Key Contributions:**

	1. Introduction of PyTOD, a dialogue agent that generates executable code for state tracking.
	2. Use of a constrained decoding approach with language models for improved dialogue management.
	3. Demonstration of superior performance in state tracking and user goal estimation on the SGD benchmark.

**Result:** PyTOD achieves state-of-the-art performance on the SGD benchmark, surpassing strong baselines in both accuracy and the ability to estimate user goals as dialogues evolve.

**Limitations:** 

**Conclusion:** The findings demonstrate the potential of execution-aware state tracking in enhancing the performance of dialogue systems.

**Abstract:** Programmable task-oriented dialogue (TOD) agents enable language models to follow structured dialogue policies, but their effectiveness hinges on accurate state tracking. We present PyTOD, an agent that generates executable code to track dialogue state and uses policy and execution feedback for efficient error correction. To this end, PyTOD employs a simple constrained decoding approach, using a language model instead of grammar rules to follow API schemata. This leads to state-of-the-art state tracking performance on the challenging SGD benchmark. Our experiments show that PyTOD surpasses strong baselines in both accuracy and robust user goal estimation as the dialogue progresses, demonstrating the effectiveness of execution-aware state tracking.

</details>


### [49] [RadReason: Radiology Report Evaluation Metric with Reasons and Sub-Scores](https://arxiv.org/abs/2508.15464)

*Yingshu Li, Yunyi Liu, Lingqiao Liu, Lei Wang, Luping Zhou*

**Main category:** cs.CL

**Keywords:** radiology reports, evaluation framework, machine learning, clinical workflows, explainable AI

**Relevance Score:** 9

**TL;DR:** RadReason is an evaluation framework for radiology reports that provides fine-grained sub-scores and human-readable justifications, improving clinical relevance and interpretability over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a pressing need for clinically grounded and interpretable metrics in the evaluation of automatically generated radiology reports to improve real-world clinical workflows.

**Method:** RadReason utilizes Group Relative Policy Optimization and introduces Sub-score Dynamic Weighting and Majority-Guided Advantage Scaling to enhance score evaluation and explainability.

**Key Contributions:**

	1. Innovative evaluation framework for radiology reports
	2. Fine-grained sub-scores across clinically defined error types
	3. Human-readable justifications for evaluation scores

**Result:** RadReason demonstrates superior performance over all previous offline metrics on the ReXVal benchmark and matches GPT-4-based evaluations, while maintaining cost-efficiency and clinical applicability.

**Limitations:** 

**Conclusion:** RadReason provides a robust evaluation method for radiology reports, enhancing the interpretability and practical utility of automated assessments in clinical settings.

**Abstract:** Evaluating automatically generated radiology reports remains a fundamental challenge due to the lack of clinically grounded, interpretable, and fine-grained metrics. Existing methods either produce coarse overall scores or rely on opaque black-box models, limiting their usefulness in real-world clinical workflows. We introduce RadReason, a novel evaluation framework for radiology reports that not only outputs fine-grained sub-scores across six clinically defined error types, but also produces human-readable justifications that explain the rationale behind each score. Our method builds on Group Relative Policy Optimization and incorporates two key innovations: (1) Sub-score Dynamic Weighting, which adaptively prioritizes clinically challenging error types based on live F1 statistics; and (2) Majority-Guided Advantage Scaling, which adjusts policy gradient updates based on prompt difficulty derived from sub-score agreement. Together, these components enable more stable optimization and better alignment with expert clinical judgment. Experiments on the ReXVal benchmark show that RadReason surpasses all prior offline metrics and achieves parity with GPT-4-based evaluations, while remaining explainable, cost-efficient, and suitable for clinical deployment. Code will be released upon publication.

</details>


### [50] [SLM4Offer: Personalized Marketing Offer Generation Using Contrastive Learning Based Fine-Tuning](https://arxiv.org/abs/2508.15471)

*Vedasamhitha Challapalli, Konduru Venkat Sai, Piyush Pratap Singh, Rupesh Prasad, Arvind Maurya, Atul Singh*

**Main category:** cs.CL

**Keywords:** personalized marketing, machine learning, customer engagement, AI, generative models

**Relevance Score:** 4

**TL;DR:** This paper introduces SLM4Offer, a generative AI model designed for personalized offer generation that outperforms traditional methods using contrastive learning techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved personalized marketing strategies that can enhance customer engagement and increase conversion rates, potentially boosting revenue significantly.

**Method:** SLM4Offer is developed by fine-tuning Google's T5-Small model using a contrastive learning approach, incorporating InfoNCE loss to align customer personas with offers in a shared embedding space.

**Key Contributions:**

	1. Introduction of SLM4Offer, a generative AI model for personalized marketing.
	2. Use of contrastive learning to improve offer acceptance rates.
	3. Development of a synthetic dataset to simulate customer behaviour and offer acceptance patterns.

**Result:** Experimental evaluations show that SLM4Offer achieves a 17% improvement in offer acceptance rates compared to a supervised fine-tuning baseline, indicating the effectiveness of the contrastive approach.

**Limitations:** 

**Conclusion:** The study concludes that the adaptive learning behavior from contrastive loss significantly enhances the model's generalizability for personalized marketing applications.

**Abstract:** Personalized marketing has emerged as a pivotal strategy for enhancing customer engagement and driving business growth. Academic and industry efforts have predominantly focused on recommendation systems and personalized advertisements. Nonetheless, this facet of personalization holds significant potential for increasing conversion rates and improving customer satisfaction. Prior studies suggest that well-executed personalization strategies can boost revenue by up to 40 percent, underscoring the strategic importance of developing intelligent, data-driven approaches for offer generation. This work introduces SLM4Offer, a generative AI model for personalized offer generation, developed by fine-tuning a pre-trained encoder-decoder language model, specifically Google's Text-to-Text Transfer Transformer (T5-Small 60M) using a contrastive learning approach. SLM4Offer employs InfoNCE (Information Noise-Contrastive Estimation) loss to align customer personas with relevant offers in a shared embedding space. A key innovation in SLM4Offer lies in the adaptive learning behaviour introduced by contrastive loss, which reshapes the latent space during training and enhances the model's generalizability. The model is fine-tuned and evaluated on a synthetic dataset designed to simulate customer behaviour and offer acceptance patterns. Experimental results demonstrate a 17 percent improvement in offer acceptance rate over a supervised fine-tuning baseline, highlighting the effectiveness of contrastive objectives in advancing personalized marketing.

</details>


### [51] [Subjective Behaviors and Preferences in LLM: Language of Browsing](https://arxiv.org/abs/2508.15474)

*Sai Sundaresan, Harshita Chopra, Atanu R. Sinha, Koustava Goswami, Nagasai Saketh Naidu, Raghav Karan, N Anushka*

**Main category:** cs.CL

**Keywords:** Language Models, Human-Computer Interaction, User Behavior

**Relevance Score:** 9

**TL;DR:** This paper investigates the performance of a small language model (LM) in capturing users' subjective browsing behaviors compared to larger models, introducing HeTLM for better alignment with individual preferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the notion that large language models are universally better suited for capturing diverse user behaviors and preferences in browsing activities.

**Method:** The study employs clusterwise LM training with HeTLM, evaluating a small LM trained with a page-level tokenizer against various larger LM configurations.

**Key Contributions:**

	1. Introduction of HeTLM for training language models on user-specific behaviors
	2. Demonstration that a small LM can outperform larger models in specific tasks
	3. Evidence of improved performance consistency with heterogeneous parameters.

**Result:** A small LM shows superior performance in representing user browsing 'language', outperforming larger models; HeTLM also yields better performance consistency across diverse user behaviors.

**Limitations:** 

**Conclusion:** The findings suggest that smaller, more specialized models can effectively capture subjective user behaviors, providing better alignment compared to larger, generalized models.

**Abstract:** A Large Language Model (LLM) offers versatility across domains and tasks, purportedly benefiting users with a wide variety of behaviors and preferences. We question this perception about an LLM when users have inherently subjective behaviors and preferences, as seen in their ubiquitous and idiosyncratic browsing of websites or apps. The sequential behavior logs of pages, thus generated, form something akin to each user's self-constructed "language", albeit without the structure and grammar imbued in natural languages. We ask: (i) Can a small LM represent the "language of browsing" better than a large LM? (ii) Can an LM with a single set of parameters (or, single LM) adequately capture myriad users' heterogeneous, subjective behaviors and preferences? (iii) Can a single LM with high average performance, yield low variance in performance to make alignment good at user level? We introduce clusterwise LM training, HeTLM (Heterogeneity aware Training of Language Model), appropriate for subjective behaviors. We find that (i) a small LM trained using a page-level tokenizer outperforms large pretrained or finetuned LMs; (ii) HeTLM with heterogeneous cluster specific set of parameters outperforms a single LM of the same family, controlling for the number of parameters; and (iii) a higher mean and a lower variance in generation ensues, implying improved alignment.

</details>


### [52] [Influence-driven Curriculum Learning for Pre-training on Limited Data](https://arxiv.org/abs/2508.15475)

*Loris Schoenegger, Lukas Thoma, Terra Blevins, Benjamin Roth*

**Main category:** cs.CL

**Keywords:** curriculum learning, language models, training data influence

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of curriculum learning for pre-training language models using a model-centric approach to difficulty metrics.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if curriculum learning can enhance language model pre-training by using a model-centric view of difficulty rather than traditional human-centered metrics.

**Method:** The authors experiment with sorting training examples based on their 'training data influence,' which measures the impact of training examples on the model’s outputs, in contrast to the random ordering typically used.

**Key Contributions:**

	1. Introduces a model-centric difficulty metric for training examples
	2. Demonstrates substantial performance improvements using this metric
	3. Provides a new perspective on curriculum learning for language models

**Result:** Models trained with the new curriculum approach demonstrated an improvement of over 10 percentage points on benchmark tests compared to those trained randomly.

**Limitations:** 

**Conclusion:** Adopting a model-centric notion of difficulty for curriculum learning significantly benefits language model pre-training.

**Abstract:** Curriculum learning, a training technique where data is presented to the model in order of example difficulty (e.g., from simpler to more complex documents), has shown limited success for pre-training language models. In this work, we investigate whether curriculum learning becomes competitive if we replace conventional human-centered difficulty metrics with one that more closely corresponds to example difficulty as observed during model training. Specifically, we experiment with sorting training examples by their \textit{training data influence}, a score which estimates the effect of individual training examples on the model's output. Models trained on our curricula are able to outperform ones trained in random order by over 10 percentage points in benchmarks, confirming that curriculum learning is beneficial for language model pre-training, as long as a more model-centric notion of difficulty is adopted.

</details>


### [53] [SLM-Bench: A Comprehensive Benchmark of Small Language Models on Environmental Impacts -- Extended Version](https://arxiv.org/abs/2508.15478)

*Nghiem Thanh Pham, Tung Kieu, Duc-Manh Nguyen, Son Ha Xuan, Nghia Duong-Trung, Danh Le-Phuoc*

**Main category:** cs.CL

**Keywords:** Small Language Models, Benchmarking, Environmental Impact, NLP, Efficiency

**Relevance Score:** 8

**TL;DR:** SLM-Bench is introduced as a benchmark for evaluating Small Language Models across various dimensions, including accuracy and sustainability.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of systematic evaluation for Small Language Models (SLMs) regarding performance and environmental impact.

**Method:** SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets across 14 domains, comparing results on 4 hardware configurations and quantifying 11 metrics.

**Key Contributions:**

	1. Introduction of SLM-Bench for evaluating SLMs
	2. Incorporation of environmental impact in NLP model assessment
	3. Development of an open-source benchmarking pipeline for reproducibility

**Result:** The evaluation shows diverse trade-offs among SLMs, revealing that some excel in accuracy while others are more energy efficient.

**Limitations:** 

**Conclusion:** SLM-Bench establishes a new evaluation standard for SLMs, promoting better understanding of their real-world applicability and resource efficiency.

**Abstract:** Small Language Models (SLMs) offer computational efficiency and accessibility, yet a systematic evaluation of their performance and environmental impact remains lacking. We introduce SLM-Bench, the first benchmark specifically designed to assess SLMs across multiple dimensions, including accuracy, computational efficiency, and sustainability metrics. SLM-Bench evaluates 15 SLMs on 9 NLP tasks using 23 datasets spanning 14 domains. The evaluation is conducted on 4 hardware configurations, providing a rigorous comparison of their effectiveness. Unlike prior benchmarks, SLM-Bench quantifies 11 metrics across correctness, computation, and consumption, enabling a holistic assessment of efficiency trade-offs. Our evaluation considers controlled hardware conditions, ensuring fair comparisons across models. We develop an open-source benchmarking pipeline with standardized evaluation protocols to facilitate reproducibility and further research. Our findings highlight the diverse trade-offs among SLMs, where some models excel in accuracy while others achieve superior energy efficiency. SLM-Bench sets a new standard for SLM evaluation, bridging the gap between resource efficiency and real-world applicability.

</details>


### [54] [HebID: Detecting Social Identities in Hebrew-language Political Text](https://arxiv.org/abs/2508.15483)

*Guy Mor-Lan, Naama Rivlin-Angert, Yael R. Kaplan, Tamir Sheafer, Shaul R. Shenhav*

**Main category:** cs.CL

**Keywords:** social identity, Hebrew, political language, multilabel corpus, language models

**Relevance Score:** 4

**TL;DR:** HebID is a multilabel Hebrew corpus introduced for social identity detection in political language, with benchmark results showing the effectiveness of Hebrew-tuned LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current datasets for identity detection are primarily English-centric and lack nuanced categories. HebID aims to provide a linguistic resource for the study of social identities in Hebrew within political contexts.

**Method:** Creation of a multilabel corpus consisting of 5,536 sentences from Israeli politicians' Facebook posts, annotated for twelve social identities, followed by benchmarking various ML models including generative LLMs to analyze identity expression.

**Key Contributions:**

	1. Introduction of the first multilabel Hebrew corpus for social identity detection.
	2. Demonstrated the effectiveness of Hebrew-tuned LLMs in achieving high performance in identity classification.
	3. Enabled comprehensive analysis of identity expression in political discourse with insights into public and elite identity alignment.

**Result:** Hebrew-tuned LLMs achieve the best performance with a macro-$F_1$ score of 0.74, and insights were gained regarding identity expressions in political discourse, including trends and gender variations.

**Limitations:** 

**Conclusion:** HebID serves not only as a foundation for Hebrew-based social identity research but also as a potential blueprint for similar studies in other linguistic contexts.

**Abstract:** Political language is deeply intertwined with social identities. While social identities are often shaped by specific cultural contexts and expressed through particular uses of language, existing datasets for group and identity detection are predominantly English-centric, single-label and focus on coarse identity categories. We introduce HebID, the first multilabel Hebrew corpus for social identity detection: 5,536 sentences from Israeli politicians' Facebook posts (Dec 2018-Apr 2021), manually annotated for twelve nuanced social identities (e.g. Rightist, Ultra-Orthodox, Socially-oriented) grounded by survey data. We benchmark multilabel and single-label encoders alongside 2B-9B-parameter generative LLMs, finding that Hebrew-tuned LLMs provide the best results (macro-$F_1$ = 0.74). We apply our classifier to politicians' Facebook posts and parliamentary speeches, evaluating differences in popularity, temporal trends, clustering patterns, and gender-related variations in identity expression. We utilize identity choices from a national public survey, enabling a comparison between identities portrayed in elite discourse and the public's identity priorities. HebID provides a comprehensive foundation for studying social identities in Hebrew and can serve as a model for similar research in other non-English political contexts.

</details>


### [55] [Dream 7B: Diffusion Large Language Models](https://arxiv.org/abs/2508.15487)

*Jiacheng Ye, Zhihui Xie, Lin Zheng, Jiahui Gao, Zirui Wu, Xin Jiang, Zhenguo Li, Lingpeng Kong*

**Main category:** cs.CL

**Keywords:** Diffusion Models, Language Models, Generative Models

**Relevance Score:** 8

**TL;DR:** Dream 7B is a highly powerful open diffusion language model that surpasses existing models in various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a more capable language model using discrete diffusion modeling techniques that improve generation quality and flexibility.

**Method:** Dream 7B employs discrete diffusion modeling for parallel sequence refinement through iterative denoising, initialized with AR-based LLM techniques and enhanced by context-adaptive token-level noise rescheduling.

**Key Contributions:**

	1. Introduction of discrete diffusion modeling for language tasks
	2. Demonstrated planning abilities and inference flexibility
	3. Release of Dream-Base and Dream-Instruct for further research

**Result:** The model outperforms existing diffusion language models on general tasks, mathematics, and coding, showcasing advanced planning abilities and flexible inference.

**Limitations:** 

**Conclusion:** Dream 7B releases both Dream-Base and Dream-Instruct versions to support ongoing research in diffusion-based language modeling.

**Abstract:** We introduce Dream 7B, the most powerful open diffusion large language model to date. Unlike autoregressive (AR) models that generate tokens sequentially, Dream 7B employs discrete diffusion modeling to refine sequences in parallel through iterative denoising. Our model consistently outperforms existing diffusion language models on general, mathematical, and coding tasks. Dream 7B demonstrates superior planning abilities and inference flexibility, including arbitrary-order generation, infilling capabilities, and tunable quality-speed trade-offs. These results are achieved through simple yet effective training techniques, including AR-based LLM initialization and context-adaptive token-level noise rescheduling. We release both Dream-Base and Dream-Instruct to facilitate further research in diffusion-based language modeling.

</details>


### [56] [The Enemy from Within: A Study of Political Delegitimization Discourse in Israeli Political Speech](https://arxiv.org/abs/2508.15524)

*Naama Rivlin-Angert, Guy Mor-Lan*

**Main category:** cs.CL

**Keywords:** political discourse, delegitimization, natural language processing, machine learning, social media

**Relevance Score:** 3

**TL;DR:** This study explores political delegitimization discourse using a large Hebrew-language corpus. A two-stage classification pipeline identifies instances of PDD and its characteristics across different platforms and time spans.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and analyze the patterns and prevalence of political delegitimization discourse in democratic contexts.

**Method:** A two-stage classification pipeline combining fine-tuned encoder models and decoder LLMs was used to classify instances of political delegitimization discourse from a curated corpus of speech and social media data.

**Key Contributions:**

	1. Introduction of a novel Hebrew-language corpus for PDD analysis.
	2. Development of DictaLM 2.0 for effective PDD detection and classification.
	3. Insights into the trends and characteristics of PDD across different political contexts.

**Result:** The best model (DictaLM 2.0) achieved an F$_1$ score of 0.74 for binary PDD detection and 0.67 for classification of delegitimization characteristics. A notable increase in PDD was observed over three decades, with varying intensity across genders, political affiliations, and platforms.

**Limitations:** 

**Conclusion:** Automated analysis of political delegitimization discourse is feasible and valuable for understanding trends in democratic discourse.

**Abstract:** We present the first large-scale computational study of political delegitimization discourse (PDD), defined as symbolic attacks on the normative validity of political entities. We curate and manually annotate a novel Hebrew-language corpus of 10,410 sentences drawn from Knesset speeches (1993-2023), Facebook posts (2018-2021), and leading news outlets, of which 1,812 instances (17.4\%) exhibit PDD and 642 carry additional annotations for intensity, incivility, target type, and affective framing. We introduce a two-stage classification pipeline combining finetuned encoder models and decoder LLMs. Our best model (DictaLM 2.0) attains an F$_1$ of 0.74 for binary PDD detection and a macro-F$_1$ of 0.67 for classification of delegitimization characteristics. Applying this classifier to longitudinal and cross-platform data, we see a marked rise in PDD over three decades, higher prevalence on social media versus parliamentary debate, greater use by male than female politicians, and stronger tendencies among right-leaning actors - with pronounced spikes during election campaigns and major political events. Our findings demonstrate the feasibility and value of automated PDD analysis for understanding democratic discourse.

</details>


### [57] [SafetyFlow: An Agent-Flow System for Automated LLM Safety Benchmarking](https://arxiv.org/abs/2508.15526)

*Xiangyang Zhu, Yuan Tian, Chunyi Li, Kaiwei Zhang, Wei Sun, Guangtao Zhai*

**Main category:** cs.CL

**Keywords:** large language models, safety evaluation, automation, benchmarking, AI safety

**Relevance Score:** 9

**TL;DR:** SafetyFlow automates the construction of LLM safety benchmarks, significantly reducing time and resource consumption while improving dataset quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and limitations of existing LLM safety evaluation benchmarks that rely on manual curation.

**Method:** SafetyFlow orchestrates seven specialized agents to automatically construct a safety benchmark, producing SafetyFlowBench with a diverse set of queries.

**Key Contributions:**

	1. First fully automated benchmarking pipeline for LLM safety evaluation.
	2. Comprehensive safety benchmark dataset constructed: SafetyFlowBench.
	3. Reduction of time and resources in benchmark creation.

**Result:** SafetyFlowBench contains 23,446 queries with low redundancy and high discriminative power, improving the efficiency of safety evaluations for LLMs.

**Limitations:** 

**Conclusion:** The study demonstrates the efficacy of fully automated safety benchmarking for LLMs, validated by extensive experimentation on 49 advanced LLMs.

**Abstract:** The rapid proliferation of large language models (LLMs) has intensified the requirement for reliable safety evaluation to uncover model vulnerabilities. To this end, numerous LLM safety evaluation benchmarks are proposed. However, existing benchmarks generally rely on labor-intensive manual curation, which causes excessive time and resource consumption. They also exhibit significant redundancy and limited difficulty. To alleviate these problems, we introduce SafetyFlow, the first agent-flow system designed to automate the construction of LLM safety benchmarks. SafetyFlow can automatically build a comprehensive safety benchmark in only four days without any human intervention by orchestrating seven specialized agents, significantly reducing time and resource cost. Equipped with versatile tools, the agents of SafetyFlow ensure process and cost controllability while integrating human expertise into the automatic pipeline. The final constructed dataset, SafetyFlowBench, contains 23,446 queries with low redundancy and strong discriminative power. Our contribution includes the first fully automated benchmarking pipeline and a comprehensive safety benchmark. We evaluate the safety of 49 advanced LLMs on our dataset and conduct extensive experiments to validate our efficacy and efficiency.

</details>


### [58] [Trained Miniatures: Low cost, High Efficacy SLMs for Sales & Marketing](https://arxiv.org/abs/2508.15617)

*Ishaan Bhola, Mukunda NS, Sravanth Kurmala, Harsh Nandwani, Arihant Jain*

**Main category:** cs.CL

**Keywords:** Small Language Models, Cost Reduction, Domain-Specific Responses

**Relevance Score:** 8

**TL;DR:** This paper discusses 'Trained Miniatures', which are Small Language Models fine-tuned for specific applications to reduce computational costs while maintaining performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need to reduce high computational costs associated with large language models for specific applications like sales and marketing.

**Method:** Introduce and define Trained Miniatures as Small Language Models (SLMs) that are fine-tuned for targeted high-value applications.

**Key Contributions:**

	1. Definition of Trained Miniatures
	2. Demonstration of cost-effectiveness of SLMs for targeted applications
	3. Potential applications in sales and marketing outreach

**Result:** Trained Miniatures generate domain-specific responses at a fraction of the computation and cost compared to larger models.

**Limitations:** 

**Conclusion:** Trained Miniatures provide a viable alternative to large language models, making advanced text generation accessible for specific applications.

**Abstract:** Large language models (LLMs) excel in text generation; however, these creative elements require heavy computation and are accompanied by a steep cost. Especially for targeted applications such as sales and marketing outreach, these costs are far from feasible. This paper introduces the concept of "Trained Miniatures" - Small Language Models(SLMs) fine-tuned for specific, high-value applications, generating similar domain-specific responses for a fraction of the cost.

</details>


### [59] [SDGO: Self-Discrimination-Guided Optimization for Consistent Safety in Large Language Models](https://arxiv.org/abs/2508.15648)

*Peng Ding, Wen Sun, Dailin Li, Wei Zou, Jiaming Wang, Jiajun Chen, Shujian Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Safety, Discrimination, Jailbreaking attacks

**Relevance Score:** 8

**TL;DR:** This paper introduces SDGO, a reinforcement learning framework designed to enhance the safety of Large Language Models by aligning their discrimination and generation capabilities against harmful content generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of jailbreaking attacks in Large Language Models (LLMs), revealing a critical inconsistency where LLMs can identify harmful requests better than they can defend against them.

**Method:** The authors propose a framework called SDGO (Self-Discrimination-Guided Optimization), which uses the model's own discrimination capabilities as a reward signal in a reinforcement learning setup to iteratively improve generation safety.

**Key Contributions:**

	1. Introduction of SDGO, a novel reinforcement learning framework for improving LLM safety.
	2. Demonstrating significant improvements in model safety compared to prompt-based and training-based baselines.
	3. Providing code and datasets for reproducibility and further research.

**Result:** Experiments show that SDGO significantly enhances model safety against out-of-distribution jailbreaking attacks while maintaining helpfulness on general benchmarks compared to existing baselines.

**Limitations:** The paper does not mention any specific limitations, but the effectiveness of SDGO in varied real-world scenarios remains to be tested.

**Conclusion:** By aligning the discrimination and generation capabilities, SDGO achieves a tight coupling that improves overall model robustness with minimal additional discriminative samples needed.

**Abstract:** Large Language Models (LLMs) excel at various natural language processing tasks but remain vulnerable to jailbreaking attacks that induce harmful content generation. In this paper, we reveal a critical safety inconsistency: LLMs can more effectively identify harmful requests as discriminators than defend against them as generators. This insight inspires us to explore aligning the model's inherent discrimination and generation capabilities. To this end, we propose SDGO (Self-Discrimination-Guided Optimization), a reinforcement learning framework that leverages the model's own discrimination capabilities as a reward signal to enhance generation safety through iterative self-improvement. Our method does not require any additional annotated data or external models during the training phase. Extensive experiments demonstrate that SDGO significantly improves model safety compared to both prompt-based and training-based baselines while maintaining helpfulness on general benchmarks. By aligning LLMs' discrimination and generation capabilities, SDGO brings robust performance against out-of-distribution (OOD) jailbreaking attacks. This alignment achieves tighter coupling between these two capabilities, enabling the model's generation capability to be further enhanced with only a small amount of discriminative samples. Our code and datasets are available at https://github.com/NJUNLP/SDGO.

</details>


### [60] [Benchmarking Computer Science Survey Generation](https://arxiv.org/abs/2508.15658)

*Weihang Su, Anzhe Xie, Qingyao Ai, Jianming Long, Jiaxin Mao, Ziyi Ye, Yiqun Liu*

**Main category:** cs.CL

**Keywords:** survey generation, large language models, benchmarking, scientific literature, automated evaluation

**Relevance Score:** 9

**TL;DR:** SurGE is a benchmark for evaluating automated scientific survey generation using large language models, addressing the lack of standardized evaluation in this area.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing volume of academic literature makes manual creation of survey articles infeasible, prompting the need for automated solutions.

**Method:** SurGE includes a collection of test instances with topic descriptions and expert-written surveys, alongside a large academic corpus of over one million papers for retrieval. An evaluation framework assesses generated surveys based on information coverage, referencing accuracy, structural organization, and content quality.

**Key Contributions:**

	1. Introduction of the SurGE benchmark for scientific survey generation
	2. Provision of a large-scale academic corpus for research
	3. Development of an automated evaluation framework for survey quality assessment

**Result:** Evaluation of various LLM-based approaches indicates that generating high-quality surveys is still a complex challenge, necessitating further research.

**Limitations:** The complexity of survey generation remains a significant challenge, indicating that the current state of LLMs is insufficient for reliable outcomes.

**Conclusion:** The study emphasizes the difficulties in automated survey generation and the need for ongoing advancements in the field.

**Abstract:** Scientific survey articles play a vital role in summarizing research progress, yet their manual creation is becoming increasingly infeasible due to the rapid growth of academic literature. While large language models (LLMs) offer promising capabilities for automating this process, progress in this area is hindered by the absence of standardized benchmarks and evaluation protocols. To address this gap, we introduce SurGE (Survey Generation Evaluation), a new benchmark for evaluating scientific survey generation in the computer science domain. SurGE consists of (1) a collection of test instances, each including a topic description, an expert-written survey, and its full set of cited references, and (2) a large-scale academic corpus of over one million papers that serves as the retrieval pool. In addition, we propose an automated evaluation framework that measures generated surveys across four dimensions: information coverage, referencing accuracy, structural organization, and content quality. Our evaluation of diverse LLM-based approaches shows that survey generation remains highly challenging, even for advanced self-reflection frameworks. These findings highlight the complexity of the task and the necessity for continued research. We have open-sourced all the code, data, and models at: https://github.com/oneal2000/SurGE

</details>


### [61] [Position Bias Mitigates Position Bias:Mitigate Position Bias Through Inter-Position Knowledge Distillation](https://arxiv.org/abs/2508.15709)

*Yifei Wang, Feng Xiong, Yong Wang, Linjing Li, Xiangxiang Chu, Daniel Dajun Zeng*

**Main category:** cs.CL

**Keywords:** positional bias, knowledge distillation, long-context comprehension, retrieval, reasoning

**Relevance Score:** 8

**TL;DR:** Pos2Distill is a framework designed to mitigate positional bias in long-context comprehension by transferring knowledge from advantageous to less favorable context positions, leading to improved performance in retrieval and reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Positional bias significantly impairs comprehension and processing in long-context situations, necessitating effective interventions.

**Method:** The Pos2Distill framework employs knowledge distillation from advantageous positions to counteract positional bias.

**Key Contributions:**

	1. Introduction of the Pos2Distill framework for addressing positional bias.
	2. Development of specialized systems Pos2Distill-R1 and Pos2Distill-R2 for distinct tasks.
	3. Demonstration of strong cross-task generalization and performance improvement.

**Result:** Enhanced uniformity in performance across all contextual positions was achieved, along with strong generalization across tasks.

**Limitations:** 

**Conclusion:** Pos2Distill can effectively reduce performance gaps caused by positional bias in long-context tasks, leading to superior outcomes in retrieval and reasoning.

**Abstract:** Positional bias (PB), manifesting as non-uniform sensitivity across different contextual locations, significantly impairs long-context comprehension and processing capabilities. While prior work seeks to mitigate PB through modifying the architectures causing its emergence, significant PB still persists. To address PB effectively, we introduce \textbf{Pos2Distill}, a position to position knowledge distillation framework. Pos2Distill transfers the superior capabilities from advantageous positions to less favorable ones, thereby reducing the huge performance gaps. The conceptual principle is to leverage the inherent, position-induced disparity to counteract the PB itself. We identify distinct manifestations of PB under \textbf{\textsc{r}}etrieval and \textbf{\textsc{r}}easoning paradigms, thereby designing two specialized instantiations: \emph{Pos2Distill-R\textsuperscript{1}} and \emph{Pos2Distill-R\textsuperscript{2}} respectively, both grounded in this core principle. By employing the Pos2Distill approach, we achieve enhanced uniformity and significant performance gains across all contextual positions in long-context retrieval and reasoning tasks. Crucially, both specialized systems exhibit strong cross-task generalization mutually, while achieving superior performance on their respective tasks.

</details>


### [62] [Stemming -- The Evolution and Current State with a Focus on Bangla](https://arxiv.org/abs/2508.15711)

*Abhijit Paul, Mashiat Amin Farin, Sharif Md. Abdullah, Ahmedul Kabir, Zarif Masud, Shebuti Rayana*

**Main category:** cs.CL

**Keywords:** Bangla, Stemming, Natural Language Processing, Morphological Analysis, Language Processing

**Relevance Score:** 4

**TL;DR:** This paper surveys stemming approaches for the Bangla language, addressing gaps in literature and suggesting directions for development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Bangla is under-represented in digital resources, necessitating effective preprocessing methods like stemming to aid language analysis.

**Method:** Comprehensive survey of existing stemming techniques for Bangla, highlighting morphologic challenges and evaluating current methodologies.

**Key Contributions:**

	1. Comprehensive survey of Bangla stemming approaches
	2. Identification of gaps in existing research
	3. Suggestions for future research directions in Bangla stemmer development

**Result:** Identifies significant gaps in existing literature and critiques evaluation methods, advocating for improved Bengali stemmers.

**Limitations:** Limited accessibility of implementations for replication and relevant performance metrics in existing works.

**Conclusion:** Robust Bangla stemmers are essential for better language analysis and further research is necessary in this area.

**Abstract:** Bangla, the seventh most widely spoken language worldwide with 300 million native speakers, faces digital under-representation due to limited resources and lack of annotated datasets. Stemming, a critical preprocessing step in language analysis, is essential for low-resource, highly-inflectional languages like Bangla, because it can reduce the complexity of algorithms and models by significantly reducing the number of words the algorithm needs to consider. This paper conducts a comprehensive survey of stemming approaches, emphasizing the importance of handling morphological variants effectively. While exploring the landscape of Bangla stemming, it becomes evident that there is a significant gap in the existing literature. The paper highlights the discontinuity from previous research and the scarcity of accessible implementations for replication. Furthermore, it critiques the evaluation methodologies, stressing the need for more relevant metrics. In the context of Bangla's rich morphology and diverse dialects, the paper acknowledges the challenges it poses. To address these challenges, the paper suggests directions for Bangla stemmer development. It concludes by advocating for robust Bangla stemmers and continued research in the field to enhance language analysis and processing.

</details>


### [63] [EcomMMMU: Strategic Utilization of Visuals for Robust Multimodal E-Commerce Models](https://arxiv.org/abs/2508.15721)

*Xinyi Ling, Hanwen Du, Zhihui Zhu, Xia Ning*

**Main category:** cs.CL

**Keywords:** e-commerce, multimodal, dataset, large language models, visual content

**Relevance Score:** 7

**TL;DR:** Introducing EcomMMMU, a dataset for multimodal product understanding in e-commerce, revealing that images may not always enhance performance in MLLM tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically examine whether product images on e-commerce platforms enhance understanding or degrade performance, given the limitations of existing datasets.

**Method:** Introduction of EcomMMMU, with 406,190 samples and nearly 9 million images, designed for multimodal language tasks. Analysis and benchmarking of multimodal large language models' capabilities.

**Key Contributions:**

	1. Introduction of EcomMMMU dataset with extensive multimodal samples
	2. Analysis shows images can degrade MLLM performance
	3. Development of SUMEI for better usage of visual content

**Result:** Product images do not consistently improve performance and can sometimes degrade it; MLLMs may struggle with effectively leveraging visual content for specific tasks.

**Limitations:** Only focusing on production images and their role in performance; other potential factors affecting performance are not addressed.

**Conclusion:** Proposing SUMEI, a method to predict visual utilities of images before downstream task utilization shows effectiveness and robustness in enhancing multimodal understanding.

**Abstract:** E-commerce platforms are rich in multimodal data, featuring a variety of images that depict product details. However, this raises an important question: do these images always enhance product understanding, or can they sometimes introduce redundancy or degrade performance? Existing datasets are limited in both scale and design, making it difficult to systematically examine this question. To this end, we introduce EcomMMMU, an e-commerce multimodal multitask understanding dataset with 406,190 samples and 8,989,510 images. EcomMMMU is comprised of multi-image visual-language data designed with 8 essential tasks and a specialized VSS subset to benchmark the capability of multimodal large language models (MLLMs) to effectively utilize visual content. Analysis on EcomMMMU reveals that product images do not consistently improve performance and can, in some cases, degrade it. This indicates that MLLMs may struggle to effectively leverage rich visual content for e-commerce tasks. Building on these insights, we propose SUMEI, a data-driven method that strategically utilizes multiple images via predicting visual utilities before using them for downstream tasks. Comprehensive experiments demonstrate the effectiveness and robustness of SUMEI. The data and code are available through https://anonymous.4open.science/r/submission25.

</details>


### [64] [End-to-End Agentic RAG System Training for Traceable Diagnostic Reasoning](https://arxiv.org/abs/2508.15746)

*Qiaoyu Zheng, Yuze Sun, Chaoyi Wu, Weike Zhao, Pengcheng Qiu, Yongguo Yu, Kun Sun, Yanfeng Wang, Ya Zhang, Weidi Xie*

**Main category:** cs.CL

**Keywords:** medical diagnosis, large language models, reinforcement learning, retrieval-augmented reasoning, health informatics

**Relevance Score:** 9

**TL;DR:** Deep-DxSearch is an end-to-end RAG system using reinforcement learning to improve medical diagnosis accuracy by integrating a large-scale medical retrieval corpus with large language models.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations in accurate medical diagnosis due to knowledge gaps and hallucinations in medical LLMs, aiming to enhance retrieval-augmented reasoning.

**Method:** An end-to-end agentic RAG system was constructed, framing the LLM as the core agent interacting with a medical retrieval corpus, with reinforcement learning employed for training against tailored rewards focusing on diagnostic accuracy and reasoning quality.

**Key Contributions:**

	1. Introduction of Deep-DxSearch, an agentic RAG system for medical diagnosis
	2. Use of reinforcement learning for improving retrieval-augmented reasoning
	3. Demonstrated significant improvements in diagnostic accuracy over existing baselines.

**Result:** Deep-DxSearch consistently outperformed existing models like GPT-4o and DeepSeek-R1 in diagnostic accuracy across various scenarios, confirming the effectiveness of the proposed approach.

**Limitations:** The research is limited to the specific data centers utilized, and further exploration might be needed on diverse datasets for broader applicability.

**Conclusion:** The study concludes that Deep-DxSearch's unique reinforcement learning framework and effective retrieval corpus significantly enhance diagnostic reasoning in medical applications, providing insights for clinicians.

**Abstract:** Accurate diagnosis with medical large language models is hindered by knowledge gaps and hallucinations. Retrieval and tool-augmented methods help, but their impact is limited by weak use of external knowledge and poor feedback-reasoning traceability. To address these challenges, We introduce Deep-DxSearch, an agentic RAG system trained end-to-end with reinforcement learning (RL) that enables steer tracebale retrieval-augmented reasoning for medical diagnosis. In Deep-DxSearch, we first construct a large-scale medical retrieval corpus comprising patient records and reliable medical knowledge sources to support retrieval-aware reasoning across diagnostic scenarios. More crutially, we frame the LLM as the core agent and the retrieval corpus as its environment, using tailored rewards on format, retrieval, reasoning structure, and diagnostic accuracy, thereby evolving the agentic RAG policy from large-scale data through RL.   Experiments demonstrate that our end-to-end agentic RL training framework consistently outperforms prompt-engineering and training-free RAG approaches across multiple data centers. After training, Deep-DxSearch achieves substantial gains in diagnostic accuracy, surpassing strong diagnostic baselines such as GPT-4o, DeepSeek-R1, and other medical-specific frameworks for both common and rare disease diagnosis under in-distribution and out-of-distribution settings. Moreover, ablation studies on reward design and retrieval corpus components confirm their critical roles, underscoring the uniqueness and effectiveness of our approach compared with traditional implementations. Finally, case studies and interpretability analyses highlight improvements in Deep-DxSearch's diagnostic policy, providing deeper insight into its performance gains and supporting clinicians in delivering more reliable and precise preliminary diagnoses. See https://github.com/MAGIC-AI4Med/Deep-DxSearch.

</details>


### [65] [Dissecting Tool-Integrated Reasoning: An Empirical Study and Analysis](https://arxiv.org/abs/2508.15754)

*Yufeng Zhao, Junnan Liu, Hongwei Liu, Dongsheng Zhu, Yuan Shen, Songyang Zhang, Kai Chen*

**Main category:** cs.CL

**Keywords:** Tool-Integrated Reasoning, Large Language Models, Reasoning Efficiency

**Relevance Score:** 9

**TL;DR:** Tool-Integrated Reasoning (TIR) improves Large Language Models' (LLMs) reasoning in mathematical and non-mathematical tasks while enhancing efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effects of Tool-Integrated Reasoning (TIR) on the reasoning capabilities of LLMs, particularly in tasks requiring precision.

**Method:** Introduced ReasonZoo, a benchmark with nine reasoning categories, and proposed new metrics (PAC and AUC-PCC) for assessing reasoning efficiency.

**Key Contributions:**

	1. Introduction of the ReasonZoo benchmark with nine reasoning categories
	2. Development of two novel metrics: Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC)
	3. Demonstration of TIR's impact on improving LLM performance and efficiency in reasoning tasks.

**Result:** TIR-enabled models consistently outperformed non-TIR models in various reasoning tasks, demonstrating improvements in both performance and efficiency metrics.

**Limitations:** 

**Conclusion:** TIR represents a domain-general enhancement for LLMs, potentially advancing capabilities in complex reasoning tasks.

**Abstract:** Large Language Models (LLMs) have made significant strides in reasoning tasks through methods like chain-of-thought (CoT) reasoning. However, they often fall short in tasks requiring precise computations. Tool-Integrated Reasoning (TIR) has emerged as a solution by incorporating external tools into the reasoning process. Nevertheless, the generalization of TIR in improving the reasoning ability of LLM is still unclear. Additionally, whether TIR has improved the model's reasoning behavior and helped the model think remains to be studied. We introduce ReasonZoo, a comprehensive benchmark encompassing nine diverse reasoning categories, to evaluate the effectiveness of TIR across various domains. Additionally, we propose two novel metrics, Performance-Aware Cost (PAC) and Area Under the Performance-Cost Curve (AUC-PCC), to assess reasoning efficiency. Our empirical evaluation demonstrates that TIR-enabled models consistently outperform their non-TIR counterparts in both mathematical and non-mathematical tasks. Furthermore, TIR enhances reasoning efficiency, as evidenced by improved PAC and AUC-PCC, indicating reduced overthinking and more streamlined reasoning. These findings underscore the domain-general benefits of TIR and its potential to advance LLM capabilities in complex reasoning tasks.

</details>


### [66] [LiveMCP-101: Stress Testing and Diagnosing MCP-enabled Agents on Challenging Queries](https://arxiv.org/abs/2508.15760)

*Ming Yin, Dinghan Shen, Silei Xu, Jianbing Han, Sixun Dong, Mian Zhang, Yebowen Hu, Shujian Liu, Simin Ma, Song Wang, Sathish Reddy Indurthi, Xun Wang, Yiran Chen, Kaiqiang Song*

**Main category:** cs.CL

**Keywords:** AI agents, tool integration, benchmarking, Model Context Protocol, LiveMCP-101

**Relevance Score:** 6

**TL;DR:** LiveMCP-101 is a benchmark for assessing AI agents' ability to use multiple tools for complex tasks, revealing significant challenges in execution success rates and tool orchestration.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in benchmarking AI agents' effectiveness in solving multi-step tasks using diverse tools under the Model Context Protocol (MCP).

**Method:** Introduced LiveMCP-101, a benchmark of 101 real-world queries that require coordinated use of multiple MCP tools and a novel evaluation approach using ground-truth execution plans.

**Key Contributions:**

	1. Development of LiveMCP-101 benchmark for AI agent evaluation
	2. Introduction of ground-truth execution plans for better assessment
	3. Identification of distinct failure modes in current models.

**Result:** Even advanced LLMs showed below 60% success rates in coordinating tool use, indicating major challenges in execution and tool orchestration.

**Limitations:** 

**Conclusion:** LiveMCP-101 sets a new standard for evaluating AI agents' capabilities in realistic scenarios, highlighting the need for advancements in model performance and tool orchestration.

**Abstract:** Tool calling has emerged as a critical capability for AI agents to interact with the real world and solve complex tasks. While the Model Context Protocol (MCP) provides a powerful standardized framework for tool integration, there is a significant gap in benchmarking how well AI agents can effectively solve multi-step tasks using diverse MCP tools in realistic, dynamic scenarios. In this work, we present LiveMCP-101, a benchmark of 101 carefully curated real-world queries, refined through iterative LLM rewriting and manual review, that require coordinated use of multiple MCP tools including web search, file operations, mathematical reasoning, and data analysis. Moreover, we introduce a novel evaluation approach that leverages ground-truth execution plans rather than raw API outputs, better reflecting the evolving nature of real-world environments. Experiments show that even frontier LLMs achieve a success rate below 60\%, highlighting major challenges in tool orchestration. Detailed ablations and error analysis further reveal distinct failure modes and inefficiencies in token usage, pointing to concrete directions for advancing current models. LiveMCP-101 sets a rigorous standard for evaluating real-world agent capabilities, advancing toward autonomous AI systems that reliably execute complex tasks through tool use.

</details>


### [67] [Unplug and Play Language Models: Decomposing Experts in Language Models at Inference Time](https://arxiv.org/abs/2404.11916)

*Nakyeong Yang, Jiwon Moon, Junseok Kim, Yunah Jang, Kyomin Jung*

**Main category:** cs.CL

**Keywords:** Decomposition of Experts, task-specific models, inference optimization, language models, natural language understanding

**Relevance Score:** 9

**TL;DR:** This paper introduces Decomposition of Experts (DoE), a framework for dynamically activating task-specific neurons in language models to improve inference efficiency without sacrificing accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies of large pre-trained language models that use all their parameters for every task, leading to increased inference costs.

**Method:** The DoE framework includes a four-step process: receiving a user request, identifying the task expert, performing inference with the expert-localized model, and restoring the original model.

**Key Contributions:**

	1. Introduction of the DoE framework for task-specific expert activation.
	2. Demonstrated inference speed-up and pruning efficiency without accuracy loss.
	3. Practical insights for optimizing inference with varying batch sizes and configurations.

**Result:** DoE achieved up to 1.73x speed-up in inference with a 65% pruning rate, without losing accuracy compared to traditional methods.

**Limitations:** 

**Conclusion:** DoE provides a practical and scalable solution for efficient task-specific inference in transformer-based architectures.

**Abstract:** Enabled by large-scale text corpora with huge parameters, pre-trained language models operate as multi-task experts using a single model architecture. However, recent studies have revealed that certain neurons play disproportionately important roles in solving specific tasks, suggesting that task-relevant substructures can be isolated and selectively activated for each task. Therefore, we introduce Decomposition of Experts (DoE), a novel framework that dynamically identifies and activates task-specific experts within a language model to reduce inference cost without sacrificing accuracy. We first define a task expert as a set of parameters that significantly influence the performance of a specific task and propose a four-step unplug-and-play process: (1) receiving a user request, (2) identifying the corresponding task expert, (3) performing inference using the expert-localized model, and (4) restoring the original model and waiting for the next task. Using attribution methods and prompt tuning, DoE isolates task-relevant neurons, minimizing computational overhead while maintaining task performance. We assume a setting where a language model receives user requests from five widely used natural language understanding benchmarks, processing one task at a time. In this setup, we demonstrate that DoE achieves up to a x1.73 inference speed-up with a 65% pruning rate, without compromising accuracy. Comparisons with various task expert localization methods reveal that DoE effectively identifies task experts, while ablation studies validate the importance of its components. Additionally, we analyze the effects of batch size, token count, and layer types on inference speed-up, providing practical insights for adopting DoE. The proposed framework is both practical and scalable, applicable to any transformer-based architecture, offering a robust solution for efficient task-specific inference.

</details>


### [68] [On the Role of Entity and Event Level Conceptualization in Generalizable Reasoning: A Survey of Tasks, Methods, Applications, and Future Directions](https://arxiv.org/abs/2406.10885)

*Weiqi Wang, Tianqing Fang, Haochen Shi, Baixuan Xu, Wenxuan Ding, Liyu Zhang, Wei Fan, Jiaxin Bai, Haoran Li, Xin Liu, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** Conceptualization, Cognition, Reasoning, Taxonomy, Survey

**Relevance Score:** 4

**TL;DR:** This paper proposes a categorized taxonomy of conceptualization types and presents a comprehensive survey of existing research on conceptualization in reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inconsistencies in understanding conceptualization and its significance in enhancing reasoning tasks, given its broad nature and varied definitions.

**Method:** The paper categorizes different types of conceptualizations into four levels based on instance types and surveys over 150 papers to unify definitions, resources, methods, and applications in a taxonomy.

**Key Contributions:**

	1. Categorization of conceptualization into four levels
	2. First comprehensive survey of over 150 papers
	3. Unified taxonomy of definitions and applications related to conceptualization.

**Result:** The survey identifies various definitions and applications of conceptualization, focusing particularly on entity and event levels, and proposes a framework for future research in the field.

**Limitations:** The paper does not provide new empirical results but rather focuses on synthesizing existing literature.

**Conclusion:** The paper concludes with a call for more attention and research on conceptualization in reasoning tasks, providing a comprehensive overview that can guide future exploration.

**Abstract:** Conceptualization, a fundamental element of human cognition, plays a pivotal role in human generalizable reasoning. Generally speaking, it refers to the process of sequentially abstracting specific instances into higher-level concepts and then forming abstract knowledge that can be applied in unfamiliar or novel situations. This enhances models' inferential capabilities and supports the effective transfer of knowledge across various domains. Despite its significance, the broad nature of this term has led to inconsistencies in understanding conceptualization across various works, as there exists different types of instances that can be abstracted in a wide variety of ways. There is also a lack of a systematic overview that comprehensively examines existing works on the definition, execution, and application of conceptualization to enhance reasoning tasks. In this paper, we address these gaps by first proposing a categorization of different types of conceptualizations into four levels based on the types of instances being conceptualized, in order to clarify the term and define the scope of our work. Then, we present the first comprehensive survey of over 150 papers, surveying various definitions, resources, methods, and downstream applications related to conceptualization into a unified taxonomy, with a focus on the entity and event levels. Furthermore, we shed light on potential future directions in this field and hope to garner more attention from the community.

</details>


### [69] [Teuken-7B-Base & Teuken-7B-Instruct: Towards European LLMs](https://arxiv.org/abs/2410.03730)

*Mehdi Ali, Michael Fromm, Klaudia Thellmann, Jan Ebert, Alexander Arno Weber, Richard Rutmann, Charvi Jain, Max Lübbering, Daniel Steinigen, Johannes Leveling, Katrin Klug, Jasper Schulze Buschhoff, Lena Jurkschat, Hammam Abdelwahab, Benny Jörg Stein, Karl-Heinz Sylla, Pavel Denisov, Nicolo' Brandizzi, Qasid Saleem, Anirban Bhowmick, Lennard Helmer, Chelsea John, Pedro Ortiz Suarez, Malte Ostendorff, Alex Jude, Lalith Manjunath, Samuel Weinbach, Carolin Penke, Oleg Filatov, Fabio Barth, Paramita Mirza, Lucas Weber, Ines Wendler, Rafet Sifa, Fabian Küch, Andreas Herten, René Jäkel, Georg Rehm, Stefan Kesselheim, Joachim Köhler, Nicolas Flores-Herr*

**Main category:** cs.CL

**Keywords:** multilingual, LLM, European languages, tokenization, machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed for all 24 EU languages to enhance linguistic diversity.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the predominance of English in existing LLMs and better support the linguistic diversity of Europe.

**Method:** The models were trained on a dataset with 60% non-English data using a custom multilingual tokenizer, focusing on data composition and training methodologies.

**Key Contributions:**

	1. Introduction of two multilingual LLMs supporting all EU languages
	2. Custom multilingual tokenizer for better performance
	3. Strong results on standardized multilingual benchmarks

**Result:** The models show strong performance on multilingual benchmarks including ARC, HellaSwag, and TruthfulQA.

**Limitations:** 

**Conclusion:** Teuken LLMs effectively cater to Europe's linguistic needs, outperforming existing models in multilingual tasks.

**Abstract:** We present two multilingual LLMs, Teuken 7B-base and Teuken 7B-instruct, designed to embrace Europe's linguistic diversity by supporting all 24 official languages of the European Union. Trained on a dataset comprising around 60% non-English data and utilizing a custom multilingual tokenizer, our models address the limitations of existing LLMs that predominantly focus on English or a few high-resource languages. We detail the models' development principles, i.e., data composition, tokenizer optimization, and training methodologies. The models demonstrate strong performance across multilingual benchmarks, as evidenced by their performance on European versions of ARC, HellaSwag, and TruthfulQA.

</details>


### [70] [Fine-tuning foundational models to code diagnoses from veterinary health records](https://arxiv.org/abs/2410.15186)

*Mayla R. Boguslav, Adam Kiehl, David Kott, G. Joseph Strecker, Tracy Webb, Nadia Saklou, Terri Ward, Michael Kirby*

**Main category:** cs.CL

**Keywords:** veterinary, NLP, clinical coding, SNOMED-CT, electronic health records

**Relevance Score:** 4

**TL;DR:** This study enhances the coding of veterinary medical records using NLP techniques and pre-trained language models to improve interoperability and quality of veterinary EHRs.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address interoperability challenges in veterinary medical records that limit data usability for clinical research.

**Method:** Fine-tuning of 13 pre-trained language models on a large dataset of veterinary patient visits to automate coding of SNOMED-CT diagnosis codes.

**Key Contributions:**

	1. Incorporation of all SNOMED-CT diagnosis codes by CSU VTH
	2. Use of 13 pre-trained language models for fine-tuning
	3. Demonstrated methods for automated coding with both large and limited data resources.

**Result:** The fine-tuned models achieved superior performance in coding accuracy, even with limited resources, compared to previous models.

**Limitations:** The study is region-specific to CSU and may not generalize to all veterinary settings.

**Conclusion:** The study advances the automation of coding in veterinary EHRs, which can benefit animal and human health research through better integration of health data.

**Abstract:** Veterinary medical records represent a large data resource for application to veterinary and One Health clinical research efforts. Use of the data is limited by interoperability challenges including inconsistent data formats and data siloing. Clinical coding using standardized medical terminologies enhances the quality of medical records and facilitates their interoperability with veterinary and human health records from other sites. Previous studies, such as DeepTag and VetTag, evaluated the application of Natural Language Processing (NLP) to automate veterinary diagnosis coding, employing long short-term memory (LSTM) and transformer models to infer a subset of Systemized Nomenclature of Medicine - Clinical Terms (SNOMED-CT) diagnosis codes from free-text clinical notes. This study expands on these efforts by incorporating all 7,739 distinct SNOMED-CT diagnosis codes recognized by the Colorado State University (CSU) Veterinary Teaching Hospital (VTH) and by leveraging the increasing availability of pre-trained language models (LMs). 13 freely-available pre-trained LMs were fine-tuned on the free-text notes from 246,473 manually-coded veterinary patient visits included in the CSU VTH's electronic health records (EHRs), which resulted in superior performance relative to previous efforts. The most accurate results were obtained when expansive labeled data were used to fine-tune relatively large clinical LMs, but the study also showed that comparable results can be obtained using more limited resources and non-clinical LMs. The results of this study contribute to the improvement of the quality of veterinary EHRs by investigating accessible methods for automated coding and support both animal and human health research by paving the way for more integrated and comprehensive health databases that span species and institutions.

</details>


### [71] [Rethinking Addressing in Language Models via Contexualized Equivariant Positional Encoding](https://arxiv.org/abs/2501.00712)

*Jiajun Zhu, Peihao Wang, Ruisi Cai, Jason D. Lee, Pan Li, Zhangyang Wang*

**Main category:** cs.CL

**Keywords:** positional encoding, transformers, context-aware, long-context, language modeling

**Relevance Score:** 8

**TL;DR:** TAPE is a novel framework for enhancing positional embeddings in transformers by incorporating sequence content across layers, enabling dynamic and context-aware positional encodings that improve reasoning and long-context capability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing positional encoding methods in transformers limit modeling flexibility and long-range dependency relationships due to their rigid patterns and lack of adaptability to specific tasks.

**Method:** TAPE introduces contextualized equivariant position encoding that allows for dynamic, context-aware adjustments to positional encodings across transformer layers, ensuring stability during updates and improving the model's long-context capabilities.

**Key Contributions:**

	1. Introduction of dynamic, context-aware positional encodings
	2. Provable enhancement of LLM reasoning abilities
	3. Effective integration into existing transformer architectures with minimal overhead

**Result:** TAPE outperforms traditional positional encoding methods in several tasks, including language modeling, arithmetic reasoning, and long-context retrieval, as demonstrated through extensive experimental evaluations.

**Limitations:** 

**Conclusion:** TAPE provides a parameter-efficient mechanism to enhance pre-trained transformers, facilitating better performance in a range of tasks requiring long-context understanding and reasoning abilities without significant overhead.

**Abstract:** Transformers rely on both content-based and position-based addressing mechanisms to make predictions, but existing positional encoding techniques often diminish the effectiveness of position-based addressing. Many current methods enforce rigid patterns in attention maps, limiting the ability to model long-range dependencies and adapt to diverse tasks. Additionally, most positional encodings are learned as general biases, lacking the specialization required for different instances within a dataset. To address this, we propose con\textbf{T}extualized equivari\textbf{A}nt \textbf{P}osition \textbf{E}ncoding (\textbf{TAPE}), a novel framework that enhances positional embeddings by incorporating sequence content across layers. TAPE introduces dynamic, context-aware positional encodings, overcoming the constraints of traditional fixed patterns. We show that TAPE can provably facilitate LLM reasoning ability by emulating a broader class of algorithms. By enforcing permutation and orthogonal equivariance, TAPE ensures the stability of positional encodings during updates, improving long-context ability. Our method can be easily integrated into pre-trained transformers, offering parameter-efficient fine-tuning with minimal overhead. Extensive experiments show that TAPE achieves superior performance in language modeling, arithmetic reasoning, and long-context retrieval tasks compared to existing positional embedding techniques. Code is available at https://github.com/VITA-Group/TAPE.

</details>


### [72] [Everybody Likes to Sleep: A Computer-Assisted Comparison of Object Naming Data from 30 Languages](https://arxiv.org/abs/2501.08312)

*Alžběta Kučerová, Johann-Mattis List*

**Main category:** cs.CL

**Keywords:** object naming, cross-linguistic study, cognitive linguistics, dataset transparency, language and vision

**Relevance Score:** 4

**TL;DR:** The study enhances transparency in object naming datasets by linking individual items to unified concepts across multiple languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding of how humans access and select names for objects and to enhance the quality of object naming datasets.

**Method:** A multilingual, computer-assisted approach that links 17 object naming datasets covering 30 languages from 10 language families to unified concepts.

**Key Contributions:**

	1. Links multiple object naming datasets across languages for better comparability
	2. Enhances understanding of cognitive processes in object naming
	3. Provides a basis for future cross-linguistic naming research

**Result:** The comparative dataset allows exploration of recurring concepts and comparisons with classical vocabulary lists, showing various conceptual spaces.

**Limitations:** 

**Conclusion:** The findings offer a foundation for advancing cross-linguistic research in object naming and provide guidelines for future studies.

**Abstract:** Object naming - the act of identifying an object with a word or a phrase - is a fundamental skill in interpersonal communication, relevant to many disciplines, such as psycholinguistics, cognitive linguistics, or language and vision research. Object naming datasets, which consist of concept lists with picture pairings, are used to gain insights into how humans access and select names for objects in their surroundings and to study the cognitive processes involved in converting visual stimuli into semantic concepts. Unfortunately, object naming datasets often lack transparency and have a highly idiosyncratic structure. Our study tries to make current object naming data transparent and comparable by using a multilingual, computer-assisted approach that links individual items of object naming lists to unified concepts. Our current sample links 17 object naming datasets that cover 30 languages from 10 different language families. We illustrate how the comparative dataset can be explored by searching for concepts that recur across the majority of datasets and comparing the conceptual spaces of covered object naming datasets with classical basic vocabulary lists from historical linguistics and linguistic typology. Our findings can serve as a basis for enhancing cross-linguistic object naming research and as a guideline for future studies dealing with object naming tasks.

</details>


### [73] [Self-Supervised Prompt Optimization](https://arxiv.org/abs/2502.06855)

*Jinyu Xiang, Jiayi Zhang, Zhaoyang Yu, Xinbing Liang, Fengwei Teng, Jinhao Tu, Fashen Ren, Xiangru Tang, Sirui Hong, Chenglin Wu, Yuyu Luo*

**Main category:** cs.CL

**Keywords:** Prompt Optimization, Large Language Models, Self-Supervised Learning

**Relevance Score:** 8

**TL;DR:** This paper introduces Self-Supervised Prompt Optimization (SPO), a framework for optimizing prompts for large language models (LLMs) without external references, achieving cost-efficient and effective results.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective prompts in LLMs that do not require costly external references or human input in real-world scenarios.

**Method:** SPO employs pairwise output comparisons to evaluate and select effective prompts, leveraging LLM assessments to align outputs with task requirements.

**Key Contributions:**

	1. Introduction of Self-Supervised Prompt Optimization (SPO)
	2. Cost-efficient prompt optimization framework
	3. Demonstrated superior performance versus existing methods

**Result:** SPO demonstrates superior performance compared to state-of-the-art methods with significantly lower costs and fewer required samples in extensive experiments.

**Limitations:** 

**Conclusion:** SPO provides a novel approach to prompt optimization, making it feasible for diverse tasks without reliance on external data.

**Abstract:** Well-designed prompts are crucial for enhancing Large language models' (LLMs) reasoning capabilities while aligning their outputs with task requirements across diverse domains. However, manually designed prompts require expertise and iterative experimentation. While existing prompt optimization methods aim to automate this process, they rely heavily on external references such as ground truth or by humans, limiting their applicability in real-world scenarios where such data is unavailable or costly to obtain. To address this, we propose Self-Supervised Prompt Optimization (SPO), a cost-efficient framework that discovers effective prompts for both closed and open-ended tasks without requiring external reference. Motivated by the observations that prompt quality manifests directly in LLM outputs and LLMs can effectively assess adherence to task requirements, we derive evaluation and optimization signals purely from output comparisons. Specifically, SPO selects superior prompts through pairwise output comparisons evaluated by an LLM evaluator, followed by an LLM optimizer that aligns outputs with task requirements. Extensive experiments demonstrate that SPO outperforms state-of-the-art prompt optimization methods, achieving comparable or superior results with significantly lower costs (e.g., 1.1% to 5.6% of existing methods) and fewer samples (e.g., three samples). The code is available at https://github.com/FoundationAgents/SPO.

</details>


### [74] [RefineCoder: Iterative Improving of Large Language Models via Adaptive Critique Refinement for Code Generation](https://arxiv.org/abs/2502.09183)

*Changzhi Zhou, Xinyu Zhang, Dandan Song, Xiancai Chen, Wanli Gu, Huipeng Ma, Yuhang Tian, Mengdi Zhang, Linmei Hu*

**Main category:** cs.CL

**Keywords:** Code Generation, Large Language Models, Self-Refinement, Machine Learning, Adaptive Critique

**Relevance Score:** 8

**TL;DR:** This paper introduces Adaptive Critique Refinement (ACR) to enhance code generation using self-generated code and external critiques, leading to improved performance in code generation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Code generation has become increasingly relevant due to Large Language Models (LLMs), yet existing methods focus on teacher model distillation without harnessing the potential of self-refinement.

**Method:** The paper presents ACR, which utilizes a composite scoring system with LLM-as-a-Judge for quality evaluation and a selective critique strategy with LLM-as-a-Critic to improve low-quality code responses.

**Key Contributions:**

	1. Introduction of Adaptive Critique Refinement (ACR) for code generation
	2. Development of the RefineCoder series leveraging self-refinement
	3. Demonstrated performance improvement with lower data requirements

**Result:** The RefineCoder series, developed through iterative application of ACR, shows continuous performance gains across multiple code generation benchmarks, performing comparably or better than baselines with less data.

**Limitations:** 

**Conclusion:** ACR represents a novel approach by enabling self-refinement in code generation, potentially exceeding performance expectations while minimizing data needs.

**Abstract:** Code generation has attracted increasing attention with the rise of Large Language Models (LLMs). Many studies have developed powerful code LLMs by synthesizing code-related instruction data and applying supervised fine-tuning. However, these methods are limited by teacher model distillation and ignore the potential of iterative refinement by self-generated code. In this paper, we propose Adaptive Critique Refinement (ACR), which enables the model to refine itself by self-generated code and external critique, rather than directly imitating the code responses of the teacher model. Concretely, ACR includes a composite scoring system with LLM-as-a-Judge to evaluate the quality of code responses and a selective critique strategy with LLM-as-a-Critic to critique self-generated low-quality code responses. We develop the RefineCoder series by iteratively applying ACR, achieving continuous performance improvement on multiple code generation benchmarks. Compared to the baselines of the same size, our proposed RefineCoder series can achieve comparable or even superior performance using less data.

</details>


### [75] [Ontology-Guided Reverse Thinking Makes Large Language Models Stronger on Knowledge Graph Question Answering](https://arxiv.org/abs/2502.11491)

*Runxuan Liu, Bei Luo, Jiaqi Li, Baoxin Wang, Ming Liu, Dayong Wu, Shijin Wang, Bing Qin*

**Main category:** cs.CL

**Keywords:** large language models, knowledge graph, question answering, multi-hop reasoning, reverse thinking

**Relevance Score:** 9

**TL;DR:** The paper presents Ontology-Guided Reverse Thinking (ORT), a framework to improve multi-hop reasoning in knowledge graph question answering using large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of abstract question matching and reasoning paths in knowledge graph question answering that lead to information loss.

**Method:** ORT uses a three-phase approach: (1) extracts purpose and condition labels with LLM, (2) constructs reasoning paths based on the KG ontology, and (3) guides knowledge retrieval using these paths.

**Key Contributions:**

	1. Introduction of Ontology-Guided Reverse Thinking (ORT) framework
	2. Novel approach to construct reasoning paths from purposes to conditions
	3. Demonstration of state-of-the-art performance on standard datasets.

**Result:** Experiments demonstrate that ORT achieves state-of-the-art performance on the WebQSP and CWQ datasets, significantly improving LLM capability in KGQA.

**Limitations:** 

**Conclusion:** The proposed ORT framework effectively enhances multi-hop reasoning in knowledge graph question answering scenarios.

**Abstract:** Large language models (LLMs) have shown remarkable capabilities in natural language processing. However, in knowledge graph question answering tasks (KGQA), there remains the issue of answering questions that require multi-hop reasoning. Existing methods rely on entity vector matching, but the purpose of the question is abstract and difficult to match with specific entities. As a result, it is difficult to establish reasoning paths to the purpose, which leads to information loss and redundancy. To address this issue, inspired by human reverse thinking, we propose Ontology-Guided Reverse Thinking (ORT), a novel framework that constructs reasoning paths from purposes back to conditions. ORT operates in three key phases: (1) using LLM to extract purpose labels and condition labels, (2) constructing label reasoning paths based on the KG ontology, and (3) using the label reasoning paths to guide knowledge retrieval. Experiments on the WebQSP and CWQ datasets show that ORT achieves state-of-the-art performance and significantly enhances the capability of LLMs for KGQA.

</details>


### [76] [Pub-Guard-LLM: Detecting Retracted Biomedical Articles with Reliable Explanations](https://arxiv.org/abs/2502.15429)

*Lihu Chen, Shuojie Fu, Gabriel Freedman, Cemre Zor, Guy Martin, James Kinross, Uddhav Vaghela, Ovidiu Serban, Francesca Toni*

**Main category:** cs.CL

**Keywords:** fraud detection, biomedical, large language model, Pub-Guard-LLM, research integrity

**Relevance Score:** 7

**TL;DR:** Pub-Guard-LLM is a large language model-based system designed to detect fraud in biomedical scientific articles, offering multiple deployment modes and enhanced performance over baselines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of fraudulent practices in published scientific articles threatens the credibility of research, especially in medicine.

**Method:** Pub-Guard-LLM employs three application modes: vanilla reasoning, retrieval-augmented generation, and multi-agent debate, each capable of providing textual explanations for its predictions.

**Key Contributions:**

	1. Introduction of a large language model for biomedical fraud detection
	2. Development of multiple application modes for enhanced usability
	3. Creation of the PubMed Retraction benchmark for evaluating model performance

**Result:** Pub-Guard-LLM outperforms various baseline models in predicting fraudulent articles and provides more coherent and relevant explanations based on an open-source benchmark of over 11K biomedical articles.

**Limitations:** 

**Conclusion:** By improving detection performance and explanation quality, Pub-Guard-LLM serves as an effective tool for safeguarding research integrity in the biomedical field.

**Abstract:** A significant and growing number of published scientific articles is found to involve fraudulent practices, posing a serious threat to the credibility and safety of research in fields such as medicine. We propose Pub-Guard-LLM, the first large language model-based system tailored to fraud detection of biomedical scientific articles. We provide three application modes for deploying Pub-Guard-LLM: vanilla reasoning, retrieval-augmented generation, and multi-agent debate. Each mode allows for textual explanations of predictions. To assess the performance of our system, we introduce an open-source benchmark, PubMed Retraction, comprising over 11K real-world biomedical articles, including metadata and retraction labels. We show that, across all modes, Pub-Guard-LLM consistently surpasses the performance of various baselines and provides more reliable explanations, namely explanations which are deemed more relevant and coherent than those generated by the baselines when evaluated by multiple assessment methods. By enhancing both detection performance and explainability in scientific fraud detection, Pub-Guard-LLM contributes to safeguarding research integrity with a novel, effective, open-source tool.

</details>


### [77] [Robust Bias Detection in MLMs and its Application to Human Trait Ratings](https://arxiv.org/abs/2502.15600)

*Ingroj Shrestha, Louis Tay, Padmini Srinivasan*

**Main category:** cs.CL

**Keywords:** bias, machine learning models, personality traits, gender bias, statistical analysis

**Relevance Score:** 8

**TL;DR:** The paper proposes a systematic statistical method to assess bias in machine learning models (MLMs) using mixed models and pseudo-perplexity weights. It finds variations in gender bias among different MLMs, correlating some findings with psychological studies on personality traits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in prior studies on bias in machine learning models that overlook random variability, assume equality among templates, and ignore bias quantification.

**Method:** Utilizes mixed models to account for random effects and pseudo-perplexity weights for sentences derived from templates to quantify bias using statistical effect sizes.

**Key Contributions:**

	1. Systematic statistical approach to assess bias in MLMs
	2. Application of mixed models for bias quantification
	3. Exploration of gender bias in personality and character traits across multiple MLMs

**Result:** The study confirms prior bias findings in MLMs, observes variability in gender bias across MLMs, and establishes some alignment between MLM bias and psychological studies on personality traits.

**Limitations:** Comparison of character traits is limited due to the scarcity of human studies on gender bias in this area.

**Conclusion:** Different MLMs exhibit varying degrees of bias, with findings that can correlate with human psychological perspectives, although character traits comparison remains limited due to a lack of studies.

**Abstract:** There has been significant prior work using templates to study bias against demographic attributes in MLMs. However, these have limitations: they overlook random variability of templates and target concepts analyzed, assume equality amongst templates, and overlook bias quantification. Addressing these, we propose a systematic statistical approach to assess bias in MLMs, using mixed models to account for random effects, pseudo-perplexity weights for sentences derived from templates and quantify bias using statistical effect sizes. Replicating prior studies, we match on bias scores in magnitude and direction with small to medium effect sizes. Next, we explore the novel problem of gender bias in the context of $\emph{personality}$ and $\textit{character}$ traits, across seven MLMs (base and large). We find that MLMs vary; ALBERT is unbiased for binary gender but the most biased for non-binary $\textit{neo}$, while RoBERTa-large is the most biased for binary gender but shows small to no bias for $\textit{neo}$. There is some alignment of MLM bias and findings in psychology (human perspective) - in $\textit{agreeableness}$ with RoBERTa-large and $\textit{emotional stability}$ with BERT-large. There is general agreement for the remaining 3 personality dimensions: both sides observe at most small differences across gender. For character traits, human studies on gender bias are limited thus comparisons are not feasible.

</details>


### [78] [Synthetic vs. Gold: The Role of LLM Generated Labels and Data in Cyberbullying Detection](https://arxiv.org/abs/2502.15860)

*Arefeh Kazemi, Sri Balaaji Natarajan Kalaivendan, Joachim Wagner, Hamza Qadeer, Kanishk Verma, Brian Davis*

**Main category:** cs.CL

**Keywords:** Cyberbullying, Large Language Models, Data Generation, BERT, Synthetic Data

**Relevance Score:** 8

**TL;DR:** The paper explores using Large Language Models to generate synthetic data for effective cyberbullying detection, addressing the gap in labeled data for children's online communication.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Cyberbullying is a significant threat to children, highlighting the need for efficient detection systems, but there is a lack of labeled data that reflects children’s communication styles.

**Method:** The paper employs Large Language Models to generate synthetic data and labels for training BERT-based classifiers, and assesses performance against authentic datasets.

**Key Contributions:**

	1. Use of Large Language Models for synthetic dataset generation
	2. Demonstrated effectiveness of synthetic data in cyberbullying classification
	3. Addressed ethical concerns related to data gathering from children

**Result:** Synthetic data allows BERT classifiers to achieve performance close to classifiers trained on authentic datasets, with accuracies of 75.8% vs. 81.5%.

**Limitations:** 

**Conclusion:** LLMs provide a scalable, ethical, and cost-effective approach for generating necessary data for cyberbullying detection systems.

**Abstract:** Cyberbullying (CB) presents a pressing threat, especially to children, underscoring the urgent need for robust detection systems to ensure online safety. While large-scale datasets on online abuse exist, there remains a significant gap in labeled data that specifically reflects the language and communication styles used by children. The acquisition of such data from vulnerable populations, such as children, is challenging due to ethical, legal and technical barriers. Moreover, the creation of these datasets relies heavily on human annotation, which not only strains resources but also raises significant concerns due to annotators exposure to harmful content. In this paper, we address these challenges by leveraging Large Language Models (LLMs) to generate synthetic data and labels. Our experiments demonstrate that synthetic data enables BERT-based CB classifiers to achieve performance close to that of those trained on fully authentic datasets (75.8% vs. 81.5% accuracy). Additionally, LLMs can effectively label authentic yet unlabeled data, allowing BERT classifiers to attain a comparable performance level (79.1% vs. 81.5% accuracy). These results highlight the potential of LLMs as a scalable, ethical, and cost-effective solution for generating data for CB detection.

</details>


### [79] [Pragmatic Inference Chain (PIC) Improving LLMs' Reasoning of Authentic Implicit Toxic Language](https://arxiv.org/abs/2503.01539)

*Xi Chen, Shuo Wang*

**Main category:** cs.CL

**Keywords:** toxic language, large language models, Pragmatic Inference Chain, implicit toxicity, ethical concerns

**Relevance Score:** 9

**TL;DR:** This study proposes the Pragmatic Inference Chain (PIC) prompting method to enhance large language models' ability to detect implicit toxic language.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing sophistication of toxic language and LLMs' ethical implications necessitate improved detection techniques that account for complex language usage.

**Method:** The study introduces the Pragmatic Inference Chain (PIC) prompting method, evaluated across several advanced LLMs including GPT-4o, to enhance reasoning in identifying implicit toxicity.

**Key Contributions:**

	1. Introduction of the Pragmatic Inference Chain (PIC) prompting method
	2. Improved detection rates of implicit toxic language in advanced LLMs
	3. Potential applications of PIC to other inference-intensive tasks.

**Result:** Using the PIC method significantly increased the success rates of multiple LLMs in identifying implicit toxic language compared to several baseline prompts.

**Limitations:** 

**Conclusion:** The PIC prompting method shows promise not only in improving toxic language detection but also in potentially benefiting other inference-intensive tasks.

**Abstract:** The rapid development of large language models (LLMs) gives rise to ethical concerns about their performance, while opening new avenues for developing toxic language detection techniques. However, LLMs' unethical output and their capability of detecting toxicity have primarily been tested on language data that do not demand complex meaning inference, such as the biased associations of 'he' with programmer and 'she' with household. Nowadays toxic language adopts a much more creative range of implicit forms, thanks to advanced censorship. In this study, we collect authentic toxic interactions that evade online censorship and that are verified by human annotators as inference-intensive. To evaluate and improve LLMs' reasoning of the authentic implicit toxic language, we propose a new prompting method, Pragmatic Inference Chain (PIC), drawn on interdisciplinary findings from cognitive science and linguistics. The PIC prompting significantly improves the success rate of GPT-4o, Llama-3.1-70B-Instruct, DeepSeek-v2.5, and DeepSeek-v3 in identifying implicit toxic language, compared to five baseline prompts, such as CoT and rule-based baselines. In addition, it also facilitates the models to produce more explicit and coherent reasoning processes, hence can potentially be generalized to other inference-intensive tasks, e.g., understanding humour and metaphors.

</details>


### [80] [Advancing the Database of Cross-Linguistic Colexifications with New Workflows and Data](https://arxiv.org/abs/2503.11377)

*Annika Tjuka, Robert Forkel, Christoph Rzymski, Johann-Mattis List*

**Main category:** cs.CL

**Keywords:** colexification, cross-linguistic analysis, lexical resources, natural language processing, linguistic databases

**Relevance Score:** 4

**TL;DR:** This paper presents an improved database for colexification studies, enhancing linguistic research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide new insights into computational models for natural language learning through an advanced database for cross-linguistic analysis.

**Method:** The paper details improvements in data handling, selection, and presentation for a database tracking words with multiple meanings across different languages.

**Key Contributions:**

	1. Introduction of an advanced database for colexification research
	2. Improved data quality and selection methodology
	3. Coverage across a wider range of language families

**Result:** The new database offers a more balanced sample from various language families and enhanced data quality, including phonetic transcription of word forms.

**Limitations:** 

**Conclusion:** The enhanced database can inspire studies linking cross-linguistic data to questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.

**Abstract:** Lexical resources are crucial for cross-linguistic analysis and can provide new insights into computational models for natural language learning. Here, we present an advanced database for comparative studies of words with multiple meanings, a phenomenon known as colexification. The new version includes improvements in the handling, selection and presentation of the data. We compare the new database with previous versions and find that our improvements provide a more balanced sample covering more language families worldwide, with enhanced data quality, given that all word forms are provided in phonetic transcription. We conclude that the new Database of Cross-Linguistic Colexifications has the potential to inspire exciting new studies that link cross-linguistic data to open questions in linguistic typology, historical linguistics, psycholinguistics, and computational linguistics.

</details>


### [81] [Leveraging Large Language Models for Explainable Activity Recognition in Smart Homes: A Critical Evaluation](https://arxiv.org/abs/2503.16622)

*Michele Fiori, Gabriele Civitarese, Priyankar Choudhary, Claudio Bettini*

**Main category:** cs.CL

**Keywords:** Explainable AI, Large Language Models, Activities of Daily Living, IoT, Sensor Data

**Relevance Score:** 9

**TL;DR:** The paper explores the integration of Large Language Models with Explainable AI for sensor-based recognition of Activities of Daily Living, aiming to enhance explanation generation and model transparency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the transparency and trust in machine learning models used in IoT systems by leveraging Explainable AI, particularly for recognizing sensor-based Activities of Daily Living.

**Method:** The paper investigates the use of LLMs in two main approaches: 1) as explainable zero-shot models for ADL recognition to minimize the need for labeled data, and 2) to automate explanation generation in data-driven XAI approaches for better recognition rates.

**Key Contributions:**

	1. Integration of LLMs with XAI for improved explanation generation.
	2. Evaluation of LLMs as zero-shot recognition models for ADLs.
	3. Identification of challenges in using LLMs for explainability in sensor data.

**Result:** The evaluation reveals potential benefits of LLMs in enhancing explanation flexibility and scalability, while also identifying challenges regarding their implementation in the context of ADL recognition.

**Limitations:** Challenges in implementing LLMs for real-time ADL recognition applications due to computational demands and integration complexity.

**Conclusion:** LLMs present promising opportunities for combining with XAI in ADL recognition, though challenges remain that need to be addressed for effective deployment.

**Abstract:** Explainable Artificial Intelligence (XAI) aims to uncover the inner reasoning of machine learning models. In IoT systems, XAI improves the transparency of models processing sensor data from multiple heterogeneous devices, ensuring end-users understand and trust their outputs. Among the many applications, XAI has also been applied to sensor-based Activities of Daily Living (ADLs) recognition in smart homes. Existing approaches highlight which sensor events are most important for each predicted activity, using simple rules to convert these events into natural language explanations for non-expert users. However, these methods produce rigid explanations lacking natural language flexibility and are not scalable. With the recent rise of Large Language Models (LLMs), it is worth exploring whether they can enhance explanation generation, considering their proven knowledge of human activities. This paper investigates potential approaches to combine XAI and LLMs for sensor-based ADL recognition. We evaluate if LLMs can be used: a) as explainable zero-shot ADL recognition models, avoiding costly labeled data collection, and b) to automate the generation of explanations for existing data-driven XAI approaches when training data is available and the goal is higher recognition rates. Our critical evaluation provides insights into the benefits and challenges of using LLMs for explainable ADL recognition.

</details>


### [82] [VerifiAgent: a Unified Verification Agent in Language Model Reasoning](https://arxiv.org/abs/2504.00406)

*Jiuzhou Han, Wray Buntine, Ehsan Shareghi*

**Main category:** cs.CL

**Keywords:** large language models, verification methods, reasoning tasks

**Relevance Score:** 8

**TL;DR:** VerifiAgent is a unified verification agent designed to enhance the reliability of large language model responses through meta-verification and adaptive tool-based verification.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability and scalability of verification methods for large language models which often produce incorrect responses.

**Method:** VerifiAgent employs a two-level verification system: meta-verification for assessing model response completeness and consistency, and adaptive verification to choose the appropriate tools for various reasoning types.

**Key Contributions:**

	1. Introduction of a unified verification agent for large language models.
	2. Meta-verification and tool-based adaptive verification integration.
	3. Significant improvements in verification performance and reasoning accuracy with reduced costs.

**Result:** VerifiAgent outperforms existing verification methods across all reasoning tasks, demonstrating improved reasoning accuracy by using feedback from verification results and showing cost efficiency in the mathematical reasoning domain.

**Limitations:** 

**Conclusion:** VerifiAgent provides a robust and efficient solution for verifying large language models, enhancing both the accuracy of reasoning tasks and the scalability of verification processes.

**Abstract:** Large language models demonstrate remarkable reasoning capabilities but often produce unreliable or incorrect responses. Existing verification methods are typically model-specific or domain-restricted, requiring significant computational resources and lacking scalability across diverse reasoning tasks. To address these limitations, we propose VerifiAgent, a unified verification agent that integrates two levels of verification: meta-verification, which assesses completeness and consistency in model responses, and tool-based adaptive verification, where VerifiAgent autonomously selects appropriate verification tools based on the reasoning type, including mathematical, logical, or commonsense reasoning. This adaptive approach ensures both efficiency and robustness across different verification scenarios. Experimental results show that VerifiAgent outperforms baseline verification methods (e.g., deductive verifier, backward verifier) among all reasoning tasks. Additionally, it can further enhance reasoning accuracy by leveraging feedback from verification results. VerifiAgent can also be effectively applied to inference scaling, achieving better results with fewer generated samples and costs compared to existing process reward models in the mathematical reasoning domain. Code is available at https://github.com/Jiuzhouh/VerifiAgent

</details>


### [83] [MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos](https://arxiv.org/abs/2504.11169)

*Laura De Grazia, Pol Pastells, Mauro Vázquez Chas, Desmond Elliott, Danae Sánchez Villegas, Mireia Farrús, Mariona Taulé*

**Main category:** cs.CL

**Keywords:** sexism detection, multimodal dataset, social media, large language models, implicit sexism

**Relevance Score:** 4

**TL;DR:** This paper presents a multimodal approach to detecting sexism in social media videos, introducing a new dataset and evaluation of machine learning models.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the amplification of sexism through social media, emphasizing the need for a comprehensive analysis of video content combining verbal, audio, and visual elements.

**Method:** We introduce MuSeD, a multimodal dataset comprising videos from platforms like TikTok and BitChute, and an innovative annotation framework for classifying sexism in content. A range of LLMs and multimodal models were evaluated for their effectiveness in detecting sexism.

**Key Contributions:**

	1. Introduction of the MuSeD dataset for multimodal sexism detection
	2. Proposed annotation framework for analyzing multimodal contributions
	3. Evaluation of various LLMs on sexism detection tasks

**Result:** The findings indicate that visual elements significantly contribute to the identification of sexist content, with models performing well on explicit sexism but struggling with implicit stereotypes.

**Limitations:** Models have difficulty with implicit sexism, as evidenced by low agreement among annotators and challenges in context sensitivity.

**Conclusion:** The research underscores the complexity of detecting implicit sexism due to its dependence on context, highlighting the need for improved model capabilities and further exploration of cultural factors.

**Abstract:** Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of $\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for analyzing the contributions of textual, vocal, and visual modalities to the classification of content as either sexist or non-sexist; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes, instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.

</details>


### [84] [Kuwain 1.5B: An Arabic SLM via Language Injection](https://arxiv.org/abs/2504.15120)

*Khalil Hennara, Sara Chrouf, Mohamed Motaism Hamed, Zeina Aldallal, Omar Hadid, Safwan AlModhayan*

**Main category:** cs.CL

**Keywords:** language model, LLM, Arabic, knowledge integration, benchmark performance

**Relevance Score:** 8

**TL;DR:** The paper presents a method for integrating new languages into large language models without losing previous knowledge.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance existing language models by adding new languages while maintaining their prior capabilities.

**Method:** The authors introduce a novel approach that incorporates a new language (Arabic) into a 1.5 billion parameter LLM trained predominantly in English, using minimal original data.

**Key Contributions:**

	1. Novel integration method for new languages in LLMs
	2. Demonstrated significant performance improvement in Arabic
	3. Cost-effective model expansion approach

**Result:** Results show an 8% average improvement in Arabic language performance across various benchmarks, while preserving the original model's knowledge.

**Limitations:** 

**Conclusion:** The proposed method offers a cost-effective way to expand language capabilities in models without extensive retraining, paving the way for efficient language model development.

**Abstract:** Enhancing existing models with new knowledge is a crucial aspect of AI development. This paper introduces a novel method for integrating a new language into a large language model (LLM). Our approach successfully incorporates a previously unseen target language into an existing LLM without compromising its prior knowledge. We trained a tiny model with 1.5 billion parameters named Kuwain by injecting the Arabic language into a small open-source model mainly trained in English. Our method demonstrates significant improvements in Arabic language performance, with an average 8% improvement across various benchmarks, while retaining the model's existing knowledge with a minimum amount of the original model's data. This offers a cost-effective alternative to training a comprehensive model in both English and Arabic. The results highlight the potential for efficient, targeted language model expansion without extensive retraining or resource-intensive processes.

</details>


### [85] [Cequel: Cost-Effective Querying of Large Language Models for Text Clustering](https://arxiv.org/abs/2504.15640)

*Hongtao Wang, Taiyan Zhang, Renchi Yang, Jianliang Xu*

**Main category:** cs.CL

**Keywords:** text clustering, large language models, constraint-based clustering, unsupervised learning, cost-effective methods

**Relevance Score:** 7

**TL;DR:** Cequel is a cost-effective framework for text clustering that minimizes LLM queries while maintaining clustering quality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Improving text clustering accuracy while managing the computational and financial costs associated with LLM queries.

**Method:** Cequel uses must-link and cannot-link constraints derived from selective querying of LLMs on informative text pairs and triplets through EdgeLLM and TriangleLLM algorithms, followed by a weighted constrained clustering algorithm.

**Key Contributions:**

	1. Introduction of EdgeLLM and TriangleLLM for efficient constraint extraction
	2. Development of a cost-effective clustering framework that leverages LLMs
	3. Demonstration of superior clustering performance on benchmark datasets under query budget constraints.

**Result:** Cequel consistently outperforms state-of-the-art methods in unsupervised text clustering while adhering to a limited query budget.

**Limitations:** 

**Conclusion:** The proposed framework, Cequel, effectively balances cost and accuracy in text clustering using LLMs.

**Abstract:** Text clustering aims to automatically partition a collection of documents into coherent groups based on their linguistic features. In the literature, this task is formulated either as metric clustering over pre-trained text embeddings or as graph clustering based on pairwise similarities derived from an oracle, e.g., a large machine learning model. Recent advances in large language models (LLMs) have significantly improved this field by providing high-quality contextualized embeddings and accurate semantic similarity estimates. However, leveraging LLMs at scale introduces substantial computational and financial costs due to the large number of required API queries or inference calls. To address this issue, we propose Cequel, a cost-effective framework that achieves accurate text clustering under a limited budget of LLM queries. At its core, Cequel constructs must-link and cannot-link constraints by selectively querying LLMs on informative text pairs or triplets, identified via our proposed algorithms, EdgeLLM and TriangleLLM. These constraints are then utilized in a weighted constrained clustering algorithm to form high-quality clusters. Specifically, EdgeLLM and TriangleLLM employ carefully designed greedy selection strategies and prompting techniques to identify and extract informative constraints efficiently. Experiments on multiple benchmark datasets demonstrate that Cequel consistently outperforms existing methods in unsupervised text clustering under the same query budget.

</details>


### [86] [NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model](https://arxiv.org/abs/2508.14444)

*NVIDIA, :, Aarti Basant, Abhijit Khairnar, Abhijit Paithankar, Abhinav Khattar, Adithya Renduchintala, Aditya Malte, Akhiad Bercovich, Akshay Hazare, Alejandra Rico, Aleksander Ficek, Alex Kondratenko, Alex Shaposhnikov, Alexander Bukharin, Ali Taghibakhshi, Amelia Barton, Ameya Sunil Mahabaleshwarkar, Amy Shen, Andrew Tao, Ann Guan, Anna Shors, Anubhav Mandarwal, Arham Mehta, Arun Venkatesan, Ashton Sharabiani, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Banghua Zhu, Barnaby Simkin, Bilal Kartal, Bita Darvish Rouhani, Bobby Chen, Boris Ginsburg, Brandon Norick, Brian Yu, Bryan Catanzaro, Charles Wang, Charlie Truong, Chetan Mungekar, Chintan Patel, Chris Alexiuk, Christian Munley, Christopher Parisien, Dan Su, Daniel Afrimi, Daniel Korzekwa, Daniel Rohrer, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Dima Rekesh, Dina Yared, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Eileen Long, Elliott Ning, Eric Chung, Erick Galinkin, Evelina Bakhturina, Gargi Prasad, Gerald Shen, Haifeng Qian, Haim Elisha, Harsh Sharma, Hayley Ross, Helen Ngo, Herman Sahota, Hexin Wang, Hoo Chang Shin, Hua Huang, Iain Cunningham, Igor Gitman, Ivan Moshkov, Jaehun Jung, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jian Zhang, Jiaqi Zeng, Jimmy Zhang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jonathan Cohen, Joseph Jennings, Julien Veron Vialard, Junkeun Yi, Jupinder Parmar, Kari Briski, Katherine Cheung, Katherine Luna, Keith Wyss, Keshav Santhanam, Kezhi Kong, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Kushan Ahmadian, Lawrence McAfee, Laya Sleiman, Leon Derczynski, Luis Vega, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Mark Cai, Markus Kliegl, Marta Stepniewska-Dziubinska, Matvei Novikov, Mehrzad Samadi, Meredith Price, Meriem Boubdir, Michael Boone, Michael Evans, Michal Bien, Michal Zawalski, Miguel Martinez, Mike Chrzanowski, Mohammad Shoeybi, Mostofa Patwary, Namit Dhameja, Nave Assaf, Negar Habibi, Nidhi Bhatia, Nikki Pope, Nima Tajbakhsh, Nirmal Kumar Juluru, Oleg Rybakov, Oleksii Hrinchuk, Oleksii Kuchaiev, Oluwatobi Olabiyi, Pablo Ribalta, Padmavathy Subramanian, Parth Chadha, Pavlo Molchanov, Peter Dykas, Peter Jin, Piotr Bialecki, Piotr Januszewski, Pradeep Thalasta, Prashant Gaikwad, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi Mahabadi, Rajen Patel, Ran El-Yaniv, Ranjit Rajan, Ria Cheruvu, Rima Shahbazyan, Ritika Borkar, Ritu Gala, Roger Waleffe, Ruoxi Zhang, Russell J. Hewett, Ryan Prenger, Sahil Jain, Samuel Kriman, Sanjeev Satheesh, Saori Kaji, Sarah Yurick, Saurav Muralidharan, Sean Narenthiran, Seonmyeong Bak, Sepehr Sameni, Seungju Han, Shanmugam Ramasamy, Shaona Ghosh, Sharath Turuvekere Sreenivas, Shelby Thomas, Shizhe Diao, Shreya Gopal, Shrimai Prabhumoye, Shubham Toshniwal, Shuoyang Ding, Siddharth Singh, Siddhartha Jain, Somshubra Majumdar, Soumye Singhal, Stefania Alborghetti, Syeda Nahida Akter, Terry Kong, Tim Moon, Tomasz Hliwiak, Tomer Asida, Tony Wang, Tugrul Konuk, Twinkle Vashishth, Tyler Poon, Udi Karpas, Vahid Noroozi, Venkat Srinivasan, Vijay Korthikanti, Vikram Fugro, Vineeth Kalluru, Vitaly Kurin, Vitaly Lavrukhin, Wasi Uddin Ahmad, Wei Du, Wonmin Byeon, Ximing Lu, Xin Dong, Yashaswi Karnati, Yejin Choi, Yian Zhang, Ying Lin, Yonggan Fu, Yoshi Suhara, Zhen Dong, Zhiyu Li, Zhongbo Zhu, Zijia Chen*

**Main category:** cs.CL

**Keywords:** language model, Mamba-Transformer, reasoning workloads, AI applications, token inference

**Relevance Score:** 7

**TL;DR:** Introducing Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model that improves reasoning throughput and accuracy compared to similar models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the throughput for reasoning workloads and achieve state-of-the-art accuracy among similarly-sized language models.

**Method:** The model is developed by replacing self-attention layers with Mamba-2 layers and is pre-trained on 20 trillion tokens. It incorporates a Minitron strategy for compression and distillation, enabling efficient inference on up to 128k tokens.

**Key Contributions:**

	1. Introduction of Mamba-2 layers for enhanced inference speed
	2. Significant increase in throughput for reasoning tasks
	3. Availability of pre-trained datasets and model checkpoints on Hugging Face

**Result:** Nemotron-Nano-9B-v2 demonstrates on-par or improved accuracy on reasoning benchmarks compared to models like Qwen3-8B, while achieving up to 6x higher inference throughput in specific settings.

**Limitations:** 

**Conclusion:** The release of Nemotron-Nano-9B-v2 along with its base checkpoints and datasets aims to support further advancements in language model research and applications.

**Abstract:** We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face.

</details>
