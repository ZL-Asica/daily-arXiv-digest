# 2025-08-01

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 33]

- [cs.CL](#cs.CL) [Total: 79]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Knowledge Is More Than Performance: How Knowledge Diversity Drives Human-Human and Human-AI Interaction Synergy and Reveals Pure-AI Interaction Shortfalls](https://arxiv.org/abs/2507.22889)

*Tom Sheffer, Alon Miron, Yaniv Dover, Ariel Goldstein*

**Main category:** cs.HC

**Keywords:** conversational AI, large language models, collective intelligence, collaborative learning, knowledge diversity

**Relevance Score:** 8

**TL;DR:** The paper investigates the synergy of human and AI-agent conversations by comparing various configurations (human, LLM-LLM) and their impact on answer accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if AI interactions can replicate the synergy seen in human discussions and improve collective problem-solving.

**Method:** The study systematically compared conversational configurations: pairs/trios of large language models (LLM-LLM), trios of humans, and mixed pairs of humans and LLMs, evaluating accuracy and behavior post-conversation.

**Key Contributions:**

	1. Empirical comparisons of various conversational configurations involving AI
	2. Demonstrates the importance of knowledge diversity for collaborative improvement
	3. Calls for a paradigm shift in AI development towards diversity over individual accuracy

**Result:** Human interactions consistently improved answer accuracy, while LLM-only interactions led to declines in accuracy, indicating limited conversational synergy among models.

**Limitations:** Limited to specific configurations and may not generalize to all AI interaction scenarios.

**Conclusion:** Cultivating diversity among AI agents may enhance collaborative performance despite slight reductions in individual accuracy, shifting the focus from optimizing single models to optimizing their collaboration.

**Abstract:** Conversations transform individual knowledge into collective insight, allowing groups of humans and increasingly groups of artificial intelligence (AI) agents to collaboratively solve complex problems. Whether interactions between AI agents can replicate the synergy observed in human discussions remains an open question. To investigate this, we systematically compared four conversational configurations: pairs of large language models (LLM-LLM), trios of LLMs, trios of humans, and mixed human-LLM pairs. After agents answered questions individually, they engaged in open-ended discussions and then reconsidered their initial answers. Interactions involving humans consistently led to accuracy improvements after the conversations, benefiting both stronger and weaker participants. By contrast, purely LLM-based pairs and trios exhibited declines in accuracy, demonstrating limited conversational synergy. Analysis of participants' confidence and answer-switching behavior revealed that knowledge diversity is a critical factor enabling collaborative improvement. Crucially, the lack of gains in LLM-LLM interactions did not stem from a fundamental limitation of the models' ability to collaborate, but from highly similar knowledge states that left little room for productive exchange. Our findings argue for a paradigm shift in AI development: rather than optimizing individual models solely for standalone performance, explicitly cultivating diversity across agents, even at the cost of slightly lower individual accuracy, may yield AI collaborators that are more effective in group settings with humans or other AI systems.

</details>


### [2] [Evaluating LLMs for Visualization Generation and Understanding](https://arxiv.org/abs/2507.22890)

*Saadiq Rauf Khan, Vinit Chandak, Sougata Mukherjea*

**Main category:** cs.HC

**Keywords:** Information Visualization, Large Language Models, Code Generation, Data Visualization, HCI

**Relevance Score:** 8

**TL;DR:** The paper examines the abilities of various LLMs in generating code for information visualizations and answering related questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of Large Language Models in generating visualization code and understanding visual data representations.

**Method:** Different popular LLMs were tested to generate code for visualizations from prompts and answer questions related to these visualizations.

**Key Contributions:**

	1. Showcasing LLM capabilities in generating visualization code
	2. Analyzing LLM performance in understanding visualizations
	3. Identifying limitations of LLMs in complex visualization scenarios

**Result:** LLMs successfully generated code for simpler visualizations like bar and pie charts and answered basic questions about them, but struggled with complex visualizations and had some inaccuracies in responses.

**Limitations:** LLMs struggled with complex visualizations and made errors in understanding relationships in visual data.

**Conclusion:** LLMs have potential in enhancing information visualization but also exhibit limitations that need addressing to improve their functionality and accuracy.

**Abstract:** Information Visualization has been utilized to gain insights from complex data. In recent times, Large Language models (LLMs) have performed very well in many tasks. In this paper, we showcase the capabilities of different popular LLMs to generate code for visualization based on simple prompts. We also analyze the power of LLMs to understand some common visualizations by answering questions. Our study shows that LLMs could generate code for some simpler visualizations such as bar and pie charts. Moreover, they could answer simple questions about visualizations. However, LLMs also have several limitations. For example, some of them had difficulty generating complex visualizations, such as violin plot. LLMs also made errors in answering some questions about visualizations, for example, identifying relationships between close boundaries and determining lengths of shapes. We believe that our insights can be used to improve both LLMs and Information Visualization systems.

</details>


### [3] [Real-time energy monitoring infrastructure for residential collective self-consumption operations using Linky meter](https://arxiv.org/abs/2507.22891)

*Jérôme Ferrari, Benoit Delinchant, Frédéric Wurtz, Olga Rouchouze*

**Main category:** cs.HC

**Keywords:** Energy Transition, Real-time Monitoring, Collective Self-Consumption

**Relevance Score:** 3

**TL;DR:** This article presents an open-source infrastructure for real-time energy monitoring to enhance collective self-consumption operations in France, utilizing Linky meter data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide real-time feedback for participants in collective self-consumption operations, thereby helping them adapt their energy consumption behaviors in response to rising energy prices.

**Method:** Development of an infrastructure that includes xKy devices and gateways in participants' homes, along with a real-time monitoring website.

**Key Contributions:**

	1. Open-source infrastructure for real-time monitoring of energy consumption
	2. Implementation of the xKy device for monitoring
	3. Development of a user-friendly real-time monitoring website

**Result:** Enhanced decision-making capabilities for nine participants in a collective self-consumption operation, leading to increased self-consumption rates.

**Limitations:** 

**Conclusion:** The implementation of real-time monitoring can significantly improve energy consumption behavior among participants in collective self-consumption initiatives.

**Abstract:** As part of the energy transition and the rise in energy prices, the number of collective self-consumption operations in France is steadily increasing. However, energy flow monitoring currently relies on historical ''day+1'' data provided by Linky meters, which does not offer real time feedback to help participants adapt their energy consumption behaviors. This article introduces a new open-source infrastructure for real-time monitoring based on Linky meter data, enabling participants to make informed decisions and take timely actions. It includes a description of the xKy device, applied to a collective self-consumption operation involving nine participants, supported by the Energy Transition Observatory (OTE). The project encompasses the implementation of gateways in participants' homes and the development and operation of real-time monitoring website, aimed at increasing participants' self-consumption rate.

</details>


### [4] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)

*Ismail Hossain, Mridul Banik*

**Main category:** cs.HC

**Keywords:** augmentative and alternative communication, brain-computer interfaces, language rehabilitation

**Relevance Score:** 9

**TL;DR:** A hybrid framework combining EEG-based BCIs and LLMs for real-time language rehabilitation for users with severe impairments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional AAC systems and language-learning platforms lack adaptability to users' real-time cognitive and linguistic needs, particularly for individuals with neurological conditions.

**Method:** The proposed system utilizes real-time EEG signals to control an LLM-powered language rehabilitation assistant, allowing users to navigate learning modules through mental commands.

**Key Contributions:**

	1. Combines EEG-based BCIs with LLMs for real-time interaction
	2. Personalizes language-learning experiences based on cognitive effort
	3. Enables navigation of language modules via mental commands

**Result:** The system dynamically personalizes learning content and adjusts task difficulty based on users' neural markers, significantly enhancing language rehabilitation efforts.

**Limitations:** 

**Conclusion:** This hybrid approach offers a novel method of supporting language learning tailored to individual cognitive states, providing immediate feedback and adaptability to enhance user experience.

**Abstract:** Conventional augmentative and alternative communication (AAC) systems and language-learning platforms often fail to adapt in real time to the user's cognitive and linguistic needs, especially in neurological conditions such as post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in noninvasive electroencephalography (EEG)--based brain-computer interfaces (BCIs) and transformer--based large language models (LLMs) offer complementary strengths: BCIs capture users' neural intent with low fatigue, while LLMs generate contextually tailored language content. We propose and evaluate a novel hybrid framework that leverages real-time EEG signals to drive an LLM-powered language rehabilitation assistant. This system aims to: (1) enable users with severe speech or motor impairments to navigate language-learning modules via mental commands; (2) dynamically personalize vocabulary, sentence-construction exercises, and corrective feedback; and (3) monitor neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [5] [Invisible Architectures of Thought: Toward a New Science of AI as Cognitive Infrastructure](https://arxiv.org/abs/2507.22893)

*Giuseppe Riva*

**Main category:** cs.HC

**Keywords:** Cognitive Infrastructure Studies, human-AI interaction, cognition, epistemic agency, algorithmic influence

**Relevance Score:** 8

**TL;DR:** This paper introduces Cognitive Infrastructure Studies (CIS) to understand how AI systems fundamentally reshape human cognition and social epistemologies through their semantically rich, often invisible structures.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked impact of AI systems on human cognition and the need for a nuanced understanding of distributed cognition in digital societies.

**Method:** The paper proposes a new interdisciplinary framework, Cognitive Infrastructure Studies (CIS), combining narrative scenarios with infrastructure breakdown methodologies to study the automation of relevance judgment and cognitive dependencies.

**Key Contributions:**

	1. Introduction of Cognitive Infrastructure Studies (CIS) as a new research domain
	2. Proposes infrastructure breakdown methodologies to study AI's cognitive impact
	3. Framework for understanding the role of AI in shaping public reasoning and social epistemologies

**Result:** CIS demonstrates that AI functions as cognitive infrastructures that influence individual, collective, and cultural cognition, altering public reasoning and governance.

**Limitations:** 

**Conclusion:** The study underscores the necessity of integrating diverse disciplinary methods to comprehensively analyze the impact of AI on cognition, public reasoning, and social dynamics.

**Abstract:** Contemporary human-AI interaction research overlooks how AI systems fundamentally reshape human cognition pre-consciously, a critical blind spot for understanding distributed cognition. This paper introduces "Cognitive Infrastructure Studies" (CIS) as a new interdisciplinary domain to reconceptualize AI as "cognitive infrastructures": foundational, often invisible systems conditioning what is knowable and actionable in digital societies. These semantic infrastructures transport meaning, operate through anticipatory personalization, and exhibit adaptive invisibility, making their influence difficult to detect. Critically, they automate "relevance judgment," shifting the "locus of epistemic agency" to non-human systems. Through narrative scenarios spanning individual (cognitive dependency), collective (democratic deliberation), and societal (governance) scales, we describe how cognitive infrastructures reshape human cognition, public reasoning, and social epistemologies. CIS aims to address how AI preprocessing reshapes distributed cognition across individual, collective, and cultural scales, requiring unprecedented integration of diverse disciplinary methods. The framework also addresses critical gaps across disciplines: cognitive science lacks population-scale preprocessing analysis capabilities, digital sociology cannot access individual cognitive mechanisms, and computational approaches miss cultural transmission dynamics. To achieve this goal CIS also provides methodological innovations for studying invisible algorithmic influence: "infrastructure breakdown methodologies", experimental approaches that reveal cognitive dependencies by systematically withdrawing AI preprocessing after periods of habituation.

</details>


### [6] [When no one shows up (at first): Navigating the uncertainties of participatory workshops in interdisciplinary research](https://arxiv.org/abs/2507.22894)

*Monique Munarini*

**Main category:** cs.HC

**Keywords:** co-design, participatory workshops, equity in AI, early career researchers, methodological challenges

**Relevance Score:** 7

**TL;DR:** Reflective paper on challenges in co-design workshops, offering strategies for early career researchers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unspoken challenges faced during co-design and participatory workshops, particularly for early career researchers.

**Method:** The paper reflects on personal experiences from a series of workshops focused on equity in the AI ecosystem, detailing the entire workshop process from planning to facilitation.

**Key Contributions:**

	1. Offers insights into challenges of co-design workshops
	2. Provides practical strategies for engagement without institutional support
	3. Highlights the importance of participant experience in research

**Result:** Despite initial challenges like low attendance, the workshops led to rich discussions and demonstrated the potential for participant engagement, exemplified by one participant moving to a co-facilitator role.

**Limitations:** 

**Conclusion:** By reframing challenges as learning opportunities, the paper provides valuable strategies for conducting interdisciplinary participatory research.

**Abstract:** This reflective paper explores often-unspoken challenges of designing and facilitating co-design and participatory workshops, offering practical strategies for early career researchers (ECRs) navigating these methods. Drawing from personal experience conducting a series of workshops titled: How to Think About Equity in the AI Ecosystem. It follows the full arc of the workshop experience, from conceptualization and activity planning to participant recruitment and facilitation, offering a grounded account of what happens when participation does not go as expected. The paper examines the methodological challenges of engaging non-expert participants, particularly when operating without institutional support, financial incentives, or integration into larger events. Despite initial difficulties such as low attendance, the workshop fostered rich discussions among a demographically diverse group and ultimately led to one participant volunteering to co-facilitate a subsequent session. This transition from participant to co-facilitator exemplifies the redistribution of epistemic authority, positioning lived experience as central to research and engagement practices. By reframing perceived failure as a productive site of learning, the paper offers practical strategies for ECRs working across disciplines who often navigate unfamiliar methodological terrains, contributing to broader conversations on the realities of doing interdisciplinary, participatory work in practice.

</details>


### [7] [Brain motor intention Extraction Amplifier: Non-invasive brain-muscle interface](https://arxiv.org/abs/2507.22895)

*Ye Sun, Bowei Zhao, Dezhong Yao, Rui Zhang, Bohan Zhang, Xiaoyuan Li, Jing Wang, Mingxuan Qu, Gang Liu*

**Main category:** cs.HC

**Keywords:** Brain-computer interfaces, motor intention extraction, EMG, real-time interaction, neural signals

**Relevance Score:** 4

**TL;DR:** This paper introduces a novel motor intention extraction framework based on a brain-muscle interface (BMuI) to improve the accuracy of brain-computer interfaces (BCIs) in recognizing motor intentions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing BCI paradigms face challenges in accuracy due to imprecise labeling in real-world applications, leading to pseudo-labels that impact decoding performance.

**Method:** The proposed BMuI framework simulates the neural pathway from the brain to the muscles, utilizing EMG signals as a relay to enhance motor intention recognition.

**Key Contributions:**

	1. Introduction of the BMuI framework for motor intention extraction
	2. Demonstration of improved accuracy through EMG relay medium
	3. Validation through both offline and online experiments

**Result:** The BMuI method achieved a prediction accuracy of 0.8314 in offline experiments and effectively controlled a Unity virtual arm in online trials, demonstrating its feasibility and effectiveness.

**Limitations:** 

**Conclusion:** The study validates the BMuI framework as a promising approach to improving BCI accuracy and robustness in real-time applications.

**Abstract:** Brain-computer interfaces (BCIs) enable real-time interaction between the brain and external devices by decoding neural signals. However, existing motor-based BCI paradigms, like motor imagery BCI, face challenges with imprecise labeling in real-world use. This mismatch between EEG signals and true behavioral intentions leads to pseudo-labels, undermining decoding accuracy and system robustness. To overcome this bottleneck, this paper first proposes a novel motor intention extraction framework based on a non-invasive brain-muscle interface (BMuI)($\text{BCI} = \frac{\text{Brain}}{\text{Computer}} \text{ Interface} = \frac{\text{Brain}}{\not\text{Muscle}}\! \text{ (BMuI)} \times \!\frac{\not\text{Muscle}}{\text{Computer}}\! \text{ Interface}$). This method simulates the neural pathway from the brain to the muscles in order to capture and enhance the weak motor intention signals originating in the brain. It then uses EMG as a high-fidelity relay medium to achieve more accurate intention recognition and transmission. To systematically validate the feasibility and effectiveness of this approach, we conducted both offline experiments (to repeatedly verify feasibility) and online experiments (to construct a real-time interactive system and evaluate its performance). The results show that BMuI is feasible, achieving a prediction accuracy of 0.8314; in the online experiment, all participants are able to successfully control the Unity virtual arm.

</details>


### [8] [iLearnRobot: An Interactive Learning-Based Multi-Modal Robot with Continuous Improvement](https://arxiv.org/abs/2507.22896)

*Kohou Wang, ZhaoXiang Liu, Lin Bai, Kun Fan, Xiang Liu, Huan Hu, Kai Wang, Shiguo Lian*

**Main category:** cs.HC

**Keywords:** robotics, interactive learning, Multi-modal Large Language Model, user interaction, adaptability

**Relevance Score:** 7

**TL;DR:** This paper introduces an interactive learning-based robotic system leveraging a Multi-modal Large Language Model to improve robot performance post-deployment through user dialogues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable robots to adapt and learn from novel scenarios after deployment, enhancing their performance through user interactions.

**Method:** The proposed system incorporates an MLLM that facilitates learning from natural dialogues with users, along with a chain of questions to clarify user intent and dual-modality retrieval modules for mistake correction.

**Key Contributions:**

	1. Introduction of a learning-based approach for robots using MLLM.
	2. Implementation of dual-modality retrieval modules to enhance user interactions.
	3. Demonstration of improved robot performance through interactive learning.

**Result:** Experiments demonstrate that the system significantly enhances robot adaptability and performance in diverse environments.

**Limitations:** The paper does not explore the long-term effects of continuous learning in dynamic environments.

**Conclusion:** Integrating interactive learning into robotic systems offers a novel pathway for achieving better adaptability and user experience in robotics.

**Abstract:** It is crucial that robots' performance can be improved after deployment, as they are inherently likely to encounter novel scenarios never seen before. This paper presents an innovative solution: an interactive learning-based robot system powered by a Multi-modal Large Language Model(MLLM). A key feature of our system is its ability to learn from natural dialogues with non-expert users. We also propose chain of question to clarify the exact intent of the question before providing an answer and dual-modality retrieval modules to leverage these interaction events to avoid repeating same mistakes, ensuring a seamless user experience before model updates, which is in contrast to current mainstream MLLM-based robotic systems. Our system marks a novel approach in robotics by integrating interactive learning, paving the way for superior adaptability and performance in diverse environments. We demonstrate the effectiveness and improvement of our method through experiments, both quantitively and qualitatively.

</details>


### [9] [RecUserSim: A Realistic and Diverse User Simulator for Evaluating Conversational Recommender Systems](https://arxiv.org/abs/2507.22897)

*Luyu Chen, Quanyu Dai, Zeyu Zhang, Xueyang Feng, Mingyu Zhang, Pengcheng Tang, Xu Chen, Yue Zhu, Zhenhua Dong*

**Main category:** cs.HC

**Keywords:** Conversational Recommender Systems, User Simulation, Large Language Models

**Relevance Score:** 8

**TL;DR:** RecUserSim is an LLM-based user simulator designed to enhance the evaluation of conversational recommender systems (CRS) by providing realistic interactions and explicit scores.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Conversational recommender systems improve user experience through rich interactions, but evaluating their effectiveness poses challenges. Existing user simulators lack realism and diversity, which are essential for accurate evaluation.

**Method:** RecUserSim employs an LLM agent to simulate user interactions, incorporating a profile module for diverse personas, a memory module for interaction history, and a decision-making module based on Bounded Rationality theory.

**Key Contributions:**

	1. LLM-based user simulation for CRS evaluation
	2. Realistic user persona profiles
	3. Enhanced interaction tracking and decision-making

**Result:** RecUserSim generates high-quality dialogues with diverse outputs and maintains consistency in ratings across different base LLMs.

**Limitations:** 

**Conclusion:** RecUserSim significantly improves the evaluation of CRS by offering realistic simulations and effective scoring mechanisms.

**Abstract:** Conversational recommender systems (CRS) enhance user experience through multi-turn interactions, yet evaluating CRS remains challenging. User simulators can provide comprehensive evaluations through interactions with CRS, but building realistic and diverse simulators is difficult. While recent work leverages large language models (LLMs) to simulate user interactions, they still fall short in emulating individual real users across diverse scenarios and lack explicit rating mechanisms for quantitative evaluation. To address these gaps, we propose RecUserSim, an LLM agent-based user simulator with enhanced simulation realism and diversity while providing explicit scores. RecUserSim features several key modules: a profile module for defining realistic and diverse user personas, a memory module for tracking interaction history and discovering unknown preferences, and a core action module inspired by Bounded Rationality theory that enables nuanced decision-making while generating more fine-grained actions and personalized responses. To further enhance output control, a refinement module is designed to fine-tune final responses. Experiments demonstrate that RecUserSim generates diverse, controllable outputs and produces realistic, high-quality dialogues, even with smaller base LLMs. The ratings generated by RecUserSim show high consistency across different base LLMs, highlighting its effectiveness for CRS evaluation.

</details>


### [10] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)

*Julian Acosta, Scott Adams, Julius Kernbach, Romain Hardy, Sung Eun Kim, Luyang Luo, Xiaoman Zhang, Shreya Johri, Mohammed Baharoon, Pranav Rajpurkar*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Stroke Evaluation, Emergency Care, Natural Conversation, Diagnostic Accuracy

**Relevance Score:** 9

**TL;DR:** A voice-driven AI system for stroke evaluations shows promise in improving diagnostic accuracy and user confidence but requires human oversight due to some inaccuracies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the inconsistent and inaccurate stroke recognition by first responders, thereby reducing delays in treatment.

**Method:** The AI system guides users through stroke evaluations using natural conversation and captures video for documentation. Testing involved non-medical volunteers assessing simulated stroke patients.

**Key Contributions:**

	1. Development of a voice-driven AI for emergency stroke evaluations
	2. Demonstrated high user confidence and ease of use
	3. Identified critical need for AI oversight in medical applications

**Result:** The AI achieved 84% accuracy in identifying stroke signs and 75% detection of large vessel occlusions, with evaluations completed in about 6 minutes. Users reported high confidence and ease of use, but the system misidentified some non-stroke cases.

**Limitations:** The system incorrectly flags some non-stroke cases and needs human oversight due to inaccuracies.

**Conclusion:** While the AI system requires human oversight due to errors, advancements in AI suggest future iterations may provide highly accurate assessments, revolutionizing emergency medical care.

**Abstract:** We developed a voice-driven artificial intelligence (AI) system that guides anyone - from paramedics to family members - through expert-level stroke evaluations using natural conversation, while also enabling smartphone video capture of key examination components for documentation and potential expert review. This addresses a critical gap in emergency care: current stroke recognition by first responders is inconsistent and often inaccurate, with sensitivity for stroke detection as low as 58%, causing life-threatening delays in treatment. Three non-medical volunteers used our AI system to assess ten simulated stroke patients, including cases with likely large vessel occlusion (LVO) strokes and stroke-like conditions, while we measured diagnostic accuracy, completion times, user confidence, and expert physician review of the AI-generated reports. The AI system correctly identified 84% of individual stroke signs and detected 75% of likely LVOs, completing evaluations in just over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use (mean 4.67/5). The system successfully identified 86% of actual strokes but also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert physician reviewed the AI reports with videos, they identified the correct diagnosis in 100% of cases, but felt confident enough to make preliminary treatment decisions in only 40% of cases due to observed AI errors including incorrect scoring and false information. While the current system's limitations necessitate human oversight, ongoing rapid advancements in speech-to-speech AI models suggest that future versions are poised to enable highly accurate assessments. Achieving human-level voice interaction could transform emergency medical care, putting expert-informed assessment capabilities in everyone's hands.

</details>


### [11] [A visual analytics tool for taxonomy-based trajectory data exploration](https://arxiv.org/abs/2507.22899)

*Ivan A. Hanono Cozzetti, Ahmad Abdou*

**Main category:** cs.HC

**Keywords:** spatio-temporal data, data visualization, machine learning, behavior analysis

**Relevance Score:** 4

**TL;DR:** A spatio-temporal data analytics tool utilizing ML for categorization and visualization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in analyzing complex and heterogeneous movement patterns in spatio-temporal data.

**Method:** The tool employs a multi-level approach combining data visualization and statistical computation to categorize moving objects using Machine Learning models.

**Key Contributions:**

	1. Development of a multi-level spatio-temporal data analytics tool
	2. Application of Machine Learning for categorizing movement patterns
	3. Case studies demonstrating practical applications in wildlife and meteorology

**Result:** Case studies on Arctic fox trajectories and tropical cyclone data demonstrated the effectiveness of the methodology in identifying and labeling distinct movement behaviors.

**Limitations:** 

**Conclusion:** The proposed method allows detailed analysis of spatio-temporal data, offering a framework applicable across various domains.

**Abstract:** The analysis of spatio-temporal data presents significant challenges due to the complexity and heterogeneity of movement patterns. This project proposes a data analytics tool that combines data visualization and statistical computation to facilitate spatio-temporal data analysis through a multi-level approach. The tool categorizes moving objects into distinct taxonomies using Machine Learning models, adding meaningful structure to the analysis. Two case studies demonstrate the methodology's effectiveness. The first analyzed Arctic fox trajectories, successfully identifying and labeling foxes with Geometric or Kinematic-based behaviors, further categorized into Curvature and Acceleration groups. Statistical indicators revealed that foxes with Acceleration-based behavior showed constant, steady acceleration, while those with Curvature-based behavior exhibited acceleration peaks and sudden deceleration. The second case study examined tropical cyclone data, labeling trajectories with Speed, Curvature, and hybrid Geometric-based behaviors through unique statistical variables. Analysis of hybrid Geometric behavior (Curvature and Indentation combined) identified specific angles with the highest impact on hurricane shape and geometry. The proposed method and tool demonstrate that spatio-temporal data, despite inherent complexity, can be analyzed and explained in detail, providing a theoretical and practical blueprint applicable to multiple domains.

</details>


### [12] [Tool or Trouble? Exploring Student Attitudes Toward AI Coding Assistants](https://arxiv.org/abs/2507.22900)

*Sergio Rojas-Galeano*

**Main category:** cs.HC

**Keywords:** AI code assistants, novice programmers, programming education, pedagogical strategies, knowledge transfer

**Relevance Score:** 7

**TL;DR:** The study explores the impact of AI code assistants on novice programmers' experiences in a programming course, highlighting helpfulness in task completion but challenges in knowledge transfer.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effects of AI code assistants on the learning experiences of novice programmers and identify challenges in knowledge retention.

**Method:** The study involved 20 students who completed programming tasks first with AI assistance and then without. Data was collected through Likert-scale responses and open-ended feedback.

**Key Contributions:**

	1. Identified the dual role of AI tools in enhancing confidence and understanding among novice programmers.
	2. Revealed challenges regarding knowledge transfer when switching from AI-supported to unaided tasks.
	3. Suggested the necessity for improved pedagogical strategies in programming education.

**Result:** Students found AI tools helpful, boosting understanding and confidence in initial development tasks; however, they faced difficulties in applying knowledge without AI support.

**Limitations:** Limited sample size of 20 students may affect generalizability of findings; further research needed with larger groups.

**Conclusion:** There is a need for pedagogical strategies that effectively integrate AI assistance into programming education while reinforcing foundational skills to mitigate overreliance.

**Abstract:** This exploratory study examines how AI code assistants shape novice programmers' experiences during a two-part exam in an introductory programming course. In the first part, students completed a programming task with access to AI support; in the second, they extended their solutions without AI. We collected Likert-scale and open-ended responses from 20 students to evaluate their perceptions and challenges. Findings suggest that AI tools were perceived as helpful for understanding code and increasing confidence, particularly during initial development. However, students reported difficulties transferring knowledge to unaided tasks, revealing possible overreliance and gaps in conceptual understanding. These insights highlight the need for pedagogical strategies that integrate AI meaningfully while reinforcing foundational programming skills.

</details>


### [13] [Accelerated and Optimized Search of Imperceptible Color Vibration for Embedding Information into LCD images](https://arxiv.org/abs/2507.22901)

*Shingo Hattori, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** human-computer interaction, color vibration, information superimposition, public displays, LCD

**Relevance Score:** 5

**TL;DR:** The paper proposes an optimized method for superimposing invisible information on LCD displays using color vibration, enabling communication without compromising visual content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for invisible information embedding on displays compromise visual content presentation.

**Method:** The authors developed a parallelized search method for efficiently identifying color pairs that create imperceptible color vibrations for information embedding on liquid-crystal displays.

**Key Contributions:**

	1. Optimized search method for color pairs in invisible information embedding
	2. Parallel processing approach for faster results
	3. Enhanced capacity for information superimposition in images

**Result:** The proposed method significantly accelerates the search process for color pairs, enhancing the amount of information that can be embedded in images displayed on LCDs.

**Limitations:** 

**Conclusion:** The optimized method improves the feasibility of unseen information embedding in visual displays while maintaining aesthetic integrity.

**Abstract:** Large, high-resolution displays are installed throughout the city as public displays. By superimposing invisible information on the images of these displays, large numbers of devices with cameras and sensors can communicate with the displays without prior pairing. Several applications have been proposed, such as operating robots or communicating information to users by displaying 2D codes on images. However, the display of 2D codes has the problem of compromising the appearance of displayed content.   Abe et al. proposed a method of communicating with devices by superimposing invisible information using color vibration on images displayed on off-the-shelf liquid-crystal displays (LCD). Using this method, we can embed the information for devices in images without interfering with the displayed content. Abe et al. uses a simple serial loop operation to search for color pairs comprising a color vibration, which requires a very long processing time due to the huge search space.   In this paper, we propose an accelerated and optimized search method for color pairs that constitute the imperceptible color vibration for embedding information on LCD images. To achieve fast color pair search, we parallelized the search process, which is previously done individually, by using arrays representing the amount of movement and an operation to extract elements from the array that satisfy the conditions. In addition, we investigate the amount of information that can be superimposed on nine color images using the imperceptible color vibration and clarify the applicability of embedding information into images using the color vibration.

</details>


### [14] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)

*Hashim Hayat, Maksim Kudrautsau, Evgeniy Makarov, Vlad Melnichenko, Tim Tsykunou, Piotr Varaksin, Matt Pavelle, Adam Z. Oskowitz*

**Main category:** cs.HC

**Keywords:** AI doctor, LLM, urgent care, diagnostic concordance, healthcare workforce

**Relevance Score:** 9

**TL;DR:** Multi-agent LLM-based AI system Doctronic evaluated against clinicians in urgent care settings, showing strong results in diagnostics and treatment plans.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the projected healthcare practitioner shortage and reduce administrative burdens in clinical settings using AI.

**Method:** Performance of Doctronic compared to board-certified clinicians in 500 telehealth encounters, assessing diagnostic concordance, treatment plan consistency, and safety metrics.

**Key Contributions:**

	1. First large-scale validation of an autonomous AI doctor
	2. Strong concordance in diagnostics and treatment with human clinicians
	3. Potential solution for healthcare workforce shortages

**Result:** Doctronic matched clinician diagnoses in 81% of cases and treatment plans in 99.2%. AI outperformed human clinicians in 36.1% of discordant cases while showing no clinical hallucinations.

**Limitations:** 

**Conclusion:** The study demonstrates that multi-agent AI systems can achieve clinical decision-making on par with human providers, helpful in addressing workforce shortages.

**Abstract:** Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.

</details>


### [15] [A blessing or a burden? Exploring worker perspectives of using a social robot in a church](https://arxiv.org/abs/2507.22903)

*Andrew Blair, Peggy Gregory, Mary Ellen Foster*

**Main category:** cs.HC

**Keywords:** social robots, stakeholder perspectives, service sector, robot implementation, human-robot interaction

**Relevance Score:** 4

**TL;DR:** This paper explores the introduction of social robots in a church setting, revealing mixed stakeholder perspectives on their use and emphasizing the balance between social benefits and financial considerations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of robots in organizations primarily motivated by social benefits rather than profits, particularly in a church context.

**Method:** Interviews were conducted with 15 participants from different stakeholder groups within the church, followed by reflexive thematic analysis of the responses.

**Key Contributions:**

	1. Exploration of social robot implementation in a non-profit environment.
	2. Insight into stakeholder perspectives regarding robot use in service contexts.
	3. Identification of potential use cases beyond financial motivations.

**Result:** The analysis revealed mixed responses to the introduction of a social robot, highlighting concerns about empathetic responsibilities and unintended consequences, while identifying potential uses such as information provision and alleviating mundane tasks.

**Limitations:** The study is based on a specific case of a church, which may limit generalizability to other sectors.

**Conclusion:** The findings underscore the importance of considering social and intangible values alongside financial implications when introducing robots in organizational settings.

**Abstract:** Recent technological advances have allowed robots to assist in the service sector, and consequently accelerate job and sector transformation. Less attention has been paid to the use of robots in real-world organisations where social benefits, as opposed to profits, are the primary motivator. To explore these opportunities, we have partnered with a working church and visitor attraction. We conducted interviews with 15 participants from a range of stakeholder groups within the church to understand worker perspectives of introducing a social robot to the church and analysed the results using reflexive thematic analysis. Findings indicate mixed responses to the use of a robot, with participants highlighting the empathetic responsibility the church has towards people and the potential for unintended consequences. However, information provision and alleviation of menial or mundane tasks were identified as potential use cases. This highlights the need to consider not only the financial aspects of robot introduction, but also how social and intangible values shape what roles a robot should take on within an organisation.

</details>


### [16] [SketchMind: A Multi-Agent Cognitive Framework for Assessing Student-Drawn Scientific Sketches](https://arxiv.org/abs/2507.22904)

*Ehsan Latif, Zirak Khan, Xiaoming Zhai*

**Main category:** cs.HC

**Keywords:** AI in education, sketch evaluation, multi-agent framework, cognitive alignment, feedback systems

**Relevance Score:** 8

**TL;DR:** A framework called SketchMind is introduced for evaluating scientific sketches by students, integrating multi-agent systems for personalized feedback and achieving higher accuracy than existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of automated assessment of student-drawn scientific sketches using AI, given that existing solutions lack interpretability and adaptability.

**Method:** SketchMind employs modular agents for rubric parsing, sketch perception, cognitive alignment, and providing iterative feedback on sketches, allowing for a more personalized approach to evaluation.

**Key Contributions:**

	1. Introduction of SketchMind as a framework for sketch evaluation
	2. Integration of multi-agent orchestration that improves accuracy and feedback
	3. Demonstration of significant performance improvement over baseline models

**Result:** SketchMind achieved an average accuracy of 77.1% on a dataset of 3,575 sketches, outperforming baseline models significantly. Human evaluators provided a high satisfaction score to the feedback generated.

**Limitations:** 

**Conclusion:** The research demonstrates the potential of SketchMind to enhance conceptual understanding and support educational growth through its tailored feedback mechanisms.

**Abstract:** Scientific sketches (e.g., models) offer a powerful lens into students' conceptual understanding, yet AI-powered automated assessment of such free-form, visually diverse artifacts remains a critical challenge. Existing solutions often treat sketch evaluation as either an image classification task or monolithic vision-language models, which lack interpretability, pedagogical alignment, and adaptability across cognitive levels. To address these limitations, we present SketchMind, a cognitively grounded, multi-agent framework for evaluating and improving student-drawn scientific sketches. SketchMind comprises modular agents responsible for rubric parsing, sketch perception, cognitive alignment, and iterative feedback with sketch modification, enabling personalized and transparent evaluation. We evaluate SketchMind on a curated dataset of 3,575 student-generated sketches across six science assessment items with different highest order of Bloom's level that require students to draw models to explain phenomena. Compared to baseline GPT-4o performance without SRG (average accuracy: 55.6%), and with SRG integration achieves 77.1% average accuracy (+21.4% average absolute gain). We also demonstrate that multi-agent orchestration with SRG enhances SketchMind performance, for example, GPT-4.1 gains an average 8.9% increase in sketch prediction accuracy, outperforming single-agent pipelines across all items. Human evaluators rated the feedback and co-created sketches generated by \textsc{SketchMind} with GPT-4.1, which achieved an average of 4.1 out of 5, significantly higher than those of baseline models (e.g., 2.3 for GPT-4o). Experts noted the system's potential to meaningfully support conceptual growth through guided revision. Our code and (pending approval) dataset will be released to support reproducibility and future research in AI-driven education.

</details>


### [17] [Exploring LLM-generated Culture-specific Affective Human-Robot Tactile Interaction](https://arxiv.org/abs/2507.22905)

*Qiaoqiao Ren, Tony Belpaeme*

**Main category:** cs.HC

**Keywords:** Large Language Models, Human-Robot Interaction, Affective Touch, Cultural Adaptability, Emotional Decoding

**Relevance Score:** 9

**TL;DR:** This study explores how LLMs can generate culturally adaptive tactile behaviors for emotional expression in human-robot interaction.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the integration of LLMs into robotic systems for generating culturally appropriate affective touch.

**Method:** The study produced text-based touch descriptions representing 12 distinct emotions across three cultural contexts, engaging 90 participants to evaluate emotional decoding and appropriateness in both robot-to-human and human-to-robot scenarios.

**Key Contributions:**

	1. Demonstrated LLMs' ability to generate culturally adaptive tactile behaviors for robots.
	2. Identified the impact of cultural context on emotional decoding in human-robot interactions.
	3. Revealed the asymmetry in appropriateness of tactile behaviors based on interaction roles.

**Result:** Participants decoded six out of twelve emotions effectively under matched cultural conditions; behaviors were perceived as more appropriate in human-to-robot interactions and cultural mismatches led to reduced decoding accuracy and increased inappropriate ratings.

**Limitations:** Study limited to three cultural contexts and a specific set of emotions; results may not generalize across all cultures or emotional expressions.

**Conclusion:** The findings suggest that LLM-generated tactile behaviors can enhance emotional communication in human-robot interactions, but must be culturally adapted to avoid misinterpretations.

**Abstract:** As large language models (LLMs) become increasingly integrated into robotic systems, their potential to generate socially and culturally appropriate affective touch remains largely unexplored. This study investigates whether LLMs-specifically GPT-3.5, GPT-4, and GPT-4o --can generate culturally adaptive tactile behaviours to convey emotions in human-robot interaction. We produced text based touch descriptions for 12 distinct emotions across three cultural contexts (Chinese, Belgian, and unspecified), and examined their interpretability in both robot-to-human and human-to-robot scenarios. A total of 90 participants (36 Chinese, 36 Belgian, and 18 culturally unspecified) evaluated these LLM-generated tactile behaviours for emotional decoding and perceived appropriateness. Results reveal that: (1) under matched cultural conditions, participants successfully decoded six out of twelve emotions-mainly socially oriented emotions such as love and Ekman emotions such as anger, however, self-focused emotions like pride and embarrassment were more difficult to interpret; (2) tactile behaviours were perceived as more appropriate when directed from human to robot than from robot to human, revealing an asymmetry in social expectations based on interaction roles; (3) behaviours interpreted as aggressive (e.g., anger), overly intimate (e.g., love), or emotionally ambiguous (i.e., not clearly decodable) were significantly more likely to be rated as inappropriate; and (4) cultural mismatches reduced decoding accuracy and increased the likelihood of behaviours being judged as inappropriate.

</details>


### [18] [Automated Label Placement on Maps via Large Language Models](https://arxiv.org/abs/2507.22952)

*Harry Shomer, Jiejun Xu*

**Main category:** cs.HC

**Keywords:** automatic label placement, large language models, cartographic standards, data editing, map design

**Relevance Score:** 7

**TL;DR:** This paper introduces a paradigm for automatic label placement (ALP) using large language models to enhance clarity and interpretability in map design, supported by a new benchmarking dataset called MAPLE.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Label placement in map design is essential but remains manual and difficult to scale due to the limitations of existing automated systems.

**Method:** The task is formulated as a data editing problem, utilizing large language models and a benchmark dataset (MAPLE) to evaluate performance and generalization across various landmarks.

**Key Contributions:**

	1. Introduction of MAPLE, a benchmarking dataset for ALP
	2. Application of LLMs for context-aware spatial annotation
	3. Demonstration of effective AI-assisted map editing framework

**Result:** LLMs guided by structured prompts and domain-specific retrieval demonstrated the ability to perform accurate spatial edits, aligning with expert cartographic standards.

**Limitations:** 

**Conclusion:** This work provides a scalable framework for AI-assisted map finishing and showcases the effectiveness of foundation models in structured data editing tasks.

**Abstract:** Label placement is a critical aspect of map design, serving as a form of spatial annotation that directly impacts clarity and interpretability. Despite its importance, label placement remains largely manual and difficult to scale, as existing automated systems struggle to integrate cartographic conventions, adapt to context, or interpret labeling instructions. In this work, we introduce a new paradigm for automatic label placement (ALP) that formulates the task as a data editing problem and leverages large language models (LLMs) for context-aware spatial annotation. To support this direction, we curate MAPLE, the first known benchmarking dataset for evaluating ALP on real-world maps, encompassing diverse landmark types and label placement annotations from open-source data. Our method retrieves labeling guidelines relevant to each landmark type leveraging retrieval-augmented generation (RAG), integrates them into prompts, and employs instruction-tuned LLMs to generate ideal label coordinates. We evaluate four open-source LLMs on MAPLE, analyzing both overall performance and generalization across different types of landmarks. This includes both zero-shot and instruction-tuned performance. Our results demonstrate that LLMs, when guided by structured prompts and domain-specific retrieval, can learn to perform accurate spatial edits, aligning the generated outputs with expert cartographic standards. Overall, our work presents a scalable framework for AI-assisted map finishing and demonstrates the potential of foundation models in structured data editing tasks. The code and data can be found at https://github.com/HarryShomer/MAPLE.

</details>


### [19] [ChatVis: Large Language Model Agent for Generating Scientific Visualizations](https://arxiv.org/abs/2507.23096)

*Tom Peterka, Tanwi Mallick, Orcun Yildiz, David Lenz, Cory Quammen, Berk Geveci*

**Main category:** cs.HC

**Keywords:** large language models, scientific visualization, ChatVis

**Relevance Score:** 8

**TL;DR:** ChatVis is an LLM assistant designed to improve Python code generation for scientific visualization tasks in ParaView, enhancing performance without retraining the LLM.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs are advancing but still find it challenging to perform specialized programming tasks such as scientific visualization.

**Method:** ChatVis integrates chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database, and iterative error checking to assist LLMs in generating correct visualization code.

**Key Contributions:**

	1. Introduction of ChatVis for enhancing LLM capabilities in visualization tasks
	2. Utilization of chain-of-thought techniques and retrieval-augmented methods
	3. Comprehensive evaluation metrics with a benchmark suite for performance assessment

**Result:** Comparison with unassisted LLMs shows that ChatVis significantly improves all evaluation metrics for scientific visualization tasks.

**Limitations:** 

**Conclusion:** ChatVis provides a robust solution for generating Python code in scientific visualization, demonstrating improved outcomes over traditional LLM approaches.

**Abstract:** Large language models (LLMs) are rapidly increasing in capability, but they still struggle with highly specialized programming tasks such as scientific visualization. We present an LLM assistant, ChatVis, that aids the LLM to generate Python code for ParaView scientific visualization tasks, without the need for retraining or fine-tuning the LLM. ChatVis employs chain-of-thought prompt simplification, retrieval-augmented prompt generation using a vector database of documentation and code examples, and error checking with iterative prompt feedback to correct errors until a visualization is produced. An integral part of our approach is a benchmark suite of canonical visualization tasks, ParaView regression tests, and scientific use cases that includes comprehensive evaluation metrics. We evaluate our visualization assistant by comparing results with a variety of top-performing unassisted LLMs. We find that all the metrics are significantly improved with ChatVis.

</details>


### [20] [Accessibility Scout: Personalized Accessibility Scans of Built Environments](https://arxiv.org/abs/2507.23190)

*William Huang, Xia Su, Jon E. Froehlich, Yang Zhang*

**Main category:** cs.HC

**Keywords:** accessibility, large language models, human-computer interaction, personalization, built environments

**Relevance Score:** 9

**TL;DR:** Accessibility Scout is an LLM-based system for personalized accessibility scanning of built environments, addressing the limitations of manual assessments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for scalable and personalized assessments of accessibility in built environments for people with disabilities.

**Method:** Development of Accessibility Scout, an LLM-based system that analyzes photos to identify accessibility concerns and tailors results based on user-specific preferences.

**Key Contributions:**

	1. Introduction of Accessibility Scout, an LLM-driven assessment tool.
	2. Demonstration of personalized accessibility scans that exceed traditional ADA guidelines.
	3. Findings from multiple user studies validating the system's effectiveness.

**Result:** Accessibility Scout generated personalized scans that consider individual mobility levels and environmental interests, outperforming traditional assessments.

**Limitations:** The studies involved a limited number of participants and focused on a specific set of environments.

**Conclusion:** The system enhances accessibility assessments and highlights future directions for more adaptive technologies in this field.

**Abstract:** Assessing the accessibility of unfamiliar built environments is critical for people with disabilities. However, manual assessments, performed by users or their personal health professionals, are laborious and unscalable, while automatic machine learning methods often neglect an individual user's unique needs. Recent advances in Large Language Models (LLMs) enable novel approaches to this problem, balancing personalization with scalability to enable more adaptive and context-aware assessments of accessibility. We present Accessibility Scout, an LLM-based accessibility scanning system that identifies accessibility concerns from photos of built environments. With use, Accessibility Scout becomes an increasingly capable "accessibility scout", tailoring accessibility scans to an individual's mobility level, preferences, and specific environmental interests through collaborative Human-AI assessments. We present findings from three studies: a formative study with six participants to inform the design of Accessibility Scout, a technical evaluation of 500 images of built environments, and a user study with 10 participants of varying mobility. Results from our technical evaluation and user study show that Accessibility Scout can generate personalized accessibility scans that extend beyond traditional ADA considerations. Finally, we conclude with a discussion on the implications of our work and future steps for building more scalable and personalized accessibility assessments of the physical world.

</details>


### [21] [Silent Impact: Tracking Tennis Shots from the Passive Arm](https://arxiv.org/abs/2507.23215)

*Junyong Park, Saelyne Yang, Sungho Jo*

**Main category:** cs.HC

**Keywords:** wearable technology, sports analytics, tennis, passive arm, machine learning

**Relevance Score:** 7

**TL;DR:** The paper presents Silent Impact, a user-friendly system for analyzing tennis shots using sensors on the passive arm, achieving high classification accuracy and user satisfaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sports analytics by reducing the physical and mental burden of existing sensor setups in tennis.

**Method:** Developed neural networks that use data from Inertial Measurement Units on the passive arm to classify six types of tennis shots, and created a mobile app prototype displaying the results.

**Key Contributions:**

	1. Introduction of a novel system for tennis shot analysis using passive arm data.
	2. Demonstrated high accuracy and effectiveness compared to existing solutions.
	3. User study results indicated reduced physical and mental burden for users.

**Result:** Achieved a classification accuracy of 88.2% and a detection F1 score of 86.0%, comparable to solutions using the dominant arm.

**Limitations:** Limited to recreational tennis players and requires further validation in competitive settings.

**Conclusion:** The study shows that analysis through the passive arm is an effective and comfortable alternative, enhancing the user experience in sports analytics.

**Abstract:** Wearable technology has transformed sports analytics, offering new dimensions in enhancing player experience. Yet, many solutions involve cumbersome setups that inhibit natural motion. In tennis, existing products require sensors on the racket or dominant arm, causing distractions and discomfort. We propose Silent Impact, a novel and user-friendly system that analyzes tennis shots using a sensor placed on the passive arm. Collecting Inertial Measurement Unit sensor data from 20 recreational tennis players, we developed neural networks that exclusively utilize passive arm data to detect and classify six shots, achieving a classification accuracy of 88.2% and a detection F1 score of 86.0%, comparable to the dominant arm. These models were then incorporated into an end-to-end prototype, which records passive arm motion through a smartwatch and displays a summary of shots on a mobile app. User study (N=10) showed that participants felt less burdened physically and mentally using Silent Impact on the passive arm. Overall, our research establishes the passive arm as an effective, comfortable alternative for tennis shot analysis, advancing user-friendly sports analytics.

</details>


### [22] [Real-time Generation of Various Types of Nodding for Avatar Attentive Listening System](https://arxiv.org/abs/2507.23298)

*Kazushi Kato, Koji Inoue, Divesh Lala, Keiko Ochi, Tatsuya Kawahara*

**Main category:** cs.HC

**Keywords:** nonverbal communication, dialogue systems, real-time prediction

**Relevance Score:** 7

**TL;DR:** This paper presents a real-time model for predicting nodding behavior in dialogue systems, enhancing attentive listening through nonverbal cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve spoken dialogue systems by integrating nonverbal behaviors, specifically nodding, to enhance user interaction and attentiveness.

**Method:** The model builds on the voice activity projection (VAP) framework, extending it to predict nodding timing and type in real time using multi-task learning and verbal backchannel predictions.

**Key Contributions:**

	1. Real-time nodding prediction in dialogue systems
	2. Integration of multi-task learning for backchannel prediction
	3. Improved performance over conventional nodding methods

**Result:** The model demonstrates significant improvements in nodding prediction accuracy and real-time performance compared to conventional methods, verified through subjective evaluations in an avatar listening system.

**Limitations:** 

**Conclusion:** The proposed prediction model effectively integrates nonverbal cues into dialogue systems, achieving real-time operation and improved user experience.

**Abstract:** In human dialogue, nonverbal information such as nodding and facial expressions is as crucial as verbal information, and spoken dialogue systems are also expected to express such nonverbal behaviors. We focus on nodding, which is critical in an attentive listening system, and propose a model that predicts both its timing and type in real time. The proposed model builds on the voice activity projection (VAP) model, which predicts voice activity from both listener and speaker audio. We extend it to prediction of various types of nodding in a continuous and real-time manner unlike conventional models. In addition, the proposed model incorporates multi-task learning with verbal backchannel prediction and pretraining on general dialogue data. In the timing and type prediction task, the effectiveness of multi-task learning was significantly demonstrated. We confirmed that reducing the processing rate enables real-time operation without a substantial drop in accuracy, and integrated the model into an avatar attentive listening system. Subjective evaluations showed that it outperformed the conventional method, which always does nodding in sync with verbal backchannel. The code and trained models are available at https://github.com/MaAI-Kyoto/MaAI.

</details>


### [23] [Breaking the mould of Social Mixed Reality -- State-of-the-Art and Glossary](https://arxiv.org/abs/2507.23454)

*Marta Bieńkiewicz, Julia Ayache, Panayiotis Charalambous, Cristina Becchio, Marco Corragio, Bertram Taetz, Francesco De Lellis, Antonio Grotta, Anna Server, Daniel Rammer, Richard Kulpa, Franck Multon, Azucena Garcia-Palacios, Jessica Sutherland, Kathleen Bryson, Stéphane Donikian, Didier Stricker, Benoît Bardy*

**Main category:** cs.HC

**Keywords:** Mixed Reality, Human-Centric Innovation, Social Interaction, Ethics by Design, Virtual Characters

**Relevance Score:** 6

**TL;DR:** The paper addresses the limitations of Mixed Reality (MR) in replicating human embodiment and socio-motor interaction, advocating for advancements in multi-modal data and multi-agent interactions to enhance social experiences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve meaningful social interactions in Mixed Reality (MR) environments, addressing gaps in technology that hinder authentic embodiment and interaction.

**Method:** The article provides a comprehensive glossary of key topics related to Mixed Reality, including Virtual Characters, Responsible AI, Ethics by Design, and scientific challenges linked to Neuroscience and Technology.

**Key Contributions:**

	1. Comprehensive glossary encompassing key MR topics
	2. Emphasis on human-centric innovation in MR
	3. Discussion on ethical considerations in MR design

**Result:** The paper emphasizes the need for MR technologies that enhance human-centric innovation and social collaboration between humans and virtual agents.

**Limitations:** 

**Conclusion:** The authors argue for MR systems that ensure ethical design, inclusivity, and psychological safety to foster richer social interactions in digital environments.

**Abstract:** This article explores a critical gap in Mixed Reality (MR) technology: while advances have been made, MR still struggles to authentically replicate human embodiment and socio-motor interaction. For MR to enable truly meaningful social experiences, it needs to incorporate multi-modal data streams and multi-agent interaction capabilities. To address this challenge, we present a comprehensive glossary covering key topics such as Virtual Characters and Autonomisation, Responsible AI, Ethics by Design, and the Scientific Challenges of Social MR within Neuroscience, Embodiment, and Technology. Our aim is to drive the transformative evolution of MR technologies that prioritize human-centric innovation, fostering richer digital connections. We advocate for MR systems that enhance social interaction and collaboration between humans and virtual autonomous agents, ensuring inclusivity, ethical design and psychological safety in the process.

</details>


### [24] [Automated Feedback on Student-Generated UML and ER Diagrams Using Large Language Models](https://arxiv.org/abs/2507.23470)

*Sebastian Gürtl, Gloria Schimetta, David Kerschbaumer, Michael Liut, Alexander Steinmaurer*

**Main category:** cs.HC

**Keywords:** LLM, UML, ER diagrams, feedback, education

**Relevance Score:** 7

**TL;DR:** DUET is an LLM-based tool designed to provide structured feedback on UML and ER diagrams, enhancing learning in computer science.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional teaching methods struggle to offer scalable, personalized feedback in UML and ER diagram education, which is needed for student mastery.

**Method:** DUET converts reference and student-submitted diagrams into textual representations, using a multi-stage LLM pipeline for comparison and feedback generation.

**Key Contributions:**

	1. Introduction of an LLM-based tool for diagram feedback
	2. Structured feedback generation from diagram comparisons
	3. Insights for educators to improve instructional strategies

**Result:** Evaluation through interviews revealed strengths of DUET in accessibility and scalability, as well as limitations like reliability and potential misuse.

**Limitations:** Reliability and potential misuse of the tool were identified.

**Conclusion:** DUET shows promise for integrating LLMs in modeling education, with suggestions for improvement and implications for future classroom use.

**Abstract:** UML and ER diagrams are foundational in computer science education but come with challenges for learners due to the need for abstract thinking, contextual understanding, and mastery of both syntax and semantics. These complexities are difficult to address through traditional teaching methods, which often struggle to provide scalable, personalized feedback, especially in large classes. We introduce DUET (Diagrammatic UML & ER Tutor), a prototype of an LLM-based tool, which converts a reference diagram and a student-submitted diagram into a textual representation and provides structured feedback based on the differences. It uses a multi-stage LLM pipeline to compare diagrams and generate reflective feedback. Furthermore, the tool enables analytical insights for educators, aiming to foster self-directed learning and inform instructional strategies. We evaluated DUET through semi-structured interviews with six participants, including two educators and four teaching assistants. They identified strengths such as accessibility, scalability, and learning support alongside limitations, including reliability and potential misuse. Participants also suggested potential improvements, such as bulk upload functionality and interactive clarification features. DUET presents a promising direction for integrating LLMs into modeling education and offers a foundation for future classroom integration and empirical evaluation.

</details>


### [25] [Digital literacy interventions can boost humans in discerning deepfakes](https://arxiv.org/abs/2507.23492)

*Dominique Geissler, Claire Robertson, Stefan Feuerriegel*

**Main category:** cs.HC

**Keywords:** deepfakes, digital literacy, interventions, human-computer interaction, trust

**Relevance Score:** 4

**TL;DR:** This paper examines five interventions aimed at improving digital literacy for discerning deepfakes, demonstrating significant effectiveness in enhancing detection skills.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of deepfakes threatens trust in institutions and democratic processes, necessitating effective strategies to enhance digital literacy.

**Method:** The study compares five interventions aimed at improving deepfake discernment among 1,200 participants, measuring immediate and long-term effectiveness.

**Key Contributions:**

	1. Comparison of five digital literacy interventions for deepfake detection
	2. Demonstrated significant improvements in discernment capabilities
	3. Maintained trust in authentic images while boosting detection rates.

**Result:** The interventions improved participants' ability to identify deepfakes by up to 13 percentage points while preserving trust in genuine images.

**Limitations:** The study's findings may not generalize across different cultural contexts or age groups.

**Conclusion:** The proposed digital literacy interventions are scalable and effective, making them suitable for diverse populations to enhance deepfake detection skills.

**Abstract:** Deepfakes, i.e., images generated by artificial intelligence (AI), can erode trust in institutions and compromise election outcomes, as people often struggle to discern real images from deepfakes. Improving digital literacy can help address these challenges, yet scalable and effective approaches remain largely unexplored. Here, we compare the efficacy of five digital literacy interventions to boost people's ability to discern deepfakes: (1) textual guidance on common indicators of deepfakes; (2) visual demonstrations of these indicators; (3) a gamified exercise for identifying deepfakes; (4) implicit learning through repeated exposure and feedback; and (5) explanations of how deepfakes are generated with the help of AI. We conducted an experiment with N=1,200 participants from the United States to test the immediate and long-term effectiveness of our interventions. Our results show that our interventions can boost deepfake discernment by up to 13 percentage points while maintaining trust in real images. Altogether, our approach is scalable, suitable for diverse populations, and highly effective for boosting deepfake detection while maintaining trust in truthful information.

</details>


### [26] [Agency Among Agents: Designing with Hypertextual Friction in the Algorithmic Web](https://arxiv.org/abs/2507.23585)

*Sophia Liu, Shm Garanganao Almeda*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Algorithmic Interfaces, User Agency

**Relevance Score:** 8

**TL;DR:** This paper discusses 'Hypertextual Friction' to reclaim user agency in algorithm-driven interfaces.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Algorithm-driven interfaces often compromise user agency for efficiency and engagement, necessitating a design approach that emphasizes user control.

**Method:** A comparative analysis of real-world interfaces such as Wikipedia and Instagram Explore, examining their impact on user experience, navigation, and authorship.

**Key Contributions:**

	1. Comparative analysis of interface structures affecting user agency
	2. Introduction of 'Hypertextual Friction' as a design stance
	3. Recommendations for integrating hypertextual values in algorithmic interfaces

**Result:** Hypertext systems prioritize provenance and user-driven meaning-making, which contrasts with algorithmic systems that obscure processes and flatten participation.

**Limitations:** 

**Conclusion:** The paper proposes hypertextual values as actionable design commitments for enhancing user agency in algorithmically mediated environments.

**Abstract:** Today's algorithm-driven interfaces, from recommendation feeds to GenAI tools, often prioritize engagement and efficiency at the expense of user agency. As systems take on more decision-making, users have less control over what they see and how meaning or relationships between content are constructed. This paper introduces "Hypertextual Friction," a conceptual design stance that repositions classical hypertext principles--friction, traceability, and structure--as actionable values for reclaiming agency in algorithmically mediated environments. Through a comparative analysis of real-world interfaces--Wikipedia vs. Instagram Explore, and Are.na vs. GenAI image tools--we examine how different systems structure user experience, navigation, and authorship. We show that hypertext systems emphasize provenance, associative thinking, and user-driven meaning-making, while algorithmic systems tend to obscure process and flatten participation. We contribute: (1) a comparative analysis of how interface structures shape agency in user-driven versus agent-driven systems, and (2) a conceptual stance that offers hypertextual values as design commitments for reclaiming agency in an increasingly algorithmic web.

</details>


### [27] [An Efficient Intelligent Semi-Automated Warehouse Inventory Stocktaking System](https://arxiv.org/abs/2309.12365)

*Chunan Tong*

**Main category:** cs.HC

**Keywords:** inventory management, big data analytics, AI forecasting

**Relevance Score:** 2

**TL;DR:** This paper presents an intelligent inventory management system integrating barcode and distributed application technologies with big data analytics to enhance automation and decision-making in supply chain management.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of inefficient inventory management and inaccuracy in forecasting faced by conventional methods in supply chain management.

**Method:** The research employs bar code technology, a distributed flutter application, and big data analytics to create an intelligent inventory management system, validated through system design and simulation.

**Key Contributions:**

	1. Integration of barcode anddistributed flutter applications for intelligent perception.
	2. Utilization of big data analytics for data-driven decision-making.
	3. AI-driven forecasting to improve inventory management accuracy.

**Result:** The proposed system enhances automation, precision, and intelligence in inventory management, enabling second-level monitoring and AI-driven forecasting, leading to cost reductions and optimized inventory sizes.

**Limitations:** 

**Conclusion:** The intelligent inventory management system effectively mitigates issues related to inaccurate data and delayed monitoring, driving informed decision-making in supply chains.

**Abstract:** In the context of evolving supply chain management, the significance of efficient inventory management has grown substantially for businesses. However, conventional manual and experience-based approaches often struggle to meet the complexities of modern market demands. This research introduces an intelligent inventory management system to address challenges related to inaccurate data, delayed monitoring, and overreliance on subjective experience in forecasting. The proposed system integrates bar code and distributed flutter application technologies for intelligent perception, alongside comprehensive big data analytics to enable data-driven decision-making. Through meticulous analysis, system design, critical technology exploration, and simulation validation, the effectiveness of the proposed system is successfully demonstrated. The intelligent system facilitates second-level monitoring, high-frequency checks, and artificial intelligence-driven forecasting, consequently enhancing the automation, precision, and intelligence of inventory management. This system contributes to cost reduction and optimized inventory sizes through accurate predictions and informed decisions, ultimately achieving a mutually beneficial scenario. The outcomes of this research offer

</details>


### [28] [EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition](https://arxiv.org/abs/2408.09186)

*Qile Liu, Weishan Ye, Lingli Zhang, Zhen Liang*

**Main category:** cs.HC

**Keywords:** EEG, emotion recognition, cross-corpus, machine learning, self-supervised learning

**Relevance Score:** 5

**TL;DR:** This paper presents a novel framework, Soft Contrastive Masked Modeling (SCMM), for improving emotion recognition from EEG signals across different datasets without retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for EEG-based emotion recognition often fail to generalize across different datasets due to varying data distributions and recording conditions.

**Method:** SCMM integrates soft contrastive learning with a hybrid masking strategy; it uses a soft weighting mechanism for sample pairs to capture emotional dynamics, and employs a similarity-aware aggregator to enhance representation learning.

**Key Contributions:**

	1. Introduction of Soft Contrastive Masked Modeling (SCMM) framework
	2. Combining soft contrastive learning with a hybrid masking strategy
	3. Achieving state-of-the-art performance in cross-corpus emotion recognition

**Result:** SCMM outperforms the second-best method by an average accuracy of 4.26% in cross-corpus settings on SEED, SEED-IV, and DEAP datasets, achieving state-of-the-art results.

**Limitations:** 

**Conclusion:** The proposed framework contributes significantly to better cross-corpus generalization in EEG emotion recognition tasks.

**Abstract:** Emotion recognition using electroencephalography (EEG) signals has attracted increasing attention in recent years. However, existing methods often lack generalization in cross-corpus settings, where a model trained on one dataset is directly applied to another without retraining, due to differences in data distribution and recording conditions. To tackle the challenge of cross-corpus EEG-based emotion recognition, we propose a novel framework termed Soft Contrastive Masked Modeling (SCMM). Grounded in the theory of emotional continuity, SCMM integrates soft contrastive learning with a hybrid masking strategy to effectively capture emotion dynamics (refer to short-term continuity). Specifically, in the self-supervised learning stage, we propose a soft weighting mechanism that assigns similarity scores to sample pairs, enabling fine-grained modeling of emotional transitions and capturing the temporal continuity of human emotions. To further enhance representation learning, we design a similarity-aware aggregator that fuses complementary information from semantically related samples based on pairwise similarities, thereby improving feature expressiveness and reconstruction quality. This dual design contributes to a more discriminative and transferable representation, which is crucial for robust cross-corpus generalization. Extensive experiments on the SEED, SEED-IV, and DEAP datasets show that SCMM achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy of 4.26% under both same-class and different-class cross-corpus settings. The source code is available at https://github.com/Kyler-RL/SCMM.

</details>


### [29] [AI vs. Human Paintings? Deciphering Public Interactions and Perceptions towards AI-Generated Paintings on TikTok](https://arxiv.org/abs/2409.11911)

*Jiajun Wang, Xiangzhe Yuan, Siying Hu, Zhicong Lu*

**Main category:** cs.HC

**Keywords:** AI-generated paintings, public perception, user engagement, sentiment analysis, generative AI

**Relevance Score:** 6

**TL;DR:** The paper investigates public perceptions of AI-generated paintings (AIGP) on social media, highlighting user engagement and sentiment analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand public interactions and concerns regarding AI-generated paintings amidst a backdrop of recent anti-AI movements by artists, reflecting potential issues in human-AI collaboration.

**Method:** Analyzed user engagement level and comment sentiment scores for AI-generated paintings and compared these to human painting videos; utilized topic modeling to identify reasons for negative perceptions.

**Key Contributions:**

	1. Analysis of user engagement and sentiment regarding AIGP
	2. Identification of key reasons for negative perceptions of AIGP
	3. Suggestions for improving future generative AI technologies

**Result:** Identified seven key reasons for negative public perceptions, including hyperrealistic quality and perceived theft of art, contributing to a nuanced understanding of user sentiments.

**Limitations:** Focused solely on social media interactions, may not represent broader public opinions.

**Conclusion:** The findings may help guide future generative AI technology development and address societal concerns, ensuring better human-AI collaboration.

**Abstract:** With the development of generative AI technology, a vast array of AI-generated paintings (AIGP) have gone viral on social media like TikTok. However, some negative news about AIGP has also emerged. For example, in 2022, numerous painters worldwide organized a large-scale anti-AI movement because of the infringement in generative AI model training. This event reflected a social issue that, with the development and application of generative AI, public feedback and feelings towards it may have been overlooked. Therefore, to investigate public interactions and perceptions towards AIGP on social media, we analyzed user engagement level and comment sentiment scores of AIGP using human painting videos as a baseline. In analyzing user engagement, we also considered the possible moderating effect of the aesthetic quality of Paintings. Utilizing topic modeling, we identified seven reasons, including hyperrealistic quality, ambivalent reactions, perceived theft of art, etc., leading to negative public perceptions of AIGP. Our work may provide instructive suggestions for future generative AI technology development and avoid potential crises in human-AI collaboration.

</details>


### [30] [Visual Story-Writing: Writing by Manipulating Visual Representations of Stories](https://arxiv.org/abs/2410.07486)

*Damien Masson, Zixin Zhao, Fanny Chevalier*

**Main category:** cs.HC

**Keywords:** Visual Story-Writing, Narrative Text Editing, User Interface Design

**Relevance Score:** 9

**TL;DR:** This paper presents a text editor that utilizes visual representations to aid in writing and revising narrative texts, supporting creativity through interactive visualizations of story elements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance narrative writing and revision by integrating visual tools that support the writing process and encourage creativity.

**Method:** Developed a text editor that visualizes entity interactions, movements, and timelines in narrative texts, suggesting edits based on user interactions with these visualizations.

**Key Contributions:**

	1. Development of a novel text editor integrating visual story elements.
	2. User studies demonstrating the benefits of visualization in narrative writing.
	3. Insights into how visuals can aid in narrative planning and revision.

**Result:** User studies showed that visualizations helped participants in planning revisions, tracking story elements, and exploring narrative variations, ultimately fostering creativity.

**Limitations:** 

**Conclusion:** Visual story-writing tools can significantly aid in the narrative writing process by providing interactive visual aids that enhance communication and creativity.

**Abstract:** We define "visual story-writing" as using visual representations of story elements to support writing and revising narrative texts. To demonstrate this approach, we developed a text editor that automatically visualizes a graph of entity interactions, movement between locations, and a timeline of story events. Interacting with these visualizations results in suggested text edits: for example, connecting two characters in the graph creates an interaction between them, moving an entity updates their described location, and rearranging events on the timeline reorganizes the narrative sequence. Through two user studies on narrative text editing and writing, we found that visuals supported participants in planning high-level revisions, tracking story elements, and exploring story variations in ways that encourage creativity. Broadly, our work lays the foundation for writing support, not just through words, but also visuals.

</details>


### [31] [Hybrid EEG--Driven Brain--Computer Interface: A Large Language Model Framework for Personalized Language Rehabilitation](https://arxiv.org/abs/2507.22892)

*Ismail Hossain, Mridul Banik*

**Main category:** cs.HC

**Keywords:** Augmentative and Alternative Communication, Brain-Computer Interfaces, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper proposes a hybrid framework utilizing EEG signals and LLMs for language rehabilitation in users with severe impairments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional AAC systems lack real-time adaptability to users' cognitive and linguistic needs, particularly for those with neurological conditions.

**Method:** The proposed framework integrates EEG-based BCIs to interpret neural intent and LLMs to provide dynamic language-learning content.

**Key Contributions:**

	1. Development of a hybrid framework combining EEG and LLMs for language rehabilitation.
	2. Real-time personalization of language exercises based on user cognitive state.
	3. Dynamic adjustment of task difficulty based on neural markers.

**Result:** Preliminary evaluations indicate that the hybrid system effectively personalizes language exercises and monitors cognitive effort in real-time.

**Limitations:** The current study may require larger sample sizes for robust validation and long-term adaptation assessments.

**Conclusion:** The integration of EEG with LLMs holds promise for enhancing AAC systems, enabling better support for users with severe speech or motor impairments.

**Abstract:** Conventional augmentative and alternative communication (AAC) systems and language-learning platforms often fail to adapt in real time to the user's cognitive and linguistic needs, especially in neurological conditions such as post-stroke aphasia or amyotrophic lateral sclerosis. Recent advances in noninvasive electroencephalography (EEG)--based brain-computer interfaces (BCIs) and transformer--based large language models (LLMs) offer complementary strengths: BCIs capture users' neural intent with low fatigue, while LLMs generate contextually tailored language content. We propose and evaluate a novel hybrid framework that leverages real-time EEG signals to drive an LLM-powered language rehabilitation assistant. This system aims to: (1) enable users with severe speech or motor impairments to navigate language-learning modules via mental commands; (2) dynamically personalize vocabulary, sentence-construction exercises, and corrective feedback; and (3) monitor neural markers of cognitive effort to adjust task difficulty on the fly.

</details>


### [32] [Voice-guided Orchestrated Intelligence for Clinical Evaluation (VOICE): A Voice AI Agent System for Prehospital Stroke Assessment](https://arxiv.org/abs/2507.22898)

*Julian Acosta, Scott Adams, Julius Kernbach, Romain Hardy, Sung Eun Kim, Luyang Luo, Xiaoman Zhang, Shreya Johri, Mohammed Baharoon, Pranav Rajpurkar*

**Main category:** cs.HC

**Keywords:** Stroke evaluation, Voice-driven AI, Emergency care, Diagnostic accuracy, Human oversight

**Relevance Score:** 9

**TL;DR:** Development of a voice-driven AI system to improve stroke evaluations in emergency care.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current stroke recognition by first responders is inconsistent and often inaccurate, leading to delays in treatment.

**Method:** Non-medical volunteers used the AI system to assess simulated stroke patients and measured accuracy, completion times, user confidence, and expert reviews.

**Key Contributions:**

	1. Voice-driven AI for stroke evaluation
	2. High accuracy in identifying stroke signs
	3. Documenting evaluations through smartphone video

**Result:** The AI system identified 84% of stroke signs and 75% of likely LVOs, with overall stroke detection accuracy at 86%.

**Limitations:** Human oversight is necessary due to AI errors affecting scoring and false information.

**Conclusion:** While current limitations require human oversight, advancements in AI may soon enable highly accurate assessments and improve emergency care.

**Abstract:** We developed a voice-driven artificial intelligence (AI) system that guides anyone - from paramedics to family members - through expert-level stroke evaluations using natural conversation, while also enabling smartphone video capture of key examination components for documentation and potential expert review. This addresses a critical gap in emergency care: current stroke recognition by first responders is inconsistent and often inaccurate, with sensitivity for stroke detection as low as 58%, causing life-threatening delays in treatment. Three non-medical volunteers used our AI system to assess ten simulated stroke patients, including cases with likely large vessel occlusion (LVO) strokes and stroke-like conditions, while we measured diagnostic accuracy, completion times, user confidence, and expert physician review of the AI-generated reports. The AI system correctly identified 84% of individual stroke signs and detected 75% of likely LVOs, completing evaluations in just over 6 minutes. Users reported high confidence (median 4.5/5) and ease of use (mean 4.67/5). The system successfully identified 86% of actual strokes but also incorrectly flagged 2 of 3 non-stroke cases as strokes. When an expert physician reviewed the AI reports with videos, they identified the correct diagnosis in 100% of cases, but felt confident enough to make preliminary treatment decisions in only 40% of cases due to observed AI errors including incorrect scoring and false information. While the current system's limitations necessitate human oversight, ongoing rapid advancements in speech-to-speech AI models suggest that future versions are poised to enable highly accurate assessments. Achieving human-level voice interaction could transform emergency medical care, putting expert-informed assessment capabilities in everyone's hands.

</details>


### [33] [Toward the Autonomous AI Doctor: Quantitative Benchmarking of an Autonomous Agentic AI Versus Board-Certified Clinicians in a Real World Setting](https://arxiv.org/abs/2507.22902)

*Hashim Hayat, Maksim Kudrautsau, Evgeniy Makarov, Vlad Melnichenko, Tim Tsykunou, Piotr Varaksin, Matt Pavelle, Adam Z. Oskowitz*

**Main category:** cs.HC

**Keywords:** Artificial Intelligence, Health Informatics, Large Language Models, Telehealth, Clinical Decision-Making

**Relevance Score:** 9

**TL;DR:** A study evaluating a multi-agent AI framework as an autonomous AI doctor in urgent care describes strong performance in diagnostics and treatment parallels with human clinicians.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the projected shortage of healthcare practitioners and aims to explore the potential of AI to alleviate the administrative burden in clinical settings by evaluating an autonomous LLM-based AI system in a real-world context.

**Method:** The study retrospectively compared the multi-agent AI system Doctronic with board-certified clinicians through 500 consecutive urgent-care telehealth encounters, focusing on diagnostic concordance, treatment plan consistency, and safety metrics.

**Key Contributions:**

	1. First large-scale validation of an autonomous AI doctor
	2. Demonstrated strong diagnostic and treatment alignment with clinicians
	3. Highlighted the potential of multi-agent AI to ease healthcare workforce shortage

**Result:** Doctronic matched clinicians in the top diagnosis 81% of the time and aligned treatment plans in 99.2% of cases, with no hallucinations noted. AI outperformed human clinicians in 36.1% of discordant cases.

**Limitations:** 

**Conclusion:** The study shows that autonomous AI systems can achieve diagnostic and treatment decision-making levels comparable to human healthcare providers, addressing workforce shortages.

**Abstract:** Background: Globally we face a projected shortage of 11 million healthcare practitioners by 2030, and administrative burden consumes 50% of clinical time. Artificial intelligence (AI) has the potential to help alleviate these problems. However, no end-to-end autonomous large language model (LLM)-based AI system has been rigorously evaluated in real-world clinical practice. In this study, we evaluated whether a multi-agent LLM-based AI framework can function autonomously as an AI doctor in a virtual urgent care setting. Methods: We retrospectively compared the performance of the multi-agent AI system Doctronic and board-certified clinicians across 500 consecutive urgent-care telehealth encounters. The primary end points: diagnostic concordance, treatment plan consistency, and safety metrics, were assessed by blinded LLM-based adjudication and expert human review. Results: The top diagnosis of Doctronic and clinician matched in 81% of cases, and the treatment plan aligned in 99.2% of cases. No clinical hallucinations occurred (e.g., diagnosis or treatment not supported by clinical findings). In an expert review of discordant cases, AI performance was superior in 36.1%, and human performance was superior in 9.3%; the diagnoses were equivalent in the remaining cases. Conclusions: In this first large-scale validation of an autonomous AI doctor, we demonstrated strong diagnostic and treatment plan concordance with human clinicians, with AI performance matching and in some cases exceeding that of practicing clinicians. These findings indicate that multi-agent AI systems achieve comparable clinical decision-making to human providers and offer a potential solution to healthcare workforce shortages.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [34] [Large Language Models in the Travel Domain: An Industrial Experience](https://arxiv.org/abs/2507.22910)

*Sergio Di Meglio, Aniello Somma, Luigi Libero Lucio Starace, Fabio Scippacercola, Giancarlo Sperlì, Sergio Di Martino*

**Main category:** cs.CL

**Keywords:** Large Language Models, property booking, data consistency, LLM performance, accommodation data

**Relevance Score:** 8

**TL;DR:** This paper discusses the integration of Large Language Models into a property booking platform to improve data consistency and quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Online property booking platforms face challenges with inconsistent and incomplete data from third-party sources, which can lead to user frustration and market loss.

**Method:** The study evaluates two LLMs, Mistral 7B and Mixtral 8x7B, by assessing their ability to generate consistent and concise descriptions of accommodation facilities with minimal hallucinations.

**Key Contributions:**

	1. Integration of LLMs in property booking systems
	2. Comparative analysis of Mistral 7B and Mixtral 8x7B
	3. Insights on model quality vs. resource efficiency

**Result:** Mixtral 8x7B outperformed Mistral 7B in completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), while also producing shorter content on average (249 vs. 277 words).

**Limitations:** Higher computational cost associated with Mixtral 8x7B compared to Mistral 7B.

**Conclusion:** These results highlight the trade-offs between LLM quality and computational efficiency, providing guidance for deploying LLMs in production environments for improved data reliability.

**Abstract:** Online property booking platforms are widely used and rely heavily on consistent, up-to-date information about accommodation facilities, often sourced from third-party providers. However, these external data sources are frequently affected by incomplete or inconsistent details, which can frustrate users and result in a loss of market. In response to these challenges, we present an industrial case study involving the integration of Large Language Models (LLMs) into CALEIDOHOTELS, a property reservation platform developed by FERVENTO. We evaluate two well-known LLMs in this context: Mistral 7B, fine-tuned with QLoRA, and Mixtral 8x7B, utilized with a refined system prompt. Both models were assessed based on their ability to generate consistent and homogeneous descriptions while minimizing hallucinations. Mixtral 8x7B outperformed Mistral 7B in terms of completeness (99.6% vs. 93%), precision (98.8% vs. 96%), and hallucination rate (1.2% vs. 4%), producing shorter yet more concise content (249 vs. 277 words on average). However, this came at a significantly higher computational cost: 50GB VRAM and $1.61/hour versus 5GB and $0.16/hour for Mistral 7B. Our findings provide practical insights into the trade-offs between model quality and resource efficiency, offering guidance for deploying LLMs in production environments and demonstrating their effectiveness in enhancing the consistency and reliability of accommodation data.

</details>


### [35] [ElectriQ: A Benchmark for Assessing the Response Capability of Large Language Models in Power Marketing](https://arxiv.org/abs/2507.22911)

*Jinzhi Wang, Qingke Peng, Haozhou Li, Zeyuan Zeng, Qinfeng Song, Kaixuan Yang, Jiangbo Zhang, Yaoying Wang, Ruimeng Li, Biyi Zhou*

**Main category:** cs.CL

**Keywords:** Electric power marketing, Large language models, Benchmark, Customer service, Knowledge augmentation

**Relevance Score:** 6

**TL;DR:** Introducing ElectriQ, a benchmark for evaluating and enhancing LLMs for electric power marketing customer service, addressing limitations of existing systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Electric power marketing customer service is crucial for addressing inquiries and complaints, yet current systems suffer from slow responses and inflexible procedures. The need for domain-specific capabilities and empathy in LLMs drives this research.

**Method:** ElectriQ provides a dialogue dataset across six service categories and employs four evaluation metrics: professionalism, popularity, readability, and user-friendliness. A domain-specific knowledge base and a knowledge augmentation method are used to improve model performance.

**Key Contributions:**

	1. Introduction of the ElectriQ benchmark for LLMs in power marketing
	2. Development of a dialogue dataset and evaluation metrics tailored for customer service
	3. Knowledge augmentation method to enhance model performance

**Result:** Experiments showed that smaller fine-tuned models like LLama3-8B can outperform larger models like GPT-4o in professionalism and user-friendliness after knowledge augmentation.

**Limitations:** Limited to electric power marketing domain; results may not generalize to other sectors.

**Conclusion:** ElectriQ lays the groundwork for developing LLMs that are specifically designed to improve customer service in electric power marketing, highlighting the importance of contextual understanding and user engagement.

**Abstract:** Electric power marketing customer service plays a critical role in addressing inquiries, complaints, and service requests. However, current systems, such as China's 95598 hotline, often struggle with slow response times, inflexible procedures, and limited accuracy in domain-specific tasks. While large language models (LLMs) like GPT-4o and Claude 3 demonstrate strong general capabilities, they lack the domain expertise and empathy required in this field. To bridge this gap, we introduce ElectriQ, the first benchmark designed to evaluate and enhance LLMs in electric power marketing scenarios. ElectriQ consists of a dialogue dataset covering six key service categories and introduces four evaluation metrics: professionalism, popularity, readability, and user-friendliness. We further incorporate a domain-specific knowledge base and propose a knowledge augmentation method to boost model performance. Experiments on 13 LLMs reveal that smaller models such as LLama3-8B, when fine-tuned and augmented, can surpass GPT-4o in terms of professionalism and user-friendliness. ElectriQ establishes a comprehensive foundation for developing LLMs tailored to the needs of power marketing services.

</details>


### [36] [A Language Model-Driven Semi-Supervised Ensemble Framework for Illicit Market Detection Across Deep/Dark Web and Social Platforms](https://arxiv.org/abs/2507.22912)

*Navid Yazdanjue, Morteza Rakhshaninejad, Hossein Yazdanjouei, Mohammad Sadegh Khorshidi, Mikko S. Niemela, Fang Chen, Amir H. Gandomi*

**Main category:** cs.CL

**Keywords:** illicit content detection, language models, semi-supervised learning, deep web, dark web

**Relevance Score:** 3

**TL;DR:** This paper presents a hierarchical classification framework for detecting and classifying illicit marketplace content on the deep and dark web using fine-tuned language models and a semi-supervised learning strategy.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing shift of illegal marketplaces to the deep and dark web complicates the detection and classification of illicit content due to limited labeled data and evolving language.

**Method:** A hierarchical classification framework is proposed that combines fine-tuned language models such as ModernBERT with a semi-supervised ensemble approach utilizing XGBoost, Random Forest, and SVM for document detection and classification.

**Key Contributions:**

	1. Development of a hierarchical classification framework combining language models with semi-supervised learning
	2. Use of ModernBERT fine-tuned on domain-specific data
	3. Incorporation of engineered features like document structure and embedded patterns.

**Result:** The proposed model outperforms several baseline models, achieving an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388.

**Limitations:** 

**Conclusion:** The framework demonstrates strong generalization and robustness for detecting illicit content with limited supervision, proving effective in real-world applications.

**Abstract:** Illegal marketplaces have increasingly shifted to concealed parts of the internet, including the deep and dark web, as well as platforms such as Telegram, Reddit, and Pastebin. These channels enable the anonymous trade of illicit goods including drugs, weapons, and stolen credentials. Detecting and categorizing such content remains challenging due to limited labeled data, the evolving nature of illicit language, and the structural heterogeneity of online sources. This paper presents a hierarchical classification framework that combines fine-tuned language models with a semi-supervised ensemble learning strategy to detect and classify illicit marketplace content across diverse platforms. We extract semantic representations using ModernBERT, a transformer model for long documents, finetuned on domain-specific data from deep and dark web pages, Telegram channels, Subreddits, and Pastebin pastes to capture specialized jargon and ambiguous linguistic patterns. In addition, we incorporate manually engineered features such as document structure, embedded patterns including Bitcoin addresses, emails, and IPs, and metadata, which complement language model embeddings. The classification pipeline operates in two stages. The first stage uses a semi-supervised ensemble of XGBoost, Random Forest, and SVM with entropy-based weighted voting to detect sales-related documents. The second stage further classifies these into drug, weapon, or credential sales. Experiments on three datasets, including our multi-source corpus, DUTA, and CoDA, show that our model outperforms several baselines, including BERT, ModernBERT, DarkBERT, ALBERT, Longformer, and BigBird. The model achieves an accuracy of 0.96489, an F1-score of 0.93467, and a TMCC of 0.95388, demonstrating strong generalization, robustness under limited supervision, and effectiveness in real-world illicit content detection.

</details>


### [37] [A Hybrid Framework for Subject Analysis: Integrating Embedding-Based Regression Models with Large Language Models](https://arxiv.org/abs/2507.22913)

*Jinyu Liu, Xiaoying Song, Diana Zhang, Jason Thomale, Daqing He, Lingzi Hong*

**Main category:** cs.CL

**Keywords:** hybrid framework, large language models, subject analysis, multi-label classification, LCSH

**Relevance Score:** 7

**TL;DR:** This paper proposes a hybrid framework integrating ML models and LLMs for improved subject analysis in library management systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance subject access in library systems using LLMs, addressing their limitations in subject analysis.

**Method:** A hybrid framework combining embedding-based ML models with LLMs, using ML for initial predictions of LCSH labels and LLMs for post-editing these predictions.

**Key Contributions:**

	1. Development of a hybrid framework for subject analysis
	2. Improved predictions for Library of Congress Subject Headings
	3. Mitigation of LLM hallucination effects

**Result:** A controlled and vocabulary-aligned prediction of subject terms for books, outperforming traditional ML-only models.

**Limitations:** The study may be limited by the specific dataset of books and LCSH used in experiments.

**Conclusion:** The hybrid approach demonstrates better accuracy and reliability in LCSH term predictions compared to solely relying on LLMs.

**Abstract:** Providing subject access to information resources is an essential function of any library management system. Large language models (LLMs) have been widely used in classification and summarization tasks, but their capability to perform subject analysis is underexplored. Multi-label classification with traditional machine learning (ML) models has been used for subject analysis but struggles with unseen cases. LLMs offer an alternative but often over-generate and hallucinate. Therefore, we propose a hybrid framework that integrates embedding-based ML models with LLMs. This approach uses ML models to (1) predict the optimal number of LCSH labels to guide LLM predictions and (2) post-edit the predicted terms with actual LCSH terms to mitigate hallucinations. We experimented with LLMs and the hybrid framework to predict the subject terms of books using the Library of Congress Subject Headings (LCSH). Experiment results show that providing initial predictions to guide LLM generations and imposing post-edits result in more controlled and vocabulary-aligned outputs.

</details>


### [38] [Full Triple Matcher: Integrating all triple elements between heterogeneous Knowledge Graphs](https://arxiv.org/abs/2507.22914)

*Victor Eiti Yamamoto, Hideaki Takeda*

**Main category:** cs.CL

**Keywords:** knowledge graphs, context matching, entity matching, KG integration, triple matching

**Relevance Score:** 4

**TL;DR:** The paper presents a novel method for knowledge graph integration focusing on context matching, addressing limitations in existing entity matching approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in knowledge graph integration related to context matching, which remains largely unexplored despite its importance in handling diverse real-world KGs.

**Method:** The proposed method consists of label matching and triple matching using string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels and identify mappings between triples conveying comparable information.

**Key Contributions:**

	1. Novel approach to context matching in knowledge graph integration
	2. Use of string manipulation, fuzzy matching, and vector similarity for entity label alignment
	3. Introduction of a new dataset for evaluating triple matching

**Result:** The method demonstrates competitive performance in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases.

**Limitations:** 

**Conclusion:** The novel KG integration method improves entity-matching accuracy and is validated using a new dataset for comprehensive evaluation of the triple-matching step.

**Abstract:** Knowledge graphs (KGs) are powerful tools for representing and reasoning over structured information. Their main components include schema, identity, and context. While schema and identity matching are well-established in ontology and entity matching research, context matching remains largely unexplored. This is particularly important because real-world KGs often vary significantly in source, size, and information density - factors not typically represented in the datasets on which current entity matching methods are evaluated. As a result, existing approaches may fall short in scenarios where diverse and complex contexts need to be integrated.   To address this gap, we propose a novel KG integration method consisting of label matching and triple matching. We use string manipulation, fuzzy matching, and vector similarity techniques to align entity and predicate labels. Next, we identify mappings between triples that convey comparable information, using these mappings to improve entity-matching accuracy. Our approach demonstrates competitive performance compared to leading systems in the OAEI competition and against supervised methods, achieving high accuracy across diverse test cases. Additionally, we introduce a new dataset derived from the benchmark dataset to evaluate the triple-matching step more comprehensively.

</details>


### [39] [Theoretical Foundations and Mitigation of Hallucination in Large Language Models](https://arxiv.org/abs/2507.22915)

*Esmail Gumaan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination, Detection Strategies, Mitigation Techniques, Evaluation Protocols

**Relevance Score:** 9

**TL;DR:** This paper rigorously explores hallucination in Large Language Models (LLMs), defining it, analyzing its risks, and proposing detection and mitigation strategies.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of hallucination in LLMs, which generates content not aligned with factual information, impacting the reliability of AI systems.

**Method:** Formal definitions are provided for intrinsic and extrinsic hallucinations, and a 'hallucination risk' metric is derived using PAC-Bayes and Rademacher complexity. A survey of detection strategies and mitigation techniques is also included.

**Key Contributions:**

	1. Rigorous definitions and classifications of hallucination in LLMs.
	2. Derivation of bounds on hallucination risk using learning-theoretic frameworks.
	3. Proposed unified workflow for detection and mitigation strategies for hallucinations.

**Result:** A unified detection and mitigation workflow is proposed to integrate strategies for addressing hallucination, along with recommended evaluation protocols.

**Limitations:** 

**Conclusion:** The paper lays a theoretical foundation and offers practical guidelines to reduce hallucinations, enhancing the reliability of LLMs.

**Abstract:** Hallucination in Large Language Models (LLMs) refers to the generation of content that is not faithful to the input or the real-world facts. This paper provides a rigorous treatment of hallucination in LLMs, including formal definitions and theoretical analyses. We distinguish between intrinsic and extrinsic hallucinations, and define a \textit{hallucination risk} for models. We derive bounds on this risk using learning-theoretic frameworks (PAC-Bayes and Rademacher complexity). We then survey detection strategies for hallucinations, such as token-level uncertainty estimation, confidence calibration, and attention alignment checks. On the mitigation side, we discuss approaches including retrieval-augmented generation, hallucination-aware fine-tuning, logit calibration, and the incorporation of fact-verification modules. We propose a unified detection and mitigation workflow, illustrated with a diagram, to integrate these strategies. Finally, we outline evaluation protocols for hallucination, recommending datasets, metrics, and experimental setups to quantify and reduce hallucinations. Our work lays a theoretical foundation and practical guidelines for addressing the crucial challenge of hallucination in LLMs.

</details>


### [40] [Reading Between the Timelines: RAG for Answering Diachronic Questions](https://arxiv.org/abs/2507.22917)

*Kwun Hang Lau, Ruiyuan Zhang, Weijie Shi, Xiaofang Zhou, Xiaojun Cheng*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, temporal logic, longitudinal queries, Analytical Diachronic Question Answering Benchmark, accuracy

**Relevance Score:** 9

**TL;DR:** The paper proposes a new framework for Retrieval-Augmented Generation (RAG) that incorporates temporal logic to effectively handle longitudinal queries, improving accuracy in answer retrieval by redesigning the RAG pipeline.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of conventional retrieval methods in handling temporal queries, which require tracking entities and phenomena over time.

**Method:** The methodology involves disentangling a user's query into its subject and temporal window and using a specialized retriever that matches semantic and temporal relevance to collect a contiguous evidence set over the queried period.

**Key Contributions:**

	1. Introduction of a temporal logic framework into RAG systems
	2. Development of the Analytical Diachronic Question Answering Benchmark (ADQAB)
	3. Demonstrated significant accuracy improvements in answer retrieval for temporal queries

**Result:** Empirical results show that the proposed approach improves answer accuracy by 13% to 27% compared to standard RAG implementations on the Analytical Diachronic Question Answering Benchmark (ADQAB).

**Limitations:** 

**Conclusion:** This work validates a pathway for enhancing RAG systems to perform complex, real-world longitudinal analysis.

**Abstract:** While Retrieval-Augmented Generation (RAG) excels at injecting static, factual knowledge into Large Language Models (LLMs), it exhibits a critical deficit in handling longitudinal queries that require tracking entities and phenomena across time. This blind spot arises because conventional, semantically-driven retrieval methods are not equipped to gather evidence that is both topically relevant and temporally coherent for a specified duration. We address this challenge by proposing a new framework that fundamentally redesigns the RAG pipeline to infuse temporal logic. Our methodology begins by disentangling a user's query into its core subject and its temporal window. It then employs a specialized retriever that calibrates semantic matching against temporal relevance, ensuring the collection of a contiguous evidence set that spans the entire queried period. To enable rigorous evaluation of this capability, we also introduce the Analytical Diachronic Question Answering Benchmark (ADQAB), a challenging evaluation suite grounded in a hybrid corpus of real and synthetic financial news. Empirical results on ADQAB show that our approach yields substantial gains in answer accuracy, surpassing standard RAG implementations by 13% to 27%. This work provides a validated pathway toward RAG systems capable of performing the nuanced, evolutionary analysis required for complex, real-world questions. The dataset and code for this study are publicly available at https://github.com/kwunhang/TA-RAG.

</details>


### [41] [Semantic Convergence: Investigating Shared Representations Across Scaled LLMs](https://arxiv.org/abs/2507.22918)

*Daniel Son, Sanjana Rathore, Andrew Rufail, Adrian Simon, Daniel Zhang, Soham Dave, Cole Blondin, Kevin Zhu, Sean O'Brien*

**Main category:** cs.CL

**Keywords:** Feature Universality, Language Models, Sparse Autoencoders

**Relevance Score:** 7

**TL;DR:** The paper investigates feature universality in Gemma-2 language models of differing scales, finding that they converge on comparable internal concepts, particularly in middle layers.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether language models of different sizes (Gemma-2-2B and Gemma-2-9B) converge on similar internal representations, thus supporting the notion of universality in language model interpretability.

**Method:** The study uses Sparse Autoencoders (SAEs) to analyze residual-stream activations of both models, aligning monosemantic features through activation correlation and comparing feature spaces using SVCCA and RSA.

**Key Contributions:**

	1. Demonstrated feature overlap in middle layers of differently sized models
	2. Extended analysis to multi-token subspaces
	3. Strengthened the argument for universality in language model features

**Result:** Findings indicate that middle layers of both models show significant overlap in features, while early and late layers exhibit less similarity. Multi-token subspace analysis reveals interactions between semantically similar subspaces.

**Limitations:** 

**Conclusion:** The research supports the idea that large language models develop broadly similar, interpretable features regardless of their size, suggesting universality as key for cross-model interpretability.

**Abstract:** We investigate feature universality in Gemma-2 language models (Gemma-2-2B and Gemma-2-9B), asking whether models with a four-fold difference in scale still converge on comparable internal concepts. Using the Sparse Autoencoder (SAE) dictionary-learning pipeline, we utilize SAEs on each model's residual-stream activations, align the resulting monosemantic features via activation correlation, and compare the matched feature spaces with SVCCA and RSA. Middle layers yield the strongest overlap, while early and late layers show far less similarity. Preliminary experiments extend the analysis from single tokens to multi-token subspaces, showing that semantically similar subspaces interact similarly with language models. These results strengthen the case that large language models carve the world into broadly similar, interpretable features despite size differences, reinforcing universality as a foundation for cross-model interpretability.

</details>


### [42] [A novel language model for predicting serious adverse event results in clinical trials from their prospective registrations](https://arxiv.org/abs/2507.22919)

*Qixuan Hu, Xumou Zhang, Jinman Kim, Florence Bourgeois, Adam G. Dunn*

**Main category:** cs.CL

**Keywords:** Serious Adverse Events, Clinical Trials, Predictive Modeling, Transfer Learning, Health Informatics

**Relevance Score:** 8

**TL;DR:** The paper evaluates methods for predicting serious adverse events (SAEs) in clinical trials using registration data and pretrained language models, aiming to improve trial design.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate predictions of safety results can guide clinical trial design to reduce terminations and risks to participants.

**Method:** Analyzed 22,107 clinical trials from ClinicalTrials.gov; developed a classifier and regression model using a sliding window method with pretrained models (ClinicalT5, BioBERT) for feature extraction.

**Key Contributions:**

	1. Development of a sliding window method for embedding extraction from long texts
	2. Use of pretrained language models for feature extraction in clinical trial data
	3. Establishment of prediction models for SAEs using trial registration data.

**Result:** The best model achieved 77.6% AUC for predicting higher SAE rates in the experimental arm and RMSE of 18.6% for predicting control arm SAEs, with the sliding window approach proving effective.

**Limitations:** 

**Conclusion:** Utilizing summary results data can enhance clinical trial design and highlight discrepancies in safety result expectations and reports.

**Abstract:** Objectives: With accurate estimates of expected safety results, clinical trials could be designed to avoid terminations and limit exposing participants to unnecessary risks. We evaluated methods for predicting serious adverse event (SAE) results in clinical trials using information only from their registrations prior to the trial. Material and Methods: We analysed 22,107 two-arm parallel interventional clinical trials from ClinicalTrials.gov with structured summary results. Two prediction models were developed: a classifier predicting will experimental arm have higher SAE rates (area under the receiver operating characteristic curve; AUC) than control arm, and a regression model to predict the proportion of SAEs in control arms (root mean squared error; RMSE). A transfer learning approach using pretrained language models (e.g., ClinicalT5, BioBERT) was used for feature extraction, combined with downstream model for prediction. To maintain semantic representation in long trial texts exceeding localised language model input limits, a sliding window method was developed for embedding extraction. Results: The best model (ClinicalT5+Transformer+MLP) had 77.6% AUC predicting which trial arm has a higher proportion of patients with SAEs. When predicting proportion of participants experiencing SAE in the control arm, the same model achieved RMSE of 18.6%. The sliding window approach consistently outperformed methods without it. Across 12 classifiers, the average absolute AUC increase was 2.00%; across 12 regressors, the average absolute RMSE reduction was 1.58%. Discussion: Summary results data available at ClinicalTrials.gov remains underutilised. The potential to estimate results of trials before they start is an opportunity to improve trial design and flag discrepancies between expected and reported safety results.

</details>


### [43] [Discrete Tokenization for Multimodal LLMs: A Comprehensive Survey](https://arxiv.org/abs/2507.22920)

*Jindong Li, Yali Fu, Jiahong Liu, Linxiao Cao, Wei Ji, Menglin Yang, Irwin King, Ming-Hsuan Yang*

**Main category:** cs.CL

**Keywords:** vector quantization, large language models, discrete tokenization, multimodal systems, machine learning

**Relevance Score:** 9

**TL;DR:** The paper presents a comprehensive survey on discrete tokenization methods for large language models (LLMs), focusing on vector quantization (VQ) techniques, their classifications, challenges, and emerging research directions.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for effective methods to transform continuous multimodal data into discrete forms suitable for LLMs, a comprehensive survey on vector quantization techniques is conducted.

**Method:** The paper categorizes 8 representative variants of vector quantization, analyzes their principles, training dynamics, and challenges when integrated into LLM pipelines.

**Key Contributions:**

	1. First structured taxonomy of discrete tokenization methods for LLMs
	2. In-depth analysis of 8 VQ variants concerning LLM integration
	3. Identification of key challenges and emerging research directions in tokenization for LLMs

**Result:** The analysis reveals how quantization strategies impact alignment, reasoning, and generation performance in LLMs, and identifies key challenges such as codebook collapse and unstable gradient estimation.

**Limitations:** The survey may not cover all potential VQ methods and their applications across every modality.

**Conclusion:** This work serves as a foundational reference for developing efficient multimodal systems by connecting traditional vector quantization methods with modern applications in LLMs.

**Abstract:** The rapid advancement of large language models (LLMs) has intensified the need for effective mechanisms to transform continuous multimodal data into discrete representations suitable for language-based processing. Discrete tokenization, with vector quantization (VQ) as a central approach, offers both computational efficiency and compatibility with LLM architectures. Despite its growing importance, there is a lack of a comprehensive survey that systematically examines VQ techniques in the context of LLM-based systems. This work fills this gap by presenting the first structured taxonomy and analysis of discrete tokenization methods designed for LLMs. We categorize 8 representative VQ variants that span classical and modern paradigms and analyze their algorithmic principles, training dynamics, and integration challenges with LLM pipelines. Beyond algorithm-level investigation, we discuss existing research in terms of classical applications without LLMs, LLM-based single-modality systems, and LLM-based multimodal systems, highlighting how quantization strategies influence alignment, reasoning, and generation performance. In addition, we identify key challenges including codebook collapse, unstable gradient estimation, and modality-specific encoding constraints. Finally, we discuss emerging research directions such as dynamic and task-adaptive quantization, unified tokenization frameworks, and biologically inspired codebook learning. This survey bridges the gap between traditional vector quantization and modern LLM applications, serving as a foundational reference for the development of efficient and generalizable multimodal systems. A continuously updated version is available at: https://github.com/jindongli-Ai/LLM-Discrete-Tokenization-Survey.

</details>


### [44] [Fast and Accurate Contextual Knowledge Extraction Using Cascading Language Model Chains and Candidate Answers](https://arxiv.org/abs/2507.22921)

*Lee Harris*

**Main category:** cs.CL

**Keywords:** Language Model Chain, hallucinations, knowledge extraction, medical documents, multi-stage cascade

**Relevance Score:** 9

**TL;DR:** Introduction of the Language Model Chain (LMC) algorithm to enhance accuracy and reduce hallucinations in language model outputs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of high cost and hallucinations in language models, especially in the context of extracting information from text.

**Method:** The LMC algorithm utilizes a multi-stage cascade of language models to ensure responses are only correct if they exist in a candidate answer set, improving both speed and accuracy in predictions.

**Key Contributions:**

	1. Introduction of the LMC algorithm
	2. Improved accuracy and speed in language model responses
	3. Significant reduction in hallucinations in outputs

**Result:** The application of the LMC algorithm to extract patient dates of birth from medical documents resulted in significantly increased prediction speed and accuracy while reducing hallucinations.

**Limitations:** 

**Conclusion:** The LMC algorithm shows promise in advancing knowledge extraction, warranting further exploration.

**Abstract:** Language models can capture complex relationships in given text, but these are notorious for being costly and for producing information that does not exist (i.e., hallucinations). Furthermore, the resources invested into producing this information would be wasted if it were incorrect. We address these issues by proposing, implementing, and applying the Language Model Chain (LMC) algorithm. In this, a language model's response to a given prompt about given text is only correct if it exists in the collection of possible (i.e., candidate) answers, and text corresponding to incorrect responses is fed into a more predictive (but slower) language model. This process is repeated for a collection of language models, or until all predictions about the text are correct. We used the LMC algorithm to extract patient dates of birth from medical documents, and combining a collection of language models in a multi-stage cascade significantly increased prediction speed and accuracy over individual language models, while greatly reducing the number of corresponding hallucinations. We believe that the novel LMC algorithm significantly contributes to the knowledge extraction field, and that this should be explored much further in the future.

</details>


### [45] [Predicting stock prices with ChatGPT-annotated Reddit sentiment](https://arxiv.org/abs/2507.22922)

*Mateusz Kmak, Kamil Chmurzyński, Kamil Matejuk, Paweł Kotzbach, Jan Kocoń*

**Main category:** cs.CL

**Keywords:** sentiment analysis, social media, stock market, retail investors, ChatGPT

**Relevance Score:** 4

**TL;DR:** This paper investigates the predictive power of social media sentiment on stock prices, particularly focusing on discussions from Reddit's r/wallstreetbets related to GameStop and AMC.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study is motivated by the significant retail investor activity on social media, particularly during events like the GameStop short squeeze, raising questions about the impact of online sentiment on stock prices.

**Method:** The authors analyze sentiment using two existing sentiment analysis methods and introduce a novel ChatGPT-annotated, fine-tuned RoBERTa-based model to interpret informal language and emojis typical in social media discussions. Correlation and causality metrics are employed to evaluate the predictive ability of these methods.

**Key Contributions:**

	1. Introduction of a novel sentiment analysis model tailored for informal social media language
	2. Empirical analysis of sentiment's predictive power on stock prices using correlation and causality metrics
	3. Insights into the limitations of conventional sentiment analysis in forecasting market movements

**Result:** The findings reveal that social media sentiment has a weak correlation with stock prices. In contrast, simpler metrics such as comment volume and Google search trends show stronger predictive signals.

**Limitations:** The paper's findings highlight the weak correlation of sentiment with stock prices, indicating limitations in the effectiveness of sentiment analysis for trading predictions.

**Conclusion:** The results indicate that the behavior of retail investors in online discussions is complex, and traditional sentiment analysis may fail to capture the intricacies that influence stock prices.

**Abstract:** The surge of retail investor activity on social media, exemplified by the 2021 GameStop short squeeze, raised questions about the influence of online sentiment on stock prices. This paper explores whether sentiment derived from social media discussions can meaningfully predict stock market movements. We focus on Reddit's r/wallstreetbets and analyze sentiment related to two companies: GameStop (GME) and AMC Entertainment (AMC). To assess sentiment's role, we employ two existing text-based sentiment analysis methods and introduce a third, a ChatGPT-annotated and fine-tuned RoBERTa-based model designed to better interpret the informal language and emojis prevalent in social media discussions. We use correlation and causality metrics to determine these models' predictive power. Surprisingly, our findings suggest that social media sentiment has only a weak correlation with stock prices. At the same time, simpler metrics, such as the volume of comments and Google search trends, exhibit stronger predictive signals. These results highlight the complexity of retail investor behavior and suggest that traditional sentiment analysis may not fully capture the nuances of market-moving online discussions.

</details>


### [46] [How and Where to Translate? The Impact of Translation Strategies in Cross-lingual LLM Prompting](https://arxiv.org/abs/2507.22923)

*Aman Gupta, Yingying Zhuang, Zhou Yu, Ziji Zhang, Anurag Beniwal*

**Main category:** cs.CL

**Keywords:** Multilingual LLMs, Prompt translation strategies, RAG systems

**Relevance Score:** 9

**TL;DR:** This paper evaluates different prompt translation strategies in multilingual retrieval-augmented generation (RAG) systems to improve classification tasks using LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how prompt translation strategies affect performance in multilingual systems using LLMs, particularly when knowledge bases from high-resource languages are shared with low-resource languages.

**Method:** The paper systematically evaluates various prompt translation strategies for classification tasks using multilingual RAG-enhanced LLMs through experimental studies.

**Key Contributions:**

	1. Evaluation of prompt translation strategies for multilingual RAG systems.
	2. Experimental results demonstrate performance improvements in classification tasks.
	3. Advocacy for optimized multilingual resource sharing in low-resource languages.

**Result:** An optimized prompting strategy was found to significantly enhance knowledge sharing across languages, leading to improved performance in downstream classification tasks.

**Limitations:** 

**Conclusion:** The study suggests broader utilization of multilingual resource sharing and cross-lingual prompt optimization for low-resource languages.

**Abstract:** Despite advances in the multilingual capabilities of Large Language Models (LLMs), their performance varies substantially across different languages and tasks. In multilingual retrieval-augmented generation (RAG)-based systems, knowledge bases (KB) are often shared from high-resource languages (such as English) to low-resource ones, resulting in retrieved information from the KB being in a different language than the rest of the context. In such scenarios, two common practices are pre-translation to create a mono-lingual prompt and cross-lingual prompting for direct inference. However, the impact of these choices remains unclear. In this paper, we systematically evaluate the impact of different prompt translation strategies for classification tasks with RAG-enhanced LLMs in multilingual systems. Experimental results show that an optimized prompting strategy can significantly improve knowledge sharing across languages, therefore improve the performance on the downstream classification task. The findings advocate for a broader utilization of multilingual resource sharing and cross-lingual prompt optimization for non-English languages, especially the low-resource ones.

</details>


### [47] [Using Sentiment Analysis to Investigate Peer Feedback by Native and Non-Native English Speakers](https://arxiv.org/abs/2507.22924)

*Brittney Exline, Melanie Duffin, Brittany Harbison, Chrissa da Gomez, David Joyner*

**Main category:** cs.CL

**Keywords:** peer feedback, sentiment analysis, online courses, native vs non-native speakers, language background

**Relevance Score:** 7

**TL;DR:** The paper examines the impact of native vs non-native English speaker status on peer feedback experiences in online computing courses through sentiment analysis of peer reviews.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how language background affects students' peer feedback experiences in online U.S. graduate computing courses.

**Method:** Sentiment analysis using the Twitter-roBERTa-based model on peer reviews from a sample of 500 students, examining ratings related to language background.

**Key Contributions:**

	1. Analyzed peer feedback sentiment using advanced AI models
	2. Identified complex interactions between language background and feedback experiences
	3. Provided insights into the challenges faced by non-native speakers in online courses

**Result:** Native speakers rate feedback less favorably, whereas non-native speakers write more positively but receive less favorable sentiment in return, with significant interactions based on sex and age.

**Limitations:** The study focuses on a specific sample from U.S.-based courses, which may not generalize globally.

**Conclusion:** Language background has a modest but complex effect on peer feedback experiences, influencing the dynamics of how feedback is given and received among students.

**Abstract:** Graduate-level CS programs in the U.S. increasingly enroll international students, with 60.2 percent of master's degrees in 2023 awarded to non-U.S. students. Many of these students take online courses, where peer feedback is used to engage students and improve pedagogy in a scalable manner. Since these courses are conducted in English, many students study in a language other than their first. This paper examines how native versus non-native English speaker status affects three metrics of peer feedback experience in online U.S.-based computing courses. Using the Twitter-roBERTa-based model, we analyze the sentiment of peer reviews written by and to a random sample of 500 students. We then relate sentiment scores and peer feedback ratings to students' language background. Results show that native English speakers rate feedback less favorably, while non-native speakers write more positively but receive less positive sentiment in return. When controlling for sex and age, significant interactions emerge, suggesting that language background plays a modest but complex role in shaping peer feedback experiences.

</details>


### [48] [Hierarchical Memory for High-Efficiency Long-Term Reasoning in LLM Agents](https://arxiv.org/abs/2507.22925)

*Haoran Sun, Shaoning Zeng*

**Main category:** cs.CL

**Keywords:** Hierarchical Memory, Large Language Models, Index-based retrieval, Long-term memory, Dialogue systems

**Relevance Score:** 8

**TL;DR:** Proposes a Hierarchical Memory (H-MEM) architecture to enhance long-term memory in LLM Agents for better decision-making and contextual coherence.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Long-term memory influences the reasoning capabilities of Large Language Model Agents, and existing memory approaches struggle with structured memory organization and efficient retrieval.

**Method:** Introduces a H-MEM architecture that organizes memory in a multi-level fashion, integrates positional indexing, and uses an index-based routing mechanism for efficient retrieval.

**Key Contributions:**

	1. Hierarchical Memory (H-MEM) architecture designed for LLM Agents
	2. Multi-level memory organization with semantic abstraction
	3. Index-based routing mechanism for efficient retrieval

**Result:** Experimental results on the LoCoMo dataset show that H-MEM consistently outperforms five baseline methods in long-term dialogue scenarios.

**Limitations:** 

**Conclusion:** The proposed H-MEM architecture significantly enhances the reasoning capabilities of LLM Agents by improving memory organization and retrieval efficiency.

**Abstract:** Long-term memory is one of the key factors influencing the reasoning capabilities of Large Language Model Agents (LLM Agents). Incorporating a memory mechanism that effectively integrates past interactions can significantly enhance decision-making and contextual coherence of LLM Agents. While recent works have made progress in memory storage and retrieval, such as encoding memory into dense vectors for similarity-based search or organizing knowledge in the form of graph, these approaches often fall short in structured memory organization and efficient retrieval. To address these limitations, we propose a Hierarchical Memory (H-MEM) architecture for LLM Agents that organizes and updates memory in a multi-level fashion based on the degree of semantic abstraction. Each memory vector is embedded with a positional index encoding pointing to its semantically related sub-memories in the next layer. During the reasoning phase, an index-based routing mechanism enables efficient, layer-by-layer retrieval without performing exhaustive similarity computations. We evaluate our method on five task settings from the LoCoMo dataset. Experimental results show that our approach consistently outperforms five baseline methods, demonstrating its effectiveness in long-term dialogue scenarios.

</details>


### [49] [Multi-Relation Extraction in Entity Pairs using Global Context](https://arxiv.org/abs/2507.22926)

*Nilesh, Atul Gupta, Avinash C Panday*

**Main category:** cs.CL

**Keywords:** relation extraction, document-level, global context, multi-sentence reasoning, NLP

**Relevance Score:** 6

**TL;DR:** This paper introduces a novel input embedding approach for document-level relation extraction that captures entity positions throughout an entire document, improving prediction accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate prediction of relationships between entities in document-level relation extraction requires understanding the global context beyond the sentences where entities are mentioned.

**Method:** The paper proposes a new input encoding technique that represents entities as standalone segments to leverage global relationships and multi-sentence reasoning.

**Key Contributions:**

	1. Novel input embedding approach for capturing global context
	2. Enhanced relationship detection in document-level settings
	3. Improved accuracy on benchmark datasets for relation extraction

**Result:** The method was tested on three benchmark datasets and showed improved accuracy in predicting relationships between entities in a document-level context.

**Limitations:** The paper does not address the computational complexity of the proposed approach compared to previous methods.

**Conclusion:** The research enhances relationship detection in NLP applications, providing significant theoretical advancements and practical implications for entity-level insights and interpretability.

**Abstract:** In document-level relation extraction, entities may appear multiple times in a document, and their relationships can shift from one context to another. Accurate prediction of the relationship between two entities across an entire document requires building a global context spanning all relevant sentences. Previous approaches have focused only on the sentences where entities are mentioned, which fails to capture the complete document context necessary for accurate relation extraction. Therefore, this paper introduces a novel input embedding approach to capture the positions of mentioned entities throughout the document rather than focusing solely on the span where they appear. The proposed input encoding approach leverages global relationships and multi-sentence reasoning by representing entities as standalone segments, independent of their positions within the document. The performance of the proposed method has been tested on three benchmark relation extraction datasets, namely DocRED, Re-DocRED, and REBEL. The experimental results demonstrated that the proposed method accurately predicts relationships between entities in a document-level setting. The proposed research also has theoretical and practical implications. Theoretically, it advances global context modeling and multi-sentence reasoning in document-level relation extraction. Practically, it enhances relationship detection, enabling improved performance in real-world NLP applications requiring comprehensive entity-level insights and interpretability.

</details>


### [50] [PRGB Benchmark: A Robust Placeholder-Assisted Algorithm for Benchmarking Retrieval-Augmented Generation](https://arxiv.org/abs/2507.22927)

*Zhehao Tan, Yihan Jiao, Dan Yang, Lei Liu, Jie Feng, Duolin Sun, Yue Shen, Jian Wang, Peng Wei, Jinjie Gu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Benchmarking

**Relevance Score:** 9

**TL;DR:** The paper introduces a fine-grained benchmark for evaluating large language models (LLMs) in Retrieval-Augmented Generation (RAG) systems, focusing on specific capabilities rather than overall performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for RAG systems overlook detailed evaluations specific to LLMs, leading to a gap in understanding their capabilities.

**Method:** The study proposes the Placeholder-RAG-Benchmark, which evaluates LLMs on multi-level filtering, combination abilities, and reference reasoning, using a novel placeholder-based approach for analysis.

**Key Contributions:**

	1. Introduction of the Placeholder-RAG-Benchmark for fine-grained LLM evaluation.
	2. A systematic approach to decouple LLM's parametric knowledge from external knowledge.
	3. Insights into the limitations of LLMs in context understanding and error handling.

**Result:** Experiments reveal limitations in current LLMs regarding generation capabilities, especially in areas like error resilience and contextual integrity.

**Limitations:** Limited to assessing only the components involved in RAG systems, not broader LLM capabilities outside of this context.

**Conclusion:** The introduced benchmark facilitates the development of more effective RAG systems by providing a structured evaluation framework focused on LLM contributions.

**Abstract:** Retrieval-Augmented Generation (RAG) enhances large language models (LLMs) by integrating external knowledge, where the LLM's ability to generate responses based on the combination of a given query and retrieved documents is crucial. However, most benchmarks focus on overall RAG system performance, rarely assessing LLM-specific capabilities. Current benchmarks emphasize broad aspects such as noise robustness, but lack a systematic and granular evaluation framework on document utilization. To this end, we introduce \textit{Placeholder-RAG-Benchmark}, a multi-level fine-grained benchmark, emphasizing the following progressive dimensions: (1) multi-level filtering abilities, (2) combination abilities, and (3) reference reasoning. To provide a more nuanced understanding of LLMs' roles in RAG systems, we formulate an innovative placeholder-based approach to decouple the contributions of the LLM's parametric knowledge and the external knowledge. Experiments demonstrate the limitations of representative LLMs in the RAG system's generation capabilities, particularly in error resilience and context faithfulness. Our benchmark provides a reproducible framework for developing more reliable and efficient RAG systems. Our code is available in https://github.com/Alipay-Med/PRGB.

</details>


### [51] [How does Chain of Thought Think? Mechanistic Interpretability of Chain-of-Thought Reasoning with Sparse Autoencoding](https://arxiv.org/abs/2507.22928)

*Xi Chen, Aske Plaat, Niki van Stein*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Large Language Models, interpretability, activation sparsity, causal study

**Relevance Score:** 8

**TL;DR:** A study on the faithfulness of Chain-of-Thought (CoT) prompting in Large Language Models (LLMs) shows that it enhances model interpretability and accuracy on multi-step tasks, especially in larger models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether the thoughts generated by CoT prompting reflect the true internal reasoning of Large Language Models, and to examine the impact of model scale on CoT effectiveness.

**Method:** A causal study utilizing sparse autoencoders and activation patching on Pythia-70M and Pythia-2.8B models while solving math problems, comparing CoT versus noCoT prompting.

**Key Contributions:**

	1. First feature-level causal study of CoT faithfulness in LLMs
	2. Demonstrated scale threshold effects of CoT in model accuracy
	3. Introduced patch-curves and explored feature distribution relevance

**Result:** The study found that CoT prompting significantly improves answer log-probabilities in the 2.8B model and leads to higher internal activation sparsity and interpretability scores, indicating enhanced internal computation structure.

**Limitations:** Study limited to Pythia models; further investigation needed on broader model architectures and tasks.

**Conclusion:** CoT prompting enhances the interpretability and reliability of internal model processes in high-capacity LLMs, validating its effectiveness as a structured prompting technique.

**Abstract:** Chain-of-thought (CoT) prompting boosts Large Language Models accuracy on multi-step tasks, yet whether the generated "thoughts" reflect the true internal reasoning process is unresolved. We present the first feature-level causal study of CoT faithfulness. Combining sparse autoencoders with activation patching, we extract monosemantic features from Pythia-70M and Pythia-2.8B while they tackle GSM8K math problems under CoT and plain (noCoT) prompting. Swapping a small set of CoT-reasoning features into a noCoT run raises answer log-probabilities significantly in the 2.8B model, but has no reliable effect in 70M, revealing a clear scale threshold. CoT also leads to significantly higher activation sparsity and feature interpretability scores in the larger model, signalling more modular internal computation. For example, the model's confidence in generating correct answers improves from 1.2 to 4.3. We introduce patch-curves and random-feature patching baselines, showing that useful CoT information is not only present in the top-K patches but widely distributed. Overall, our results indicate that CoT can induce more interpretable internal structures in high-capacity LLMs, validating its role as a structured prompting method.

</details>


### [52] [EH-Benchmark Ophthalmic Hallucination Benchmark and Agent-Driven Top-Down Traceable Reasoning Workflow](https://arxiv.org/abs/2507.22929)

*Xiaoyu Pan, Yang Bai, Ke Zou, Yang Zhou, Jun Zhou, Huazhu Fu, Yih-Chung Tham, Yong Liu*

**Main category:** cs.CL

**Keywords:** Medical Large Language Models, ophthalmology, hallucinations, benchmark, multimodal data

**Relevance Score:** 9

**TL;DR:** The paper introduces EH-Benchmark, a new ophthalmology benchmark aimed at evaluating and mitigating hallucinations in Medical Large Language Models (MLLMs) utilized for ophthalmic diagnosis.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of Medical Large Language Models in ophthalmic diagnosis, specifically their inaccuracies due to hallucinations and insufficient multimodal data.

**Method:** The authors present a three-phase framework: Knowledge-Level Retrieval, Task-Level Case Studies, and Result-Level Validation to systematically evaluate and reduce hallucinations in MLLMs.

**Key Contributions:**

	1. Introduction of EH-Benchmark for evaluating MLLM hallucinations
	2. Categorization of hallucinations into Visual Understanding and Logical Composition
	3. Proposed framework that improves MLLM performance in ophthalmic tasks

**Result:** The multi-agent framework proposed significantly reduces hallucinations, improving the accuracy, interpretability, and reliability of ophthalmic diagnostics using MLLMs.

**Limitations:** 

**Conclusion:** The EH-Benchmark offers a structured approach to evaluate and mitigate hallucinations in MLLMs, thereby enhancing their diagnostic capabilities in ophthalmology.

**Abstract:** Medical Large Language Models (MLLMs) play a crucial role in ophthalmic diagnosis, holding significant potential to address vision-threatening diseases. However, their accuracy is constrained by hallucinations stemming from limited ophthalmic knowledge, insufficient visual localization and reasoning capabilities, and a scarcity of multimodal ophthalmic data, which collectively impede precise lesion detection and disease diagnosis. Furthermore, existing medical benchmarks fail to effectively evaluate various types of hallucinations or provide actionable solutions to mitigate them. To address the above challenges, we introduce EH-Benchmark, a novel ophthalmology benchmark designed to evaluate hallucinations in MLLMs. We categorize MLLMs' hallucinations based on specific tasks and error types into two primary classes: Visual Understanding and Logical Composition, each comprising multiple subclasses. Given that MLLMs predominantly rely on language-based reasoning rather than visual processing, we propose an agent-centric, three-phase framework, including the Knowledge-Level Retrieval stage, the Task-Level Case Studies stage, and the Result-Level Validation stage. Experimental results show that our multi-agent framework significantly mitigates both types of hallucinations, enhancing accuracy, interpretability, and reliability. Our project is available at https://github.com/ppxy1/EH-Benchmark.

</details>


### [53] [Protecting Vulnerable Voices: Synthetic Dataset Generation for Self-Disclosure Detection](https://arxiv.org/abs/2507.22930)

*Shalini Jangra, Suparna De, Nishanth Sastry, Saeed Fadaei*

**Main category:** cs.CL

**Keywords:** Personal Information Identifiers, Synthetic Data, Privacy Risks, Large Language Models, Social Media

**Relevance Score:** 8

**TL;DR:** The paper presents a novel methodology for generating synthetic data to study Personal Information Identifiers (PIIs) in social media posts, addressing privacy concerns with reproducible research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy risks associated with self-disclosure of Personal Information Identifiers (PIIs) on social platforms and facilitate reproducible research in this area.

**Method:** The authors create synthetic equivalents of PII-revealing data using a taxonomy of 19 categories and three text generation Large Language Models (LLMs), with an emphasis on making the data unlinkable and indistinguishable from real posts.

**Key Contributions:**

	1. Creation of a taxonomy of 19 PII-revealing categories for vulnerable populations.
	2. Development of a synthetic PII-labeled multi-text span dataset using three LLMs.
	3. Evaluation metrics ensuring data's unlinkability and indistinguishability from original content.

**Result:** The methodology successfully generates a synthetic multi-text span dataset, evaluated on reproducibility equivalence, unlinkability, and indistinguishability from original posts.

**Limitations:** The study may face challenges in completely ensuring indistinguishability and may not cover all types of PIIs or contexts.

**Conclusion:** The synthetic PII-labeled dataset is publicly released to advance research on privacy risks in online social media while minimizing harm to real users.

**Abstract:** Social platforms such as Reddit have a network of communities of shared interests, with a prevalence of posts and comments from which one can infer users' Personal Information Identifiers (PIIs). While such self-disclosures can lead to rewarding social interactions, they pose privacy risks and the threat of online harms. Research into the identification and retrieval of such risky self-disclosures of PIIs is hampered by the lack of open-source labeled datasets. To foster reproducible research into PII-revealing text detection, we develop a novel methodology to create synthetic equivalents of PII-revealing data that can be safely shared. Our contributions include creating a taxonomy of 19 PII-revealing categories for vulnerable populations and the creation and release of a synthetic PII-labeled multi-text span dataset generated from 3 text generation Large Language Models (LLMs), Llama2-7B, Llama3-8B, and zephyr-7b-beta, with sequential instruction prompting to resemble the original Reddit posts. The utility of our methodology to generate this synthetic dataset is evaluated with three metrics: First, we require reproducibility equivalence, i.e., results from training a model on the synthetic data should be comparable to those obtained by training the same models on the original posts. Second, we require that the synthetic data be unlinkable to the original users, through common mechanisms such as Google Search. Third, we wish to ensure that the synthetic data be indistinguishable from the original, i.e., trained humans should not be able to tell them apart. We release our dataset and code at https://netsys.surrey.ac.uk/datasets/synthetic-self-disclosure/ to foster reproducible research into PII privacy risks in online social media.

</details>


### [54] [Enhancing RAG Efficiency with Adaptive Context Compression](https://arxiv.org/abs/2507.22931)

*Shuyu Guo, Zhaochun Ren*

**Main category:** cs.CL

**Keywords:** Adaptive Context Compression, RAG, Large Language Models, Inference Efficiency, Context Compression

**Relevance Score:** 9

**TL;DR:** ACC-RAG introduces Adaptive Context Compression for RAG, optimizing inference costs by dynamically adjusting compression rates based on input complexity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate the significant inference costs incurred by retrieval-augmented generation (RAG) systems due to lengthy retrieved contexts.

**Method:** ACC-RAG combines a hierarchical compressor for multi-granular embeddings with a context selector that dynamically adjusts compression rates based on the complexity of the input.

**Key Contributions:**

	1. Introduction of Adaptive Context Compression for RAG
	2. Dynamic adjustment of compression rates based on input complexity
	3. Combination of hierarchical compression with context selection for improved efficiency.

**Result:** ACC-RAG outperforms existing fixed-rate compression methods and achieves over 4 times faster inference compared to standard RAG while maintaining or improving accuracy.

**Limitations:** 

**Conclusion:** The adaptive nature of ACC-RAG allows for optimized inference efficiency without sacrificing the accuracy of the results.

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs) with external knowledge but incurs significant inference costs due to lengthy retrieved contexts. While context compression mitigates this issue, existing methods apply fixed compression rates, over-compressing simple queries or under-compressing complex ones. We propose Adaptive Context Compression for RAG (ACC-RAG), a framework that dynamically adjusts compression rates based on input complexity, optimizing inference efficiency without sacrificing accuracy. ACC-RAG combines a hierarchical compressor (for multi-granular embeddings) with a context selector to retain minimal sufficient information, akin to human skimming. Evaluated on Wikipedia and five QA datasets, ACC-RAG outperforms fixed-rate methods and matches/unlocks over 4 times faster inference versus standard RAG while maintaining or improving accuracy.

</details>


### [55] [FinMarBa: A Market-Informed Dataset for Financial Sentiment Classification](https://arxiv.org/abs/2507.22932)

*Baptiste Lefort, Eric Benhamou, Beatrice Guez, Jean-Jacques Ohana, Ethan Setrouk, Alban Etienne*

**Main category:** cs.CL

**Keywords:** portfolio optimization, large language models, deep reinforcement learning, financial sentiment analysis, hierarchical frameworks

**Relevance Score:** 6

**TL;DR:** A hierarchical framework integrates LLMs with DRL for portfolio optimization using sentiment analysis and traditional market indicators.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve portfolio optimization by leveraging sentiment signals and market data through an advanced hierarchical approach.

**Method:** The framework consists of three tiers: base RL agents process data, meta-agents aggregate decisions, and a super-agent merges decisions for optimal portfolio management.

**Key Contributions:**

	1. Scalable cross-modal integration of market data and sentiment analysis
	2. Hierarchical reinforcement learning structure for enhanced stability
	3. Open-source framework for reproducibility

**Result:** The framework achieved a 26% annualized return and a Sharpe ratio of 1.2, outperforming traditional benchmarks.

**Limitations:** 

**Conclusion:** This novel approach demonstrates significant improvements in portfolio returns and stability by combining LLMs with reinforcement learning techniques.

**Abstract:** This paper presents a novel hierarchical framework for portfolio optimization, integrating lightweight Large Language Models (LLMs) with Deep Reinforcement Learning (DRL) to combine sentiment signals from financial news with traditional market indicators. Our three-tier architecture employs base RL agents to process hybrid data, meta-agents to aggregate their decisions, and a super-agent to merge decisions based on market data and sentiment analysis. Evaluated on data from 2018 to 2024, after training on 2000-2017, the framework achieves a 26% annualized return and a Sharpe ratio of 1.2, outperforming equal-weighted and S&P 500 benchmarks. Key contributions include scalable cross-modal integration, a hierarchical RL structure for enhanced stability, and open-source reproducibility.

</details>


### [56] [Augmented Vision-Language Models: A Systematic Review](https://arxiv.org/abs/2507.22933)

*Anthony C Davis, Burhan Sadiq, Tianmin Shu, Chien-Ming Huang*

**Main category:** cs.CL

**Keywords:** visual-language models, neural-symbolic systems, interpretability

**Relevance Score:** 8

**TL;DR:** This paper reviews methods to enhance visual-language understanding through integration with external symbolic systems, addressing issues like interpretability and resource intensity in traditional models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the interpretability and reasoning capabilities of visual-language models by integrating them with external symbolic information systems.

**Method:** The paper conducts a systematic literature review of existing techniques that enhance visual-language understanding using neural-symbolic systems.

**Key Contributions:**

	1. Systematic categorization of techniques for visual-language improvement
	2. Identification of benefits from combining neural and symbolic systems
	3. Discussion on enhancing interpretability and reasoning capabilities

**Result:** The review categorizes various methods and highlights the advantages of combining Vision-Language Models with symbolic information systems, including improved interpretability and reduced retraining needs.

**Limitations:** 

**Conclusion:** Integrating neural networks with symbolic systems presents a promising avenue for enhancing visual-language models' performance and adaptability.

**Abstract:** Recent advances in visual-language machine learning models have demonstrated exceptional ability to use natural language and understand visual scenes by training on large, unstructured datasets. However, this training paradigm cannot produce interpretable explanations for its outputs, requires retraining to integrate new information, is highly resource-intensive, and struggles with certain forms of logical reasoning. One promising solution involves integrating neural networks with external symbolic information systems, forming neural symbolic systems that can enhance reasoning and memory abilities. These neural symbolic systems provide more interpretable explanations to their outputs and the capacity to assimilate new information without extensive retraining. Utilizing powerful pre-trained Vision-Language Models (VLMs) as the core neural component, augmented by external systems, offers a pragmatic approach to realizing the benefits of neural-symbolic integration. This systematic literature review aims to categorize techniques through which visual-language understanding can be improved by interacting with external symbolic information systems.

</details>


### [57] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)

*Md Talha Mohsin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial NLP, LLM Evaluation, Prompt Sensitivity, Model Comparison

**Relevance Score:** 6

**TL;DR:** This study conducts a comparative evaluation of five leading LLMs in financial analysis using 10-K filings, assessing their performance through various methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically compare the capabilities of leading LLMs in FinNLP tasks, given their increasing importance in financial analysis.

**Method:** Evaluation of five LLMs (GPT, Claude, Perplexity, Gemini, DeepSeek) using human annotation, automated metrics (ROUGE, Cosine Similarity, Jaccard), and diagnostics on outputs from 10-K filings.

**Key Contributions:**

	1. Thorough comparative analysis of multiple LLMs in financial tasks.
	2. Use of mixed evaluation methodologies combining human and automated metrics.
	3. Insights into the sensitivity of model outputs to prompt design and source material.

**Result:** GPT provided the most coherent and contextually relevant responses, while Gemini and DeepSeek showed variability and less agreement across prompts and companies.

**Limitations:** Focuses only on five LLMs and may not generalize to other domains outside FinNLP.

**Conclusion:** The performance of LLMs is sensitive to prompt design and source material, with notable differences in stability and agreement across models and contexts.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.

</details>


### [58] [Deep Learning Approaches for Multimodal Intent Recognition: A Survey](https://arxiv.org/abs/2507.22934)

*Jingwei Zhao, Yuhua Wen, Qifei Li, Minchi Hu, Yingying Zhou, Jingyao Xue, Junyang Wu, Yingming Gao, Zhengqi Wen, Jianhua Tao, Ya Li*

**Main category:** cs.CL

**Keywords:** intent recognition, multimodal, deep learning, human-computer interaction, Transformer models

**Relevance Score:** 8

**TL;DR:** This article surveys the evolution of intent recognition methods, emphasizing deep learning and multimodal techniques incorporating various data types.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for natural human-computer interaction has driven advancements in intent recognition beyond traditional text approaches.

**Method:** The article reviews deep learning methods for intent recognition, with a focus on the transition from unimodal to multimodal techniques, including relevant datasets and applications.

**Key Contributions:**

	1. Survey of intent recognition methods involving deep learning
	2. Emphasis on multimodal data sources such as audio and vision
	3. Identifies future research directions in multimodal intent recognition

**Result:** The survey highlights significant breakthroughs achieved through the use of Transformer-based models and identifies existing challenges in the field.

**Limitations:** 

**Conclusion:** The paper provides a comprehensive overview of multimodal intent recognition, offering insights into future research directions.

**Abstract:** Intent recognition aims to identify users' underlying intentions, traditionally focusing on text in natural language processing. With growing demands for natural human-computer interaction, the field has evolved through deep learning and multimodal approaches, incorporating data from audio, vision, and physiological signals. Recently, the introduction of Transformer-based models has led to notable breakthroughs in this domain. This article surveys deep learning methods for intent recognition, covering the shift from unimodal to multimodal techniques, relevant datasets, methodologies, applications, and current challenges. It provides researchers with insights into the latest developments in multimodal intent recognition (MIR) and directions for future research.

</details>


### [59] [Trusted Knowledge Extraction for Operations and Maintenance Intelligence](https://arxiv.org/abs/2507.22935)

*Kathleen Mealey, Jonathan A. Karr Jr., Priscila Saboia Moreira, Paul R. Brenner, Charles F. Vardeman II*

**Main category:** cs.CL

**Keywords:** Knowledge Graph, Natural Language Processing, Large Language Models, Operational Intelligence, Aircraft Industry

**Relevance Score:** 6

**TL;DR:** This paper addresses the challenge of operational intelligence from organizational data while balancing data confidentiality and integration. It evaluates NLP tools and LLMs for applications in the aircraft industry, focusing on knowledge graph construction and knowledge extraction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to derive operational intelligence from data repositories while ensuring data confidentiality is a critical challenge, particularly in domains like operations and maintenance.

**Method:** The paper discusses the Knowledge Graph construction and breaks down the Knowledge Extraction process into its components, including Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction. It evaluates sixteen NLP tools alongside current capabilities of LLMs.

**Key Contributions:**

	1. Evaluation of 16 NLP tools in the context of operational intelligence in the aircraft industry
	2. Identification of performance limitations of NLP and LLM tools for trusted applications
	3. Provision of an open-source curated dataset for further testing and evaluation.

**Result:** The evaluation reveals significant performance limitations of existing NLP and LLM tools for trusted applications in the aircraft industry, particularly in a controlled environment where confidentiality is prioritized.

**Limitations:** The performance limitations of the evaluated NLP and LLM tools, especially in maintaining data confidentiality.

**Conclusion:** The paper concludes with a discussion on the challenges of using trusted NLP and LLM tools in mission-critical industries and provides recommendations for enhancement. An open-source curated dataset is offered for further testing.

**Abstract:** Deriving operational intelligence from organizational data repositories is a key challenge due to the dichotomy of data confidentiality vs data integration objectives, as well as the limitations of Natural Language Processing (NLP) tools relative to the specific knowledge structure of domains such as operations and maintenance. In this work, we discuss Knowledge Graph construction and break down the Knowledge Extraction process into its Named Entity Recognition, Coreference Resolution, Named Entity Linking, and Relation Extraction functional components. We then evaluate sixteen NLP tools in concert with or in comparison to the rapidly advancing capabilities of Large Language Models (LLMs). We focus on the operational and maintenance intelligence use case for trusted applications in the aircraft industry. A baseline dataset is derived from a rich public domain US Federal Aviation Administration dataset focused on equipment failures or maintenance requirements. We assess the zero-shot performance of NLP and LLM tools that can be operated within a controlled, confidential environment (no data is sent to third parties). Based on our observation of significant performance limitations, we discuss the challenges related to trusted NLP and LLM tools as well as their Technical Readiness Level for wider use in mission-critical industries such as aviation. We conclude with recommendations to enhance trust and provide our open-source curated dataset to support further baseline testing and evaluation.

</details>


### [60] [Evaluating Large Language Models (LLMs) in Financial NLP: A Comparative Study on Financial Report Analysis](https://arxiv.org/abs/2507.22936)

*Md Talha Mohsin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Financial NLP, Model Comparison, 10-K Filings, AI in Finance

**Relevance Score:** 5

**TL;DR:** This study evaluates and compares five leading LLMs in the context of financial analysis using 10-K filings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically compare the performance of major LLMs in financial NLP tasks amidst their increasing influence in financial analysis.

**Method:** The study uses human annotation, automated metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics to evaluate LLMs.

**Key Contributions:**

	1. Comprehensive evaluation of leading LLMs in financial contexts
	2. Domain-specific prompt creation for performance comparison
	3. Insights into the sensitivity of LLM outputs based on prompt and source material

**Result:** GPT outperforms others in coherence and contextual relevance, followed by Claude and Perplexity, with Gemini and DeepSeek showing more variability and less agreement.

**Limitations:** Limited to financial NLP tasks and specific technology companies' filings, may not generalize to other domains or prompts.

**Conclusion:** LLM performance varies significantly based on prompt design and source material, indicating a need for careful prompt crafting in financial NLP.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide variety of Financial Natural Language Processing (FinNLP) tasks. However, systematic comparisons among widely used LLMs remain underexplored. Given the rapid advancement and growing influence of LLMs in financial analysis, this study conducts a thorough comparative evaluation of five leading LLMs, GPT, Claude, Perplexity, Gemini and DeepSeek, using 10-K filings from the 'Magnificent Seven' technology companies. We create a set of domain-specific prompts and then use three methodologies to evaluate model performance: human annotation, automated lexical-semantic metrics (ROUGE, Cosine Similarity, Jaccard), and model behavior diagnostics (prompt-level variance and across-model similarity). The results show that GPT gives the most coherent, semantically aligned, and contextually relevant answers; followed by Claude and Perplexity. Gemini and DeepSeek, on the other hand, have more variability and less agreement. Also, the similarity and stability of outputs change from company to company and over time, showing that they are sensitive to how prompts are written and what source material is used.

</details>


### [61] [CoE-Ops: Collaboration of LLM-based Experts for AIOps Question-Answering](https://arxiv.org/abs/2507.22937)

*Jinkun Zhao, Yuanshuai Wang, Xingjian Zhang, Ruibo Chen, Xingchuang Liao, Junle Wang, Lei Huang, Kui Zhang, Wenjun Wu*

**Main category:** cs.CL

**Keywords:** AIOps, large language model, collaboration-of-expert, retrieval-augmented generation, DevOps

**Relevance Score:** 7

**TL;DR:** The paper presents CoE-Ops, a framework that utilizes a general-purpose large language model for improved AIOps performance through a collaboration-of-expert approach and retrieval-augmented generation.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of single models in AIOps that can only handle specific tasks, this research aims to enhance task performance using a collaborative approach with large language models.

**Method:** The paper proposes a collaboration-of-expert framework (CoE-Ops) that integrates a general-purpose large language model as a task classifier and employs a retrieval-augmented generation mechanism to handle various high-level and low-level AIOps tasks.

**Key Contributions:**

	1. Introduction of a collaboration-of-expert framework for AIOps
	2. Implementation of retrieval-augmented generation for diverse task handling
	3. Substantial experimental performance improvements over existing models

**Result:** CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks, an 8% enhancement in accuracy over single AIOps models, and outperforms larger Mixture-of-Experts models by 14% in accuracy.

**Limitations:** 

**Conclusion:** The experimental results demonstrate that CoE-Ops significantly outperforms existing approaches in AIOps, indicating its potential for broader application in the field.

**Abstract:** With the rapid evolution of artificial intelligence, AIOps has emerged as a prominent paradigm in DevOps. Lots of work has been proposed to improve the performance of different AIOps phases. However, constrained by domain-specific knowledge, a single model can only handle the operation requirement of a specific task,such as log parser,root cause analysis. Meanwhile, combining multiple models can achieve more efficient results, which have been proved in both previous ensemble learning and the recent LLM training domain. Inspired by these works,to address the similar challenges in AIOPS, this paper first proposes a collaboration-of-expert framework(CoE-Ops) incorporating a general-purpose large language model task classifier. A retrieval-augmented generation mechanism is introduced to improve the framework's capability in handling both Question-Answering tasks with high-level(Code,build,Test,etc.) and low-level(fault analysis,anomaly detection,etc.). Finally, the proposed method is implemented in the AIOps domain, and extensive experiments are conducted on the DevOps-EVAL dataset. Experimental results demonstrate that CoE-Ops achieves a 72% improvement in routing accuracy for high-level AIOps tasks compared to existing CoE methods, delivers up to 8% accuracy enhancement over single AIOps models in DevOps problem resolution, and outperforms larger-scale Mixture-of-Experts (MoE) models by up to 14% in accuracy.

</details>


### [62] [A Graph-based Approach for Multi-Modal Question Answering from Flowcharts in Telecom Documents](https://arxiv.org/abs/2507.22938)

*Sumit Soman, H. G. Ranjani, Sujoy Roychowdhury, Venkata Dharma Surya Narayana Sastry, Akshat Jain, Pranav Gangrade, Ayaaz Khan*

**Main category:** cs.CL

**Keywords:** Question-Answering, Graph Representations, Visual Language Models, Telecom, Flowcharts

**Relevance Score:** 8

**TL;DR:** This paper proposes enhancing question-answering (QA) in telecom by integrating graph representations of flowcharts with text-based retrieval systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Improving QA systems' ability to retrieve answers from figures in technical documents, especially in the telecom domain.

**Method:** Utilizing Visual Language Models to obtain graph representations of flowcharts and integrating these into a retrieval augmented generation (RAG) framework for effective QA.

**Key Contributions:**

	1. Integration of graph representations of flowcharts in text-based RAG systems
	2. Demonstration of improved retrieval performance in telecom QA
	3. Cost benefits by reducing dependency on Visual Language Models during inference

**Result:** The approach demonstrates that graph representations improve retrieval performance and have lower edit distances to ground truth compared to traditional methods.

**Limitations:** 

**Conclusion:** Integrating graph representations with text embeddings boosts QA effectiveness, reducing reliance on costly Visual Language Models during inference.

**Abstract:** Question-Answering (QA) from technical documents often involves questions whose answers are present in figures, such as flowcharts or flow diagrams. Text-based Retrieval Augmented Generation (RAG) systems may fail to answer such questions. We leverage graph representations of flowcharts obtained from Visual large Language Models (VLMs) and incorporate them in a text-based RAG system to show that this approach can enable image retrieval for QA in the telecom domain. We present the end-to-end approach from processing technical documents, classifying image types, building graph representations, and incorporating them with the text embedding pipeline for efficient retrieval. We benchmark the same on a QA dataset created based on proprietary telecom product information documents. Results show that the graph representations obtained using a fine-tuned VLM model have lower edit distance with respect to the ground truth, which illustrate the robustness of these representations for flowchart images. Further, the approach for QA using these representations gives good retrieval performance using text-based embedding models, including a telecom-domain adapted one. Our approach also alleviates the need for a VLM in inference, which is an important cost benefit for deployed QA systems.

</details>


### [63] [PARROT: An Open Multilingual Radiology Reports Dataset](https://arxiv.org/abs/2507.22939)

*Bastien Le Guellec, Kokou Adambounou, Lisa C Adams, Thibault Agripnidis, Sung Soo Ahn, Radhia Ait Chalal, Tugba Akinci D Antonoli, Philippe Amouyel, Henrik Andersson, Raphael Bentegeac, Claudio Benzoni, Antonino Andrea Blandino, Felix Busch, Elif Can, Riccardo Cau, Armando Ugo Cavallo, Christelle Chavihot, Erwin Chiquete, Renato Cuocolo, Eugen Divjak, Gordana Ivanac, Barbara Dziadkowiec Macek, Armel Elogne, Salvatore Claudio Fanni, Carlos Ferrarotti, Claudia Fossataro, Federica Fossataro, Katarzyna Fulek, Michal Fulek, Pawel Gac, Martyna Gachowska, Ignacio Garcia Juarez, Marco Gatti, Natalia Gorelik, Alexia Maria Goulianou, Aghiles Hamroun, Nicolas Herinirina, Krzysztof Kraik, Dominik Krupka, Quentin Holay, Felipe Kitamura, Michail E Klontzas, Anna Kompanowska, Rafal Kompanowski, Alexandre Lefevre, Tristan Lemke, Maximilian Lindholz, Lukas Muller, Piotr Macek, Marcus Makowski, Luigi Mannacio, Aymen Meddeb, Antonio Natale, Beatrice Nguema Edzang, Adriana Ojeda, Yae Won Park, Federica Piccione, Andrea Ponsiglione, Malgorzata Poreba, Rafal Poreba, Philipp Prucker, Jean Pierre Pruvo, Rosa Alba Pugliesi, Feno Hasina Rabemanorintsoa, Vasileios Rafailidis, Katarzyna Resler, Jan Rotkegel, Luca Saba, Ezann Siebert, Arnaldo Stanzione, Ali Fuat Tekin, Liz Toapanta Yanchapaxi, Matthaios Triantafyllou, Ekaterini Tsaoulia, Evangelia Vassalou, Federica Vernuccio, Johan Wasselius, Weilang Wang, Szymon Urban, Adrian Wlodarczak, Szymon Wlodarczak, Andrzej Wysocki, Lina Xu, Tomasz Zatonski, Shuhang Zhang, Sebastian Ziegelmayer, Gregory Kuchcinski, Keno K Bressem*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Radiology, Dataset, Open Access, Multilingual

**Relevance Score:** 8

**TL;DR:** PARROT is a large, open-access dataset of fictional radiology reports in multiple languages intended for testing NLP applications in radiology.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a large, open-access dataset of fictional radiology reports to facilitate testing of natural language processing applications.

**Method:** Radiologists contributed fictional reports adhering to standard practices, with metadata and ICD-10 codes. A differentiation study assessed human vs. AI authorship with 154 participants.

**Key Contributions:**

	1. Creation of PARROT dataset with extensive multilingual coverage
	2. Inclusion of diverse imaging modalities and clinical contexts
	3. Establishment of a benchmark for distinguishing AI-generated reports from human-authored ones

**Result:** The dataset contains 2,658 reports from 76 authors in 21 countries, covering various imaging modalities and anatomical regions. Participants in the differentiation study identified human vs. AI reports with 53.9% overall accuracy.

**Limitations:** 

**Conclusion:** PARROT is the largest open multilingual radiology report dataset, supporting NLP application development across different languages and clinical settings.

**Abstract:** Rationale and Objectives: To develop and validate PARROT (Polyglottal Annotated Radiology Reports for Open Testing), a large, multicentric, open-access dataset of fictional radiology reports spanning multiple languages for testing natural language processing applications in radiology. Materials and Methods: From May to September 2024, radiologists were invited to contribute fictional radiology reports following their standard reporting practices. Contributors provided at least 20 reports with associated metadata including anatomical region, imaging modality, clinical context, and for non-English reports, English translations. All reports were assigned ICD-10 codes. A human vs. AI report differentiation study was conducted with 154 participants (radiologists, healthcare professionals, and non-healthcare professionals) assessing whether reports were human-authored or AI-generated. Results: The dataset comprises 2,658 radiology reports from 76 authors across 21 countries and 13 languages. Reports cover multiple imaging modalities (CT: 36.1%, MRI: 22.8%, radiography: 19.0%, ultrasound: 16.8%) and anatomical regions, with chest (19.9%), abdomen (18.6%), head (17.3%), and pelvis (14.1%) being most prevalent. In the differentiation study, participants achieved 53.9% accuracy (95% CI: 50.7%-57.1%) in distinguishing between human and AI-generated reports, with radiologists performing significantly better (56.9%, 95% CI: 53.3%-60.6%, p<0.05) than other groups. Conclusion: PARROT represents the largest open multilingual radiology report dataset, enabling development and validation of natural language processing applications across linguistic, geographic, and clinical boundaries without privacy constraints.

</details>


### [64] [Trustworthy Reasoning: Evaluating and Enhancing Factual Accuracy in LLM Intermediate Thought Processes](https://arxiv.org/abs/2507.22940)

*Rui Jiao, Yue Zhang, Jinku Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, factual accuracy, reinforcement learning, healthcare, mechanistic interpretability

**Relevance Score:** 9

**TL;DR:** RELIANCE is a framework designed to enhance factual accuracy in LLM reasoning, particularly in high-stakes areas like healthcare and legal analysis, by integrating a fact-checker, a reinforcement learning approach, and a mechanistic interpretability module.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the critical issue of factual inaccuracies in LLMs' intermediate reasoning steps, which can lead to harmful consequences in vital domains such as healthcare and legal analysis.

**Method:** RELIANCE integrates a fact-checking classifier trained on counterfactually augmented data, a Group Relative Policy Optimization (GRPO) approach for balancing multiple reward dimensions, and a mechanistic interpretability module for examining model activations during reasoning.

**Key Contributions:**

	1. Introduction of a specialized fact-checking classifier
	2. Implementation of Group Relative Policy Optimization for reinforcement learning
	3. Mechanistic interpretability module to analyze reasoning processes

**Result:** Extensive evaluations show that leading models only achieved around 82% factual accuracy, while RELIANCE improved factual robustness by up to 49.90% without compromising performance on benchmarks like Math-500 and GPQA.

**Limitations:** 

**Conclusion:** RELIANCE provides a significant step forward in enhancing the factual integrity of LLMs' reasoning and informs future training methods for improved factual robustness.

**Abstract:** We present RELIANCE (Reasoning Evaluation with Logical Integrity and Accuracy for Confidence Enhancement), a novel framework addressing a critical vulnerability in Large Language Models (LLMs): the prevalence of factual inaccuracies within intermediate reasoning steps despite correct final answers. This phenomenon poses substantial risks in high-stakes domains including healthcare, legal analysis, and scientific research, where erroneous yet confidently presented reasoning can mislead users into dangerous decisions. Our framework integrates three core components: (1) a specialized fact-checking classifier trained on counterfactually augmented data to detect subtle factual inconsistencies within reasoning chains; (2) a Group Relative Policy Optimization (GRPO) reinforcement learning approach that balances factuality, coherence, and structural correctness through multi-dimensional rewards; and (3) a mechanistic interpretability module examining how factuality improvements manifest in model activations during reasoning processes. Extensive evaluation across ten state-of-the-art models reveals concerning patterns: even leading models like Claude-3.7 and GPT-o1 demonstrate reasoning factual accuracy of only 81.93% and 82.57% respectively. RELIANCE significantly enhances factual robustness (up to 49.90% improvement) while maintaining or improving performance on challenging benchmarks including Math-500, AIME-2024, and GPQA. Furthermore, our activation-level analysis provides actionable insights into how factual enhancements reshape reasoning trajectories within model architectures, establishing foundations for future training methodologies that explicitly target factual robustness through activation-guided optimization.

</details>


### [65] [SigBERT: Combining Narrative Medical Reports and Rough Path Signature Theory for Survival Risk Estimation in Oncology](https://arxiv.org/abs/2507.22941)

*Paul Minchella, Loïc Verlingue, Stéphane Chrétien, Rémi Vaucher, Guillaume Metzler*

**Main category:** cs.CL

**Keywords:** survival analysis, machine learning, electronic health records, temporal dynamics, LASSO

**Relevance Score:** 9

**TL;DR:** SigBERT is a novel temporal survival analysis framework that enhances risk estimation from sequential electronic medical reports using signature extraction and LASSO-penalized Cox models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing survival analysis methods struggle with the complexity of textual data in electronic medical reports, necessitating a better approach to leverage this information for healthcare applications.

**Method:** SigBERT processes timestamped medical reports by averaging word embeddings into sentence embeddings and extracting geometric features using signature extraction from rough path theory, which are then integrated into a LASSO-penalized Cox model.

**Key Contributions:**

	1. Introduction of SigBERT for temporal survival analysis
	2. Use of signature extraction for processing sequential medical text
	3. LASSO-penalized Cox model for risk score estimation

**Result:** The model achieved a C-index score of 0.75 on a real-world oncology dataset, indicating improved survival estimation capabilities.

**Limitations:** 

**Conclusion:** SigBERT successfully integrates sequential medical data to advance narrative-based survival analysis and improve patient-specific risk estimation.

**Abstract:** Electronic medical reports (EHR) contain a vast amount of information that can be leveraged for machine learning applications in healthcare. However, existing survival analysis methods often struggle to effectively handle the complexity of textual data, particularly in its sequential form. Here, we propose SigBERT, an innovative temporal survival analysis framework designed to efficiently process a large number of clinical reports per patient. SigBERT processes timestamped medical reports by extracting and averaging word embeddings into sentence embeddings. To capture temporal dynamics from the time series of sentence embedding coordinates, we apply signature extraction from rough path theory to derive geometric features for each patient, which significantly enhance survival model performance by capturing complex temporal dynamics. These features are then integrated into a LASSO-penalized Cox model to estimate patient-specific risk scores. The model was trained and evaluated on a real-world oncology dataset from the L\'eon B\'erard Center corpus, with a C-index score of 0.75 (sd 0.014) on the independent test cohort. SigBERT integrates sequential medical data to enhance risk estimation, advancing narrative-based survival analysis.

</details>


### [66] [A chart review process aided by natural language processing and multi-wave adaptive sampling to expedite validation of code-based algorithms for large database studies](https://arxiv.org/abs/2507.22943)

*Shirley V Wang, Georg Hahn, Sushama Kattinakere Sreedhara, Mufaddal Mahesri, Haritha S. Pillai, Rajendra Aldis, Joyce Lii, Sarah K. Dutcher, Rhoda Eniafe, Jamal T. Jones, Keewan Kim, Jiwei He, Hana Lee, Sengwee Toh, Rishi J Desai, Jie Yang*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Health Informatics, Validation Study

**Relevance Score:** 8

**TL;DR:** The paper presents an efficient validation process using NLP and adaptive sampling to assess health outcome algorithms in claims databases.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance analyses using large claims databases by validating code-based algorithms that identify health outcomes, addressing issues related to outcome misclassification.

**Method:** The study employs natural language processing to expedite the review of health records and a multi-wave adaptive sampling strategy to optimize the validation process by stopping once sufficient precision is achieved.

**Key Contributions:**

	1. Introduction of NLP to streamline chart reviews
	2. Development of a multi-wave adaptive sampling method
	3. Demonstration of significant time savings in chart validation

**Result:** NLP-assisted annotation reduced review time per chart by 40%, and the adaptive sampling approach potentially prevented the review of 77% of charts without significant compromise in measurement precision.

**Limitations:** The study's findings may be limited to the specific health outcomes and databases investigated.

**Conclusion:** The proposed methods could support routine validation of algorithms, improving the reliability of findings from database studies.

**Abstract:** Background: One of the ways to enhance analyses conducted with large claims databases is by validating the measurement characteristics of code-based algorithms used to identify health outcomes or other key study parameters of interest. These metrics can be used in quantitative bias analyses to assess the robustness of results for an inferential study given potential bias from outcome misclassification. However, extensive time and resource allocation are typically re-quired to create reference-standard labels through manual chart review of free-text notes from linked electronic health records. Methods: We describe an expedited process that introduces efficiency in a validation study us-ing two distinct mechanisms: 1) use of natural language processing (NLP) to reduce time spent by human reviewers to review each chart, and 2) a multi-wave adaptive sampling approach with pre-defined criteria to stop the validation study once performance characteristics are identified with sufficient precision. We illustrate this process in a case study that validates the performance of a claims-based outcome algorithm for intentional self-harm in patients with obesity. Results: We empirically demonstrate that the NLP-assisted annotation process reduced the time spent on review per chart by 40% and use of the pre-defined stopping rule with multi-wave samples would have prevented review of 77% of patient charts with limited compromise to precision in derived measurement characteristics. Conclusion: This approach could facilitate more routine validation of code-based algorithms used to define key study parameters, ultimately enhancing understanding of the reliability of find-ings derived from database studies.

</details>


### [67] [Opacity as Authority: Arbitrariness and the Preclusion of Contestation](https://arxiv.org/abs/2507.22944)

*Naomi Omeonga wa Kayembe*

**Main category:** cs.CL

**Keywords:** arbitrariness, semiotics, human systems, explainability, artificial intelligence

**Relevance Score:** 4

**TL;DR:** The paper redefines arbitrariness as a functional mechanism structuring human systems, challenging traditional views that link it to injustice, and extends its implications to AI explainability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To redefine arbitrariness beyond normative flaws and explore its functional role in human interactions and systems.

**Method:** The paper builds upon Ferdinand de Saussure's concept of l'arbitraire du signe and introduces a chain of 'Motivation -> Constatability -> Contestability' to analyze structural opacity in systems.

**Key Contributions:**

	1. Reconceptualization of arbitrariness as a functional mechanism rather than a flaw.
	2. Introduction of the 'Motivation -> Constatability -> Contestability' chain.
	3. Formalization of arbitrariness through Shannon's entropy model.

**Result:** It formalizes arbitrariness using Shannon's entropy model, arguing that it serves both control and care in human systems and has implications for AI explainability.

**Limitations:** 

**Conclusion:** The study proposes a contemporary theory of arbitrariness that highlights its role in protecting authority and lacks rational exposure, suggesting this can also inform AI explainability concepts.

**Abstract:** This article redefines arbitrariness not as a normative flaw or a symptom of domination, but as a foundational functional mechanism structuring human systems and interactions. Diverging from critical traditions that conflate arbitrariness with injustice, it posits arbitrariness as a semiotic trait: a property enabling systems - linguistic, legal, or social - to operate effectively while withholding their internal rationale. Building on Ferdinand de Saussure's concept of l'arbitraire du signe, the analysis extends this principle beyond language to demonstrate its cross-domain applicability, particularly in law and social dynamics. The paper introduces the "Motivation -> Constatability -> Contestability" chain, arguing that motivation functions as a crucial interface rendering an act's logic vulnerable to intersubjective contestation. When this chain is broken through mechanisms like "immotivization" or "Conflict Lateralization" (exemplified by "the blur of the wolf drowned in the fish"), acts produce binding effects without exposing their rationale, thus precluding justiciability. This structural opacity, while appearing illogical, is a deliberate design protecting authority from accountability. Drawing on Shannon's entropy model, the paper formalizes arbitrariness as A = H(L|M) (conditional entropy). It thereby proposes a modern theory of arbitrariness as a neutral operator central to control as well as care, an overlooked dimension of interpersonal relations. While primarily developed through human social systems, this framework also illuminates a new pathway for analyzing explainability in advanced artificial intelligence systems.

</details>


### [68] [C3: A Bilingual Benchmark for Spoken Dialogue Models Exploring Challenges in Complex Conversations](https://arxiv.org/abs/2507.22968)

*Chengqian Ma, Wei Tao, Yiwen Guo*

**Main category:** cs.CL

**Keywords:** Spoken Dialogue Models, benchmark dataset, human conversation, LLM evaluation, complexity in dialogue

**Relevance Score:** 8

**TL;DR:** This paper presents a benchmark dataset for evaluating Spoken Dialogue Models (SDMs) against challenges in human conversation, including ambiguity and context-dependency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in research regarding the effectiveness of Spoken Dialogue Models in understanding and emulating human conversations compared to text-based models.

**Method:** The authors developed a benchmark dataset with 1,079 instances in English and Chinese, accompanied by an LLM-based evaluation method that aligns with human judgment.

**Key Contributions:**

	1. Creation of a benchmark dataset for SDMs
	2. LLM-based evaluation method aligned with human judgment
	3. Focus on complexities of spoken dialogue versus text-based models

**Result:** The dataset allows for a comprehensive assessment of SDM capabilities in managing the complexities of human dialogues.

**Limitations:** The dataset may not encompass all possible conversational scenarios or languages beyond English and Chinese.

**Conclusion:** The findings highlight the practical challenges faced by SDMs and provide a foundational resource for evaluating their performance in voice interaction.

**Abstract:** Spoken Dialogue Models (SDMs) have recently attracted significant attention for their ability to generate voice responses directly to users' spoken queries. Despite their increasing popularity, there exists a gap in research focused on comprehensively understanding their practical effectiveness in comprehending and emulating human conversations. This is especially true compared to text-based Large Language Models (LLMs), which benefit from extensive benchmarking. Human voice interactions are inherently more complex than text due to characteristics unique to spoken dialogue. Ambiguity poses one challenge, stemming from semantic factors like polysemy, as well as phonological aspects such as heterograph, heteronyms, and stress patterns. Additionally, context-dependency, like omission, coreference, and multi-turn interaction, adds further complexity to human conversational dynamics. To illuminate the current state of SDM development and to address these challenges, we present a benchmark dataset in this paper, which comprises 1,079 instances in English and Chinese. Accompanied by an LLM-based evaluation method that closely aligns with human judgment, this dataset facilitates a comprehensive exploration of the performance of SDMs in tackling these practical challenges.

</details>


### [69] [Math Natural Language Inference: this should be easy!](https://arxiv.org/abs/2507.23063)

*Valeria de Paiva, Qiyue Gao, Hai Hu, Pavel Kovalev, Yikang Liu, Lawrence S. Moss, Zhiheng Qian*

**Main category:** cs.CL

**Keywords:** natural language inference, large language models, mathematics, corpora, inference

**Relevance Score:** 9

**TL;DR:** This paper investigates the ability of contemporary LLMs to perform natural language inference (NLI) on mathematical texts, revealing both strengths and weaknesses in their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore if LLMs can effectively handle NLI tasks within the context of mathematical texts, as this area has not been extensively examined yet.

**Method:** A corpus of Math NLI pairs was constructed, with premises taken from existing mathematical literature and hypotheses labeled by experts. The study also compares human-labeled hypotheses with those generated by LLMs, examining the performance and consistency of various LLMs.

**Key Contributions:**

	1. Construction of a unique Math NLI corpus from mathematical texts
	2. Comparison of LLM-generated hypotheses and human-labeled data
	3. Insights into LLM performance on complex inference tasks

**Result:** Findings indicate that in certain scenarios, a majority vote among LLMs yields results comparable to human labeling, although LLMs still struggle with mathematical language and basic inferences.

**Limitations:** LLMs exhibit weaknesses in understanding mathematical language and performing basic inferences; results may vary across models.

**Conclusion:** The study highlights both the potential and limitations of LLMs in Math NLI tasks, and provides a valuable resource for future research in this area through the released corpus.

**Abstract:** We ask whether contemporary LLMs are able to perform natural language inference (NLI) tasks on mathematical texts. We call this the Math NLI problem. We construct a corpus of Math NLI pairs whose premises are from extant mathematical text and whose hypotheses and gold labels were provided by people with experience in both research-level mathematics and also in the NLI field. We also investigate the quality of corpora using the same premises but whose hypotheses are provided by LLMs themselves. We not only investigate the performance but also the inter-group consistency of the diverse group of LLMs. We have both positive and negative findings. Among our positive findings: in some settings, using a majority vote of LLMs is approximately equivalent to using human-labeled data in the Math NLI area. On the negative side: LLMs still struggle with mathematical language. They occasionally fail at even basic inferences. Current models are not as prone to hypothesis-only "inference" in our data the way the previous generation had been. In addition to our findings, we also provide our corpora as data to support future work on Math NLI.

</details>


### [70] [Exploring In-Context Learning for Frame-Semantic Parsing](https://arxiv.org/abs/2507.23082)

*Diego Garat, Guillermo Moncecchi, Dina Wonsever*

**Main category:** cs.CL

**Keywords:** Frame Semantic Parsing, In-Context Learning, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper explores the use of in-context learning with large language models for frame semantic parsing without needing model fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of ICL in enhancing frame semantic parsing tasks and to provide an efficient alternative to traditional model fine-tuning.

**Method:** The study proposes a method that generates task-specific prompts from the FrameNet database for Frame Identification and Frame Semantic Role Labeling, utilized by six different LLMs.

**Key Contributions:**

	1. Introduction of task-specific prompts from FrameNet for LLMs.
	2. Demonstration of competitive performance without the need for fine-tuning.
	3. Exploration of ICL applicability in frame semantic parsing.

**Result:** The proposed method achieved F1 scores of 94.3% for Frame Identification and 77.4% for Frame Semantic Role Labeling, demonstrating competitive performance.

**Limitations:** The experiments were conducted on a limited subset of frames related to violent events, which may not generalize across all domains.

**Conclusion:** In-context learning can effectively serve as a practical alternative to traditional fine-tuning methods for domain-specific tasks in frame semantic parsing.

**Abstract:** Frame Semantic Parsing (FSP) entails identifying predicates and labeling their arguments according to Frame Semantics. This paper investigates the use of In-Context Learning (ICL) with Large Language Models (LLMs) to perform FSP without model fine-tuning. We propose a method that automatically generates task-specific prompts for the Frame Identification (FI) and Frame Semantic Role Labeling (FSRL) subtasks, relying solely on the FrameNet database. These prompts, constructed from frame definitions and annotated examples, are used to guide six different LLMs. Experiments are conducted on a subset of frames related to violent events. The method achieves competitive results, with F1 scores of 94.3% for FI and 77.4% for FSRL. The findings suggest that ICL offers a practical and effective alternative to traditional fine-tuning for domain-specific FSP tasks.

</details>


### [71] [Context-aware Rotary Position Embedding](https://arxiv.org/abs/2507.23083)

*Ali Veisi, Delaram Fartoot, Hamidreza Amirzadeh*

**Main category:** cs.CL

**Keywords:** Rotary Positional Embeddings, Context-Aware, Transformer Models

**Relevance Score:** 7

**TL;DR:** CARoPE is a context-aware enhancement of Rotary Positional Embeddings that introduces dynamic token-specific frequency patterns to improve Transformer models' performance in sequential tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Rotary Positional Embeddings (RoPE) rely on static sinusoidal frequencies, limiting their context sensitivity.

**Method:** CARoPE generates head-specific frequency patterns based on token embeddings, allowing for dynamic input-dependent phase adjustments in the rotary mechanism.

**Key Contributions:**

	1. Introduction of context-aware, token-specific positional encoding
	2. Preservation of computational efficiency and architectural simplicity
	3. Demonstration of improved performance metrics in next-token prediction tasks

**Result:** CARoPE outperforms RoPE and other baselines in terms of lowering perplexity on the FineWeb-Edu-10B dataset, especially for longer context lengths.

**Limitations:** 

**Conclusion:** CARoPE represents a scalable and efficient improvement over traditional positional encodings in Transformer architectures.

**Abstract:** Positional encoding is a vital component of Transformer architectures, enabling models to incorporate sequence order into self-attention mechanisms. Rotary Positional Embeddings (RoPE) have become a widely adopted solution due to their compatibility with relative position encoding and computational efficiency. However, RoPE relies on static, input-independent sinusoidal frequency patterns, limiting its ability to model context-sensitive relationships. In this work, we propose CARoPE (Context-Aware Rotary Positional Embedding), a novel generalization of RoPE that dynamically generates head-specific frequency patterns conditioned on token embeddings. This design introduces token- and context-sensitive positional representations while preserving RoPE efficiency and architectural simplicity. CARoPE computes input-dependent phase shifts using a bounded transformation of token embeddings and integrates them into the rotary mechanism across attention heads. We evaluate CARoPE on the FineWeb-Edu-10B dataset using GPT-2 variants trained on next-token prediction tasks. Experimental results show that CARoPE consistently outperforms RoPE and other common positional encoding baselines, achieving significantly lower perplexity, even at longer context lengths. Additionally, CARoPE enables faster training throughput without sacrificing model stability. These findings demonstrate that CARoPE offers a scalable, expressive, and efficient upgrade to existing positional encoding strategies in Transformer models.

</details>


### [72] [SMART-Editor: A Multi-Agent Framework for Human-Like Design Editing with Structural Integrity](https://arxiv.org/abs/2507.23095)

*Ishani Mondal, Meera Bharadwaj, Ayush Roy, Aparna Garimella, Jordan Lee Boyd-Graber*

**Main category:** cs.CL

**Keywords:** SMART-Editor, content editing, reward-guided refinement, layout optimization, benchmarking

**Relevance Score:** 4

**TL;DR:** SMART-Editor is a framework for editing layouts and content in both structured and unstructured domains while maintaining global coherence through reward-guided methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current editing models often fail to preserve global coherence during local edits. SMART-Editor aims to overcome this limitation.

**Method:** The framework employs two strategies: Reward-Refine for inference-time refinement and RewardDPO for training-time preference optimization using reward-aligned pairs.

**Key Contributions:**

	1. Introduction of SMART-Editor framework for diverse layout and content editing
	2. Development of SMARTEdit-Bench for multi-domain evaluation
	3. Demonstration of improved performance via reward-guided methods

**Result:** SMART-Editor shows superior performance in structured settings with up to 15% gains over models like InstructPix2Pix and HIVE, achieving visually aligned edits confirmed by evaluations.

**Limitations:** 

**Conclusion:** The study demonstrates that reward-guided planning is effective in ensuring semantic consistency and visual alignment in editing tasks across different domains.

**Abstract:** We present SMART-Editor, a framework for compositional layout and content editing across structured (posters, websites) and unstructured (natural images) domains. Unlike prior models that perform local edits, SMART-Editor preserves global coherence through two strategies: Reward-Refine, an inference-time rewardguided refinement method, and RewardDPO, a training-time preference optimization approach using reward-aligned layout pairs. To evaluate model performance, we introduce SMARTEdit-Bench, a benchmark covering multi-domain, cascading edit scenarios. SMART-Editor outperforms strong baselines like InstructPix2Pix and HIVE, with RewardDPO achieving up to 15% gains in structured settings and Reward-Refine showing advantages on natural images. Automatic and human evaluations confirm the value of reward-guided planning in producing semantically consistent and visually aligned edits.

</details>


### [73] [RASL: Retrieval Augmented Schema Linking for Massive Database Text-to-SQL](https://arxiv.org/abs/2507.23104)

*Jeffrey Eben, Aitzaz Ahmad, Stephen Lau*

**Main category:** cs.CL

**Keywords:** large language models, natural language interfaces, database metadata

**Relevance Score:** 8

**TL;DR:** The paper presents a component-based retrieval architecture for LLM-based natural language interfaces aimed at scaling enterprise-level data catalogs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scalability challenges of LLM-based natural language interfaces for databases which are not effectively handled by domain-specific fine-tuning and do not leverage semantic context from database metadata.

**Method:** The authors introduce a retrieval architecture that decomposes database schemas and metadata into semantic units for targeted retrieval, prioritizing table identification and using column-level information.

**Key Contributions:**

	1. Component-based retrieval architecture for natural language interfaces
	2. Utilization of semantic units from database metadata
	3. High scalability and accuracy without domain-specific fine-tuning

**Result:** Experiments show that the method maintains high recall and accuracy, outperforming existing baselines over large and varied databases.

**Limitations:** 

**Conclusion:** The proposed solution facilitates practical deployment of text-to-SQL systems across diverse enterprise environments without the need for specialized fine-tuning, effectively bridging a critical gap in database interactions.

**Abstract:** Despite advances in large language model (LLM)-based natural language interfaces for databases, scaling to enterprise-level data catalogs remains an under-explored challenge. Prior works addressing this challenge rely on domain-specific fine-tuning - complicating deployment - and fail to leverage important semantic context contained within database metadata. To address these limitations, we introduce a component-based retrieval architecture that decomposes database schemas and metadata into discrete semantic units, each separately indexed for targeted retrieval. Our approach prioritizes effective table identification while leveraging column-level information, ensuring the total number of retrieved tables remains within a manageable context budget. Experiments demonstrate that our method maintains high recall and accuracy, with our system outperforming baselines over massive databases with varying structure and available metadata. Our solution enables practical text-to-SQL systems deployable across diverse enterprise settings without specialized fine-tuning, addressing a critical scalability gap in natural language database interfaces.

</details>


### [74] [Uncovering the Fragility of Trustworthy LLMs through Chinese Textual Ambiguity](https://arxiv.org/abs/2507.23121)

*Xinwei Wu, Haojie Li, Hongyu Liu, Xinyu Ji, Ruohan Li, Yule Chen, Yigeng Zhang*

**Main category:** cs.CL

**Keywords:** large language models, textual ambiguity, natural language processing, AI trustworthiness, Chinese disambiguation

**Relevance Score:** 9

**TL;DR:** This study explores how large language models (LLMs) handle ambiguous narrative text, particularly in Chinese, revealing significant limitations in distinguishing between ambiguous and unambiguous text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the trustworthiness of LLMs in dealing with linguistic ambiguities, especially in the context of Chinese language, and to assess their reliability in real-world applications where ambiguity is prevalent.

**Method:** A benchmark dataset was created by collecting and generating ambiguous sentences and their corresponding disambiguated pairs, categorized into three main categories and nine subcategories. Experiments were conducted to evaluate LLM performance on this dataset.

**Key Contributions:**

	1. Creation of a benchmark dataset for Chinese textual ambiguity
	2. Systematic categorization of ambiguous sentences
	3. Experimental analysis revealing LLM limitations in handling ambiguity

**Result:** LLMs demonstrated substantial fragility when interpreting ambiguity, showing overconfidence in providing single interpretations for ambiguous text and difficulty in distinguishing it from unambiguous text.

**Limitations:** The study primarily focused on Chinese text, which may limit the applicability of the findings to LLMs in other languages.

**Conclusion:** The findings indicate a significant limitation in current LLMs with implications for their practical use, emphasizing the need for improved methods to handle linguistic ambiguity.

**Abstract:** In this work, we study a critical research problem regarding the trustworthiness of large language models (LLMs): how LLMs behave when encountering ambiguous narrative text, with a particular focus on Chinese textual ambiguity. We created a benchmark dataset by collecting and generating ambiguous sentences with context and their corresponding disambiguated pairs, representing multiple possible interpretations. These annotated examples are systematically categorized into 3 main categories and 9 subcategories. Through experiments, we discovered significant fragility in LLMs when handling ambiguity, revealing behavior that differs substantially from humans. Specifically, LLMs cannot reliably distinguish ambiguous text from unambiguous text, show overconfidence in interpreting ambiguous text as having a single meaning rather than multiple meanings, and exhibit overthinking when attempting to understand the various possible meanings. Our findings highlight a fundamental limitation in current LLMs that has significant implications for their deployment in real-world applications where linguistic ambiguity is common, calling for improved approaches to handle uncertainty in language understanding. The dataset and code are publicly available at this GitHub repository: https://github.com/ictup/LLM-Chinese-Textual-Disambiguation.

</details>


### [75] [ISO-Bench: Benchmarking Multimodal Causal Reasoning in Visual-Language Models through Procedural Plans](https://arxiv.org/abs/2507.23135)

*Ananya Sadana, Yash Kumar Lal, Jiawei Zhou*

**Main category:** cs.CL

**Keywords:** causal inference, multimodal models, benchmark, vision-language, machine learning

**Relevance Score:** 7

**TL;DR:** ISO-Bench is introduced as a benchmark to evaluate how multimodal models infer causal relations between visual tasks and procedural text, showing poor performance compared to humans.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of understanding causal relationships in multimodal models operating in real-world environments.

**Method:** A benchmark called ISO-Bench is established which tests models on determining the causal order between images of task steps and corresponding procedural text snippets.

**Key Contributions:**

	1. Introduction of ISO-Bench for causal inference evaluation in multimodal models
	2. Benchmark results highlighting the limitations of current vision-language models
	3. Identification of potential directions for improving causal reasoning capabilities.

**Result:** Evaluation of ten state-of-the-art vision-language models shows mediocre performance, with the best zero-shot F1 score at 0.57 and a slight improvement to 0.62 with chain-of-thought reasoning, significantly lower than human performance at 0.98 F1.

**Limitations:** The current models exhibit significant performance gaps compared to human reasoning, indicating a need for methodologies that better capture causal relationships.

**Conclusion:** The findings call for improved approaches to enhance causal understanding in multimodal frameworks.

**Abstract:** Understanding causal relationships across modalities is a core challenge for multimodal models operating in real-world environments. We introduce ISO-Bench, a benchmark for evaluating whether models can infer causal dependencies between visual observations and procedural text. Each example presents an image of a task step and a text snippet from a plan, with the goal of deciding whether the visual step occurs before or after the referenced text step. Evaluation results on ten frontier vision-language models show underwhelming performance: the best zero-shot F1 is only 0.57, and chain-of-thought reasoning yields only modest gains (up to 0.62 F1), largely behind humans (0.98 F1). Our analysis further highlights concrete directions for improving causal understanding in multimodal models.

</details>


### [76] [User Feedback in Human-LLM Dialogues: A Lens to Understand Users But Noisy as a Learning Signal](https://arxiv.org/abs/2507.23158)

*Yuhan Liu, Michael J. Q. Zhang, Eunsol Choi*

**Main category:** cs.CL

**Keywords:** implicit user feedback, language models, user interaction, model performance, HCI

**Relevance Score:** 8

**TL;DR:** This study focuses on harvesting implicit user feedback from interactions with language models (LMs) to improve model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve continuity and evolution of LMs based on long-term user feedback without being disruptive through direct requests for feedback.

**Method:** Analyzed implicit user feedback from two interaction datasets, examining the trajectory of user-LLM conversations and the learning signals from feedback.

**Key Contributions:**

	1. In-depth analysis of implicit user feedback in user-LLM interactions
	2. Insights into the timing and reasoning behind user feedback
	3. Empirical findings on feedback content's impact on model performance

**Result:** User feedback content improvements can enhance model performance, especially for short questions, but is limited by the complexity of questions and quality of initial prompts.

**Limitations:** User feedback effectiveness is limited on longer, complex questions due to the quality of initial prompts.

**Conclusion:** Implicit user feedback has its potential and limitations, particularly influenced by the quality of user prompts.

**Abstract:** Once language models (LMs) are deployed, they can interact with users long-term, ideally evolving continuously based on their feedback. Asking for direct user feedback can be disruptive; thus, we study harvesting user feedback from user-LM interaction logs. We study implicit user feedback in two user-LM interaction datasets (WildChat and LMSYS). First, we analyze user feedback in the user-LLM conversation trajectory, providing insights into when and why such feedback occurs. Second, we study harvesting learning signals from such implicit user feedback. We find that the contents of user feedback (e.g., user wanted clarification), not just the polarity (e.g., users were unhappy with the previous model response), can improve model performance in short human-designed questions (MTBench) but not on longer and more complex questions (WildBench). We also find that the usefulness of user feedback is largely tied to the quality of the user's initial prompt. Together, we provide an in-depth study of implicit user feedback, showing its potential and limitations.

</details>


### [77] [LENS: Learning Ensemble Confidence from Neural States for Multi-LLM Answer Integration](https://arxiv.org/abs/2507.23167)

*Jizhou Guo*

**Main category:** cs.CL

**Keywords:** Large Language Models, Ensemble Learning, Model Confidence, Internal Representations, Machine Learning

**Relevance Score:** 9

**TL;DR:** LENS is a novel method for enhancing LLM ensemble performance by learning model confidence from internal representations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current ensemble methods for LLMs fail to account for the varying confidence and reliability of models in different contexts, leading to suboptimal performance.

**Method:** LENS trains a lightweight linear confidence predictor for each LLM using layer-wise hidden states and normalized probabilities as inputs, allowing for context-dependent weighting of predictions.

**Key Contributions:**

	1. Introduction of LENS for ensemble learning of LLMs
	2. Context-dependent confidence estimation based on internal model states
	3. Demonstrated improvements in performance over traditional methods

**Result:** LENS significantly outperforms traditional ensemble methods on multiple-choice and boolean question-answering tasks, demonstrating the value of internal model representations in confidence estimation.

**Limitations:** 

**Conclusion:** Internal representations can effectively inform model confidence in ensemble learning, providing robust improvements over traditional techniques.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive performance across various tasks, with different models excelling in distinct domains and specific abilities. Effectively combining the predictions of multiple LLMs is crucial for enhancing system robustness and performance. However, existing ensemble methods often rely on simple techniques like voting or logits ensembling, which overlook the varying confidence and reliability of models in different contexts. In this work, we propose LENS (Learning ENsemble confidence from Neural States), a novel approach that learns to estimate model confidence by analyzing internal representations. For each LLM, we train a lightweight linear confidence predictor that leverages layer-wise hidden states and normalized probabilities as inputs. This allows for more nuanced weighting of model predictions based on their context-dependent reliability. Our method does not require modifying the model parameters and requires negligible additional computation. Experimental results on multiple-choice and boolean question-answering tasks demonstrate that LENS outperforms traditional ensemble methods by a substantial margin. Our findings suggest that internal representations provide valuable signals for determining model confidence and can be effectively leveraged for ensemble learning.

</details>


### [78] [Geak: Introducing Triton Kernel AI Agent & Evaluation Benchmarks](https://arxiv.org/abs/2507.23194)

*Jianghui Wang, Vinay Joshi, Saptarshi Majumder, Xu Chao, Bin Ding, Ziqiong Liu, Pratik Prabhanjan Brahma, Dong Li, Zicheng Liu, Emad Barsoum*

**Main category:** cs.CL

**Keywords:** GPU kernels, AI code generation, Triton programming language, LLMs, AMD GPUs

**Relevance Score:** 4

**TL;DR:** This paper presents GEAK, a framework for generating efficient GPU kernels using LLMs, specifically targeting AMD GPUs with the Triton programming language, outperforming traditional methods in both correctness and execution speed.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing complexity of deep learning workloads necessitates the automation of GPU kernel development to improve performance and productivity in hardware-optimized solutions.

**Method:** An evaluation suite for Triton-based GPU kernels is introduced along with GEAK, which uses LLMs and a reasoning loop inspired by Reflexion-style mechanisms to produce optimized Triton code for AMD GPUs.

**Key Contributions:**

	1. Introduction of GEAK for generating efficient Triton GPU kernels
	2. Evaluation suite for assessing Triton kernel performance
	3. Demonstrated significant improvements in correctness and execution speed over existing methods.

**Result:** GEAK achieved correctness rates up to 63% and execution speed improvements of up to 2.59X over conventional prompting and generation pipelines.

**Limitations:** 

**Conclusion:** GEAK demonstrates the potential of LLM-driven code generation to enhance access and optimize performance across diverse hardware platforms, particularly in GPU programming.

**Abstract:** The demand for AI-generated GPU kernels is rapidly growing, influenced by the need for scalable, hardware-optimized solutions in both industry and academia. As deep learning workloads grow in complexity and diversity, it is imperative to automate low-level kernel development to meet performance and productivity demands. Major cloud providers, semiconductor companies, and research institutions are now investing heavily in AI-driven code generation for GPUs, aiming to reduce manual optimization efforts while achieving near-expert performance on hardware like AMD MI300X. The Triton language, a Python-based DSL for GPU programming, has emerged as a popular target for such AI-generated kernels due to its balance of performance and ease-of-coding. In this work, we present an evaluation suite for Triton-based GPU kernels and GEAK (Generating Efficient AI-centric GPU Kernels)-a framework that leverages cutting-edge LLMs to generate performant Triton code specifically for AMD GPUs, including the AMD MI300X and MI250. GEAK leverages inference-time compute scaling to produce Triton-based GPU kernels using a reasoning loop adapted from Reflexion-style feedback mechanisms. On two evaluation benchmarks, GEAK significantly outperformed the baselines of directly prompting frontier LLMs as well as Reflexion-based generation pipelines by achieving correctness up to $63$% and execution speed up of up to $2.59$X. These results highlight the promise of GEAK-like agentic code generation for accelerating the adoption of diverse hardware platforms and democratizing access to expert-level kernel performance.

</details>


### [79] [Failures Are the Stepping Stones to Success: Enhancing Few-Shot In-Context Learning by Leveraging Negative Samples](https://arxiv.org/abs/2507.23211)

*Yunhao Liang, Ruixuan Ying, Takuya Taniguchi, Zhe Cui*

**Main category:** cs.CL

**Keywords:** Few-shot learning, In-context learning, Negative samples, Large language models, Semantic similarity

**Relevance Score:** 8

**TL;DR:** This paper presents a method that enhances few-shot in-context learning (ICL) in large language models by utilizing negative samples to improve positive sample selection during inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of existing methods which mostly focus on positive samples, neglecting the potential benefits of negative samples in few-shot ICL processes.

**Method:** The proposed method constructs positive and negative sample corpora based on Zero-Shot-Cot. It uses a semantic similarity approach during inference to select related examples from both corpora, and further refines positive examples based on the semantic proximity to negative examples.

**Key Contributions:**

	1. Introduction of negative samples in ICL for enhanced positive sample selection
	2. Development of a semantic similarity-based selection method
	3. Demonstrated improvement in performance over existing positive-only approaches

**Result:** Experimental results show that the proposed method outperforms existing techniques that rely only on the most similar positive samples, indicating that negative samples contribute positively to ICL performance through better positive sample selection.

**Limitations:** 

**Conclusion:** Incorporating negative samples in the example selection process improves the efficacy of few-shot ICL in large language models, leading to better performance in semantic task applications.

**Abstract:** Large Language Models exhibit powerful few-shot in-context learning (ICL) capabilities, but the performance is highly sensitive to provided examples.   Recent research has focused on retrieving corresponding examples for each input query, not only enhancing the efficiency and scalability of the learning process but also mitigating inherent biases in manual example selection.   However, these studies have primarily emphasized leveraging Positive samples while overlooking the additional information within Negative samples for contextual learning.   We propose a novel method that utilizes Negative samples to better select Positive sample examples, thereby enhancing the performance of few-shot ICL. Initially, we construct Positive and Negative sample corpora based on Zero-Shot-Cot. Then, during inference, we employ a semantic similarity-based approach to select the most similar examples from both the Positive and Negative corpora for a given query. Subsequently, we further retrieve Positive examples from the Positive sample corpus based on semantic similarity to the Negative examples, then concatenating them with the previously selected Positive examples to serve as ICL demonstrations. Experimental results demonstrate that our approach surpasses methods solely relying on the most similar positive examples for context, validating that the additional information in negative samples aids in enhancing ICL performance through improved Positive sample selection.

</details>


### [80] [Model Directions, Not Words: Mechanistic Topic Models Using Sparse Autoencoders](https://arxiv.org/abs/2507.23220)

*Carolina Zheng, Nicolas Beltran-Velez, Sweta Karlekar, Claudia Shi, Achille Nazaret, Asif Mallik, Amir Feder, David M. Blei*

**Main category:** cs.CL

**Keywords:** topic modeling, sparse autoencoders, LLM evaluation, text generation, semantically rich features

**Relevance Score:** 7

**TL;DR:** Mechanistic Topic Models (MTMs) enhance traditional topic modeling by using interpretable features from sparse autoencoders, allowing for richer semantic understanding and controllable text generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional topic models struggle to capture complex topics due to their reliance on bag-of-words representations. There is a need for models that can articulate deeper conceptual themes and enable controllable text generation.

**Method:** The paper introduces Mechanistic Topic Models (MTMs) which use sparse autoencoders to learn interpretable features for defining topics in a semantically rich space. It also presents an LLM-based evaluation framework named 'topic judge' for assessing these topics against traditional models.

**Key Contributions:**

	1. Introduction of Mechanistic Topic Models (MTMs) using sparse autoencoders
	2. Development of LLM-based 'topic judge' for evaluation
	3. Demonstration of controllable text generation through topic-based steering vectors

**Result:** MTMs demonstrate comparable or superior performance on coherence metrics compared to traditional and neural topic models across five datasets, and are preferred in evaluations by the topic judge.

**Limitations:** 

**Conclusion:** The results indicate that MTMs successfully capture complex topics and facilitate controllable generation of text, marking a significant advancement in topic modeling techniques.

**Abstract:** Traditional topic models are effective at uncovering latent themes in large text collections. However, due to their reliance on bag-of-words representations, they struggle to capture semantically abstract features. While some neural variants use richer representations, they are similarly constrained by expressing topics as word lists, which limits their ability to articulate complex topics. We introduce Mechanistic Topic Models (MTMs), a class of topic models that operate on interpretable features learned by sparse autoencoders (SAEs). By defining topics over this semantically rich space, MTMs can reveal deeper conceptual themes with expressive feature descriptions. Moreover, uniquely among topic models, MTMs enable controllable text generation using topic-based steering vectors. To properly evaluate MTM topics against word-list-based approaches, we propose \textit{topic judge}, an LLM-based pairwise comparison evaluation framework. Across five datasets, MTMs match or exceed traditional and neural baselines on coherence metrics, are consistently preferred by topic judge, and enable effective steering of LLM outputs.

</details>


### [81] [Enabling Few-Shot Alzheimer's Disease Diagnosis on Tabular Biomarker Data with LLMs](https://arxiv.org/abs/2507.23227)

*Sophie Kearney, Shu Yang, Zixuan Wen, Bojian Hou, Duy Duong-Tran, Tianlong Chen, Jason Moore, Marylyn Ritchie, Li Shen*

**Main category:** cs.CL

**Keywords:** Alzheimer's disease, large language models, tabular data, biomedical informatics, prediction framework

**Relevance Score:** 9

**TL;DR:** The paper presents TAP-GPT, a framework using LLMs for Alzheimer's disease diagnosis leveraging structured biomarker data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve early and accurate diagnosis of Alzheimer's disease through advanced analysis of diverse biomarker data.

**Method:** The TAP-GPT framework adapts TableGPT2 for clinical binary classification tasks, utilizing few-shot tabular prompts and finetuning with qLoRA.

**Key Contributions:**

	1. Introduction of TAP-GPT for Alzheimer's disease diagnosis using LLMs.
	2. Utilization of few-shot learning with structured biomarker data.
	3. Demonstration of improved performance over existing models.

**Result:** TAP-GPT demonstrates superior performance over general-purpose LLMs and dedicated tabular models in predicting Alzheimer's disease.

**Limitations:** 

**Conclusion:** This novel framework successfully integrates LLMs with structured biomedical data, facilitating more effective prediction of Alzheimer's disease.

**Abstract:** Early and accurate diagnosis of Alzheimer's disease (AD), a complex neurodegenerative disorder, requires analysis of heterogeneous biomarkers (e.g., neuroimaging, genetic risk factors, cognitive tests, and cerebrospinal fluid proteins) typically represented in a tabular format. With flexible few-shot reasoning, multimodal integration, and natural-language-based interpretability, large language models (LLMs) offer unprecedented opportunities for prediction with structured biomedical data. We propose a novel framework called TAP-GPT, Tabular Alzheimer's Prediction GPT, that adapts TableGPT2, a multimodal tabular-specialized LLM originally developed for business intelligence tasks, for AD diagnosis using structured biomarker data with small sample sizes. Our approach constructs few-shot tabular prompts using in-context learning examples from structured biomedical data and finetunes TableGPT2 using the parameter-efficient qLoRA adaption for a clinical binary classification task of AD or cognitively normal (CN). The TAP-GPT framework harnesses the powerful tabular understanding ability of TableGPT2 and the encoded prior knowledge of LLMs to outperform more advanced general-purpose LLMs and a tabular foundation model (TFM) developed for prediction tasks. To our knowledge, this is the first application of LLMs to the prediction task using tabular biomarker data, paving the way for future LLM-driven multi-agent frameworks in biomedical informatics.

</details>


### [82] [P-ReMIS: Pragmatic Reasoning in Mental Health and a Social Implication](https://arxiv.org/abs/2507.23247)

*Sneha Oram, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** mental health, large language models, explainability, chatbots, implying meaning

**Relevance Score:** 9

**TL;DR:** This paper investigates the reasoning capabilities of LLMs in mental health by introducing the P-ReMe dataset and conducting experiments with various models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the unexplored aspects of explainability and dialogue in mental health chatbots using LLMs.

**Method:** The study introduces a modified definition of implicature and presupposition, formulates tasks based on these, and benchmarks them using models like Llama3.1, Mistral, MentaLLaMa, and Qwen.

**Key Contributions:**

	1. Introduction of the P-ReMe dataset
	2. Modified definitions of implicature and presupposition in mental health
	3. Assessment of LLMs for handling mental health stigma

**Result:** Mistral and Qwen displayed significant reasoning abilities in the mental health domain. Additionally, Claude-3.5-haiku was found to handle stigma around mental health more responsibly than GPT-4o mini and Deepseek-chat.

**Limitations:** 

**Conclusion:** The findings highlight the potential of LLMs in enhancing dialogue in mental health contexts and addressing stigma responsibly.

**Abstract:** There has been an increase in recent advancements in the explainability and development of personalized chatbots for mental health. However, the reasoning aspects for explainability and dialogue discourse have not been explored previously for mental health. Hence, we are investigating the pragmatic reasoning capability of large language models (LLMs) in this domain. We introduce P-ReMe dataset, and propose a modified definition for the pragmatic phenomena of implicature (implied meaning) and presupposition (implicit assumption) in mental health. Following the definition, we formulate two tasks in implicature and one task in presupposition. To benchmark the dataset and the presented tasks, we consider four models - Llama3.1, Mistral, MentaLLaMa, and Qwen. The results of the experiments suggest that Mistral and Qwen show substantial reasoning capabilities in the domain. In addition, we also propose StiPRompts to study the stigma around mental health with the state-of-the-art LLMs, GPT-4o mini, Deepseek-chat, and Claude-3.5-haiku. Our evaluated findings show that Claude-3.5-haiku deals with the stigma more responsibly compared to the other two LLMs.

</details>


### [83] [Evaluating LLMs' Multilingual Capabilities for Bengali: Benchmark Creation and Performance Analysis](https://arxiv.org/abs/2507.23248)

*Shimanto Bhowmik, Tawsif Tashwar Dipto, Md Sazzad Islam, Sheryl Hsu, Tahsin Reasat*

**Main category:** cs.CL

**Keywords:** Bengali, NLP, Large Language Models, evaluation benchmarks, tokenization

**Relevance Score:** 6

**TL;DR:** This paper investigates challenges in Bengali NLP, evaluates recent LLMs, performs error analysis, and identifies performance gaps compared to English, emphasizing the need for better evaluation methodologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Bengali is underrepresented in NLP research due to unique linguistic structures and lack of standardized benchmarks.

**Method:** The study evaluates 10 recent open-source LLMs on 8 translated datasets, conducting error analysis to understand performance gaps.

**Key Contributions:**

	1. Systematic investigation of Bengali NLP challenges.
	2. Evaluation of multiple LLMs on translated datasets.
	3. Identification of tokenization efficiency impacting model accuracy.

**Result:** Findings indicate consistent performance gaps for Bengali compared to English, especially in smaller models, and an inverse relationship between tokenization efficiency and LLM accuracy.

**Limitations:** Focuses primarily on Bengali; findings may not generalize to other underrepresented languages without further research.

**Conclusion:** The research highlights areas needing improvement in dataset quality and evaluation methodologies for underrepresented languages, aiming to democratize access to NLP technologies.

**Abstract:** Bengali is an underrepresented language in NLP research. However, it remains a challenge due to its unique linguistic structure and computational constraints. In this work, we systematically investigate the challenges that hinder Bengali NLP performance by focusing on the absence of standardized evaluation benchmarks. We then evaluated 10 recent open source Large Language Models (LLMs) in 8 of the translated datasets and performed a comprehensive error analysis to pinpoint their primary failure modes. Our findings reveal consistent performance gaps for Bengali compared to English, particularly for smaller models and specific model families like Mistral. We also identified promising robustness in certain architectures, such as DeepSeek, that maintain more stable performance across languages. Our analysis reveals an inverse relationship between tokenization efficiency and LLM accuracy where models tend to perform worse when inputs are excessively tokenized, whereas more efficient \& concise tokenization results in improved performance. These findings highlight critical areas where current models fall short and underscore the need for improved dataset quality and evaluation methodologies tailored to multilingual contexts. This work will catalyze further research on NLP for underrepresented languages, helping to democratize access to advanced language technologies worldwide. The code and dataset used in this research is publicly available at https://github.com/BengaliAI/bn-llm-benchmark.

</details>


### [84] [Unveiling Super Experts in Mixture-of-Experts Large Language Models](https://arxiv.org/abs/2507.23279)

*Zunhai Su, Qingyuan Li, Hao Zhang, YuLei Qian, Yuchen Xie, Kehong Yuan*

**Main category:** cs.CL

**Keywords:** Mixture-of-Experts, Super Experts, Machine Learning, Large Language Models, Model Compression

**Relevance Score:** 9

**TL;DR:** This study investigates Super Experts in Mixture-of-Experts LLMs, revealing their crucial role in model performance and the implications of their pruning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of understanding regarding the importance of different experts in MoE LLMs and improve their efficiency.

**Method:** Analysis of expert activation patterns and performance impact through pruning in various tasks with open-source MoE LLMs.

**Key Contributions:**

	1. Identification of Super Experts (SEs) in MoE LLMs and their characteristics.
	2. Analysis of SEs' impact on model performance and attention distribution.
	3. Open-source code to assist in further research and exploration of SEs.

**Result:** Identified Super Experts (SEs) that are critical for maintaining model performance, especially in tasks requiring mathematical reasoning, and demonstrated that pruning SEs leads to significant declines in output quality.

**Limitations:** The findings are based on specific models and may not generalize to all MoE LLMs; further research is needed to explore SEs in different architectures.

**Conclusion:** SEs are essential for the model's performance, particularly in specific tasks, and their pruning severely impacts the distribution of attention scores within the model.

**Abstract:** Sparsely activated Mixture-of-Experts (MoE) models have shown promise in enhancing the learning capacity of large language models (LLMs). Leveraging the intrinsic importance differences among experts, recent research has explored expert-level compression techniques to improve the efficiency of MoE LLMs. However, existing approaches often rely on empirical criteria to identify critical experts, lacking a deeper exploration and understanding of the heterogeneous importance of experts. In this study, we present the first discovery and investigation of a distinct subset of experts that play a crucial role in the underlying mechanisms during the model's forward inference. These experts are prevalent in open-source MoE LLMs, and despite their limited number, pruning them leads to a significant decline in model performance (e.g., pruning three causes Qwen3-30B-A3B to produce repetitive and uninformative outputs). We refer to these experts as Super Experts (SEs). Our comprehensive analysis provides progressively deeper insights into SEs. (i) SEs are characterized by rare but extreme activation outliers in the output of the down_proj, which give rise to massive activations in the hidden states between decoder layers. Moreover, the distribution of SEs remains model-specific and is unaffected by post-training processes. (ii) By pruning SEs, we assess their significance across a variety of tasks, revealing their considerable impact on the model's overall performance, particularly in mathematical reasoning. (iii) We further enhance our understanding of the influence of SEs compression. Our findings confirm that MoE LLMs rely on SEs to induce attention sinks, which are crucial for the distribution of attention scores but are significantly disrupted by SE pruning. The code is available at https://github.com/ZunhaiSu/Super-Experts-Profilling.

</details>


### [85] [What's Taboo for You? - An Empirical Evaluation of LLMs Behavior Toward Sensitive Content](https://arxiv.org/abs/2507.23319)

*Alfio Ferrara, Sergio Picascia, Laura Pinnavaia, Vojimir Ranitovic, Elisabetta Rocchetti, Alice Tuveri*

**Main category:** cs.CL

**Keywords:** Large Language Models, implicit moderation, paraphrasing, sensitivity classification, GPT-4o-mini

**Relevance Score:** 8

**TL;DR:** This study examines the implicit moderation behavior of GPT-4o-mini in paraphrasing sensitive content and its ability to classify sentence sensitivity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether proprietary LLMs, specifically GPT-4o-mini, implicitly sanitize language during paraphrasing without explicit guidance.

**Method:** Empirical analysis of GPT-4o-mini's behavior when paraphrasing sensitive content, alongside evaluation of zero-shot classification capabilities against traditional methods.

**Key Contributions:**

	1. Empirical analysis of GPT-4o-mini's implicit moderation behavior.
	2. Evaluation of LLMs' zero-shot classification capabilities regarding sentence sensitivity.
	3. Comparison of LLM performance against traditional methods.

**Result:** Findings show that GPT-4o-mini systematically reduces sensitive content, indicating substantial implicit moderation towards less sensitive classes.

**Limitations:** The study focuses only on GPT-4o-mini, so findings may not generalize to other models.

**Conclusion:** The results suggest that LLMs can effectively moderate language without direct training or instructions, highlighting potential implications for safe content generation.

**Abstract:** Proprietary Large Language Models (LLMs) have shown tendencies toward politeness, formality, and implicit content moderation. While previous research has primarily focused on explicitly training models to moderate and detoxify sensitive content, there has been limited exploration of whether LLMs implicitly sanitize language without explicit instructions. This study empirically analyzes the implicit moderation behavior of GPT-4o-mini when paraphrasing sensitive content and evaluates the extent of sensitivity shifts. Our experiments indicate that GPT-4o-mini systematically moderates content toward less sensitive classes, with substantial reductions in derogatory and taboo language. Also, we evaluate the zero-shot capabilities of LLMs in classifying sentence sensitivity, comparing their performances against traditional methods.

</details>


### [86] [MUST-RAG: MUSical Text Question Answering with Retrieval Augmented Generation](https://arxiv.org/abs/2507.23334)

*Daeyong Kwon, SeungHeon Doh, Juhan Nam*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval Augmented Generation, Music Question Answering, MusWikiDB, AI

**Relevance Score:** 4

**TL;DR:** MusT-RAG is a framework that enhances the performance of general-purpose LLMs for music-related question answering by using a music-specific vector database and retrieval techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of LLMs in music-related applications, as their training data lacks sufficient music-specific knowledge.

**Method:** MusT-RAG integrates Retrieval Augmented Generation (RAG) to adapt LLMs for music question answering, utilizing a specialized music vector database (MusWikiDB) for improved retrieval of context during inference and fine-tuning.

**Key Contributions:**

	1. Introduction of MusT-RAG framework for music question answering
	2. Development of MusWikiDB as a specialized music retrieval database
	3. Demonstration of significant performance improvements over traditional methods

**Result:** MusT-RAG significantly outperforms traditional fine-tuning methods in adapting LLMs for music tasks, showing improvements on both in-domain and out-of-domain benchmarks.

**Limitations:** 

**Conclusion:** The MusWikiDB and the RAG approach provide a more efficient and effective way to utilize LLMs in music-focused applications, leading to better performance than using general databases.

**Abstract:** Recent advancements in Large language models (LLMs) have demonstrated remarkable capabilities across diverse domains. While they exhibit strong zero-shot performance on various tasks, LLMs' effectiveness in music-related applications remains limited due to the relatively small proportion of music-specific knowledge in their training data. To address this limitation, we propose MusT-RAG, a comprehensive framework based on Retrieval Augmented Generation (RAG) to adapt general-purpose LLMs for text-only music question answering (MQA) tasks. RAG is a technique that provides external knowledge to LLMs by retrieving relevant context information when generating answers to questions. To optimize RAG for the music domain, we (1) propose MusWikiDB, a music-specialized vector database for the retrieval stage, and (2) utilizes context information during both inference and fine-tuning processes to effectively transform general-purpose LLMs into music-specific models. Our experiment demonstrates that MusT-RAG significantly outperforms traditional fine-tuning approaches in enhancing LLMs' music domain adaptation capabilities, showing consistent improvements across both in-domain and out-of-domain MQA benchmarks. Additionally, our MusWikiDB proves substantially more effective than general Wikipedia corpora, delivering superior performance and computational efficiency.

</details>


### [87] [Text-to-SQL Task-oriented Dialogue Ontology Construction](https://arxiv.org/abs/2507.23358)

*Renato Vukovic, Carel van Niekerk, Michael Heck, Benjamin Ruppik, Hsien-Chin Lin, Shutong Feng, Nurul Lubis, Milica Gasic*

**Main category:** cs.CL

**Keywords:** large language models, task-oriented dialogue systems, ontology construction, explainability, dialogue theory

**Relevance Score:** 9

**TL;DR:** TeQoDO is a method for autonomously constructing task-oriented dialogue ontologies using large language models without supervision, enhancing explainability in dialogue systems.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs have limitations in explainability and trustworthiness when used as knowledge sources in task-oriented dialogue systems, necessitating structured ontologies.

**Method:** TeQoDO uses an LLM to autonomously build a dialogue ontology from scratch by leveraging its SQL programming capabilities and dialogue theory provided in the prompt, without requiring manual labels or supervised training.

**Key Contributions:**

	1. Introduction of TeQoDO for ontology construction using LLMs without supervision
	2. Demonstration of performance improvement over transfer learning methods
	3. Scalability to larger ontologies using external datasets

**Result:** TeQoDO outperforms existing transfer learning approaches, and the constructed ontology is competitive for downstream dialogue state tracking tasks. It also supports the construction of larger ontologies, as demonstrated with Wikipedia and ArXiv datasets.

**Limitations:** The method's effectiveness may depend on the quality and diversity of the dialogue theory used in the prompts.

**Conclusion:** TeQoDO represents progress towards enhancing the explainability of LLMs in dialogue systems through the automated construction of ontologies.

**Abstract:** Large language models (LLMs) are widely used as general-purpose knowledge sources, but they rely on parametric knowledge, limiting explainability and trustworthiness. In task-oriented dialogue (TOD) systems, this separation is explicit, using an external database structured by an explicit ontology to ensure explainability and controllability. However, building such ontologies requires manual labels or supervised training. We introduce TeQoDO: a Text-to-SQL task-oriented Dialogue Ontology construction method. Here, an LLM autonomously builds a TOD ontology from scratch without supervision using its inherent SQL programming capabilities combined with dialogue theory provided in the prompt. We show that TeQoDO outperforms transfer learning approaches, and its constructed ontology is competitive on a downstream dialogue state tracking task. Ablation studies demonstrate the key role of dialogue theory. TeQoDO also scales to allow construction of much larger ontologies, which we investigate on a Wikipedia and ArXiv dataset. We view this as a step towards broader application of ontologies to increase LLM explainability.

</details>


### [88] [MPCC: A Novel Benchmark for Multimodal Planning with Complex Constraints in Multimodal Large Language Models](https://arxiv.org/abs/2507.23382)

*Yiyan Ji, Haoran Chen, Qiguang Chen, Chengyue Wu, Libo Qin, Wanxiang Che*

**Main category:** cs.CL

**Keywords:** multimodal planning, MLLMs, benchmark, constraints, real-world tasks

**Relevance Score:** 7

**TL;DR:** This paper introduces the Multimodal Planning with Complex Constraints (MPCC) benchmark, designed to evaluate the ability of Multimodal Large Language Models (MLLMs) in handling real-world planning tasks with implicit multimodal constraints.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in assessing MLLMs' capabilities in multimodal planning and the lack of effective benchmarks that incorporate constraints across modalities.

**Method:** The benchmark focuses on three real-world tasks (Flight Planning, Calendar Planning, and Meeting Planning) and introduces complex constraints (budget, temporal, spatial) with graded difficulty levels (EASY, MEDIUM, HARD).

**Key Contributions:**

	1. Introduction of the MPCC benchmark for multimodal planning
	2. Focus on three specific real-world planning tasks
	3. Identification of significant challenges faced by MLLMs in planning with complex constraints

**Result:** Experiments reveal that closed-source models achieve 21.3% feasible plans, while open-source models average below 11%, indicating significant challenges in planning under constraints.

**Limitations:** The study highlights the sensitivity of MLLMs to constraint complexity and limitations of traditional multimodal prompting strategies under multi-constraint scenarios.

**Conclusion:** The work formalizes multimodal constraints in task planning, providing a new evaluation framework and calling for advancements in constraint-aware reasoning for real-world MLLM applications.

**Abstract:** Multimodal planning capabilities refer to the ability to predict, reason, and design steps for task execution with multimodal context, which is essential for complex reasoning and decision-making across multiple steps. However, current benchmarks face two key challenges: (1) they cannot directly assess multimodal real-world planning capabilities, and (2) they lack constraints or implicit constraints across modalities. To address these issues, we introduce Multimodal Planning with Complex Constraints (MPCC), the first benchmark to systematically evaluate MLLMs' ability to handle multimodal constraints in planning. To address the first challenge, MPCC focuses on three real-world tasks: Flight Planning, Calendar Planning, and Meeting Planning. To solve the second challenge, we introduce complex constraints (e.g. budget, temporal, and spatial) in these tasks, with graded difficulty levels (EASY, MEDIUM, HARD) to separate constraint complexity from search space expansion. Experiments on 13 advanced MLLMs reveal significant challenges: closed-source models achieve only 21.3% feasible plans, while open-source models average below 11%. Additionally, we observe that MLLMs are highly sensitive to constraint complexity and that traditional multimodal prompting strategies fail in multi-constraint scenarios. Our work formalizes multimodal constraints in planning, provides a rigorous evaluation framework, and highlights the need for advancements in constraint-aware reasoning for real-world MLLM applications.

</details>


### [89] [Causal2Vec: Improving Decoder-only LLMs as Versatile Embedding Models](https://arxiv.org/abs/2507.23386)

*Ailiang Lin, Zhuoyun Li, Kotaro Funakoshi*

**Main category:** cs.CL

**Keywords:** Causal2Vec, embedding models, large language models, natural language processing, computational efficiency

**Relevance Score:** 8

**TL;DR:** Causal2Vec enhances decoder-only LLMs for embedding tasks without altering their architecture, achieving state-of-the-art performance while significantly reducing computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods modify causal attention in LLMs, potentially degrading their ability to extract semantic information, leading to increased computational costs.

**Method:** Causal2Vec uses a BERT-style model to create a Contextual token, which is added to the LLM's input sequence to capture contextual information. It combines this Contextual token with the last hidden states of other tokens to generate embeddings.

**Key Contributions:**

	1. Introduction of a Contextual token for better semantic information capture
	2. Significant reduction in sequence length and inference time
	3. State-of-the-art performance on MTEB benchmarks

**Result:** Causal2Vec shows state-of-the-art results on the MTEB, reducing required sequence length by up to 85% and inference time by 82% compared to top methods.

**Limitations:** 

**Conclusion:** Causal2Vec improves existing practices in embedding model design for decoder-only LLMs without compromising architecture or increasing costs.

**Abstract:** Decoder-only large language models (LLMs) are increasingly used to build embedding models that effectively encode the semantic information of natural language texts into dense vector representations for various embedding tasks. However, many existing methods primarily focus on removing the causal attention mask in LLMs to enable bidirectional attention, potentially undermining the model's ability to extract semantic information acquired during pretraining. Additionally, leading unidirectional approaches often rely on extra input text to overcome the inherent limitations of causal attention, inevitably increasing computational costs. In this work, we propose Causal2Vec, a general-purpose embedding model tailored to enhance the performance of decoder-only LLMs without altering their original architectures or introducing significant computational overhead. Specifically, we first employ a lightweight BERT-style model to pre-encode the input text into a single Contextual token, which is then prepended to the LLM's input sequence, allowing each token to capture contextualized information even without attending to future tokens. Furthermore, to mitigate the recency bias introduced by last-token pooling and help LLMs better leverage the semantic information encoded in the Contextual token, we concatenate the last hidden states of Contextual and EOS tokens as the final text embedding. In practice, Causal2Vec achieves state-of-the-art performance on the Massive Text Embeddings Benchmark (MTEB) among models trained solely on publicly available retrieval datasets, while reducing the required sequence length by up to 85% and inference time by up to 82% compared to best-performing methods.

</details>


### [90] [Beyond the Cloud: Assessing the Benefits and Drawbacks of Local LLM Deployment for Translators](https://arxiv.org/abs/2507.23399)

*Peter Sandrini*

**Main category:** cs.CL

**Keywords:** Large Language Models, local deployment, data privacy, translation studies, open-source models

**Relevance Score:** 7

**TL;DR:** This paper explores the potential of locally deployable, free language models as alternatives to commercial cloud-based AI chatbots in translation, emphasizing data privacy and accessibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges and opportunities posed by the rapid growth of Large Language Models in translation, highlighting the need for alternative deployment models due to data privacy and access concerns.

**Method:** The study evaluates three open-source language models installed on CPU-based platforms and compares their performance with commercial AI chatbots, focusing on functional performance.

**Key Contributions:**

	1. Evaluation of local language models for translation
	2. Focus on data privacy and control
	3. Support for democratization of AI technology

**Result:** The findings underscore the advantages of local deployment, including better data control and privacy, while recognizing the challenges it presents.

**Limitations:** 

**Conclusion:** Local language models can democratize AI technology, providing accessible options for individual translators and small businesses, despite the inherent challenges of local deployment.

**Abstract:** The rapid proliferation of Large Language Models presents both opportunities and challenges for the translation field. While commercial, cloud-based AI chatbots have garnered significant attention in translation studies, concerns regarding data privacy, security, and equitable access necessitate exploration of alternative deployment models. This paper investigates the feasibility and performance of locally deployable, free language models as a viable alternative to proprietary, cloud-based AI solutions. This study evaluates three open-source models installed on CPU-based platforms and compared against commercially available online chat-bots. The evaluation focuses on functional performance rather than a comparative analysis of human-machine translation quality, an area already subject to extensive research. The platforms assessed were chosen for their accessibility and ease of use across various operating systems. While local deployment introduces its own challenges, the benefits of enhanced data control, improved privacy, and reduced dependency on cloud services are compelling. The findings of this study contribute to a growing body of knowledge concerning the democratization of AI technology and inform future research and development efforts aimed at making LLMs more accessible and practical for a wider range of users, specifically focusing on the needs of individual translators and small businesses.

</details>


### [91] [MRGSEM-Sum: An Unsupervised Multi-document Summarization Framework based on Multi-Relational Graphs and Structural Entropy Minimization](https://arxiv.org/abs/2507.23400)

*Yongbing Zhang, Fang Nan, Shengxiang Gao, Yuxin Huang, Kaiwen Tan, Zhengtao Yu*

**Main category:** cs.CL

**Keywords:** multi-document summarization, graph clustering, structural entropy minimization, semantic relations, discourse relations

**Relevance Score:** 6

**TL;DR:** MRGSEM-Sum is an unsupervised multi-document summarization framework that employs multi-relational graphs and structural entropy minimization to generate concise summaries, outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing multi-document summarization methods that only consider single-relational graphs and require a predefined number of clusters, which restricts their adaptability and redundancy reduction capabilities.

**Method:** The framework constructs a multi-relational graph to model semantic and discourse relations among sentences and then applies a two-dimensional structural entropy minimization algorithm to determine optimal clustering rather than using a predefined number of clusters. Additionally, a position-aware compression mechanism is introduced to create concise summaries.

**Key Contributions:**

	1. Introduction of multi-relational graphs for summarization
	2. Implementation of structural entropy minimization for adaptive clustering
	3. Position-aware compression mechanism for enhanced summary quality

**Result:** Experiments on four benchmark datasets showcase MRGSEM-Sum's performance consistently outperforming previous unsupervised techniques and, in some instances, matching the results of supervised models and large language models.

**Limitations:** 

**Conclusion:** MRGSEM-Sum produces high-quality summaries that exhibit human-level consistency and coverage, indicating its effectiveness in multi-document summarization tasks.

**Abstract:** The core challenge faced by multi-document summarization is the complexity of relationships among documents and the presence of information redundancy. Graph clustering is an effective paradigm for addressing this issue, as it models the complex relationships among documents using graph structures and reduces information redundancy through clustering, achieving significant research progress. However, existing methods often only consider single-relational graphs and require a predefined number of clusters, which hinders their ability to fully represent rich relational information and adaptively partition sentence groups to reduce redundancy. To overcome these limitations, we propose MRGSEM-Sum, an unsupervised multi-document summarization framework based on multi-relational graphs and structural entropy minimization. Specifically, we construct a multi-relational graph that integrates semantic and discourse relations between sentences, comprehensively modeling the intricate and dynamic connections among sentences across documents. We then apply a two-dimensional structural entropy minimization algorithm for clustering, automatically determining the optimal number of clusters and effectively organizing sentences into coherent groups. Finally, we introduce a position-aware compression mechanism to distill each cluster, generating concise and informative summaries. Extensive experiments on four benchmark datasets (Multi-News, DUC-2004, PubMed, and WikiSum) demonstrate that our approach consistently outperforms previous unsupervised methods and, in several cases, achieves performance comparable to supervised models and large language models. Human evaluation demonstrates that the summaries generated by MRGSEM-Sum exhibit high consistency and coverage, approaching human-level quality.

</details>


### [92] [Enhanced Arabic Text Retrieval with Attentive Relevance Scoring](https://arxiv.org/abs/2507.23404)

*Salah Eddine Bekhouche, Azeddine Benlamoudi, Yazid Bounab, Fadi Dornaika, Abdenour Hadid*

**Main category:** cs.CL

**Keywords:** Arabic NLP, Dense Passage Retrieval, Information Retrieval, Attentive Relevance Scoring, Question Answering

**Relevance Score:** 7

**TL;DR:** The paper presents an enhanced Dense Passage Retrieval framework for Arabic, improving retrieval performance using a novel Attentive Relevance Scoring mechanism.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges in NLP and information retrieval for Arabic due to its morphological complexity and underrepresentation in research.

**Method:** The methodology includes the development of an enhanced Dense Passage Retrieval framework with an Attentive Relevance Scoring mechanism that models semantic relevance between questions and passages.

**Key Contributions:**

	1. Introduction of Attentive Relevance Scoring for Arabic
	2. Enhanced Dense Passage Retrieval framework tailored for Arabic
	3. Public availability of the code for further research

**Result:** The approach integrates pre-trained Arabic language models and yields significant improvements in ranking accuracy for Arabic question answering.

**Limitations:** 

**Conclusion:** The enhanced DPR framework demonstrates improved retrieval performance for Arabic, highlighting the importance of focusing on underrepresented languages in NLP research.

**Abstract:** Arabic poses a particular challenge for natural language processing (NLP) and information retrieval (IR) due to its complex morphology, optional diacritics and the coexistence of Modern Standard Arabic (MSA) and various dialects. Despite the growing global significance of Arabic, it is still underrepresented in NLP research and benchmark resources. In this paper, we present an enhanced Dense Passage Retrieval (DPR) framework developed specifically for Arabic. At the core of our approach is a novel Attentive Relevance Scoring (ARS) that replaces standard interaction mechanisms with an adaptive scoring function that more effectively models the semantic relevance between questions and passages. Our method integrates pre-trained Arabic language models and architectural refinements to improve retrieval performance and significantly increase ranking accuracy when answering Arabic questions. The code is made publicly available at \href{https://github.com/Bekhouche/APR}{GitHub}.

</details>


### [93] [Beyond Passive Critical Thinking: Fostering Proactive Questioning to Enhance Human-AI Collaboration](https://arxiv.org/abs/2507.23407)

*Ante Wang, Yujie Lin, Jingyao Liu, Suhang Wu, Hao Liu, Xinyan Xiao, Jinsong Su*

**Main category:** cs.CL

**Keywords:** proactive critical thinking, AI collaboration, mathematical reasoning, reinforcement learning, benchmarking

**Relevance Score:** 7

**TL;DR:** This paper introduces proactive critical thinking in AI systems, where models actively seek missing information from users to improve problem-solving. It presents two benchmarks to evaluate this capability and demonstrates significant improvements in model performance using reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance AI models' interaction with users by encouraging proactive critical thinking rather than passive data rejection or problem avoidance.

**Method:** The paper introduces two benchmarks, GSM-MC and GSM-MCE, to evaluate proactive critical thinking in models. Experiments were conducted on Qwen3 and Llama series models, measuring their performance on these benchmarks and implementing reinforcement learning to improve results.

**Key Contributions:**

	1. Introduction of proactive critical thinking framework for AI models.
	2. Development of two novel benchmarks (GSM-MC, GSM-MCE) for assessing mathematical reasoning under challenging conditions.
	3. Demonstration of significant performance improvement using reinforcement learning.

**Result:** Models like Qwen3 show significant improvement in proactive critical thinking tasks, with accuracy increasing from 0.15% to 73.98% on the GSM-MC benchmark after applying the enhanced reinforcement learning algorithm.

**Limitations:** The study primarily focuses on mathematical reasoning and may not generalize to all types of queries or tasks.

**Conclusion:** This work highlights the importance of proactive critical thinking in AI, aiming to foster better collaboration between models and users in problem-solving scenarios.

**Abstract:** Critical thinking is essential for building robust AI systems, preventing them from blindly accepting flawed data or biased reasoning. However, prior work has primarily focused on passive critical thinking, where models simply reject problematic queries without taking constructive steps to address user requests. In this work, we introduce proactive critical thinking, a paradigm where models actively seek missing or clarifying information from users to resolve their queries better. To evaluate this capability, we present GSM-MC and GSM-MCE, two novel benchmarks based on GSM8K for assessing mathematical reasoning under incomplete or misleading conditions. GSM-MC contains 1,368 math problems with a key variable deliberately removed, requiring models to identify and request the missing information. GSM-MCE further increases the difficulty by introducing irrelevant details to test robustness against distractions. Experiments on Qwen3 and Llama series models show that, while these models excel in traditional reasoning tasks due to extensive post-training and inference-time scaling, they struggle with proactive critical thinking, especially smaller ones. However, we demonstrate that reinforcement learning (RL) can significantly improve this ability. Using our enhanced RL algorithm, we achieve substantial gains, boosting the Qwen3-1.7B's accuracy from 0.15% to 73.98% on GSM-MC. We hope this work advances models that collaborate more effectively with users in problem-solving through proactive critical thinking.

</details>


### [94] [Role-Aware Language Models for Secure and Contextualized Access Control in Organizations](https://arxiv.org/abs/2507.23465)

*Saeed Almheiri, Yerulan Kongrat, Adrian Santosh, Ruslan Tasmukhanov, Josemaria Vera, Muhammad Dehan Al Kautsar, Fajri Koto*

**Main category:** cs.CL

**Keywords:** large language models, role-specific access, enterprise applications, fine-tuning, safety methods

**Relevance Score:** 9

**TL;DR:** This paper explores fine-tuning large language models (LLMs) to generate responses based on user role access privileges in enterprise settings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for LLMs to behave according to user roles in enterprise settings is critical, as current safety methods do not consider role-specific access constraints.

**Method:** The authors investigate three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. Two datasets were created for evaluation: one from existing corpora and another synthetically generated for realistic scenarios.

**Key Contributions:**

	1. Introduction of role-sensitive fine-tuning for LLMs
	2. Creation of two novel datasets for evaluation
	3. Analysis of model robustness in the context of organizational roles

**Result:** The study assesses model performance across various organizational structures, focusing on robustness to prompt injection, role mismatch, and jailbreak attempts.

**Limitations:** The study may not address all potential user roles or organizational contexts, and the synthetic dataset may have limitations in realism.

**Conclusion:** Fine-tuning LLMs for role-sensitive responses can enhance safety and control in enterprise applications, addressing a significant gap in current methods.

**Abstract:** As large language models (LLMs) are increasingly deployed in enterprise settings, controlling model behavior based on user roles becomes an essential requirement. Existing safety methods typically assume uniform access and focus on preventing harmful or toxic outputs, without addressing role-specific access constraints. In this work, we investigate whether LLMs can be fine-tuned to generate responses that reflect the access privileges associated with different organizational roles. We explore three modeling strategies: a BERT-based classifier, an LLM-based classifier, and role-conditioned generation. To evaluate these approaches, we construct two complementary datasets. The first is adapted from existing instruction-tuning corpora through clustering and role labeling, while the second is synthetically generated to reflect realistic, role-sensitive enterprise scenarios. We assess model performance across varying organizational structures and analyze robustness to prompt injection, role mismatch, and jailbreak attempts.

</details>


### [95] [A Novel Evaluation Benchmark for Medical LLMs: Illuminating Safety and Effectiveness in Clinical Domains](https://arxiv.org/abs/2507.23486)

*Shirui Wang, Zhihui Tang, Huaxia Yang, Qiuhong Gong, Tiantian Gu, Hongyang Ma, Yongxin Wang, Wubin Sun, Zeliang Lian, Kehang Mao, Yinan Jiang, Zhicheng Huang, Lingyun Ma, Wenjie Shen, Yajie Ji, Yunhui Tan, Chunbo Wang, Yunlu Gao, Qianling Ye, Rui Lin, Mingyu Chen, Lijuan Niu, Zhihao Wang, Peng Yu, Mengran Lang, Yue Liu, Huimin Zhang, Haitao Shen, Long Chen, Qiguang Zhao, Si-Xuan Liu, Lina Zhou, Hua Gao, Dongqiang Ye, Lingmin Meng, Youtao Yu, Naixin Liang, Jianxiong Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Decision Support, Safety Evaluation, Effectiveness Validation, Health Informatics

**Relevance Score:** 9

**TL;DR:** The paper presents a benchmark framework for evaluating the safety and effectiveness of large language models in clinical settings, identifying significant performance variances across different scenarios.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in safety evaluation and effectiveness validation of large language models in clinical decision support.

**Method:** Developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB) with input from 32 physicians, covering 30 criteria and 2,069 Q&A items, comparing six LLMs on performance.

**Key Contributions:**

	1. Development of the CSEDB framework
	2. Establishment of a standardized evaluation metric for clinical LLMs
	3. Identification of performance differences between domain-specific and general-purpose LLMs

**Result:** Moderate overall performance of LLMs with a mean score of 57.2%, a significant drop in performance under high-risk scenarios, and domain-specific LLMs outperforming general models.

**Limitations:** 

**Conclusion:** The study provides a standardized metric for evaluating medical LLMs, which can enhance their safety and effectiveness in healthcare applications.

**Abstract:** Large language models (LLMs) hold promise in clinical decision support but face major challenges in safety evaluation and effectiveness validation. We developed the Clinical Safety-Effectiveness Dual-Track Benchmark (CSEDB), a multidimensional framework built on clinical expert consensus, encompassing 30 criteria covering critical areas like critical illness recognition, guideline adherence, and medication safety, with weighted consequence measures. Thirty-two specialist physicians developed and reviewed 2,069 open-ended Q&A items aligned with these criteria, spanning 26 clinical departments to simulate real-world scenarios. Benchmark testing of six LLMs revealed moderate overall performance (average total score 57.2%, safety 54.7%, effectiveness 62.3%), with a significant 13.3% performance drop in high-risk scenarios (p < 0.0001). Domain-specific medical LLMs showed consistent performance advantages over general-purpose models, with relatively higher top scores in safety (0.912) and effectiveness (0.861). The findings of this study not only provide a standardized metric for evaluating the clinical application of medical LLMs, facilitating comparative analyses, risk exposure identification, and improvement directions across different scenarios, but also hold the potential to promote safer and more effective deployment of large language models in healthcare environments.

</details>


### [96] [Med-R$^3$: Enhancing Medical Retrieval-Augmented Reasoning of LLMs via Progressive Reinforcement Learning](https://arxiv.org/abs/2507.23541)

*Keer Lu, Zheng Liang, Youquan Li, Jiejun Tan, Da Pan, Shusen Zhang, Guosheng Dong, Huang Leng*

**Main category:** cs.CL

**Keywords:** medical retrieval, augmented reasoning, reinforcement learning, logical reasoning, knowledge corpus

**Relevance Score:** 9

**TL;DR:** Med-R$^3$ is a framework combining retrieval-augmented reasoning with reinforcement learning for improved medical problem-solving.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The framework aims to address the limitations of current methods that separately optimize retrieval and reasoning, which restricts model generalization in medical contexts.

**Method:** The authors develop a model to perform logical reasoning on medical problems and then adaptively optimize its retrieval capabilities while jointly optimizing both retrieval and reasoning processes.

**Key Contributions:**

	1. Introduction of the Med-R$^3$ framework for medical retrieval-augmented reasoning.
	2. Joint optimization approach to improve retrieval and reasoning interaction.
	3. Enhanced model performance with reinforcement learning in the medical domain.

**Result:** Med-R$^3$ achieves state-of-the-art performance, surpassing GPT-4o-mini by 3.93% and showing a 13.53% improvement with Qwen2.5-14B.

**Limitations:** 

**Conclusion:** The joint optimization in Med-R$^3$ significantly enhances both retrieval and reasoning in medical applications, demonstrating better performance compared to existing models.

**Abstract:** In medical scenarios, effectively retrieving external knowledge and leveraging it for rigorous logical reasoning is of significant importance. Despite their potential, existing work has predominantly focused on enhancing either retrieval or reasoning capabilities of the models in isolation, with little attention given to their joint optimization, which leads to limited coordination between the two processes. Additionally, current methods rely heavily on supervised fine-tuning (SFT), which can cause models to memorize existing problem-solving pathways, thereby restricting their generalization ability when confronted with novel problem contexts. Furthermore, while some studies have explored to improve retrieval-augmented reasoning in general domains via reinforcement learning, their reward function designs do not adequately capture the specific demands of the medical domain. To address these challenges, we introduce **Med-R$^3$**, a **Med**ical **R**etrieval-augmented **R**easoning framework driven by progressive **R**einforcement learning. In this framework, we first develop the model's ability to perform logical reasoning over medical problems. Subsequently, on the basis of this foundation, we adaptively optimize the retrieval capability to better align with the characteristics of knowledge corpus and external information utilization throughout the reasoning process. Finally, we conduct joint optimization of the model's retrieval and reasoning coordination. Extensive experiments indicate that **Med-R$^3$** could achieve state-of-the-art performances, with LLaMA3.1-8B-Instruct + Med-R$^3$ surpassing closed-sourced GPT-4o-mini by 3.93\% at a comparable parameter scale, while Qwen2.5-14B augmented with Med-R$^3$ shows a more substantial gain of 13.53\%.

</details>


### [97] [T-Detect: Tail-Aware Statistical Normalization for Robust Detection of Adversarial Machine-Generated Text](https://arxiv.org/abs/2507.23577)

*Alva West, Luodan Zhang, Liuliu Zhang, Minjun Zhu, Yixuan Weng, Yue Zhang*

**Main category:** cs.CL

**Keywords:** text generation, detection methods, heavy-tailed statistics

**Relevance Score:** 8

**TL;DR:** Introducing T-Detect, a novel method for detecting machine-generated text, designed to be robust against adversarial perturbations by utilizing a heavy-tailed statistical approach based on the Student's t-distribution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Develop robust detection methods to identify machine-generated content that can evade existing detection systems through adversarial techniques.

**Method:** T-Detect replaces standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution, enhancing resilience to statistical outliers.

**Key Contributions:**

	1. New statistical foundation for text detection
	2. Ablation-validated method demonstrating superior robustness
	3. Comprehensive performance analysis under adversarial conditions

**Result:** T-Detect improves AUROC by up to 3.9% over strong baselines on the RAID benchmark and achieves state-of-the-art performance with an AUROC of 0.926 on the Books domain of RAID.

**Limitations:** 

**Conclusion:** T-Detect provides a theoretically justified and effective method for the detection of adversarial texts, validated through extensive experiments.

**Abstract:** The proliferation of sophisticated text generation models necessitates the development of robust detection methods capable of identifying machine-generated content, particularly text designed to evade detection through adversarial perturbations. Existing zero-shot detectors often rely on statistical measures that implicitly assume Gaussian distributions, a premise that falters when confronted with the heavy-tailed statistical artifacts characteristic of adversarial or non-native English texts. This paper introduces T-Detect, a novel detection method that fundamentally redesigns the statistical core of curvature-based detectors. Our primary innovation is the replacement of standard Gaussian normalization with a heavy-tailed discrepancy score derived from the Student's t-distribution. This approach is theoretically grounded in the empirical observation that adversarial texts exhibit significant leptokurtosis, rendering traditional statistical assumptions inadequate. T-Detect computes a detection score by normalizing the log-likelihood of a passage against the expected moments of a t-distribution, providing superior resilience to statistical outliers. We validate our approach on the challenging RAID benchmark for adversarial text and the comprehensive HART dataset. Experiments show that T-Detect provides a consistent performance uplift over strong baselines, improving AUROC by up to 3.9\% in targeted domains. When integrated into a two-dimensional detection framework (CT), our method achieves state-of-the-art performance, with an AUROC of 0.926 on the Books domain of RAID. Our contributions are a new, theoretically-justified statistical foundation for text detection, an ablation-validated method that demonstrates superior robustness, and a comprehensive analysis of its performance under adversarial conditions. Ours code are released at https://github.com/ResearAI/t-detect.

</details>


### [98] [DiffLoRA: Differential Low-Rank Adapters for Large Language Models](https://arxiv.org/abs/2507.23588)

*Alexandre Misrahi, Nadezhda Chirkova, Maxime Louis, Vassilina Nikoulina*

**Main category:** cs.CL

**Keywords:** Differential Transformer, LoRA, NLP tasks, attention mechanism, parameter-efficient

**Relevance Score:** 6

**TL;DR:** DiffLoRA introduces a parameter-efficient adaptation of differential attention for Transformers, leveraging low-rank adapters to enhance performance in NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance in Transformer models by addressing noise through a denoiser attention mechanism.

**Method:** DiffLoRA combines differential attention with low-rank adapters on both positive and negative attention terms.

**Key Contributions:**

	1. Introduction of DiffLoRA as a parameter-efficient adaptation
	2. Utilization of low-rank adapters in differential attention
	3. Demonstration of specific performance gains in NLP tasks

**Result:** DiffLoRA shows mixed results across NLP tasks, demonstrating improvements in specific areas, such as a +11 pts performance gain on LoRA for HumanEval.

**Limitations:** DiffLoRA underperforms in many evaluation tasks compared to other parameter-efficient methods.

**Conclusion:** While DiffLoRA is less effective compared to other fine-tuning methods in many tasks, it yields noteworthy results in certain domains.

**Abstract:** Differential Transformer has recently been proposed to improve performance in Transformer models by canceling out noise through a denoiser attention mechanism. In this work, we introduce DiffLoRA, a parameter-efficient adaptation of the differential attention mechanism, with low-rank adapters on both positive and negative attention terms. This approach retains the efficiency of LoRA while aiming to benefit from the performance gains of differential attention. We evaluate DiffLoRA across a broad range of NLP tasks, including general benchmarks, many-shot in-context learning, RAG, and long-context tests. We observe that, although DiffLoRA falls short of other parameter-efficient fine-tuning methods in most evaluation tasks, it shows interesting results in certain domains (+11 pts on LoRA for HumanEval). We analyze the attention patterns post-finetuning to identify the reasons for this behavior.

</details>


### [99] [Arabic Hate Speech Identification and Masking in Social Media using Deep Learning Models and Pre-trained Models Fine-tuning](https://arxiv.org/abs/2507.23661)

*Salam Thabet Doghmash, Motaz Saad*

**Main category:** cs.CL

**Keywords:** hate speech, Arabic text, machine learning, deep learning, text cleaning

**Relevance Score:** 4

**TL;DR:** This paper addresses detection and cleaning of hate speech in Arabic text using deep learning and machine translation techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of hate speech on social media necessitates effective detection and cleaning methods, particularly for Arabic text.

**Method:** Deep learning models and transformers were employed to detect hate speech, while a separate approach treated text cleaning as a machine translation task.

**Key Contributions:**

	1. Development of a hate speech detection model for Arabic text achieving high accuracy
	2. Introduction of a novel approach for cleaning hate speech through text masking
	3. Demonstration of deep learning and transformer capabilities in natural language processing tasks

**Result:** Achieved a 92% Macro F1 score and 95% accuracy for hate speech detection, and a BLEU score of 0.3 for text cleaning.

**Limitations:** 

**Conclusion:** The proposed methods outperformed existing systems in both detecting and cleaning hate speech, demonstrating their effectiveness.

**Abstract:** Hate speech identification in social media has become an increasingly important issue in recent years. In this research, we address two problems: 1) to detect hate speech in Arabic text, 2) to clean a given text from hate speech. The meaning of cleaning here is replacing each bad word with stars based on the number of letters for each word. Regarding the first problem, we conduct several experiments using deep learning models and transformers to determine the best model in terms of the F1 score. Regarding second problem, we consider it as a machine translation task, where the input is a sentence containing dirty text and the output is the same sentence with masking the dirty text. The presented methods achieve the best model in hate speech detection with a 92\% Macro F1 score and 95\% accuracy. Regarding the text cleaning experiment, the best result in the hate speech masking model reached 0.3 in BLEU score with 1-gram, which is a good result compared with the state of the art machine translation systems.

</details>


### [100] [Rule2Text: Natural Language Explanation of Logical Rules in Knowledge Graphs](https://arxiv.org/abs/2507.23740)

*Nasim Shirvani-Mahdavi, Devin Wingfield, Amin Ghasemi, Chengkai Li*

**Main category:** cs.CL

**Keywords:** knowledge graphs, explanation generation, large language models, logical rules, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper investigates the use of large language models to generate natural language explanations for logical rules extracted from knowledge graphs, improving understanding and reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The complexity of logical rules in knowledge graphs hinders human understanding, thus enhancing their interpretability through natural language explanations is valuable.

**Method:** The authors apply AMIE 3.5.1 to extract rules from datasets including FB15k-237 and employ various prompting strategies with large language models for explanation generation.

**Key Contributions:**

	1. Explored the generation of natural language explanations for logical rules in knowledge graphs using LLMs.
	2. Conducted comprehensive human evaluations of explanation clarity and correctness.
	3. Provided public access to scripts and datasets used in the study.

**Result:** The generated explanations show promising correctness and clarity, but some challenges related to hallucination and complexity persist.

**Limitations:** Remaining challenges with explanation correctness and instances of hallucination need to be addressed in future work.

**Conclusion:** While the results indicate potential for large language models in generating understandable explanations for knowledge graph rules, further research is needed to overcome existing challenges.

**Abstract:** Knowledge graphs (KGs) often contain sufficient information to support the inference of new facts. Identifying logical rules not only improves the completeness of a knowledge graph but also enables the detection of potential errors, reveals subtle data patterns, and enhances the overall capacity for reasoning and interpretation. However, the complexity of such rules, combined with the unique labeling conventions of each KG, can make them difficult for humans to understand. In this paper, we explore the potential of large language models to generate natural language explanations for logical rules. Specifically, we extract logical rules using the AMIE 3.5.1 rule discovery algorithm from the benchmark dataset FB15k-237 and two large-scale datasets, FB-CVT-REV and FB+CVT-REV. We examine various prompting strategies, including zero- and few-shot prompting, including variable entity types, and chain-of-thought reasoning. We conduct a comprehensive human evaluation of the generated explanations based on correctness, clarity, and hallucination, and also assess the use of large language models as automatic judges. Our results demonstrate promising performance in terms of explanation correctness and clarity, although several challenges remain for future research. All scripts and data used in this study are publicly available at https://github.com/idirlab/KGRule2NL}{https://github.com/idirlab/KGRule2NL.

</details>


### [101] [Cascaded Information Disclosure for Generalized Evaluation of Problem Solving Capabilities](https://arxiv.org/abs/2507.23776)

*Yunxiang Yan, Tomohiro Sawada, Kartik Goyal*

**Main category:** cs.CL

**Keywords:** LLM Evaluation, Cascaded Question Disclosure, Problem-Solving, Generalized Reasoning, QA Benchmarking

**Relevance Score:** 9

**TL;DR:** The paper proposes a new framework for evaluating LLMs' problem-solving capabilities through cascaded question disclosure, which reveals information stagewise to enable better reasoning assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a more accurate and scalable method for evaluating the problem-solving capabilities of LLMs, as traditional QA benchmarks are indirect and may overestimate performance differences.

**Method:** A cascaded question disclosure framework that collects model responses in stages, revealing partial information to facilitate generalized reasoning in LLMs.

**Key Contributions:**

	1. Introduction of cascaded question disclosure for LLM evaluation
	2. Improved methods to assess LLM problem-solving capabilities
	3. Ablation studies validate findings and framework effectiveness

**Result:** Empirical validation shows improved comparisons of LLM performance and better intermediate traces in model responses compared to standard QA methods across various datasets.

**Limitations:** 

**Conclusion:** The proposed framework narrows performance gaps seen in traditional QA evaluations, suggesting they may not accurately reflect true model capabilities.

**Abstract:** While question-answering~(QA) benchmark performance is an automatic and scalable method to compare LLMs, it is an indirect method of evaluating their underlying problem-solving capabilities. Therefore, we propose a holistic and generalizable framework based on \emph{cascaded question disclosure} that provides a more accurate estimate of the models' problem-solving capabilities while maintaining the scalability and automation. This approach collects model responses in a stagewise manner with each stage revealing partial information about the question designed to elicit generalized reasoning in LLMs. We find that our approach not only provides a better comparison between LLMs, but also induces better intermediate traces in models compared to the standard QA paradigm. We empirically verify this behavior on diverse reasoning and knowledge-heavy QA datasets by comparing LLMs of varying sizes and families. Our approach narrows the performance gap observed in the standard QA evaluation settings, indicating that the prevalent indirect QA paradigm of evaluation overestimates the differences in performance between models. We further validate our findings by extensive ablation studies.

</details>


### [102] [LiMe: a Latin Corpus of Late Medieval Criminal Sentences](https://arxiv.org/abs/2404.12829)

*Alessandra Bassani, Beatrice Del Bo, Alfio Ferrara, Marta Mangini, Sergio Picascia, Ambra Stefanello*

**Main category:** cs.CL

**Keywords:** Latin, NLP, dataset, masked language model, corpus

**Relevance Score:** 4

**TL;DR:** Introduction of the LiMe dataset for Latin texts to enhance NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve NLP resources and model performance for the Latin language, which lags behind modern languages due to less available data.

**Method:** The paper presents the construction of the LiMe dataset, consisting of 325 expert-annotated documents from medieval manuscripts for NLP applications.

**Key Contributions:**

	1. Introduction of the LiMe dataset for Latin texts
	2. Expert annotation of historical documents
	3. Enhancement of NLP resources for Latin language

**Result:** The dataset is suitable for masked language models and supervised NLP tasks, enhancing the capabilities of Latin text processing.

**Limitations:** 

**Conclusion:** The LiMe dataset will contribute to advancing NLP techniques for Latin, bridging the gap with modern language models.

**Abstract:** The Latin language has received attention from the computational linguistics research community, which has built, over the years, several valuable resources, ranging from detailed annotated corpora to sophisticated tools for linguistic analysis. With the recent advent of large language models, researchers have also started developing models capable of generating vector representations of Latin texts. The performances of such models remain behind the ones for modern languages, given the disparity in available data. In this paper, we present the LiMe dataset, a corpus of 325 documents extracted from a series of medieval manuscripts called Libri sententiarum potestatis Mediolani, and thoroughly annotated by experts, in order to be employed for masked language model, as well as supervised natural language processing tasks.

</details>


### [103] [Explaining vague language](https://arxiv.org/abs/2404.18154)

*Paul Égré, Benjamin Spector*

**Main category:** cs.CL

**Keywords:** vagueness, language, semantic account, game-theoretic, Bayesian

**Relevance Score:** 2

**TL;DR:** This paper compares game-theoretic and Bayesian accounts of vagueness in language, arguing the necessity of a semantic understanding of vagueness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore why vague language may be more useful than precise language by comparing established theories.

**Method:** The paper analyzes the game-theoretic approach of Lipman and the Bayesian approach of 'Egr'é et al. to reconcile their findings on vagueness.

**Key Contributions:**

	1. Comparison of game-theoretic and Bayesian frameworks for vagueness
	2. Arguments for the necessity of semantic considerations in vagueness
	3. Resolution of apparent contradictions between existing theories

**Result:** The paper concludes that a semantic account is essential for understanding vagueness, as it provides better insight than purely signaling strategies.

**Limitations:** 

**Conclusion:** Understanding vagueness in terms of semantics rather than just signaling strategies allows for a more comprehensive explanation of its utility.

**Abstract:** Why is language vague? Vagueness may be explained and rationalized if it can be shown that vague language is more useful to speaker and hearer than precise language. In a well-known paper, Lipman proposes a game-theoretic account of vagueness in terms of mixed strategy that leads to a puzzle: vagueness cannot be strictly better than precision at equilibrium. More recently, \'Egr\'e, Spector, Mortier and Verheyen have put forward a Bayesian account of vagueness establishing that using vague words can be strictly more informative than using precise words. This paper proposes to compare both results and to explain why they are not in contradiction. Lipman's definition of vagueness relies exclusively on a property of signaling strategies, without making any assumptions about the lexicon, whereas \'Egr\'e et al.'s involves a layer of semantic content. We argue that the semantic account of vagueness is needed, and more adequate and explanatory of vagueness.

</details>


### [104] [Iterative Repair with Weak Verifiers for Few-shot Transfer in KBQA with Unanswerability](https://arxiv.org/abs/2406.14313)

*Riya Sawhney, Samrat Yadav, Indrajit Bhattacharya, Mausam*

**Main category:** cs.CL

**Keywords:** Knowledge Base Question Answering, Few-shot transfer, Unanswerable questions

**Relevance Score:** 6

**TL;DR:** The paper introduces the task of few-shot transfer for Knowledge Base Question Answering (KBQA) that addresses unanswerable questions, proposing a novel solution FUn-FuSIC and new datasets for evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve KBQA models' ability to handle unanswerable questions with limited labeled training data.

**Method:** The proposed method, FUn-FuSIC, enhances the existing FuSIC model by incorporating Feedback for Unanswerability (FUn), implementing iterative repair techniques and self-consistency for determining question answerability.

**Key Contributions:**

	1. Introduction of the few-shot transfer task for KBQA with unanswerable questions.
	2. Development of the FUn-FuSIC model that integrates feedback mechanisms for unanswerability.
	3. Creation of two new datasets for evaluating KBQA performance.

**Result:** FUn-FuSIC significantly outperforms existing LLM-based and supervised state-of-the-art models in addressing unanswerable questions in KBQA, while achieving a new state-of-the-art for answerable few-shot transfer.

**Limitations:** 

**Conclusion:** The approach shows promise in handling few-shot KBQA with unanswerable questions and sets new benchmarks in the field.

**Abstract:** Real-world applications of KBQA require models to handle unanswerable questions with a limited volume of in-domain labeled training data. We propose the novel task of few-shot transfer for KBQA with unanswerable questions and contribute two new datasets for performance evaluation. We present FUn-FuSIC - a novel solution for our task that extends FuSIC KBQA, the state-of-the-art few-shot transfer model for answerable-only KBQA. We first note that FuSIC-KBQA's iterative repair makes a strong assumption that all questions are unanswerable. As a remedy, we propose Feedback for Unanswerability (FUn), which uses iterative repair using feedback from a suite of strong and weak verifiers, and an adaptation of self consistency for unanswerabilty to better assess the answerability of a question. Our experiments show that FUn-FuSIC significantly outperforms suitable adaptations of multiple LLM based and supervised SoTA models on our task, while establishing a new SoTA for answerable few-shot transfer as well.

</details>


### [105] [Cutting Through the Noise: Boosting LLM Performance on Math Word Problems](https://arxiv.org/abs/2406.15444)

*Ujjwala Anantheswaran, Himanshu Gupta, Kevin Scaria, Shreyas Verma, Chitta Baral, Swaroop Mishra*

**Main category:** cs.CL

**Keywords:** Large Language Models, math word problems, adversarial training, dataset, HCI

**Relevance Score:** 9

**TL;DR:** This paper presents a prompting framework to create adversarial math word problems (MWPs) that contain irrelevant information, revealing vulnerabilities in large language models (LLMs) and proposing a solution through fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of LLMs in solving real-world math word problems by addressing their vulnerability to irrelevant information.

**Method:** The authors developed a prompting framework to generate adversarial MWPs and introduced a new dataset, PROBLEMATHIC, containing both types of problems. They conducted experiments and fine-tuned LLMs on adversarial samples to improve their robustness.

**Key Contributions:**

	1. Introduction of the PROBLEMATHIC dataset for adversarial MWPs
	2. Creation of the prompting framework for generating adversarial problems
	3. Fine-tuning approach that improves LLM performance against distractions

**Result:** Experiments indicated a ~26% performance drop on adversarial MWPs, with fine-tuning showing an improvement of ~8% in handling these adversarial cases. Performance decreased by up to 6% when LLMs faced the adversarial GSM-8K benchmark.

**Limitations:** The study primarily focuses on adversarial MWPs, and further research is needed to explore other real-world applications and further validate the generalizability of results.

**Conclusion:** The prompting framework helps in assessing the robustness of LLMs against irrelevant data, and fine-tuning can enhance their performance on adversarial tasks.

**Abstract:** Large Language Models (LLMs) excel at various tasks, including solving math word problems (MWPs), but struggle with real-world problems containing irrelevant information. To address this, we propose a prompting framework that generates adversarial variants of MWPs by adding irrelevant variables. We introduce a dataset, PROBLEMATHIC, containing both adversarial and non-adversarial MWPs. Our experiments reveal that LLMs are susceptible to distraction by numerical noise, resulting in an average relative performance drop of ~26% on adversarial MWPs. To mitigate this, we fine-tune LLMs (Llama-2, Mistral) on the adversarial samples from our dataset. Fine-tuning on adversarial training instances improves performance on adversarial MWPs by ~8%, indicating increased robustness to noise and improved ability to identify relevant data for reasoning. Finally, to assess the generalizability of our prompting framework, we introduce GSM-8K-Adv, an adversarial variant of the GSM-8K benchmark. LLMs continue to struggle when faced with adversarial information, reducing performance by up to 6%.

</details>


### [106] [Cultural Palette: Pluralising Culture Alignment via Multi-agent Palette](https://arxiv.org/abs/2412.11167)

*Jiahao Yuan, Zixiang Di, Shangzixin Zhao, Zhiqing Cui, Hanqing Wang, Guisong Yang, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Cultural Alignment, Large Language Models, Multi-agent Framework, Cultural Geography, Attention-gated

**Relevance Score:** 8

**TL;DR:** The paper proposes a multi-agent framework called Cultural Palette for aligning large language models (LLMs) with diverse cultural values.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the performance of LLMs, they struggle with cultural alignment due to monocultural biases and challenges in capturing nuanced cultural semantics.

**Method:** The Cultural Palette framework synthesizes a dataset using GPT-4o, utilizes continent-level alignment agents for region-specific responses, and employs a Meta Agent for dynamic cultural blending via attention-gated parameter merging.

**Key Contributions:**

	1. Cultural Palette as a new multi-agent framework for cultural alignment
	2. Synthesis of the Pentachromatic Cultural Palette Dataset using GPT-4o
	3. Introduction of attention-gated parameter merging for cultural blending

**Result:** Cultural Palette outperforms existing baselines in cultural alignment through a novel adaptive color-blending process.

**Limitations:** 

**Conclusion:** The proposed method effectively resolves cultural conflicts and enhances LLM performance in generating culturally-aligned responses.

**Abstract:** Large language models (LLMs) face challenges in aligning with diverse cultural values despite their remarkable performance in generation, which stems from inherent monocultural biases and difficulties in capturing nuanced cultural semantics. Existing methods struggle to adapt to unknown culture after fine-tuning. Inspired by cultural geography across five continents, we propose Cultural Palette, a multi-agent framework that redefines cultural alignment as an adaptive "color-blending" process for country-specific adaptation. Our approach harnesses cultural geography across five continents (Africa, America, Asia, Europe, Oceania) through three key steps: First, we synthesize the Pentachromatic Cultural Palette Dataset using GPT-4o, refining continental-level dialogues with Hofstede's cultural dimensions to establish foundational cultural representations. Second, five continent-level alignment agents form specialized cultural communities that generate region-specific draft responses. Third, a Meta Agent employs Cultural MoErges to dynamically blend these cultural "colors" through attention-gated parameter merging, akin to mixing pigments on a palette, resolving conflicts while preserving cultural nuances to produce the final culturally-aligned response. Extensive experiments across various countries demonstrate that Cultural Palette surpasses existing baselines in cultural alignment.

</details>


### [107] [Inside-Out: Hidden Factual Knowledge in LLMs](https://arxiv.org/abs/2503.15299)

*Zorik Gekhman, Eyal Ben David, Hadas Orgad, Eran Ofek, Yonatan Belinkov, Idan Szpektor, Jonathan Herzig, Roi Reichart*

**Main category:** cs.CL

**Keywords:** large language models, knowledge quantification, closed-book Q&A, HCI, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for quantifying the hidden knowledge in large language models (LLMs) by comparing internal knowledge to external expression. It shows that LLMs consistently encode more factual knowledge internally than they express outwardly.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether large language models encode more factual knowledge in their parameters than what they express in their outputs, providing a clearer understanding of their capabilities.

**Method:** A formal definition of knowledge is proposed, quantifying it for specific questions and dividing it into external and internal knowledge. A case study is conducted on three open-weights LLMs in a closed-book QA setup.

**Key Contributions:**

	1. Proposes a formal definition and quantification of knowledge in LLMs
	2. Demonstrates through a case study that LLMs often encode more knowledge than they express
	3. Highlights practical implications for closed-book QA tasks

**Result:** The findings show that LLMs encode an average of 40% more factual knowledge internally compared to what they generate externally. Some knowledge remains deeply hidden despite perfect internal knowledge, and practical constraints limit performance improvements in QA settings.

**Limitations:** The study focuses on open-weights LLMs in a closed-book QA setup, which may not generalize across all types of LLMs or interactions.

**Conclusion:** The paper concludes that there are significant limitations to the generation capabilities of LLMs, revealing challenges in scaling their test-time computation effectively due to hidden knowledge.

**Abstract:** This work presents a framework for assessing whether large language models (LLMs) encode more factual knowledge in their parameters than what they express in their outputs. While a few studies hint at this possibility, none has clearly defined or demonstrated this phenomenon. We first propose a formal definition of knowledge, quantifying it for a given question as the fraction of correct-incorrect answer pairs where the correct one is ranked higher. This gives rise to external and internal knowledge, depending on the information used to score individual answer candidates: either the model's observable token-level probabilities or its intermediate computations. Hidden knowledge arises when internal knowledge exceeds external knowledge. We then present a case study, applying this framework to three popular open-weights LLMs in a closed-book QA setup. Our results indicate that: (1) LLMs consistently encode more factual knowledge internally than what they express externally, with an average relative gap of 40%. (2) Surprisingly, some knowledge is so deeply hidden that a model can internally know an answer perfectly, yet fail to generate it even once, despite large-scale repeated sampling of 1,000 answers. This reveals fundamental limitations in the generation capabilities of LLMs, which (3) put a practical constraint on scaling test-time compute via repeated answer sampling in closed-book QA: significant performance improvements remain inaccessible because some answers are practically never sampled, yet if they were, we would be guaranteed to rank them first.

</details>


### [108] [Can one size fit all?: Measuring Failure in Multi-Document Summarization Domain Transfer](https://arxiv.org/abs/2503.15768)

*Alexandra DeLucia, Mark Dredze*

**Main category:** cs.CL

**Keywords:** multi-document summarization, domain transfer, factuality, summary quality, evaluation metrics

**Relevance Score:** 8

**TL;DR:** This paper assesses various approaches for multi-document summarization (MDS) and their effectiveness in zero-shot domain transfer scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for robust multi-document summarization models that can generalize across different domains, such as news, science, and conversation.

**Method:** Evaluation of MDS models using four training approaches and across different document domains analyzing reference similarity, quality, and factuality.

**Key Contributions:**

	1. Comprehensive evaluation of MDS model training approaches.
	2. Identification of domain transfer issues in MDS.
	3. Critique of existing summarization metrics for out-of-the-box use.

**Result:** Models trained in one domain often suffer in factuality and summary quality when applied to other domains in a zero-shot context.

**Limitations:** 

**Conclusion:** Domain transfer failure is identified by decreased factuality and summary quality, prompting a need for better evaluation metrics.

**Abstract:** Abstractive multi-document summarization (MDS) is the task of automatically summarizing information in multiple documents, from news articles to conversations with multiple speakers. The training approaches for current MDS models can be grouped into four approaches: end-to-end with special pre-training ("direct"), chunk-then-summarize, extract-then-summarize, and inference with GPT-style models. In this work, we evaluate MDS models across training approaches, domains, and dimensions (reference similarity, quality, and factuality), to analyze how and why models trained on one domain can fail to summarize documents from another (News, Science, and Conversation) in the zero-shot domain transfer setting. We define domain-transfer "failure" as a decrease in factuality, higher deviation from the target, and a general decrease in summary quality. In addition to exploring domain transfer for MDS models, we examine potential issues with applying popular summarization metrics out-of-the-box.

</details>


### [109] [Splits! A Flexible Dataset and Evaluation Framework for Sociocultural Linguistic Investigation](https://arxiv.org/abs/2504.04640)

*Eylon Caplan, Tania Chakraborty, Dan Goldwasser*

**Main category:** cs.CL

**Keywords:** Sociocultural Linguistic Phenomena, Reddit dataset, Hypothesis validation

**Relevance Score:** 4

**TL;DR:** The paper introduces Splits!, a large dataset from Reddit for studying Sociocultural Linguistic Phenomena (SLP) with a framework for validating hypotheses using this data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To facilitate the computational study of Sociocultural Linguistic Phenomena (SLP) and overcome limitations of previous bespoke analyses.

**Method:** A dataset of 9.7 million Reddit posts from over 53,000 users across 6 demographic groups, organized into 89 topics, is created and validated. A framework to evaluate hypotheses using efficient retrieval methods is introduced.

**Key Contributions:**

	1. Creation of the Splits! dataset for SLP research
	2. Introduction of a framework for validating Sociocultural Linguistic hypotheses
	3. Demonstration of improved efficiency in identifying significant findings

**Result:** The dataset and framework allow for systematic comparative analysis and successfully replicate known SLPs. The framework also reduces the number of findings needing manual inspection by 1.5-1.8 times.

**Limitations:** 

**Conclusion:** The integration of the Splits! dataset with a hypothesis validation framework enhances the study of cultural perspectives in language use.

**Abstract:** Variation in language use, shaped by speakers' sociocultural background and specific context of use, offers a rich lens into cultural perspectives, values, and opinions. However, the computational study of these Sociocultural Linguistic Phenomena (SLP) has often been limited to bespoke analyses of specific groups or topics, hindering the pace of scientific discovery. To address this, we introduce Splits!, a 9.7 million-post dataset from Reddit designed for systematic and flexible research. The dataset contains posts from over 53,000 users across 6 demographic groups, organized into 89 discussion topics to enable comparative analysis. We validate Splits! via self-identification and by successfully replicating several known SLPs from existing literature. We complement this dataset with a framework that leverages efficient retrieval methods to rapidly validate potential SLPs (PSLPs) by automatically evaluating whether a given hypothesis is supported by our data. Crucially, to distinguish between novel and obvious insights, the framework incorporates a human-validated measure of a hypothesis's ``unexpectedness.'' We demonstrate that the two-stage process reduces the number of statistically significant findings requiring manual inspection by a factor of 1.5-1.8x, streamlining the discovery of promising phenomena for further investigation.

</details>


### [110] [AutoSchemaKG: Autonomous Knowledge Graph Construction through Dynamic Schema Induction from Web-Scale Corpora](https://arxiv.org/abs/2505.23628)

*Jiaxin Bai, Wei Fan, Qi Hu, Qing Zong, Chunyang Li, Hong Ting Tsang, Hongyu Luo, Yauwai Yim, Haoyu Huang, Xiao Zhou, Feng Qin, Tianshi Zheng, Xi Peng, Xin Yao, Huiwen Yang, Leijie Wu, Yi Ji, Gong Zhang, Renhai Chen, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** knowledge graph, schema induction, large language models, autoamtic construction, natural language processing

**Relevance Score:** 8

**TL;DR:** A framework called AutoSchemaKG enables autonomous knowledge graph construction without predefined schemas, using large language models to extract knowledge and create schemas from text.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective and autonomous knowledge graph construction methods that do not rely on predefined schemas.

**Method:** Leveraging large language models to extract knowledge triples and induce schemas from text while organizing them into semantic categories.

**Key Contributions:**

	1. Introduction of AutoSchemaKG for autonomous knowledge graph construction
	2. Achievement of 95% semantic alignment with human schemas
	3. Demonstration of scalability with knowledge graphs exceeding 900 million nodes

**Result:** Automatically constructs ATLAS knowledge graphs containing over 900 million nodes and 5.9 billion edges, outperforming existing baselines in multi-hop QA tasks.

**Limitations:** 

**Conclusion:** The system shows that billions of dynamically induced schemas can enhance large language models' factuality without manual intervention.

**Abstract:** We present AutoSchemaKG, a framework for fully autonomous knowledge graph construction that eliminates the need for predefined schemas. Our system leverages large language models to simultaneously extract knowledge triples and induce comprehensive schemas directly from text, modeling both entities and events while employing conceptualization to organize instances into semantic categories. Processing over 50 million documents, we construct ATLAS (Automated Triple Linking And Schema induction), a family of knowledge graphs with 900+ million nodes and 5.9 billion edges. This approach outperforms state-of-the-art baselines on multi-hop QA tasks and enhances LLM factuality. Notably, our schema induction achieves 95\% semantic alignment with human-crafted schemas with zero manual intervention, demonstrating that billion-scale knowledge graphs with dynamically induced schemas can effectively complement parametric knowledge in large language models.

</details>


### [111] [Theorem-of-Thought: A Multi-Agent Framework for Abductive, Deductive, and Inductive Reasoning in Language Models](https://arxiv.org/abs/2506.07106)

*Samir Abdaljalil, Hasan Kurban, Khalid Qaraqe, Erchin Serpedin*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, Bayesian belief propagation, inference, natural language

**Relevance Score:** 8

**TL;DR:** Theorem-of-Thought (ToTh) enhances reasoning in large language models by employing three parallel inference agents and formal reasoning graphs, outperforming existing prompting techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with brittle and difficult-to-interpret reasoning processes, warranting approaches that improve logical structure and coherence assessment.

**Method:** Theorem-of-Thought (ToTh) involves three parallel agents simulating abductive, deductive, and inductive inference, producing structured reasoning traces formalized into a reasoning graph. Bayesian belief propagation and natural language inference are applied to evaluate coherence and consistency.

**Key Contributions:**

	1. Introduction of the Theorem-of-Thought framework for LLM reasoning.
	2. Collaboration of three inference agents enhances logical structure.
	3. Application of Bayesian belief propagation for coherence assessment.

**Result:** ToTh consistently outperformed Chain-of-Thought, Self-Consistency, and CoT-Decoding on reasoning benchmarks (WebOfLies and MultiArith), providing interpretable and logically grounded reasoning chains.

**Limitations:** 

**Conclusion:** The findings indicate that ToTh offers a robust, cognitively inspired framework for enhancing LLM reasoning. Its implementation is publicly available.

**Abstract:** Large language models (LLMs) have shown strong performance across natural language reasoning tasks, yet their reasoning processes remain brittle and difficult to interpret. Prompting techniques like Chain-of-Thought (CoT) enhance reliability by eliciting intermediate reasoning steps or aggregating multiple outputs. However, they lack mechanisms for enforcing logical structure and assessing internal coherence. We introduce Theorem-of-Thought (ToTh), a novel framework that models reasoning as collaboration among three parallel agents, each simulating a distinct mode of inference: abductive, deductive, and inductive. Each agent produces a reasoning trace, which is structured into a formal reasoning graph. To evaluate consistency, we apply Bayesian belief propagation guided by natural language inference (NLI), assigning confidence scores to each step. The most coherent graph is selected to derive the final answer. Experiments on symbolic (WebOfLies) and numerical (MultiArith) reasoning benchmarks show that ToTh consistently outperforms CoT, Self-Consistency, and CoT-Decoding across multiple LLMs, while producing interpretable and logically grounded reasoning chains. Our findings suggest a promising direction for building more robust and cognitively inspired LLM reasoning. The implementation is available at https://github.com/KurbanIntelligenceLab/theorem-of-thought.

</details>


### [112] [RAVine: Reality-Aligned Evaluation for Agentic Search](https://arxiv.org/abs/2507.16725)

*Yilong Xu, Xiang Long, Zhi Zheng, Jinhua Gao*

**Main category:** cs.CL

**Keywords:** agentic search, LLM, evaluation framework, user intents, RAVine

**Relevance Score:** 8

**TL;DR:** RAVine is a new evaluation framework for agentic LLMs that addresses shortcomings in existing benchmarks by focusing on realistic user queries, improving ground truth extraction, and evaluating iterative search processes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of agentic search systems, which have been inadequately assessed by existing frameworks that do not consider realistic user scenarios and the iterative nature of searches.

**Method:** RAVine introduces multi-point queries and long-form answers to reflect user intents better, along with a new strategy for ground truth construction to enhance evaluation accuracy.

**Key Contributions:**

	1. Introduction of RAVine, a new evaluation framework.
	2. Focus on realistic user scenarios with multi-point queries.
	3. Enhanced accuracy in ground truth extraction for evaluation.

**Result:** RAVine has been used to benchmark various models and provides insights that could advance the development of intelligent search systems.

**Limitations:** 

**Conclusion:** The implementation of RAVine could lead to better assessment frameworks that align more closely with real-world user intents and improve agentic search capabilities.

**Abstract:** Agentic search, as a more autonomous and adaptive paradigm of retrieval augmentation, is driving the evolution of intelligent search systems. However, existing evaluation frameworks fail to align well with the goals of agentic search. First, the complex queries commonly used in current benchmarks often deviate from realistic user search scenarios. Second, prior approaches tend to introduce noise when extracting ground truth for end-to-end evaluations, leading to distorted assessments at a fine-grained level. Third, most current frameworks focus solely on the quality of final answers, neglecting the evaluation of the iterative process inherent to agentic search. To address these limitations, we propose RAVine -- a Reality-Aligned eValuation framework for agentic LLMs with search. RAVine targets multi-point queries and long-form answers that better reflect user intents, and introduces an attributable ground truth construction strategy to enhance the accuracy of fine-grained evaluation. Moreover, RAVine examines model's interaction with search tools throughout the iterative process, and accounts for factors of efficiency. We benchmark a series of models using RAVine and derive several insights, which we hope will contribute to advancing the development of agentic search systems. The code and datasets are available at https://github.com/SwordFaith/RAVine.

</details>
