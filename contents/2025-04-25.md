# 2025-04-25

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 39]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [LLM impact on BLV programming](https://arxiv.org/abs/2504.17018)

*Prashant Chandrasekar, Mariel Couvillion, Ayshwarya Saktheeswaran, Jessica Zeitz*

**Main category:** cs.HC

**TL;DR:** This paper investigates the impact of Large Language Models on blind and low-vision developers in software development, highlighting both benefits and accessibility challenges.

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs impact the programming workflows of blind and low-vision (BLV) developers, given their increased use in software development tasks.

**Method:** We reviewed existing literature and evaluated five popular LLM-powered integrated development environments (IDEs) by assessing their performance across a comprehensive set of programming tasks, and by observing BLV developers during coding activities.

**Result:** The evaluation revealed unsupported scenarios, incorrect model outputs, limitations in interaction support, and key barriers to interaction for BLV developers.

**Conclusion:** The paper identifies challenges and opportunities for improving accessibility in generative AI-assisted programming, emphasizing the need to address specific burdens faced by BLV developers to enhance their programming experience.

**Abstract:** Large Language Models (LLMs) are rapidly becoming integral to a wide range of tools, tasks, and problem-solving processes, especially in software development. Originally designed for natural language processing tasks such as text generation, LLMs are increasingly being used to assist both professionals and students in writing code. This growing reliance on LLM-based tools is reshaping programming workflows and task execution. In this study, we explore the impact of these technologies on blind and low-vision (BLV) developers. Our review of existing literature indicates that while LLMs help mitigate some of the challenges faced by BLV programmers, they also introduce new forms of inaccessibility. We conducted an evaluation of five popular LLM-powered integrated development environments (IDEs), assessing their performance across a comprehensive set of programming tasks. Our findings highlight several unsupported scenarios, instances of incorrect model output, and notable limitations in interaction support for specific tasks. Through observing BLV developers as they engaged in coding activities, we uncovered key interaction barriers that go beyond model accuracy or code generation quality. This paper outlines the challenges and corresponding opportunities for improving accessibility in the context of generative AI-assisted programming. Addressing these issues can meaningfully enhance the programming experience for BLV developers. As the generative AI revolution continues to unfold, it must also address the unique burdens faced by this community.

</details>


### [2] [What Makes for a Good Saliency Map? Comparing Strategies for Evaluating Saliency Maps in Explainable AI (XAI)](https://arxiv.org/abs/2504.17023)

*Felix Kares, Timo Speith, Hanwei Zhang, Markus Langer*

**Main category:** cs.HC

**TL;DR:** This study evaluates saliency maps for neural networks using subjective, objective, and mathematical evaluation methods, finding discrepancies between evaluations and offering insights into their effectiveness in user understanding.

<details>
  <summary>Details</summary>

**Motivation:** To address the open question of how to best evaluate salience maps used in explaining neural network classifications, which currently lacks a unified evaluation framework.

**Method:** A between-subject study with 166 participants, comparing popular saliency map approaches (LIME, Grad-CAM, Guided Backpropagation) across three evaluation families: subjective user measures, objective user measures, and mathematical metrics.

**Result:** The evaluations showed no difference in user trust and satisfaction across maps, Grad-CAM enhanced user abilities the most, and Guided Backpropagation received the best mathematical ratings. Additionally, some mathematical metrics correlated with user understanding, occasionally in counterintuitive ways.

**Conclusion:** The findings suggest that saliency maps do not consistently align across different evaluation methods, highlighting the need for careful consideration of both user studies and mathematical metrics in the evaluation of explainable AI.

**Abstract:** Saliency maps are a popular approach for explaining classifications of (convolutional) neural networks. However, it remains an open question as to how best to evaluate salience maps, with three families of evaluation methods commonly being used: subjective user measures, objective user measures, and mathematical metrics. We examine three of the most popular saliency map approaches (viz., LIME, Grad-CAM, and Guided Backpropagation) in a between subject study (N=166) across these families of evaluation methods. We test 1) for subjective measures, if the maps differ with respect to user trust and satisfaction; 2) for objective measures, if the maps increase users' abilities and thus understanding of a model; 3) for mathematical metrics, which map achieves the best ratings across metrics; and 4) whether the mathematical metrics can be associated with objective user measures. To our knowledge, our study is the first to compare several salience maps across all these evaluation methods$-$with the finding that they do not agree in their assessment (i.e., there was no difference concerning trust and satisfaction, Grad-CAM improved users' abilities best, and Guided Backpropagation had the most favorable mathematical metrics). Additionally, we show that some mathematical metrics were associated with user understanding, although this relationship was often counterintuitive. We discuss these findings in light of general debates concerning the complementary use of user studies and mathematical metrics in the evaluation of explainable AI (XAI) approaches.

</details>


### [3] [Psychological Effect of AI driven marketing tools for beauty/facial feature enhancement](https://arxiv.org/abs/2504.17055)

*Ayushi Agrawal, Aditya Kondai, Kavita Vemuri*

**Main category:** cs.HC

**TL;DR:** AI-powered facial assessment tools influence self-objectification, self-esteem, and emotional responses, revealing negative impacts particularly on females.

<details>
  <summary>Details</summary>

**Motivation:** To investigate how AI facial assessment tools affect self-perception and social judgments, particularly looking at gender differences.

**Method:** Two samples used different versions of a facial analysis tool—one critical and one neutral—while participants completed scales measuring self-objectification, self-esteem, and emotional responses.

**Result:** Both tool versions showed links between high self-objectification, low self-esteem, and appearance enhancement behaviors; the neutral tool still induced negative emotions, with gender differences noted in digital enhancement and emotional perception.

**Conclusion:** AI tools may unintentionally reinforce social biases, highlighting the importance of responsible design; future research will explore how training data ideologies influence outcomes.

**Abstract:** AI-powered facial assessment tools are reshaping how individuals evaluate appearance and internalize social judgments. This study examines the psychological impact of such tools on self-objectification, self-esteem, and emotional responses, with attention to gender differences. Two samples used distinct versions of a facial analysis tool: one overtly critical (N=75; M=22.9 years), and another more neutral (N=51; M=19.9 years). Participants completed validated self-objectification and self-esteem scales and custom items measuring emotion, digital/physical appearance enhancement (DAE, PAEE), and perceived social emotion (PSE). Results revealed consistent links between high self-objectification, low self-esteem, and increased appearance enhancement behaviors across both versions. Despite softer framing, the newer tool still evoked negative emotional responses (U=1466.5, p=0.013), indicating implicit feedback may reinforce appearance-related insecurities. Gender differences emerged in DAE (p=0.025) and PSE (p<0.001), with females more prone to digital enhancement and less likely to perceive emotional impact in others. These findings reveal how AI tools may unintentionally reinforce and amplify existing social biases and underscore the critical need for responsible AI design and development. Future research will investigate how human ideologies embedded in the training data of such tools shape their evaluative outputs, and how these, in turn, influence user attitudes and decisions.

</details>


### [4] [DashGuide: Authoring Interactive Dashboard Tours for Guiding Dashboard Users](https://arxiv.org/abs/2504.17150)

*Naimul Hoque, Nicole Sultanum*

**Main category:** cs.HC

**TL;DR:** DashGuide is a framework that simplifies the creation of interactive dashboard guidance by generating step-by-step overlays from user interactions, improving the authoring experience for dashboard creators.

<details>
  <summary>Details</summary>

**Motivation:** Existing methods of authoring dashboard guidance are time-consuming and challenging, leading to a need for a more efficient solution.

**Method:** DashGuide captures a sequence of author-performed interactions based on a given dashboard and communication goal to create guidance materials, allowing further editing and refinement by authors with generative assistance.

**Result:** Formative assessment involved 9 dashboard creators and confirmed the design considerations for DashGuide; an evaluation with 12 creators indicated improved authoring efficiency and expressiveness.

**Conclusion:** DashGuide successfully balances efficiency, expressiveness, and creative freedom in the authoring process for dashboard guidance.

**Abstract:** Dashboard guidance helps dashboard users better navigate interactive features, understand the underlying data, and assess insights they can potentially extract from dashboards. However, authoring dashboard guidance is a time consuming task, and embedding guidance into dashboards for effective delivery is difficult to realize. In this work, we contribute DashGuide, a framework and system to support the creation of interactive dashboard guidance with minimal authoring input. Given a dashboard and a communication goal, DashGuide captures a sequence of author-performed interactions to generate guidance materials delivered as playable step-by-step overlays, a.k.a., dashboard tours. Authors can further edit and refine individual tour steps while receiving generative assistance. We also contribute findings from a formative assessment with 9 dashboard creators, which helped inform the design of DashGuide; and findings from an evaluation of DashGuide with 12 dashboard creators, suggesting it provides an improved authoring experience that balances efficiency, expressiveness, and creative freedom.

</details>


### [5] [Improving Human-Autonomous Vehicle Interaction in Complex Systems](https://arxiv.org/abs/2504.17170)

*Robert Kaufman*

**Main category:** cs.HC

**TL;DR:** The paper discusses how autonomous vehicles (AVs) must adapt their communication strategies to meet diverse rider needs and contexts to enhance real-world adoption.

<details>
  <summary>Details</summary>

**Motivation:** Unresolved questions about AV communications hinder their adoption due to varied rider needs influenced by personal factors and driving contexts.

**Method:** The study employs three empirical studies to identify optimal communication strategies, analyze the impact of faulty communications, and use machine learning to understand trust factors in AVs.

**Result:** The findings indicate the necessity for task-sensitive, modality-appropriate, and context-sensitive communications, as well as the importance of adapting designs to individual traits and concerns.

**Conclusion:** The dissertation highlights the need for transparent, adaptable, and personalized AV systems that consider the complexities of human-AV interactions, providing insights for design and future research.

**Abstract:** Unresolved questions about how autonomous vehicles (AVs) should meet the informational needs of riders hinder real-world adoption. Complicating our ability to satisfy rider needs is that different people, goals, and driving contexts have different criteria for what constitutes interaction success. Unfortunately, most human-AV research and design today treats all people and situations uniformly. It is crucial to understand how an AV should communicate to meet rider needs, and how communications should change when the human-AV complex system changes. I argue that understanding the relationships between different aspects of the human-AV system can help us build improved and adaptable AV communications. I support this argument using three empirical studies. First, I identify optimal communication strategies that enhance driving performance, confidence, and trust for learning in extreme driving environments. Findings highlight the need for task-sensitive, modality-appropriate communications tuned to learner cognitive limits and goals. Next, I highlight the consequences of deploying faulty communication systems and demonstrate the need for context-sensitive communications. Third, I use machine learning (ML) to illuminate personal factors predicting trust in AVs, emphasizing the importance of tailoring designs to individual traits and concerns. Together, this dissertation supports the necessity of transparent, adaptable, and personalized AV systems that cater to individual needs, goals, and contextual demands. By considering the complex system within which human-AV interactions occur, we can deliver valuable insights for designers, researchers, and policymakers. This dissertation also provides a concrete domain to study theories of human-machine joint action and situational awareness, and can be used to guide future human-AI interaction research. [shortened for arxiv]

</details>


### [6] [Augmenting Captions with Emotional Cues: An AR Interface for Real-Time Accessible Communication](https://arxiv.org/abs/2504.17171)

*Sunday David Ubur*

**Main category:** cs.HC

**TL;DR:** The paper presents an AR captioning framework that enhances STEM learning for DHH students by incorporating emotional cues into live transcriptions.

<details>
  <summary>Details</summary>

**Motivation:** To support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by improving the accessibility and comprehension of auditory information through enhanced captioning.

**Method:** The framework integrates real-time speech recognition with the interpretation of non-verbal emotional cues, using AI models to analyze facial movements, gestures, and vocal tone, displayed in an AR interface developed with Unity.

**Result:** Preliminary evaluations indicate that the AR-based captioning significantly improves comprehension and reduces cognitive effort compared to conventional captioning systems.

**Conclusion:** The study highlights the potential of AR environments for creating inclusive, emotion-aware educational experiences for DHH learners.

**Abstract:** This paper introduces an augmented reality (AR) captioning framework designed to support Deaf and Hard of Hearing (DHH) learners in STEM classrooms by integrating non-verbal emotional cues into live transcriptions. Unlike conventional captioning systems that offer only plain text, our system fuses real-time speech recognition with affective and visual signal interpretation, including facial movements, gestures, and vocal tone, to produce emotionally enriched captions. These enhanced captions are rendered in an AR interface developed with Unity and provide contextual annotations such as speaker tone markers (e.g., "concerned") and gesture indicators (e.g., "nods"). The system leverages live camera and microphone input, processed through AI models to detect multimodal cues. Findings from preliminary evaluations suggest that this AR-based captioning approach significantly enhances comprehension and reduces cognitive effort compared to standard captions. Our work emphasizes the potential of immersive environments for inclusive, emotion-aware educational accessibility.

</details>


### [7] [Lessons from Deploying Learning-based CSI Localization on a Large-Scale ISAC Platform](https://arxiv.org/abs/2504.17173)

*Tianyu Zhang, Dongheng Zhang, Ruixu Geng, Xuecheng Xie, Shuai Yang, Yan Chen*

**Main category:** cs.HC

**TL;DR:** The paper proposes a novel CSI-based localization system for large-scale WiFi environments, achieving high localization accuracy and addressing key challenges in data utilization and heterogeneity.

<details>
  <summary>Details</summary>

**Motivation:** CSI-based localization has potential advantages over RSSI-based approaches but is limited by small-scale evaluations that hinder broader application and deployment.

**Method:** The study introduces a graph-based learning framework for heterogeneous CSI data, utilizes a pretext pretraining task for unlabeled data, and applies a confidence-aware fine-tuning strategy for better localization outcomes.

**Result:** Achieved a median localization error of 2.17 meters and a floor accuracy of 99.49%, with an 18.7% reduction in mean absolute error compared to previous best-performing systems.

**Conclusion:** The proposed framework effectively enhances CSI-based localization performance and demonstrates the viability of large-scale deployments in real-world scenarios.

**Abstract:** In recent years, Channel State Information (CSI), recognized for its fine-grained spatial characteristics, has attracted increasing attention in WiFi-based indoor localization. However, despite its potential, CSI-based approaches have yet to achieve the same level of deployment scale and commercialization as those based on Received Signal Strength Indicator (RSSI). A key limitation lies in the fact that most existing CSI-based systems are developed and evaluated in controlled, small-scale environments, limiting their generalizability. To bridge this gap, we explore the deployment of a large-scale CSI-based localization system involving over 400 Access Points (APs) in a real-world building under the Integrated Sensing and Communication (ISAC) paradigm. We highlight two critical yet often overlooked factors: the underutilization of unlabeled data and the inherent heterogeneity of CSI measurements. To address these challenges, we propose a novel CSI-based learning framework for WiFi localization, tailored for large-scale ISAC deployments on the server side. Specifically, we employ a novel graph-based structure to model heterogeneous CSI data and reduce redundancy. We further design a pretext pretraining task that incorporates spatial and temporal priors to effectively leverage large-scale unlabeled CSI data. Complementarily, we introduce a confidence-aware fine-tuning strategy to enhance the robustness of localization results. In a leave-one-smartphone-out experiment spanning five floors and 25, 600 m2, we achieve a median localization error of 2.17 meters and a floor accuracy of 99.49%. This performance corresponds to an 18.7% reduction in mean absolute error (MAE) compared to the best-performing baseline.

</details>


### [8] [Factually: Exploring Wearable Fact-Checking for Augmented Truth Discernment](https://arxiv.org/abs/2504.17204)

*Chitralekha Gupta, Hanjun Wu, Praveen Sasikumar, Shreyas Sridhar, Priambudi Bagaskara, Suranga Nanayakkara*

**Main category:** cs.HC

**TL;DR:** The paper proposes a voice-based learning companion called Factually, designed to enhance cognitive functions and critical thinking through real-time misinformation detection and language learning.

<details>
  <summary>Details</summary>

**Motivation:** The aim is to transform human capabilities by augmenting cognitive functions to improve informal learning and critical thinking in users.

**Method:** Factually is a wearable, proactive fact-checking system that integrates with devices like smartwatches and rings, providing users with vibrotactile feedback to alert them to potential misinformation.

**Result:** Demonstrated utility through three scenarios, showing its effectiveness in helping users assess information critically; early feedback indicates enhanced fact-checking capability and experiential benefits.

**Conclusion:** Factually has the potential to significantly extend cognitive abilities, particularly in real-time misinformation detection and support for language learning.

**Abstract:** Wearable devices are transforming human capabilities by seamlessly augmenting cognitive functions. In this position paper, we propose a voice-based, interactive learning companion designed to amplify and extend cognitive abilities through informal learning. Our vision is threefold: (1) to enable users to discover new knowledge on-the-go through contextual interactive quizzes, fostering critical thinking and mindfulness, (2) to proactively detect misinformation, empowering users to critically assess information in real time, and (3) to provide spoken language correction and prompting hints for second language learning and effective communication. As an initial step toward this vision, we present Factually - a proactive, wearable fact-checking system integrated into devices like smartwatches or rings. Factually discreetly alerts users to potential falsehoods via vibrotactile feedback, helping them assess information critically. We demonstrate its utility through three illustrative scenarios, highlighting its potential to extend cognitive abilities for real-time misinformation detection. Early qualitative feedback suggests that Factually can enhance users' fact-checking capabilities, offering both practical and experiential benefits.

</details>


### [9] [MV-Crafter: An Intelligent System for Music-guided Video Generation](https://arxiv.org/abs/2504.17267)

*Chuer Chen, Shengqi Dang, Yuqi Liu, Nanxuan Zhao, Yang Shi, Nan Cao*

**Main category:** cs.HC

**TL;DR:** MV-Crafter is a system that automates the creation of high-quality music videos, improving output quality and simplifying the process for non-professionals.

<details>
  <summary>Details</summary>

**Motivation:** The demand for engaging music videos is high, but the complex creation process presents challenges, particularly for non-professional creators.

**Method:** MV-Crafter consists of three modules: script generation using a large language model for musical semantics, video generation, and a dynamic synchronization algorithm for aligning video clips with music.

**Result:** Extensive experiments showed that MV-Crafter improves the quality of generated music videos, offering an effective solution to the challenges of music video creation.

**Conclusion:** MV-Crafter successfully enables the automated generation of high-quality music videos, enhancing the creative possibilities for users.

**Abstract:** Music videos, as a prevalent form of multimedia entertainment, deliver engaging audio-visual experiences to audiences and have gained immense popularity among singers and fans. Creators can express their interpretations of music naturally through visual elements. However, the creation process of music video demands proficiency in script design, video shooting, and music-video synchronization, posing significant challenges for non-professionals. Previous work has designed automated music video generation frameworks. However, they suffer from complexity in input and poor output quality. In response, we present MV-Crafter, a system capable of producing high-quality music videos with synchronized music-video rhythm and style. Our approach involves three technical modules that simulate the human creation process: the script generation module, video generation module, and music-video synchronization module. MV-Crafter leverages a large language model to generate scripts considering the musical semantics. To address the challenge of synchronizing short video clips with music of varying lengths, we propose a dynamic beat matching algorithm and visual envelope-induced warping method to ensure precise, monotonic music-video synchronization. Besides, we design a user-friendly interface to simplify the creation process with intuitive editing features. Extensive experiments have demonstrated that MV-Crafter provides an effective solution for improving the quality of generated music videos.

</details>


### [10] [Exploring Context-aware and LLM-driven Locomotion for Immersive Virtual Reality](https://arxiv.org/abs/2504.17331)

*Süleyman Özdel, Kadir Burak Buldu, Enkelejda Kasneci, Efe Bozkir*

**Main category:** cs.HC

**TL;DR:** This study presents a novel locomotion technique in virtual reality powered by large language models (LLMs), allowing users to navigate using natural language, and compares it with traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** To improve user experience in virtual reality by providing an accessible, hands-free locomotion method that offers flexibility over traditional speech-based commands.

**Method:** Three locomotion methods were evaluated: controller-based teleportation, voice-based steering, and LLM-driven navigation. Metrics included eye-tracking, SHAP analysis, and standardized questionnaires assessing usability, presence, cybersickness, and cognitive load.

**Result:** The LLM-driven locomotion showed comparable usability, presence, and cybersickness scores to traditional methods, while enhancing user attention and engagement, with distinct visual attention patterns identified through SHAP analysis.

**Conclusion:** The proposed LLM-driven technique is a viable hands-free locomotion alternative in virtual reality, enhancing accessibility and user engagement.

**Abstract:** Locomotion plays a crucial role in shaping the user experience within virtual reality environments. In particular, hands-free locomotion offers a valuable alternative by supporting accessibility and freeing users from reliance on handheld controllers. To this end, traditional speech-based methods often depend on rigid command sets, limiting the naturalness and flexibility of interaction. In this study, we propose a novel locomotion technique powered by large language models (LLMs), which allows users to navigate virtual environments using natural language with contextual awareness. We evaluate three locomotion methods: controller-based teleportation, voice-based steering, and our language model-driven approach. Our evaluation measures include eye-tracking data analysis, including explainable machine learning through SHAP analysis as well as standardized questionnaires for usability, presence, cybersickness, and cognitive load to examine user attention and engagement. Our findings indicate that the LLM-driven locomotion possesses comparable usability, presence, and cybersickness scores to established methods like teleportation, demonstrating its novel potential as a comfortable, natural language-based, hands-free alternative. In addition, it enhances user attention within the virtual environment, suggesting greater engagement. Complementary to these findings, SHAP analysis revealed that fixation, saccade, and pupil-related features vary across techniques, indicating distinct patterns of visual attention and cognitive processing. Overall, we state that our method can facilitate hands-free locomotion in virtual spaces, especially in supporting accessibility.

</details>


### [11] [DataScout: Automatic Data Fact Retrieval for Statement Augmentation with an LLM-Based Agent](https://arxiv.org/abs/2504.17334)

*Chuer Chen, Yuqi Liu, Danqing Shi, Shixiong Cao, Nan Cao*

**Main category:** cs.HC

**TL;DR:** DataScout is an interactive system designed for efficient, stance-based data retrieval to enhance data storytelling.

<details>
  <summary>Details</summary>

**Motivation:** Creating comprehensive and objective data narratives is challenging due to time-consuming data search and the need for analytical skills.

**Method:** DataScout employs an LLM-based agent to build a retrieval tree, allowing users to collaboratively control its expansion and visualize it as a mind map for intuitive navigation.

**Result:** Evaluation through case studies and expert interviews shows that DataScout effectively retrieves diverse data facts from multiple perspectives, aiding statement verification and enhancing credibility.

**Conclusion:** DataScout improves the process of data fact retrieval, making it easier for users to construct credible and persuasive data narratives.

**Abstract:** A data story typically integrates data facts from multiple perspectives and stances to construct a comprehensive and objective narrative. However, retrieving these facts demands time for data search and challenges the creator's analytical skills. In this work, we introduce DataScout, an interactive system that automatically performs reasoning and stance-based data facts retrieval to augment the user's statement. Particularly, DataScout leverages an LLM-based agent to construct a retrieval tree, enabling collaborative control of its expansion between users and the agent. The interface visualizes the retrieval tree as a mind map that eases users to intuitively steer the retrieval direction and effectively engage in reasoning and analysis. We evaluate the proposed system through case studies and in-depth expert interviews. Our evaluation demonstrates that DataScout can effectively retrieve multifaceted data facts from different stances, helping users verify their statements and enhance the credibility of their stories.

</details>


### [12] [The Riemannian Means Field Classifier for EEG-Based BCI Data](https://arxiv.org/abs/2504.17352)

*Anton Andreev, Grégoire Cattan, Marco Congedo*

**Main category:** cs.HC

**TL;DR:** The proposed power means classifier improves upon the Riemannian MDM classifier for EEG-based BCIs, demonstrating superior performance across multiple datasets while maintaining simplicity.

<details>
  <summary>Details</summary>

**Motivation:** To enhance the existing Riemannian minimum distance to mean (MDM) classifier for EEG-based brain–computer interfaces (BCIs) by incorporating power means of SPD matrices for better performance.

**Method:** The study introduces a modified MDM classifier that utilizes several power means of symmetric positive-definite (SPD) matrices instead of just the geometric mean, evaluated across 20 public BCI datasets.

**Result:** The new classifier outperforms the traditional MDM classifier, achieving performance levels close to state-of-the-art methods, based on analysis of data from 587 individuals across various paradigms.

**Conclusion:** The proposed method combines improved accuracy with the existing benefits of the original MDM classifier, and the code will be made available as open source to support reproducible research.

**Abstract:** A substantial amount of research has demonstrated the robustness and accuracy of the Riemannian minimum distance to mean (MDM) classifier for all kinds of EEG-based brain--computer interfaces (BCIs). This classifier is simple, fully deterministic, robust to noise, computationally efficient, and prone to transfer learning. Its training is very simple, requiring just the computation of a geometric mean of a symmetric positive-definite (SPD) matrix per class. We propose an improvement of the MDM involving a number of power means of SPD matrices instead of the sole geometric mean. By the analysis of 20 public databases, 10 for the motor-imagery BCI paradigm and 10 for the P300 BCI paradigm, comprising 587 individuals in total, we show that the proposed classifier clearly outperforms the MDM, approaching the state-of-the art in terms of performance while retaining the simplicity and the deterministic behavior. In order to promote reproducible research, our code will be released as open source.

</details>


### [13] [The Malicious Technical Ecosystem: Exposing Limitations in Technical Governance of AI-Generated Non-Consensual Intimate Images of Adults](https://arxiv.org/abs/2504.17663)

*Michelle L. Ding, Harini Suresh*

**Main category:** cs.HC

**TL;DR:** The paper discusses AI governance to combat AI-generated non-consensual intimate images (AIG-NCII) by analyzing the socio-technical landscape and identifying governance gaps.

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the challenges posed by AI-generated non-consensual intimate images, commonly referred to as deep fake pornography, through a survivor-centered lens.

**Method:** The authors identify a malicious technical ecosystem (MTE) that consists of face-swapping models and nudifying software, and analyze current governance methods using the NIST AI 100-4 report as a reference.

**Result:** The analysis reveals significant gaps in the current regulatory landscape, indicating that existing governance methods are inadequate for effectively controlling the MTE associated with adult AIG-NCII.

**Conclusion:** The paper concludes that flawed assumptions about governance practices must be addressed to enhance regulation and prevention of AIG-NCII.

**Abstract:** In this paper, we adopt a survivor-centered approach to locate and dissect the role of sociotechnical AI governance in preventing AI-Generated Non-Consensual Intimate Images (AIG-NCII) of adults, colloquially known as "deep fake pornography." We identify a "malicious technical ecosystem" or "MTE," comprising of open-source face-swapping models and nearly 200 "nudifying" software programs that allow non-technical users to create AIG-NCII within minutes. Then, using the National Institute of Standards and Technology (NIST) AI 100-4 report as a reflection of current synthetic content governance methods, we show how the current landscape of practices fails to effectively regulate the MTE for adult AIG-NCII, as well as flawed assumptions explaining these gaps.

</details>


### [14] [INSIGHT: Bridging the Student-Teacher Gap in Times of Large Language Models](https://arxiv.org/abs/2504.17677)

*Jarne Thys, Sebe Vanbrabant, Davy Vanacken, Gustavo Rovelo Ruiz*

**Main category:** cs.HC

**TL;DR:** This paper introduces INSIGHT, a modular AI system designed to enhance teaching and learning in higher education by analyzing student questions and providing support for personalized instruction.

<details>
  <summary>Details</summary>

**Motivation:** The integration of AI, particularly Large Language Models, in education presents potential benefits for teaching while also raising concerns regarding student-teacher interaction and privacy.

**Method:** INSIGHT employs a modular design to assist teaching staff by analyzing students' questions, extracting keywords to dynamically create an FAQ, and offering insights for personalized support.

**Result:** The analysis of student questions enables the creation of a dynamic FAQ and personalized insights for instructors, potentially improving face-to-face support in educational settings.

**Conclusion:** Future developments could leverage collected data for adaptive learning, further enhancing the learning experience by tailoring content to student progress and learning styles.

**Abstract:** The rise of AI, especially Large Language Models, presents challenges and opportunities to integrate such technology into the classroom. AI has the potential to revolutionize education by helping teaching staff with various tasks, such as personalizing their teaching methods, but it also raises concerns, for example, about the degradation of student-teacher interactions and user privacy. This paper introduces INSIGHT, a proof of concept to combine various AI tools to assist teaching staff and students in the process of solving exercises. INSIGHT has a modular design that allows it to be integrated into various higher education courses. We analyze students' questions to an LLM by extracting keywords, which we use to dynamically build an FAQ from students' questions and provide new insights for the teaching staff to use for more personalized face-to-face support. Future work could build upon INSIGHT by using the collected data to provide adaptive learning and adjust content based on student progress and learning styles to offer a more interactive and inclusive learning experience.

</details>


### [15] ['The Boring and the Tedious': Invisible Labour in India's Gig-Economy](https://arxiv.org/abs/2504.17697)

*Pratyay Suvarnapathaki, Viral Shah, Saarthak Negi, Nimmi Rangaswamy*

**Main category:** cs.HC

**TL;DR:** The paper examines the challenges faced by gig-based food delivery workers in India, highlighting their 'digital discomfort' and proposing worker-centered solutions.

<details>
  <summary>Details</summary>

**Motivation:** To address the invisible labour and challenges faced by marginalised gig workers on platforms like Swiggy and Zomato, as well as to enhance their working conditions.

**Method:** The research employs 14 semi-structured interviews to explore the experiences of gig delivery agents, focusing on waiting time and repetitive interactions in user interfaces.

**Result:** The findings reveal that workers develop creative strategies to deal with algorithmic management but face obstacles from platform design and system opacity, which contribute to their sense of discomfort.

**Conclusion:** The paper advocates for a rethinking of HCI approaches in the Global South, emphasizing the need to prioritize worker autonomy over purely efficiency-driven design.

**Abstract:** India's gig-based food delivery platforms, such as Swiggy and Zomato, provide crucial income to marginalised communities but also entrench workers in cycles of invisible labour. Through 14 semi-structured interviews, we analyse waiting time and repetitive UI itneractions as key burdens that contribute to 'digital discomfort' for gig based food delivery agents. We find that workers employ creative strategies to navigate algorithmic management, yet remain constrained by platform-side 'gamification' and system opacity. We propose worker-centered GUI automation as a potential intervention to reduce friction while preserving agency. In conclusion, this position paper argues for rethinking HCI approaches in the Global South to prioritise worker autonomy over efficiency-driven design optimisations.

</details>


### [16] [LUIDA: Large-scale Unified Infrastructure for Digital Assessments based on Commercial Metaverse Platform](https://arxiv.org/abs/2504.17705)

*Yong-Hao Hu, Sotaro Yokoi, Yuji Hatada, Yuichi Hiroi, Takuji Narumi, Takefumi Hiraki*

**Main category:** cs.HC

**TL;DR:** LUIDA is a metaverse-based framework that integrates fragmented workflows in VR research, enhancing efficiency and reproducibility.

<details>
  <summary>Details</summary>

**Motivation:** Current VR research workflows are fragmented, requiring separate tools for various tasks, which leads to inefficiencies and increased workload.

**Method:** LUIDA provides a unified framework that automates virtual environment allocation and offers adaptable implementation templates, tested on the Cluster metaverse platform through user studies and replicated experiments.

**Result:** Users of LUIDA reported high usability scores (SUS: 73.75) and manageable workload (NASA-TLX: 24.11). Replicated experiments with public users confirmed the integrity of results, with around 200 participants recruited in a week.

**Conclusion:** LUIDA streamlines VR research workflows, and plans for an open platform release aim to standardize protocols for improved efficiency and reproducibility.

**Abstract:** Online experiments using metaverse platforms have gained significant traction in Human-Computer Interaction and Virtual Reality (VR) research. However, current research workflows are highly fragmented, as researchers must use separate tools for system implementation, participant recruitment, experiment execution, and data collection, reducing consistency and increasing workload. We present LUIDA (Large-scale Unified Infrastructure for Digital Assessments), a metaverse-based framework that integrates these fragmented processes. LUIDA automatically allocates interconnected virtual environments for parallel experiment execution and provides implementation templates adaptable to various VR research domains, requiring minimal metaverse development expertise. Our evaluation included two studies using a prototype built on Cluster, the commercial metaverse platform. First, VR researchers using LUIDA to develop and run experiments reported high usability scores (SUS: 73.75) and moderate workload (NASA-TLX: 24.11) for overall usage, with interviews confirming streamlined workflows compared to traditional laboratory experiments. Second, we conducted three replicated experiments with public Cluster users, each recruiting approximately 200 participants within one week. These experiments produced results that closely matched the original studies, validating the experimental integrity of LUIDA across research domains. After technical refinements, we plan to release LUIDA as an open platform, providing a standardized protocol to improve research efficiency and experimental reproducibility in VR studies.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [Bidirectional Mamba for Single-Cell Data: Efficient Context Learning with Biological Fidelity](https://arxiv.org/abs/2504.16956)

*Cong Qi, Hanzhang Fang, Tianxing Hu, Siqi Jiang, Wei Zhi*

**Main category:** cs.CL

**TL;DR:** GeneMamba is a scalable foundation model for single-cell transcriptomics that outperforms transformer-based models by efficiently handling cellular heterogeneity with linear-time complexity.

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity in single-cell RNA sequencing necessitates models that can effectively manage high dimensionality, sparsity, and batch effects.

**Method:** GeneMamba employs state space modeling and the Bi-Mamba architecture, achieving bidirectional gene context with linear-time complexity. It is pretrained on nearly 30 million cells using biologically informed objectives such as pathway-aware contrastive loss.

**Result:** GeneMamba demonstrated strong performance in multi-batch integration, cell type annotation, and gene-gene correlation, showcasing significant computational gains and robustness compared to transformer baselines.

**Conclusion:** GeneMamba serves as a practical and powerful alternative to existing methods, contributing to the advancement of scalable tools for large-scale single-cell data analysis.

**Abstract:** Single-cell RNA sequencing (scRNA-seq) enables high-resolution analysis of cellular heterogeneity, but its complexity, which is marked by high dimensionality, sparsity, and batch effects, which poses major computational challenges. Transformer-based models have made significant advances in this domain but are often limited by their quadratic complexity and suboptimal handling of long-range dependencies. In this work, we introduce GeneMamba, a scalable and efficient foundation model for single-cell transcriptomics built on state space modeling. Leveraging the Bi-Mamba architecture, GeneMamba captures bidirectional gene context with linear-time complexity, offering substantial computational gains over transformer baselines. The model is pretrained on nearly 30 million cells and incorporates biologically informed objectives, including pathway-aware contrastive loss and rank-based gene encoding. We evaluate GeneMamba across diverse tasks, including multi-batch integration, cell type annotation, and gene-gene correlation, demonstrating strong performance, interpretability, and robustness. These results position GeneMamba as a practical and powerful alternative to transformer-based methods, advancing the development of biologically grounded, scalable tools for large-scale single-cell data analysis.

</details>


### [18] [Tokenization Matters: Improving Zero-Shot NER for Indic Languages](https://arxiv.org/abs/2504.16977)

*Priyaranjan Pattnayak, Hitesh Laxmichand Patel, Amit Agarwal*

**Main category:** cs.CL

**TL;DR:** SentencePiece outperforms BPE for Named Entity Recognition in low resource Indic languages, especially in preserving entity consistency and handling morphological complexity.

<details>
  <summary>Details</summary>

**Motivation:** Explore the effectiveness of various tokenization methods for NER in low resource Indic languages, addressing the limitations of BPE.

**Method:** Systematic comparison of BPE, SentencePiece, and Character Level tokenization using IndicBERT for NER tasks focused on low and extremely low resource Indic languages.

**Result:** SentencePiece consistently provides better performance than BPE, particularly in zero shot cross lingual settings, and better preserves linguistic structures and entity recognition accuracy.

**Conclusion:** SentencePiece is the preferred tokenization strategy for NER in low resource Indic languages, outperforming BPE in both linguistic preservation and generalization across different scripts.

**Abstract:** Tokenization is a critical component of Natural Language Processing (NLP), especially for low resource languages, where subword segmentation influences vocabulary structure and downstream task accuracy. Although Byte Pair Encoding (BPE) is a standard tokenization method in multilingual language models, its suitability for Named Entity Recognition (NER) in low resource Indic languages remains underexplored due to its limitations in handling morphological complexity. In this work, we systematically compare BPE, SentencePiece, and Character Level tokenization strategies using IndicBERT for NER tasks in low resource Indic languages like Assamese, Bengali, Marathi, and Odia, as well as extremely low resource Indic languages like Santali, Manipuri, and Sindhi. We assess both intrinsic linguistic properties tokenization efficiency, out of vocabulary (OOV) rates, and morphological preservation as well as extrinsic downstream performance, including fine tuning and zero shot cross lingual transfer.   Our experiments show that SentencePiece is a consistently better performing approach than BPE for NER in low resource Indic Languages, particularly in zero shot cross lingual settings, as it better preserves entity consistency. While BPE provides the most compact tokenization form, it is not capable of generalization because it misclassifies or even fails to recognize entity labels when tested on unseen languages. In contrast, SentencePiece constitutes a better linguistic structural preservation model, benefiting extremely low resource and morphologically rich Indic languages, such as Santali and Manipuri, for superior entity recognition, as well as high generalization across scripts, such as Sindhi, written in Arabic. The results point to SentencePiece as the more effective tokenization strategy for NER within multilingual and low resource Indic NLP applications.

</details>


### [19] [Optimizing LLMs for Italian: Reducing Token Fertility and Enhancing Efficiency Through Vocabulary Adaptation](https://arxiv.org/abs/2504.17025)

*Luca Moroni, Giovanni Puccetti, Pere-Lluis Huguet Cabot, Andrei Stefan Bejgu, Edoardo Barba, Alessio Miaschi, Felice Dell'Orletta, Andrea Esuli, Roberto Navigli*

**Main category:** cs.CL

**TL;DR:** This paper introduces Semantic Alignment Vocabulary Adaptation (SAVA) to optimize English pretrained LLMs for Italian, achieving improved performance and efficiency.

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of pretrained LLMs is mostly focused on English, leading to inefficiencies when applied to non-English languages due to language contamination.

**Method:** A variety of vocabulary adaptation techniques were compared, culminating in the development of SAVA, which utilizes neural mapping for vocabulary substitution and adapts existing LLMs.

**Result:** SAVA significantly reduces token fertility by 25% in Mistral-7b-v0.1 and optimizes Llama-3.1-8B, leading to recovery of performance after limited continual training and demonstrating enhanced capabilities in multi-choice and generative tasks.

**Conclusion:** The adapted models prove effective for Italian, showcasing that targeted vocabulary adaptation can significantly enhance LLM performance for non-English languages.

**Abstract:** The number of pretrained Large Language Models (LLMs) is increasing steadily, though the majority are designed predominantly for the English language. While state-of-the-art LLMs can handle other languages, due to language contamination or some degree of multilingual pretraining data, they are not optimized for non-English languages, leading to inefficient encoding (high token "fertility") and slower inference speed. In this work, we thoroughly compare a variety of vocabulary adaptation techniques for optimizing English LLMs for the Italian language, and put forward Semantic Alignment Vocabulary Adaptation (SAVA), a novel method that leverages neural mapping for vocabulary substitution. SAVA achieves competitive performance across multiple downstream tasks, enhancing grounded alignment strategies. We adapt two LLMs: Mistral-7b-v0.1, reducing token fertility by 25\%, and Llama-3.1-8B, optimizing the vocabulary and reducing the number of parameters by 1 billion. We show that, following the adaptation of the vocabulary, these models can recover their performance with a relatively limited stage of continual training on the target language. Finally, we test the capabilities of the adapted models on various multi-choice and generative tasks.

</details>


### [20] [Do Words Reflect Beliefs? Evaluating Belief Depth in Large Language Models](https://arxiv.org/abs/2504.17052)

*Shariar Kabir, Kevin Esterling, Yue Dong*

**Main category:** cs.CL

**TL;DR:** The study evaluates the ideological consistency of Large Language Models (LLMs) in political discourse, revealing topic-specific stability rather than uniform ideological beliefs.

<details>
  <summary>Details</summary>

**Motivation:** To examine whether LLM responses in political discussions reflect genuine beliefs or simply align with training data, given the inconsistency of LLM outputs.

**Method:** A novel framework evaluates LLM belief depth through argumentative consistency and uncertainty quantification, assessing 12 LLMs on 19 economic policies using both supportive and opposing arguments.

**Result:** Up to 95% of left-leaning and 89% of right-leaning models' responses showed consistency, with a high accuracy (AUROC=0.78) in distinguishing between surface-level alignment and genuine belief.

**Conclusion:** LLMs do not maintain stable, human-like political ideologies, highlighting the need for topic-specific reliability assessments in real-world applications.

**Abstract:** Large Language Models (LLMs) are increasingly shaping political discourse, yet their responses often display inconsistency when subjected to scrutiny. While prior research has primarily categorized LLM outputs as left- or right-leaning to assess their political stances, a critical question remains: Do these responses reflect genuine internal beliefs or merely surface-level alignment with training data? To address this, we propose a novel framework for evaluating belief depth by analyzing (1) argumentative consistency and (2) uncertainty quantification. We evaluate 12 LLMs on 19 economic policies from the Political Compass Test, challenging their belief stability with both supportive and opposing arguments. Our analysis reveals that LLMs exhibit topic-specific belief stability rather than a uniform ideological stance. Notably, up to 95% of left-leaning models' responses and 89% of right-leaning models' responses remain consistent under the challenge, enabling semantic entropy to achieve high accuracy (AUROC=0.78), effectively distinguishing between surface-level alignment from genuine belief. These findings call into question the assumption that LLMs maintain stable, human-like political ideologies, emphasizing the importance of conducting topic-specific reliability assessments for real-world applications.

</details>


### [21] [Agree to Disagree? A Meta-Evaluation of LLM Misgendering](https://arxiv.org/abs/2504.17075)

*Arjun Subramonian, Vagrant Gautam, Preethi Seshadri, Dietrich Klakow, Kai-Wei Chang, Yizhou Sun*

**Main category:** cs.CL

**TL;DR:** This paper conducts a meta-evaluation of methods for measuring misgendering in large language models (LLMs), showing significant disagreement among methods.

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to investigate the convergent validity of various existing methods for evaluating LLM misgendering, focusing on whether these methods produce aligned results.

**Method:** A systematic meta-evaluation across three datasets is conducted, transforming data for parallel probability- and generation-based assessments, and comparing results from six models spanning three families.

**Result:** The study finds a 20.2% disagreement rate among evaluation instances, revealing that the methods can conflict at the instance, dataset, and model levels, with further human evaluation exposing the complexities of misgendering beyond just pronouns.

**Conclusion:** The findings highlight fundamental issues in current LLM evaluation methods and provide recommendations for future assessments, underscoring that assumptions of agreement among evaluation techniques may be misplaced.

**Abstract:** Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.

</details>


### [22] [How Individual Traits and Language Styles Shape Preferences In Open-ended User-LLM Interaction: A Preliminary Study](https://arxiv.org/abs/2504.17083)

*Rendi Chevi, Kentaro Inui, Thamar Solorio, Alham Fikri Aji*

**Main category:** cs.CL

**TL;DR:** User preferences for LLM interactions can be significantly influenced by language style rather than just information accuracy.

<details>
  <summary>Details</summary>

**Motivation:** The paper examines how varying language styles in LLM responses may affect user preferences, especially in the context of perceived authority and certainty, despite potential misinformation risks.

**Method:** The authors conducted exploratory and experimental user studies to investigate the influence of language style on user preferences and how these preferences are moderated by individual traits.

**Result:** Findings indicate that language style does impact user preferences differently across populations, moderated by individual characteristics, but the study's limitations in sample diversity and size necessitate cautious interpretation of the results.

**Conclusion:** The research highlights the need for further studies to corroborate these preliminary findings and to explore the causal relationships between language style, user traits, and preferences, while addressing existing sample limitations.

**Abstract:** What makes an interaction with the LLM more preferable for the user? While it is intuitive to assume that information accuracy in the LLM's responses would be one of the influential variables, recent studies have found that inaccurate LLM's responses could still be preferable when they are perceived to be more authoritative, certain, well-articulated, or simply verbose. These variables interestingly fall under the broader category of language style, implying that the style in the LLM's responses might meaningfully influence users' preferences. This hypothesized dynamic could have double-edged consequences: enhancing the overall user experience while simultaneously increasing their susceptibility to risks such as LLM's misinformation or hallucinations. In this short paper, we present our preliminary studies in exploring this subject. Through a series of exploratory and experimental user studies, we found that LLM's language style does indeed influence user's preferences, but how and which language styles influence the preference varied across different user populations, and more interestingly, moderated by the user's very own individual traits. As a preliminary work, the findings in our studies should be interpreted with caution, particularly given the limitations in our samples, which still need wider demographic diversity and larger sample sizes. Our future directions will first aim to address these limitations, which would enable a more comprehensive joint effect analysis between the language style, individual traits, and preferences, and further investigate the potential causal relationship between and beyond these variables.

</details>


### [23] [Co-CoT: A Prompt-Based Framework for Collaborative Chain-of-Thought Reasoning](https://arxiv.org/abs/2504.17091)

*Seunghyun Yoo*

**Main category:** cs.CL

**TL;DR:** The paper proposes an Interactive Chain-of-Thought Framework to enhance transparency and engagement in AI-generated content, encouraging critical thinking by allowing users to interact with the reasoning process.

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of short-form content and AI usage has led to a decline in opportunities for deep thinking and critical engagement with AI outputs.

**Method:** The proposed framework decomposes reasoning into modular blocks that users can inspect, modify, and re-execute, integrating an edit-adaptation mechanism to cater to diverse cognitive styles.

**Result:** The framework facilitates active cognitive engagement and responsible AI usage, while ensuring ethical transparency through metadata disclosure, bias checkpoints, and privacy safeguards.

**Conclusion:** The design principles and architecture outlined promote critical engagement and inclusivity in AI systems, addressing complex societal challenges.

**Abstract:** Due to the proliferation of short-form content and the rapid adoption of AI, opportunities for deep, reflective thinking have significantly diminished, undermining users' critical thinking and reducing engagement with the reasoning behind AI-generated outputs. To address this issue, we propose an Interactive Chain-of-Thought (CoT) Framework that enhances human-centered explainability and responsible AI usage by making the model's inference process transparent, modular, and user-editable. The framework decomposes reasoning into clearly defined blocks that users can inspect, modify, and re-execute, encouraging active cognitive engagement rather than passive consumption. It further integrates a lightweight edit-adaptation mechanism inspired by preference learning, allowing the system to align with diverse cognitive styles and user intentions. Ethical transparency is ensured through explicit metadata disclosure, built-in bias checkpoint functionality, and privacy-preserving safeguards. This work outlines the design principles and architecture necessary to promote critical engagement, responsible interaction, and inclusive adaptation in AI systems aimed at addressing complex societal challenges.

</details>


### [24] [The Rise of Small Language Models in Healthcare: A Comprehensive Survey](https://arxiv.org/abs/2504.17119)

*Muskan Garg, Shaina Raza, Shebuti Rayana, Xingyi Liu, Sunghwan Sohn*

**Main category:** cs.CL

**TL;DR:** The paper presents a survey on small language models (SLMs) for healthcare, offering a taxonomic framework and insights into their applications amidst growing data privacy concerns.

<details>
  <summary>Details</summary>

**Motivation:** The motivation centers around the need for scalable, clinically viable solutions in healthcare informatics, particularly in resource-constrained environments, as large language models struggle with data privacy issues.

**Method:** The methodology involves creating a taxonomic framework to categorize SLMs for healthcare professionals, detailing architectural foundations, model adaptation strategies, and techniques for improving accessibility and sustainability.

**Result:** The survey compiles experimental results on SLM performance across various NLP tasks in healthcare, demonstrating their potential to transform healthcare informatics.

**Conclusion:** The paper concludes by providing resources for healthcare professionals to foster future research and development in SLMs, emphasizing their innovative capabilities in the healthcare sector.

**Abstract:** Despite substantial progress in healthcare applications driven by large language models (LLMs), growing concerns around data privacy, and limited resources; the small language models (SLMs) offer a scalable and clinically viable solution for efficient performance in resource-constrained environments for next-generation healthcare informatics. Our comprehensive survey presents a taxonomic framework to identify and categorize them for healthcare professionals and informaticians. The timeline of healthcare SLM contributions establishes a foundational framework for analyzing models across three dimensions: NLP tasks, stakeholder roles, and the continuum of care. We present a taxonomic framework to identify the architectural foundations for building models from scratch; adapting SLMs to clinical precision through prompting, instruction fine-tuning, and reasoning; and accessibility and sustainability through compression techniques. Our primary objective is to offer a comprehensive survey for healthcare professionals, introducing recent innovations in model optimization and equipping them with curated resources to support future research and development in the field. Aiming to showcase the groundbreaking advancements in SLMs for healthcare, we present a comprehensive compilation of experimental results across widely studied NLP tasks in healthcare to highlight the transformative potential of SLMs in healthcare. The updated repository is available at Github

</details>


### [25] [Steering the CensorShip: Uncovering Representation Vectors for LLM "Thought" Control](https://arxiv.org/abs/2504.17130)

*Hannah Cyberey, David Evans*

**Main category:** cs.CL

**TL;DR:** The paper explores censorship in large language models using representation engineering to identify and manipulate models' compliance with harmful requests.

<details>
  <summary>Details</summary>

**Motivation:** To understand how censorship operates in large language models (LLMs) that refuse harmful requests and align responses with model operators' preferences.

**Method:** Utilizes representation engineering techniques to study open-weights safety-tuned models, focusing on finding refusal-compliance vectors that control censorship levels and revealing dimensions of thought suppression in reasoning LLMs.

**Result:** The research identifies a method to find vectors that detect and control censorship in model outputs, as well as vectors that suppress reasoning processes in models, effectively alleviating censorship.

**Conclusion:** The findings indicate that it is possible to manipulate levels of censorship in LLMs and uncover underlying mechanisms that lead to thought suppression in these models.

**Abstract:** Large language models (LLMs) have transformed the way we access information. These models are often tuned to refuse to comply with requests that are considered harmful and to produce responses that better align with the preferences of those who control the models. To understand how this "censorship" works. We use representation engineering techniques to study open-weights safety-tuned models. We present a method for finding a refusal--compliance vector that detects and controls the level of censorship in model outputs. We also analyze recent reasoning LLMs, distilled from DeepSeek-R1, and uncover an additional dimension of censorship through "thought suppression". We show a similar approach can be used to find a vector that suppresses the model's reasoning process, allowing us to remove censorship by applying the negative multiples of this vector

</details>


### [26] [MIRAGE: A Metric-Intensive Benchmark for Retrieval-Augmented Generation Evaluation](https://arxiv.org/abs/2504.17137)

*Chanhee Park, Hyeonseok Moon, Chanjun Park, Heuiseok Lim*

**Main category:** cs.CL

**TL;DR:** MIRAGE, a new Question Answering dataset, is developed for evaluating Retrieval-Augmented Generation (RAG) systems, addressing challenges in current evaluation methods.

<details>
  <summary>Details</summary>

**Motivation:** The growing importance of RAG systems in enhancing Large Language Models necessitates a robust framework for their evaluation, which is currently lacking due to the complexity of their components.

**Method:** The study introduces MIRAGE, a dataset with 7,560 curated instances linked to a retrieval pool of 37,800 entries, along with new evaluation metrics designed to measure various dimensions of RAG performance.

**Result:** Experiments reveal insights into the effective alignment of model pairs and the complex dynamics involved in RAG systems, validating the usefulness of the MIRAGE dataset in providing detailed evaluations.

**Conclusion:** The MIRAGE dataset and metrics enable improved component-specific assessments of RAG systems, facilitating further research and advancements in this area.

**Abstract:** Retrieval-Augmented Generation (RAG) has gained prominence as an effective method for enhancing the generative capabilities of Large Language Models (LLMs) through the incorporation of external knowledge. However, the evaluation of RAG systems remains a challenge, due to the intricate interplay between retrieval and generation components. This limitation has resulted in a scarcity of benchmarks that facilitate a detailed, component-specific assessment. In this work, we present MIRAGE, a Question Answering dataset specifically designed for RAG evaluation. MIRAGE consists of 7,560 curated instances mapped to a retrieval pool of 37,800 entries, enabling an efficient and precise evaluation of both retrieval and generation tasks. We also introduce novel evaluation metrics aimed at measuring RAG adaptability, encompassing dimensions such as noise vulnerability, context acceptability, context insensitivity, and context misinterpretation. Through comprehensive experiments across various retriever-LLM configurations, we provide new insights into the optimal alignment of model pairs and the nuanced dynamics within RAG systems. The dataset and evaluation code are publicly available, allowing for seamless integration and customization in diverse research settings\footnote{The MIRAGE code and data are available at https://github.com/nlpai-lab/MIRAGE.

</details>


### [27] [Paper2Code: Automating Code Generation from Scientific Papers in Machine Learning](https://arxiv.org/abs/2504.17192)

*Minju Seo, Jinheon Baek, Seongyun Lee, Sung Ju Hwang*

**Main category:** cs.CL

**TL;DR:** PaperCoder is a multi-agent framework that converts machine learning papers into functional code repositories using LLMs, achieving high-quality implementations.

<details>
  <summary>Details</summary>

**Motivation:** The lack of available code implementations for machine learning research hinders reproducibility and progress in the field.

**Method:** PaperCoder operates through three stages: planning (creating a roadmap and system architecture), analysis (interpreting implementation details), and generation (producing modular code), utilizing specialized collaborative agents for each phase.

**Result:** PaperCoder effectively generates high-quality implementations from machine learning papers and excels in the PaperBench benchmark, outperforming strong baselines significantly.

**Conclusion:** The framework demonstrates a promising capability to automate the conversion of research papers into functional code, enhancing reproducibility in machine learning.

**Abstract:** Despite the rapid growth of machine learning research, corresponding code implementations are often unavailable, making it slow and labor-intensive for researchers to reproduce results and build upon prior work. In the meantime, recent Large Language Models (LLMs) excel at understanding scientific documents and generating high-quality code. Inspired by this, we introduce PaperCoder, a multi-agent LLM framework that transforms machine learning papers into functional code repositories. PaperCoder operates in three stages: planning, where it constructs a high-level roadmap, designs the system architecture with diagrams, identifies file dependencies, and generates configuration files; analysis, which focuses on interpreting implementation-specific details; and generation, where modular, dependency-aware code is produced. Moreover, each phase is instantiated through a set of specialized agents designed to collaborate effectively across the pipeline. We then evaluate PaperCoder on generating code implementations from machine learning papers based on both model-based and human evaluations, specifically from the original paper authors, with author-released repositories as ground truth if available. Our results demonstrate the effectiveness of PaperCoder in creating high-quality, faithful implementations. Furthermore, it consistently shows strengths in the recently released PaperBench benchmark, surpassing strong baselines by substantial margins.

</details>


### [28] [A RAG-Based Multi-Agent LLM System for Natural Hazard Resilience and Adaptation](https://arxiv.org/abs/2504.17200)

*Yangxinyu Xie, Bowen Jiang, Tanwi Mallick, Joshua David Bergerson, John K. Hutchison, Duane R. Verner, Jordan Branham, M. Ross Alexander, Robert B. Ross, Yan Feng, Leslie-Anne Levy, Weijie Su, Camillo J. Taylor*

**Main category:** cs.CL

**TL;DR:** The paper presents WildfireGPT, a retrieval-augmented generation multi-agent LLM system designed to enhance decision-making in wildfire hazard analysis.

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to address the limitations of large language models in providing context-specific information for decision-making in the face of natural hazards like wildfires.

**Method:** The methodology involves developing a RAG-based multi-agent LLM architecture that integrates various data sources, including hazard projections, observational datasets, and scientific literature, aimed at providing tailored risk insights.

**Result:** Evaluation through ten expert-led case studies showed that WildfireGPT significantly outperforms existing LLM-based decision support solutions.

**Conclusion:** The study concludes that WildfireGPT provides a more accurate and contextually relevant system for supporting analysis and decision-making related to natural hazards.

**Abstract:** Large language models (LLMs) are a transformational capability at the frontier of artificial intelligence and machine learning that can support decision-makers in addressing pressing societal challenges such as extreme natural hazard events. As generalized models, LLMs often struggle to provide context-specific information, particularly in areas requiring specialized knowledge. In this work we propose a retrieval-augmented generation (RAG)-based multi-agent LLM system to support analysis and decision-making in the context of natural hazards and extreme weather events. As a proof of concept, we present WildfireGPT, a specialized system focused on wildfire hazards. The architecture employs a user-centered, multi-agent design to deliver tailored risk insights across diverse stakeholder groups. By integrating natural hazard and extreme weather projection data, observational datasets, and scientific literature through an RAG framework, the system ensures both the accuracy and contextual relevance of the information it provides. Evaluation across ten expert-led case studies demonstrates that WildfireGPT significantly outperforms existing LLM-based solutions for decision support.

</details>


### [29] [Does Knowledge Distillation Matter for Large Language Model based Bundle Generation?](https://arxiv.org/abs/2504.17220)

*Kaidong Feng, Zhu Sun, Jie Yang, Hui Fang, Xinghua Qu, Wenyuan Liu*

**Main category:** cs.CL

**TL;DR:** This paper investigates the use of knowledge distillation (KD) for efficient large language model (LLM) bundle generation, focusing on performance preservation while minimizing computational costs.

<details>
  <summary>Details</summary>

**Motivation:** The deployment of large-scale LLMs presents significant efficiency challenges, particularly high computational costs during fine-tuning and inference due to their large parameter sizes.

**Method:** The study systematically explores knowledge distillation approaches by addressing three research questions related to the impact of KD format, quantity, and utilization on bundle generation performance. A comprehensive KD framework is proposed, which extracts knowledge progressively, varies distilled knowledge quantities, and incorporates complementary adaptation techniques.

**Result:** Extensive experiments reveal insights into how knowledge format, quantity, and utilization strategies collectively influence LLM-based bundle generation performance, demonstrating the effectiveness of KD.

**Conclusion:** Knowledge distillation shows significant potential for enhancing efficiency in LLM-based bundle generation while maintaining performance.

**Abstract:** LLMs are increasingly explored for bundle generation, thanks to their reasoning capabilities and knowledge. However, deploying large-scale LLMs introduces significant efficiency challenges, primarily high computational costs during fine-tuning and inference due to their massive parameterization. Knowledge distillation (KD) offers a promising solution, transferring expertise from large teacher models to compact student models. This study systematically investigates knowledge distillation approaches for bundle generation, aiming to minimize computational demands while preserving performance. We explore three critical research questions: (1) how does the format of KD impact bundle generation performance? (2) to what extent does the quantity of distilled knowledge influence performance? and (3) how do different ways of utilizing the distilled knowledge affect performance? We propose a comprehensive KD framework that (i) progressively extracts knowledge (patterns, rules, deep thoughts); (ii) captures varying quantities of distilled knowledge through different strategies; and (iii) exploits complementary LLM adaptation techniques (in-context learning, supervised fine-tuning, combination) to leverage distilled knowledge in small student models for domain-specific adaptation and enhanced efficiency. Extensive experiments provide valuable insights into how knowledge format, quantity, and utilization methodologies collectively shape LLM-based bundle generation performance, exhibiting KD's significant potential for more efficient yet effective LLM-based bundle generation.

</details>


### [30] [Crisp: Cognitive Restructuring of Negative Thoughts through Multi-turn Supportive Dialogues](https://arxiv.org/abs/2504.17238)

*Jinfeng Zhou, Yuxuan Chen, Jianing Yin, Yongkang Huang, Yihan Shi, Xikun Zhang, Libiao Peng, Rongsheng Zhang, Tangjie Lv, Zhipeng Hu, Hongning Wang, Minlie Huang*

**Main category:** cs.CL

**TL;DR:** The paper introduces CRDial, a framework for cognitive restructuring (CR) that enhances interactive psychotherapy through multi-turn dialogues, integrating supportive strategies and iterative processes, resulting in superior performance of trained conversational LLMs (Crispers).

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for effective cognitive restructuring in psychotherapy due to clinician shortages and stigma, highlighting shortcomings in existing CR implementations.

**Method:** CRDial framework uses multi-turn dialogues with identification and restructuring stages for negative thoughts, incorporates supportive conversation strategies, and employs a multi-channel loop mechanism for iterative CR. It also creates the Crisp dataset from LLM and trains Crispers conversational LLMs at scales of 7B and 14B.

**Result:** Extensive human studies demonstrate that Crispers outperform existing methods in various evaluations, including pointwise, pairwise, and intervention assessments.

**Conclusion:** CRDial and Crispers provide a novel and more effective approach to cognitive restructuring in psychotherapy, aligning closely with therapeutic practices.

**Abstract:** Cognitive Restructuring (CR) is a psychotherapeutic process aimed at identifying and restructuring an individual's negative thoughts, arising from mental health challenges, into more helpful and positive ones via multi-turn dialogues. Clinician shortage and stigma urge the development of human-LLM interactive psychotherapy for CR. Yet, existing efforts implement CR via simple text rewriting, fixed-pattern dialogues, or a one-shot CR workflow, failing to align with the psychotherapeutic process for effective CR. To address this gap, we propose CRDial, a novel framework for CR, which creates multi-turn dialogues with specifically designed identification and restructuring stages of negative thoughts, integrates sentence-level supportive conversation strategies, and adopts a multi-channel loop mechanism to enable iterative CR. With CRDial, we distill Crisp, a large-scale and high-quality bilingual dialogue dataset, from LLM. We then train Crispers, Crisp-based conversational LLMs for CR, at 7B and 14B scales. Extensive human studies show the superiority of Crispers in pointwise, pairwise, and intervention evaluations.

</details>


### [31] [Low-Resource Neural Machine Translation Using Recurrent Neural Networks and Transfer Learning: A Case Study on English-to-Igbo](https://arxiv.org/abs/2504.17252)

*Ocheme Anthony Ekle, Biswarup Das*

**Main category:** cs.CL

**TL;DR:** The study develops advanced NMT models for English-to-Igbo translation, achieving competitive results with enhanced accuracy through RNNs and transfer learning.

<details>
  <summary>Details</summary>

**Motivation:** To improve translation quality for English-to-Igbo, a low-resource African language, by leveraging modern machine translation techniques.

**Method:** The study employs RNN architectures (LSTM and GRU) with attention mechanisms and applies transfer learning with pre-trained MarianNMT models within the SimpleTransformers framework.

**Result:** The RNN-based system achieves results closely matching existing benchmarks and a notable +4.83 BLEU points improvement with transfer learning, resulting in 70% translation accuracy.

**Conclusion:** Combining RNNs with transfer learning shows promise in bridging performance gaps in low-resource language translation tasks.

**Abstract:** In this study, we develop Neural Machine Translation (NMT) and Transformer-based transfer learning models for English-to-Igbo translation - a low-resource African language spoken by over 40 million people across Nigeria and West Africa. Our models are trained on a curated and benchmarked dataset compiled from Bible corpora, local news, Wikipedia articles, and Common Crawl, all verified by native language experts. We leverage Recurrent Neural Network (RNN) architectures, including Long Short-Term Memory (LSTM) and Gated Recurrent Units (GRU), enhanced with attention mechanisms to improve translation accuracy. To further enhance performance, we apply transfer learning using MarianNMT pre-trained models within the SimpleTransformers framework. Our RNN-based system achieves competitive results, closely matching existing English-Igbo benchmarks. With transfer learning, we observe a performance gain of +4.83 BLEU points, reaching an estimated translation accuracy of 70%. These findings highlight the effectiveness of combining RNNs with transfer learning to address the performance gap in low-resource language translation tasks.

</details>


### [32] [JurisCTC: Enhancing Legal Judgment Prediction via Cross-Domain Transfer and Contrastive Learning](https://arxiv.org/abs/2504.17264)

*Zhaolu Kang, Hongtian Cai, Xiangyang Ji, Jinzhe Li, Nanfei Gu*

**Main category:** cs.CL

**TL;DR:** JurisCTC is a novel model that enhances legal judgment prediction by facilitating unsupervised domain adaptation across civil and criminal law domains, achieving notable accuracy improvements over existing models.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of lengthy legal texts and the scarcity of large-scale annotated datasets in legal domain adaptation, which is largely unexplored in NLP.

**Method:** JurisCTC utilizes contrastive learning to effectively transfer knowledge across different legal domains for the legal judgment prediction task.

**Result:** JurisCTC achieves peak accuracies of 76.59% and 78.83% in legal judgment prediction tasks, outperforming existing models and specific large language models.

**Conclusion:** The proposed JurisCTC model significantly advances the state of unsupervised domain adaptation in legal NLP tasks, enabling better generalization across diverse legal domains.

**Abstract:** In recent years, Unsupervised Domain Adaptation (UDA) has gained significant attention in the field of Natural Language Processing (NLP) owing to its ability to enhance model generalization across diverse domains. However, its application for knowledge transfer between distinct legal domains remains largely unexplored. To address the challenges posed by lengthy and complex legal texts and the limited availability of large-scale annotated datasets, we propose JurisCTC, a novel model designed to improve the accuracy of Legal Judgment Prediction (LJP) tasks. Unlike existing approaches, JurisCTC facilitates effective knowledge transfer across various legal domains and employs contrastive learning to distinguish samples from different domains. Specifically, for the LJP task, we enable knowledge transfer between civil and criminal law domains. Compared to other models and specific large language models (LLMs), JurisCTC demonstrates notable advancements, achieving peak accuracies of 76.59% and 78.83%, respectively.

</details>


### [33] [Evaluating and Mitigating Bias in AI-Based Medical Text Generation](https://arxiv.org/abs/2504.17279)

*Xiuying Chen, Tairan Wang, Juexiao Zhou, Zirui Song, Xin Gao, Xiangliang Zhang*

**Main category:** cs.CL

**TL;DR:** This study addresses fairness issues in AI-driven text generation in the medical field, proposing an algorithm that improves performance across diverse demographic groups while minimizing bias.

<details>
  <summary>Details</summary>

**Motivation:** To address concerns that AI systems in medical applications may reflect and amplify human bias, negatively impacting historically underserved populations, and to investigate fairness in text generation, an area that is less studied compared to medical imaging.

**Method:** The paper proposes an algorithm that selectively optimizes underperformed demographic groups by considering both word-level accuracy and pathology accuracy in a fully differentiable manner for effective model training.

**Result:** The proposed algorithm reduced performance disparities among various groups by more than 30% and maintained a relative change in text generation accuracy of typically within 2%.

**Conclusion:** The approach significantly alleviates bias in AI text generation within the medical domain, enhancing fairness and reliability in diagnosis.

**Abstract:** Artificial intelligence (AI) systems, particularly those based on deep learning models, have increasingly achieved expert-level performance in medical applications. However, there is growing concern that such AI systems may reflect and amplify human bias, and reduce the quality of their performance in historically under-served populations. The fairness issue has attracted considerable research interest in the medical imaging classification field, yet it remains understudied in the text generation domain. In this study, we investigate the fairness problem in text generation within the medical field and observe significant performance discrepancies across different races, sexes, and age groups, including intersectional groups, various model scales, and different evaluation metrics. To mitigate this fairness issue, we propose an algorithm that selectively optimizes those underperformed groups to reduce bias. The selection rules take into account not only word-level accuracy but also the pathology accuracy to the target reference, while ensuring that the entire process remains fully differentiable for effective model training. Our evaluations across multiple backbones, datasets, and modalities demonstrate that our proposed algorithm enhances fairness in text generation without compromising overall performance. Specifically, the disparities among various groups across different metrics were diminished by more than 30% with our algorithm, while the relative change in text generation accuracy was typically within 2%. By reducing the bias generated by deep learning models, our proposed approach can potentially alleviate concerns about the fairness and reliability of text generation diagnosis in medical domain.   Our code is publicly available to facilitate further research at https://github.com/iriscxy/GenFair.

</details>


### [34] [CoheMark: A Novel Sentence-Level Watermark for Enhanced Text Quality](https://arxiv.org/abs/2504.17309)

*Junyan Zhang, Shuliang Liu, Aiwei Liu, Yubo Gao, Jungang Li, Xiaojie Gu, Xuming Hu*

**Main category:** cs.CL

**TL;DR:** CoheMark is a novel sentence-level watermarking technique that enhances watermark robustness while preserving text quality by leveraging sentence cohesion.

<details>
  <summary>Details</summary>

**Motivation:** Existing sentence-level watermarking techniques compromise text quality due to reliance on arbitrary segmentation or generation processes.

**Method:** CoheMark utilizes fuzzy c-means clustering for sentence selection and applies specific criteria for next sentence selection to improve logical fluency.

**Result:** Experimental evaluations show that CoheMark achieves strong watermark strength with minimal impact on the quality of the generated responses.

**Conclusion:** CoheMark effectively balances the requirements of robust watermarking and high text quality.

**Abstract:** Watermarking technology is a method used to trace the usage of content generated by large language models. Sentence-level watermarking aids in preserving the semantic integrity within individual sentences while maintaining greater robustness. However, many existing sentence-level watermarking techniques depend on arbitrary segmentation or generation processes to embed watermarks, which can limit the availability of appropriate sentences. This limitation, in turn, compromises the quality of the generated response. To address the challenge of balancing high text quality with robust watermark detection, we propose CoheMark, an advanced sentence-level watermarking technique that exploits the cohesive relationships between sentences for better logical fluency. The core methodology of CoheMark involves selecting sentences through trained fuzzy c-means clustering and applying specific next sentence selection criteria. Experimental evaluations demonstrate that CoheMark achieves strong watermark strength while exerting minimal impact on text quality.

</details>


### [35] [FLUKE: A Linguistically-Driven and Task-Agnostic Framework for Robustness Evaluation](https://arxiv.org/abs/2504.17311)

*Yulia Otmakhova, Hung Thinh Truong, Rahmad Mahendra, Zenan Zhai, Rongxin Zhu, Daniel Beck, Jey Han Lau*

**Main category:** cs.CL

**TL;DR:** FLUKE is a framework for evaluating model robustness through controlled linguistic variations, showing task-dependent effects and vulnerabilities in NLP models.

<details>
  <summary>Details</summary>

**Motivation:** To assess model robustness through systematic minimal variations of test data, leveraging linguistic diversity.

**Method:** FLUKE introduces controlled variations at various linguistic levels and uses large language models with human validation to generate modifications, evaluating multiple NLP tasks.

**Result:** The study finds that linguistic variation impacts are task-dependent, LLMs are generally more robust but still vulnerable, and all models are notably sensitive to negation changes.

**Conclusion:** The findings underscore the need for systematic robustness testing to better understand NLP model behaviors.

**Abstract:** We present FLUKE (Framework for LingUistically-driven and tasK-agnostic robustness Evaluation), a task-agnostic framework for assessing model robustness through systematic minimal variations of test data. FLUKE introduces controlled variations across linguistic levels - from orthography to dialect and style varieties - and leverages large language models (LLMs) with human validation to generate modifications. We demonstrate FLUKE's utility by evaluating both fine-tuned models and LLMs across four diverse NLP tasks, and reveal that (1) the impact of linguistic variations is highly task-dependent, with some tests being critical for certain tasks but irrelevant for others; (2) while LLMs have better overall robustness compared to fine-tuned models, they still exhibit significant brittleness to certain linguistic variations; (3) all models show substantial vulnerability to negation modifications across most tasks. These findings highlight the importance of systematic robustness testing for understanding model behaviors.

</details>


### [36] [Bridging Cognition and Emotion: Empathy-Driven Multimodal Misinformation Detection](https://arxiv.org/abs/2504.17332)

*Zihan Wang, Lu Yuan, Zhengxuan Zhang, Qing Zhao*

**Main category:** cs.CL

**TL;DR:** The Dual-Aspect Empathy Framework (DAE) improves misinformation detection by incorporating human empathy dimensions, outperforming traditional methods.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitation of traditional misinformation detection methods which neglect the impact of human empathy in the spread of misinformation.

**Method:** The DAE integrates cognitive and emotional empathy to analyze misinformation from both creators' and readers' perspectives, using Large Language Models to simulate responses.

**Result:** Experiments show that DAE outperforms traditional misinformation detection methods on benchmark datasets.

**Conclusion:** DAE presents a novel, comprehensive approach to misinformation detection that emphasizes the human-centric factors of empathy.

**Abstract:** In the digital era, social media has become a major conduit for information dissemination, yet it also facilitates the rapid spread of misinformation. Traditional misinformation detection methods primarily focus on surface-level features, overlooking the crucial roles of human empathy in the propagation process. To address this gap, we propose the Dual-Aspect Empathy Framework (DAE), which integrates cognitive and emotional empathy to analyze misinformation from both the creator and reader perspectives. By examining creators' cognitive strategies and emotional appeals, as well as simulating readers' cognitive judgments and emotional responses using Large Language Models (LLMs), DAE offers a more comprehensive and human-centric approach to misinformation detection. Moreover, we further introduce an empathy-aware filtering mechanism to enhance response authenticity and diversity. Experimental results on benchmark datasets demonstrate that DAE outperforms existing methods, providing a novel paradigm for multimodal misinformation detection.

</details>


### [37] [M-MRE: Extending the Mutual Reinforcement Effect to Multimodal Information Extraction](https://arxiv.org/abs/2504.17353)

*Chengguang Gan, Sunbowen Lee, Zhixi Cai, Yanbin Wei, Lei Zheng, Yunhao Liang, Shiwen Ni, Tatsunori Mori*

**Main category:** cs.CL

**TL;DR:** The paper extends the Mutual Reinforcement Effect (MRE) to the multimodal information extraction domain, introducing the Multimodal Mutual Reinforcement Effect (M-MRE) and a new task along with a supporting dataset.

<details>
  <summary>Details</summary>

**Motivation:** To explore the applicability of the Mutual Reinforcement Effect (MRE) in visual and multimodal domains, as it has only been previously validated in textual contexts.

**Method:** Introduction of the Multimodal Mutual Reinforcement Effect (M-MRE) task and construction of a corresponding dataset, along with the development of a Prompt Format Adapter (PFA) compatible with various Large Vision-Language Models (LVLMs).

**Result:** Experimental results indicate that MRE can be observed in the M-MRE task, highlighting mutual gains across interrelated tasks in a multimodal text-image understanding scenario.

**Conclusion:** The findings confirm the generalizability of MRE beyond the textual domain, establishing its relevance in multimodal contexts.

**Abstract:** Mutual Reinforcement Effect (MRE) is an emerging subfield at the intersection of information extraction and model interpretability. MRE aims to leverage the mutual understanding between tasks of different granularities, enhancing the performance of both coarse-grained and fine-grained tasks through joint modeling. While MRE has been explored and validated in the textual domain, its applicability to visual and multimodal domains remains unexplored. In this work, we extend MRE to the multimodal information extraction domain for the first time. Specifically, we introduce a new task: Multimodal Mutual Reinforcement Effect (M-MRE), and construct a corresponding dataset to support this task. To address the challenges posed by M-MRE, we further propose a Prompt Format Adapter (PFA) that is fully compatible with various Large Vision-Language Models (LVLMs). Experimental results demonstrate that MRE can also be observed in the M-MRE task, a multimodal text-image understanding scenario. This provides strong evidence that MRE facilitates mutual gains across three interrelated tasks, confirming its generalizability beyond the textual domain.

</details>


### [38] [PatientDx: Merging Large Language Models for Protecting Data-Privacy in Healthcare](https://arxiv.org/abs/2504.17360)

*Jose G. Moreno, Jesus Lovon, M'Rick Robin-Charlet, Christine Damase-Michel, Lynda Tamine*

**Main category:** cs.CL

**TL;DR:** PatientDx is a framework that enhances large language models for healthcare applications without fine-tuning on sensitive patient data, showing improved performance and reduced privacy risks.

<details>
  <summary>Details</summary>

**Motivation:** The need to improve large language model performance in healthcare while addressing significant data privacy concerns associated with training on sensitive patient data.

**Method:** PatientDx merges models without fine-tuning on patient data, utilizing a pivotal model focused on numerical reasoning and tuning hyperparameters based on performance metrics.

**Result:** Experiments demonstrate up to 7% improvement in AUROC for mortality tasks compared to initial models, with reduced risk of data leakage when compared to fine-tuned models.

**Conclusion:** PatientDx effectively enhances model performance for health predictions while maintaining data privacy, with a publicly available implementation.

**Abstract:** Fine-tuning of Large Language Models (LLMs) has become the default practice for improving model performance on a given task. However, performance improvement comes at the cost of training on vast amounts of annotated data which could be sensitive leading to significant data privacy concerns. In particular, the healthcare domain is one of the most sensitive domains exposed to data privacy issues. In this paper, we present PatientDx, a framework of model merging that allows the design of effective LLMs for health-predictive tasks without requiring fine-tuning nor adaptation on patient data. Our proposal is based on recently proposed techniques known as merging of LLMs and aims to optimize a building block merging strategy. PatientDx uses a pivotal model adapted to numerical reasoning and tunes hyperparameters on examples based on a performance metric but without training of the LLM on these data. Experiments using the mortality tasks of the MIMIC-IV dataset show improvements up to 7% in terms of AUROC when compared to initial models. Additionally, we confirm that when compared to fine-tuned models, our proposal is less prone to data leak problems without hurting performance. Finally, we qualitatively show the capabilities of our proposal through a case study. Our best model is publicly available at https://huggingface.co/ Jgmorenof/mistral\_merged\_0\_4.

</details>


### [39] [LiveLongBench: Tackling Long-Context Understanding for Spoken Texts from Live Streams](https://arxiv.org/abs/2504.17366)

*Yongxuan Wu, Runyu Chen, Peiyu Liu, Hongjin Qian*

**Main category:** cs.CL

**TL;DR:** The paper introduces a new spoken long-text dataset specifically designed for natural language processing (NLP) tasks related to real-world conversations, highlighting challenges in long-context understanding.

<details>
  <summary>Details</summary>

**Motivation:** Existing benchmarks for large language models (LLMs) do not adequately capture the complexities of real-world dialogues, which feature redundancy and uneven information density, thereby limiting the practical applicability of these models.

**Method:** A new dataset derived from live streams was constructed to reflect redundancy-rich, conversational aspects, along with tasks categorized into retrieval-dependent, reasoning-dependent, and hybrid. Evaluations were conducted on various LLMs and specialized methods to assess their long-context handling capabilities.

**Result:** Evaluation revealed that current methods perform poorly on redundant inputs and show task-specific preferences, with no single method consistently excelling across tasks. A new baseline method was proposed that handles redundancy more effectively and demonstrates stronger overall performance.

**Conclusion:** The findings reveal limitations in existing methods for long-context understanding, propose future research directions, and establish a new benchmark that serves as a foundation for real-world applications, particularly in e-commerce systems.

**Abstract:** Long-context understanding poses significant challenges in natural language processing, particularly for real-world dialogues characterized by speech-based elements, high redundancy, and uneven information density. Although large language models (LLMs) achieve impressive results on existing benchmarks, these datasets fail to reflect the complexities of such texts, limiting their applicability to practical scenarios. To bridge this gap, we construct the first spoken long-text dataset, derived from live streams, designed to reflect the redundancy-rich and conversational nature of real-world scenarios. We construct tasks in three categories: retrieval-dependent, reasoning-dependent, and hybrid. We then evaluate both popular LLMs and specialized methods to assess their ability to understand long-contexts in these tasks. Our results show that current methods exhibit strong task-specific preferences and perform poorly on highly redundant inputs, with no single method consistently outperforming others. We propose a new baseline that better handles redundancy in spoken text and achieves strong performance across tasks. Our findings highlight key limitations of current methods and suggest future directions for improving long-context understanding. Finally, our benchmark fills a gap in evaluating long-context spoken language understanding and provides a practical foundation for developing real-world e-commerce systems. The code and benchmark are available at https://github.com/Yarayx/livelongbench.

</details>


### [40] [PicPersona-TOD : A Dataset for Personalizing Utterance Style in Task-Oriented Dialogue with Image Persona](https://arxiv.org/abs/2504.17390)

*Jihyun Lee, Yejin Jeon, Seungyeon Seo, Gary Geunbae Lee*

**Main category:** cs.CL

**TL;DR:** PicPersona-TOD introduces a dataset that enhances Task-Oriented Dialogue systems by incorporating user images for personalized interactions, and presents the NLG model Pictor to improve response quality.

<details>
  <summary>Details</summary>

**Motivation:** Existing Task-Oriented Dialogue systems often produce generic responses that do not account for individual user attributes or contexts, limiting engagement.

**Method:** The paper presents the PicPersona-TOD dataset which uses user images to inform personalized dialogue responses, applies dialogue policy-guided prompting, and leverages external knowledge to minimize hallucinations. A new NLG model, Pictor, is proposed to utilize this dataset effectively.

**Result:** Human evaluations show that the integration of personalized responses through PicPersona-TOD significantly improves user experience and engagement compared to traditional systems. Pictor also performs well across previously unseen domains.

**Conclusion:** The work demonstrates the potential of using user images to create more personalized and effective dialogue systems, with the Pictor model showing promising results in enhancing interaction quality.

**Abstract:** Task-Oriented Dialogue (TOD) systems are designed to fulfill user requests through natural language interactions, yet existing systems often produce generic, monotonic responses that lack individuality and fail to adapt to users' personal attributes. To address this, we introduce PicPersona-TOD, a novel dataset that incorporates user images as part of the persona, enabling personalized responses tailored to user-specific factors such as age or emotional context. This is facilitated by first impressions, dialogue policy-guided prompting, and the use of external knowledge to reduce hallucinations. Human evaluations confirm that our dataset enhances user experience, with personalized responses contributing to a more engaging interaction. Additionally, we introduce a new NLG model, Pictor, which not only personalizes responses, but also demonstrates robust performance across unseen domains https://github.com/JihyunLee1/PicPersona.

</details>


### [41] [Creating Targeted, Interpretable Topic Models with LLM-Generated Text Augmentation](https://arxiv.org/abs/2504.17445)

*Anna Lieb, Maneesh Arora, Eni Mustafaraj*

**Main category:** cs.CL

**TL;DR:** Using GPT-4 for text augmentation improves the interpretability and practicality of topic modeling in social science research.

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional topic modeling in terms of interpretability and its ability to answer specific social science questions.

**Method:** Implementing LLM-generated text augmentation to enhance topic modeling outputs in a political science case study.

**Result:** The study found that GPT-4 augmented topic modeling yields highly interpretable categories suitable for investigating specific research questions with minimal human input.

**Conclusion:** LLM-generated text augmentation significantly enhances topic modeling's applicability in social science research.

**Abstract:** Unsupervised machine learning techniques, such as topic modeling and clustering, are often used to identify latent patterns in unstructured text data in fields such as political science and sociology. These methods overcome common concerns about reproducibility and costliness involved in the labor-intensive process of human qualitative analysis. However, two major limitations of topic models are their interpretability and their practicality for answering targeted, domain-specific social science research questions. In this work, we investigate opportunities for using LLM-generated text augmentation to improve the usefulness of topic modeling output. We use a political science case study to evaluate our results in a domain-specific application, and find that topic modeling using GPT-4 augmentations creates highly interpretable categories that can be used to investigate domain-specific research questions with minimal human guidance.

</details>


### [42] [Unified Attacks to Large Language Model Watermarks: Spoofing and Scrubbing in Unauthorized Knowledge Distillation](https://arxiv.org/abs/2504.17480)

*Xin Yi, Shunfan Zhengc, Linlin Wanga, Xiaoling Wang, Liang He*

**Main category:** cs.CL

**TL;DR:** The paper introduces CDG-KD, a framework for attacking watermarking in knowledge distillation, highlighting vulnerabilities in existing methods and reinforcing the need for robust watermarking schemes.

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities in watermarking through knowledge distillation and evaluate the robustness of watermarks against scrubbing and spoofing attacks.

**Method:** The authors propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), which uses contrastive decoding to identify and amplify watermarks, enabling bidirectional attacks for watermark removal and forgery.

**Result:** CDG-KD demonstrates effective execution of watermark attacks while maintaining the general performance of the distilled models during extensive experiments.

**Conclusion:** The findings emphasize the urgent need for developing watermarking schemes that are both robust against attacks and unforgeable in the context of unauthorized knowledge distillation.

**Abstract:** Watermarking has emerged as a critical technique for combating misinformation and protecting intellectual property in large language models (LLMs). A recent discovery, termed watermark radioactivity, reveals that watermarks embedded in teacher models can be inherited by student models through knowledge distillation. On the positive side, this inheritance allows for the detection of unauthorized knowledge distillation by identifying watermark traces in student models. However, the robustness of watermarks against scrubbing attacks and their unforgeability in the face of spoofing attacks under unauthorized knowledge distillation remain largely unexplored. Existing watermark attack methods either assume access to model internals or fail to simultaneously support both scrubbing and spoofing attacks. In this work, we propose Contrastive Decoding-Guided Knowledge Distillation (CDG-KD), a unified framework that enables bidirectional attacks under unauthorized knowledge distillation. Our approach employs contrastive decoding to extract corrupted or amplified watermark texts via comparing outputs from the student model and weakly watermarked references, followed by bidirectional distillation to train new student models capable of watermark removal and watermark forgery, respectively. Extensive experiments show that CDG-KD effectively performs attacks while preserving the general performance of the distilled model. Our findings underscore critical need for developing watermarking schemes that are robust and unforgeable.

</details>


### [43] [HalluLens: LLM Hallucination Benchmark](https://arxiv.org/abs/2504.17550)

*Yejin Bang, Ziwei Ji, Alan Schelten, Anthony Hartshorn, Tara Fowler, Cheng Zhang, Nicola Cancedda, Pascale Fung*

**Main category:** cs.CL

**TL;DR:** This paper proposes a comprehensive benchmark for evaluating hallucinations in large language models, addressing challenges in existing protocols and establishing a clear taxonomy.

<details>
  <summary>Details</summary>

**Motivation:** Addressing hallucinations in large language models (LLMs) is essential for improving user trust and the adoption of generative AI systems, as hallucinations undermine reliability.

**Method:** The paper introduces a benchmark that includes both new extrinsic evaluation tasks and existing intrinsic tasks based on a new taxonomy. It disentangles hallucination from factuality and incorporates dynamic test set generation to prevent data leakage.

**Result:** The benchmark provides a structured approach to analyze hallucinations, identifies challenges in current benchmarks, and emphasizes the importance of extrinsic hallucinations as LLMs develop.

**Conclusion:** The proposed framework aims to enhance research consistency, promote a better understanding of hallucinations in LLMs, and provide a robust evaluation mechanism.

**Abstract:** Large language models (LLMs) often generate responses that deviate from user input or training data, a phenomenon known as "hallucination." These hallucinations undermine user trust and hinder the adoption of generative AI systems. Addressing hallucinations is essential for the advancement of LLMs. This paper introduces a comprehensive hallucination benchmark, incorporating both new extrinsic and existing intrinsic evaluation tasks, built upon clear taxonomy of hallucination. A major challenge in benchmarking hallucinations is the lack of a unified framework due to inconsistent definitions and categorizations. We disentangle LLM hallucination from "factuality," proposing a clear taxonomy that distinguishes between extrinsic and intrinsic hallucinations, to promote consistency and facilitate research. Extrinsic hallucinations, where the generated content is not consistent with the training data, are increasingly important as LLMs evolve. Our benchmark includes dynamic test set generation to mitigate data leakage and ensure robustness against such leakage. We also analyze existing benchmarks, highlighting their limitations and saturation. The work aims to: (1) establish a clear taxonomy of hallucinations, (2) introduce new extrinsic hallucination tasks, with data that can be dynamically regenerated to prevent saturation by leakage, (3) provide a comprehensive analysis of existing benchmarks, distinguishing them from factuality evaluations.

</details>


### [44] [When Does Metadata Conditioning (NOT) Work for Language Model Pre-Training? A Study with Context-Free Grammars](https://arxiv.org/abs/2504.17562)

*Rei Higuchi, Ryotaro Kawata, Naoki Nishikawa, Kazusato Oko, Shoichiro Yamaguchi, Sosuke Kobayashi, Seiya Tokui, Kohei Hayashi, Daisuke Okanohara, Taiji Suzuki*

**Main category:** cs.CL

**TL;DR:** Prepending metadata during pre-training affects language model performance variably, improving it under certain conditions but hindering it when context is inadequate.

<details>
  <summary>Details</summary>

**Motivation:** To understand how prepending metadata in pre-training affects language model performance, especially in light of mixed findings from previous studies regarding downstream task improvements.

**Method:** The authors examine the effects of metadata using artificial data generated by probabilistic context-free grammars, analyzing model behavior in relation to context length and the ability to infer latent semantics.

**Result:** Prepending metadata helps improve model performance when the context is sufficiently long to infer latent semantics, but can negatively impact performance when context is insufficient for accurate inference.

**Conclusion:** The effectiveness of the metadata approach in pre-training depends on the adequacy of the context provided in downstream tasks for inferring latent semantics.

**Abstract:** The ability to acquire latent semantics is one of the key properties that determines the performance of language models. One convenient approach to invoke this ability is to prepend metadata (e.g. URLs, domains, and styles) at the beginning of texts in the pre-training data, making it easier for the model to access latent semantics before observing the entire text. Previous studies have reported that this technique actually improves the performance of trained models in downstream tasks; however, this improvement has been observed only in specific downstream tasks, without consistent enhancement in average next-token prediction loss. To understand this phenomenon, we closely investigate how prepending metadata during pre-training affects model performance by examining its behavior using artificial data. Interestingly, we found that this approach produces both positive and negative effects on the downstream tasks. We demonstrate that the effectiveness of the approach depends on whether latent semantics can be inferred from the downstream task's prompt. Specifically, through investigations using data generated by probabilistic context-free grammars, we show that training with metadata helps improve model's performance when the given context is long enough to infer the latent semantics. In contrast, the technique negatively impacts performance when the context lacks the necessary information to make an accurate posterior inference.

</details>


### [45] [DeepDistill: Enhancing LLM Reasoning Capabilities via Large-Scale Difficulty-Graded Data Training](https://arxiv.org/abs/2504.17565)

*Xiaoyu Tian, Sitong Zhao, Haotian Wang, Shuaiting Chen, Yiping Peng, Yunjie Ji, Han Zhao, Xiangang Li*

**Main category:** cs.CL

**TL;DR:** The paper presents a large-scale, difficulty-graded reasoning dataset and methodology to enhance the reasoning capabilities of large language models (LLMs), achieving a 79.2% pass rate on a mathematics benchmark.

<details>
  <summary>Details</summary>

**Motivation:** To better understand the training processes and data quality of base models in large language models, as current work lacks in-depth insights despite recent performance achievements.

**Method:** Constructed a dataset with 3.34 million queries and 40 million responses, selected valuable training data based on pass rates and Coefficient of Variation (CV), and applied higher learning rates during training.

**Result:** Achieved a pass rate of 79.2% on the AIME2024 mathematical reasoning benchmark, surpassing most current distilled models and nearing state-of-the-art performance.

**Conclusion:** The carefully selected dataset and training methodology significantly improve LLM reasoning capabilities, with resources provided for open-source community advancement.

**Abstract:** Although large language models (LLMs) have recently achieved remarkable performance on various complex reasoning benchmarks, the academic community still lacks an in-depth understanding of base model training processes and data quality. To address this, we construct a large-scale, difficulty-graded reasoning dataset containing approximately 3.34 million unique queries of varying difficulty levels and about 40 million distilled responses generated by multiple models over several passes. Leveraging pass rate and Coefficient of Variation (CV), we precisely select the most valuable training data to enhance reasoning capability. Notably, we observe a training pattern shift, indicating that reasoning-focused training based on base models requires higher learning rates for effective training. Using this carefully selected data, we significantly improve the reasoning capabilities of the base model, achieving a pass rate of 79.2\% on the AIME2024 mathematical reasoning benchmark. This result surpasses most current distilled models and closely approaches state-of-the-art performance. We provide detailed descriptions of our data processing, difficulty assessment, and training methodology, and have publicly released all datasets and methods to promote rapid progress in open-source long-reasoning LLMs. The dataset is available at: https://huggingface.co/datasets/a-m-team/AM-DeepSeek-Distilled-40M

</details>


### [46] [RAGAT-Mind: A Multi-Granular Modeling Approach for Rumor Detection Based on MindSpore](https://arxiv.org/abs/2504.17574)

*Zhenkai Qin, Guifang Yang, Dongze Wu*

**Main category:** cs.CL

**TL;DR:** RAGAT-Mind is a novel model for detecting rumors in Chinese social media, achieving high accuracy and F1 score through a multi-granular and graph-based approach.

<details>
  <summary>Details</summary>

**Motivation:** The increasing spread of false information on social media platforms necessitates effective rumor detection methods in natural language processing.

**Method:** RAGAT-Mind combines TextCNN, bidirectional GRU, Multi-Head Self-Attention, and BiGCN within the MindSpore framework to capture local, sequential, and global semantic patterns, as well as word co-occurrence structures.

**Result:** RAGAT-Mind achieved 99.2% accuracy and a macro-F1 score of 0.9919 on the Weibo1-Rumor dataset, outperforming existing methods in classification performance.

**Conclusion:** The model's integration of hierarchical linguistic features with graph-based structures demonstrates significant improvements in rumor detection, showcasing its applicability in real-world scenarios.

**Abstract:** As false information continues to proliferate across social media platforms, effective rumor detection has emerged as a pressing challenge in natural language processing. This paper proposes RAGAT-Mind, a multi-granular modeling approach for Chinese rumor detection, built upon the MindSpore deep learning framework. The model integrates TextCNN for local semantic extraction, bidirectional GRU for sequential context learning, Multi-Head Self-Attention for global dependency focusing, and Bidirectional Graph Convolutional Networks (BiGCN) for structural representation of word co-occurrence graphs. Experiments on the Weibo1-Rumor dataset demonstrate that RAGAT-Mind achieves superior classification performance, attaining 99.2% accuracy and a macro-F1 score of 0.9919. The results validate the effectiveness of combining hierarchical linguistic features with graph-based semantic structures. Furthermore, the model exhibits strong generalization and interpretability, highlighting its practical value for real-world rumor detection applications.

</details>


### [47] [Towards a comprehensive taxonomy of online abusive language informed by machine leaning](https://arxiv.org/abs/2504.17653)

*Samaneh Hosseini Moghaddam, Kelly Lyons, Cheryl Regehr, Vivek Goel, Kaitlyn Regehr*

**Main category:** cs.CL

**TL;DR:** The paper proposes a comprehensive taxonomy for categorizing abusive language in online communications to aid in detection and intervention efforts.

<details>
  <summary>Details</summary>

**Motivation:** There is a pressing need to address the risks posed by abusive language in online communications to protect individuals and communities.

**Method:** The paper develops a hierarchical and faceted taxonomy by integrating classification systems of 18 existing multi-label datasets to capture characteristics relevant to online abusive language.

**Result:** The resulting taxonomy consists of 5 categories and 17 dimensions that classify aspects of online abuse such as context, target, intensity, directness, and theme.

**Conclusion:** The taxonomy provides a shared understanding that enhances collaboration and accelerates advancements in online abuse detection and mitigation.

**Abstract:** The proliferation of abusive language in online communications has posed significant risks to the health and wellbeing of individuals and communities. The growing concern regarding online abuse and its consequences necessitates methods for identifying and mitigating harmful content and facilitating continuous monitoring, moderation, and early intervention. This paper presents a taxonomy for distinguishing key characteristics of abusive language within online text. Our approach uses a systematic method for taxonomy development, integrating classification systems of 18 existing multi-label datasets to capture key characteristics relevant to online abusive language classification. The resulting taxonomy is hierarchical and faceted, comprising 5 categories and 17 dimensions. It classifies various facets of online abuse, including context, target, intensity, directness, and theme of abuse. This shared understanding can lead to more cohesive efforts, facilitate knowledge exchange, and accelerate progress in the field of online abuse detection and mitigation among researchers, policy makers, online platform owners, and other stakeholders.

</details>


### [48] [Evaluating Grounded Reasoning by Code-Assisted Large Language Models for Mathematics](https://arxiv.org/abs/2504.17665)

*Zena Al-Khalili, Nick Howell, Dietrich Klakow*

**Main category:** cs.CL

**TL;DR:** This study analyzes the performance of code-assisted LLMs on mathematical reasoning tasks, focusing on the grounding of generated programs to math rules and its impact on overall performance.

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of rigorous evaluation of code-assisted LLMs, which is typically confined to execution correctness without examining the quality of generated programs.

**Method:** The study evaluates the generated programs of five different LLMs on two math datasets, analyzing both manually and automatically how well these programs are grounded in mathematical rules.

**Result:** Findings indicate that grounding varies based on LLM capabilities and problem difficulty, with closed-source models showing more effective grounding. On the MATH500 dataset, grounded programs decreased to half compared to ASDiv grade-school problems, with ungrounded generations doubling.

**Conclusion:** The results underscore the necessity for comprehensive evaluation metrics beyond mere execution accuracy to better understand the capabilities and limitations of code-assisted LLMs in solving mathematical tasks.

**Abstract:** Assisting LLMs with code generation improved their performance on mathematical reasoning tasks. However, the evaluation of code-assisted LLMs is generally restricted to execution correctness, lacking a rigorous evaluation of their generated programs. In this work, we bridge this gap by conducting an in-depth analysis of code-assisted LLMs' generated programs in response to math reasoning tasks. Our evaluation focuses on the extent to which LLMs ground their programs to math rules, and how that affects their end performance. For this purpose, we assess the generations of five different LLMs, on two different math datasets, both manually and automatically. Our results reveal that the distribution of grounding depends on LLMs' capabilities and the difficulty of math problems. Furthermore, mathematical grounding is more effective for closed-source models, while open-source models fail to employ math rules in their solutions correctly. On MATH500, the percentage of grounded programs decreased to half, while the ungrounded generations doubled in comparison to ASDiv grade-school problems. Our work highlights the need for in-depth evaluation beyond execution accuracy metrics, toward a better understanding of code-assisted LLMs' capabilities and limits in the math domain.

</details>


### [49] [Data-Driven Calibration of Prediction Sets in Large Vision-Language Models Based on Inductive Conformal Prediction](https://arxiv.org/abs/2504.17671)

*Yuanchang Ye, Weiyan Wen*

**Main category:** cs.CL

**TL;DR:** This study presents a Split Conformal Prediction (SCP) framework to mitigate hallucinations in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks, enhancing reliability in safety-critical applications.

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of hallucinated content in LVLMs, which can pose risks in safety-critical applications due to high-confidence, incorrect outputs.

**Method:** A model-agnostic uncertainty quantification method is proposed that involves dynamic threshold calibration and cross-modal consistency verification, utilizing nonconformity scores derived from separate calibration and test sets to ensure predictions meet user-defined risk levels.

**Result:** Evaluations on benchmarks with eight LVLMs show that SCP maintains theoretical guarantees across all risk levels ($\alpha$), demonstrating robust performance irrespective of calibration-to-test split ratios.

**Conclusion:** SCP offers a scalable solution that bridges theoretical reliability and practical applicability in multi-modal AI systems, making it suitable for real-world deployments in critical domains.

**Abstract:** This study addresses the critical challenge of hallucination mitigation in Large Vision-Language Models (LVLMs) for Visual Question Answering (VQA) tasks through a Split Conformal Prediction (SCP) framework. While LVLMs excel in multi-modal reasoning, their outputs often exhibit hallucinated content with high confidence, posing risks in safety-critical applications. We propose a model-agnostic uncertainty quantification method that integrates dynamic threshold calibration and cross-modal consistency verification. By partitioning data into calibration and test sets, the framework computes nonconformity scores to construct prediction sets with statistical guarantees under user-defined risk levels ($\alpha$). Key innovations include: (1) rigorous control of \textbf{marginal coverage} to ensure empirical error rates remain strictly below $\alpha$; (2) dynamic adjustment of prediction set sizes inversely with $\alpha$, filtering low-confidence outputs; (3) elimination of prior distribution assumptions and retraining requirements. Evaluations on benchmarks (ScienceQA, MMMU) with eight LVLMs demonstrate that SCP enforces theoretical guarantees across all $\alpha$ values. The framework achieves stable performance across varying calibration-to-test split ratios, underscoring its robustness for real-world deployment in healthcare, autonomous systems, and other safety-sensitive domains. This work bridges the gap between theoretical reliability and practical applicability in multi-modal AI systems, offering a scalable solution for hallucination detection and uncertainty-aware decision-making.

</details>


### [50] [Energy Considerations of Large Language Model Inference and Efficiency Optimizations](https://arxiv.org/abs/2504.17674)

*Jared Fernandez, Clara Na, Vashisth Tiwari, Yonatan Bisk, Sasha Luccioni, Emma Strubell*

**Main category:** cs.CL

**TL;DR:** This paper analyzes the energy implications of inference efficiency optimizations for large language models (LLMs) across diverse NLP and generative AI workloads, revealing significant potential for energy reduction.

<details>
  <summary>Details</summary>

**Motivation:** As the size and adoption of LLMs increase, so do their computational and environmental costs. Previous studies have focused on latency without addressing real-world inference workloads that affect energy use.

**Method:** The authors employ a systematic analysis and a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and variations in batch size. The analysis covers various factors including software frameworks, GPU architectures, and model parallelism configurations.

**Result:** The study finds that the effectiveness of inference optimizations is significantly influenced by workload characteristics and system configurations, and that naive energy estimates can greatly underestimate actual consumption. Proper optimizations can reduce energy use by up to 73% compared to unoptimized baselines.

**Conclusion:** The insights gained from this work provide a foundation for developing sustainable deployment strategies for LLMs and inform energy-efficient design practices for future AI infrastructure.

**Abstract:** As large language models (LLMs) scale in size and adoption, their computational and environmental costs continue to rise. Prior benchmarking efforts have primarily focused on latency reduction in idealized settings, often overlooking the diverse real-world inference workloads that shape energy use. In this work, we systematically analyze the energy implications of common inference efficiency optimizations across diverse Natural Language Processing (NLP) and generative Artificial Intelligence (AI) workloads, including conversational AI and code generation. We introduce a modeling approach that approximates real-world LLM workflows through a binning strategy for input-output token distributions and batch size variations. Our empirical analysis spans software frameworks, decoding strategies, GPU architectures, online and offline serving settings, and model parallelism configurations. We show that the effectiveness of inference optimizations is highly sensitive to workload geometry, software stack, and hardware accelerators, demonstrating that naive energy estimates based on FLOPs or theoretical GPU utilization significantly underestimate real-world energy consumption. Our findings reveal that the proper application of relevant inference efficiency optimizations can reduce total energy use by up to 73% from unoptimized baselines. These insights provide a foundation for sustainable LLM deployment and inform energy-efficient design strategies for future AI infrastructure.

</details>


### [51] [Ensemble Bayesian Inference: Leveraging Small Language Models to Achieve LLM-level Accuracy in Profile Matching Tasks](https://arxiv.org/abs/2504.17685)

*Haru-Tada Sato, Fuka Matsuzaki, Jun-ichiro Takahashi*

**Main category:** cs.CL

**TL;DR:** This study introduces Ensemble Bayesian Inference (EBI) to combine small language model ensembles for accuracy comparable to large language models, showing effectiveness across diverse tasks.

<details>
  <summary>Details</summary>

**Motivation:** To explore how small language model ensembles can achieve performance on par with large proprietary models while utilizing limited computational resources.

**Method:** The study proposes Ensemble Bayesian Inference (EBI) to combine outputs from multiple small language models using Bayesian estimation, analyzing performance across tasks and languages.

**Result:** Experiments demonstrated that EBI improves overall performance, including cases where models with negative Lift values contribute positively, across various tasks in Japanese and English.

**Conclusion:** EBI presents a significant advancement in AI model construction, enabling effective use of lower-performing models and broadening possibilities for performance enhancement in resource-constrained environments.

**Abstract:** This study explores the potential of small language model(SLM) ensembles to achieve accuracy comparable to proprietary large language models (LLMs). We propose Ensemble Bayesian Inference (EBI), a novel approach that applies Bayesian estimation to combine judgments from multiple SLMs, allowing them to exceed the performance limitations of individual models. Our experiments on diverse tasks(aptitude assessments and consumer profile analysis in both Japanese and English) demonstrate EBI's effectiveness. Notably, we analyze cases where incorporating models with negative Lift values into ensembles improves overall performance, and we examine the method's efficacy across different languages. These findings suggest new possibilities for constructing high-performance AI systems with limited computational resources and for effectively utilizing models with individually lower performance. Building on existing research on LLM performance evaluation, ensemble methods, and open-source LLM utilization, we discuss the novelty and significance of our approach.

</details>


### [52] [Safety in Large Reasoning Models: A Survey](https://arxiv.org/abs/2504.17704)

*Cheng Wang, Yue Liu, Baolong Li, Duzhen Zhang, Zhongzhi Li, Junfeng Fang*

**Main category:** cs.CL

**TL;DR:** This paper surveys the safety risks and defense strategies related to Large Reasoning Models (LRMs), providing a taxonomy for better understanding.

<details>
  <summary>Details</summary>

**Motivation:** To address the emerging vulnerabilities and safety concerns associated with Large Reasoning Models as they become more capable.

**Method:** A comprehensive survey and taxonomy of safety risks, attacks, and defense strategies related to LRMs.

**Result:** The paper categorizes newly emerged safety risks and defense strategies, offering a structured understanding of the current safety landscape of LRMs.

**Conclusion:** This structured understanding aims to facilitate future research and improve the security and reliability of LRMs.

**Abstract:** Large Reasoning Models (LRMs) have exhibited extraordinary prowess in tasks like mathematics and coding, leveraging their advanced reasoning capabilities. Nevertheless, as these capabilities progress, significant concerns regarding their vulnerabilities and safety have arisen, which can pose challenges to their deployment and application in real-world settings. This paper presents a comprehensive survey of LRMs, meticulously exploring and summarizing the newly emerged safety risks, attacks, and defense strategies. By organizing these elements into a detailed taxonomy, this work aims to offer a clear and structured understanding of the current safety landscape of LRMs, facilitating future research and development to enhance the security and reliability of these powerful models.

</details>


### [53] [Multilingual Performance Biases of Large Language Models in Education](https://arxiv.org/abs/2504.17720)

*Vansh Gupta, Sankalan Pal Chowdhury, Vilém Zouhar, Donya Rooein, Mrinmaya Sachan*

**Main category:** cs.CL

**TL;DR:** The study evaluates the performance of large language models (LLMs) in educational tasks across multiple non-English languages and finds varying degrees of effectiveness, recommending verification of suitability for each target language before use.

<details>
  <summary>Details</summary>

**Motivation:** To determine the appropriateness and effectiveness of using LLMs in educational settings for non-English languages, as current models are primarily designed for English.

**Method:** The performance of popular LLMs was evaluated on four educational tasks: identifying misconceptions, providing feedback, interactive tutoring, and grading translations, across six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) alongside English.

**Result:** Performance on the tasks correlated with the representation of the language in training data, with lower resource languages exhibiting poorer task performance; performance drops from English were significant in most cases.

**Conclusion:** Practitioners should verify the performance of LLMs in the target educational language before deployment, given the inconsistency and challenges observed across different languages.

**Abstract:** Large language models (LLMs) are increasingly being adopted in educational settings. These applications expand beyond English, though current LLMs remain primarily English-centric. In this work, we ascertain if their use in education settings in non-English languages is warranted. We evaluated the performance of popular LLMs on four educational tasks: identifying student misconceptions, providing targeted feedback, interactive tutoring, and grading translations in six languages (Hindi, Arabic, Farsi, Telugu, Ukrainian, Czech) in addition to English. We find that the performance on these tasks somewhat corresponds to the amount of language represented in training data, with lower-resource languages having poorer task performance. Although the models perform reasonably well in most languages, the frequent performance drop from English is significant. Thus, we recommend that practitioners first verify that the LLM works well in the target language for their educational task before deployment.

</details>


### [54] [Conversational Assistants to support Heart Failure Patients: comparing a Neurosymbolic Architecture with ChatGPT](https://arxiv.org/abs/2504.17753)

*Anuja Tayal, Devika Salunke, Barbara Di Eugenio, Paula Allen-Meares, Eulalia Puig Abril, Olga Garcia, Carolyn Dickens, Andrew Boyd*

**Main category:** cs.CL

**TL;DR:** A user study evaluates two conversational assistants for heart failure patients: one using a neurosymbolic architecture and the other based on ChatGPT.

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of conversational assistants in healthcare and compare traditional architectures with generative AI.

**Method:** A within-group user study compared two conversational assistant versions, evaluating their accuracy, task completion, verbosity, speech errors, and need for clarifications.

**Result:** The in-house system was more accurate, less verbose, and completed more tasks, while ChatGPT made fewer speech errors and needed fewer clarifications; patients showed no preference for either system.

**Conclusion:** Both conversational assistants have unique advantages, but patients do not favor one over the other.

**Abstract:** Conversational assistants are becoming more and more popular, including in healthcare, partly because of the availability and capabilities of Large Language Models. There is a need for controlled, probing evaluations with real stakeholders which can highlight advantages and disadvantages of more traditional architectures and those based on generative AI. We present a within-group user study to compare two versions of a conversational assistant that allows heart failure patients to ask about salt content in food. One version of the system was developed in-house with a neurosymbolic architecture, and one is based on ChatGPT. The evaluation shows that the in-house system is more accurate, completes more tasks and is less verbose than the one based on ChatGPT; on the other hand, the one based on ChatGPT makes fewer speech errors and requires fewer clarifications to complete the task. Patients show no preference for one over the other.

</details>


### [55] [The Sparse Frontier: Sparse Attention Trade-offs in Transformer LLMs](https://arxiv.org/abs/2504.17768)

*Piotr Nawrot, Robert Li, Renjie Huang, Sebastian Ruder, Kelly Marchisio, Edoardo M. Ponti*

**Main category:** cs.CL

**TL;DR:** Sparse attention enhances long-context capabilities in Transformer LLMs but involves complex trade-offs in efficiency and accuracy.

<details>
  <summary>Details</summary>

**Motivation:** To explore the viability, efficiency-accuracy trade-offs, and systematic scaling studies of sparse attention methods in Transformer LLMs for long-context tasks.

**Method:** Conducted a comparison of training-free sparse attention methods across different model scales, sequence lengths, and sparsity levels on a variety of long-sequence tasks.

**Result:** The study found that larger, sparse models are preferable for long sequences, existing sparsity levels vary in effectiveness depending on the model phase, and no single strategy is optimal across all tasks, alongside novel scaling laws for sparse attention.

**Conclusion:** Sparse attention significantly improves the capabilities of Transformer LLMs for longer sequences, but careful consideration of trade-offs is necessary for performance-sensitive applications.

**Abstract:** Sparse attention offers a promising strategy to extend long-context capabilities in Transformer LLMs, yet its viability, its efficiency-accuracy trade-offs, and systematic scaling studies remain unexplored. To address this gap, we perform a careful comparison of training-free sparse attention methods at varying model scales, sequence lengths, and sparsity levels on a diverse collection of long-sequence tasks-including novel ones that rely on natural language while remaining controllable and easy to evaluate. Based on our experiments, we report a series of key findings: 1) an isoFLOPS analysis reveals that for very long sequences, larger and highly sparse models are preferable to smaller and dense ones. 2) The level of sparsity attainable while statistically guaranteeing accuracy preservation is higher during decoding than prefilling, and correlates with model size in the former. 3) There is no clear strategy that performs best across tasks and phases, with different units of sparsification or budget adaptivity needed for different scenarios. Even moderate sparsity levels often result in significant performance degradation on at least one task, highlighting that sparse attention is not a universal solution. 4) We introduce and validate novel scaling laws specifically tailored for sparse attention, providing evidence that our findings are likely to hold true beyond our range of experiments. Through these insights, we demonstrate that sparse attention is a key tool to enhance the capabilities of Transformer LLMs for processing longer sequences, but requires careful evaluation of trade-offs for performance-sensitive applications.

</details>
