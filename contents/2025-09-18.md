# 2025-09-18

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 58]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [AI Behavioral Science](https://arxiv.org/abs/2509.13323)

*Matthew O. Jackson, Qiaozhu Me, Stephanie W. Wang, Yutong Xie, Walter Yuan, Seth Benzell, Erik Brynjolfsson, Colin F. Camerer, James Evans, Brian Jabarian, Jon Kleinberg, Juanjuan Meng, Sendhil Mullainathan, Asuman Ozdaglar, Thomas Pfeiffer, Moshe Tennenholtz, Robb Willer, Diyi Yang, Teng Ye*

**Main category:** cs.HC

**Keywords:** AI, Behavioral Science, Human-AI Interaction

**Relevance Score:** 4

**TL;DR:** The paper explores the intersection of AI and behavioral sciences, discussing AI's potential to enhance research and how behavioral sciences can inform AI design and human-AI interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI can contribute to behavioral science research and how behavioral sciences can inform the design of AI systems.

**Method:** 

**Key Contributions:**

	1. Exploration of AI's role in enhancing behavioral science research
	2. Discussion on using behavioral science principles to improve AI design
	3. Insights on the evolving interaction between AI and humans

**Result:** 

**Limitations:** 

**Conclusion:** 

**Abstract:** We discuss the three main areas comprising the new and emerging field of "AI Behavioral Science". This includes not only how AI can enhance research in the behavioral sciences, but also how the behavioral sciences can be used to study and better design AI and to understand how the world will change as AI and humans interact in increasingly layered and complex ways.

</details>


### [2] [Designing Psychometric Bias Measures for ChatBots: An Application to Racial Bias Measurement](https://arxiv.org/abs/2509.13324)

*Mouhacine Benosman*

**Main category:** cs.HC

**Keywords:** artificial intelligence, large language models, chatbots, bias measurement, human-machine interaction

**Relevance Score:** 9

**TL;DR:** This paper proposes a framework for measuring biases in AI chatbots, addressing the impact of such biases in various human-machine interaction applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of LLMs in daily life raises concerns about whether these systems will perpetuate existing human biases in their interactions.

**Method:** The authors propose a principled framework for designing psychometric measures to evaluate biases in chatbots, aiming to rigorously assess their interactions with users.

**Key Contributions:**

	1. Proposed a framework for designing psychometric measures for chatbot bias evaluation
	2. Identified various domains of application for chatbots where bias measurement is critical
	3. Highlighted the importance of addressing biases in AI interactions to avoid societal reinforcement.

**Result:** The framework is designed to evaluate bias in various applications, including corporate hiring, loan approvals, and psychotherapy, indicating a need for careful measurement in these contexts.

**Limitations:** 

**Conclusion:** Addressing chatbot biases is crucial to ensure they do not reinforce existing societal biases in their applications.

**Abstract:** Artificial intelligence (AI), particularly in the form of large language models (LLMs) or chatbots, has become increasingly integrated into our daily lives. In the past five years, several LLMs have been introduced, including ChatGPT by OpenAI, Claude by Anthropic, and Llama by Meta, among others. These models have the potential to be employed across a wide range of human-machine interaction applications, such as chatbots for information retrieval, assistance in corporate hiring decisions, college admissions, financial loan approvals, parole determinations, and even in medical fields like psychotherapy delivered through chatbots. The key question is whether these chatbots will interact with humans in a bias-free manner or if they will further reinforce the existing pathological biases present in human-to-human interactions. If the latter is true, then how to rigorously measure these biases? We aim to address this challenge by proposing a principled framework for designing psychometric measures to evaluate chatbot biases.

</details>


### [3] [LLM Chatbot-Creation Approaches](https://arxiv.org/abs/2509.13326)

*Hemil Mehta, Tanvi Raut, Kohav Yadav, Edward F. Gehringer*

**Main category:** cs.HC

**Keywords:** chatbots, LLMs, low-code, education, custom-coded

**Relevance Score:** 8

**TL;DR:** This paper explores the comparison of low-code platforms and custom-coded solutions for developing educational chatbots, emphasizing the integration of LLMs and their implications on scalability and customization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of LLMs in educational contexts necessitates effective chatbot development strategies that balance ease of use with technical demands.

**Method:** The study evaluates chatbot prototypes built with low-code platforms like AnythingLLM and Botpress against custom-coded solutions using LangChain and FastAPI, employing prompt engineering and retrieval-augmented generation.

**Key Contributions:**

	1. Comparison of low-code vs custom-coded chatbot development
	2. Framework for selecting development strategy
	3. Evaluation of chatbot prototypes for educational use

**Result:** Low-code platforms allow for quick prototyping but lack deep customization and scalability, whereas custom-coded solutions provide control at the cost of requiring greater technical skills. Both methods maintain effective research principles for user engagement.

**Limitations:** Low-code platforms are limited in customization and scalability; custom-coded solutions demand significant technical expertise.

**Conclusion:** The paper proposes a framework to guide the selection of development strategies based on institutional goals, highlighting potential future innovations in hybrid solutions for chatbots.

**Abstract:** This full research-to-practice paper explores approaches for developing course chatbots by comparing low-code platforms and custom-coded solutions in educational contexts. With the rise of Large Language Models (LLMs) like GPT-4 and LLaMA, LLM-based chatbots are being integrated into teaching workflows to automate tasks, provide assistance, and offer scalable support. However, selecting the optimal development strategy requires balancing ease of use, customization, data privacy, and scalability. This study compares two development approaches: low-code platforms like AnythingLLM and Botpress, with custom-coded solutions using LangChain, FAISS, and FastAPI. The research uses Prompt engineering, Retrieval-augmented generation (RAG), and personalization to evaluate chatbot prototypes across technical performance, scalability, and user experience. Findings indicate that while low-code platforms enable rapid prototyping, they face limitations in customization and scaling, while custom-coded systems offer more control but require significant technical expertise. Both approaches successfully implement key research principles such as adaptive feedback loops and conversational continuity. The study provides a framework for selecting the appropriate development strategy based on institutional goals and resources. Future work will focus on hybrid solutions that combine low-code accessibility with modular customization and incorporate multimodal input for intelligent tutoring systems.

</details>


### [4] [DuetUI: A Bidirectional Context Loop for Human-Agent Co-Generation of Task-Oriented Interfaces](https://arxiv.org/abs/2509.13444)

*Yuan Xu, Shaowen Xiang, Yizhi Song, Ruoting Sun, Xin Tong*

**Main category:** cs.HC

**Keywords:** Human-Agent Collaboration, Large Language Models, Task Automation

**Relevance Score:** 9

**TL;DR:** This paper introduces DuetUI, an LLM-empowered system that enhances human-agent collaboration in complex task automation through a human-agent co-generation paradigm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models face challenges in automating complex tasks that require understanding and aligning with user intent.

**Method:** Based on a formative study with 12 participants, a new paradigm called human-agent co-generation was developed and implemented in the DuetUI prototype. The system operates through a bidirectional context loop that allows for dynamic user interaction and agent response.

**Key Contributions:**

	1. Introduction of the human-agent co-generation paradigm
	2. Development of the DuetUI prototype
	3. Empirical insights into user-agent interaction dynamics

**Result:** In a user study with 24 participants, DuetUI significantly improved task efficiency and interface usability compared to a baseline, indicating success in enhancing human-agent collaboration.

**Limitations:** 

**Conclusion:** The study validates the human-agent co-generation paradigm and demonstrates that the bidirectional loop effectively aligns agents with user intent, providing empirical insights into its benefits.

**Abstract:** Large Language Models are reshaping task automation, yet remain limited in complex, multi-step real-world tasks that require aligning with vague user intent and enabling dynamic user override. From a formative study with 12 participants, we found that end-users actively seek to shape generative interfaces rather than relying on one-shot outputs. To address this, we introduce the human-agent co-generation paradigm, materialized in DuetUI. This LLM-empowered system unfolds alongside task progress through a bidirectional context loop--the agent scaffolds the interface by decomposing the task, while the user's direct manipulations implicitly steer the agent's next generation step. In a user study with 24 participants, DuetUI significantly improved task efficiency and interface usability compared to a baseline, fostering seamless human-agent collaboration. Our contributions include the proposal and validation of this novel paradigm, the design of the DuetUI prototype embodying it, and empirical insights into how this bidirectional loop better aligns agents with human intent.

</details>


### [5] [Do We Need Subsidiarity in Software?](https://arxiv.org/abs/2509.13466)

*Louisa Conwill, Megan Levis Scheirer, Walter Scheirer*

**Main category:** cs.HC

**Keywords:** subsidiarity, data privacy, user control, surveillance capitalism, human-computer interaction

**Relevance Score:** 6

**TL;DR:** The study explores data privacy through the lens of subsidiarity, examining user control versus convenience in everyday technologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze the trade-offs between user control of software and surrendering control to larger tech companies in the context of data privacy.

**Method:** A multi-method approach involving data flow monitoring and user interviews to assess user control levels in everyday technologies.

**Key Contributions:**

	1. Introduction of subsidiarity as a lens for analyzing data privacy
	2. Insights into user willingness to compromise privacy for convenience
	3. Evaluation of control levels in popular chat platforms

**Result:** Chat platforms like Slack and Discord were identified as violating the principle of subsidiarity the most; users are willing to trade privacy for convenience.

**Limitations:** 

**Conclusion:** Subsidiarity can guide the design of technologies that enhance human dignity and promote better user autonomy in the digital landscape.

**Abstract:** Subsidiarity is a principle of social organization that promotes human dignity and resists over-centralization by balancing personal autonomy with intervention from higher authorities only when necessary. Thus it is a relevant, but not previously explored, critical lens for discerning the tradeoffs between complete user control of software and surrendering control to "big tech" for convenience, as is common in surveillance capitalism. Our study explores data privacy through the lens of subsidiarity: we employ a multi-method approach of data flow monitoring and user interviews to determine the level of control different everyday technologies currently operate at, and the level of control everyday computer users think is necessary. We found that chat platforms like Slack and Discord violate subsidiarity the most. Our work provides insight into when users are willing to surrender privacy for convenience and demonstrates how subsidiarity can inform designs that promote human flourishing.

</details>


### [6] [AR-TMT: Investigating the Impact of Distraction Types on Attention and Behavior in AR-based Trail Making Test](https://arxiv.org/abs/2509.13468)

*Sihun Baek, Zhehan Qu, Maria Gorlatova*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Distraction, User Behavior, Eye Tracking, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** The paper presents AR-TMT, an AR adaptation of the Trail Making Test, exploring user behavior under different types of distraction in augmented reality environments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically understand how different types of distraction affect user behavior in AR environments, especially in safety-critical domains.

**Method:** Implemented AR-TMT on Magic Leap 2 with distractions categorized as top-down, bottom-up, and spatial; performance metrics, gaze, motor behavior, and subjective load measures were captured.

**Key Contributions:**

	1. Development of AR-TMT for studying distraction in AR environments
	2. Identification of the effects of top-down, bottom-up, and spatial distractions on user performance
	3. Correlation between attention control and performance under object-based distractions

**Result:** A study with 34 participants revealed that top-down distraction led to performance degradation, bottom-up distraction affected initial attention, and spatial distraction caused unstable gaze patterns. Performance correlated with attention control in object-based distraction scenarios.

**Limitations:** 

**Conclusion:** The study provides insights into distraction mechanisms in AR, suggesting the need to address unique demands of AR tasks, with potential generalization to ecologically relevant AR applications.

**Abstract:** Despite the growing use of AR in safety-critical domains, the field lacks a systematic understanding of how different types of distraction affect user behavior in AR environments. To address this gap, we present AR-TMT, an AR adaptation of the Trail Making Test that spatially renders targets for sequential selection on the Magic Leap 2. We implemented distractions in three categories: top-down, bottom-up, and spatial distraction based on Wolfe's Guided Search model, and captured performance, gaze, motor behavior, and subjective load measures to analyze user attention and behavior. A user study with 34 participants revealed that top-down distraction degraded performance through semantic interference, while bottom-up distraction disrupted initial attentional engagement. Spatial distraction destabilized gaze behavior, leading to more scattered and less structured visual scanning patterns. We also found that performance was correlated with attention control ($R^2 = .20$--$.35$) under object-based distraction conditions, where distractors possessed task-relevant features. The study offers insights into distraction mechanisms and their impact on users, providing opportunities for generalization to ecologically relevant AR tasks while underscoring the need to address the unique demands of AR environments.

</details>


### [7] [Py maidr: Bridging Visual and Non-Visual Data Experiences Through a Unified Python Framework](https://arxiv.org/abs/2509.13532)

*JooYoung Seo, Saairam Venkatesh, Daksh Pokar, Sanchita Kamath, Krishna Anandan Ganesan*

**Main category:** cs.HC

**Keywords:** accessible visualization, blind and low-vision users, data communication, Python package, collaborative design

**Relevance Score:** 7

**TL;DR:** This paper introduces Py maidr, a Python package designed to create accessible data visualizations for both blind and low-vision (BLV) users and sighted individuals, promoting collaboration and reducing accessibility gaps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unidirectional design approach that perpetuates a divide between sighted creators and BLV consumers in data visualization tools.

**Method:** The paper presents the Py maidr package that enables the encoding of multimodal data representations into visual plots using existing libraries like Matplotlib and Seaborn, facilitating the creation of accessible visualizations by diverse users.

**Key Contributions:**

	1. Introduction of a bidirectional approach to data visualization design that includes both sighted and BLV users.
	2. Development of the Py maidr package that integrates with existing Python libraries for accessible visualizations.
	3. Demonstration of minimal performance overhead and integration with interactive platforms.

**Result:** The benchmarks show that Py maidr adds minimal overhead to plot rendering and can easily integrate with platforms like Jupyter Notebook and Google Colab, enhancing accessibility without sacrificing performance.

**Limitations:** 

**Conclusion:** Py maidr significantly narrows the accessibility gap in data visualization and fosters collaboration between sighted and BLV users by providing a unified tool for creating and interpreting data visualizations.

**Abstract:** Although recent efforts have developed accessible data visualization tools for blind and low-vision (BLV) users, most follow a "design for them" approach that creates an unintentional divide between sighted creators and BLV consumers. This unidirectional paradigm perpetuates a power dynamic where sighted creators produce non-visual content boundaries for BLV consumers to access. This paper proposes a bidirectional approach, "design for us," where both sighted and BLV collaborators can employ the same tool to create, interpret, and communicate data visualizations for each other. We introduce Py maidr, a Python package that seamlessly encodes multimodal (e.g., tactile, auditory, conversational) data representations into visual plots generated by Matplotlib and Seaborn. By simply importing the maidr package and invoking the maidr.show() method, users can generate accessible plots with minimal changes to their existing codebase regardless of their visual dis/abilities. Our technical case studies demonstrate how this tool is scalable and can be integrated into interactive computing (e.g., Jupyter Notebook, Google Colab), reproducible and literate programming (e.g., Quarto), and reactive dashboards (e.g., Shiny, Streamlit). Our performance benchmarks demonstrate that Py maidr introduces minimal and consistent overhead during the rendering and export of plots against Matplotlib and Seaborn baselines. This work significantly contributes to narrowing the accessibility gap in data visualization by providing a unified framework that fosters collaboration and communication between sighted and BLV individuals.

</details>


### [8] [Vistoria: A Multimodal System to Support Fictional Story Writing through Instrumental Text-Image Co-Editing](https://arxiv.org/abs/2509.13646)

*Kexue Fu, Jingfei Huang, Long Ling, Sumin Hong, Yihang Zuo, Ray LC, Toby Jia-jun Li*

**Main category:** cs.HC

**Keywords:** multimodal interaction, creative writing, co-editing, visual metaphors, narrative exploration

**Relevance Score:** 6

**TL;DR:** Vistoria is a text-image co-editing system for creative writing that enhances expressiveness and collaboration among authors by integrating visual and textual elements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Creative writing tools currently focus on text, which limits authors' ability to visually plan and organize their stories. The paper aims to address this gap by integrating visual elements into the writing process.

**Method:** The authors conducted a formative Wizard-of-Oz co-design study with 10 story writers to understand how visuals aid in writing, followed by a controlled study with 12 participants testing the co-editing system's effectiveness.

**Key Contributions:**

	1. Introduction of the Vistoria system for synchronized text-image co-editing
	2. Identification of multimodal operations that facilitate narrative exploration
	3. Empirical evidence supporting the benefits of multimodal co-editing for writers

**Result:** The study found that co-editing with visuals improves expressiveness, immersion, and collaboration, helping writers to explore new narrative directions and feel a stronger sense of authorship.

**Limitations:** Increased cognitive demand was reported by participants when utilizing multimodal features.

**Conclusion:** Multimodal co-editing can enhance creative potential in narrative development by balancing abstraction and concreteness despite increasing cognitive demands.

**Abstract:** Humans think visually-we remember in images, dream in pictures, and use visual metaphors to communicate. Yet, most creative writing tools remain text-centric, limiting how authors plan and translate ideas. We present Vistoria, a system for synchronized text-image co-editing in fictional story writing that treats visuals and text as coequal narrative materials. A formative Wizard-of-Oz co-design study with 10 story writers revealed how sketches, images, and annotations serve as essential instruments for ideation and organization. Drawing on theories of Instrumental Interaction and Structural Mapping, Vistoria introduces multimodal operations-lasso, collage, filters, and perspective shifts that enable seamless narrative exploration across modalities. A controlled study with 12 participants shows that co-editing enhances expressiveness, immersion, and collaboration, enabling writers to explore divergent directions, embrace serendipitous randomness, and trace evolving storylines. While multimodality increased cognitive demand, participants reported stronger senses of authorship and agency. These findings demonstrate how multimodal co-editing expands creative potential by balancing abstraction and concreteness in narrative development.

</details>


### [9] [I, Robot? Socio-Technical Implications of Ultra-Personalized AI-Powered AAC; an Autoethnographic Account](https://arxiv.org/abs/2509.13671)

*Tobias Weinberg, Ricardo E. Gonzalez Penuela, Stephanie Valencia, Thijs Roumen*

**Main category:** cs.HC

**Keywords:** Augmentative and Alternative Communication, personalization, AI auto-complete, privacy, self-expression

**Relevance Score:** 8

**TL;DR:** This paper examines the personalization of AI auto-complete systems for users of Augmentative and Alternative Communication (AAC) devices by reflecting on a study that involved data collection, model fine-tuning, and daily usage, while addressing privacy and self-expression issues.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The need for personalized AI systems for AAC users to reduce editing burdens and improve communication effectiveness while considering socio-technical implications.

**Method:** An autoethnographic study consisting of data collection over seven months, fine-tuning a language model on this dataset, and three months of using personalized AI suggestions, supplemented by continuous diary entries and interaction logs.

**Key Contributions:**

	1. Exploration of personalization effects on AAC communication
	2. Insights into privacy and authorship implications
	3. A comprehensive autoethnographic approach to studying AI personalization

**Result:** Personalization of AI suggestions significantly reduced the need for edits in communication, revealing both advantages and challenges regarding privacy, authorship, and self-expression.

**Limitations:** The study is based on the lead author's experience, which may not be generalizable to all AAC users.

**Conclusion:** The study highlights the dual role of personalization in enhancing communication for AAC users while raising important socio-technical questions.

**Abstract:** Generic AI auto-complete for message composition often fails to capture the nuance of personal identity, requiring significant editing. While harmless in low-stakes settings, for users of Augmentative and Alternative Communication (AAC) devices, who rely on such systems for everyday communication, this editing burden is particularly acute. Intuitively, the need for edits would be lower if language models were personalized to the communication of the specific user. While technically feasible, such personalization raises socio-technical questions: what are the implications of logging one's own conversations, and how does personalization affect privacy, authorship, and control? We explore these questions through an autoethnographic study in three phases: (1) seven months of collecting all the lead author's AAC communication data, (2) fine-tuning a model on this dataset, and (3) three months of daily use of personalized AI suggestions. We reflect on these phases through continuous diary entries and interaction logs. Our findings highlight the value of personalization as well as implications on privacy, authorship, and blurring the boundaries of self-expression.

</details>


### [10] [From Prompts to Reflection: Designing Reflective Play for GenAI Literacy](https://arxiv.org/abs/2509.13679)

*Qianou Ma, Megan Chai, Yike Tan, Jihun Choi, Jini Kim, Erik Harpstead, Geoff Kauffman, Tongshuang Wu*

**Main category:** cs.HC

**Keywords:** Generative AI, AI literacy, reflective play, game-based learning, adult education

**Relevance Score:** 8

**TL;DR:** The paper presents ImaginAItion, a multiplayer game designed to enhance adults' understanding of Generative AI through reflective play.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective AI literacy interventions for adults, as many current efforts focus on children and do not address adult understanding of Generative AI capabilities and biases.

**Method:** The study involved ten gameplay sessions with 30 adult participants, where the game facilitated discussions about biases in Generative AI and differences in human-AI interpretation.

**Key Contributions:**

	1. Introduction of a multiplayer party game for AI literacy among adults.
	2. Empirical evidence showing enhanced recognition of biases and prompting strategies through gameplay.
	3. Insights on how group dynamics affect learning outcomes.

**Result:** Participants recognized systematic biases in Generative AI and improved their prompting strategies through gameplay reflection. Group dynamics influenced the depth of reflection.

**Limitations:** The study is limited to a small sample size and specific group settings, which may affect the generalizability of results.

**Conclusion:** ImaginAItion serves as a valuable tool for fostering critical literacy about Generative AI in adults, with potential for scaling such interventions.

**Abstract:** The wide adoption of Generative AI (GenAI) in everyday life highlights the need for greater literacy around its evolving capabilities, biases, and limitations. While many AI literacy efforts focus on children through game-based learning, few interventions support adults in developing a nuanced, reflective understanding of GenAI via playful exploration. To address the gap, we introduce ImaginAItion, a multiplayer party game inspired by Drawful and grounded in the reflective play framework to surface model defaults, biases, and human-AI perception gaps through prompting and discussion. From ten sessions (n=30), we show how gameplay helped adults recognize systematic biases in GenAI, reflect on humans and AI interpretation differences, and adapt their prompting strategies. We also found that group dynamics and composition, such as expertise and diversity, amplified or muted reflection. Our work provides a starting point to scale critical GenAI literacy through playful, social interventions resilient to rapidly evolving technologies.

</details>


### [11] [Spatial Balancing: Harnessing Spatial Reasoning to Balance Scientific Exposition and Narrative Engagement in LLM-assisted Science Communication Writing](https://arxiv.org/abs/2509.13742)

*Kexue Fu, Jiaye Leng, Yawen Zhang, Jingfei Huang, Yihang Zuo, Runze Cai, Zijian Ding, Ray LC, Shengdong Zhao, Qinyuan Lei*

**Main category:** cs.HC

**Keywords:** science communication, large language models, SpatialBalancing, revision strategies, metacognitive reflection

**Relevance Score:** 8

**TL;DR:** The paper presents SpatialBalancing, a co-writing system designed to enhance science communication by balancing scientific exposition and narrative engagement through spatial reasoning and large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the challenge of achieving a balance between scientific exposition and narrative engagement in science communication.

**Method:** A formative study with science communicators and a literature review to understand workflows, followed by the development of SpatialBalancing, which visualizes revision trade-offs in a dual-axis space.

**Key Contributions:**

	1. Development of SpatialBalancing, a novel co-writing system.
	2. Introduction of a dual-axis visualization for revision trade-offs.
	3. Demonstration of enhanced metacognitive reflection and creative exploration through the system.

**Result:** SpatialBalancing significantly enhanced participants' metacognitive reflection, flexibility, and creative exploration during the science communication writing process.

**Limitations:** 

**Conclusion:** Combining spatial reasoning with linguistic generation in SpatialBalancing enables intentional iterations that improve the quality of science communication by balancing rigor and appeal.

**Abstract:** Balancing scientific exposition and narrative engagement is a central challenge in science communication. To examine how to achieve balance, we conducted a formative study with four science communicators and a literature review of science communication practices, focusing on their workflows and strategies. These insights revealed how creators iteratively shift between exposition and engagement but often lack structured support. Building on this, we developed SpatialBalancing, a co-writing system that connects human spatial reasoning with the linguistic intelligence of large language models. The system visualizes revision trade-offs in a dual-axis space, where users select strategy-based labels to generate, compare, and refine versions during the revision process. This spatial externalization transforms revision into spatial navigation, enabling intentional iterations that balance scientific rigor with narrative appeal. In a within-subjects study (N=16), SpatialBalancing enhanced metacognitive reflection, flexibility, and creative exploration, demonstrating how coupling spatial reasoning with linguistic generation fosters monitoring in iterative science communication writing.

</details>


### [12] [Synthetic Data Generation for Screen Time and App Usage](https://arxiv.org/abs/2509.13892)

*Gustavo Kruger, Nikhil Sachdeva, Michael Sobolev*

**Main category:** cs.HC

**Keywords:** smartphone usage, synthetic data, large language models, human behavior, data generation

**Relevance Score:** 9

**TL;DR:** This paper explores using large language models (LLMs) to generate synthetic smartphone usage datasets to overcome challenges in real-world data collection.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** Smartphone usage data is crucial for understanding human-computer interaction and behavior, but gathering such data is hindered by cost, privacy, and sample biases.

**Method:** The authors conducted a case study comparing four prompt strategies for generating smartphone usage data using an LLM, evaluating the impact of prompt detail and seed data inclusion.

**Key Contributions:**

	1. Exploration of LLMs for synthetic smartphone usage data generation
	2. Analysis of prompt strategies affecting data quality
	3. Insights into trade-offs between data fidelity and diversity

**Result:** The study found that detailed prompts significantly improved the plausibility and structure of synthetic smartphone usage datasets, though capturing diverse human behavior remains a challenge.

**Limitations:** Challenges in capturing diverse human behavioral patterns and evaluating data quality remain.

**Conclusion:** While LLMs can generate usable synthetic datasets, careful prompt design and evaluation metrics specific to use cases are essential for balancing data fidelity and diversity.

**Abstract:** Smartphone usage data can provide valuable insights for understanding interaction with technology and human behavior. However, collecting large-scale, in-the-wild smartphone usage logs is challenging due to high costs, privacy concerns, under representative user samples and biases like non-response that can skew results. These challenges call for exploring alternative approaches to obtain smartphone usage datasets. In this context, large language models (LLMs) such as Open AI's ChatGPT present a novel approach for synthetic smartphone usage data generation, addressing limitations of real-world data collection. We describe a case study on how four prompt strategies influenced the quality of generated smartphone usage data. We contribute with insights on prompt design and measures of data quality, reporting a prompting strategy comparison combining two factors, prompt level of detail (describing a user persona, describing the expected results characteristics) and seed data inclusion (with versus without an initial real usage example). Our findings suggest that using LLMs to generate structured and behaviorally plausible smartphone use datasets is feasible for some use cases, especially when using detailed prompts. Challenges remain in capturing diverse nuances of human behavioral patterns in a single synthetic dataset, and evaluating tradeoffs between data fidelity and diversity, suggesting the need for use-case-specific evaluation metrics and future research with more diverse seed data and different LLM models.

</details>


### [13] [AI as a teaching tool and learning partner](https://arxiv.org/abs/2509.13899)

*Steven Watterson, Sarah Atkinson, Elaine Murray, Andrew McDowell*

**Main category:** cs.HC

**Keywords:** Large Language Models, education, AI tools, chatbot, podcasts

**Relevance Score:** 8

**TL;DR:** This paper explores the impact of LLM-based tools in undergraduate and postgraduate teaching programs, evaluating a chatbot and audio podcasts for enhancing learning.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To determine how to effectively integrate LLMs into education and assess their impact on teaching and learning.

**Method:** Implemented a LLM-powered chatbot and AI-generated audio podcasts in two courses, then surveyed students on their experiences and attitudes.

**Key Contributions:**

	1. Demonstrated positive student reactions to an LLM-based chatbot in education.
	2. Identified mixed responses to AI-generated audio content in teaching.
	3. Provided insights on the potential of AI tools in enhancing learning experiences.

**Result:** Students positively perceived the chatbot, reporting it enhanced their learning experience, while the audio podcasts had mixed reception, with only localized interest among users.

**Limitations:** Limited to biological courses and small class sizes, which may not generalize to other subjects or larger populations.

**Conclusion:** Integrating LLMs into education tools can be beneficial, particularly chatbots, but the effectiveness of other formats like podcasts may vary among students.

**Abstract:** The arrival of AI tools and in particular Large Language Models (LLMs) has had a transformative impact on teaching and learning and institutes are still trying to determine how to integrate LLMs into education in constructive ways. Here, we explore the adoption of LLM-based tools into two teaching programmes, one undergraduate and one postgraduate. We provided to our classes (1) a LLM-powered chatbot that had access to course materials by RAG and (2) AI-generated audio-only podcasts for each week$\text{'}$s teaching material. At the end of the semester, we surveyed the classes to gauge attitudes towards these tools. The classes were small and from biological courses. The students felt positive about AI generally and that AI tools made a positive impact on teaching. Students found the LLM-powered chatbot easy and enjoyable to use and felt that it enhanced their learning. The podcasts were less popular and only a small proportion of the class listened weekly. The class as a whole was indifferent to whether the podcasts should be used more widely across courses, but those who listened enjoyed them and were in favour.

</details>


### [14] [AI For Privacy in Smart Homes: Exploring How Leveraging AI-Powered Smart Devices Enhances Privacy Protection](https://arxiv.org/abs/2509.14050)

*Wael Albayaydh, Ivan Flechais, Rui Zhao, Jood Albayaydh*

**Main category:** cs.HC

**Keywords:** AI, privacy, smart homes, user-centered design, ethics

**Relevance Score:** 8

**TL;DR:** This study examines how AI can enhance privacy in smart home devices through user-centered design and addresses privacy concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address misunderstandings around data privacy in smart home devices and explore AI’s potential to enhance user privacy.

**Method:** Conducted 23 in-depth interviews with users, AI developers, designers, and regulators; analyzed data using Grounded Theory.

**Key Contributions:**

	1. Identified user aspirations for AI-driven privacy solutions
	2. Outlined key ethical and security challenges
	3. Provided actionable recommendations for smart device designers and AI developers

**Result:** Identified themes of user aspirations for AI-enhanced privacy and ethical/security challenges; provided recommendations for stakeholders.

**Limitations:** 

**Conclusion:** The study offers insights for co-designing AI tools that enhance privacy in smart homes, bridging user expectations with AI capabilities.

**Abstract:** Privacy concerns and fears of unauthorized access in smart home devices often stem from misunderstandings about how data is collected, used, and protected. This study explores how AI-powered tools can offer innovative privacy protections through clear, personalized, and contextual support to users. Through 23 in-depth interviews with users, AI developers, designers, and regulators, and using Grounded Theory analysis, we identified two key themes: Aspirations for AI-Enhanced Privacy - how users perceive AI's potential to empower them, address power imbalances, and improve ease of use- and AI Ethical, Security, and Regulatory Considerations-challenges in strengthening data security, ensuring regulatory compliance, and promoting ethical AI practices. Our findings contribute to the field by uncovering user aspirations for AI-driven privacy solutions, identifying key security and ethical challenges, and providing actionable recommendations for all stakeholders, particularly targeting smart device designers and AI developers, to guide the co-design of AI tools that enhance privacy protection in smart home devices. By bridging the gap between user expectations, AI capabilities, and regulatory frameworks, this work offers practical insights for shaping the future of privacy-conscious AI integration in smart homes.

</details>


### [15] [EEG-Based Cognitive Load Classification During Landmark-Based VR Navigation](https://arxiv.org/abs/2509.14056)

*Jiahui An, Bingjie Cheng, Dmitriy Rudyka, Elisa Donati, Sara Fabrikant*

**Main category:** cs.HC

**Keywords:** cognitive load, EEG, navigation, machine learning, brain-computer interfaces

**Relevance Score:** 7

**TL;DR:** The study explores the effectiveness of EEG signals in classifying cognitive load during VR navigation tasks with varying map complexity.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how well EEG signals can classify cognitive load in dynamic navigation contexts, and to understand the influence of task complexity versus individual traits.

**Method:** EEG signals from 46 participants were analyzed as they navigated routes with different numbers of map landmarks (3, 5, 7), using a nested cross-validation framework across various machine learning models.

**Key Contributions:**

	1. Demonstrated high classification accuracy of cognitive load using EEG during navigation tasks
	2. Identified that task complexity impacts cognitive load assessment more than individual traits
	3. Proposed potential for adaptive navigation systems based on cognitive states

**Result:** Mean classification accuracies reached 90.8% for binary contrasts and 78.7% for the three-class problem, indicating that task demands were more significant than individual differences in shaping results.

**Limitations:** 

**Conclusion:** The study concludes that task demands strongly influence cognitive load classification, suggesting potential for developing navigation systems that adapt map complexity based on real-time cognitive load.

**Abstract:** Brain computer interfaces enable real-time monitoring of cognitive load, but their effectiveness in dynamic navigation contexts is not well established. Using an existing VR navigation dataset, we examined whether EEG signals can classify cognitive load during map-based wayfinding and whether classification accuracy depends more on task complexity or on individual traits. EEG recordings from forty-six participants navigating routes with 3, 5, or 7 map landmarks were analyzed with a nested cross-validation framework across multiple machine learning models. Classification achieved mean accuracies up to 90.8% for binary contrasts (3 vs. 7 landmarks) and 78.7% for the three-class problem, both well above chance. Demographic and cognitive variables (age, gender, spatial ability, working memory) showed no significant influence. These findings demonstrate that task demands outweigh individual differences in shaping classification performance, highlighting the potential for task-adaptive navigation systems that dynamically adjust map complexity in response to real-time cognitive states.

</details>


### [16] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)

*Julia S. Dollis, Iago A. Brito, Fernanda B. Färber, Pedro S. F. B. Ribeiro, Rafael T. Sousa, Arlindo R. Galvão Filho*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Large Language Models, Medical Education, Interpersonal Skills, Training Enhancement

**Relevance Score:** 9

**TL;DR:** This paper presents a framework that integrates large language models into virtual reality to train interpersonal skills in medical education.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of virtual reality for training interpersonal skills is hampered by a lack of psychologically plausible virtual humans, particularly in high-stakes fields like medical education.

**Method:** A mixed-method, within-subjects study was conducted with licensed physicians using simulated consultations with a new VR system that integrates LLMs to create coherent virtual patients.

**Key Contributions:**

	1. Integration of LLMs into VR for medical training
	2. Identification of design principles for virtual patients
	3. Validation of the effectiveness in a physician training context

**Result:** The approach is feasible and perceived as rewarding and effective by physicians, highlighting important design principles for future training environments.

**Limitations:** 

**Conclusion:** The validated framework offers insights into creating socially intelligent VR training systems and emphasizes the importance of perceived authenticity in challenges.

**Abstract:** While virtual reality (VR) excels at simulating physical environments, its effectiveness for training complex interpersonal skills is limited by a lack of psychologically plausible virtual humans. This is a critical gap in high-stakes domains like medical education, where communication is a core competency. This paper introduces a framework that integrates large language models (LLMs) into immersive VR to create medically coherent virtual patients with distinct, consistent personalities, built on a modular architecture that decouples personality from clinical data. We evaluated our system in a mixed-method, within-subjects study with licensed physicians who engaged in simulated consultations. Results demonstrate that the approach is not only feasible but is also perceived by physicians as a highly rewarding and effective training enhancement. Furthermore, our analysis uncovers critical design principles, including a ``realism-verbosity paradox" where less communicative agents can seem more artificial, and the need for challenges to be perceived as authentic to be instructive. This work provides a validated framework and key insights for developing the next generation of socially intelligent VR training environments.

</details>


### [17] [AppAgent v2: Advanced Agent for Flexible Mobile Interactions](https://arxiv.org/abs/2408.11824)

*Yanda Li, Chi Zhang, Wenjia Jiang, Wanqi Yang, Bin Fu, Pei Cheng, Xin Chen, Ling Chen, Yunchao Wei*

**Main category:** cs.HC

**Keywords:** Multimodal Large Language Models, LLM-driven agents, Mobile applications

**Relevance Score:** 9

**TL;DR:** This paper presents a novel framework for LLM-driven multimodal agents on mobile devices, enhancing user interface interactions through adaptable task execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The advancement of Multimodal Large Language Models has led to the need for more sophisticated LLM-driven agents that can improve interactions with software interfaces.

**Method:** The framework includes an exploration phase for documenting UI functionalities and a deployment phase utilizing RAG technology for efficient task execution.

**Key Contributions:**

	1. Introduced a novel LLM-based multimodal agent framework for mobile devices.
	2. Demonstrated adaptability in handling customized workflows across multiple applications.
	3. Showcased superior performance through experimental benchmarks.

**Result:** The experimental results show superior performance of the agent across various benchmarks, demonstrating effectiveness in real-world applications.

**Limitations:** 

**Conclusion:** The framework successfully adapts to various applications while performing complex task workflows, showing promise for future mobile applications.

**Abstract:** With the advancement of Multimodal Large Language Models (MLLM), LLM-driven visual agents are increasingly impacting software interfaces, particularly those with graphical user interfaces. This work introduces a novel LLM-based multimodal agent framework for mobile devices. This framework, capable of navigating mobile devices, emulates human-like interactions. Our agent constructs a flexible action space that enhances adaptability across various applications including parser, text and vision descriptions. The agent operates through two main phases: exploration and deployment. During the exploration phase, functionalities of user interface elements are documented either through agent-driven or manual explorations into a customized structured knowledge base. In the deployment phase, RAG technology enables efficient retrieval and update from this knowledge base, thereby empowering the agent to perform tasks effectively and accurately. This includes performing complex, multi-step operations across various applications, thereby demonstrating the framework's adaptability and precision in handling customized task workflows. Our experimental results across various benchmarks demonstrate the framework's superior performance, confirming its effectiveness in real-world scenarios. Our code will be open source soon.

</details>


### [18] [The Role of Human Creativity in the Presence of AI Creativity Tools at Work: A Case Study on AI-Driven Content Transformation in Journalism](https://arxiv.org/abs/2502.05347)

*Sitong Wang, Jocelyn McKinnon-Crowley, Tao Long, Kian Loong Lua, Keren Henderson, Kevin Crowston, Jeffrey V. Nickerson, Mark Hansen, Lydia B. Chilton*

**Main category:** cs.HC

**Keywords:** AI in creativity, human-computer interaction, content creation, social media, student newsroom

**Relevance Score:** 7

**TL;DR:** A study on how human creativity persists in jobs using AI, showing that creators viewed AI as a tool for inspiration rather than a replacement.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of human creativity in jobs augmented by AI tools in a newsroom setting.

**Method:** A 14-week study involving a student newsroom using an AI tool to convert web articles into social media videos, observing creator interactions with the tool.

**Key Contributions:**

	1. Demonstrated the role of AI as a creative springboard in content creation.
	2. Showed how human authority evolves when working with AI tools.
	3. Highlighted the challenges and successes of using AI in a collaborative environment.

**Result:** The AI tool allowed creators to produce content that achieved over 500,000 views, highlighting the importance of human creativity in refining AI outputs.

**Limitations:** The study is context-specific to a student newsroom and may not generalize to all industries relying on AI tools.

**Conclusion:** Human creativity remains crucial even when AI tools are utilized, as creators learn to combine AI outputs with their own creative insights.

**Abstract:** As AI becomes more capable, it is unclear how human creativity will remain essential in jobs that incorporate AI. We conducted a 14-week study of a student newsroom using an AI tool to convert web articles into social media videos. Most creators treated the tool as a creative springboard, not as a completion mechanism. They edited the AI outputs. The tool enabled the team to publish successful content that received over 500,000 views. Human creativity remained essential: after AI produced templated outputs, creators took ownership of the task, injecting their own creativity, especially when AI failed to create appropriate content. AI was initially seen as an authority, due to creators' lack of experience, but they ultimately learned to assert their own authority.

</details>


### [19] [Social Media Should Feel Like Minecraft, Not Instagram: 3D Gamer Youth Visions for Meaningful Social Connections through Fictional Inquiry](https://arxiv.org/abs/2502.06696)

*JaeWon Kim, Hyunsung Cho, Fannie Liu, Alexis Hiniker*

**Main category:** cs.HC

**Keywords:** social interactions, co-design, spatial integrity, social media, youth perspectives

**Relevance Score:** 6

**TL;DR:** This research explores youths' visions for ideal remote social interactions through co-design interviews in a 3D gaming context, highlighting their priorities for social media design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand young people's expectations for remote social interactions and their vision of ideal social media experiences.

**Method:** Co-design interviews with 23 participants aged 15-24 using a Fictional Inquiry method set in the Harry Potter universe.

**Key Contributions:**

	1. Identification of six key priorities for social media design based on user input.
	2. Introduction of the spatial integrity framework for designing immersive online environments.
	3. Highlighting the meaningfulness of the co-design process for participants.

**Result:** Participants identified six priorities for social media design, and a spatial integrity framework was introduced to guide the creation of more immersive and meaningful online spaces.

**Limitations:** 

**Conclusion:** The research emphasizes the importance of intentional design in social media to facilitate real connections and empower users to shape their experiences.

**Abstract:** We investigate youth visions for ideal remote social interactions, drawing on co-design interviews with 23 participants (aged 15-24) experienced with 3D gaming environments. Using a Fictional Inquiry (FI) method set in the Harry Potter universe, this research reveals that young people desire social media that functions more like immersive, navigable shared social spaces. Across these interviews, participants identified six key priorities for meaningful social connection over social media: intuitive social navigation, shared collaborative experiences, communal environments fostering close relationships, flexible self-presentation, intentional engagement, and playful social mechanics. We introduce the \textit{spatial integrity} framework, a set of four interrelated design principles: spatial presence, spatial composition, spatial configuration, and spatial depth. Together, these principles outline how online spaces can be designed to feel more like meaningful environments, spaces where relationships can grow through shared presence, movement, and intentional interaction. Participants also described the FI process itself as meaningful, not only for generating new ideas but for empowering them to imagine and shape the future of social media.

</details>


### [20] [When Avatars Have Personality: Effects on Engagement and Communication in Immersive Medical Training](https://arxiv.org/abs/2509.14132)

*Julia S. Dollis, Iago A. Brito, Fernanda B. Färber, Pedro S. F. B. Ribeiro, Rafael T. Sousa, Arlindo R. Galvão Filho*

**Main category:** cs.HC

**Keywords:** virtual reality, large language models, medical education, interpersonal skills, socially intelligent systems

**Relevance Score:** 9

**TL;DR:** A framework integrating LLMs into VR for training interpersonal skills, particularly in medical education, by creating coherent virtual patients with distinct personalities.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The need for psychologically plausible virtual humans in VR for training complex interpersonal skills, especially in high-stakes fields like medical education, where communication is essential.

**Method:** The study involved a mixed-method, within-subjects evaluation with licensed physicians participating in simulated consultations using the developed system.

**Key Contributions:**

	1. Introduction of a modular architecture for VR virtual patients
	2. Validation of feasibility and perceived effectiveness among physicians
	3. Identification of key design principles like the 'realism-verbosity paradox'

**Result:** The approach is feasible and perceived as a rewarding training enhancement by physicians.

**Limitations:** 

**Conclusion:** The framework provides valuable insights and design principles for creating socially intelligent VR training environments.

**Abstract:** While virtual reality (VR) excels at simulating physical environments, its effectiveness for training complex interpersonal skills is limited by a lack of psychologically plausible virtual humans. This is a critical gap in high-stakes domains like medical education, where communication is a core competency. This paper introduces a framework that integrates large language models (LLMs) into immersive VR to create medically coherent virtual patients with distinct, consistent personalities, built on a modular architecture that decouples personality from clinical data. We evaluated our system in a mixed-method, within-subjects study with licensed physicians who engaged in simulated consultations. Results demonstrate that the approach is not only feasible but is also perceived by physicians as a highly rewarding and effective training enhancement. Furthermore, our analysis uncovers critical design principles, including a ``realism-verbosity paradox" where less communicative agents can seem more artificial, and the need for challenges to be perceived as authentic to be instructive. This work provides a validated framework and key insights for developing the next generation of socially intelligent VR training environments.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [Gender-Neutral Rewriting in Italian: Models, Approaches, and Trade-offs](https://arxiv.org/abs/2509.13480)

*Andrea Piergentili, Beatrice Savoldi, Matteo Negri, Luisa Bentivogli*

**Main category:** cs.CL

**Keywords:** gender-neutral rewriting, large language models, Italian, semantic fidelity, neutrality

**Relevance Score:** 8

**TL;DR:** This paper evaluates large language models for gender-neutral rewriting (GNR) in Italian, introducing a framework to measure fairness and semantics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To eliminate unnecessary gender specifications in Italian text, enhancing inclusivity without losing meaning.

**Method:** Systematic evaluation of state-of-the-art LLMs for GNR with few-shot prompting, model fine-tuning, and targeted data cleaning.

**Key Contributions:**

	1. Introduced a novel framework for evaluating GNR in Italian
	2. Showcased the superior performance of open-weight LLMs over dedicated GNR models
	3. Demonstrated the effectiveness of fine-tuning for specific GNR tasks

**Result:** Open-weight LLMs outperform existing models for Italian GNR, with fine-tuned models matching or exceeding performance at a reduced size.

**Limitations:** 

**Conclusion:** Optimizing training data presents trade-offs between neutrality and meaning preservation in GNR tasks.

**Abstract:** Gender-neutral rewriting (GNR) aims to reformulate text to eliminate unnecessary gender specifications while preserving meaning, a particularly challenging task in grammatical-gender languages like Italian. In this work, we conduct the first systematic evaluation of state-of-the-art large language models (LLMs) for Italian GNR, introducing a two-dimensional framework that measures both neutrality and semantic fidelity to the input. We compare few-shot prompting across multiple LLMs, fine-tune selected models, and apply targeted cleaning to boost task relevance. Our findings show that open-weight LLMs outperform the only existing model dedicated to GNR in Italian, whereas our fine-tuned models match or exceed the best open-weight LLM's performance at a fraction of its size. Finally, we discuss the trade-off between optimizing the training data for neutrality and meaning preservation.

</details>


### [22] [Op-Fed: Opinion, Stance, and Monetary Policy Annotations on FOMC Transcripts Using Active Learning](https://arxiv.org/abs/2509.13539)

*Alisa Kanganis, Katherine A. Keith*

**Main category:** cs.CL

**Keywords:** dataset, monetary policy, opinion classification, active learning, language model

**Relevance Score:** 2

**TL;DR:** A dataset called Op-Fed is released, containing 1044 annotated sentences from FOMC transcripts, addressing challenges in opinion and stance classification related to monetary policy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding and classification of sentiments in monetary policy discussions that affect public financial decisions.

**Method:** Developed a five-stage hierarchical schema to tackle imbalanced classes and inter-sentence dependence, employing active learning to improve positive instance annotation.

**Key Contributions:**

	1. Release of a comprehensive dataset for sentiment analysis in monetary policy discussions
	2. Development of a hierarchical schema addressing classification challenges
	3. Demonstration of LLM performance against human baselines in a new domain

**Result:** The dataset allowed a closed-weight LLM to achieve 0.80 zero-shot accuracy in opinion classification, but only 0.61 in stance classification, significantly below the human baseline of 0.89.

**Limitations:** The dataset is limited to the FOMC transcripts and may not generalize to other types of texts or contexts.

**Conclusion:** Op-Fed can aid in model training, calibrating confidence, and serve as a foundation for future sentiment annotation tasks in monetary policy.

**Abstract:** The U.S. Federal Open Market Committee (FOMC) regularly discusses and sets monetary policy, affecting the borrowing and spending decisions of millions of people. In this work, we release Op-Fed, a dataset of 1044 human-annotated sentences and their contexts from FOMC transcripts. We faced two major technical challenges in dataset creation: imbalanced classes -- we estimate fewer than 8% of sentences express a non-neutral stance towards monetary policy -- and inter-sentence dependence -- 65% of instances require context beyond the sentence-level. To address these challenges, we developed a five-stage hierarchical schema to isolate aspects of opinion, monetary policy, and stance towards monetary policy as well as the level of context needed. Second, we selected instances to annotate using active learning, roughly doubling the number of positive instances across all schema aspects. Using Op-Fed, we found a top-performing, closed-weight LLM achieves 0.80 zero-shot accuracy in opinion classification but only 0.61 zero-shot accuracy classifying stance towards monetary policy -- below our human baseline of 0.89. We expect Op-Fed to be useful for future model training, confidence calibration, and as a seed dataset for future annotation efforts.

</details>


### [23] [Overview of Dialog System Evaluation Track: Dimensionality, Language, Culture and Safety at DSTC 12](https://arxiv.org/abs/2509.13569)

*John Mendonça, Lining Zhang, Rahul Mallidi, Alon Lavie, Isabel Trancoso, Luis Fernando D'Haro, João Sedoc*

**Main category:** cs.CL

**Keywords:** dialogue systems, evaluation metrics, cultural safety, multilingual, Large Language Models

**Relevance Score:** 9

**TL;DR:** This paper reviews the DSTC12 Track 1 on dialogue system evaluation, focusing on dimensionality, language, culture, and safety in dialogue systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inadequacies of traditional dialogue evaluation metrics and highlights the increasing importance of culturally aware and safe dialogue systems due to the growth of LLMs.

**Method:** The evaluation encompasses two tasks: 1) Dialogue-level, Multi-dimensional Automatic Evaluation Metrics, and 2) Multilingual and Multicultural Safety Detection, with emphasis on the performance of a Llama-3-8B baseline against various metrics.

**Key Contributions:**

	1. Overview of dialogue system evaluation methodologies
	2. Results from multi-dimensional evaluation metrics
	3. Insights on the need for culturally-aware safety detection

**Result:** The Llama-3-8B baseline achieved a Spearman's correlation of 0.1681 in Task 1, indicating room for improvement, while in Task 2, teams outperformed a Llama-Guard-3-1B baseline with a top ROC-AUC of 0.9648 for the multilingual safety subset.

**Limitations:** The evaluation metrics still showed substantial room for improvement, particularly in dialogue-level metrics.

**Conclusion:** Results underline the need for better dialogue evaluation methods and culturally aware safety measures, with significant variation in baseline performances across tasks.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has intensified the need for robust dialogue system evaluation, yet comprehensive assessment remains challenging. Traditional metrics often prove insufficient, and safety considerations are frequently narrowly defined or culturally biased. The DSTC12 Track 1, "Dialog System Evaluation: Dimensionality, Language, Culture and Safety," is part of the ongoing effort to address these critical gaps. The track comprised two subtasks: (1) Dialogue-level, Multi-dimensional Automatic Evaluation Metrics, and (2) Multilingual and Multicultural Safety Detection. For Task 1, focused on 10 dialogue dimensions, a Llama-3-8B baseline achieved the highest average Spearman's correlation (0.1681), indicating substantial room for improvement. In Task 2, while participating teams significantly outperformed a Llama-Guard-3-1B baseline on the multilingual safety subset (top ROC-AUC 0.9648), the baseline proved superior on the cultural subset (0.5126 ROC-AUC), highlighting critical needs in culturally-aware safety. This paper describes the datasets and baselines provided to participants, as well as submission evaluation results for each of the two proposed subtasks.

</details>


### [24] [Latent Traits and Cross-Task Transfer: Deconstructing Dataset Interactions in LLM Fine-tuning](https://arxiv.org/abs/2509.13624)

*Shambhavi Krishna, Atharva Naik, Chaitali Agarwal, Sudharshan Govindan, Taesung Lee, Haw-Shiuan Chang*

**Main category:** cs.CL

**Keywords:** transfer learning, large language models, cross-task interactions, latent abilities, statistical factors

**Relevance Score:** 9

**TL;DR:** This paper presents an analysis framework for transfer learning in large language models (LLMs), uncovering hidden factors that influence performance across diverse tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to adapt large language models to a wide range of tasks without having comprehensive training data for each one.

**Method:** An analysis framework utilizing a transfer learning matrix and dimensionality reduction to evaluate cross-task interactions in LLMs, along with training and analyzing 10 models to identify latent abilities.

**Key Contributions:**

	1. Developed an analysis framework for LLM transfer learning
	2. Identified latent abilities of models
	3. Revealed hidden statistical factors affecting performance

**Result:** The findings demonstrate that performance improvements cannot be solely explained by dataset similarity or quality; instead, underlying statistical factors like class distribution and linguistic features play a crucial role.

**Limitations:** 

**Conclusion:** The study offers valuable insights into the dynamics of transfer learning, enhancing LLM adaptability across various tasks.

**Abstract:** Large language models are increasingly deployed across diverse applications. This often includes tasks LLMs have not encountered during training. This implies that enumerating and obtaining the high-quality training data for all tasks is infeasible. Thus, we often need to rely on transfer learning using datasets with different characteristics, and anticipate out-of-distribution requests. Motivated by this practical need, we propose an analysis framework, building a transfer learning matrix and dimensionality reduction, to dissect these cross-task interactions. We train and analyze 10 models to identify latent abilities (e.g., Reasoning, Sentiment Classification, NLU, Arithmetic) and discover the side effects of the transfer learning. Our findings reveal that performance improvements often defy explanations based on surface-level dataset similarity or source data quality. Instead, hidden statistical factors of the source dataset, such as class distribution and generation length proclivities, alongside specific linguistic features, are actually more influential. This work offers insights into the complex dynamics of transfer learning, paving the way for more predictable and effective LLM adaptation.

</details>


### [25] [Sparse Neurons Carry Strong Signals of Question Ambiguity in LLMs](https://arxiv.org/abs/2509.13664)

*Zhuoxuan Zhang, Jinhao Duan, Edward Kim, Kaidi Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Ambiguity Detection, Neural Networks

**Relevance Score:** 9

**TL;DR:** The paper investigates how large language models (LLMs) encode ambiguity in questions and how this encoding can be detected and controlled at the neuron level.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs handle ambiguity in natural language questions is crucial for improving their interpretability and control in applications.

**Method:** The authors identify Ambiguity-Encoding Neurons (AENs) in LLMs that encode question ambiguity, and investigate their detection and manipulation through layerwise analysis.

**Key Contributions:**

	1. Identification of Ambiguity-Encoding Neurons in LLMs
	2. Development of effective probes for ambiguity detection
	3. Demonstration of behavior control in LLMs through manipulation of AENs

**Result:** Probes trained on these AENs effectively detect ambiguity and outperform existing methods in various contexts.

**Limitations:** 

**Conclusion:** LLMs contain compact internal representations of question ambiguity, allowing for interpretable and controllable responses to ambiguous queries.

**Abstract:** Ambiguity is pervasive in real-world questions, yet large language models (LLMs) often respond with confident answers rather than seeking clarification. In this work, we show that question ambiguity is linearly encoded in the internal representations of LLMs and can be both detected and controlled at the neuron level. During the model's pre-filling stage, we identify that a small number of neurons, as few as one, encode question ambiguity information. Probes trained on these Ambiguity-Encoding Neurons (AENs) achieve strong performance on ambiguity detection and generalize across datasets, outperforming prompting-based and representation-based baselines. Layerwise analysis reveals that AENs emerge from shallow layers, suggesting early encoding of ambiguity signals in the model's processing pipeline. Finally, we show that through manipulating AENs, we can control LLM's behavior from direct answering to abstention. Our findings reveal that LLMs form compact internal representations of question ambiguity, enabling interpretable and controllable behavior.

</details>


### [26] [CL$^2$GEC: A Multi-Discipline Benchmark for Continual Learning in Chinese Literature Grammatical Error Correction](https://arxiv.org/abs/2509.13672)

*Shang Qin, Jingheng Ye, Yinghui Li, Hai-Tao Zheng, Qi Li, Jinxiao Shan, Zhixing Li, Hong-Gee Kim*

**Main category:** cs.CL

**Keywords:** CGEC, continual learning, grammatical error correction, large language models, benchmark

**Relevance Score:** 4

**TL;DR:** The paper introduces CL$^2$GEC, the first Continual Learning benchmark for Chinese Grammatical Error Correction, designed to evaluate error correction across multiple academic disciplines with distinct linguistic styles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a demand for automated writing assistance tailored to diverse academic domains, but existing systems lack benchmarks for multi-disciplinary applications, particularly in handling linguistic variation through continual learning.

**Method:** The authors developed a benchmark that includes 10,000 human-annotated sentences across 10 disciplines, simulating sequential exposure to diverse styles in a continual learning setting. Large language models are evaluated using standard and continual learning metrics, including tuning and adaptation methods.

**Key Contributions:**

	1. Introduction of the CL$^2$GEC benchmark for CGEC in multi-disciplinary contexts.
	2. Evaluation of large language models using continual learning metrics.
	3. Demonstration of the effectiveness of regularization methods in mitigating forgetting.

**Result:** Experimental results demonstrated that regularization-based methods are more effective at preventing forgetting compared to other methods, providing insight into the adaptability of error correction systems.

**Limitations:** 

**Conclusion:** The CL$^2$GEC benchmark lays the groundwork for advancing research in adaptive grammatical error correction across various academic fields, enabling better writing assistance tools.

**Abstract:** The growing demand for automated writing assistance in diverse academic domains highlights the need for robust Chinese Grammatical Error Correction (CGEC) systems that can adapt across disciplines. However, existing CGEC research largely lacks dedicated benchmarks for multi-disciplinary academic writing, overlooking continual learning (CL) as a promising solution to handle domain-specific linguistic variation and prevent catastrophic forgetting. To fill this crucial gap, we introduce CL$^2$GEC, the first Continual Learning benchmark for Chinese Literature Grammatical Error Correction, designed to evaluate adaptive CGEC across multiple academic fields. Our benchmark includes 10,000 human-annotated sentences spanning 10 disciplines, each exhibiting distinct linguistic styles and error patterns. CL$^2$GEC focuses on evaluating grammatical error correction in a continual learning setting, simulating sequential exposure to diverse academic disciplines to reflect real-world editorial dynamics. We evaluate large language models under sequential tuning, parameter-efficient adaptation, and four representative CL algorithms, using both standard GEC metrics and continual learning metrics adapted to task-level variation. Experimental results reveal that regularization-based methods mitigate forgetting more effectively than replay-based or naive sequential approaches. Our benchmark provides a rigorous foundation for future research in adaptive grammatical error correction across diverse academic domains.

</details>


### [27] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)

*Xinxu Zhou, Jiaqi Bai, Zhenqi Sun, Fanxiang Zeng, Yue Liu*

**Main category:** cs.CL

**Keywords:** Controlled Text Generation, Multi-Agent Workflows, Natural Language Processing, Character-Driven Rewriting, Text Generation

**Relevance Score:** 8

**TL;DR:** AgentCTG is a novel framework for Controlled Text Generation that improves precise control in text generation through multi-agent workflows and introduces an auto-prompt module, achieving state-of-the-art results.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in NLP, Controlled Text Generation faces challenges in achieving fine control and scalability, necessitating a scalable solution for better text generation.

**Method:** The paper introduces AgentCTG, which leverages multi-agent workflows for simulation of control mechanisms and an auto-prompt module to enhance generation effectiveness.

**Key Contributions:**

	1. Introduction of AgentCTG framework for improved text generation control
	2. Implementation of multi-agent workflows for enhanced collaboration
	3. Development of a Character-Driven Rewriting task.

**Result:** AgentCTG achieves state-of-the-art performance on several public datasets and significantly improves online navigation experiences in role-playing applications.

**Limitations:** 

**Conclusion:** The proposed framework enhances text generation control, improves user engagement, and offers a personalized experience in online communities.

**Abstract:** Although significant progress has been made in many tasks within the field of Natural Language Processing (NLP), Controlled Text Generation (CTG) continues to face numerous challenges, particularly in achieving fine-grained conditional control over generation. Additionally, in real scenario and online applications, cost considerations, scalability, domain knowledge learning and more precise control are required, presenting more challenge for CTG. This paper introduces a novel and scalable framework, AgentCTG, which aims to enhance precise and complex control over the text generation by simulating the control and regulation mechanisms in multi-agent workflows. We explore various collaboration methods among different agents and introduce an auto-prompt module to further enhance the generation effectiveness. AgentCTG achieves state-of-the-art results on multiple public datasets. To validate its effectiveness in practical applications, we propose a new challenging Character-Driven Rewriting task, which aims to convert the original text into new text that conform to specific character profiles and simultaneously preserve the domain knowledge. When applied to online navigation with role-playing, our approach significantly enhances the driving experience through improved content delivery. By optimizing the generation of contextually relevant text, we enable a more immersive interaction within online communities, fostering greater personalization and user engagement.

</details>


### [28] [Improving Context Fidelity via Native Retrieval-Augmented Reasoning](https://arxiv.org/abs/2509.13683)

*Suyuchen Wang, Jinlin Wang, Xinyu Wang, Shiqi Li, Xiangru Tang, Sirui Hong, Xiao-Wen Chang, Chenglin Wu, Bang Liu*

**Main category:** cs.CL

**Keywords:** large language models, context fidelity, retrieval-augmented reasoning, evidence integration, QA benchmarks

**Relevance Score:** 9

**TL;DR:** CARE is a novel framework that enhances LLM context fidelity by integrating in-context evidence during reasoning without requiring extensive labeled data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the consistency and accuracy of LLM responses by enabling them to effectively use provided context during reasoning.

**Method:** CARE teaches LLMs to utilize their own retrieval capabilities to integrate in-context evidence directly within their reasoning processes.

**Key Contributions:**

	1. Proposes a framework (CARE) for enhanced context integration in LLMs
	2. Shows significant improvements over existing fine-tuning and retrieval methods
	3. Demonstrates effectiveness across real-world and counterfactual QA benchmarks

**Result:** CARE significantly enhances retrieval accuracy and answer generation performance, outpacing traditional methods and supervised fine-tuning on various QA benchmarks.

**Limitations:** 

**Conclusion:** The CARE framework marks a significant improvement in making large language models more accurate, reliable, and efficient for knowledge-intensive applications.

**Abstract:** Large language models (LLMs) often struggle with context fidelity, producing inconsistent answers when responding to questions based on provided information. Existing approaches either rely on expensive supervised fine-tuning to generate evidence post-answer or train models to perform web searches without necessarily improving utilization of the given context. We propose CARE, a novel native retrieval-augmented reasoning framework that teaches LLMs to explicitly integrate in-context evidence within their reasoning process with the model's own retrieval capabilities. Our method requires limited labeled evidence data while significantly enhancing both retrieval accuracy and answer generation performance through strategically retrieved in-context tokens in the reasoning chain. Extensive experiments on multiple real-world and counterfactual QA benchmarks demonstrate that our approach substantially outperforms supervised fine-tuning, traditional retrieval-augmented generation methods, and external retrieval solutions. This work represents a fundamental advancement in making LLMs more accurate, reliable, and efficient for knowledge-intensive tasks.

</details>


### [29] [Can Large Language Models Robustly Perform Natural Language Inference for Japanese Comparatives?](https://arxiv.org/abs/2509.13695)

*Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Large Language Models, Comparatives, Japanese, Zero-shot Learning

**Relevance Score:** 7

**TL;DR:** This paper explores the performance of Large Language Models (LLMs) in Natural Language Inference (NLI) with a focus on comparatives in Japanese, highlighting their challenges and the impact of prompt formats.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the robustness of LLMs in handling NLI tasks involving numerical and logical expressions, particularly in non-dominant languages like Japanese.

**Method:** The authors constructed a Japanese NLI dataset centering on comparatives and assessed various LLMs in zero-shot and few-shot scenarios.

**Key Contributions:**

	1. Creation of a Japanese NLI dataset focusing on comparatives
	2. Demonstration of LLMs' sensitivity to prompt formats and few-shot examples
	3. Identification of unique linguistic challenges in Japanese NLI.

**Result:** The findings indicate that model performance is highly sensitive to prompt formats in zero-shot conditions and affected by gold labels in few-shot contexts. LLMs had difficulty with linguistic phenomena unique to Japanese, though prompts with logical semantic representations improved performance.

**Limitations:** The study primarily focuses on Japanese, limiting its applicability to other languages. Further exploration of different prompt formats and their effects is needed.

**Conclusion:** The study highlights significant challenges faced by LLMs in NLI tasks for Japanese and suggests that prompt design can enhance their inferential capabilities.

**Abstract:** Large Language Models (LLMs) perform remarkably well in Natural Language Inference (NLI). However, NLI involving numerical and logical expressions remains challenging. Comparatives are a key linguistic phenomenon related to such inference, but the robustness of LLMs in handling them, especially in languages that are not dominant in the models' training data, such as Japanese, has not been sufficiently explored. To address this gap, we construct a Japanese NLI dataset that focuses on comparatives and evaluate various LLMs in zero-shot and few-shot settings. Our results show that the performance of the models is sensitive to the prompt formats in the zero-shot setting and influenced by the gold labels in the few-shot examples. The LLMs also struggle to handle linguistic phenomena unique to Japanese. Furthermore, we observe that prompts containing logical semantic representations help the models predict the correct labels for inference problems that they struggle to solve even with few-shot examples.

</details>


### [30] [Integrating Text and Time-Series into (Large) Language Models to Predict Medical Outcomes](https://arxiv.org/abs/2509.13696)

*Iyadh Ben Cheikh Larbi, Ajay Madhavan Ravichandran, Aljoscha Burchardt, Roland Roller*

**Main category:** cs.CL

**Keywords:** large language models, clinical classification, structured data, EHR, prompt optimization

**Relevance Score:** 9

**TL;DR:** This work explores the use of instruction-tuned LLMs for clinical classification tasks, integrating clinical notes and structured EHR data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the potential of LLMs in handling clinical classification tasks with structured data, an area that has been largely unexplored.

**Method:** We implemented DSPy-based prompt optimization to adapt instruction-tuned LLMs for processing clinical notes along with structured Electronic Health Record (EHR) inputs.

**Key Contributions:**

	1. Adopting instruction-tuned LLMs to address structured data in clinical settings
	2. Demonstrating comparable performance to multimodal systems with lower complexity
	3. Implementing DSPy-based prompt optimization for improved adaptability

**Result:** The adapted LLMs achieved performance comparable to specialized multimodal systems, while maintaining lower complexity and enhanced adaptability across various tasks.

**Limitations:** 

**Conclusion:** The findings suggest that LLMs can effectively support clinical classification tasks with both qualitative and quantitative data.

**Abstract:** Large language models (LLMs) excel at text generation, but their ability to handle clinical classification tasks involving structured data, such as time series, remains underexplored. In this work, we adapt instruction-tuned LLMs using DSPy-based prompt optimization to process clinical notes and structured EHR inputs jointly. Our results show that this approach achieves performance on par with specialized multimodal systems while requiring less complexity and offering greater adaptability across tasks.

</details>


### [31] [DSCC-HS: A Dynamic Self-Reinforcing Framework for Hallucination Suppression in Large Language Models](https://arxiv.org/abs/2509.13702)

*Xiao Zheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination Suppression, Dynamic Calibration, Factual Consistency, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces DSCC-HS, a new approach for suppressing hallucinations in LLMs by dynamically calibrating model outputs during decoding.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Hallucinations in large language models hinder their reliable use in applications, prompting the need for more proactive solutions.

**Method:** The authors developed Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS), which utilizes a compact proxy model to adjust outputs in real-time based on adversarial training.

**Key Contributions:**

	1. Introduction of a proactive framework for hallucination suppression in LLMs.
	2. Utilization of dual-process cognitive theory for real-time output adjustment.
	3. Achievement of state-of-the-art results in benchmarks like TruthfulQA and BioGEN.

**Result:** DSCC-HS achieved state-of-the-art performance on both TruthfulQA (99.2% Factual Consistency Rate) and BioGEN (highest FActScore of 46.50).

**Limitations:** 

**Conclusion:** The approach effectively improves the factual accuracy of large language models without altering their architecture.

**Abstract:** Large Language Model (LLM) hallucination is a significant barrier to their reliable deployment. Current methods like Retrieval-Augmented Generation (RAG) are often reactive. We introduce **Dynamic Self-reinforcing Calibration for Hallucination Suppression (DSCC-HS)**, a novel, proactive framework that intervenes during autoregressive decoding. Inspired by dual-process cognitive theory, DSCC-HS uses a compact proxy model, trained in adversarial roles as a Factual Alignment Proxy (FAP) and a Hallucination Detection Proxy (HDP). During inference, these proxies dynamically steer a large target model by injecting a real-time steering vector, which is the difference between FAP and HDP logits, at each decoding step. This plug-and-play approach requires no modification to the target model. Our experiments on TruthfulQA and BioGEN show DSCC-HS achieves state-of-the-art performance. On TruthfulQA, it reached a 99.2% Factual Consistency Rate (FCR). On the long-form BioGEN benchmark, it attained the highest FActScore of 46.50. These results validate DSCC-HS as a principled and efficient solution for enhancing LLM factuality.

</details>


### [32] [Automated Triaging and Transfer Learning of Incident Learning Safety Reports Using Large Language Representational Models](https://arxiv.org/abs/2509.13706)

*Peter Beidler, Mark Nguyen, Kevin Lybarger, Ola Holmberg, Eric Ford, John Kang*

**Main category:** cs.CL

**Keywords:** NLP, incident reports, radiation oncology, BlueBERT, healthcare

**Relevance Score:** 9

**TL;DR:** The paper presents an NLP screening tool for detecting high-severity incident reports in radiation oncology, showing that models like BlueBERT can achieve performance similar to human reviewers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of reviewing incident reports in healthcare, which is currently a manual and time-intensive process requiring expert knowledge.

**Method:** Two types of NLP models were trained and evaluated: support vector machines (SVM) and BlueBERT, using incident report datasets from one institution and the IAEA SAFRON database.

**Key Contributions:**

	1. Development of NLP models for incident report analysis in radiation oncology.
	2. Introduction of a transfer learning approach with BlueBERT for improved performance across institutions.
	3. Demonstration of NLP model effectiveness in comparison to human performance on curated datasets.

**Result:** The SVM and BlueBERT models achieved AUROC scores of 0.82 and 0.81 respectively on the institution's test set, while the BlueBERT_TRANSFER enhanced cross-institution performance to an AUROC of 0.78.

**Limitations:** Performance on external datasets without transfer learning was limited, indicating a need for further refinement.

**Conclusion:** Cross-institution NLP models developed for incident report analysis can detect high-severity reports with performance comparable to human experts.

**Abstract:** PURPOSE: Incident reports are an important tool for safety and quality improvement in healthcare, but manual review is time-consuming and requires subject matter expertise. Here we present a natural language processing (NLP) screening tool to detect high-severity incident reports in radiation oncology across two institutions.   METHODS AND MATERIALS: We used two text datasets to train and evaluate our NLP models: 7,094 reports from our institution (Inst.), and 571 from IAEA SAFRON (SF), all of which had severity scores labeled by clinical content experts. We trained and evaluated two types of models: baseline support vector machines (SVM) and BlueBERT which is a large language model pretrained on PubMed abstracts and hospitalized patient data. We assessed for generalizability of our model in two ways. First, we evaluated models trained using Inst.-train on SF-test. Second, we trained a BlueBERT_TRANSFER model that was first fine-tuned on Inst.-train then on SF-train before testing on SF-test set. To further analyze model performance, we also examined a subset of 59 reports from our Inst. dataset, which were manually edited for clarity.   RESULTS Classification performance on the Inst. test achieved AUROC 0.82 using SVM and 0.81 using BlueBERT. Without cross-institution transfer learning, performance on the SF test was limited to an AUROC of 0.42 using SVM and 0.56 using BlueBERT. BlueBERT_TRANSFER, which was fine-tuned on both datasets, improved the performance on SF test to AUROC 0.78. Performance of SVM, and BlueBERT_TRANSFER models on the manually curated Inst. reports (AUROC 0.85 and 0.74) was similar to human performance (AUROC 0.81).   CONCLUSION: In summary, we successfully developed cross-institution NLP models on incident report text from radiation oncology centers. These models were able to detect high-severity reports similarly to humans on a curated dataset.

</details>


### [33] [DSPC: Dual-Stage Progressive Compression Framework for Efficient Long-Context Reasoning](https://arxiv.org/abs/2509.13723)

*Yaxin Gao, Yao Lu, Zongfei Zhang, Jiaqi Nie, Shanqing Yu, Qi Xuan*

**Main category:** cs.CL

**Keywords:** large language models, prompt compression, natural language processing, DSPC, machine learning

**Relevance Score:** 8

**TL;DR:** The paper presents Dual-Stage Progressive Compression (DSPC), a training-free method for compressing prompts in large language models to reduce computational costs without sacrificing performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issue of prompt inflation in large language models which increases computational costs while trying to improve output accuracy.

**Method:** DSPC employs a two-stage approach: the coarse-grained stage filters low-value sentences based on TF-IDF, and the fine-grained stage prunes low-utility tokens based on attention contribution, loss difference, and positional importance.

**Key Contributions:**

	1. Introduction of a training-free prompt compression method (DSPC)
	2. Dual-stage approach: semantic filtering and token pruning
	3. Significant performance improvements on standard benchmarks with reduced token requirements.

**Result:** DSPC demonstrates consistent improvements on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo, achieving a performance score of 49.17 on the FewShot task of the Longbench dataset using 3x fewer tokens compared to existing methods.

**Limitations:** 

**Conclusion:** The DSPC method effectively reduces token usage while maintaining or improving the performance of language models, thus addressing prompt inflation without additional training costs.

**Abstract:** Large language models (LLMs) have achieved remarkable success in many natural language processing (NLP) tasks. To achieve more accurate output, the prompts used to drive LLMs have become increasingly longer, which incurs higher computational costs. To address this prompt inflation problem, prompt compression has been proposed. However, most existing methods require training a small auxiliary model for compression, incurring a significant amount of additional computation. To avoid this, we propose a two-stage, training-free approach, called Dual-Stage Progressive Compression (DSPC). In the coarse-grained stage, semantic-related sentence filtering removes sentences with low semantic value based on TF-IDF. In the fine-grained stage, token importance is assessed using attention contribution, cross-model loss difference, and positional importance, enabling the pruning of low-utility tokens while preserving semantics. We validate DSPC on LLaMA-3.1-8B-Instruct and GPT-3.5-Turbo under a constrained token budget and observe consistent improvements. For instance, in the FewShot task of the Longbench dataset, DSPC achieves a performance of 49.17 by using only 3x fewer tokens, outperforming the best state-of-the-art baseline LongLLMLingua by 7.76.

</details>


### [34] [Implementing a Logical Inference System for Japanese Comparatives](https://arxiv.org/abs/2509.13734)

*Yosuke Mikami, Daiki Matsuoka, Hitomi Yanaka*

**Main category:** cs.CL

**Keywords:** Natural Language Inference, Japanese, comparatives, large language models, compositional semantics

**Relevance Score:** 6

**TL;DR:** This paper presents ccg-jcomp, a logical inference system designed for Natural Language Inference (NLI) involving Japanese comparatives, evaluating its performance against existing LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in Natural Language Inference (NLI) with Japanese comparatives, which differ significantly from English comparatives.

**Method:** The study proposes a logic-based approach using compositional semantics to create ccg-jcomp, specifically designed for handling Japanese comparative expressions.

**Key Contributions:**

	1. Development of a novel logical inference system (ccg-jcomp) for Japanese comparatives.
	2. Evaluation against existing LLMs to highlight effectiveness and accuracy.
	3. Addressing morphological and semantic differences between Japanese and English comparatives.

**Result:** The proposed ccg-jcomp system demonstrates superior accuracy in NLI tasks involving Japanese comparatives when compared to existing Large Language Models (LLMs).

**Limitations:** The focus is solely on Japanese comparatives, limiting applicability to other languages or comparative forms.

**Conclusion:** ccg-jcomp effectively addresses the limitations of applying existing logical inference systems designed for English to Japanese, providing a robust solution for NLI in Japanese.

**Abstract:** Natural Language Inference (NLI) involving comparatives is challenging because it requires understanding quantities and comparative relations expressed by sentences. While some approaches leverage Large Language Models (LLMs), we focus on logic-based approaches grounded in compositional semantics, which are promising for robust handling of numerical and logical expressions. Previous studies along these lines have proposed logical inference systems for English comparatives. However, it has been pointed out that there are several morphological and semantic differences between Japanese and English comparatives. These differences make it difficult to apply such systems directly to Japanese comparatives. To address this gap, this study proposes ccg-jcomp, a logical inference system for Japanese comparatives based on compositional semantics. We evaluate the proposed system on a Japanese NLI dataset containing comparative expressions. We demonstrate the effectiveness of our system by comparing its accuracy with that of existing LLMs.

</details>


### [35] [Exploring Data and Parameter Efficient Strategies for Arabic Dialect Identifications](https://arxiv.org/abs/2509.13775)

*Vani Kanjirangat, Ljiljana Dolamic, Fabio Rinaldi*

**Main category:** cs.CL

**Keywords:** Arabic Dialect Identification, Large Language Models, soft prompting strategies, parameter-efficient tuning, LoRA

**Relevance Score:** 7

**TL;DR:** This paper explores data-efficient and parameter-efficient strategies for Arabic Dialect Identification (ADI) using Large Language Models (LLMs), focusing on soft-prompting and few-shot inferences.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the identification of Arabic dialects using LLMs through data-efficient and parameter-efficient methodologies, which is crucial given the nuanced nature of Arabic dialects.

**Method:** The researchers investigated various soft-prompting strategies (prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2), as well as LoRA reparameterizations, and conducted experiments using Arabic-specific encoder models on major datasets while analyzing n-shot inferences on different multilingual models.

**Key Contributions:**

	1. Investigation of various soft-prompting strategies for Arabic dialect identification
	2. Comparison of performance between hard prompting and soft prompting methodologies
	3. Demonstration of superiority of LoRA-based models over traditional fine-tuning approaches

**Result:** The results indicated that LLMs face challenges in distinguishing dialectal nuances in few-shot or zero-shot scenarios, but soft-prompted encoder variants showed improved performance, with LoRA-based fine-tuned models achieving the best results, even outperforming full fine-tuning.

**Limitations:** The models still exhibit struggles in distinguishing dialects in few-shot and zero-shot conditions, indicating a need for further improvements.

**Conclusion:** The study concludes that while LLMs struggle with Arabic dialect identification under limited data conditions, the use of efficient prompting techniques and architectures can enhance performance significantly.

**Abstract:** This paper discusses our exploration of different data-efficient and parameter-efficient approaches to Arabic Dialect Identification (ADI). In particular, we investigate various soft-prompting strategies, including prefix-tuning, prompt-tuning, P-tuning, and P-tuning V2, as well as LoRA reparameterizations. For the data-efficient strategy, we analyze hard prompting with zero-shot and few-shot inferences to analyze the dialect identification capabilities of Large Language Models (LLMs). For the parameter-efficient PEFT approaches, we conducted our experiments using Arabic-specific encoder models on several major datasets. We also analyzed the n-shot inferences on open-source decoder-only models, a general multilingual model (Phi-3.5), and an Arabic-specific one(SILMA). We observed that the LLMs generally struggle to differentiate the dialectal nuances in the few-shot or zero-shot setups. The soft-prompted encoder variants perform better, while the LoRA-based fine-tuned models perform best, even surpassing full fine-tuning.

</details>


### [36] [Teaching According to Talents! Instruction Tuning LLMs with Competence-Aware Curriculum Learning](https://arxiv.org/abs/2509.13790)

*Yangning Li, Tingwei Lu, Yinghui Li, Yankai Chen, Wei-Chieh Huang, Wenhao Jiang, Hui Wang, Hai-Tao Zheng, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** Instruction tuning, Curriculum learning, Large language models, Adaptive learning, Competency-aware tuning

**Relevance Score:** 8

**TL;DR:** CAMPUS is a novel framework for dynamic instruction tuning of LLMs, addressing the limitations of static curriculum methods by adapting to evolving model capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of large language models in instruction tuning, addressing the shortcomings of static curriculum tuning methods that do not adapt to model progress.

**Method:** CAMPUS introduces a dynamic selection of sub-curriculum, enables competency-aware adjustments to the curriculum schedule, and implements multiple difficulty-based scheduling techniques.

**Key Contributions:**

	1. Dynamic selection for sub-curriculum
	2. Competency-aware curriculum schedule adjustments
	3. Multiple difficulty-based scheduling

**Result:** Extensive experiments demonstrate that CAMPUS outperforms existing state-of-the-art methods in instruction tuning efficiency.

**Limitations:** 

**Conclusion:** CAMPUS provides a more adaptive and effective approach for instruction tuning, surpassing traditional static methods in performance.

**Abstract:** Efficient instruction tuning aims to enhance the ultimate performance of large language models (LLMs) trained on a given instruction dataset. Curriculum learning as a typical data organization strategy has shown preliminary effectiveness in instruction tuning. However, current curriculum tuning methods suffer from the curriculum rigidity, since they rely solely on static heuristic difficulty metrics. These methods fail to adapt to the evolving capabilities of models during training, resulting in a fixed and potentially sub-optimal learning trajectory. To address the issue, Competence-Aware Multi-Perspective cUrriculum inStruction tuning framework termed CAMPUS is proposed. CAMPUS offers several advantages: (1) Dynamic selection for sub-curriculum. (2) Competency-aware adjustment to the curriculum schedule. (3) Multiple difficulty-based scheduling. Extensive experiments prove the superior performance of CAMPUS, compared to other state-of-the-art baselines for efficient instruction tuning.

</details>


### [37] [Measuring Gender Bias in Job Title Matching for Grammatical Gender Languages](https://arxiv.org/abs/2509.13803)

*Laura García-Sardiña, Hermenegildo Fabregat, Daniel Deniz, Rabih Zbib*

**Main category:** cs.CL

**Keywords:** gender bias, job ranking, RBO, multilingual models, grammatical gender

**Relevance Score:** 6

**TL;DR:** This paper investigates how grammatical gender in job titles impacts automatic job ranking systems, proposing a methodology to evaluate and compare gender bias.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address gender bias in automatic job ranking systems caused by grammatical gender assignment in job titles.

**Method:** The paper introduces metrics for ranking comparison controlling for gender, specifically using RBO (Rank-Biased Overlap), and generates test sets for a job title matching task across four grammatical gender languages.

**Key Contributions:**

	1. Introduction of RBO for gender bias evaluation in job titles
	2. Creation of test sets for job title matching in multiple languages
	3. Empirical evaluation of existing multilingual models for gender bias

**Result:** All evaluated multilingual models exhibited varying degrees of gender bias in job title ranking.

**Limitations:** The study focuses primarily on grammatical gender and may not consider other forms of bias.

**Conclusion:** The findings highlight the importance of considering grammatical gender in job ranking systems to mitigate bias.

**Abstract:** This work sets the ground for studying how explicit grammatical gender assignment in job titles can affect the results of automatic job ranking systems. We propose the usage of metrics for ranking comparison controlling for gender to evaluate gender bias in job title ranking systems, in particular RBO (Rank-Biased Overlap). We generate and share test sets for a job title matching task in four grammatical gender languages, including occupations in masculine and feminine form and annotated by gender and matching relevance. We use the new test sets and the proposed methodology to evaluate the gender bias of several out-of-the-box multilingual models to set as baselines, showing that all of them exhibit varying degrees of gender bias.

</details>


### [38] [Geometric Uncertainty for Detecting and Correcting Hallucinations in LLMs](https://arxiv.org/abs/2509.13813)

*Edward Phillips, Sean Wu, Soheila Molaei, Danielle Belgrave, Anshul Thakur, David Clifton*

**Main category:** cs.CL

**Keywords:** large language models, uncertainty quantification, hallucination detection, archetypal analysis, medical datasets

**Relevance Score:** 9

**TL;DR:** This paper presents a geometric framework for uncertainty quantification in large language models that improves hallucination detection by providing both global and local uncertainty estimates using only black-box model access.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination problem in large language models, which generate plausible but incorrect answers, by providing a robust black-box method for uncertainty quantification.

**Method:** The proposed framework uses archetypal analysis to derive global and local uncertainty estimates; it introduces Geometric Volume for global uncertainty and Geometric Suspicion for ranking response reliability.

**Key Contributions:**

	1. Geometric Volume for global uncertainty estimation
	2. Geometric Suspicion for ranking individual response reliability
	3. A black-box approach for uncertainty quantification without needing internal model access.

**Result:** The framework shows competitive performance on short answer datasets and demonstrates superior performance on medical datasets, which are critical due to the risks associated with hallucinations.

**Limitations:** 

**Conclusion:** The framework provides valuable insights into reliability attribution for individual responses and is theoretically justified by linking convex hull volume to entropy.

**Abstract:** Large language models demonstrate impressive results across diverse tasks but are still known to hallucinate, generating linguistically plausible but incorrect answers to questions. Uncertainty quantification has been proposed as a strategy for hallucination detection, but no existing black-box approach provides estimates for both global and local uncertainty. The former attributes uncertainty to a batch of responses, while the latter attributes uncertainty to individual responses. Current local methods typically rely on white-box access to internal model states, whilst black-box methods only provide global uncertainty estimates. We introduce a geometric framework to address this, based on archetypal analysis of batches of responses sampled with only black-box model access. At the global level, we propose Geometric Volume, which measures the convex hull volume of archetypes derived from response embeddings. At the local level, we propose Geometric Suspicion, which ranks responses by reliability and enables hallucination reduction through preferential response selection. Unlike prior dispersion methods which yield only a single global score, our approach provides semantic boundary points which have utility for attributing reliability to individual responses. Experiments show that our framework performs comparably to or better than prior methods on short form question-answering datasets, and achieves superior results on medical datasets where hallucinations carry particularly critical risks. We also provide theoretical justification by proving a link between convex hull volume and entropy.

</details>


### [39] [Findings of the Third Automatic Minuting (AutoMin) Challenge](https://arxiv.org/abs/2509.13814)

*Kartik Shinde, Laurent Besacier, Ondrej Bojar, Thibaut Thonet, Tirthankar Ghosal*

**Main category:** cs.CL

**Keywords:** automatic meeting summarization, question answering, large language models, multilingual, conference proceedings

**Relevance Score:** 7

**TL;DR:** This paper discusses AutoMin 2025, focusing on automatic meeting summarization and question answering based on meeting transcripts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The AutoMin shared task aims to advance the field of automatic meeting summarization and evaluate the capabilities of large language models (LLMs) in this context.

**Method:** The task included minuting structured meeting minutes in English and Czech, as well as a QA component based on meeting transcripts, with evaluations involving baseline systems and LLMs.

**Key Contributions:**

	1. Introduction of a new QA task based on meeting transcripts
	2. Evaluation of LLMs using multiple baseline systems
	3. Coverage of two languages and domains for the minuting task

**Result:** Limited participation was noted in 2025, with only one team in the minuting task and two in QA, but baseline evaluations were included for comprehensive analysis.

**Limitations:** Limited participant engagement reduced the variety of approaches in evaluation.

**Conclusion:** Despite reduced participation, the inclusion of baseline systems aims to provide insights into the effectiveness of current LLMs in summarizing and understanding meeting content.

**Abstract:** This paper presents the third edition of AutoMin, a shared task on automatic meeting summarization into minutes. In 2025, AutoMin featured the main task of minuting, the creation of structured meeting minutes, as well as a new task: question answering (QA) based on meeting transcripts.   The minuting task covered two languages, English and Czech, and two domains: project meetings and European Parliament sessions. The QA task focused solely on project meetings and was available in two settings: monolingual QA in English, and cross-lingual QA, where questions were asked and answered in Czech based on English meetings.   Participation in 2025 was more limited compared to previous years, with only one team joining the minuting task and two teams participating in QA. However, as organizers, we included multiple baseline systems to enable a comprehensive evaluation of current (2025) large language models (LLMs) on both tasks.

</details>


### [40] [AgentCTG: Harnessing Multi-Agent Collaboration for Fine-Grained Precise Control in Text Generation](https://arxiv.org/abs/2509.13677)

*Xinxu Zhou, Jiaqi Bai, Zhenqi Sun, Fanxiang Zeng, Yue Liu*

**Main category:** cs.CL

**Keywords:** Controlled Text Generation, Multi-agent workflows, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** The paper presents AgentCTG, a novel framework for Controlled Text Generation that enhances fine-grained control over text creation by simulating multi-agent workflows and introducing an auto-prompt module.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in Controlled Text Generation (CTG), including cost, scalability, and the need for domain knowledge learning.

**Method:** The paper introduces AgentCTG, which utilizes collaboration methods among agents and an auto-prompt module to enhance text generation effectiveness. It includes a new Character-Driven Rewriting task.

**Key Contributions:**

	1. Introduction of AgentCTG framework for enhanced control in CTG
	2. Development of the auto-prompt module
	3. Creation of the Character-Driven Rewriting task
	4. Demonstration of significant improvements in practical applications.

**Result:** AgentCTG achieves state-of-the-art results on multiple public datasets and significantly improves the driving experience in role-playing online navigation applications.

**Limitations:** 

**Conclusion:** The framework enables more immersive interactions by optimizing contextual text generation, enhancing personalization and user engagement in online platforms.

**Abstract:** Although significant progress has been made in many tasks within the field of Natural Language Processing (NLP), Controlled Text Generation (CTG) continues to face numerous challenges, particularly in achieving fine-grained conditional control over generation. Additionally, in real scenario and online applications, cost considerations, scalability, domain knowledge learning and more precise control are required, presenting more challenge for CTG. This paper introduces a novel and scalable framework, AgentCTG, which aims to enhance precise and complex control over the text generation by simulating the control and regulation mechanisms in multi-agent workflows. We explore various collaboration methods among different agents and introduce an auto-prompt module to further enhance the generation effectiveness. AgentCTG achieves state-of-the-art results on multiple public datasets. To validate its effectiveness in practical applications, we propose a new challenging Character-Driven Rewriting task, which aims to convert the original text into new text that conform to specific character profiles and simultaneously preserve the domain knowledge. When applied to online navigation with role-playing, our approach significantly enhances the driving experience through improved content delivery. By optimizing the generation of contextually relevant text, we enable a more immersive interaction within online communities, fostering greater personalization and user engagement.

</details>


### [41] [Large Language Models Discriminate Against Speakers of German Dialects](https://arxiv.org/abs/2509.13835)

*Minh Duc Bui, Carolin Holtermann, Valentin Hofmann, Anne Lauscher, Katharina von der Wense*

**Main category:** cs.CL

**Keywords:** Dialect Bias, Large Language Models, Sociolinguistics, German Dialects, AI Ethics

**Relevance Score:** 7

**TL;DR:** The paper investigates biases in large language models (LLMs) regarding dialect perception in German, revealing significant negative stereotypes against dialect speakers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore societal stereotypes faced by dialect speakers and their reflection in the behaviors of large language models (LLMs).

**Method:** The study employs two tasks (an association task and a decision task) and constructs a novel evaluation corpus comparing dialect sentences with standard German ones.

**Key Contributions:**

	1. Introduction of a novel evaluation corpus for dialect bias assessment
	2. Identification of significant biases in LLMs against German dialect speakers
	3. Demonstration that explicit demographic labeling intensifies bias in LLMs

**Result:** The findings indicate that all evaluated LLMs show significant bias against German dialect speakers, with negative associations and decision-making patterns.

**Limitations:** 

**Conclusion:** Labeling linguistic demographics amplifies bias more than implicit mentions, highlighting the need for awareness in LLM behaviors.

**Abstract:** Dialects represent a significant component of human culture and are found across all regions of the world. In Germany, more than 40% of the population speaks a regional dialect (Adler and Hansen, 2022). However, despite cultural importance, individuals speaking dialects often face negative societal stereotypes. We examine whether such stereotypes are mirrored by large language models (LLMs). We draw on the sociolinguistic literature on dialect perception to analyze traits commonly associated with dialect speakers. Based on these traits, we assess the dialect naming bias and dialect usage bias expressed by LLMs in two tasks: an association task and a decision task. To assess a model's dialect usage bias, we construct a novel evaluation corpus that pairs sentences from seven regional German dialects (e.g., Alemannic and Bavarian) with their standard German counterparts. We find that: (1) in the association task, all evaluated LLMs exhibit significant dialect naming and dialect usage bias against German dialect speakers, reflected in negative adjective associations; (2) all models reproduce these dialect naming and dialect usage biases in their decision making; and (3) contrary to prior work showing minimal bias with explicit demographic mentions, we find that explicitly labeling linguistic demographics--German dialect speakers--amplifies bias more than implicit cues like dialect usage.

</details>


### [42] [Do LLMs Align Human Values Regarding Social Biases? Judging and Explaining Social Biases with LLMs](https://arxiv.org/abs/2509.13869)

*Yang Liu, Chenhui Chu*

**Main category:** cs.CL

**Keywords:** large language models, human values, social biases, alignment, explanation generation

**Relevance Score:** 9

**TL;DR:** This study analyzes the alignment of large language models (LLMs) with human values regarding social biases across different bias scenarios, revealing inconsistencies and preferences among models.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the differences in alignment of LLMs with human values in various bias scenarios, focusing on social biases.

**Method:** The study analyzes 12 LLMs from four model families across four datasets to assess misalignment rates and preferences for different scenario types, along with understanding capacity evaluated through generated explanations.

**Key Contributions:**

	1. Analysis of LLM alignment with human values in social bias scenarios
	2. Comparison of explanation generation between small and large models
	3. Insights on judgment consistency within model families

**Result:** LLMs with larger parameter scales do not show lower misalignment rates; they exhibit alignment preferences for certain scenario types, with consistency in judgment among the same model family but no significant differences in understanding of social biases across LLMs.

**Limitations:** Study may not cover all variations of biases or LLM architectures, potential observer bias in scenario selection.

**Conclusion:** Fine-tuned smaller language models can provide more readable explanations of social biases, albeit with lower agreement to model norms; different LLMs have unique strengths and weaknesses in misalignment.

**Abstract:** Large language models (LLMs) can lead to undesired consequences when misaligned with human values, especially in scenarios involving complex and sensitive social biases. Previous studies have revealed the misalignment of LLMs with human values using expert-designed or agent-based emulated bias scenarios. However, it remains unclear whether the alignment of LLMs with human values differs across different types of scenarios (e.g., scenarios containing negative vs. non-negative questions). In this study, we investigate the alignment of LLMs with human values regarding social biases (HVSB) in different types of bias scenarios. Through extensive analysis of 12 LLMs from four model families and four datasets, we demonstrate that LLMs with large model parameter scales do not necessarily have lower misalignment rate and attack success rate. Moreover, LLMs show a certain degree of alignment preference for specific types of scenarios and the LLMs from the same model family tend to have higher judgment consistency. In addition, we study the understanding capacity of LLMs with their explanations of HVSB. We find no significant differences in the understanding of HVSB across LLMs. We also find LLMs prefer their own generated explanations. Additionally, we endow smaller language models (LMs) with the ability to explain HVSB. The generation results show that the explanations generated by the fine-tuned smaller LMs are more readable, but have a relatively lower model agreeability.

</details>


### [43] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)

*Sami Ul Haq, Sheila Castilho, Yvette Graham*

**Main category:** cs.CL

**Keywords:** Machine Translation, Speech Evaluation, Crowd-sourced Assessment, Multimodal Evaluation, Statistical Testing

**Relevance Score:** 4

**TL;DR:** This study compares audio-based evaluations with traditional text-only assessments in machine translation, highlighting the importance of incorporating speech in future evaluation frameworks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of traditional text-centric assessments in machine translation by proposing a more natural evaluation method through speech.

**Method:** The research compares text-only and audio-based evaluations of 10 machine translation systems using crowd-sourced judgments from Amazon Mechanical Turk, along with statistical significance testing.

**Key Contributions:**

	1. Proposed a novel audio-based evaluation method for machine translation quality assessment.
	2. Demonstrated statistical consistency between audio and text-only evaluations while uncovering significant differences among systems.
	3. Argued for the inclusion of speech as a critical component in future MT evaluation methodologies.

**Result:** The results show that crowd-sourced audio assessments yield rankings consistent with text-only evaluations but also highlight significant differences in some cases between systems, indicating the value of speech-based assessments.

**Limitations:** Limited to evaluation of 10 MT systems; findings may not generalize to other MT applications; reliance on crowd-sourced judgments could introduce variability.

**Conclusion:** The authors suggest that incorporating audio-based evaluations into machine translation assessment frameworks could enhance quality evaluations and reflect real-world usage more effectively.

**Abstract:** Machine Translation (MT) has achieved remarkable performance, with growing interest in speech translation and multimodal approaches. However, despite these advancements, MT quality assessment remains largely text centric, typically relying on human experts who read and compare texts. Since many real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK Translator) involve translation being spoken rather printed or read, a more natural way to assess translation quality would be through speech as opposed text-only evaluations. This study compares text-only and audio-based evaluations of 10 MT systems from the WMT General MT Shared Task, using crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally, performed statistical significance testing and self-replication experiments to test reliability and consistency of audio-based approach. Crowd-sourced assessments based on audio yield rankings largely consistent with text only evaluations but, in some cases, identify significant differences between translation systems. We attribute this to speech richer, more natural modality and propose incorporating speech-based assessments into future MT evaluation frameworks.

</details>


### [44] [Combining Evidence and Reasoning for Biomedical Fact-Checking](https://arxiv.org/abs/2509.13879)

*Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato*

**Main category:** cs.CL

**Keywords:** Misinformation, Biomedical Fact-Checking, Large Language Models

**Relevance Score:** 9

**TL;DR:** CER is a novel biomedical fact-checking framework integrating evidence retrieval with large language model reasoning, mitigating misinformation risks in healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the risks posed by misinformation in healthcare and enhance public trust in medical systems through automated fact-checking.

**Method:** The CER framework combines scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction to validate biomedical claims.

**Key Contributions:**

	1. Introduction of the CER framework for biomedical fact-checking.
	2. Integration of scientific evidence retrieval and large language model reasoning.
	3. Demonstration of state-of-the-art performance on expert-annotated datasets.

**Result:** Evaluations on expert-annotated datasets show CER achieves state-of-the-art performance and demonstrates promising cross-dataset generalization.

**Limitations:** 

**Conclusion:** The integration of advanced retrieval techniques and large language models in CER effectively mitigates the risk of misinformation, providing grounded evidence-based outputs.

**Abstract:** Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https: //github.com/PRAISELab-PicusLab/CER.

</details>


### [45] [Combating Biomedical Misinformation through Multi-modal Claim Detection and Evidence-based Verification](https://arxiv.org/abs/2509.13888)

*Mariano Barone, Antonio Romano, Giuseppe Riccio, Marco Postiglione, Vincenzo Moscato*

**Main category:** cs.CL

**Keywords:** biomedical fact-checking, machine learning, healthcare misinformation

**Relevance Score:** 9

**TL;DR:** Introducing CER, a framework for biomedical fact-checking that combines evidence retrieval, reasoning via LLMs, and supervised veracity prediction to enhance the accuracy of health-related claims.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address misinformation in healthcare which poses risks to public health and trust in medical systems, highlighting the unique challenges in validating biomedical claims.

**Method:** CER integrates scientific evidence retrieval, reasoning with large language models, and uses supervised learning for veracity prediction.

**Key Contributions:**

	1. Introduction of CER framework for biomedical fact-checking
	2. Integration of large language models with evidence retrieval
	3. Demonstration of state-of-the-art performance on benchmark datasets

**Result:** Evaluations show CER achieves state-of-the-art performance on expert-annotated datasets and demonstrates promising generalization across different datasets.

**Limitations:** 

**Conclusion:** The integration of LLM capabilities with advanced retrieval techniques effectively reduces the risk of hallucinations in health-related claims, promoting evidence-based outputs.

**Abstract:** Misinformation in healthcare, from vaccine hesitancy to unproven treatments, poses risks to public health and trust in medical systems. While machine learning and natural language processing have advanced automated fact-checking, validating biomedical claims remains uniquely challenging due to complex terminology, the need for domain expertise, and the critical importance of grounding in scientific evidence. We introduce CER (Combining Evidence and Reasoning), a novel framework for biomedical fact-checking that integrates scientific evidence retrieval, reasoning via large language models, and supervised veracity prediction. By integrating the text-generation capabilities of large language models with advanced retrieval techniques for high-quality biomedical scientific evidence, CER effectively mitigates the risk of hallucinations, ensuring that generated outputs are grounded in verifiable, evidence-based sources. Evaluations on expert-annotated datasets (HealthFC, BioASQ-7b, SciFact) demonstrate state-of-the-art performance and promising cross-dataset generalization. Code and data are released for transparency and reproducibility: https://github.com/PRAISELab-PicusLab/CER

</details>


### [46] [Do Large Language Models Understand Word Senses?](https://arxiv.org/abs/2509.13905)

*Domenico Meconi, Simone Stirpe, Federico Martelli, Leonardo Lavalle, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** Word Sense Disambiguation, Large Language Models, natural language understanding

**Relevance Score:** 8

**TL;DR:** This paper evaluates the Word Sense Disambiguation (WSD) capabilities of instruction-tuned Large Language Models (LLMs) and their ability to understand word meanings in various generative contexts.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the underexplored area of whether LLMs truly comprehend word senses in context, an important aspect of natural language understanding.

**Method:** The authors evaluate the WSD capabilities of instruction-tuned LLMs and compare them against specialized WSD systems. They assess two top-performing LLMs in three generative tasks: definition generation, free-form explanation, and example generation.

**Key Contributions:**

	1. Evaluated LLMs against state-of-the-art WSD systems.
	2. Demonstrated LLMs' performance in generative tasks related to word meaning understanding.
	3. Highlighted the robustness of LLMs across domains and difficulty levels.

**Result:** Top-performing LLMs such as GPT-4o and DeepSeek-V3 demonstrate WSD performance comparable to specialized systems, with 98% accuracy in explaining word meanings in context, particularly excelling in free-form explanations.

**Limitations:** 

**Conclusion:** The findings suggest that instruction-tuned LLMs not only perform well in WSD tasks but also exhibit strong generative capabilities in providing contextual word meanings.

**Abstract:** Understanding the meaning of words in context is a fundamental capability for Large Language Models (LLMs). Despite extensive evaluation efforts, the extent to which LLMs show evidence that they truly grasp word senses remains underexplored. In this paper, we address this gap by evaluating both i) the Word Sense Disambiguation (WSD) capabilities of instruction-tuned LLMs, comparing their performance to state-of-the-art systems specifically designed for the task, and ii) the ability of two top-performing open- and closed-source LLMs to understand word senses in three generative settings: definition generation, free-form explanation, and example generation. Notably, we find that, in the WSD task, leading models such as GPT-4o and DeepSeek-V3 achieve performance on par with specialized WSD systems, while also demonstrating greater robustness across domains and levels of difficulty. In the generation tasks, results reveal that LLMs can explain the meaning of words in context up to 98\% accuracy, with the highest performance observed in the free-form explanation task, which best aligns with their generative capabilities.

</details>


### [47] [Linguistic Nepotism: Trading-off Quality for Language Preference in Multilingual RAG](https://arxiv.org/abs/2509.13930)

*Dayeon Ki, Marine Carpuat, Paul McNamee, Daniel Khashabi, Eugene Yang, Dawn Lawrie, Kevin Duh*

**Main category:** cs.CL

**Keywords:** multilingual, retrieval-augmented generation, citation bias, language models, HCI

**Relevance Score:** 8

**TL;DR:** This paper investigates how different document languages impact citation and generation in multilingual retrieval-augmented generation (mRAG) systems, revealing biases in citation choices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of different document languages on the citation behavior of multilingual retrieval-augmented generation (mRAG) systems.

**Method:** A controlled methodology utilizing model internals to measure language preference while keeping document relevance constant across eight languages and six open-weight models was employed.

**Key Contributions:**

	1. Introduces a methodology to study language preference in mRAG systems.
	2. Demonstrates bias towards English citation in multilingual contexts.
	3. Highlights trade-offs between document relevance and language preference.

**Result:** Models tend to cite English sources preferentially when queries are in English, a bias that is amplified for lower-resource languages and affects citation choices, indicating a trade-off between relevance and language preference.

**Limitations:** Limited to the investigation of eight languages and six open-weight models; may not generalize across all multilingual settings.

**Conclusion:** Understanding how language models utilize multilingual context can inform the design of more equitable mRAG systems by addressing citation biases.

**Abstract:** Multilingual Retrieval-Augmented Generation (mRAG) systems enable language models to answer knowledge-intensive queries with citation-supported responses across languages. While such systems have been proposed, an open questions is whether the mixture of different document languages impacts generation and citation in unintended ways. To investigate, we introduce a controlled methodology using model internals to measure language preference while holding other factors such as document relevance constant. Across eight languages and six open-weight models, we find that models preferentially cite English sources when queries are in English, with this bias amplified for lower-resource languages and for documents positioned mid-context. Crucially, we find that models sometimes trade-off document relevance for language preference, indicating that citation choices are not always driven by informativeness alone. Our findings shed light on how language models leverage multilingual context and influence citation behavior.

</details>


### [48] [Long-context Reference-based MT Quality Estimation](https://arxiv.org/abs/2509.13980)

*Sami Ul Haq, Chinonso Cynthia Osuji, Sheila Castilho, Brian Davis*

**Main category:** cs.CL

**Keywords:** Machine Translation, Automated Evaluation, COMET Framework, Long-context Data, Quality Assessment

**Relevance Score:** 4

**TL;DR:** This paper presents a submission to WMT25 focusing on automated translation quality evaluation using the COMET framework and segment-level Error Span Annotation scores with long-context data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance the accuracy of automated translation quality evaluation by leveraging long-context data to better reflect human judgments.

**Method:** The authors concatenated in-domain human-annotated sentences and computed weighted averages of their Error Span Annotation scores. They integrated multiple human judgment datasets and trained multilingual regression models to predict quality scores from various translations.

**Key Contributions:**

	1. Utilization of long-context data for quality score predictions.
	2. Integration of multiple human judgment datasets for improved training.
	3. Demonstration of improved correlations between automated scores and human judgments.

**Result:** The experimental results indicate that models using long-context information outperform those based solely on short segments in terms of correlation with human judgment.

**Limitations:** 

**Conclusion:** Incorporating long-context data significantly enhances automated translation quality evaluation by aligning more closely with human assessments.

**Abstract:** In this paper, we present our submission to the Tenth Conference on Machine Translation (WMT25) Shared Task on Automated Translation Quality Evaluation.   Our systems are built upon the COMET framework and trained to predict segment-level Error Span Annotation (ESA) scores using augmented long-context data.   To construct long-context training data, we concatenate in-domain, human-annotated sentences and compute a weighted average of their scores.   We integrate multiple human judgment datasets (MQM, SQM, and DA) by normalising their scales and train multilingual regression models to predict quality scores from the source, hypothesis, and reference translations.   Experimental results show that incorporating long-context information improves correlations with human judgments compared to models trained only on short segments.

</details>


### [49] [Slim-SC: Thought Pruning for Efficient Scaling with Self-Consistency](https://arxiv.org/abs/2509.13990)

*Colin Hong, Xu Guo, Anand Chaanan Singh, Esha Choukse, Dmitrii Ustiugov*

**Main category:** cs.CL

**Keywords:** Test-Time Scaling, Self-Consistency, LLMs, Inference Latency, Pruning Strategy

**Relevance Score:** 8

**TL;DR:** This paper presents Slim-SC, an efficient variant of the Self-Consistency technique for Test-Time Scaling in LLMs, which reduces computational overhead while maintaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to address the high computational overhead of Self-Consistency in LLMs during test time, which limits its wider applicability.

**Method:** The paper proposes Slim-SC, a pruning strategy that identifies and eliminates redundant reasoning chains by analyzing inter-chain similarity at the thought level.

**Key Contributions:**

	1. Introduction of Slim-SC as an efficient alternative to Self-Consistency.
	2. Theoretical and empirical analysis of inefficiencies in Self-Consistency.
	3. Demonstration of significant reductions in computational resources with maintained accuracy.

**Result:** Experiments demonstrate that Slim-SC decreases inference latency by up to 45% and KVC usage by 26% on three STEM reasoning datasets, while also maintaining or enhancing accuracy.

**Limitations:** 

**Conclusion:** Slim-SC provides an efficient alternative to Self-Consistency in LLM reasoning, making Test-Time Scaling more accessible.

**Abstract:** Recently, Test-Time Scaling (TTS) has gained increasing attention for improving LLM reasoning performance at test time without retraining the model. A notable TTS technique is Self-Consistency (SC), which generates multiple reasoning chains in parallel and selects the final answer via majority voting. While effective, the order-of-magnitude computational overhead limits its broad deployment. Prior attempts to accelerate SC mainly rely on model-based confidence scores or heuristics with limited empirical support. For the first time, we theoretically and empirically analyze the inefficiencies of SC and reveal actionable opportunities for improvement. Building on these insights, we propose Slim-SC, a step-wise pruning strategy that identifies and removes redundant chains using inter-chain similarity at the thought level. Experiments on three STEM reasoning datasets and two recent LLM architectures show that Slim-SC reduces inference latency and KVC usage by up to 45% and 26%, respectively, with R1-Distill, while maintaining or improving accuracy, thus offering a simple yet efficient TTS alternative for SC.

</details>


### [50] [Early Stopping Chain-of-thoughts in Large Language Models](https://arxiv.org/abs/2509.14004)

*Minjia Mao, Bowen Yin, Yu Zhu, Xiao Fang*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought, inference cost, answer convergence, ES-CoT

**Relevance Score:** 8

**TL;DR:** This study introduces ES-CoT, a method to shorten chain-of-thought generation in large language models by detecting answer convergence, achieving a 41% reduction in inference tokens while maintaining accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of large language models (LLMs) in generating chain-of-thought (CoT) responses while minimizing inference costs.

**Method:** The proposed ES-CoT method tracks the run length of consecutive identical step answers to determine answer convergence and stops generation early when convergence is detected.

**Key Contributions:**

	1. Introduction of ES-CoT for efficient CoT generation.
	2. Demonstration of significant reduction in inference costs with maintained accuracy.
	3. Empirical validation across multiple reasoning datasets.

**Result:** ES-CoT reduces the number of inference tokens by approximately 41% on average across five reasoning datasets from three different LLMs, while maintaining accuracy comparable to standard CoT methods.

**Limitations:** 

**Conclusion:** ES-CoT is a practical and effective approach that enables efficient reasoning in LLMs without sacrificing performance, integrating well with existing prompting techniques.

**Abstract:** Reasoning large language models (LLMs) have demonstrated superior capacities in solving complicated problems by generating long chain-of-thoughts (CoT), but such a lengthy CoT incurs high inference costs. In this study, we introduce ES-CoT, an inference-time method that shortens CoT generation by detecting answer convergence and stopping early with minimal performance loss. At the end of each reasoning step, we prompt the LLM to output its current final answer, denoted as a step answer. We then track the run length of consecutive identical step answers as a measure of answer convergence. Once the run length exhibits a sharp increase and exceeds a minimum threshold, the generation is terminated. We provide both empirical and theoretical support for this heuristic: step answers steadily converge to the final answer, and large run-length jumps reliably mark this convergence. Experiments on five reasoning datasets across three LLMs show that ES-CoT reduces the number of inference tokens by about 41\% on average while maintaining accuracy comparable to standard CoT. Further, ES-CoT integrates seamlessly with self-consistency prompting and remains robust across hyperparameter choices, highlighting it as a practical and effective approach for efficient reasoning.

</details>


### [51] [Hala Technical Report: Building Arabic-Centric Instruction & Translation Models at Scale](https://arxiv.org/abs/2509.14008)

*Hasan Abed Al Kader Hammoud, Mohammad Zbeeb, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** Arabic NLP, translation models, instruction following

**Relevance Score:** 4

**TL;DR:** This paper presents Hala, a family of Arabic-centric instruction and translation models that achieve state-of-the-art results in Arabic NLP tasks by leveraging a novel translate-and-tune pipeline.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Hala is to enhance Arabic instruction following and translation capabilities while maintaining high throughput without quality loss.

**Method:** The approach includes compressing a strong AR↔EN teacher model to FP8 and using it to create bilingual supervision, followed by fine-tuning a lightweight language model LFM2-1.2B on this data to translate English instructions into Arabic.

**Key Contributions:**

	1. Introduction of the translate-and-tune pipeline for Arabic-centric models.
	2. State-of-the-art performance on Arabic NLP tasks.
	3. Release of models and data to foster Arabic NLP research.

**Result:** Hala models have been trained at various sizes (350M, 700M, 1.2B, and 9B parameters) and have achieved state-of-the-art results on Arabic-centric benchmarks, outperforming their base models in both 'nano' and 'small' categories.

**Limitations:** 

**Conclusion:** The paper concludes that the Hala models provide significant advancements in Arabic NLP and encourages further research with the released models, data, and evaluation strategies.

**Abstract:** We present Hala, a family of Arabic-centric instruction and translation models built with our translate-and-tune pipeline. We first compress a strong AR$\leftrightarrow$EN teacher to FP8 (yielding $\sim$2$\times$ higher throughput with no quality loss) and use it to create high-fidelity bilingual supervision. A lightweight language model LFM2-1.2B is then fine-tuned on this data and used to translate high-quality English instruction sets into Arabic, producing a million-scale corpus tailored to instruction following. We train Hala models at 350M, 700M, 1.2B, and 9B parameters, and apply slerp merging to balance Arabic specialization with base-model strengths. On Arabic-centric benchmarks, Hala achieves state-of-the-art results within both the "nano" ($\leq$2B) and "small" (7-9B) categories, outperforming their bases. We release models, data, evaluation, and recipes to accelerate research in Arabic NLP.

</details>


### [52] [DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue](https://arxiv.org/abs/2410.09252)

*Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Science, Multi-turn Retrieval, Model-based Planning, Human-like Reasoning

**Relevance Score:** 8

**TL;DR:** A novel scientific agent, DAVIS, improves AI task performance in laboratory settings by integrating structured and temporal memory and agentic multi-turn retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a generalist scientific agent that can assist researchers in laboratory settings by addressing the complexities of scientific tasks with advanced reasoning and safety.

**Method:** DAVIS employs structured and temporal memory for model-based planning, alongside an agentic multi-turn retrieval system that mimics human reasoning, enhancing its ability to perform in complex environments.

**Key Contributions:**

	1. Introduction of structured and temporal memory in a RAG agent
	2. First implementation of an agentic multi-turn retrieval system for AI tasks
	3. Demonstrated improved performance on multiple benchmarks relevant to science tasks.

**Result:** DAVIS outperforms previous models on the ScienceWorld benchmark across 8 of 9 science subjects and also shows competitive performance on multi-hop question answering tasks in HotpotQA and MusiqueQA datasets.

**Limitations:** 

**Conclusion:** DAVIS represents a significant advancement in RAG agents, being the first to use an interactive retrieval method within a RAG framework, enhancing scientific task performance in AI.

**Abstract:** Designing a generalist scientific agent capable of performing tasks in laboratory settings to assist researchers has become a key goal in recent Artificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks are inherently more delicate and complex, requiring agents to possess a higher level of reasoning ability, structured and temporal understanding of their environment, and a strong emphasis on safety. Existing approaches often fail to address these multifaceted requirements. To tackle these challenges, we present DAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches, DAVIS incorporates structured and temporal memory, which enables model-based planning. Additionally, DAVIS implements an agentic, multi-turn retrieval system, similar to a human's inner monologue, allowing for a greater degree of reasoning over past experiences. DAVIS demonstrates substantially improved performance on the ScienceWorld benchmark comparing to previous approaches on 8 out of 9 elementary science subjects. In addition, DAVIS's World Model demonstrates competitive performance on the famous HotpotQA and MusiqueQA dataset for multi-hop question answering. To the best of our knowledge, DAVIS is the first RAG agent to employ an interactive retrieval method in a RAG pipeline.

</details>


### [53] [Audio-Based Crowd-Sourced Evaluation of Machine Translation Quality](https://arxiv.org/abs/2509.14023)

*Sami Ul Haq, Sheila Castilho, Yvette Graham*

**Main category:** cs.CL

**Keywords:** Machine Translation, Speech Translation, Multimodal Assessment

**Relevance Score:** 6

**TL;DR:** This study evaluates Machine Translation (MT) systems using both text-only and audio-based assessments, finding that audio evaluations can reveal significant differences in translation quality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Despite advancements in Machine Translation, quality assessments are still mainly text-centric, which does not reflect real-world applications where translations are often spoken.

**Method:** The study compares evaluations of 10 MT systems using crowd-sourced judgments from Amazon Mechanical Turk, performing statistical tests to assess the reliability of audio-based evaluations versus text-only evaluations.

**Key Contributions:**

	1. Proposes audio-based evaluations as a natural alternative to traditional text-centric assessments.
	2. Demonstrates that audio evaluations can reveal significant differences between MT systems not apparent in text-only assessments.
	3. Provides statistical evidence for the reliability and consistency of crowd-sourced audio evaluations.

**Result:** Audio-based assessments yield rankings consistent with text-only evaluations, but also highlight significant differences in some cases due to the richer modality of speech.

**Limitations:** Limited to the evaluation of 10 MT systems and reliance on crowd-sourced judgments which may introduce variability.

**Conclusion:** Incorporating speech-based assessments into MT evaluation frameworks is proposed to enhance the quality and relevance of evaluations.

**Abstract:** Machine Translation (MT) has achieved remarkable performance, with growing interest in speech translation and multimodal approaches. However, despite these advancements, MT quality assessment remains largely text centric, typically relying on human experts who read and compare texts. Since many real-world MT applications (e.g Google Translate Voice Mode, iFLYTEK Translator) involve translation being spoken rather printed or read, a more natural way to assess translation quality would be through speech as opposed text-only evaluations. This study compares text-only and audio-based evaluations of 10 MT systems from the WMT General MT Shared Task, using crowd-sourced judgments collected via Amazon Mechanical Turk. We additionally, performed statistical significance testing and self-replication experiments to test reliability and consistency of audio-based approach. Crowd-sourced assessments based on audio yield rankings largely consistent with text only evaluations but, in some cases, identify significant differences between translation systems. We attribute this to speech richer, more natural modality and propose incorporating speech-based assessments into future MT evaluation frameworks.

</details>


### [54] [Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script](https://arxiv.org/abs/2412.12478)

*Xi Cao, Yuan Sun, Jiajun Li, Quzong Gesang, Nuo Qun, Tashi Nyima*

**Main category:** cs.CL

**Keywords:** adversarial text generation, lower-resourced languages, HITL-GAT

**Relevance Score:** 8

**TL;DR:** This paper presents HITL-GAT, an interactive system for generating adversarial texts aimed at addressing the challenges in crafting robust benchmarks for lower-resourced languages like Tibetan.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the lack of high-quality adversarial robustness benchmarks for lower-resourced languages, which are often overlooked in adversarial text generation studies.

**Method:** The study introduces HITL-GAT, an interactive system that utilizes a human-in-the-loop approach for generating adversarial texts tailored to lower-resourced languages, demonstrated through a case study on Tibetan script.

**Key Contributions:**

	1. Development of HITL-GAT for human-in-the-loop adversarial text generation
	2. First adversarial robustness benchmark for Tibetan script
	3. Customized adversarial text generation methods for lower-resourced languages

**Result:** The development and demonstration of HITL-GAT led to the first validated benchmark for adversarial robustness in Tibetan, showcasing effective methods for generating adversarial texts.

**Limitations:** The research focuses on Tibetan and may not generalize over all lower-resourced languages; the complexity of linguistic differences remains a challenge.

**Conclusion:** HITL-GAT proves to be a valuable tool in advancing adversarial text generation for lower-resourced languages and sets a precedent for future research in this area.

**Abstract:** DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.

</details>


### [55] [You Are What You Train: Effects of Data Composition on Training Context-aware Machine Translation Models](https://arxiv.org/abs/2509.14031)

*Paweł Mąka, Yusuf Can Semerci, Jan Scholtes, Gerasimos Spanakis*

**Main category:** cs.CL

**Keywords:** machine translation, context utilization, data sparsity, multilingual settings, training strategies

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of contextually rich training data on machine translation performance, finding that increased context utilization significantly enhances accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in achieving human-level translations, especially in handling contextual phenomena such as pronoun disambiguation, and to explore the impact of training data sparsity.

**Method:** The authors constructed training datasets with controlled proportions of contextually relevant examples to systematically validate the association between data sparsity and model performance across single- and multilingual settings.

**Key Contributions:**

	1. Validated the impact of contextually rich data on translation performance.
	2. Revealed limited cross-lingual transfer within language sub-families.
	3. Proposed and evaluated training strategies that enhance context utilization.

**Result:** The results confirm a strong correlation between training data sparsity and model performance, with accuracy improvements of up to 6 and 8 percentage points observed in single- and multilingual evaluations, respectively.

**Limitations:** Improvements in context utilization are specific to certain phenomena and do not generalize across all areas of translation.

**Conclusion:** The study highlights the importance of contextually rich training data in translation models and proposes effective strategies to enhance context utilization, although improvements in one contextual area do not generalize to others.

**Abstract:** Achieving human-level translations requires leveraging context to ensure coherence and handle complex phenomena like pronoun disambiguation. Sparsity of contextually rich examples in the standard training data has been hypothesized as the reason for the difficulty of context utilization. In this work, we systematically validate this claim in both single- and multilingual settings by constructing training datasets with a controlled proportions of contextually relevant examples. We demonstrate a strong association between training data sparsity and model performance confirming sparsity as a key bottleneck. Importantly, we reveal that improvements in one contextual phenomenon do no generalize to others. While we observe some cross-lingual transfer, it is not significantly higher between languages within the same sub-family. Finally, we propose and empirically evaluate two training strategies designed to leverage the available data. These strategies improve context utilization, resulting in accuracy gains of up to 6 and 8 percentage points on the ctxPro evaluation in single- and multilingual settings respectively.

</details>


### [56] [Enhancing Multi-Agent Debate System Performance via Confidence Expression](https://arxiv.org/abs/2509.14034)

*Zijie Lin, Bryan Hooi*

**Main category:** cs.CL

**Keywords:** Generative Large Language Models, Multi-Agent Debate, Confidence Expression

**Relevance Score:** 8

**TL;DR:** The paper introduces ConfMAD, a Multi-Agent Debate framework that incorporates confidence expression in Generative Large Language Models to improve debate effectiveness and system performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of inappropriate confidence expression in Multi-Agent Debate systems, which leads to suboptimal reasoning and belief maintenance by LLMs during debates.

**Method:** The authors develop the ConfMAD framework, integrating mechanisms for LLMs to express their confidence levels throughout the debate process.

**Key Contributions:**

	1. Introduction of the ConfMAD framework for confidence expression in MAD systems.
	2. Demonstration of improved debate effectiveness through experimental validation.
	3. Insights into how confidence levels affect debate dynamics and outcomes.

**Result:** Experimental results indicate that incorporating confidence expression improves the effectiveness of debates and influences debate dynamics positively.

**Limitations:** 

**Conclusion:** The study highlights the importance of confidence expression in MAD systems, suggesting that it can lead to better task performance by enabling clearer communication of LLMs' knowledge and reasoning strengths.

**Abstract:** Generative Large Language Models (LLMs) have demonstrated remarkable performance across a wide range of tasks. Recent research has introduced Multi-Agent Debate (MAD) systems, which leverage multiple LLMs to simulate human debate and thereby improve task performance. However, while some LLMs may possess superior knowledge or reasoning capabilities for specific tasks, they often struggle to clearly communicate this advantage during debates, in part due to a lack of confidence expression. Moreover, inappropriate confidence expression can cause agents in MAD systems to either stubbornly maintain incorrect beliefs or converge prematurely on suboptimal answers, ultimately reducing debate effectiveness and overall system performance. To address these challenges, we propose incorporating confidence expression into MAD systems to allow LLMs to explicitly communicate their confidence levels. To validate this approach, we develop ConfMAD, a MAD framework that integrates confidence expression throughout the debate process. Experimental results demonstrate the effectiveness of our method, and we further analyze how confidence influences debate dynamics, offering insights into the design of confidence-aware MAD systems.

</details>


### [57] [SSL-SSAW: Self-Supervised Learning with Sigmoid Self-Attention Weighting for Question-Based Sign Language Translation](https://arxiv.org/abs/2509.14036)

*Zekang Liu, Wei Feng, Fanhua Shang, Lianyu Hu, Jichao Feng, Liqing Gao*

**Main category:** cs.CL

**Keywords:** Sign Language Translation, Question-based Translation, Self-supervised Learning

**Relevance Score:** 6

**TL;DR:** This paper introduces Question-based Sign Language Translation (QB-SLT), which enhances sign language translation by integrating dialogue context through a novel Self-supervised Learning method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sign language translation by incorporating dialogue context for better performance than traditional gloss annotations.

**Method:** The authors propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) method that utilizes contrastive learning to align multimodality features for translation.

**Key Contributions:**

	1. Introduction of Question-based Sign Language Translation (QB-SLT)
	2. Development of SSL-SSAW for effective dialogue integration
	3. Demonstration of superior performance with easily accessible question assistance over gloss assistance

**Result:** The proposed approach achieved state-of-the-art performance on the CSL-Daily-QA and PHOENIX-2014T-QA datasets, effectively demonstrating that dialogue assistance can improve translation quality. Visualization results support the use of dialogue for enhancement.

**Limitations:** 

**Conclusion:** Incorporating dialogue context in Sign Language Translation significantly improves translation quality, surpassing traditional gloss methods.

**Abstract:** Sign Language Translation (SLT) bridges the communication gap between deaf people and hearing people, where dialogue provides crucial contextual cues to aid in translation. Building on this foundational concept, this paper proposes Question-based Sign Language Translation (QB-SLT), a novel task that explores the efficient integration of dialogue. Unlike gloss (sign language transcription) annotations, dialogue naturally occurs in communication and is easier to annotate. The key challenge lies in aligning multimodality features while leveraging the context of the question to improve translation. To address this issue, we propose a cross-modality Self-supervised Learning with Sigmoid Self-attention Weighting (SSL-SSAW) fusion method for sign language translation. Specifically, we employ contrastive learning to align multimodality features in QB-SLT, then introduce a Sigmoid Self-attention Weighting (SSAW) module for adaptive feature extraction from question and sign language sequences. Additionally, we leverage available question text through self-supervised learning to enhance representation and translation capabilities. We evaluated our approach on newly constructed CSL-Daily-QA and PHOENIX-2014T-QA datasets, where SSL-SSAW achieved SOTA performance. Notably, easily accessible question assistance can achieve or even surpass the performance of gloss assistance. Furthermore, visualization results demonstrate the effectiveness of incorporating dialogue in improving translation quality.

</details>


### [58] [Canary-1B-v2 & Parakeet-TDT-0.6B-v3: Efficient and High-Performance Models for Multilingual ASR and AST](https://arxiv.org/abs/2509.14128)

*Monica Sekoyan, Nithin Rao Koluguri, Nune Tadevosyan, Piotr Zelasko, Travis Bartley, Nick Karpov, Jagadeesh Balam, Boris Ginsburg*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Speech-to-Text Translation, FastConformer, multilingual, nGPT

**Relevance Score:** 6

**TL;DR:** Canary-1B-v2 is a fast multilingual ASR and AST model, evaluated against existing models with promising results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient and accurate multilingual ASR and AST systems to handle diverse language inputs, particularly for applications involving European languages.

**Method:** Canary-1B-v2 employs a FastConformer encoder with a Transformer decoder, trained on 1.7M hours of data, implementing a two-stage pre-training and fine-tuning process with dynamic data balancing.

**Key Contributions:**

	1. Introduction of a robust multilingual ASR and AST model
	2. Demonstration of FastConformer and nGPT with competitive speed and performance
	3. Release of Parakeet-TDT-0.6B-v3 providing efficient ASR across 25 languages

**Result:** Canary-1B-v2 outperforms Whisper-large-v3 in English ASR, performing 10x faster, and remains competitive with larger models like Seamless-M4T-v2-large for multilingual performance.

**Limitations:** 

**Conclusion:** The introduction of both Canary-1B-v2 and its successor, Parakeet-TDT-0.6B-v3, represents significant advancements in multilingual ASR and AST capabilities.

**Abstract:** This report introduces Canary-1B-v2, a fast, robust multilingual model for Automatic Speech Recognition (ASR) and Speech-to-Text Translation (AST). Built with a FastConformer encoder and Transformer decoder, it supports 25 languages primarily European. The model was trained on 1.7M hours of total data samples, including Granary and NeMo ASR Set 3.0, with non-speech audio added to reduce hallucinations for ASR and AST. We describe its two-stage pre-training and fine-tuning process with dynamic data balancing, as well as experiments with an nGPT encoder. Results show nGPT scales well with massive data, while FastConformer excels after fine-tuning. For timestamps, Canary-1B-v2 uses the NeMo Forced Aligner (NFA) with an auxiliary CTC model, providing reliable segment-level timestamps for ASR and AST. Evaluations show Canary-1B-v2 outperforms Whisper-large-v3 on English ASR while being 10x faster, and delivers competitive multilingual ASR and AST performance against larger models like Seamless-M4T-v2-large and LLM-based systems. We also release Parakeet-TDT-0.6B-v3, a successor to v2, offering multilingual ASR across the same 25 languages with just 600M parameters.

</details>


### [59] [CS-FLEURS: A Massively Multilingual and Code-Switched Speech Dataset](https://arxiv.org/abs/2509.14161)

*Brian Yan, Injy Hamed, Shuichiro Shimizu, Vasista Lodagala, William Chen, Olga Iakovenko, Bashar Talafha, Amir Hussein, Alexander Polok, Kalvin Chang, Dominik Klement, Sara Althubaiti, Puyuan Peng, Matthew Wiesner, Thamar Solorio, Ahmed Ali, Sanjeev Khudanpur, Shinji Watanabe, Chih-Chen Chen, Zhen Wu, Karim Benharrak, Anuj Diwan, Samuele Cornell, Eunjung Yeo, Kwanghee Choi, Carlos Carvalho, Karen Rosero*

**Main category:** cs.CL

**Keywords:** code-switching, speech recognition, low-resourced languages, dataset, speech translation

**Relevance Score:** 5

**TL;DR:** CS-FLEURS is a new dataset to advance code-switched speech recognition and translation for low-resourced languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The dataset aims to support research in code-switched speech systems, especially for languages with fewer resources.

**Method:** CS-FLEURS includes four test sets covering 113 unique language pairs and a training set with 128 hours of data across various setups.

**Key Contributions:**

	1. Introduction of a dataset covering 113 code-switched language pairs across 52 languages.
	2. Inclusion of various speech generation methods including generative text-to-speech.
	3. Provision of both test and training sets focused on low-resource languages.

**Result:** The dataset consists of diverse pairs, including real voices, generative text-to-speech, and lower-resourced language pairs.

**Limitations:** 

**Conclusion:** CS-FLEURS aims to broaden the research scope for code-switched speech, offering materials for various language pairs and resources.

**Abstract:** We present CS-FLEURS, a new dataset for developing and evaluating code-switched speech recognition and translation systems beyond high-resourced languages. CS-FLEURS consists of 4 test sets which cover in total 113 unique code-switched language pairs across 52 languages: 1) a 14 X-English language pair set with real voices reading synthetically generated code-switched sentences, 2) a 16 X-English language pair set with generative text-to-speech 3) a 60 {Arabic, Mandarin, Hindi, Spanish}-X language pair set with the generative text-to-speech, and 4) a 45 X-English lower-resourced language pair test set with concatenative text-to-speech. Besides the four test sets, CS-FLEURS also provides a training set with 128 hours of generative text-to-speech data across 16 X-English language pairs. Our hope is that CS-FLEURS helps to broaden the scope of future code-switched speech research. Dataset link: https://huggingface.co/datasets/byan/cs-fleurs.

</details>


### [60] [AssoCiAm: A Benchmark for Evaluating Association Thinking while Circumventing Ambiguity](https://arxiv.org/abs/2509.14171)

*Yifan Liu, Wenkuan Zhao, Shanshan Zhong, Jinghui Qin, Mingfu Liang, Zhongzhan Huang, Wushao Wen*

**Main category:** cs.CL

**Keywords:** multimodal large language models, associative ability, ambiguity, artificial general intelligence, benchmark

**Relevance Score:** 8

**TL;DR:** The paper presents AssoCiAm, a novel benchmark to evaluate the associative ability of multimodal large language models (MLLMs), addressing ambiguity issues and demonstrating a correlation between cognition and association.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of the associative ability of MLLMs, as existing frameworks overlook ambiguity inherent in association tasks, leading to unreliable assessments.

**Method:** The authors decompose ambiguity into internal and external types and propose the AssoCiAm benchmark, employing a hybrid computational method to evaluate MLLMs while minimizing ambiguity.

**Key Contributions:**

	1. Introduction of AssoCiAm benchmark for evaluating associative ability in MLLMs
	2. Decomposition of ambiguity into internal and external types
	3. Demonstration of correlation between cognition and association in MLLMs

**Result:** Experiments show a strong positive correlation between cognition and association in MLLMs. Ambiguity in evaluations correlates with increased randomness in MLLM behavior.

**Limitations:** The study may not comprehensively address all forms of ambiguity in association tasks beyond the explored types.

**Conclusion:** The AssoCiAm benchmark effectively enhances the reliability of associative ability evaluations in MLLMs, helping to clarify the relationship between cognition and association.

**Abstract:** Recent advancements in multimodal large language models (MLLMs) have garnered significant attention, offering a promising pathway toward artificial general intelligence (AGI). Among the essential capabilities required for AGI, creativity has emerged as a critical trait for MLLMs, with association serving as its foundation. Association reflects a model' s ability to think creatively, making it vital to evaluate and understand. While several frameworks have been proposed to assess associative ability, they often overlook the inherent ambiguity in association tasks, which arises from the divergent nature of associations and undermines the reliability of evaluations. To address this issue, we decompose ambiguity into two types-internal ambiguity and external ambiguity-and introduce AssoCiAm, a benchmark designed to evaluate associative ability while circumventing the ambiguity through a hybrid computational method. We then conduct extensive experiments on MLLMs, revealing a strong positive correlation between cognition and association. Additionally, we observe that the presence of ambiguity in the evaluation process causes MLLMs' behavior to become more random-like. Finally, we validate the effectiveness of our method in ensuring more accurate and reliable evaluations. See Project Page for the data and codes.

</details>


### [61] [Synthesizing Behaviorally-Grounded Reasoning Chains: A Data-Generation Framework for Personal Finance LLMs](https://arxiv.org/abs/2509.14180)

*Akhil Theerthala*

**Main category:** cs.CL

**Keywords:** personal finance, LLM, behavioral finance, dataset, financial advice

**Relevance Score:** 4

**TL;DR:** The paper introduces a novel framework for personalized financial advice, integrating behavioral finance with a dataset for fine-tuning a smaller LLM model, yielding competitive performance at lower costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance personalized financial advice by integrating user goals, constraints, and behavioral finance to reduce the maintenance costs and improve financial returns.

**Method:** The authors developed a reproducible framework that combines financial context and behavioral insights to construct supervision data and fine-tune the Qwen-3-8B model on a 19k sample reasoning dataset.

**Key Contributions:**

	1. Introduced a novel framework for constructing supervision data for financial advisors.
	2. Created a large reasoning dataset for training scenarios in personal finance.
	3. Achieved competitive performance of a smaller model compared to larger models at reduced costs.

**Result:** The fine-tuned 8B model demonstrates performance comparable to larger models (14-32B parameters) across various metrics while incurring significantly lower costs (80% savings).

**Limitations:** The paper does not address the generalizability of the model beyond the specific financial contexts studied.

**Conclusion:** Carefully curated data and integration of behavioral finance enable smaller models to perform competitively, making personalized financial advice more accessible and cost-effective.

**Abstract:** Personalized financial advice requires consideration of user goals, constraints, risk tolerance, and jurisdiction. Prior LLM work has focused on support systems for investors and financial planners. Simultaneously, numerous recent studies examine broader personal finance tasks, including budgeting, debt management, retirement, and estate planning, through agentic pipelines that incur high maintenance costs, yielding less than 25% of their expected financial returns. In this study, we introduce a novel and reproducible framework that integrates relevant financial context with behavioral finance studies to construct supervision data for end-to-end advisors. Using this framework, we create a 19k sample reasoning dataset and conduct a comprehensive fine-tuning of the Qwen-3-8B model on the dataset. Through a held-out test split and a blind LLM-jury study, we demonstrate that through careful data curation and behavioral integration, our 8B model achieves performance comparable to significantly larger baselines (14-32B parameters) across factual accuracy, fluency, and personalization metrics while incurring 80% lower costs than the larger counterparts.

</details>


### [62] [Framing Migration: A Computational Analysis of UK Parliamentary Discourse](https://arxiv.org/abs/2509.14197)

*Vahid Ghafouri, Robert McNeil, Teodor Yankov, Madeleine Sumption, Luc Rocher, Scott A. Hale, Adam Mahdi*

**Main category:** cs.CL

**Keywords:** Migration Discourse, LLMs, Political Analysis, Narrative Frames, UK Parliament

**Relevance Score:** 5

**TL;DR:** A computational analysis of migration discourse in UK and US legislative debates using LLMs, revealing changes in attitudes over time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the evolution of migration-related discourse in UK parliamentary debates compared to US congressional discourse over 75 years.

**Method:** Large-scale computational analysis using open-weight LLMs to annotate statements and track stances and tones, alongside a semi-automated framework for extracting narrative frames.

**Key Contributions:**

	1. Development of a framework for fine-grained narrative frame extraction
	2. Insights into the ideological gap between UK political parties
	3. Demonstrated the utility of LLMs in discourse trends analysis over time

**Result:** The study found increasing polarization in US discourse while UK attitudes remained aligned across parties, with a notable shift towards securitized narratives and a decline in integration-focused discussions.

**Limitations:** 

**Conclusion:** LLMs can effectively facilitate scalable and detailed discourse analysis, providing insights into political and historical contexts.

**Abstract:** We present a large-scale computational analysis of migration-related discourse in UK parliamentary debates spanning over 75 years and compare it with US congressional discourse. Using open-weight LLMs, we annotate each statement with high-level stances toward migrants and track the net tone toward migrants across time and political parties. For the UK, we extend this with a semi-automated framework for extracting fine-grained narrative frames to capture nuances of migration discourse. Our findings show that, while US discourse has grown increasingly polarised, UK parliamentary attitudes remain relatively aligned across parties, with a persistent ideological gap between Labour and the Conservatives, reaching its most negative level in 2025. The analysis of narrative frames in the UK parliamentary statements reveals a shift toward securitised narratives such as border control and illegal immigration, while longer-term integration-oriented frames such as social integration have declined. Moreover, discussions of national law about immigration have been replaced over time by international law and human rights, revealing nuances in discourse trends. Taken together broadly, our findings demonstrate how LLMs can support scalable, fine-grained discourse analysis in political and historical contexts.

</details>


### [63] [Apertus: Democratizing Open and Compliant LLMs for Global Language Environments](https://arxiv.org/abs/2509.14233)

*Alejandro Hernández-Cano, Alexander Hägele, Allen Hao Huang, Angelika Romanou, Antoni-Joan Solergibert, Barna Pasztor, Bettina Messmer, Dhia Garbaya, Eduard Frank Ďurech, Ido Hakimi, Juan García Giraldo, Mete Ismayilzada, Negar Foroutan, Skander Moalla, Tiancheng Chen, Vinko Sabolčec, Yixuan Xu, Michael Aerni, Badr AlKhamissi, Ines Altemir Marinas, Mohammad Hossein Amani, Matin Ansaripour, Ilia Badanin, Harold Benoit, Emanuela Boros, Nicholas Browning, Fabian Bösch, Maximilian Böther, Niklas Canova, Camille Challier, Clement Charmillot, Jonathan Coles, Jan Deriu, Arnout Devos, Lukas Drescher, Daniil Dzenhaliou, Maud Ehrmann, Dongyang Fan, Simin Fan, Silin Gao, Miguel Gila, María Grandury, Diba Hashemi, Alexander Hoyle, Jiaming Jiang, Mark Klein, Andrei Kucharavy, Anastasiia Kucherenko, Frederike Lübeck, Roman Machacek, Theofilos Manitaras, Andreas Marfurt, Kyle Matoba, Simon Matrenok, Henrique Mendoncça, Fawzi Roberto Mohamed, Syrielle Montariol, Luca Mouchel, Sven Najem-Meyer, Jingwei Ni, Gennaro Oliva, Matteo Pagliardini, Elia Palme, Andrei Panferov, Léo Paoletti, Marco Passerini, Ivan Pavlov, Auguste Poiroux, Kaustubh Ponkshe, Nathan Ranchin, Javi Rando, Mathieu Sauser, Jakhongir Saydaliev, Muhammad Ali Sayfiddinov, Marian Schneider, Stefano Schuppli, Marco Scialanga, Andrei Semenov, Kumar Shridhar, Raghav Singhal, Anna Sotnikova, Alexander Sternfeld, Ayush Kumar Tarun, Paul Teiletche, Jannis Vamvas, Xiaozhe Yao, Hao Zhao Alexander Ilic, Ana Klimovic, Andreas Krause, Caglar Gulcehre, David Rosenthal, Elliott Ash, Florian Tramèr, Joost VandeVondele, Livio Veraldi, Martin Rajman, Thomas Schulthess, Torsten Hoefler, Antoine Bosselut, Martin Jaggi, Imanol Schlag*

**Main category:** cs.CL

**Keywords:** large language models, multilingual representation, data compliance

**Relevance Score:** 9

**TL;DR:** Apertus is an open suite of large language models aimed at improving data compliance and multilingual representation by using openly available data and enhancing support for non-English languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address systemic shortcomings in the open model ecosystem regarding data compliance and multilingual representation.

**Method:** Pretrained on openly available data while respecting content-owner rights and using the Goldfish objective to suppress verbatim data recall.

**Key Contributions:**

	1. Fully open models with respect for data compliance
	2. Enhanced multilingual representation with a significant portion of non-English data
	3. Release of all development artifacts for transparency and extension

**Result:** Apertus achieves state-of-the-art results among fully open models on multilingual benchmarks, with a strong focus on non-English content.

**Limitations:** 

**Conclusion:** Apertus provides a comprehensive suite of scientific artifacts for transparency, enabling further research and development in the open model space.

**Abstract:** We present Apertus, a fully open suite of large language models (LLMs) designed to address two systemic shortcomings in today's open model ecosystem: data compliance and multilingual representation. Unlike many prior models that release weights without reproducible data pipelines or regard for content-owner rights, Apertus models are pretrained exclusively on openly available data, retroactively respecting robots.txt exclusions and filtering for non-permissive, toxic, and personally identifiable content. To mitigate risks of memorization, we adopt the Goldfish objective during pretraining, strongly suppressing verbatim recall of data while retaining downstream task performance. The Apertus models also expand multilingual coverage, training on 15T tokens from over 1800 languages, with ~40% of pretraining data allocated to non-English content. Released at 8B and 70B scales, Apertus approaches state-of-the-art results among fully open models on multilingual benchmarks, rivalling or surpassing open-weight counterparts. Beyond model weights, we release all scientific artifacts from our development cycle with a permissive license, including data preparation scripts, checkpoints, evaluation suites, and training code, enabling transparent audit and extension.

</details>


### [64] [Large Language Models for Information Retrieval: A Survey](https://arxiv.org/abs/2308.07107)

*Yutao Zhu, Huaying Yuan, Shuting Wang, Jiongnan Liu, Wenhan Liu, Chenlong Deng, Haonan Chen, Zheng Liu, Zhicheng Dou, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Information Retrieval, Large Language Models, Neural Models, Natural Language Processing, Search Systems

**Relevance Score:** 8

**TL;DR:** This survey examines the integration of large language models (LLMs) with information retrieval (IR) systems, highlighting advancements and challenges.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To consolidate existing methodologies and provide insights into how LLMs can enhance IR systems amidst their evolving landscape.

**Method:** The survey reviews the confluence of traditional IR methods with modern neural architectures, focusing on LLM applications in query rewriting, retrieval, ranking, and reading tasks.

**Key Contributions:**

	1. Comprehensive overview of LLMs in IR systems
	2. Identifies key challenges and suggests future directions
	3. Highlights the integration of traditional and neural methods

**Result:** Identifies challenges such as data scarcity and interpretability while showcasing advancements in LLM-driven IR methods, along with potential future directions like search agents.

**Limitations:** The survey may not cover the very latest advancements post-publication and is limited to existing literature.

**Conclusion:** An emphasis on integrating both traditional and modern approaches is crucial for overcoming existing challenges in IR systems.

**Abstract:** As a primary means of information acquisition, information retrieval (IR) systems, such as search engines, have integrated themselves into our daily lives. These systems also serve as components of dialogue, question-answering, and recommender systems. The trajectory of IR has evolved dynamically from its origins in term-based methods to its integration with advanced neural models. While the neural models excel at capturing complex contextual signals and semantic nuances, thereby reshaping the IR landscape, they still face challenges such as data scarcity, interpretability, and the generation of contextually plausible yet potentially inaccurate responses. This evolution requires a combination of both traditional methods (such as term-based sparse retrieval methods with rapid response) and modern neural architectures (such as language models with powerful language understanding capacity). Meanwhile, the emergence of large language models (LLMs), typified by ChatGPT and GPT-4, has revolutionized natural language processing due to their remarkable language understanding, generation, generalization, and reasoning abilities. Consequently, recent research has sought to leverage LLMs to improve IR systems. Given the rapid evolution of this research trajectory, it is necessary to consolidate existing methodologies and provide nuanced insights through a comprehensive overview. In this survey, we delve into the confluence of LLMs and IR systems, including crucial aspects such as query rewriters, retrievers, rerankers, and readers. Additionally, we explore promising directions, such as search agents, within this expanding field.

</details>


### [65] [Annotation-Efficient Language Model Alignment via Diverse and Representative Response Texts](https://arxiv.org/abs/2405.13541)

*Yuu Jinnai, Ukyo Honda*

**Main category:** cs.CL

**Keywords:** preference optimization, annotation efficiency, large language models

**Relevance Score:** 8

**TL;DR:** Proposes Annotation-Efficient Preference Optimization (AEPO) to enhance preference dataset effectiveness despite limited annotations for fine-tuning large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addresses challenges in obtaining large preference datasets necessary for effective fine-tuning of language models according to human preferences.

**Method:** AEPO strategically selects a small, diverse, and representative subset of responses from a larger pool for preference annotation, optimizing the use of limited annotation resources.

**Key Contributions:**

	1. Introduces AEPO for efficient preference dataset creation.
	2. Demonstrates AEPO's superiority in preference learning with limited annotations.
	3. Provides an open-source implementation for reproducibility.

**Result:** AEPO consistently outperforms baseline methods on three datasets while using the same amount of annotation budget, demonstrating its effectiveness in preference learning.

**Limitations:** 

**Conclusion:** The proposed method provides a means to efficiently compile preference data, improving the alignment of language models with human user preferences under budget constraints.

**Abstract:** Preference optimization is a standard approach to fine-tuning large language models to align with human preferences. The quantity, diversity, and representativeness of the preference dataset are critical to the effectiveness of preference optimization. However, obtaining a large amount of preference annotations is difficult in many applications. This raises the question of how to use the limited annotation budget to create an effective preference dataset. To this end, we propose Annotation-Efficient Preference Optimization (AEPO). Instead of exhaustively annotating preference over all available response texts, AEPO selects a subset of responses that maximizes diversity and representativeness from the available responses and then annotates preference over the selected ones. In this way, AEPO focuses the annotation budget on labeling preferences over a smaller but informative subset of responses. We evaluate the performance of preference learning using AEPO on three datasets and show that it outperforms the baselines with the same annotation budget. Our code is available at https://github.com/CyberAgentAILab/annotation-efficient-po

</details>


### [66] [Database-Augmented Query Representation for Information Retrieval](https://arxiv.org/abs/2406.16013)

*Soyeong Jeong, Jinheon Baek, Sukmin Cho, Sung Ju Hwang, Jong C. Park*

**Main category:** cs.CL

**Keywords:** Information Retrieval, Query Expansion, Relational Databases, Metadata, Graph-based Encoding

**Relevance Score:** 6

**TL;DR:** This paper presents DAQu, a novel retrieval framework that enhances information retrieval by augmenting user queries with relevant metadata from relational databases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve document retrieval accuracy for short user queries by utilizing additional metadata from relational databases, addressing limitations in existing query expansion methods.

**Method:** DAQu augments the original query with metadata across multiple tables in a relational database and utilizes a graph-based set-encoding strategy to encode this metadata without order.

**Key Contributions:**

	1. Introduction of the Database-Augmented Query framework (DAQu)
	2. Application of graph-based set-encoding for metadata features
	3. Validation of DAQu in various retrieval scenarios showing increased performance.

**Result:** DAQu significantly enhances retrieval performance across diverse scenarios compared to relevant baselines.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates that leveraging relational database metadata can substantially improve document retrieval accuracy, with code availability for further research.

**Abstract:** Information retrieval models that aim to search for documents relevant to a query have shown multiple successes, which have been applied to diverse tasks. Yet, the query from the user is oftentimes short, which challenges the retrievers to correctly fetch relevant documents. To tackle this, previous studies have proposed expanding the query with a couple of additional (user-related) features related to it. However, they may be suboptimal to effectively augment the query, and there is plenty of other information available to augment it in a relational database. Motivated by this fact, we present a novel retrieval framework called Database-Augmented Query representation (DAQu), which augments the original query with various (query-related) metadata across multiple tables. In addition, as the number of features in the metadata can be very large and there is no order among them, we encode them with the graph-based set-encoding strategy, which considers hierarchies of features in the database without order. We validate our DAQu in diverse retrieval scenarios, demonstrating that it significantly enhances overall retrieval performance over relevant baselines. Our code is available at \href{https://github.com/starsuzi/DAQu}{this https URL}.

</details>


### [67] [MAgICoRe: Multi-Agent, Iterative, Coarse-to-Fine Refinement for Reasoning](https://arxiv.org/abs/2409.12147)

*Justin Chih-Yao Chen, Archiki Prasad, Swarnadeep Saha, Elias Stengel-Eskin, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** Large Language Models, refinement, multi-agent systems, error localization, machine learning

**Relevance Score:** 8

**TL;DR:** MAgICoRe enhances Large Language Model reasoning through targeted multi-agent refinement, improving performance on math datasets while addressing common pitfalls of excessive, insufficient, and error-prone refinements.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning of Large Language Models (LLMs) using test-time aggregation strategies and refine solutions with mechanisms that address common challenges in LLM performance.

**Method:** MAgICoRe categorizes problem difficulty and utilizes a multi-agent loop involving a Solver, Reviewer, and Refiner to optimize feedback and iterative refinement based on external reward model scores.

**Key Contributions:**

	1. Introduction of MAgICoRe for effective LLM refinement
	2. Use of multi-agent systems for enhanced error localization
	3. Proven effectiveness over existing aggregation strategies in LLM performance

**Result:** MAgICoRe outperforms existing methods, showing a 3.4% improvement over Self-Consistency and greater efficacy in multi-agent learning, continuing to improve with more iterations.

**Limitations:** 

**Conclusion:** MAgICoRe effectively enhances LLM reasoning through a structured approach to problem difficulty and iterative refinement, outperforming traditional methods across multiple math datasets.

**Abstract:** Large Language Models' (LLM) reasoning can be improved using test-time aggregation strategies, i.e., generating multiple samples and voting among generated samples. While these improve performance, they often reach a saturation point. Refinement offers an alternative by using LLM-generated feedback to improve solution quality. However, refinement introduces 3 key challenges: (1) Excessive refinement: Uniformly refining all instances can over-correct and reduce the overall performance. (2) Inability to localize and address errors: LLMs have a limited ability to self-correct and struggle to identify and correct their own mistakes. (3) Insufficient refinement: Deciding how many iterations of refinement are needed is non-trivial, and stopping too soon could leave errors unaddressed. To tackle these issues, we propose MAgICoRe, which avoids excessive refinement by categorizing problem difficulty as easy or hard, solving easy problems with coarse-grained aggregation and hard ones with fine-grained and iterative multi-agent refinement. To improve error localization, we incorporate external step-wise reward model (RM) scores. Moreover, to ensure effective refinement, we employ a multi-agent loop with three agents: Solver, Reviewer (which generates targeted feedback based on step-wise RM scores), and the Refiner (which incorporates feedback). To ensure sufficient refinement, we re-evaluate updated solutions, iteratively initiating further rounds of refinement. We evaluate MAgICoRe on Llama-3-8B and GPT-3.5 and show its effectiveness across 5 math datasets. Even one iteration of MAgICoRe beats Self-Consistency by 3.4%, Best-of-k by 3.2%, and Self-Refine by 4.0% while using less than half the samples. Unlike iterative refinement with baselines, MAgICoRe continues to improve with more iterations. Finally, our ablations highlight the importance of MAgICoRe's RMs and multi-agent communication.

</details>


### [68] [DAVIS: Planning Agent with Knowledge Graph-Powered Inner Monologue](https://arxiv.org/abs/2410.09252)

*Minh Pham Dinh, Munira Syed, Michael G Yankoski, Trenton W. Ford*

**Main category:** cs.CL

**Keywords:** AI, Human-Computer Interaction, RAG, Machine Learning, Scientific Agents

**Relevance Score:** 9

**TL;DR:** The paper presents DAVIS, a scientific AI agent that integrates structured and temporal memory for enhanced reasoning and task performance in laboratory settings, achieving significant improvements over existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to design a generalist scientific agent capable of assisting researchers in complex laboratory tasks, which require advanced reasoning and safety considerations.

**Method:** DAVIS combines structured and temporal memory with model-based planning, and incorporates a multi-turn retrieval system to facilitate reasoning akin to human inner monologue.

**Key Contributions:**

	1. Introduction of structured and temporal memory in RAG systems
	2. Implementation of a multi-turn retrieval system for enhanced reasoning
	3. First RAG agent to use interactive retrieval in its pipeline

**Result:** DAVIS shows substantial performance improvements on the ScienceWorld benchmark in 8 out of 9 science subjects and competitive results on HotpotQA and MusiqueQA datasets.

**Limitations:** 

**Conclusion:** DAVIS represents a novel approach to retrieval-augmented generation by utilizing interactive retrieval within a RAG pipeline, enhancing the agent’s reasoning capabilities.

**Abstract:** Designing a generalist scientific agent capable of performing tasks in laboratory settings to assist researchers has become a key goal in recent Artificial Intelligence (AI) research. Unlike everyday tasks, scientific tasks are inherently more delicate and complex, requiring agents to possess a higher level of reasoning ability, structured and temporal understanding of their environment, and a strong emphasis on safety. Existing approaches often fail to address these multifaceted requirements. To tackle these challenges, we present DAVIS. Unlike traditional retrieval-augmented generation (RAG) approaches, DAVIS incorporates structured and temporal memory, which enables model-based planning. Additionally, DAVIS implements an agentic, multi-turn retrieval system, similar to a human's inner monologue, allowing for a greater degree of reasoning over past experiences. DAVIS demonstrates substantially improved performance on the ScienceWorld benchmark comparing to previous approaches on 8 out of 9 elementary science subjects. In addition, DAVIS's World Model demonstrates competitive performance on the famous HotpotQA and MusiqueQA dataset for multi-hop question answering. To the best of our knowledge, DAVIS is the first RAG agent to employ an interactive retrieval method in a RAG pipeline.

</details>


### [69] [Mirror-Consistency: Harnessing Inconsistency in Majority Voting](https://arxiv.org/abs/2410.10857)

*Siyuan Huang, Zhiyuan Ma, Jintao Du, Changhua Meng, Weiqiang Wang, Zhouhan Lin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Consistency, Mirror-Consistency, Confidence Calibration, Reasoning Accuracy

**Relevance Score:** 9

**TL;DR:** Presentation of Mirror-Consistency, an improved decoding strategy for LLMs that enhances reasoning capabilities and confidence calibration by addressing the limitations of traditional Self-Consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this paper is to overcome the limitations of the Self-Consistency decoding strategy used in Large Language Models, which ignores minority responses that could reveal important areas of uncertainty.

**Method:** The proposed Mirror-Consistency method integrates a reflective mirror into the self-ensemble decoding process, allowing LLMs to analyze and incorporate inconsistencies from multiple generations for better decision-making.

**Key Contributions:**

	1. Introduction of Mirror-Consistency as a decoding enhancement for LLMs.
	2. Demonstration of improved reasoning accuracy compared to Self-Consistency.
	3. Proposed method for better confidence calibration of LLM outputs.

**Result:** Experimental results show that Mirror-Consistency outperforms traditional Self-Consistency in both reasoning accuracy and confidence calibration.

**Limitations:** 

**Conclusion:** The proposed method offers a new approach to enhance LLM performance by addressing overconfidence and improving the understanding of model generation uncertainties.

**Abstract:** Self-Consistency, a widely-used decoding strategy, significantly boosts the reasoning capabilities of Large Language Models (LLMs). However, it depends on the plurality voting rule, which focuses on the most frequent answer while overlooking all other minority responses. These inconsistent minority views often illuminate areas of uncertainty within the model's generation process. To address this limitation, we present Mirror-Consistency, an enhancement of the standard Self-Consistency approach. Our method incorporates a 'reflective mirror' into the self-ensemble decoding process and enables LLMs to critically examine inconsistencies among multiple generations. Additionally, just as humans use the mirror to better understand themselves, we propose using Mirror-Consistency to enhance the sample-based confidence calibration methods, which helps to mitigate issues of overconfidence. Our experimental results demonstrate that Mirror-Consistency yields superior performance in both reasoning accuracy and confidence calibration compared to Self-Consistency.

</details>


### [70] [Unlocking Legal Knowledge: A Multilingual Dataset for Judicial Summarization in Switzerland](https://arxiv.org/abs/2410.13456)

*Luca Rolshoven, Vishvaksenan Rasiah, Srinanda Brügger Bose, Sarah Hostettler, Lara Burkhalter, Matthias Stürmer, Joel Niklaus*

**Main category:** cs.CL

**Keywords:** legal research, automated summarization, court rulings, cross-lingual, dataset

**Relevance Score:** 3

**TL;DR:** Automated headnote creation can enhance legal research efficiency by summarizing court rulings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Improving accessibility of legal cases through automated summarization to assist legal professionals in Switzerland.

**Method:** Introduced the Swiss Leading Decision Summarization (SLDS) dataset comprising 18K court rulings in multiple languages and fine-tuned various mT5 models for summarization tasks.

**Key Contributions:**

	1. Creation of the SLDS dataset with multilingual legal data
	2. Evaluation of mT5 models for legal summarization
	3. Insights into zero-shot vs. fine-tuned model performance in legal contexts

**Result:** Proprietary models excel in zero-shot settings, while fine-tuned smaller models offer competitive performance for summarization of legal documents.

**Limitations:** 

**Conclusion:** The dataset is publicly available to encourage further research and the development of legal assistive technologies.

**Abstract:** Legal research is a time-consuming task that most lawyers face on a daily basis. A large part of legal research entails looking up relevant caselaw and bringing it in relation to the case at hand. Lawyers heavily rely on summaries (also called headnotes) to find the right cases quickly. However, not all decisions are annotated with headnotes and writing them is time-consuming. Automated headnote creation has the potential to make hundreds of thousands of decisions more accessible for legal research in Switzerland alone. To kickstart this, we introduce the Swiss Leading Decision Summarization ( SLDS) dataset, a novel cross-lingual resource featuring 18K court rulings from the Swiss Federal Supreme Court (SFSC), in German, French, and Italian, along with German headnotes. We fine-tune and evaluate three mT5 variants, along with proprietary models. Our analysis highlights that while proprietary models perform well in zero-shot and one-shot settings, fine-tuned smaller models still provide a strong competitive edge. We publicly release the dataset to facilitate further research in multilingual legal summarization and the development of assistive technologies for legal professionals

</details>


### [71] [KBM: Delineating Knowledge Boundary for Adaptive Retrieval in Large Language Models](https://arxiv.org/abs/2411.06207)

*Zhen Zhang, Xinyu Wang, Yong Jiang, Zile Qiao, Zhuo Chen, Guangyu Li, Feiteng Mu, Mengting Hu, Pengjun Xie, Fei Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Knowledge Boundary Model, dynamic knowledge, long-tail static knowledge

**Relevance Score:** 9

**TL;DR:** This paper introduces a Knowledge Boundary Model (KBM) to optimize Retrieval-Augmented Generation (RAG) for Large Language Models (LLMs) by determining when RAG is necessary, thereby improving computational efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with keeping up-to-date with dynamic knowledge and managing unknown static information, requiring a more efficient RAG approach.

**Method:** The authors propose the KBM to identify the known and unknown components of a question, enabling selective triggering of RAG processes.

**Key Contributions:**

	1. Proposed the Knowledge Boundary Model (KBM) for LLMs
	2. Demonstrated efficiency in reducing computational costs during text generation
	3. Evaluated KBM in diverse contexts including dynamic and long-tail knowledge scenarios.

**Result:** Experiments show that KBM effectively reduces unnecessary retrievals while maintaining or improving LLM performance across multiple datasets in English and Chinese.

**Limitations:** 

**Conclusion:** The KBM model significantly enhances performance by decreasing retrievals needed, demonstrating its utility in dynamic knowledge handling and as an LLM plug-in.

**Abstract:** Large Language Models (LLMs) often struggle with dynamically changing knowledge and handling unknown static information. Retrieval-Augmented Generation (RAG) is employed to tackle these challenges and has a significant impact on improving LLM performance. In fact, we find that not all questions need to trigger RAG. By retrieving parts of knowledge unknown to the LLM and allowing the LLM to answer the rest, we can effectively reduce both time and computational costs. In our work, we propose a Knowledge Boundary Model (KBM) to express the known/unknown of a given question, and to determine whether a RAG needs to be triggered. Experiments conducted on 11 English and Chinese datasets illustrate that the KBM effectively delineates the knowledge boundary, significantly decreasing the proportion of retrievals required for optimal end-to-end performance. Furthermore, we evaluate the effectiveness of KBM in three complex scenarios: dynamic knowledge, long-tail static knowledge, and multi-hop problems, as well as its functionality as an external LLM plug-in.

</details>


### [72] [Human-in-the-Loop Generation of Adversarial Texts: A Case Study on Tibetan Script](https://arxiv.org/abs/2412.12478)

*Xi Cao, Yuan Sun, Jiajun Li, Quzong Gesang, Nuo Qun, Tashi Nyima*

**Main category:** cs.CL

**Keywords:** adversarial attacks, NLP security, lower-resourced languages

**Relevance Score:** 7

**TL;DR:** HITL-GAT is an interactive system for generating adversarial texts to improve NLP security, specifically focusing on lower-resourced languages like Tibetan.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the vulnerability of DNN-based language models to adversarial attacks, especially for lower-resourced languages that lack established benchmarks.

**Method:** An interactive system called HITL-GAT is introduced, utilizing human-in-the-loop techniques to generate adversarial texts tailored to linguistic and resource challenges of lower-resourced languages.

**Key Contributions:**

	1. Introduction of HITL-GAT for adversarial text generation
	2. Developed adversarial methods tailored for Tibetan script
	3. Established the first adversarial robustness benchmark for lower-resourced languages

**Result:** HITL-GAT was applied to develop adversarial text generation methods for Tibetan script and established a first adversarial robustness benchmark for this language.

**Limitations:** The effectiveness of the methods may vary with other languages and continuous evolution of language models may affect past adversarial texts.

**Conclusion:** The proposed system offers significant advancements for assessing adversarial robustness in lower-resourced languages, providing foundational benchmarks and methods for future research.

**Abstract:** DNN-based language models excel across various NLP tasks but remain highly vulnerable to textual adversarial attacks. While adversarial text generation is crucial for NLP security, explainability, evaluation, and data augmentation, related work remains overwhelmingly English-centric, leaving the problem of constructing high-quality and sustainable adversarial robustness benchmarks for lower-resourced languages both difficult and understudied. First, method customization for lower-resourced languages is complicated due to linguistic differences and limited resources. Second, automated attacks are prone to generating invalid or ambiguous adversarial texts. Last but not least, language models continuously evolve and may be immune to parts of previously generated adversarial texts. To address these challenges, we introduce HITL-GAT, an interactive system based on a general approach to human-in-the-loop generation of adversarial texts. Additionally, we demonstrate the utility of HITL-GAT through a case study on Tibetan script, employing three customized adversarial text generation methods and establishing its first adversarial robustness benchmark, providing a valuable reference for other lower-resourced languages.

</details>


### [73] [Enhancing the De-identification of Personally Identifiable Information in Educational Data](https://arxiv.org/abs/2501.09765)

*Zilyu Ji, Yuntian Shen, Jionghao Lin, Kenneth R. Koedinger*

**Main category:** cs.CL

**Keywords:** PII detection, GPT-4o-mini, educational data, privacy protection, fine-tuning

**Relevance Score:** 7

**TL;DR:** The study investigates the GPT-4o-mini model for PII detection in educational data, assessing its fine-tuning versus established frameworks and demonstrating superior performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To protect Personally Identifiable Information (PII) in educational technologies while maintaining data utility.

**Method:** The research employs prompting and fine-tuning approaches to evaluate and compare the performance of the GPT-4o-mini model against established frameworks.

**Key Contributions:**

	1. Investigation of GPT-4o-mini model for PII detection tasks in educational data.
	2. Comparison of fine-tuning and prompting approaches against established frameworks.
	3. Demonstration of the model's cost-effectiveness and improved performance metrics.

**Result:** The fine-tuned GPT-4o-mini model achieved a recall of 0.9589 on the CRAPII dataset, showing improved precision and reduced computational costs compared to Azure AI Language.

**Limitations:** 

**Conclusion:** Fine-tuned GPT-4o-mini proves to be a cost-effective and accurate tool for PII detection, ensuring privacy protection and enhancing data utility for educational purposes.

**Abstract:** Protecting Personally Identifiable Information (PII), such as names, is a critical requirement in learning technologies to safeguard student and teacher privacy and maintain trust. Accurate PII detection is an essential step toward anonymizing sensitive information while preserving the utility of educational data. Motivated by recent advancements in artificial intelligence, our study investigates the GPT-4o-mini model as a cost-effective and efficient solution for PII detection tasks. We explore both prompting and fine-tuning approaches and compare GPT-4o-mini's performance against established frameworks, including Microsoft Presidio and Azure AI Language. Our evaluation on two public datasets, CRAPII and TSCC, demonstrates that the fine-tuned GPT-4o-mini model achieves superior performance, with a recall of 0.9589 on CRAPII. Additionally, fine-tuned GPT-4o-mini significantly improves precision scores (a threefold increase) while reducing computational costs to nearly one-tenth of those associated with Azure AI Language. Furthermore, our bias analysis reveals that the fine-tuned GPT-4o-mini model consistently delivers accurate results across diverse cultural backgrounds and genders. The generalizability analysis using the TSCC dataset further highlights its robustness, achieving a recall of 0.9895 with minimal additional training data from TSCC. These results emphasize the potential of fine-tuned GPT-4o-mini as an accurate and cost-effective tool for PII detection in educational data. It offers robust privacy protection while preserving the data's utility for research and pedagogical analysis. Our code is available on GitHub: https://github.com/AnonJD/PrivacyAI

</details>


### [74] [Beyond checkmate: exploring the creative chokepoints in AI text](https://arxiv.org/abs/2501.19301)

*Nafis Irtiza Tripto, Saranya Venkatraman, Mahjabin Nahar, Dongwon Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Text Generation, Human-AI Text Distinction, Natural Language Processing, Detection Strategies

**Relevance Score:** 9

**TL;DR:** This study investigates the distinctions between human and AI-generated texts across various segments to enhance detection strategies for LLM-generated content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of Large Language Models, concerns about the misuse of text generation have grown, necessitating effective detection methods for LLM-generated text.

**Method:** The research analyzes human and AI texts through segment-specific patterns, drawing an analogy to chess games, to uncover differences in writing styles across segments like introduction, body, and conclusion.

**Key Contributions:**

	1. Introduces segment-specific analysis to differentiate human and AI-generated texts
	2. Reveals that the body segment, despite its length, is less unique compared to other segments
	3. Highlights human stylistic variation as a key factor in distinguishing between text types.

**Result:** AI texts, while closely resembling human writing in the body section, show significant differences in features such as linguistic flow, with human texts displaying greater stylistic variation across segments.

**Limitations:** 

**Conclusion:** The study provides insights into the differences between human and AI texts that can inform better detection techniques and understanding of LLM capabilities.

**Abstract:** The rapid advancement of Large Language Models (LLMs) has revolutionized text generation but also raised concerns about potential misuse, making detecting LLM-generated text (AI text) increasingly essential. While prior work has focused on identifying AI text and effectively checkmating it, our study investigates a less-explored territory: portraying the nuanced distinctions between human and AI texts across text segments (introduction, body, and conclusion). Whether LLMs excel or falter in incorporating linguistic ingenuity across text segments, the results will critically inform their viability and boundaries as effective creative assistants to humans. Through an analogy with the structure of chess games, comprising opening, middle, and end games, we analyze segment-specific patterns to reveal where the most striking differences lie. Although AI texts closely resemble human writing in the body segment due to its length, deeper analysis shows a higher divergence in features dependent on the continuous flow of language, making it the most informative segment for detection. Additionally, human texts exhibit greater stylistic variation across segments, offering a new lens for distinguishing them from AI. Overall, our findings provide fresh insights into human-AI text differences and pave the way for more effective and interpretable detection strategies. Codes available at https://github.com/tripto03/chess_inspired_human_ai_text_distinction.

</details>


### [75] [Forget What You Know about LLMs Evaluations -- LLMs are Like a Chameleon](https://arxiv.org/abs/2502.07445)

*Nurit Cohen-Inger, Yehonatan Elisha, Bracha Shapira, Lior Rokach, Seffi Cohen*

**Main category:** cs.CL

**Keywords:** large language models, benchmark overfitting, C-BOD, model evaluation, language understanding

**Relevance Score:** 8

**TL;DR:** The paper introduces the Chameleon Benchmark Overfit Detector (C-BOD) to evaluate overfitting in large language models (LLMs) by distorting benchmark prompts and assessing true language understanding beyond surface cues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns that high benchmark scores of LLMs may not reflect genuine language understanding but rather an overreliance on surface patterns in the training data.

**Method:** C-BOD introduces a framework that systematically distorts benchmark prompts while preserving semantic content to evaluate LLMs' susceptibility to memorized patterns.

**Key Contributions:**

	1. Introduction of C-BOD for detecting overfitting in LLMs
	2. Revealing performance biases in LLMs based on prompt rephrasing
	3. Highlighting the importance of robustness and generalization in LLM evaluations

**Result:** The evaluation revealed a 2.15% average performance decrease across 26 LLMs, with 20 models showing significant performance differences under perturbation, indicating overfitting tendencies, especially in higher accuracy models.

**Limitations:** 

**Conclusion:** C-BOD provides a method to improve LLM training processes by promoting models that exhibit resilience and generalization instead of simply high leaderboard scores.

**Abstract:** Large language models (LLMs) often appear to excel on public benchmarks, but these high scores may mask an overreliance on dataset-specific surface cues rather than true language understanding. We introduce the Chameleon Benchmark Overfit Detector (C-BOD), a meta-evaluation framework that systematically distorts benchmark prompts via a parametric transformation and detects overfitting of LLMs. By rephrasing inputs while preserving their semantic content and labels, C-BOD exposes whether a model's performance is driven by memorized patterns. Evaluated on the MMLU benchmark using 26 leading LLMs, our method reveals an average performance degradation of 2.15% under modest perturbations, with 20 out of 26 models exhibiting statistically significant differences. Notably, models with higher baseline accuracy exhibit larger performance differences under perturbation, and larger LLMs tend to be more sensitive to rephrasings, indicating that both cases may overrely on fixed prompt patterns. In contrast, the Llama family and models with lower baseline accuracy show insignificant degradation, suggesting reduced dependency on superficial cues. Moreover, C-BOD's dataset- and model-agnostic design allows easy integration into training pipelines to promote more robust language understanding. Our findings challenge the community to look beyond leaderboard scores and prioritize resilience and generalization in LLM evaluation.

</details>


### [76] [What's Not Said Still Hurts: A Description-Based Evaluation Framework for Measuring Social Bias in LLMs](https://arxiv.org/abs/2502.19749)

*Jinhao Pan, Chahat Raj, Ziyu Yao, Ziwei Zhu*

**Main category:** cs.CL

**Keywords:** Large Language Models, social bias, benchmarks, natural language processing, semantics

**Relevance Score:** 9

**TL;DR:** This paper introduces a new benchmark for evaluating social bias in Large Language Models (LLMs) by focusing on semantic-level biases hidden in context rather than term-based associations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the shortcomings of existing bias benchmarks that primarily focus on direct term associations, which fail to identify more subtle, contextually hidden biases in LLMs.

**Method:** The authors present the Description-based Bias Benchmark (DBB), a dataset that assesses LLMs for biases at the semantic level in real-world contexts.

**Key Contributions:**

	1. Introduction of the Description-based Bias Benchmark (DBB) for evaluating biases in LLMs.
	2. Demonstration of subtle biases in LLMs that are not captured by traditional metrics.
	3. Release of data, code, and results to promote further evaluation of bias in LLMs.

**Result:** Analysis of six state-of-the-art LLMs shows that while they reduce bias at the term level, they continue to reflect biases in more nuanced and subtle contexts.

**Limitations:** 

**Conclusion:** DBB provides a more accurate measure of bias in LLMs, revealing persistent issues that traditional benchmarks miss; the dataset and results are freely available for further research.

**Abstract:** Large Language Models (LLMs) often exhibit social biases inherited from their training data. While existing benchmarks evaluate bias by term-based mode through direct term associations between demographic terms and bias terms, LLMs have become increasingly adept at avoiding biased responses, leading to seemingly low levels of bias. However, biases persist in subtler, contextually hidden forms that traditional benchmarks fail to capture. We introduce the Description-based Bias Benchmark (DBB), a novel dataset designed to assess bias at the semantic level that bias concepts are hidden within naturalistic, subtly framed contexts in real-world scenarios rather than superficial terms. We analyze six state-of-the-art LLMs, revealing that while models reduce bias in response at the term level, they continue to reinforce biases in nuanced settings. Data, code, and results are available at https://github.com/JP-25/Description-based-Bias-Benchmark.

</details>


### [77] [Puzzled by Puzzles: When Vision-Language Models Can't Take a Hint](https://arxiv.org/abs/2505.23759)

*Heekyung Lee, Jiaxin Ge, Tsung-Han Wu, Minwoo Kang, Trevor Darrell, David M. Chan*

**Main category:** cs.CL

**Keywords:** vision-language models, rebus puzzles, multi-modal reasoning, symbolic reasoning, cultural understanding

**Relevance Score:** 6

**TL;DR:** The paper evaluates vision-language models (VLMs) in solving rebus puzzles, highlighting their strengths in simple visual decoding and weaknesses in abstract reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how current VLMs interpret and solve rebus puzzles, which require complex reasoning and cultural understanding beyond standard tasks.

**Method:** The authors created a hand-generated and annotated benchmark of English rebus puzzles and analyzed the performance of various VLMs on these tasks.

**Key Contributions:**

	1. Creation of a benchmark for rebus puzzles for VLM assessment
	2. Analysis of VLM capabilities in solving complex, culturally nuanced puzzles
	3. Identification of specific limitations in abstract reasoning and metaphor understanding in VLMs

**Result:** VLMs show some ability to decode simple visual clues but struggle significantly with abstract reasoning and understanding visual metaphors.

**Limitations:** The puzzles constructed may not encompass all possible linguistic and visual challenges present in rebus puzzles.

**Conclusion:** The evaluation reveals both the potential and limitations of VLMs in tackling tasks that require multi-modal abstraction.

**Abstract:** Rebus puzzles, visual riddles that encode language through imagery, spatial arrangement, and symbolic substitution, pose a unique challenge to current vision-language models (VLMs). Unlike traditional image captioning or question answering tasks, rebus solving requires multi-modal abstraction, symbolic reasoning, and a grasp of cultural, phonetic and linguistic puns. In this paper, we investigate the capacity of contemporary VLMs to interpret and solve rebus puzzles by constructing a hand-generated and annotated benchmark of diverse English-language rebus puzzles, ranging from simple pictographic substitutions to spatially-dependent cues ("head" over "heels"). We analyze how different VLMs perform, and our findings reveal that while VLMs exhibit some surprising capabilities in decoding simple visual clues, they struggle significantly with tasks requiring abstract reasoning, lateral thinking, and understanding visual metaphors.

</details>


### [78] [A Culturally-diverse Multilingual Multimodal Video Benchmark & Model](https://arxiv.org/abs/2506.07032)

*Bhuiyan Sanjid Shafique, Ashmal Vayani, Muhammad Maaz, Hanoona Abdul Rasheed, Dinura Dissanayake, Mohammed Irfan Kurpath, Yahya Hmaiti, Go Inoue, Jean Lahoud, Md. Safirur Rashid, Shadid Intisar Quasem, Maheen Fatima, Franco Vidal, Mykola Maslych, Ketan Pravin More, Sanoojan Baliah, Hasindri Watawana, Yuhao Li, Fabian Farestam, Leon Schaller, Roman Tymtsiv, Simon Weber, Hisham Cholakkal, Ivan Laptev, Shin'ichi Satoh, Michael Felsberg, Mubarak Shah, Salman Khan, Fahad Shahbaz Khan*

**Main category:** cs.CL

**Keywords:** multimodal models, multilingual evaluation, video analysis, cultural inclusivity, machine learning

**Relevance Score:** 8

**TL;DR:** Introducing ViMUL-Bench, a benchmark to evaluate multilingual video large multimodal models (LMMs) across 14 languages; it includes a dataset and a model designed for inclusivity in cultural representation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for cultural and linguistic inclusivity in video large multimodal models (LMMs) beyond the English language has not been adequately addressed in previous research.

**Method:** Introduction of ViMUL-Bench to evaluate Video LMMs across 14 languages with a specific focus on low- and high-resource languages; creation of a machine translated multilingual video training set.

**Key Contributions:**

	1. ViMUL-Bench for evaluating multilingual video LMM capabilities across diverse languages
	2. Creation of a large-scale multilingual video training dataset
	3. Development of ViMUL, a multilingual LMM that offers better tradeoff for language resources

**Result:** ViMUL-Bench includes 8,000 manually verified samples across various categories and a multilingual video LMM named ViMUL that improves understanding for both high- and low-resource languages.

**Limitations:** 

**Conclusion:** ViMUL-Bench and the corresponding multilingual video model and training set aim to facilitate research towards developing more inclusive multilingual video LMMs and are publicly available.

**Abstract:** Large multimodal models (LMMs) have recently gained attention due to their effectiveness to understand and generate descriptions of visual content. Most existing LMMs are in English language. While few recent works explore multilingual image LMMs, to the best of our knowledge, moving beyond the English language for cultural and linguistic inclusivity is yet to be investigated in the context of video LMMs. In pursuit of more inclusive video LMMs, we introduce a multilingual Video LMM benchmark, named ViMUL-Bench, to evaluate Video LMMs across 14 languages, including both low- and high-resource languages: English, Chinese, Spanish, French, German, Hindi, Arabic, Russian, Bengali, Urdu, Sinhala, Tamil, Swedish, and Japanese. Our ViMUL-Bench is designed to rigorously test video LMMs across 15 categories including eight culturally diverse categories, ranging from lifestyles and festivals to foods and rituals and from local landmarks to prominent cultural personalities. ViMUL-Bench comprises both open-ended (short and long-form) and multiple-choice questions spanning various video durations (short, medium, and long) with 8k samples that are manually verified by native language speakers. In addition, we also introduce a machine translated multilingual video training set comprising 1.2 million samples and develop a simple multilingual video LMM, named ViMUL, that is shown to provide a better tradeoff between high-and low-resource languages for video understanding. We hope our ViMUL-Bench and multilingual video LMM along with a large-scale multilingual video training set will help ease future research in developing cultural and linguistic inclusive multilingual video LMMs. Our proposed benchmark, video LMM and training data will be publicly released at https://mbzuai-oryx.github.io/ViMUL/.

</details>
