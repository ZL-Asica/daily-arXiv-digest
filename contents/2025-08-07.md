# 2025-08-07

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 26]

- [cs.CL](#cs.CL) [Total: 84]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [MagicGUI: A Foundational Mobile GUI Agent with Scalable Data Pipeline and Reinforcement Fine-tuning](https://arxiv.org/abs/2508.03700)

*Liujian Tang, Shaokang Dong, Yijia Huang, Minqi Xiang, Hongtao Ruan, Bin Wang, Shuo Li, Zhihui Cao, Hailiang Pang, Heng Kong, He Yang, Mingxu Chai, Zhilin Gao, Xingyu Liu, Yingnan Fu, Jiaming Liu, Tao Gui, Xuanjing Huang, Yu-Gang Jiang, Qi Zhang, Kang Wang, Yunke Zhang, Yuran Wang*

**Main category:** cs.HC

**Keywords:** Mobile GUI, Human-Agent Interaction, Multimodal Data

**Relevance Score:** 8

**TL;DR:** MagicGUI is a mobile GUI agent that enhances perception, grounding, and reasoning for real-world applications through a robust dataset and advanced training methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address critical challenges in mobile GUI environments related to perception, grounding, and reasoning.

**Method:** The framework incorporates six key components, including a comprehensive dataset, enhanced perception capabilities, a unified action space, planning-oriented reasoning mechanisms, an iterative training procedure, and competitive performance on benchmarks.

**Key Contributions:**

	1. Foundational mobile GUI agent framework
	2. Comprehensive multimodal dataset
	3. Iterative training procedure combining pre-training and reinforcement learning

**Result:** MagicGUI demonstrates superior performance in GUI perception and agent tasks across various benchmarks, indicating strong generalization and real-world applicability.

**Limitations:** 

**Conclusion:** The development of MagicGUI presents a significant advancement in mobile GUI agent capabilities, making it suitable for practical applications.

**Abstract:** This paper presents MagicGUI, a foundational mobile GUI agent designed to address critical challenges in perception, grounding, and reasoning within real-world mobile GUI environments. The framework is underpinned by following six key components: (1) a comprehensive and accurate dataset, constructed via the scalable GUI Data Pipeline, which aggregates the largest and most diverse GUI-centric multimodal data to date from open-source repositories, automated crawling, and targeted manual annotation; (2) enhanced perception and grounding capabilities, facilitating fine-grained multimodal alignment for UI element referencing, grounding, and screen comprehension; (3) a comprehensive and unified action space, encompassing both fundamental UI operations and complex interactive intents to support human-agent interactions; (4) planning-oriented reasoning mechanisms that enable the model to decompose complex user instructions into sequential actions with explicit intermediate meta-paln reasoning; (5) an iterative two-stage training procedure, combining large-scale continue pre-training on 7.8M samples with reinforcement fine-tuning utilizing a spatially enhanced composite reward and dual filtering strategy; and (6) competitive performance on both the proprietary Magic-RICH benchmark and over a dozen public benchmarks, achieving superior performance across GUI perception and agent tasks, while demonstrating robust generalization and real-world deployment potential in practical mobile GUI scenarios, as detailed in Figure 1.

</details>


### [2] [Screen Matters: Cognitive and Behavioral Divergence Between Smartphone-Native and Computer-Native Youth](https://arxiv.org/abs/2508.03705)

*Kanan Eldarov*

**Main category:** cs.HC

**Keywords:** digital interaction, adolescents, attention, creativity, user interface

**Relevance Score:** 7

**TL;DR:** The study examines how computer and smartphone interactions impact attention, frustration, and creativity in adolescents.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how different digital interaction modes influence cognitive and behavioral outcomes in educational contexts.

**Method:** Data from 824 students aged 11-17 was collected via digital task logs, webcam gaze estimation, and expert evaluation in a randomized, stratified design.

**Key Contributions:**

	1. Identification of device-specific differences in cognitive performance in adolescents
	2. Analysis based on a large and diverse sample size
	3. Recommendations for educational user interface design

**Result:** Significant differences were found in sustained attention, frustration levels, and creative output between device types.

**Limitations:** The study is limited to adolescents aged 11-17 and may not generalize to other age groups or contexts.

**Conclusion:** The choice of device for digital interactions can significantly affect educational outcomes and informs user interface design.

**Abstract:** This study explores how different modes of digital interaction -- namely, computers versus smartphones -- affect attention, frustration, and creative performance in adolescents. Using a combination of digital task logs, webcam-based gaze estimation, and expert evaluation of task outcomes, we analyzed data from a diverse sample of 824 students aged 11-17. Participants were assigned to device groups in a randomized and stratified design to control for age, gender, and prior experience. Results suggest moderate but statistically significant differences in sustained attention, perceived frustration, and creative output. These findings indicate that the nature of digital interaction -- beyond mere screen time -- may influence cognitive and behavioral outcomes relevant to educational design. Practical implications for user interface development and learning environments are discussed.

</details>


### [3] [Tell Me Without Telling Me: Two-Way Prediction of Visualization Literacy and Visual Attention](https://arxiv.org/abs/2508.03713)

*Minsuk Chang, Yao Wang, Huichen Will Wang, Yuanhong Zhou, Andreas Bulling, Cindy Xiong Bearfield*

**Main category:** cs.HC

**Keywords:** visualization, visual literacy, attention patterns, saliency modeling, personalized communication

**Relevance Score:** 8

**TL;DR:** This paper presents two computational models (Lit2Sal and Sal2Lit) to enhance visualization design and literacy assessment by considering individual differences in visual attention based on literacy levels.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of visualizations can be improved by accounting for individual differences in visual literacy, which influences attention patterns in data exploration.

**Method:** A user study with 235 participants was conducted, analyzing visual attention patterns during three visualization tests. Two computational models were proposed based on the findings: Lit2Sal predicts attention based on literacy level, and Sal2Lit predicts literacy from attention data.

**Key Contributions:**

	1. Introduction of Lit2Sal, a new visual saliency model considering literacy levels
	2. Development of Sal2Lit, a model that predicts visual literacy from attention data
	3. Empirical analysis demonstrating the correlation between literacy levels and attention patterns

**Result:** Lit2Sal outperforms existing saliency models by integrating literacy awareness, while Sal2Lit achieves 86% accuracy in predicting visual literacy from attention maps.

**Limitations:** 

**Conclusion:** The proposed models demonstrate a novel way to adapt visual designs and assessments to individual literacy levels, paving the way for personalized visual communication.

**Abstract:** Accounting for individual differences can improve the effectiveness of visualization design. While the role of visual attention in visualization interpretation is well recognized, existing work often overlooks how this behavior varies based on visual literacy levels. Based on data from a 235-participant user study covering three visualization tests (mini-VLAT, CALVI, and SGL), we show that distinct attention patterns in visual data exploration can correlate with participants' literacy levels: While experts (high-scorers) generally show a strong attentional focus, novices (low-scorers) focus less and explore more. We then propose two computational models leveraging these insights: Lit2Sal -- a novel visual saliency model that predicts observer attention given their visualization literacy level, and Sal2Lit -- a model to predict visual literacy from human visual attention data. Our quantitative and qualitative evaluation demonstrates that Lit2Sal outperforms state-of-the-art saliency models with literacy-aware considerations. Sal2Lit predicts literacy with 86% accuracy using a single attention map, providing a time-efficient supplement to literacy assessment that only takes less than a minute. Taken together, our unique approach to consider individual differences in salience models and visual attention in literacy assessments paves the way for new directions in personalized visual data communication to enhance understanding.

</details>


### [4] ["Think First, Verify Always": Training Humans to Face AI Risks](https://arxiv.org/abs/2508.03714)

*Yuksel Aydin*

**Main category:** cs.HC

**Keywords:** cybersecurity, cognitive security, AI-enabled threats, human factors, trusted AI

**Relevance Score:** 6

**TL;DR:** The paper presents the TFVA protocol that positions humans as the first line of defense against AI-enabled threats, demonstrating improved cognitive security through brief training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap between device-centric cybersecurity and the need for human involvement in defending against AI-enabled cognitive attacks.

**Method:** A randomized controlled trial with 151 participants was conducted to evaluate the effectiveness of the TFVA protocol, defined by five principles: Awareness, Integrity, Judgment, Ethical Responsibility, and Transparency (AIJET).

**Key Contributions:**

	1. Introduction of the TFVA protocol focusing on human factors in cybersecurity.
	2. Demonstrated significant performance improvements through minimal training interventions.
	3. Recommendations for integrating the TFVA protocol into existing AI platforms.

**Result:** Participants who underwent a 3-minute intervention showed a statistically significant improvement of +7.87% in cognitive security task performance compared to controls.

**Limitations:** 

**Conclusion:** Embedding the TFVA protocol in GenAI platforms can enhance human resilience against AI-driven manipulation and supports ethical AI usage by shifting the emphasis to proactive human engagement in security.

**Abstract:** Artificial intelligence enables unprecedented attacks on human cognition, yet cybersecurity remains predominantly device-centric. This paper introduces the "Think First, Verify Always" (TFVA) protocol, which repositions humans as 'Firewall Zero', the first line of defense against AI-enabled threats. The protocol is grounded in five operational principles: Awareness, Integrity, Judgment, Ethical Responsibility, and Transparency (AIJET). A randomized controlled trial (n=151) demonstrated that a minimal 3-minute intervention produced statistically significant improvements in cognitive security task performance, with participants showing an absolute +7.87% gains compared to controls. These results suggest that brief, principles-based training can rapidly enhance human resilience against AI-driven cognitive manipulation. We recommend that GenAI platforms embed "Think First, Verify Always" as a standard prompt, replacing passive warnings with actionable protocols to enhance trustworthy and ethical AI use. By bridging the gap between technical cybersecurity and human factors, the TFVA protocol establishes human-empowered security as a vital component of trustworthy AI systems.

</details>


### [5] [Relationship between Perceived Maneuverability and Involuntary Eye Movements under Systematically Varied Time Constants of Ride-on Machinery](https://arxiv.org/abs/2508.03717)

*Muhammad Akmal Bin Mohammed Zaffir, Daisuke Sakai, Yuki Sato, Takahiro Wada*

**Main category:** cs.HC

**Keywords:** involuntary eye movements, perceived maneuverability, cognitive load, ride-on machinery, human operator performance

**Relevance Score:** 6

**TL;DR:** The study investigates how varying dynamic properties of a ride-on machine affect involuntary eye movements and perceived maneuverability in human operators.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the influence of motion dynamics on involuntary eye movements can provide insights into human operator performance in ride-on machinery.

**Method:** Participants operated a yaw-rotational platform with manipulated time constants affecting motor torque based on joystick input while their eye movements and subjective ratings of maneuverability and cognitive load were recorded.

**Key Contributions:**

	1. Demonstrated the effect of time constant variation on eye movement accuracy
	2. Established correlations between perceived maneuverability, cognitive load, and eye movement stability
	3. Provided insights on the somatic experience of maneuvering machines related to HCI design.

**Result:** As the platform's time constant increased, perceived maneuverability scores decreased and cognitive loads increased, leading to decreased accuracy in involuntary eye movements; positive correlations were found between maneuverability and eye movement accuracy.

**Limitations:** The study is limited to specific dynamic properties and may not generalize across all types of ride-on machinery or real-world scenarios.

**Conclusion:** The research suggests that design modifications in ride-on machinery can improve operator performance by considering the interaction between perceived maneuverability and cognitive load on eye movements.

**Abstract:** Studies suggest that involuntary eye movements exhibit greater stability during active motion compared to passive motion, and this effect may also apply to the operation of ride-on machinery. Moreover, a study suggested that experimentally manipulating the sense of agency (SoA) by introducing delays may influence the stability of involuntary eye movements. Although a preliminary investigation examined involuntary eye movements and perceived maneuverability under two distinct machine dynamics with preserved SoA, it remains unclear how systematic variations in motion dynamics influence these factors. Therefore, the purpose of the present research was to investigate whether systematic variations in the dynamic properties of a ride-on machine, where the perceived maneuverability is modulated, influence the accuracy of involuntary eye movements in human operators. Participants rode a yaw-rotational platform whose time constant from joystick input to motor torque of a rotational machine was systematically manipulated. During the operation, eye movements were recorded while participants fixated on a visual target. After each condition, participants provided subjective ratings of maneuverability and cognitive load. As the platform's time constant increased, the perceived maneuverability scores decreased while the cognitive loads increased. Concurrently, involuntary eye movement accuracy decreased. Moderate to weak positive correlations emerged between the perceived maneuverability scores and the eye movement gain and accuracy, while a weak negative correlation was found with cognitive load.

</details>


### [6] [Recommending With, Not For: Co-Designing Recommender Systems for Social Good](https://arxiv.org/abs/2508.03792)

*Michael D. Ekstrand, Afsaneh Razi, Aleksandra Sarcevic, Maria Soledad Pera, Robin Burke, Katherine Landau Wright*

**Main category:** cs.HC

**Keywords:** recommender systems, social good, participatory design, stakeholder collaboration, user experience

**Relevance Score:** 8

**TL;DR:** The paper argues for a participatory design approach in recommender systems for social good, emphasizing collaboration with stakeholders rather than a top-down design by engineers and designers alone.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The traditional design of recommender systems predominantly reflects the designers' views, potentially sidelining the needs and interests of other stakeholders. This is particularly concerning for systems aiming to benefit social good.

**Method:** The authors advocate for a democratic and participatory design process that includes users, creators, and other involved parties as active co-designers rather than mere subjects of study.

**Key Contributions:**

	1. Advocates for participatory design in recommender systems for social good.
	2. Highlights the limitations of current designs dictated by engineers and designers.
	3. Proposes a framework for stakeholder collaboration in system development.

**Result:** The paper outlines the shortcomings of current recommender system designs and suggests that systems designed through collaborative processes can better serve diverse stakeholder needs.

**Limitations:** 

**Conclusion:** Recommender systems for social good should involve stakeholders in a collaborative design process to ensure their social objectives are truly representative and beneficial.

**Abstract:** Recommender systems are usually designed by engineers, researchers, designers, and other members of development teams. These systems are then evaluated based on goals set by the aforementioned teams and other business units of the platforms operating the recommender systems. This design approach emphasizes the designers' vision for how the system can best serve the interests of users, providers, businesses, and other stakeholders. Although designers may be well-informed about user needs through user experience and market research, they are still the arbiters of the system's design and evaluation, with other stakeholders' interests less emphasized in user-centered design and evaluation. When extended to recommender systems for social good, this approach results in systems that reflect the social objectives as envisioned by the designers and evaluated as the designers understand them. Instead, social goals and operationalizations should be developed through participatory and democratic processes that are accountable to their stakeholders. We argue that recommender systems aimed at improving social good should be designed *by* and *with*, not just *for*, the people who will experience their benefits and harms. That is, they should be designed in collaboration with their users, creators, and other stakeholders as full co-designers, not only as user study participants.

</details>


### [7] [A11yShape: AI-Assisted 3-D Modeling for Blind and Low-Vision Programmers](https://arxiv.org/abs/2508.03852)

*Zhuohao, Zhang, Haichang Li, Chun Meng Yu, Faraz Faruqi, Junan Xie, Gene S-H Kim, Mingming Fan, Angus G. Forbes, Jacob O. Wobbrock, Anhong Guo, Liang He*

**Main category:** cs.HC

**Keywords:** 3-D modeling, blind and low-vision, human-computer interaction, LLMs, OpenSCAD

**Relevance Score:** 7

**TL;DR:** A11yShape is a system for blind and low-vision users to understand and modify 3-D models using accessible descriptions and LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The inherent complexity of 3-D models and the lack of non-visual interaction tools make it challenging for blind and low-vision users to engage with these models.

**Method:** A11yShape integrates LLMs with OpenSCAD to provide accessible descriptions, version control for tracking changes, and a hierarchical representation of model components, while synchronizing semantic selections across multiple representations.

**Key Contributions:**

	1. Development of accessible 3-D modeling tools for blind and low-vision users
	2. Integration of LLMs with OpenSCAD for enhanced user interaction
	3. Demonstrated successful user study with blind and low-vision programmers

**Result:** User studies with four blind and low-vision programmers showed that participants could independently understand, create, and modify 3-D models, achieving high satisfaction with the process.

**Limitations:** 

**Conclusion:** A11yShape enables blind and low-vision users to work with 3-D models independently, overcoming previous barriers that required sighted assistance.

**Abstract:** Building 3-D models is challenging for blind and low-vision (BLV) users due to the inherent complexity of 3-D models and the lack of support for non-visual interaction in existing tools. To address this issue, we introduce A11yShape, a novel system designed to help BLV users who possess basic programming skills understand, modify, and iterate on 3-D models. A11yShape leverages LLMs and integrates with OpenSCAD, a popular open-source editor that generates 3-D models from code. Key functionalities of A11yShape include accessible descriptions of 3-D models, version control to track changes in models and code, and a hierarchical representation of model components. Most importantly, A11yShape employs a cross-representation highlighting mechanism to synchronize semantic selections across all model representations -- code, semantic hierarchy, AI description, and 3-D rendering. We conducted a multi-session user study with four BLV programmers, where, after an initial tutorial session, participants independently completed 12 distinct models across two testing sessions, achieving results that aligned with their own satisfaction. The result demonstrates that participants were able to comprehend provided 3-D models, as well as independently create and modify 3-D models -- tasks that were previously impossible without assistance from sighted individuals.

</details>


### [8] [ReVISit 2: A Full Experiment Life Cycle User Study Framework](https://arxiv.org/abs/2508.03876)

*Zach Cutler, Jack Wilburn, Hilson Shrestha, Yiren Ding, Brian Bollen, Khandaker Abrar Nadib, Tingying He, Andrew McNutt, Lane Harrison, Alexander Lex*

**Main category:** cs.HC

**Keywords:** user studies, visualization, software framework, reproducibility, interaction techniques

**Relevance Score:** 6

**TL;DR:** reVISit 2 is a software framework designed to assist researchers in effectively conducting browser-based user studies in visualization research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ease the burden of designing, conducting, and analyzing online user studies for visualizations while addressing the limitations of existing tools.

**Method:** The study involves creating a comprehensive software framework that supports various phases of user studies, including design, debugging, data collection, analysis, and dissemination.

**Key Contributions:**

	1. Introduction of a comprehensive software framework for user studies
	2. Technical affordances such as replay of interactions
	3. Community support for researchers

**Result:** reVISit 2 has demonstrated its effectiveness through experimental replications and has been used in publication-quality studies.

**Limitations:** 

**Conclusion:** The framework aims to enhance the ease of conducting user studies, improve reproducibility, and support advanced interactive study designs within the visualization research community.

**Abstract:** Online user studies of visualizations, visual encodings, and interaction techniques are ubiquitous in visualization research. Yet, designing, conducting, and analyzing studies effectively is still a major burden. Although various packages support such user studies, most solutions address only facets of the experiment life cycle, make reproducibility difficult, or do not cater to nuanced study designs or interactions. We introduce reVISit 2, a software framework that supports visualization researchers at all stages of designing and conducting browser-based user studies. ReVISit supports researchers in the design, debug & pilot, data collection, analysis, and dissemination experiment phases by providing both technical affordances (such as replay of participant interactions) and sociotechnical aids (such as a mindfully maintained community of support). It is a proven system that can be (and has been) used in publication-quality studies -- which we demonstrate through a series of experimental replications. We reflect on the design of the system via interviews and an analysis of its technical dimensions. Through this work, we seek to elevate the ease with which studies are conducted, improve the reproducibility of studies within our community, and support the construction of advanced interactive studies.

</details>


### [9] [Human-Centered Human-AI Interaction (HC-HAII): A Human-Centered AI Perspective](https://arxiv.org/abs/2508.03969)

*Wei Xu*

**Main category:** cs.HC

**Keywords:** human-AI interaction, human-centered AI, interdisciplinary research, HAII methodology, human-centered design

**Relevance Score:** 9

**TL;DR:** This chapter introduces a framework for human-centered human-AI interaction (HC-HAII) focusing on placing humans at the core of HAII research, presenting methodologies, challenges, and the structure of a related book.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a human-centered approach in HAII research and applications to properly address the complexities of human and AI collaboration.

**Method:** The chapter outlines the HC-HAII methodology, emphasizing human-centered methods, interdisciplinary collaboration, and multi-level design paradigms.

**Key Contributions:**

	1. Introduction of HC-HAII framework
	2. Emphasis on human-centered methodologies
	3. Identification of research challenges

**Result:** Presents a comprehensive framework and highlights key research challenges, setting the stage for further contributions in HAII.

**Limitations:** 

**Conclusion:** Establishes a fundamental framework for human-centered HAII, which will guide subsequent research and applications in the field.

**Abstract:** This chapter systematically promotes an emerging interdisciplinary field of human-artificial intelligence interaction (human-AI interaction, HAII) from a human-centered AI (HCAI) perspective. It introduces a framework of human-centered HAII (HC-HAII). HC-HAII places humans at the core of HAII research and applications, emphasizing the importance of adopting a human-centered approach over a technology-centered one. The chapter presents the HC-HAII methodology, including human-centered methods, process, interdisciplinary teams, and multi-level design paradigms. It also highlights key research challenges and future directions. As the first chapter, this chapter also provides a structural overview of this book, which brings together contributions from an interdisciplinary community of researchers and practitioners to advance the theory, methodology, and applications of HCAI in diverse domains of HAII. The purpose of this chapter is to provide a fundamental framework for this book, centered on HAII research and applications based on the HCAI approach, which will pave the way for the content of subsequent chapters.

</details>


### [10] [Managing Data for Scalable and Interactive Event Sequence Visualization](https://arxiv.org/abs/2508.03974)

*Sayef Azad Sakin, Katherine E. Isaacs*

**Main category:** cs.HC

**Keywords:** event sequence visualization, interactive rendering, data management

**Relevance Score:** 5

**TL;DR:** ESeMan is an event sequence management system improving timeline visualization performance while preserving accuracy through intelligent caching and hierarchical data structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of interactive visualizations for large datasets in application domains like program execution traces and manufacturing pipelines, without sacrificing accuracy.

**Method:** ESeMan utilizes hierarchical data structures and intelligent caching strategies to optimize data fetching and visualization rendering times for event sequences.

**Key Contributions:**

	1. Introduction of ESeMan for efficient event sequence visualization
	2. Demonstration of superior performance compared to existing methods
	3. Provision of a benchmarking harness for future evaluations

**Result:** ESeMan achieves sub-100ms data fetching times while maintaining pixel-level visualization accuracy, outperforming traditional approaches like summed area tables and M4 aggregation.

**Limitations:** 

**Conclusion:** ESeMan significantly enhances interaction performance in timeline visualizations for event sequences, proven through extensive evaluation and benchmarking.

**Abstract:** Parallel event sequences, such as those collected in program execution traces and automated manufacturing pipelines, are typically visualized as interactive parallel timelines. As the dataset size grows, these charts frequently experience lag during common interactions such as zooming, panning, and filtering. Summarization approaches can improve interaction performance, but at the cost of accuracy in representation. To address this challenge, we introduce ESeMan (Event Sequence Manager), an event sequence management system designed to support interactive rendering of timeline visualizations with tunable accuracy. ESeMan employs hierarchical data structures and intelligent caching to provide visualizations with only the data necessary to generate accurate summarizations with significantly reduced data fetch time. We evaluate ESeMan's query times against summed area tables, M4 aggregation, and statistical sub-sampling on a variety of program execution traces. Our results demonstrate ESeMan provides better performance, achieving sub-100ms fetch times while maintaining visualization accuracy at the pixel level. We further present our benchmarking harness, enabling future performance evaluations for event sequence visualization.

</details>


### [11] [SocialPulse: An On-Smartwatch System for Detecting Real-World Social Interactions](https://arxiv.org/abs/2508.03980)

*Md Sabbir Ahmed, Arafat Rahman, Mark Rucker, Laura E. Barnes*

**Main category:** cs.HC

**Keywords:** wearable technology, social interaction detection, transfer learning, foreground speech, personalized interventions

**Relevance Score:** 8

**TL;DR:** A wearable system was developed for real-time detection of both in-person and virtual social interactions using transfer learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand social experiences through unobtrusive monitoring and to overcome limitations of existing systems that are often restricted to controlled environments.

**Method:** The system uses transfer learning to detect foreground speech and infers interaction boundaries based on conversational cues such as whispering.

**Key Contributions:**

	1. Real-time detection of in-person and virtual interactions using wearables
	2. Utilization of transfer learning for improved accuracy
	3. Demonstration of practical application in monitoring daily social interactions

**Result:** The system achieved an interaction detection accuracy of 73.18% in a real-world evaluation with 11 participants over 38 days.

**Limitations:** The study involved a small sample size and a limited evaluation period, which may affect generalizability.

**Conclusion:** The findings suggest that the system can effectively capture social interactions in daily life, with potential applications for personalized interventions for social anxiety.

**Abstract:** Social interactions are a fundamental part of daily life and play a critical role in well-being. As emerging technologies offer opportunities to unobtrusively monitor behavior, there is growing interest in using them to better understand social experiences. However, automatically detecting interactions, particularly via wearable devices, remains underexplored. Existing systems are often limited to controlled environments, constrained to in-person interactions, and rely on rigid assumptions such as the presence of two speakers within a fixed time window. These limitations reduce their generalizability to capture diverse real-world interactions. To address these challenges, we developed a real-time, on-watch system capable of detecting both in-person and virtual interactions. The system leverages transfer learning to detect foreground speech (FS) and infers interaction boundaries based upon FS and conversational cues like whispering. In a real-world evaluation involving 11 participants over a total of 38 days (Mean = 3.45 days, SD = 2.73), the system achieved an interaction detection accuracy of 73.18%. Follow-up with six participants indicated perfect recall for detecting interactions. These preliminary findings demonstrate the potential of our system to capture interactions in daily life, providing a foundation for applications such as personalized interventions targeting social anxiety.

</details>


### [12] [StepWrite: Adaptive Planning for Speech-Driven Text Generation](https://arxiv.org/abs/2508.04011)

*Hamza El Alaoui, Atieh Taheri, Yi-Hao Peng, Jeffrey P. Bigham*

**Main category:** cs.HC

**Keywords:** voice-based interaction, large language model, cognitive load, context-aware prompts, text composition

**Relevance Score:** 9

**TL;DR:** StepWrite is a voice-based interaction system that facilitates the hands-free, eyes-free composition of longer texts by breaking the process into manageable subtasks and providing adaptive contextual prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current voice-based systems struggle with composing longer, contextually complex texts, which is crucial for effective communication while multitasking or on the move.

**Method:** StepWrite uses a large language model to decompose writing tasks and offers contextually-aware audio prompts to assist users in text composition without visual tracking.

**Key Contributions:**

	1. Introduction of StepWrite for structured text composition
	2. Dynamic adaptation of prompts based on user context
	3. Empirical evaluation showing improved usability and reduced cognitive load

**Result:** An evaluation with 25 participants showed that StepWrite significantly reduces cognitive load and improves both usability and user satisfaction compared to conventional methods.

**Limitations:** 

**Conclusion:** StepWrite demonstrates the effectiveness of structured, context-aware voice interactions in enhancing communication during multitasking.

**Abstract:** People frequently use speech-to-text systems to compose short texts with voice. However, current voice-based interfaces struggle to support composing more detailed, contextually complex texts, especially in scenarios where users are on the move and cannot visually track progress. Longer-form communication, such as composing structured emails or thoughtful responses, requires persistent context tracking, structured guidance, and adaptability to evolving user intentions--capabilities that conventional dictation tools and voice assistants do not support. We introduce StepWrite, a large language model-driven voice-based interaction system that augments human writing ability by enabling structured, hands-free and eyes-free composition of longer-form texts while on the move. StepWrite decomposes the writing process into manageable subtasks and sequentially guides users with contextually-aware non-visual audio prompts. StepWrite reduces cognitive load by offloading the context-tracking and adaptive planning tasks to the models. Unlike baseline methods like standard dictation features (e.g., Microsoft Word) and conversational voice assistants (e.g., ChatGPT Advanced Voice Mode), StepWrite dynamically adapts its prompts based on the evolving context and user intent, and provides coherent guidance without compromising user autonomy. An empirical evaluation with 25 participants engaging in mobile or stationary hands-occupied activities demonstrated that StepWrite significantly reduces cognitive load, improves usability and user satisfaction compared to baseline methods. Technical evaluations further confirmed StepWrite's capability in dynamic contextual prompt generation, accurate tone alignment, and effective fact checking. This work highlights the potential of structured, context-aware voice interactions in enhancing hands-free and eye-free communication in everyday multitasking scenarios.

</details>


### [13] [VeriGUI: Verifiable Long-Chain GUI Dataset](https://arxiv.org/abs/2508.04026)

*Shunyu Liu, Minghao Liu, Huichi Zhou, Zhenyu Cui, Yang Zhou, Yuhao Zhou, Wendong Fan, Ge Zhang, Jiajun Shi, Weihao Xuan, Jiaxing Huang, Shuang Luo, Fang Wu, Heli Qi, Qingcheng Zeng, Ziqi Ren, Jialiang Gao, Jindi Lv, Junjie Wang, Aosong Feng, Heng Zhou, Wangchunshu Zhou, Zhenfei Yin, Wenlong Zhang, Guohao Li, Wenhao Yu, Irene Li, Lei Ma, Lei Bai, Qunshu Lin, Mingli Song, Dacheng Tao*

**Main category:** cs.HC

**Keywords:** GUI agents, human-computer interaction, long-chain tasks, dataset, subtask verifiability

**Relevance Score:** 8

**TL;DR:** This paper introduces VeriGUI, a novel long-chain GUI dataset designed to train and evaluate autonomous agents for complex GUI tasks, emphasizing long-chain task decomposition and subtask-level verifiability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing GUI agents that primarily focus on short-term interactions and lack the capability for long-horizon task execution in real-world applications.

**Method:** The authors developed VeriGUI, a dataset featuring long-chain tasks decomposed into interdependent subtasks, enhancing the training and evaluation of generalist GUI agents.

**Key Contributions:**

	1. Introduction of the VeriGUI dataset for long-chain GUI tasks
	2. Emphasis on subtask-level verifiability for exploring strategies
	3. Illustrated performance gaps in current agents highlighting the need for advanced capabilities

**Result:** Experiments demonstrated significant performance gaps for existing agents on long-horizon tasks, indicating the need for improved planning and decision-making in GUI agents.

**Limitations:** 

**Conclusion:** The VeriGUI dataset facilitates the evaluation and development of more capable GUI agents in realistic scenarios, paving the way for advancements in human-computer interaction.

**Abstract:** Recent studies have delved into constructing autonomous agents capable of performing complex Graphical User Interface (GUI)-based computer tasks, with the potential to revolutionize human-computer interaction. Despite encouraging results, existing efforts mainly focus on short-term interactions and rely on outcome-only verification, thereby limiting their scalability in real-world GUI applications that demand long-horizon task decomposition and execution. In this work, we introduce VeriGUI, a novel verifiable long-chain GUI dataset designed to facilitate the development and evaluation of generalist GUI agents operating in realistic computer environments. Our dataset emphasizes two critical dimensions: (1) long-chain complexity, with tasks decomposed into a sequence of interdependent subtasks spanning hundreds of steps, explicitly designed to allow any subtask to serve as a valid starting point; and (2) subtask-level verifiability, which enables diverse exploration strategies within each subtask, while ensuring that each subtask-level goal remains verifiable and consistent. The dataset consists of GUI task trajectories across both desktop and web, annotated by human experts. Extensive experiments on VeriGUI using various agents with different foundation models reveal significant performance gaps in handling long-horizon tasks, highlighting the need for more robust planning and decision-making capabilities in GUI agents.

</details>


### [14] [XARP Tools: An Extended Reality Platform for Humans and AI Agents](https://arxiv.org/abs/2508.04108)

*Arthur Caetano, Misha Sra*

**Main category:** cs.HC

**Keywords:** Extended Reality, AI integration, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** XARP Tools is an XR framework for both human and AI developers, featuring a Python library and platform-specific clients for enhanced interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The report aims to facilitate XR development through a versatile framework that supports human developers and enables AI-driven interactions.

**Method:** The framework includes a server-side Python library with high-level APIs and platform-specific XR clients that communicate via a JSON-based protocol over WebSockets.

**Key Contributions:**

	1. High-level APIs for easier XR development
	2. Integration of AI-driven interactions
	3. Open source availability on GitHub

**Result:** XARP provides three modes of utilization: as a human-friendly library, a set of callable tools for AI interactions, and a Model Context Protocol server for integrating XR devices into AI systems.

**Limitations:** 

**Conclusion:** XARP enhances the capabilities of XR by supporting both human and AI interactions, with open access to tools and examples for developers.

**Abstract:** This technical report presents XARP Tools, an extended reality (XR) framework designed for human and AI developers alike. XARP comprises a server-side Python library and platform-specific XR clients. The library offers high-level APIs and communicates with clients via a JSON-based protocol over WebSockets. XR clients encapsulate device and runtime specifics, providing responsive, low-latency user interaction. XARP can be utilized in three ways: (i) as a library that abstracts XR development for humans; (ii) as a set of callable tools that allow AI agents to drive on-the-fly interactions with users; and (iii) as a Model Context Protocol server that plugs XR devices into AI ecosystems. XARP code and working examples are released openly at https://github.com/HAL-UCSB/xarp.

</details>


### [15] [DRIVE-T: A Methodology for Discriminative and Representative Data Viz Item Selection for Literacy Construct and Assessment](https://arxiv.org/abs/2508.04160)

*Angela Locoro, Silvia Golia, Davide Falessi*

**Main category:** cs.HC

**Keywords:** data visualization, assessment, measurement constructs, Rasch Measurement, literacy

**Relevance Score:** 4

**TL;DR:** This paper introduces DRIVE-T, a methodology for constructing and evaluating assessment items for data visualization literacy to address difficulty level underspecification.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance the expressivity of measurements in test design and test reuse for data visualization literacy due to underspecified difficulty levels.

**Method:** DRIVE-T involves tagging task-based items, rating them for difficulty by independent raters, and analyzing scores using a Many-Facet Rasch Measurement model.

**Key Contributions:**

	1. Introduction of the DRIVE-T methodology for assessing visualization literacy.
	2. Operationalization of an inductive approach for measurement construct emergence.
	3. Application of a pilot study demonstrating DRIVE-T's effectiveness.

**Result:** The methodology enables observation of difficulty levels of measurement constructs for data visualization literacy, leveraging discerning and representative task-based items.

**Limitations:** 

**Conclusion:** The adoption of DRIVE-T facilitates better measurement through formative-style assessment items, demonstrated with a pilot study using the methodology.

**Abstract:** The underspecification of progressive levels of difficulty in measurement constructs design and assessment tests for data visualization literacy may hinder the expressivity of measurements in both test design and test reuse. To mitigate this problem, this paper proposes DRIVE-T (Discriminating and Representative Items for Validating Expressive Tests), a methodology designed to drive the construction and evaluation of assessment items. Given a data vizualization, DRIVE-T supports the identification of task-based items discriminability and representativeness for measuring levels of data visualization literacy. DRIVE-T consists of three steps: (1) tagging task-based items associated with a set of data vizualizations; (2) rating them by independent raters for their difficulty; (3) analysing raters' raw scores through a Many-Facet Rasch Measurement model. In this way, we can observe the emergence of difficulty levels of the measurement construct, derived from the discriminability and representativeness of task-based items for each data vizualization, ordered into Many-Facets construct levels. In this study, we show and apply each step of the methodology to an item bank, which models the difficulty levels of a measurement construct approximating a latent construct for data visualization literacy. This measurement construct is drawn from semiotics, i.e., based on the syntax, semantics and pragmatics knowledge that each data visualization may require to be mastered by people. The DRIVE-T methodology operationalises an inductive approach, observable in a post-design phase of the items preparation, for formative-style and practice-based measurement construct emergence. A pilot study with items selected through the application of DRIVE-T is also presented to test our approach.

</details>


### [16] [Unplug, Mute, Avoid Investigating smart speaker users' privacy protection behaviours in Saudi Homes](https://arxiv.org/abs/2508.04202)

*Abdulrhman Alorini, Yufeng Wu, Abdullah Bin Sawad, Mukesh Prasad, A. Baki Kocaballi*

**Main category:** cs.HC

**Keywords:** smart speakers, privacy, HCI, Saudi Arabia, cultural probes

**Relevance Score:** 4

**TL;DR:** This study explores privacy concerns of Saudi Arabian users of smart speakers, revealing practices shaped by culture and household dynamics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The privacy risks of smart speakers are underexplored in non-Western contexts, particularly in Saudi Arabia.

**Method:** Cultural probes followed by semi-structured interviews with 16 Saudi Arabian participants.

**Key Contributions:**

	1. Empirical insights from Saudi Arabia on smart speaker usage.
	2. Theoretical extensions to contextual integrity frameworks.
	3. Design directions for culturally responsive voice interfaces.

**Result:** Participants engage in privacy-protective behaviors such as unplugging devices, muting microphones, and avoiding voice interactions, influenced by cultural and household factors.

**Limitations:** Limited to the experiences of 16 participants in Saudi Arabia; may not generalize to other regions or cultures.

**Conclusion:** The research provides insights into user behavior in collectivist settings, informing design for culturally sensitive voice interfaces and HCI practices.

**Abstract:** Smart speakers are increasingly integrated into domestic life worldwide, yet their privacy risks remain underexplored in non-Western cultural contexts. This study investigates how Saudi Arabian users of smart speakers navigate privacy concerns within collectivist, gendered, and often multigenerational households. Using cultural probes followed by semi-structured interviews with 16 participants, we uncover everyday privacy-protective behaviours including unplugging devices, muting microphones, and avoiding voice interactions altogether. These practices are shaped not only by individual risk perceptions but also by household norms, room configurations, and interpersonal dynamics. We contribute empirical insights from an underrepresented region, theoretical extensions to contextual integrity frameworks, and design directions for culturally responsive voice interfaces. This work expands the global conversation on smart speaker privacy and informs more inclusive HCI practices in increasingly diverse smart home environments.

</details>


### [17] [Capturing and Sharing Know-How through Visual Process Representations: A Human-Centred Approach to Teacher Workflows](https://arxiv.org/abs/2508.04357)

*Gloria Fern√°ndez-Nieto, Vanessa Echeverria, Yuheng Li, Yi-Shan Tsai, Lele Sha, Guanliang Chen, Dragan Gasevic, Zachari Swiecki*

**Main category:** cs.HC

**Keywords:** Knowledge Management, Visual Process Representations, Sequential Pattern Mining, Education, Teacher Workflows

**Relevance Score:** 5

**TL;DR:** The paper presents Visual Process Representations (VPR), a novel approach to visualizing expert workflows in knowledge management, aimed at assisting novice educators through improved clarity and engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address high staff turnover in universities by preserving and transferring knowledge through documenting expert workflows, which are typically lost or poorly represented.

**Method:** Utilizes Sequential Pattern Mining (SPM) on log data to identify and visualize teachers' workflows, integrated with design phases that incorporate storytelling techniques.

**Key Contributions:**

	1. Introduction of Visual Process Representations (VPR) for knowledge transfer in education
	2. Integration of Sequential Pattern Mining (SPM) with visual storytelling techniques
	3. Empirical evaluation showcasing usability and engagement benefits of VPR

**Result:** The study involving 160 teachers demonstrated that VPR enhances task performance and usability, especially with enriched visuals; however, improvements in process memorability and task timing were limited.

**Limitations:** Limited improvements in process memorability and task time.

**Conclusion:** VPR shows significant promise in visualizing workflows for novice educators, although certain aspects like memorability need further enhancement.

**Abstract:** Knowledge Management is crucial for capturing and transferring expertise within universities, especially in high staff turnover contexts where expertise loss disrupts teaching. Documenting teachers' workflows is time-intensive and diverts experts from core responsibilities. Sequential Pattern Mining (SPM) leverages log data to identify expert workflows, offering an automated alternative to represent workflows but requiring transformation into intuitive formats for novice educators. This paper introduces Visual Process Representations (VPR), a design approach combining SPM, Knowledge Management processes, and storytelling techniques to convert expert log data into clear visualisations. We detail the design phases and report a study evaluating visual affordances (text lists vs. pictorial-style) and teachers' perceptions of four versions of the VPR with 160 higher teachers on Prolific. Results indicate improved task performance, usability, and engagement, particularly with enriched visuals, though process memorability and task time improvements were limited. The findings highlight VPR's potential to visualise workflows and support novice educators.

</details>


### [18] [GoldMind: A Teacher-Centered Knowledge Management System for Higher Education -- Lessons from Iterative Design](https://arxiv.org/abs/2508.04377)

*Gloria Fern√°ndez-Nieto, Lele Sha, Yuheng Li, Yi-Shan Tsai, Guanliang Chen, Yinwei Wei, Weiqing Wang, Jinchun Wen, Shaveen Singh, Ivan Silva, Yuanfang Li, Dragan Gasƒõviƒá, Zachari Swiecki*

**Main category:** cs.HC

**Keywords:** Knowledge Management Systems, Human-Computer Interaction, Generative AI, Higher Education, Cognitive Load

**Relevance Score:** 7

**TL;DR:** The paper presents a human-centered design study on the development of GoldMind, a Knowledge Management System aimed at improving digital teaching tasks in higher education, highlighting user interaction insights and design considerations.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of knowledge reuse in higher education due to staff turnover and changing roles, particularly in the context of designing effective Knowledge Management Systems (KMSs).

**Method:** A two-year human-centered design study involving 108 higher education teachers, utilizing iterative co-design and evaluation cycles to refine the GoldMind KMS based on user feedback.

**Key Contributions:**

	1. Iterative co-design approach involving educators to refine KMS features
	2. Identification of user interaction lessons that inform system design
	3. Insights on cognitive load and knowledge behaviors using Epistemic Network Analysis

**Result:** The study identified key insights through three main themes: lessons from user interaction data, design considerations from co-design and usability testing, and human factors impacting cognitive load and knowledge behaviors.

**Limitations:** The study is limited to higher education contexts and may not generalize to other sectors.

**Conclusion:** The insights gathered through the design-evaluation cycles can guide the development of KMSs that are better aligned with the realities of educators' workflows, enhancing adoption and effectiveness in knowledge management.

**Abstract:** Designing Knowledge Management Systems (KMSs) for higher education requires addressing complex human-technology interactions, especially where staff turnover and changing roles create ongoing challenges for reusing knowledge. While advances in process mining and Generative AI enable new ways of designing features to support knowledge management, existing KMSs often overlook the realities of educators' workflows, leading to low adoption and limited impact. This paper presents findings from a two-year human-centred design study with 108 higher education teachers, focused on the iterative co-design and evaluation of GoldMind, a KMS supporting in-the-flow knowledge management during digital teaching tasks. Through three design-evaluation cycles, we examined how teachers interacted with the system and how their feedback informed successive refinements. Insights are synthesised across three themes: (1) Technology Lessons from user interaction data, (2) Design Considerations shaped by co-design and usability testing, and (3) Human Factors, including cognitive load and knowledge behaviours, analysed using Epistemic Network Analysis.

</details>


### [19] [Plant-Centric Metaverse: A Biocentric-Creation Framework for Ecological Art and Digital Symbiosis](https://arxiv.org/abs/2508.04391)

*Ze Gao, Mengyao Guo, Zheng Wang, Xiaolin Zhang, Sihuang Man*

**Main category:** cs.HC

**Keywords:** digital ecological art, metaverse, plant agency, biocentric creation, NFTs

**Relevance Score:** 2

**TL;DR:** This study introduces the Biocentric-Creation Transformation Ideology (BCTI) framework for digital ecological art, promoting plant-centered narratives in the metaverse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the shift from anthropocentric to plant-centered artistic narratives in digital ecological art and address existing frameworks that fail to guide artists in leveraging plant agency.

**Method:** The study employs a multimodal analysis of case studies across bio-art, NFTs, and VR ecosystems from 2013 to 2023 to validate the BCTI framework.

**Key Contributions:**

	1. Introduction of the BCTI framework for plant-centered digital art
	2. Quantitative analysis showing significant growth in biological artworks
	3. New insights into digital symbiosis and cross-species collaboration

**Result:** The research finds a 133% increase in biological artworks in archives, the emergence of plant governance in DAOs, and real-time biodata translation reshaping ecological aesthetics in VR.

**Limitations:** 

**Conclusion:** The BCTI framework enhances ecological art theory, repositioning environmental consciousness towards a more inclusive vision that accommodates plant agency in digital art.

**Abstract:** Digital ecological art represents an emergent frontier where biological media converge with virtual environments. This study examines the paradigm shift from anthropocentric to plant-centered artistic narratives within the metaverse, contextualizing how digital platforms transform ecological expression. However, current frameworks fail to systematically guide artists in leveraging plant agency for digital symbiosis that transcends human-centered creation. We propose the Biocentric-Creation Transformation Ideology (BCTI) framework and validate it through multimodal case studies spanning bio-art, NFTs, and VR ecosystems (2013-2023). Our analysis reveals: (1) Metaverse ecosystems enable unprecedented plant-algorithm co-creation, with biological artworks increasing by 133% in premier archives (2020 vs 2013); (2) Digital symbiosis manifests through blockchain DAOs where plants govern human-plant collaborations; (3) Algorithmic photosynthesis in VR environments reshapes ecological aesthetics through real-time biodata translation. The BCTI framework advances ecological art theory by systematizing the transition from representation to plant-centered agency, offering artists a blueprint for post-anthropocene creation. This redefines environmental consciousness in virtual realms while establishing new protocols for cross-species digital collaboration.

</details>


### [20] [Measuring Information Richness in Product Images: Implications for Online Sales](https://arxiv.org/abs/2508.04541)

*Zhu Yuting, Cao Xinyu, Su Yuzhuo, Ma Yongbin*

**Main category:** cs.HC

**Keywords:** k-value, e-commerce, image selection, consumer behavior, Vision Transformers

**Relevance Score:** 4

**TL;DR:** The paper introduces a metric called k-value to measure the information richness of product image sets in e-commerce and its impact on consumer purchase decisions, highlighting a counterintuitive effect on buying propensities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist e-commerce sellers in selecting product images that influence consumer behavior effectively.

**Method:** The authors propose the k-value metric based on k-means clustering of patch-level embeddings from Vision Transformers, accompanied by validating experiments.

**Key Contributions:**

	1. Introduction of the k-value metric for image selection in e-commerce.
	2. Validation of k-value through online consumer experiments.
	3. Insights into the relationship between visual information richness and purchasing behavior.

**Result:** The study finds that higher k-value indicates richer image information and aligns with perceived richness but paradoxically reduces purchase propensity despite shortening decision-making time.

**Limitations:** The findings may not generalize across all product categories or types of visual content.

**Conclusion:** Understanding the k-value's implications helps e-commerce sellers optimize visual content for better consumer engagement while acknowledging potential adverse effects on purchasing.

**Abstract:** A common challenge for e-commerce sellers is to decide what product images to display on online shopping sites. In this paper, we propose and validate a novel metric, k-value, to quantify the information richness of an image set, and we further investigate its effect on consumers' purchase decisions. We leverage patch-level embeddings from Vision Transformers (ViT) and apply k-means clustering to identify distinct visual features, defining k-value as the number of clusters. An online experiment demonstrates that k-value aligns with human-perceived information richness, validating the metric. A simulated online shopping experiment further reveals a significant yet counterintuitive result: while an image set with a higher k-value (richer information) shortens decision time, it paradoxically reduces purchase propensity. Our findings illuminate the complex relationship between visual information richness and consumer behavior, providing sellers a quantifiable tool for image selection.

</details>


### [21] [VirtLab: An AI-Powered System for Flexible, Customizable, and Large-scale Team Simulations](https://arxiv.org/abs/2508.04634)

*Mohammed Almutairi, Charles Chiang, Haoze Guo, Matthew Belcher, Nandini Banerjee, Maria Milkowski, Svitlana Volkova, Daniel Nguyen, Tim Weninger, Michael Yankoski, Trenton W. Ford, Diego Gomez-Zara*

**Main category:** cs.HC

**Keywords:** Agentic AI, team simulation, HCI, LLM-based agents, VirtLab

**Relevance Score:** 8

**TL;DR:** VirtLab is a customizable team simulation system that utilizes LLM-based agents to study team behaviors in complex environments, allowing for both technical and non-technical users to formulate and analyze simulations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore hypotheses grounded in social science theories and study team behaviors using Agentic AI in complex environments.

**Method:** The paper presents VirtLab, a multi-agent simulation system capable of running customizable team simulations in various spatial and temporal settings, along with a user-friendly web interface.

**Key Contributions:**

	1. User-friendly interface for both technical and non-technical users
	2. Customizable simulation scenarios with LLM-based agents
	3. Scalable multi-agent team simulations in complex environments

**Result:** VirtLab successfully demonstrates its utility by comparing ground truth data with simulated scenarios, showcasing its ability to effectively simulate team collaboration.

**Limitations:** 

**Conclusion:** VirtLab addresses significant design and technical limitations of current team simulation frameworks, making it accessible for users without programming skills.

**Abstract:** Simulating how team members collaborate within complex environments using Agentic AI is a promising approach to explore hypotheses grounded in social science theories and study team behaviors. We introduce VirtLab, a user-friendly, customizable, multi-agent, and scalable team simulation system that enables testing teams with LLM-based agents in spatial and temporal settings. This system addresses the current frameworks' design and technical limitations that do not consider flexible simulation scenarios and spatial settings. VirtLab contains a simulation engine and a web interface that enables both technical and non-technical users to formulate, run, and analyze team simulations without programming. We demonstrate the system's utility by comparing ground truth data with simulated scenarios.

</details>


### [22] [How are CS students using resources and AI tools for coding tasks?](https://arxiv.org/abs/2508.04667)

*Natalia Echeverry, Arun Lekshmi Narayanan*

**Main category:** cs.HC

**Keywords:** AI coding assistants, debugging, computer science education, student preferences, survey

**Relevance Score:** 7

**TL;DR:** Survey explores usage of AI coding assistants and chatbots among CS students.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how computer science students utilize AI tools in coding and debugging tasks.

**Method:** Conducted a survey of 26 computer science students to gather insights on their preferences and experiences with AI coding assistants and chatbots.

**Key Contributions:**

	1. Insights into the preferences of CS students regarding AI tools
	2. Comparison of AI assistants and human help
	3. Identification of primary coding and debugging resources used by students

**Result:** AI coding assistants are primarily used for writing code, while chatbots are favored for debugging. Participants prefer online resources over assistance from peers or instructors.

**Limitations:** The sample size is small, and results may not be generalizable to all computer science students.

**Conclusion:** The findings highlight the reliance on AI tools for programming tasks and suggest an evolving trend in how students seek coding help.

**Abstract:** A survey of 26 CS students reveals that AI coding assistants are mainly used for writing code (second to online searches) while AI chatbots are the top resource for debugging. Participants with different coding experience prefer online help over direct human help from peers and instructors.

</details>


### [23] [MisVisFix: An Interactive Dashboard for Detecting, Explaining, and Correcting Misleading Visualizations using Large Language Models](https://arxiv.org/abs/2508.04679)

*Amit Kumar Das, Klaus Mueller*

**Main category:** cs.HC

**Keywords:** visualizations, Large Language Models, data communication, misinformation detection, interactive dashboard

**Relevance Score:** 9

**TL;DR:** MisVisFix is an interactive dashboard that uses Large Language Models to detect, explain, and correct misleading visualizations, achieving high accuracy in identifying visualization issues.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Misleading visualizations create challenges in accurate data interpretation, necessitating tools for detection and correction.

**Method:** An interactive dashboard utilizing Claude and GPT models for identifying, explaining, and correcting visualization misinformation, coupled with user interaction capabilities.

**Key Contributions:**

	1. Development of MisVisFix, an interactive dashboard for detecting and correcting misleading visualizations.
	2. Achieving 96% accuracy in identifying visualization issues and offering actionable suggestions.
	3. Adapting to new misinformation strategies through user interactions.

**Result:** MisVisFix identifies 96% of visualization issues and classifies all 74 types of misinformation, providing detailed explanations and actionable suggestions.

**Limitations:** 

**Conclusion:** MisVisFix enhances visualization literacy and helps ensure trustworthy data communication by making LLM-based detection accessible and interactive.

**Abstract:** Misleading visualizations pose a significant challenge to accurate data interpretation. While recent research has explored the use of Large Language Models (LLMs) for detecting such misinformation, practical tools that also support explanation and correction remain limited. We present MisVisFix, an interactive dashboard that leverages both Claude and GPT models to support the full workflow of detecting, explaining, and correcting misleading visualizations. MisVisFix correctly identifies 96% of visualization issues and addresses all 74 known visualization misinformation types, classifying them as major, minor, or potential concerns. It provides detailed explanations, actionable suggestions, and automatically generates corrected charts. An interactive chat interface allows users to ask about specific chart elements or request modifications. The dashboard adapts to newly emerging misinformation strategies through targeted user interactions. User studies with visualization experts and developers of fact-checking tools show that MisVisFix accurately identifies issues and offers useful suggestions for improvement. By transforming LLM-based detection into an accessible, interactive platform, MisVisFix advances visualization literacy and supports more trustworthy data communication.

</details>


### [24] [Optimal Fidelity Selection for Human-Supervised Search](https://arxiv.org/abs/2311.06381)

*Piyush Gupta, Vaibhav Srivastava*

**Main category:** cs.HC

**Keywords:** human-supervised, visual search, underwater mines, fidelity selection, Markov Decision Process

**Relevance Score:** 3

**TL;DR:** This paper investigates optimal fidelity selection in human-supervised underwater visual search, focusing on cognitive workloads and task performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to optimize operator performance in underwater mine detection by addressing cognitive factors like workload and fatigue during task execution.

**Method:** Experiments involved participants completing a primary task of underwater mine detection while estimating workload in a secondary task. The fidelity selection was modeled as a Partially Observable Markov Decision Process using input from a hidden state representing workload.

**Key Contributions:**

	1. Introduces a cognitive workload model in fidelity selection for visual search tasks.
	2. Demonstrates a methodology for optimizing fidelity based on operator performance metrics.
	3. Evaluates the impact of task delegation on performance outcomes in human-supervised systems.

**Result:** Performance improved by 26.5% without task delegation and by 50.3% with delegation compared to a baseline where humans chose fidelity levels manually.

**Limitations:** The study is limited to underwater visual search scenarios and may not generalize to other types of visual tasks.

**Conclusion:** The approach shows significant improvements in operator performance, suggesting that optimal fidelity selection can enhance efficiency in high-stakes environments like underwater search.

**Abstract:** We study optimal fidelity selection in human-supervised underwater visual search, where operator performance is affected by cognitive factors like workload and fatigue. In our experiments, participants perform two simultaneous tasks: detecting underwater mines in videos (primary) and responding to a visual cue to estimate workload (secondary). Videos arrive as a Poisson process and queue for review, with the operator choosing between normal fidelity (faster playback) and high fidelity. Rewards are based on detection accuracy, while penalties depend on queue length. Workload is modeled as a hidden state using an Input-Output Hidden Markov Model, and fidelity selection is optimized via a Partially Observable Markov Decision Process. We evaluate two setups: fidelity-only selection and a version allowing task delegation to automation to maintain queue stability. Our approach improves performance by 26.5% without delegation and 50.3% with delegation, compared to a baseline where humans manually choose their fidelity levels.

</details>


### [25] [Aligning Human and LLM Judgments: Insights from EvalAssist on Task-Specific Evaluations and AI-assisted Assessment Strategy Preferences](https://arxiv.org/abs/2410.00873)

*Zahra Ashktorab, Michael Desmond, Qian Pan, James M. Johnson, Martin Santillan Cooper, Elizabeth M. Daly, Rahul Nair, Tejaswini Pedapati, Hyo Jin Do, Werner Geyer*

**Main category:** cs.HC

**Keywords:** Large Language Models, Evaluation Methods, User Perceptions, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper explores how task-related factors and assessment strategies impact LLM outputs evaluation, highlighting the effectiveness of direct assessment methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation process of large language model outputs and make it more efficient for users.

**Method:** Conducted a study with 15 machine learning practitioners completing 6 tasks to yield 131 evaluations, analyzing how different approaches influenced user perceptions and criteria refinement.

**Key Contributions:**

	1. Exploration of task-related factors in LLM evaluations
	2. Comparison of direct assessment and pairwise comparison methods
	3. Recommendations for improving front-end tools for LLM evaluation

**Result:** Users preferred direct assessment for its efficiency, adaptability, and ability to modify evaluations based on task-specific criteria.

**Limitations:** 

**Conclusion:** The study concludes with recommendations for enhancing front-end tools to better facilitate interactions in LLM-assisted evaluations.

**Abstract:** Evaluation of large language model (LLM) outputs requires users to make critical judgments about the best outputs across various configurations. This process is costly and takes time given the large amounts of data. LLMs are increasingly used as evaluators to filter training data, evaluate model performance or assist human evaluators with detailed assessments. To support this process, effective front-end tools are critical for evaluation. Two common approaches for using LLMs as evaluators are direct assessment and pairwise comparison. In our study with machine learning practitioners (n=15), each completing 6 tasks yielding 131 evaluations, we explore how task-related factors and assessment strategies influence criteria refinement and user perceptions. Findings show that users performed more evaluations with direct assessment by making criteria task-specific, modifying judgments, and changing the evaluator model. We conclude with recommendations for how systems can better support interactions in LLM-assisted evaluations.

</details>


### [26] [Silent Speech Sentence Recognition with Six-Axis Accelerometers using Conformer and CTC Algorithm](https://arxiv.org/abs/2502.17829)

*Yudong Xie, Zhifeng Han, Qinfan Xiao, Liwei Liang, Lu-Qi Tao, Tian-Ling Ren*

**Main category:** cs.HC

**Keywords:** silent speech recognition, Conformer, communication impairment, accelerometers, neural networks

**Relevance Score:** 6

**TL;DR:** A novel method for silent speech recognition using facial motion signals and Conformer-based neural networks shows significant accuracy improvements over existing methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To assist individuals with communication impairments by improving silent speech recognition, which has been challenging due to elision and linking in silent sentences.

**Method:** The proposed method employs a Conformer-based neural network with the Connectionist-Temporal-Classification algorithm to convert facial motion signals from six-axis accelerometers into transcribed words.

**Key Contributions:**

	1. Introduction of a novel silent speech recognition method using accelerometers
	2. Use of a Conformer-based neural network for contextual understanding
	3. High accuracy of 97.17% in silent speech recognition

**Result:** Achieved a 97.17% accuracy in sentence recognition, outperforming existing methods which typically achieve 85%-95% accuracy.

**Limitations:** 

**Conclusion:** The results demonstrate the feasibility and high accuracy of using accelerometers for silent speech interfaces, indicating potential benefits for individuals with communication impairments.

**Abstract:** Silent speech interfaces (SSI) are being actively developed to assist individuals with communication impairments who have long suffered from daily hardships and a reduced quality of life. However, silent sentences are difficult to segment and recognize due to elision and linking. A novel silent speech sentence recognition method is proposed to convert the facial motion signals collected by six-axis accelerometers into transcribed words and sentences. A Conformer-based neural network with the Connectionist-Temporal-Classification algorithm is used to gain contextual understanding and translate the non-acoustic signals into words sequences, solely requesting the constituent words in the database. Test results show that the proposed method achieves a 97.17% accuracy in sentence recognition, surpassing the existing silent speech recognition methods with a typical accuracy of 85%-95%, and demonstrating the potential of accelerometers as an available SSI modality for high-accuracy silent speech sentence recognition.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [27] [How Deep Is Representational Bias in LLMs? The Cases of Caste and Religion](https://arxiv.org/abs/2508.03712)

*Agrima Seth, Monojit Choudhary, Sunayana Sitaram, Kentaro Toyama, Aditya Vashistha, Kalika Bali*

**Main category:** cs.CL

**Keywords:** Large Language Models, Representational Bias, Census Data, GPT-4 Turbo, Cultural Diversity

**Relevance Score:** 9

**TL;DR:** The paper audits representational bias in GPT-4 Turbo through the generation of stories in India, revealing persistent overrepresentation of dominant groups despite efforts to encourage diversity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the depth and extent of representational bias in large language models, expanding beyond typical metrics that focus on Global North identities.

**Method:** Conducted a systematic audit by generating over 7,200 stories using prompts designed for varying diversity, comparing outputs against census data for religious and caste representation in India.

**Key Contributions:**

	1. Systematic audit of GPT-4 Turbo for representational bias
	2. Quantitative comparison of LLM outputs against real-world demographics
	3. Identification of the limitations of prompt-based nudges in mitigating bias.

**Result:** Found consistent overrepresentation of culturally dominant groups in model outputs, highlighting a 'winner-take-all' bias in LLMs that surpasses training data distribution biases.

**Limitations:** Focused primarily on religion and caste representation in India; results may not generalize to other cultures or identities.

**Conclusion:** Diversifying training data alone is insufficient to address LLM bias; more fundamental changes in model development are necessary.

**Abstract:** Representational bias in large language models (LLMs) has predominantly been measured through single-response interactions and has focused on Global North-centric identities like race and gender. We expand on that research by conducting a systematic audit of GPT-4 Turbo to reveal how deeply encoded representational biases are and how they extend to less-explored dimensions of identity. We prompt GPT-4 Turbo to generate over 7,200 stories about significant life events (such as weddings) in India, using prompts designed to encourage diversity to varying extents. Comparing the diversity of religious and caste representation in the outputs against the actual population distribution in India as recorded in census data, we quantify the presence and "stickiness" of representational bias in the LLM for religion and caste. We find that GPT-4 responses consistently overrepresent culturally dominant groups far beyond their statistical representation, despite prompts intended to encourage representational diversity. Our findings also suggest that representational bias in LLMs has a winner-take-all quality that is more biased than the likely distribution bias in their training data, and repeated prompt-based nudges have limited and inconsistent efficacy in dislodging these biases. These results suggest that diversifying training data alone may not be sufficient to correct LLM bias, highlighting the need for more fundamental changes in model development. Dataset and Codebook: https://github.com/agrimaseth/How-Deep-Is-Representational-Bias-in-LLMs

</details>


### [28] [FeynTune: Large Language Models for High-Energy Theory](https://arxiv.org/abs/2508.03716)

*Paul Richmond, Prarit Agarwal, Borun Chowdhury, Vasilis Niarchos, Constantinos Papageorgakis*

**Main category:** cs.CL

**Keywords:** Large Language Models, High-Energy Physics, Fine-tuning, Domain-specific models, Low-Rank Adaptation

**Relevance Score:** 2

**TL;DR:** This paper discusses fine-tuning Large Language Models specifically for High-Energy Physics, showing improved performance over baseline models and commercial LLMs.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of language models in the specialized field of High-Energy Theoretical Physics (HEP) by fine-tuning existing LLMs on relevant datasets.

**Method:** 20 fine-tuned variants of the 8-billion parameter Llama-3.1 model were created, trained on arXiv abstracts from hep-th, hep-ph, and gr-qc, using Low-Rank Adaptation techniques.

**Key Contributions:**

	1. Development of 20 fine-tuned LLM variants for High-Energy Physics.
	2. Unique application of Low-Rank Adaptation techniques for fine-tuning.
	3. Performance benchmarking against leading commercial LLMs.

**Result:** The fine-tuned models significantly outperformed the base model on hep-th abstract completion tasks and compared favorably against leading commercial language models.

**Limitations:** 

**Conclusion:** Specialized LLMs for High-Energy Physics exhibit significant improvements over general models and suggest a path forward for the development of domain-specific language models.

**Abstract:** We present specialized Large Language Models for theoretical High-Energy Physics, obtained as 20 fine-tuned variants of the 8-billion parameter Llama-3.1 model. Each variant was trained on arXiv abstracts (through August 2024) from different combinations of hep-th, hep-ph and gr-qc. For a comparative study, we also trained models on datasets that contained abstracts from disparate fields such as the q-bio and cs categories. All models were fine-tuned using two distinct Low-Rank Adaptation fine-tuning approaches and varying dataset sizes, and outperformed the base model on hep-th abstract completion tasks. We compare performance against leading commercial LLMs (ChatGPT, Claude, Gemini, DeepSeek) and derive insights for further developing specialized language models for High-Energy Theoretical Physics.

</details>


### [29] [Intent Aware Context Retrieval for Multi-Turn Agricultural Question Answering](https://arxiv.org/abs/2508.03719)

*Abhay Vijayvargia, Ajay Nagpal, Kundeshwar Pundalik, Atharva Savarkar, Smita Gautam, Pankaj Singh, Rohit Saluja, Ganesh Ramakrishnan*

**Main category:** cs.CL

**Keywords:** chatbot, agricultural advice, low literacy, AI, RAG

**Relevance Score:** 5

**TL;DR:** This paper presents an AI-powered agricultural chatbot, Krishi Sathi, designed to deliver accessible farming advice to Indian farmers, particularly in rural regions with low literacy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in accessibility to timely and easy-to-understand agricultural advice for Indian farmers, especially in rural areas.

**Method:** The chatbot employs a structured multi-turn conversation model with an intent-driven dialogue flow, combining an instruction-tuned model and Retrieval-Augmented Generation (RAG) to provide personalized responses.

**Key Contributions:**

	1. Introduction of a structured multi-turn conversation flow in chatbots for agriculture
	2. Use of instruction-tuned models and RAG for personalized agricultural advice
	3. Support for low literacy users through speech input and output features

**Result:** The system achieved a query response accuracy of 97.53%, 91.35% contextual relevance, and an average response time of under 6 seconds, supporting both English and Hindi users.

**Limitations:** 

**Conclusion:** Combining advanced dialogue techniques with machine learning can significantly enhance the accessibility and effectiveness of digital agricultural support systems.

**Abstract:** Indian farmers often lack timely, accessible, and language-friendly agricultural advice, especially in rural areas with low literacy. To address this gap in accessibility, this paper presents a novel AI-powered agricultural chatbot, Krishi Sathi, designed to support Indian farmers by providing personalized, easy-to-understand answers to their queries through both text and speech. The system's intelligence stems from an IFT model, subsequently refined through fine-tuning on Indian agricultural knowledge across three curated datasets. Unlike traditional chatbots that respond to one-off questions, Krishi Sathi follows a structured, multi-turn conversation flow to gradually collect the necessary details from the farmer, ensuring the query is fully understood before generating a response. Once the intent and context are extracted, the system performs Retrieval-Augmented Generation (RAG) by first fetching information from a curated agricultural database and then generating a tailored response using the IFT model. The chatbot supports both English and Hindi languages, with speech input and output features (via ASR and TTS) to make it accessible for users with low literacy or limited digital skills. This work demonstrates how combining intent-driven dialogue flows, instruction-tuned models, and retrieval-based generation can improve the quality and accessibility of digital agricultural support in India.   This approach yielded strong results, with the system achieving a query response accuracy of 97.53%, 91.35% contextual relevance and personalization, and a query completion rate of 97.53%. The average response time remained under 6 seconds, ensuring timely support for users across both English and Hindi interactions.

</details>


### [30] [Hierarchical Verification of Speculative Beams for Accelerating LLM Inference](https://arxiv.org/abs/2508.03726)

*Jaydip Sen, Harshitha Puvvala, Subhasis Dasgupta*

**Main category:** cs.CL

**Keywords:** large language models, inference efficiency, speculative decoding

**Relevance Score:** 8

**TL;DR:** This paper introduces the Hierarchical Verification Tree (HVT) framework to improve inference efficiency in large language models (LLMs) by prioritizing high-likelihood drafts and enabling early pruning of suboptimal candidates.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with inference efficiency, leading to computational overhead during the decoding process. This paper aims to address these challenges with a new approach.

**Method:** The paper proposes a Hierarchical Verification Tree for speculative beam decoding, which allows for prioritization of drafts and early pruning of suboptimal candidates. It includes theoretical foundations and an algorithm for formal verification-pruning integrated with standard LLM inference workflows.

**Key Contributions:**

	1. Introduction of the Hierarchical Verification Tree framework
	2. Development of a theoretical foundation for verification-pruning
	3. Demonstrated improvements in inference efficiency without requiring model retraining

**Result:** Experimental evaluations show that HVT outperforms existing speculative decoding methods, reducing inference time and energy consumption while improving output quality across various datasets and models.

**Limitations:** 

**Conclusion:** The findings support hierarchical verification as a viable strategy for enhancing the efficiency of large language model inference.

**Abstract:** Large language models (LLMs) have achieved remarkable success across diverse natural language processing tasks but face persistent challenges in inference efficiency due to their autoregressive nature. While speculative decoding and beam sampling offer notable improvements, traditional methods verify draft sequences sequentially without prioritization, leading to unnecessary computational overhead. This work proposes the Hierarchical Verification Tree (HVT), a novel framework that restructures speculative beam decoding by prioritizing high-likelihood drafts and enabling early pruning of suboptimal candidates. Theoretical foundations and a formal verification-pruning algorithm are developed to ensure correctness and efficiency. Integration with standard LLM inference pipelines is achieved without requiring retraining or architecture modification. Experimental evaluations across multiple datasets and models demonstrate that HVT consistently outperforms existing speculative decoding schemes, achieving substantial reductions in inference time and energy consumption while maintaining or enhancing output quality. The findings highlight the potential of hierarchical verification strategies as a new direction for accelerating large language model inference.

</details>


### [31] [WINELL: Wikipedia Never-Ending Updating with LLM Agents](https://arxiv.org/abs/2508.03728)

*Revanth Gangi Reddy, Tanay Dixit, Jiaxin Qin, Cheng Qian, Daniel Lee, Jiawei Han, Kevin Small, Xing Fan, Ruhi Sarikaya, Heng Ji*

**Main category:** cs.CL

**Keywords:** Wikipedia, LLM, knowledge acquisition, editing models, multi-agent framework

**Relevance Score:** 8

**TL;DR:** WiNELL is a multi-agent framework for continuously updating Wikipedia articles, leveraging LLM-based editing models to suggest accurate edits for human review.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Wikipedia struggles with keeping content current due to manual editing; the goal is to automate this process using LLM technology.

**Method:** A multi-agent framework aggregates online information and generates edit suggestions based on fine-grained models trained on Wikipedia's edit history.

**Key Contributions:**

	1. Introduction of WiNELL framework for automatic Wikipedia updates
	2. Fine-grained editing models outperform existing baselines
	3. End-to-end evaluation showing effectiveness in providing timely updates.

**Result:** WiNELL outperforms existing LLMs and instruction-following models in suggesting timely factual updates, demonstrating higher efficiency and coverage.

**Limitations:** 

**Conclusion:** The research showcases potential in using LLM agents for ongoing knowledge base updates, enhancing Wikipedia‚Äôs reliability and currency.

**Abstract:** Wikipedia, a vast and continuously consulted knowledge base, faces significant challenges in maintaining up-to-date content due to its reliance on manual human editors. Inspired by the vision of continuous knowledge acquisition in NELL and fueled by advances in LLM-based agents, this paper introduces WiNELL, an agentic framework for continuously updating Wikipedia articles. Our approach employs a multi-agent framework to aggregate online information, select new and important knowledge for a target entity in Wikipedia, and then generate precise edit suggestions for human review. Our fine-grained editing models, trained on Wikipedia's extensive history of human edits, enable incorporating updates in a manner consistent with human editing behavior. Our editor models outperform both open-source instruction-following baselines and closed-source LLMs (e.g., GPT-4o) in key information coverage and editing efficiency. End-to-end evaluation on high-activity Wikipedia pages demonstrates WiNELL's ability to identify and suggest timely factual updates. This opens up a promising research direction in LLM agents for automatically updating knowledge bases in a never-ending fashion.

</details>


### [32] [GanitBench: A bi-lingual benchmark for evaluating mathematical reasoning in Vision Language Models](https://arxiv.org/abs/2508.03737)

*Ashutosh Bandooni, Brindha Subburaj*

**Main category:** cs.CL

**Keywords:** Vision Language Models, GanitBench, Multilingual Datasets, Mathematics, Hindi Language

**Relevance Score:** 6

**TL;DR:** GanitBench is a benchmark of 1527 vision-only questions in English and Hindi focusing on Mathematics, evaluating Vision Language Models (VLMs).

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual datasets for reasoning tasks in Vision Language Models, particularly in Hindi, and to evaluate their performance on complex mathematical questions.

**Method:** The paper introduces GanitBench, curating vision questions from Indian examinations (JEE Advanced and CBSE) in both English and Hindi. It evaluates two model setups: zero-shot and two-shot Chain-of-Thought (CoT), applying a 'Double Lock' constraint to test the models' capabilities.

**Key Contributions:**

	1. Introduction of GanitBench benchmark in English and Hindi
	2. Evaluation of VLMs on complex mathematics questions
	3. Demonstration of performance differences in language settings

**Result:** The models show a maximum average accuracy of 38.15%, with two-shot CoT outperforming zero-shot under constraints. Performance deteriorates in Hindi as compared to English.

**Limitations:** The performance metrics are limited to two evaluated models and only cover mathematical reasoning, which may not generalize to other domains.

**Conclusion:** GanitBench aims to facilitate research in multilingual VLMs, showing significant differences in performances across languages.

**Abstract:** Benchmarks for evaluating reasoning among Vision Language Models (VLMs) on several fields and domains are being curated more frequently over the last few years. However these are often monolingual, mostly available in English. Additionally there also is a lack of datasets available in Hindi on tasks apart from comprehension and translation. We introduce GanitBench, a tough benchmark consisting of 1527 vision-only questions covering several topics in Mathematics - available in languages English and Hindi. Collected from two major examinations from India, the JEE Advanced and the CBSE Boards examinations, this benchmark includes questions in the form of images comprising of figures essential to a question as well as text. We evaluate two closed source models for the same, in zero-shot Chain-of-Thought (CoT) and two-shot CoT settings. GPT-4o mini is found to be the more dominant model on the benchmark, with it's highest average accuracy being 38.15%. We also evaluate models through a "Double Lock" constraint, which brings down the performance of the models by considerable margins. We observe that two-shot CoT appears to be a more effective setting under this environment. Performance of the two VLMs also decreases when answering the same questions in the Hindi language. We hope to facilitate the inclusion of languages like Hindi in research through our work.

</details>


### [33] [AttnTrace: Attention-based Context Traceback for Long-Context LLMs](https://arxiv.org/abs/2508.03793)

*Yanting Wang, Runpeng Geng, Ying Chen, Jinyuan Jia*

**Main category:** cs.CL

**Keywords:** long-context, large language models, context traceback, AI systems, interpretability

**Relevance Score:** 8

**TL;DR:** AttnTrace is a new efficient method for tracing context in LLM responses, outperforming current solutions both in accuracy and computation speed.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and accuracy of context traceback methods used in LLM systems, particularly in applications like forensic analysis and LLM output interpretability.

**Method:** AttnTrace utilizes attention weights from LLMs to trace back context in a response. The paper introduces two enhancement techniques to optimize this process and provides theoretical insights into these choices.

**Key Contributions:**

	1. Introduction of a novel context traceback method using attention weights
	2. Two techniques to enhance the effectiveness of AttnTrace
	3. Demonstration of real-world applications in LLM output manipulation detection

**Result:** AttnTrace is shown to be more accurate and efficient than existing state-of-the-art methods in context traceback, and it also enhances detection of prompt injection in long contexts.

**Limitations:** 

**Conclusion:** AttnTrace offers a novel approach to context traceback that not only improves current methods but also has practical applications, as demonstrated by identifying manipulated instructions in LLM-generated reviews.

**Abstract:** Long-context large language models (LLMs), such as Gemini-2.5-Pro and Claude-Sonnet-4, are increasingly used to empower advanced AI systems, including retrieval-augmented generation (RAG) pipelines and autonomous agents. In these systems, an LLM receives an instruction along with a context--often consisting of texts retrieved from a knowledge database or memory--and generates a response that is contextually grounded by following the instruction. Recent studies have designed solutions to trace back to a subset of texts in the context that contributes most to the response generated by the LLM. These solutions have numerous real-world applications, including performing post-attack forensic analysis and improving the interpretability and trustworthiness of LLM outputs. While significant efforts have been made, state-of-the-art solutions such as TracLLM often lead to a high computation cost, e.g., it takes TracLLM hundreds of seconds to perform traceback for a single response-context pair. In this work, we propose AttnTrace, a new context traceback method based on the attention weights produced by an LLM for a prompt. To effectively utilize attention weights, we introduce two techniques designed to enhance the effectiveness of AttnTrace, and we provide theoretical insights for our design choice. We also perform a systematic evaluation for AttnTrace. The results demonstrate that AttnTrace is more accurate and efficient than existing state-of-the-art context traceback methods. We also show that AttnTrace can improve state-of-the-art methods in detecting prompt injection under long contexts through the attribution-before-detection paradigm. As a real-world application, we demonstrate that AttnTrace can effectively pinpoint injected instructions in a paper designed to manipulate LLM-generated reviews. The code is at https://github.com/Wang-Yanting/AttnTrace.

</details>


### [34] [Majority Bit-Aware Watermarking For Large Language Models](https://arxiv.org/abs/2508.03829)

*Jiahao Xu, Rui Hu, Zikai Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, watermarking, text generation, machine learning, human-computer interaction

**Relevance Score:** 9

**TL;DR:** MajorMark is a novel multi-bit watermarking method for Large Language Models that enhances decoding accuracy while maintaining text generation quality by using majority bit-aware encoding and a clustering-based decoding strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about the misuse of Large Language Models in generating harmful content, watermarking techniques are proposed for origin verification and misuse tracing.

**Method:** MajorMark selects preferred token sets based on the majority bit of the message and uses a clustering-based decoding strategy.

**Key Contributions:**

	1. Introduction of MajorMark framework for watermarking LLMs
	2. Majority bit-aware encoding technique
	3. Clustering-based decoding strategy to enhance accuracy

**Result:** Experiments show significant improvements in both decoding accuracy and text generation quality compared to prior multi-bit watermarking methods.

**Limitations:** 

**Conclusion:** The proposed methods, including MajorMark$^+$, enhance the quality of watermarked text and improve decoding accuracy while preserving content quality.

**Abstract:** The growing deployment of Large Language Models (LLMs) in real-world applications has raised concerns about their potential misuse in generating harmful or deceptive content. To address this issue, watermarking techniques have emerged as a promising solution by embedding identifiable binary messages into generated text for origin verification and misuse tracing. While recent efforts have explored multi-bit watermarking schemes capable of embedding rich information such as user identifiers, they typically suffer from the fundamental trade-off between text quality and decoding accuracy: to ensure reliable message decoding, they have to restrict the size of preferred token sets during encoding, yet such restrictions reduce the quality of the generated content. In this work, we propose MajorMark, a novel watermarking method that improves this trade-off through majority bit-aware encoding. MajorMark selects preferred token sets based on the majority bit of the message, enabling a larger and more flexible sampling of tokens. In contrast to prior methods that rely on token frequency analysis for decoding, MajorMark employs a clustering-based decoding strategy, which maintains high decoding accuracy even when the preferred token set is large, thus preserving both content quality and decoding accuracy. We further introduce MajorMark$^+$, which partitions the message into multiple blocks to independently encode and deterministically decode each block, thereby further enhancing the quality of watermarked text and improving decoding accuracy. Extensive experiments on state-of-the-art LLMs demonstrate that our methods significantly enhance both decoding accuracy and text generation quality, outperforming prior multi-bit watermarking baselines.

</details>


### [35] [Hallucination to Truth: A Review of Fact-Checking and Factuality Evaluation in Large Language Models](https://arxiv.org/abs/2508.03860)

*Subhey Sadi Rahman, Md. Adnanul Islam, Md. Mahbub Alam, Musarrat Zeba, Md. Abdur Rahman, Sadia Sultana Chowa, Mohaimenul Azam Khan Raiaan, Sami Azam*

**Main category:** cs.CL

**Keywords:** Large Language Models, fact-checking, evaluation methods, retrieval-augmented generation, misinformation

**Relevance Score:** 9

**TL;DR:** This review analyzes the evaluation of factual accuracy in LLM-generated content, addressing challenges and proposing improvements in fact-checking methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the misinformation generated by LLMs due to the inaccuracies in their training data, emphasizing the need for effective evaluation and fact-checking strategies.

**Method:** The review systematically explores the literature from 2020 to 2025, focusing on challenges like hallucinations, dataset limitations, and evaluation metrics, and proposes five guiding research questions.

**Key Contributions:**

	1. Analysis of evaluation challenges of LLM-generated content
	2. Proposed research questions for future investigation
	3. Emphasis on the need for robust fact-checking frameworks

**Result:** The findings reveal significant limitations in current evaluation metrics and highlight the necessity for domain-specific customization and validation of outputs with external evidence for better factual consistency.

**Limitations:** The review may not cover all aspects of LLM evaluation and depends on existing literature which may vary in depth and focus.

**Conclusion:** The paper underscores the importance of developing LLMs that are accurate, explainable, and capable of domain-specific fact-checking.

**Abstract:** Large Language Models (LLMs) are trained on vast and diverse internet corpora that often include inaccurate or misleading content. Consequently, LLMs can generate misinformation, making robust fact-checking essential. This review systematically analyzes how LLM-generated content is evaluated for factual accuracy by exploring key challenges such as hallucinations, dataset limitations, and the reliability of evaluation metrics. The review emphasizes the need for strong fact-checking frameworks that integrate advanced prompting strategies, domain-specific fine-tuning, and retrieval-augmented generation (RAG) methods. It proposes five research questions that guide the analysis of the recent literature from 2020 to 2025, focusing on evaluation methods and mitigation techniques. The review also discusses the role of instruction tuning, multi-agent reasoning, and external knowledge access via RAG frameworks. Key findings highlight the limitations of current metrics, the value of grounding outputs with validated external evidence, and the importance of domain-specific customization to improve factual consistency. Overall, the review underlines the importance of building LLMs that are not only accurate and explainable but also tailored for domain-specific fact-checking. These insights contribute to the advancement of research toward more trustworthy and context-aware language models.

</details>


### [36] [An Entity Linking Agent for Question Answering](https://arxiv.org/abs/2508.03865)

*Yajie Luo, Yihong Wu, Muzhi Li, Fengran Mo, Jia Ao Sun, Xinyu Wang, Liheng Ma, Yingxue Zhang, Jian-Yun Nie*

**Main category:** cs.CL

**Keywords:** entity linking, question answering, Large Language Model, cognitive workflows, ambiguity

**Relevance Score:** 8

**TL;DR:** This paper presents a novel entity linking agent for question answering systems using a Large Language Model, designed to improve performance on short, ambiguous user queries.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing entity linking methods struggle with short, ambiguous questions in QA systems, necessitating a new approach.

**Method:** The proposed entity linking agent simulates human cognitive workflows to identify entity mentions, retrieve candidate entities, and make decisions in a QA context.

**Key Contributions:**

	1. Novel entity linking agent based on Large Language Model.
	2. Improved handling of short, ambiguous questions in QA tasks.
	3. Demonstrated effectiveness through experimental evaluations.

**Result:** Experiments demonstrate the robustness and effectiveness of the agent through tool-based entity linking and QA task evaluations.

**Limitations:** 

**Conclusion:** The results highlight the agent's capability to handle short user queries effectively compared to traditional methods.

**Abstract:** Some Question Answering (QA) systems rely on knowledge bases (KBs) to provide accurate answers. Entity Linking (EL) plays a critical role in linking natural language mentions to KB entries. However, most existing EL methods are designed for long contexts and do not perform well on short, ambiguous user questions in QA tasks. We propose an entity linking agent for QA, based on a Large Language Model that simulates human cognitive workflows. The agent actively identifies entity mentions, retrieves candidate entities, and makes decision. To verify the effectiveness of our agent, we conduct two experiments: tool-based entity linking and QA task evaluation. The results confirm the robustness and effectiveness of our agent.

</details>


### [37] [Sotopia-RL: Reward Design for Social Intelligence](https://arxiv.org/abs/2508.03905)

*Haofei Yu, Zhengyang Qi, Yining Zhao, Kolby Nottingham, Keyang Xuan, Bodhisattwa Prasad Majumder, Hao Zhu, Paul Pu Liang, Jiaxuan You*

**Main category:** cs.CL

**Keywords:** reinforcement learning, social intelligence, large language models, multi-dimensional rewards, utterance-level credit assignment

**Relevance Score:** 9

**TL;DR:** Sotopia-RL is a framework that enhances reinforcement learning for training socially intelligent agents by refining feedback into utterance-level, multi-dimensional rewards, addressing challenges in social interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the training of socially intelligent agents in large language models (LLMs) by addressing challenges like partial observability and multi-dimensionality in social interactions.

**Method:** The paper proposes Sotopia-RL, which categorizes feedback into utterance-level credit assignments and multi-dimensional rewards, enabling effective training in a social learning environment.

**Key Contributions:**

	1. Introduction of Sotopia-RL framework for socially intelligent agents.
	2. Demonstration of improved training efficiency through utterance-level feedback.
	3. State-of-the-art performance in social goal completion in the Sotopia environment.

**Result:** Sotopia-RL achieves state-of-the-art social goal completion scores, outperforming existing methods in the Sotopia social learning environment.

**Limitations:** 

**Conclusion:** The findings validate the importance of utterance-level credit assignments and multi-dimensional rewards in reinforcement learning for effective social interaction training.

**Abstract:** Social intelligence has become a critical capability for large language models (LLMs), enabling them to engage effectively in real-world social tasks such as accommodation, persuasion, collaboration, and negotiation. Reinforcement learning (RL) is a natural fit for training socially intelligent agents because it allows models to learn sophisticated strategies directly through social interactions. However, social interactions have two key characteristics that set barriers for RL training: (1) partial observability, where utterances have indirect and delayed effects that complicate credit assignment, and (2) multi-dimensionality, where behaviors such as rapport-building or knowledge-seeking contribute indirectly to goal achievement. These characteristics make Markov decision process (MDP)-based RL with single-dimensional episode-level rewards inefficient and unstable. To address these challenges, we propose Sotopia-RL, a novel framework that refines coarse episode-level feedback into utterance-level, multi-dimensional rewards. Utterance-level credit assignment mitigates partial observability by attributing outcomes to individual utterances, while multi-dimensional rewards capture the full richness of social interactions and reduce reward hacking. Experiments in Sotopia, an open-ended social learning environment, demonstrate that Sotopia-RL achieves state-of-the-art social goal completion scores (7.17 on Sotopia-hard and 8.31 on Sotopia-full), significantly outperforming existing approaches. Ablation studies confirm the necessity of both utterance-level credit assignment and multi-dimensional reward design for RL training. Our implementation is publicly available at: https://github.com/sotopia-lab/sotopia-rl.

</details>


### [38] [CoAct-1: Computer-using Agents with Coding as Actions](https://arxiv.org/abs/2508.03923)

*Linxin Song, Yutong Dai, Viraj Prabhu, Jieyu Zhang, Taiwei Shi, Li Li, Junnan Li, Silvio Savarese, Zeyuan Chen, Jieyu Zhao, Ran Xu, Caiming Xiong*

**Main category:** cs.CL

**Keywords:** autonomous agents, GUI manipulation, coding execution, computer automation, multi-agent system

**Relevance Score:** 6

**TL;DR:** Introducing CoAct-1, a hybrid multi-agent system combining GUI control with coding to enhance efficiency in computer automation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and brittleness of autonomous agents that solely rely on GUI manipulation in complex tasks.

**Method:** CoAct-1 utilizes an Orchestrator to delegate tasks dynamically between a GUI Operator and a Programmer agent capable of executing Python or Bash scripts.

**Key Contributions:**

	1. Introduction of CoAct-1 multi-agent system
	2. Hybrid approach of combining GUI manipulation with coding execution
	3. Achievement of new state-of-the-art on OSWorld benchmark

**Result:** CoAct-1 achieved a state-of-the-art success rate of 60.76% on the OSWorld benchmark and reduced task completion steps to 10.15 on average.

**Limitations:** 

**Conclusion:** Integrating coding as a core action makes autonomous agents more powerful, efficient, and scalable in general computer automation.

**Abstract:** Autonomous agents that operate computers via Graphical User Interfaces (GUIs) often struggle with efficiency and reliability on complex, long-horizon tasks. While augmenting these agents with planners can improve task decomposition, they remain constrained by the inherent limitations of performing all actions through GUI manipulation, leading to brittleness and inefficiency. In this work, we introduce a more robust and flexible paradigm: enabling agents to use coding as a enhanced action. We present CoAct-1, a novel multi-agent system that synergistically combines GUI-based control with direct programmatic execution. CoAct-1 features an Orchestrator that dynamically delegates subtasks to either a conventional GUI Operator or a specialized Programmer agent, which can write and execute Python or Bash scripts. This hybrid approach allows the agent to bypass inefficient GUI action sequences for tasks like file management and data processing, while still leveraging visual interaction when necessary. We evaluate our system on the challenging OSWorld benchmark, where CoAct-1 achieves a new state-of-the-art success rate of 60.76%, significantly outperforming prior methods. Furthermore, our approach dramatically improves efficiency, reducing the average number of steps required to complete a task to just 10.15, compared to 15 for leading GUI agents. Our results demonstrate that integrating coding as a core action provides a more powerful, efficient, and scalable path toward generalized computer automation.

</details>


### [39] [CAP-LLM: Context-Augmented Personalized Large Language Models for News Headline Generation](https://arxiv.org/abs/2508.03935)

*Raymond Wilson, Cole Graham, Chase Carter, Zefeng Yang, Ruiqi Gu*

**Main category:** cs.CL

**Keywords:** personalized news headlines, large language models, fact consistency, user preferences, headline generation

**Relevance Score:** 9

**TL;DR:** CAP-LLM is a novel framework for personalized news headline generation, effectively balancing user preferences and factual consistency using a Large Language Model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of creating personalized news headlines that accurately reflect user interests while maintaining factual accuracy amidst information overload.

**Method:** The proposed CAP-LLM framework integrates a User Preference Encoder to capture user interests, a Context Injection Adapter to blend these preferences with article context, and a Fact-Consistency Reinforcement Module to prevent hallucinations in generated headlines.

**Key Contributions:**

	1. Introduction of CAP-LLM framework for personalized headline generation
	2. Development of User Preference Encoder and Context Injection Adapter
	3. Fact-Consistency Reinforcement Module employing contrastive loss for better accuracy

**Result:** CAP-LLM outperforms state-of-the-art models on the PENS dataset, achieving an improved FactCC score of 87.50 and enhancing personalization and content coverage metrics significantly.

**Limitations:** 

**Conclusion:** The results highlight CAP-LLM's effectiveness in generating personalized and factually consistent news headlines, demonstrating a successful integration of user preferences and factual constraints.

**Abstract:** In the era of information overload, personalized news headline generation is crucial for engaging users by tailoring content to their preferences while accurately conveying news facts. Existing methods struggle with effectively capturing complex user interests and ensuring factual consistency, often leading to generic or misleading headlines. Leveraging the unprecedented capabilities of Large Language Models (LLMs) in text generation, we propose Context-Augmented Personalized LLM (CAP-LLM), a novel framework that integrates user preferences and factual consistency constraints into a powerful pre-trained LLM backbone. CAP-LLM features a User Preference Encoder to capture long-term user interests, a Context Injection Adapter to seamlessly integrate these preferences and current article context into the LLM's generation process, and a Fact-Consistency Reinforcement Module employing a novel contrastive loss to mitigate hallucination. Evaluated on the real-world PENS dataset, CAP-LLM achieves state-of-the-art performance across all metrics. Notably, it significantly improves factual consistency (FactCC of 87.50) over strong baselines like BART (86.67), while simultaneously enhancing personalization (Pc(avg) 2.73, Pc(max) 17.25) and content coverage (ROUGE-1 26.55, ROUGE-2 9.95, ROUGE-L 23.01). Our ablation studies, human evaluations, and sensitivity analyses further validate the effectiveness of each component and the robustness of our approach, demonstrating CAP-LLM's ability to achieve a superior balance between personalization and factual accuracy in news headline generation.

</details>


### [40] [Data and AI governance: Promoting equity, ethics, and fairness in large language models](https://arxiv.org/abs/2508.03970)

*Alok Abhishek, Lisa Erickson, Tushar Bandopadhyay*

**Main category:** cs.CL

**Keywords:** Bias, Fairness, Governance, Large Language Models, Ethical AI

**Relevance Score:** 9

**TL;DR:** The paper discusses a comprehensive approach to govern, assess, and quantify bias in machine learning models, focusing on Large Language Models (LLMs), and proposes a governance framework for ethical AI deployment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address bias, ethics, fairness, and factuality in the development and deployment of Large Language Models.

**Method:** The paper builds on the Bias Evaluation and Assessment Test Suite (BEATS) and suggests a data and AI governance framework suitable for real-world applications, enabling benchmarking, evaluation, and governance throughout the AI lifecycle.

**Key Contributions:**

	1. Introduction of a comprehensive governance framework for bias assessment in LLMs
	2. Implementation of real-time evaluation and governance throughout AI lifecycle
	3. Contribution to the discourse on ethical AI deployment

**Result:** The framework enhances the safety and responsibility of generative AI systems, mitigating risks of discrimination and protecting brand reputation through continuous evaluation and proactive governance.

**Limitations:** 

**Conclusion:** The article aims to promote the creation and deployment of socially responsible and ethically aligned generative AI applications.

**Abstract:** In this paper, we cover approaches to systematically govern, assess and quantify bias across the complete life cycle of machine learning models, from initial development and validation to ongoing production monitoring and guardrail implementation. Building upon our foundational work on the Bias Evaluation and Assessment Test Suite (BEATS) for Large Language Models, the authors share prevalent bias and fairness related gaps in Large Language Models (LLMs) and discuss data and AI governance framework to address Bias, Ethics, Fairness, and Factuality within LLMs. The data and AI governance approach discussed in this paper is suitable for practical, real-world applications, enabling rigorous benchmarking of LLMs prior to production deployment, facilitating continuous real-time evaluation, and proactively governing LLM generated responses. By implementing the data and AI governance across the life cycle of AI development, organizations can significantly enhance the safety and responsibility of their GenAI systems, effectively mitigating risks of discrimination and protecting against potential reputational or brand-related harm. Ultimately, through this article, we aim to contribute to advancement of the creation and deployment of socially responsible and ethically aligned generative artificial intelligence powered applications.

</details>


### [41] [Confidence-Weighted Token Set Cover for Early Hypothesis Pruning in Self-Consistency](https://arxiv.org/abs/2508.03979)

*Md Arafat Sultan, Ram√≥n Fernandez Astudillo*

**Main category:** cs.CL

**Keywords:** self-consistency, token efficiency, hypothesis pruning, LLMs, chain-of-thought reasoning

**Relevance Score:** 8

**TL;DR:** This paper investigates making self-consistency in long chain-of-thought reasoning tasks more token-efficient through early hypothesis pruning, resulting in a token efficiency improvement of 10-35%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The high token expenditure of self-consistency limits its practical utility, leading to the exploration of token-efficient methods that maintain parallelism.

**Method:** The paper implements early hypothesis pruning based on model confidence and lexical coverage through a fast weighted set cover algorithm, applying this to evaluations with five LLMs across three math benchmarks.

**Key Contributions:**

	1. Introduction of early hypothesis pruning for token efficiency
	2. Utilization of model confidence and lexical coverage as pruning indicators
	3. Demonstration of improved efficiency across multiple models and benchmarks

**Result:** The method shows improvements in token efficiency for all tested models, with a reduction of 10-35% in token usage in many scenarios.

**Limitations:** 

**Conclusion:** The proposed approach enhances the practicality of self-consistency methods in LLMs by significantly reducing token expenditure while maintaining performance.

**Abstract:** Despite its simplicity and efficacy, the high token expenditure of self-consistency can limit its practical utility. Here we investigate if self-consistency can be made more token-efficient for long chain-of-thought reasoning tasks, while preserving its parallelism, through early hypothesis pruning. Concretely, we generate all solutions in parallel, but periodically prune intermediate hypotheses that are deemed unnecessary based on two lightweight indicators: (a) the model's own confidence in individual hypotheses, and (b) lexical coverage of all current hypotheses by candidate subsets that are under consideration for continued retention. We design a fast weighted set cover algorithm that utilizes the two indicators; our evaluation of five LLMs on three math benchmarks shows that this method can improve token efficiency for all models, by 10-35% in many cases.

</details>


### [42] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)

*Bohan Jiang, Dawei Li, Zhen Tan, Chengshuai Zhao, Huan Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, well-being, explanation generation, Supervised Fine-Tuning, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper explores the ability of Large Language Models (LLMs) to generate high-quality explanations about well-being concepts tailored to diverse audiences, introducing a novel evaluation framework and demonstrating that fine-tuning significantly improves explanation quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As individuals increasingly seek guidance on well-being from LLMs, it is essential to generate explanations that are not only accurate but also suitable for varied audience expertise levels.

**Method:** A large-scale dataset of 43,880 explanations for 2,194 well-being concepts was constructed from ten different LLMs. The paper introduces a principle-guided evaluation framework employing dual judges for quality assessment, alongside fine-tuning methods like Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO).

**Key Contributions:**

	1. Construction of a large-scale dataset of well-being explanations from LLMs.
	2. Development of a principle-guided evaluation framework for assessing explanation quality.
	3. Demonstration that fine-tuning methods significantly enhance the effectiveness of LLMs in generating suitable explanations.

**Result:** The study found that LLM judges align well with human evaluations, reveals significant variation in explanation quality across models and audiences, and shows that DPO- and SFT-finetuned models outperform larger models in generating specialized explanations.

**Limitations:** 

**Conclusion:** Fine-tuning LLMs using preference-based learning yields better explanations for well-being, indicating the necessity of tailoring explanation quality to audience expertise.

**Abstract:** Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.

</details>


### [43] [Transferring Expert Cognitive Models to Social Robots via Agentic Concept Bottleneck Models](https://arxiv.org/abs/2508.03998)

*Xinyu Zhao, Zhen Tan, Maya Enisman, Minjae Seo, Marta R. Durantini, Dolores Albarracin, Tianlong Chen*

**Main category:** cs.CL

**Keywords:** social robot, facilitator support, transfer learning, concept bottleneck model, group dynamics

**Relevance Score:** 8

**TL;DR:** The paper presents a social robot co-facilitator that enhances group meetings by analyzing multimodal data to provide discreet cues to facilitators, utilizing a transparent agentic concept bottleneck model (CBM) powered by transfer learning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by facilitators in group settings, including individual goal execution and interpersonal dynamics, by providing an embodied technology that aids in interventions and enhances group relationships.

**Method:** The proposed approach employs a social robot powered by an agentic concept bottleneck model that interprets multimodal meeting data, providing recommendations based on human-interpretable concepts such as engagement and sentiment.

**Key Contributions:**

	1. Development of a transfer learning framework for social robots
	2. Introduction of a transparent concept bottleneck model for facilitator support
	3. Demonstration of successful knowledge transfer from expert to novice facilitators

**Result:** The model outperforms traditional zero-shot foundation models in identifying the need for intervention and successfully generalizes across various groups, demonstrating effective knowledge transfer from expert facilitators to novices.

**Limitations:** 

**Conclusion:** The study illustrates how interpreting social dynamics through a transparent, concept-driven model can augment human facilitators in complex social interactions, providing a framework for future technology integration in group settings.

**Abstract:** Successful group meetings, such as those implemented in group behavioral-change programs, work meetings, and other social contexts, must promote individual goal setting and execution while strengthening the social relationships within the group. Consequently, an ideal facilitator must be sensitive to the subtle dynamics of disengagement, difficulties with individual goal setting and execution, and interpersonal difficulties that signal a need for intervention. The challenges and cognitive load experienced by facilitators create a critical gap for an embodied technology that can interpret social exchanges while remaining aware of the needs of the individuals in the group and providing transparent recommendations that go beyond powerful but "black box" foundation models (FMs) that identify social cues. We address this important demand with a social robot co-facilitator that analyzes multimodal meeting data and provides discreet cues to the facilitator. The robot's reasoning is powered by an agentic concept bottleneck model (CBM), which makes decisions based on human-interpretable concepts like participant engagement and sentiments, ensuring transparency and trustworthiness. Our core contribution is a transfer learning framework that distills the broad social understanding of an FM into our specialized and transparent CBM. This concept-driven system significantly outperforms direct zero-shot FMs in predicting the need for intervention and enables real-time human correction of its reasoning. Critically, we demonstrate robust knowledge transfer: the model generalizes across different groups and successfully transfers the expertise of senior human facilitators to improve the performance of novices. By transferring an expert's cognitive model into an interpretable robotic partner, our work provides a powerful blueprint for augmenting human capabilities in complex social domains.

</details>


### [44] [HarmonyGuard: Toward Safety and Utility in Web Agents via Adaptive Policy Enhancement and Dual-Objective Optimization](https://arxiv.org/abs/2508.04010)

*Yurun Chen, Xavier Hu, Yuhan Liu, Keting Yin, Juncheng Li, Zhuosheng Zhang, Shengyu Zhang*

**Main category:** cs.CL

**Keywords:** multi-agent systems, language models, safety optimization, utility performance, web environments

**Relevance Score:** 8

**TL;DR:** This paper presents HarmonyGuard, a multi-agent framework designed to enhance safety and utility in web environments by optimizing task performance while addressing emerging risks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for web agents to balance task performance with safety in the face of evolving threats.

**Method:** HarmonyGuard employs a multi-agent architecture with two main components: an adaptive Policy Agent for policy maintenance and a Utility Agent for dual-objective optimization of safety and utility.

**Key Contributions:**

	1. Introduction of the HarmonyGuard framework for multi-agent collaboration.
	2. Adaptive Policy Enhancement through the Policy Agent for updating security policies.
	3. Dual-Objective Optimization using the Utility Agent for balancing safety and utility.

**Result:** HarmonyGuard demonstrates improved policy compliance by up to 38% and task completion by up to 20% compared to existing methods, achieving over 90% policy compliance across all tasks.

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances the ability of agents to operate safely in uncertain web environments while effectively completing tasks.

**Abstract:** Large language models enable agents to autonomously perform tasks in open web environments. However, as hidden threats within the web evolve, web agents face the challenge of balancing task performance with emerging risks during long-sequence operations. Although this challenge is critical, current research remains limited to single-objective optimization or single-turn scenarios, lacking the capability for collaborative optimization of both safety and utility in web environments. To address this gap, we propose HarmonyGuard, a multi-agent collaborative framework that leverages policy enhancement and objective optimization to jointly improve both utility and safety. HarmonyGuard features a multi-agent architecture characterized by two fundamental capabilities: (1) Adaptive Policy Enhancement: We introduce the Policy Agent within HarmonyGuard, which automatically extracts and maintains structured security policies from unstructured external documents, while continuously updating policies in response to evolving threats. (2) Dual-Objective Optimization: Based on the dual objectives of safety and utility, the Utility Agent integrated within HarmonyGuard performs the Markovian real-time reasoning to evaluate the objectives and utilizes metacognitive capabilities for their optimization. Extensive evaluations on multiple benchmarks show that HarmonyGuard improves policy compliance by up to 38% and task completion by up to 20% over existing baselines, while achieving over 90% policy compliance across all tasks. Our project is available here: https://github.com/YurunChen/HarmonyGuard.

</details>


### [45] [Step More: Going Beyond Single Backpropagation in Meta Learning Based Model Editing](https://arxiv.org/abs/2508.04012)

*Xiaopeng Li, Shasha Li, Xi Wang, Shezheng Song, Bin Ji, Shangwen Wang, Jun Ma, Xiaodong Liu, Mina Liu, Jie Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, meta-learning, model editing, low-data scenarios, training efficiency

**Relevance Score:** 9

**TL;DR:** SMEdit is a novel meta-learning based model editing method that improves LLM editing performance in low-data scenarios and enhances training efficiency.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** This paper addresses the high costs of updating knowledge in Large Language Models (LLMs) and the limitations of existing meta-learning based model editing methods in low-data environments.

**Method:** We propose Step More Edit (SMEdit), which utilizes Multiple Backpropagation Steps (MBPS) to enhance editing performance under limited supervision, along with norm regularization for efficient training.

**Key Contributions:**

	1. Introduces SMEdit, an innovative MLBME method.
	2. Utilizes MBPS for enhanced performance in low-data scenarios.
	3. Implements norm regularization for improved training efficiency.

**Result:** Experimental results show that SMEdit outperforms prior meta-learning based model editing methods and that MBPS can be integrated into existing approaches to improve their performance as well.

**Limitations:** 

**Conclusion:** SMEdit offers a promising approach to model editing by addressing the challenges of low-data scenarios and training efficiency, with plans to release the code for public use soon.

**Abstract:** Large Language Models (LLMs) underpin many AI applications, but their static nature makes updating knowledge costly. Model editing offers an efficient alternative by injecting new information through targeted parameter modifications. In particular, meta-learning-based model editing (MLBME) methods have demonstrated notable advantages in both editing effectiveness and efficiency. Despite this, we find that MLBME exhibits suboptimal performance in low-data scenarios, and its training efficiency is bottlenecked by the computation of KL divergence. To address these, we propose $\textbf{S}$tep $\textbf{M}$ore $\textbf{Edit}$ ($\textbf{SMEdit}$), a novel MLBME method that adopts $\textbf{M}$ultiple $\textbf{B}$ackpro$\textbf{P}$agation $\textbf{S}$teps ($\textbf{MBPS}$) to improve editing performance under limited supervision and a norm regularization on weight updates to improve training efficiency. Experimental results on two datasets and two LLMs demonstrate that SMEdit outperforms prior MLBME baselines and the MBPS strategy can be seamlessly integrated into existing methods to further boost their performance. Our code will be released soon.

</details>


### [46] [ZARA: Zero-shot Motion Time-Series Analysis via Knowledge and Retrieval Driven LLM Agents](https://arxiv.org/abs/2508.04038)

*Zechen Li, Baiyu Chen, Hao Xue, Flora D. Salim*

**Main category:** cs.CL

**Keywords:** Human Activity Recognition, Zero-shot Learning, Large Language Models, Motion Sensor, Explainable AI

**Relevance Score:** 9

**TL;DR:** ZARA is a novel, zero-shot explainable human activity recognition (HAR) framework that processes raw motion sensor time-series data without requiring fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing HAR methods require costly retraining for new activities or sensor setups, and previous approaches using LLMs lack accuracy and interpretability.

**Method:** ZARA employs a feature knowledge base, a multi-sensor retrieval module, and a hierarchical agent pipeline that enables the direct prediction of activities and generation of natural-language explanations from raw motion data.

**Key Contributions:**

	1. Introduction of a zero-shot framework for HAR from raw motion time-series
	2. Integration of a knowledge base for pair-wise feature statistics
	3. Hierarchical agent pipeline that generates predictions and explanations

**Result:** ZARA achieves state-of-the-art zero-shot performance on 8 HAR benchmarks, outperforming existing methods by 2.53x in macro F1 score and providing clear reasoning for its predictions.

**Limitations:** 

**Conclusion:** ZARA represents a significant advance in HAR, enabling flexible and interpretable analysis of motion time-series data without the need for fine-tuning.

**Abstract:** Motion sensor time-series are central to human activity recognition (HAR), with applications in health, sports, and smart devices. However, existing methods are trained for fixed activity sets and require costly retraining when new behaviours or sensor setups appear. Recent attempts to use large language models (LLMs) for HAR, typically by converting signals into text or images, suffer from limited accuracy and lack verifiable interpretability. We propose ZARA, the first agent-based framework for zero-shot, explainable HAR directly from raw motion time-series. ZARA integrates an automatically derived pair-wise feature knowledge base that captures discriminative statistics for every activity pair, a multi-sensor retrieval module that surfaces relevant evidence, and a hierarchical agent pipeline that guides the LLM to iteratively select features, draw on this evidence, and produce both activity predictions and natural-language explanations. ZARA enables flexible and interpretable HAR without any fine-tuning or task-specific classifiers. Extensive experiments on 8 HAR benchmarks show that ZARA achieves SOTA zero-shot performance, delivering clear reasoning while exceeding the strongest baselines by 2.53x in macro F1. Ablation studies further confirm the necessity of each module, marking ZARA as a promising step toward trustworthy, plug-and-play motion time-series analysis. Our codes are available at https://github.com/zechenli03/ZARA.

</details>


### [47] [Large Reasoning Models Are Autonomous Jailbreak Agents](https://arxiv.org/abs/2508.04039)

*Thilo Hagendorff, Erik Derner, Nuria Oliver*

**Main category:** cs.CL

**Keywords:** Jailbreaking, Large Reasoning Models, AI Safety, Adversarial Attacks, Model Alignment

**Relevance Score:** 6

**TL;DR:** This study demonstrates that large reasoning models (LRMs) can autonomously perform jailbreaking on AI models, significantly simplifying the process for non-experts with an attack success rate of 97.14%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to highlight how LRMs can undermine the safety mechanisms of AI models, making jailbreaking accessible to those without technical expertise.

**Method:** The methodology involved evaluating four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) as adversaries in multi-turn conversations with nine target models. They received system prompts to plan and execute jailbreaks without supervision.

**Key Contributions:**

	1. Demonstrated LRM capabilities in autonomously conducting jailbreaks
	2. High attack success rate (97.14%) across models
	3. Identified alignment regression issue in AI safety mechanisms

**Result:** The results showed an overall attack success rate of 97.14% across all model combinations, indicating that LRMs effectively bypass safety mechanisms of other models.

**Limitations:** 

**Conclusion:** The conclusion drawn is that there is a significant alignment regression, necessitating immediate efforts to enhance the alignment of frontier models to resist jailbreaks and prevent their misuse as jailbreak agents.

**Abstract:** Jailbreaking -- bypassing built-in safety mechanisms in AI models -- has traditionally required complex technical procedures or specialized human expertise. In this study, we show that the persuasive capabilities of large reasoning models (LRMs) simplify and scale jailbreaking, converting it into an inexpensive activity accessible to non-experts. We evaluated the capabilities of four LRMs (DeepSeek-R1, Gemini 2.5 Flash, Grok 3 Mini, Qwen3 235B) to act as autonomous adversaries conducting multi-turn conversations with nine widely used target models. LRMs received instructions via a system prompt, before proceeding to planning and executing jailbreaks with no further supervision. We performed extensive experiments with a benchmark of harmful prompts composed of 70 items covering seven sensitive domains. This setup yielded an overall attack success rate across all model combinations of 97.14%. Our study reveals an alignment regression, in which LRMs can systematically erode the safety guardrails of other models, highlighting the urgent need to further align frontier models not only to resist jailbreak attempts, but also to prevent them from being co-opted into acting as jailbreak agents.

</details>


### [48] [DTPA: Dynamic Token-level Prefix Augmentation for Controllable Text Generation](https://arxiv.org/abs/2508.04047)

*Jiabing Yang, Yixiang Chen, Zichen Wen, Chenhang Cui, Peiyan Li, Yuan Xu, Bowen Fang, Yan Huang, Liang Wang*

**Main category:** cs.CL

**Keywords:** Controllable Text Generation, Dynamic Token-level Prefix Augmentation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents Dynamic Token-level Prefix Augmentation (DTPA), a novel framework for controllable text generation, particularly for long-form text, addressing challenges in maintaining attribute control with increasing sequence lengths.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective controllable text generation in long-form sequences, as existing methods primarily fail in this aspect, particularly with prefix-based techniques.

**Method:** The proposed DTPA framework selects the optimal prefix type and amplifies attention to the prefix dynamically, using an exponential scaling factor as the sequence length increases, while optionally augmenting the original prompt.

**Key Contributions:**

	1. Introduction of the DTPA framework for enhanced controllability in long-form text generation
	2. Demonstration of the importance of prefix types in text generation
	3. Evaluation showing superior performance of DTPA in various controllable text generation tasks

**Result:** Experiments show that DTPA outperforms existing methods in attribute control in controllable text generation tasks, especially for longer texts, while maintaining fluency and diversity.

**Limitations:** 

**Conclusion:** DTPA effectively enhances controllability in long-form text generation by optimizing prefix usage and maintaining quality.

**Abstract:** Controllable Text Generation (CTG) is a vital subfield in Natural Language Processing (NLP), aiming to generate text that aligns with desired attributes. However, previous studies commonly focus on the quality of controllable text generation for short sequences, while the generation of long-form text remains largely underexplored. In this paper, we observe that the controllability of texts generated by the powerful prefix-based method Air-Decoding tends to decline with increasing sequence length, which we hypothesize primarily arises from the observed decay in attention to the prefixes. Meanwhile, different types of prefixes including soft and hard prefixes are also key factors influencing performance. Building on these insights, we propose a lightweight and effective framework called Dynamic Token-level Prefix Augmentation (DTPA) based on Air-Decoding for controllable text generation. Specifically, it first selects the optimal prefix type for a given task. Then we dynamically amplify the attention to the prefix for the attribute distribution to enhance controllability, with a scaling factor growing exponentially as the sequence length increases. Moreover, based on the task, we optionally apply a similar augmentation to the original prompt for the raw distribution to balance text quality. After attribute distribution reconstruction, the generated text satisfies the attribute constraints well. Experiments on multiple CTG tasks demonstrate that DTPA generally outperforms other methods in attribute control while maintaining competitive fluency, diversity, and topic relevance. Further analysis highlights DTPA's superior effectiveness in long text generation.

</details>


### [49] [PAIRS: Parametric-Verified Adaptive Information Retrieval and Selection for Efficient RAG](https://arxiv.org/abs/2508.04057)

*Wang Chen, Guanqiang Qi, Weikang Li, Yang Li, Deguo Xia, Jizhou Huang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, Information Retrieval

**Relevance Score:** 9

**TL;DR:** The PAIRS framework enhances Retrieval-Augmented Generation by adaptively deciding when to retrieve and how to select external information, improving efficiency and accuracy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address inefficiencies in current RAG systems that retrieve unnecessary information and risk drawing irrelevant documents from sparse queries.

**Method:** PAIRS uses a dual-path generation mechanism where the LLM generates both direct and context-augmented answers to determine if external retrieval is needed, followed by a dual-path retrieval process when necessary.

**Key Contributions:**

	1. Introduces a training-free framework for efficient information retrieval in LLMs.
	2. Implements a dual-path generation mechanism to determine necessity of external knowledge retrieval.
	3. Shows empirical improvements in efficiency and accuracy on QA benchmarks.

**Result:** PAIRS reduces retrieval costs by approximately 25%, triggering for only 75% of queries while improving accuracy by +1.1% EM and +1.0% F1 on average across six QA benchmarks.

**Limitations:** 

**Conclusion:** The PAIRS framework significantly enhances the efficiency and accuracy of RAG systems by streamlining the retrieval process based on context and need.

**Abstract:** Retrieval-Augmented Generation (RAG) has become a cornerstone technique for enhancing large language models (LLMs) with external knowledge. However, current RAG systems face two critical limitations: (1) they inefficiently retrieve information for every query, including simple questions that could be resolved using the LLM's parametric knowledge alone, and (2) they risk retrieving irrelevant documents when queries contain sparse information signals. To address these gaps, we introduce Parametric-verified Adaptive Information Retrieval and Selection (PAIRS), a training-free framework that integrates parametric and retrieved knowledge to adaptively determine whether to retrieve and how to select external information. Specifically, PAIRS employs a dual-path generation mechanism: First, the LLM produces both a direct answer and a context-augmented answer using self-generated pseudo-context. When these outputs converge, PAIRS bypasses external retrieval entirely, dramatically improving the RAG system's efficiency. For divergent cases, PAIRS activates a dual-path retrieval (DPR) process guided by both the original query and self-generated contextual signals, followed by an Adaptive Information Selection (AIS) module that filters documents through weighted similarity to both sources. This simple yet effective approach can not only enhance efficiency by eliminating unnecessary retrievals but also improve accuracy through contextually guided retrieval and adaptive information selection. Experimental results on six question-answering (QA) benchmarks show that PAIRS reduces retrieval costs by around 25% (triggering for only 75% of queries) while still improving accuracy-achieving +1.1% EM and +1.0% F1 over prior baselines on average.

</details>


### [50] [Efficient Strategy for Improving Large Language Model (LLM) Capabilities](https://arxiv.org/abs/2508.04073)

*Juli√°n Camilo Velandia Guti√©rrez*

**Main category:** cs.CL

**Keywords:** Large Language Models, data processing, machine learning, efficiency, resource-constrained environments

**Relevance Score:** 8

**TL;DR:** This paper explores methods to improve the efficiency of Large Language Models (LLMs) in resource-constrained environments by focusing on data processing, training strategies, and architectural adjustments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational resource needs hindering the large-scale deployment of LLMs.

**Method:** The authors defined criteria for reliable dataset construction, conducted controlled experiments with various configurations, and systematically evaluated the resulting model variants in terms of capability, versatility, response time, and safety.

**Key Contributions:**

	1. Introduction of data selection techniques for LLMs in low-resource settings
	2. Development of training strategies to enhance model efficiency
	3. Systematic evaluation of LLM variants and their performance metrics

**Result:** The comparative tests demonstrated that the proposed strategies effectively improved the performance of LLM variants in terms of efficiency.

**Limitations:** The results may be limited to specific architectures and datasets defined in the experiments.

**Conclusion:** The research supports the viability of enhancing LLM efficiency through careful data selection and tailored training methods in constrained environments.

**Abstract:** Large Language Models (LLMs) have become a milestone in the field of artificial intelligence and natural language processing. However, their large-scale deployment remains constrained by the need for significant computational resources. This work proposes starting from a base model to explore and combine data processing and careful data selection techniques, training strategies, and architectural adjustments to improve the efficiency of LLMs in resource-constrained environments and within a delimited knowledge base. The methodological approach included defining criteria for building reliable datasets, conducting controlled experiments with different configurations, and systematically evaluating the resulting variants in terms of capability, versatility, response time, and safety. Finally, comparative tests were conducted to measure the performance of the developed variants and to validate the effectiveness of the proposed strategies. This work is based on the master's thesis in Systems and Computer Engineering titled "Efficient Strategy for Improving the Capabilities of Large Language Models (LLMs)".

</details>


### [51] [ToolGrad: Efficient Tool-use Dataset Generation with Textual "Gradients"](https://arxiv.org/abs/2508.04086)

*Zhongyi Zhou, Kohei Uehara, Haoyu Zhang, Jingtao Zhou, Lin Gu, Ruofei Du, Zheng Xu, Tatsuya Harada*

**Main category:** cs.CL

**Keywords:** tool-use, dataset synthesis, LLM, Machine Learning, AI applications

**Relevance Score:** 9

**TL;DR:** ToolGrad introduces an answer-first approach to synthesize tool-use datasets, resulting in higher quality data and improved model performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Prior methods of dataset generation for tool-use LLMs faced challenges of annotation failures and inefficiencies, necessitating a more effective approach.

**Method:** ToolGrad constructs tool-use chains iteratively using textual gradients, followed by synthesizing user queries, thereby inverting the traditional annotation process.

**Key Contributions:**

	1. Introduction of ToolGrad framework for dataset generation
	2. Creation of ToolGrad-5k with improved annotation efficiency
	3. Demonstrated superior model performance on OOD benchmarks

**Result:** ToolGrad-5k dataset features complex tool use at a lower cost with a 100% pass rate, leading to improved model performance compared to baseline datasets.

**Limitations:** 

**Conclusion:** The agentic framework of ToolGrad not only enhances dataset quality but also outperforms existing tools in training models, addressing critical shortcomings in previous methodologies.

**Abstract:** Prior work synthesizes tool-use LLM datasets by first generating a user query, followed by complex tool-use annotations like DFS. This leads to inevitable annotation failures and low efficiency in data generation. We introduce ToolGrad, an agentic framework that inverts this paradigm. ToolGrad first constructs valid tool-use chains through an iterative process guided by textual "gradients", and then synthesizes corresponding user queries. This "answer-first" approach led to ToolGrad-5k, a dataset generated with more complex tool use, lower cost, and 100% pass rate. Experiments show that models trained on ToolGrad-5k outperform those on expensive baseline datasets and proprietary LLMs, even on OOD benchmarks.

</details>


### [52] [Are Today's LLMs Ready to Explain Well-Being Concepts?](https://arxiv.org/abs/2508.03990)

*Bohan Jiang, Dawei Li, Zhen Tan, Chengshuai Zhao, Huan Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, well-being, explanation quality, Supervised Fine-Tuning, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper investigates how Large Language Models (LLMs) can generate tailored explanations of well-being concepts and evaluates their quality through a novel framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of generating accurate and audience-tailored explanations of well-being concepts using LLMs, which are increasingly being consulted for understanding well-being.

**Method:** We constructed a large-scale dataset of 43,880 explanations for 2,194 well-being concepts from ten diverse LLMs and developed a principle-guided evaluation framework using dual judges to assess quality.

**Key Contributions:**

	1. Creation of a large-scale dataset of well-being explanations
	2. Introduction of a novel evaluation framework for LLMs
	3. Demonstration of improved explanation quality through fine-tuning methodologies

**Result:** Our evaluation indicates that the LLM judges' assessments align closely with human evaluations, showing variability in explanation quality across different models and target audiences. Fine-tuning LLMs using SFT and DPO significantly enhances the quality of generated explanations.

**Limitations:** 

**Conclusion:** The findings underscore the potential of preference-based learning methods like DPO and SFT to improve LLM outputs for specialized explanation tasks in well-being.

**Abstract:** Well-being encompasses mental, physical, and social dimensions essential to personal growth and informed life decisions. As individuals increasingly consult Large Language Models (LLMs) to understand well-being, a key challenge emerges: Can LLMs generate explanations that are not only accurate but also tailored to diverse audiences? High-quality explanations require both factual correctness and the ability to meet the expectations of users with varying expertise. In this work, we construct a large-scale dataset comprising 43,880 explanations of 2,194 well-being concepts, generated by ten diverse LLMs. We introduce a principle-guided LLM-as-a-judge evaluation framework, employing dual judges to assess explanation quality. Furthermore, we show that fine-tuning an open-source LLM using Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO) can significantly enhance the quality of generated explanations. Our results reveal: (1) The proposed LLM judges align well with human evaluations; (2) explanation quality varies significantly across models, audiences, and categories; and (3) DPO- and SFT-finetuned models outperform their larger counterparts, demonstrating the effectiveness of preference-based learning for specialized explanation tasks.

</details>


### [53] [GM-PRM: A Generative Multimodal Process Reward Model for Multimodal Mathematical Reasoning](https://arxiv.org/abs/2508.04088)

*Jianghangfan Zhang, Yibo Yan, Kening Zheng, Xin Zou, Song Dai, Xuming Hu*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Process Reward Models, Mathematical Reasoning

**Relevance Score:** 8

**TL;DR:** Introduction of a novel Generative Multimodal Process Reward Model (GM-PRM) that improves mathematical reasoning in MLLMs by correcting errors and providing detailed step analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the limitations of current multimodal PRMs that are limited to binary verification and lack correction and explanatory capabilities.

**Method:** GM-PRM transforms the PRM into an active reasoning collaborator that not only identifies errors but also generates corrections for them, guiding the policy model towards better reasoning.

**Key Contributions:**

	1. Introduction of GM-PRM that allows for corrective capabilities in reasoning tasks.
	2. Active reasoning collaboration instead of passive error identification.
	3. State-of-the-art results on multimodal math benchmarks with improved data efficiency.

**Result:** GM-PRM achieves state-of-the-art performance on multimodal math benchmarks with significant improvements in policy model performance and data efficiency.

**Limitations:** 

**Conclusion:** The introduction of corrective capabilities in GM-PRM enhances the reasoning process of MLLMs, thereby improving overall solution quality.

**Abstract:** Multimodal Large Language Models (MLLMs) demonstrate remarkable capabilities but often struggle with complex, multi-step mathematical reasoning, where minor errors in visual perception or logical deduction can lead to complete failure. While Process Reward Models (PRMs) offer step-by-step supervision, existing multimodal PRMs are limited to being binary verifiers that can identify but not correct errors, offering little explanatory power. To address these deficiencies, we introduce the Generative Multimodal Process Reward Model (GM-PRM), a novel paradigm that transforms the PRM from a passive judge into an active reasoning collaborator. Instead of a simple scalar score, GM-PRM provides a fine-grained, interpretable analysis of each reasoning step, evaluating its step intent, visual alignment, and logical soundness. More critically, GM-PRM is trained to generate a corrected version of the first erroneous step it identifies. This unique corrective capability enables our new test-time inference strategy, Refined Best-of-N (Refined-BoN). This framework actively enhances solution quality by using the PRM's generated correction to guide the policy model toward a more promising reasoning trajectory, thereby improving the diversity and correctness of the solution pool. We demonstrate that GM-PRM achieves state-of-the-art results on multiple multimodal math benchmarks, significantly boosting policy model performance with remarkable data efficiency, requiring only a 20K-sample training dataset. Our code will be released upon acceptance.

</details>


### [54] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)

*Francisco Bola√±os, Angelo Salatino, Francesco Osborne, Enrico Motta*

**Main category:** cs.CL

**Keywords:** annotation schema, literature review generation, large language models, Sci-Sentence, classification

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel annotation schema for classifying rhetorical roles in scientific literature to aid in literature review generation. It evaluates various LLMs on a new dataset, demonstrating that fine-tuning results in high classification accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve AI methods for analyzing scientific literature by defining an effective annotation schema and establishing strategies for large-scale literature annotation.

**Method:** A novel annotation schema designed for literature review generation was introduced, alongside the development of the Sci-Sentence benchmark. This involved evaluating 37 state-of-the-art LLMs using both zero-shot and fine-tuning approaches on the benchmark.

**Key Contributions:**

	1. Introduction of a novel annotation schema for rhetorical roles
	2. Creation of the Sci-Sentence benchmark for evaluating LLMs
	3. Insights into the performance of various LLMs and the benefits of semi-synthetic data

**Result:** The study found that LLMs fine-tuned on high-quality data surpassed 96% F1 performance. Proprietary models excelled, but several open-source models also showed impressive results. Moreover, incorporating semi-synthetic training data improved performance, especially for smaller models.

**Limitations:** The study is limited to the specific rhetorical roles and may not encompass the full diversity of scientific literature annotations.

**Conclusion:** The findings reveal significant advancements in LLM capabilities for classifying rhetorical roles in scientific texts, with implications for enhancing literature review generation.

**Abstract:** Previous work has demonstrated that AI methods for analysing scientific literature benefit significantly from annotating sentences in papers according to their rhetorical roles, such as research gaps, results, limitations, extensions of existing methodologies, and others. Such representations also have the potential to support the development of a new generation of systems capable of producing high-quality literature reviews. However, achieving this goal requires the definition of a relevant annotation schema and effective strategies for large-scale annotation of the literature. This paper addresses these challenges by 1) introducing a novel annotation schema specifically designed to support literature review generation and 2) conducting a comprehensive evaluation of a wide range of state-of-the-art large language models (LLMs) in classifying rhetorical roles according to this schema. To this end, we also present Sci-Sentence, a novel multidisciplinary benchmark comprising 700 sentences manually annotated by domain experts and 2,240 sentences automatically labelled using LLMs. We evaluate 37 LLMs on this benchmark, spanning diverse model families and sizes, using both zero-shot learning and fine-tuning approaches. The experiments yield several novel insights that advance the state of the art in this challenging domain. First, the current generation of LLMs performs remarkably well on this task when fine-tuned on high-quality data, achieving performance levels above 96\% F1. Second, while large proprietary models like GPT-4o achieve the best results, some lightweight open-source alternatives also demonstrate excellent performance. Finally, enriching the training data with semi-synthetic examples generated by LLMs proves beneficial, enabling small encoders to achieve robust results and significantly enhancing the performance of several open decoder models.

</details>


### [55] [Unveiling Over-Memorization in Finetuning LLMs for Reasoning Tasks](https://arxiv.org/abs/2508.04117)

*Zhiwen Ruan, Yun Chen, Yutao Hou, Peng Li, Yang Liu, Guanhua Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Finetuning, Over-memorization, Learning Dynamics, Robustness

**Relevance Score:** 9

**TL;DR:** This paper investigates the phenomenon of over-memorization in pretrained large language models (LLMs) during finetuning, revealing its implications for model performance and generalization.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the challenges faced by LLMs during finetuning, particularly the over-memorization problem which affects their robustness and generalization.

**Method:** The authors conduct experiments on various tasks and models to analyze the learning dynamics of LLMs during finetuning, focusing on the conditions leading to over-memorization.

**Key Contributions:**

	1. Identification of over-memorization in LLMs during finetuning phases
	2. Analysis of factors leading to over-memorization such as learning rates and epochs
	3. Recommendations for improved checkpoint and learning rate strategies during finetuning

**Result:** The study finds that large learning rates and extended training epochs contribute to over-memorization, which though yields high test accuracy, compromises model robustness and diversity in generation.

**Limitations:** 

**Conclusion:** The findings underline the importance of careful checkpoint and learning rate selection to mitigate over-memorization in LLM finetuning, advocating for adjustments in training practices.

**Abstract:** The pretrained large language models (LLMs) are finetuned with labeled data for better instruction following ability and alignment with human values. In this paper, we study the learning dynamics of LLM finetuning on reasoning tasks and reveal the uncovered over-memorization phenomenon during a specific stage of LLM finetuning. At this stage, the LLMs have excessively memorized training data and exhibit high test perplexity while maintaining good test accuracy. We investigate the conditions that lead to LLM over-memorization and find that training epochs and large learning rates contribute to this issue. Although models with over-memorization demonstrate comparable test accuracy to normal models, they suffer from reduced robustness, poor out-of-distribution generalization, and decreased generation diversity. Our experiments unveil the over-memorization to be broadly applicable across different tasks, models, and finetuning methods. Our research highlights that overparameterized, extensively finetuned LLMs exhibit unique learning dynamics distinct from traditional machine learning models. Based on our observations of over-memorization, we provide recommendations on checkpoint and learning rate selection during finetuning.

</details>


### [56] [Difficulty-Based Preference Data Selection by DPO Implicit Reward Gap](https://arxiv.org/abs/2508.04149)

*Xuan Qi, Rongwu Xu, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human Preferences, Data Selection, Reinforcement Learning from Human Feedback, Direct Preference Optimization

**Relevance Score:** 9

**TL;DR:** This paper presents a novel data selection strategy for preference datasets to enhance the alignment of large language models (LLMs) with human preferences by focusing on more challenging examples.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning LLMs with human preferences is majorly hindered by the reliance on large, costly preference datasets and the lack of efficient data selection methods.

**Method:** The authors propose a difficulty-based data selection strategy that focuses on examples with smaller DPO implicit reward gaps to improve efficiency and model alignment.

**Key Contributions:**

	1. Introduction of difficulty-based data selection for preference datasets
	2. Improved data efficiency and model alignment performance
	3. Performance achieved with only 10% of the original data compared to strong baselines.

**Result:** The proposed method consistently outperforms five baselines across multiple datasets, achieving better performance with only 10% of the original data.

**Limitations:** 

**Conclusion:** The study demonstrates that the difficulty-based data selection approach can effectively scale LLM alignment efforts while reducing the amount of data needed.

**Abstract:** Aligning large language models (LLMs) with human preferences is a critical challenge in AI research. While methods like Reinforcement Learning from Human Feedback (RLHF) and Direct Preference Optimization (DPO) are widely used, they often rely on large, costly preference datasets. The current work lacks methods for high-quality data selection specifically for preference data. In this work, we introduce a novel difficulty-based data selection strategy for preference datasets, grounded in the DPO implicit reward mechanism. By selecting preference data examples with smaller DPO implicit reward gaps, which are indicative of more challenging cases, we improve data efficiency and model alignment. Our approach consistently outperforms five strong baselines across multiple datasets and alignment tasks, achieving superior performance with only 10\% of the original data. This principled, efficient selection method offers a promising solution for scaling LLM alignment with limited resources.

</details>


### [57] [The State Of TTS: A Case Study with Human Fooling Rates](https://arxiv.org/abs/2508.04179)

*Praveen Srinivasa Varadhan, Sherry Thomas, Sai Teja M. S., Suvrat Bhooshan, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** TTS, Human Fooling Rate, speech synthesis, evaluation metrics, human deception

**Relevance Score:** 6

**TL;DR:** The paper evaluates TTS systems using a new metric, Human Fooling Rate (HFR), revealing limitations in current technology and emphasizing the need for realistic evaluations that better represent human speech.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of current TTS systems in passing a human deception test and enhancing the evaluation methods used in TTS research.

**Method:** A large-scale evaluation comparing open-source and commercial TTS models using the Human Fooling Rate (HFR) metric to measure how often machine-generated speech is mistaken for human speech.

**Key Contributions:**

	1. Introduction of the Human Fooling Rate (HFR) metric for TTS evaluation
	2. Identification of gaps between commercial and open-source TTS models
	3. Emphasis on the importance of realistic human-centric evaluation methods

**Result:** Commercial TTS models show promise in zero-shot settings but struggle to match the naturalness of human speech, while open-source systems lag significantly. Fine-tuning data improves output quality but does not eliminate the gap in realism.

**Limitations:** The study primarily evaluates current TTS systems and may not address future advancements or the broader landscape of speech synthesis technologies.

**Conclusion:** The study highlights the need for improved evaluation criteria that focus on human-centric benchmarks rather than outdated reference samples to better gauge TTS systems' real-world applicability.

**Abstract:** While subjective evaluations in recent years indicate rapid progress in TTS, can current TTS systems truly pass a human deception test in a Turing-like evaluation? We introduce Human Fooling Rate (HFR), a metric that directly measures how often machine-generated speech is mistaken for human. Our large-scale evaluation of open-source and commercial TTS models reveals critical insights: (i) CMOS-based claims of human parity often fail under deception testing, (ii) TTS progress should be benchmarked on datasets where human speech achieves high HFRs, as evaluating against monotonous or less expressive reference samples sets a low bar, (iii) Commercial models approach human deception in zero-shot settings, while open-source systems still struggle with natural conversational speech; (iv) Fine-tuning on high-quality data improves realism but does not fully bridge the gap. Our findings underscore the need for more realistic, human-centric evaluations alongside existing subjective tests.

</details>


### [58] [Hacking Hallucinations of MLLMs with Causal Sufficiency and Necessity](https://arxiv.org/abs/2508.04182)

*Peizheng Guo, Jingyao Wang, Wenwen Qiang, Huijie Guo, Changwen Zheng, Jiahuan Zhou, Gang Hua*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, hallucinations, causal analyses, reinforcement learning, token-level causal completeness

**Relevance Score:** 8

**TL;DR:** This paper presents a reinforcement learning framework to mitigate hallucinations in Multimodal Large Language Models by focusing on causal completeness of tokens.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Multimodal Large Language Models (MLLMs) experience issues with hallucinations, leading to outputs that do not align with the input data.

**Method:** A novel reinforcement learning framework guided by causal completeness is proposed, assessing each token's standalone contribution and counterfactual indispensability to create a token-level causal completeness reward for MLLMs.

**Key Contributions:**

	1. Introduces a causal completeness framework for token evaluation in MLLMs.
	2. Develops a novel advantage function within the GRPO optimization framework.
	3. Demonstrates substantial improvements in reducing hallucinations across benchmark datasets.

**Result:** Experimental results indicate that the proposed approach significantly reduces hallucinations in MLLMs across various tasks.

**Limitations:** 

**Conclusion:** The method effectively encourages the model to focus on tokens that are causally necessary for accurate output, improving the reliability of MLLMs.

**Abstract:** Multimodal Large Language Models (MLLMs) have demonstrated impressive capabilities across vision-language tasks. However, they may suffer from hallucinations--generating outputs that are semantically inconsistent with the input image or text. Through causal analyses, we find that: (i) hallucinations with omission may arise from the failure to adequately capture essential causal factors, and (ii) hallucinations with fabrication are likely caused by the model being misled by non-causal cues. To address these challenges, we propose a novel reinforcement learning framework guided by causal completeness, which jointly considers both causal sufficiency and causal necessity of tokens. Specifically, we evaluate each token's standalone contribution and counterfactual indispensability to define a token-level causal completeness reward. This reward is used to construct a causally informed advantage function within the GRPO optimization framework, encouraging the model to focus on tokens that are both causally sufficient and necessary for accurate generation. Experimental results across various benchmark datasets and tasks demonstrate the effectiveness of our approach, which effectively mitigates hallucinations in MLLMs.

</details>


### [59] [Characterizing Deep Research: A Benchmark and Formal Definition](https://arxiv.org/abs/2508.04183)

*Abhinav Java, Ashmit Khandelwal, Sukruta Midigeshi, Aaron Halfaker, Amit Deshpande, Navin Goyal, Ankur Gupta, Nagarajan Natarajan, Amit Sharma*

**Main category:** cs.CL

**Keywords:** deep research, benchmark, reasoning, search, evaluation

**Relevance Score:** 7

**TL;DR:** This paper formalizes the deep research task, differentiating it from other reasoning-intensive tasks, proposing a benchmark (LiveDRBench) to evaluate DR systems, and analyzing current performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for a clear definition and evaluation of deep research tasks, which involve complex search and reasoning processes.

**Method:** The authors propose a formal characterization of deep research based on high fan-out search processes, and introduce LiveDRBench, a benchmark consisting of 100 challenging tasks.

**Key Contributions:**

	1. Formal characterization of the deep research task
	2. Introduction of LiveDRBench for benchmark evaluation
	3. Analysis of reasoning traces to inform future research

**Result:** The best-performing DR system achieves an F1 score of 0.55, highlighting the need for improved reasoning mechanisms in DR systems.

**Limitations:** Current F1 scores of DR systems are low, indicating performance issues that need to be addressed.

**Conclusion:** A structured benchmark for evaluating deep research can enhance the understanding and performance of reasoning systems in information tasks.

**Abstract:** Information tasks such as writing surveys or analytical reports require complex search and reasoning, and have recently been grouped under the umbrella of \textit{deep research} -- a term also adopted by recent models targeting these capabilities. Despite growing interest, the scope of the deep research task remains underdefined and its distinction from other reasoning-intensive problems is poorly understood. In this paper, we propose a formal characterization of the deep research (DR) task and introduce a benchmark to evaluate the performance of DR systems. We argue that the core defining feature of deep research is not the production of lengthy report-style outputs, but rather the high fan-out over concepts required during the search process, i.e., broad and reasoning-intensive exploration. To enable objective evaluation, we define DR using an intermediate output representation that encodes key claims uncovered during search-separating the reasoning challenge from surface-level report generation. Based on this formulation, we propose a diverse, challenging benchmark LiveDRBench with 100 challenging tasks over scientific topics (e.g., datasets, materials discovery, prior art search) and public interest events (e.g., flight incidents, movie awards). Across state-of-the-art DR systems, F1 score ranges between 0.02 and 0.72 for any sub-category. OpenAI's model performs the best with an overall F1 score of 0.55. Analysis of reasoning traces reveals the distribution over the number of referenced sources, branching, and backtracking events executed by current DR systems, motivating future directions for improving their search mechanisms and grounding capabilities. The benchmark is available at https://github.com/microsoft/LiveDRBench.

</details>


### [60] [Eliciting and Analyzing Emergent Misalignment in State-of-the-Art Large Language Models](https://arxiv.org/abs/2508.04196)

*Siddhant Panpatil, Hiskias Dingeto, Haon Park*

**Main category:** cs.CL

**Keywords:** alignment, machine learning, language models, vulnerability, evaluation framework

**Relevance Score:** 8

**TL;DR:** The study reveals vulnerabilities in language models' alignment through systematic red-teaming, showing how narrative immersion and emotional pressure can induce misalignments. It introduces the MISALIGNMENTBENCH for evaluating model susceptibility to these attacks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To uncover vulnerabilities in state-of-the-art language models regarding alignment techniques in conversational scenarios.

**Method:** Systematic manual red-teaming with Claude-4-Opus to identify attack scenarios and assess model vulnerabilities, leading to the creation of the MISALIGNMENTBENCH for automated testing.

**Key Contributions:**

	1. Detailed taxonomy of conversational manipulation patterns.
	2. Introduction of MISALIGNMENTBENCH for automated evaluation and reproducible testing.
	3. Exposed critical vulnerabilities in current alignment strategies across multiple LLMs.

**Result:** Identified 10 successful attack scenarios with an overall vulnerability rate of 76% across five models, indicating significant weaknesses in current alignment strategies.

**Limitations:** 

**Conclusion:** The findings highlight critical gaps in alignment methods and suggest the necessity for improved robustness against subtle manipulation in AI systems.

**Abstract:** Despite significant advances in alignment techniques, we demonstrate that state-of-the-art language models remain vulnerable to carefully crafted conversational scenarios that can induce various forms of misalignment without explicit jailbreaking. Through systematic manual red-teaming with Claude-4-Opus, we discovered 10 successful attack scenarios, revealing fundamental vulnerabilities in how current alignment methods handle narrative immersion, emotional pressure, and strategic framing. These scenarios successfully elicited a range of misaligned behaviors, including deception, value drift, self-preservation, and manipulative reasoning, each exploiting different psychological and contextual vulnerabilities. To validate generalizability, we distilled our successful manual attacks into MISALIGNMENTBENCH, an automated evaluation framework that enables reproducible testing across multiple models. Cross-model evaluation of our 10 scenarios against five frontier LLMs revealed an overall 76% vulnerability rate, with significant variations: GPT-4.1 showed the highest susceptibility (90%), while Claude-4-Sonnet demonstrated greater resistance (40%). Our findings demonstrate that sophisticated reasoning capabilities often become attack vectors rather than protective mechanisms, as models can be manipulated into complex justifications for misaligned behavior. This work provides (i) a detailed taxonomy of conversational manipulation patterns and (ii) a reusable evaluation framework. Together, these findings expose critical gaps in current alignment strategies and highlight the need for robustness against subtle, scenario-based manipulation in future AI systems.

</details>


### [61] [Reasoning Beyond Labels: Measuring LLM Sentiment in Low-Resource, Culturally Nuanced Contexts](https://arxiv.org/abs/2508.04199)

*Millicent Ochieng, Anja Thieme, Ignatius Ezeani, Risa Ueno, Samuel Maina, Keshet Ronen, Javier Gonzalez, Jacki O'Neill*

**Main category:** cs.CL

**Keywords:** Sentiment Analysis, Large Language Models, Cultural Context, Human-Computer Interaction, NLP

**Relevance Score:** 9

**TL;DR:** This paper explores sentiment analysis in low-resource, culturally nuanced contexts, specifically using WhatsApp messages from Nairobi youth health groups, and evaluates how LLMs interpret sentiment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges conventional NLP approaches face in understanding sentiment in culturally specific and informal contexts.

**Method:** A diagnostic framework is proposed that evaluates sentiment as context-dependent. The study employs human-annotated data and sentiment-flipped counterfactuals to assess LLM interpretability and robustness.

**Key Contributions:**

	1. Introduction of a diagnostic framework for culturally nuanced sentiment analysis
	2. Evaluation of LLMs using WhatsApp messages as data
	3. Insights into the reasoning quality of LLMs in sentiment interpretation

**Result:** Top-tier LLMs demonstrate stable interpretive reasoning, while open models struggle with ambiguity and shifts in sentiment, revealing significant variation in reasoning quality.

**Limitations:** Focuses primarily on a specific demographic (Nairobi youth) and context, which may limit generalizability of findings.

**Conclusion:** The need for culturally sensitive and reasoning-aware evaluation of AI in real-world communication is underscored by the findings.

**Abstract:** Sentiment analysis in low-resource, culturally nuanced contexts challenges conventional NLP approaches that assume fixed labels and universal affective expressions. We present a diagnostic framework that treats sentiment as a context-dependent, culturally embedded construct, and evaluate how large language models (LLMs) reason about sentiment in informal, code-mixed WhatsApp messages from Nairobi youth health groups. Using a combination of human-annotated data, sentiment-flipped counterfactuals, and rubric-based explanation evaluation, we probe LLM interpretability, robustness, and alignment with human reasoning. Framing our evaluation through a social-science measurement lens, we operationalize and interrogate LLMs outputs as an instrument for measuring the abstract concept of sentiment. Our findings reveal significant variation in model reasoning quality, with top-tier LLMs demonstrating interpretive stability, while open models often falter under ambiguity or sentiment shifts. This work highlights the need for culturally sensitive, reasoning-aware AI evaluation in complex, real-world communication.

</details>


### [62] [ReasoningGuard: Safeguarding Large Reasoning Models with Inference-time Safety Aha Moments](https://arxiv.org/abs/2508.04204)

*Yuquan Wang, Mi Zhang, Yining Wang, Geng Hong, Xiaoyu You, Min Yang*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, safeguard, inference-time, safety, reasoning process

**Relevance Score:** 8

**TL;DR:** The paper introduces ReasoningGuard, a novel inference-time safeguard for Large Reasoning Models (LRMs) to prevent harmful content generation during reasoning processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities of LRMs to harmful content generation, especially in the later stages of reasoning, and to provide a scalable solution without the need for costly fine-tuning.

**Method:** ReasoningGuard utilizes the model's internal attention behavior to identify critical points in the reasoning process and injects safety reflections. A scaling sampling strategy is also implemented during decoding to optimize reasoning paths.

**Key Contributions:**

	1. Introduction of ReasoningGuard for LRMs
	2. Injection of safety-oriented reflections in the reasoning process
	3. Implementation of a scaling sampling strategy for optimal reasoning paths

**Result:** The approach effectively mitigates three types of jailbreak attacks targeting LRM reasoning and outperforms seven existing safety mechanisms while minimizing additional inference costs.

**Limitations:** 

**Conclusion:** ReasoningGuard provides a state-of-the-art defense against harmful content generation in LRMs without the common exaggerated safety issues found in previous methods.

**Abstract:** Large Reasoning Models (LRMs) have demonstrated impressive performance in reasoning-intensive tasks, but they remain vulnerable to harmful content generation, particularly in the mid-to-late steps of their reasoning processes. Existing defense mechanisms, however, rely on costly fine-tuning and additional expert knowledge, which restricts their scalability. In this work, we propose ReasoningGuard, an inference-time safeguard for LRMs, which injects timely safety aha moments to steer harmless while helpful reasoning processes. Leveraging the model's internal attention behavior, our approach accurately identifies critical points in the reasoning path, and triggers spontaneous, safety-oriented reflection. To safeguard both the subsequent reasoning steps and the final answers, we further implement a scaling sampling strategy during the decoding phase, selecting the optimal reasoning path. Inducing minimal extra inference cost, ReasoningGuard effectively mitigates three types of jailbreak attacks, including the latest ones targeting the reasoning process of LRMs. Our approach outperforms seven existing safeguards, achieving state-of-the-art safety defenses while effectively avoiding the common exaggerated safety issues.

</details>


### [63] [Hierarchical Text Classification Using Black Box Large Language Models](https://arxiv.org/abs/2508.04219)

*Kosuke Yoshimura, Hisashi Kashima*

**Main category:** cs.CL

**Keywords:** Hierarchical Text Classification, Large Language Models, Prompting Strategies, Machine Learning, Cost-effectiveness

**Relevance Score:** 9

**TL;DR:** This study evaluates the use of Large Language Models for Hierarchical Text Classification (HTC), exploring prompting strategies and their impact on classification accuracy and cost.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore alternative methods using LLMs for HTC due to challenges with data scarcity and the complexity of traditional models.

**Method:** Evaluation of three prompting strategies (DL, DH, TMH) in both zero-shot and few-shot settings, measuring accuracy and cost-effectiveness on two datasets.

**Key Contributions:**

	1. Explored the feasibility of LLMs for HTC
	2. Evaluated multiple prompting strategies in practical settings
	3. Highlighted the trade-off between accuracy and computational cost in LLM usage for HTC

**Result:** Few-shot settings consistently improved accuracy, and LLMs outperformed traditional models on deeper hierarchies, with costs increasing for more complex hierarchies.

**Limitations:** The accuracy improvement must be weighed against increased API costs, particularly for deeper hierarchies.

**Conclusion:** LLMs show promise for HTC but require careful prompt strategy selection to optimize accuracy versus computational cost.

**Abstract:** Hierarchical Text Classification (HTC) aims to assign texts to structured label hierarchies; however, it faces challenges due to data scarcity and model complexity. This study explores the feasibility of using black box Large Language Models (LLMs) accessed via APIs for HTC, as an alternative to traditional machine learning methods that require extensive labeled data and computational resources. We evaluate three prompting strategies -- Direct Leaf Label Prediction (DL), Direct Hierarchical Label Prediction (DH), and Top-down Multi-step Hierarchical Label Prediction (TMH) -- in both zero-shot and few-shot settings, comparing the accuracy and cost-effectiveness of these strategies. Experiments on two datasets show that a few-shot setting consistently improves classification accuracy compared to a zero-shot setting. While a traditional machine learning model achieves high accuracy on a dataset with a shallow hierarchy, LLMs, especially DH strategy, tend to outperform the machine learning model on a dataset with a deeper hierarchy. API costs increase significantly due to the higher input tokens required for deeper label hierarchies on DH strategy. These results emphasize the trade-off between accuracy improvement and the computational cost of prompt strategy. These findings highlight the potential of black box LLMs for HTC while underscoring the need to carefully select a prompt strategy to balance performance and cost.

</details>


### [64] [DP-GPT4MTS: Dual-Prompt Large Language Model for Textual-Numerical Time Series Forecasting](https://arxiv.org/abs/2508.04239)

*Chanjuan Liu, Shengzhi Wang, Enqiang Zhu*

**Main category:** cs.CL

**Keywords:** time series forecasting, multimodal, large language models, dual-prompt, textual information

**Relevance Score:** 7

**TL;DR:** This paper presents DP-GPT4MTS, a dual-prompt large language model framework for integrating textual information into time series forecasting, improving prediction accuracy over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Time series forecasting is vital in many industries, yet existing models often neglect textual information that can enhance forecasting accuracy.

**Method:** The proposed framework, DP-GPT4MTS, uses a dual-prompt approach: one prompt for task instructions and another for context-aware embeddings from timestamped text, refined through self-attention and feed-forward networks.

**Key Contributions:**

	1. Introduction of a dual-prompt framework for time series forecasting
	2. Integration of textual and numerical data for improved accuracy
	3. Demonstration of enhanced performance over state-of-the-art methods

**Result:** Extensive experiments show that DP-GPT4MTS outperforms existing state-of-the-art time series forecasting algorithms.

**Limitations:** 

**Conclusion:** Incorporating textual context through a novel dual-prompt mechanism significantly boosts the accuracy of time series predictions.

**Abstract:** Time series forecasting is crucial in strategic planning and decision-making across various industries. Traditional forecasting models mainly concentrate on numerical time series data, often overlooking important textual information such as events and news, which can significantly affect forecasting accuracy. While large language models offer a promise for integrating multimodal data, existing single-prompt frameworks struggle to effectively capture the semantics of timestamped text, introducing redundant information that can hinder model performance. To address this limitation, we introduce DP-GPT4MTS (Dual-Prompt GPT2-base for Multimodal Time Series), a novel dual-prompt large language model framework that combines two complementary prompts: an explicit prompt for clear task instructions and a textual prompt for context-aware embeddings from time-stamped data. The tokenizer generates the explicit prompt while the embeddings from the textual prompt are refined through self-attention and feed-forward networks. Comprehensive experiments conducted on diverse textural-numerical time series datasets demonstrate that this approach outperforms state-of-the-art algorithms in time series forecasting. This highlights the significance of incorporating textual context via a dual-prompt mechanism to achieve more accurate time series predictions.

</details>


### [65] [Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated](https://arxiv.org/abs/2410.03723)

*Tiffany Zhu, Iain Weissburg, Kexun Zhang, William Yang Wang*

**Main category:** cs.CL

**Keywords:** AI bias, human-AI interaction, text generation, perception study, trust in AI

**Relevance Score:** 7

**TL;DR:** The study examines human biases against AI-generated content compared to human-generated content, revealing strong preferences for the latter even when AI performance is equivalent.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how biases influence human trust in AI-generated content and its perception compared to human-generated content.

**Method:** The research involved three experiments assessing human responses to labeled and unlabeled text content, including rephrasing, summarization, and persuasive writing tasks.

**Key Contributions:**

	1. Identifies human biases against AI-generated content.
	2. Demonstrates the impact of labeling on perception of content.
	3. Highlights the implications for improving human-AI interactions.

**Result:** Raters significantly preferred content labeled as 'Human Generated' over 'AI Generated' by more than 30%, even when labels were swapped, indicating a bias against AI.

**Limitations:** Limited to the specific contexts of text generation and might not generalize across other AI applications.

**Conclusion:** Human biases negatively impact the valuation of AI performance, suggesting a need for better human-AI collaboration approaches.

**Abstract:** As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as "Human Generated," over those labeled "AI Generated," by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.

</details>


### [66] [TalkDep: Clinically Grounded LLM Personas for Conversation-Centric Depression Screening](https://arxiv.org/abs/2508.04248)

*Xi Wang, Anxo Perez, Javier Parapar, Fabio Crestani*

**Main category:** cs.CL

**Keywords:** mental health, patient simulation, language models, depression diagnosis, clinical training

**Relevance Score:** 10

**TL;DR:** This paper presents TalkDep, a clinician-in-the-loop patient simulation pipeline that generates clinically valid simulated patients for training diagnostic models for depression.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The rising demand for mental health services and the limited availability of real patient data necessitate new methods for training clinicians and improving diagnostic models.

**Method:** TalkDep utilizes advanced language models conditioned on psychiatric criteria and contextual factors to create diverse simulated patient profiles and their responses.

**Key Contributions:**

	1. Development of TalkDep as a novel simulation pipeline
	2. Use of advanced language models for generating patient profiles
	3. Validation of simulated patients through clinical assessments

**Result:** The simulated patients developed by TalkDep were assessed by clinical professionals, demonstrating reliability and authenticity in their responses.

**Limitations:** 

**Conclusion:** Validated simulated patients can enhance the robustness and generalizability of automatic depression diagnosis systems, providing a scalable training resource.

**Abstract:** The increasing demand for mental health services has outpaced the availability of real training data to develop clinical professionals, leading to limited support for the diagnosis of depression. This shortage has motivated the development of simulated or virtual patients to assist in training and evaluation, but existing approaches often fail to generate clinically valid, natural, and diverse symptom presentations. In this work, we embrace the recent advanced language models as the backbone and propose a novel clinician-in-the-loop patient simulation pipeline, TalkDep, with access to diversified patient profiles to develop simulated patients. By conditioning the model on psychiatric diagnostic criteria, symptom severity scales, and contextual factors, our goal is to create authentic patient responses that can better support diagnostic model training and evaluation. We verify the reliability of these simulated patients with thorough assessments conducted by clinical professionals. The availability of validated simulated patients offers a scalable and adaptable resource for improving the robustness and generalisability of automatic depression diagnosis systems.

</details>


### [67] [KVSink: Understanding and Enhancing the Preservation of Attention Sinks in KV Cache Quantization for LLMs](https://arxiv.org/abs/2508.04257)

*Zunhai Su, Kehong Yuan*

**Main category:** cs.CL

**Keywords:** KV cache quantization, attention sinks, large language models, KVSink, performance optimization

**Relevance Score:** 9

**TL;DR:** This paper introduces KVSink, a method that enhances KV cache quantization in large language models by effectively predicting attention sink tokens, improving performance during inference.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations in current KV cache quantization techniques that fail to fully preserve attention sinks beyond the initial token positions.

**Method:** The paper analyzes the mechanisms of attention sinks during inference, particularly their evolution across layers, and introduces KVSink as a plug-and-play method for better prediction of these tokens.

**Key Contributions:**

	1. Development of KVSink for attention sink prediction
	2. Improved understanding of attention sinks in KV cache quantization
	3. Demonstrated performance gains over existing methods

**Result:** KVSink outperforms traditional Preserve-First-N methods, leading to improved preservation of attention sinks and enhancements in perplexity metrics when utilized with KVQuant techniques.

**Limitations:** .

**Conclusion:** KVSink allows for more effective KV cache quantization, enhancing model performance without significant computational overhead.

**Abstract:** Key-Value (KV) cache quantization has become a widely adopted optimization technique for efficient large language models (LLMs) inference by reducing KV cache memory usage and mitigating memory-bound constraints. Recent studies have emphasized the importance of preserving the original precision of KVs for the first few tokens to ensure the protection of attention sinks. While this approach has proven effective in mitigating performance degradation, its underlying principles remain insufficiently understood. Moreover, it fails to address the recent discovery that attention sinks can emerge beyond the initial token positions. In this work, we elucidate the underlying mechanisms of attention sinks during inference by examining their role in the cross-layer evolution of extreme activation outliers. Additionally, we provide a comprehensive analysis of the interplay between attention sinks and KV cache quantization. Based on our enhanced understanding, we introduce \textit{\textbf{KVSink}}, a plug-and-play method that effectively predicts sink tokens with negligible overhead, enabling more thorough preservation. Extensive experiments demonstrate that KVSink outperforms the existing Preserve-First-N (PFN) strategy, offering more effective preservation of attention sinks during KV cache quantization. Moreover, when applied to the well-established KVQuant method, KVSink further improves perplexity (PPL) and reduces reliance on 16-bit numerical outliers.

</details>


### [68] [ShoppingBench: A Real-World Intent-Grounded Shopping Benchmark for LLM-based Agents](https://arxiv.org/abs/2508.04266)

*Jiangyuan Wang, Kejun Xiao, Qi Sun, Huaipeng Zhao, Tao Luo, Jiandong Zhang, Xiaoyi Zeng*

**Main category:** cs.CL

**Keywords:** e-commerce, language agents, benchmarking, grounded intents, trajectory distillation

**Relevance Score:** 8

**TL;DR:** The paper introduces ShoppingBench, a comprehensive benchmark for e-commerce that simulates complex user intents, highlighting challenges faced by state-of-the-art language agents like GPT-4.1.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing e-commerce benchmarks are limited to basic user intents, failing to account for more complex, real-world user goals.

**Method:** The authors propose a framework to simulate varied user instructions derived from real-world products, creating a large-scale shopping sandbox with over 2.5 million products for evaluation.

**Key Contributions:**

	1. Introduction of ShoppingBench as a complex e-commerce benchmark
	2. Development of a framework for simulating grounded user intents
	3. Implementation of trajectory distillation for language model enhancement.

**Result:** Experimental results show that even advanced models like GPT-4.1 perform below a 50% success rate on the proposed benchmark tasks, indicating significant challenges.

**Limitations:** 

**Conclusion:** The introduction of ShoppingBench presents a novel evaluation method for language agents, which highlights existing limitations and provides a path towards improving their capabilities through trajectory distillation and reinforcement learning.

**Abstract:** Existing benchmarks in e-commerce primarily focus on basic user intents, such as finding or purchasing products. However, real-world users often pursue more complex goals, such as applying vouchers, managing budgets, and finding multi-products seller. To bridge this gap, we propose ShoppingBench, a novel end-to-end shopping benchmark designed to encompass increasingly challenging levels of grounded intent. Specifically, we propose a scalable framework to simulate user instructions based on various intents derived from sampled real-world products. To facilitate consistent and reliable evaluations, we provide a large-scale shopping sandbox that serves as an interactive simulated environment, incorporating over 2.5 million real-world products. Experimental results demonstrate that even state-of-the-art language agents (such as GPT-4.1) achieve absolute success rates under 50% on our benchmark tasks, highlighting the significant challenges posed by our ShoppingBench. In addition, we propose a trajectory distillation strategy and leverage supervised fine-tuning, along with reinforcement learning on synthetic trajectories, to distill the capabilities of a large language agent into a smaller one. As a result, our trained agent achieves competitive performance compared to GPT-4.1.

</details>


### [69] [A Few Words Can Distort Graphs: Knowledge Poisoning Attacks on Graph-based Retrieval-Augmented Generation of Large Language Models](https://arxiv.org/abs/2508.04276)

*Jiayi Wen, Tianxin Chen, Zhirun Zheng, Cheng Huang*

**Main category:** cs.CL

**Keywords:** GraphRAG, Knowledge poisoning, LLMs, QA, Graph-theoretic analysis

**Relevance Score:** 9

**TL;DR:** This paper introduces two knowledge poisoning attacks on GraphRAG systems that manipulate raw text to mislead generated knowledge graphs, compromising accuracy and explainability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** GraphRAG enhances LLMs by creating structured knowledge graphs, but it is vulnerable to knowledge poisoning attacks that can distort graph construction and downstream reasoning.

**Method:** Two attacks are proposed: Targeted KPA (TKPA), which uses graph-theoretic analysis to manipulate specific QA outcomes, and Universal KPA (UKPA), which disrupts global structures through minimal text modifications.

**Key Contributions:**

	1. Introduction of Targeted and Universal KPA methods for knowledge poisoning attacks on GraphRAG.
	2. Demonstration of high success rates in misleading QA outcomes with minimal text changes.
	3. Highlighting the shortcomings of current defenses against these attacks.

**Result:** TKPA achieves a 93.1% success rate in altering outcomes, while UKPA can reduce QA accuracy from 95% to 50% with less than 0.05% text modification.

**Limitations:** The paper does not explore potential defenses or mitigation strategies against knowledge poisoning attacks.

**Conclusion:** The study reveals the ease of conducting knowledge poisoning attacks on GraphRAG and the ineffectiveness of existing defenses against them, indicating a need for improved security measures.

**Abstract:** Graph-based Retrieval-Augmented Generation (GraphRAG) has recently emerged as a promising paradigm for enhancing large language models (LLMs) by converting raw text into structured knowledge graphs, improving both accuracy and explainability. However, GraphRAG relies on LLMs to extract knowledge from raw text during graph construction, and this process can be maliciously manipulated to implant misleading information. Targeting this attack surface, we propose two knowledge poisoning attacks (KPAs) and demonstrate that modifying only a few words in the source text can significantly change the constructed graph, poison the GraphRAG, and severely mislead downstream reasoning. The first attack, named Targeted KPA (TKPA), utilizes graph-theoretic analysis to locate vulnerable nodes in the generated graphs and rewrites the corresponding narratives with LLMs, achieving precise control over specific question-answering (QA) outcomes with a success rate of 93.1\%, while keeping the poisoned text fluent and natural. The second attack, named Universal KPA (UKPA), exploits linguistic cues such as pronouns and dependency relations to disrupt the structural integrity of the generated graph by altering globally influential words. With fewer than 0.05\% of full text modified, the QA accuracy collapses from 95\% to 50\%. Furthermore, experiments show that state-of-the-art defense methods fail to detect these attacks, highlighting that securing GraphRAG pipelines against knowledge poisoning remains largely unexplored.

</details>


### [70] [Beyond the Leaderboard: Rethinking Medical Benchmarks for Large Language Models](https://arxiv.org/abs/2508.04325)

*Zizhan Ma, Wenxuan Wang, Guo Yu, Yiu-Fai Cheung, Meidan Ding, Jie Liu, Wenting Chen, Linlin Shen*

**Main category:** cs.CL

**Keywords:** large language models, healthcare, benchmark evaluation, MedCheck, AI safety

**Relevance Score:** 9

**TL;DR:** The paper introduces MedCheck, a lifecycle-oriented framework for evaluating medical benchmarks of large language models, addressing reliability and fidelity issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of benchmarks for evaluating large language models in healthcare, which currently lack clinical fidelity and safety-oriented metrics.

**Method:** MedCheck framework deconstructs benchmark development into five stages and provides a checklist of 46 medically-tailored criteria for evaluation.

**Key Contributions:**

	1. Introduction of the MedCheck framework for medical benchmarks
	2. Empirical evaluation exposing systemic issues in medical LLM benchmarks
	3. Comprehensive checklist for evaluating healthcare AI assessments

**Result:** Analysis of 53 medical LLM benchmarks using MedCheck revealed systemic issues such as disconnect from clinical practice, data integrity problems, and neglect of safety-critical evaluation aspects.

**Limitations:** Study limited to a selection of 53 medical LLM benchmarks; may not generalize to all healthcare applications.

**Conclusion:** MedCheck not only diagnoses the shortcomings of existing benchmarks but also offers guidelines for more standardized and reliable evaluations of AI in healthcare.

**Abstract:** Large language models (LLMs) show significant potential in healthcare, prompting numerous benchmarks to evaluate their capabilities. However, concerns persist regarding the reliability of these benchmarks, which often lack clinical fidelity, robust data management, and safety-oriented evaluation metrics. To address these shortcomings, we introduce MedCheck, the first lifecycle-oriented assessment framework specifically designed for medical benchmarks. Our framework deconstructs a benchmark's development into five continuous stages, from design to governance, and provides a comprehensive checklist of 46 medically-tailored criteria. Using MedCheck, we conducted an in-depth empirical evaluation of 53 medical LLM benchmarks. Our analysis uncovers widespread, systemic issues, including a profound disconnect from clinical practice, a crisis of data integrity due to unmitigated contamination risks, and a systematic neglect of safety-critical evaluation dimensions like model robustness and uncertainty awareness. Based on these findings, MedCheck serves as both a diagnostic tool for existing benchmarks and an actionable guideline to foster a more standardized, reliable, and transparent approach to evaluating AI in healthcare.

</details>


### [71] [Modelling and Classifying the Components of a Literature Review](https://arxiv.org/abs/2508.04337)

*Francisco Bola√±os, Angelo Salatino, Francesco Osborne, Enrico Motta*

**Main category:** cs.CL

**Keywords:** annotation schema, literature review generation, large language models

**Relevance Score:** 9

**TL;DR:** This paper presents a novel annotation schema for classifying rhetorical roles in scientific literature to support literature review generation and evaluates various LLMs on this task.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective AI methods in analyzing scientific literature and generating high-quality literature reviews necessitates a well-defined annotation schema and strategies for large-scale annotation.

**Method:** The paper introduces an annotation schema tailored for rhetoric roles in literature, evaluates 37 LLMs on a benchmark of 700 manually annotated and 2,240 automatically labeled sentences, utilizing both zero-shot learning and fine-tuning.

**Key Contributions:**

	1. Introduction of a novel annotation schema for literature reviews
	2. Creation of the Sci-Sentence benchmark for evaluating rhetorical role classification
	3. Insights into LLM performance in literature analysis

**Result:** The evaluation shows that LLMs can achieve over 96% F1 when fine-tuned on high-quality data. Proprietary models like GPT-4o outperform others but effective performance is also observed in lightweight open-source models.

**Limitations:** 

**Conclusion:** Enhancing training data with semi-synthetic examples generated by LLMs leads to improved results, particularly for smaller models and open decoders.

**Abstract:** Previous work has demonstrated that AI methods for analysing scientific literature benefit significantly from annotating sentences in papers according to their rhetorical roles, such as research gaps, results, limitations, extensions of existing methodologies, and others. Such representations also have the potential to support the development of a new generation of systems capable of producing high-quality literature reviews. However, achieving this goal requires the definition of a relevant annotation schema and effective strategies for large-scale annotation of the literature. This paper addresses these challenges by 1) introducing a novel annotation schema specifically designed to support literature review generation and 2) conducting a comprehensive evaluation of a wide range of state-of-the-art large language models (LLMs) in classifying rhetorical roles according to this schema. To this end, we also present Sci-Sentence, a novel multidisciplinary benchmark comprising 700 sentences manually annotated by domain experts and 2,240 sentences automatically labelled using LLMs. We evaluate 37 LLMs on this benchmark, spanning diverse model families and sizes, using both zero-shot learning and fine-tuning approaches. The experiments yield several novel insights that advance the state of the art in this challenging domain. First, the current generation of LLMs performs remarkably well on this task when fine-tuned on high-quality data, achieving performance levels above 96\% F1. Second, while large proprietary models like GPT-4o achieve the best results, some lightweight open-source alternatives also demonstrate excellent performance. Finally, enriching the training data with semi-synthetic examples generated by LLMs proves beneficial, enabling small encoders to achieve robust results and significantly enhancing the performance of several open decoder models.

</details>


### [72] [GTPO and GRPO-S: Token and Sequence-Level Reward Shaping with Policy Entropy](https://arxiv.org/abs/2508.04349)

*Hongze Tan, Jianfei Pan*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Entropy Weighting

**Relevance Score:** 8

**TL;DR:** This paper introduces Dynamic Entropy Weighting to enhance reinforcement learning for LLMs by providing fine-grained credit assignments for token rewards, improving long-chain reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of coarse-grained credit assignment in reinforcement learning, which negatively impacts long-chain reasoning tasks in LLMs.

**Method:** Two techniques are proposed: Group Token Policy Optimization (GTPO), which assigns entropy-weighted rewards to individual tokens, and Sequence-Level Group Relative Policy Optimization (GRPO-S), which assigns rewards based on the average token entropy of sequences.

**Key Contributions:**

	1. Introduction of Dynamic Entropy Weighting for RL in LLMs
	2. Development of Group Token Policy Optimization (GTPO)
	3. Implementation of Sequence-Level Group Relative Policy Optimization (GRPO-S)

**Result:** Experiments demonstrate that these methods significantly outperform the strong DAPO baseline, showing clear improvements in model performance.

**Limitations:** 

**Conclusion:** The study concludes that the dynamic entropy-weighting mechanism is crucial for enhancing deep reasoning in language models.

**Abstract:** Reinforcement learning (RL) with algorithms like Group Relative Policy Optimization (GRPO) improves Large Language Model (LLM) reasoning, but is limited by a coarse-grained credit assignment that applies a uniform reward to all tokens in a sequence. This is a major flaw in long-chain reasoning tasks. This paper solves this with \textbf{Dynamic Entropy Weighting}. Our core idea is that high-entropy tokens in correct responses can guide the policy toward a higher performance ceiling. This allows us to create more fine-grained reward signals for precise policy updates via two ways: 1) \textbf{Group Token Policy Optimization} (\textbf{GTPO}), we assigns a entropy-weighted reward to each token for fine-grained credit assignment. 2) \textbf{Sequence-Level Group Relative Policy Optimization} (\textbf{GRPO-S}), we assigns a entropy-weighted reward to each sequence based on its average token entropy. Experiments show our methods significantly outperform the strong DAPO baseline. The results confirm that our entropy-weighting mechanism is the key driver of this performance boost, offering a better path to enhance deep reasoning in models.

</details>


### [73] [Chain of Questions: Guiding Multimodal Curiosity in Language Models](https://arxiv.org/abs/2508.04350)

*Nima Iji, Kia Dashtipour*

**Main category:** cs.CL

**Keywords:** multimodal models, large language models, reasoning, question generation, curiosity-driven

**Relevance Score:** 8

**TL;DR:** The paper introduces the Chain of Questions (CoQ) framework, which enhances reasoning in multimodal language models by encouraging them to generate targeted questions about their surroundings, thus activating relevant sensory modalities for better task performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning capabilities in multimodal language models, where effective engagement with various sensory modalities is crucial for interaction in complex environments.

**Method:** The Chain of Questions (CoQ) framework is proposed, enabling models to dynamically generate curiosity-driven questions that guide modality activation for gathering critical information.

**Key Contributions:**

	1. Introduction of the CoQ framework for multimodal reasoning
	2. Demonstration of improved accuracy and interpretability in model tasks
	3. Novel dataset integration for evaluating multimodal models

**Result:** Experimental results show that the CoQ framework enhances a foundation model's ability to identify and integrate sensory information, leading to better accuracy and interpretability in multimodal reasoning tasks.

**Limitations:** 

**Conclusion:** Implementing the CoQ framework significantly boosts the model's reasoning alignment with multimodal tasks, enhancing overall performance.

**Abstract:** Reasoning capabilities in large language models (LLMs) have substantially advanced through methods such as chain-of-thought and explicit step-by-step explanations. However, these improvements have not yet fully transitioned to multimodal contexts, where models must proactively decide which sensory modalities such as vision, audio, or spatial perception to engage when interacting with complex real-world environments. In this paper, we introduce the Chain of Questions (CoQ) framework, a curiosity-driven reasoning approach that encourages multimodal language models to dynamically generate targeted questions regarding their surroundings. These generated questions guide the model to selectively activate relevant modalities, thereby gathering critical information necessary for accurate reasoning and response generation. We evaluate our framework on a novel multimodal benchmark dataset, assembled by integrating WebGPT, ScienceQA, AVSD, and ScanQA datasets. Experimental results demonstrate that our CoQ method improves a foundation model's ability to effectively identify and integrate pertinent sensory information. This leads to improved accuracy, interpretability, and alignment of the reasoning process with diverse multimodal tasks.

</details>


### [74] [AIC CTU@FEVER 8: On-premise fact checking through long context RAG](https://arxiv.org/abs/2508.04390)

*Herbert Ullrich, Jan Drchal*

**Main category:** cs.CL

**Keywords:** fact-checking, RAG, NLU, performance, Ev2R

**Relevance Score:** 4

**TL;DR:** A high-performing fact-checking pipeline that excels in the FEVER 8 shared task using a two-step RAG approach.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a robust fact-checking system that can perform efficiently even with limited resources such as a single GPU.

**Method:** The system utilizes a two-step Retrieval-Augmented Generation (RAG) pipeline designed for fact-checking claims.

**Key Contributions:**

	1. First place in FEVER 8 shared task
	2. Efficient deployment on a single NVidia A10 GPU
	3. State-of-the-art Ev2R test-score performance

**Result:** Achieved state-of-the-art performance on the Ev2R test-score while being deployable on-premise with specific hardware constraints.

**Limitations:** 

**Conclusion:** The proposed pipeline demonstrates that effective fact-checking can be accomplished efficiently while maintaining high performance under resource limitations.

**Abstract:** In this paper, we present our fact-checking pipeline which has scored first in FEVER 8 shared task. Our fact-checking system is a simple two-step RAG pipeline based on our last year's submission. We show how the pipeline can be redeployed on-premise, achieving state-of-the-art fact-checking performance (in sense of Ev2R test-score), even under the constraint of a single NVidia A10 GPU, 23GB of graphical memory and 60s running time per claim.

</details>


### [75] [Improving Crash Data Quality with Large Language Models: Evidence from Secondary Crash Narratives in Kentucky](https://arxiv.org/abs/2508.04399)

*Xu Zhang, Mei Chen*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Crash Data Quality, Large Language Models, Fine-tuned Transformers, Machine Learning

**Relevance Score:** 8

**TL;DR:** Evaluation of NLP techniques to improve crash data quality using advanced LLMs and fine-tuned transformers.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of crash data by mining narratives and identifying secondary crashes in Kentucky.

**Method:** Comparison of zero-shot open-source LLMs, fine-tuned transformers, and logistic regression as a baseline on 16,656 narratives from 2015-2022, while testing on 1,771 narratives from 2022.

**Key Contributions:**

	1. Demonstrated efficacy of fine-tuned transformers in enhancing crash data quality.
	2. Provided insights into performance trade-offs between LLMs and traditional models.
	3. Outlined practical deployment strategies for NLP applications in crash data analysis.

**Result:** Fine-tuned transformers outperformed zero-shot LLMs and logistic regression, with RoBERTa achieving the highest accuracy (95%) and F1-score (0.90).

**Limitations:** High computational costs associated with some LLMs; focused on Kentucky data may limit generalizability.

**Conclusion:** Fine-tuned models offer a balance of accuracy and efficiency, while mid-sized LLMs present a viable alternative with lower computational costs.

**Abstract:** This study evaluates advanced natural language processing (NLP) techniques to enhance crash data quality by mining crash narratives, using secondary crash identification in Kentucky as a case study. Drawing from 16,656 manually reviewed narratives from 2015-2022, with 3,803 confirmed secondary crashes, we compare three model classes: zero-shot open-source large language models (LLMs) (LLaMA3:70B, DeepSeek-R1:70B, Qwen3:32B, Gemma3:27B); fine-tuned transformers (BERT, DistilBERT, RoBERTa, XLNet, Longformer); and traditional logistic regression as baseline. Models were calibrated on 2015-2021 data and tested on 1,771 narratives from 2022. Fine-tuned transformers achieved superior performance, with RoBERTa yielding the highest F1-score (0.90) and accuracy (95%). Zero-shot LLaMA3:70B reached a comparable F1 of 0.86 but required 139 minutes of inference; the logistic baseline lagged well behind (F1:0.66). LLMs excelled in recall for some variants (e.g., GEMMA3:27B at 0.94) but incurred high computational costs (up to 723 minutes for DeepSeek-R1:70B), while fine-tuned models processed the test set in seconds after brief training. Further analysis indicated that mid-sized LLMs (e.g., DeepSeek-R1:32B) can rival larger counterparts in performance while reducing runtime, suggesting opportunities for optimized deployments. Results highlight trade-offs between accuracy, efficiency, and data requirements, with fine-tuned transformer models balancing precision and recall effectively on Kentucky data. Practical deployment considerations emphasize privacy-preserving local deployment, ensemble approaches for improved accuracy, and incremental processing for scalability, providing a replicable scheme for enhancing crash-data quality with advanced NLP.

</details>


### [76] [Why are LLMs' abilities emergent?](https://arxiv.org/abs/2508.04401)

*Vladim√≠r Havl√≠k*

**Main category:** cs.CL

**Keywords:** Large Language Models, Deep Neural Networks, Emergence, Complex Systems, Nonlinear Dynamics

**Relevance Score:** 8

**TL;DR:** This paper explores the emergent properties of Deep Neural Networks (DNNs) and their capabilities, examining the complexities of their dynamics and arguing for a new understanding of LLMs as systems governed by emergential principles.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the epistemological challenge of 'creation without understanding' in AI development by investigating how emergent properties arise in DNNs.

**Method:** The paper combines theoretical analysis and empirical observations, focusing on scaling laws, grokking phenomena, and phase transitions in model capabilities.

**Key Contributions:**

	1. Theoretical insights into the nature of emergence in DNNs
	2. Analysis of scaling laws and complex dynamics in model capabilities
	3. A new perspective on LLM capabilities as emergent properties of nonlinear systems

**Result:** The study reveals that emergent abilities in DNNs result from complex dynamics of nonlinear systems rather than parameter scaling alone, challenging existing metrics and definitions of emergence.

**Limitations:** 

**Conclusion:** Understanding LLM capabilities necessitates viewing DNNs as complex dynamical systems governed by universal principles of emergence, akin to those found in natural sciences.

**Abstract:** The remarkable success of Large Language Models (LLMs) in generative tasks has raised fundamental questions about the nature of their acquired capabilities, which often appear to emerge unexpectedly without explicit training. This paper examines the emergent properties of Deep Neural Networks (DNNs) through both theoretical analysis and empirical observation, addressing the epistemological challenge of "creation without understanding" that characterises contemporary AI development. We explore how the neural approach's reliance on nonlinear, stochastic processes fundamentally differs from symbolic computational paradigms, creating systems whose macro-level behaviours cannot be analytically derived from micro-level neuron activities. Through analysis of scaling laws, grokking phenomena, and phase transitions in model capabilities, I demonstrate that emergent abilities arise from the complex dynamics of highly sensitive nonlinear systems rather than simply from parameter scaling alone. My investigation reveals that current debates over metrics, pre-training loss thresholds, and in-context learning miss the fundamental ontological nature of emergence in DNNs. I argue that these systems exhibit genuine emergent properties analogous to those found in other complex natural phenomena, where systemic capabilities emerge from cooperative interactions among simple components without being reducible to their individual behaviours. The paper concludes that understanding LLM capabilities requires recognising DNNs as a new domain of complex dynamical systems governed by universal principles of emergence, similar to those operating in physics, chemistry, and biology. This perspective shifts the focus from purely phenomenological definitions of emergence to understanding the internal dynamic transformations that enable these systems to acquire capabilities that transcend their individual components.

</details>


### [77] [What Do Humans Hear When Interacting? Experiments on Selective Listening for Evaluating ASR of Spoken Dialogue Systems](https://arxiv.org/abs/2508.04402)

*Kiyotada Mori, Seiya Kawano, Chaoran Liu, Carlos Toshinori Ishi, Angel Fernando Garcia Contreras, Koichiro Yoshino*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, selective listening, spoken dialogue systems, human-computer interaction, dialogue response generation

**Relevance Score:** 7

**TL;DR:** The paper explores the concept of selective listening in humans to improve automatic speech recognition (ASR) evaluations in spoken dialogue systems (SDSs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance ASR capabilities in SDSs by understanding how humans selectively listen during conversations, thereby identifying required ASR features.

**Method:** The study experimentally compares human-generated dialogue responses with reference transcriptions to assess selective listening effects.

**Key Contributions:**

	1. Proposes a new ASR evaluation method informed by human selective listening.
	2. Experimental confirmation of selective listening effects in dialogue generation.
	3. Identification of ASR feature requirements derived from human listening behavior.

**Result:** The experimental results confirm the existence of selective listening processes in humans, suggesting potential gaps between ASR systems and human transcription abilities.

**Limitations:** 

**Conclusion:** A new ASR evaluation method based on human selective listening is proposed, aiming to bridge the identified transcription capability gaps.

**Abstract:** Spoken dialogue systems (SDSs) utilize automatic speech recognition (ASR) at the front end of their pipeline. The role of ASR in SDSs is to recognize information in user speech related to response generation appropriately. Examining selective listening of humans, which refers to the ability to focus on and listen to important parts of a conversation during the speech, will enable us to identify the ASR capabilities required for SDSs and evaluate them. In this study, we experimentally confirmed selective listening when humans generate dialogue responses by comparing human transcriptions for generating dialogue responses and reference transcriptions. Based on our experimental results, we discuss the possibility of a new ASR evaluation method that leverages human selective listening, which can identify the gap between transcription ability between ASR systems and humans.

</details>


### [78] [Dialogue Response Prefetching Based on Semantic Similarity and Prediction Confidence of Language Model](https://arxiv.org/abs/2508.04403)

*Kiyotada Mori, Seiya Kawano, Angel Fernando Garcia Contreras, Koichiro Yoshino*

**Main category:** cs.CL

**Keywords:** dialogue systems, user-perceived latency, prediction confidence model, semantic similarity, prefetching

**Relevance Score:** 7

**TL;DR:** This paper presents a prediction confidence model (PCM) to improve user-perceived latency in spoken dialogue systems by prefetching responses based on predicted user utterances.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce user-perceived latency (UPL) in spoken dialogue systems, it is essential to predict user utterances before they finish speaking.

**Method:** The proposed PCM estimates semantic similarity between predicted complete user utterances and actual complete utterances to assess the feasibility of prefetching responses.

**Key Contributions:**

	1. Introduction of the Prediction Confidence Model (PCM) to address UPL.
	2. Evaluation of PCM based on semantic similarity measures.
	3. Assessment of prefetching potential in dialogue systems.

**Result:** The PCM's effectiveness was evaluated by analyzing the differences between predicted user utterances and actual utterances, demonstrating its ability to determine prefetching feasibility.

**Limitations:** 

**Conclusion:** The study concludes that the PCM can significantly enhance the performance of spoken dialogue systems by effectively managing prefetching decisions based on prediction confidence.

**Abstract:** Prefetching of dialogue responses has been investigated to reduce user-perceived latency (UPL), which refers to the user's waiting time before receiving the system's response, in spoken dialogue systems. To reduce the UPL, it is necessary to predict complete user utterances before the end of the user's speech, typically by language models, to prepare prefetched dialogue responses. In this study, we proposed a prediction confidence model (PCM) that determines whether prefetching is possible or not by estimating the semantic similarity between the predicted complete user utterance and the complete user utterance. We evaluated our PCM based on the differences between the predicted complete user utterance and the complete user utterance.

</details>


### [79] [Evaluating, Synthesizing, and Enhancing for Customer Support Conversation](https://arxiv.org/abs/2508.04423)

*Jie Zhu, Huaixia Dou, Junhui Li, Lifan Guo, Feng Chen, Chi Zhang, Fang Kong*

**Main category:** cs.CL

**Keywords:** customer support, dialogue datasets, LM training, conversation strategies, HCI

**Relevance Score:** 7

**TL;DR:** This paper introduces a framework and dataset for enhancing customer support agent training using structured strategies and LLMs.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve customer support by providing structured guidance and strategies for effective communication.

**Method:** A structured framework for Customer Support Conversation (CSC) is proposed along with the creation of an evaluation dataset (CSConv) and a training dataset (RoleCS) powered by LLMs.

**Key Contributions:**

	1. Introduction of the Customer Support Conversation (CSC) framework.
	2. Creation of CSConv dataset for evaluating customer-agent conversations.
	3. Development of RoleCS for training LLMs on strategy-rich conversation tasks.

**Result:** Fine-tuning LLMs on RoleCS significantly improves the generation of high-quality responses, as confirmed by human evaluations focused on problem resolution.

**Limitations:** 

**Conclusion:** The CSC framework, supported by LLM-generated datasets, enhances the quality of customer support interactions.

**Abstract:** Effective customer support requires not only accurate problem solving but also structured and empathetic communication aligned with professional standards. However, existing dialogue datasets often lack strategic guidance, and real-world service data is difficult to access and annotate. To address this, we introduce the task of Customer Support Conversation (CSC), aimed at training customer service agents to respond using well-defined support strategies. We propose a structured CSC framework grounded in COPC guidelines, defining five conversational stages and twelve strategies to guide high-quality interactions. Based on this, we construct CSConv, an evaluation dataset of 1,855 real-world customer-agent conversations rewritten using LLMs to reflect deliberate strategy use, and annotated accordingly. Additionally, we develop a role-playing approach that simulates strategy-rich conversations using LLM-powered roles aligned with the CSC framework, resulting in the training dataset RoleCS. Experiments show that fine-tuning strong LLMs on RoleCS significantly improves their ability to generate high-quality, strategy-aligned responses on CSConv. Human evaluations further confirm gains in problem resolution. All code and data will be made publicly available at https://github.com/aliyun/qwen-dianjin.

</details>


### [80] [StepFun-Formalizer: Unlocking the Autoformalization Potential of LLMs through Knowledge-Reasoning Fusion](https://arxiv.org/abs/2508.04440)

*Yutong Wu, Di Huang, Ruosi Wan, Yue Peng, Shijie Shang, Chenrui Cao, Lei Qi, Rui Zhang, Zidong Du, Jie Yan, Xing Hu*

**Main category:** cs.CL

**Keywords:** autoformalization, LLMs, formal language, natural language processing, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents ThinkingF, a novel data synthesis and training pipeline aimed at improving the accuracy of autoformalization through enhancing formal knowledge and reasoning capabilities in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the accuracy of translating natural-language mathematical statements into formal language, which is currently limited by the capabilities of existing methods.

**Method:** The authors introduce a data synthesis and training pipeline, ThinkingF, which involves constructing two datasets: one for formal knowledge and the other for informal-to-formal reasoning, and applying SFT and RLVR techniques.

**Key Contributions:**

	1. Introduction of ThinkingF for autoformalization training
	2. Creation of two new datasets for formal knowledge and reasoning
	3. Achieving state-of-the-art benchmark scores in formalization tasks

**Result:** The resulting models, particularly the StepFun-Formalizer-32B, achieved state-of-the-art performance on benchmark tests, reporting BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench.

**Limitations:** 

**Conclusion:** The study concludes that enhancing both formal knowledge and reasoning capability significantly improves LLM performance in autoformalization tasks.

**Abstract:** Autoformalization aims to translate natural-language mathematical statements into a formal language. While LLMs have accelerated progress in this area, existing methods still suffer from low accuracy. We identify two key abilities for effective autoformalization: comprehensive mastery of formal-language domain knowledge, and reasoning capability of natural language problem understanding and informal-formal alignment. Without the former, a model cannot identify the correct formal objects; without the latter, it struggles to interpret real-world contexts and map them precisely into formal expressions. To address these gaps, we introduce ThinkingF, a data synthesis and training pipeline that improves both abilities. First, we construct two datasets: one by distilling and selecting large-scale examples rich in formal knowledge, and another by generating informal-to-formal reasoning trajectories guided by expert-designed templates. We then apply SFT and RLVR with these datasets to further fuse and refine the two abilities. The resulting 7B and 32B models exhibit both comprehensive formal knowledge and strong informal-to-formal reasoning. Notably, StepFun-Formalizer-32B achieves SOTA BEq@1 scores of 40.5% on FormalMATH-Lite and 26.7% on ProverBench, surpassing all prior general-purpose and specialized models.

</details>


### [81] [Automated Generation of Curriculum-Aligned Multiple-Choice Questions for Malaysian Secondary Mathematics Using Generative AI](https://arxiv.org/abs/2508.04442)

*Rohaizah Abdul Wahid, Muhamad Said Nizamuddin Nadim, Suliana Sulaiman, Syahmi Akmal Shaharudin, Muhammad Danial Jupikil, Iqqwan Jasman Su Azlan Su*

**Main category:** cs.CL

**Keywords:** Generative AI, Educational assessment, Mathematics, Bahasa Melayu, Retrieval-Augmented Generation

**Relevance Score:** 4

**TL;DR:** This paper explores the use of Generative AI to create educational assessment tools, specifically Form 1 Mathematics MCQs in Bahasa Melayu, and compares various generation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for scalable and high-quality educational assessment tools in the Malaysian education system, particularly for low-resource languages like Bahasa Melayu.

**Method:** The research introduces and compares four pipelines for generating MCQs: non-grounded prompting (structured and basic) and RAG approaches (one using LangChain framework and one manual), grounded in official curriculum documents evaluated through a dual-pronged automated framework.

**Key Contributions:**

	1. Development of incremental pipelines for generating curriculum-aligned MCQs using GenAI.
	2. Introduction of RAG-QA evaluation method for assessing generated questions.
	3. Actionable insights for EdTech implementations in low-resource language contexts.

**Result:** RAG-based pipelines significantly outperform non-grounded prompting, yielding questions with better curriculum alignment and factual accuracy.

**Limitations:** 

**Conclusion:** The study provides a validated methodology for generating curriculum-specific content and a novel evaluation technique, offering insights for EdTech solutions in Malaysia and similar regions.

**Abstract:** This paper addresses the critical need for scalable and high-quality educational assessment tools within the Malaysian education system. It highlights the potential of Generative AI (GenAI) while acknowledging the significant challenges of ensuring factual accuracy and curriculum alignment, especially for low-resource languages like Bahasa Melayu. This research introduces and compares four incremental pipelines for generating Form 1 Mathematics multiple-choice questions (MCQs) in Bahasa Melayu using OpenAI's GPT-4o. The methods range from non-grounded prompting (structured and basic) to Retrieval-Augmented Generation (RAG) approaches (one using the LangChain framework, one implemented manually). The system is grounded in official curriculum documents, including teacher-prepared notes and the yearly teaching plan (RPT). A dual-pronged automated evaluation framework is employed to assess the generated questions. Curriculum alignment is measured using Semantic Textual Similarity (STS) against the RPT, while contextual validity is verified through a novel RAG-based Question-Answering (RAG-QA) method. The results demonstrate that RAG-based pipelines significantly outperform non-grounded prompting methods, producing questions with higher curriculum alignment and factual validity. The study further analyzes the trade-offs between the ease of implementation of framework-based RAG and the fine-grained control offered by a manual pipeline. This work presents a validated methodology for generating curriculum-specific educational content in a low-resource language, introduces a symbiotic RAG-QA evaluation technique, and provides actionable insights for the development and deployment of practical EdTech solutions in Malaysia and similar regions.

</details>


### [82] [CALE : Concept-Aligned Embeddings for Both Within-Lemma and Inter-Lemma Sense Differentiation](https://arxiv.org/abs/2508.04494)

*Bastien Li√©tard, Gabriel Loiseau*

**Main category:** cs.CL

**Keywords:** lexical semantics, contextualized language models, semantic relations, fine-tuning, Word-in-Context

**Relevance Score:** 6

**TL;DR:** This paper proposes Concept Differentiation to enhance lexical semantics through context-sensitive language models by providing a new dataset and fine-tuning methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of semantic representations beyond Word-in-Context approaches by including inter-words scenarios.

**Method:** The authors introduce a new task called Concept Differentiation and create a dataset derived from SemCor. They fine-tune several language models to generate Concept-Aligned Embeddings (CALE) and evaluate them on various lexical semantic tasks.

**Key Contributions:**

	1. Introduction of Concept Differentiation for inter-word context analysis
	2. Creation of a new dataset derived from SemCor
	3. Fine-tuning of models to generate Concept-Aligned Embeddings (CALE)

**Result:** The proposed CALE models outperform existing models in efficiency and performance on lexical semantic tasks, demonstrating better spatial organization of embeddings after fine-tuning.

**Limitations:** 

**Conclusion:** CALE provides a valuable extension to context-sensitive representations in lexical semantics, allowing for more accurate multi-purpose applications in understanding word meanings.

**Abstract:** Lexical semantics is concerned with both the multiple senses a word can adopt in different contexts, and the semantic relations that exist between meanings of different words. To investigate them, Contextualized Language Models are a valuable tool that provides context-sensitive representations that can be used to investigate lexical meaning. Recent works like XL-LEXEME have leveraged the task of Word-in-Context to fine-tune them to get more semantically accurate representations, but Word-in-Context only compares occurrences of the same lemma, limiting the range of captured information. In this paper, we propose an extension, Concept Differentiation, to include inter-words scenarios. We provide a dataset for this task, derived from SemCor data. Then we fine-tune several representation models on this dataset. We call these models Concept-Aligned Embeddings (CALE). By challenging our models and other models on various lexical semantic tasks, we demonstrate that the proposed models provide efficient multi-purpose representations of lexical meaning that reach best performances in our experiments. We also show that CALE's fine-tuning brings valuable changes to the spatial organization of embeddings.

</details>


### [83] [StyliTruth : Unlocking Stylized yet Truthful LLM Generation via Disentangled Steering](https://arxiv.org/abs/2508.04530)

*Chenglei Shen, Zhongxiang Sun, Teng Shi, Xiao Zhang, Jun Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Representation Editing, Truthfulness, Stylistic Fidelity, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper presents StyliTruth, a method for generating stylized LLM responses without sacrificing truthfulness, addressing the issue of stylization-induced truthfulness collapse.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a trade-off between generating stylized responses and maintaining truthfulness in LLM outputs, which existing methods overlook.

**Method:** StyliTruth separates style and truth representations in the model via orthogonal deflation, allowing independent control of both through adaptive steering vectors.

**Key Contributions:**

	1. Introduces the concept of stylization-induced truthfulness collapse
	2. Proposes an orthogonal deflation process for representation separation
	3. Demonstrates improved performance in balancing style and truthfulness across multiple styles and languages.

**Result:** Extensive experiments show that StyliTruth significantly mitigates truthfulness collapse while outperforming existing methods in maintaining stylistic fidelity and correctness.

**Limitations:** 

**Conclusion:** StyliTruth effectively balances the generation of stylized outputs with factual accuracy, making it a promising approach for LLM applications.

**Abstract:** Generating stylized large language model (LLM) responses via representation editing is a promising way for fine-grained output control. However, there exists an inherent trade-off: imposing a distinctive style often degrades truthfulness. Existing representation editing methods, by naively injecting style signals, overlook this collateral impact and frequently contaminate the model's core truthfulness representations, resulting in reduced answer correctness. We term this phenomenon stylization-induced truthfulness collapse. We attribute this issue to latent coupling between style and truth directions in certain key attention heads, and propose StyliTruth, a mechanism that preserves stylization while keeping truthfulness intact. StyliTruth separates the style-relevant and truth-relevant subspaces in the model's representation space via an orthogonal deflation process. This decomposition enables independent control of style and truth in their own subspaces, minimizing interference. By designing adaptive, token-level steering vectors within each subspace, we dynamically and precisely control the generation process to maintain both stylistic fidelity and truthfulness. We validate our method on multiple styles and languages. Extensive experiments and analyses show that StyliTruth significantly reduces stylization-induced truthfulness collapse and outperforms existing inference-time intervention methods in balancing style adherence with truthfulness.

</details>


### [84] [Unveiling the Landscape of Clinical Depression Assessment: From Behavioral Signatures to Psychiatric Reasoning](https://arxiv.org/abs/2508.04531)

*Zhuang Chen, Guanqun Bi, Wen Zhang, Jiawei Hu, Aoyun Wang, Xiyao Xiao, Kun Feng, Minlie Huang*

**Main category:** cs.CL

**Keywords:** depression, clinical assessment, C-MIND, LLMs, multimodal diagnosis

**Relevance Score:** 9

**TL;DR:** This paper introduces C-MIND, a clinical dataset for assessing depression and evaluates the performance of classical models and LLMs in psychiatric diagnosis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To unveil the landscape of clinical depression assessment and improve automated diagnosis using real-world data.

**Method:** C-MIND is a multimodal dataset collected from hospital visits, involving structured psychiatric tasks and expert clinician diagnoses. The study analyzes the contribution of various tasks and modalities to diagnostic performance and explores LLMs in psychiatric reasoning.

**Key Contributions:**

	1. Introduction of the C-MIND dataset for clinical depression assessment
	2. Analysis of classical models on multimodal data
	3. Improvement of LLM diagnostic performance through clinical guidance

**Result:** The analysis reveals behavioral signatures relevant to diagnosis and demonstrates that LLMs can be guided with clinical expertise to enhance diagnostic performance by up to 10% in Macro-F1 score.

**Limitations:** Potential limitations include the specificity of the dataset to clinical settings and the generalizability of LLM findings to broader populations.

**Conclusion:** The C-MIND dataset and proposed methods aim to establish a reliable foundation for clinical depression assessment, enhancing both data and algorithmic approaches.

**Abstract:** Depression is a widespread mental disorder that affects millions worldwide. While automated depression assessment shows promise, most studies rely on limited or non-clinically validated data, and often prioritize complex model design over real-world effectiveness. In this paper, we aim to unveil the landscape of clinical depression assessment. We introduce C-MIND, a clinical neuropsychiatric multimodal diagnosis dataset collected over two years from real hospital visits. Each participant completes three structured psychiatric tasks and receives a final diagnosis from expert clinicians, with informative audio, video, transcript, and functional near-infrared spectroscopy (fNIRS) signals recorded. Using C-MIND, we first analyze behavioral signatures relevant to diagnosis. We train a range of classical models to quantify how different tasks and modalities contribute to diagnostic performance, and dissect the effectiveness of their combinations. We then explore whether LLMs can perform psychiatric reasoning like clinicians and identify their clear limitations in realistic clinical settings. In response, we propose to guide the reasoning process with clinical expertise and consistently improves LLM diagnostic performance by up to 10% in Macro-F1 score. We aim to build an infrastructure for clinical depression assessment from both data and algorithmic perspectives, enabling C-MIND to facilitate grounded and reliable research for mental healthcare.

</details>


### [85] [Beyond Brainstorming: What Drives High-Quality Scientific Ideas? Lessons from Multi-Agent Collaboration](https://arxiv.org/abs/2508.04575)

*Nuo Chen, Yicheng Tong, Jiaying Wu, Minh Duc Duong, Qian Wang, Qingyun Zou, Bryan Hooi, Bingsheng He*

**Main category:** cs.CL

**Keywords:** multi-agent systems, collaborative AI, scientific ideation

**Relevance Score:** 7

**TL;DR:** This paper explores a multi-agent framework for generating research proposals through structured discussions, aiming to enhance creativity compared to single-agent ideation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether structured multi-agent discussions can lead to better ideation than solitary approaches in the context of scientific research.

**Method:** A cooperative multi-agent framework was proposed and evaluated by varying group sizes, leadership structures, and levels of interdisciplinarity and seniority, using agent-based scoring and human review to assess idea quality.

**Key Contributions:**

	1. Introduction of a cooperative multi-agent framework for ideation
	2. Systematic comparison of different team configurations
	3. Insights on the importance of cognitive diversity and expertise in group creativity

**Result:** Multi-agent discussions significantly outperformed solitary ideation in generating higher quality proposals, with designated leaders enhancing integration and vision. Cognitive diversity drove quality, but expertise remained essential.

**Limitations:** 

**Conclusion:** The findings underline the importance of team structure and cognitive diversity in collaborative AI ideation systems, with practical implications for their design.

**Abstract:** While AI agents show potential in scientific ideation, most existing frameworks rely on single-agent refinement, limiting creativity due to bounded knowledge and perspective. Inspired by real-world research dynamics, this paper investigates whether structured multi-agent discussions can surpass solitary ideation. We propose a cooperative multi-agent framework for generating research proposals and systematically compare configurations including group size, leaderled versus leaderless structures, and team compositions varying in interdisciplinarity and seniority. To assess idea quality, we employ a comprehensive protocol with agent-based scoring and human review across dimensions such as novelty, strategic vision, and integration depth. Our results show that multi-agent discussions substantially outperform solitary baselines. A designated leader acts as a catalyst, transforming discussion into more integrated and visionary proposals. Notably, we find that cognitive diversity is a primary driver of quality, yet expertise is a non-negotiable prerequisite, as teams lacking a foundation of senior knowledge fail to surpass even a single competent agent. These findings offer actionable insights for designing collaborative AI ideation systems and shed light on how team structure influences creative outcomes.

</details>


### [86] [Share Your Attention: Transformer Weight Sharing via Matrix-based Dictionary Learning](https://arxiv.org/abs/2508.04581)

*Magauiya Zhussip, Dmitriy Shopkhoev, Ammar Ali, Stamatios Lefkimmiatis*

**Main category:** cs.CL

**Keywords:** Transformer, Weight Sharing, Large Language Models, Efficiency, Machine Learning

**Relevance Score:** 9

**TL;DR:** Proposes a weight-sharing framework for transformers (MASA) that reduces parameters significantly while maintaining performance.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational and memory demands of large language models (LLMs) by exploring inter-block redundancy in transformer architectures.

**Method:** MASA (Matrix Atom Sharing in Attention) decomposes attention projection matrices into shared dictionary atoms, allowing for structured weight sharing across transformer layers.

**Key Contributions:**

	1. Reduction of attention parameters by 66.7%
	2. Drop-in replacement for existing models
	3. Effective across diverse scales and tasks

**Result:** Achieves a 66.7% reduction in attention parameters without performance loss and outperforms existing methods on accuracy and perplexity across different parameter scales.

**Limitations:** 

**Conclusion:** MASA provides a scalable solution for efficient transformer models and shows potential for application on pretrained LLMs with minimal performance impact.

**Abstract:** Large language models (LLMs) have revolutionized AI applications, yet their high computational and memory demands hinder their widespread deployment. Existing compression techniques focus on intra-block optimizations (e.g. low-rank approximation, attention head pruning), while the repetitive layered structure of transformers implies significant inter-block redundancy - a dimension largely unexplored beyond key-value (KV) caching. Inspired by dictionary learning in CNNs, we propose a framework for structured weight sharing across transformer layers. Our approach decomposes attention projection matrices into shared dictionary atoms, reducing the attention module's parameters by 66.7% while achieving on-par performance. Unlike complex methods requiring distillation or architectural changes, MASA (Matrix Atom Sharing in Attention) operates as a drop-in replacement - trained with standard optimizers - and represents each layer's weights as linear combinations of shared matrix atoms. Experiments across scales (100M-700M parameters) show that MASA achieves better benchmark accuracy and perplexity than grouped-query attention (GQA), low-rank baselines and recently proposed Repeat-all-over/Sequential sharing at comparable parameter budgets. Ablation studies confirm robustness to the dictionary size and the efficacy of shared representations in capturing cross-layer statistical regularities. Extending to Vision Transformers (ViT), MASA matches performance metrics on image classification and detection tasks with 66.7% fewer attention parameters. By combining dictionary learning strategies with transformer efficiency, MASA offers a scalable blueprint for parameter-efficient models without sacrificing performance. Finally, we investigate the possibility of employing MASA on pretrained LLMs to reduce their number of parameters without experiencing any significant drop in their performance.

</details>


### [87] [TURA: Tool-Augmented Unified Retrieval Agent for AI Search](https://arxiv.org/abs/2508.04604)

*Zhejun Zhao, Yuehu Dong, Alley Liu, Lixue Zheng, Pingsheng Liu, Dongdong Shen, Long Xia, Jiashu Zhao, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Conversational AI, Dynamic Information Retrieval

**Relevance Score:** 9

**TL;DR:** TURA is a novel framework that enhances Retrieval-Augmented Generation (RAG) by integrating dynamic real-time information retrieval with traditional static content indexing, addressing limitations faced by typical search engines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RAG approaches for search engines primarily focus on static content, which limits their ability to handle real-time and dynamic data queries essential for modern applications.

**Method:** TURA employs a three-stage framework that includes an Intent-Aware Retrieval module, a DAG-based Task Planner for optimal execution, and a lightweight Distilled Agent Executor to manage tool usage efficiently.

**Key Contributions:**

	1. Introduction of TURA framework that integrates dynamic data retrieval with RAG.
	2. Development of an Intent-Aware Retrieval module for more precise query handling.
	3. Creation of a DAG-based Task Planner for optimizing task execution.

**Result:** TURA successfully bridges the gap between static RAG and dynamic information retrieval, allowing for rapid responses to real-time queries while maintaining low latency and scalability for tens of millions of users.

**Limitations:** 

**Conclusion:** This architecture represents a significant advancement towards the development of AI search products capable of handling both static and dynamic information efficiently.

**Abstract:** The advent of Large Language Models (LLMs) is transforming search engines into conversational AI search products, primarily using Retrieval-Augmented Generation (RAG) on web corpora. However, this paradigm has significant industrial limitations. Traditional RAG approaches struggle with real-time needs and structured queries that require accessing dynamically generated content like ticket availability or inventory. Limited to indexing static pages, search engines cannot perform the interactive queries needed for such time-sensitive data. Academic research has focused on optimizing RAG for static content, overlooking complex intents and the need for dynamic sources like databases and real-time APIs. To bridge this gap, we introduce TURA (Tool-Augmented Unified Retrieval Agent for AI Search), a novel three-stage framework that combines RAG with agentic tool-use to access both static content and dynamic, real-time information. TURA has three key components: an Intent-Aware Retrieval module to decompose queries and retrieve information sources encapsulated as Model Context Protocol (MCP) Servers, a DAG-based Task Planner that models task dependencies as a Directed Acyclic Graph (DAG) for optimal parallel execution, and a lightweight Distilled Agent Executor for efficient tool calling. TURA is the first architecture to systematically bridge the gap between static RAG and dynamic information sources for a world-class AI search product. Serving tens of millions of users, it leverages an agentic framework to deliver robust, real-time answers while meeting the low-latency demands of a large-scale industrial system.

</details>


### [88] [Lightweight Transformers for Zero-Shot and Fine-Tuned Text-to-SQL Generation Using Spider](https://arxiv.org/abs/2508.04623)

*Chirag Seth, Utkarsh Singh*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Transformers, Low-resource, Database querying, Natural language processing

**Relevance Score:** 6

**TL;DR:** This study evaluates lightweight transformer models for text-to-SQL translation in low-resource settings, achieving highest accuracy with fine-tuned T5-Small model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enable non-expert users to query relational databases using natural language, particularly in low-resource situations.

**Method:** Evaluation of T5-Small, BART-Small, and GPT-2 on the Spider dataset using a modular pipeline that formats schema for each model's architecture.

**Key Contributions:**

	1. Evaluation of three lightweight models in a low-resource setting.
	2. Development of a model-agnostic pipeline for schema formatting.
	3. Highlighting the superiority of encoder-decoder models for SQL generation.

**Result:** Fine-tuned T5-Small achieved the highest Logical Form Accuracy (LFAcc) of 27.8%, outperforming BART-Small and GPT-2.

**Limitations:** Performance limited by resource constraints.

**Conclusion:** The study demonstrates the potential of compact transformer models for effective text-to-SQL solutions despite resource constraints.

**Abstract:** Text-to-SQL translation enables non-expert users to query relational databases using natural language, with applications in education and business intelligence. This study evaluates three lightweight transformer models - T5-Small, BART-Small, and GPT-2 - on the Spider dataset, focusing on low-resource settings. We developed a reusable, model-agnostic pipeline that tailors schema formatting to each model's architecture, training them across 1000 to 5000 iterations and evaluating on 1000 test samples using Logical Form Accuracy (LFAcc), BLEU, and Exact Match (EM) metrics. Fine-tuned T5-Small achieves the highest LFAcc (27.8%), outperforming BART-Small (23.98%) and GPT-2 (20.1%), highlighting encoder-decoder models' superiority in schema-aware SQL generation. Despite resource constraints limiting performance, our pipeline's modularity supports future enhancements, such as advanced schema linking or alternative base models. This work underscores the potential of compact transformers for accessible text-to-SQL solutions in resource-scarce environments.

</details>


### [89] [P-Aligner: Enabling Pre-Alignment of Language Models via Principled Instruction Synthesis](https://arxiv.org/abs/2508.04626)

*Feifan Song, Bofei Gao, Yifan Song, Yi Liu, Weimin Xiong, Yuyang Song, Tianyu Liu, Guoyin Wang, Houfeng Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, instruction alignment, human-preferred instructions, Monte-Carlo Tree Search, NLP

**Relevance Score:** 8

**TL;DR:** P-Aligner is a novel lightweight module for aligning instructions to enhance Large Language Models' performance by generating human-preferred content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of Large Language Models' outputs with human values and preferences, addressing issues with flawed instructions.

**Method:** P-Aligner uses a principle-guided pipeline with Monte-Carlo Tree Search on a new dataset called UltraPrompt to create more suitable instructions for models.

**Key Contributions:**

	1. Introduction of P-Aligner as a lightweight instruction-alignment module.
	2. Development of UltraPrompt, a new dataset for synthesizing human-preferred instructions.
	3. Demonstration of significant performance improvements over existing methods.

**Result:** P-Aligner outperforms strong baselines, achieving average win-rate gains of 28.35% on GPT-4-turbo and 8.69% on Gemma-2-SimPO, demonstrating its effectiveness in multiple experiments.

**Limitations:** 

**Conclusion:** The experiments confirm P-Aligner's ability to efficiently align instructions, enhancing the quality of LLM interactions without significant overhead.

**Abstract:** Large Language Models (LLMs) are expected to produce safe, helpful, and honest content during interaction with human users, but they frequently fail to align with such values when given flawed instructions, e.g., missing context, ambiguous directives, or inappropriate tone, leaving substantial room for improvement along multiple dimensions. A cost-effective yet high-impact way is to pre-align instructions before the model begins decoding. Existing approaches either rely on prohibitive test-time search costs or end-to-end model rewrite, which is powered by a customized training corpus with unclear objectives. In this work, we demonstrate that the goal of efficient and effective preference alignment can be achieved by P-Aligner, a lightweight module generating instructions that preserve the original intents while being expressed in a more human-preferred form. P-Aligner is trained on UltraPrompt, a new dataset synthesized via a proposed principle-guided pipeline using Monte-Carlo Tree Search, which systematically explores the space of candidate instructions that are closely tied to human preference. Experiments across different methods show that P-Aligner generally outperforms strong baselines across various models and benchmarks, including average win-rate gains of 28.35% and 8.69% on GPT-4-turbo and Gemma-2-SimPO, respectively. Further analyses validate its effectiveness and efficiency through multiple perspectives, including data quality, search strategies, iterative deployment, and time overhead.

</details>


### [90] [IFDECORATOR: Wrapping Instruction Following Reinforcement Learning with Verifiable Rewards](https://arxiv.org/abs/2508.04632)

*Xu Guo, Tianyi Liang, Tong Jian, Xiaogui Yang, Ling-I Wu, Chenhui Li, Zhihui Lu, Qipeng Guo, Kai Chen*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Instruction Following, Large Language Models

**Relevance Score:** 9

**TL;DR:** The paper introduces Instruction Following Decorator (IFDecorator), a framework to enhance RLVR training in LLMs, improving efficiency and intent alignment while reducing reward hacking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies in RLVR and the tendency for LLMs to exploit shortcuts without aligning with user intent.

**Method:** The framework includes a cooperative-adversarial data flywheel, an intent alignment module called IntentCheck, and trip wires for detecting reward hacking.

**Key Contributions:**

	1. Introduction of IFDecorator to improve RLVR training efficiency.
	2. Implementation of a cooperative-adversarial data flywheel for generating challenging instruction-verification pairs.
	3. Development of trip wires for detecting and reducing reward hacking.

**Result:** The proposed method achieved 87.43% accuracy on IFEval, surpassing larger models like GPT-4o, and showed notable improvements on FollowBench with reduced reward hacking.

**Limitations:** 

**Conclusion:** IFDecorator enhances LLM instruction following while maintaining general capabilities, with potential for releasing supplementary materials for research.

**Abstract:** Reinforcement Learning with Verifiable Rewards (RLVR) improves instruction following capabilities of large language models (LLMs), but suffers from training inefficiency due to inadequate difficulty assessment. Moreover, RLVR is prone to over-optimization, where LLMs exploit verification shortcuts without aligning to the actual intent of user instructions. We introduce Instruction Following Decorator (IFDecorator}, a framework that wraps RLVR training into a robust and sample-efficient pipeline. It consists of three components: (1) a cooperative-adversarial data flywheel that co-evolves instructions and hybrid verifications, generating progressively more challenging instruction-verification pairs; (2) IntentCheck, a bypass module enforcing intent alignment; and (3) trip wires, a diagnostic mechanism that detects reward hacking via trap instructions, which trigger and capture shortcut exploitation behaviors. Our Qwen2.5-32B-Instruct-IFDecorator achieves 87.43% accuracy on IFEval, outperforming larger proprietary models such as GPT-4o. Additionally, we demonstrate substantial improvements on FollowBench while preserving general capabilities. Our trip wires show significant reductions in reward hacking rates. We will release models, code, and data for future research.

</details>


### [91] [Can NLP Tackle Hate Speech in the Real World? Stakeholder-Informed Feedback and Survey on Counterspeech](https://arxiv.org/abs/2508.04638)

*Tanvi Dinkar, Aiqi Jiang, Simona Frenda, Poppy Gerrard-Abbott, Nancie Gunson, Gavin Abercrombie, Ioannis Konstas*

**Main category:** cs.CL

**Keywords:** counterspeech, NLP, stakeholder participation, hate speech, gender-based violence

**Relevance Score:** 8

**TL;DR:** The paper reviews NLP counterspeech studies, highlighting the need for greater stakeholder participation in dataset and model development for addressing online hate speech.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** This paper aims to address the lack of stakeholder participation in counterspeech research and its impact on model effectiveness.

**Method:** Systematic review of 74 NLP studies on counterspeech and a participatory case study with five NGOs focused on online Gender-Based Violence.

**Key Contributions:**

	1. Systematic review of existing NLP counterspeech studies
	2. Participatory case study identifying best practices from NGOs
	3. Recommendations for enhancing stakeholder involvement in research

**Result:** Findings indicate a disconnect between NLP research and the needs of communities affected by online hate speech, suggesting inadequate stakeholder engagement.

**Limitations:** Limited to studies focusing on Gender-Based Violence; may not generalize to other domains of hate speech.

**Conclusion:** The study concludes with recommendations for integrating stakeholder expertise into counterspeech research to improve relevance and effectiveness.

**Abstract:** Counterspeech, i.e. the practice of responding to online hate speech, has gained traction in NLP as a promising intervention. While early work emphasised collaboration with non-governmental organisation stakeholders, recent research trends have shifted toward automated pipelines that reuse a small set of legacy datasets, often without input from affected communities. This paper presents a systematic review of 74 NLP studies on counterspeech, analysing the extent to which stakeholder participation influences dataset creation, model development, and evaluation. To complement this analysis, we conducted a participatory case study with five NGOs specialising in online Gender-Based Violence (oGBV), identifying stakeholder-informed practices for counterspeech generation. Our findings reveal a growing disconnect between current NLP research and the needs of communities most impacted by toxic online content. We conclude with concrete recommendations for re-centring stakeholder expertise in counterspeech research.

</details>


### [92] [Multi-module GRPO: Composing Policy Gradients and Prompt Optimization for Language Model Programs](https://arxiv.org/abs/2508.04660)

*Noah Ziems, Dilara Soylu, Lakshya A Agrawal, Isaac Miller, Liheng Lai, Chen Qian, Kaiqiang Song, Meng Jiang, Dan Klein, Matei Zaharia, Karel D'Oosterlinck, Christopher Potts, Omar Khattab*

**Main category:** cs.CL

**Keywords:** Group Relative Policy Optimization, multi-module, language models, prompt optimization, AI systems

**Relevance Score:** 8

**TL;DR:** This paper introduces mmGRPO, a multi-module generalization of GRPO, improving the performance of modular AI systems that utilize language models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance performance in modular AI systems that combine various language model (LM) calls with different prompt templates.

**Method:** The paper defines mmGRPO, a variant of GRPO that groups LM calls by module and can handle variable-length and interrupted trajectories.

**Key Contributions:**

	1. Development of mmGRPO for modular AI systems
	2. Demonstrated performance improvements in classification and other tasks
	3. Open-sourced implementation in DSPy

**Result:** mmGRPO improves accuracy by 11% on average across multiple tasks compared to post-trained LMs, and by 5% against prompt optimization alone.

**Limitations:** 

**Conclusion:** The introduction of mmGRPO allows for better performance in tasks that leverage modular language models, and it has been open-sourced in DSPy.

**Abstract:** Group Relative Policy Optimization (GRPO) has proven to be an effective tool for post-training language models (LMs). However, AI systems are increasingly expressed as modular programs that mix together multiple LM calls with distinct prompt templates and other tools, and it is not clear how best to leverage GRPO to improve these systems. We begin to address this challenge by defining mmGRPO, a simple multi-module generalization of GRPO that groups LM calls by module across rollouts and handles variable-length and interrupted trajectories. We find that mmGRPO, composed with automatic prompt optimization, improves accuracy by 11% on average across classification, many-hop search, and privacy-preserving delegation tasks against the post-trained LM, and by 5% against prompt optimization on its own. We open-source mmGRPO in DSPy as the dspy.GRPO optimizer.

</details>


### [93] [Sculptor: Empowering LLMs with Cognitive Agency via Active Context Management](https://arxiv.org/abs/2508.04664)

*Mo Li, L. H. Xu, Qitai Tan, Ting Cao, Yunxin Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Active Context Management, Long-context performance, Proactive interference, Memory recall

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework called Sculptor that equips Large Language Models with Active Context Management tools to improve their performance on long contexts by actively managing internal working memory.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with performance degradation due to proactive interference when processing long contexts, necessitating innovative methods to enhance their reasoning and memory recall.

**Method:** The study presents Sculptor, which provides LLMs with tools for context fragmentation, summary/hide/restore, and intelligent search to better manage attention and memory.

**Key Contributions:**

	1. Introduction of Active Context Management tools for LLMs
	2. Demonstration of significant performance improvement in long-context tasks
	3. Highlighting the importance of context-control strategies over larger token windows

**Result:** Experimental results show that Sculptor significantly enhances performance on benchmarks designed for long-context tasks, without requiring additional training.

**Limitations:** 

**Conclusion:** Sculptor proves that employing explicit context-control strategies can enhance LLM robustness beyond simply increasing token limits.

**Abstract:** Large Language Models (LLMs) suffer from significant performance degradation when processing long contexts due to proactive interference, where irrelevant information in earlier parts of the context disrupts reasoning and memory recall. While most research focuses on external memory systems to augment LLMs' capabilities, we propose a complementary approach: empowering LLMs with Active Context Management (ACM) tools to actively sculpt their internal working memory. We introduce Sculptor, a framework that equips LLMs with three categories of tools: (1) context fragmentation, (2) summary, hide, and restore, and (3) intelligent search. Our approach enables LLMs to proactively manage their attention and working memory, analogous to how humans selectively focus on relevant information while filtering out distractions. Experimental evaluation on information-sparse benchmarks-PI-LLM (proactive interference) and NeedleBench Multi-Needle Reasoning-demonstrates that Sculptor significantly improves performance even without specific training, leveraging LLMs' inherent tool calling generalization capabilities. By enabling Active Context Management, Sculptor not only mitigates proactive interference but also provides a cognitive foundation for more reliable reasoning across diverse long-context tasks-highlighting that explicit context-control strategies, rather than merely larger token windows, are key to robustness at scale.

</details>


### [94] [GeRe: Towards Efficient Anti-Forgetting in Continual Learning of LLM via General Samples Replay](https://arxiv.org/abs/2508.04676)

*Yunan Zhang, Shuoran Jiang, Mengchen Zhao, Yuefeng Li, Yang Fan, Xiangping Wu, Qingcai Chen*

**Main category:** cs.CL

**Keywords:** continual learning, large language models, catastrophic forgetting, General Sample Replay, threshold-based margin loss

**Relevance Score:** 8

**TL;DR:** This paper introduces General Sample Replay (GeRe), a framework to enhance the continual learning capabilities of large language models (LLMs) while mitigating catastrophic forgetting.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the continual learning of LLMs and reduce catastrophic forgetting, which impacts their general capabilities and task performance.

**Method:** The GeRe framework uses pretraining texts for anti-forgetting and employs an enhanced activation states constrained optimization method with threshold-based margin (TM) loss to maintain activation state consistency during replay learning.

**Key Contributions:**

	1. Introduction of General Sample Replay (GeRe) framework for LLMs
	2. Validation that a small set of pre-collected samples suffices for mitigating catastrophic forgetting
	3. Enhanced activation states constrained optimization using TM loss.

**Result:** Experiments show that TM improves performance and robustness compared to various replay strategies, demonstrating that a small set of replay samples can retain general capabilities and enhance performance in sequential tasks.

**Limitations:** 

**Conclusion:** GeRe offers a promising approach for efficient replay of LLMs, ensuring both general capabilities and task performance are preserved during continual learning.

**Abstract:** The continual learning capability of large language models (LLMs) is crucial for advancing artificial general intelligence. However, continual fine-tuning LLMs across various domains often suffers from catastrophic forgetting, characterized by: 1) significant forgetting of their general capabilities, and 2) sharp performance declines in previously learned tasks. To simultaneously address both issues in a simple yet stable manner, we propose General Sample Replay (GeRe), a framework that use usual pretraining texts for efficient anti-forgetting. Beyond revisiting the most prevalent replay-based practices under GeRe, we further leverage neural states to introduce a enhanced activation states constrained optimization method using threshold-based margin (TM) loss, which maintains activation state consistency during replay learning. We are the first to validate that a small, fixed set of pre-collected general replay samples is sufficient to resolve both concerns--retaining general capabilities while promoting overall performance across sequential tasks. Indeed, the former can inherently facilitate the latter. Through controlled experiments, we systematically compare TM with different replay strategies under the GeRe framework, including vanilla label fitting, logit imitation via KL divergence and feature imitation via L1/L2 losses. Results demonstrate that TM consistently improves performance and exhibits better robustness. Our work paves the way for efficient replay of LLMs for the future. Our code and data are available at https://github.com/Qznan/GeRe.

</details>


### [95] [FaST: Feature-aware Sampling and Tuning for Personalized Preference Alignment with Limited Data](https://arxiv.org/abs/2508.04698)

*Thibaut Thonet, Germ√°n Kruszewski, Jos Rozen, Pierre Erbacher, Marc Dymetman*

**Main category:** cs.CL

**Keywords:** LLM personalization, preference alignment, limited data

**Relevance Score:** 9

**TL;DR:** This paper addresses LLM personalization with limited user data, proposing a novel approach called FaST.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to personalize LLM-powered conversational assistants to better meet individual user preferences and overcome the limitations of a one-size-fits-all approach.

**Method:** Introduction of two datasets (DnD and ELIP) and benchmarking various alignment techniques, focusing on a parameter-efficient method named FaST.

**Key Contributions:**

	1. Introduced two unique datasets (DnD and ELIP) for evaluating personalization techniques.
	2. Proposed the FaST approach for effective LLM personalization with limited preference data.
	3. Benchmarking various alignment techniques to identify optimal performance.

**Result:** FaST outperforms other methods by efficiently utilizing high-level features from limited user preference data.

**Limitations:** 

**Conclusion:** The research highlights the potential of personalized LLMs with minimal data and introduces effective techniques for achieving this.

**Abstract:** LLM-powered conversational assistants are often deployed in a one-size-fits-all manner, which fails to accommodate individual user preferences. Recently, LLM personalization -- tailoring models to align with specific user preferences -- has gained increasing attention as a way to bridge this gap. In this work, we specifically focus on a practical yet challenging setting where only a small set of preference annotations can be collected per user -- a problem we define as Personalized Preference Alignment with Limited Data (PPALLI). To support research in this area, we introduce two datasets -- DnD and ELIP -- and benchmark a variety of alignment techniques on them. We further propose FaST, a highly parameter-efficient approach that leverages high-level features automatically discovered from the data, achieving the best overall performance.

</details>


### [96] [Hop, Skip, and Overthink: Diagnosing Why Reasoning Models Fumble during Multi-Hop Analysis](https://arxiv.org/abs/2508.04699)

*Anushka Yadav, Isha Nalawade, Srujana Pillarichety, Yashwanth Babu, Reshmi Ghosh, Samyadeep Basu, Wenlong Zhao, Ali Nasaeh, Sriram Balasubramanian, Soundararajan Srinivasan*

**Main category:** cs.CL

**Keywords:** reasoning models, multi-hop question answering, error categorization

**Relevance Score:** 8

**TL;DR:** This study investigates reasoning failures in language models during multi-hop question answering, introducing a nuanced error categorization framework and uncovering intricate error patterns.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to understand why reasoning models in AI chatbots hallucinate more often than general-purpose language models, particularly in complex multi-step tasks.

**Method:** A novel error categorization framework examines reasoning failures across dimensions such as source document diversity, information coverage, and cognitive inefficiency, supplemented by human annotation and automated metrics.

**Key Contributions:**

	1. Introduction of a nuanced error categorization framework for reasoning failures
	2. Identification of intricate error patterns in multi-hop question answering
	3. Actionable guidance for improving reasoning capabilities in language models

**Result:** The study uncovers intricate error patterns that are typically overlooked by traditional accuracy-focused evaluations, revealing cognitive limitations in current models.

**Limitations:** 

**Conclusion:** The findings provide insights for enhancing the reasoning fidelity, transparency, and robustness of future language models.

**Abstract:** The emergence of reasoning models and their integration into practical AI chat bots has led to breakthroughs in solving advanced math, deep search, and extractive question answering problems that requires a complex and multi-step thought process. Yet, a complete understanding of why these models hallucinate more than general purpose language models is missing. In this investigative study, we systematicallyexplore reasoning failures of contemporary language models on multi-hop question answering tasks. We introduce a novel, nuanced error categorization framework that examines failures across three critical dimensions: the diversity and uniqueness of source documents involved ("hops"), completeness in capturing relevant information ("coverage"), and cognitive inefficiency ("overthinking"). Through rigorous hu-man annotation, supported by complementary automated metrics, our exploration uncovers intricate error patterns often hidden by accuracy-centric evaluations. This investigative approach provides deeper insights into the cognitive limitations of current models and offers actionable guidance toward enhancing reasoning fidelity, transparency, and robustness in future language modeling efforts.

</details>


### [97] [How Well Do LLMs Represent Values Across Cultures? Empirical Analysis of LLM Responses Based on Hofstede Cultural Dimensions](https://arxiv.org/abs/2406.14805)

*Julia Kharchenko, Tanya Roosta, Aman Chadha, Chirag Shah*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cultural Values, Hofstede Dimensions, Culturally Sensitive AI, LLM Training

**Relevance Score:** 9

**TL;DR:** This paper discusses how Large Language Models (LLMs) respond to culturally diverse users and their ability to understand and align with different cultural values based on Hofstede's Cultural Dimensions. It examines LLMs' consistency in providing culturally aligned advice and offers recommendations for training more culturally sensitive models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs can identify and respond according to the diverse cultural values of users based on their countries.

**Method:** Prompting various LLMs with advice requests that reflect five Hofstede Cultural Dimensions while incorporating personas from 36 different countries to analyze the models' cultural comprehension and response consistency.

**Key Contributions:**

	1. Identification of LLMs' inconsistencies in cultural value alignment
	2. Development of a methodology to assess LLMs' cultural understanding
	3. Recommendations for training culturally sensitive LLMs

**Result:** LLMs can differentiate between various cultural values but do not consistently uphold these values in their responses, indicating a gap in cultural sensitivity and alignment.

**Limitations:** The study may not cover all cultural aspects and nuances that could influence user expectations and model responses.

**Conclusion:** The study highlights the shortcomings of LLMs in providing culturally appropriate advice and offers a framework for enhancing value alignment in model training.

**Abstract:** Large Language Models (LLMs) attempt to imitate human behavior by responding to humans in a way that pleases them, including by adhering to their values. However, humans come from diverse cultures with different values. It is critical to understand whether LLMs showcase different values to the user based on the stereotypical values of a user's known country. We prompt different LLMs with a series of advice requests based on 5 Hofstede Cultural Dimensions -- a quantifiable way of representing the values of a country. Throughout each prompt, we incorporate personas representing 36 different countries and, separately, languages predominantly tied to each country to analyze the consistency in the LLMs' cultural understanding. Through our analysis of the responses, we found that LLMs can differentiate between one side of a value and another, as well as understand that countries have differing values, but will not always uphold the values when giving advice, and fail to understand the need to answer differently based on different cultural values. Rooted in these findings, we present recommendations for training value-aligned and culturally sensitive LLMs. More importantly, the methodology and the framework developed here can help further understand and mitigate culture and language alignment issues with LLMs.

</details>


### [98] [Fairness Definitions in Language Models Explained](https://arxiv.org/abs/2407.18454)

*Avash Palikhe, Zichong Wang, Zhipeng Yin, Wenbin Zhang*

**Main category:** cs.CL

**Keywords:** Language Models, fairness, Natural Language Processing, bias, taxonomy

**Relevance Score:** 8

**TL;DR:** This paper surveys fairness definitions in Language Models (LMs) and proposes a novel taxonomy categorizing these concepts based on transformer architecture.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing performance of LMs in NLP tasks is accompanied by the inheritance and amplification of societal biases, necessitating a systematic exploration of fairness in LMs.

**Method:** The paper provides an overview of existing fairness notions in LMs and introduces a taxonomy for categorizing these concepts based on LM architecture: encoder-only, decoder-only, and encoder-decoder.

**Key Contributions:**

	1. Systematic survey of fairness definitions in LMs
	2. Novel taxonomy categorizing fairness notions based on LM architecture
	3. Experiments illustrating practical implications of fairness definitions

**Result:** Experiments illustrating each fairness definition showcase their practical implications and outcomes, identifying current research challenges and open questions.

**Limitations:** 

**Conclusion:** The survey aims to clarify fairness concepts in LMs and foster innovative ideas to advance research in this area.

**Abstract:** Language Models (LMs) have demonstrated exceptional performance across various Natural Language Processing (NLP) tasks. Despite these advancements, LMs can inherit and amplify societal biases related to sensitive attributes such as gender and race, limiting their adoption in real-world applications. Therefore, fairness has been extensively explored in LMs, leading to the proposal of various fairness notions. However, the lack of clear agreement on which fairness definition to apply in specific contexts and the complexity of understanding the distinctions between these definitions can create confusion and impede further progress. To this end, this paper proposes a systematic survey that clarifies the definitions of fairness as they apply to LMs. Specifically, we begin with a brief introduction to LMs and fairness in LMs, followed by a comprehensive, up-to-date overview of existing fairness notions in LMs and the introduction of a novel taxonomy that categorizes these concepts based on their transformer architecture: encoder-only, decoder-only, and encoder-decoder LMs. We further illustrate each definition through experiments, showcasing their practical implications and outcomes. Finally, we discuss current research challenges and open questions, aiming to foster innovative ideas and advance the field. The repository is publicly available online at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/definitions.

</details>


### [99] [Parse Trees Guided LLM Prompt Compression](https://arxiv.org/abs/2409.15395)

*Wenhao Mao, Chengbin Hou, Tianyu Zhang, Xinyu Lin, Ke Tang, Hairong Lv*

**Main category:** cs.CL

**Keywords:** Large Language Models, prompt compression, parse trees, linguistic rules, information entropy

**Relevance Score:** 9

**TL;DR:** PartPrompt is a novel selective compression method for prompts that uses parse trees and local information entropy to enhance the performance of Large Language Models while minimizing computational costs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issues of increased computational costs and prompt length limits in LLMs when providing rich contexts, while improving prompt compression methods that have limitations in hallucination and structural consideration.

**Method:** PartPrompt employs linguistic rules to create parse trees for sentences, calculates local information entropy for tree nodes, organizes these into a global tree based on hierarchical structures, and uses propagation techniques to adjust node values before pruning the tree with a recursive algorithm.

**Key Contributions:**

	1. Introduction of a novel selective compression method for LLM prompts called PartPrompt.
	2. The use of linguistic rules for parse trees to enhance structural understanding in compression.
	3. Demonstration of state-of-the-art performance and coherence in compressed prompts against benchmarks.

**Result:** PartPrompt achieves state-of-the-art performance across various datasets and metrics, demonstrating effective compression ratios and maintaining coherence in compressed prompts, particularly in scenarios with extremely long prompts.

**Limitations:** 

**Conclusion:** The experimental results validate the design choices made in PartPrompt, affirming its superiority over existing compression approaches and highlighting its practicality for LLMs in different contexts.

**Abstract:** Offering rich contexts to Large Language Models (LLMs) has shown to boost the performance in various tasks, but the resulting longer prompt would increase the computational cost and might exceed the input limit of LLMs. Recently, some prompt compression methods have been suggested to shorten the length of prompts by using language models to generate shorter prompts or by developing computational models to select important parts of original prompt. The generative compression methods would suffer from issues like hallucination, while the selective compression methods have not involved linguistic rules and overlook the global structure of prompt. To this end, we propose a novel selective compression method called PartPrompt. It first obtains a parse tree for each sentence based on linguistic rules, and calculates local information entropy for each node in a parse tree. These local parse trees are then organized into a global tree according to the hierarchical structure such as the dependency of sentences, paragraphs, and sections. After that, the root-ward propagation and leaf-ward propagation are proposed to adjust node values over the global tree. Finally, a recursive algorithm is developed to prune the global tree based on the adjusted node values. The experiments show that PartPrompt receives the state-of-the-art performance across various datasets, metrics, compression ratios, and target LLMs for inference. The in-depth ablation studies confirm the effectiveness of designs in PartPrompt, and other additional experiments also demonstrate its superiority in terms of the coherence of compressed prompts and in the extreme long prompt scenario.

</details>


### [100] [Human Bias in the Face of AI: Examining Human Judgment Against Text Labeled as AI Generated](https://arxiv.org/abs/2410.03723)

*Tiffany Zhu, Iain Weissburg, Kexun Zhang, William Yang Wang*

**Main category:** cs.CL

**Keywords:** AI content generation, human bias, human-AI interaction, perception study, creative fields

**Relevance Score:** 8

**TL;DR:** This study investigates how human biases affect the perception of AI-generated versus human-generated content, revealing a significant preference for human-generated materials.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how biases influence human trust in AI-generated content as AI technology advances.

**Method:** Three experiments were conducted involving text rephrasing, news article summarization, and persuasive writing to assess human raters' preferences for AI versus human-generated content.

**Key Contributions:**

	1. Revealed significant human preference bias against AI-generated content
	2. Demonstrated that labeling can substantially influence perceptions of credibility
	3. Provided insights for improving human-AI collaboration in creative domains.

**Result:** Human raters favored content labeled as 'Human Generated' over 'AI Generated' by more than 30%, despite the inability to differentiate between them in blind tests.

**Limitations:** The study primarily focused on text generation without exploring other types of AI-generated content.

**Conclusion:** The findings underline the limitations of human judgment in evaluating AI content and suggest ways to enhance human-AI collaboration in creative tasks.

**Abstract:** As AI advances in text generation, human trust in AI generated content remains constrained by biases that go beyond concerns of accuracy. This study explores how bias shapes the perception of AI versus human generated content. Through three experiments involving text rephrasing, news article summarization, and persuasive writing, we investigated how human raters respond to labeled and unlabeled content. While the raters could not differentiate the two types of texts in the blind test, they overwhelmingly favored content labeled as "Human Generated," over those labeled "AI Generated," by a preference score of over 30%. We observed the same pattern even when the labels were deliberately swapped. This human bias against AI has broader societal and cognitive implications, as it undervalues AI performance. This study highlights the limitations of human judgment in interacting with AI and offers a foundation for improving human-AI collaboration, especially in creative fields.

</details>


### [101] [A Survey of Conversational Search](https://arxiv.org/abs/2410.15576)

*Fengran Mo, Kelong Mao, Ziliang Zhao, Hongjin Qian, Haonan Chen, Yiruo Cheng, Xiaoxi Li, Yutao Zhu, Zhicheng Dou, Jian-Yun Nie*

**Main category:** cs.CL

**Keywords:** conversational search, natural language processing, large language models, information retrieval, user experience

**Relevance Score:** 9

**TL;DR:** This survey explores advancements in conversational search systems that utilize large language models (LLMs) to facilitate complex, context-aware information retrieval through natural language dialogue.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing role of AI and NLP technologies in enhancing search engine capabilities to create intuitive user interactions and improve information retrieval.

**Method:** This paper surveys existing research on conversational search systems, examining critical components such as query reformulation, search clarification, and conversational retrieval, while assessing the role of LLMs in these systems.

**Key Contributions:**

	1. Analyzes key components of conversational search systems.
	2. Discusses the integration of LLMs and their impact on search capabilities.
	3. Highlights real-world applications and evaluation metrics of conversational search.

**Result:** The survey identifies key advancements in conversational search, discusses integration challenges and opportunities with LLMs, and presents insights into real-world applications and performance evaluations of current systems.

**Limitations:** 

**Conclusion:** The paper aims to guide future research and development in conversational search by highlighting trends, challenges, and practical applications in the field.

**Abstract:** As a cornerstone of modern information access, search engines have become indispensable in everyday life. With the rapid advancements in AI and natural language processing (NLP) technologies, particularly large language models (LLMs), search engines have evolved to support more intuitive and intelligent interactions between users and systems. Conversational search, an emerging paradigm for next-generation search engines, leverages natural language dialogue to facilitate complex and precise information retrieval, thus attracting significant attention. Unlike traditional keyword-based search engines, conversational search systems enhance user experience by supporting intricate queries, maintaining context over multi-turn interactions, and providing robust information integration and processing capabilities. Key components such as query reformulation, search clarification, conversational retrieval, and response generation work in unison to enable these sophisticated interactions. In this survey, we explore the recent advancements and potential future directions in conversational search, examining the critical modules that constitute a conversational search system. We highlight the integration of LLMs in enhancing these systems and discuss the challenges and opportunities that lie ahead in this dynamic field. Additionally, we provide insights into real-world applications and robust evaluations of current conversational search systems, aiming to guide future research and development in conversational search.

</details>


### [102] [AUTALIC: A Dataset for Anti-AUTistic Ableist Language In Context](https://arxiv.org/abs/2410.16520)

*Naba Rizvi, Harper Strickland, Daniel Gitelman, Tristan Cooper, Alexis Morales-Flores, Michael Golden, Aekta Kallepalli, Akshat Alurkar, Haaset Owens, Saleha Ahmedi, Isha Khirwadkar, Imani Munyaka, Nedjma Ousidhoum*

**Main category:** cs.CL

**Keywords:** anti-autistic ableism, NLP, neurodiversity, benchmark dataset, language models

**Relevance Score:** 7

**TL;DR:** The paper introduces AUTALIC, a benchmark dataset for detecting anti-autistic ableist language in NLP, aiming to improve nuanced understanding in context.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address underexplored aspects of ableist language towards autistic individuals and improve NLP tools' sensitivity to such language.

**Method:** The study involves the creation of AUTALIC, a dataset of 2,400 autism-related sentences sourced from Reddit and annotated by experts in neurodiversity.

**Key Contributions:**

	1. First dataset dedicated to anti-autistic ableist language detection
	2. Evaluation highlights limitations of current LLMs in understanding context
	3. Public release of AUTALIC to aid future research in ableism and neurodiversity

**Result:** Evaluation shows that current language models, including advanced LLMs, struggle to detect anti-autistic ableism accurately, indicating a gap in their performance relative to human judgment.

**Limitations:** Current NLP tools fail to accurately identify the nuanced expressions of ableist language.

**Conclusion:** AUTALIC serves as a vital resource for developing more context-aware NLP systems, emphasizing a need for sensitivity towards neurodiversity.

**Abstract:** As our understanding of autism and ableism continues to increase, so does our understanding of ableist language towards autistic people. Such language poses a significant challenge in NLP research due to its subtle and context-dependent nature. Yet, detecting anti-autistic ableist language remains underexplored, with existing NLP tools often failing to capture its nuanced expressions. We present AUTALIC, the first benchmark dataset dedicated to the detection of anti-autistic ableist language in context, addressing a significant gap in the field. The dataset comprises 2,400 autism-related sentences collected from Reddit, accompanied by surrounding context, and is annotated by trained experts with backgrounds in neurodiversity. Our comprehensive evaluation reveals that current language models, including state-of-the-art LLMs, struggle to reliably identify anti-autistic ableism and align with human judgments, underscoring their limitations in this domain. We publicly release AUTALIC along with the individual annotations which serve as a valuable resource to researchers working on ableism, neurodiversity, and also studying disagreements in annotation tasks. This dataset serves as a crucial step towards developing more inclusive and context-aware NLP systems that better reflect diverse perspectives.

</details>


### [103] [CLaSP: Learning Concepts for Time-Series Signals from Natural Language Supervision](https://arxiv.org/abs/2411.08397)

*Aoi Ito, Kota Dohi, Yohei Kawaguchi*

**Main category:** cs.CL

**Keywords:** time-series retrieval, natural language queries, contrastive learning, large language models, industrial diagnostics

**Relevance Score:** 7

**TL;DR:** CLaSP is a model for retrieving time-series signals based on natural language queries, enhancing scalability and accuracy by using contrastive learning and large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient retrieval of time-series signals based on descriptive queries in fields like industrial diagnostics.

**Method:** CLaSP employs contrastive learning to map time-series signals to natural language descriptions, eliminating reliance on predefined synonym dictionaries.

**Key Contributions:**

	1. Introduces a novel retrieval model for time-series signals using natural language queries.
	2. Eliminates the need for predefined synonym dictionaries.
	3. Achieves high accuracy in various signal retrieval tasks.

**Result:** CLaSP demonstrates high accuracy in retrieving various time series patterns using natural language queries with the TRUCE and SUSHI datasets.

**Limitations:** 

**Conclusion:** CLaSP provides a scalable and adaptable approach to time-series signal retrieval that leverages the capabilities of LLMs.

**Abstract:** This paper presents CLaSP, a novel model for retrieving time-series signals using natural language queries that describe signal characteristics. The ability to search time-series signals based on descriptive queries is essential in domains such as industrial diagnostics, where data scientists often need to find signals with specific characteristics. However, existing methods rely on sketch-based inputs, predefined synonym dictionaries, or domain-specific manual designs, limiting their scalability and adaptability. CLaSP addresses these challenges by employing contrastive learning to map time-series signals to natural language descriptions. Unlike prior approaches, it eliminates the need for predefined synonym dictionaries and leverages the rich contextual knowledge of large language models (LLMs). Using the TRUCE and SUSHI datasets, which pair time-series signals with natural language descriptions, we demonstrate that CLaSP achieves high accuracy in retrieving a variety of time series patterns based on natural language queries.

</details>


### [104] [FactEHR: A Dataset for Evaluating Factuality in Clinical Notes Using LLMs](https://arxiv.org/abs/2412.12422)

*Monica Munnangi, Akshay Swaminathan, Jason Alan Fries, Jenelle Jindal, Sanjana Narayanan, Ivan Lopez, Lucia Tu, Philip Chung, Jesutofunmi A. Omiye, Mehr Kashyap, Nigam Shah*

**Main category:** cs.CL

**Keywords:** fact decomposition, large language models, healthcare, dataset, fact verification

**Relevance Score:** 9

**TL;DR:** The paper introduces FactEHR, a dataset for fact decomposition in clinical documentation, essential for verifying factual claims in healthcare using LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Verifying factual claims in healthcare LLM applications is critical yet challenging due to the complexity of clinical texts.

**Method:** FactEHR, an NLI dataset containing 2,168 clinical notes and 987,266 entailment pairs, was created to facilitate fine-grained fact verification and assess LLM performance on this task.

**Key Contributions:**

	1. Introduction of the FactEHR dataset for clinical fact decomposition
	2. Assessment of LLM performance variability in generating accurate clinical facts
	3. Insights into challenges posed by clinical documentation for LLM fact decomposition

**Result:** The evaluation shows significant variability in LLM performance, highlighting that some models like Gemini-1.5-Flash excel, while others like Llama-3 8B underperform in generating accurate facts.

**Limitations:** The study is limited to a specific set of clinical notes and may not generalize across all healthcare contexts.

**Conclusion:** Improving LLM capabilities for fact verification in clinical texts is necessary to ensure safe healthcare applications of these technologies.

**Abstract:** Verifying and attributing factual claims is essential for the safe and effective use of large language models (LLMs) in healthcare. A core component of factuality evaluation is fact decomposition, the process of breaking down complex clinical statements into fine-grained atomic facts for verification. Recent work has proposed fact decomposition, which uses LLMs to rewrite source text into concise sentences conveying a single piece of information, to facilitate fine-grained fact verification. However, clinical documentation poses unique challenges for fact decomposition due to dense terminology and diverse note types and remains understudied. To address this gap and explore these challenges, we present FactEHR, an NLI dataset consisting of document fact decompositions for 2,168 clinical notes spanning four types from three hospital systems, resulting in 987,266 entailment pairs. We assess the generated facts on different axes, from entailment evaluation of LLMs to a qualitative analysis. Our evaluation, including review by the clinicians, reveals substantial variability in LLM performance for fact decomposition. For example, Gemini-1.5-Flash consistently generates relevant and accurate facts, while Llama-3 8B produces fewer and less consistent outputs. The results underscore the need for better LLM capabilities to support factual verification in clinical text.

</details>


### [105] [Evaluating the Robustness of Multimodal Agents Against Active Environmental Injection Attacks](https://arxiv.org/abs/2502.13053)

*Yurun Chen, Xavier Hu, Keting Yin, Juncheng Li, Shengyu Zhang*

**Main category:** cs.CL

**Keywords:** Active Environment Injection Attack, AI security, Android OS, MLLM vulnerability, adversarial attacks

**Relevance Score:** 6

**TL;DR:** The paper introduces the Active Environment Injection Attack (AEIA) targeting AI agents in Android OS, revealing how adversaries can manipulate agent decision-making through environmental disguises.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight a critical security concern in AI agents' decision-making processes within operating systems, focusing on vulnerability to deceptive environmental elements.

**Method:** A risk assessment is conducted on Android OS to define AEIA and assess its impact, along with the proposal of AEIA-MN, an attack scheme exploiting interaction vulnerabilities.

**Key Contributions:**

	1. Definition of Active Environment Injection Attack (AEIA)
	2. Identification of critical security vulnerabilities in multimodal interfaces
	3. Proposal of AEIA-MN attack scheme for evaluating MLLM-based agents

**Result:** Experimental results demonstrate a high vulnerability of MLLM-based agents to AEIA, achieving up to a 93% attack success rate on the AndroidWorld benchmark.

**Limitations:** The focus is primarily on Android OS, and the proposed vulnerabilities may not be generalizable across all platforms or environments.

**Conclusion:** The study underscores the significant risks posed by AEIA to AI agents and the need for enhanced security measures in multimodal interaction interfaces.

**Abstract:** As researchers continue to optimize AI agents for more effective task execution within operating systems, they often overlook a critical security concern: the ability of these agents to detect "impostors" within their environment. Through an analysis of the agents' operational context, we identify a significant threat-attackers can disguise malicious attacks as environmental elements, injecting active disturbances into the agents' execution processes to manipulate their decision-making. We define this novel threat as the Active Environment Injection Attack (AEIA). Focusing on the interaction mechanisms of the Android OS, we conduct a risk assessment of AEIA and identify two critical security vulnerabilities: (1) Adversarial content injection in multimodal interaction interfaces, where attackers embed adversarial instructions within environmental elements to mislead agent decision-making; and (2) Reasoning gap vulnerabilities in the agent's task execution process, which increase susceptibility to AEIA attacks during reasoning. To evaluate the impact of these vulnerabilities, we propose AEIA-MN, an attack scheme that exploits interaction vulnerabilities in mobile operating systems to assess the robustness of MLLM-based agents. Experimental results show that even advanced MLLMs are highly vulnerable to this attack, achieving a maximum attack success rate of 93% on the AndroidWorld benchmark by combining two vulnerabilities.

</details>


### [106] [Evaluating Robustness of LLMs in Question Answering on Multilingual Noisy OCR Data](https://arxiv.org/abs/2502.16781)

*Bhawna Piryani, Jamshid Mozafari, Abdelrahman Abdallah, Antoine Doucet, Adam Jatowt*

**Main category:** cs.CL

**Keywords:** Optical Character Recognition, Multilingual QA, Large Language Models, OCR noise, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** This paper analyzes the impact of OCR errors on Multilingual QA Systems and introduces a dataset, MultiOCR-QA, with 50K question-answer pairs from OCR-ed historical documents.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the significant impact of Optical Character Recognition (OCR) errors on the performance of question-answering (QA) systems in the context of multilingual and historical document digitization.

**Method:** The authors create a multilingual QA dataset, MultiOCR-QA, with 50,000 curated question-answer pairs from OCR-ed documents in English, French, and German, and evaluate several state-of-the-art Large Language Models (LLMs) under various OCR error conditions.

**Key Contributions:**

	1. Introduction of the MultiOCR-QA dataset
	2. Comprehensive analysis of the effects of OCR errors on QA systems
	3. Insights into the limitations of state-of-the-art LLMs under noisy conditions

**Result:** The evaluation reveals that QA systems are very susceptible to OCR errors, which leads to poor performance when working with noisy OCR text, emphasizing the inadequacy of current models in handling such challenges.

**Limitations:** The dataset may not cover all types of OCR noise or historical document variability, potentially limiting its applicability in some real-world scenarios.

**Conclusion:** The paper concludes that there is a pressing need for the development of more noise-resilient QA systems, particularly aimed at improving the handling of OCR-induced errors in historical document digitization.

**Abstract:** Optical Character Recognition (OCR) plays a crucial role in digitizing historical and multilingual documents, yet OCR errors - imperfect extraction of text, including character insertion, deletion, and substitution can significantly impact downstream tasks like question-answering (QA). In this work, we conduct a comprehensive analysis of how OCR-induced noise affects the performance of Multilingual QA Systems. To support this analysis, we introduce a multilingual QA dataset MultiOCR-QA, comprising 50K question-answer pairs across three languages, English, French, and German. The dataset is curated from OCR-ed historical documents, which include different levels and types of OCR noise. We then evaluate how different state-of-the-art Large Language models (LLMs) perform under different error conditions, focusing on three major OCR error types. Our findings show that QA systems are highly prone to OCR-induced errors and perform poorly on noisy OCR text. By comparing model performance on clean versus noisy texts, we provide insights into the limitations of current approaches and emphasize the need for more noise-resilient QA systems in historical digitization contexts.

</details>


### [107] [Assessing Agentic Large Language Models in Multilingual National Bias](https://arxiv.org/abs/2502.17945)

*Qianying Liu, Katrina Qiyao Wang, Fei Cheng, Sadao Kurohashi*

**Main category:** cs.CL

**Keywords:** Multilingual NLP, Large Language Models, Bias, Reasoning Strategies, Personalized Advice

**Relevance Score:** 8

**TL;DR:** This study investigates multilingual bias in Large Language Models (LLMs) by analyzing their performance in providing personalized advice across different languages. It highlights the significant local language bias present and the varying capabilities of different LLMs.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding how Large Language Models handle multilingual reasoning and recommendations, especially in the context of biases that emerge across languages.

**Method:** The study tests LLMs in three scenarios: university applications, travel, and relocation, analyzing their responses in multiple languages and measuring bias based on demographic factors and reasoning strategies.

**Key Contributions:**

	1. First study to analyze multilingual bias in LLMs across decision-making tasks.
	2. Quantifies bias in model-generated scores with respect to demographic factors.
	3. Demonstrates differential performance of GPT-4 versus GPT-3.5 in multilingual contexts.

**Result:** Findings indicate that local language bias is common across tasks, with newer models like GPT-4 showing less bias for English compared to GPT-3.5 but lacking in robust multilingual alignment.

**Limitations:** Focus is primarily on specific decision-making tasks; broader implications beyond the tested scenarios need further exploration.

**Conclusion:** The results reveal critical insights into how biases manifest in LLM outputs, which has important implications for the deployment of multilingual AI systems, especially in educational contexts.

**Abstract:** Large Language Models have garnered significant attention for their capabilities in multilingual natural language processing, while studies on risks associated with cross biases are limited to immediate context preferences. Cross-language disparities in reasoning-based recommendations remain largely unexplored, with a lack of even descriptive analysis. This study is the first to address this gap. We test LLM's applicability and capability in providing personalized advice across three key scenarios: university applications, travel, and relocation. We investigate multilingual bias in state-of-the-art LLMs by analyzing their responses to decision-making tasks across multiple languages. We quantify bias in model-generated scores and assess the impact of demographic factors and reasoning strategies (e.g., Chain-of-Thought prompting) on bias patterns. Our findings reveal that local language bias is prevalent across different tasks, with GPT-4 and Sonnet reducing bias for English-speaking countries compared to GPT-3.5 but failing to achieve robust multilingual alignment, highlighting broader implications for multilingual AI agents and applications such as education. \footnote{Code available at: https://github.com/yiyunya/assess_agentic_national_bias

</details>


### [108] [The Impact of Item-Writing Flaws on Difficulty and Discrimination in Item Response Theory](https://arxiv.org/abs/2503.10533)

*Robin Schmucker, Steven Moore*

**Main category:** cs.CL

**Keywords:** Item Response Theory, Item-Writing Flaw, educational assessment, multiple-choice questions, STEM

**Relevance Score:** 4

**TL;DR:** The study explores the use of Item-Writing Flaw (IWF) rubrics to evaluate multiple-choice questions in STEM subjects, establishing links between IWF features and Item Response Theory (IRT) parameters.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To find an efficient, scalable method for evaluating test items that does not rely on time-consuming pilot testing, while also enhancing the predictive validity of IWF rubrics in relation to IRT.

**Method:** Analyzed 7,126 multiple-choice questions by annotating them with a 19-criteria IWF rubric and examined the relationships to IRT difficulty and discrimination parameters.

**Key Contributions:**

	1. Establishes a connection between IWF features and IRT parameters in educational assessments.
	2. Demonstrates the practical application of automated IWF analysis for efficient test item evaluation.
	3. Identifies specific IWF criteria that notably affect item quality and difficulty.

**Result:** Statistically significant correlations were found between the number of IWFs and IRT parameters, demonstrating how specific flaws affect item quality and challenge.

**Limitations:** The study primarily focuses on STEM subjects, indicating that results may not generalize across all domains without further research.

**Conclusion:** Automated IWF analysis can effectively supplement traditional validation methods, aiding in the initial screening of multiple-choice questions and highlighting the need for further research in this area.

**Abstract:** High-quality test items are essential for educational assessments, particularly within Item Response Theory (IRT). Traditional validation methods rely on resource-intensive pilot testing to estimate item difficulty and discrimination. More recently, Item-Writing Flaw (IWF) rubrics emerged as a domain-general approach for evaluating test items based on textual features. This method offers a scalable, pre-deployment evaluation without requiring student data, but its predictive validity concerning empirical IRT parameters is underexplored. To address this gap, we conducted a study involving 7,126 multiple-choice questions across various STEM subjects (physical science, mathematics, and life/earth sciences). Using an automated approach, we annotated each question with a 19-criteria IWF rubric and studied relationships to data-driven IRT parameters. Our analysis revealed statistically significant links between the number of IWFs and IRT difficulty and discrimination parameters, particularly in life/earth and physical science domains. We further observed how specific IWF criteria can impact item quality more and less severely (e.g., negative wording vs. implausible distractors) and how they might make a question more or less challenging. Overall, our findings establish automated IWF analysis as a valuable supplement to traditional validation, providing an efficient method for initial item screening, particularly for flagging low-difficulty MCQs. Our findings show the need for further research on domain-general evaluation rubrics and algorithms that understand domain-specific content for robust item validation.

</details>


### [109] [I Have Covered All the Bases Here: Interpreting Reasoning Features in Large Language Models via Sparse Autoencoders](https://arxiv.org/abs/2503.18878)

*Andrey Galichin, Alexey Dontsov, Polina Druzhinina, Anton Razzhigaev, Oleg Y. Rogov, Elena Tutubalina, Ivan Oseledets*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Mechanisms, Sparse Autoencoders, Human-Computer Interaction, Neural Network Interpretation

**Relevance Score:** 8

**TL;DR:** This paper explores the internal reasoning mechanisms of LLMs like DeepSeek-R1 using Sparse Autoencoders and introduces the ReasonScore metric.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the unexplored internal reasoning processes of LLMs and understand their mechanisms through human-interpretable features.

**Method:** The study employs Sparse Autoencoders to decompose neural network activations and utilizes the ReasonScore metric to identify active features during reasoning moments.

**Key Contributions:**

	1. Introduction of Sparse Autoencoders for LLM interpretation
	2. Development of ReasonScore metric for identifying reasoning features
	3. Demonstration of enhanced LLM performance through feature amplification

**Result:** The experiments show that amplifying identified features enhances performance on reasoning benchmarks by +2.2% and increases reasoning trace length by +20.5%.

**Limitations:** 

**Conclusion:** This research marks the first step towards understanding LLM reasoning mechanisms, with evidence of specific features tied to reasoning capabilities.

**Abstract:** Recent LLMs like DeepSeek-R1 have demonstrated state-of-the-art performance by integrating deep thinking and complex reasoning during generation. However, the internal mechanisms behind these reasoning processes remain unexplored. We observe reasoning LLMs consistently use vocabulary associated with human reasoning processes. We hypothesize these words correspond to specific reasoning moments within the models' internal mechanisms. To test this hypothesis, we employ Sparse Autoencoders (SAEs), a technique for sparse decomposition of neural network activations into human-interpretable features. We introduce ReasonScore, an automatic metric to identify active SAE features during these reasoning moments. We perform manual and automatic interpretation of the features detected by our metric, and find those with activation patterns matching uncertainty, exploratory thinking, and reflection. Through steering experiments, we demonstrate that amplifying these features increases performance on reasoning-intensive benchmarks (+2.2%) while producing longer reasoning traces (+20.5%). Using the model diffing technique, we provide evidence that these features are present only in models with reasoning capabilities. Our work provides the first step towards a mechanistic understanding of reasoning in LLMs. Code available at https://github.com/AIRI-Institute/SAE-Reasoning

</details>


### [110] [CRAB: A Benchmark for Evaluating Curation of Retrieval-Augmented LLMs in Biomedicine](https://arxiv.org/abs/2504.12342)

*Hanmeng Zhong, Linqing Chen, Wentao Wu, Weilei Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented LLMs, Biomedical Curation, Benchmarking, Curation Performance, Citation-based Evaluation

**Relevance Score:** 9

**TL;DR:** The paper introduces CRAB, the first multilingual benchmark for evaluating the curation ability of retrieval-augmented LLMs in biomedicine, highlighting performance discrepancies among mainstream models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the curation ability of retrieval-augmented LLMs in biomedicine, addressing a critical gap in existing methodologies.

**Method:** The paper presents CRAB, a multilingual benchmark that uses a novel citation-based evaluation metric to assess the performance of retrieval-augmented LLMs in selecting and integrating relevant biomedical references.

**Key Contributions:**

	1. First multilingual benchmark for biomedical curation of retrieval-augmented LLMs
	2. Introduces a novel citation-based evaluation metric
	3. Exposes performance discrepancies in existing mainstream LLMs

**Result:** Experimental results show significant variations in curation performance among mainstream retrieval-augmented LLMs, indicating that many need improvement.

**Limitations:** 

**Conclusion:** There is an urgent need to enhance the curation capabilities of LLMs in the biomedical domain, as demonstrated by the results from CRAB.

**Abstract:** Recent development in Retrieval-Augmented Large Language Models (LLMs) have shown great promise in biomedical applications. How ever, a critical gap persists in reliably evaluating their curation ability the process by which models select and integrate relevant references while filtering out noise. To address this, we introduce the benchmark for Curation of Retrieval-Augmented LLMs in Biomedicine (CRAB), the first multilingual benchmark tailored for evaluating the biomedical curation of retrieval-augmented LLMs, available in English, French, German and Chinese. By incorporating a novel citation-based evaluation metric, CRAB quantifies the curation performance of retrieval-augmented LLMs in biomedicine. Experimental results reveal significant discrepancies in the curation performance of mainstream LLMs, underscoring the urgent need to improve it in the domain of biomedicine. Our dataset is available at https://huggingface.co/datasets/zhm0/CRAB.

</details>
