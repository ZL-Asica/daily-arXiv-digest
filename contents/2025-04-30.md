# 2025-04-30

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 16]

- [cs.CL](#cs.CL) [Total: 68]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Online Safety for All: Sociocultural Insights from a Systematic Review of Youth Online Safety in the Global South](https://arxiv.org/abs/2504.20308)

*Ozioma C. Oguine, Oghenemaro Anuyah, Zainab Agha, Iris Melgarez, Adriana Alvarado Garcia, Karla Badillo-Urquiola*

**Main category:** cs.HC

**Keywords:** youth online safety, Global South, cultural contexts, cyberbullying, inclusive research

**Relevance Score:** 4

**TL;DR:** This paper reviews youth online safety research in the Global South, highlighting the predominance of quantitative methods and the need for culturally sensitive approaches.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of research on youth online safety in the Global South and emphasize the importance of cultural contexts.

**Method:** Systematic review of 66 studies published between 2014 and 2024, focusing on regions in the Global South.

**Key Contributions:** Systematic review of youth online safety in the Global South, Highlighting the need for culturally sensitive research methodologies, Proposals for future research agenda focusing on diverse sociocultural contexts

**Result:** Findings indicate a focus on Asian countries, with most studies employing quantitative methods and emphasizing risks of cyberbullying while neglecting marginalized youth.

**Limitations:** Limited exploration of marginalized youth populations and reliance on quantitative methods.

**Future Work:** Encouragement of situated, culturally aware research methodologies and inclusivity in youth online safety studies.

**Conclusion:** The paper calls for culturally sensitive methodologies and youth-centered approaches to improve online safety research in the Global South.

**Abstract:** Youth online safety research in HCI has historically centered on perspectives from the Global North, often overlooking the unique particularities and cultural contexts of regions in the Global South. This paper presents a systematic review of 66 youth online safety studies published between 2014 and 2024, specifically focusing on regions in the Global South. Our findings reveal a concentrated research focus in Asian countries and predominance of quantitative methods. We also found limited research on marginalized youth populations and a primary focus on risks related to cyberbullying. Our analysis underscores the critical role of cultural factors in shaping online safety, highlighting the need for educational approaches that integrate social dynamics and awareness. We propose methodological recommendations and a future research agenda that encourages the adoption of situated, culturally sensitive methodologies and youth-centered approaches to researching youth online safety regions in the Global South. This paper advocates for greater inclusivity in youth online safety research, emphasizing the importance of addressing varied sociocultural contexts to better understand and meet the online safety needs of youth in the Global South.

</details>


### [2] ["I've talked to ChatGPT about my issues last night.": Examining Mental Health Conversations with Large Language Models through Reddit Analysis](https://arxiv.org/abs/2504.20320)

*Kyuha Jung, Gyuho Lee, Yuanhui Huang, Yunan Chen*

**Main category:** cs.HC

**Keywords:** large language models, mental health, ChatGPT, user experience, health informatics

**Relevance Score:** 10

**TL;DR:** The paper examines how large language models, particularly ChatGPT, support mental health through Reddit discussions, highlighting their advantages and potential risks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the function of LLMs in facilitating mental health support and the user perception surrounding this technology.

**Method:** Analysis of Reddit posts and comments regarding mental health interactions with ChatGPT.

**Key Contributions:** Insights into user preferences for LLMs in mental health over human support, Identification of innovative uses for LLMs in therapy preparation, Discussion on mitigating risks associated with LLM use in mental health contexts

**Result:** Users appreciate ChatGPT for its accessibility and for providing emotional support, advice, and validation, though concerns about misinformation and privacy arose.

**Limitations:** The study is limited to Reddit conversations and may not reflect broader user experiences with LLMs for mental health.

**Future Work:** Further research on LLMs in diverse mental health settings and strategies for improving accuracy and privacy.

**Conclusion:** While LLMs can enhance mental health support, precautions are necessary to address the associated risks, particularly in clinical settings.

**Abstract:** We investigate the role of large language models (LLMs) in supporting mental health by analyzing Reddit posts and comments about mental health conversations with ChatGPT. Our findings reveal that users value ChatGPT as a safe, non-judgmental space, often favoring it over human support due to its accessibility, availability, and knowledgeable responses. ChatGPT provides a range of support, including actionable advice, emotional support, and validation, while helping users better understand their mental states. Additionally, we found that ChatGPT offers innovative support for individuals facing mental health challenges, such as assistance in navigating difficult conversations, preparing for therapy sessions, and exploring therapeutic interventions. However, users also voiced potential risks, including the spread of incorrect health advice, ChatGPT's overly validating nature, and privacy concerns. We discuss the implications of LLMs as tools for mental health support in both everyday health and clinical therapy settings and suggest strategies to mitigate risks in LLM-powered interactions.

</details>


### [3] [Narrative-Centered Emotional Reflection: Scaffolding Autonomous Emotional Literacy with AI](https://arxiv.org/abs/2504.20342)

*Shou-Tzu Han*

**Main category:** cs.HC

**Keywords:** AI, emotional self-reflection, affective computing, psychological resilience, cognitive reframing

**Relevance Score:** 7

**TL;DR:** Reflexion is an AI platform that facilitates structured emotional self-reflection using real-time emotion detection and metaphorical storytelling, aiming to enhance emotional literacy and psychological resilience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper introduces Reflexion as a tool to support emotional self-reflection and exploration, moving beyond basic emotional recognition.

**Method:** Reflexion integrates real-time emotion detection, layered reflective prompts, and storytelling generation, based on various psychological theories.

**Key Contributions:** Introduction of a novel AI-powered tool for emotional self-reflection., Integration of various psychological theories into the platform's design., Preliminary evidence demonstrating positive emotional and cognitive outcomes.

**Result:** Initial pilot studies indicate improved emotional articulation, cognitive reframing, and perceived psychological resilience among participants.

**Limitations:** The study presents preliminary results and is considered early-stage work intended for future refinement and conference submission.

**Future Work:** Further development and testing of Reflexion to enhance its efficacy and applicability in diverse contexts.

**Conclusion:** Reflexion shows potential as an effective intervention for emotional literacy and psychological growth in various settings.

**Abstract:** Reflexion is an AI-powered platform designed to enable structured emotional self-reflection at scale. By integrating real-time emotion detection, layered reflective prompting, and metaphorical storytelling generation, Reflexion empowers users to engage in autonomous emotional exploration beyond basic sentiment categorization. Grounded in theories of expressive writing, cognitive restructuring, self-determination, and critical consciousness development, the system scaffolds a progressive journey from surface-level emotional recognition toward value-aligned action planning. Initial pilot studies with diverse participants demonstrate positive outcomes in emotional articulation, cognitive reframing, and perceived psychological resilience. Reflexion represents a promising direction for scalable, theory-informed affective computing interventions aimed at fostering emotional literacy and psychological growth across educational, therapeutic, and public health contexts.

</details>


### [4] [Thoughtful, Confused, or Untrustworthy: How Text Presentation Influences Perceptions of AI Writing Tools](https://arxiv.org/abs/2504.20365)

*David Zhou, John R. Gallagher, Sarah Sterman*

**Main category:** cs.HC

**Keywords:** AI writing tools, text generation speed, user perceptions, AI trustworthiness, interface design

**Relevance Score:** 8

**TL;DR:** This paper explores how varying text generation speed in AI writing tools affects users' perceptions of these tools and the quality of generated text.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the effects of AI text presentation speed on users' perceptions and decision-making in AI-assisted writing.

**Method:** An online study with 297 participants analyzed the impact of different text generation speeds in creative and professional writing contexts.

**Key Contributions:** Demonstrated the correlation between text generation speed and user perceptions, Provided insights into the intentional design of AI writing tools, Identified implications for creative and writing processes involving AI

**Result:** The study found that faster text generation speeds enhance perceived humanness and trustworthiness of AI tools, as well as the quality of the generated content.

**Limitations:** The study is limited to specific writing scenarios and may not generalize to all types of AI tools or writing contexts.

**Future Work:** Future research could explore text presentation in different AI applications and its effects on various user demographics and writing styles.

**Conclusion:** Understanding the influence of text presentation speed can lead to better-designed AI writing interfaces that positively affect user interactions and acceptance.

**Abstract:** AI writing tools have been shown to dramatically change the way people write, yet the effects of AI text presentation are not well understood nor always intentionally designed. Although text presentation in existing large language model interfaces is linked to the speed of the underlying model, text presentation speed can impact perceptions of AI systems, potentially influencing whether AI suggestions are accepted or rejected. In this paper, we analyze the effects of varying text generation speed in creative and professional writing scenarios on an online platform (n=297). We find that speed is correlated with perceived humanness and trustworthiness of the AI tool, as well as the perceived quality of the generated text. We discuss its implications on creative and writing processes, along with future steps in the intentional design of AI writing tool interfaces.

</details>


### [5] [Perception-aware Sampling for Scatterplot Visualizations](https://arxiv.org/abs/2504.20369)

*Zafeiria Moumoulidou, Hamza Elhamdadi, Ke Yang, Subrata Mitra, Cindy Xiong Bearfield, Alexandra Meliou*

**Main category:** cs.HC

**Keywords:** Data Visualization, Sampling Methods, Perception-awareness

**Relevance Score:** 6

**TL;DR:** This paper introduces perception-aware sampling methods for scatterplots to enhance data visualization in analytics workflows.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With increasing data sizes, effective visualization becomes challenging, necessitating improved sampling techniques to ensure meaningful data representation.

**Method:** The paper proposes two methods: PAwS, a perception-aware sampling strategy utilizing saliency maps to focus on human attention, and ApproPAwS, an approximate visualization method achieving speed-ups while maintaining visual integrity.

**Key Contributions:** Introduction of perception-augmented databases, Development of PAwS and ApproPAwS sampling methods, Establishment of perceptual similarity as a sample quality metric

**Result:** Experimental results demonstrate that both methods achieve higher perceptual similarity compared to existing techniques, with ApproPAwS offering up to 100x speed improvements.

**Limitations:** The methods may have limitations in very high-dimensional data spaces where perception may vary more widely.

**Future Work:** Further exploration of perception-aware sampling methods for other visualization types and larger data dimensions.

**Conclusion:** The proposed perception-aware methods significantly enhance sample efficacy and are preferred in user studies, suggesting their practical application in data analytics.

**Abstract:** Visualizing data is often a crucial first step in data analytics workflows, but growing data sizes pose challenges due to computational and visual perception limitations. As a result, data analysts commonly down-sample their data and work with subsets. Deriving representative samples, however, remains a challenge. This paper focuses on scatterplots, a widely-used visualization type, and introduces a novel sampling objective -- perception-awareness -- aiming to improve sample efficacy by targeting humans' perception of a visualization.   We make the following contributions: (1) We propose perception-augmented databases and design PAwS: a novel perception-aware sampling method for scatterplots that leverages saliency maps -- a computer vision tool for predicting areas of attention focus in visualizations -- and models perception-awareness via saliency, density, and coverage objectives. (2) We design ApproPAwS: a fast, perception-aware method for approximate visualizations, which exploits the fact that small visual perturbations are often imperceptible to humans. (3) We introduce the concept of perceptual similarity as a metric for sample quality, and present a novel method that compares saliency maps to measure it. (4) Our extensive experimental evaluation shows that our methods consistently outperform prior art in producing samples with high perceptual similarity, while ApproPAwS achieves up to 100x speed-ups with minimal loss in visual fidelity. Our user study shows that PAwS is often preferred by humans, validating our quantitative findings.

</details>


### [6] [Explanation format does not matter; but explanations do -- An Eggsbert study on explaining Bayesian Optimisation tasks](https://arxiv.org/abs/2504.20567)

*Tanmay Chakraborty, Marion Koelle, Jörg Schlötterer, Nadine Schlicker, Christian Wirth, Christin Seifert*

**Main category:** cs.HC

**Keywords:** Explainable Bayesian Optimization, user performance, parameter tuning, human-computer interaction, explanation formats

**Relevance Score:** 8

**TL;DR:** The paper investigates the impact of different explanation formats in Explainable Bayesian Optimization (XBO) on user performance and understanding in a parameter tuning task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is limited guidance on effectively presenting Explainable Bayesian Optimization (XBO) results to users involved in tuning tasks.

**Method:** A between-subjects online study was conducted with 213 participants, where three explanation formats (bar chart, list of rules, textual explanation) were compared in a task involving tuning parameters for cooking a soft-boiled egg.

**Key Contributions:** Investigation of XBO explanation formats on user performance, Demonstration of improved task success and user confidence with any explanation format, Finding that explanation format does not affect task load among users

**Result:** Adding explanations improved task success, reduced the number of trials needed for success, and enhanced comprehension and confidence, with no increase in task load; the effect was consistent across all explanation formats.

**Limitations:** The study did not explore prolonged use of explanation formats and their long-term effects on mental models.

**Future Work:** Future research could investigate the effects of prolonged use of different explanation formats on users' mental models of the system.

**Conclusion:** Explanations can be incorporated into Bayesian Optimization tasks without the burden of selecting specific formats, potentially simplifying the implementation of such systems.

**Abstract:** Bayesian Optimisation (BO) is a family of methods for finding optimal parameters when the underlying function to be optimised is unknown. BO is used, for example, for hyperparameter tuning in machine learning and as an expert support tool for tuning cyberphysical systems. For settings where humans are involved in the tuning task, methods have been developed to explain BO (Explainable Bayesian Optimization, XBO). However, there is little guidance on how to present XBO results to humans so that they can tune the system effectively and efficiently. In this paper, we investigate how the XBO explanation format affects users' task performance, task load, understanding and trust in XBO. We chose a task that is accessible to a wide range of users. Specifically, we set up an egg cooking scenario with 6 parameters that participants had to adjust to achieve a perfect soft-boiled egg. We compared three different explanation formats: a bar chart, a list of rules and a textual explanation in a between-subjects online study with 213 participants. Our results show that adding any type of explanation increases task success, reduces the number of trials needed to achieve success, and improves comprehension and confidence. While explanations add more information for participants to process, we found no increase in user task load. We also found that the aforementioned results were independent of the explanation format; all formats had a similar effect.This is an interesting finding for practical applications, as it suggests that explanations can be added to BO tuning tasks without the burden of designing or selecting specific explanation formats. In the future, it would be interesting to investigate scenarios of prolonged use of the explanation formats and whether they have different effects on users' mental models of the underlying system.

</details>


### [7] [In defence of post-hoc explanations in medical AI](https://arxiv.org/abs/2504.20741)

*Joshua Hatherley, Lauritz Munch, Jens Christian Bjerring*

**Main category:** cs.HC

**Keywords:** Explainable AI, post-hoc explanations, medical AI

**Relevance Score:** 8

**TL;DR:** This paper defends the value of post-hoc explanations in medical AI, arguing they enhance user understanding and clinician accuracy despite not replicating exact reasoning processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address criticisms of post-hoc explanations in AI, particularly in the medical field, which challenge their effectiveness in supporting user understanding and decision-making.

**Method:** 

**Key Contributions:** Defends post-hoc explanations in medical AI, Explains their role in improving user understanding, Discusses their importance in clinician decision-making

**Result:** The paper argues that post-hoc explanations improve users' understanding of black box systems and aid clinicians in making justifiable AI-informed decisions, despite limitations.

**Limitations:** 

**Future Work:** 

**Conclusion:** Post-hoc explanations, while not perfect, are beneficial in navigating the complexities of black box AI in medical contexts.

**Abstract:** Since the early days of the Explainable AI movement, post-hoc explanations have been praised for their potential to improve user understanding, promote trust, and reduce patient safety risks in black box medical AI systems. Recently, however, critics have argued that the benefits of post-hoc explanations are greatly exaggerated since they merely approximate, rather than replicate, the actual reasoning processes that black box systems take to arrive at their outputs. In this article, we aim to defend the value of post-hoc explanations against this recent critique. We argue that even if post-hoc explanations do not replicate the exact reasoning processes of black box systems, they can still improve users' functional understanding of black box systems, increase the accuracy of clinician-AI teams, and assist clinicians in justifying their AI-informed decisions. While post-hoc explanations are not a "silver bullet" solution to the black box problem in medical AI, we conclude that they remain a useful strategy for addressing the black box problem in medical AI.

</details>


### [8] [Integrating Human Feedback into a Reinforcement Learning-Based Framework for Adaptive User Interfaces](https://arxiv.org/abs/2504.20782)

*Daniel Gaspar-Figueiredo, Marta Fernández-Diego, Silvia Abrahão, Emilio Insfran*

**Main category:** cs.HC

**Keywords:** Adaptive User Interfaces, Reinforcement Learning, User Experience, Personalized Feedback, Machine Learning

**Relevance Score:** 8

**TL;DR:** The paper presents an enhanced framework for RL-based Adaptive User Interfaces that integrates personalized human feedback into the learning process, leading to improved user experience (UX).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of real-time responsiveness in existing adaptive user interface strategies and to enhance personalization by incorporating user feedback in reinforcement learning.

**Method:** The study developed a unique RL agent for each user, allowing them to actively influence their agent’s policy through feedback during interactions with adaptive user interfaces.

**Key Contributions:** Introduction of personalized RL agents for each user in adaptive interfaces, Demonstration of the positive impact of human feedback on user experience in AUIs, Evaluation across two application domains (e-learning and trip planning), Empirical evidence supporting UX enhancement through adaptive methods

**Result:** Empirical study results indicated that integrating human feedback into RL-based adaptations significantly improved user experience compared to non-adaptive interfaces.

**Limitations:** The study is limited to two application domains and a relatively small participant group, which may impact generalizability.

**Future Work:** Future research should explore broader application domains and larger, more diverse participant groups to validate findings and improve RL-based UI adaptation.

**Conclusion:** Enhancing RL-based AUIs with personalized human feedback can lead to more responsive and user-centered design, paving the way for future research in this area.

**Abstract:** Adaptive User Interfaces (AUI) play a crucial role in modern software applications by dynamically adjusting interface elements to accommodate users' diverse and evolving needs. However, existing adaptation strategies often lack real-time responsiveness. Reinforcement Learning (RL) has emerged as a promising approach for addressing complex, sequential adaptation challenges, enabling adaptive systems to learn optimal policies based on previous adaptation experiences. Although RL has been applied to AUIs,integrating RL agents effectively within user interactions remains a challenge.   In this paper, we enhance a RL-based Adaptive User Interface adaption framework by incorporating personalized human feedback directly into the leaning process. Unlike prior approaches that rely on a single pre-trained RL model, our approach trains a unique RL agent for each user, allowing individuals to actively shape their personal RL agent's policy, potentially leading to more personalized and responsive UI adaptations. To evaluate this approach, we conducted an empirical study to assess the impact of integrating human feedback into the RL-based Adaptive User Interface adaption framework and its effect on User Experience (UX). The study involved 33 participants interacting with AUIs incorporating human feedback and non-adaptive user interfaces in two domains: an e-learning platform and a trip-planning application. The results suggest that incorporating human feedback into RL-driven adaptations significantly enhances UX, offering promising directions for advancing adaptive capabilities and user-centered design in AUIs.

</details>


### [9] [Effect of Avatar Head Movement on Communication Behaviour, Experience of Presence and Conversation Success in Triadic Conversations](https://arxiv.org/abs/2504.20844)

*Angelika Kothe, Volker Hohmann, Giso Grimm*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Communication Behavior, Animated Avatars, Ecological Validity, Telepresence

**Relevance Score:** 6

**TL;DR:** The study evaluates how virtual animated characters' head movements affect communication behavior in virtual reality, showing that realistic head movements enhance conversation success and participants' engagement.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ecological validity of hearing device evaluations in virtual environments by mimicking natural communication behavior.

**Method:** Triadic conversations were conducted in telepresence, using a low-latency system to manipulate and transmit head movements between participants and avatars with varying levels of head movement animations.

**Key Contributions:** Demonstrated the impact of animated avatars on natural communication behaviors in VR., Showed that more realistic head movements improve conversational effectiveness and user experience., Introduced a telepresence method for evaluating communication behaviors with virtual characters.

**Result:** Significant effects of the animation level on speech and head movement behavior were observed, with increased realism leading to enhanced presence and conversational outcomes.

**Limitations:** Focused primarily on head movements without exploring other non-verbal cues or environmental factors that may influence communication.

**Future Work:** Explore further non-verbal cues and their effects on communication in virtual reality settings, and assess the implications for various applications such as remote collaboration and education.

**Conclusion:** Realistic head movements in virtual avatars are crucial for fostering natural communication behavior among users.

**Abstract:** Interactive communication in virtual reality can be used in experimental paradigms to increase the ecological validity of hearing device evaluations. This requires the virtual environment to elicit natural communication behaviour in listeners. This study evaluates the effect of virtual animated characters' head movements on participants' communication behaviour and experience.   Triadic conversations were conducted between a test participant and two confederates. To facilitate the manipulation of head movements, the conversation was conducted in telepresence using a system that transmitted audio, head movement data and video with low delay. The confederates were represented by virtual animated characters (avatars) with different levels of animation: Static heads, automated head movement animations based on speech level onsets, and animated head movements based on the transmitted head movements of the interlocutors. A condition was also included in which the videos of the interlocutors' heads were embedded in the visual scene.   The results show significant effects of animation level on the participants' speech and head movement behaviour as recorded by physical sensors, as well as on the subjective sense of presence and the success of the conversation. The largest effects were found for the range of head orientation during speech and the perceived realism of avatars. Participants reported that they were spoken to in a more helpful way when the avatars showed head movements transmitted from the interlocutors than when the avatars' heads were static.   We therefore conclude that the representation of interlocutors must include sufficiently realistic head movements in order to elicit natural communication behaviour.

</details>


### [10] [Mapping a Movement: Exploring a Proposed Police Training Facility in Atlanta and the Stop Cop City Movement through Online Maps](https://arxiv.org/abs/2504.20886)

*Camille Harris, Clio Andris*

**Main category:** cs.HC

**Keywords:** police training facility, mapping technologies, social movements, critical cartography, community perspectives

**Relevance Score:** 3

**TL;DR:** This paper analyzes the use of online maps in the opposition movement against the Atlanta police training facility, examining their implications for social movements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation for this study is to understand how diverse stakeholders utilize maps and mapping technologies in social movements, particularly in opposition to contentious projects like the police training facility in Atlanta.

**Method:** The study conducts a content analysis of 32 publicly available maps sourced from various platforms including Google Search API, Twitter, Instagram, and Reddit, using a framework of critical cartography.

**Key Contributions:** Analysis of online maps in social movements, Insights into stakeholder mapping behaviors and access, Recommendations for accessible mapmaking tools

**Result:** The findings reveal that stakeholders use geospatial tools differently and exhibit varying access to mapping technologies, influencing how they communicate information about the facility and the opposition movement.

**Limitations:** The study is limited to publicly available maps and may not capture all perspectives in the opposition movement.

**Future Work:** Future research should explore the impact of accessible mapping tools on community engagement and the effectiveness of geospatial communication in social movements.

**Conclusion:** The study concludes that documenting map usage in social movements can reveal community perspectives and stresses the importance of making mapmaking tools accessible to enhance public discourse.

**Abstract:** In 2021, the City of Atlanta and Atlanta Police Foundation launched plans to build a large police training facility in the South River Forest in unincorporated DeKalb County, GA. Residents of Atlanta and DeKalb County, environmental activists, police and prison abolitionists, and other activists and concerned individuals formed the movement in opposition to the facility, known as the Stop Cop City / Defend the Atlanta Forest movement. Social media and digital maps became common tools for communicating information about the facility and the movement. Here, we examine online maps about the facility and the opposition movement, originating from grassroots organizations, the City of Atlanta, news media outlets, the Atlanta Police Foundation, and individuals. We gather and examine 32 publicly available maps collected through the Google Search API, Twitter (now X), Instagram and reddit. Using a framework of critical cartography, we conduct a content analysis of these maps to identify the mapping technologies and techniques (data, cartographic elements, styles) used by different stakeholders and roles that maps and mapping technologies can play in social movements. We examine the extent to which these maps provide data to confirm or contradict concerns raised by grassroots organizations and local residents about the facility. We find that stakeholders and mapmakers use geospatial tools in different ways and likely have varied access to mapping technologies. We argue that documenting the use of maps to communicate information about a contentious project can help enumerate community positions and perspectives, and we advocate for accessible mapmaking tools. We conclude by discussing the implications of accessibility of mapping technology and posting maps to social media, and share example map images that extend the geographic information systems (GIS) techniques seen in the retrieved maps.

</details>


### [11] [Real-Time Wayfinding Assistant for Blind and Low-Vision Users](https://arxiv.org/abs/2504.20976)

*Dabbrata Das, Argho Deb Das, Farhan Sadaf*

**Main category:** cs.HC

**Keywords:** navigation, assistive technology, blind and visually impaired, Vision Language Models, depth estimation

**Relevance Score:** 8

**TL;DR:** PathFinder is a novel map-less navigation system for blind and visually limited individuals that utilizes Vision Language Models and depth estimation for obstacle-free pathway detection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in existing assistive technologies for navigation by blind or visually limited individuals, particularly in real-time obstacle avoidance and adaptability to dynamic environments.

**Method:** PathFinder employs monocular depth estimation and integrates a Depth-First Search algorithm on depth images to identify the longest obstacle-free path, enriching traditional navigation systems with advanced AI techniques.

**Key Contributions:** Introduction of PathFinder as a map-less navigation solution for BLV individuals., Integration of Vision Language Models and Depth-First Search for obstacle detection., Empirical evidence showcasing significant improvements in navigation accuracy and user satisfaction.

**Result:** PathFinder demonstrates improved accuracy, computational efficiency, and real-time responsiveness compared to existing AI-powered navigation methods, showcasing reduced mean absolute error and enhanced decision-making speed.

**Limitations:** Issues in complex indoor locations and low-light conditions were identified during usability tests.

**Future Work:** Further research is needed to enhance performance in challenging environments like indoor scenarios and low-light conditions.

**Conclusion:** The usability study indicates PathFinder's effectiveness in outdoor navigation and its user-friendly design, although challenges remain in complex indoor and low-light environments.

**Abstract:** Navigating unfamiliar places continues to be one of the most persistent and essential everyday obstacles for those who are blind or have limited vision (BLV). Existing assistive technologies, such as GPS-based navigation systems, AI-powered smart glasses, and sonar-equipped canes, often face limitations in real-time obstacle avoidance, precise localization, and adaptability to dynamic surroundings. To investigate potential solutions, we introduced PathFinder, a novel map-less navigation system that explores different models for understanding 2D images, including Vision Language Models (VLMs), Large Language Models (LLMs), and employs monocular depth estimation for free-path detection. Our approach integrates a Depth-First Search (DFS) algorithm on depth images to determine the longest obstacle-free path, ensuring optimal route selection while maintaining computational efficiency. We conducted comparative evaluations against existing AI-powered navigation methods and performed a usability study with BLV participants. The results demonstrate that PathFinder achieves a favorable balance between accuracy, computational efficiency, and real-time responsiveness. Notably, it reduces mean absolute error (MAE) and improves decision-making speed in outdoor navigation compared to AI-based alternatives. Participant feedback emphasizes the system's usability and effectiveness in outside situations, but also identifies issues in complicated indoor locations and low-light conditions. Usability testing revealed that 73% of participants understood how to use the app in about a minute, and 80% praised its balance of accuracy, quick response, and overall convenience.

</details>


### [12] [GaitGuard: Towards Private Gait in Mixed Reality](https://arxiv.org/abs/2312.04470)

*Diana Romero, Ruchi Jagdish Patel, Athina Markopoulou, Salma Elmalaki*

**Main category:** cs.HC

**Keywords:** Gait privacy, Augmented reality, Privacy-preserving, Mixed reality, Human-computer interaction

**Relevance Score:** 7

**TL;DR:** This paper presents GaitGuard, a real-time system that protects gait privacy in Augmented/Mixed Reality environments against video-based extraction attacks, achieving a balance between privacy and user experience.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of AR/MR technologies, there's a growing need to address privacy concerns, particularly regarding gait information that can reveal sensitive personal data.

**Method:** GaitGuard employs a multi-threaded framework to process video frames, using specific modules for stream capture, body detection, tracking, and privacy leak mitigation. Multiple techniques for privacy protection and their impacts on privacy, video quality, and performance are thoroughly analyzed through experiments involving 248 settings.

**Key Contributions:** Development of GaitGuard for gait privacy in MR environments., Analysis of various gait mitigation techniques and their impacts., Real-time performance and privacy efficacy in user surveys.

**Result:** GaitGuard significantly reduces the effectiveness of gait extraction attacks, achieving a Jensen-Shannon Divergence of 0.63 and a 68% reduction in identification risks, while maintaining a processing speed of 29 FPS and video clarity.

**Limitations:** The study primarily focuses on gait privacy, and further research may be needed to address other aspects of privacy in MR technologies.

**Future Work:** Expansion of the framework to include additional privacy features and further testing in diverse real-world scenarios.

**Conclusion:** GaitGuard offers an effective solution for ensuring gait privacy in MR applications, enhancing user security without compromising overall experience based on subjective user feedback.

**Abstract:** Augmented/Mixed Reality (AR/MR) technologies usher in a new era of immersive, collective experiences, differentiating them from traditional mobile systems. As these technologies evolve, prioritizing privacy and security is critical. This paper focuses on gait privacy, where gait, the way a person walks, can reveal sensitive information such as age, ethnicity, or disorders. We present GaitGuard, a real-time system that protects gait privacy against video-based gait extraction attacks in MR environments. GaitGuard leverages a multi-threaded framework to efficiently process video frames, incorporating dedicated modules for stream capture, body detection and tracking, and privacy leak mitigation. We compare and combine multiple mitigation techniques, offering guidance to navigate the privacy-utility tradeoff. Through extensive experiments covering 248 settings across mitigation regions, types, and tunable parameters, we assess the impact of these techniques on privacy, video quality, and system performance. GaitGuard reduces the confidence of video-based gait extraction attacks by introducing a substantial distribution shift (Jensen-Shannon Divergence of 0.63, indicating highly altered gait features) and a decrease in identification risks by up to 68%, while maintaining 29 FPS and preserving video clarity. GaitGuard provides a practical real-time solution for privacy-preserving MR applications without affecting the MR user experience based on 20 subjective user surveys.

</details>


### [13] [Perception in Pixels: Effects of Avatar Representation in Video-Mediated Collaborative Interactions](https://arxiv.org/abs/2405.03844)

*Pitch Sinlapanuntakul, Mark Zachry*

**Main category:** cs.HC

**Keywords:** video conferencing, avatars, collaboration, self-esteem, mixed-methods

**Relevance Score:** 8

**TL;DR:** This study investigates the effects of avatar-mediated video conferencing on collaboration satisfaction and self-esteem compared to traditional video. Findings indicate benefits in using avatars.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper explores challenges in traditional video conferencing and how avatars might enhance self-representation and collaboration in activity-oriented group settings.

**Method:** A mixed-methods, within-subject study with 32 participants engaged in goal-directed activities, followed by qualitative group interviews to gather insights.

**Key Contributions:** Investigated avatar use in remote collaboration settings, Provided evidence of increased collaboration satisfaction and self-esteem with avatars, Highlighted user experiences and perceptions regarding avatar adoption

**Result:** Results show that avatars significantly enhance self-esteem and collaboration satisfaction, with qualitative feedback highlighting both positives and challenges in their use.

**Limitations:** The study is limited to 32 participants and may not generalize to all remote work contexts.

**Future Work:** Further research is needed to explore avatar use in diverse settings and with different populations to assess broader implications.

**Conclusion:** The study contributes to the understanding of avatar use in video-mediated collaboration, showing potential benefits while also highlighting challenges.

**Abstract:** Interactive collaborative video is now a common part of remote work. Despite its prevalence, traditional video conferencing can be challenging, sometimes causing social discomforts that undermine process and outcomes. Avatars on 2D displays offer a promising alternative for enhancing self-representation, bridging the gap between virtual reality (VR) and traditional non-immersive video. However, the use of such avatars in activity-oriented group settings remains underexplored. To address this gap, we conducted a mixed-methods, within-subject study investigating the impacts of avatar-mediated versus traditional video representations on collaboration satisfaction and self-esteem. 32 participants (8 groups of 4 with pre-established relationships) engaged in goal-directed activities, followed by group interviews. Results indicate that avatars significantly enhance self-esteem and collaboration satisfaction, while qualitative insights reveal the dynamic perceptions and experiences of avatars, including benefits, challenges, and factors influencing adoption likelihood. Our study contributes to understanding and implications of avatars as a camera-driven representation in video-mediated collaborative interactions.

</details>


### [14] [Scideator: Human-LLM Scientific Idea Generation Grounded in Research-Paper Facet Recombination](https://arxiv.org/abs/2409.14634)

*Marissa Radensky, Simra Shahid, Raymond Fok, Pao Siangliulue, Tom Hope, Daniel S. Weld*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Creativity Support, Mixed-Initiative Systems, Scientific Ideation

**Relevance Score:** 9

**TL;DR:** A novel ideation tool, Scideator, blends facets from existing scientific papers to assist in creativity and idea generation using LLMs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can aid in the scientific ideation process by blending existing ideas to generate new ones.

**Method:** Scideator extracts key facets from user-provided scientific papers and related literature, allowing users to recombine these facets interactively. It includes LLM-powered modules for finding analogous papers, generating ideas, and checking idea novelty.

**Key Contributions:** Introduction of an interactive ideation tool leveraging LLMs., Three innovative RAG modules to assist in ideation process., Empirical evidence showing improved creativity support through user studies.

**Result:** Scideator significantly enhanced creativity support in a study with computer-science researchers, particularly in exploring new ideas compared to a strong baseline.

**Limitations:** The study had a small sample size (N=22) and may not represent the broader research community.

**Future Work:** Future iterations could expand the user study and enhance the tool's capabilities based on feedback.

**Conclusion:** The findings suggest that user-initiated interaction with the tool leads to higher support for generating original ideas in scientific research.

**Abstract:** The scientific ideation process often involves blending salient aspects of existing papers to create new ideas, and facet-based ideation is an established framework for idea generation. To see how large language models (LLMs) might assist in this process, we contribute a novel mixed-initiative ideation tool called Scideator. Starting from a user-provided set of scientific papers, Scideator extracts key facets -- purposes, mechanisms, and evaluations -- from these and related papers, allowing users to explore the idea space by interactively recombining facets to synthesize inventive ideas. Scideator also helps users gauge idea originality by searching the literature for overlaps, assessing idea novelty and providing explanations. To support these tasks, Scideator introduces three LLM-powered retrieval-augmented generation (RAG) modules: Analogous Paper Facet Finder, Faceted Idea Generator, and Idea Novelty Checker. In a within-subjects user study (N=22) with computer-science researchers comparing Scideator to a strong baseline, our tool provided significantly more creativity support, particularly with respect to exploration, which participants considered the most important factor for idea generation.

</details>


### [15] [Perfectly to a Tee: Understanding User Perceptions of Personalized LLM-Enhanced Narrative Interventions](https://arxiv.org/abs/2409.16732)

*Ananya Bhattacharjee, Sarah Yi Xu, Pranav Rao, Yuchen Zeng, Jonah Meyerhoff, Syed Ishtiaque Ahmed, David C Mohr, Michael Liut, Alex Mariakakis, Rachel Kornfield, Joseph Jay Williams*

**Main category:** cs.HC

**Keywords:** large language models, mental health, narrative generation, personalization, digital interventions

**Relevance Score:** 9

**TL;DR:** The study explores how large language models can generate personalized narratives to help young adults manage personal struggles, finding these AI-generated stories more relatable and effective than human-written ones.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To illustrate the application of psychological theories through personalized narratives that resonate with individuals' experiences.

**Method:** Employing large language models (LLMs) to create tailored stories for 346 young adults in different settings.

**Key Contributions:** Demonstrated effectiveness of LLMs in generating personalized narratives for mental health., Established the perception of LLM-generated stories as relatable and authentic., Provided design considerations for future narrative-based mental health interventions.

**Result:** Personalized LLM-enhanced stories were perceived as superior to human-written narratives in terms of relatability, promoting reflection, and reducing negative thoughts.

**Limitations:** The study may not generalize to other demographics beyond young adults.

**Future Work:** Investigate the optimal balance between relatability and plausibility in AI-generated narratives.

**Conclusion:** LLMs have significant potential in creating effective narrative-based digital mental health interventions for young adults.

**Abstract:** Stories about overcoming personal struggles can effectively illustrate the application of psychological theories in real life, yet they may fail to resonate with individuals' experiences. In this work, we employ large language models (LLMs) to create tailored narratives that acknowledge and address unique challenging thoughts and situations faced by individuals. Our study, involving 346 young adults across two settings, demonstrates that personalized LLM-enhanced stories were perceived to be better than human-written ones in conveying key takeaways, promoting reflection, and reducing belief in negative thoughts. These stories were not only seen as more relatable but also similarly authentic to human-written ones, highlighting the potential of LLMs in helping young adults manage their struggles. The findings of this work provide crucial design considerations for future narrative-based digital mental health interventions, such as the need to maintain relatability without veering into implausibility and refining the wording and tone of AI-enhanced content.

</details>


### [16] [WeAudit: Scaffolding User Auditors and AI Practitioners in Auditing Generative AI](https://arxiv.org/abs/2501.01397)

*Wesley Hanwen Deng, Wang Claire, Howard Ziyu Han, Jason I. Hong, Kenneth Holstein, Motahhare Eslami*

**Main category:** cs.HC

**Keywords:** AI auditing, user engagement, WeAudit, machine learning, ethical AI

**Relevance Score:** 7

**TL;DR:** The paper presents WeAudit, a system designed to engage end users in AI auditing, enhancing actionable insights for practitioners through a structured workflow.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need to involve end users in AI auditing to leverage their unique insights and experiences, which could lead to better AI practices.

**Method:** The authors conducted formative studies with users and AI practitioners to establish design goals, developed the WeAudit system, and evaluated it through a three-week user study and interviews.

**Key Contributions:** Introduction of WeAudit as a user-engaged AI auditing system, Insights from user studies on AI auditing practices, Identified opportunities for enhancing user involvement in AI auditing

**Result:** WeAudit effectively helps users identify potential AI harms and communicate their findings to industry practitioners, facilitating better AI auditing processes.

**Limitations:** The study's findings are based on a limited user group and specific contexts, which may not generalize to all scenarios.

**Future Work:** Further research is needed to explore scalable methods for user engagement in AI auditing and the implications of findings across different industries.

**Conclusion:** The study reveals critical opportunities for improving user engagement in AI auditing and emphasizes the need for responsible practices in this field.

**Abstract:** There has been growing interest from both practitioners and researchers in engaging end users in AI auditing, to draw upon users' unique knowledge and lived experiences. However, we know little about how to effectively scaffold end users in auditing in ways that can generate actionable insights for AI practitioners. Through formative studies with both users and AI practitioners, we first identified a set of design goals to support user-engaged AI auditing. We then developed WeAudit, a workflow and system that supports end users in auditing AI both individually and collectively. We evaluated WeAudit through a three-week user study with user auditors and interviews with industry Generative AI practitioners. Our findings offer insights into how WeAudit supports users in noticing and reflecting upon potential AI harms and in articulating their findings in ways that industry practitioners can act upon. Based on our observations and feedback from both users and practitioners, we identify several opportunities to better support user engagement in AI auditing processes. We discuss implications for future research to support effective and responsible user engagement in AI auditing and red-teaming.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [17] [It's the same but not the same: Do LLMs distinguish Spanish varieties?](https://arxiv.org/abs/2504.20049)

*Marina Mayor-Rocher, Cristina Pozo, Nina Melero, Gonzalo Martínez, María Grandury, Pedro Reviriego*

**Main category:** cs.CL

**Keywords:** large language models, Spanish varieties, morphosyntax, lexical peculiarities, GPT-4o

**Relevance Score:** 7

**TL;DR:** This study evaluates nine language models' ability to distinguish morphosyntactic and lexical peculiarities of seven Spanish varieties, finding GPT-4o the most capable.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capacity of language models in recognizing the diversity of the Spanish language due to its regional variations.

**Method:** A multiple-choice test was conducted to evaluate nine language models on their ability to identify different Spanish varieties.

**Key Contributions:** Evaluation of nine LLMs on Spanish language varieties, Identification of Peninsular Spanish as the most recognizable variety, Demonstration of GPT-4o's capability in recognizing language variability

**Result:** The models showed varying levels of performance, with Peninsular Spanish being the most accurately identified and GPT-4o recognizing the diversity best.

**Limitations:** The study is limited to only seven varieties of Spanish and nine models, which may not represent all regional differences.

**Future Work:** Further research could expand the evaluation to include more Spanish varieties and additional language models.

**Conclusion:** The study highlights the importance of recognizing the regional variations in language processing, particularly for Spanish.

**Abstract:** In recent years, large language models (LLMs) have demonstrated a high capacity for understanding and generating text in Spanish. However, with five hundred million native speakers, Spanish is not a homogeneous language but rather one rich in diatopic variations spanning both sides of the Atlantic. For this reason, in this study, we evaluate the ability of nine language models to identify and distinguish the morphosyntactic and lexical peculiarities of seven varieties of Spanish (Andean, Antillean, Continental Caribbean, Chilean, Peninsular, Mexican and Central American and Rioplatense) through a multiple-choice test. The results indicate that the Peninsular Spanish variety is the best identified by all models and that, among them, GPT-4o is the only model capable of recognizing the variability of the Spanish language.   --   En los \'ultimos a\~nos, los grandes modelos de lenguaje (LLMs, por sus siglas en ingl\'es) han demostrado una alta capacidad para comprender y generar texto en espa\~nol. Sin embargo, con quinientos millones de hablantes nativos, la espa\~nola no es una lengua homog\'enea, sino rica en variedades diat\'opicas que se extienden a ambos lados del Atl\'antico. Por todo ello, evaluamos en este trabajo la capacidad de nueve modelos de lenguaje de identificar y discernir las peculiaridades morfosint\'acticas y l\'exicas de siete variedades de espa\~nol (andino, antillano, caribe\~no continental, chileno, espa\~nol peninsular, mexicano y centroamericano y rioplatense) mediante un test de respuesta m\'ultiple. Los resultados obtenidos indican que la variedad de espa\~nol peninsular es la mejor identificada por todos los modelos y que, de entre todos, GPT-4o es el \'unico modelo capaz de identificar la variabilidad de la lengua espa\~nola.

</details>


### [18] [Evaluating Large Language Models on Multiword Expressions in Multilingual and Code-Switched Contexts](https://arxiv.org/abs/2504.20051)

*Frances Laureano De Leon, Harish Tayyar Madabushi, Mark G. Lee*

**Main category:** cs.CL

**Keywords:** multiword expressions, large language models, ambiguity, code-switching, language tasks

**Relevance Score:** 8

**TL;DR:** This paper evaluates how large language models handle the ambiguity of multiword expressions in different languages, finding significant challenges in processing nuanced language.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ability of large language models in processing ambiguous multiword expressions across different languages and contexts.

**Method:** The authors evaluated various language models, including GPT-4, on a novel code-switched dataset involving Portuguese, Galician, and English in detecting and understanding multiword expressions.

**Key Contributions:** Introduction of a novel code-switched dataset for evaluating multiword expressions., Demonstration of state-of-the-art language models' limitations in handling nuanced language., Comparison of performance across English, Portuguese, and Galician.

**Result:** The study found that large language models, including GPT-4, struggled with nuanced language involving multiword expressions, failing to outperform xlm-roBERTa-base baselines in detection and semantic tasks.

**Limitations:** The study may not cover all dimensions of linguistic nuance and only evaluates specific models in limited contexts.

**Future Work:** Future research could explore additional languages and a broader range of linguistic features affecting model performance.

**Conclusion:** Despite their advanced capabilities, large language models continue to face challenges in interpreting ambiguous multiword expressions, indicating a need for further improvements.

**Abstract:** Multiword expressions, characterised by non-compositional meanings and syntactic irregularities, are an example of nuanced language. These expressions can be used literally or idiomatically, leading to significant changes in meaning. While large language models have demonstrated strong performance across many tasks, their ability to handle such linguistic subtleties remains uncertain. Therefore, this study evaluates how state-of-the-art language models process the ambiguity of potentially idiomatic multiword expressions, particularly in contexts that are less frequent, where models are less likely to rely on memorisation. By evaluating models across in Portuguese and Galician, in addition to English, and using a novel code-switched dataset and a novel task, we find that large language models, despite their strengths, struggle with nuanced language. In particular, we find that the latest models, including GPT-4, fail to outperform the xlm-roBERTa-base baselines in both detection and semantic tasks, with especially poor performance on the novel tasks we introduce, despite its similarity to existing tasks. Overall, our results demonstrate that multiword expressions, especially those which are ambiguous, continue to be a challenge to models.

</details>


### [19] [Understanding and Mitigating Risks of Generative AI in Financial Services](https://arxiv.org/abs/2504.20086)

*Sebastian Gehrmann, Claire Huang, Xian Teng, Sergei Yurovski, Iyanuoluwa Shode, Chirag S. Patel, Arjun Bhorkar, Naveen Thomas, John Doucette, David Rosenberg, Mark Dredze, David Rabinowitz*

**Main category:** cs.CL

**Keywords:** Generative AI, Content safety, Financial services, AI risk taxonomy, Red-teaming

**Relevance Score:** 4

**TL;DR:** The paper discusses AI content safety in financial services, proposing a risk taxonomy and evaluating existing guardrails.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To responsibly develop Generative AI products, it is essential to define acceptable inputs and outputs, particularly focusing on specialized domains like financial services.

**Method:** The authors outline an AI content risk taxonomy specific to the financial services domain and compare it to existing work. They also assess open-source technical guardrail solutions through red-teaming activities.

**Key Contributions:** Introduction of a novel AI content risk taxonomy for financial services, Comparison of the risk taxonomy with existing work, Evaluation of open-source guardrail solutions' effectiveness

**Result:** The evaluation shows that existing guardrails fail to detect most content risks identified in the proposed taxonomy.

**Limitations:** The study focuses specifically on the financial services domain and may not generalize to other areas.

**Future Work:** Future research should explore tailored AI content safety mechanisms across different industries and improve current guardrail solutions.

**Conclusion:** The findings highlight the gaps in current guardrail solutions and suggest urgent attention to product-specific AI content safety.

**Abstract:** To responsibly develop Generative AI (GenAI) products, it is critical to define the scope of acceptable inputs and outputs. What constitutes a "safe" response is an actively debated question. Academic work puts an outsized focus on evaluating models by themselves for general purpose aspects such as toxicity, bias, and fairness, especially in conversational applications being used by a broad audience. In contrast, less focus is put on considering sociotechnical systems in specialized domains. Yet, those specialized systems can be subject to extensive and well-understood legal and regulatory scrutiny. These product-specific considerations need to be set in industry-specific laws, regulations, and corporate governance requirements. In this paper, we aim to highlight AI content safety considerations specific to the financial services domain and outline an associated AI content risk taxonomy. We compare this taxonomy to existing work in this space and discuss implications of risk category violations on various stakeholders. We evaluate how existing open-source technical guardrail solutions cover this taxonomy by assessing them on data collected via red-teaming activities. Our results demonstrate that these guardrails fail to detect most of the content risks we discuss.

</details>


### [20] [Toward Evaluative Thinking: Meta Policy Optimization with Evolving Reward Models](https://arxiv.org/abs/2504.20157)

*Zae Myung Kim, Chanwoo Park, Vipul Raheja, Dongyeop Kang*

**Main category:** cs.CL

**Keywords:** Meta Policy Optimization, large language models, reward-based alignment, meta-learning, policy optimization

**Relevance Score:** 8

**TL;DR:** Meta Policy Optimization (MPO) is proposed as a solution to the limitations of reward-based alignment methods for large language models (LLMs), particularly focusing on the issues of reward hacking and the necessity of manual prompt engineering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Reward-based alignment methods for LLMs face significant challenges including reward hacking and intensive prompt engineering, necessitating a more robust solution.

**Method:** MPO integrates a meta-reward model that dynamically refines the reward model's prompt during training, adapting to the evolving context to provide a stable and effective reward signal.

**Key Contributions:** Introduction of a meta-reward model that refines prompts dynamically during training, Reduction of manual labor in reward prompt design, Demonstration of effectiveness across diverse tasks without specialized reward designs

**Result:** MPO demonstrates performance comparable to or better than traditional methods relying on labor-intensive prompt engineering across various tasks without requiring specialized reward designs.

**Limitations:** 

**Future Work:** Further exploration of MPO's applicability in higher-level alignment frameworks and its integration into existing alignment strategies.

**Conclusion:** MPO addresses both theoretical and practical challenges in reward-based RL alignment for LLMs, suggesting a pathway for more robust alignment strategies with the potential for broader application.

**Abstract:** Reward-based alignment methods for large language models (LLMs) face two key limitations: vulnerability to reward hacking, where models exploit flaws in the reward signal; and reliance on brittle, labor-intensive prompt engineering when LLMs are used as reward models. We introduce Meta Policy Optimization (MPO), a framework that addresses these challenges by integrating a meta-reward model that dynamically refines the reward model's prompt throughout training. In MPO, the meta-reward model monitors the evolving training context and continuously adjusts the reward model's prompt to maintain high alignment, providing an adaptive reward signal that resists exploitation by the policy. This meta-learning approach promotes a more stable policy optimization, and greatly reduces the need for manual reward prompt design. It yields performance on par with or better than models guided by extensively hand-crafted reward prompts. Furthermore, we show that MPO maintains its effectiveness across diverse tasks, such as question answering and mathematical reasoning, without requiring specialized reward designs. Beyond standard RLAIF, MPO's meta-learning formulation is readily extensible to higher-level alignment frameworks. Overall, this method addresses theoretical and practical challenges in reward-based RL alignment for LLMs, paving the way for more robust and adaptable alignment strategies. The code and models will be publicly shared.

</details>


### [21] [MICE for CATs: Model-Internal Confidence Estimation for Calibrating Agents with Tools](https://arxiv.org/abs/2504.20168)

*Nishant Subramani, Jason Eisner, Justin Svegliato, Benjamin Van Durme, Yu Su, Sam Thomson*

**Main category:** cs.CL

**Keywords:** confidence estimation, tool usage, model calibration, probabilistic classifier, machine learning

**Relevance Score:** 8

**TL;DR:** Introducing model-internal confidence estimators (MICE) to improve the calibration of model confidences for tool-using agents.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance safety and utility of tool-using AI agents by improving the calibration of model confidences.

**Method:** MICE decodes from intermediate layers of language models and computes similarity scores to assess confidence via a learned probabilistic classifier.

**Key Contributions:** Introduction of model-internal confidence estimators (MICE), Improvement in expected calibration error, Enhanced tool-calling utility in risk-variable scenarios

**Result:** MICE outperforms or matches baselines on expected calibration error and significantly improves tool-calling utility for varying risk levels.

**Limitations:** None specified.

**Future Work:** Potential exploration of further integration with other model architectures and real-world applications of MICE.

**Conclusion:** MICE shows promise in being sample-efficient and generalizing to unseen APIs, making it a valuable tool for enhancing AI safety.

**Abstract:** Tool-using agents that act in the world need to be both useful and safe. Well-calibrated model confidences can be used to weigh the risk versus reward of potential actions, but prior work shows that many models are poorly calibrated. Inspired by interpretability literature exploring the internals of models, we propose a novel class of model-internal confidence estimators (MICE) to better assess confidence when calling tools. MICE first decodes from each intermediate layer of the language model using logitLens and then computes similarity scores between each layer's generation and the final output. These features are fed into a learned probabilistic classifier to assess confidence in the decoded output. On the simulated trial and error (STE) tool-calling dataset using Llama3 models, we find that MICE beats or matches the baselines on smoothed expected calibration error. Using MICE confidences to determine whether to call a tool significantly improves over strong baselines on a new metric, expected tool-calling utility. Further experiments show that MICE is sample-efficient, can generalize zero-shot to unseen APIs, and results in higher tool-calling utility in scenarios with varying risk levels. Our code is open source, available at https://github.com/microsoft/mice_for_cats.

</details>


### [22] [A Multimodal Pipeline for Clinical Data Extraction: Applying Vision-Language Models to Scans of Transfusion Reaction Reports](https://arxiv.org/abs/2504.20220)

*Henning Schäfer, Cynthia S. Schmidt, Johannes Wutzkowsky, Kamil Lorek, Lea Reinartz, Johannes Rückert, Christian Temme, Britta Böckmann, Peter A. Horn, Christoph M. Friedrich*

**Main category:** cs.CL

**Keywords:** Electronic Health Records, Checkbox Detection, Multilingual OCR, Data Extraction, Vision-Language Models

**Relevance Score:** 8

**TL;DR:** This study introduces an open-source pipeline for extracting and categorizing checkbox data from scanned healthcare documents, aiming to improve data handling efficiencies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and errors in manual transcription processes related to healthcare data transfer from paper to digital formats.

**Method:** The pipeline incorporates checkbox detection, multilingual OCR, and multilingual vision-language models, applied specifically to transfusion reaction reports while adaptable to other document types.

**Key Contributions:** Development of an open-source pipeline for checkbox data extraction., Integration of multilingual OCR and VLMs for improved accuracy., Demonstrated effectiveness on transfusion reaction reports with adaptable methodologies.

**Result:** The proposed pipeline exhibits high precision and recall metrics and successfully reduces administrative workload along with ensuring accurate regulatory reporting.

**Limitations:** The study is limited to checkbox-rich document types and requires evaluation on a wider range of healthcare documents for robustness.

**Future Work:** Future research may focus on extending the pipeline capabilities to other types of healthcare documents beyond checkbox forms and enhancing multi-lingual support.

**Conclusion:** The open-source nature of the pipeline allows for broader application and self-hosting opportunities in parsing checkbox forms across various healthcare scenarios.

**Abstract:** Despite the growing adoption of electronic health records, many processes still rely on paper documents, reflecting the heterogeneous real-world conditions in which healthcare is delivered. The manual transcription process is time-consuming and prone to errors when transferring paper-based data to digital formats. To streamline this workflow, this study presents an open-source pipeline that extracts and categorizes checkbox data from scanned documents. Demonstrated on transfusion reaction reports, the design supports adaptation to other checkbox-rich document types. The proposed method integrates checkbox detection, multilingual optical character recognition (OCR) and multilingual vision-language models (VLMs). The pipeline achieves high precision and recall compared against annually compiled gold-standards from 2017 to 2024. The result is a reduction in administrative workload and accurate regulatory reporting. The open-source availability of this pipeline encourages self-hosted parsing of checkbox forms.

</details>


### [23] [A Platform for Generating Educational Activities to Teach English as a Second Language](https://arxiv.org/abs/2504.20251)

*Aiala Rosá, Santiago Góngora, Juan Pablo Filevich, Ignacio Sastre, Laura Musto, Brian Carpenter, Luis Chiruzzo*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Education Technology, Game-Based Learning

**Relevance Score:** 6

**TL;DR:** A platform for generating English language educational activities using NLP techniques, offering both pre-made and customizable games and exercises.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for interactive and adaptable educational tools for teaching English as a foreign language to enhance learning engagement.

**Method:** The platform utilizes Natural Language Processing for the creation of language exercises and games, allowing for semi-automated content generation and manual curation.

**Key Contributions:** Development of an NLP-based platform for educational activities, Integration of customizable game generation for language learning, Plans to incorporate image and text generation improvements

**Result:** The platform allows teachers to generate a variety of educational activities, facilitates the integration of text and image generation, and has been successfully deployed for user engagement.

**Limitations:** 

**Future Work:** Migrate the platform to a more powerful server and further enhance game complexity and integration of additional media types.

**Conclusion:** The paper discusses the platform's development, the deployment challenges faced, and outlines future work to enhance performance and expand functionality.

**Abstract:** We present a platform for the generation of educational activities oriented to teaching English as a foreign language. The different activities -- games and language practice exercises -- are strongly based on Natural Language Processing techniques. The platform offers the possibility of playing out-of-the-box games, generated from resources created semi-automatically and then manually curated. It can also generate games or exercises of greater complexity from texts entered by teachers, providing a stage of review and edition of the generated content before use. As a way of expanding the variety of activities in the platform, we are currently experimenting with image and text generation. In order to integrate them and improve the performance of other neural tools already integrated, we are working on migrating the platform to a more powerful server. In this paper we describe the development of our platform and its deployment for end users, discussing the challenges faced and how we overcame them, and also detail our future work plans.

</details>


### [24] [Enhancing Systematic Reviews with Large Language Models: Using GPT-4 and Kimi](https://arxiv.org/abs/2504.20276)

*Dandan Chen Kaptur, Yue Huang, Xuejun Ryan Ji, Yanhui Guo, Bradley Kaptur*

**Main category:** cs.CL

**Keywords:** Large Language Models, Systematic Reviews, GPT-4, Kimi, Assessment

**Relevance Score:** 8

**TL;DR:** This research focuses on evaluating the performance of GPT-4 and Kimi in generating systematic review codes compared to human-generated codes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the competency of Large Language Models (LLMs) like GPT-4 and Kimi in conducting systematic reviews.

**Method:** Comparison of LLM-generated codes against human-generated codes from a peer-reviewed systematic review.

**Key Contributions:** Evaluation of LLM capabilities in systematic reviews, Comparison between LLM-generated and human-generated codes, Insights into factors affecting LLM performance in systematic reviews

**Result:** The performance of LLMs was found to vary based on the complexity of questions and the volume of data.

**Limitations:** Performance varies significantly based on data volume and question complexity, indicating limitations under specific conditions.

**Future Work:** Further exploration of LLMs in different contexts and with varying data types to enhance systematic review processes.

**Conclusion:** LLMs show potential in systematic reviews but their effectiveness is contingent on specific factors such as question complexity.

**Abstract:** This research delved into GPT-4 and Kimi, two Large Language Models (LLMs), for systematic reviews. We evaluated their performance by comparing LLM-generated codes with human-generated codes from a peer-reviewed systematic review on assessment. Our findings suggested that the performance of LLMs fluctuates by data volume and question complexity for systematic reviews.

</details>


### [25] [UD-English-CHILDES: A Collected Resource of Gold and Silver Universal Dependencies Trees for Child Language Interactions](https://arxiv.org/abs/2504.20304)

*Xiulin Yang, Zhuoxuan Ju, Lanni Bu, Zoey Liu, Nathan Schneider*

**Main category:** cs.CL

**Keywords:** UD-English-CHILDES, dependency-annotated, child-directed speech

**Relevance Score:** 4

**TL;DR:** Introduction of UD-English-CHILDES, a new resource derived from CHILDES data providing unified annotation for child and child-directed speech.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a consistent resource for computational and linguistic research by harmonizing previously dependency-annotated CHILDES data.

**Method:** Development of a Universal Dependencies (UD) treebank with validated gold-standard annotations and an additional silver-standard dataset.

**Key Contributions:** First officially released UD treebank from CHILDES data, Validation of existing gold-standard annotations under UD v2 framework, Provision of a large silver-standard dataset for various research applications

**Result:** Creation of a treebank with over 48k sentences from 11 children and caregivers, alongside 1M silver-standard sentences.

**Limitations:** 

**Future Work:** Encouragement for further enhancements and applications of the treebank in computational linguistics and child language research.

**Conclusion:** UD-English-CHILDES serves as a comprehensive resource for further research in child language acquisition and related fields.

**Abstract:** CHILDES is a widely used resource of transcribed child and child-directed speech. This paper introduces UD-English-CHILDES, the first officially released Universal Dependencies (UD) treebank derived from previously dependency-annotated CHILDES data with consistent and unified annotation guidelines. Our corpus harmonizes annotations from 11 children and their caregivers, totaling over 48k sentences. We validate existing gold-standard annotations under the UD v2 framework and provide an additional 1M silver-standard sentences, offering a consistent resource for computational and linguistic research.

</details>


### [26] [Labeling Case Similarity based on Co-Citation of Legal Articles in Judgment Documents with Empirical Dispute-Based Evaluation](https://arxiv.org/abs/2504.20323)

*Chao-Lin Liu, Po-Hsien Wu, Yi-Ting Yu*

**Main category:** cs.CL

**Keywords:** legal recommender systems, co-citation, labor disputes, text embedding, BiLSTM

**Relevance Score:** 3

**TL;DR:** This report proposes a new approach to develop legal recommender systems using co-citation of legal articles within cases for similarity measurement and algorithmic annotation, especially for labor disputes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of limited labeled datasets in specialized legal domains necessitates innovative approaches for developing recommender systems that can improve access to legal information and support decision-making.

**Method:** The approach leverages the co-citation of legal articles to establish case similarity, employing finetuned text embedding models and a BiLSTM module to recommend relevant labor cases based on the similarities of plaintiffs' accusations, defendants' rebuttals, and points of dispute.

**Key Contributions:** Introduction of a new algorithmic annotation approach using co-citation in legal articles, Development of a recommender system for labor disputes, Finetuning of text embedding models and integration of BiLSTM for improved case similarity recommendations.

**Result:** The evaluation of the system demonstrated that it can effectively recommend labor cases based on the co-citation of legal articles, facilitating automated annotation and enhancing the ability to navigate legal disputes.

**Limitations:** The effectiveness may be constrained by the availability and quality of legal article citations in databases.

**Future Work:** Exploration of expanding the approach to other areas of law and improving the models with larger datasets from comprehensive legal databases.

**Conclusion:** The research offers a significant advancement in automated annotation techniques for legal documents, particularly benefitting areas with limited access to legal databases.

**Abstract:** This report addresses the challenge of limited labeled datasets for developing legal recommender systems, particularly in specialized domains like labor disputes. We propose a new approach leveraging the co-citation of legal articles within cases to establish similarity and enable algorithmic annotation. This method draws a parallel to the concept of case co-citation, utilizing cited precedents as indicators of shared legal issues. To evaluate the labeled results, we employ a system that recommends similar cases based on plaintiffs' accusations, defendants' rebuttals, and points of disputes. The evaluation demonstrates that the recommender, with finetuned text embedding models and a reasonable BiLSTM module can recommend labor cases whose similarity was measured by the co-citation of the legal articles. This research contributes to the development of automated annotation techniques for legal documents, particularly in areas with limited access to comprehensive legal databases.

</details>


### [27] [Local Prompt Optimization](https://arxiv.org/abs/2504.20355)

*Yash Jain, Vishal Chowdhary*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Optimization, Local Prompt Optimization

**Relevance Score:** 8

**TL;DR:** Introduction of Local Prompt Optimization (LPO) for improving the efficiency of prompt engineering in Large Language Models by optimizing specific tokens rather than the entire prompt.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Experts struggle with crafting effective prompts for Large Language Models, leading to the need for better optimization methods.

**Method:** Local Prompt Optimization (LPO) selects and optimizes specific tokens in prompts, allowing for focused improvements in their generation.

**Key Contributions:** Development of Local Prompt Optimization (LPO) technique, Improved performance on Math Reasoning and BIG-bench benchmarks, Faster convergence to optimal prompts compared to global methods

**Result:** Remarkable performance enhancements were observed in benchmarks like GSM8k, MultiArith, and BIG-bench Hard, with LPO converging to optimal prompts more quickly than traditional global methods.

**Limitations:** 

**Future Work:** Exploring further enhancements to LPO and integrating it with various automatic prompt engineering techniques.

**Conclusion:** LPO demonstrates a more efficient and effective approach to prompt engineering by localizing the optimization process.

**Abstract:** In recent years, the use of prompts to guide the output of Large Language Models have increased dramatically. However, even the best of experts struggle to choose the correct words to stitch up a prompt for the desired task. To solve this, LLM driven prompt optimization emerged as an important problem. Existing prompt optimization methods optimize a prompt globally, where in all the prompt tokens have to be optimized over a large vocabulary while solving a complex task. The large optimization space (tokens) leads to insufficient guidance for a better prompt. In this work, we introduce Local Prompt Optimization (LPO) that integrates with any general automatic prompt engineering method. We identify the optimization tokens in a prompt and nudge the LLM to focus only on those tokens in its optimization step. We observe remarkable performance improvements on Math Reasoning (GSM8k and MultiArith) and BIG-bench Hard benchmarks across various automatic prompt engineering methods. Further, we show that LPO converges to the optimal prompt faster than global methods.

</details>


### [28] [What Causes Knowledge Loss in Multilingual Language Models?](https://arxiv.org/abs/2504.20356)

*Maria Khelli, Samuel Cahyawijaya, Ayu Purwarianti, Genta Indra Winata*

**Main category:** cs.CL

**Keywords:** Cross-lingual transfer, catastrophic forgetting, LoRA adapters, multilingual NLP, linguistic diversity

**Relevance Score:** 7

**TL;DR:** This study investigates cross-lingual transfer in NLP models, focusing on catastrophic forgetting and the use of LoRA adapters to enhance multilingual performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of catastrophic forgetting in NLP models during cross-lingual transfer.

**Method:** The study experiments with 52 languages using LoRA adapters of varying ranks to evaluate the effects of non-shared, partially shared, and fully shared parameters.

**Key Contributions:** Investigation of catastrophic forgetting in multilingual NLP contexts, Analysis of LoRA adapters in managing parameter sharing, Comparison of non-Latin and Latin script languages in cross-lingual transfer efficiency

**Result:** The findings suggest that languages with non-Latin scripts are more prone to catastrophic forgetting than those with Latin scripts, indicating a difference in effectiveness for cross-lingual transfer depending on the script.

**Limitations:** The study primarily examines 52 languages; results may not generalize to all languages or other NLP tasks.

**Future Work:** Further research could explore more languages and different adapter configurations to fully understand their impact on multilingual performance.

**Conclusion:** Parameter sharing through adapters can help mitigate forgetting while maintaining prior knowledge, though effectiveness varies with language script.

**Abstract:** Cross-lingual transfer in natural language processing (NLP) models enhances multilingual performance by leveraging shared linguistic knowledge. However, traditional methods that process all data simultaneously often fail to mimic real-world scenarios, leading to challenges like catastrophic forgetting, where fine-tuning on new tasks degrades performance on previously learned ones. Our study explores this issue in multilingual contexts, focusing on linguistic differences affecting representational learning rather than just model parameters. We experiment with 52 languages using LoRA adapters of varying ranks to evaluate non-shared, partially shared, and fully shared parameters. Our aim is to see if parameter sharing through adapters can mitigate forgetting while preserving prior knowledge. We find that languages using non-Latin scripts are more susceptible to catastrophic forgetting, whereas those written in Latin script facilitate more effective cross-lingual transfer.

</details>


### [29] [DMDTEval: An Evaluation and Analysis of LLMs on Disambiguation in Multi-domain Translation](https://arxiv.org/abs/2504.20371)

*Zhibo Man, Yuanmeng Chen, Yujie Zhang, Yufeng Chen, Jinan Xu*

**Main category:** cs.CL

**Keywords:** multi-domain translation, disambiguation, large language models, evaluation framework, prompting strategies

**Relevance Score:** 8

**TL;DR:** This paper presents DMDTEval, a framework to evaluate the disambiguation ability of LLMs in multi-domain translation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequate performance of LLMs in multi-domain translation (MDT) due to ambiguity in meanings of words across domains.

**Method:** The authors developed a systematic evaluation framework (DMDTEval) that includes constructing a translation test set with annotated ambiguous words, curating disambiguation prompting templates, and designing disambiguation metrics to evaluate various prompting strategies on LLMs.

**Key Contributions:** Development of the DMDTEval framework for evaluating LLMs in MDT, Construction of a test set with multi-domain ambiguous word annotations, Design of new disambiguation metrics and prompting strategies for LLMs

**Result:** The extensive experiments conducted through the framework revealed significant findings regarding the effectiveness of different prompting strategies in improving disambiguation.

**Limitations:** The paper does not explore the real-world application scenarios of the developed framework and its methodologies.

**Future Work:** Future research could focus on applying the framework to real-world MDT applications and exploring additional strategies for disambiguation.

**Conclusion:** The findings aim to enhance understanding and further research into improving LLMs' disambiguation capabilities in multi-domain translation contexts.

**Abstract:** Currently, Large Language Models (LLMs) have achieved remarkable results in machine translation. However, their performance in multi-domain translation (MDT) is less satisfactory; the meanings of words can vary across different domains, highlighting the significant ambiguity inherent in MDT. Therefore, evaluating the disambiguation ability of LLMs in MDT remains an open problem. To this end, we present an evaluation and analysis of LLMs on disambiguation in multi-domain translation (DMDTEval), our systematic evaluation framework consisting of three critical aspects: (1) we construct a translation test set with multi-domain ambiguous word annotation, (2) we curate a diverse set of disambiguation prompting templates, and (3) we design precise disambiguation metrics, and study the efficacy of various prompting strategies on multiple state-of-the-art LLMs. Our extensive experiments reveal a number of crucial findings that we believe will pave the way and also facilitate further research in the critical area of improving the disambiguation of LLMs.

</details>


### [30] [On Psychology of AI -- Does Primacy Effect Affect ChatGPT and Other LLMs?](https://arxiv.org/abs/2504.20444)

*Mika Hämäläinen*

**Main category:** cs.CL

**Keywords:** primacy effect, large language models, candidate preference, adjective order, HCI

**Relevance Score:** 8

**TL;DR:** This study investigates the primacy effect in three commercial LLMs by replicating a classic human experiment, revealing varied preferences in candidate descriptions based on the order of positive and negative adjectives.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the primacy effect influences the decision-making of large language models in selecting candidates based on adjectives used in descriptions.

**Method:** Two experiments were conducted; the first presented candidates simultaneously in a single prompt and the second presented them separately, both focusing on 200 candidate pairs.

**Key Contributions:** Repurposing a classic psychological experiment to analyze LLM behavior., Demonstrating diverse preferences among different LLMs based on input structure., Highlighting the impact of adjective order on LLM decision-making.

**Result:** ChatGPT preferred positive adjectives first in simultaneous prompts, while Gemini showed no preference. In separate prompts, both ChatGPT and Claude favored negative adjectives first over positive ones when they did not rank equally.

**Limitations:** The study only considers three LLMs, and the findings may not generalize to all models or contexts.

**Future Work:** Exploration of additional LLMs, further understanding of the primacy effects across different tasks, and potential applications in enhancing LLM responses.

**Conclusion:** The findings indicate that LLMs display distinct decision-making patterns influenced by the order of adjectives, with implications for their response generation.

**Abstract:** We study the primacy effect in three commercial LLMs: ChatGPT, Gemini and Claude. We do this by repurposing the famous experiment Asch (1946) conducted using human subjects. The experiment is simple, given two candidates with equal descriptions which one is preferred if one description has positive adjectives first before negative ones and another description has negative adjectives followed by positive ones. We test this in two experiments. In one experiment, LLMs are given both candidates simultaneously in the same prompt, and in another experiment, LLMs are given both candidates separately. We test all the models with 200 candidate pairs. We found that, in the first experiment, ChatGPT preferred the candidate with positive adjectives listed first, while Gemini preferred both equally often. Claude refused to make a choice. In the second experiment, ChatGPT and Claude were most likely to rank both candidates equally. In the case where they did not give an equal rating, both showed a clear preference to a candidate that had negative adjectives listed first. Gemini was most likely to prefer a candidate with negative adjectives listed first.

</details>


### [31] [Team ACK at SemEval-2025 Task 2: Beyond Word-for-Word Machine Translation for English-Korean Pairs](https://arxiv.org/abs/2504.20451)

*Daniel Lee, Harsh Sharma, Jieun Han, Sunny Jeong, Alice Oh, Vered Shwartz*

**Main category:** cs.CL

**Keywords:** Machine Translation, Cultural Nuances, Language Models, Entity Translation, Error Taxonomy

**Relevance Score:** 8

**TL;DR:** This paper evaluates 13 models for translating English and Korean, finding LLMs outperform traditional MT yet struggle with culturally nuanced entity translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve machine translation between English and Korean while preserving cultural nuances and entity richness.

**Method:** Evaluation of 13 models (LLMs and MT models) using both automatic metrics and human assessment by bilingual annotators.

**Key Contributions:** Development of an error taxonomy for translation errors, Identification of entity translation challenges in MT, Comparison of LLMs and traditional MT systems for cultural nuances

**Result:** LLMs outperform traditional MT systems, but encounter issues with culturally nuanced entity translation; performance varies by entity type.

**Limitations:** Findings are based on specific language pairs and may not generalize to others; results may vary with different entity types or contexts.

**Future Work:** Encouragement of further research on enhancing machine translation systems to better handle culturally nuanced content and entity translation.

**Conclusion:** The study identifies gaps in automatic evaluation metrics and proposes a need for improved handling of culturally nuanced translations in future research.

**Abstract:** Translating knowledge-intensive and entity-rich text between English and Korean requires transcreation to preserve language-specific and cultural nuances beyond literal, phonetic or word-for-word conversion. We evaluate 13 models (LLMs and MT models) using automatic metrics and human assessment by bilingual annotators. Our findings show LLMs outperform traditional MT systems but struggle with entity translation requiring cultural adaptation. By constructing an error taxonomy, we identify incorrect responses and entity name errors as key issues, with performance varying by entity type and popularity level. This work exposes gaps in automatic evaluation metrics and hope to enable future work in completing culturally-nuanced machine translation.

</details>


### [32] [Refiner: Restructure Retrieval Content Efficiently to Advance Question-Answering Capabilities](https://arxiv.org/abs/2406.11357)

*Zhonghao Li, Xuming Hu, Aiwei Liu, Kening Zheng, Sirui Huang, Hui Xiong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Information Structuring

**Relevance Score:** 9

**TL;DR:** This paper introduces $	extit{Refiner}$, an end-to-end extract-and-restructure paradigm designed to enhance the performance of Retrieval-Augmented Generation (RAG) by improving how LLMs utilize key information.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often experience hallucinations in knowledge-intensive tasks due to their limited parametric knowledge and struggle with utilizing scattered key information, often termed as the "lost-in-the-middle" syndrome.

**Method:** Refiner leverages a single decoder-only LLM to adaptively extract content relevant to the query while also providing necessary context, restructuring this information for better connectivity and recognition by downstream LLMs.

**Key Contributions:** Introduces a new paradigm for extracting and restructuring content in RAG contexts., Demonstrates significant improvements in multi-hop QA tasks and reduction of token usage., Offers a plug-and-play solution for RAG systems.

**Result:** Refiner, a 7B parameter model, significantly improves answer accuracy in downstream LLMs, achieving an 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop QA tasks compared to state-of-the-art alternatives.

**Limitations:** 

**Future Work:** Exploration of further enhancements in the restructuring algorithms and broader applications across various NLP tasks.

**Conclusion:** Refiner can be easily integrated with existing RAG systems, making it a flexible solution for enhancing LLM performance across various open-source frameworks.

**Abstract:** Large Language Models (LLMs) are limited by their parametric knowledge, leading to hallucinations in knowledge-extensive tasks. To address this, Retrieval-Augmented Generation (RAG) incorporates external document chunks to expand LLM knowledge. Furthermore, compressing information from document chunks through extraction or summarization can improve LLM performance. Nonetheless, LLMs still struggle to notice and utilize scattered key information, a problem known as the "lost-in-the-middle" syndrome. Therefore, we typically need to restructure the content for LLM to recognize the key information. We propose $\textit{Refiner}$, an end-to-end extract-and-restructure paradigm that operates in the post-retrieval process of RAG. $\textit{Refiner}$ leverages a single decoder-only LLM to adaptively extract query-relevant contents verbatim along with the necessary context, and section them based on their interconnectedness, thereby highlights information distinction, and aligns downstream LLMs with the original context effectively. Experiments show that a trained $\textit{Refiner}$ (with 7B parameters) exhibits significant gain to downstream LLM in improving answer accuracy, and outperforms other state-of-the-art advanced RAG and concurrent compressing approaches in various single-hop and multi-hop QA tasks. Notably, $\textit{Refiner}$ achieves a 80.5% tokens reduction and a 1.6-7.0% improvement margin in multi-hop tasks compared to the next best solution. $\textit{Refiner}$ is a plug-and-play solution that can be seamlessly integrated with RAG systems, facilitating its application across diverse open-source frameworks.

</details>


### [33] [Fane at SemEval-2025 Task 10: Zero-Shot Entity Framing with Large Language Models](https://arxiv.org/abs/2504.20469)

*Enfa Fane, Mihai Surdeanu, Eduardo Blanco, Steven R. Corman*

**Main category:** cs.CL

**Keywords:** large language models, news narratives, entity framing

**Relevance Score:** 8

**TL;DR:** This study evaluates large language models' (LLMs) ability to classify framing roles in news narratives, highlighting the impact of input context and prompting strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how news narratives frame entities and their implications on societal perceptions.

**Method:** The paper employs systematic experiments to assess the zero-shot capabilities of LLMs, focusing on input context, prompting strategies, and task decomposition.

**Key Contributions:** Evaluation of LLM zero-shot capabilities in framing roles, Introduction of a hierarchical approach for classification, Demonstration of the variation in prompt effectiveness per task level

**Result:** A hierarchical classification approach yields a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, outperforming single-step methods.

**Limitations:** 

**Future Work:** Further exploration of subtask-specific strategies and their impacts on LLM performance.

**Conclusion:** The study emphasizes the necessity for tailored prompt design and optimal input context to enhance LLM performance in framing classification tasks.

**Abstract:** Understanding how news narratives frame entities is crucial for studying media's impact on societal perceptions of events. In this paper, we evaluate the zero-shot capabilities of large language models (LLMs) in classifying framing roles. Through systematic experimentation, we assess the effects of input context, prompting strategies, and task decomposition. Our findings show that a hierarchical approach of first identifying broad roles and then fine-grained roles, outperforms single-step classification. We also demonstrate that optimal input contexts and prompts vary across task levels, highlighting the need for subtask-specific strategies. We achieve a Main Role Accuracy of 89.4% and an Exact Match Ratio of 34.5%, demonstrating the effectiveness of our approach. Our findings emphasize the importance of tailored prompt design and input context optimization for improving LLM performance in entity framing.

</details>


### [34] [Enhancing LLM Language Adaption through Cross-lingual In-Context Pre-training](https://arxiv.org/abs/2504.20484)

*Linjuan Wu, Haoran Wei, Huan Lin, Tianhao Li, Baosong Yang, Weiming Lu*

**Main category:** cs.CL

**Keywords:** Cross-lingual transfer, Multilingual performance, Large language models

**Relevance Score:** 8

**TL;DR:** The paper proposes Cross-lingual In-context Pre-training (CrossIC-PT), a method for enhancing cross-lingual transfer in LLMs using semantically related bilingual texts.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of parallel resources in existing methods for enhancing cross-lingual transfer in language models.

**Method:** CrossIC-PT samples are constructed by interleaving semantically related bilingual Wikipedia documents into a single context window and managing window size with a systematic segmentation policy.

**Key Contributions:** Introduction of Cross-lingual In-context Pre-training (CrossIC-PT) methodology., Demonstrated improvement in multilingual performance of state-of-the-art LLMs., Implementation of a semantic retrieval framework to enhance data availability.

**Result:** CrossIC-PT improves multilingual performance across three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) in six target languages, achieving notable performance gains.

**Limitations:** The study is still under review, indicating unverified results; further validation on a wider range of languages is needed.

**Future Work:** Investigation into more diverse cross-lingual datasets and potential scalability of the methodology.

**Conclusion:** The proposed approach shows promise in enhancing multilingual capabilities of LLMs without relying heavily on parallel resources.

**Abstract:** Large language models (LLMs) exhibit remarkable multilingual capabilities despite English-dominated pre-training, attributed to cross-lingual mechanisms during pre-training. Existing methods for enhancing cross-lingual transfer remain constrained by parallel resources, suffering from limited linguistic and domain coverage. We propose Cross-lingual In-context Pre-training (CrossIC-PT), a simple and scalable approach that enhances cross-lingual transfer by leveraging semantically related bilingual texts via simple next-word prediction. We construct CrossIC-PT samples by interleaving semantic-related bilingual Wikipedia documents into a single context window. To access window size constraints, we implement a systematic segmentation policy to split long bilingual document pairs into chunks while adjusting the sliding window mechanism to preserve contextual coherence. We further extend data availability through a semantic retrieval framework to construct CrossIC-PT samples from web-crawled corpus. Experimental results demonstrate that CrossIC-PT improves multilingual performance on three models (Llama-3.1-8B, Qwen2.5-7B, and Qwen2.5-1.5B) across six target languages, yielding performance gains of 3.79%, 3.99%, and 1.95%, respectively, with additional improvements after data augmentation.

</details>


### [35] [UniDetox: Universal Detoxification of Large Language Models via Dataset Distillation](https://arxiv.org/abs/2504.20500)

*Huimin Lu, Masaru Isonuma, Junichiro Mori, Ichiro Sakata*

**Main category:** cs.CL

**Keywords:** detoxification, large language models, dataset distillation, contrastive decoding, bias reduction

**Relevance Score:** 9

**TL;DR:** UniDetox is a universal method for detoxifying LLMs without model-specific tuning, using a novel dataset distillation approach and contrastive decoding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing detoxification methods that are model-specific and require extensive hyperparameter tuning.

**Method:** A dataset distillation technique utilizing contrastive decoding to create synthetic text data for detoxification across various LLMs.

**Key Contributions:** Universal application across multiple LLMs without model-specific tuning, Use of a novel dataset distillation technique for producing detoxifying representations, Reduction of politically biased content in LLM outputs

**Result:** Experiments show that detoxifying text distilled from GPT-2 effectively detoxifies larger models like OPT, Falcon, and LLaMA-2 without the need for separate hyperparameter tuning.

**Limitations:** 

**Future Work:** Exploration of further optimization techniques and evaluation of UniDetox on newer LLM architectures.

**Conclusion:** UniDetox offers a streamlined and effective way to mitigate toxicity in LLMs while reducing politically biased content in the generated text.

**Abstract:** We present UniDetox, a universally applicable method designed to mitigate toxicity across various large language models (LLMs). Previous detoxification methods are typically model-specific, addressing only individual models or model families, and require careful hyperparameter tuning due to the trade-off between detoxification efficacy and language modeling performance. In contrast, UniDetox provides a detoxification technique that can be universally applied to a wide range of LLMs without the need for separate model-specific tuning. Specifically, we propose a novel and efficient dataset distillation technique for detoxification using contrastive decoding. This approach distills detoxifying representations in the form of synthetic text data, enabling universal detoxification of any LLM through fine-tuning with the distilled text. Our experiments demonstrate that the detoxifying text distilled from GPT-2 can effectively detoxify larger models, including OPT, Falcon, and LLaMA-2. Furthermore, UniDetox eliminates the need for separate hyperparameter tuning for each model, as a single hyperparameter configuration can be seamlessly applied across different models. Additionally, analysis of the detoxifying text reveals a reduction in politically biased content, providing insights into the attributes necessary for effective detoxification of LLMs.

</details>


### [36] [Revisiting the MIMIC-IV Benchmark: Experiments Using Language Models for Electronic Health Records](https://arxiv.org/abs/2504.20547)

*Jesus Lovon, Thouria Ben-Haddi, Jules Di Scala, Jose G. Moreno, Lynda Tamine*

**Main category:** cs.CL

**Keywords:** MIMIC-IV, electronic health records, natural language processing, health informatics, LLM

**Relevance Score:** 9

**TL;DR:** This paper revisits the MIMIC-IV benchmark to standardize evaluation in medical text inputs, integrating EHR data with Hugging Face and exploring template usage for data conversion.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of standardized evaluation benchmarks in the medical domain for natural language models, which hinders their adoption in health tasks.

**Method:** Integration of MIMIC-IV data into Hugging Face datasets, followed by experiments using fine-tuned and zero-shot LLMs to assess their performance on the mortality of patients task.

**Key Contributions:** Integration of MIMIC-IV EHR data into the Hugging Face datasets library, Investigation of template methods for converting EHR tabular data to text, Evaluation of LLMs on mortality prediction tasks revealing strengths of fine-tuned models.

**Result:** Fine-tuned text-based models compete well against traditional tabular classifiers, while zero-shot LLMs perform poorly with EHR data.

**Limitations:** The study primarily focuses on mortality prediction, not addressing the full spectrum of EHR-related tasks.

**Future Work:** Further exploration of text-based methods and improvement directions for zero-shot LLM performance on EHR data.

**Conclusion:** Text-based models show potential in medical applications, highlighting a need for further development in zero-shot learning approaches for EHR data.

**Abstract:** The lack of standardized evaluation benchmarks in the medical domain for text inputs can be a barrier to widely adopting and leveraging the potential of natural language models for health-related downstream tasks. This paper revisited an openly available MIMIC-IV benchmark for electronic health records (EHRs) to address this issue. First, we integrate the MIMIC-IV data within the Hugging Face datasets library to allow an easy share and use of this collection. Second, we investigate the application of templates to convert EHR tabular data to text. Experiments using fine-tuned and zero-shot LLMs on the mortality of patients task show that fine-tuned text-based models are competitive against robust tabular classifiers. In contrast, zero-shot LLMs struggle to leverage EHR representations. This study underlines the potential of text-based approaches in the medical field and highlights areas for further improvement.

</details>


### [37] [BrAIcht, a theatrical agent that speaks like Bertolt Brecht's characters](https://arxiv.org/abs/2504.20552)

*Baz Roland, Kristina Malyseva, Anna Pappa, Tristan Cazenave*

**Main category:** cs.CL

**Keywords:** AI conversational agent, Bertolt Brecht, large language model, fine-tuning, dialogue generation

**Relevance Score:** 6

**TL;DR:** BrAIcht is an AI conversational agent that generates dialogues in the style of Bertolt Brecht using a fine-tuned large language model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a conversational agent that can replicate the distinctive dialogue style of German playwright Bertolt Brecht.

**Method:** BrAIcht was fine-tuned using German LeoLM, a 7 billion parameter LLM, employing a parameter-efficient technique called QLoRA on a dataset containing 29 of Brecht's plays and 907 similar German plays.

**Key Contributions:** Introduction of BrAIcht, an AI agent for generating dialogues in Brecht's style, Use of QLoRA for parameter-efficient fine-tuning, Development of a diverse training dataset from multiple German plays

**Result:** The performance of BrAIcht in generating dialogues was evaluated using BLEU score and perplexity, yielding promising results.

**Limitations:** 

**Future Work:** Further improvements on generating more nuanced dialogues and expanding the model's capabilities to other playwrights.

**Conclusion:** BrAIcht shows a significant capability in generating dialogues that mimic Bertolt Brecht's style, indicating the potential of using fine-tuned LLMs for creative applications in language generation.

**Abstract:** This project introduces BrAIcht, an AI conversational agent that creates dialogues in the distinctive style of the famous German playwright Bertolt Brecht. BrAIcht is fine-tuned using German LeoLM, a large language model with 7 billion parameters and a modified version of the base Llama2 suitable for German language tasks. For fine-tuning, 29 plays of Bertolt Brecht and 907 of other German plays that are stylistically similar to Bertolt Brecht are used to form a more di-erse dataset. Due to the limited memory capacity, a parameterefficient fine-tuning technique called QLoRA is implemented to train the large language model. The results, based on BLEU score and perplexity, show very promising performance of BrAIcht in generating dialogues in the style of Bertolt Brecht.

</details>


### [38] [ClonEval: An Open Voice Cloning Benchmark](https://arxiv.org/abs/2504.20581)

*Iwona Christop, Tomasz Kuczyński, Marek Kubis*

**Main category:** cs.CL

**Keywords:** voice cloning, text-to-speech, benchmark, evaluation, leaderboard

**Relevance Score:** 4

**TL;DR:** Introduction of a benchmark for evaluating voice cloning text-to-speech models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a standardized method for assessing the performance of voice cloning models.

**Method:** Development of an evaluation protocol, an open-source library, and a leaderboard for benchmarking.

**Key Contributions:** Novel benchmark for voice cloning models, Open-source performance assessment library, Evaluation protocol and leaderboard for consistent assessment

**Result:** Provides a structured way to evaluate and compare voice cloning models through a software library and leaderboard.

**Limitations:** 

**Future Work:** Improvement of the benchmark and expansion to cover additional voice characteristics and model types.

**Conclusion:** The benchmark serves as a comprehensive tool for researchers to evaluate their voice cloning technologies.

**Abstract:** We present a novel benchmark for voice cloning text-to-speech models. The benchmark consists of an evaluation protocol, an open-source library for assessing the performance of voice cloning models, and an accompanying leaderboard. The paper discusses design considerations and presents a detailed description of the evaluation procedure. The usage of the software library is explained, along with the organization of results on the leaderboard.

</details>


### [39] [TF1-EN-3M: Three Million Synthetic Moral Fables for Training Small, Open Language Models](https://arxiv.org/abs/2504.20605)

*Mihai Nadas, Laura Diosan, Andrei Piscoran, Andreea Tomescu*

**Main category:** cs.CL

**Keywords:** moral storytelling, NLP, dataset, instruction tuning, narrative intelligence

**Relevance Score:** 4

**TL;DR:** The paper presents TF1-EN-3M, an open dataset of three million English-language fables generated by instruction-tuned models, providing a structured corpus for moral storytelling.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a large, structured corpus of narratives paired with explicit ethical lessons, as modern NLP lacks such a resource.

**Method:** The fables are generated using a combinatorial prompt engine based on a six-slot scaffold. A hybrid evaluation pipeline is used to assess grammar, creativity, moral clarity, template adherence, diversity, and readability.

**Key Contributions:** Release of the first open dataset of three million moral fables, Introduction of a hybrid evaluation pipeline for narrative quality, Demonstration of cost-effective narrative generation using smaller models

**Result:** An 8B-parameter Llama-3 variant produced high-quality fables efficiently, utilizing minimal GPU resources at a low cost.

**Limitations:** 

**Future Work:** Future research can explore instruction following, narrative intelligence, and the integration of moral storytelling into child-friendly educational AI.

**Conclusion:** The TF1-EN-3M dataset facilitates research in various NLP areas, demonstrating effective narrative generation without reliance on large proprietary models.

**Abstract:** Moral stories are a time-tested vehicle for transmitting values, yet modern NLP lacks a large, structured corpus that couples coherent narratives with explicit ethical lessons. We close this gap with TF1-EN-3M, the first open dataset of three million English-language fables generated exclusively by instruction-tuned models no larger than 8B parameters. Each story follows a six-slot scaffold (character -> trait -> setting -> conflict -> resolution -> moral), produced through a combinatorial prompt engine that guarantees genre fidelity while covering a broad thematic space.   A hybrid evaluation pipeline blends (i) a GPT-based critic that scores grammar, creativity, moral clarity, and template adherence with (ii) reference-free diversity and readability metrics. Among ten open-weight candidates, an 8B-parameter Llama-3 variant delivers the best quality-speed trade-off, producing high-scoring fables on a single consumer GPU (<24 GB VRAM) at approximately 13.5 cents per 1,000 fables.   We release the dataset, generation code, evaluation scripts, and full metadata under a permissive license, enabling exact reproducibility and cost benchmarking. TF1-EN-3M opens avenues for research in instruction following, narrative intelligence, value alignment, and child-friendly educational AI, demonstrating that large-scale moral storytelling no longer requires proprietary giant models.

</details>


### [40] [WenyanGPT: A Large Language Model for Classical Chinese Tasks](https://arxiv.org/abs/2504.20609)

*Xinyu Yao, Mengdi Wang, Bo Chen, Xiaobing Zhao*

**Main category:** cs.CL

**Keywords:** Classical Chinese, natural language processing, WenyanGPT, language model, evaluation benchmark

**Relevance Score:** 2

**TL;DR:** This paper presents WenyanGPT, a language model tailored for Classical Chinese, which outperforms existing models on Classical Chinese tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Classical Chinese is crucial for the study of ancient literature, yet existing NLP models are optimized for Modern Chinese and perform poorly on Classical Chinese tasks.

**Method:** The approach involves continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model to create WenyanGPT, along with developing an evaluation benchmark dataset named WenyanBENCH.

**Key Contributions:** Introduction of WenyanGPT for Classical Chinese processing, Development of the WenyanBENCH evaluation benchmark dataset, Public availability of training and evaluation data for research advancement

**Result:** WenyanGPT significantly outperforms current advanced LLMs on various Classical Chinese tasks, as demonstrated by experimental results on WenyanBENCH.

**Limitations:** 

**Future Work:** Encouraging further exploration and development in the domain of Classical Chinese processing.

**Conclusion:** The paper highlights the importance of specialized models for Classical Chinese and makes resources publicly available to foster further research.

**Abstract:** Classical Chinese, as the core carrier of Chinese culture, plays a crucial role in the inheritance and study of ancient literature. However, existing natural language processing models primarily optimize for Modern Chinese, resulting in inadequate performance on Classical Chinese. This paper presents a comprehensive solution for Classical Chinese language processing. By continuing pre-training and instruction fine-tuning on the LLaMA3-8B-Chinese model, we construct a large language model, WenyanGPT, which is specifically designed for Classical Chinese tasks. Additionally, we develop an evaluation benchmark dataset, WenyanBENCH. Experimental results on WenyanBENCH demonstrate that WenyanGPT significantly outperforms current advanced LLMs in various Classical Chinese tasks. We make the model's training data, instruction fine-tuning data\footnote, and evaluation benchmark dataset publicly available to promote further research and development in the field of Classical Chinese processing.

</details>


### [41] [Cooking Up Creativity: A Cognitively-Inspired Approach for Enhancing LLM Creativity through Structured Representations](https://arxiv.org/abs/2504.20643)

*Moran Mizrahi, Chen Shani, Gabriel Stanovsky, Dan Jurafsky, Dafna Shahaf*

**Main category:** cs.CL

**Keywords:** Large Language Models, creativity, structured representations, cognitive manipulations, culinary recipes

**Relevance Score:** 8

**TL;DR:** Introducing a novel approach that enhances creativity in Large Language Models (LLMs) using structured representations, demonstrated through culinary recipe generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of creativity in existing Large Language Models by exploring structured representations and manipulations for idea generation.

**Method:** The approach couples LLMs with structured representations and cognitively inspired techniques to generate creative outputs, specifically in the domain of culinary recipes.

**Key Contributions:** Introduction of a structured representation approach to enhance LLM creativity, Demonstration of improved novelty and diversity in creative tasks, Expert evaluations supporting the effectiveness of the proposed method

**Result:** The model, DishCOVER, was tested against GPT-4o, showing greater diversity and novelty in generated recipes, with expert evaluations highlighting superior coherence and feasibility.

**Limitations:** 

**Future Work:** Inspiration for further exploration into structured creativity in AI and its applications across various domains.

**Conclusion:** The proposed method surpasses existing LLMs in generating creative outputs, suggesting new avenues for research in structured creativity within AI.

**Abstract:** Large Language Models (LLMs) excel at countless tasks, yet struggle with creativity. In this paper, we introduce a novel approach that couples LLMs with structured representations and cognitively inspired manipulations to generate more creative and diverse ideas. Our notion of creativity goes beyond superficial token-level variations; rather, we explicitly recombine structured representations of existing ideas, allowing our algorithm to effectively explore the more abstract landscape of ideas. We demonstrate our approach in the culinary domain with DishCOVER, a model that generates creative recipes. Experiments comparing our model's results to those of GPT-4o show greater diversity. Domain expert evaluations reveal that our outputs, which are mostly coherent and feasible culinary creations, significantly surpass GPT-4o in terms of novelty, thus outperforming it in creative generation. We hope our work inspires further research into structured creativity in AI.

</details>


### [42] [A Generative-AI-Driven Claim Retrieval System Capable of Detecting and Retrieving Claims from Social Media Platforms in Multiple Languages](https://arxiv.org/abs/2504.20668)

*Ivan Vykopal, Martin Hyben, Robert Moro, Michal Gregor, Jakub Simko*

**Main category:** cs.CL

**Keywords:** disinformation, fact-checking, large language models, efficiency, validation

**Relevance Score:** 8

**TL;DR:** The paper presents a method using large language models (LLMs) to enhance the efficiency of fact-checkers by retrieving and summarizing previously fact-checked claims, thus reducing redundancy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the global challenge of online disinformation by improving the efficiency of fact-checkers who verify claims.

**Method:** The proposed method utilizes large language models to filter and summarize previously fact-checked claims, assessing their relevance for current claims.

**Key Contributions:** Introduction of a method leveraging LLMs for relevance filtering of fact-checks, Development of a tool that assists fact-checkers with concise summaries of past claims, Evaluation through both automated and human assessments to validate effectiveness

**Result:** The evaluation shows that LLMs can filter out irrelevant fact-checks effectively, streamlining the fact-checking process.

**Limitations:** The paper does not extensively discuss the handling of nuanced claims that may require deeper contextual understanding beyond existing fact-checks.

**Future Work:** Exploration of improving LLM capabilities to handle a wider range of claims and integrating more diverse data sources for fact-checking.

**Conclusion:** The introduced approach demonstrates significant potential in reducing the workload of fact-checkers and improving their response time to new claims.

**Abstract:** Online disinformation poses a global challenge, placing significant demands on fact-checkers who must verify claims efficiently to prevent the spread of false information. A major issue in this process is the redundant verification of already fact-checked claims, which increases workload and delays responses to newly emerging claims. This research introduces an approach that retrieves previously fact-checked claims, evaluates their relevance to a given input, and provides supplementary information to support fact-checkers. Our method employs large language models (LLMs) to filter irrelevant fact-checks and generate concise summaries and explanations, enabling fact-checkers to faster assess whether a claim has been verified before. In addition, we evaluate our approach through both automatic and human assessments, where humans interact with the developed tool to review its effectiveness. Our results demonstrate that LLMs are able to filter out many irrelevant fact-checks and, therefore, reduce effort and streamline the fact-checking process.

</details>


### [43] [Non-native Children's Automatic Speech Assessment Challenge (NOCASA)](https://arxiv.org/abs/2504.20678)

*Yaroslav Getman, Tamás Grósz, Mikko Kurimo, Giampiero Salvi*

**Main category:** cs.CL

**Keywords:** automatic speech assessment, non-native children, pronunciation training, gamification, machine learning

**Relevance Score:** 6

**TL;DR:** Introduction of the NOCASA competition for automatic speech assessment of L2 learners, emphasizing development challenges and baseline systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a gamified app for pronunciation training by assessing L2 learners' speech, addressing challenges like limited training data and category imbalance.

**Method:** The competition provides pseudo-anonymized training data of 10,334 recordings from 44 speakers and features two baseline systems: an SVM classifier and a multi-task wav2vec 2.0 model.

**Key Contributions:** Introduction of NOCASA data competition for speech assessment, Provision of a large dataset for L2 learners, Release of baseline models for benchmarking

**Result:** The wav2vec 2.0 model achieves the best performance on the test set with an unweighted average recall of 36.37%.

**Limitations:** Limited training data and highly unbalanced pronunciation level categories.

**Future Work:** Encouraging further research into better assessment systems and data diversity to improve L2 learner training.

**Conclusion:** The NOCASA competition aims to stimulate development in speech assessment systems for young L2 learners, offering valuable datasets and models for further research.

**Abstract:** This paper presents the "Non-native Children's Automatic Speech Assessment" (NOCASA) - a data competition part of the IEEE MLSP 2025 conference. NOCASA challenges participants to develop new systems that can assess single-word pronunciations of young second language (L2) learners as part of a gamified pronunciation training app. To achieve this, several issues must be addressed, most notably the limited nature of available training data and the highly unbalanced distribution among the pronunciation level categories. To expedite the development, we provide a pseudo-anonymized training data (TeflonNorL2), containing 10,334 recordings from 44 speakers attempting to pronounce 205 distinct Norwegian words, human-rated on a 1 to 5 scale (number of stars that should be given in the game). In addition to the data, two already trained systems are released as official baselines: an SVM classifier trained on the ComParE_16 acoustic feature set and a multi-task wav2vec 2.0 model. The latter achieves the best performance on the challenge test set, with an unweighted average recall (UAR) of 36.37%.

</details>


### [44] [Are Information Retrieval Approaches Good at Harmonising Longitudinal Survey Questions in Social Science?](https://arxiv.org/abs/2504.20679)

*Wing Yan Li, Zeqiang Wang, Jon Johnson, Suparna De*

**Main category:** cs.CL

**Keywords:** information retrieval, longitudinal studies, semantic equivalence, unsupervised learning, social science

**Relevance Score:** 4

**TL;DR:** This paper addresses the challenges of detecting semantically equivalent questions in longitudinal social science surveys to enhance harmonization across studies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Automated detection of semantically equivalent questions is crucial for improving empirical research in social, economic, and health sciences.

**Method:** The paper explores multiple unsupervised approaches, including probabilistic models, linear probing of language models, and pre-trained neural networks to identify concept equivalence in longitudinal survey data from 1946-2020.

**Key Contributions:** Introduction of a new information retrieval task for harmonizing longitudinal surveys, Evaluation of various unsupervised methods on historical survey data, Insights into model sensitivity regarding lexical overlap in questions

**Result:** IR-specialised neural models achieve the highest performance, with modest improvements when re-ranking results from the probabilistic model.

**Limitations:** Models show low sensitivity to questions with high lexical overlap, especially with mismatched sub-concepts.

**Future Work:** Further research required to improve harmonization techniques and sensitivity of models to specific question structures.

**Conclusion:** The findings emphasize the need for better sensitivity to questions with high lexical overlap during the harmonization process of longitudinal studies.

**Abstract:** Automated detection of semantically equivalent questions in longitudinal social science surveys is crucial for long-term studies informing empirical research in the social, economic, and health sciences. Retrieving equivalent questions faces dual challenges: inconsistent representation of theoretical constructs (i.e. concept/sub-concept) across studies as well as between question and response options, and the evolution of vocabulary and structure in longitudinal text. To address these challenges, our multi-disciplinary collaboration of computer scientists and survey specialists presents a new information retrieval (IR) task of identifying concept (e.g. Housing, Job, etc.) equivalence across question and response options to harmonise longitudinal population studies. This paper investigates multiple unsupervised approaches on a survey dataset spanning 1946-2020, including probabilistic models, linear probing of language models, and pre-trained neural networks specialised for IR. We show that IR-specialised neural models achieve the highest overall performance with other approaches performing comparably. Additionally, the re-ranking of the probabilistic model's results with neural models only introduces modest improvements of 0.07 at most in F1-score. Qualitative post-hoc evaluation by survey specialists shows that models generally have a low sensitivity to questions with high lexical overlap, particularly in cases where sub-concepts are mismatched. Altogether, our analysis serves to further research on harmonising longitudinal studies in social science.

</details>


### [45] [Can LLMs Detect Intrinsic Hallucinations in Paraphrasing and Machine Translation?](https://arxiv.org/abs/2504.20699)

*Evangelia Gogoulou, Shorouq Zahra, Liane Guillou, Luise Dürlich, Joakim Nivre*

**Main category:** cs.CL

**Keywords:** hallucination, LLMs, NLI models, translation, paraphrasing

**Relevance Score:** 8

**TL;DR:** Study evaluates open-access LLMs on hallucination detection in translation and paraphrasing tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of hallucination in LLM outputs and evaluate the capability of these models in detecting such occurrences across different tasks and languages.

**Method:** Evaluation of various open-access LLMs on their ability to detect intrinsic hallucinations in translation and paraphrasing tasks, considering factors like model size, instruction tuning, and prompt choice.

**Key Contributions:** Introduced the HalluciGen task for hallucination detection and generation., Showcased the variability in LLM performance across tasks and languages., Demonstrated the effectiveness of NLI models in detecting hallucinations.

**Result:** Performance varies across models but is consistent across prompts; NLI models perform comparably well to LLM-based detectors for this task.

**Limitations:** The study may not cover all possible tasks or languages, limiting the generalizability of the results.

**Future Work:** Further exploration of hallucination detection across additional tasks and model architectures.

**Conclusion:** The findings suggest that while LLMs can detect hallucinations, NLI models are also effective, indicating a broader range of viable options for detecting hallucination.

**Abstract:** A frequently observed problem with LLMs is their tendency to generate output that is nonsensical, illogical, or factually incorrect, often referred to broadly as hallucination. Building on the recently proposed HalluciGen task for hallucination detection and generation, we evaluate a suite of open-access LLMs on their ability to detect intrinsic hallucinations in two conditional generation tasks: translation and paraphrasing. We study how model performance varies across tasks and language and we investigate the impact of model size, instruction tuning, and prompt choice. We find that performance varies across models but is consistent across prompts. Finally, we find that NLI models perform comparably well, suggesting that LLM-based detectors are not the only viable option for this specific task.

</details>


### [46] [BrightCookies at SemEval-2025 Task 9: Exploring Data Augmentation for Food Hazard Classification](https://arxiv.org/abs/2504.20703)

*Foteini Papadopoulou, Osman Mutlu, Neris Özen, Bas H. M. van der Velden, Iris Hendrickx, Ali Hürriyetoğlu*

**Main category:** cs.CL

**Keywords:** Food Hazard Detection, Text Augmentation, Transformer Models

**Relevance Score:** 6

**TL;DR:** The paper develops a system for the Food Hazard Detection Challenge, evaluating the impact of text augmentation techniques on classification performance of hazards in food recall reports.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the classification of food hazards and products using explainable classification systems that can handle minority classes effectively.

**Method:** The authors employed three word-level data augmentation techniques: synonym replacement, random word swapping, and contextual word insertion, and compared their effects on various transformer and machine learning models.

**Key Contributions:** Development of a classification system for food hazard detection, Evaluation of text augmentation techniques across different models, Demonstrated effectiveness of contextual word insertion for minority classes

**Result:** The results indicated that while transformer models generally performed better, the augmentation techniques did not consistently yield improvements across all categories; however, contextual word insertion showed a 6% accuracy gain for minority classes when using the BERT model.

**Limitations:** The three augmentation techniques did not consistently improve overall performance across all hazard categories.

**Future Work:** Future research could explore additional augmentation methods or more sophisticated models that might better address the classification of minority classes.

**Conclusion:** Targeted augmentation for minority classes can enhance the performance of transformer models, particularly for fine-grained categories in food hazard classification.

**Abstract:** This paper presents our system developed for the SemEval-2025 Task 9: The Food Hazard Detection Challenge. The shared task's objective is to evaluate explainable classification systems for classifying hazards and products in two levels of granularity from food recall incident reports. In this work, we propose text augmentation techniques as a way to improve poor performance on minority classes and compare their effect for each category on various transformer and machine learning models. We explore three word-level data augmentation techniques, namely synonym replacement, random word swapping, and contextual word insertion. The results show that transformer models tend to have a better overall performance. None of the three augmentation techniques consistently improved overall performance for classifying hazards and products. We observed a statistically significant improvement (P < 0.05) in the fine-grained categories when using the BERT model to compare the baseline with each augmented model. Compared to the baseline, the contextual words insertion augmentation improved the accuracy of predictions for the minority hazard classes by 6%. This suggests that targeted augmentation of minority classes can improve the performance of transformer models.

</details>


### [47] [Beyond the Last Answer: Your Reasoning Trace Uncovers More than You Think](https://arxiv.org/abs/2504.20708)

*Hasan Abed Al Kader Hammoud, Hani Itani, Bernard Ghanem*

**Main category:** cs.CL

**Keywords:** Large Language Models, subthoughts, reasoning, accuracy improvement, mathematical reasoning

**Relevance Score:** 9

**TL;DR:** This paper challenges the reliance on final answers in LLM evaluations, proposing a method that analyzes intermediate reasoning steps (subthoughts) for improved accuracy.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to evaluate the assurance of final answers produced by LLMs and explore alternative reasoning paths.

**Method:** The proposed method segments reasoning traces into subthoughts based on linguistic cues and generates continuations from the end-points of these subthoughts to extract potential answers.

**Key Contributions:** Introduction of the concept of 'subthoughts' for reasoning analysis in LLMs, A method for aggregating intermediate answers to improve evaluation accuracy, Demonstration of significant accuracy improvements across multiple LLMs and datasets.

**Result:** Aggregating answers from different subthoughts using the mode often yields higher accuracy than relying solely on the initial answer, with improvements noted in accuracy on various LLMs and datasets (up to 13% and 10%).

**Limitations:** 

**Future Work:** Future research could further explore the characteristics of subthoughts and their role in improving LLM performance in various reasoning tasks.

**Conclusion:** The findings suggest that analyzing subthoughts can enhance the identification of reliable answers and improve overall model accuracy in reasoning tasks.

**Abstract:** Large Language Models (LLMs) leverage step-by-step reasoning to solve complex problems. Standard evaluation practice involves generating a complete reasoning trace and assessing the correctness of the final answer presented at its conclusion. In this paper, we challenge the reliance on the final answer by posing the following two questions: Does the final answer reliably represent the model's optimal conclusion? Can alternative reasoning paths yield different results? To answer these questions, we analyze intermediate reasoning steps, termed subthoughts, and propose a method based on our findings. Our approach involves segmenting a reasoning trace into sequential subthoughts based on linguistic cues. We start by prompting the model to generate continuations from the end-point of each intermediate subthought. We extract a potential answer from every completed continuation originating from different subthoughts. We find that aggregating these answers by selecting the most frequent one (the mode) often yields significantly higher accuracy compared to relying solely on the answer derived from the original complete trace. Analyzing the consistency among the answers derived from different subthoughts reveals characteristics that correlate with the model's confidence and correctness, suggesting potential for identifying less reliable answers. Our experiments across various LLMs and challenging mathematical reasoning datasets (AIME2024 and AIME2025) show consistent accuracy improvements, with gains reaching up to 13\% and 10\% respectively. Implementation is available at: https://github.com/hammoudhasan/SubthoughtReasoner.

</details>


### [48] [UniversalRAG: Retrieval-Augmented Generation over Multiple Corpora with Diverse Modalities and Granularities](https://arxiv.org/abs/2504.20734)

*Woongyeong Yeo, Kangsan Kim, Soyeong Jeong, Jinheon Baek, Sung Ju Hwang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Heterogeneous Sources, Modality-aware Routing

**Relevance Score:** 9

**TL;DR:** UniversalRAG is a novel retrieval-augmented generation framework that integrates knowledge from multiple heterogeneous sources with diverse modalities to improve factual accuracy in responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing RAG approaches that focus on single modality or unified representation, leading to a modality gap.

**Method:** UniversalRAG employs a modality-aware routing mechanism to identify the relevant modality-specific corpus for targeted retrieval and organizes each modality into multiple granularity levels for more precise querying.

**Key Contributions:** Introduction of a novel RAG framework that retrieves knowledge from heterogeneous modalities., Developed a modality-aware routing mechanism for more accurate retrieval., Organized modalities into multiple granularity levels to improve query handling.

**Result:** UniversalRAG outperforms existing modality-specific and unified retrieval approaches across 8 benchmarks involving various modalities.

**Limitations:** 

**Future Work:** Further exploration of additional modalities and improvements in the routing mechanism for even better retrieval accuracy.

**Conclusion:** The framework enhances the effectiveness of retrieval-augmented generation by allowing for cross-modal and granularity-sensitive knowledge integration.

**Abstract:** Retrieval-Augmented Generation (RAG) has shown substantial promise in improving factual accuracy by grounding model responses with external knowledge relevant to queries. However, most existing RAG approaches are limited to a text-only corpus, and while recent efforts have extended RAG to other modalities such as images and videos, they typically operate over a single modality-specific corpus. In contrast, real-world queries vary widely in the type of knowledge they require, which a single type of knowledge source cannot address. To address this, we introduce UniversalRAG, a novel RAG framework designed to retrieve and integrate knowledge from heterogeneous sources with diverse modalities and granularities. Specifically, motivated by the observation that forcing all modalities into a unified representation space derived from a single combined corpus causes a modality gap, where the retrieval tends to favor items from the same modality as the query, we propose a modality-aware routing mechanism that dynamically identifies the most appropriate modality-specific corpus and performs targeted retrieval within it. Also, beyond modality, we organize each modality into multiple granularity levels, enabling fine-tuned retrieval tailored to the complexity and scope of the query. We validate UniversalRAG on 8 benchmarks spanning multiple modalities, showing its superiority over modality-specific and unified baselines.

</details>


### [49] [Grokking in the Wild: Data Augmentation for Real-World Multi-Hop Reasoning with Transformers](https://arxiv.org/abs/2504.20752)

*Roman Abramov, Felix Steinbauer, Gjergji Kasneci*

**Main category:** cs.CL

**Keywords:** Transformers, grokking, multi-step reasoning, synthetic data, knowledge graphs

**Relevance Score:** 9

**TL;DR:** This paper extends the concept of grokking to real-world factual data by augmenting knowledge graphs with synthetic data, enhancing multi-step reasoning in Transformers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address deficiencies in multi-step factual reasoning in Transformers when real-world knowledge is sparse, leveraging the concept of grokking.

**Method:** Augment existing knowledge graphs with synthetic data to surpass the threshold ratio of inferred to atomic facts, focusing on enhancing emergent reasoning capabilities in Transformers.

**Key Contributions:** First to apply grokking to real-world factual data, Demonstrated that factually incorrect synthetic data can improve reasoning capabilities, Achieved state-of-the-art results in multi-hop reasoning benchmarks

**Result:** Achieved 95-100% accuracy on the 2WikiMultiHopQA multi-hop reasoning benchmark, outperforming strong baselines and matching current state-of-the-art results.

**Limitations:** 

**Future Work:** Exploration of further enhancing reasoning capabilities in broader contexts and potential applications of grokking-based data augmentation.

**Conclusion:** Synthetic data, even when factually incorrect, can enhance reasoning circuits by promoting reliance on relational structures, thus improving model generalization.

**Abstract:** Transformers have achieved great success in numerous NLP tasks but continue to exhibit notable gaps in multi-step factual reasoning, especially when real-world knowledge is sparse. Recent advances in grokking have demonstrated that neural networks can transition from memorizing to perfectly generalizing once they detect underlying logical patterns - yet these studies have primarily used small, synthetic tasks. In this paper, for the first time, we extend grokking to real-world factual data and address the challenge of dataset sparsity by augmenting existing knowledge graphs with carefully designed synthetic data to raise the ratio $\phi_r$ of inferred facts to atomic facts above the threshold required for grokking. Surprisingly, we find that even factually incorrect synthetic data can strengthen emergent reasoning circuits rather than degrade accuracy, as it forces the model to rely on relational structure rather than memorization. When evaluated on multi-hop reasoning benchmarks, our approach achieves up to 95-100% accuracy on 2WikiMultiHopQA - substantially improving over strong baselines and matching or exceeding current state-of-the-art results. We further provide an in-depth analysis of how increasing $\phi_r$ drives the formation of generalizing circuits inside Transformers. Our findings suggest that grokking-based data augmentation can unlock implicit multi-hop reasoning capabilities, opening the door to more robust and interpretable factual reasoning in large-scale language models.

</details>


### [50] [Chain-of-Defensive-Thought: Structured Reasoning Elicits Robustness in Large Language Models against Reference Corruption](https://arxiv.org/abs/2504.20769)

*Wenxiao Wang, Parsa Hosseini, Soheil Feizi*

**Main category:** cs.CL

**Keywords:** chain-of-thought, robustness, large language models, reference corruption, natural questions

**Relevance Score:** 8

**TL;DR:** This paper introduces a method called chain-of-defensive-thought to enhance the robustness of large language models against reference corruption.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Chain-of-thought prompting has been successful in improving reasoning in large language models. This work aims to leverage these reasoning enhancements to increase the robustness of models in tasks beyond pure reasoning.

**Method:** The method, chain-of-defensive-thought, involves providing a few exemplars with structured and defensive reasoning as demonstrations to large language models.

**Key Contributions:** Introduction of the chain-of-defensive-thought method., Demonstrated empirical improvements in model robustness across diverse tasks., Revealed significant accuracy retention in the presence of prompt injection attacks.

**Result:** The approach significantly improves the models' robustness against reference corruption, as demonstrated by maintaining a high accuracy in the Natural Questions task despite reference corruption.

**Limitations:** 

**Future Work:** Further exploration of the applicability of chain-of-defensive-thought across various tasks and models.

**Conclusion:** Chain-of-defensive-thought is a simple yet effective method that can be broadly applied to enhance the performance and robustness of large language models.

**Abstract:** Chain-of-thought prompting has demonstrated great success in facilitating the reasoning abilities of large language models. In this work, we explore how these enhanced reasoning abilities can be exploited to improve the robustness of large language models in tasks that are not necessarily reasoning-focused. In particular, we show how a wide range of large language models exhibit significantly improved robustness against reference corruption using a simple method called chain-of-defensive-thought, where only a few exemplars with structured and defensive reasoning are provided as demonstrations. Empirically, the improvements can be astounding, especially given the simplicity and applicability of the method. For example, in the Natural Questions task, the accuracy of GPT-4o degrades from 60% to as low as 3% with standard prompting when 1 out of 10 references provided is corrupted with prompt injection attacks. In contrast, GPT-4o using chain-of-defensive-thought prompting maintains an accuracy of 50%.

</details>


### [51] [Turing Machine Evaluation for Large Language Model](https://arxiv.org/abs/2504.20771)

*Haitao Wu, Zongbo Han, Huaxi Huang, Changqing Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, computational reasoning, evaluation framework, Turing Machine, TMBench

**Relevance Score:** 9

**TL;DR:** This paper presents TMBench, an evaluation framework for assessing the computational reasoning abilities of Large Language Models (LLMs) using Universal Turing Machine (UTM) simulation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for rigorous evaluation of LLMs, particularly their computational reasoning abilities, crucial for complex tasks like code generation and multi-step problem-solving.

**Method:** A framework based on Universal Turing Machine simulation is proposed, requiring LLMs to follow instructions and track states during computations.

**Key Contributions:** Development of TMBench for evaluating LLMs' computational reasoning, Knowledge-agnostic and adjustable difficulty in evaluation, Correlation of TMBench results with established reasoning benchmarks

**Result:** TMBench shows a strong correlation (Pearson coefficient of 0.73) with other reasoning benchmarks, highlighting its effectiveness in evaluating LLMs' reasoning capabilities.

**Limitations:** 

**Future Work:** Further enhancement and expansion of the benchmark to include more complex reasoning tasks.

**Conclusion:** Computational reasoning is a key dimension for assessing the advanced capabilities of LLMs, and TMBench provides a scalable and rigorous evaluation method.

**Abstract:** With the rapid development and widespread application of Large Language Models (LLMs), rigorous evaluation has become particularly crucial. This research adopts a novel perspective, focusing on evaluating the core computational reasoning ability of LLMs, defined as the capacity of model to accurately understand rules, and execute logically computing operations. This capability assesses the reliability of LLMs as precise executors, and is critical to advanced tasks such as complex code generation and multi-step problem-solving. We propose an evaluation framework based on Universal Turing Machine (UTM) simulation. This framework requires LLMs to strictly follow instructions and track dynamic states, such as tape content and read/write head position, during multi-step computations. To enable standardized evaluation, we developed TMBench, a benchmark for systematically studying the computational reasoning capabilities of LLMs. TMBench provides several key advantages, including knowledge-agnostic evaluation, adjustable difficulty, foundational coverage through Turing machine encoding, and unlimited capacity for instance generation, ensuring scalability as models continue to evolve. We find that model performance on TMBench correlates strongly with performance on other recognized reasoning benchmarks (Pearson correlation coefficient is 0.73), clearly demonstrating that computational reasoning is a significant dimension for measuring the deep capabilities of LLMs. Code and data are available at https://github.com/HaitaoWuTJU/Turing-Machine-Bench.

</details>


### [52] [Universal language model with the intervention of quantum theory](https://arxiv.org/abs/2504.20839)

*D. -F. Qin*

**Main category:** cs.CL

**Keywords:** quantum mechanics, language modeling, word embeddings, quantum statistics, generative models

**Relevance Score:** 4

**TL;DR:** This paper explores the application of quantum mechanics in language modeling, proposing improvements to word embedding techniques through quantum principles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To build a representation model of natural language that incorporates quantum mechanics and addresses the limitations of traditional language modeling techniques.

**Method:** The paper introduces quantum mechanics to the symbol-meaning pairs of language and proposes improvements to word embeddings using quantum statistics and related theories.

**Key Contributions:** Introduction of quantum mechanics into language modeling, Improvement of word embeddings through quantum statistical methods, Preliminary framework for future research in quantum computing applications for natural language

**Result:** The feasibility of using quantum theory for natural language modeling is demonstrated through experimental code, suggesting improvements in generative models and future applications in quantum computing.

**Limitations:** The practical implementation of quantum theories in language processing remains exploratory and requires extensive validation.

**Future Work:** Further exploration of quantum properties in information theory and their applications in quantum computing for natural language processing.

**Conclusion:** This research suggests a new paradigm for language modeling that integrates quantum mechanics, potentially enhancing the understanding and application of natural language processing techniques.

**Abstract:** This paper examines language modeling based on the theory of quantum mechanics. It focuses on the introduction of quantum mechanics into the symbol-meaning pairs of language in order to build a representation model of natural language. At the same time, it is realized that word embedding, which is widely used as a basic technique for statistical language modeling, can be explained and improved by the mathematical framework of quantum mechanics. On this basis, this paper continues to try to use quantum statistics and other related theories to study the mathematical representation, natural evolution and statistical properties of natural language. It is also assumed that the source of such quantum properties is the physicality of information. The feasibility of using quantum theory to model natural language is pointed out through the construction of a experimental code. The paper discusses, in terms of applications, the possible help of the theory in constructing generative models that are popular nowadays. A preliminary discussion of future applications of the theory to quantum computers is also presented.

</details>


### [53] [JaccDiv: A Metric and Benchmark for Quantifying Diversity of Generated Marketing Text in the Music Industry](https://arxiv.org/abs/2504.20849)

*Anum Afzal, Alexandre Mercier, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Data-to-Text, Generative Models, Marketing Texts, Diversity Evaluation, Language Models

**Relevance Score:** 7

**TL;DR:** This paper explores LLM-based data-to-text methods for generating diverse marketing texts, addressing issues of repetitiveness in automated content generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the problem of monotony in generated content through traditional methods, enabling more effective marketing text creation.

**Method:** Utilizing Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 with fine-tuning, few-shot, and zero-shot techniques, while proposing a new metric, JaccDiv, to measure text diversity.

**Key Contributions:** Introduction of JaccDiv metric for evaluating text diversity., Application of multiple LLMs for improving marketing text generation., Demonstration of LLM effectiveness beyond music, applicable in diverse fields.

**Result:** Demonstrated the ability to generate higher quality and more diverse marketing texts, establishing a baseline for future applications.

**Limitations:** Focus primarily on marketing texts; further research needed to expand applicability to other forms of content.

**Future Work:** Exploration of additional applications of the proposed methods in different fields and further refinement of the JaccDiv metric.

**Conclusion:** The findings suggest that LLMs can produce better outcomes for marketing text generation across various sectors, reducing repetitive patterns.

**Abstract:** Online platforms are increasingly interested in using Data-to-Text technologies to generate content and help their users. Unfortunately, traditional generative methods often fall into repetitive patterns, resulting in monotonous galleries of texts after only a few iterations. In this paper, we investigate LLM-based data-to-text approaches to automatically generate marketing texts that are of sufficient quality and diverse enough for broad adoption. We leverage Language Models such as T5, GPT-3.5, GPT-4, and LLaMa2 in conjunction with fine-tuning, few-shot, and zero-shot approaches to set a baseline for diverse marketing texts. We also introduce a metric JaccDiv to evaluate the diversity of a set of texts. This research extends its relevance beyond the music industry, proving beneficial in various fields where repetitive automated content generation is prevalent.

</details>


### [54] [DYNAMAX: Dynamic computing for Transformers and Mamba based architectures](https://arxiv.org/abs/2504.20922)

*Miguel Nogales, Matteo Gambella, Manuel Roveri*

**Main category:** cs.CL

**Keywords:** early exits, Mamba architectures, computational efficiency, large language models, dynamic processing

**Relevance Score:** 8

**TL;DR:** DYNAMAX introduces a framework for early exits (EEs) in Mamba architectures, optimizing computational efficiency in LLMs and demonstrating their effectiveness across various NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the integration of early exits in Mamba architectures, which remain underutilized compared to encoder-only Transformers, and to enhance computational efficiency in large language models (LLMs).

**Method:** The study presents DYNAMAX, a framework that incorporates early exit mechanisms into Mamba architectures and transforms Mamba into an efficient classifier for EEs on both Mamba and transformer-based models, evaluated using multiple NLP datasets.

**Key Contributions:** Introduction of DYNAMAX framework for early exits in Mamba architectures., Demonstration of Mamba as an efficient EE classifier for different LLMs., Empirical results showcasing the computational savings and performance of Mamba in various NLP tasks.

**Result:** Experiments show that Mamba can efficiently serve as an early exit classifier, achieving a balance between computational cost savings and performance consistency across various NLP tasks.

**Limitations:** 

**Future Work:** Further exploration of Mamba's capabilities in varied computational settings and extending the framework's applicability to additional models and tasks.

**Conclusion:** The adaptation of Mamba for early exits can significantly enhance computational efficiency and dynamic processing in LLMs, suggesting new avenues for research and application, especially in resource-limited environments.

**Abstract:** Early exits (EEs) offer a promising approach to reducing computational costs and latency by dynamically terminating inference once a satisfactory prediction confidence on a data sample is achieved. Although many works integrate EEs into encoder-only Transformers, their application to decoder-only architectures and, more importantly, Mamba models, a novel family of state-space architectures in the LLM realm, remains insufficiently explored. This work introduces DYNAMAX, the first framework to exploit the unique properties of Mamba architectures for early exit mechanisms. We not only integrate EEs into Mamba but also repurpose Mamba as an efficient EE classifier for both Mamba-based and transformer-based LLMs, showcasing its versatility. Our experiments employ the Mistral 7B transformer compared to the Codestral 7B Mamba model, using data sets such as TruthfulQA, CoQA, and TriviaQA to evaluate computational savings, accuracy, and consistency. The results highlight the adaptability of Mamba as a powerful EE classifier and its efficiency in balancing computational cost and performance quality across NLP tasks. By leveraging Mamba's inherent design for dynamic processing, we open pathways for scalable and efficient inference in embedded applications and resource-constrained environments. This study underscores the transformative potential of Mamba in redefining dynamic computing paradigms for LLMs.

</details>


### [55] [Trace-of-Thought: Enhanced Arithmetic Problem Solving via Reasoning Distillation From Large to Small Language Models](https://arxiv.org/abs/2504.20946)

*Tyler McDonald, Ali Emami*

**Main category:** cs.CL

**Keywords:** prompt engineering, Large Language Models, open-source, arithmetic reasoning, computational linguistics

**Relevance Score:** 9

**TL;DR:** A new prompt engineering method, Trace-of-Thought Prompting, is introduced for enhancing arithmetic reasoning in open-source LLMs under 7 billion parameters, showing significant performance gains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and adaptability of prompt engineering in LLMs for specialized tasks like arithmetic reasoning, especially using open-source models.

**Method:** The paper proposes Trace-of-Thought Prompting, a zero-shot prompting technique that encourages LLMs to break down problems into observable subproblems to enhance reasoning.

**Key Contributions:** Introduction of Trace-of-Thought Prompting for LLMs., Demonstration of significant performance gain (up to 125%) in smaller open-source models., Highlighting the advantages of open-source models over proprietary ones in research scalability.

**Result:** The use of Trace-of-Thought Prompting led to performance improvements of up to 125% in language models with 7 billion parameters or fewer, demonstrating the effectiveness of this new approach.

**Limitations:** 

**Future Work:** Exploration of further enhancements in prompt engineering techniques for various computational tasks using open-source models.

**Conclusion:** The approach highlights the utility of open-source LLMs in making AI research more accessible and effective, especially in computational linguistics tasks.

**Abstract:** As Large Language Models (LLMs) continue to be leveraged for daily tasks, prompt engineering remains an active field of contribution within computational linguistics, particularly in domains requiring specialized knowledge such as arithmetic reasoning. While these LLMs are optimized for a variety of tasks, their exhaustive employment may become computationally or financially cumbersome for small teams. Additionally, complete reliance on proprietary, closed-source models often limits customization and adaptability, posing significant challenges in research and application scalability. Instead, by leveraging open-source models at or below 7 billion parameters, we can optimize our resource usage while still observing remarkable gains over standard prompting approaches. To cultivate this notion, we introduce Trace-of-Thought Prompting, a simple, zero-shot prompt engineering method that instructs LLMs to create observable subproblems using critical problem-solving, specifically designed to enhance arithmetic reasoning capabilities. When applied to open-source models in tandem with GPT-4, we observe that Trace-of-Thought not only allows novel insight into the problem-solving process but also introduces performance gains as large as 125% on language models at or below 7 billion parameters. This approach underscores the potential of open-source initiatives in democratizing AI research and improving the accessibility of high-quality computational linguistics applications.

</details>


### [56] [Information Gravity: A Field-Theoretic Model for Token Selection in Large Language Models](https://arxiv.org/abs/2504.20951)

*Maryna Vyshnyvetska*

**Main category:** cs.CL

**Keywords:** information gravity, large language models, text generation, semantic space, hallucinations

**Relevance Score:** 9

**TL;DR:** The paper introduces a theoretical model called 'information gravity' to explain text generation in large language models (LLMs), leveraging concepts from field theory and geometry.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a formal framework that elucidates the interaction between user queries and token generation in LLMs, addressing complex behaviors like hallucinations and variability in output.

**Method:** The model conceptualizes user queries as objects with 'information mass' that influence the semantic space of LLMs, analogous to gravitational forces, thereby shaping the probabilities of token generation.

**Key Contributions:** Introduction of the 'information gravity' model for LLM text generation, Linking the behavior of LLMs to concepts from physics and geometry, Providing explanations for specific LLM phenomena, such as hallucinations and query sensitivity

**Result:** The model successfully explains several LLM behaviors, including the occurrence of hallucinations, sensitivity to how queries are phrased, and how sampling temperature affects the diversity of outputs.

**Limitations:** The theoretical nature of the model requires empirical validation and may not account for all LLM behaviors.

**Future Work:** Further research is needed to empirically test the model and refine its predictions based on real-world usage and performance.

**Conclusion:** The 'information gravity' framework offers valuable insights into the underlying mechanisms of LLMs, potentially guiding future improvements in their design and utility.

**Abstract:** We propose a theoretical model called "information gravity" to describe the text generation process in large language models (LLMs). The model uses physical apparatus from field theory and spacetime geometry to formalize the interaction between user queries and the probability distribution of generated tokens. A query is viewed as an object with "information mass" that curves the semantic space of the model, creating gravitational potential wells that "attract" tokens during generation. This model offers a mechanism to explain several observed phenomena in LLM behavior, including hallucinations (emerging from low-density semantic voids), sensitivity to query formulation (due to semantic field curvature changes), and the influence of sampling temperature on output diversity.

</details>


### [57] [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)

*Maxime Peyrard, Martin Josifoski, Robert West*

**Main category:** cs.CL

**Keywords:** semantic decoding, LLMs, collaboration, semantic tokens, AI systems

**Relevance Score:** 8

**TL;DR:** This paper introduces semantic decoding, a perspective on orchestrating collaborations between LLMs, human input, and tools, framing them as optimization procedures in semantic space.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inherent limitations of LLMs by optimizing collaborative processes involving LLMs, human input, and various tools.

**Method:** Conceptualizes LLMs as semantic processors that exchange semantic tokens to construct high-utility outputs, formalizing the transition from syntactic to semantic tokens.

**Key Contributions:** Introduction of the semantic decoding framework, Formalization of the transition from syntactic to semantic tokens, Identification of new research opportunities in the field of AI optimization.

**Result:** Offers a new framework for AI systems that can manage greater complexity and capabilities by focusing on semantic rather than syntactic decoding.

**Limitations:** 

**Future Work:** Exploration of research questions and opportunities that arise from the semantic decoding perspective.

**Conclusion:** Identifies research opportunities and questions stemming from the semantic decoding perspective, suggesting a functional abstraction for computation based on meaningful concepts.

**Abstract:** Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.

</details>


### [58] [OSVBench: Benchmarking LLMs on Specification Generation Tasks for Operating System Verification](https://arxiv.org/abs/2504.20964)

*Shangyu Li, Juyong Jiang, Tiancheng Zhao, Jiasi Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Operating System Verification, Program Synthesis

**Relevance Score:** 8

**TL;DR:** OSVBench is a new benchmark designed to evaluate Large Language Models in generating specification code for operating system kernel verification tasks, demonstrating their performance limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a standard benchmark for assessing the abilities of LLMs in generating accurate specification code required for verifying operating system kernels.

**Method:** The benchmark reformulates the specification generation problem as a program synthesis task, offering LLMs a programming model along with specific verification assumptions and tasks, based on the Hyperkernel operating system.

**Key Contributions:** Introduction of OSVBench benchmark for LLMs, Evaluation of LLMs on a real-world operating system kernel, Insights into LLM performance limitations in long-context tasks

**Result:** Evaluation of 12 different LLMs showed that they struggled significantly with the specification generation tasks, particularly in handling long-context code generation of 20k-30k tokens.

**Limitations:** Limited performance of current LLMs on specification generation tasks; requires further refinement and testing.

**Future Work:** Investigating ways to improve LLM performance on long-context specification generation tasks and expanding the benchmark.

**Conclusion:** The benchmark reveals important performance disparities among LLMs and provides a useful tool for future research in LLMs' applications in program synthesis.

**Abstract:** We introduce OSVBench, a new benchmark for evaluating Large Language Models (LLMs) in generating complete specification code pertaining to operating system kernel verification tasks. The benchmark first defines the specification generation problem into a program synthesis problem within a confined scope of syntax and semantics by providing LLMs with the programming model. The LLMs are required to understand the provided verification assumption and the potential syntax and semantics space to search for, then generate the complete specification for the potentially buggy operating system code implementation under the guidance of the high-level functional description of the operating system. This benchmark is built upon a real-world operating system kernel, Hyperkernel, and consists of 245 complex specification generation tasks in total, each is a long context task of about 20k-30k tokens. Our comprehensive evaluation of 12 LLMs exhibits the limited performance of the current LLMs on the specification generation tasks for operating system verification. Significant disparities in their performance on the benchmark highlight differences in their ability to handle long-context code generation tasks. The evaluation toolkit and benchmark are available at https://github.com/lishangyu-hkust/OSVBench.

</details>


### [59] [SetKE: Knowledge Editing for Knowledge Elements Overlap](https://arxiv.org/abs/2504.20972)

*Yifan Wei, Xiaoyan Yu, Ran Song, Hao Peng, Angsheng Li*

**Main category:** cs.CL

**Keywords:** Knowledge Editing, Large Language Models, Knowledge Element Overlap, SetKE, EditSet

**Relevance Score:** 8

**TL;DR:** The paper proposes Knowledge Set Editing (KSE) and the SetKE method to address the Knowledge Element Overlap (KEO) phenomenon in Knowledge Editing, demonstrating improved performance with a new dataset, EditSet.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of Knowledge Editing in Large Language Models by addressing the challenges posed by Knowledge Element Overlap, which can lead to performance degradation during knowledge updates.

**Method:** The authors propose a new approach called Knowledge Set Editing (KSE) that allows for the simultaneous editing of sets of triplets to better manage Knowledge Element Overlap (KEO) issues in existing datasets and editing methods.

**Key Contributions:** Introduction of Knowledge Set Editing (KSE) framework, Development of SetKE method for simultaneous triplet editing, Creation of EditSet dataset for benchmarking KEO scenarios

**Result:** Experimental results indicate that SetKE outperforms existing Knowledge Editing methods when dealing with KEO scenarios, showcasing its robustness in maintaining performance during knowledge updates.

**Limitations:** The paper primarily focuses on KEO within knowledge editing and may not address other potential challenges in LLM updates.

**Future Work:** Further exploration of KEO's effects on various knowledge editing techniques and applications in other domains.

**Conclusion:** SetKE represents a significant advancement in Knowledge Editing for Large Language Models, providing a more efficient alternative that mitigates the impact of Knowledge Element Overlap.

**Abstract:** Large Language Models (LLMs) excel in tasks such as retrieval and question answering but require updates to incorporate new knowledge and reduce inaccuracies and hallucinations. Traditional updating methods, like fine-tuning and incremental learning, face challenges such as overfitting and high computational costs. Knowledge Editing (KE) provides a promising alternative but often overlooks the Knowledge Element Overlap (KEO) phenomenon, where multiple triplets share common elements, leading to editing conflicts. We identify the prevalence of KEO in existing KE datasets and show its significant impact on current KE methods, causing performance degradation in handling such triplets. To address this, we propose a new formulation, Knowledge Set Editing (KSE), and introduce SetKE, a method that edits sets of triplets simultaneously. Experimental results demonstrate that SetKE outperforms existing methods in KEO scenarios on mainstream LLMs. Additionally, we introduce EditSet, a dataset containing KEO triplets, providing a comprehensive benchmark.

</details>


### [60] [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)

*Xi Wang, Fanfei Meng, Shiyang Zhang, Lan Li*

**Main category:** cs.CL

**Keywords:** Electronic Dictionaries, Machine Learning, Dictionary Literacy, Language Learning, AI Models

**Relevance Score:** 4

**TL;DR:** This study evaluates the accuracy of the Youdao electronic dictionary used by L2 learners in China, revealing issues in definitions and user consultation habits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigating the reliability of electronic dictionaries which are vital tools for language learners, as their definitions are often taken for granted.

**Method:** A combined approach of experimentation, user surveys, and dictionary critique, focusing on a translation task where participants engaged with Youdao.

**Key Contributions:** Identification of inaccuracies in electronic dictionary definitions, Analysis of user consultation behavior, Recommendations for improving AI-driven dictionary models

**Result:** Incomplete or misleading definitions led to serious misunderstandings among users, alongside problematic consultation behaviors.

**Limitations:** Limited to Youdao as a case study, results may not generalize to other E-dictionaries.

**Future Work:** Further research on a broader range of electronic dictionaries and analyses of user interactions and dictionary literacy efforts.

**Conclusion:** The study indicates a critical need for enhanced dictionary literacy training and improvements in AI models for E-dictionary construction.

**Abstract:** Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.

</details>


### [61] [Semantic Consistency for Assuring Reliability of Large Language Models](https://arxiv.org/abs/2308.09138)

*Harsh Raj, Vipul Gupta, Domenic Rosati, Subhabrata Majumdar*

**Main category:** cs.CL

**Keywords:** Large Language Models, semantic consistency, prompting strategy, natural language generation, evaluation metrics

**Relevance Score:** 9

**TL;DR:** This paper introduces a measure of semantic consistency for LLM outputs and presents a prompting strategy to improve their consistency in open-ended text generation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for reliable output consistency from Large Language Models when prompted with semantically equivalent expressions.

**Method:** Developing a measure of semantic consistency and a novel prompting strategy called Ask-to-Choose (A2C) for evaluating and enhancing LLM performance.

**Key Contributions:** Introduction of a general measure of semantic consistency for LLMs., Development of the Ask-to-Choose (A2C) prompting strategy., A2C's demonstrated effectiveness in improving LLM performance metrics.

**Result:** A2C improves accuracy metrics for LLMs by up to 47% and enhances semantic consistency metrics by up to 7-fold compared to traditional metrics.

**Limitations:** The study focuses on specific metrics and may not encompass all aspects of LLM performance in diverse contexts.

**Future Work:** Exploration of additional prompting strategies and further refinements to the consistency measure in varied language tasks.

**Conclusion:** The proposed methods provide a more robust evaluation of LLM outputs and significant improvements in consistency and accuracy for generative tasks.

**Abstract:** Large Language Models (LLMs) exhibit remarkable fluency and competence across various natural language tasks. However, recent research has highlighted their sensitivity to variations in input prompts. To deploy LLMs in a safe and reliable manner, it is crucial for their outputs to be consistent when prompted with expressions that carry the same meaning or intent. While some existing work has explored how state-of-the-art LLMs address this issue, their evaluations have been confined to assessing lexical equality of single- or multi-word answers, overlooking the consistency of generative text sequences. For a more comprehensive understanding of the consistency of LLMs in open-ended text generation scenarios, we introduce a general measure of semantic consistency, and formulate multiple versions of this metric to evaluate the performance of various LLMs. Our proposal demonstrates significantly higher consistency and stronger correlation with human evaluations of output consistency than traditional metrics based on lexical consistency. Finally, we propose a novel prompting strategy, called Ask-to-Choose (A2C), to enhance semantic consistency. When evaluated for closed-book question answering based on answer variations from the TruthfulQA benchmark, A2C increases accuracy metrics for pretrained and finetuned LLMs by up to 47%, and semantic consistency metrics for instruction-tuned models by up to 7-fold.

</details>


### [62] [LLM-Coordination: Evaluating and Analyzing Multi-agent Coordination Abilities in Large Language Models](https://arxiv.org/abs/2310.03903)

*Saaket Agashe, Yue Fan, Anthony Reyna, Xin Eric Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Theory of Mind, Coordination Games, Agentic Coordination, AI Agents

**Relevance Score:** 8

**TL;DR:** This study presents the LLM-Coordination Benchmark to analyze LLMs in pure coordination settings, focusing on their emergent common-sense reasoning and Theory of Mind capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to evaluate how large language models can function as agents in coordination scenarios, maximizing gains through cooperation.

**Method:** The benchmark includes two tasks: Agentic Coordination, where LLMs engage in pure coordination games, and Coordination Question Answering (CoordQA), which tests their reasoning and planning capabilities through multiple-choice questions.

**Key Contributions:** Introduction of the LLM-Coordination Benchmark, Identification of LLM capabilities in Agentic Coordination tasks, Highlighting gaps in Theory of Mind reasoning and joint planning

**Result:** Results show that while LLMs perform well in environments based mainly on variables, they struggle with reasoning about partners' beliefs and intentions. There is significant room for improvement in their Theory of Mind reasoning and joint planning skills.

**Limitations:** The study primarily focuses on specific coordination tasks, which may limit the generalizability of the findings to broader contexts.

**Future Work:** Future research could explore advanced strategies for improving LLMs' reasoning about partners' beliefs and intentions in coordination tasks.

**Conclusion:** The findings suggest that LLMs have potential as agents in coordination setups; however, they require further development in specific reasoning areas.

**Abstract:** Large Language Models (LLMs) have demonstrated emergent common-sense reasoning and Theory of Mind (ToM) capabilities, making them promising candidates for developing coordination agents. This study introduces the LLM-Coordination Benchmark, a novel benchmark for analyzing LLMs in the context of Pure Coordination Settings, where agents must cooperate to maximize gains. Our benchmark evaluates LLMs through two distinct tasks. The first is Agentic Coordination, where LLMs act as proactive participants in four pure coordination games. The second is Coordination Question Answering (CoordQA), which tests LLMs on 198 multiple-choice questions across these games to evaluate three key abilities: Environment Comprehension, ToM Reasoning, and Joint Planning. Results from Agentic Coordination experiments reveal that LLM-Agents excel in multi-agent coordination settings where decision-making primarily relies on environmental variables but face challenges in scenarios requiring active consideration of partners' beliefs and intentions. The CoordQA experiments further highlight significant room for improvement in LLMs' Theory of Mind reasoning and joint planning capabilities. Zero-Shot Coordination (ZSC) experiments in the Agentic Coordination setting demonstrate that LLM agents, unlike RL methods, exhibit robustness to unseen partners. These findings indicate the potential of LLMs as Agents in pure coordination setups and underscore areas for improvement. Code Available at https://github.com/eric-ai-lab/llm_coordination.

</details>


### [63] [Evaluating the Symbol Binding Ability of Large Language Models for Multiple-Choice Questions in Vietnamese General Education](https://arxiv.org/abs/2310.12059)

*Duc-Vu Nguyen, Quoc-Nam Nguyen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vietnamese, Multiple Choice Question Answering, Dataset Creation, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper evaluates large language models' performance on multiple choice question answering tasks in Vietnamese, introducing a novel dataset for assessing MCSB ability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of LLMs on MCQA tasks in Vietnamese, which has limited datasets compared to English, and to create a high-quality dataset focused on various subjects.

**Method:** The paper evaluates LLMs in zero-shot, one-shot, and few-shot settings using the proposed structured dataset that incorporates LaTeX typing for subjects like mathematics and sciences.

**Key Contributions:** Creation of a novel Vietnamese MCQA dataset using structured LaTeX guidelines, Evaluation of multiple well-known LLMs on Vietnamese benchmarks, Insights into MCSB performance of LLMs for educational subjects.

**Result:** The evaluation of LLMs like BLOOMZ, LLaMA, and GPT models indicates promising MCSB capabilities on Vietnamese benchmarks and the newly proposed dataset.

**Limitations:** The focus is primarily on Vietnamese, limiting the applicability of findings to other languages or cultures; further validation needed across diverse topics.

**Future Work:** Further research could explore larger datasets and evaluate LLMs in more diverse contexts or with more complex question structures.

**Conclusion:** The study provides insights into the performance of LLMs in Vietnamese MCQA tasks and introduces a dataset that can further aid research in this area.

**Abstract:** In this paper, we evaluate the ability of large language models (LLMs) to perform multiple choice symbol binding (MCSB) for multiple choice question answering (MCQA) tasks in zero-shot, one-shot, and few-shot settings. We focus on Vietnamese, with fewer challenging MCQA datasets than in English. The two existing datasets, ViMMRC 1.0 and ViMMRC 2.0, focus on literature. Recent research in Vietnamese natural language processing (NLP) has focused on the Vietnamese National High School Graduation Examination (VNHSGE) from 2019 to 2023 to evaluate ChatGPT. However, these studies have mainly focused on how ChatGPT solves the VNHSGE step by step. We aim to create a novel and high-quality dataset by providing structured guidelines for typing LaTeX formulas for mathematics, physics, chemistry, and biology. This dataset can be used to evaluate the MCSB ability of LLMs and smaller language models (LMs) because it is typed in a strict LaTeX style. We focus on predicting the character (A, B, C, or D) that is the most likely answer to a question, given the context of the question. Our evaluation of six well-known LLMs, namely BLOOMZ-7.1B-MT, LLaMA-2-7B, LLaMA-2-70B, GPT-3, GPT-3.5, and GPT-4.0, on the ViMMRC 1.0 and ViMMRC 2.0 benchmarks and our proposed dataset shows promising results on the MCSB ability of LLMs for Vietnamese. The dataset is available for research purposes only.

</details>


### [64] [Agentic AI: The Era of Semantic Decoding](https://arxiv.org/abs/2403.14562)

*Maxime Peyrard, Martin Josifoski, Robert West*

**Main category:** cs.CL

**Keywords:** semantic decoding, LLMs, human-computer interaction, semantic tokens, AI optimization

**Relevance Score:** 8

**TL;DR:** This paper introduces semantic decoding, a novel approach that redefines interactions among LLMs, humans, and tools as optimization processes in semantic space, proposing new algorithms for optimizing meaningful outputs.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inherent limitations of LLMs and enhance collaborative interactions between LLMs, human input, and tools by focusing on semantic processes.

**Method:** The paper conceptualizes LLMs and other semantic processors as entities that exchange semantic tokens and formulates the orchestration of their interactions as semantic decoding algorithms.

**Key Contributions:** Introduction of the semantic decoding framework for AI systems, Formalization of the transition from syntactic to semantic tokens, Exploration of research questions related to semantic optimization

**Result:** The framework emphasizes optimizing within semantic space, offering a new perspective on AI system design that transcends traditional syntactic approaches.

**Limitations:** 

**Future Work:** Further exploration of semantic decoding algorithms and their applications in complex AI systems.

**Conclusion:** The authors provide insights into future research directions, proposing that semantic decoding can lead to more advanced AI capabilities by focusing on meaningful concepts.

**Abstract:** Recent work demonstrated great promise in the idea of orchestrating collaborations between LLMs, human input, and various tools to address the inherent limitations of LLMs. We propose a novel perspective called semantic decoding, which frames these collaborative processes as optimization procedures in semantic space. Specifically, we conceptualize LLMs as semantic processors that manipulate meaningful pieces of information that we call semantic tokens (known thoughts). LLMs are among a large pool of other semantic processors, including humans and tools, such as search engines or code executors. Collectively, semantic processors engage in dynamic exchanges of semantic tokens to progressively construct high-utility outputs. We refer to these orchestrated interactions among semantic processors, optimizing and searching in semantic space, as semantic decoding algorithms. This concept draws a direct parallel to the well-studied problem of syntactic decoding, which involves crafting algorithms to best exploit auto-regressive language models for extracting high-utility sequences of syntactic tokens. By focusing on the semantic level and disregarding syntactic details, we gain a fresh perspective on the engineering of AI systems, enabling us to imagine systems with much greater complexity and capabilities. In this position paper, we formalize the transition from syntactic to semantic tokens as well as the analogy between syntactic and semantic decoding. Subsequently, we explore the possibilities of optimizing within the space of semantic tokens via semantic decoding algorithms. We conclude with a list of research opportunities and questions arising from this fresh perspective. The semantic decoding perspective offers a powerful abstraction for search and optimization directly in the space of meaningful concepts, with semantic tokens as the fundamental units of a new type of computation.

</details>


### [65] [A Practical Analysis of Human Alignment with *PO](https://arxiv.org/abs/2407.15229)

*Kian Ahrabian, Xihui Lin, Barun Patra, Vishrav Chaudhary, Alon Benhaim, Jay Pujara, Xia Song*

**Main category:** cs.CL

**Keywords:** Human Alignment, Preference Optimization, Length-normalized DPO, Out-of-distribution, Performance Robustness

**Relevance Score:** 8

**TL;DR:** This paper examines the robustness of human alignment methods under varying hyperparameters in out-of-distribution scenarios, introducing LN-DPO to enhance performance and stability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To empirically investigate the robustness of state-of-the-art human alignment methods in real-world applications and identify strategies to improve their performance across varying hyperparameters.

**Method:** The authors analyze existing preference optimization methods, particularly in OOD settings, using metrics like KL divergence and response length, and introduce LN-DPO for better stability and performance.

**Key Contributions:** Identification of robustness issues in existing human alignment methods under varying hyperparameters., Introduction of LN-DPO, a length-normalized method that improves stability and response performance., Analysis of performance patterns in OOD scenarios for reference-free and reference-dependent methods.

**Result:** The introduction of LN-DPO leads to more stable performance across hyperparameters, reduces average response length, and maintains competitive results with other state-of-the-art methods.

**Limitations:** The focus on certain metrics may not encompass all aspects of human alignment efficacy in practice.

**Future Work:** Further exploration of other metrics and real-world applications to enhance the robustness and applicability of these methods.

**Conclusion:** The study reveals that while top methods perform similarly at their best, their performance can vary significantly as conditions change, underlining the importance of robustness in human alignment methods.

**Abstract:** At the forefront of state-of-the-art human alignment methods are preference optimization methods (*PO). Prior research has often concentrated on identifying the best-performing method, typically involving a grid search over hyperparameters, which can be impractical for general practitioners. In this paper, we examine the robustness of existing state-of-the-art methods to varying hyperparameters in a realistic out-of-distribution (OOD) scenario that mirrors real-world applications of human alignment. Our goal is to empirically find the method that increases the likelihood of achieving better results through the lens of various metrics, such as KL divergence and response length. We also introduce LN-DPO, a simple length-normalized version of DPO that is more stable across hyperparameters, effectively reduces the average response length, and improves performance. Our analysis of state-of-the-art reference-free (i.e., SimPO) and reference-dependent (i.e., DPO and LN-DPO) methods reveals that they perform similarly at their peak (i.e., best possible scenario). However, we uncover that the pattern of change in performance greatly varies as we move away from the best possible scenario.

</details>


### [66] [Correcting Negative Bias in Large Language Models through Negative Attention Score Alignment](https://arxiv.org/abs/2408.00137)

*Sangwon Yu, Jongyoon Song, Bongkyu Hwang, Hoyoung Kang, Sooah Cho, Junhwa Choi, Seongho Joe, Taehee Lee, Youngjune L. Gwon, Sungroh Yoon*

**Main category:** cs.CL

**Keywords:** negative bias, language models, attention heads, fine-tuning, reasoning tasks

**Relevance Score:** 7

**TL;DR:** The paper investigates the negative bias in language models' binary decision making and proposes a method to address this issue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Binary decision tasks are common in user interactions, and understanding negative biases in language models can improve their performance in real-world applications.

**Method:** The authors propose a negative attention score (NAS) to quantify negative bias and use it to identify attention heads that focus on negative tokens. They also introduce the negative attention score alignment (NASA) method for fine-tuning these heads.

**Key Contributions:** Introduction of the negative attention score (NAS) for quantifying negative bias in language models., Development of the NASA fine-tuning method to mitigate negative bias in attention heads., Empirical demonstration of NASA's effectiveness in improving reasoning task performance.

**Result:** Experimental validation shows that NASA significantly improves precision and recall in reasoning tasks without sacrificing generalization capabilities.

**Limitations:** 

**Future Work:** Further exploration of negative bias across different model architectures and extending the application of NAS and NASA to other types of tasks.

**Conclusion:** Addressing negative bias in language models is critical for enhancing their decision-making performance, and NAS and NASA provide effective solutions to this problem.

**Abstract:** A binary decision task, like yes-no questions or answer verification, reflects a significant real-world scenario such as where users look for confirmation about the correctness of their decisions on specific issues. In this work, we observe that language models exhibit a negative bias in the binary decisions of complex reasoning tasks. Based on our observations and the rationale about attention-based model dynamics, we propose a negative attention score (NAS) to systematically and quantitatively formulate negative bias. Based on NAS, we identify attention heads that attend to negative tokens provided in the instructions as answer candidate of binary decisions, regardless of the question in the prompt, and validate their association with the negative bias. Additionally, we propose the negative attention score alignment (NASA) method, which is a parameter-efficient fine-tuning technique to address the extracted negatively biased attention heads. Experimental results from various domains of reasoning tasks and large model search space demonstrate that NASA significantly reduces the gap between precision and recall caused by negative bias while preserving their generalization abilities.

</details>


### [67] [AdaCAD: Adaptively Decoding to Balance Conflicts between Contextual and Parametric Knowledge](https://arxiv.org/abs/2409.07394)

*Han Wang, Archiki Prasad, Elias Stengel-Eskin, Mohit Bansal*

**Main category:** cs.CL

**Keywords:** Knowledge conflict, AdaCAD, LLM performance, Dynamic adjustment, Contrastive methods

**Relevance Score:** 9

**TL;DR:** The paper introduces AdaCAD, a fine-grained instance-level approach that improves LLM performance in the presence of knowledge conflict by dynamically adjusting weights based on the degree of conflict detected.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing contrastive methods that fail to account for varying degrees of knowledge conflict in LLM outputs.

**Method:** AdaCAD uses Jensen-Shannon divergence to measure the conflict between the contextual and parametric knowledge distributions, adjusting outputs accordingly.

**Key Contributions:** Introduction of AdaCAD for dynamic conflict adjustment in LLMs., Demonstrated substantial performance improvements on QA and summarization tasks., Mitigates performance issues in absence of knowledge conflict.

**Result:** AdaCAD achieved an average QA accuracy gain of 14.21% over static contrastive baselines and improved summarization factuality by 6.19 in AlignScore, outperforming previous methods on diverse datasets.

**Limitations:** Performance may vary across different types of tasks and LLMs, necessitating further evaluation in broader contexts.

**Future Work:** Exploring adaptations of AdaCAD for more LLM architectures and varied NLP tasks.

**Conclusion:** The approach effectively mitigates performance drops in scenarios without conflict, making it well-suited for real-world applications with mixed conflict levels in LLM outputs.

**Abstract:** Knowledge conflict arises from discrepancies between information in the context of a large language model (LLM) and the knowledge stored in its parameters. This can hurt performance when using standard decoding techniques, which tend to ignore the context. Existing test-time contrastive methods seek to address this by comparing the LLM's output distribution with and without the context and adjust the model according to the contrast between them. However, we find that these methods frequently misjudge the degree of conflict and struggle to handle instances that vary in their amount of conflict, with static methods over-adjusting when conflict is absent. We propose a fine-grained, instance-level approach called AdaCAD, which dynamically infers the weight of adjustment based on the degree of conflict, as measured by the Jensen-Shannon divergence between distributions representing contextual and parametric knowledge. Across four LLMs, six question-answering (QA) and three summarization datasets, we demonstrate that ADACAD consistently outperforms other decoding baselines with average QA accuracy gains of 14.21% (absolute) over a static contrastive baseline, and improves the factuality of summaries by 6.19 (AlignScore). Lastly, we show that while contrastive baselines hurt performance when conflict is absent, ADACAD mitigates these losses, making it more applicable to real-world datasets in which some examples have conflict and others do not.

</details>


### [68] [Racing Thoughts: Explaining Contextualization Errors in Large Language Models](https://arxiv.org/abs/2410.02102)

*Michael A. Lepori, Michael C. Mozer, Asma Ghandeharioun*

**Main category:** cs.CL

**Keywords:** transformer-based models, contextualization, token dependencies, mechanistic interpretability, inference-time interventions

**Relevance Score:** 9

**TL;DR:** The paper proposes the LLM Race Conditions Hypothesis to explain errors in contextualization within transformer-based language models, suggesting that these errors stem from violating token dependencies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how transformer-based models manage contextual information is crucial for improving their performance and reliability.

**Method:** The authors use mechanistic interpretability techniques to analyze token dependencies and provide both correlational and causal evidence for their hypothesis.

**Key Contributions:** Introduction of the LLM Race Conditions Hypothesis for contextualization errors., Evidence supporting the role of token dependencies in model performance., Proposed interventions to mitigate errors during inference.

**Result:** The study shows that contextualization errors arise when dependencies between tokens are not properly managed, leading to incorrect model responses.

**Limitations:** The hypothesis needs further empirical validation across various model architectures and tasks.

**Future Work:** Further studies to explore the applicability of the hypothesis to different language models and contexts, as well as the development of more effective interventions.

**Conclusion:** Inference-time interventions can be used to address the violations of token dependencies and enhance model accuracy.

**Abstract:** The profound success of transformer-based language models can largely be attributed to their ability to integrate relevant contextual information from an input sequence in order to generate a response or complete a task. However, we know very little about the algorithms that a model employs to implement this capability, nor do we understand their failure modes. For example, given the prompt "John is going fishing, so he walks over to the bank. Can he make an ATM transaction?", a model may incorrectly respond "Yes" if it has not properly contextualized "bank" as a geographical feature, rather than a financial institution. We propose the LLM Race Conditions Hypothesis as an explanation of contextualization errors of this form. This hypothesis identifies dependencies between tokens (e.g., "bank" must be properly contextualized before the final token, "?", integrates information from "bank"), and claims that contextualization errors are a result of violating these dependencies. Using a variety of techniques from mechanistic intepretability, we provide correlational and causal evidence in support of the hypothesis, and suggest inference-time interventions to address it.

</details>


### [69] [Unleashing Multi-Hop Reasoning Potential in Large Language Models through Repetition of Misordered Context](https://arxiv.org/abs/2410.07103)

*Sangwon Yu, Ik-hwan Kim, Jongyoon Song, Saehyung Lee, Junsung Park, Sungroh Yoon*

**Main category:** cs.CL

**Keywords:** multi-hop reasoning, large language models, context repetition

**Relevance Score:** 8

**TL;DR:** This paper addresses the misordered context problem in multi-hop reasoning for LLMs by proposing a method called context repetition (CoRe), improving performance on QA tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Multi-hop reasoning is challenging for LLMs, especially regarding the sensitivity to the order and position of supporting documents.

**Method:** The authors propose CoRe, a method that involves repeatedly presenting context to ensure reasoning segments are optimally ordered.

**Key Contributions:** Introduction of the misordered context problem in LLMs, Development of the context repetition (CoRe) method, Demonstrated significant performance improvements on multi-hop QA tasks

**Result:** CoRe improves the F1 score by up to 30% on multi-hop QA tasks and increases accuracy by up to 70% on a synthetic task.

**Limitations:** The study may not address all scenarios in multi-hop reasoning or generalize across different domains.

**Future Work:** Future research could explore the long-term impacts of CoRe across various tasks and deeper integration with retrieval approaches.

**Conclusion:** The CoRe method effectively guides a model's reasoning and mitigates the lost-in-the-middle problem while being compatible with retrieval-based Chain-of-Thought reasoning.

**Abstract:** Multi-hop reasoning, which requires multi-step reasoning based on the supporting documents within a given context, remains challenging for large language models (LLMs). LLMs often struggle to filter out irrelevant documents within the context, and their performance is sensitive to the absolute position of supporting documents within that context. In this paper, we identify an additional challenge: LLMs' performance is also sensitive to the order, relative position, in which the supporting documents are presented. We refer to this as the misordered context problem. To address this issue, based on the theoretical approach, we propose a simple yet effective method called context repetition (CoRe), which involves prompting the model by repeatedly presenting the context. This ensures that certain contiguous reasoning segments within supporting documents are presented in the optimal order, effectively guiding the model's reasoning in the appropriate direction. Applying CoRe, we improve the F1 score by up to 30%p on multi-hop QA tasks and increase accuracy by up to 70%p on a synthetic task. Additionally, CoRe helps mitigate the well-known "lost-in-the-middle" problem in LLMs and can be effectively combined with retrieval-based approaches utilizing Chain-of-Thought (CoT) reasoning.

</details>


### [70] [MDCure: A Scalable Pipeline for Multi-Document Instruction-Following](https://arxiv.org/abs/2410.23463)

*Gabrielle Kaili-May Liu, Bowen Shi, Avi Caciularu, Idan Szpektor, Arman Cohan*

**Main category:** cs.CL

**Keywords:** multi-document processing, instruction generation, reward model

**Relevance Score:** 9

**TL;DR:** MDCure is an instruction data generation framework aimed at enhancing multi-document (MD) processing capabilities of LLMs through synthetic data generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ability of LLMs to handle multi-document contexts, addressing unique challenges like inter-document dependencies and redundancy.

**Method:** MDCure generates high-quality synthetic MD instruction data using targeted prompts and incorporates a reward model called MDCureRM to score and filter the data based on its utility.

**Key Contributions:** Introduction of MDCure framework for synthetic MD instruction data generation., Development of MDCureRM, a cost-effective MD-specific reward model., Demonstrated capability to improve LLM performance on various MD tasks by significant margins.

**Result:** MDCure enables fine-tuning of various LLMs, with up to 75.1% improvement in performance on multi-document and long-context benchmarks compared to pre-trained baselines.

**Limitations:** 

**Future Work:** 

**Conclusion:** MDCure significantly enhances the MD instruction data generation without the need for extensive pre-training or human annotations, making it accessible for both closed and open-source models.

**Abstract:** Multi-document (MD) processing is crucial for LLMs to handle real-world tasks such as summarization and question-answering across large sets of documents. While LLMs have improved at processing long inputs, MD contexts still present unique difficulties, including management of inter-document dependencies, redundancy, and incoherent structures. To address this challenge, we introduce MDCure, a scalable and effective instruction data generation framework to enhance the MD capabilities of LLMs without the computational cost of pre-training or reliance on human-annotated data. MDCure generates high-quality synthetic MD instruction data over sets of articles via targeted prompts. We also introduce MDCureRM, a cost-effective, MD-specific reward model to score and filter generated data based on their training utility for MD settings. MDCure is compatible with open- and closed-source models in addition to policy optimization methods such as PPO, enabling even small open-source models to surpass proprietary LLMs as strong generators of high-quality MD instruction data without further data filtering. With MDCure, we fine-tune a wide variety of LLMs up to 70B parameters in size from the FlanT5, Qwen2, and LLAMA3.1 model families. Extensive evaluations on a wide range of MD and long-context benchmarks spanning various tasks and domains show MDCure consistently improves performance over pre-trained baselines and base models by up to 75.1%. Our code, datasets, and models are available at https://github.com/yale-nlp/MDCure.

</details>


### [71] [Constraint Back-translation Improves Complex Instruction Following of Large Language Models](https://arxiv.org/abs/2410.24175)

*Yunjia Qi, Hao Peng, Xiaozhi Wang, Bin Xu, Lei Hou, Juanzi Li*

**Main category:** cs.CL

**Keywords:** large language models, instruction-following, constraint back-translation, dataset generation, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper introduces a new method for generating high-quality instruction-response pairs for large language models (LLMs) using a technique called constraint back-translation, improving their ability to follow complex instructions.

**Read time:** 14 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with complex instruction-following due to limitations in existing instruction-tuning methods and data generation techniques.

**Method:** The authors propose constraint back-translation, where high-quality instruction-response pairs are modified by advanced LLMs to include complex constraints present in responses, thereby enhancing the dataset quality.

**Key Contributions:** Introduction of the constraint back-translation method for dataset generation, Creation of the CRAB dataset that improves complex instruction-following, Demonstration of the effectiveness of CRAB on various LLMs in extensive benchmarks

**Result:** The resulting dataset, CRAB, was shown to improve the complex instruction-following abilities of multiple backbone LLMs when used for post-training, validated through extensive benchmarks.

**Limitations:** The method assumes that existing datasets adequately represent complex constraints and may not account for all possible instruction complexities.

**Future Work:** Future research could explore broader applications of constraint back-translation in other domains or investigate further improvements to dataset quality.

**Conclusion:** The study demonstrates that utilizing implicitly complex constraints in existing datasets can significantly enhance the training process for LLMs, suggesting a new avenue for research in dataset generation.

**Abstract:** Large language models (LLMs) struggle to follow instructions with complex constraints in format, length, etc. Following the conventional instruction-tuning practice, previous works conduct post-training on complex instruction-response pairs generated by feeding complex instructions to advanced LLMs. However, even advanced LLMs cannot follow complex instructions well, thus limiting the quality of generated data. In this work, we find that existing datasets inherently contain implicit complex constraints and propose a novel data generation technique, constraint back-translation. Specifically, we take the high-quality instruction-response pairs in existing datasets and only adopt advanced LLMs to add complex constraints already met by the responses to the instructions, which naturally reduces costs and data noise. In the experiments, we adopt Llama3-70B-Instruct to back-translate constraints and create a high-quality complex instruction-response dataset, named CRAB. We present that post-training on CRAB improves multiple backbone LLMs' complex instruction-following ability, evaluated on extensive instruction-following benchmarks. We further find that constraint back-translation also serves as a useful auxiliary training objective in post-training. Our code, data, and models will be released to facilitate future research.

</details>


### [72] [Benchmarking LLMs' Judgments with No Gold Standard](https://arxiv.org/abs/2411.07127)

*Shengwei Xu, Yuxuan Lu, Grant Schoenebeck, Yuqing Kong*

**Main category:** cs.CL

**Keywords:** Generative Estimator for Mutual Information, Large Language Models, peer review, evaluation metric, GRE-bench

**Relevance Score:** 9

**TL;DR:** The GEM metric evaluates language generation by LLMs without gold standard references, particularly in subjective tasks like academic peer review, demonstrating robust correlations with human scores and introducing GRE-bench for assessing LLM peer review quality.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To create a reliable metric for evaluating language generation by LLMs that can operate in subjective scenarios lacking clear gold standards.

**Method:** GEM estimates mutual information between candidate and reference responses using a generative model, while GRE-bench evaluates LLMs based on high-quality peer review generation capabilities utilizing a dataset of continuously updated open-access research papers.

**Key Contributions:** Introduction of GEM as a new evaluation metric for LLMs., Development of GRE-bench for assessing LLMs in peer review generation., Demonstration of robustness against manipulation tactics in evaluating LLM performance.

**Result:** GEM shows competitive correlations with human evaluation scores and outperforms existing metrics. GRE-bench demonstrates effective assessment results of various LLMs on their peer review capabilities.

**Limitations:** 

**Future Work:** Further exploration of the applications of GEM in diverse generative tasks and enhancement of GRE-bench as more data becomes available.

**Conclusion:** GEM and GRE-bench provide robust tools for evaluating LLM performance in generation tasks without relying on traditional gold standards.

**Abstract:** We introduce the GEM (Generative Estimator for Mutual Information), an evaluation metric for assessing language generation by Large Language Models (LLMs), particularly in generating informative judgments, without the need for a gold standard reference. GEM broadens the scenarios where we can benchmark LLM generation performance-from traditional ones, like machine translation and summarization, where gold standard references are readily available, to subjective tasks without clear gold standards, such as academic peer review.   GEM uses a generative model to estimate mutual information between candidate and reference responses, without requiring the reference to be a gold standard. In experiments on a human-annotated dataset, GEM demonstrates competitive correlations with human scores compared to the state-of-the-art GPT-4o Examiner, and outperforms all other baselines. Additionally, GEM is more robust against strategic manipulations, such as rephrasing or elongation, which can artificially inflate scores under a GPT-4o Examiner.   We also present GRE-bench (Generating Review Evaluation Benchmark) which evaluates LLMs based on how well they can generate high-quality peer reviews for academic research papers. Because GRE-bench is based upon GEM, it inherits its robustness properties. Additionally, GRE-bench circumvents data contamination problems (or data leakage) by using the continuous influx of new open-access research papers and peer reviews each year. We show GRE-bench results of various popular LLMs on their peer review capabilities using the ICLR2023 dataset.

</details>


### [73] [Beyond the Safety Bundle: Auditing the Helpful and Harmless Dataset](https://arxiv.org/abs/2411.08243)

*Khaoula Chehbouni, Jonathan Colaço Carr, Yash More, Jackie CK Cheung, Golnoosh Farnadi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human Feedback, Safety Mitigation, Data Quality, Demographic Disparities

**Relevance Score:** 9

**TL;DR:** This study audits the Helpful and Harmless (HH) dataset used in learning from human feedback (LHF) for large language models, revealing quality issues and potential safety disparities across demographic groups.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the quality and effectiveness of human feedback in steering large language models towards safety.

**Method:** Conducting manual and automated evaluations of the HH dataset, running experiments to assess its impact on model safety, and analyzing influential research citing this dataset.

**Key Contributions:** Audit of the HH dataset's content and its safety implications, Experiments demonstrating the dataset's effect on model behavior, Analysis of influential papers related to the dataset

**Result:** Identified conceptualization failures and quality concerns in the HH dataset, which can exacerbate safety issues and lead to uneven model behaviors among different demographic groups.

**Limitations:** Focus is on a specific dataset and its immediate implications, may not cover other datasets or broader contexts.

**Future Work:** Encourages the development of more nuanced approaches to LHF and safety in LLMs.

**Conclusion:** The findings emphasize the need for more refined and context-aware methods for ensuring safety in large language models.

**Abstract:** In an effort to mitigate the harms of large language models (LLMs), learning from human feedback (LHF) has been used to steer LLMs towards outputs that are intended to be both less harmful and more helpful. Despite the widespread adoption of LHF in practice, the quality of this feedback and its effectiveness as a safety mitigation technique remain unclear. This study addresses these issues by auditing the widely-used Helpful and Harmless (HH) dataset by Anthropic. Our work includes: (1) a thorough investigation of the dataset's content through both manual and automated evaluation; (2) experiments demonstrating the dataset's impact on models' safety; and (3) an analysis of the 100 most influential papers citing this dataset. Through our audit, we showcase how conceptualization failures and quality issues identified in the HH dataset can create additional harms by leading to disparate safety behaviors across demographic groups. Our findings highlight the need for more nuanced, context-sensitive approaches to safety mitigation in LLMs.

</details>


### [74] [A Bayesian Optimization Approach to Machine Translation Reranking](https://arxiv.org/abs/2411.09694)

*Julius Cheng, Maike Züfle, Vilém Zouhar, Andreas Vlachos*

**Main category:** cs.CL

**Keywords:** machine translation, Bayesian optimization, reranking, scoring model, multi-fidelity

**Relevance Score:** 6

**TL;DR:** This paper presents a Bayesian optimization approach for reranking candidates in machine translation, significantly reducing the number of scoring evaluations needed while maintaining output quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the growing computational cost associated with reranking in machine translation systems due to the increasing size of translation scoring models.

**Method:** The authors frame reranking as a BayesOpt problem, emphasizing strategic candidate selection based on a trade-off between exploration and exploitation to achieve high-quality translations with fewer scoring evaluations.

**Key Contributions:** Introduction of a Bayesian optimization framework for reranking in machine translation., Demonstration of significant reduction in required scoring evaluations while maintaining translation quality., Implementation of a multi-fidelity approach to optimize scoring efficiency.

**Result:** The proposed method attains the same CometKiwi score using only 70 scoring evaluations, in contrast to 180 evaluations required by the baseline system.

**Limitations:** 

**Future Work:** Further exploration of optimizing candidate selection strategies and enhancing the fidelity of proxy scoring models.

**Conclusion:** The use of a multi-fidelity BayesOpt setting allows for initial scoring with a less expensive, noisier proxy model, enhancing the cost-performance aspect when utilizing smaller, well-trained proxy scorers.

**Abstract:** Reranking a list of candidates from a machine translation system with an external scoring model and returning the highest-scoring candidate remains a simple and effective method for improving the overall output quality. Translation scoring models continue to grow in size, with the best models being comparable to generation models. Thus, reranking can add substantial computational cost to the translation pipeline. In this work, we pose reranking as a Bayesian optimization (BayesOpt) problem. By strategically selecting candidates to score based on a balance of exploration and exploitation, we show that it is possible to find top-scoring candidates when scoring only a fraction of the candidate list. For instance, our method achieves the same CometKiwi score using only 70 scoring evaluations compared a baseline system using 180. We present a multi-fidelity setting for BayesOpt, where the candidates are first scored with a cheaper but noisier proxy scoring model, which further improves the cost-performance tradeoff when using smaller but well-trained distilled proxy scorers.

</details>


### [75] [Massive Values in Self-Attention Modules are the Key to Contextual Knowledge Understanding](https://arxiv.org/abs/2502.01563)

*Mingyu Jin, Kai Mei, Wujiang Xu, Mingjie Sun, Ruixiang Tang, Mengnan Du, Zirui Liu, Yongfeng Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Mechanisms, Contextual Knowledge, Rotary Positional Encoding, Quantization Strategies

**Relevance Score:** 8

**TL;DR:** This paper investigates the role of concentrated massive values in attention queries and keys of large language models and their impact on contextual knowledge understanding.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how specific regions of attention in transformer-based large language models contribute to contextual knowledge interpretation.

**Method:** The authors conducted extensive experiments comparing the behavior of attention queries, keys, and values in various modern LLMs, as well as analyzing the effects of quantization strategies.

**Key Contributions:** Identification of critical concentrated massive values in attention mechanisms., Insights into the influence of Rotary Positional Encoding on value concentration., Demonstration of the performance impact of quantization neglecting these values.

**Result:** The research shows that concentrated massive values are crucial for interpreting contextual knowledge, and their neglect leads to significant performance drops in understanding tasks.

**Limitations:** The study primarily focuses on attention mechanisms and may not cover other factors affecting LLM performance.

**Future Work:** Further exploration into different encoding strategies and their influence on LLM performance.

**Conclusion:** The study highlights that concentrated massive values, particularly influenced by Rotary Positional Encoding (RoPE), play a vital role in the functioning of attention mechanisms in LLMs and offers insights for future model optimizations.

**Abstract:** Large language models (LLMs) have achieved remarkable success in contextual knowledge understanding. In this paper, we show that these concentrated massive values consistently emerge in specific regions of attention queries (Q) and keys (K) while not having such patterns in values (V) in various modern transformer-based LLMs (Q, K, and V mean the representations output by the query, key, and value layers respectively). Through extensive experiments, we further demonstrate that these massive values play a critical role in interpreting contextual knowledge (knowledge obtained from the current context window) rather than in retrieving parametric knowledge stored within the model's parameters. Our further investigation of quantization strategies reveals that ignoring these massive values leads to a pronounced drop in performance on tasks requiring rich contextual understanding, aligning with our analysis. Finally, we trace the emergence of concentrated massive values and find that such concentration is caused by Rotary Positional Encoding (RoPE), which has appeared since the first layers. These findings shed new light on how Q and K operate in LLMs and offer practical insights for model design and optimization. The Code is Available at https://github.com/MingyuJ666/Rope_with_LLM.

</details>


### [76] [An LLM-Powered Agent for Physiological Data Analysis: A Case Study on PPG-based Heart Rate Estimation](https://arxiv.org/abs/2502.12836)

*Mohammad Feli, Iman Azimi, Pasi Liljeberg, Amir M. Rahmani*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Physiological Time-Series, Heart Rate Estimation, Open Source

**Relevance Score:** 10

**TL;DR:** This paper presents an LLM-powered agent for analyzing physiological time-series data to improve healthcare insights, particularly in heart rate estimation and decision-making support.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to enhance the integration of LLMs with analytical tools for better interpretation of physiological time-series data in healthcare contexts.

**Method:** Developed an agent using OpenCHA and GPT-3.5-turbo, evaluating its effectiveness through a case study for heart rate estimation from PPG signals.

**Key Contributions:** Development of an LLM-powered agent tailored for physiological time-series analysis., Successful case study implementation demonstrating superior performance in heart rate estimation., Open-source availability of the agent to foster further research and application.

**Result:** The agent outperformed models based on GPT-4o-mini and GPT-4o, boasting lower error rates and increased reliability in heart rate estimations.

**Limitations:** The study is focused on heart rate estimation, and further validation is needed across diverse physiological metrics and conditions.

**Future Work:** Exploration of additional physiological measurements and further refinements of the LLM agent's analytical capabilities.

**Conclusion:** The LLM-powered agent shows significant promise in integrating advanced language models with classical data analysis tools for improved healthcare applications.

**Abstract:** Large language models (LLMs) are revolutionizing healthcare by improving diagnosis, patient care, and decision support through interactive communication. More recently, they have been applied to analyzing physiological time-series like wearable data for health insight extraction. Existing methods embed raw numerical sequences directly into prompts, which exceeds token limits and increases computational costs. Additionally, some studies integrated features extracted from time-series in textual prompts or applied multimodal approaches. However, these methods often produce generic and unreliable outputs due to LLMs' limited analytical rigor and inefficiency in interpreting continuous waveforms. In this paper, we develop an LLM-powered agent for physiological time-series analysis aimed to bridge the gap in integrating LLMs with well-established analytical tools. Built on the OpenCHA, an open-source LLM-powered framework, our agent powered by OpenAI's GPT-3.5-turbo model features an orchestrator that integrates user interaction, data sources, and analytical tools to generate accurate health insights. To evaluate its effectiveness, we implement a case study on heart rate (HR) estimation from Photoplethysmogram (PPG) signals using a dataset of PPG and Electrocardiogram (ECG) recordings in a remote health monitoring study. The agent's performance is benchmarked against OpenAI GPT-4o-mini and GPT-4o, with ECG serving as the gold standard for HR estimation. Results demonstrate that our agent significantly outperforms benchmark models by achieving lower error rates and more reliable HR estimations. The agent implementation is publicly available on GitHub.

</details>


### [77] [MEMERAG: A Multilingual End-to-End Meta-Evaluation Benchmark for Retrieval Augmented Generation](https://arxiv.org/abs/2502.17163)

*María Andrea Cruz Blandón, Jayasimha Talur, Bruno Charron, Dong Liu, Saab Mansour, Marcello Federico*

**Main category:** cs.CL

**Keywords:** retrieval augmented generation, multilingual evaluation, large language models, automatic evaluators, human judgement

**Relevance Score:** 9

**TL;DR:** Development of a multilingual meta-evaluation benchmark for retrieval augmented generation (RAG) systems addressing cultural nuances in user experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automatic evaluation of RAG systems by integrating native-language perspectives that reflect cultural nuances overlooked by existing benchmarks.

**Method:** Creation of the Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG) utilizing native-language questions and responses generated by diverse LLMs, assessed by expert annotators.

**Key Contributions:** Introduction of the MEMERAG benchmark for multilingual evaluation of RAG systems, High-quality annotation process achieving strong inter-annotator agreement, Application of the dataset for benchmarking LLMs as evaluators of RAG outputs

**Result:** High inter-annotator agreement was achieved in the annotation process. The dataset enables reliable performance analysis of LLMs across languages and effective benchmarking of multilingual automatic evaluators.

**Limitations:** The study may be limited by the scope of languages included and the generalizability of findings to all RAG systems.

**Future Work:** Future research may explore expanding the benchmark to additional languages and further refining the evaluation criteria.

**Conclusion:** The MEMERAG benchmark demonstrates potential to enhance the evaluation of RAG systems, showcasing the impact of advanced prompting techniques on performance.

**Abstract:** Automatic evaluation of retrieval augmented generation (RAG) systems relies on fine-grained dimensions like faithfulness and relevance, as judged by expert human annotators. Meta-evaluation benchmarks support the development of automatic evaluators that correlate well with human judgement. However, existing benchmarks predominantly focus on English or use translated data, which fails to capture cultural nuances. A native approach provides a better representation of the end user experience.   In this work, we develop a Multilingual End-to-end Meta-Evaluation RAG benchmark (MEMERAG). Our benchmark builds on the popular MIRACL dataset, using native-language questions and generating responses with diverse large language models (LLMs), which are then assessed by expert annotators for faithfulness and relevance. We describe our annotation process and show that it achieves high inter-annotator agreement. We then analyse the performance of the answer-generating LLMs across languages as per the human evaluators. Finally we apply the dataset to our main use-case which is to benchmark multilingual automatic evaluators (LLM-as-a-judge). We show that our benchmark can reliably identify improvements offered by advanced prompting techniques and LLMs. Our dataset is available at https://github.com/amazon-science/MEMERAG

</details>


### [78] [SemEval-2025 Task 1: AdMIRe -- Advancing Multimodal Idiomaticity Representation](https://arxiv.org/abs/2503.15358)

*Thomas Pickard, Aline Villavicencio, Maggie Mi, Wei He, Dylan Phelps, Marco Idiart*

**Main category:** cs.CL

**Keywords:** idiomatic expressions, NLP, multimodal, LLM, SemEval-2025

**Relevance Score:** 8

**TL;DR:** The paper discusses a challenge in NLP regarding the interpretation of idiomatic expressions using LLMs and presents datasets for SemEval-2025 Task 1.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding idiomatic expressions is crucial for improving semantic representation in NLP, yet it remains a challenge even with advanced LLMs.

**Method:** Participants in the SemEval-2025 Task 1 competed in subtasks involving ranking images based on their relevance to idiomatic meanings and predicting the next image in a sequence, utilizing pretrained LLMs and vision-language models.

**Key Contributions:** Introduction of datasets for idiomaticity representation, Implementation of multimodal tasks for idiomatic expressions, Demonstration of human-level performance using advanced LLM techniques.

**Result:** Methods that combined pretrained models with a mixture of experts achieved human-level performance in understanding idiomaticity.

**Limitations:** The effectiveness of idiomatic understanding may vary across different languages and cultural contexts.

**Future Work:** Further exploration of the generalization of results to other languages and the integration of more diverse modalities.

**Conclusion:** Utilizing multimodal contexts and improved model architectures can significantly enhance the understanding of idiomatic expressions in NLP.

**Abstract:** Idiomatic expressions present a unique challenge in NLP, as their meanings are often not directly inferable from their constituent words. Despite recent advancements in Large Language Models (LLMs), idiomaticity remains a significant obstacle to robust semantic representation. We present datasets and tasks for SemEval-2025 Task 1: AdMiRe (Advancing Multimodal Idiomaticity Representation), which challenges the community to assess and improve models' ability to interpret idiomatic expressions in multimodal contexts and in multiple languages. Participants competed in two subtasks: ranking images based on their alignment with idiomatic or literal meanings, and predicting the next image in a sequence. The most effective methods achieved human-level performance by leveraging pretrained LLMs and vision-language models in mixture-of-experts settings, with multiple queries used to smooth over the weaknesses in these models' representations of idiomaticity.

</details>


### [79] [CausalRAG: Integrating Causal Graphs into Retrieval-Augmented Generation](https://arxiv.org/abs/2503.19878)

*Nengbo Wang, Xiaotian Han, Jagdip Singh, Jing Ma, Vipin Chaudhary*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, causal reasoning, natural language processing, knowledge-intensive tasks

**Relevance Score:** 9

**TL;DR:** CausalRAG introduces a novel retrieval framework that improves LLMs by integrating causal graphs, enhancing contextual integrity and retrieval precision.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of traditional Retrieval-Augmented Generation (RAG) systems in maintaining contextual integrity and retrieval accuracy.

**Method:** A new framework called CausalRAG is proposed, integrating causal graphs into the retrieval process to construct and trace causal relationships.

**Key Contributions:** Introduction of CausalRAG framework, Integration of causal graphs for improved context preservation, Demonstrated superiority over traditional RAG and graph-based RAG approaches

**Result:** CausalRAG outperforms regular RAG and graph-based RAG methods across various metrics, showing improved contextual continuity and response accuracy.

**Limitations:** Specific limitations not detailed in the abstract.

**Future Work:** To explore further applications of causal reasoning in various NLP and retrieval contexts.

**Conclusion:** Integrating causal reasoning into retrieval processes offers a promising approach for enhancing performance in knowledge-intensive tasks.

**Abstract:** Large language models (LLMs) have revolutionized natural language processing (NLP), particularly through Retrieval-Augmented Generation (RAG), which enhances LLM capabilities by integrating external knowledge. However, traditional RAG systems face critical limitations, including disrupted contextual integrity due to text chunking, and over-reliance on semantic similarity for retrieval. To address these issues, we propose CausalRAG, a novel framework that incorporates causal graphs into the retrieval process. By constructing and tracing causal relationships, CausalRAG preserves contextual continuity and improves retrieval precision, leading to more accurate and interpretable responses. We evaluate CausalRAG against regular RAG and graph-based RAG approaches, demonstrating its superiority across several metrics. Our findings suggest that grounding retrieval in causal reasoning provides a promising approach to knowledge-intensive tasks.

</details>


### [80] [Inaccuracy of an E-Dictionary and Its Influence on Chinese Language Users](https://arxiv.org/abs/2504.00799)

*Xi Wang, Fanfei Meng, Shiyang Zhang, Lan Li*

**Main category:** cs.CL

**Keywords:** Electronic Dictionaries, Youdao, Dictionary Literacy, AI in Dictionaries, User Consultation Behavior

**Relevance Score:** 3

**TL;DR:** This study investigates the reliability of electronic dictionaries, particularly Youdao, highlighting issues in definition accuracy and consultation habits among users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the reliability of electronic dictionaries for language learners and address the lack of scrutiny on their definitions.

**Method:** The study employed an experimental approach, a user survey, and a critique of Youdao, analyzing the translation tasks performed by participants and their consultation behaviors.

**Key Contributions:** Investigates the reliability of Youdao as an electronic dictionary., Analyzes user consultation habits and their impact on comprehension., Suggests improvements in AI model training for dictionary definitions.

**Result:** Findings indicate that incomplete or misleading definitions lead to misunderstandings, and users demonstrate problematic consultation habits.

**Limitations:** Limited to one electronic dictionary (Youdao) and may not generalize to others.

**Future Work:** Future research could address broader dictionary platforms and investigate user education in dictionary literacy.

**Conclusion:** The study calls for improved dictionary literacy among users and enhancements to AI models used in dictionary construction.

**Abstract:** Electronic dictionaries have largely replaced paper dictionaries and become central tools for L2 learners seeking to expand their vocabulary. Users often assume these resources are reliable and rarely question the validity of the definitions provided. The accuracy of major E-dictionaries is seldom scrutinized, and little attention has been paid to how their corpora are constructed. Research on dictionary use, particularly the limitations of electronic dictionaries, remains scarce. This study adopts a combined method of experimentation, user survey, and dictionary critique to examine Youdao, one of the most widely used E-dictionaries in China. The experiment involved a translation task paired with retrospective reflection. Participants were asked to translate sentences containing words that are insufficiently or inaccurately defined in Youdao. Their consultation behavior was recorded to analyze how faulty definitions influenced comprehension. Results show that incomplete or misleading definitions can cause serious misunderstandings. Additionally, students exhibited problematic consultation habits. The study further explores how such flawed definitions originate, highlighting issues in data processing and the integration of AI and machine learning technologies in dictionary construction. The findings suggest a need for better training in dictionary literacy for users, as well as improvements in the underlying AI models used to build E-dictionaries.

</details>


### [81] [LLM-based Automated Grading with Human-in-the-Loop](https://arxiv.org/abs/2504.05239)

*Hang Li, Yucheng Chu, Kaiqi Yang, Yasemin Copur-Gencturk, Jiliang Tang*

**Main category:** cs.CL

**Keywords:** automatic short answer grading, human-in-the-loop, large language models, dynamic grading rubrics, educational assessment

**Relevance Score:** 9

**TL;DR:** This paper introduces GradeHITL, an interactive framework that utilizes large language models (LLMs) in automatic short answer grading (ASAG) by integrating human insights for enhanced accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Exploring the potential of LLMs in automatic short answer grading tasks and addressing the limitations of existing methods in achieving human-level grading performance.

**Method:** The proposed GradeHITL framework employs a human-in-the-loop approach where LLMs interact with human experts to refine grading rubrics dynamically, improving performance.

**Key Contributions:** Introduction of the GradeHITL framework for ASAG using LLMs and human insights., Demonstrating substantial improvements in grading accuracy compared to traditional methods., Showcasing the potential of interactive LLM capabilities in educational assessment.

**Result:** The framework significantly improves grading accuracy, outperforming existing LLM-powered grading methods and getting closer to human-level evaluation.

**Limitations:** Challenges remain in achieving full automation and seamless integration of human feedback in ASAG processes.

**Future Work:** Further exploration of automated elements in the grading process while maintaining the effectiveness of human feedback could be beneficial.

**Conclusion:** GradeHITL allows for sophisticated grading scenarios by effectively leveraging human expertise alongside AI technologies, enhancing the grading process in education.

**Abstract:** The rise of artificial intelligence (AI) technologies, particularly large language models (LLMs), has brought significant advancements to the field of education. Among various applications, automatic short answer grading (ASAG), which focuses on evaluating open-ended textual responses, has seen remarkable progress with the introduction of LLMs. These models not only enhance grading performance compared to traditional ASAG approaches but also move beyond simple comparisons with predefined "golden" answers, enabling more sophisticated grading scenarios, such as rubric-based evaluation. However, existing LLM-powered methods still face challenges in achieving human-level grading performance in rubric-based assessments due to their reliance on fully automated approaches. In this work, we explore the potential of LLMs in ASAG tasks by leveraging their interactive capabilities through a human-in-the-loop (HITL) approach. Our proposed framework, GradeHITL, utilizes the generative properties of LLMs to pose questions to human experts, incorporating their insights to refine grading rubrics dynamically. This adaptive process significantly improves grading accuracy, outperforming existing methods and bringing ASAG closer to human-level evaluation.

</details>


### [82] [Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation](https://arxiv.org/abs/2504.07072)

*Israfel Salazar, Manuel Fernández Burda, Shayekh Bin Islam, Arshia Soltani Moakhar, Shivalika Singh, Fabian Farestam, Angelika Romanou, Danylo Boiko, Dipika Khullar, Mike Zhang, Dominik Krzemiński, Jekaterina Novikova, Luísa Shimabucoro, Joseph Marvin Imperial, Rishabh Maheshwary, Sharad Duwal, Alfonso Amayuelas, Swati Rajwal, Jebish Purbey, Ahmed Ruby, Nicholas Popovič, Marek Suppa, Azmine Toushik Wasi, Ram Mohan Rao Kadiyala, Olga Tsymboi, Maksim Kostritsya, Bardia Soltani Moakhar, Gabriel da Costa Merlin, Otávio Ferracioli Coletti, Maral Jabbari Shiviari, MohammadAmin farahani fard, Silvia Fernandez, María Grandury, Dmitry Abulkhanov, Drishti Sharma, Andre Guarnier De Mitri, Leticia Bossatto Marchezi, Setayesh Heydari, Johan Obando-Ceron, Nazar Kohut, Beyza Ermis, Desmond Elliott, Enzo Ferrante, Sara Hooker, Marzieh Fadaee*

**Main category:** cs.CL

**Keywords:** vision-language models, multilingual benchmarks, cultural diversity

**Relevance Score:** 6

**TL;DR:** Kaleidoscope is a new comprehensive multilingual and multicultural benchmark for evaluating vision-language models, addressing gaps in current evaluation metrics.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of comprehensive multilingual and multicultural evaluation benchmarks for vision-language models, which often rely on English and fail to capture cultural nuances.

**Method:** Kaleidoscope, a large-scale multimodal benchmark, evaluates vision-language models across 18 languages and 14 subjects, comprising 20,911 multiple-choice questions, developed through global researcher collaboration.

**Key Contributions:** Introduction of Kaleidoscope, a comprehensive multimodal benchmark for vision-language models., Focus on linguistic and cultural authenticity in evaluation benchmarks., Highlighting performance inadequacies of current models on low-resource languages.

**Result:** Evaluation of leading multilingual vision-language models reveals poor performance on low-resource languages and in complex scenarios, indicating significant room for improvement in culturally inclusive frameworks.

**Limitations:** The benchmark may not cover all languages and visual contexts; performance insights are based on current top models which may evolve.

**Future Work:** Future research should concentrate on enhancing performance in low-resource languages and exploring additional cultural contexts in multimodal evaluation.

**Conclusion:** The study underscores the necessity for more inclusive multimodal evaluation frameworks that take into account linguistic and cultural diversity.

**Abstract:** The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.

</details>


### [83] [Seed1.5-Thinking: Advancing Superb Reasoning Models with Reinforcement Learning](https://arxiv.org/abs/2504.13914)

*ByteDance Seed, :, Jiaze Chen, Tiantian Fan, Xin Liu, Lingjun Liu, Zhiqi Lin, Mingxuan Wang, Chengyi Wang, Xiangpeng Wei, Wenyuan Xu, Yufeng Yuan, Yu Yue, Lin Yan, Qiying Yu, Xiaochen Zuo, Chi Zhang, Ruofei Zhu, Zhecheng An, Zhihao Bai, Yu Bao, Xingyan Bin, Jiangjie Chen, Feng Chen, Hongmin Chen, Riwei Chen, Liangqiang Chen, Zixin Chen, Jinsong Chen, Siyan Chen, Kaiyuan Chen, Zhi Chen, Jin Chen, Jiecao Chen, Jinxin Chi, Weinan Dai, Ning Dai, Jiahui Dai, Shihan Dou, Yantao Du, Zhengyin Du, Jianhui Duan, Chen Dun, Ting-Han Fan, Jiazhan Feng, Junda Feng, Ziyuan Feng, Yuwei Fu, Wenqi Fu, Hanjie Fu, Hao Ge, Hongyi Guo, Mingji Han, Li Han, Wenhao Hao, Xintong Hao, Qianyu He, Jerry He, Feng He, Wen Heng, Zehua Hong, Qi Hou, Liang Hu, Shengding Hu, Nan Hu, Kai Hua, Qi Huang, Ziyue Huang, Hongzhi Huang, Zihao Huang, Ting Huang, Wenhao Huang, Wei Jia, Bin Jia, Xiaoying Jia, Yuhua Jiang, Haobin Jiang, Ziheng Jiang, Kaihua Jiang, Chengquan Jiang, Jianpeng Jiao, Xiaoran Jin, Xing Jin, Xunhao Lai, Zheng Li, Xiang Li, Liyi Li, Hongkai Li, Zheng Li, Shengxian Wan, Ya Wang, Yunshui Li, Chenggang Li, Niuniu Li, Siyu Li, Xi Li, Xiao Li, Aoyan Li, Yuntao Li, Nianning Liang, Xinnian Liang, Haibin Lin, Weijian Lin, Ye Lin, Zhicheng Liu, Guanlin Liu, Guanlin Liu, Chenxiao Liu, Yan Liu, Gaohong Liu, Juncai Liu, Chundian Liu, Deyi Liu, Kaibo Liu, Siyao Liu, Qi Liu, Yongfei Liu, Kang Liu, Gan Liu, Boyi Liu, Rui Long, Weiqiang Lou, Chenwei Lou, Xiang Luo, Yao Luo, Caiping Lv, Heyang Lv, Bole Ma, Qianli Ma, Hongzhi Ma, Yiyuan Ma, Jin Ma, Wenchang Ma, Tingting Ma, Chen Mao, Qiyang Min, Zhe Nan, Guanghan Ning, Jinxiang Ou, Haojie Pan, Renming Pang, Yanghua Peng, Tao Peng, Lihua Qian, Lihua Qian, Mu Qiao, Meng Qu, Cheng Ren, Hongbin Ren, Yong Shan, Wei Shen, Ke Shen, Kai Shen, Guangming Sheng, Jinlong Shi, Wenlei Shi, Guang Shi, Shuai Shuai Cao, Yuxin Song, Zuquan Song, Jing Su, Yifan Sun, Tao Sun, Zewei Sun, Borui Wan, Zihan Wang, Xiaohui Wang, Xi Wang, Shuguang Wang, Jun Wang, Qinlong Wang, Chenyuan Wang, Shuai Wang, Zihan Wang, Changbao Wang, Jiaqiang Wang, Shihang Wang, Xuwu Wang, Zaiyuan Wang, Yuxuan Wang, Wenqi Wang, Taiqing Wang, Chengzhi Wei, Houmin Wei, Ziyun Wei, Shufa Wei, Zheng Wu, Yonghui Wu, Yangjun Wu, Bohong Wu, Shuang Wu, Jingqiao Wu, Ning Wu, Shuangzhi Wu, Jianmin Wu, Chenguang Xi, Fan Xia, Yuqiao Xian, Liang Xiang, Boren Xiang, Bowen Xiao, Zhen Xiao, Xia Xiao, Yongsheng Xiao, Chao Xin, Shulin Xin, Yuwen Xiong, Jingjing Xu, Ziwen Xu, Chenyin Xu, Jiayi Xu, Yifan Xu, Wei Xu, Yufei Xu, Shikun Xu, Shipeng Yan, Shen Yan, Qingping Yang, Xi Yang, Tianhao Yang, Yuehang Yang, Yuan Yang, Ximing Yang, Zeyu Yang, Guang Yang, Yifan Yang, Xuesong Yao, Bairen Yi, Fan Yin, Jianian Yin, Ziqiang Ying, Xiangyu Yu, Hongli Yu, Song Yu, Menghan Yu, Huan Yu, Siyu Yuan, Jun Yuan, Yutao Zeng, Tianyang Zhan, Zheng Zhang, Yun Zhang, Mofan Zhang, Wang Zhang, Ru Zhang, Zhi Zhang, Tianqi Zhang, Xinyi Zhang, Zhexi Zhang, Sijun Zhang, Wenqiang Zhang, Xiangxiang Zhang, Yongtao Zhang, Yuyu Zhang, Ge Zhang, He Zhang, Yue Zhang, Renjie Zheng, Ningxin Zheng, Zhuolin Zheng, Yaowei Zheng, Chen Zheng, Xiaoyun Zhi, Wanjun Zhong, Cheng Zhong, Zheng Zhong, Baoquan Zhong, Xun Zhou, Na Zhou, Huan Zhou, Hang Zhu, Defa Zhu, Wenjia Zhu, Lei Zuo*

**Main category:** cs.CL

**Keywords:** Seed1.5-Thinking, reasoning models, Mixture-of-Experts, AI benchmarks, generalization

**Relevance Score:** 7

**TL;DR:** Introduction of Seed1.5-Thinking, a reasoning model with strong performance on various benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to enhance reasoning capabilities in AI models and demonstrate improved performance across diverse domains, particularly in STEM and coding tasks.

**Method:** Seed1.5-Thinking is a Mixture-of-Experts model with 20B active parameters and 200B total parameters, designed to perform reasoning through a structured thought process before responding.

**Key Contributions:** Introduction of a new reasoning methodology with notable performance improvements., Development of new benchmarks to assess generalized reasoning capabilities., Demonstration of applicability beyond reasoning tasks with higher win rates than existing models.

**Result:** Achieves high scores on multiple benchmarks: 86.7 on AIME 2024, 55.0 on Codeforces, and 77.3 on GPQA, also surpassing DeepSeek R1 in non-reasoning tasks.

**Limitations:** The paper does not address the implications of model size on efficiency or practical applications.

**Future Work:** Further research will focus on improving the reasoning model and exploring its applications in various real-world scenarios.

**Conclusion:** Seed1.5-Thinking's performance showcases its effectiveness in reasoning tasks and generalization capabilities across different domains, supported by new benchmarks for future research.

**Abstract:** We introduce Seed1.5-Thinking, capable of reasoning through thinking before responding, resulting in improved performance on a wide range of benchmarks. Seed1.5-Thinking achieves 86.7 on AIME 2024, 55.0 on Codeforces and 77.3 on GPQA, demonstrating excellent reasoning abilities in STEM and coding. Beyond reasoning tasks, the method demonstrates notable generalization across diverse domains. For instance, it surpasses DeepSeek R1 by 8% in win rate on non-reasoning tasks, indicating its broader applicability. Compared to other state-of-the-art reasoning models, Seed1.5-Thinking is a Mixture-of-Experts (MoE) model with a relatively small size, featuring 20B activated and 200B total parameters. As part of our effort to assess generalized reasoning, we develop two internal benchmarks, BeyondAIME and Codeforces, both of which will be publicly released to support future research. Model trial link: https://www.volcengine.com/experience/ark.

</details>


### [84] [Computational Typology](https://arxiv.org/abs/2504.15642)

*Gerhard Jäger*

**Main category:** cs.CL

**Keywords:** typology, computational methods, linguistics, statistical modeling, language structures

**Relevance Score:** 2

**TL;DR:** This article discusses the role of computational methods in the study of language typology and illustrates the benefits of computational statistical modeling for analyzing large-scale linguistic data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the classification of languages based on structural features and the significance of computational methods in linguistic research.

**Method:** The article illustrates the application of computational statistical modeling to linguistic data analysis and hypothesis testing in typology.

**Key Contributions:** Demonstrates the integration of computational techniques in linguistic typology., Illustrates the benefits of statistical modeling for understanding language universals and structures.

**Result:** The use of computational methods enhances the analysis of linguistic structures and supports testing hypotheses about language diversity and evolution.

**Limitations:** 

**Future Work:** Further exploration of computational methods in other areas of linguistics.

**Conclusion:** Computational statistical modeling is beneficial for advancing research in language typology by enabling large-scale data analysis.

**Abstract:** Typology is a subfield of linguistics that focuses on the study and classification of languages based on their structural features. Unlike genealogical classification, which examines the historical relationships between languages, typology seeks to understand the diversity of human languages by identifying common properties and patterns, known as universals. In recent years, computational methods have played an increasingly important role in typological research, enabling the analysis of large-scale linguistic data and the testing of hypotheses about language structure and evolution. This article provides an illustration of the benefits of computational statistical modeling in typology.

</details>
