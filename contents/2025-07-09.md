# 2025-07-09

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 17]

- [cs.CL](#cs.CL) [Total: 76]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Esports and expertise: what competitive gaming can teach us about mastery](https://arxiv.org/abs/2507.05446)

*Ben Boudaoud, Josef Spjut, Joohwan Kim, Arjun Madhusudan, Benjamin Watson*

**Main category:** cs.HC

**Keywords:** human-computer interaction, esports, performance evaluation, holistic mastery, competitive gaming

**Relevance Score:** 8

**TL;DR:** This paper explores a holistic approach to evaluating human performance in HCI, emphasizing mastery in complex tasks like competitive gaming rather than focusing solely on atomic task completion.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper argues that traditional measures of productivity in HCI, which emphasize individual task completion time, overlook the deeper aspects of human performance demonstrated in competitive gaming and esports.

**Method:** The authors analyze performance metrics in esports, contrasting them with conventional HCI research focusing on generalizable task completion.

**Key Contributions:**

	1. Proposes a new evaluation framework for HCI centered on mastery and holistic task performance.
	2. Draws parallels between artistic mastery and skills in competitive gaming in terms of performance evaluation.
	3. Advocates for expanding HCI research methodologies to include complex, task-specific skills.

**Result:** The study highlights that skilled gamers optimize specific, task-related skills over time, suggesting that HCI should adopt a similar focus on holistic task mastery.

**Limitations:** 

**Conclusion:** A shift in HCI research towards valuing holistic mastery in user performance can lead to richer evaluations of user interaction, particularly in complex domains like games.

**Abstract:** Historically, much research and development in human computer interaction has focused on atomic and generalizable tasks, where task completion time indicates productivity. However, the emergence of competitive games and esports reminds us of an alternative perspective on human performance in HCI: mastery of higher-level, holistic practices. Just as a world-renowned artist is rarely evaluated for their individual brush strokes, so skilled competitive gamers rarely succeed solely by completing individual mouse movements or keystrokes as quickly as possible. Instead, they optimize more task-specific skills, adeptly performing challenges deep in the learning curve for their game of choice.

</details>


### [2] [NRXR-ID: Two-Factor Authentication (2FA) in VR Using Near-Range Extended Reality and Smartphones](https://arxiv.org/abs/2507.05447)

*Aiur Nanzatov, Lourdes Pe√±a-Castillo, Oscar Meruvia-Pastor*

**Main category:** cs.HC

**Keywords:** two-factor authentication, virtual reality, extended reality, user study, authenticating challenges

**Relevance Score:** 5

**TL;DR:** NRXR-ID is a technique for two-factor authentication in extended reality that allows users to authenticate using smartphones without removing their head-mounted displays (HMD).

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of implementing two-factor authentication in virtual reality environments where users cannot see their real-world surroundings due to head-mounted displays.

**Method:** We conducted a user study exploring four types of authentication challenges, including a novel checkers-style challenge, with participants responding under three different configurations, including gaze-based selection using smartphones.

**Key Contributions:**

	1. Introduction of NRXR-ID for 2FA in VR
	2. Evaluation of multiple authentication challenge types in a user study
	3. Identification of the most effective authentication challenge format for VR

**Result:** The study involved 30 participants and found that the checkers-style visual matching challenge was the most effective, followed by the PIN entry challenge performed via smartphone in the VR environment.

**Limitations:** 

**Conclusion:** The NRXR-ID technique presents an effective way to implement two-factor authentication in VR, enhancing user experience and security without requiring removal of HMDs.

**Abstract:** Two-factor authentication (2FA) has become widely adopted as an efficient and secure way to validate someone's identity online. Two-factor authentication is difficult in virtual reality (VR) because users are usually wearing a head-mounted display (HMD) which does not allow them to see their real-world surroundings. We present NRXR-ID, a technique to implement two-factor authentication while using extended reality systems and smartphones. The proposed method allows users to complete an authentication challenge using their smartphones without removing their HMD. We performed a user study where we explored four types of challenges for users, including a novel checkers-style challenge. Users responded to these challenges under three different configurations, including a technique that uses the smartphone to support gaze-based selection without the use of VR controllers. A 4X3 within-subjects design allowed us to study all the variations proposed. We collected performance metrics and performed user experience questionnaires to collect subjective impressions from 30 participants. Results suggest that the checkers-style visual matching challenge was the most appropriate option, followed by entering a digital PIN challenge submitted via the smartphone and answered within the VR environment.

</details>


### [3] [GLOSS: Group of LLMs for Open-Ended Sensemaking of Passive Sensing Data for Health and Wellbeing](https://arxiv.org/abs/2507.05461)

*Akshat Choube, Ha Le, Jiachen Li, Kaixin Ji, Vedant Das Swain, Varun Mishra*

**Main category:** cs.HC

**Keywords:** sensemaking, passive sensing, health informatics, HCI, UbiComp

**Relevance Score:** 9

**TL;DR:** This paper presents a new system, GLOSS, for open-ended sensemaking that effectively utilizes passive sensing data from smartphones and wearables to derive insights about health and behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a better understanding of individual behavior and context using passive sensing data, which is challenging due to the need for domain knowledge and technical expertise.

**Method:** We developed the Group of LLMs for Open-ended Sensemaking (GLOSS) system, which allows for complex multimodal triangulation and open-ended sensemaking.

**Key Contributions:**

	1. Introduction of GLOSS for open-ended sensemaking
	2. Demonstration of significant performance improvements over RAG
	3. Use case applications in HCI and UbiComp communities.

**Result:** GLOSS significantly outperforms the Retrieval-Augmented Generation (RAG) method, achieving an accuracy of 87.93% and a consistency of 66.19%, compared to RAG's 29.31% accuracy and 52.85% consistency.

**Limitations:** The limitations of our work need to be further explored and discussed.

**Conclusion:** GLOSS showcases the potential for improved insights through passive sensing and has broader implications for HCI and Ubicomp fields, although there are limitations in our approach.

**Abstract:** The ubiquitous presence of smartphones and wearables has enabled researchers to build prediction and detection models for various health and behavior outcomes using passive sensing data from these devices. Achieving a high-level, holistic understanding of an individual's behavior and context, however, remains a significant challenge. Due to the nature of passive sensing data, sensemaking -- the process of interpreting and extracting insights -- requires both domain knowledge and technical expertise, creating barriers for different stakeholders. Existing systems designed to support sensemaking are either not open-ended or cannot perform complex data triangulation. In this paper, we present a novel sensemaking system, Group of LLMs for Open-ended Sensemaking (GLOSS), capable of open-ended sensemaking and performing complex multimodal triangulation to derive insights. We demonstrate that GLOSS significantly outperforms the commonly used Retrieval-Augmented Generation (RAG) technique, achieving 87.93% accuracy and 66.19% consistency, compared to RAG's 29.31% accuracy and 52.85% consistency. Furthermore, we showcase the promise of GLOSS through four use cases inspired by prior and ongoing work in the UbiComp and HCI communities. Finally, we discuss the potential of GLOSS, its broader implications, and the limitations of our work.

</details>


### [4] [W2W: A Simulated Exploration of IMU Placement Across the Human Body for Designing Smarter Wearable](https://arxiv.org/abs/2507.05532)

*Lala Shakti Swarup Ray, Bo Zhou, Paul Lukowicz*

**Main category:** cs.HC

**Keywords:** Inertial measurement units, Sensor placement, Wearable systems, Activity recognition, Simulation

**Relevance Score:** 6

**TL;DR:** This paper presents W2W, a simulation framework for optimizing the placement of inertial measurement units (IMUs) in wearable systems for improved activity recognition and pose estimation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically explore and improve IMU sensor placement for better performance in wearable systems, moving beyond heuristic methods.

**Method:** The W2W framework generates synthetic IMU signals based on motion capture data at various points on the body to evaluate sensor performance across multiple tasks.

**Key Contributions:**

	1. Introduction of the W2W framework for IMU placement evaluation
	2. Validation with real data supporting the framework's reliability
	3. Identification of overlooked high-utility sensor placement regions

**Result:** W2W demonstrated reliable predictions of sensor performance placement, showing strong agreement with real IMU data and highlighting neglected high-utility regions.

**Limitations:** The framework may still require validation in diverse real-world conditions and may be limited to specific types of activities tested.

**Conclusion:** W2W provides an effective tool for optimizing sensor placement through simulation, making data-driven design more feasible.

**Abstract:** Inertial measurement units (IMUs) are central to wearable systems for activity recognition and pose estimation, but sensor placement remains largely guided by heuristics and convention. In this work, we introduce Where to Wear (W2W), a simulation-based framework for systematic exploration of IMU placement utility across the body. Using labeled motion capture data, W2W generates realistic synthetic IMU signals at 512 anatomically distributed surface patches, enabling high-resolution, task-specific evaluation of sensor performance. We validate reliability of W2W by comparing spatial performance rankings from synthetic data with real IMU recordings in two multimodal datasets, confirming strong agreement in activity-wise trends. Further analysis reveals consistent spatial trends across activity types and uncovers overlooked high-utility regions that are rarely used in commercial systems. These findings challenge long-standing placement norms and highlight opportunities for more efficient, task-adaptive sensor configurations. Overall, our results demonstrate that simulation with W2W can serve as a powerful design tool for optimizing sensor placement, enabling scalable, data-driven strategies that are impractical to obtain through physical experimentation alone.

</details>


### [5] [Information Needs and Practices Supported by ChatGPT](https://arxiv.org/abs/2507.05537)

*Tim Gorichanaz*

**Main category:** cs.HC

**Keywords:** ChatGPT, information needs, human-computer interaction, generative AI, user practices

**Relevance Score:** 8

**TL;DR:** The study analyzes how people use ChatGPT for various information needs across different life domains, identifying supported practices like writing and ideation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the information needs and practices that ChatGPT supports based on user interactions.

**Method:** Qualitative content analysis of 205 user vignettes detailing how individuals engage with ChatGPT.

**Key Contributions:**

	1. Identification of six categories of information practices supported by ChatGPT
	2. Insight into the diverse information needs people have when using ChatGPT
	3. Framework for future research on generative AI and information experience

**Result:** ChatGPT is utilized across diverse life areas for multiple needs, revealing six categories of information practices: Writing, Deciding, Identifying, Ideating, Talking, and Critiquing.

**Limitations:** 

**Conclusion:** Information needs in the AI age should encompass both understanding and action, highlighting potential for future research at the intersection of generative AI and information practices.

**Abstract:** This study considers ChatGPT as an information source, investigating the information needs that people come to ChatGPT with and the information practices that ChatGPT supports, through a qualitative content analysis of 205 user vignettes. The findings show that ChatGPT is used in a range of life domains (home/family, work, leisure, etc.) and for a range of human needs (writing/editing, learning, simple programming tasks, etc.), constituting the information needs that people use ChatGPT to address. Related to these information needs, the findings show six categories of information practices that ChatGPT supports: Writing, Deciding, Identifying, Ideating, Talking, and Critiquing. This work suggests that, in the AI age, information need should be conceptualized not just as a matter of "getting questions answered" or even "making sense," but as skillfully coping in the world, a notion that includes both understanding and action. This study leads to numerous opportunities for future work at the junction of generative AI and information needs, seeking, use and experience.

</details>


### [6] [AnatomyCarve: A VR occlusion management technique for medical images based on segment-aware clipping](https://arxiv.org/abs/2507.05572)

*Andrey Titov, Tina N. H. Nantenaina, Marta Kersten-Oertel, Simon Drouin*

**Main category:** cs.HC

**Keywords:** 3D visualization, medical imaging, VR, anatomical illustrations, user satisfaction

**Relevance Score:** 7

**TL;DR:** AnatomyCarve is a VR technique that generates high-quality medical illustrations by allowing users to selectively clip 3D segments of medical volumes, enhancing visualization and preserving context, with positive feedback from usability studies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Visualizing 3D medical images is difficult due to self-occlusion, which obscures anatomical structures. Traditional methods do not effectively represent internal anatomy in context, necessitating an improved approach.

**Method:** AnatomyCarve employs a novel technique in a VR setting to clip segments from 3D medical volumes, integrating advanced rendering and natural interactions to enhance visualization.

**Key Contributions:**

	1. Introduction of a novel VR technique for medical illustration
	2. Enhanced visualization by combining rendering with user interactions
	3. Positive usability and effectiveness feedback from medical professionals

**Result:** User studies indicated that AnatomyCarve provided customized anatomical visualizations, leading to high user satisfaction among non-experts and improved effectiveness for surgical planning among practicing neurosurgeons.

**Limitations:** 

**Conclusion:** AnatomyCarve shows promise for both educational purposes and clinical applications in the visualization of complex 3D medical images.

**Abstract:** Visualizing 3D medical images is challenging due to self-occlusion, where anatomical structures of interest can be obscured by surrounding tissues. Existing methods, such as slicing and interactive clipping, are limited in their ability to fully represent internal anatomy in context. In contrast, hand-drawn medical illustrations in anatomy books manage occlusion effectively by selectively removing portions based on tissue type, revealing 3D structures while preserving context. This paper introduces AnatomyCarve, a novel technique developed for a VR environment that creates high-quality illustrations similar to those in anatomy books, while remaining fast and interactive. AnatomyCarve allows users to clip selected segments from 3D medical volumes, preserving spatial relations and contextual information. This approach enhances visualization by combining advanced rendering techniques with natural user interactions in VR. Usability of AnatomyCarve was assessed through a study with non-experts, while surgical planning effectiveness was evaluated with practicing neurosurgeons and residents. The results show that AnatomyCarve enables customized anatomical visualizations, with high user satisfaction, suggesting its potential for educational and clinical applications.

</details>


### [7] [StoryGrid: A Tangible Interface for Student Expression](https://arxiv.org/abs/2507.05600)

*Tom Moher, Louis Gomez, Janet Kim, Claudia Hindo, Benjamin Watson, Stephen Fransen, Tim McEneany*

**Main category:** cs.HC

**Keywords:** interactive multimedia, education technology, literary interpretation

**Relevance Score:** 4

**TL;DR:** StorySpace is an interactive multimedia design system for classrooms, allowing students to engage with literature through physical tokens and multimedia manipulation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to improve student engagement and interpretation of literature in an urban high school setting.

**Method:** Utilizing a design framework that incorporates user feedback, StorySpace enables group interactions with multimedia content on a shared board.

**Key Contributions:**

	1. Innovative use of physical tokens for digital interactions in literature education
	2. Design adaptations based on direct feedback from users
	3. Enhanced student engagement through multimedia integration

**Result:** Student interactions with StorySpace led to improved understanding of audience considerations and the recognition of diverse perspectives in literary analysis.

**Limitations:** 

**Conclusion:** Ongoing refinements of StorySpace based on feedback have enhanced its effectiveness as a tool for literary interpretation.

**Abstract:** StorySpace is a classroom-based design and presentation system for interactive multimedia posters. Employing the technology base first used in Eden's PITAboard [2002], StorySpace allows groups of learners to manipulate projected multimedia objects on a horizontal board using a small collection of shared physical tokens. In this paper, we present the ongoing design history of StorySpace in the context of its introduction within an urban high school literature class. Interface modifications based on student and teacher feedback led on changes in token semantics and media importing methods. We describe how StorySpace features enriched students' interpretations of literature, with particular emphasis in two areas: (1) attention to audience, and (2) reflection of multiple perspectives.

</details>


### [8] [Hapster: Using Apple Watch Haptics to Enable Live Low-Friction Student Feedback in the Physical Classroom](https://arxiv.org/abs/2507.05605)

*Oleg Aleksandrovich Golev, Michelle Huang, Chanketya Nop, Kritin Vongthongsri, Andr√©s Monroy-Hern√°ndez, Parastoo Abtahi*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Student Response Systems, Haptic Feedback, Teacher Engagement, Accessibility

**Relevance Score:** 8

**TL;DR:** Hapster, a prototype SRS using Apple Watch for live feedback via haptics, improves engagement but poses challenges in feedback perception.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance student response systems (SRSs) beyond visual interfaces by incorporating haptic feedback.

**Method:** Prototype evaluation involving 6 instructors and 155 students at a U.S. university.

**Key Contributions:**

	1. Introduction of haptic feedback in SRSs
	2. Evaluation of user experience with live feedback
	3. Insights into instructor-student engagement dynamics

**Result:** Participants reported effective delivery of feedback and improved engagement, but instructors faced challenges with haptic feedback perception.

**Limitations:** Challenges noted in instructor's ability to differentiate haptic sequences during lectures.

**Conclusion:** Haptics can effectively relay live feedback but require further research on accessibility and interaction mechanisms.

**Abstract:** The benefits of student response systems (SRSs) for in-person lectures are well-researched. However, all current SRSs only rely on a visual interface to relay information to the instructor. We describe the design and evaluation of Hapster, a prototype system that uses an Apple Watch to deliver live, aggregated student feedback to the instructor via both visual and vibro-tactile modalities. We evaluated this system with 6 instructors and 155 students at a U.S. university. Participants reported that the system was effective at delivering live student feedback and facilitating better engagement from both the instructor and the students. However, instructors also noted several challenges with differentiating and perceiving the haptic sequences while lecturing. We conclude by discussing the tradeoff between system flexibility and abuse potential while identifying opportunities for further research regarding accessibility, content moderation, and additional interaction modalities. Our results suggest that haptics can be used as an effective live feedback mechanism for instructors in the physical classroom.

</details>


### [9] [Breaking the Plane: Exploring Real-Time Visualization of 3D Surfaces in Augmented Reality with Handwritten Input](https://arxiv.org/abs/2507.05616)

*Liam Franco Esparraguera, Kristoffer Selberg, Brian Lou, Jenny Sun, Beza Desta, Andr√©s Monroy-Hern√°ndez, Parastoo Abtahi*

**Main category:** cs.HC

**Keywords:** augmented reality, mathematics education, handwritten input, 3D visualization, human-computer interaction

**Relevance Score:** 8

**TL;DR:** An AR application that visualizes 3D mathematical functions through handwritten input, enhancing comprehension and engagement in learning.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** Current AR educational tools lack integration of real-time interactions with 3D visualizations. This study aims to bridge that gap to improve learning outcomes in mathematics.

**Method:** Development of a system featuring handwritten equation parsing, graph manipulation, and a 3D function plotter, followed by user engagement studies.

**Key Contributions:**

	1. Combines real-time handwritten equation parsing with dynamic 3D visualizations.
	2. Demonstrates significant engagement improvements over existing educational tools.
	3. Offers effective problem-solving support in augmented learning settings.

**Result:** The system significantly outperformed others in engagement and was as easy to use as a popular tool, aiding problem-solving effectively and gaining high user preference.

**Limitations:** 

**Conclusion:** Breaking the Plane demonstrates the benefits of integrating handwritten input and 3D visualizations in AR to enhance educational experiences in mathematics.

**Abstract:** We introduce Breaking the Plane, an augmented reality (AR) application built for AR headsets that enables users to visualize 3D mathematical functions using handwritten input. Researchers have demonstrated overlaying 3D visualizations of mathematical concepts through AR enhances learning motivation and comprehension, and equation parsing makes the authoring of teaching materials more time-efficient for instructors. Previous works have developed AR systems that separately employ equation parsing and 3D mathematical visualizations, but work has yet to be done to combine those features by enabling real-time interactions and dynamic visualizations that help users learn in situ. We explore this by developing an AR system featuring handwritten equation parsing, graph manipulation, and a 3D function plotter. We found that our system significantly surpassed other systems in engagement, achieved comparable ease of use to a popular visualization tool, was considered the most effective in aiding problem-solving, and was highly preferred by participants for future use.

</details>


### [10] [Constella: Supporting Storywriters' Interconnected Character Creation through LLM-based Multi-Agents](https://arxiv.org/abs/2507.05820)

*Syemin Park, Soobin Park, Youn-kyung Lim*

**Main category:** cs.HC

**Keywords:** human-computer interaction, storywriting, LLM, character creation, multi-agent systems

**Relevance Score:** 7

**TL;DR:** Constella is an LLM-based tool designed to assist storywriters in creating interconnected characters, improving their understanding of character dynamics.

**Read time:** 50 min

<details>
  <summary>Details</summary>

**Motivation:** Storywriters face challenges in creating characters that influence each other and in fleshing out their relationships.

**Method:** Constella features character suggestion, inner mindscape journaling, and inter-character response mechanisms, employed in a study with storywriters over 7-8 days.

**Key Contributions:**

	1. Development of the Constella tool to aid character creation
	2. Introduction of innovative features like FRIENDS DISCOVERY, JOURNALS, and COMMENTS
	3. Insights from user deployment showing improved character interconnectedness

**Result:** The deployment study demonstrated that Constella helped writers create expansive character communities, compare character thoughts and emotions, and enhance the understanding of relationships.

**Limitations:** 

**Conclusion:** Multi-agent interactions in tools like Constella can distribute the writers' attention and effort effectively across multiple characters.

**Abstract:** Creating a cast of characters by attending to their relational dynamics is a critical aspect of most long-form storywriting. However, our formative study (N=14) reveals that writers struggle to envision new characters that could influence existing ones, to balance similarities and differences among characters, and to intricately flesh out their relationships. Based on these observations, we designed Constella, an LLM-based multi-agent tool that supports storywriters' interconnected character creation process. Constella suggests related characters (FRIENDS DISCOVERY feature), reveals the inner mindscapes of several characters simultaneously (JOURNALS feature), and manifests relationships through inter-character responses (COMMENTS feature). Our 7-8 day deployment study with storywriters (N=11) shows that Constella enabled the creation of expansive communities composed of related characters, facilitated the comparison of characters' thoughts and emotions, and deepened writers' understanding of character relationships. We conclude by discussing how multi-agent interactions can help distribute writers' attention and effort across the character cast.

</details>


### [11] [Evaluation of Large Language Model-Driven AutoML in Data and Model Management from Human-Centered Perspective](https://arxiv.org/abs/2507.05962)

*Jiapeng Yao, Lantian Zhang, Jiping Huang*

**Main category:** cs.HC

**Keywords:** Large Language Models, Automated Machine Learning, Human-AI collaboration

**Relevance Score:** 9

**TL;DR:** This research explores how Large Language Models (LLMs) can simplify Machine Learning (ML) adoption through a human-centered Automated Machine Learning (AutoML) approach, demonstrating significant improvements in implementation success and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address barriers to ML adoption due to technical complexity and to assess how LLMs can enhance accessibility to ML technologies.

**Method:** Conducted a user study with 15 professionals across various roles and technical backgrounds to evaluate the impact of an LLM-based AutoML framework versus traditional methods.

**Key Contributions:**

	1. Pioneering evidence of improved ML implementation success rates using LLMs.
	2. Natural language interfaces reduce implementation time by 50% while enhancing accuracy.
	3. Insights for human-AI collaborative systems, drastically reducing error resolution time.

**Result:** The study found that 93.34% of users achieved better performance with LLM-based interfaces, with notable improvements in accuracy and significant reductions in development time and error resolution.

**Limitations:** 

**Conclusion:** LLM-based AutoML can democratize ML capabilities in organizations, improving success rates, efficiency, and bridging the technical skills gap.

**Abstract:** As organizations increasingly seek to leverage machine learning (ML) capabilities, the technical complexity of implementing ML solutions creates significant barriers to adoption and impacts operational efficiency. This research examines how Large Language Models (LLMs) can transform the accessibility of ML technologies within organizations through a human-centered Automated Machine Learning (AutoML) approach. Through a comprehensive user study involving 15 professionals across various roles and technical backgrounds, we evaluate the organizational impact of an LLM-based AutoML framework compared to traditional implementation methods. Our research offers four significant contributions to both management practice and technical innovation: First, we present pioneering evidence that LLM-based interfaces can dramatically improve ML implementation success rates, with 93.34% of users achieved superior performance in the LLM condition, with 46.67% showing higher accuracy (10-25% improvement over baseline) and 46.67% demonstrating significantly higher accuracy (>25% improvement over baseline), while 6.67% maintained comparable performance levels; and 60% reporting substantially reduced development time. Second, we demonstrate how natural language interfaces can effectively bridge the technical skills gap in organizations, cutting implementation time by 50% while improving accuracy across all expertise levels. Third, we provide valuable insights for organizations designing human-AI collaborative systems, showing that our approach reduced error resolution time by 73% and significantly accelerated employee learning curves. Finally, we establish empirical support for natural language as an effective interface for complex technical systems, offering organizations a path to democratize ML capabilities without compromising quality or performance.

</details>


### [12] [Exploring Collaboration Patterns and Strategies in Human-AI Co-creation through the Lens of Agency: A Scoping Review of the Top-tier HCI Literature](https://arxiv.org/abs/2507.06000)

*Shuning Zhang, Hui Wang, Xin Yi*

**Main category:** cs.HC

**Keywords:** agency, Human-Computer Interaction, co-creation, control mechanisms, CSCW

**Relevance Score:** 9

**TL;DR:** This paper reviews HCI/CSCW literature on agency in AI co-creation, identifying control mechanisms and interaction dynamics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the complexities of agency in AI co-creation, particularly in HCI/CSCW contexts, addressing a notable gap in systematic reviews.

**Method:** A review of 134 papers from top-tier HCI and CSCW venues over the last 20 years to synthesize agency configurations and control mechanisms.

**Key Contributions:**

	1. Theoretical framework for agency patterns
	2. Comprehensive catalog of control mechanisms
	3. Cross-context map for co-creative practices

**Result:** The review identifies four contributions: a theoretical framework for agency patterns, a catalog of control mechanisms, a cross-context map for co-creative practices, and guidance for future CSCW research.

**Limitations:** 

**Conclusion:** The findings provide actionable insights into the design of co-creative systems, emphasizing trust and ethics in AI interactions.

**Abstract:** As Artificial Intelligence (AI) increasingly becomes an active collaborator in co-creation, understanding the distribution and dynamic of agency is paramount. The Human-Computer Interaction (HCI) perspective is crucial for this analysis, as it uniquely reveals the interaction dynamics and specific control mechanisms that dictate how agency manifests in practice. Despite this importance, a systematic synthesis mapping agency configurations and control mechanisms within the HCI/CSCW literature is lacking. Addressing this gap, we reviewed 134 papers from top-tier HCI/CSCW venues (e.g., CHI, UIST, CSCW) over the past 20 years. This review yields four primary contributions: (1) an integrated theoretical framework structuring agency patterns, control mechanisms, and interaction contexts, (2) a comprehensive operational catalog of control mechanisms detailing how agency is implemented; (3) an actionable cross-context map linking agency configurations to diverse co-creative practices; and (4) grounded implications and guidance for future CSCW research and the design of co-creative systems, addressing aspects like trust and ethics.

</details>


### [13] [Large Language Models Predict Human Well-being -- But Not Equally Everywhere](https://arxiv.org/abs/2507.06141)

*Pat Pataranutaporn, Nattavudh Powdthavee, Chayapatr Archiwaranguprok, Pattie Maes*

**Main category:** cs.HC

**Keywords:** large language models, well-being prediction, systematic bias, global inequality, data representation

**Relevance Score:** 8

**TL;DR:** This paper evaluates the ability of large language models (LLMs) to predict subjective well-being across diverse global populations, revealing systematic biases and limitations in their predictive accuracy, especially in underrepresented countries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if large language models can accurately predict well-being metrics in diverse populations, thereby informing their use in economic, medical, and policy decision-making.

**Method:** Evaluated four leading LLMs using data from 64,000 individuals across 64 countries, focusing on their predictive accuracy and bias in relation to training data representation.

**Key Contributions:**

	1. Identification of biases in LLM predictions based on training data representation
	2. Demonstration of the importance of context-specific data for improving LLM accuracy
	3. Highlighting the pitfalls of relying solely on surface-level linguistic similarity for well-being predictions

**Result:** LLMs capture broad correlations like income and health, but their accuracy diminishes for underrepresented countries, indicating systemic biases and misestimations based on linguistic similarity instead of conceptual understanding.

**Limitations:** Significant predictive accuracy gaps remain even after injecting findings from underrepresented contexts, suggesting ongoing issues with LLM understanding.

**Conclusion:** LLMs show potential in predicting well-being but require robust validation and improvement, especially in underrepresented contexts, to avoid misestimations before implementation.

**Abstract:** Subjective well-being is a key metric in economic, medical, and policy decision-making. As artificial intelligence provides scalable tools for modelling human outcomes, it is crucial to evaluate whether large language models (LLMs) can accurately predict well-being across diverse global populations. We evaluate four leading LLMs using data from 64,000 individuals in 64 countries. While LLMs capture broad correlates such as income and health, their predictive accuracy decreases in countries underrepresented in the training data, highlighting systematic biases rooted in global digital and economic inequality. A pre-registered experiment demonstrates that LLMs rely on surface-level linguistic similarity rather than conceptual understanding, leading to systematic misestimations in unfamiliar or resource-limited settings. Injecting findings from underrepresented contexts substantially enhances performance, but a significant gap remains. These results highlight both the promise and limitations of LLMs in predicting global well-being, underscoring the importance of robust validation prior to their implementation across these areas.

</details>


### [14] [V(is)owel: An Interactive Vowel Chart to Understand What Makes Visual Pronunciation Effective in Second Language Learning](https://arxiv.org/abs/2507.06202)

*Charlotte Kiesel, Dipayan Mukherjee, Mark Hasegawa-Johnson, Karrie Karahalios*

**Main category:** cs.HC

**Keywords:** visual feedback, pronunciation improvement, vowel chart, language learning, interaction design

**Relevance Score:** 5

**TL;DR:** V(is)owel, an interactive vowel chart, enhances pronunciation learning in second language acquisition through visual feedback, outperforming auditory-only methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the effects of visual feedback on pronunciation improvement in second language learners.

**Method:** Comparison of V(is)owel, a visual feedback tool, with an auditory-only method to analyze learner responses to both feedback types.

**Key Contributions:**

	1. Development of V(is)owel as an interactive tool for pronunciation improvement.
	2. Demonstration of the effectiveness of visual feedback in language learning.
	3. Insights into learner preferences for feedback methods.

**Result:** Participants using V(is)owel reported higher motivation and practiced more effectively than with audio alone.

**Limitations:** Limited sample size of eight participants may affect generalizability of findings.

**Conclusion:** V(is)owel provides actionable feedback by mapping physical movements, leading to better pronunciation outcomes for phonetically untrained learners.

**Abstract:** Visual feedback speeds up learners' improvement of pronunciation in a second language. The visual combined with audio allows speakers to see sounds and differences in pronunciation that they are unable to hear. Prior studies have tested different visual methods for improving pronunciation, however, we do not have conclusive understanding of what aspects of the visualizations contributed to improvements. Based on previous work, we created V(is)owel, an interactive vowel chart. Vowel charts provide actionable feedback by directly mapping physical tongue movement onto a chart. We compared V(is)owel with an auditory-only method to explore how learners parse visual and auditory feedback to understand how and why visual feedback is effective for pronunciation improvement. The findings suggest that designers should include explicit anatomical feedback that directly maps onto physical movement for phonetically untrained learners. Furthermore, visual feedback has the potential to motivate more practice since all eight of the participants cited using the visuals as a goal with V(is)owel versus relying on their own judgment with audio alone. Their statements are backed up by all participants practicing words with V(is)owel more than with audio-only. Our results indicate that V(is)owel is effective at providing actionable feedback, demonstrating the potential of visual feedback methods in second language learning.

</details>


### [15] [EvAlignUX: Advancing UX Evaluation through LLM-Supported Metrics Exploration](https://arxiv.org/abs/2409.15471)

*Qingxiao Zheng, Minrui Chen, Pranav Sharma, Yiliu Tang, Mehul Oswal, Yiren Liu, Yun Huang*

**Main category:** cs.HC

**Keywords:** UX evaluation, HCI, large language models, design evaluation, AI

**Relevance Score:** 9

**TL;DR:** EvAlignUX is a system that assists HCI researchers in developing UX evaluation plans using large language models, improving evaluation quality and critical thinking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in evaluating UX due to AI's complexity and unpredictability, aiming to support HCI researchers in creating comprehensive evaluation plans.

**Method:** Introduction of EvAlignUX, a system leveraging large language models to explore evaluation metrics related to research outcomes, followed by a user study with HCI scholars.

**Key Contributions:**

	1. Introduction of EvAlignUX for UX evaluation planning
	2. Creation of a 'UX Question Bank'
	3. Insights into researchers' concerns about AI over-reliance

**Result:** Participants reported improved perceived quality and confidence in their UX evaluation plans, leading to the creation of a 'UX Question Bank' and enhanced thought processes.

**Limitations:** 

**Conclusion:** The study suggests a shift from a 'method-centric' to a 'mindset-centric' approach in UX evaluation is necessary for meaningful design evaluation.

**Abstract:** Evaluating UX in the context of AI's complexity, unpredictability, and generative nature presents unique challenges. How can we support HCI researchers to create comprehensive UX evaluation plans? In this paper, we introduce EvAlignUX, a system powered by large language models and grounded in scientific literature, designed to help HCI researchers explore evaluation metrics and their relationship to research outcomes. A user study with 19 HCI scholars showed that EvAlignUX improved the perceived quality and confidence in UX evaluation plans while prompting deeper consideration of research impact and risks. The system enhanced participants' thought processes, leading to the creation of a ``UX Question Bank'' to guide UX evaluation development. Findings also highlight how researchers' backgrounds influence their inspiration and concerns about AI over-reliance, pointing to future research on AI's role in fostering critical thinking. In a world where experience defines impact, we discuss the importance of shifting UX evaluation from a ``method-centric'' to a ``mindset-centric'' approach as the key to meaningful and lasting design evaluation.

</details>


### [16] [Aria-UI: Visual Grounding for GUI Instructions](https://arxiv.org/abs/2412.16256)

*Yuhao Yang, Yue Wang, Dongxu Li, Ziyang Luo, Bei Chen, Chao Huang, Junnan Li*

**Main category:** cs.HC

**Keywords:** human-computer interaction, GUI grounding, multimodal model

**Relevance Score:** 8

**TL;DR:** Aria-UI is a multimodal model designed for GUI grounding from language instructions, achieving state-of-the-art performance by using a pure-vision approach and a scalable data pipeline.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing demand for digital agents that can automate tasks by directly manipulating graphical user interfaces (GUIs), but grounding language instructions to target elements remains a challenge.

**Method:** Aria-UI utilizes a pure-vision approach without relying on HTML or AXTree inputs and implements a scalable data pipeline for synthesizing diverse instruction samples. It also incorporates interleaved textual and text-image action histories to enhance context-aware reasoning during task performance.

**Key Contributions:**

	1. Introduction of the Aria-UI model for GUI grounding using a pure-vision approach.
	2. Development of a scalable data pipeline for diverse instruction synthesis.
	3. Implementation of context-aware reasoning through interleaved action histories.

**Result:** Aria-UI achieves new state-of-the-art results in both offline and online benchmarks for agent performance, surpassing previous models that relied on vision-only methods and AXTree inputs.

**Limitations:** 

**Conclusion:** The performance improvements of Aria-UI demonstrate the efficacy of a vision-focused approach for GUI grounding, and the resources provided can aid further research in this area.

**Abstract:** Digital agents for automating tasks across different platforms by directly manipulating the GUIs are increasingly important. For these agents, grounding from language instructions to target elements remains a significant challenge due to reliance on HTML or AXTree inputs. In this paper, we introduce Aria-UI, a large multimodal model specifically designed for GUI grounding. Aria-UI adopts a pure-vision approach, eschewing reliance on auxiliary inputs. To adapt to heterogeneous planning instructions, we propose a scalable data pipeline that synthesizes diverse and high-quality instruction samples for grounding. To handle dynamic contexts in task performing, Aria-UI incorporates textual and text-image interleaved action histories, enabling robust context-aware reasoning for grounding. Aria-UI sets new state-of-the-art results across offline and online agent benchmarks, outperforming both vision-only and AXTree-reliant baselines. We release all training data and model checkpoints to foster further research at https://ariaui.github.io.

</details>


### [17] [Make yourself comfortable: Nudging urban heat and noise mitigation with smartwatch-based Just-in-time Adaptive Interventions (JITAI)](https://arxiv.org/abs/2501.09530)

*Clayton Miller, Yun Xuan Chua, Matias Quintana, Binyu Lei, Filip Biljecki, Mario Frei*

**Main category:** cs.HC

**Keywords:** Just-in-Time Adaptive Interventions, built environment, human comfort, Cozie platform, behavior change

**Relevance Score:** 8

**TL;DR:** The paper discusses implementing Just-in-Time Adaptive Interventions (JITAI) in built environments to enhance comfort through tailored information delivery. An eight-month study in Singapore with 103 participants yielded insights into behavior changes related to noise and thermal comfort.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human comfort in built environments by providing relevant information at appropriate times.

**Method:** Utilized the Cozie iOS smartwatch platform for data collection via micro-surveys and delivering JITAI messages based on environmental, contextual, and personal conditions during an eight-month deployment.

**Key Contributions:**

	1. Development of JITAI framework for built environments
	2. Empirical evidence from a large-scale deployment
	3. Insights on individual traits influencing intervention effectiveness

**Result:** Participants reported increased perceived usefulness and adjusted their behaviors in response to JITAI interventions, including location changes to mitigate noise and adjustments for thermal comfort.

**Limitations:** Further research needed to refine JITAI delivery based on user traits and conditions.

**Conclusion:** Tailoring JITAI strategies based on personality traits and environmental preferences is crucial for enhancing their effectiveness.

**Abstract:** Humans can play a more active role in improving their comfort in the built environment if given the right information at the right place and time. This paper outlines the use of Just-in-Time Adaptive Interventions (JITAI) implemented in the context of the built environment to provide information that helps humans minimize the impact of heat and noise on their daily lives. This framework is based on the open-source Cozie iOS smartwatch platform. It includes data collection through micro-surveys and intervention messages triggered by environmental, contextual, and personal history conditions. An eight-month deployment of the method was completed in Singapore with 103 participants who submitted more than 12,000 micro-surveys and had more than 3,600 JITAI intervention messages delivered to them. A weekly survey conducted during two deployment phases revealed an overall increase in perceived usefulness ranging from 8-19% over the first three weeks of data collection. For noise-related interventions, participants showed an overall increase in location changes ranging from 4-11% and a 2-17% increase in earphone use to mitigate noise distractions. For thermal comfort-related interventions, participants demonstrated a 3-13\% increase in adjustments to their location or thermostat to feel more comfortable. The analysis found evidence that personality traits (such as conscientiousness), gender, and environmental preferences could be factors in determining the perceived helpfulness of JITAIs and influencing behavior change. These findings underscore the importance of tailoring intervention strategies to individual traits and environmental conditions, setting the stage for future research to refine the delivery, timing, and content of intervention messages.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [18] [TokenShapley: Token Level Context Attribution with Shapley Value](https://arxiv.org/abs/2507.05261)

*Yingtai Xiao, Yuqing Zhu, Sirat Samyoun, Wanrong Zhang, Jiachen T. Wang, Jian Du*

**Main category:** cs.CL

**Keywords:** token-level attribution, Shapley value, KNN retrieval, LLMs, data attribution

**Relevance Score:** 7

**TL;DR:** TokenShapley is a new method for token-level attribution in LLMs that improves the verification of generated responses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind TokenShapley is to enhance the verification of specific tokens in LLM responses, addressing limitations of existing methods that only provide sentence-level attribution.

**Method:** TokenShapley integrates Shapley value-based data attribution with KNN-based retrieval techniques to assess token importance using a precomputed datastore.

**Key Contributions:**

	1. Introduction of TokenShapley for token-level attribution
	2. Combination of Shapley values and KNN-based retrieval
	3. Demonstrated substantial improvements in attribution accuracy

**Result:** Extensive evaluations demonstrate that TokenShapley outperforms state-of-the-art baselines, achieving an 11-23% improvement in accuracy for token-level attribution.

**Limitations:** 

**Conclusion:** TokenShapley allows for a fine-grained understanding of token contributions to LLM responses, improving overall response verification.

**Abstract:** Large language models (LLMs) demonstrate strong capabilities in in-context learning, but verifying the correctness of their generated responses remains a challenge. Prior work has explored attribution at the sentence level, but these methods fall short when users seek attribution for specific keywords within the response, such as numbers, years, or names. To address this limitation, we propose TokenShapley, a novel token-level attribution method that combines Shapley value-based data attribution with KNN-based retrieval techniques inspired by recent advances in KNN-augmented LLMs. By leveraging a precomputed datastore for contextual retrieval and computing Shapley values to quantify token importance, TokenShapley provides a fine-grained data attribution approach. Extensive evaluations on four benchmarks show that TokenShapley outperforms state-of-the-art baselines in token-level attribution, achieving an 11-23% improvement in accuracy.

</details>


### [19] [User Behavior Prediction as a Generic, Robust, Scalable, and Low-Cost Evaluation Strategy for Estimating Generalization in LLMs](https://arxiv.org/abs/2507.05266)

*Sougata Saha, Monojit Choudhury*

**Main category:** cs.CL

**Keywords:** Large Language Models, generalization ability, user behavior prediction, personalization, recommendation systems

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel framework for evaluating the generalization ability of Large Language Models (LLMs) through user behavior prediction instead of traditional knowledge-retrieval tasks, demonstrating its effectiveness using recommendation datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of measuring the generalization ability of LLMs amidst data contamination and the limitations of current evaluation methods.

**Method:** The authors propose user behavior prediction as a new method for assessing LLM performance and evaluate it using datasets related to movie and music recommendations.

**Key Contributions:**

	1. Proposed user behavior prediction as a novel approach for evaluating LLMs
	2. Introduced a framework for the new evaluation method
	3. Demonstrated effectiveness using movie and music recommendation datasets

**Result:** The results indicate that GPT-4o outperforms its smaller counterpart GPT-4o-mini and Llama-3.1-8B-Instruct across the tested datasets, while all models show significant potential for further improvements.

**Limitations:** The models, including GPT-4o, GPT-4o-mini, and Llama, still exhibit room for improvement in their performance.

**Conclusion:** User behavior prediction is presented as a more robust and scalable alternative for evaluating LLM generalization ability, proposing a shift in focus from knowledge-retrieval tasks.

**Abstract:** Measuring the generalization ability of Large Language Models (LLMs) is challenging due to data contamination. As models grow and computation becomes cheaper, ensuring tasks and test cases are unseen during training phases will become nearly impossible. We argue that knowledge-retrieval and reasoning tasks are not ideal for measuring generalization, as LLMs are not trained for specific tasks. Instead, we propose user behavior prediction, also a key aspect of personalization, as a theoretically sound, scalable, and robust alternative. We introduce a novel framework for this approach and test it on movie and music recommendation datasets for GPT-4o, GPT-4o-mini, and Llama-3.1-8B-Instruct. Results align with our framework's predictions, showing GPT-4o outperforms GPT-4o-mini and Llama, though all models have much room for improvement, especially Llama.

</details>


### [20] [An Adaptive Supervised Contrastive Learning Framework for Implicit Sexism Detection in Digital Social Networks](https://arxiv.org/abs/2507.05271)

*Mohammad Zia Ur Rehman, Aditya Shah, Nagendra Kumar*

**Main category:** cs.CL

**Keywords:** implicit sexism, contrastive learning, social media detection

**Relevance Score:** 4

**TL;DR:** This paper introduces ASCEND, an adaptive contrastive learning framework for detecting implicit sexism in social media content.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overlooked implicit sexism in social media that conventional detection methods fail to recognize.

**Method:** The ASCEND framework uses threshold-based contrastive learning to refine the embedding space of textual data, optimizing a contrastive loss alongside cross-entropy loss.

**Key Contributions:**

	1. Introduction of the Adaptive Supervised Contrastive learning framework for implicit sexism detection.
	2. Use of threshold-based contrastive learning for refining textual embeddings.
	3. Improvement of detection performance as evidenced by quantitative evaluations on benchmark datasets.

**Result:** ASCEND achieved significant performance improvements, outperforming existing methods with average Macro F1 improvements of 9.86%, 29.63%, and 32.51% on the EXIST2021 and MLSC datasets.

**Limitations:** 

**Conclusion:** The integration of a word-level attention module and additional textual features enhances the detection of implicit sexism effectively.

**Abstract:** The global reach of social media has amplified the spread of hateful content, including implicit sexism, which is often overlooked by conventional detection methods. In this work, we introduce an Adaptive Supervised Contrastive lEarning framework for implicit sexism detectioN (ASCEND). A key innovation of our method is the incorporation of threshold-based contrastive learning: by computing cosine similarities between embeddings, we selectively treat only those sample pairs as positive if their similarity exceeds a learnable threshold. This mechanism refines the embedding space by robustly pulling together representations of semantically similar texts while pushing apart dissimilar ones, thus reducing false positives and negatives. The final classification is achieved by jointly optimizing a contrastive loss with a cross-entropy loss. Textual features are enhanced through a word-level attention module. Additionally, we employ sentiment, emotion, and toxicity features. Evaluations on the EXIST2021 and MLSC datasets demonstrate that ASCEND significantly outperforms existing methods, with average Macro F1 improvements of 9.86%, 29.63%, and 32.51% across multiple tasks, highlighting its efficacy in capturing the subtle cues of implicit sexist language.

</details>


### [21] [Beyond classical and contemporary models: a transformative ai framework for student dropout prediction in distance learning using rag, prompt engineering, and cross-modal fusion](https://arxiv.org/abs/2507.05285)

*Miloud Mihoubi, Meriem Zerkouk, Belkacem Chikhaoui*

**Main category:** cs.CL

**Keywords:** dropout prediction, machine learning, sentiment analysis, educational interventions, cross-modal learning

**Relevance Score:** 9

**TL;DR:** A novel AI framework for dropout prediction in distance learning that integrates sentiment analysis, prompt engineering, and cross-modal attention to address student retention challenges effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Student dropout in distance learning poses significant societal and economic challenges. Existing models fail to incorporate emotional and contextual factors influencing dropout risk.

**Method:** The framework employs Retrieval-Augmented Generation (RAG) for sentiment analysis, utilizes prompt engineering to identify academic stressors, and implements cross-modal attention fusion to merge textual, behavioral, and demographic insights.

**Key Contributions:**

	1. Introduction of RAG for sentiment analysis in educational contexts.
	2. Development of a cross-modal attention mechanism for dropout prediction.
	3. Creation of interpretable interventions based on predicted risks.

**Result:** The framework reaches 89% accuracy and an F1-score of 0.88, outperforming traditional models by 7% and reducing false negatives by 21%.

**Limitations:** 

**Conclusion:** This work presents a scalable solution that connects predictive analytics with actionable pedagogy, helping mitigate dropout risks in education.

**Abstract:** Student dropout in distance learning remains a critical challenge, with profound societal and economic consequences. While classical machine learning models leverage structured socio-demographic and behavioral data, they often fail to capture the nuanced emotional and contextual factors embedded in unstructured student interactions. This paper introduces a transformative AI framework that redefines dropout prediction through three synergistic innovations: Retrieval-Augmented Generation (RAG) for domain-specific sentiment analysis, prompt engineering to decode academic stressors, and cross-modal attention fusion to dynamically align textual, behavioral, and socio-demographic insights. By grounding sentiment analysis in a curated knowledge base of pedagogical content, our RAG-enhanced BERT model interprets student comments with unprecedented contextual relevance, while optimized prompts isolate indicators of academic distress (e.g., "isolation," "workload anxiety"). A cross-modal attention layer then fuses these insights with temporal engagement patterns, creating holistic risk profiles. Evaluated on a longitudinal dataset of 4 423 students, the framework achieves 89% accuracy and an F1-score of 0.88, outperforming conventional models by 7% and reducing false negatives by 21%. Beyond prediction, the system generates interpretable interventions by retrieving contextually aligned strategies (e.g., mentorship programs for isolated learners). This work bridges the gap between predictive analytics and actionable pedagogy, offering a scalable solution to mitigate dropout risks in global education systems

</details>


### [22] [LCDS: A Logic-Controlled Discharge Summary Generation System Supporting Source Attribution and Expert Review](https://arxiv.org/abs/2507.05319)

*Cheng Yuan, Xinkai Rui, Yongqi Fan, Yawei Fan, Boyang Zhong, Jiacheng Wang, Weiyan Zhang, Tong Ruan*

**Main category:** cs.CL

**Keywords:** Large Language Models, discharge summary, source attribution, electronic medical records, health informatics

**Relevance Score:** 9

**TL;DR:** LCDS is a Logic-Controlled Discharge Summary generation system aimed at enhancing the reliability of automated discharge summaries generated by LLMs by addressing issues of hallucination and source attribution.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models face challenges in generating accurate discharge summaries from electronic medical records due to hallucination issues and difficulties in source attribution.

**Method:** LCDS develops a source mapping table based on textual similarity between EMRs and discharge summaries and uses logical rules to improve the reliability of the generated summaries.

**Key Contributions:**

	1. Logic-controlled generation to reduce hallucinations
	2. Source mapping for content accuracy
	3. Support for expert feedback in summary generation

**Result:** LCDS generates reliable silver discharge summaries that are tailored to specific clinical fields and supports source attribution for expert feedback, leading to accurate golden summaries for fine-tuning of LLMs.

**Limitations:** 

**Conclusion:** LCDS offers a significant advancement in the generation of discharge summaries by addressing critical challenges through logical controls and content sourcing.

**Abstract:** Despite the remarkable performance of Large Language Models (LLMs) in automated discharge summary generation, they still suffer from hallucination issues, such as generating inaccurate content or fabricating information without valid sources. In addition, electronic medical records (EMRs) typically consist of long-form data, making it challenging for LLMs to attribute the generated content to the sources. To address these challenges, we propose LCDS, a Logic-Controlled Discharge Summary generation system. LCDS constructs a source mapping table by calculating textual similarity between EMRs and discharge summaries to constrain the scope of summarized content. Moreover, LCDS incorporates a comprehensive set of logical rules, enabling it to generate more reliable silver discharge summaries tailored to different clinical fields. Furthermore, LCDS supports source attribution for generated content, allowing experts to efficiently review, provide feedback, and rectify errors. The resulting golden discharge summaries are subsequently recorded for incremental fine-tuning of LLMs. Our project and demo video are in the GitHub repository https://github.com/ycycyc02/LCDS.

</details>


### [23] [MindFlow: Revolutionizing E-commerce Customer Support with Multimodal LLM Agents](https://arxiv.org/abs/2507.05330)

*Ming Gong, Xucheng Huang, Chenghan Yang, Xianhan Peng, Haoxin Wang, Yang Liu, Ling Jiang*

**Main category:** cs.CL

**Keywords:** multimodal LLM, e-commerce, user satisfaction, memory integration, decision-making

**Relevance Score:** 7

**TL;DR:** MindFlow is an open-source multimodal LLM agent designed for e-commerce, enhancing user satisfaction and efficiency in handling complex queries.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address constraints of large language models in complex, multimodal e-commerce scenarios.

**Method:** Developed on the CoALA framework, MindFlow integrates memory, decision-making, and action modules while employing a modular 'MLLM-as-Tool' strategy for effective visual-textual reasoning.

**Key Contributions:**

	1. First open-source multimodal LLM agent for e-commerce
	2. Integration of memory and decision-making modules
	3. Demonstrated effectiveness through A/B testing and real-world deployment

**Result:** MindFlow shows substantial improvements in managing complex queries, enhancing user satisfaction, and decreasing operational costs, with a 93.53% improvement in real-world scenarios.

**Limitations:** 

**Conclusion:** MindFlow presents a novel approach to leveraging multimodal LLMs in e-commerce, leading to significant operational benefits.

**Abstract:** Recent advances in large language models (LLMs) have enabled new applications in e-commerce customer service. However, their capabilities remain constrained in complex, multimodal scenarios. We present MindFlow, the first open-source multimodal LLM agent tailored for e-commerce. Built on the CoALA framework, it integrates memory, decision-making, and action modules, and adopts a modular "MLLM-as-Tool" strategy for effect visual-textual reasoning. Evaluated via online A/B testing and simulation-based ablation, MindFlow demonstrates substantial gains in handling complex queries, improving user satisfaction, and reducing operational costs, with a 93.53% relative improvement observed in real-world deployments.

</details>


### [24] [LoRA-Augmented Generation (LAG) for Knowledge-Intensive Language Tasks](https://arxiv.org/abs/2507.05346)

*William Fleshman, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** LoRA, language models, task-specific, retrieval-augmented generation, knowledge-intensive tasks

**Relevance Score:** 8

**TL;DR:** The paper presents LoRA-Augmented Generation (LAG), a method for efficient selection and combination of fine-tuned language model experts without additional training, enhancing performance on knowledge-intensive tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing need for efficient methods to select and combine fine-tuned language models tailored for specific tasks due to their increasing proliferation.

**Method:** LAG utilizes task-specific LoRA adapters to filter, retrieve, and apply expert models on a per-token and layer basis without requiring additional training or access to data.

**Key Contributions:**

	1. Introduction of LoRA-Augmented Generation (LAG) method.
	2. Demonstration of LAG's efficiency in utilizing fine-tuned language model experts.
	3. Evaluation showing LAG's superiority in knowledge-intensive tasks over existing methods.

**Result:** LAG shows superior performance on various knowledge-intensive tasks compared to existing data-free methods, and is compatible with retrieval-augmented generation in scenarios with additional data.

**Limitations:** The method's performance may still depend on the quality of the fine-tuned models available.

**Conclusion:** The proposed method establishes a framework for effectively leveraging large libraries of language model experts, improving upon traditional approaches without the need for extensive data or retraining.

**Abstract:** The proliferation of fine-tuned language model experts for specific tasks and domains signals the need for efficient selection and combination methods. We propose LoRA-Augmented Generation (LAG) for leveraging large libraries of knowledge and task-specific LoRA adapters. LAG requires no additional training or access to data, and efficiently filters, retrieves, and applies experts on a per-token and layer basis. We evaluate LAG on various knowledge-intensive tasks, achieving superior performance over existing data-free methods. We explore scenarios where additional data is available, demonstrating LAG's compatibility with alternative solutions such as retrieval-augmented generation (RAG).

</details>


### [25] [On the Bias of Next-Token Predictors Toward Systematically Inefficient Reasoning: A Shortest-Path Case Study](https://arxiv.org/abs/2507.05362)

*Riccardo Alberghi, Elizaveta Demyanenko, Luca Biggio, Luca Saglietti*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Machine Learning, Natural Language Processing, Generalization

**Relevance Score:** 8

**TL;DR:** Study on improving reasoning in large language models (LLMs) through systematic and structured chains of thought while analyzing the impact of compute allocation during testing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the key factors influencing reasoning improvement in LLMs, specifically the effects of test-time compute allocation and structured reasoning.

**Method:** Implemented a controlled setting using shortest-path tasks in layered graphs, training decoder-only transformers on various traces from dynamic programming tasks, comparing those with backtracking and optimal traces.

**Key Contributions:**

	1. Introduction of a controlled experimental setting for studying reasoning in LLMs.
	2. Empirical evidence that longer incoherent reasoning does not improve generalization and can hinder performance.
	3. Identification of model confidence in next-token prediction as a critical factor for effective training signals.

**Result:** Models trained on inefficient reasoning traces (longer valid backtracking traces) demonstrated better generalization to unseen graphs compared to optimal traces with the same training budget.

**Limitations:** 

**Conclusion:** The length of reasoning traces alone does not ensure better performance; rather, coherent and incrementally structured reasoning is beneficial for optimizing training signals for LLMs.

**Abstract:** Recent advances in natural language processing highlight two key factors for improving reasoning in large language models (LLMs): (i) allocating more test-time compute tends to help on harder problems but often introduces redundancy in the reasoning trace, and (ii) compute is most effective when reasoning is systematic and incremental, forming structured chains of thought (CoTs) akin to human problem-solving. To study these factors in isolation, we introduce a controlled setting based on shortest-path tasks in layered graphs. We train decoder-only transformers on question-trace-answer triples using a custom tokenizer, comparing models trained on optimal bottom-up dynamic programming traces with those trained on longer, valid traces involving backtracking. Surprisingly, with the same training-token budget, models trained on inefficient traces generalize better to unseen graphs. This benefit is not due to length alone-injecting arbitrary redundancy into reasoning traces fails to help and can even hurt performance. Instead, we find that generalization correlates with the model's confidence in next-token prediction, suggesting that long, coherent, and locally incremental traces make the training signal easier to optimize.

</details>


### [26] [EduCoder: An Open-Source Annotation System for Education Transcript Data](https://arxiv.org/abs/2507.05385)

*Guanzhong Pan, Mei Tan, Hyunji Nam, Luc√≠a Langlois, James Malamut, Liliana Deonizio, Dorottya Demszky*

**Main category:** cs.CL

**Keywords:** EduCoder, annotation tool, educational dialogue, open-source, NLP

**Relevance Score:** 4

**TL;DR:** EduCoder is an open-source tool for utterance-level annotation of educational dialogue, aiding researchers in defining complex codebooks and improving data reliability through collaborative annotation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of coding educational dialogue transcripts effectively, particularly in defining codebooks and managing diverse interactions.

**Method:** Developed an open-source tool that supports both categorical and open-ended coding, allows contextualization of utterances, and features side-by-side comparison of annotations.

**Key Contributions:**

	1. Open-source platform for educational dialogue annotation
	2. Supports both categorical and open-ended coding
	3. Allows for comparison of annotators' responses to improve reliability

**Result:** EduCoder facilitates the collaborative definition of complex codebooks and enhances the reliability of educational dialogue annotations through comparative analysis.

**Limitations:** 

**Conclusion:** The implementation of EduCoder provides an innovative solution for researchers in education to reliably annotate and analyze dialogue transcripts, benefiting from collaborative efforts.

**Abstract:** We introduce EduCoder, a domain-specialized tool designed to support utterance-level annotation of educational dialogue. While general-purpose text annotation tools for NLP and qualitative research abound, few address the complexities of coding education dialogue transcripts -- with diverse teacher-student and peer interactions. Common challenges include defining codebooks for complex pedagogical features, supporting both open-ended and categorical coding, and contextualizing utterances with external features, such as the lesson's purpose and the pedagogical value of the instruction. EduCoder is designed to address these challenges by providing a platform for researchers and domain experts to collaboratively define complex codebooks based on observed data. It incorporates both categorical and open-ended annotation types along with contextual materials. Additionally, it offers a side-by-side comparison of multiple annotators' responses, allowing comparison and calibration of annotations with others to improve data reliability. The system is open-source, with a demo video available.

</details>


### [27] [The Generalization Ridge: Information Flow in Natural Language Generation](https://arxiv.org/abs/2507.05387)

*Ruidi Chang, Chunyuan Deng, Hanjie Chen*

**Main category:** cs.CL

**Keywords:** transformer models, generalization, information theory, machine learning, NLG

**Relevance Score:** 8

**TL;DR:** This paper introduces InfoRidge, an information-theoretic framework exploring how predictive information varies across transformer layers during training, revealing critical insights into generalization and memorization dynamics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how predictive information varies across layers in transformer-based language models and its implications for generalization.

**Method:** An information-theoretic framework is proposed to trace the predictive information between hidden representations and target outputs across model depth, using residual scaling coefficients to analyze individual layer importance.

**Key Contributions:**

	1. Introduction of the InfoRidge framework for analyzing predictive information in transformers.
	2. Identification of a generalization ridge in upper-middle layers during training.
	3. Demonstration of the importance of residual scaling coefficients in assessing transformer layers' significance.

**Result:** The experiments show a non-monotonic trend in predictive information, peaking in upper-middle layers and declining in final layers, indicating a transition from generalization to memorization.

**Limitations:** 

**Conclusion:** Intermediate transformer layers play a crucial role in generalization, particularly under distribution shifts, which are highlighted by downweighting of final layers.

**Abstract:** Transformer-based language models have achieved state-of-the-art performance in natural language generation (NLG) tasks, yet their internal mechanisms for synthesizing task-relevant information remain insufficiently understood. While prior studies suggest that intermediate layers often yield more generalizable representations than final layers, how this generalization ability emerges and propagates across layers during training remains unclear. To address this gap, we propose InfoRidge, an information-theoretic framework, to characterize how predictive information-the mutual information between hidden representations and target outputs-varies across depth. Estimating this quantity enables us to trace the flow of task-relevant information throughout the model during training. Our experiments across various models and datasets reveal a consistent non-monotonic trend: predictive information peaks in upper-middle layers-forming a generalization ridge-before declining in final layers, reflecting a transition between generalization and memorization. To further investigate this phenomenon, we introduce residual scaling coefficients-trainable scalar parameters applied to each residual block-which serve as functional probes for assessing the relative importance of individual transformer layers. These coefficients reveal that, under distribution shift, models downweight final layers and increasingly rely on ridge layers, highlighting their role in generalization. Together, these findings offer new insights into the internal mechanisms of transformers and underscore the critical role of intermediate layers in supporting generalization.

</details>


### [28] [Controlling What You Share: Assessing Language Model Adherence to Privacy Preferences](https://arxiv.org/abs/2507.05391)

*Guillem Ram√≠rez, Alexandra Birch, Ivan Titov*

**Main category:** cs.CL

**Keywords:** Privacy Profiles, Large Language Models, User Data Control, Natural Language Instructions, Privacy Compliance

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for using privacy profiles to protect user data when accessing large language models (LLMs) via APIs by rewriting queries to balance privacy and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To allow users to retain control over their data while interacting with commercial LLMs, minimizing the exposure of sensitive information.

**Method:** The authors developed a framework where a local model rewrites user queries based on privacy profiles that specify what information should be kept private before sending the modified queries to an external LLM.

**Key Contributions:**

	1. Introduction of privacy profiles for LLM queries
	2. Creation of the PEEP multilingual dataset with annotated user queries
	3. Empirical evaluation of lightweight LLMs' adherence to privacy instructions.

**Result:** Experiments demonstrate that lightweight LLMs can partially comply with privacy instructions, but significant challenges remain in fully understanding and adhering to user privacy preferences.

**Limitations:** The compliance of models with privacy instructions is limited and presents challenges that need addressing in future developments.

**Conclusion:** The study emphasizes the importance of improving LLMs to better respect user-defined privacy parameters, suggesting directions for future work in enhancing model compliance with privacy guidelines.

**Abstract:** Large language models (LLMs) are primarily accessed via commercial APIs, but this often requires users to expose their data to service providers. In this paper, we explore how users can stay in control of their data by using privacy profiles: simple natural language instructions that say what should and should not be revealed. We build a framework where a local model uses these instructions to rewrite queries, only hiding details deemed sensitive by the user, before sending them to an external model, thus balancing privacy with performance. To support this research, we introduce PEEP, a multilingual dataset of real user queries annotated to mark private content and paired with synthetic privacy profiles. Our experiments with lightweight LLMs show they can follow these instructions to some extent, but also face consistent challenges, highlighting the need for models that better understand and comply with user-defined privacy preferences.

</details>


### [29] [Learn Globally, Speak Locally: Bridging the Gaps in Multilingual Reasoning](https://arxiv.org/abs/2507.05418)

*Jaedong Hwang, Kumar Tanmay, Seok-Jin Lee, Ayush Agrawal, Hamid Palangi, Kumar Ayush, Ila Fiete, Paul Pu Liang*

**Main category:** cs.CL

**Keywords:** Multilingual reasoning, Large Language Models, BRIDGE, GeoFact-X, Cross-lingual generalization

**Relevance Score:** 7

**TL;DR:** The paper introduces GeoFact-X, a multilingual factual reasoning benchmark, and proposes BRIDGE, a training method for improving multilingual reasoning in LLMs, particularly for low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve multilingual reasoning capabilities of LLMs, especially for low-resource languages, and to address biases in reasoning related to language.

**Method:** Introducing GeoFact-X as a benchmark with annotated reasoning traces and proposing the BRIDGE method, which includes supervised fine-tuning and reinforcement learning for language-consistency.

**Key Contributions:**

	1. Development of GeoFact-X multilingual reasoning benchmark
	2. Introduction of BRIDGE training method for LLMs
	3. Implementation of an automatic evaluation protocol for nuanced analysis of reasoning

**Result:** BRIDGE significantly enhances multilingual reasoning fidelity and demonstrates effective cross-lingual generalization.

**Limitations:** 

**Conclusion:** Reasoning-aware multilingual reinforcement learning is essential for LLMs to achieve robust performance in multilingual contexts.

**Abstract:** Large Language Models (LLMs) have achieved strong performance in domains like mathematics, factual QA, and code generation, yet their multilingual reasoning capabilities in these tasks remain underdeveloped. Especially for low-resource languages such as Swahili or Thai, LLMs can often misinterpret prompts or default to reasoning in English. This implicit bias toward high-resource languages undermines factual accuracy, interpretability, and trust. Current multilingual benchmarks focus only on final answers, overlooking whether models actually reason in the target language. To address this gap, we introduce GeoFact-X, a geography-based multilingual factual reasoning benchmark with annotated reasoning traces in five languages: English, Hindi, Japanese, Swahili, and Thai. We further propose BRIDGE, a novel training method that guides supervised fine-tuning and test-time reinforcement learning with a language-consistency reward to align reasoning with the input language. Finally, we develop an automatic evaluation protocol using LLM-as-a-judge to assess answer correctness and the quality and language consistency of reasoning traces, enabling nuanced and scalable analysis beyond surface-level metrics. Our results show that BRIDGE significantly enhances multilingual reasoning fidelity, demonstrating that reasoning-aware multilingual reinforcement learning is crucial for robust cross-lingual generalization. https://jd730.github.io/projects/GeoFact-X_BRIDGE

</details>


### [30] ["Lost-in-the-Later": Framework for Quantifying Contextual Grounding in Large Language Models](https://arxiv.org/abs/2507.05424)

*Yufei Tao, Adam Hiatt, Rahul Seetharaman, Ameeta Agrawal*

**Main category:** cs.CL

**Keywords:** Large Language Models, Contextual Knowledge, Parametric Knowledge, Chain-of-Thought, AI Evaluation

**Relevance Score:** 9

**TL;DR:** The paper introduces CoPE, an evaluation framework assessing how large language models (LLMs) integrate contextual and parametric knowledge, revealing a positional bias in information prioritization during open-ended question answering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how large language models prioritize and integrate different types of knowledge is crucial for improving their performance in tasks like question answering and summarization.

**Method:** The CoPE framework evaluates contextual knowledge (CK) and parametric knowledge (PK) using a dataset (MultiWikiAtomic) across multiple languages, analyzing LLM responses to open-ended questions.

**Key Contributions:**

	1. Introduces CoPE framework for evaluating knowledge integration in LLMs
	2. Identifies 'lost-in-the-later' bias affecting LLM performance
	3. Demonstrates improved grounding through CK-informed prompting

**Result:** The analysis identifies a phenomenon termed 'lost-in-the-later,' where LLMs deprioritize later information in the context, leading to positional bias that negatively impacts their ability to ground answers contextually. Moreover, prompting with chain-of-thought (CoT) worsens the recall and quality of contextual responses.

**Limitations:** The findings might be limited to the languages and tasks assessed; broader implications need further exploration.

**Conclusion:** CoPE shows that effective leveraging of contextual input can significantly improve LLM responses, as demonstrated through a case study in summarization that reduces hallucination.

**Abstract:** Large language models are capable of leveraging both contextual and parametric knowledge but how they prioritize and integrate these sources remains underexplored. We introduce CoPE, a novel evaluation framework that systematically measures contextual knowledge (CK) and parametric knowledge (PK) across models and languages. Using our MultiWikiAtomic dataset in English, Spanish, and Danish, we analyze how large language models (LLMs) integrate context, prioritize information, and incorporate PK in open-ended question answering. Our analysis uncovers a phenomenon we call lost-in-the-later, where LLMs tend to overlook or deprioritize information that appears later in a given context, revealing a strong positional bias that affects contextual grounding. We further find that reasoning models, as well as non-reasoning models prompted with chain-of-thought (CoT), use context even less than non-reasoning models without CoT and fail to mitigate the lost-in-the-later effect. CoT prompting, in particular, results in lower recall and shorter responses, leading to degraded contextual grounding. Based on these insights, we design prompt-based methods to effectively leverage input context. A case study applying CoPE to summarization demonstrates that CK-informed prompting improves factual grounding and reduces hallucination.

</details>


### [31] [Gendered Divides in Online Discussions about Reproductive Rights](https://arxiv.org/abs/2507.05443)

*Ashwin Rao, Sze Yuh Nina Wang, Kristina Lerman*

**Main category:** cs.CL

**Keywords:** abortion, gender, political discourse, sociopolitical context, public engagement

**Relevance Score:** 2

**TL;DR:** The study analyzes nearly 10 million abortion-related posts on X to explore how gender and local contexts influence public discourse on abortion following the 2022 Dobbs ruling.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the interaction between gender, local sociopolitical contexts, and their impact on public discourse around abortion following the Supreme Court's Dobbs decision.

**Method:** The authors analyzed 10 million abortion-related posts on X, categorizing users by inferred gender, ideology, and location to assess how these factors influenced attitudes and emotional expressions regarding abortion.

**Key Contributions:**

	1. Identifies the role of gender in moderating abortion attitudes and emotional responses.
	2. Highlights the influence of local sociopolitical contexts on public discourse around abortion.
	3. Demonstrates increased engagement among pro-abortion women in response to the Dobbs ruling leak.

**Result:** The findings indicate that gender significantly moderates abortion attitudes and emotional expressions, particularly in conservative regions, creating a notable gender gap that intensifies in such contexts. The leak of the Dobbs draft motivated increased engagement among pro-abortion women in areas facing restricted access.

**Limitations:** Focuses primarily on social media posts which may not fully represent broader public opinions or offline discussions.

**Conclusion:** Abortion discourse is shaped by both ideological polarization and gender, underscoring the importance of identity in political expression during institutional changes.

**Abstract:** The U.S. Supreme Court's 2022 ruling in Dobbs v. Jackson Women's Health Organization marked a turning point in the national debate over reproductive rights. While the ideological divide over abortion is well documented, less is known about how gender and local sociopolitical contexts interact to shape public discourse. Drawing on nearly 10 million abortion-related posts on X (formerly Twitter) from users with inferred gender, ideology and location, we show that gender significantly moderates abortion attitudes and emotional expression, particularly in conservative regions, and independently of ideology. This creates a gender gap in abortion attitudes that grows more pronounced in conservative regions. The leak of the Dobbs draft opinion further intensified online engagement, disproportionately mobilizing pro-abortion women in areas where access was under threat. These findings reveal that abortion discourse is not only ideologically polarized but also deeply structured by gender and place, highlighting the central role of identity in shaping political expression during moments of institutional disruption.

</details>


### [32] [PhoniTale: Phonologically Grounded Mnemonic Generation for Typologically Distant Language Pairs](https://arxiv.org/abs/2507.05444)

*Sana Kang, Myeongseok Gwon, Su Young Kwon, Jaewook Lee, Andrew Lan, Bhiksha Raj, Rita Singh*

**Main category:** cs.CL

**Keywords:** cross-lingual, mnemonics, large language models, vocabulary acquisition, phonological similarity

**Relevance Score:** 8

**TL;DR:** PhoniTale is a novel system for generating cross-lingual mnemonics using LLMs to assist L2 learners in vocabulary acquisition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Vocabulary acquisition is a challenge for L2 learners, particularly with typologically distant languages, and existing research has mainly focused on native English speakers.

**Method:** PhoniTale retrieves L1 keyword sequences based on phonological similarity and utilizes LLMs to create mnemonics for L2 vocabulary learning.

**Key Contributions:**

	1. Introduction of a cross-lingual mnemonic generation system.
	2. Utilization of phonological similarity for keyword retrieval.
	3. Comparison of automated mnemonics with human-generated ones.

**Result:** PhoniTale's mnemonics were found to perform comparably to human-created mnemonics in evaluations.

**Limitations:** The study primarily focuses on short-term recall and may require further exploration of long-term effectiveness.

**Conclusion:** The study emphasizes the effectiveness of PhoniTale while identifying areas for future research to enhance mnemonic quality.

**Abstract:** Vocabulary acquisition poses a significant challenge for second-language (L2) learners, especially when learning typologically distant languages such as English and Korean, where phonological and structural mismatches complicate vocabulary learning. Recently, large language models (LLMs) have been used to generate keyword mnemonics by leveraging similar keywords from a learner's first language (L1) to aid in acquiring L2 vocabulary. However, most of this research has focused on native English speakers learning other languages, rather than the reverse. In this paper, we present PhoniTale, a novel cross-lingual mnemonic generation system that retrieves L1 keyword sequence based on phonological similarity and uses LLMs to generate mnemonics. We evaluate PhoniTale using both automated metrics and human evaluations, comparing its output to mnemonics created by humans and by previous automated approaches. To assess practical effectiveness, we also conduct a short-term recall test measuring mnemonic helpfulness. Our findings show that PhoniTale performs comparably to human-authored mnemonics. We also highlight key areas for future improvement in mnemonic quality and methodology.

</details>


### [33] [On the Semantics of Large Language Models](https://arxiv.org/abs/2507.05448)

*Martin Schuele*

**Main category:** cs.CL

**Keywords:** Large Language Models, semantics, Frege, Russell, language understanding

**Relevance Score:** 8

**TL;DR:** The paper investigates the semantic understanding of Large Language Models at the word and sentence level using classical semantic theories.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To clarify the extent to which LLMs, like ChatGPT, understand language, particularly focusing on semantics.

**Method:** The inner workings of LLMs are examined alongside their language representation, referencing classical theories by Frege and Russell to analyze semantic capabilities.

**Key Contributions:**

	1. Analysis of LLM semantics using classical theories
	2. Insights into the limitations of LLMs in understanding language
	3. Nuanced evaluation of LLMs' capabilities at word and sentence levels

**Result:** A nuanced assessment of LLMs' semantic abilities at different levels is provided, highlighting the complexities of their language understanding.

**Limitations:** The paper may not cover the full range of LLM capabilities beyond semantics.

**Conclusion:** The study offers insights into the semantic potential of LLMs, acknowledging both capabilities and limitations in replicating human language understanding.

**Abstract:** Large Language Models (LLMs) such as ChatGPT demonstrated the potential to replicate human language abilities through technology, ranging from text generation to engaging in conversations. However, it remains controversial to what extent these systems truly understand language. We examine this issue by narrowing the question down to the semantics of LLMs at the word and sentence level. By examining the inner workings of LLMs and their generated representation of language and by drawing on classical semantic theories by Frege and Russell, we get a more nuanced picture of the potential semantic capabilities of LLMs.

</details>


### [34] [ModelCitizens:Representing Community Voices in Online Safety](https://arxiv.org/abs/2507.05455)

*Ashima Suvarna, Christina Chance, Hamid Palangi, Sophie Hao, Thomas Hartvigsen, Saadia Gabriel*

**Main category:** cs.CL

**Keywords:** toxic language detection, MODELCITIZENS, community-informed annotation, LLM-generated scenarios, content moderation

**Relevance Score:** 8

**TL;DR:** This paper presents MODELCITIZENS, a dataset and models for improved toxic language detection, emphasizing the importance of community-influenced annotations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance online safety and inclusivity by addressing the limitations of current toxicity detection models in reflecting diverse perspectives.

**Method:** The authors created MODELCITIZENS, consisting of 6.8K social media posts with 40K toxicity annotations, and incorporated LLM-generated scenarios to better capture conversational context.

**Key Contributions:**

	1. Introduction of MODELCITIZENS dataset for toxic language detection
	2. Incorporation of LLM-generated scenarios for context
	3. Development of new models that outperform existing toxicity detection tools

**Result:** State-of-the-art toxicity detection tools showed underperformance on the MODELCITIZENS dataset, particularly with context-augmented posts. Newly released models LLAMACITIZEN-8B and GEMMACITIZEN-12B outperformed existing models by 5.5% in evaluations.

**Limitations:** Existing models were not designed to handle diverse community norms and context variability, which may limit generalization.

**Conclusion:** Community-informed annotations and modeling are crucial for effective and inclusive content moderation in social media settings.

**Abstract:** Automatic toxic language detection is critical for creating safe, inclusive online spaces. However, it is a highly subjective task, with perceptions of toxic language shaped by community norms and lived experience. Existing toxicity detection models are typically trained on annotations that collapse diverse annotator perspectives into a single ground truth, erasing important context-specific notions of toxicity such as reclaimed language. To address this, we introduce MODELCITIZENS, a dataset of 6.8K social media posts and 40K toxicity annotations across diverse identity groups. To capture the role of conversational context on toxicity, typical of social media posts, we augment MODELCITIZENS posts with LLM-generated conversational scenarios. State-of-the-art toxicity detection tools (e.g. OpenAI Moderation API, GPT-o4-mini) underperform on MODELCITIZENS, with further degradation on context-augmented posts. Finally, we release LLAMACITIZEN-8B and GEMMACITIZEN-12B, LLaMA- and Gemma-based models finetuned on MODELCITIZENS, which outperform GPT-o4-mini by 5.5% on in-distribution evaluations. Our findings highlight the importance of community-informed annotation and modeling for inclusive content moderation.

</details>


### [35] [Empowering Healthcare Practitioners with Language Models: Structuring Speech Transcripts in Two Real-World Clinical Applications](https://arxiv.org/abs/2507.05517)

*Jean-Philippe Corbeil, Asma Ben Abacha, George Michalopoulos, Phillip Swazinna, Miguel Del-Agua, Jerome Tremblay, Akila Jeeson Daniel, Cari Bader, Kevin Cho, Pooja Krishnan, Nathan Bodenstab, Thomas Lin, Wenxuan Teng, Francois Beaulieu, Paul Vozila*

**Main category:** cs.CL

**Keywords:** Large language models, Natural language processing, Health informatics, Open-source datasets, Clinical documentation

**Relevance Score:** 9

**TL;DR:** This paper explores the application of large language models (LLMs) for structured tabular reporting and medical order extraction in clinical settings, addressing data sensitivity and scarcity issues.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to address the underexplored NLP tasks of structured reporting from nurse dictations and medical order extraction due to data scarcity and sensitivity, aiming to alleviate documentation burdens on healthcare providers.

**Method:** The authors investigate the performance of various open- and closed-weight LLMs using private and open-source clinical datasets, proposing an agentic pipeline for generating nurse dictations.

**Key Contributions:**

	1. Evaluation of LLMs on clinically relevant NLP tasks
	2. Introduction of novel open-source datasets for nurse dictation and order extraction
	3. Development of an agentic pipeline for structured healthcare documentation

**Result:** The study reveals the strengths and limitations of different LLMs in handling structured tabular reporting and medical order extraction, enhancing understanding of their practical applications in clinical environments.

**Limitations:** The paper does not address all potential challenges related to the deployment of LLMs in clinical settings.

**Conclusion:** The introduction of open-source datasets SYNUR and SIMORD supports further research in nurse observation extraction and medical order extraction, contributing to the development of practical solutions in healthcare NLP.

**Abstract:** Large language models (LLMs) such as GPT-4o and o1 have demonstrated strong performance on clinical natural language processing (NLP) tasks across multiple medical benchmarks. Nonetheless, two high-impact NLP tasks - structured tabular reporting from nurse dictations and medical order extraction from doctor-patient consultations - remain underexplored due to data scarcity and sensitivity, despite active industry efforts. Practical solutions to these real-world clinical tasks can significantly reduce the documentation burden on healthcare providers, allowing greater focus on patient care. In this paper, we investigate these two challenging tasks using private and open-source clinical datasets, evaluating the performance of both open- and closed-weight LLMs, and analyzing their respective strengths and limitations. Furthermore, we propose an agentic pipeline for generating realistic, non-sensitive nurse dictations, enabling structured extraction of clinical observations. To support further research in both areas, we release SYNUR and SIMORD, the first open-source datasets for nurse observation extraction and medical order extraction.

</details>


### [36] [Enhancing Test-Time Scaling of Large Language Models with Hierarchical Retrieval-Augmented MCTS](https://arxiv.org/abs/2507.05557)

*Alex ZH Dou, Zhongwei Wan, Dongfei Cui, Xin Wang, Jing Xiong, Haokun Lin, Chaofan Tao, Shen Yan, Mi Zhang*

**Main category:** cs.CL

**Keywords:** language modeling, test-time scaling, hierarchical retrieval, reasoning, large language models

**Relevance Score:** 8

**TL;DR:** Introduction of R2-LLMs, a hierarchical retrieval-augmented framework to improve test-time scaling in large language models without requiring distillation for CoT training data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance model performance at inference time using additional computational resources through an improved framework for large language models.

**Method:** R2-LLMs utilizes dual-level retrieval-based in-context learning, with coarse-level extraction of templates and fine-level reasoning supported by Monte Carlo Tree Search.

**Key Contributions:**

	1. Hierarchical retrieval-augmented reasoning framework
	2. Dual-level retrieval-based in-context learning
	3. Integration of Monte Carlo Tree Search with a process reward model

**Result:** Empirical evaluations show a relative improvement of up to 16% on complex reasoning datasets using LLaMA-3.1-8B compared to existing baselines.

**Limitations:** 

**Conclusion:** R2-LLMs enhances inference-time generalization and reasoning accuracy, showcasing effective integration with step-level tree search methods.

**Abstract:** Test-time scaling has emerged as a promising paradigm in language modeling, leveraging additional computational resources at inference time to enhance model performance. In this work, we introduce R2-LLMs, a novel and versatile hierarchical retrieval-augmented reasoning framework designed to improve test-time scaling in large language models (LLMs) without requiring distillation from more advanced models to obtain chain-of-thought (CoT) training data. R2-LLMs enhances inference-time generalization by integrating dual-level retrieval-based in-context learning: (1) At the coarse level, our approach extracts abstract templates from complex reasoning problems and retrieves similar problem-answer pairs to facilitate high-level in-context learning; (2) At the fine level, during Monte Carlo Tree Search (MCTS), R2-LLMs efficiently retrieves analogous intermediate solution steps from reference mathematical problem datasets, refining step-wise reasoning with the aid of a process reward model (PRM) for scoring. R2-LLMs is a robust hierarchical reasoning-augmentation method that enhances in-context-level reasoning while seamlessly integrating with step-level tree search methods. Utilizing PRM, it refines both candidate generation and decision-making for improved reasoning accuracy. Empirical evaluations on the MATH500, GSM8K, and OlympiadBench-TO datasets achieve substantial relative improvement with an increase of up to 16% using LLaMA-3.1-8B compared to the baselines, showcasing the effectiveness of our approach in complex reasoning tasks.

</details>


### [37] [Self-Review Framework for Enhancing Instruction Following Capability of LLM](https://arxiv.org/abs/2507.05598)

*Sihyun Park*

**Main category:** cs.CL

**Keywords:** large language models, self-evaluation, revision framework, instruction adherence, machine learning

**Relevance Score:** 9

**TL;DR:** Re5 is a self-evaluation and revision framework aimed at improving LLMs' instruction adherence while preserving output quality, achieving results comparable to high-performance models with minimal data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance large language models' adherence to formatting and instruction constraints without escalating costs associated with iterative revisions.

**Method:** Re5 extracts task components from user instructions, conducts structural evaluations to prevent errors, and performs selective revisions based on fine-grained constraint evaluations.

**Key Contributions:**

	1. Introduction of Re5 framework for self-evaluation and revision in LLMs
	2. Preservation of output quality during iterative revisions
	3. Demonstration of competitive performance using limited data

**Result:** Re5 achieves instruction-following performance comparable to models trained on high-quality data, with a win rate of 64.24% over non-revised responses.

**Limitations:** Potential degradation in output quality with excessive revision cycles remains a concern.

**Conclusion:** Re5 is validated as an efficient solution for enhancing instruction adherence in LLMs with minimal external supervision.

**Abstract:** Various techniques have been proposed to improve large language models (LLMs) adherence to formatting and instruction constraints. One of the most effective approaches involves utilizing high-quality data generated by powerful models. However, such models often fail to fully comply with complex instructions in a single generation. To address this limitation, iterative revision methods have been introduced. Nevertheless, as the number of data points and revision iterations increases, the associated monetary costs grow significantly. As a resource-efficient alternative, methods have been proposed that leverage high-performance evaluation tools to compensate for the limited self-evaluation capabilities of open-source LLMs. However, these approaches often lead to a degradation in output quality due to excessive revision. To overcome these challenges, we propose Re5, a self-evaluation and revision framework designed to enhance instruction-following performance while preserving the quality of the generated content. Re5 extracts task and constraint components from user instructions, performs structural evaluations to prevent error accumulation, and applies fine-grained constraint-specific content evaluations followed by selective revisions. This process ensures precise and quality-preserving improvements. The final high-quality outputs are used for alignment tuning, enabling long-term alignment improvements through a data-centric iterative refinement loop. Experimental results demonstrate that Re5 achieves instruction-following performance comparable to models trained on data generated by GPT-4o-mini, a high-performance model, even with a small amount of data while maintaining response quality with a 64.24%-win rate over the non-revised initial responses. These results validate Re5 as an efficient and effective solution for enhancing instruction adherence with minimal external supervision.

</details>


### [38] [Flipping Knowledge Distillation: Leveraging Small Models' Expertise to Enhance LLMs in Text Matching](https://arxiv.org/abs/2507.05617)

*Mingzhe Li, Jing Xiang, Qishen Zhang, Kaiyang Wan, Xiuying Chen*

**Main category:** cs.CL

**Keywords:** knowledge distillation, Large Language Models, smaller models, Contrastive Learning, healthcare

**Relevance Score:** 9

**TL;DR:** Introduces a flipped knowledge distillation method where LLM learns from SLM, enhancing performance in text matching tasks with a Margin-aware Contrastive Learning approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Leverage the strengths of small models and LLMs to improve text matching tasks by allowing LLMs to learn from fine-tuned smaller models.

**Method:** Develop a flipped knowledge distillation paradigm using LoRA to reinterpret LLMs as encoder-decoder models, where the encoder generates representations that align with teacher similarities through the Margin-aware Contrastive Learning approach.

**Key Contributions:**

	1. Flipped knowledge distillation paradigm where LLM learns from SLM
	2. Introduction of Margin-aware Contrastive Learning for better similarity alignment
	3. Deployment of model in real-world applications, showcasing effectiveness

**Result:** The proposed method demonstrates improved performance on financial and healthcare benchmarks and has been deployed online, effectively combining the strengths of SLMs and LLMs.

**Limitations:** 

**Conclusion:** The flipped knowledge distillation paradigm enables LLMs to achieve better results with the assistance of reasonably good-performing SLMs in various applications.

**Abstract:** Knowledge distillation typically involves transferring knowledge from a Large Language Model (LLM) to a Smaller Language Model (SLM). However, in tasks such as text matching, fine-tuned smaller models often yield more effective domain-specific representations, as they focus on optimizing the similarity of input pairs. To leverage both the specialized strengths of small models and the rich semantic understanding of LLMs, we introduce a flipped knowledge distillation paradigm, where LLM learns from SLM. Specifically, we address the architectural gap between decoder-only LLMs and smaller encoder-based models by reinterpreting LLMs in an encoder-decoder manner using LoRA. The encoder generates compressed representations, while the decoder maps them to the output space. During training, the encoder produces representations and their similarities, which are then aligned with the similarity scores produced by the teacher, using our proposed Margin-aware Contrastive Learning (MCL) approach. The MCL ensures accurate similarity for both positive and negative pairs, and adaptively handles the internal differences within positive and negative samples. Our paradigm requires only a reasonably good-performing SLM, allowing the LLM to achieve improved performance. Experiments on financial and healthcare benchmarks, as well as real-world applications, confirm its effectiveness, and the model has been fully deployed in an online environment.

</details>


### [39] [SARA: Selective and Adaptive Retrieval-augmented Generation with Context Compression](https://arxiv.org/abs/2507.05633)

*Yiqiao Jin, Kartik Sharma, Vineeth Rakesh, Yingtong Dou, Menghai Pan, Mahashweta Das, Srijan Kumar*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented Generation, Large Language Models, Semantic Compression

**Relevance Score:** 9

**TL;DR:** SARA is a unified Retrieval-augmented Generation framework that enhances context efficiency and answer correctness by integrating natural-language snippets and semantic compression vectors.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in Retrieval-augmented Generation, such as limited context length and redundancy in retrieved documents while maintaining factual accuracy.

**Method:** SARA combines fine-grained natural-language spans with compact semantic compression vectors and uses an iterative evidence-selection module for dynamic context reranking.

**Key Contributions:**

	1. Introduction of a unified RAG framework (SARA) that balances local precision and global knowledge coverage.
	2. Combination of natural-language snippets and semantic compression vectors.
	3. Demonstration of significant performance improvements across diverse datasets and LLMs.

**Result:** SARA achieves significant improvements in answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53) across multiple datasets and LLM families.

**Limitations:** 

**Conclusion:** Integrating textual representations and compressed data enhances the efficiency and reliability of RAG frameworks in retrieving context.

**Abstract:** Retrieval-augmented Generation (RAG) extends large language models (LLMs) with external knowledge but faces key challenges: restricted effective context length and redundancy in retrieved documents. Pure compression-based approaches reduce input size but often discard fine-grained details essential for factual accuracy. We propose SARA, a unified RAG framework that balances local precision and global knowledge coverage under tight context budgets. SARA combines natural-language text snippets with semantic compression vectors to jointly enhance context efficiency and answer correctness. It represents contexts at two complementary levels: 1) fine-grained natural-language spans that preserve critical entities and numerical values, and 2) compact, interpretable vectors that summarize high-level semantics. An iterative evidence-selection module employs the compression vectors for dynamic reranking of contexts. Across 9 datasets and 5 open-source LLMs spanning 3 model families (Mistral, Llama, and Gemma), SARA consistently improves answer relevance (+17.71), answer correctness (+13.72), and semantic similarity (+15.53), demonstrating the importance of integrating textual and compressed representations for robust, context-efficient RAG.

</details>


### [40] [ECom-Bench: Can LLM Agent Resolve Real-World E-commerce Customer Support Issues?](https://arxiv.org/abs/2507.05639)

*Haoxin Wang, Xianhan Peng, Xucheng Huang, Yizhe Huang, Ming Gong, Chenghan Yang, Yang Liu, Ling Jiang*

**Main category:** cs.CL

**Keywords:** E-commerce, LLM, Benchmark, Customer support, Multimodal

**Relevance Score:** 8

**TL;DR:** ECom-Bench is the first benchmark framework for evaluating LLM agents in e-commerce customer support, featuring dynamic user simulation and realistic task datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To create a robust framework for evaluating LLM agents in the context of e-commerce customer support due to the complexities involved.

**Method:** Dynamic user simulation based on real e-commerce customer interactions and authentic dialogue datasets.

**Key Contributions:**

	1. Introduction of a benchmark for LLM evaluation in e-commerce
	2. Use of real customer interaction data for task simulation
	3. Open-sourcing code and datasets for community use

**Result:** Advanced models like GPT-4o score only 10-20% on the benchmark, indicating significant challenges in the e-commerce scenarios.

**Limitations:** The benchmark may not cover every possible e-commerce scenario, and results might vary with different models beyond those tested.

**Conclusion:** ECom-Bench highlights the need for improved LLM capabilities in complex e-commerce contexts and will be open-sourced for further research.

**Abstract:** In this paper, we introduce ECom-Bench, the first benchmark framework for evaluating LLM agent with multimodal capabilities in the e-commerce customer support domain. ECom-Bench features dynamic user simulation based on persona information collected from real e-commerce customer interactions and a realistic task dataset derived from authentic e-commerce dialogues. These tasks, covering a wide range of business scenarios, are designed to reflect real-world complexities, making ECom-Bench highly challenging. For instance, even advanced models like GPT-4o achieve only a 10-20% pass^3 metric in our benchmark, highlighting the substantial difficulties posed by complex e-commerce scenarios. Upon publication, the code and data will be open-sourced to facilitate further research and development in this domain.

</details>


### [41] [Smoothie-Qwen: Post-Hoc Smoothing to Reduce Language Bias in Multilingual LLMs](https://arxiv.org/abs/2507.05686)

*SeungWon Ji, Jungyup Lee, Jemin Kim, Sang Park, SeungJae Lee*

**Main category:** cs.CL

**Keywords:** multilingual, large language models, language controllability, token-level output, post-hoc method

**Relevance Score:** 9

**TL;DR:** Smoothie-Qwen is a method designed to reduce language confusion in multilingual LLMs by adjusting token-level output probabilities without retraining.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the issue of language confusion in multilingual large language models, which often generate responses in a dominant language.

**Method:** Smoothie-Qwen is a lightweight, post-hoc method that selectively adjusts token-level output probabilities to suppress undesired language generation.

**Key Contributions:**

	1. Introduction of a lightweight, post-hoc method to mitigate language bias in multilingual LLMs.
	2. Demonstrated effectiveness by reducing unintended language output by over 95%.
	3. Preservation of task accuracy in multilingual settings.

**Result:** The technique applied to the Qwen model reduces unintended Chinese output by over 95% while maintaining task accuracy on multilingual benchmarks.

**Limitations:** 

**Conclusion:** Smoothie-Qwen offers a practical solution for enhancing language controllability in LLMs, improving their reliability in global contexts.

**Abstract:** Multilingual large language models (LLMs) often exhibit language confusion, a tendency to generate responses in a dominant language irrespective of the prompt's language. To address this, we propose Smoothie-Qwen, a lightweight, post-hoc method that mitigates language bias without retraining. This technique selectively adjusts token-level output probabilities to effectively suppress undesired language generation. Applied to the Qwen model, our method reduces unintended Chinese output by over 95% while preserving task accuracy on multilingual benchmarks. This work provides a practical and efficient solution for enhancing the language controllability of LLMs, making them more reliable for global applications.

</details>


### [42] [Agentic-R1: Distilled Dual-Strategy Reasoning](https://arxiv.org/abs/2507.05707)

*Weihua Du, Pranjal Aggarwal, Sean Welleck, Yiming Yang*

**Main category:** cs.CL

**Keywords:** long chain-of-thought, multi-strategy reasoning, fine-tuning framework

**Relevance Score:** 7

**TL;DR:** A fine-tuning framework, DualDistill, enhances reasoning in long chain-of-thought models by combining multiple reasoning strategies into a single model, Agentic-R1, which adapts its approach based on the task type.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of current long-CoT models in mathematical and logical reasoning.

**Method:** DualDistill distills complementary reasoning strategies from multiple teacher models into a unified student model, which adapts its strategy depending on the query type.

**Key Contributions:**

	1. Introduction of the DualDistill fine-tuning framework
	2. Development of Agentic-R1 that dynamically selects reasoning strategies
	3. Demonstration of improved accuracy in both arithmetic and abstract tasks

**Result:** Agentic-R1 demonstrates improved accuracy in various computation-intensive and standard benchmarks by selecting optimal strategies for arithmetic and algorithmic problems.

**Limitations:** 

**Conclusion:** The multi-strategy distillation approach proves effective for robust and efficient reasoning across diverse tasks.

**Abstract:** Current long chain-of-thought (long-CoT) models excel at mathematical reasoning but rely on slow and error-prone natural language traces. Tool-augmented agents address arithmetic via code execution, but often falter on complex logical tasks. We introduce a fine-tuning framework, DualDistill, that distills complementary reasoning strategies from multiple teachers into a unified student model. Using this approach, we train Agentic-R1, which dynamically selects the optimal strategy for each query, invoking tools for arithmetic and algorithmic problems, and using text-based reasoning for abstract ones. Our method improves accuracy across a range of tasks, including both computation-intensive and standard benchmarks, demonstrating the effectiveness of multi-strategy distillation in achieving robust and efficient reasoning. Our project is available at https://github.com/StigLidu/DualDistill

</details>


### [43] [DRAGON: Dynamic RAG Benchmark On News](https://arxiv.org/abs/2507.05713)

*Fedor Chernogorskii, Sergei Averkiev, Liliya Kudraleeva, Zaven Martirosian, Maria Tikhonova, Valentin Malykh, Alena Fenogenova*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Russian language, dynamic benchmark

**Relevance Score:** 9

**TL;DR:** DRAGON is a dynamic benchmark for evaluating Retrieval-Augmented Generation (RAG) systems in Russian, which includes a regularly updated news corpus and supports automatic question generation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of RAG evaluation resources for languages other than English, specifically focusing on Russian, by providing a dynamic and updated evaluation framework.

**Method:** The benchmark is built on a continually updated corpus of Russian news and public documents, featuring automated question generation using a Knowledge Graph that extracts core question types.

**Key Contributions:**

	1. First dynamic benchmark for RAG systems in Russian
	2. Supports automatic question generation
	3. Includes a public leaderboard for community engagement

**Result:** The DRAGON benchmark allows for comprehensive evaluation of the retriever and generator components of RAG systems, offers reusable evaluation scripts, and features a public leaderboard.

**Limitations:** 

**Conclusion:** The framework aims to enhance the evaluation of RAG systems in Russian and support community participation in advancing this domain.

**Abstract:** Retrieval-Augmented Generation (RAG) is a widely adopted approach for improving the factuality of large language models (LLMs) by incorporating external knowledge at inference time. Although there exist multiple RAG benchmarks for English, evaluation resources for other languages, including Russian, remain scarce and static, failing to capture the dynamic nature of real-world deployments.   In this work, we present DRAGON (Dynamic RAG Benchmark On News), the first dynamic benchmark for evaluating RAG systems in Russian on a changing news corpora. DRAGON is built upon a regularly updated corpus of Russian news and public documents and supports comprehensive evaluation of both the retriever and generator components. Question generation is performed automatically with the use of Knowledge Graph constructed from the corpus and enables the extraction of four core question types aligned with distinct subgraph patterns. We release a complete evaluation framework comprising the pipeline for automatic question generation, evaluation scripts, which are potentially reusable for other languages and multilingual settings, and benchmark data. We also launch a public leaderboard to encourage community participation and comparison.

</details>


### [44] [HIRAG: Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation](https://arxiv.org/abs/2507.05714)

*YiHan Jiao, ZheHao Tan, Dan Yang, DuoLin Sun, Jie Feng, Jian Wang, Peng Wei*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented generation, Hierarchical reasoning, Fine-tuning methods

**Relevance Score:** 9

**TL;DR:** The paper introduces HIRAG, a new instruction fine-tuning method for RAG models that enhances their reasoning capabilities through a hierarchical approach.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of RAG systems, which face challenges in handling real-time information and domain-specific problems due to inconsistent document quality and retrieval system limitations.

**Method:** The proposed method, HIRAG, involves a 'think before answering' strategy that incorporates hierarchical abilities: filtering relevant information, combining semantic information across paragraphs, and RAG-specific reasoning.

**Key Contributions:**

	1. Introduction of the HIRAG method for RAG instruction fine-tuning
	2. Establishment of a hierarchical approach for RAG model reasoning
	3. Demonstration of significant performance improvements across multiple datasets

**Result:** Experiments demonstrate that the HIRAG training strategy significantly boosts the performance of RAG models on various datasets including RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

**Limitations:** 

**Conclusion:** HIRAG enhances the open-book examination capability of RAG models and addresses critical limitations in existing systems.

**Abstract:** Retrieval-augmented generation (RAG) has become a fundamental paradigm for addressing the challenges faced by large language models in handling real-time information and domain-specific problems. Traditional RAG systems primarily rely on the in-context learning (ICL) capabilities of the large language model itself. Still, in-depth research on the specific capabilities needed by the RAG generation model is lacking, leading to challenges with inconsistent document quality and retrieval system imperfections. Even the limited studies that fine-tune RAG generative models often \textit{lack a granular focus on RAG task} or \textit{a deeper utilization of chain-of-thought processes}. To address this, we propose that RAG models should possess three progressively hierarchical abilities (1) Filtering: the ability to select relevant information; (2) Combination: the ability to combine semantic information across paragraphs; and (3) RAG-specific reasoning: the ability to further process external knowledge using internal knowledge. Thus, we introduce our new RAG instruction fine-tuning method, Hierarchical-Thought Instruction-Tuning Retrieval-Augmented Generation (HIRAG) incorporates a "think before answering" strategy. This method enhances the model's open-book examination capability by utilizing multi-level progressive chain-of-thought. Experiments show that the HIRAG training strategy significantly improves the model's performance on datasets such as RGB, PopQA, MuSiQue, HotpotQA, and PubmedQA.

</details>


### [45] [Omni-Router: Sharing Routing Decisions in Sparse Mixture-of-Experts for Speech Recognition](https://arxiv.org/abs/2507.05724)

*Zijin Gu, Tatiana Likhomanenko, Navdeep Jaitly*

**Main category:** cs.CL

**Keywords:** Mixture-of-experts, Omni-router Transformer, Automatic speech recognition

**Relevance Score:** 4

**TL;DR:** The paper introduces the Omni-router Transformer, a mixture-of-experts model that uses a shared router across layers to improve cooperation and specialization among ASR experts, resulting in reduced word error rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of MoE architectures in automatic speech recognition by increasing expert cooperation across layers and encouraging specialization.

**Method:** A shared router is implemented in the Omni-router Transformer that operates across different MoE layers, instead of using independent routers for each layer as in traditional approaches.

**Key Contributions:**

	1. Introduction of the Omni-router Transformer model
	2. Demonstration of improved performance on ASR benchmarks
	3. Insight into expert cooperation within MoE architectures

**Result:** The Omni-router Transformer reduces average word error rates by 11.2% compared to dense models and 8.2% compared to the Switch Transformer, while demonstrating lower training loss and robust performance across diverse ASR benchmarks.

**Limitations:** 

**Conclusion:** The Omni-router Transformer model demonstrates superior performance in ASR tasks, indicating the benefits of enhanced cooperation between experts.

**Abstract:** Mixture-of-experts (MoE) architectures have expanded from language modeling to automatic speech recognition (ASR). Traditional MoE methods, such as the Switch Transformer, route experts independently within each layer. Our analysis reveals that routers in most layers make expert choices that are not strongly correlated with the choices of the routers in other layers. To increase the cooperation between experts in different layers and encourage greater specialization, we use a shared router across different MoE layers. We call this model \emph{Omni-router Transformer}. Extensive experiments on a large-scale pseudo-labeled dataset and evaluations across 10 diverse, out-of-domain ASR benchmarks demonstrate that the Omni-router Transformer is able to achieve lower training loss and consistently outperform dense and Switch Transformer models, reducing average word error rates by 11.2% and 8.2%, respectively, while providing structured expert usage and improved robustness to diverse data.

</details>


### [46] [GPTKB v1.5: A Massive Knowledge Base for Exploring Factual LLM Knowledge](https://arxiv.org/abs/2507.05740)

*Yujia Hu, Tuan-Phong Nguyen, Shrestha Ghosh, Moritz M√ºller, Simon Razniewski*

**Main category:** cs.CL

**Keywords:** knowledge base, LLM exploration, SPARQL querying

**Relevance Score:** 7

**TL;DR:** Introduction of GPTKB v1.5, a knowledge base built with LLM technology for improved exploration and querying of LLM knowledge.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding of language models' factual knowledge and provide scalable analysis tools.

**Method:** Demonstration of GPTKB v1.5, a 100-million-triple knowledge base, focusing on link-traversal, SPARQL querying, and comparative analysis of LLM knowledge.

**Key Contributions:**

	1. Introduction of a 100-million-triple knowledge base
	2. Demonstration of three practical use cases
	3. Advancement in systematic LLM knowledge analysis

**Result:** GPTKB v1.5 showcases capabilities in exploring and querying LLM knowledge, highlighting the limitations and strengths of such models.

**Limitations:** The knowledge base may not cover all aspects of LLM knowledge, and the exploratory methods require user engagement.

**Conclusion:** Massive-recursive LLM knowledge materialization offers new research opportunities and enhances automated knowledge base construction.

**Abstract:** Language models are powerful tools, yet their factual knowledge is still poorly understood, and inaccessible to ad-hoc browsing and scalable statistical analysis. This demonstration introduces GPTKB v1.5, a densely interlinked 100-million-triple knowledge base (KB) built for $14,000 from GPT-4.1, using the GPTKB methodology for massive-recursive LLM knowledge materialization (Hu et al., ACL 2025). The demonstration experience focuses on three use cases: (1) link-traversal-based LLM knowledge exploration, (2) SPARQL-based structured LLM knowledge querying, (3) comparative exploration of the strengths and weaknesses of LLM knowledge. Massive-recursive LLM knowledge materialization is a groundbreaking opportunity both for the research area of systematic analysis of LLM knowledge, as well as for automated KB construction. The GPTKB demonstrator is accessible at https://gptkb.org.

</details>


### [47] [DocTalk: Scalable Graph-based Dialogue Synthesis for Enhancing LLM Conversational Capabilities](https://arxiv.org/abs/2507.05750)

*Jing Yang Lee, Hamed Bonab, Nasser Zalmout, Ming Zeng, Sanket Lokegaonkar, Colin Lockard, Binxuan Huang, Ritesh Sarkhel, Haodong Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, multi-turn dialogues, synthesized conversational data

**Relevance Score:** 9

**TL;DR:** This paper introduces DocTalk, a synthesized multi-turn dialogue corpus developed from existing text corpora, aiming to enhance the multi-turn capabilities of Large Language Models (LLMs).

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the mismatch between the training data of LLMs, which is mainly continuous prose, and the requirements of multi-turn conversational tasks.

**Method:** A pipeline is developed to transform clusters of related documents into extended multi-turn, multi-topic dialogues, creating the DocTalk corpus from Wikipedia articles.

**Key Contributions:**

	1. Creation of the DocTalk corpus with over 730k conversations
	2. Demonstration of significant improvements in LLM performance metrics
	3. Introduction of a novel pipeline for synthesizing dialogue from text corpora

**Result:** Incorporating the DocTalk corpus into LLM pre-training leads to improvements of up to 40% in context memory and understanding without compromising overall performance.

**Limitations:** 

**Conclusion:** The DocTalk corpus offers a valuable resource for improving the conversational capabilities of LLMs in multi-turn dialogues.

**Abstract:** Large Language Models (LLMs) are increasingly employed in multi-turn conversational tasks, yet their pre-training data predominantly consists of continuous prose, creating a potential mismatch between required capabilities and training paradigms. We introduce a novel approach to address this discrepancy by synthesizing conversational data from existing text corpora. We present a pipeline that transforms a cluster of multiple related documents into an extended multi-turn, multi-topic information-seeking dialogue. Applying our pipeline to Wikipedia articles, we curate DocTalk, a multi-turn pre-training dialogue corpus consisting of over 730k long conversations. We hypothesize that exposure to such synthesized conversational structures during pre-training can enhance the fundamental multi-turn capabilities of LLMs, such as context memory and understanding. Empirically, we show that incorporating DocTalk during pre-training results in up to 40% gain in context memory and understanding, without compromising base performance. DocTalk is available at https://huggingface.co/datasets/AmazonScience/DocTalk.

</details>


### [48] [Flippi: End To End GenAI Assistant for E-Commerce](https://arxiv.org/abs/2507.05788)

*Anand A. Rajasekar, Praveen Tangarajan, Anjali Nainani, Amogh Batwal, Vinay Rao Dandin, Anusua Trivedi, Ozan Ersoy*

**Main category:** cs.CL

**Keywords:** Conversational Assistant, E-commerce, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** Flippi is an innovative conversational assistant aimed at enhancing e-commerce experiences by leveraging LLMs for personalized product discovery and comparisons.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve user interactions with e-commerce platforms by providing an effective means to navigate and discover products using natural language dialogue.

**Method:** Flippi employs advanced NLP techniques, including Query Reformulation, Intent Detection, RAG, NER, and Context Reduction, to interpret customer queries and deliver tailored product information.

**Key Contributions:**

	1. Introduction of an end-to-end conversational assistant for e-commerce
	2. Utilization of advanced NLP techniques for personalized product discovery
	3. Robust architecture designed for integration across various e-commerce platforms

**Result:** Flippi successfully enables users to find products efficiently, compare options, and make informed purchasing decisions, leading to improved customer satisfaction and engagement in e-commerce.

**Limitations:** 

**Conclusion:** Flippi bridges the gap between online shopping convenience and personalized assistance, setting a new benchmark for customer satisfaction in digital marketplaces.

**Abstract:** The emergence of conversational assistants has fundamentally reshaped user interactions with digital platforms. This paper introduces Flippi-a cutting-edge, end-to-end conversational assistant powered by large language models (LLMs) and tailored for the e-commerce sector. Flippi addresses the challenges posed by the vast and often overwhelming product landscape, enabling customers to discover products more efficiently through natural language dialogue. By accommodating both objective and subjective user requirements, Flippi delivers a personalized shopping experience that surpasses traditional search methods. This paper details how Flippi interprets customer queries to provide precise product information, leveraging advanced NLP techniques such as Query Reformulation, Intent Detection, Retrieval-Augmented Generation (RAG), Named Entity Recognition (NER), and Context Reduction. Flippi's unique capability to identify and present the most attractive offers on an e-commerce site is also explored, demonstrating how it empowers users to make cost-effective decisions. Additionally, the paper discusses Flippi's comparative analysis features, which help users make informed choices by contrasting product features, prices, and other relevant attributes. The system's robust architecture is outlined, emphasizing its adaptability for integration across various e-commerce platforms and the technological choices underpinning its performance and accuracy. Finally, a comprehensive evaluation framework is presented, covering performance metrics, user satisfaction, and the impact on customer engagement and conversion rates. By bridging the convenience of online shopping with the personalized assistance traditionally found in physical stores, Flippi sets a new standard for customer satisfaction and engagement in the digital marketplace.

</details>


### [49] [Bridging Perception and Language: A Systematic Benchmark for LVLMs' Understanding of Amodal Completion Reports](https://arxiv.org/abs/2507.05799)

*Amane Watahiki, Tomoki Doi, Taiga Shinozaki, Satoshi Nishida, Takuya Niikawa, Katsunori Miyahara, Hitomi Yanaka*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Amodal Completion, Benchmark, Cognitive Science, Japanese Linguistic Competence

**Relevance Score:** 8

**TL;DR:** This paper investigates the inferential abilities of large vision-language models (LVLMs) in relation to amodal completion, a phenomenon where incomplete objects are perceived based on contextual information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the gap in research regarding LVLMs' performance on amodal completion tasks, particularly how these models handle occluded information in multimodal contexts.

**Method:** The authors developed a benchmark based on Basic Formal Ontology to classify amodal completion and evaluated various LVLMs against it.

**Key Contributions:**

	1. Introduction of a benchmark for amodal completion in LVLMs
	2. Analysis of LVLMs' performance disparities based on object type
	3. Identification of language-specific deficiencies in LVLMs

**Result:** The findings reveal that while many LVLMs perform comparably to humans overall, their accuracy varies for different object types, with certain models showing reduced accuracy in specific conditions, such as when prompted in Japanese.

**Limitations:** The study primarily focuses on Japanese language prompting which may limit the generalizability of findings to other languages and contexts.

**Conclusion:** The results highlight the need for improved linguistic competence among LVLMs, especially for non-English languages, when addressing tasks related to amodal completion.

**Abstract:** One of the main objectives in developing large vision-language models (LVLMs) is to engineer systems that can assist humans with multimodal tasks, including interpreting descriptions of perceptual experiences. A central phenomenon in this context is amodal completion, in which people perceive objects even when parts of those objects are hidden. Although numerous studies have assessed whether computer-vision algorithms can detect or reconstruct occluded regions, the inferential abilities of LVLMs on texts related to amodal completion remain unexplored. To address this gap, we constructed a benchmark grounded in Basic Formal Ontology to achieve a systematic classification of amodal completion. Our results indicate that while many LVLMs achieve human-comparable performance overall, their accuracy diverges for certain types of objects being completed. Notably, in certain categories, some LLaVA-NeXT variants and Claude 3.5 Sonnet exhibit lower accuracy on original images compared to blank stimuli lacking visual content. Intriguingly, this disparity emerges only under Japanese prompting, suggesting a deficiency in Japanese-specific linguistic competence among these models.

</details>


### [50] [How to Evaluate Automatic Speech Recognition: Comparing Different Performance and Bias Measures](https://arxiv.org/abs/2507.05885)

*Tanvina Patel, Wiebke Hutiri, Aaron Yi Ding, Odette Scharenborg*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Bias, Performance Measurement, Bias Mitigation, Evaluation Methods

**Relevance Score:** 4

**TL;DR:** The study evaluates performance and bias measures of ASR systems, highlighting the need for comprehensive reporting beyond averaged error rates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address bias in automatic speech recognition systems against various speaker groups and improve evaluation methodologies.

**Method:** Comparative analysis of different performance and bias measures for ASR systems, along with implementing bias mitigation strategies.

**Key Contributions:**

	1. Comparison of performance and bias measures in ASR
	2. Implementation of bias mitigation strategies
	3. Recommendations for better reporting practices in ASR

**Result:** The study found that sole reliance on averaged error rates is inadequate; other measures are essential for comprehensive evaluation.

**Limitations:** 

**Conclusion:** Recommendations are provided for better reporting of ASR performance and bias, aiming to improve representation for diverse speakers.

**Abstract:** There is increasingly more evidence that automatic speech recognition (ASR) systems are biased against different speakers and speaker groups, e.g., due to gender, age, or accent. Research on bias in ASR has so far primarily focused on detecting and quantifying bias, and developing mitigation approaches. Despite this progress, the open question is how to measure the performance and bias of a system. In this study, we compare different performance and bias measures, from literature and proposed, to evaluate state-of-the-art end-to-end ASR systems for Dutch. Our experiments use several bias mitigation strategies to address bias against different speaker groups. The findings reveal that averaged error rates, a standard in ASR research, alone is not sufficient and should be supplemented by other measures. The paper ends with recommendations for reporting ASR performance and bias to better represent a system's performance for diverse speaker groups, and overall system bias.

</details>


### [51] [Psychometric Item Validation Using Virtual Respondents with Trait-Response Mediators](https://arxiv.org/abs/2507.05890)

*Sungjib Lim, Woojung Song, Eun-Ju Lee, Yohan Jo*

**Main category:** cs.CL

**Keywords:** survey item generation, LLMs, construct validity, trait measurement, virtual respondents

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for scalable survey item generation using LLMs, focusing on ensuring construct validity by simulating virtual respondents with diverse mediators.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for scalable survey item generation for assessing traits of LLMs and ensuring construct validity without extensive human data collection.

**Method:** The proposed framework uses LLMs to simulate respondents with different mediators, which allows for the identification of survey items that robustly measure intended traits.

**Key Contributions:**

	1. Framework for virtual respondent simulation using LLMs
	2. Identification of high-validity survey items
	3. Insight into how LLMs can replicate human-like behavior

**Result:** Experiments demonstrate that the mediator generation and simulation framework effectively identifies high-validity items across three psychological trait theories (Big5, Schwartz, VIA).

**Limitations:** 

**Conclusion:** The work opens a new direction for cost-effective survey development and enhances understanding of LLMs' replication of human-like behavior. The dataset and code will be publicly released.

**Abstract:** As psychometric surveys are increasingly used to assess the traits of large language models (LLMs), the need for scalable survey item generation suited for LLMs has also grown. A critical challenge here is ensuring the construct validity of generated items, i.e., whether they truly measure the intended trait. Traditionally, this requires costly, large-scale human data collection. To make it efficient, we present a framework for virtual respondent simulation using LLMs. Our central idea is to account for mediators: factors through which the same trait can give rise to varying responses to a survey item. By simulating respondents with diverse mediators, we identify survey items that robustly measure intended traits. Experiments on three psychological trait theories (Big5, Schwartz, VIA) show that our mediator generation methods and simulation framework effectively identify high-validity items. LLMs demonstrate the ability to generate plausible mediators from trait definitions and to simulate respondent behavior for item validation. Our problem formulation, metrics, methodology, and dataset open a new direction for cost-effective survey development and a deeper understanding of how LLMs replicate human-like behavior. We will publicly release our dataset and code to support future work.

</details>


### [52] [Few-shot text-based emotion detection](https://arxiv.org/abs/2507.05918)

*Teodor-George Marchitan, Claudiu Creanga, Liviu P. Dinu*

**Main category:** cs.CL

**Keywords:** text-based emotion detection, large language models, multi-label detection

**Relevance Score:** 8

**TL;DR:** The Unibuc - NLP team's approach for SemEval 2025 focused on using large language models for text-based emotion detection and achieved notable F1-macro scores across multiple languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the state of emotion detection in text by leveraging recent developments in large language models and addressing the challenges posed in multiple languages.

**Method:** Experiments were conducted using large language models (Gemini, Qwen, DeepSeek) with techniques such as few-shot prompting and fine-tuning for emotion detection tasks.

**Key Contributions:**

	1. Implementation of large language models for multi-label emotion detection
	2. Exploration of few-shot prompting and fine-tuning techniques
	3. Performance benchmark across multiple languages in emotion detection tasks

**Result:** The final system achieved an F1-macro score of 0.7546 for the English subset, 0.1727 for the Portuguese subset, and 0.325 for the Emakhuwa subset in the multi-label emotion detection track.

**Limitations:** Results are significantly varied across languages, indicating room for improvement in non-English emotion detection.

**Conclusion:** The team's approach demonstrated effectiveness in English and highlighted challenges in other languages, suggesting further work is needed for broader applicability.

**Abstract:** This paper describes the approach of the Unibuc - NLP team in tackling the SemEval 2025 Workshop, Task 11: Bridging the Gap in Text-Based Emotion Detection. We mainly focused on experiments using large language models (Gemini, Qwen, DeepSeek) with either few-shot prompting or fine-tuning. With our final system, for the multi-label emotion detection track (track A), we got an F1-macro of $0.7546$ (26/96 teams) for the English subset, $0.1727$ (35/36 teams) for the Portuguese (Mozambican) subset and $0.325$ (\textbf{1}/31 teams) for the Emakhuwa subset.

</details>


### [53] [Towards a Principled Evaluation of Knowledge Editors](https://arxiv.org/abs/2507.05937)

*Sebastian Pohl, Max Ploner, Alan Akbik*

**Main category:** cs.CL

**Keywords:** Model Editing, Knowledge Editing, Evaluation Methodology, False Positives, Language Understanding

**Relevance Score:** 6

**TL;DR:** This paper investigates the robustness and fairness of evaluation methodologies for knowledge editing in models, highlighting the impact of different metrics and edit batch sizes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing attention to model editing and knowledge editing demands a thorough evaluation of existing methodologies, particularly in terms of their robustness and fairness towards different editors.

**Method:** The paper analyzes various evaluation metrics and methodologies while assessing their effect on the ranking of knowledge editors. It also conducts manual assessments of string matching methods used in knowledge editing evaluations.

**Key Contributions:**

	1. Analysis of the robustness of knowledge editing evaluation methods.
	2. Revealing the impact of different metrics and batch sizes on editor rankings.
	3. Manual assessment showing limitations of commonly used string matching evaluations.

**Result:** Different metrics and edit batch sizes significantly affect the ranking of knowledge editors, and the favored string matching method tends to yield false positives.

**Limitations:** The study primarily focuses on existing evaluation methodologies and may not cover all aspects of knowledge editing efficacy in practice.

**Conclusion:** Evaluation methodologies for knowledge editing need to be critically evaluated for their robustness and fairness, as they can influence the perceived effectiveness of editors.

**Abstract:** Model editing has been gaining increasing attention over the past few years. For Knowledge Editing in particular, more challenging evaluation datasets have recently been released. These datasets use different methodologies to score the success of editors. Yet, it remains under-explored how robust these methodologies are and whether they unfairly favor some editors. Moreover, the disruptive impact of these editors on overall model capabilities remains a constant blind spot.   We address both of these problems and show that choosing different metrics and evaluation methodologies as well as different edit batch sizes can lead to a different ranking of knowledge editors. Crucially we demonstrate this effect also on general language understanding tasks evaluated alongside the knowledge editing tasks. Further we include a manual assessment of the string matching based evaluation method for knowledge editing that is favored by recently released datasets, revealing a tendency to produce false positive matches.

</details>


### [54] [Remember Past, Anticipate Future: Learning Continual Multimodal Misinformation Detectors](https://arxiv.org/abs/2507.05939)

*Bing Wang, Ximing Li, Mengzhe Ye, Changchun Li, Bo Fu, Jianfeng Qu, Lin Yuanbo Wu*

**Main category:** cs.CL

**Keywords:** Multimodal Misinformation Detection, continual learning, mixture-of-experts, online data streams, social media

**Relevance Score:** 6

**TL;DR:** The paper presents a new continual Multimodal Misinformation Detection method called DAEDCMD, which effectively addresses the challenges of knowledge forgetting and evolving social environments in misinformation detection using a mixture-of-expert structure and a continuous-time dynamics model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The growing problem of misinformation propagation on social media necessitates advanced methods for Multimodal Misinformation Detection that can adapt to evolving data streams in real-time.

**Method:** The proposed approach employs a Dirichlet process-based mixture-of-expert structure to isolate interference between event-specific parameters and utilizes a continuous-time dynamics model to anticipate future environmental distributions.

**Key Contributions:**

	1. Introduction of a continual MMD method DAEDCMD.
	2. Implementation of a Dirichlet process-based mixture-of-expert structure to mitigate past knowledge forgetting.
	3. Development of a continuous-time dynamics model to improve generalization for future data.

**Result:** DAEDCMD significantly outperforms existing methods, including six baseline MMD methods and three continual learning techniques, in detecting misinformation in the presence of changing data.

**Limitations:** 

**Conclusion:** The continual MMD method DAEDCMD is effective in overcoming challenges faced in traditional MMD approaches, thus enhancing the detection performance in dynamic environments.

**Abstract:** Nowadays, misinformation articles, especially multimodal ones, are widely spread on social media platforms and cause serious negative effects. To control their propagation, Multimodal Misinformation Detection (MMD) becomes an active topic in the community to automatically identify misinformation. Previous MMD methods focus on supervising detectors by collecting offline data. However, in real-world scenarios, new events always continually emerge, making MMD models trained on offline data consistently outdated and ineffective. To address this issue, training MMD models under online data streams is an alternative, inducing an emerging task named continual MMD. Unfortunately, it is hindered by two major challenges. First, training on new data consistently decreases the detection performance on past data, named past knowledge forgetting. Second, the social environment constantly evolves over time, affecting the generalization on future data. To alleviate these challenges, we propose to remember past knowledge by isolating interference between event-specific parameters with a Dirichlet process-based mixture-of-expert structure, and anticipate future environmental distributions by learning a continuous-time dynamics model. Accordingly, we induce a new continual MMD method DAEDCMD. Extensive experiments demonstrate that DAEDCMD can consistently and significantly outperform the compared methods, including six MMD baselines and three continual learning methods.

</details>


### [55] [Chat-Ghosting: A Comparative Study of Methods for Auto-Completion in Dialog Systems](https://arxiv.org/abs/2507.05940)

*Sandeep Mishra, Anubhab Mandal, Bishal Santra, Tushar Abhishek, Pawan Goyal, Manish Gupta*

**Main category:** cs.CL

**Keywords:** ghosting, auto-completion, natural language processing, machine learning, dialog systems

**Relevance Score:** 8

**TL;DR:** This paper studies ghosting in text input prediction for chat systems, comparing various auto-completion methods and proposing an entropy-based strategy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Ghosting enhances user experience by predicting text input, crucial for accessibility and usability in chat interfaces, yet has received little attention in research.

**Method:** The study employs four dialog datasets to evaluate existing auto-completion methods, including n-gram and deep learning techniques, and introduces a novel early stopping strategy based on entropy.

**Key Contributions:**

	1. Comparison of deep learning and non-deep learning methods for ghosting
	2. Proposal of an entropy-based dynamic early stopping strategy
	3. Public availability of code and datasets for future research

**Result:** N-gram models outperformed deep learning methods in efficiency for known queries, while advanced neural models performed better with unseen queries when context is included.

**Limitations:** Limited to four datasets; may not generalize to all dialog contexts.

**Conclusion:** Incorporating conversational context improves ghosting performance considerably; the research provides necessary benchmarks for future studies and publicly available resources.

**Abstract:** Ghosting, the ability to predict a user's intended text input for inline query auto-completion, is an invaluable feature for modern search engines and chat interfaces, greatly enhancing user experience. By suggesting completions to incomplete queries (or prefixes), ghosting aids users with slow typing speeds, disabilities, or limited language proficiency. Ghosting is a challenging problem and has become more important with the ubiquitousness of chat-based systems like ChatGPT, Copilot, etc. Despite the increasing prominence of chat-based systems utilizing ghosting, this challenging problem of Chat-Ghosting has received little attention from the NLP/ML research community. There is a lack of standardized benchmarks and relative performance analysis of deep learning and non-deep learning methods. We address this through an open and thorough study of this problem using four publicly available dialog datasets: two human-human (DailyDialog and DSTC7-Ubuntu) and two human-bot (Open Assistant and ShareGPT). We experiment with various existing query auto-completion methods (using tries), n-gram methods and deep learning methods, with and without dialog context. We also propose a novel entropy-based dynamic early stopping strategy. Our analysis finds that statistical n-gram models and tries outperform deep learning based models in terms of both model performance and inference efficiency for seen prefixes. For unseen queries, neural models like T5 and Phi-2 lead to better results. Adding conversational context leads to significant improvements in ghosting quality, especially for Open-Assistant and ShareGPT. We make code and data publicly available

</details>


### [56] [OpenFActScore: Open-Source Atomic Evaluation of Factuality in Text Generation](https://arxiv.org/abs/2507.05965)

*Lucas Fonseca Lage, Simon Ostermann*

**Main category:** cs.CL

**Keywords:** FActuality evaluation, Open-source LLMs, Atomic Fact Generation, Hugging Face, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** OpenFActScore is an open-source framework for evaluating the factual accuracy of text produced by large language models, using Atomic Fact Generation and Validation techniques.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a transparent and open-source method for evaluating the factuality of text generated by LLMs, allowing for broader accessibility and reproducibility in research.

**Method:** The framework utilizes Atomic Fact Generation (AFG) to extract factual claims from text and Atomic Fact Validation (AFV) to verify these claims against trusted knowledge sources, using any Hugging Face-compatible model.

**Key Contributions:**

	1. Introduction of an open-source FActScore framework for LLM evaluation
	2. Support for any Hugging Face-compatible model for factuality assessment
	3. Demonstrated efficacy of open models in factual accuracy tasks

**Result:** The evaluation shows that open-source models can closely match the performance of closed-source counterparts, with Gemma performing the best and the final setup achieving a 0.99 Pearson correlation with original FActScore benchmarks.

**Limitations:** 

**Conclusion:** OpenFActScore promotes transparency in evaluating LLMs and provides a cost-effective alternative to closed-source systems, making it accessible for broader use in the research community.

**Abstract:** We introduce OpenFActScore, an open-source implementation of the FActScore framework for evaluating the factuality of text generated by large language models (LLMs). FActScore evaluates the factual accuracy of long-form text by using Atomic Fact Generation (AFG) to extract individual factual claims and Atomic Fact Validation (AFV) to verify each claim against a trusted knowledge source. While the original FActScore relies on closed-source and commercial models such as InstructGPT and ChatGPT, OpenFActScore enables the use of any Hugging Face-compatible model for both AFG and AFV. We provide a detailed technical overview of our implementation, highlighting design choices and modifications made to support open models. We evaluate multiple open-source LLMs on both AFG and AFV using the original FActScore benchmark, reporting BERTScore-F1 for AFG and Error Rate relative to human annotations for AFV. Our results show that open models can approximate the performance of closed-source systems, with Gemma achieving the best overall performance, and our final setup obtains a 0.99 Pearson correlation with the original FActScore experiments. OpenFActScore promotes transparency, reproducibility, and cost-effective evaluation, and is available at: https://github.com/lflage/OpenFActScore.

</details>


### [57] [We Should Evaluate Real-World Impact](https://arxiv.org/abs/2507.05973)

*Ehud Reiter*

**Main category:** cs.CL

**Keywords:** NLP, real-world impact, ACL Anthology, effectiveness evaluations, community standards

**Relevance Score:** 6

**TL;DR:** The paper critiques the lack of real-world impact evaluations in NLP research, noting that only 0.1% of ACL papers address this issue, emphasizing the necessity for a shift towards understanding NLP technology's practical impacts.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the inadequacy of real-world impact evaluations in the current NLP research literature, particularly within the ACL Anthology.

**Method:** Analysis of NLP papers published in the ACL Anthology to assess the extent of real-world impact evaluations provided.

**Key Contributions:**

	1. NLP research needs to focus more on real-world evaluations.
	2. Identification of the stark contrast between impact and metric evaluations in NLP papers.
	3. Encouragement for the ACL community to adopt a more impactful approach to NLP research.

**Result:** It was found that only a minuscule percentage (0.1%) of papers included evaluations of their real-world impact, and those that did often presented them incompletely.

**Limitations:** 

**Conclusion:** A call to action for the NLP community to prioritize the understanding and evaluation of the real-world impact of their technologies to increase their adoption and usefulness.

**Abstract:** The ACL community has very little interest in evaluating the real-world impact of NLP systems. A structured survey of the ACL Anthology shows that perhaps 0.1% of its papers contain such evaluations; furthermore most papers which include impact evaluations present them very sketchily and instead focus on metric evaluations. NLP technology would be more useful and more quickly adopted if we seriously tried to understand and evaluate its real-world impact.

</details>


### [58] [RabakBench: Scaling Human Annotations to Construct Localized Multilingual Safety Benchmarks for Low-Resource Languages](https://arxiv.org/abs/2507.05980)

*Gabriel Chua, Leanne Tan, Ziyu Ge, Roy Ka-Wei Lee*

**Main category:** cs.CL

**Keywords:** multilingual, safety benchmark, low-resource languages, large language models, Singapore

**Relevance Score:** 7

**TL;DR:** RabakBench is a new multilingual safety benchmark for low-resource languages that focuses on Singapore's linguistic context, revealing performance issues in existing safety classifiers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the poor performance of large language models and their safety classifiers on low-resource languages by creating a specialized benchmark.

**Method:** The benchmark is constructed using a three-stage pipeline: 1) Generating adversarial examples with LLMs, 2) Semi-automated labeling using majority-vote LLM labelers, and 3) Translating while preserving linguistic nuance and toxicity.

**Key Contributions:**

	1. Introduction of RabakBench for low-resource languages
	2. Scalable three-stage pipeline for dataset generation
	3. Public availability of the dataset and evaluation code

**Result:** The final dataset contains over 5,000 safety-labeled examples across four languages and identifies significant performance degradation in existing classifiers.

**Limitations:** 

**Conclusion:** RabakBench facilitates robust safety evaluations in multilingual settings and offers a framework for creating localized safety datasets in low-resource environments.

**Abstract:** Large language models (LLMs) and their safety classifiers often perform poorly on low-resource languages due to limited training data and evaluation benchmarks. This paper introduces RabakBench, a new multilingual safety benchmark localized to Singapore's unique linguistic context, covering Singlish, Chinese, Malay, and Tamil. RabakBench is constructed through a scalable three-stage pipeline: (i) Generate - adversarial example generation by augmenting real Singlish web content with LLM-driven red teaming; (ii) Label - semi-automated multi-label safety annotation using majority-voted LLM labelers aligned with human judgments; and (iii) Translate - high-fidelity translation preserving linguistic nuance and toxicity across languages. The final dataset comprises over 5,000 safety-labeled examples across four languages and six fine-grained safety categories with severity levels. Evaluations of 11 popular open-source and closed-source guardrail classifiers reveal significant performance degradation. RabakBench not only enables robust safety evaluation in Southeast Asian multilingual settings but also offers a reproducible framework for building localized safety datasets in low-resource environments. The benchmark dataset, including the human-verified translations, and evaluation code are publicly available.

</details>


### [59] [Evolution without Large Models: Training Language Model with Task Principles](https://arxiv.org/abs/2507.05991)

*Minghang Zhu, Shen Gao, Zhengliang Shi, Jiabao Fang, Pengjie Ren, Zhaochun Ren, Zhumin Chen, Shuo Shang*

**Main category:** cs.CL

**Keywords:** language models, data augmentation, carbon emissions

**Relevance Score:** 8

**TL;DR:** The paper proposes a self-evolution method for language models that improves data generation processes while reducing carbon emissions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of high carbon emissions and data leakage associated with using large-scale language models for data augmentation.

**Method:** The method involves Multi-level Principle Generation to summarize task-completion principles and Principle-based Instance Generation where a smaller model generates data based on these principles.

**Key Contributions:**

	1. Introduction of Multi-level Principle Generation for summarizing task principles
	2. Development of Principle-based Instance Generation for efficient data production
	3. Reduction of carbon emissions in model training processes

**Result:** Experiments show significant improvements in model performance using the proposed method compared to traditional data generation approaches.

**Limitations:** 

**Conclusion:** The proposed approach allows for more efficient training with lower environmental impact, utilizing the strengths of both large and small models effectively.

**Abstract:** A common training approach for language models involves using a large-scale language model to expand a human-provided dataset, which is subsequently used for model training.This method significantly reduces training costs by eliminating the need for extensive human data annotation. However, it still faces challenges such as high carbon emissions during data augmentation and the risk of data leakage when we use closed-source LLMs. To address these issues, we propose a self-evolution method for language models. First, we introduce the Multi-level Principle Generation, which enables a large-scale model to summarize task-completion principles based on a small amount of task data. Then, we propose the Principle-based Instance Generation, in which a smaller-scale language model uses these task principles to generate a large amount of data. This data is then used for model training. Experimental results show that our proposed method significantly improves model performance compared to directly using a smaller-scale language model to generate data. Additionally, since we only use the large-scale language model to generate the task-completion principles, the carbon emissions associated with training the model are greatly reduced.

</details>


### [60] [DocIE@XLLM25: In-Context Learning for Information Extraction using Fully Synthetic Demonstrations](https://arxiv.org/abs/2507.05997)

*Nicholas Popoviƒç, Ashish Kangen, Tim Schopf, Michael F√§rber*

**Main category:** cs.CL

**Keywords:** document-level extraction, synthetic data, in-context learning, large language models, zero-shot learning

**Relevance Score:** 9

**TL;DR:** A novel LLM-based pipeline for automatic synthetic data generation and in-context learning in document-level entity and relation extraction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of high-quality annotated corpora in document-level entity and relation extraction in zero-shot or few-shot settings.

**Method:** The proposed method integrates synthetic data generation with retrieval-based in-context learning using a reasoning-optimized language model, allowing for the creation of a database of quality demonstrations without manual annotation.

**Key Contributions:**

	1. Introduction of a fully automatic LLM-based pipeline for synthetic data generation.
	2. Creation of a large synthetic dataset for document-level entity and relation extraction.
	3. Evaluation of zero-shot in-context learning performance on the DocIE task.

**Result:** Generated a synthetic dataset of over 5,000 Wikipedia abstracts containing approximately 59,000 entities and 30,000 relation triples, and evaluated in-context learning performance on the DocIE shared task, uncovering challenges in zero-shot document-level extraction.

**Limitations:** The study highlights that joint extraction remains complicated even with state-of-the-art models.

**Conclusion:** Even with advanced LLMs, joint entity and relation extraction at the document level poses significant challenges.

**Abstract:** Large, high-quality annotated corpora remain scarce in document-level entity and relation extraction in zero-shot or few-shot settings. In this paper, we present a fully automatic, LLM-based pipeline for synthetic data generation and in-context learning for document-level entity and relation extraction. In contrast to existing approaches that rely on manually annotated demonstrations or direct zero-shot inference, our method combines synthetic data generation with retrieval-based in-context learning, using a reasoning-optimized language model. This allows us to build a high-quality demonstration database without manual annotation and to dynamically retrieve relevant examples at inference time. Based on our approach we produce a synthetic dataset of over $5k$ Wikipedia abstracts with approximately $59k$ entities and $30k$ relation triples. Finally, we evaluate in-context learning performance on the DocIE shared task, extracting entities and relations from long documents in a zero-shot setting. We find that in-context joint entity and relation extraction at document-level remains a challenging task, even for state-of-the-art large language models.

</details>


### [61] [Conditional Multi-Stage Failure Recovery for Embodied Agents](https://arxiv.org/abs/2507.06016)

*Youmna Farag, Svetlana Stoyanchev, Mohan Li, Simon Keizer, Rama Doddipatla*

**Main category:** cs.CL

**Keywords:** Failure Recovery, Embodied Agents, Large Language Models, Task Execution, Error Handling

**Relevance Score:** 7

**TL;DR:** This paper presents a failure recovery framework for embodied agents that utilizes zero-shot chain prompting to improve execution resilience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of tasks for embodied agents raises the risk of execution failures, necessitating robust failure recovery mechanisms.

**Method:** The proposed framework comprises four stages for error handling, using reasoning capabilities of large language models (LLMs) to analyze task execution challenges and formulate solutions.

**Key Contributions:**

	1. Introduction of a four-stage failure recovery framework
	2. Utilization of zero-shot chain prompting with LLMs
	3. Demonstrated state-of-the-art performance on the TEACH dataset

**Result:** The framework was evaluated on the TfD benchmark of the TEACH dataset, achieving state-of-the-art performance with an 11.5% improvement over a baseline lacking error recovery and a 19% increase over the best existing model.

**Limitations:** 

**Conclusion:** The introduction of a multistage failure recovery process significantly enhances the reliability of embodied agents during complex task execution.

**Abstract:** Embodied agents performing complex tasks are susceptible to execution failures, motivating the need for effective failure recovery mechanisms. In this work, we introduce a conditional multistage failure recovery framework that employs zero-shot chain prompting. The framework is structured into four error-handling stages, with three operating during task execution and one functioning as a post-execution reflection phase. Our approach utilises the reasoning capabilities of LLMs to analyse execution challenges within their environmental context and devise strategic solutions. We evaluate our method on the TfD benchmark of the TEACH dataset and achieve state-of-the-art performance, outperforming a baseline without error recovery by 11.5% and surpassing the strongest existing model by 19%.

</details>


### [62] [Entropy-Memorization Law: Evaluating Memorization Difficulty of Data in LLMs](https://arxiv.org/abs/2507.06056)

*Yizhan Huang, Zhe Yang, Meifang Chen, Jianping Zhang, Michael R. Lyu*

**Main category:** cs.CL

**Keywords:** Large Language Models, memorization, data entropy, entropy-memorization law, dataset inference

**Relevance Score:** 9

**TL;DR:** The paper investigates how to characterize the memorization difficulty of training data in Large Language Models (LLMs) and introduces the Entropy-Memorization Law, highlighting the relationship between data entropy and memorization scores.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the memorization characteristics of training data in LLMs and address how memorization difficulty can be quantified.

**Method:** Empirical experiments were conducted on OLMo models to investigate memorization difficulty, leading to the development of the Entropy-Memorization Law that correlates data entropy with memorization scores.

**Key Contributions:**

	1. Introduction of the Entropy-Memorization Law
	2. Empirical analysis of memorization difficulty in LLMs
	3. Method to differentiate training and testing data effectively

**Result:** The study found that the memorization score of training data is linearly correlated with its data entropy. In particular, sequences known as 'gibberish' showed lower empirical entropy than expected, allowing for better differentiation between training and testing data.

**Limitations:** 

**Conclusion:** The findings provide a simple and effective method to distinguish between training and testing data based on memorization difficulty using the Entropy-Memorization Law, facilitating Dataset Inference.

**Abstract:** Large Language Models (LLMs) are known to memorize portions of their training data, sometimes reproducing content verbatim when prompted appropriately. In this work, we investigate a fundamental yet under-explored question in the domain of memorization: How to characterize memorization difficulty of training data in LLMs? Through empirical experiments on OLMo, a family of open models, we present the Entropy-Memorization Law. It suggests that data entropy is linearly correlated with memorization score. Moreover, in a case study of memorizing highly randomized strings, or "gibberish", we observe that such sequences, despite their apparent randomness, exhibit unexpectedly low empirical entropy compared to the broader training corpus. Adopting the same strategy to discover Entropy-Memorization Law, we derive a simple yet effective approach to distinguish training and testing data, enabling Dataset Inference (DI).

</details>


### [63] [A Survey on Prompt Tuning](https://arxiv.org/abs/2507.06085)

*Zongqian Li, Yixuan Su, Nigel Collier*

**Main category:** cs.CL

**Keywords:** prompt tuning, language models, parameter-efficient, transfer learning, direct prompt learning

**Relevance Score:** 8

**TL;DR:** This survey reviews and categorizes existing prompt tuning methods for adapting language models, exploring their designs, advantages, and challenges.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the parameter-efficient method of prompt tuning in language models and identify innovations, advantages, and challenges.

**Method:** The paper categorizes prompt tuning methods into direct prompt learning and transfer learning, analyzing various design approaches and frameworks for each category.

**Key Contributions:**

	1. Classification of prompt tuning methods into direct and transfer learning categories.
	2. Visual comparisons of different prompt tuning frameworks.
	3. Identification of challenges in computational efficiency and training stability.

**Result:** It provides comparative visualizations of different methods and identifies challenges in efficiency and stability in training.

**Limitations:** The paper discusses known challenges but may not provide comprehensive solutions.

**Conclusion:** Future directions for research include improving training robustness and expanding the application scope of prompt tuning in language models.

**Abstract:** This survey reviews prompt tuning, a parameter-efficient approach for adapting language models by prepending trainable continuous vectors while keeping the model frozen. We classify existing approaches into two categories: direct prompt learning and transfer learning. Direct prompt learning methods include: general optimization approaches, encoder-based methods, decomposition strategies, and mixture-of-experts frameworks. Transfer learning methods consist of: general transfer approaches, encoder-based methods, and decomposition strategies. For each method, we analyze method designs, innovations, insights, advantages, and disadvantages, with illustrative visualizations comparing different frameworks. We identify challenges in computational efficiency and training stability, and discuss future directions in improving training robustness and broadening application scope.

</details>


### [64] [NeoBabel: A Multilingual Open Tower for Visual Generation](https://arxiv.org/abs/2507.06137)

*Mohammad Mahdi Derakhshani, Dheeraj Varghese, Marzieh Fadaee, Cees G. M. Snoek*

**Main category:** cs.CL

**Keywords:** multilingual image generation, text-to-image generation, cultural fidelity

**Relevance Score:** 4

**TL;DR:** NeoBabel is a multilingual image generation framework that improves performance, efficiency, and inclusivity, supporting multiple languages while maintaining strong English capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the barriers created by English-centric text-to-image generation systems that perpetuate digital inequities and perform poorly with translation pipelines.

**Method:** The model combines large-scale multilingual pretraining and high-resolution instruction tuning, evaluated using expanded multilingual benchmarks m-GenEval and m-DPG.

**Key Contributions:**

	1. Introduced a novel multilingual image generation framework NeoBabel.
	2. Expanded English-only benchmarks to multilingual equivalents for evaluation.
	3. Released an open toolkit with model checkpoints and a curated dataset.

**Result:** NeoBabel achieves state-of-the-art multilingual performance, scoring 0.75 on m-GenEval and 0.68 on m-DPG while being 2-4x smaller than leading models.

**Limitations:** 

**Conclusion:** NeoBabel shows that multilingual capability enhances robustness, efficiency, and cultural fidelity in generative AI, and an open toolkit is released to facilitate further research.

**Abstract:** Text-to-image generation advancements have been predominantly English-centric, creating barriers for non-English speakers and perpetuating digital inequities. While existing systems rely on translation pipelines, these introduce semantic drift, computational overhead, and cultural misalignment. We introduce NeoBabel, a novel multilingual image generation framework that sets a new Pareto frontier in performance, efficiency and inclusivity, supporting six languages: English, Chinese, Dutch, French, Hindi, and Persian. The model is trained using a combination of large-scale multilingual pretraining and high-resolution instruction tuning. To evaluate its capabilities, we expand two English-only benchmarks to multilingual equivalents: m-GenEval and m-DPG. NeoBabel achieves state-of-the-art multilingual performance while retaining strong English capability, scoring 0.75 on m-GenEval and 0.68 on m-DPG. Notably, it performs on par with leading models on English tasks while outperforming them by +0.11 and +0.09 on multilingual benchmarks, even though these models are built on multilingual base LLMs. This demonstrates the effectiveness of our targeted alignment training for preserving and extending crosslingual generalization. We further introduce two new metrics to rigorously assess multilingual alignment and robustness to code-mixed prompts. Notably, NeoBabel matches or exceeds English-only models while being 2-4x smaller. We release an open toolkit, including all code, model checkpoints, a curated dataset of 124M multilingual text-image pairs, and standardized multilingual evaluation protocols, to advance inclusive AI research. Our work demonstrates that multilingual capability is not a trade-off but a catalyst for improved robustness, efficiency, and cultural fidelity in generative AI.

</details>


### [65] [Coding Triangle: How Does Large Language Model Understand Code?](https://arxiv.org/abs/2507.06138)

*Taolin Zhang, Zihan Ma, Maosong Cao, Junnan Liu, Songyang Zhang, Kai Chen*

**Main category:** cs.CL

**Keywords:** large language models, code generation, editorial analysis, test case generation, human-computer interaction

**Relevance Score:** 9

**TL;DR:** The paper introduces the Code Triangle framework to evaluate LLMs on code generation, revealing their limitations in diversity and robustness compared to human programmers. The study suggests incorporating human inputs to enhance LLM performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically evaluate the programming competence of large language models (LLMs) in order to understand their strengths and weaknesses in code generation tasks.

**Method:** Introduces the Code Triangle framework that assesses LLMs across three dimensions: editorial analysis, code implementation, and test case generation, using competitive programming benchmarks for extensive experiments.

**Key Contributions:**

	1. Introduction of the Code Triangle framework for evaluating LLMs
	2. Discovery of significant biases and errors in LLMs compared to humans
	3. Recommendations for enhancing LLM performance through human inputs and model mixtures

**Result:** LLMs often lack the diversity and robustness of human-generated solutions, with model errors clustering due to training data biases, highlighting a significant gap between LLM cognition and human expertise.

**Limitations:** The study primarily highlights the shortcomings of LLMs without providing an exhaustive solution to all identified issues.

**Conclusion:** Incorporating human-generated materials and model mixtures can improve LLM performance, and understanding LLM cognition can lead to self-improvement in coding models.

**Abstract:** Large language models (LLMs) have achieved remarkable progress in code generation, yet their true programming competence remains underexplored. We introduce the Code Triangle framework, which systematically evaluates LLMs across three fundamental dimensions: editorial analysis, code implementation, and test case generation. Through extensive experiments on competitive programming benchmarks, we reveal that while LLMs can form a self-consistent system across these dimensions, their solutions often lack the diversity and robustness of human programmers. We identify a significant distribution shift between model cognition and human expertise, with model errors tending to cluster due to training data biases and limited reasoning transfer. Our study demonstrates that incorporating human-generated editorials, solutions, and diverse test cases, as well as leveraging model mixtures, can substantially enhance both the performance and robustness of LLMs. Furthermore, we reveal both the consistency and inconsistency in the cognition of LLMs that may facilitate self-reflection and self-improvement, providing a potential direction for developing more powerful coding models.

</details>


### [66] [Skywork-R1V3 Technical Report](https://arxiv.org/abs/2507.06167)

*Wei Shen, Jiangbo Pei, Yi Peng, Xuchen Song, Yang Liu, Jian Peng, Haofeng Sun, Yunzhuo Hao, Peiyu Wang, Yahui Zhou*

**Main category:** cs.CL

**Keywords:** vision-language model, reinforcement learning, multimodal reasoning

**Relevance Score:** 7

**TL;DR:** Skywork-R1V3 is an open-source vision-language model that improves visual reasoning by transferring skills from text-only LLMs through a post-training RL framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to enhance visual reasoning capabilities in multimodal reasoning models by utilizing reasoning skills from LLMs.

**Method:** An advanced open-source VLM model using reinforcement learning (RL) for post-training to enhance reasoning abilities without additional pre-training.

**Key Contributions:**

	1. Introduction of the Skywork-R1V3 vision-language model
	2. Development of a novel post-training RL framework for reasoning skill enhancement
	3. Identification of entropy of reasoning tokens as a checkpoint selection metric

**Result:** Skywork-R1V3 achieved state-of-the-art performance on the MMMU benchmark, improving from 64.3% to 76.0%, indicative of entry-level human capabilities.

**Limitations:** 

**Conclusion:** Skywork-R1V3 demonstrates significant advancements in multimodal reasoning and shows that RL can effectively enhance open-source VLMs.

**Abstract:** We introduce Skywork-R1V3, an advanced, open-source vision-language model (VLM) that pioneers a new approach to visual reasoning. Its key innovation lies in effectively transferring reasoning skills from text-only Large Language Models (LLMs) to visual tasks. The strong performance of Skywork-R1V3 primarily stems from our elaborate post-training RL framework, which effectively activates and enhances the model's reasoning ability, without the need for additional continue pre-training. Through this framework, we further uncover the fundamental role of the connector module in achieving robust cross-modal alignment for multimodal reasoning models. In addition, we introduce a unique indicator of reasoning capability, the entropy of critical reasoning tokens, which has proven highly effective for checkpoint selection during RL training. Skywork-R1V3 achieves state-of-the-art results on MMMU, significantly improving from 64.3% to 76.0%. This performance matches entry-level human capabilities. Remarkably, our RL-powered post-training approach enables even the 38B parameter model to rival top closed-source VLMs. The implementation successfully transfers mathematical reasoning to other subject-related reasoning tasks. We also include an analysis of curriculum learning and reinforcement finetuning strategies, along with a broader discussion on multimodal reasoning. Skywork-R1V3 represents a significant leap in multimodal reasoning, showcasing RL as a powerful engine for advancing open-source VLM capabilities.

</details>


### [67] [CriticLean: Critic-Guided Reinforcement Learning for Mathematical Formalization](https://arxiv.org/abs/2507.06181)

*Zhongyuan Peng, Yifan Yao, Kaijing Ma, Shuyue Guo, Yizhe Li, Yichi Zhang, Chenchen Zhang, Yifan Zhang, Zhouliang Yu, Luming Li, Minghao Liu, Yihang Xia, Jiawei Shen, Yuchen Wu, Yixin Cao, Zhaoxiang Zhang, Wenhao Huang, Jiaheng Liu, Ge Zhang*

**Main category:** cs.CL

**Keywords:** automated theorem proving, semantic fidelity, reinforcement learning

**Relevance Score:** 5

**TL;DR:** CriticLean introduces a new framework for evaluating natural language to formal code translation in automated theorem proving, focusing on semantic fidelity with CriticLeanGPT and benchmarking with CriticLeanBench.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in evaluating whether generated formalizations capture the semantic intent of natural language mathematical statements in automated theorem proving.

**Method:** The authors propose CriticLean, a critic-guided reinforcement learning framework, which includes a model called CriticLeanGPT trained for semantic fidelity assessment, and a benchmark called CriticLeanBench to measure model performance.

**Key Contributions:**

	1. Introduction of CriticLean framework for critic-guided learning
	2. Development of CriticLeanGPT for evaluating semantic fidelity
	3. Creation of FineLeanCorpus, a large diverse dataset for problem assessment

**Result:** CriticLeanGPT shows significant improvements over several strong baseline models in distinguishing semantically correct from incorrect formalizations, demonstrating the effectiveness of the critic phase optimization.

**Limitations:** 

**Conclusion:** The study concludes that enhancing the critic role is crucial for reliable formalizations in automated theorem proving and hopes to inspire further developments in formal mathematical reasoning.

**Abstract:** Translating natural language mathematical statements into formal, executable code is a fundamental challenge in automated theorem proving. While prior work has focused on generation and compilation success, little attention has been paid to the critic phase-the evaluation of whether generated formalizations truly capture the semantic intent of the original problem. In this paper, we introduce CriticLean, a novel critic-guided reinforcement learning framework that elevates the role of the critic from a passive validator to an active learning component. Specifically, first, we propose the CriticLeanGPT, trained via supervised fine-tuning and reinforcement learning, to rigorously assess the semantic fidelity of Lean 4 formalizations. Then, we introduce CriticLeanBench, a benchmark designed to measure models' ability to distinguish semantically correct from incorrect formalizations, and demonstrate that our trained CriticLeanGPT models can significantly outperform strong open- and closed-source baselines. Building on the CriticLean framework, we construct FineLeanCorpus, a dataset comprising over 285K problems that exhibits rich domain diversity, broad difficulty coverage, and high correctness based on human evaluation. Overall, our findings highlight that optimizing the critic phase is essential for producing reliable formalizations, and we hope our CriticLean will provide valuable insights for future advances in formal mathematical reasoning.

</details>


### [68] [DS@GT at CheckThat! 2025: Detecting Subjectivity via Transfer-Learning and Corrective Data Augmentation](https://arxiv.org/abs/2507.06189)

*Maximilian Heil, Dionne Bang*

**Main category:** cs.CL

**Keywords:** subjectivity detection, transfer-learning, data augmentation

**Relevance Score:** 6

**TL;DR:** This paper explores subjectivity detection in English news text using transfer-learning and stylistic data augmentation, finding that specialized encoders and curated augmentation improve model effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to improve the classification of subjective and objective sentences in English news articles by leveraging advanced machine learning techniques.

**Method:** The authors contrast fine-tuning pre-trained encoders with transfer-learning from fine-tuned transformers and introduce a controlled data augmentation pipeline using GPT-4o for generating paraphrases with consistent subjectivity styles.

**Key Contributions:**

	1. Introduction of a controlled augmentation pipeline using GPT-4o
	2. Demonstration that encoder specialization leads to better performance
	3. Empirical evidence showing improvement in robustness through curated data augmentation

**Result:** The study found that transfer-learning from specified encoders outperformed fine-tuning general-purpose ones, and that stylistic data augmentation enhanced model robustness in detecting subjective content.

**Limitations:** 

**Conclusion:** Combining encoder specialization with label-consistent augmentation significantly improves subjectivity detection capabilities.

**Abstract:** This paper presents our submission to Task 1, Subjectivity Detection, of the CheckThat! Lab at CLEF 2025. We investigate the effectiveness of transfer-learning and stylistic data augmentation to improve classification of subjective and objective sentences in English news text. Our approach contrasts fine-tuning of pre-trained encoders and transfer-learning of fine-tuned transformer on related tasks. We also introduce a controlled augmentation pipeline using GPT-4o to generate paraphrases in predefined subjectivity styles. To ensure label and style consistency, we employ the same model to correct and refine the generated samples. Results show that transfer-learning of specified encoders outperforms fine-tuning general-purpose ones, and that carefully curated augmentation significantly enhances model robustness, especially in detecting subjective content. Our official submission placed us $16^{th}$ of 24 participants. Overall, our findings underscore the value of combining encoder specialization with label-consistent augmentation for improved subjectivity detection. Our code is available at https://github.com/dsgt-arc/checkthat-2025-subject.

</details>


### [69] [DS@GT at CheckThat! 2025: Evaluating Context and Tokenization Strategies for Numerical Fact Verification](https://arxiv.org/abs/2507.06195)

*Maximilian Heil, Aleksandar Pramov*

**Main category:** cs.CL

**Keywords:** automated fact-checking, numerical claims, veracity prediction, evidence retrieval, natural language inference

**Relevance Score:** 6

**TL;DR:** This study investigates modeling strategies for automated fact-checking of numerical claims, emphasizing the importance of evidence quality over context length and tokenization methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by numerical claims in automated fact-checking systems.

**Method:** Evaluation of the QuanTemp dataset and a custom evidence retrieval pipeline, testing the effects of context length and right-to-left tokenization on veracity prediction.

**Key Contributions:**

	1. Development of an evidence retrieval pipeline for numerical claims
	2. Evaluation of R2L tokenization effects on natural language inference
	3. Demonstration of the importance of evidence quality in classification tasks

**Result:** The best system achieved a macro-average F1 score of 0.57, ranking among the Top-4 submissions in Task 3 of CheckThat! 2025, with findings showing evidence quality as a critical factor.

**Limitations:** 

**Conclusion:** Longer context windows and R2L tokenization do not significantly improve classification performance; enhancing evidence quality is essential for better veracity predictions.

**Abstract:** Numerical claims, statements involving quantities, comparisons, and temporal references, pose unique challenges for automated fact-checking systems. In this study, we evaluate modeling strategies for veracity prediction of such claims using the QuanTemp dataset and building our own evidence retrieval pipeline. We investigate three key factors: (1) the impact of more evidences with longer input context windows using ModernBERT, (2) the effect of right-to-left (R2L) tokenization, and (3) their combined influence on classification performance. Contrary to prior findings in arithmetic reasoning tasks, R2L tokenization does not boost natural language inference (NLI) of numerical tasks. A longer context window does also not enhance veracity performance either, highlighting evidence quality as the dominant bottleneck. Our best-performing system achieves competitive macro-average F1 score of 0.57 and places us among the Top-4 submissions in Task 3 of CheckThat! 2025. Our code is available at https://github.com/dsgt-arc/checkthat-2025-numerical.

</details>


### [70] [UQLM: A Python Package for Uncertainty Quantification in Large Language Models](https://arxiv.org/abs/2507.06196)

*Dylan Bouchard, Mohit Singh Chauhan, David Skarbrevik, Ho-Kyeong Ra, Viren Bajaj, Zeya Ahmad*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, uncertainty quantification, Python, toolkit

**Relevance Score:** 8

**TL;DR:** UQLM is a Python package designed for detecting hallucinations in Large Language Models (LLMs) using uncertainty quantification techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hallucinations in LLMs, which generate false or misleading content, impacting the safety and trust of applications.

**Method:** The paper presents UQLM, offering UQ-based scorers that compute confidence scores for LLM responses, facilitating easy integration into existing systems.

**Key Contributions:**

	1. Introduction of the UQLM package for hallucination detection
	2. Implementation of state-of-the-art uncertainty quantification techniques
	3. Easy integration for enhancing LLM reliability

**Result:** UQLM provides an off-the-shelf solution for hallucination detection, improving the reliability of LLM outputs with quantifiable confidence metrics.

**Limitations:** 

**Conclusion:** The toolkit enhances trust in LLM-generated content by providing measurable uncertainty, thereby aiding developers in building safer applications.

**Abstract:** Hallucinations, defined as instances where Large Language Models (LLMs) generate false or misleading content, pose a significant challenge that impacts the safety and trust of downstream applications. We introduce UQLM, a Python package for LLM hallucination detection using state-of-the-art uncertainty quantification (UQ) techniques. This toolkit offers a suite of UQ-based scorers that compute response-level confidence scores ranging from 0 to 1. This library provides an off-the-shelf solution for UQ-based hallucination detection that can be easily integrated to enhance the reliability of LLM outputs.

</details>


### [71] [A Survey on Latent Reasoning](https://arxiv.org/abs/2507.06203)

*Rui-Jie Zhu, Tianhao Peng, Tianhao Cheng, Xingwei Qu, Jinfa Huang, Dawei Zhu, Hao Wang, Kaiwen Xue, Xuanliang Zhang, Yong Shan, Tianle Cai, Taylor Kergan, Assel Kembay, Andrew Smith, Chenghua Lin, Binh Nguyen, Yuqi Pan, Yuhong Chou, Zefan Cai, Zhenhe Wu, Yongchi Zhao, Tianyu Liu, Jian Yang, Wangchunshu Zhou, Chujie Zheng, Chongxuan Li, Yuyin Zhou, Zhoujun Li, Zhaoxiang Zhang, Jiaheng Liu, Ge Zhang, Wenhao Huang, Jason Eshraghian*

**Main category:** cs.CL

**Keywords:** Large Language Models, Latent Reasoning, Chain-of-Thought, Neural Networks, Cognition

**Relevance Score:** 8

**TL;DR:** This paper surveys the emerging field of latent reasoning in Large Language Models (LLMs), focusing on methodologies that allow multi-step inference in the model's hidden state without token-level supervision.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of chain-of-thought reasoning in LLMs, particularly its dependence on natural language, by exploring latent reasoning that operates in the model's hidden state.

**Method:** The survey examines foundational neural network layers, diverse latent reasoning methodologies including activation-based recurrence and hidden state propagation, and advanced paradigms like infinite-depth latent reasoning using masked diffusion models.

**Key Contributions:**

	1. Comprehensive overview of latent reasoning methodologies in LLMs
	2. Introduction to advanced paradigms like masked diffusion models
	3. Discussion on the role of neural network layers in supporting reasoning transformations.

**Result:** The paper clarifies the conceptual landscape of latent reasoning and provides insights into future research directions for enhancing LLM cognition.

**Limitations:** 

**Conclusion:** The unification of various latent reasoning methodologies could lead to improved reasoning capabilities in LLMs and encourages further exploration in this area.

**Abstract:** Large Language Models (LLMs) have demonstrated impressive reasoning capabilities, especially when guided by explicit chain-of-thought (CoT) reasoning that verbalizes intermediate steps. While CoT improves both interpretability and accuracy, its dependence on natural language reasoning limits the model's expressive bandwidth. Latent reasoning tackles this bottleneck by performing multi-step inference entirely in the model's continuous hidden state, eliminating token-level supervision. To advance latent reasoning research, this survey provides a comprehensive overview of the emerging field of latent reasoning. We begin by examining the foundational role of neural network layers as the computational substrate for reasoning, highlighting how hierarchical representations support complex transformations. Next, we explore diverse latent reasoning methodologies, including activation-based recurrence, hidden state propagation, and fine-tuning strategies that compress or internalize explicit reasoning traces. Finally, we discuss advanced paradigms such as infinite-depth latent reasoning via masked diffusion models, which enable globally consistent and reversible reasoning processes. By unifying these perspectives, we aim to clarify the conceptual landscape of latent reasoning and chart future directions for research at the frontier of LLM cognition. An associated GitHub repository collecting the latest papers and repos is available at: https://github.com/multimodal-art-projection/LatentCoT-Horizon/.

</details>


### [72] [DS@GT at CheckThat! 2025: Ensemble Methods for Detection of Scientific Discourse on Social Media](https://arxiv.org/abs/2507.06205)

*Ayush Parikh, Hoang Thanh Thanh Truong, Jeanette Schofield, Maximilian Heil*

**Main category:** cs.CL

**Keywords:** Scientific Claims Detection, LLM, Social Media, Machine Learning, Natural Language Processing

**Relevance Score:** 6

**TL;DR:** This paper presents methods for detecting scientific web discourse in tweets, focusing on classifying mentions of scientific claims, studies, and entities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To explore effective methods for detecting scientific discourse in social media and improve information accuracy in public communication of science.

**Method:** The paper discusses three modeling approaches: transformer fine-tuning, few-shot prompting of LLMs, and a combined ensemble model informed by earlier experiments.

**Key Contributions:**

	1. Development of multiple modeling approaches for scientific discourse detection
	2. Improvement over baseline performance in a competitive setting
	3. Open-source codebase for future research and development

**Result:** The team achieved a macro-averaged F1 score of 0.8611, surpassing the DeBERTaV3 baseline score of 0.8375, and placed 7th in the competition.

**Limitations:** 

**Conclusion:** The proposed methods demonstrate effective detection of scientific claims on social media, suggesting promising avenues for future research in this area.

**Abstract:** In this paper, we, as the DS@GT team for CLEF 2025 CheckThat! Task 4a Scientific Web Discourse Detection, present the methods we explored for this task. For this multiclass classification task, we determined if a tweet contained a scientific claim, a reference to a scientific study or publication, and/or mentions of scientific entities, such as a university or a scientist. We present 3 modeling approaches for this task: transformer finetuning, few-shot prompting of LLMs, and a combined ensemble model whose design was informed by earlier experiments. Our team placed 7th in the competition, achieving a macro-averaged F1 score of 0.8611, an improvement over the DeBERTaV3 baseline of 0.8375. Our code is available on Github at https://github.com/dsgt-arc/checkthat-2025-swd/tree/main/subtask-4a.

</details>


### [73] [Efficiency-Effectiveness Reranking FLOPs for LLM-based Rerankers](https://arxiv.org/abs/2507.06223)

*Zhiyuan Peng, Ting-ruen Wei, Tingyu Song, Yilun Zhao, Yi Fang*

**Main category:** cs.CL

**Keywords:** Large Language Models, information retrieval, efficiency-effectiveness tradeoff, FLOPs estimator, reranker

**Relevance Score:** 8

**TL;DR:** The paper proposes new metrics for evaluating the efficiency of LLM-based rerankers in information retrieval, focusing on compute effectiveness and throughput, while also introducing an estimator for FLOPs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of LLM-based rerankers by providing metrics that account for model size and hardware differences, enabling better understanding of efficiency-effectiveness trade-offs.

**Method:** The authors propose E^2R-FLOPs metrics, which include ranking metrics per PetaFLOP (RPP) and queries per PetaFLOP (QPP), along with a FLOPs estimator for LLM-based rerankers that does not require experimental runs.

**Key Contributions:**

	1. Introduction of E^2R-FLOPs metrics for better evaluation of LLM-based rerankers
	2. Development of a FLOPs estimator for LLM-based rerankers without experimental validation
	3. Comprehensive experiments demonstrating the effectiveness of the new metrics

**Result:** Using the introduced metrics, extensive experiments were performed to evaluate various LLM-based rerankers, revealing insights into their efficiency-effectiveness balance.

**Limitations:** The study relies on certain assumptions regarding the architectural diversity of LLMs and may not cover all practical scenarios.

**Conclusion:** The proposed metrics and FLOPs estimator can significantly aid the research community in accurately assessing the performance of LLM-based rerankers, ultimately advancing the field of information retrieval.

**Abstract:** Large Language Models (LLMs) have recently been applied to reranking tasks in information retrieval, achieving strong performance. However, their high computational demands often hinder practical deployment. Existing studies evaluate the efficiency of LLM-based rerankers using proxy metrics such as latency, the number of forward passes, input tokens, and output tokens. However, these metrics depend on hardware and running-time choices (\eg parallel or not, batch size, etc), and often fail to account for model size, making it difficult to interpret and obscuring the evaluation of the efficiency-effectiveness tradeoff. To address this issue, we propose E\textsuperscript{2}R-FLOPs, for LLM-based rerankers: ranking metrics per PetaFLOP (RPP) for relevance per compute and queries per PetaFLOP (QPP) for hardware-agnostic throughput. Companied with the new metrics, an interpretable FLOPs estimator is built to estimate the FLOPs of an LLM-based reranker even without running any experiments. Based on the proposed metrics, we conduct comprehensive experiments to evaluate a wide range of LLM-based rerankers with different architecture, studying the efficiency-effectiveness trade-off and bringing this issue to the attention of the research community.

</details>


### [74] [Agent KB: Leveraging Cross-Domain Experience for Agentic Problem Solving](https://arxiv.org/abs/2507.06229)

*Xiangru Tang, Tianrui Qin, Tianhao Peng, Ziyang Zhou, Daniel Shao, Tingting Du, Xinming Wei, Peng Xia, Fang Wu, He Zhu, Ge Zhang, Jiaheng Liu, Xingyao Wang, Sirui Hong, Chenglin Wu, Hao Cheng, Chi Wang, Wangchunshu Zhou*

**Main category:** cs.CL

**Keywords:** experience reuse, language agents, knowledge transfer

**Relevance Score:** 7

**TL;DR:** Agent KB is a hierarchical experience framework for language agents that improves error correction and experience reuse, enhancing performance on complex tasks.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of language agents that cannot learn from each other's experiences, thus improving their problem-solving capabilities in complex tasks.

**Method:** The study introduces a Reason-Retrieve-Refine pipeline within Agent KB that captures both high-level strategies and execution logs to facilitate cross-agent knowledge transfer.

**Key Contributions:**

	1. Introduction of the Reason-Retrieve-Refine pipeline
	2. Creation of a shared knowledge base for cross-agent learning
	3. Demonstrated significant performance improvements on benchmark tasks

**Result:** Agent KB significantly improves success rates on the GAIA benchmark, with Claude-3 improving from 38.46% to 57.69% and GPT-4 from 53.49% to 73.26%. On SWE-bench, it helps Claude-3 improve from 41.33% to 53.33%.

**Limitations:** 

**Conclusion:** Agent KB offers a modular, framework-agnostic infrastructure for agents to learn from past experiences and generalize effective strategies.

**Abstract:** As language agents tackle increasingly complex tasks, they struggle with effective error correction and experience reuse across domains. We introduce Agent KB, a hierarchical experience framework that enables complex agentic problem solving via a novel Reason-Retrieve-Refine pipeline. Agent KB addresses a core limitation: agents traditionally cannot learn from each other's experiences. By capturing both high-level strategies and detailed execution logs, Agent KB creates a shared knowledge base that enables cross-agent knowledge transfer. Evaluated on the GAIA benchmark, Agent KB improves success rates by up to 16.28 percentage points. On the most challenging tasks, Claude-3 improves from 38.46% to 57.69%, while GPT-4 improves from 53.49% to 73.26% on intermediate tasks. On SWE-bench code repair, Agent KB enables Claude-3 to improve from 41.33% to 53.33%. Our results suggest that Agent KB provides a modular, framework-agnostic infrastructure for enabling agents to learn from past experiences and generalize successful strategies to new tasks.

</details>


### [75] [The distribution of syntactic dependency distances](https://arxiv.org/abs/2211.14620)

*Sonia Petrini, Ramon Ferrer-i-Cancho*

**Main category:** cs.CL

**Keywords:** syntactic structure, dependency distances, two-regime model

**Relevance Score:** 4

**TL;DR:** This paper proposes a new model for characterizing syntactic dependency distances in sentences, demonstrating that a two-regime model best fits various languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To characterize the distribution of syntactic dependency distances, which has been suggested to follow a power-law distribution, and to propose a model that captures this behavior.

**Method:** A new two-regime model is introduced where the probability decay of syntactic dependency distances changes after a breakpoint, analyzed across 20 languages with varying sentence lengths and annotation styles.

**Key Contributions:**

	1. Proposed a new two-regime model for syntactic dependency distances.
	2. Showed that the breakpoint in the dependency distance consistently averages 4-5 words across languages.
	3. Related the model to a universal processing mechanism for syntactic dependencies.

**Result:** The two-regime model is favored across all languages studied, with a break-point averaging 4-5 words, suggesting that similar processing limits exist across different languages.

**Limitations:** 

**Conclusion:** The findings support a universal processing mechanism for word chunks to higher structures, showing consistent properties of syntactic dependencies across languages.

**Abstract:** The syntactic structure of a sentence can be represented as a graph, where vertices are words and edges indicate syntactic dependencies between them. In this setting, the distance between two linked words is defined as the difference between their positions. Here we wish to contribute to the characterization of the actual distribution of syntactic dependency distances, which has previously been argued to follow a power-law distribution. Here we propose a new model with two exponential regimes in which the probability decay is allowed to change after a break-point. This transition could mirror the transition from the processing of word chunks to higher-level structures. We find that a two-regime model - where the first regime follows either an exponential or a power-law decay - is the most likely one in all 20 languages we considered, independently of sentence length and annotation style. Moreover, the break-point exhibits low variation across languages and averages values of 4-5 words, suggesting that the amount of words that can be simultaneously processed abstracts from the specific language to a high degree. The probability decay slows down after the breakpoint, consistently with a universal chunk-and-pass mechanism. Finally, we give an account of the relation between the best estimated model and the closeness of syntactic dependencies as function of sentence length, according to a recently introduced optimality score.

</details>


### [76] [Detecting value-expressive text posts in Russian social media](https://arxiv.org/abs/2312.08968)

*Maria Milkova, Maksim Rudnev, Lidia Okolskaya*

**Main category:** cs.CL

**Keywords:** social media, value detection, machine learning, transformer models, natural language processing

**Relevance Score:** 5

**TL;DR:** This paper investigates how to detect value-expressive content in Russian social media using an AI-assisted approach to improve upon traditional survey methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand societal values through personal values expressed on social media, particularly in populations that are hard to reach through traditional surveys.

**Method:** A model was developed to detect value-expressive posts in VKontakte using a training dataset of 5,035 annotated posts, combining human experts and AI (ChatGPT) for annotation and employing various transformer-based language model embeddings for classification.

**Key Contributions:**

	1. Development of a model for detecting value-expressive posts on VKontakte
	2. Integration of human and AI-assisted annotation for improved classification
	3. Achievement of high detection quality using transformer-based embeddings.

**Result:** The best-performing model was based on the fine-tuned rubert-tiny2 model, achieving value detection quality with an F1 score of 0.75 and F1-macro of 0.80.

**Limitations:** Moderate agreement among human annotators and challenges in spam detection by ChatGPT.

**Conclusion:** This work advances the detection of values expressed on Russian social media, providing insights into the evolution of societal values.

**Abstract:** Basic values are concepts or beliefs which pertain to desirable end-states and transcend specific situations. Studying personal values in social media can illuminate how and why societal values evolve especially when the stimuli-based methods, such as surveys, are inefficient, for instance, in hard-to-reach populations. On the other hand, user-generated content is driven by the massive use of stereotyped, culturally defined speech constructions rather than authentic expressions of personal values. We aimed to find a model that can accurately detect value-expressive posts in Russian social media VKontakte. A training dataset of 5,035 posts was annotated by three experts, 304 crowd-workers and ChatGPT. Crowd-workers and experts showed only moderate agreement in categorizing posts. ChatGPT was more consistent but struggled with spam detection. We applied an ensemble of human- and AI-assisted annotation involving active learning approach, subsequently trained several classification models using embeddings from various pre-trained transformer-based language models. The best performance was achieved with embeddings from a fine-tuned rubert-tiny2 model, yielding high value detection quality (F1 = 0.75, F1-macro = 0.80). This model provides a crucial step to a study of values within and between Russian social media users.

</details>


### [77] [MEIT: Multimodal Electrocardiogram Instruction Tuning on Large Language Models for Report Generation](https://arxiv.org/abs/2403.04945)

*Zhongwei Wan, Che Liu, Xin Wang, Chaofan Tao, Hui Shen, Jing Xiong, Rossella Arcucci, Huaxiu Yao, Mi Zhang*

**Main category:** cs.CL

**Keywords:** ECG report generation, LLM, multimodal instructions, clinical application, automated report generation

**Relevance Score:** 9

**TL;DR:** The paper proposes the Multimodal ECG Instruction Tuning (MEIT) framework to automate ECG report generation using LLMs, establishing benchmarks for evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate the time-consuming process of ECG report generation, which requires clinical expertise.

**Method:** A framework called Multimodal ECG Instruction Tuning (MEIT) was proposed, evaluated with various LLMs across two large ECG datasets using over 800,000 reports.

**Key Contributions:**

	1. Introduction of the Multimodal ECG Instruction Tuning (MEIT) framework
	2. Establishing a benchmark for evaluating LLMs in ECG report generation
	3. Demonstrating the effectiveness of LLMs in clinical applications for report generation

**Result:** MEIT showcases superior performance in quality report generation and resilience to signal perturbation, demonstrating effectiveness in real-world clinical applications.

**Limitations:** 

**Conclusion:** The findings highlight the potential of LLMs in automating ECG report generation and improving clinical workflows.

**Abstract:** Electrocardiogram (ECG) is the primary non-invasive diagnostic tool for monitoring cardiac conditions and is crucial in assisting clinicians. Recent studies have concentrated on classifying cardiac conditions using ECG data but have overlooked ECG report generation, which is time-consuming and requires clinical expertise. To automate ECG report generation and ensure its versatility, we propose the Multimodal ECG Instruction Tuning (MEIT) framework, the first attempt to tackle ECG report generation with LLMs and multimodal instructions. To facilitate future research, we establish a benchmark to evaluate MEIT with various LLMs backbones across two large-scale ECG datasets. Our approach uniquely aligns the representations of the ECG signal and the report, and we conduct extensive experiments to benchmark MEIT with nine open-source LLMs using more than 800,000 ECG reports. MEIT's results underscore the superior performance of instruction-tuned LLMs, showcasing their proficiency in quality report generation, zero-shot capabilities, resilience to signal perturbation, and alignment with human expert evaluation. These findings emphasize the efficacy of MEIT and its potential for real-world clinical application.

</details>


### [78] [News and Load: Social and Economic Drivers of Regional Multi-horizon Electricity Demand Forecasting](https://arxiv.org/abs/2406.06641)

*Yun Bai, Simon Camal, Andrea Michiorri*

**Main category:** cs.CL

**Keywords:** electricity demand, natural language processing, social indicators, economic factors, forecasting

**Relevance Score:** 4

**TL;DR:** This paper investigates the influence of social factors on electricity demand, integrating natural language processing to analyze news data alongside economic variables.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To deepen the understanding of how social aspects impact electricity demand and improve forecasting methods.

**Method:** Utilized natural language processing on a large corpus of news articles, combined with economic variables such as GDP, unemployment, and inflation, across five regions in the UK and Ireland.

**Key Contributions:**

	1. Integration of social factors into energy demand models using NLP
	2. Demonstrated causal relationships between textual features and electricity demand
	3. Improved forecasting accuracy with the inclusion of social indicators

**Result:** Textual features extracted from news data show causal relationships with regional electricity demand, with significant regional variations in influence from social versus economic factors.

**Limitations:** 

**Conclusion:** Incorporating social indicators into electricity demand forecasting enhances accuracy by about 6%, with varying levels of influence depending on the region studied.

**Abstract:** The relationship between electricity demand and variables such as economic activity and weather patterns is well established. However, this paper explores the connection between electricity demand and social aspects. It further embeds dynamic information about the state of society into energy demand modelling and forecasting approaches. Through the use of natural language processing on a large news corpus, we highlight this important link. This study is conducted in five regions of the UK and Ireland and considers multiple time horizons from 1 to 30 days. It also considers economic variables such as GDP, unemployment and inflation. The textual features used in this study represent central constructs from the word frequencies, topics, word embeddings extracted from the news. The findings indicate that: 1) the textual features are related to various contents, such as military conflicts, transportation, the global pandemic, regional economics, and the international energy market. They exhibit causal relationships with regional electricity demand, which are validated using Granger causality and Double Machine Learning methods. 2) Economic indicators play a more important role in the East Midlands and Northern Ireland, while social indicators are more influential in the West Midlands and the South West of England. 3) The use of these factors improves deterministic forecasting by around 6%.

</details>


### [79] [Healing Powers of BERT: How Task-Specific Fine-Tuning Recovers Corrupted Language Models](https://arxiv.org/abs/2406.14459)

*Shijie Han, Zhenyu Zhang, Andrei Arsene Simion*

**Main category:** cs.CL

**Keywords:** language models, BERT, parameter corruption, robustness, NLP systems

**Relevance Score:** 6

**TL;DR:** This paper investigates the robustness of BERT and its variants to parameter corruption and recovery through fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how parameter corruption affects the performance of language models like BERT and to inform the development of resilient NLP systems.

**Method:** The authors strategically corrupt BERT variants at different levels and analyze their performance recovery through fine-tuning.

**Key Contributions:**

	1. Exploration of parameter corruption and recovery in BERT variants.
	2. Finding that bottom-layer corruption is more detrimental than top-layer corruption.
	3. Insights into developing resilient NLP systems.

**Result:** Corrupted models struggle to recover their original performance, with more severe degradation resulting from higher levels of corruption. Bottom-layer corruption is more detrimental than top-layer corruption.

**Limitations:** The study primarily focuses on BERT variants and may not generalize to all language models.

**Conclusion:** The study provides insights into language model robustness and adaptability, highlighting the importance of foundational linguistic features in maintaining model performance.

**Abstract:** Language models like BERT excel at sentence classification tasks due to extensive pre-training on general data, but their robustness to parameter corruption is unexplored. To understand this better, we look at what happens if a language model is "broken", in the sense that some of its parameters are corrupted and then recovered by fine-tuning. Strategically corrupting BERT variants at different levels, we find corrupted models struggle to fully recover their original performance, with higher corruption causing more severe degradation. Notably, bottom-layer corruption affecting fundamental linguistic features is more detrimental than top-layer corruption. Our insights contribute to understanding language model robustness and adaptability under adverse conditions, informing strategies for developing resilient NLP systems against parameter perturbations.

</details>


### [80] [A Multi-Task and Multi-Label Classification Model for Implicit Discourse Relation Recognition](https://arxiv.org/abs/2408.08971)

*Nelson Filipe Costa, Leila Kosseim*

**Main category:** cs.CL

**Keywords:** multi-label classification, discourse relation recognition, PDTB 3.0, transfer learning, NLP

**Relevance Score:** 6

**TL;DR:** A novel multi-label classification approach for implicit discourse relation recognition (IDRR), establishing a benchmark and achieving state-of-the-art results in single-label IDRR.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve implicit discourse relation recognition using a multi-label approach, and to establish a benchmark for this task.

**Method:** A multi-task model that learns multi-label representations of implicit discourse relations based on the PDTB 3.0 framework, with adaptations for single-label settings.

**Key Contributions:**

	1. Establishes a benchmark for multi-label implicit discourse relation recognition (IDRR).
	2. Achieves state-of-the-art results for traditional single-label IDRR using the DiscoGeM model.
	3. Presents the first analysis of transfer learning between the DiscoGeM and PDTB 3.0 corpora for IDRR.

**Result:** The proposed model sets the first benchmark for multi-label IDRR and achieves state-of-the-art results in single-label IDRR when evaluated on the PDTB 3.0 corpus.

**Limitations:** 

**Conclusion:** The model demonstrates the effectiveness of multi-label classification for IDRR and provides insights into transfer learning between different corpora.

**Abstract:** We propose a novel multi-label classification approach to implicit discourse relation recognition (IDRR). Our approach features a multi-task model that jointly learns multi-label representations of implicit discourse relations across all three sense levels in the PDTB 3.0 framework. The model can also be adapted to the traditional single-label IDRR setting by selecting the sense with the highest probability in the multi-label representation. We conduct extensive experiments to identify optimal model configurations and loss functions in both settings. Our approach establishes the first benchmark for multi-label IDRR and achieves SOTA results on single-label IDRR using DiscoGeM. Finally, we evaluate our model on the PDTB 3.0 corpus in the single-label setting, presenting the first analysis of transfer learning between the DiscoGeM and PDTB 3.0 corpora for IDRR.

</details>


### [81] [What Would You Ask When You First Saw $a^2+b^2=c^2$? Evaluating LLM on Curiosity-Driven Questioning](https://arxiv.org/abs/2409.17172)

*Shashidhar Reddy Javaji, Zining Zhu*

**Main category:** cs.CL

**Keywords:** large language models, knowledge acquisition, evaluation framework, science education, question generation

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel evaluation framework for assessing large language models' (LLMs) ability to acquire new knowledge by generating questions from provided scientific statements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand LLMs' potential for knowledge acquisition, which has been largely unexplored.

**Method:** The framework prompts LLMs to generate questions about scientific statements and scores the quality of those questions. Controlled ablation studies were utilized for validation, and a synthetic dataset across various disciplines was created for evaluation.

**Key Contributions:**

	1. Introduction of a novel evaluation framework for LLM knowledge acquisition.
	2. Creation of a synthetic dataset for testing across different difficulty levels.
	3. Human evaluation results that validate the proposed scoring methodology.

**Result:** The study finds that larger models generate coherent questions but also shows that smaller models like Phi-2 can perform equally or more effectively, indicating that model size alone does not determine knowledge acquisition potential.

**Limitations:** Focuses only on knowledge acquisition through question generation, not exploring other potential capabilities or methods.

**Conclusion:** The proposed evaluation framework quantifies a critical capability in LLMs and suggests new avenues for enhancing AI knowledge acquisition.

**Abstract:** Large language models (LLMs) can store a massive amount of knowledge, yet their potential to acquire new knowledge remains unknown. We propose a novel evaluation framework that evaluates this capability. This framework prompts LLMs to generate questions about a statement introducing scientific knowledge, simulating a curious person when facing the statement for the first time. We score the qualities of the generated questions, thereby evaluating the knowledge acquisition potential of the LLM. We apply controlled ablation studies to validate our scoring procedures. Additionally, we created a synthetic dataset consisting of 1101 statements in physics, chemistry, and maths with distinct levels of difficulties, 300 general knowledge statements, and 567 incorrect statements. Human evaluations were conducted to validate our model assessments, achieving an approximate weighted Cohen's kappa of 0.7 on all three metrics considered. We find that while large models like GPT-4 and Mistral 8x7b are adept at generating coherent and relevant questions, the smaller Phi-2 model is equally or more effective. This indicates that size does not solely determine a model's knowledge acquisition potential. The proposed framework quantifies a critical model capability that was commonly overlooked and opens up research opportunities for developing more knowledgeable AI systems

</details>


### [82] [Evaluation of OpenAI o1: Opportunities and Challenges of AGI](https://arxiv.org/abs/2409.18486)

*Tianyang Zhong, Zhengliang Liu, Yi Pan, Yutong Zhang, Yifan Zhou, Shizhe Liang, Zihao Wu, Yanjun Lyu, Peng Shu, Xiaowei Yu, Chao Cao, Hanqi Jiang, Hanxu Chen, Yiwei Li, Junhao Chen, Huawen Hu, Yiheng Liu, Huaqin Zhao, Shaochen Xu, Haixing Dai, Lin Zhao, Ruidong Zhang, Wei Zhao, Zhenyuan Yang, Jingyuan Chen, Peilong Wang, Wei Ruan, Hui Wang, Huan Zhao, Jing Zhang, Yiming Ren, Shihuan Qin, Tong Chen, Jiaxi Li, Arif Hassan Zidan, Afrar Jahin, Minheng Chen, Sichen Xia, Jason Holmes, Yan Zhuang, Jiaqi Wang, Bochen Xu, Weiran Xia, Jichao Yu, Kaibo Tang, Yaxuan Yang, Bolun Sun, Tao Yang, Guoyu Lu, Xianqiao Wang, Lilong Chai, He Li, Jin Lu, Xin Zhang, Bao Ge, Xintao Hu, Lian Zhang, Hua Zhou, Lu Zhang, Shu Zhang, Zhen Xiang, Yudan Ren, Jun Liu, Xi Jiang, Yu Bao, Wei Zhang, Xiang Li, Gang Li, Wei Liu, Dinggang Shen, Andrea Sikora, Xiaoming Zhai, Dajiang Zhu, Tuo Zhang, Tianming Liu*

**Main category:** cs.CL

**Keywords:** large language model, reasoning tasks, artificial general intelligence

**Relevance Score:** 9

**TL;DR:** This study evaluates OpenAI's o1-preview large language model across various reasoning tasks, showcasing high performance in multiple domains including medicine and mathematics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the capabilities of OpenAI's o1-preview model in solving complex tasks across diverse fields and to evaluate its potential progress towards artificial general intelligence.

**Method:** Rigorous testing across domains such as computer science, medicine, and social sciences, focusing on various reasoning tasks including coding challenges and scientific reasoning.

**Key Contributions:**

	1. High success rate in competitive programming surpassing human experts
	2. Superior performance in generating medical reports
	3. 100% accuracy in mathematical reasoning tasks

**Result:** The o1-preview model achieved an 83.3% success rate in competitive programming, superior accuracy in radiology report generation, and 100% in high school-level math tasks, excelling in various specialized domains.

**Limitations:** Occasional errors on simpler problems and challenges with highly specialized concepts.

**Conclusion:** The model demonstrates significant advancements in reasoning and knowledge integration, indicating progress towards artificial general intelligence despite some limitations in specific areas.

**Abstract:** This comprehensive study evaluates the performance of OpenAI's o1-preview large language model across a diverse array of complex reasoning tasks, spanning multiple domains, including computer science, mathematics, natural sciences, medicine, linguistics, and social sciences. Through rigorous testing, o1-preview demonstrated remarkable capabilities, often achieving human-level or superior performance in areas ranging from coding challenges to scientific reasoning and from language processing to creative problem-solving. Key findings include:   -83.3% success rate in solving complex competitive programming problems, surpassing many human experts.   -Superior ability in generating coherent and accurate radiology reports, outperforming other evaluated models.   -100% accuracy in high school-level mathematical reasoning tasks, providing detailed step-by-step solutions.   -Advanced natural language inference capabilities across general and specialized domains like medicine.   -Impressive performance in chip design tasks, outperforming specialized models in areas such as EDA script generation and bug analysis.   -Remarkable proficiency in anthropology and geology, demonstrating deep understanding and reasoning in these specialized fields.   -Strong capabilities in quantitative investing. O1 has comprehensive financial knowledge and statistical modeling skills.   -Effective performance in social media analysis, including sentiment analysis and emotion recognition.   The model excelled particularly in tasks requiring intricate reasoning and knowledge integration across various fields. While some limitations were observed, including occasional errors on simpler problems and challenges with certain highly specialized concepts, the overall results indicate significant progress towards artificial general intelligence.

</details>


### [83] [Joint Beamforming and Speaker-Attributed ASR for Real Distant-Microphone Meeting Transcription](https://arxiv.org/abs/2410.21849)

*Can Cui, Imran Ahamad Sheikh, Mostafa Sadeghi, Emmanuel Vincent*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, beamforming, machine learning, meeting transcription, neural networks

**Relevance Score:** 5

**TL;DR:** This paper presents a joint beamforming and speaker-attributed automatic speech recognition (SA-ASR) approach to improve distant-microphone meeting transcription.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing end-to-end SA-ASR architectures due to the lack of effective multichannel noise and reverberation reduction techniques.

**Method:** The authors introduce a method for data alignment and augmentation to pretrain a neural beamformer on real meeting data, followed by comparisons of fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model, culminating in a joint optimization of the beamformer and SA-ASR.

**Key Contributions:**

	1. Introduction of a joint beamforming and SA-ASR approach.
	2. Development of a data alignment and augmentation method for neural beamformer pretraining.
	3. Empirical comparison showcasing performance improvements over traditional methods.

**Result:** The proposed methods, especially the fine-tuning of SA-ASR on outputs from the fixed beamformer and the joint fine-tuning with the neural beamformer, achieved a reduction in word error rate by 8% and 9%, respectively, on the real AMI corpus.

**Limitations:** 

**Conclusion:** The integration of a neural beamformer with SA-ASR significantly enhances performance, demonstrating the necessity of addressing noise and reverberation in meeting transcription tasks.

**Abstract:** Distant-microphone meeting transcription is a challenging task. State-of-the-art end-to-end speaker-attributed automatic speech recognition (SA-ASR) architectures lack a multichannel noise and reverberation reduction front-end, which limits their performance. In this paper, we introduce a joint beamforming and SA-ASR approach for real meeting transcription. We first describe a data alignment and augmentation method to pretrain a neural beamformer on real meeting data. We then compare fixed, hybrid, and fully neural beamformers as front-ends to the SA-ASR model. Finally, we jointly optimize the fully neural beamformer and the SA-ASR model. Experiments on the real AMI corpus show that, while state-of-the-art multi-frame cross-channel attention based channel fusion fails to improve ASR performance, fine-tuning SA-ASR on the fixed beamformer's output and jointly fine-tuning SA-ASR with the neural beamformer reduce the word error rate by 8% and 9% relative, respectively.

</details>


### [84] [One fish, two fish, but not the whole sea: Alignment reduces language models' conceptual diversity](https://arxiv.org/abs/2411.04427)

*Sonia K. Murthy, Tomer Ullman, Jennifer Hu*

**Main category:** cs.CL

**Keywords:** large language models, conceptual diversity, alignment, behavioral research, human-like representation

**Relevance Score:** 7

**TL;DR:** This paper investigates the conceptual diversity of large language models (LLMs) in behavioral research, comparing non-aligned and aligned models to human models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is growing interest in using LLMs for behavioral research, raising concerns about their ability to represent human-like conceptual diversity and the impact of post-training alignment on this diversity.

**Method:** The authors propose a new metric that relates the internal variability of simulated individuals from LLMs to the overall population-level variability, comparing diverse model types across domains with human behavioral data.

**Key Contributions:**

	1. Introduced a new method for measuring conceptual diversity in LLMs.
	2. Evaluated the diversity of aligned vs. non-aligned LLMs in behavioral domains.
	3. Highlighted the trade-off between value alignment and conceptual diversity in LLMs.

**Result:** While none of the models achieved human-like diversity, aligned LLMs exhibited lower diversity than those fine-tuned for instructions, indicating trade-offs in increasing value alignment versus maintaining conceptual diversity.

**Limitations:** The study does not claim that any model fully captures human-like diversity, and findings may vary across additional domains not tested.

**Conclusion:** The study suggests that aligning models with human values may compromise their conceptual diversity, which is crucial for accurately modeling human behavior.

**Abstract:** Researchers in social science and psychology have recently proposed using large language models (LLMs) as replacements for humans in behavioral research. In addition to arguments about whether LLMs accurately capture population-level patterns, this has raised questions about whether LLMs capture human-like conceptual diversity. Separately, it is debated whether post-training alignment (RLHF or RLAIF) affects models' internal diversity. Inspired by human studies, we use a new way of measuring the conceptual diversity of synthetically-generated LLM "populations" by relating the internal variability of simulated individuals to the population-level variability. We use this approach to evaluate non-aligned and aligned LLMs on two domains with rich human behavioral data. While no model reaches human-like diversity, aligned models generally display less diversity than their instruction fine-tuned counterparts. Our findings highlight potential trade-offs between increasing models' value alignment and decreasing the diversity of their conceptual representations.

</details>


### [85] [Are LLMs Prescient? A Continuous Evaluation using Daily News as the Oracle](https://arxiv.org/abs/2411.08324)

*Hui Dai, Ryan Teehan, Mengye Ren*

**Main category:** cs.CL

**Keywords:** Large Language Models, temporal generalization, future event prediction, continuous evaluation, Retrieval Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper introduces Daily Oracle, a benchmark for evaluating LLMs that focuses on future event prediction to measure their temporal generalization and forecasting capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for an evaluation benchmark that adapts to the evolving capabilities of Large Language Models (LLMs) and assesses their performance over time is critical as existing benchmarks quickly become outdated.

**Method:** The proposed methodology uses future event prediction as a continuous evaluation method by automatically generating question-answer pairs from daily news events.

**Key Contributions:**

	1. Introduction of the Daily Oracle benchmark for continuous evaluation of LLMs.
	2. Demonstration of LLM performance degradation over time due to outdated training data.
	3. Analysis of the role of RAG in improving prediction accuracy alongside the need for model updates.

**Result:** The findings indicate that LLMs' performance declines over time as their pre-training data ages. While Retrieval Augmented Generation (RAG) can improve prediction accuracy, performance degradation still occurs, necessitating regular model updates.

**Limitations:** The benchmark relies on the availability and accuracy of daily news for generating QA pairs, which may introduce variability in evaluation.

**Conclusion:** The study emphasizes the importance of continuous evaluation and highlights the limitations of static benchmarks in measuring the evolving capabilities of LLMs.

**Abstract:** Many existing evaluation benchmarks for Large Language Models (LLMs) quickly become outdated due to the emergence of new models and training data. These benchmarks also fall short in assessing how LLM performance changes over time, as they consist of a static set of questions without a temporal dimension. To address these limitations, we propose using future event prediction as a continuous evaluation method to assess LLMs' temporal generalization and forecasting abilities. Our benchmark, Daily Oracle, automatically generates question-answer (QA) pairs from daily news, challenging LLMs to predict "future" event outcomes. Our findings reveal that as pre-training data becomes outdated, LLM performance degrades over time. While Retrieval Augmented Generation (RAG) has the potential to enhance prediction accuracy, the performance degradation pattern persists, highlighting the need for continuous model updates. Code and data are available at https://agenticlearning.ai/daily-oracle.

</details>


### [86] [Rethinking Associative Memory Mechanism in Induction Head](https://arxiv.org/abs/2412.11459)

*Shuo Wang, Issei Sato*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-context Learning, Transformers, Bigram Model, Associative Memory

**Relevance Score:** 8

**TL;DR:** This paper investigates how two-layer transformers capture in-context information and balance it with pretrained bigram knowledge in next token prediction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand the coordination of in-context information over long contexts and the utilization of global knowledge in large language models, which is not well understood.

**Method:** The authors analyze weight matrices in attention layers and the resulting logits when transformers process prompts generated by a bigram model, followed by specific experimental prompts to evaluate actual outputs against theoretical expectations.

**Key Contributions:**

	1. Theoretical analysis of transformer weight matrices and logits representation.
	2. Experimental evaluation of transformer outputs against theoretical models.
	3. Insights into the balance of in-context information and pretrained knowledge.

**Result:** The experiments reveal the alignment between the outputs of the transformer and theoretical predictions based on the analysis of associative memory.

**Limitations:** 

**Conclusion:** Understanding the interaction between in-context learning and pretrained knowledge enhances our grasp of LLM capabilities, influencing their application in various domains.

**Abstract:** Induction head mechanism is a part of the computational circuits for in-context learning (ICL) that enable large language models (LLMs) to adapt to new tasks without fine-tuning. Most existing work explains the training dynamics behind acquiring such a powerful mechanism. However, the model's ability to coordinate in-context information over long contexts and global knowledge acquired during pretraining remains poorly understood. This paper investigates how a two-layer transformer thoroughly captures in-context information and balances it with pretrained bigram knowledge in next token prediction, from the viewpoint of associative memory. We theoretically analyze the representation of weight matrices in attention layers and the resulting logits when a transformer is given prompts generated by a bigram model. In the experiments, we design specific prompts to evaluate whether the outputs of the trained transformer align with the theoretical results.

</details>


### [87] [Tractable Transformers for Flexible Conditional Generation](https://arxiv.org/abs/2502.07616)

*Anji Liu, Xuejie Liu, Dayuan Zhao, Mathias Niepert, Yitao Liang, Guy Van den Broeck*

**Main category:** cs.CL

**Keywords:** Non-autoregressive models, Conditional generation, Tracformer

**Relevance Score:** 8

**TL;DR:** This paper introduces Tractable Transformers (Tracformer), a new generative model that improves conditional generation by incorporating local and global contextual information.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in conditional generation performance found in non-autoregressive models compared to autoregressive models.

**Method:** The paper proposes Tracformer, a Transformer-based model that uses a sparse encoder for local and global context capture, facilitating better conditional generation tasks.

**Key Contributions:**

	1. Introduction of the Tractable Transformers (Tracformer) model
	2. Use of a sparse Transformer encoder for improved context capture
	3. Demonstrated state-of-the-art performance in conditional generation tasks

**Result:** Tracformers demonstrate state-of-the-art performance in conditional generation on text modeling compared to previous diffusion and autoregressive models.

**Limitations:** 

**Conclusion:** Tracformers improve the robustness and quality of conditional generation tasks, showing that strong unconditional performance doesn't always translate to conditional effectiveness.

**Abstract:** Non-autoregressive (NAR) generative models are valuable because they can handle diverse conditional generation tasks in a more principled way than their autoregressive (AR) counterparts, which are constrained by sequential dependency requirements. Recent advancements in NAR models, such as diffusion language models, have demonstrated superior performance in unconditional generation compared to AR models (e.g., GPTs) of similar sizes. However, such improvements do not always lead to improved conditional generation performance. We show that a key reason for this gap is the difficulty in generalizing to conditional probability queries (i.e., the set of unknown variables) unseen during training. As a result, strong unconditional generation performance does not guarantee high-quality conditional generation. This paper proposes Tractable Transformers (Tracformer), a Transformer-based generative model that is more robust to different conditional generation tasks. Unlike existing models that rely solely on global contextual features derived from full inputs, Tracformers incorporate a sparse Transformer encoder to capture both local and global contextual information. This information is routed through a decoder for conditional generation. Empirical results demonstrate that Tracformers achieve state-of-the-art conditional generation performance on text modeling compared to recent diffusion and AR model baselines.

</details>


### [88] [Early-Exit and Instant Confidence Translation Quality Estimation](https://arxiv.org/abs/2502.14429)

*Vil√©m Zouhar, Maike Z√ºfle, Beni Egressy, Julius Cheng, Mrinmaya Sachan, Jan Niehues*

**Main category:** cs.CL

**Keywords:** quality estimation, machine translation, uncertainty estimation, computational efficiency, Early-Exit COMET

**Relevance Score:** 8

**TL;DR:** This paper presents two models, Instant Confidence COMET and Early-Exit COMET, that enhance quality estimation for machine translation by reducing computational costs and providing uncertainty estimation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient and cost-effective quality estimation methods in machine translation for both evaluation and generation tasks.

**Method:** We introduce Instant Confidence COMET for uncertainty-aware quality estimation and Early-Exit COMET for early computation of quality scores, combined with a bandit algorithm for candidate selection in reranking tasks.

**Key Contributions:**

	1. Development of Instant Confidence COMET for cost-efficient uncertainty estimation
	2. Introduction of Early-Exit COMET for early score computation
	3. Application of bandit algorithms for effective candidate reranking

**Result:** Our methods achieve a 50% reduction in computational costs while maintaining performance levels similar to prior approaches.

**Limitations:** 

**Conclusion:** The proposed models enable more efficient quality estimation and focus human evaluators on the most relevant translations, significantly decreasing the computational burden without compromising accuracy.

**Abstract:** Quality estimation is omnipresent in machine translation, for both evaluation and generation. Unfortunately, quality estimation models are often opaque and computationally expensive, making them impractical to be part of large-scale pipelines. In this work, we tackle two connected challenges: (1) reducing the cost of quality estimation at scale, and (2) developing an inexpensive uncertainty estimation method for quality estimation. To address the latter, we introduce Instant Confidence COMET, an uncertainty-aware quality estimation model that matches the performance of previous approaches at a fraction of their costs. We extend this to Early-Exit COMET, a quality estimation model that can compute quality scores and associated confidences already at early model layers, allowing us to early-exit computations and reduce evaluation costs. We also apply our model to machine translation reranking. We combine Early-Exit COMET with an upper confidence bound bandit algorithm to find the best candidate from a large pool without having to run the full evaluation model on all candidates. In both cases (evaluation and reranking) our methods reduce the required compute by 50% with very little degradation in performance. Finally, we show how Instant Confidence COMET can be used to decide which translations a human evaluator should score rather than relying on the COMET score.

</details>


### [89] [MAMUT: A Novel Framework for Modifying Mathematical Formulas for the Generation of Specialized Datasets for Language Model Training](https://arxiv.org/abs/2502.20855)

*Jonathan Drechsel, Anja Reusch, Steffen Herbold*

**Main category:** cs.CL

**Keywords:** mathematics, LaTeX, transformer models, dataset generation, mathematical retrieval

**Relevance Score:** 4

**TL;DR:** This study introduces Math Mutator (MAMUT), a framework that generates equivalent and falsified LaTeX versions of mathematical formulas to improve mathematical notation processing in transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome challenges faced by transformer models in processing and understanding complex mathematical notation and to enhance mathematical content encoding through specialized training datasets.

**Method:** Development of a framework (MAMUT) to generate diverse LaTeX representations of mathematical formulas and creation of four large datasets for training models on mathematical retrieval tasks.

**Key Contributions:**

	1. Introduction of Math Mutator (MAMUT) framework for generating LaTeX mathematical formulas.
	2. Creation of four large datasets with diverse mathematical notation.
	3. Demonstration of state-of-the-art performance in mathematical retrieval tasks using these datasets.

**Result:** Models trained on the generated datasets achieved state-of-the-art performance in mathematical retrieval tasks.

**Limitations:** 

**Conclusion:** The proposed framework and datasets significantly improve model performance in understanding mathematical notation, facilitating better integration into scientific applications.

**Abstract:** Mathematical formulas are a fundamental and widely used component in various scientific fields, serving as a universal language for expressing complex concepts and relationships. While state-of-the-art transformer models excel in processing and understanding natural language, they encounter challenges with mathematical notation, which involves a complex structure and diverse representations. This study focuses on the development of specialized training datasets to enhance the encoding of mathematical content. We introduce Math Mutator (MAMUT), a framework capable of generating equivalent and falsified versions of a given mathematical formula in LaTeX notation, effectively capturing the mathematical variety in notation of the same concept. Based on MAMUT, we have generated four large mathematical datasets containing diverse notation. Experiments show that models trained on these datasets exhibit new SoTA performance on mathematical retrieval tasks. We publish our code, generated datasets, and pretrained mathematical models: https://github.com/aieng-lab/math-mutator.

</details>


### [90] [Enhancing LLM Reliability via Explicit Knowledge Boundary Modeling](https://arxiv.org/abs/2503.02233)

*Hang Zheng, Hongshen Xu, Yuncong Liu, Lu Chen, Pascale Fung, Kai Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination, Explicit Knowledge Boundary Modeling, Self-awareness, Computational Efficiency

**Relevance Score:** 8

**TL;DR:** The EKBM framework improves LLM reliability and usability by integrating fast and slow reasoning for processing queries exceeding their knowledge limits.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the hallucination issues in LLMs arising from misaligned self-awareness, particularly under conditions exceeding their knowledge.

**Method:** The EKBM framework combines a fast-thinking model for generating confidence-labeled responses with a slow refinement model for improving uncertain predictions, along with a hybrid training pipeline to enhance self-awareness and performance.

**Key Contributions:**

	1. Integration of fast and slow reasoning systems in LLMs
	2. Hybrid training pipeline enhancing model self-awareness
	3. Demonstrated improvement in reliability and accuracy for error-sensitive applications

**Result:** EKBM demonstrates superior model reliability on dialogue state tracking tasks compared to existing uncertainty-based approaches, with substantial accuracy improvements and low computational overhead.

**Limitations:** 

**Conclusion:** The proposed framework provides a scalable solution for deploying reliable LLMs in sensitive applications by effectively balancing accuracy and practicality.

**Abstract:** Large language models (LLMs) are prone to hallucination stemming from misaligned self-awareness, particularly when processing queries exceeding their knowledge boundaries. While existing mitigation strategies employ uncertainty estimation or query rejection mechanisms, they suffer from computational efficiency and sacrificed helpfulness. To address these issues, we propose the Explicit Knowledge Boundary Modeling (EKBM) framework, integrating fast and slow reasoning systems to harmonize reliability and usability. The framework first employs a fast-thinking model to generate confidence-labeled responses, enabling immediate utilization of high-confidence outputs, whereas uncertain predictions trigger a slow refinement model for accuracy improvement. To align model behavior with our proposed object, we propose a hybrid training pipeline, enhancing self-awareness without degrading task performance. Evaluations on dialogue state tracking tasks demonstrate that EKBM achieves superior model reliability over uncertainty-based baselines. Further analysis reveals that refinement substantially boosts accuracy while maintaining low computational overhead. The framework establishes a scalable paradigm for deploying reliable LLMs in error-sensitive applications, effectively balancing accuracy and practical utility.

</details>


### [91] [Seeing Sarcasm Through Different Eyes: Analyzing Multimodal Sarcasm Perception in Large Vision-Language Models](https://arxiv.org/abs/2503.12149)

*Junjie Chen, Xuyang Liu, Subin Huang, Linfeng Zhang, Hang Yu*

**Main category:** cs.CL

**Keywords:** large vision-language models, sarcasm, multimodal analysis, interpretive reasoning, machine learning

**Relevance Score:** 7

**TL;DR:** The paper examines how different large vision-language models (LVLMs) interpret multimodal sarcasm, revealing significant variations in their understanding and suggesting a need for a more nuanced approach in modeling sarcasm.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As large vision-language models become more advanced, it is important to understand if and how they interpret sarcasm differently from humans.

**Method:** The authors introduce an analytical framework that utilizes systematically designed prompts on multimodal sarcasm datasets to evaluate 12 state-of-the-art LVLMs over 2,409 samples.

**Key Contributions:**

	1. Analytical framework for evaluating LVLMs on sarcasm interpretation.
	2. Empirical insights into varying interpretations of sarcasm across different models.
	3. Recommendations for evolving model training approaches that account for sarcasm's subjectivity.

**Result:** The study finds notable discrepancies in sarcasm interpretation both across different LVLMs and within the same model under different prompts, especially in classifications and interpretive reasoning.

**Limitations:** The study is limited to existing multimodal sarcasm datasets and may not capture all nuances of sarcasm in real-world applications.

**Conclusion:** The results call for an evolution in sarcasm modeling to embrace multi-perspective and uncertainty-aware approaches, moving away from binary labeling schemes.

**Abstract:** With the advent of large vision-language models (LVLMs) demonstrating increasingly human-like abilities, a pivotal question emerges: do different LVLMs interpret multimodal sarcasm differently, and can a single model grasp sarcasm from multiple perspectives like humans? To explore this, we introduce an analytical framework using systematically designed prompts on existing multimodal sarcasm datasets. Evaluating 12 state-of-the-art LVLMs over 2,409 samples, we examine interpretive variations within and across models, focusing on confidence levels, alignment with dataset labels, and recognition of ambiguous "neutral" cases. Our findings reveal notable discrepancies -- across LVLMs and within the same model under varied prompts. While classification-oriented prompts yield higher internal consistency, models diverge markedly when tasked with interpretive reasoning. These results challenge binary labeling paradigms by highlighting sarcasm's subjectivity. We advocate moving beyond rigid annotation schemes toward multi-perspective, uncertainty-aware modeling, offering deeper insights into multimodal sarcasm comprehension. Our code and data are available at: https://github.com/CoderChen01/LVLMSarcasmAnalysis

</details>


### [92] [A Survey on Transformer Context Extension: Approaches and Evaluation](https://arxiv.org/abs/2503.13299)

*Yijun Liu, Jinzheng Yu, Yang Xu, Zhongyang Li, Qingfu Zhu*

**Main category:** cs.CL

**Keywords:** Long Context, Large Language Models, Natural Language Processing, Evaluation Metrics, Survey

**Relevance Score:** 9

**TL;DR:** This survey reviews challenges and approaches for applying large language models (LLMs) to long context scenarios in NLP, categorizing them into four main types and summarizing evaluation metrics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the degradation in performance of LLMs in long context scenarios despite their strong performance in short text tasks.

**Method:** The paper systematically reviews existing approaches related to long context and proposes a taxonomy categorizing them into positional encoding, context compression, retrieval augmented, and attention pattern.

**Key Contributions:**

	1. Systematic taxonomy of approaches for long context processing.
	2. Comprehensive review of challenges faced by LLMs with long contexts.
	3. Organization of evaluation metrics and benchmarks for long context tasks.

**Result:** The authors organized relevant data, tasks, and metrics based on existing long context benchmarks and identified unresolved issues in the long context domain.

**Limitations:** 

**Conclusion:** The paper offers insights into potential future developments for improving LLM performance in long context scenarios.

**Abstract:** Large language models (LLMs) based on Transformer have been widely applied in the filed of natural language processing (NLP), demonstrating strong performance, particularly in handling short text tasks. However, when it comes to long context scenarios, the performance of LLMs degrades due to some challenges. To alleviate this phenomenon, there is a number of work proposed recently. In this survey, we first list the challenges of applying pre-trained LLMs to process long contexts. Then systematically review the approaches related to long context and propose our taxonomy categorizing them into four main types: positional encoding, context compression, retrieval augmented, and attention pattern. In addition to the approaches, we focus on the evaluation of long context, organizing relevant data, tasks, and metrics based on existing long context benchmarks. Finally, we summarize unresolved issues in the long context domain and put forward our views on future developments.

</details>


### [93] [OLMoTrace: Tracing Language Model Outputs Back to Trillions of Training Tokens](https://arxiv.org/abs/2504.07096)

*Jiacheng Liu, Taylor Blanton, Yanai Elazar, Sewon Min, YenSung Chen, Arnavi Chheda-Kothary, Huy Tran, Byron Bischoff, Eric Marsh, Michael Schmitz, Cassidy Trier, Aaron Sarnat, Jenna James, Jon Borchardt, Bailey Kuehl, Evie Cheng, Karen Farley, Sruthi Sreeram, Taira Anderson, David Albright, Carissa Schoenick, Luca Soldaini, Dirk Groeneveld, Rock Yuren Pang, Pang Wei Koh, Noah A. Smith, Sophie Lebrecht, Yejin Choi, Hannaneh Hajishirzi, Ali Farhadi, Jesse Dodge*

**Main category:** cs.CL

**Keywords:** Language Models, Training Data, Real-Time Tracing, Fact-Checking, Hallucination

**Relevance Score:** 8

**TL;DR:** OLMoTrace enables real-time tracing of language model outputs to their training data, identifying verbatim matches to improve understanding of model behavior.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the understanding of language model outputs and their relation to training data, addressing issues like fact checking and hallucination.

**Method:** OLMoTrace utilizes an extended version of infini-gram to efficiently find and display outputs matching the training corpus in real-time.

**Key Contributions:**

	1. Real-time tracing of language model outputs to training data.
	2. Open-source implementation available for public use.
	3. Adnosis to understanding model behavior through direct data correlation.

**Result:** The system can provide verbatim matches between language model outputs and documents from a multi-trillion-token training dataset within seconds.

**Limitations:** 

**Conclusion:** OLMoTrace provides a tool for exploring language model outputs, facilitating better understanding of their behavior and aiding in areas like fact checking and creativity assessment.

**Abstract:** We present OLMoTrace, the first system that traces the outputs of language models back to their full, multi-trillion-token training data in real time. OLMoTrace finds and shows verbatim matches between segments of language model output and documents in the training text corpora. Powered by an extended version of infini-gram (Liu et al., 2024), our system returns tracing results within a few seconds. OLMoTrace can help users understand the behavior of language models through the lens of their training data. We showcase how it can be used to explore fact checking, hallucination, and the creativity of language models. OLMoTrace is publicly available and fully open-source.

</details>
