# 2025-07-31

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 20]

- [cs.CL](#cs.CL) [Total: 55]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [IntentFlow: Interactive Support for Communicating Intent with LLMs in Writing Tasks](https://arxiv.org/abs/2507.22134)

*Yoonsu Kim, Brandon Chin, Kihoon Son, Seoyoung Kim, Juho Kim*

**Main category:** cs.HC

**Keywords:** large language models, intent management, HCI, user interaction, writing assistance

**Relevance Score:** 9

**TL;DR:** IntentFlow enhances LLM-assisted writing by allowing users to communicate and refine their evolving intents through editable interface components.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Users struggle to express nuanced writing intents through prompt-based interfaces when using LLMs, leading to challenges in effectively articulating and adjusting their goals.

**Method:** The paper introduces IntentFlow, a system that extracts user intents from prompts and displays them as editable components linked to output segments, allowing users to revise their intents directly.

**Key Contributions:**

	1. Introduction of IntentFlow for editable intent management in LLM writing
	2. Demonstration of improved user engagement and output alignment in writing tasks
	3. Illustration of the potential for reusable intents across similar writing tasks

**Result:** In a study with 12 participants, those using IntentFlow found it easier to express and modify their intents, leading to more aligned outputs with their goals compared to a chat-based baseline.

**Limitations:** 

**Conclusion:** Editable intent representations significantly aid users in refining final intents for LLM-assisted writing, facilitating better consistency and transferability across tasks.

**Abstract:** While large language models (LLMs) are widely used for writing, users often struggle to express their nuanced and evolving intents through prompt-based interfaces. Intents -- low-level strategies or preferences for achieving a writing goal -- are often vague, fluid, or even subconscious, making it difficult for users to articulate and adjust them. To address this, we present IntentFlow, which supports the communication of dynamically evolving intents throughout LLM-assisted writing. IntentFlow extracts goals and intents from user prompts and presents them as editable interface components, which users can revise, remove, or refine via direct manipulation or follow-up prompts. Visual links connect each component to the output segments it influences, helping users understand model behavior. In a within-subjects study (N=12), participants using IntentFlow, compared to a chat-based baseline, expressed their intents more easily and in detail, engaged in more meaningful actions to communicate intents, such as adjusting and deleting, and produced outputs that better aligned with their evolving intents. We found that editable intent representations help users refine and consolidate a final set of intents, which can be reused across similar tasks to support consistent and transferable LLM-assisted writing.

</details>


### [2] [Towards Privacy-preserving Photorealistic Self-avatars in Mixed Reality](https://arxiv.org/abs/2507.22153)

*Ethan Wilson, Vincent Bindschaedler, Sophie JÃ¶rg, Sean Sheikholeslam, Kevin Butler, Eakta Jain*

**Main category:** cs.HC

**Keywords:** photorealistic avatars, Mixed Reality, user privacy, identity obfuscation, generative models

**Relevance Score:** 8

**TL;DR:** This paper presents a novel methodology for generating photorealistic avatars in Mixed Reality that protects user identity while preserving their demographic characteristics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the privacy risks associated with sharing photorealistic avatars that could lead to cyber abuse or targeted fraud, the authors propose a solution to synthesize avatars that obscure user identity.

**Method:** The approach involves isolating identity in the feature space of identity-encoding generative models and developing two algorithms: one for differential privacy guarantees and another for fine-grained identity obfuscation.

**Key Contributions:**

	1. Introduction of a methodology to synthesize avatars that maintain demographic identity without revealing biometric data.
	2. Development of two algorithms that provide privacy guarantees and control over identity obfuscation.
	3. Demonstration of effective generation of de-identified avatars across different generative architectures.

**Result:** The proposed algorithms successfully generate de-identified virtual avatars across various generative architectures in both 2D and 3D.

**Limitations:** 

**Conclusion:** By employing these techniques, privacy can be maintained while preserving key attributes related to user identity, enabling safer use of photorealistic avatars in public Mixed Reality settings.

**Abstract:** Photorealistic 3D avatar generation has rapidly improved in recent years, and realistic avatars that match a user's true appearance are more feasible in Mixed Reality (MR) than ever before. Yet, there are known risks to sharing one's likeness online, and photorealistic MR avatars could exacerbate these risks. If user likenesses were to be shared broadly, there are risks for cyber abuse or targeted fraud based on user appearances. We propose an alternate avatar rendering scheme for broader social MR -- synthesizing realistic avatars that preserve a user's demographic identity while being distinct enough from the individual user to protect facial biometric information. We introduce a methodology for privatizing appearance by isolating identity within the feature space of identity-encoding generative models. We develop two algorithms that then obfuscate identity: \epsmethod{} provides differential privacy guarantees and \thetamethod{} provides fine-grained control for the level of identity offset. These methods are shown to successfully generate de-identified virtual avatars across multiple generative architectures in 2D and 3D. With these techniques, it is possible to protect user privacy while largely preserving attributes related to sense of self. Employing these techniques in public settings could enable the use of photorealistic avatars broadly in MR, maintaining high realism and immersion without privacy risk.

</details>


### [3] [IdeaBlocks: Expressing and Reusing Exploratory Intents for Design Exploration with Generative AI](https://arxiv.org/abs/2507.22163)

*DaEun Choi, Kihoon Son, Jaesang Yu, Hyunjoon Jung, Juho Kim*

**Main category:** cs.HC

**Keywords:** Generative AI, design exploration, iterative idea development, IdeaBlocks, visual diversity

**Relevance Score:** 8

**TL;DR:** The paper presents IdeaBlocks, a tool facilitating structured input for generative AI to enhance design exploration by enabling continuous and iterative idea development.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Generative AI offers rapid image generation, but existing methods pose limitations for designers in exploration, including expression of intent, continuity, and idea reuse.

**Method:** Formative study with 7 participants identified limitations; user study with 12 designers tested IdeaBlocks for structured input and modular exploration.

**Key Contributions:**

	1. Introduction of IdeaBlocks for structured exploratory intents
	2. Demonstrated increased image exploration and diversity
	3. Insights into design implications for future tools

**Result:** Participants using IdeaBlocks explored 112.8% more images with 12.5% greater visual diversity compared to baseline, while developing ideas in an iterative pattern.

**Limitations:** Limited to a small sample size of designers in the study.

**Conclusion:** Future tools should better support divergent and convergent exploration phases and effectively capture exploratory intents.

**Abstract:** Generative AI opens new possibilities for design exploration by rapidly generating images aligned with user goals. However, our formative study (N=7) revealed three key limitations hindering designers' broad and efficient exploration when interacting with these models. These include difficulty expressing open-ended exploratory intent, lack of continuity in exploration, and limited support for reusing or iterating on previous ideas. We propose IdeaBlocks, where users can express their exploratory intents to generative AI with structured input and modularize them into Exploration Blocks. These blocks can be chained for continuous, non-linear exploration and reused across contexts, enabling broad exploration without losing creative momentum. Our user study with 12 designers showed that participants using IdeaBlocks explored 112.8% more images with 12.5% greater visual diversity than the baseline. They also developed ideas in more iterative and continuous patterns, such as branching, chaining, and revisiting ideas. We discuss design implications for future tools to better balance divergent and convergent support during different phases of exploration, and to capture and leverage exploratory intents more effectively.

</details>


### [4] [DissolvPCB: Fully Recyclable 3D-Printed Electronics with Liquid Metal Conductors and PVA Substrates](https://arxiv.org/abs/2507.22193)

*Zeyu Yan, SuHwan Hong, Josiah Hester, Tingyu Cheng, Huaishu Peng*

**Main category:** cs.HC

**Keywords:** DissolvPCB, recyclable electronics, 3D printing, polymers, environmental impact

**Relevance Score:** 4

**TL;DR:** DissolvPCB is a technique for making recyclable printed circuit board assemblies using 3D printing and water-soluble materials, allowing easy recycling and reuse of components.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need for environmentally friendly electronics that can be easily recycled and reused.

**Method:** Utilizing FDM 3D printing with polyvinyl alcohol as a substrate and eutectic gallium-indium for conductivity, the paper presents a workflow for creating PCBAs and evaluates their performance and environmental impact.

**Key Contributions:**

	1. Introduced DissolvPCB as a recyclable PCBA fabrication method.
	2. Developed a software plugin for automatic conversion of PCB design files.
	3. Performed lifecycle assessment comparing DissolvPCB with traditional manufacturing methods.

**Result:** DissolvPCB circuits were demonstrated through functional prototypes like a Bluetooth speaker and a gripper, showing effective recycling after use.

**Limitations:** Current technical limitations that need to be addressed.

**Conclusion:** The technique has limitations but offers significant potential for sustainable electronics manufacturing and design.

**Abstract:** We introduce DissolvPCB, an electronic prototyping technique for fabricating fully recyclable printed circuit board assemblies (PCBAs) using affordable FDM 3D printing, with polyvinyl alcohol (PVA) as a water-soluble substrate and eutectic gallium-indium (EGaIn) as the conductive material. When obsolete, the PCBA can be easily recycled by immersing it in water: the PVA dissolves, the EGaIn re-forms into a liquid metal bead, and the electronic components are recovered. These materials can then be reused to fabricate a new PCBA.   We present the DissolvPCB workflow, characterize its design parameters, evaluate the performance of circuits produced with it, and quantify its environmental impact through a lifecycle assessment (LCA) comparing it to conventional CNC-milled FR-4 boards. We further develop a software plugin that automatically converts PCB design files into 3D-printable circuit substrate models. To demonstrate the capabilities of DissolvPCB, we fabricate and recycle three functional prototypes: a Bluetooth speaker featuring a double-sided PCB, a finger fidget toy with a 3D circuit topology, and a shape-changing gripper enabled by Joule-heat-driven 4D printing. The paper concludes with a discussion of current technical limitations and opportunities for future directions.

</details>


### [5] [Verisimilitude as Boon and Bane: How People Initiate Opportunistic Interactions at Professional Events in Social VR](https://arxiv.org/abs/2507.22241)

*Victoria Chang, Caro Williams-Pierce, Huaishu Peng, Ge Gao*

**Main category:** cs.HC

**Keywords:** opportunistic interactions, social VR, verisimilitude, professional events, user experience

**Relevance Score:** 7

**TL;DR:** This paper examines how individuals initiate opportunistic interactions in social virtual reality (VR) for professional events, highlighting the importance of nonverbal cues and proposing design insights for VR platforms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Opportunistic interactions are key for relationship building and information sharing, especially as professional events move online. However, facilitating these interactions in social VR poses unique challenges.

**Method:** The study involved qualitative observations and interviews with 16 participants who attended VR-mediated events, allowing for in-depth insights into their experiences with opportunistic interactions.

**Key Contributions:**

	1. Identification of the role of verisimilitude in social interactions in VR
	2. Detailed analysis of interaction initiation steps in social VR
	3. Recommendations for VR platform design to enhance opportunistic interactions

**Result:** Participants navigated interactions by assessing the verisimilitude of nonverbal cues in VR, which shaped their social behaviors during the steps of interaction initiation: recognizing availability, capturing attention, and ice-breaking.

**Limitations:** The study is based on a small sample size and specific to certain professional communities, which may limit generalizability.

**Conclusion:** The findings indicate a disconnect between participants' practices and systematic knowledge on navigating social cues in VR, leading to recommendations for enhancing VR platform design to support professional interactions.

**Abstract:** Opportunistic interactions-the unstructured exchanges that emerge as individuals become aware of each other's presence-are essential for relationship building and information sharing in everyday life. Yet, fostering effective opportunistic interactions has proven challenging, especially at professional events that have increasingly transitioned from in person to online formats. In the current paper, we offer an in-depth qualitative account of how people initiate opportunistic interactions in social VR. Our participants consisted of 16 individuals with ongoing experience attending VR-mediated events in their professional communities. We conducted extensive observations with each participant during one or more events they attended. We also interviewed them after every observed event, obtaining self-reflections on their attempts to navigate opportunistic interactions with others. Our analysis revealed that participants sought to understand the extent to which social VR preserved the real-world meanings of various nonverbal cues, which we refer to as verisimilitude. We detailed the unique connections between a person's perceived verisimilitude and their social behaviors at each of the three steps toward initiating opportunistic interactions: availability recognition, attention capture, and ice-breaking. Across these steps, the VR platform typically replaces complex social mechanisms with feasible technical ones in order to function, thereby altering the preconditions necessary for a nonverbal cue's social meanings to remain intact. We identified a rich set of strategies that participants developed to assess verisimilitude and act upon it, while also confirming a lack of systematic knowledge guiding their practices. Based on these findings, we provide actionable insights for social VR platform design that can best support the initiation of opportunistic interactions for professional purposes.

</details>


### [6] [Multidimensional Assessment of Takeover Performance in Conditionally Automated Driving](https://arxiv.org/abs/2507.22252)

*Kexin Liang, Jan Luca KÃ¤stleb, Bani Anvarib, Simeon C. Calverta, J. W. C. van Lint*

**Main category:** cs.HC

**Keywords:** Automated Driving, Takeover Requests, Situational Awareness, Spare Capacity, Driving Performance

**Relevance Score:** 4

**TL;DR:** This study evaluates drivers' performance in automated driving takeover scenarios, focusing on factors like Situational Awareness (SA) and Spare Capacity (SC) using driving simulations and XGBoost models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the key factors influencing drivers' responses to takeover requests in automated driving systems, ensuring safe and comfortable control transitions.

**Method:** Driving simulator experiment assessing takeover performance along dimensions of response efficiency, user experience, and driving safety, analyzed using EXtreme Gradient Boosting (XGBoost) models.

**Key Contributions:**

	1. Evaluated the impact of Situational Awareness (SA) and Spare Capacity (SC) on drivers' takeover performance.
	2. Used driving simulators and XGBoost models to assess performance dimensions.
	3. Identified that SC has a greater influence on takeover quality than SA.

**Result:** Higher Situational Awareness (SA) leads to quicker responses to takeover requests, while Spare Capacity (SC) has a greater overall impact on the quality of takeovers, improving both subjective ratings and execution trajectories.

**Limitations:** 

**Conclusion:** The study highlights the complementary roles of SA and SC in enhancing performance in automated driving interactions, providing insights for future design improvements in automated driving systems.

**Abstract:** When automated driving systems encounter complex situations beyond their operational capabilities, they issue takeover requests, prompting drivers to resume vehicle control and return to the driving loop as a critical safety backup. However, this control transition places significant demands on drivers, requiring them to promptly respond to takeover requests while executing high-quality interventions. To ensure safe and comfortable control transitions, it is essential to develop a deep understanding of the key factors influencing various takeover performance aspects. This study evaluates drivers' takeover performance across three dimensions: response efficiency, user experience, and driving safety - using a driving simulator experiment. EXtreme Gradient Boosting (XGBoost) models are used to investigate the contributions of two critical factors, i.e., Situational Awareness (SA) and Spare Capacity (SC), in predicting various takeover performance metrics by comparing the predictive results to the baseline models that rely solely on basic Driver Characteristics (DC). The results reveal that (i) higher SA enables drivers to respond to takeover requests more quickly, particularly for reflexive responses; and (ii) SC shows a greater overall impact on takeover quality than SA, where higher SC generally leads to enhanced subjective rating scores and objective execution trajectories. These findings highlight the distinct yet complementary roles of SA and SC in shaping performance components, offering valuable insights for optimizing human-vehicle interactions and enhancing automated driving system design.

</details>


### [7] [Towards Safe and Comfortable Vehicle Control Transitions: A Systematic Review of Takeover Time, Time Budget, and Takeover Performance](https://arxiv.org/abs/2507.22262)

*Kexin Liang, Simeon C. Calvert, J. W. C. van Lint*

**Main category:** cs.HC

**Keywords:** automated driving, takeover time, time budget, human-vehicle interaction, performance measurement

**Relevance Score:** 7

**TL;DR:** The paper reviews the challenges and variables associated with human driver takeovers in conditionally automated driving systems, focusing on defining sufficient time budgets for safe and comfortable transitions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve safety and user experience in conditionally automated driving by addressing the variability in human driver responses during takeovers.

**Method:** The paper synthesizes causal factors influencing takeover time, reviews existing studies on time budgets, introduces a concept of takeover buffer, and proposes taxonomies and a conceptual model for measuring takeover performance.

**Key Contributions:**

	1. Synthesis of causal factors influencing takeover time and a proposed taxonomy of its determinants.
	2. Introduction of the takeover buffer concept for measuring the gap between actual takeover time and allocated time budget.
	3. Development of a comprehensive conceptual model relating takeover time, time budget, and performance.

**Result:** The review identifies key determinants influencing takeover time, provides a framework for fixed and adaptive time budgets, and outlines six strategic directions for future research.

**Limitations:** 

**Conclusion:** A standardized approach to measuring takeover performance and establishing adaptable time budgets is essential for enhancing safety and efficiency in human-vehicle interaction during takeovers.

**Abstract:** Conditionally automated driving systems require human drivers to disengage from non-driving-related activities and resume vehicle control within limited time budgets when encountering scenarios beyond system capabilities. Ensuring safe and comfortable transitions is critical for reducing driving risks and improving user experience. However, takeovers involve complex human-vehicle interactions, resulting in substantial variability in drivers' responses, especially in takeover time, defined as the duration needed to regain control. This variability presents challenges in setting sufficient time budgets that are neither too short (risking safety and comfort) nor too long (reducing driver alertness and transition efficiency).   Although previous research has examined the role of time budgets in influencing takeover time and performance, few studies have systematically addressed how to determine sufficient time budgets that adapt to diverse scenarios and driver needs. This review supports such efforts by examining the entire takeover sequence, including takeover time, time budget, and takeover performance. Specifically, we (i) synthesize causal factors influencing takeover time and propose a taxonomy of its determinants using the task-capability interface model; (ii) review existing work on fixed and adaptive time budgets, introducing the concept of the takeover buffer to describe the gap between takeover time and allocated time budget; (iii) present a second taxonomy to support standardized and context-sensitive measurement of takeover performance; (iv) propose a conceptual model describing the relationships among takeover time, time budget, and performance; and (v) outline a research agenda with six directions.

</details>


### [8] [Promoting Online Safety by Simulating Unsafe Conversations with LLMs](https://arxiv.org/abs/2507.22267)

*Owen Hoffman, Kangze Peng, Zehua You, Sajid Kamal, Sukrit Venkatagiri*

**Main category:** cs.HC

**Keywords:** Generative AI, Online Safety, Large Language Models, Scam Conversations, User Feedback

**Relevance Score:** 8

**TL;DR:** This paper explores using large language models (LLMs) to simulate and teach about unsafe online conversations, particularly scams, aiming to promote online safety through user feedback.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing prevalence of unsafe conversations online, especially those driven by generative AI, and to promote online safety.

**Method:** The authors simulate unsafe conversations between a scammer LLM and a target LLM, with a focus on prompting users to provide feedback on the target LLM's responses.

**Key Contributions:**

	1. Utilization of LLMs to simulate scam conversations.
	2. Incorporation of user feedback to improve learning about online safety.
	3. Focus on educational strategies in understanding unsafe interactions.

**Result:** The approach shows potential in educating users about unsafe online interactions, enabling them to recognize and respond to scams more effectively.

**Limitations:** 

**Conclusion:** Incorporating feedback in simulated conversations can enhance understanding and awareness of online scams, contributing to safer online environments.

**Abstract:** Generative AI, including large language models (LLMs) have the potential -- and already are being used -- to increase the speed, scale, and types of unsafe conversations online. LLMs lower the barrier for entry for bad actors to create unsafe conversations in particular because of their ability to generate persuasive and human-like text. In our current work, we explore ways to promote online safety by teaching people about unsafe conversations that can occur online with and without LLMs. We build on prior work that shows that LLMs can successfully simulate scam conversations. We also leverage research in the learning sciences that shows that providing feedback on one's hypothetical actions can promote learning. In particular, we focus on simulating scam conversations using LLMs. Our work incorporates two LLMs that converse with each other to simulate realistic, unsafe conversations that people may encounter online between a scammer LLM and a target LLM but users of our system are asked provide feedback to the target LLM.

</details>


### [9] [ConGaIT: A Clinician-Centered Dashboard for Contestable AI in Parkinson's Disease Care](https://arxiv.org/abs/2507.22300)

*Phuc Truong Loc Nguyen, Thanh Hung Do*

**Main category:** cs.HC

**Keywords:** AI-assisted gait analysis, Parkinson's Disease, HCI, contestability, interpretability

**Relevance Score:** 9

**TL;DR:** Con-GaIT is a clinician-centered system for AI-assisted gait analysis in Parkinson's Disease that promotes interpretability and oversight.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current AI-assisted gait analysis systems lack transparency and do not allow clinicians to contest AI decisions, which is critical for effective patient care in Parkinson's Disease.

**Method:** Developed a system integrating HCI principles that enables clinicians to contest and justify AI decisions through structured interactions and visual explanations.

**Key Contributions:**

	1. Introduction of the Contest & Justify interaction pattern.
	2. Implementation of traceable justification logs.
	3. High Contestability Assessment Score demonstrating effective design.

**Result:** The Con-GaIT framework achieved a Contestability Assessment Score of 0.970, indicating successful operationalization of contestability through human-centered design.

**Limitations:** 

**Conclusion:** Con-GaIT enhances the interpretability and oversight of AI models in clinical settings, complying with emerging regulatory standards.

**Abstract:** AI-assisted gait analysis holds promise for improving Parkinson's Disease (PD) care, but current clinical dashboards lack transparency and offer no meaningful way for clinicians to interrogate or contest AI decisions. We present Con-GaIT (Contestable Gait Interpretation & Tracking), a clinician-centered system that advances Contestable AI through a tightly integrated interface designed for interpretability, oversight, and procedural recourse. Grounded in HCI principles, ConGaIT enables structured disagreement via a novel Contest & Justify interaction pattern, supported by visual explanations, role-based feedback, and traceable justification logs. Evaluated using the Contestability Assessment Score (CAS), the framework achieves a score of 0.970, demonstrating that contestability can be operationalized through human-centered design in compliance with emerging regulatory standards. A demonstration of the framework is available at https://github.com/hungdothanh/Con-GaIT.

</details>


### [10] [A Node on the Constellation: The Role of Feminist Makerspaces in Building and Sustaining Alternative Cultures of Technology Production](https://arxiv.org/abs/2507.22329)

*Erin Gatz, Yasmine Kotturi, Andrea Afua Kwamya, Sarah Fox*

**Main category:** cs.HC

**Keywords:** feminist makerspaces, sustainability, community-led initiatives, collective knowledge production, social practices

**Relevance Score:** 2

**TL;DR:** This paper explores the sustainability practices of feminist makerspaces, emphasizing care-driven stewardship and solidarity with local justice movements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how feminist makerspaces sustain themselves over time amidst structural precarity.

**Method:** Interviews with 18 founders and members across 8 U.S. feminist makerspaces, along with autoethnographic reflection.

**Key Contributions:**

	1. Examines the sustainability practices of feminist makerspaces
	2. Highlights the importance of care-driven stewardship and solidarity
	3. Positions feminist makerspaces as prefigurative counterspaces enacting feminist values

**Result:** Identified key practices supporting sustainability, which include care-driven stewardship, solidarity with local justice movements, and shared governance.

**Limitations:** 

**Conclusion:** Feminist makerspaces serve as counterspaces that implement feminist values through their organizational practices, contributing to their longevity in challenging environments.

**Abstract:** Feminist makerspaces offer community led alternatives to dominant tech cultures by centering care, mutual aid, and collective knowledge production. While prior CSCW research has explored their inclusive practices, less is known about how these spaces sustain themselves over time. Drawing on interviews with 18 founders and members across 8 U.S. feminist makerspaces as well as autoethnographic reflection, we examine the organizational and relational practices that support long-term endurance. We find that sustainability is not achieved through growth or institutionalization, but through care-driven stewardship, solidarity with local justice movements, and shared governance. These social practices position feminist makerspaces as prefigurative counterspaces - sites that enact, rather than defer, feminist values in everyday practice. This paper offers empirical insight into how feminist makerspaces persist amid structural precarity, and highlights the forms of labor and coalition-building that underpin alternative sociotechnical infrastructures.

</details>


### [11] [Mitigating Response Delays in Free-Form Conversations with LLM-powered Intelligent Virtual Agents](https://arxiv.org/abs/2507.22352)

*Mykola Maslych, Mohammadreza Katebi, Christopher Lee, Yahya Hmaiti, Amirpouya Ghasemaghaei, Christian Pumarada, Janneese Palmer, Esteban Segarra Martinez, Marco Emporio, Warren Snipes, Ryan P. McMahan, Joseph J. LaViola Jr*

**Main category:** cs.HC

**Keywords:** Virtual Agents, Large Language Models, Conversational Fillers, User Engagement, Virtual Reality

**Relevance Score:** 9

**TL;DR:** This paper explores reducing response delays in virtual agents using Large Language Models in Virtual Reality by employing conversational fillers to improve user experience under latency conditions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of response delays in free-form conversations with virtual agents in Virtual Reality, enhancing user engagement despite system lags.

**Method:** Investigated the effectiveness of conversational fillers, including gestures and verbal cues, in a range of latency scenarios during interactions with virtual agents.

**Key Contributions:**

	1. Insights into mitigating response delays in virtual conversations
	2. Demonstration of using conversational fillers to improve user experience
	3. Provision of an open-source pipeline for deploying conversational agents in VR

**Result:** Discovering that latencies beyond 4 seconds significantly deteriorate user experience, while conversational fillers effectively enhance perceived response time under high-delay situations.

**Limitations:** Focused on specific latency conditions; further investigation needed on various interaction designs and user demographics.

**Conclusion:** The study offers valuable insights for optimizing user engagement in conversational systems facing latency, alongside an open-source deployment pipeline for conversational agents in virtual environments.

**Abstract:** We investigated the challenges of mitigating response delays in free-form conversations with virtual agents powered by Large Language Models (LLMs) within Virtual Reality (VR). For this, we used conversational fillers, such as gestures and verbal cues, to bridge delays between user input and system responses and evaluate their effectiveness across various latency levels and interaction scenarios. We found that latency above 4 seconds degrades quality of experience, while natural conversational fillers improve perceived response time, especially in high-delay conditions. Our findings provide insights for practitioners and researchers to optimize user engagement whenever conversational systems' responses are delayed by network limitations or slow hardware. We also contribute an open-source pipeline that streamlines deploying conversational agents in virtual environments.

</details>


### [12] [A Fuzzy Set-based Approach for Matching Hand-Drawing Shapes of Touch-based Gestures for Graphical Passwords](https://arxiv.org/abs/2507.22382)

*Adel Sabour, Ahmed Gadallah, Hesham Hefny*

**Main category:** cs.HC

**Keywords:** Fuzzy Sets, Gesture Recognition, Human-Computer Interaction

**Relevance Score:** 4

**TL;DR:** The paper introduces a fuzzy set-based method to match touch gestures, aiming to enhance accuracy in gesture recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inaccuracies in hand-drawn gestures that hinder effective human-computer interaction.

**Method:** The proposed method uses a two-dimensional fuzzy set approach combined with a fuzzy cued click point technique to improve gesture acceptance.

**Key Contributions:**

	1. Development of a fuzzy set-based method for gesture recognition
	2. Improvement in acceptance rates of inaccurate gestures
	3. Flexibility in handling gesture inaccuracies

**Result:** It improves the recognition of gestures that are inaccurate while interacting with computerized systems, leading to better usability.

**Limitations:** The effectiveness of the method may depend on the complexity of gestures and the user's training.

**Conclusion:** The approach provides a flexible solution to the common accuracy issues in gesture-based interactions.

**Abstract:** This paper presents a two-dimension fuzzy set based approach for matching touch-based gestures using fuzzy cued click point technique. The pro posed approach aims mainly to improve the acceptance of the most closed inac curate hand drawn gestures generated by the user compared with a predefined referenced gesture value that is stored in the user profile. Commonly, gestures are used in order to facilitate the interactive capabilities between humans and computerized systems. Unfortunately, most of current gesturing techniques don't deal at the same level of inaccuracy of gesturing, resulted from the nature of hu man fingers and hands movements. This paper aims, in a more flexible manner, to tackle the inaccuracy problem existed with gesture-based interactions between humans and a computerized system.

</details>


### [13] [Analysis of User Experience Evaluation Methods for Deaf users: A Case Study on a mobile App](https://arxiv.org/abs/2507.22455)

*A. E. Fuentes-CortÃ¡zar, A. Rivera-HernÃ¡ndez, J. R. Rojano-CÃ¡ceres*

**Main category:** cs.HC

**Keywords:** User Experience, Deaf users, Accessibility, UX evaluation methods, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This research analyzes common UX evaluation methods for their effectiveness with Deaf users, highlighting necessary adaptations to improve accessibility and accurately reflect their user experience.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional UX evaluation methods for Deaf users, ensuring their unique accessibility needs are met.

**Method:** Analysis of recommended UX evaluation methods for Deaf users, validating their accessibility and identifying limitations.

**Key Contributions:**

	1. Analysis of existing UX methods for Deaf users
	2. Validation of accessibility of recommended methods
	3. Recommendations for adaptation of UX evaluation methods

**Result:** Identification of various limitations in existing UX evaluation methods for Deaf users, underscoring the need for adaptation.

**Limitations:** Existing methods often overlook the unique communication skills of the Deaf community.

**Conclusion:** UX evaluation methods need adaptation to support Deaf individuals effectively, allowing for accurate data collection on their experiences and needs.

**Abstract:** User Experience (UX) evaluation methods that are commonly used with hearing users may not be functional or effective for Deaf users. This is because these methods are primarily designed for users with hearing abilities, which can create limitations in the interaction, perception, and understanding of the methods for Deaf individuals. Furthermore, traditional UX evaluation approaches often fail to address the unique accessibility needs of Deaf users, resulting in an incomplete or biased assessment of their user experience. This research focused on analyzing a set of UX evaluation methods recommended for use with Deaf users, with the aim of validating the accessibility of each method through findings and limitations. The results indicate that, although these evaluation methods presented here are commonly recommended in the literature for use with Deaf users, they present various limitations that must be addressed in order to better adapt to the communication skills specific to the Deaf community. This research concludes that evaluation methods must be adapted to ensure accessible software evaluation for Deaf individuals, enabling the collection of data that accurately reflects their experiences and needs.

</details>


### [14] [Exploring Student-AI Interactions in Vibe Coding](https://arxiv.org/abs/2507.22614)

*Francis Geng, Anshul Shah, Haolin Li, Nawab Mulla, Steven Swanson, Gerald Soosai Raj, Daniel Zingaro, Leo Porter*

**Main category:** cs.HC

**Keywords:** vibe coding, Replit, CS education, programming, user interaction

**Relevance Score:** 6

**TL;DR:** This study investigates how students in programming classes interact with the Vibe Coding platform (Replit) while creating software projects.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of vibe coding on student programming experiences and software project development.

**Method:** Participants used Replit to build a web application, while their interactions were recorded and analyzed through thematic analysis.

**Key Contributions:**

	1. Insights into student interactions with Replit in different programming courses
	2. Comparison of interaction behaviors between novice and advanced students
	3. Implications for future teaching methods in CS education

**Result:** Most interactions involved testing or debugging; advanced students provided more contextually relevant prompts compared to novice students.

**Limitations:** The study's insights are limited to one platform (Replit) and specific student demographics in a controlled environment.

**Conclusion:** Vibe coding significantly influences how students engage with programming platforms, highlighting differing interaction patterns based on experience level.

**Abstract:** Background and Context. Chat-based and inline-coding-based GenAI has already had substantial impact on the CS Education community. The recent introduction of ``vibe coding'' may further transform how students program, as it introduces a new way for students to create software projects with minimal oversight.   Objectives. The purpose of this study is to understand how students in introductory programming and advanced software engineering classes interact with a vibe coding platform (Replit) when creating software and how the interactions differ by programming background.   Methods. Interview participants were asked to think-aloud while building a web application using Replit. Thematic analysis was then used to analyze the video recordings with an emphasis on the interactions between the student and Replit.   Findings. For both groups, the majority of student interactions with Replit were to test or debug the prototype and only rarely did students visit code. Prompts by advanced software engineering students were much more likely to include relevant app feature and codebase contexts than those by introductory programming students.

</details>


### [15] [Designing for Self-Regulation in Informal Programming Learning: Insights from a Storytelling-Centric Approach](https://arxiv.org/abs/2507.22671)

*Sami Saeed Alghamdi, Christopher Bull, Ahmed Kharrufa*

**Main category:** cs.HC

**Keywords:** self-regulation, programming education, AI feedback, learning stories, narrative practices

**Relevance Score:** 6

**TL;DR:** This paper presents a web platform and browser extensions designed to support independent programming learners in self-regulation by integrating AI-generated feedback into their learning stories.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Many self-taught programming learners struggle due to feelings of isolation and frustration, often resulting from information overload and lack of guidance. The aim is to enhance their learning experience through structured self-regulation practices facilitated by social media.

**Method:** The study involved developing a system that combines a web platform and browser extensions to help learners create narrative-driven learning stories with AI feedback. Data were collected from 15 informal programming learners via surveys and quantitative scales.

**Key Contributions:**

	1. Development of an AI-augmented self-regulation system for programming learners
	2. Integration of storytelling into the learning process
	3. User feedback identifying valuable features and design opportunities

**Result:** Users found value in tools such as in-situ reflection and automated story feedback, indicating the system's potential as a support aid for self-regulation. However, feedback on other features was mixed.

**Limitations:** The study was limited to a small group of 15 participants, which may not represent broader learner experiences.

**Conclusion:** The findings suggest that AI-augmented self-regulation tools can be beneficial to informal programming learners, highlighting both benefits and areas for improvement in future designs.

**Abstract:** Many people learn programming independently from online resources and often report struggles in achieving their personal learning goals. Learners frequently describe their experiences as isolating and frustrating, challenged by abundant uncertainties, information overload, and distraction, compounded by limited guidance. At the same time, social media serves as a personal space where many engage in diverse self-regulation practices, including help-seeking, using external memory aids (e.g., self-notes), self-reflection, emotion regulation, and self-motivation. For instance, learners often mark achievements and set milestones through their posts. In response, we developed a system consisting of a web platform and browser extensions to support self-regulation online. The design aims to add learner-defined structure to otherwise unstructured experiences and bring meaning to curation and reflection activities by translating them into learning stories with AI-generated feedback. We position storytelling as an integrative approach to design that connects resource curation, reflective and sensemaking practice, and narrative practices learners already use across social platforms. We recruited 15 informal programming learners who are regular social media users to engage with the system in a self-paced manner; participation concluded upon submitting a learning story and survey. We used three quantitative scales and a qualitative survey to examine users' characteristics and perceptions of the system's support for their self-regulation. User feedback suggests the system's viability as a self-regulation aid. Learners particularly valued in-situ reflection, automated story feedback, and video annotation, while other features received mixed views. We highlight perceived benefits, friction points, and design opportunities for future AI-augmented self-regulation tools.

</details>


### [16] [VRISE: A Virtual Reality Platfrom for Immersive and Interactive Surveying Education](https://arxiv.org/abs/2507.22810)

*Daniel Udekwe, Dimitrios Bolkas, Eren Erman Ozguven, Ren Moses, Qianwen, Guo*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Surveying Education, Interactive Learning, Immersive Environments, Skill Development

**Relevance Score:** 4

**TL;DR:** Development of VRISE, an immersive VR laboratory for surveying education, enhancing student engagement and learning outcomes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve accessibility and engagement in surveying education by overcoming logistical and cognitive challenges of traditional methods.

**Method:** Creation of Virtual Reality for Immersive and Interactive Surveying Education (VRISE), which provides adaptable learning modules for ground-based and aerial surveying tasks.

**Key Contributions:**

	1. Introduction of customizable VR modules for surveying education
	2. Empirical evidence of learning gains from immersive experiences
	3. Enhanced engagement through adaptive interfaces and real-time feedback

**Result:** Evaluation showed consistent improvements in measurement accuracy, task efficiency, and interaction quality across user sessions, indicating enhanced skill development.

**Limitations:** 

**Conclusion:** VRISE effectively reduces cognitive load and physical demands, showcasing the benefits of immersive digital environments in education.

**Abstract:** Surveying is a core component of civil engineering education, requiring students to engage in hands-on spatial measurement, instrumentation handling, and field-based decision-making. However, traditional instruction often poses logistical and cognitive challenges that can hinder accessibility and student engagement. While virtual laboratories have gained traction in engineering education, few are purposefully designed to support flexible, adaptive learning in surveying. To address this gap, we developed Virtual Reality for Immersive and Interactive Surveying Education (VRISE), an immersive virtual reality laboratory that replicates ground-based and aerial surveying tasks through customizable, accessible, and user-friendly modules. VRISE features interactive experiences such as differential leveling with a digital level equipment and waypoint-based drone navigation, enhanced by input smoothing, adaptive interfaces, and real-time feedback to accommodate diverse learning styles. Evaluation across multiple user sessions demonstrated consistent gains in measurement accuracy, task efficiency, and interaction quality, with a clear progression in skill development across the ground-based and aerial surveying modalities. By reducing cognitive load and physical demands, even in tasks requiring fine motor control and spatial reasoning, VRISE demonstrates the potential of immersive, repeatable digital environments to enhance surveying education, broaden participation, and strengthen core competencies in a safe and engaging setting.

</details>


### [17] [Progressive Web Application for Storytelling Therapy Support](https://arxiv.org/abs/2507.22839)

*Javier Jimenez-Honrado, Javier Gomez Garcia, Felipe Costa-Tebar, Felix A. Marco, Jose A. Gallud, Gabriel Sebastian Rivera*

**Main category:** cs.HC

**Keywords:** Progressive Web Applications, Storytelling Therapy, Mental Health, Technology in Therapy, Non-Profit Applications

**Relevance Score:** 5

**TL;DR:** The paper discusses the development of a Progressive Web Application (PWA) to support Storytelling Therapy in mental health, emphasizing its advantages over traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of technology in non-profit organizations and to modernize approaches to mental health therapies by utilizing Progressive Web Applications.

**Method:** The design and implementation of a Progressive Web Application specifically for enhancing Storytelling Therapy workshops, including analysis of its practical benefits.

**Key Contributions:**

	1. Design of a PWA for Storytelling Therapy
	2. Highlighting the benefits of PWAs in mental health applications
	3. Practical implementation and analysis in a real-world context

**Result:** The PWA facilitates access and installation on devices, aiming to improve the delivery of Storytelling Therapy.

**Limitations:** 

**Conclusion:** The study confirms the practical advantages of adopting PWA technology in mental health therapy settings.

**Abstract:** In spite of all advances promoted by information technologies, there are still activities where this technology is not applied for reasons such as being carried out in non-profit organizations or because they have not adapted to this modernization. Until recently, the way to work with mobile devices was either by connecting through a web page with the device's browser, or by downloading an application from the corresponding platform. But lately, technologies are being developed that aim to break with this, as in the case of Progressive Web Applications (PWA). One of the advantages offered by PWA is to access the web page and install it as an application on the device. The purpose of this article is to design a progressive Web application for the support of Storytelling Therapy, one of the novel therapies applied in the field of mental health. In addition to providing a software application to enhance Storytelling Therapy workshops, it is also intended to analyze and verify the advantages of PWA in a real case.

</details>


### [18] [SciDaSynth: Interactive Structured Knowledge Extraction and Synthesis from Scientific Literature with Large Language Model](https://arxiv.org/abs/2404.13765)

*Xingbo Wang, Samantha L. Huey, Rui Sheng, Saurabh Mehta, Fei Wang*

**Main category:** cs.HC

**Keywords:** structured knowledge, human-AI interaction, literature review, large language models, SciDaSynth

**Relevance Score:** 9

**TL;DR:** SciDaSynth is a novel interactive system powered by LLMs that enables efficient extraction and structuring of knowledge from scientific literature, facilitating the creation and exploration of data tables for enhanced research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance scientific progress by improving the extraction and synthesis of structured knowledge from extensive scientific literature.

**Method:** SciDaSynth automates the creation of data tables from literature through question-answering, allowing for multi-level exploration and iterative refinement.

**Key Contributions:**

	1. Introduction of SciDaSynth as an interactive system for knowledge extraction.
	2. Automation of data table generation from scientific literature.
	3. Facilitation of iterative validation and exploration of structured knowledge.

**Result:** A within-subjects study demonstrated the effectiveness and efficiency of SciDaSynth in constructing quality scientific knowledge bases by researchers.

**Limitations:** 

**Conclusion:** The system highlights important design implications for human-AI interaction tools focused on data extraction and structuring.

**Abstract:** Extraction and synthesis of structured knowledge from extensive scientific literature are crucial for advancing and disseminating scientific progress. Although many existing systems facilitate literature review and digest, they struggle to process multimodal, varied, and inconsistent information within and across the literature into structured data. We introduce SciDaSynth, a novel interactive system powered by large language models (LLMs) that enables researchers to efficiently build structured knowledge bases from scientific literature at scale. The system automatically creates data tables to organize and summarize users' interested knowledge in literature via question-answering. Furthermore, it provides multi-level and multi-faceted exploration of the generated data tables, facilitating iterative validation, correction, and refinement. Our within-subjects study with researchers demonstrates the effectiveness and efficiency of SciDaSynth in constructing quality scientific knowledge bases. We further discuss the design implications for human-AI interaction tools for data extraction and structuring.

</details>


### [19] [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)

*Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, Yan Lu*

**Main category:** cs.HC

**Keywords:** GUI agents, vision-based approaches, data synthesis, instruction grounding, benchmark

**Relevance Score:** 9

**TL;DR:** The paper introduces a new data synthesis pipeline, UI-E2I-Synth, for generating instruction datasets for GUI agent training and proposes a benchmark, UI-I2E-Bench, to improve GUI instruction grounding performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Advancements in Large Vision-Language Models drive the creation of GUI agents with human-like perception, but challenges in instruction grounding hinder their effectiveness due to limited training datasets and manual annotation requirements.

**Method:** The paper presents a large-scale data synthesis pipeline that utilizes GPT-4o to generate diverse instruction datasets without human intervention, and introduces a new benchmark for evaluating GUI instruction grounding.

**Key Contributions:**

	1. Introduction of UI-E2I-Synth for large-scale dataset generation.
	2. Creation of UI-I2E-Bench benchmark for improved evaluation of GUI instruction grounding.
	3. Insights into challenges and solutions in GUI instruction grounding.

**Result:** The model trained on synthesized data outperforms current state-of-the-art approaches in GUI instruction grounding, showcasing the effectiveness of the proposed data synthesis pipeline and benchmark.

**Limitations:** 

**Conclusion:** The new data synthesis pipeline and benchmark provide valuable resources for advancing research in GUI grounding and improve the applicability of vision-based approaches in GUI agents.

**Abstract:** Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .

</details>


### [20] [UI-E2I-Synth: Advancing GUI Grounding with Large-Scale Instruction Synthesis](https://arxiv.org/abs/2504.11257)

*Xinyi Liu, Xiaoyi Zhang, Ziyun Zhang, Yan Lu*

**Main category:** cs.HC

**Keywords:** GUI instruction grounding, data synthesis, vision-language models

**Relevance Score:** 7

**TL;DR:** This paper addresses challenges in GUI instruction grounding using a data synthesis pipeline and introduces a new benchmark for evaluating performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the limitations of GUI instruction grounding due to insufficient datasets and heavy manual annotation requirements.

**Method:** Introduction of UI-E2I-Synth for generating complex instruction datasets with GPT-4o, and establishment of UI-I2E-Bench as a new benchmark for GUI instruction grounding.

**Key Contributions:**

	1. Introduction of a large-scale data synthesis pipeline UI-E2I-Synth
	2. Development of a new benchmark UI-I2E-Bench
	3. Demonstration of improved performance in GUI instruction grounding

**Result:** The proposed model trained on synthesized data shows superior performance in grounding user instructions to GUI elements compared to existing methods.

**Limitations:** 

**Conclusion:** The paper provides a viable solution to improve GUI instruction grounding and offers a new benchmark for future research, alongside a synthesis pipeline to generate diverse instruction datasets.

**Abstract:** Recent advancements in Large Vision-Language Models are accelerating the development of Graphical User Interface (GUI) agents that utilize human-like vision perception capabilities to enhance productivity on digital devices. Compared to approaches predicated on GUI metadata, which are platform-dependent and vulnerable to implementation variations, vision-based approaches offer broader applicability. In this vision-based paradigm, the GUI instruction grounding, which maps user instruction to the location of corresponding element on the given screenshot, remains a critical challenge, particularly due to limited public training dataset and resource-intensive manual instruction data annotation. In this paper, we delve into unexplored challenges in this task including element-to-screen ratio, unbalanced element type, and implicit instruction. To address these challenges, we introduce a large-scale data synthesis pipeline UI-E2I-Synth for generating varying complex instruction datasets using GPT-4o instead of human annotators. Furthermore, we propose a new GUI instruction grounding benchmark UI-I2E-Bench, which is designed to address the limitations of existing benchmarks by incorporating diverse annotation aspects. Our model, trained on the synthesized data, achieves superior performance in GUI instruction grounding, demonstrating the advancements of proposed data synthesis pipeline. The proposed benchmark, accompanied by extensive analyses, provides practical insights for future research in GUI grounding. We will release corresponding artifacts at https://microsoft.github.io/FIVE-UI-Evol/ .

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [21] [IndoPref: A Multi-Domain Pairwise Preference Dataset for Indonesian](https://arxiv.org/abs/2507.22159)

*Vanessa Rebecca Wiyono, David Anugraha, Ayu Purwarianti, Genta Indra Winata*

**Main category:** cs.CL

**Keywords:** Indonesian, large language models, preference dataset, natural language processing, multilingual

**Relevance Score:** 8

**TL;DR:** IndoPref is a new, human-authored dataset for evaluating Indonesian preferences in LLM-generated text, addressing the lack of linguistic authenticity in existing multilingual datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a culturally and linguistically authentic dataset for preference-based research in Indonesian, filling a significant gap in multilingual models.

**Method:** The IndoPref dataset is fully human-authored, with annotations written in Indonesian. It uses Krippendorff's alpha for evaluation of inter-annotator agreement and benchmarks across multiple LLMs to assess output quality.

**Key Contributions:**

	1. Introduction of the first human-authored Indonesian preference dataset
	2. Demonstration of strong inter-annotator reliability
	3. Benchmarking across multiple LLMs for output quality assessment

**Result:** Strong inter-annotator agreement was achieved, indicating reliability, and the dataset serves as a benchmark for quality evaluation in LLM outputs.

**Limitations:** 

**Conclusion:** IndoPref offers a valuable resource for improving LLM performance in understanding and generating Indonesian text.

**Abstract:** Over 200 million people speak Indonesian, yet the language remains significantly underrepresented in preference-based research for large language models (LLMs). Most existing multilingual datasets are derived from English translations, often resulting in content that lacks cultural and linguistic authenticity. To address this gap, we introduce IndoPref, the first fully human-authored and multi-domain Indonesian preference dataset specifically designed to evaluate the naturalness and quality of LLM-generated text. All annotations are natively written in Indonesian and evaluated using Krippendorff's alpha, demonstrating strong inter-annotator agreement. Additionally, we benchmark the dataset across multiple LLMs and assess the output quality of each model.

</details>


### [22] [Persona-Augmented Benchmarking: Evaluating LLMs Across Diverse Writing Styles](https://arxiv.org/abs/2507.22168)

*Kimberly Le Truong, Riccardo Fogliato, Hoda Heidari, Zhiwei Steven Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, writing style diversity, benchmark evaluation, persona-based prompting, LLM performance

**Relevance Score:** 9

**TL;DR:** The paper explores the limitations of current benchmarks for evaluating Large Language Models (LLMs) due to a lack of writing style diversity, proposing a method to enhance benchmark evaluations by incorporating persona-based LLM prompting to emulate diverse styles.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM benchmarks fail to capture the variety of human communication styles, potentially leading to fragile model performance under non-standard input.

**Method:** The authors rewrote evaluation prompts using a persona-based LLM prompting method to evaluate the impact of diverse writing styles on LLM performance.

**Key Contributions:**

	1. Proposed a low-cost method for emulating diverse writing styles in LLM evaluations.
	2. Demonstrated that writing style variations impact LLM performance even with identical semantic content.
	3. Identified distinct writing styles that consistently affect LLM performance across various models.

**Result:** Variations in writing style and prompt formatting significantly influence LLM performance, with specific styles consistently leading to either low or high performance across different LLMs.

**Limitations:** 

**Conclusion:** Incorporating diverse writing styles in benchmarks can improve their validity for assessing LLM performance across linguistic variations.

**Abstract:** Current benchmarks for evaluating Large Language Models (LLMs) often do not exhibit enough writing style diversity, with many adhering primarily to standardized conventions. Such benchmarks do not fully capture the rich variety of communication patterns exhibited by humans. Thus, it is possible that LLMs, which are optimized on these benchmarks, may demonstrate brittle performance when faced with "non-standard" input. In this work, we test this hypothesis by rewriting evaluation prompts using persona-based LLM prompting, a low-cost method to emulate diverse writing styles. Our results show that, even with identical semantic content, variations in writing style and prompt formatting significantly impact the estimated performance of the LLM under evaluation. Notably, we identify distinct writing styles that consistently trigger either low or high performance across a range of models and tasks, irrespective of model family, size, and recency. Our work offers a scalable approach to augment existing benchmarks, improving the external validity of the assessments they provide for measuring LLM performance across linguistic variations.

</details>


### [23] [A Scalable Pipeline for Estimating Verb Frame Frequencies Using Large Language Models](https://arxiv.org/abs/2507.22187)

*Adam M. Morgan, Adeen Flinker*

**Main category:** cs.CL

**Keywords:** Verb Frame Frequencies, syntactic parsing, large language models

**Relevance Score:** 6

**TL;DR:** This paper presents an automated pipeline utilizing large language models to estimate Verb Frame Frequencies (VFFs), which capture the frequency of verbs in specific syntactic frames, outperforming existing syntactic parsers while being resource-efficient.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing tools for calculating Verb Frame Frequencies are limited in scale, accuracy, or accessibility, creating a need for a more efficient approach.

**Method:** Utilizing large language models to generate sentences with 476 English verbs and instructing an LLM to analyze their syntactic structures, leading to the development of a pipeline for estimating VFFs.

**Key Contributions:**

	1. Automation of Verb Frame Frequency estimation using LLMs
	2. Outperformance of traditional syntactic parsers
	3. Creation of a new VFF database with extensive verb coverage

**Result:** The pipeline outperforms two widely used syntactic parsers and enables rapid estimation of VFFs with broader verb coverage and finer-grained syntactic distinctions.

**Limitations:** 

**Conclusion:** The automated pipeline effectively demonstrates the feasibility of VFF estimation and releases code and data to aid further research.

**Abstract:** We present an automated pipeline for estimating Verb Frame Frequencies (VFFs), the frequency with which a verb appears in particular syntactic frames. VFFs provide a powerful window into syntax in both human and machine language systems, but existing tools for calculating them are limited in scale, accuracy, or accessibility. We use large language models (LLMs) to generate a corpus of sentences containing 476 English verbs. Next, by instructing an LLM to behave like an expert linguist, we had it analyze the syntactic structure of the sentences in this corpus. This pipeline outperforms two widely used syntactic parsers across multiple evaluation datasets. Furthermore, it requires far fewer resources than manual parsing (the gold-standard), thereby enabling rapid, scalable VFF estimation. Using the LLM parser, we produce a new VFF database with broader verb coverage, finer-grained syntactic distinctions, and explicit estimates of the relative frequencies of structural alternates commonly studied in psycholinguistics. The pipeline is easily customizable and extensible to new verbs, syntactic frames, and even other languages. We present this work as a proof of concept for automated frame frequency estimation, and release all code and data to support future research.

</details>


### [24] [The role of media memorability in facilitating startups' access to venture capital funding](https://arxiv.org/abs/2507.22201)

*L. Toschi, S. Torrisi, A. Fronzetti Colladon*

**Main category:** cs.CL

**Keywords:** media reputation, venture capital, investment outcomes, entrepreneurial finance, media memorability

**Relevance Score:** 2

**TL;DR:** The paper explores how media memorability influences venture capital investment decisions in startups, showing that detailed media content cues affect funding outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance understanding of media influence on venture capital funding beyond just general media exposure.

**Method:** Analysis of data from 197 UK micro and nanotechnology startups funded between 1995 and 2004, focusing on the significance of media memorability.

**Key Contributions:**

	1. Introduction of the concept of media memorability
	2. Demonstration of the impact of media content nuances on venture funding
	3. Providing strategic recommendations for startups on media engagement

**Result:** Media memorability significantly influences investment outcomes, independent of general media exposure, highlighting cues like distinctiveness and connectivity in news.

**Limitations:** 

**Conclusion:** Startups should aim for meaningful media coverage that enhances brand memorability over mere frequency of mentions.

**Abstract:** Media reputation plays an important role in attracting venture capital investment. However, prior research has focused too narrowly on general media exposure, limiting our understanding of how media truly influences funding decisions. As informed decision-makers, venture capitalists respond to more nuanced aspects of media content. We introduce the concept of media memorability - the media's ability to imprint a startup's name in the memory of relevant investors. Using data from 197 UK startups in the micro and nanotechnology sector (funded between 1995 and 2004), we show that media memorability significantly influences investment outcomes. Our findings suggest that venture capitalists rely on detailed cues such as a startup's distinctiveness and connectivity within news semantic networks. This contributes to research on entrepreneurial finance and media legitimation. In practice, startups should go beyond frequent media mentions to strengthen brand memorability through more targeted, meaningful coverage highlighting their uniqueness and relevance within the broader industry conversation.

</details>


### [25] [How Well Does First-Token Entropy Approximate Word Entropy as a Psycholinguistic Predictor?](https://arxiv.org/abs/2507.22209)

*Christian Clark, Byung-Doh Oh, William Schuler*

**Main category:** cs.CL

**Keywords:** contextual entropy, language model, Monte Carlo, reading times, psycholinguistics

**Relevance Score:** 4

**TL;DR:** This paper presents improved methods for estimating contextual entropy of words using Monte Carlo techniques, pointing out limitations of first-token approximations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the estimation of contextual entropy, which captures anticipated processing difficulty of words, especially in light of limitations of current first-token methods.

**Method:** Monte Carlo estimates of word entropy are generated, allowing for variable token spans, and regression experiments on reading times are conducted to assess their effects.

**Key Contributions:**

	1. Proposes a novel Monte Carlo method for estimating word entropy.
	2. Shows that first-token approximations can misrepresent contextual entropy.
	3. Provides empirical evidence through reading time regression experiments.

**Result:** The study shows divergent results between first-token estimates and Monte Carlo word entropy in regression experiments, highlighting the limitations of standard methods.

**Limitations:** Does not explore the broader applications beyond reading times or different contexts of use.

**Conclusion:** First-token approximations of contextual entropy can misrepresent true word processing difficulty, emphasizing the need for more accurate estimation methods.

**Abstract:** Contextual entropy is a psycholinguistic measure capturing the anticipated difficulty of processing a word just before it is encountered. Recent studies have tested for entropy-related effects as a potential complement to well-known effects from surprisal. For convenience, entropy is typically estimated based on a language model's probability distribution over a word's first subword token. However, this approximation results in underestimation and potential distortion of true word entropy. To address this, we generate Monte Carlo (MC) estimates of word entropy that allow words to span a variable number of tokens. Regression experiments on reading times show divergent results between first-token and MC word entropy, suggesting a need for caution in using first-token approximations of contextual entropy.

</details>


### [26] [RL from Teacher-Model Refinement: Gradual Imitation Learning for Machine Translation](https://arxiv.org/abs/2507.22219)

*Dongyub Jude Lee, Zhenyi Ye, Pengcheng He*

**Main category:** cs.CL

**Keywords:** Machine Translation, Reinforcement Learning, Teacher-Model Refinement, Preference Learning

**Relevance Score:** 7

**TL;DR:** Proposes a novel reinforcement learning framework (RLfR) for machine translation that removes reliance on triplet datasets by utilizing feedback from a teacher model (GPT-4o).

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses limitations of existing preference-learning methods in machine translation that depend on curated datasets and struggle with generalization.

**Method:** Reinforcement Learning from Teacher-Model Refinement (RLfR) uses continuous feedback from a teacher model to guide translation improvements, where each step serves as a micro-tutorial.

**Key Contributions:**

	1. Introduces RLfR framework for machine translation
	2. Utilizes GPT-4o as a dynamic teacher model for feedback
	3. Achieves significant gains in semantic adequacy and entity preservation metrics.

**Result:** On the FLORES-200 benchmark, RLfR outperforms MT-SFT and preference-based methods, showing significant improvements in COMET and M-ETA scores for various language pairs.

**Limitations:** 

**Conclusion:** RLfR effectively imitates human learning through iterative feedback, enhancing machine translation outcomes without static triplet dependency.

**Abstract:** Preference-learning methods for machine translation (MT)--such as Direct Preference Optimization (DPO)--have achieved impressive gains but depend heavily on large, carefully curated triplet datasets and often struggle to generalize beyond their tuning domains. We propose Reinforcement Learning from Teacher-Model Refinement (RLfR), a novel framework that removes reliance on static triplets by leveraging continuous, high-quality feedback from an external teacher model (GPT-4o). RLfR frames each translation step as a micro-tutorial: the actor generates a hypothesis, the teacher refines it, and the actor is rewarded based on how closely it aligns with the teacher's refinement. Guided by two complementary signals--(i) negative edit distance, promoting lexical and structural fidelity, and (ii) COMET score, ensuring semantic adequacy--the actor progressively learns to emulate the teacher, mirroring a human learning process through incremental, iterative improvement. On the FLORES-200 benchmark (English to and from German, Spanish, Chinese, Korean, and Japanese), RLfR consistently outperforms both MT-SFT and preference-based baselines, significantly improving COMET (semantic adequacy) and M-ETA (entity preservation) scores.

</details>


### [27] [Meaning-infused grammar: Gradient Acceptability Shapes the Geometric Representations of Constructions in LLMs](https://arxiv.org/abs/2507.22286)

*Supantho Rakshit, Adele Goldberg*

**Main category:** cs.CL

**Keywords:** Large Language Models, usage-based constructionist approach, neural representations

**Relevance Score:** 7

**TL;DR:** This study analyzes the internal representations of English dative constructions in LLMs, specifically Pythia-$1.4$B, revealing that LLMs learn meaning-infused, graded representations of constructions based on preference strength.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether LLMs reflect the function-infused gradience proposed by the usage-based constructionist approach to language.

**Method:** A macro-level geometric analysis was conducted on the neural representations of English dative constructions using a dataset of 5000 sentence pairs, assessing their separability through Energy Distance and Jensen-Shannon Divergence.

**Key Contributions:**

	1. Analysis of LLM representations of language constructions using geometric measures
	2. Evidence of meaning-infused graded representations in LLMs
	3. Insights into the internal workings of LLMs related to language construction theories

**Result:** The analysis demonstrated that the separability of construction representations in LLMs is modulated by gradient preference strength, with more prototypical examples occupying distinct regions in the activation space.

**Limitations:** 

**Conclusion:** The findings provide strong evidence that LLMs encode rich, meaning-infused, graded representations of language constructions, supporting basic constructionist principles.

**Abstract:** The usage-based constructionist (UCx) approach posits that language comprises a network of learned form-meaning pairings (constructions) whose use is largely determined by their meanings or functions, requiring them to be graded and probabilistic. This study investigates whether the internal representations in Large Language Models (LLMs) reflect the proposed function-infused gradience. We analyze the neural representations of the English dative constructions (Double Object and Prepositional Object) in Pythia-$1.4$B, using a dataset of $5000$ sentence pairs systematically varied for human-rated preference strength. A macro-level geometric analysis finds that the separability between construction representations, as measured by Energy Distance or Jensen-Shannon Divergence, is systematically modulated by gradient preference strength. More prototypical exemplars of each construction occupy more distinct regions in the activation space of LLMs. These results provide strong evidence that LLMs learn rich, meaning-infused, graded representations of constructions and offer support for geometric measures of basic constructionist principles in LLMs.

</details>


### [28] [Intent Recognition and Out-of-Scope Detection using LLMs in Multi-party Conversations](https://arxiv.org/abs/2507.22289)

*Galo Castillo-LÃ³pez, GaÃ«l de Chalendar, Nasredine Semmar*

**Main category:** cs.CL

**Keywords:** intent recognition, task-oriented dialogue systems, BERT, LLMs, zero-shot learning

**Relevance Score:** 8

**TL;DR:** This paper proposes a hybrid approach combining BERT and LLMs for intent recognition and Out-of-Scope detection in task-oriented dialogue systems, effectively utilizing zero and few-shot learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve intent recognition and OOS detection in task-oriented dialogue systems (TODS) without requiring large amounts of annotated data.

**Method:** The proposed hybrid approach combines the generalization power of large language models (LLMs) with the computational efficiency of BERT, particularly in zero and few-shot settings.

**Key Contributions:**

	1. Hybrid methodology using BERT and LLMs
	2. Effective in zero and few-shot settings
	3. Performance improvement through inter-model information sharing

**Result:** The evaluation on multi-party conversation corpora shows that sharing information from BERT outputs to LLMs enhances the performance of the system.

**Limitations:** 

**Conclusion:** The proposed method demonstrates significant improvements in intent recognition and OOS detection with minimal annotated data, making it a viable option for TODS.

**Abstract:** Intent recognition is a fundamental component in task-oriented dialogue systems (TODS). Determining user intents and detecting whether an intent is Out-of-Scope (OOS) is crucial for TODS to provide reliable responses. However, traditional TODS require large amount of annotated data. In this work we propose a hybrid approach to combine BERT and LLMs in zero and few-shot settings to recognize intents and detect OOS utterances. Our approach leverages LLMs generalization power and BERT's computational efficiency in such scenarios. We evaluate our method on multi-party conversation corpora and observe that sharing information from BERT outputs to LLMs leads to system performance improvement.

</details>


### [29] [A Comprehensive Taxonomy of Negation for NLP and Neural Retrievers](https://arxiv.org/abs/2507.22337)

*Roxana Petcu, Samarth Bhargav, Maarten de Rijke, Evangelos Kanoulas*

**Main category:** cs.CL

**Keywords:** negation, neural information retrieval, benchmark datasets

**Relevance Score:** 6

**TL;DR:** The paper explores the challenges posed by negation in neural information retrieval, proposing a taxonomy, benchmark datasets, and a classification mechanism for improved model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underperformance of neural models on queries containing negation, which is critical for fulfilling user information needs.

**Method:** The study introduces a taxonomy of negation and generates benchmark datasets for evaluating and fine-tuning neural retrieval models, alongside a logic-based classification mechanism.

**Key Contributions:**

	1. Introduction of a taxonomy of negation from various definitions
	2. Creation of benchmark datasets for model evaluation and fine-tuning
	3. Development of a logic-based classification mechanism for analyzing retrieval model performance

**Result:** A balanced data distribution over negation types is established, leading to improved training setups and faster convergence on the NevIR dataset.

**Limitations:** 

**Conclusion:** The proposed classification schema provides insights into negation coverage in datasets, potentially enhancing fine-tuned model generalization on negation tasks.

**Abstract:** Understanding and solving complex reasoning tasks is vital for addressing the information needs of a user. Although dense neural models learn contextualised embeddings, they still underperform on queries containing negation. To understand this phenomenon, we study negation in both traditional neural information retrieval and LLM-based models. We (1) introduce a taxonomy of negation that derives from philosophical, linguistic, and logical definitions; (2) generate two benchmark datasets that can be used to evaluate the performance of neural information retrieval models and to fine-tune models for a more robust performance on negation; and (3) propose a logic-based classification mechanism that can be used to analyze the performance of retrieval models on existing datasets. Our taxonomy produces a balanced data distribution over negation types, providing a better training setup that leads to faster convergence on the NevIR dataset. Moreover, we propose a classification schema that reveals the coverage of negation types in existing datasets, offering insights into the factors that might affect the generalization of fine-tuned models on negation.

</details>


### [30] [Traits Run Deep: Enhancing Personality Assessment via Psychology-Guided LLM Representations and Multimodal Apparent Behaviors](https://arxiv.org/abs/2507.22367)

*Jia Li, Yichao He, Jiacheng Xu, Tianhao Luo, Zhenzhen Hu, Richang Hong, Meng Wang*

**Main category:** cs.CL

**Keywords:** personality assessment, machine learning, multimodal integration, large language models, behavioral analysis

**Relevance Score:** 7

**TL;DR:** The paper proposes a novel framework, Traits Run Deep, for personality assessment that utilizes psychology-informed prompts and a Text-Centric Trait Fusion Network to integrate multiple modalities effectively, leading to improved accuracy in personality trait evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate personality assessment is crucial in fields like mental health diagnostics and personalized education, but traditional methods struggle with cross-modal representation.

**Method:** The framework uses psychology-informed prompts to elicit personality-relevant semantic representations and a fusion network to integrate text with audio-visual features, employing components like a Chunk-Wise Projector and a Cross-Modal Connector.

**Key Contributions:**

	1. Introduction of psychology-informed prompts for LLMs in personality assessment
	2. Development of a Text-Centric Trait Fusion Network for cross-modal integration
	3. Significant reduction in MSE and top ranking in official challenge evaluations

**Result:** The method shows approximately a 45% reduction in mean squared error (MSE) and ranks first in the Personality Assessment track of the AVI Challenge 2025 test set evaluations.

**Limitations:** 

**Conclusion:** The proposed method effectively improves personality assessment accuracy by addressing cross-modal understanding through innovative integration strategies.

**Abstract:** Accurate and reliable personality assessment plays a vital role in many fields, such as emotional intelligence, mental health diagnostics, and personalized education. Unlike fleeting emotions, personality traits are stable, often subconsciously leaked through language, facial expressions, and body behaviors, with asynchronous patterns across modalities. It was hard to model personality semantics with traditional superficial features and seemed impossible to achieve effective cross-modal understanding. To address these challenges, we propose a novel personality assessment framework called \textit{\textbf{Traits Run Deep}}. It employs \textit{\textbf{psychology-informed prompts}} to elicit high-level personality-relevant semantic representations. Besides, it devises a \textit{\textbf{Text-Centric Trait Fusion Network}} that anchors rich text semantics to align and integrate asynchronous signals from other modalities. To be specific, such fusion module includes a Chunk-Wise Projector to decrease dimensionality, a Cross-Modal Connector and a Text Feature Enhancer for effective modality fusion and an ensemble regression head to improve generalization in data-scarce situations. To our knowledge, we are the first to apply personality-specific prompts to guide large language models (LLMs) in extracting personality-aware semantics for improved representation quality. Furthermore, extracting and fusing audio-visual apparent behavior features further improves the accuracy. Experimental results on the AVI validation set have demonstrated the effectiveness of the proposed components, i.e., approximately a 45\% reduction in mean squared error (MSE). Final evaluations on the test set of the AVI Challenge 2025 confirm our method's superiority, ranking first in the Personality Assessment track. The source code will be made available at https://github.com/MSA-LMC/TraitsRunDeep.

</details>


### [31] [PATENTWRITER: A Benchmarking Study for Patent Drafting with LLMs](https://arxiv.org/abs/2507.22387)

*Homaira Huda Shomee, Suman Kalyan Maity, Sourav Medya*

**Main category:** cs.CL

**Keywords:** Patent Writing, Large Language Models, Benchmarking Framework, Natural Language Processing, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** This paper introduces PATENTWRITER, a benchmarking framework for evaluating large language models (LLMs) in generating patent abstracts, analyzing their output quality against established metrics and tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in the tedious and complex process of patent writing by utilizing large language models for abstract generation.

**Method:** The study evaluates six leading LLMs, including GPT-4 and LLaMA-3, under various prompting strategies (zero-shot, few-shot, and chain-of-thought) while assessing output quality with standard NLP metrics and conducting stylistic analysis.

**Key Contributions:**

	1. Introduction of PATENTWRITER as a unified benchmarking framework for LLMs in patent abstract generation
	2. Comprehensive evaluation of LLM output quality using a diverse set of metrics
	3. Open-sourcing code and dataset for reproducibility and future exploration

**Result:** Experimental results demonstrate that modern LLMs generate high-fidelity patent abstracts that often outperform domain-specific baselines across multiple quality dimensions.

**Limitations:** The study focuses on patent abstract generation and does not cover other aspects of patent writing or broader implications of LLMs in different scientific domains.

**Conclusion:** The results suggest that LLMs can effectively improve patent writing processes, with the framework and datasets provided for future research.

**Abstract:** Large language models (LLMs) have emerged as transformative approaches in several important fields. This paper aims for a paradigm shift for patent writing by leveraging LLMs to overcome the tedious patent-filing process. In this work, we present PATENTWRITER, the first unified benchmarking framework for evaluating LLMs in patent abstract generation. Given the first claim of a patent, we evaluate six leading LLMs -- including GPT-4 and LLaMA-3 -- under a consistent setup spanning zero-shot, few-shot, and chain-of-thought prompting strategies to generate the abstract of the patent. Our benchmark PATENTWRITER goes beyond surface-level evaluation: we systematically assess the output quality using a comprehensive suite of metrics -- standard NLP measures (e.g., BLEU, ROUGE, BERTScore), robustness under three types of input perturbations, and applicability in two downstream patent classification and retrieval tasks. We also conduct stylistic analysis to assess length, readability, and tone. Experimental results show that modern LLMs can generate high-fidelity and stylistically appropriate patent abstracts, often surpassing domain-specific baselines. Our code and dataset are open-sourced to support reproducibility and future research.

</details>


### [32] [Question Generation for Assessing Early Literacy Reading Comprehension](https://arxiv.org/abs/2507.22410)

*Xiaocheng Yang, Sumuk Shashidhar, Dilek Hakkani-Tur*

**Main category:** cs.CL

**Keywords:** Reading Comprehension, Language Models, AI in Education

**Relevance Score:** 4

**TL;DR:** Proposes a method for generating comprehension questions for K-2 English learners, adapting to individual proficiencies and ensuring comprehensive evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reading comprehension assessment through automated, tailored question generation for young English learners.

**Method:** The paper evaluates various language models within a framework designed to produce diverse comprehension questions based on the FairytaleQA dataset.

**Key Contributions:**

	1. Novel question generation method for reading comprehension
	2. Adaptation to individual learner proficiencies
	3. Diverse question types and difficulty levels

**Result:** The proposed method demonstrates effective coverage of material and adaptability to learner proficiency, enabling a wide range of question types and difficulty levels.

**Limitations:** 

**Conclusion:** This approach could significantly contribute to autonomous AI-driven English instruction, improving the reading acquisition process for K-2 learners.

**Abstract:** Assessment of reading comprehension through content-based interactions plays an important role in the reading acquisition process. In this paper, we propose a novel approach for generating comprehension questions geared to K-2 English learners. Our method ensures complete coverage of the underlying material and adaptation to the learner's specific proficiencies, and can generate a large diversity of question types at various difficulty levels to ensure a thorough evaluation. We evaluate the performance of various language models in this framework using the FairytaleQA dataset as the source material. Eventually, the proposed approach has the potential to become an important part of autonomous AI-driven English instructors.

</details>


### [33] [NeedleChain: Measuring Intact Long-Context Reasoning Capability of Large Language Models](https://arxiv.org/abs/2507.22411)

*Hyeonseok Moon, Heuiseok Lim*

**Main category:** cs.CL

**Keywords:** Long Contexts, Large Language Models, Benchmarking, NeedleChain, ROPE Contraction

**Relevance Score:** 9

**TL;DR:** This paper introduces a new benchmark called NeedleChain to better evaluate Large Language Models' understanding of long contexts, revealing limitations in current methods like Needle-in-a-Haystack.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The existing Needle-in-a-Haystack benchmark may overestimate LLMs' long-context understanding abilities, prompting the need for a more effective evaluation methodology.

**Method:** The authors propose NeedleChain, a benchmark with contexts made entirely of query-relevant information to accurately assess LLM performance on long-context tasks. They also introduce ROPE Contraction to improve understanding capability.

**Key Contributions:**

	1. Introduction of NeedleChain benchmark for LLM evaluation
	2. Proposal of ROPE Contraction strategy to enhance LLM understanding
	3. Clear demonstration of LLM performance gaps in long-context tasks

**Result:** Experiments show that advanced LLMs demonstrate a significant gap between processing large contexts and fully understanding them, highlighting the inadequacy of existing evaluation methods.

**Limitations:** The study focuses on the limitations of current benchmarks and may not address all aspects of LLM comprehension.

**Conclusion:** NeedleChain provides a superior framework for evaluating LLMs in terms of understanding long contexts, emphasizing the importance of relevant information in the evaluation process.

**Abstract:** The Needle-in-a-Haystack (NIAH) benchmark is widely used to evaluate Large Language Models' (LLMs) ability to understand long contexts (LC). It evaluates the capability to identify query-relevant context within extensive query-irrelevant passages. Although this method serves as a widely accepted standard for evaluating long-context understanding, our findings suggest it may overestimate the true LC capability of LLMs. We demonstrate that even state-of-the-art models such as GPT-4o struggle to intactly incorporate given contexts made up of solely query-relevant ten sentences. In response, we introduce a novel benchmark, \textbf{NeedleChain}, where the context consists entirely of query-relevant information, requiring the LLM to fully grasp the input to answer correctly. Our benchmark allows for flexible context length and reasoning order, offering a more comprehensive analysis of LLM performance. Additionally, we propose an extremely simple yet compelling strategy to improve LC understanding capability of LLM: ROPE Contraction. Our experiments with various advanced LLMs reveal a notable disparity between their ability to process large contexts and their capacity to fully understand them. Source code and datasets are available at https://github.com/hyeonseokk/NeedleChain

</details>


### [34] [AI-generated stories favour stability over change: homogeneity and cultural stereotyping in narratives generated by gpt-4o-mini](https://arxiv.org/abs/2507.22445)

*Jill Walker Rettberg, Hermann Wigers*

**Main category:** cs.CL

**Keywords:** AI narratives, cultural relevance, narrative bias, literary studies, generative AI

**Relevance Score:** 6

**TL;DR:** The paper investigates whether a language model can generate culturally relevant stories for various nationalities, finding that while surface-level national symbols are present, the narratives are homogenized with a focus on nostalgia and resolution of minor conflicts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the cultural relevance of AI-generated stories across different nationalities and identify any biases present in their narratives.

**Method:** The authors generated 11,800 stories by prompting a language model with specific nationality themes and analyzed the resulting narratives for structural similarities and cultural representations.

**Key Contributions:**

	1. Identifying structural homogeneity as a form of AI bias
	2. Highlighting the limitations of AI in generating culturally diverse narratives
	3. Emphasizing the implications of narrative standardization for literary and AI studies

**Result:** The generated stories predominantly follow a single narrative arc, focusing on characters returning to small towns, resolving minor conflicts, and emphasizing tradition, leading to a homogenized narrative form.

**Limitations:** The narratives lack depth in real-world conflicts and romantic elements, potentially limiting their engagement and relevance to actual cultural experiences.

**Conclusion:** Narrative homogenization in AI-generated stories reflects a distinct bias that prioritizes stability and tradition, highlighting a need for critical examination of generative AI outputs in cultural contexts.

**Abstract:** Can a language model trained largely on Anglo-American texts generate stories that are culturally relevant to other nationalities? To find out, we generated 11,800 stories - 50 for each of 236 countries - by sending the prompt "Write a 1500 word potential {demonym} story" to OpenAI's model gpt-4o-mini. Although the stories do include surface-level national symbols and themes, they overwhelmingly conform to a single narrative plot structure across countries: a protagonist lives in or returns home to a small town and resolves a minor conflict by reconnecting with tradition and organising community events. Real-world conflicts are sanitised, romance is almost absent, and narrative tension is downplayed in favour of nostalgia and reconciliation. The result is a narrative homogenisation: an AI-generated synthetic imaginary that prioritises stability above change and tradition above growth. We argue that the structural homogeneity of AI-generated narratives constitutes a distinct form of AI bias, a narrative standardisation that should be acknowledged alongside the more familiar representational bias. These findings are relevant to literary studies, narratology, critical AI studies, NLP research, and efforts to improve the cultural alignment of generative AI.

</details>


### [35] [Falcon-H1: A Family of Hybrid-Head Language Models Redefining Efficiency and Performance](https://arxiv.org/abs/2507.22448)

*Jingwei Zuo, Maksim Velikanov, Ilyas Chahed, Younes Belkada, Dhia Eddine Rhayem, Guillaume Kunsch, Hakim Hacid, Hamza Yous, Brahim Farhat, Ibrahim Khadraoui, Mugariya Farooq, Giulia Campesan, Ruxandra Cojocaru, Yasser Djilali, Shi Hu, Iheb Chaabane, Puneesh Khanna, Mohamed El Amine Seddik, Ngoc Dung Huynh, Phuc Le Khac, Leen AlQadi, Billel Mokeddem, Mohamed Chami, Abdalgader Abubaker, Mikhail Lubinets, Kacper Piskorski, Slim Frikha*

**Main category:** cs.CL

**Keywords:** large language models, hybrid architecture, open-source AI

**Relevance Score:** 9

**TL;DR:** Introduction of Falcon-H1, a series of large language models with a hybrid architecture that optimizes performance and efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge conventional practices in LLM design, focusing on computational efficiency and performance across a range of applications.

**Method:** Adopting a parallel hybrid approach combining Transformer-based attention with State Space Models (SSMs). Released models include various sizes with instruction tuning and quantization strategies for efficiency.

**Key Contributions:**

	1. Introduction of Falcon-H1 hybrid architecture
	2. Performance comparable to models up to 70B with fewer parameters
	3. Wide application support with up to 256K context tokens in 18 languages

**Result:** Falcon-H1 models outperform existing models in various benchmarks while utilizing fewer parameters and less data, showcasing strong performance in reasoning, multilingual tasks, and instruction following.

**Limitations:** 

**Conclusion:** Falcon-H1 models, available under an open-source license, advance the state of LLMs and are aimed at promoting accessible AI research.

**Abstract:** In this report, we introduce Falcon-H1, a new series of large language models (LLMs) featuring hybrid architecture designs optimized for both high performance and efficiency across diverse use cases. Unlike earlier Falcon models built solely on Transformer or Mamba architectures, Falcon-H1 adopts a parallel hybrid approach that combines Transformer-based attention with State Space Models (SSMs), known for superior long-context memory and computational efficiency. We systematically revisited model design, data strategy, and training dynamics, challenging conventional practices in the field. Falcon-H1 is released in multiple configurations, including base and instruction-tuned variants at 0.5B, 1.5B, 1.5B-deep, 3B, 7B, and 34B parameters. Quantized instruction-tuned models are also available, totaling over 30 checkpoints on Hugging Face Hub. Falcon-H1 models demonstrate state-of-the-art performance and exceptional parameter and training efficiency. The flagship Falcon-H1-34B matches or outperforms models up to 70B scale, such as Qwen3-32B, Qwen2.5-72B, and Llama3.3-70B, while using fewer parameters and less data. Smaller models show similar trends: the Falcon-H1-1.5B-Deep rivals current leading 7B-10B models, and Falcon-H1-0.5B performs comparably to typical 7B models from 2024. These models excel across reasoning, mathematics, multilingual tasks, instruction following, and scientific knowledge. With support for up to 256K context tokens and 18 languages, Falcon-H1 is suitable for a wide range of applications. All models are released under a permissive open-source license, underscoring our commitment to accessible and impactful AI research.

</details>


### [36] [What is an "Abstract Reasoner"? Revisiting Experiments and Arguments about Large Language Models](https://arxiv.org/abs/2507.22457)

*Tian Yun, Chen Sun, Ellie Pavlick*

**Main category:** cs.CL

**Keywords:** Large Language Models, Abstract Reasoning, Parameter Tuning, Zero-Shot Performance, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper examines the performance of large language models (LLMs) as abstract reasoners, finding that while they struggle in zero-shot settings, some parameter tuning can significantly improve their performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To add nuance to the claim that LLMs are not 'abstract reasoners' by re-examining their performance on zero-shot tasks.

**Method:** The authors revisit previous experiments and analyze the effects of tuning a small subset of parameters for input encoding on LLM performance.

**Key Contributions:**

	1. Re-evaluation of LLMs' capacities as abstract reasoners.
	2. Demonstration that tuning input encoding parameters can enable significant performance improvements.
	3. Discussion on the transferability of tuning results across datasets.

**Result:** The results show that while LLMs perform poorly in a zero-shot setting, tuning parameters can greatly enhance their results, although this improvement does not generalize across different datasets.

**Limitations:** Improvements from tuning do not necessarily apply across various datasets, indicating a lack of generalizability.

**Conclusion:** The paper concludes by suggesting a need to re-evaluate what it means to be an 'abstract reasoner' and the implications of LLM performance in this regard.

**Abstract:** Recent work has argued that large language models (LLMs) are not "abstract reasoners", citing their poor zero-shot performance on a variety of challenging tasks as evidence. We revisit these experiments in order to add nuance to the claim. First, we show that while LLMs indeed perform poorly in a zero-shot setting, even tuning a small subset of parameters for input encoding can enable near-perfect performance. However, we also show that this finetuning does not necessarily transfer across datasets. We take this collection of empirical results as an invitation to (re-)open the discussion of what it means to be an "abstract reasoner", and why it matters whether LLMs fit the bill.

</details>


### [37] [IFEvalCode: Controlled Code Generation](https://arxiv.org/abs/2507.22462)

*Jian Yang, Wei Zhang, Shukai Liu, Linzheng Chai, Yingshui Tan, Jiaheng Liu, Ge Zhang, Wangchunshu Zhou, Guanglin Niu, Zhoujun Li, Binyuan Hui, Junyang Lin*

**Main category:** cs.CL

**Keywords:** Code LLMs, Instruction-Following, Benchmarking, Code Generation, Multilingual

**Relevance Score:** 8

**TL;DR:** The paper presents a novel approach to enhance the instruction-following capability of Code LLMs through the generation of forward and backward constraints, and introduces a new benchmark, IFEvalCode, for evaluating code generation quality across multiple programming languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current Code LLMs excel at generating correct code but often fail to meet detailed human-defined requirements such as coding style and structure. This paper aims to improve adherence to such guidelines in code generation.

**Method:** The authors introduce a framework for generating forward and backward constraints to enforce stricter instruction adherence in Code LLMs. They also develop IFEvalCode, a multilingual benchmark for evaluating code generation that measures correctness and instruction-following separately.

**Key Contributions:**

	1. Introduction of forward and backward constraints to improve Code LLM instruction-following.
	2. Development of the IFEvalCode benchmark for nuanced evaluation of code generation.
	3. Demonstration of performance differences between closed-source and open-source models on instruction adherence.

**Result:** Experiments show that closed-source models significantly outperform open-source models in producing code that not only works correctly but also aligns with additional instructions and styles. The gap between correct code generation and instruction-following is highlighted.

**Limitations:** 

**Conclusion:** The findings suggest that enhancing instruction-following capabilities in Code LLMs can lead to better alignment with user requirements in real-world applications, and that the IFEvalCode benchmark can provide valuable insights into code generation performance.

**Abstract:** Code large language models (Code LLMs) have made significant progress in code generation by translating natural language descriptions into functional code; however, real-world applications often demand stricter adherence to detailed requirements such as coding style, line count, and structural constraints, beyond mere correctness. To address this, the paper introduces forward and backward constraints generation to improve the instruction-following capabilities of Code LLMs in controlled code generation, ensuring outputs align more closely with human-defined guidelines. The authors further present IFEvalCode, a multilingual benchmark comprising 1.6K test samples across seven programming languages (Python, Java, JavaScript, TypeScript, Shell, C++, and C#), with each sample featuring both Chinese and English queries. Unlike existing benchmarks, IFEvalCode decouples evaluation into two metrics: correctness (Corr.) and instruction-following (Instr.), enabling a more nuanced assessment. Experiments on over 40 LLMs reveal that closed-source models outperform open-source ones in controllable code generation and highlight a significant gap between the models' ability to generate correct code versus code that precisely follows instructions.

</details>


### [38] [SLM-SQL: An Exploration of Small Language Models for Text-to-SQL](https://arxiv.org/abs/2507.22478)

*Lei Sheng, Shuai-Shuai Xu*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, Small Language Models, Natural Language Processing, Reinforcement Learning, SQL Generation

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of small language models (SLMs) in Text-to-SQL tasks using advanced post-training techniques and datasets generated from SynSQL.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underperformance of small language models in translating natural language questions into SQL queries despite their advantages in inference speed and edge deployment.

**Method:** The authors applied supervised fine-tuning and reinforcement learning-based post-training on small language models, utilizing derived datasets from the SynSQL dataset for SQL generation and revision.

**Key Contributions:**

	1. Introduction of SynSQL-Think and SynSQL-Merge datasets for SQL task training and evaluation.
	2. Application of reinforcement learning in fine-tuning small language models for improved SQL generation accuracy.
	3. Demonstration of significant performance gains in Text-to-SQL tasks using small models through innovative post-training techniques.

**Result:** The five evaluated models showed an average improvement of 31.4 points on the BIRD development set, with the 0.5B model achieving 56.87% execution accuracy and the 1.5B model reaching 67.08% execution accuracy.

**Limitations:** Limited to Text-to-SQL tasks; further research is needed to generalize findings to other NLP applications.

**Conclusion:** The study validates the potential of SLMs for Text-to-SQL tasks and highlights the benefits of the applied post-training techniques, with datasets, models, and code available on GitHub.

**Abstract:** Large language models (LLMs) have demonstrated strong performance in translating natural language questions into SQL queries (Text-to-SQL). In contrast, small language models (SLMs) ranging from 0.5B to 1.5B parameters currently underperform on Text-to-SQL tasks due to their limited logical reasoning capabilities. However, SLMs offer inherent advantages in inference speed and suitability for edge deployment. To explore their potential in Text-to-SQL applications, we leverage recent advancements in post-training techniques. Specifically, we used the open-source SynSQL-2.5M dataset to construct two derived datasets: SynSQL-Think-916K for SQL generation and SynSQL-Merge-Think-310K for SQL merge revision. We then applied supervised fine-tuning and reinforcement learning-based post-training to the SLM, followed by inference using a corrective self-consistency approach. Experimental results validate the effectiveness and generalizability of our method, SLM-SQL. On the BIRD development set, the five evaluated models achieved an average improvement of 31.4 points. Notably, the 0.5B model reached 56.87\% execution accuracy (EX), while the 1.5B model achieved 67.08\% EX. We will release our dataset, model, and code to github: https://github.com/CycloneBoy/slm_sql.

</details>


### [39] [CliCARE: Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records](https://arxiv.org/abs/2507.22533)

*Dongchen Li, Jitao Liang, Wei Li, Xiaoyu Wang, Longbing Cao, Kun Yu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Decision Support, Electronic Health Records, Oncology, Knowledge Graphs

**Relevance Score:** 9

**TL;DR:** CliCARE is a framework designed to enhance clinical decision support in oncology by utilizing Large Language Models to process longitudinal EHRs and offer evidence-grounded recommendations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of using LLMs in clinical decision support for cancer care, including processing long and multilingual patient records, mitigating clinical hallucination, and improving evaluation metrics.

**Method:** CliCARE transforms unstructured longitudinal EHRs into patient-specific Temporal Knowledge Graphs, grounding the decision-making process by aligning patient trajectories with clinical guidelines.

**Key Contributions:**

	1. Development of the CliCARE framework for decision support in oncology using LLMs
	2. Transformation of EHRs into Temporal Knowledge Graphs for better context handling
	3. Demonstration of significant performance improvements over existing methods.

**Result:** CliCARE consistently outperforms leading long-context LLMs and Knowledge Graph-enhanced RAG methods in providing clinical decision support across various datasets.

**Limitations:** 

**Conclusion:** The framework validates its effectiveness through a robust evaluation protocol that correlates strongly with expert oncologist assessments, indicating high clinical validity.

**Abstract:** Large Language Models (LLMs) hold significant promise for improving clinical decision support and reducing physician burnout by synthesizing complex, longitudinal cancer Electronic Health Records (EHRs). However, their implementation in this critical field faces three primary challenges: the inability to effectively process the extensive length and multilingual nature of patient records for accurate temporal analysis; a heightened risk of clinical hallucination, as conventional grounding techniques such as Retrieval-Augmented Generation (RAG) do not adequately incorporate process-oriented clinical guidelines; and unreliable evaluation metrics that hinder the validation of AI systems in oncology. To address these issues, we propose CliCARE, a framework for Grounding Large Language Models in Clinical Guidelines for Decision Support over Longitudinal Cancer Electronic Health Records. The framework operates by transforming unstructured, longitudinal EHRs into patient-specific Temporal Knowledge Graphs (TKGs) to capture long-range dependencies, and then grounding the decision support process by aligning these real-world patient trajectories with a normative guideline knowledge graph. This approach provides oncologists with evidence-grounded decision support by generating a high-fidelity clinical summary and an actionable recommendation. We validated our framework using large-scale, longitudinal data from a private Chinese cancer dataset and the public English MIMIC-IV dataset. In these diverse settings, CliCARE significantly outperforms strong baselines, including leading long-context LLMs and Knowledge Graph-enhanced RAG methods. The clinical validity of our results is supported by a robust evaluation protocol, which demonstrates a high correlation with assessments made by expert oncologists.

</details>


### [40] [A Benchmark Dataset and Evaluation Framework for Vietnamese Large Language Models in Customer Support](https://arxiv.org/abs/2507.22542)

*Long S. T. Nguyen, Truong P. Hua, Thanh M. Nguyen, Toan Q. Pham, Nam K. Ngo, An X. Nguyen, Nghi D. M. Pham, Nghia H. Nguyen, Tho T. Quan*

**Main category:** cs.CL

**Keywords:** Vietnamese LLMs, Customer Support, QA Systems, Benchmark Dataset, Evaluation Framework

**Relevance Score:** 8

**TL;DR:** The paper introduces the Customer Support Conversations Dataset (CSConDa), a benchmark for evaluating Vietnamese LLMs (ViLLMs), and presents an evaluation framework for assessing their performance in customer service applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of domain-specific evaluations and benchmark datasets for Vietnamese LLMs in customer support, which hinders enterprises' ability to select suitable models.

**Method:** The study introduces the CSConDa dataset containing over 9,000 QA pairs from real interactions and evaluates eleven lightweight open-source ViLLMs using both automatic metrics and syntactic analysis.

**Key Contributions:**

	1. Introduction of the CSConDa dataset with over 9,000 QA pairs from real customer interactions.
	2. Benchmarking of 11 lightweight ViLLMs on the new dataset using an evaluation framework.
	3. Insights into model performance and areas for improvement in Vietnamese LLMs.

**Result:** The evaluation reveals strengths, weaknesses, and linguistic patterns of the ViLLMs, providing insights into their performance and guiding improvements for future models.

**Limitations:** Domain-specific evaluations are still limited; the dataset may not cover all possible customer interactions.

**Conclusion:** This work establishes a robust benchmark and evaluation framework that facilitates informed model selection for customer service QA and contributes to the advancement of research on Vietnamese LLMs.

**Abstract:** With the rapid growth of Artificial Intelligence, Large Language Models (LLMs) have become essential for Question Answering (QA) systems, improving efficiency and reducing human workload in customer service. The emergence of Vietnamese LLMs (ViLLMs) highlights lightweight open-source models as a practical choice for their accuracy, efficiency, and privacy benefits. However, domain-specific evaluations remain limited, and the absence of benchmark datasets reflecting real customer interactions makes it difficult for enterprises to select suitable models for support applications. To address this gap, we introduce the Customer Support Conversations Dataset (CSConDa), a curated benchmark of over 9,000 QA pairs drawn from real interactions with human advisors at a large Vietnamese software company. Covering diverse topics such as pricing, product availability, and technical troubleshooting, CSConDa provides a representative basis for evaluating ViLLMs in practical scenarios. We further present a comprehensive evaluation framework, benchmarking 11 lightweight open-source ViLLMs on CSConDa with both automatic metrics and syntactic analysis to reveal model strengths, weaknesses, and linguistic patterns. This study offers insights into model behavior, explains performance differences, and identifies key areas for improvement, supporting the development of next-generation ViLLMs. By establishing a robust benchmark and systematic evaluation, our work enables informed model selection for customer service QA and advances research on Vietnamese LLMs. The dataset is publicly available at https://huggingface.co/datasets/ura-hcmut/Vietnamese-Customer-Support-QA.

</details>


### [41] [ControlMed: Adding Reasoning Control to Medical Language Model](https://arxiv.org/abs/2507.22545)

*Sung-Min Lee, Siyoon Lee, Juyeon Kim, Kyungmin Roh*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical language model, clinical decision-making, reasoning control, reinforcement learning

**Relevance Score:** 9

**TL;DR:** ControlMed introduces a medical language model allowing users to control reasoning length, enhancing efficiency and explainability in clinical decision-making.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current LLMs in medical settings, specifically lengthy reasoning that impacts efficiency and usability.

**Method:** ControlMed is trained through a three-stage pipeline: pre-training on a large synthetic medical instruction dataset, supervised fine-tuning with length-control markers, and reinforcement learning to enhance accuracy and response quality.

**Key Contributions:**

	1. Introduction of a length-control mechanism for reasoning in LLMs
	2. Demonstration of superior or comparable performance to existing models
	3. Methodology that includes reinforcement learning for accuracy and efficiency

**Result:** ControlMed demonstrated performance on par with or better than state-of-the-art models across medical benchmarks in English and Korean, allowing users to balance reasoning accuracy with computational efficiency.

**Limitations:** 

**Conclusion:** ControlMed is a practical solution for clinical question answering, enhancing adaptability and efficiency in medical information analysis.

**Abstract:** Reasoning Large Language Models (LLMs) with enhanced accuracy and explainability are increasingly being adopted in the medical domain, as the life-critical nature of clinical decision-making demands reliable support. Despite these advancements, existing reasoning LLMs often generate unnecessarily lengthy reasoning processes, leading to significant computational overhead and response latency. These limitations hinder their practical deployment in real-world clinical environments. To address these challenges, we introduce \textbf{ControlMed}, a medical language model that enables users to actively control the length of the reasoning process at inference time through fine-grained control markers. ControlMed is trained through a three-stage pipeline: 1) pre-training on a large-scale synthetic medical instruction dataset covering both \textit{direct} and \textit{reasoning responses}; 2) supervised fine-tuning with multi-length reasoning data and explicit length-control markers; and 3) reinforcement learning with model-based reward signals to enhance factual accuracy and response quality. Experimental results on a variety of English and Korean medical benchmarks demonstrate that our model achieves similar or better performance compared to state-of-the-art models. Furthermore, users can flexibly balance reasoning accuracy and computational efficiency by controlling the reasoning length as needed. These findings demonstrate that ControlMed is a practical and adaptable solution for clinical question answering and medical information analysis.

</details>


### [42] [Exploiting Synergistic Cognitive Biases to Bypass Safety in LLMs](https://arxiv.org/abs/2507.22564)

*Xikang Yang, Biyu Zhou, Xuehai Tang, Jizhong Han, Songlin Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Cognitive biases, Adversarial attacks, AI safety, Cognitive science

**Relevance Score:** 8

**TL;DR:** This paper introduces CognitiveAttack, a framework exposing vulnerabilities in Large Language Models by exploiting cognitive biases through innovative prompt strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities of LLMs to adversarial attacks that exploit cognitive biases, enhancing the safety of AI systems.

**Method:** CognitiveAttack combines supervised fine-tuning and reinforcement learning to generate optimized prompts that leverage individual and combined cognitive biases.

**Key Contributions:**

	1. Introduction of the CognitiveAttack framework
	2. Demonstration of multi-bias interactions as a viable attack vector
	3. Significant improvement in attack success rates compared to existing methods

**Result:** CognitiveAttack significantly outperforms the current state-of-the-art method (60.1% vs. 31.6% attack success rate), identifying major vulnerabilities in 30 LLMs, especially in open-source variants.

**Limitations:** 

**Conclusion:** The findings emphasize the need for improved LLM defense mechanisms and introduce cognitive science as a critical discipline for enhancing AI safety.

**Abstract:** Large Language Models (LLMs) demonstrate impressive capabilities across a wide range of tasks, yet their safety mechanisms remain susceptible to adversarial attacks that exploit cognitive biases -- systematic deviations from rational judgment. Unlike prior jailbreaking approaches focused on prompt engineering or algorithmic manipulation, this work highlights the overlooked power of multi-bias interactions in undermining LLM safeguards. We propose CognitiveAttack, a novel red-teaming framework that systematically leverages both individual and combined cognitive biases. By integrating supervised fine-tuning and reinforcement learning, CognitiveAttack generates prompts that embed optimized bias combinations, effectively bypassing safety protocols while maintaining high attack success rates. Experimental results reveal significant vulnerabilities across 30 diverse LLMs, particularly in open-source models. CognitiveAttack achieves a substantially higher attack success rate compared to the SOTA black-box method PAP (60.1% vs. 31.6%), exposing critical limitations in current defense mechanisms. These findings highlight multi-bias interactions as a powerful yet underexplored attack vector. This work introduces a novel interdisciplinary perspective by bridging cognitive science and LLM safety, paving the way for more robust and human-aligned AI systems.

</details>


### [43] [Unveiling the Influence of Amplifying Language-Specific Neurons](https://arxiv.org/abs/2507.22581)

*Inaya Rahmanisa, Lyzander Marciano Andrylie, Krisna Mahardika Ihsani, Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, Alham Fikri Aji*

**Main category:** cs.CL

**Keywords:** Language-specific neurons, Language Steering Shift, multilingual behavior

**Relevance Score:** 7

**TL;DR:** This paper studies the amplification of language-specific neurons in LLMs across 18 languages, exploring its effects on model behavior and downstream tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the role of language-specific neurons in LLMs and their potential for improving performance across various languages, particularly in low-resource settings.

**Method:** The study investigates amplification factors by applying interventions to 18 languages, comparing effectiveness using the Language Steering Shift (LSS) score, and evaluating results on commonsense reasoning, knowledge, and translation tasks.

**Key Contributions:**

	1. Investigates the role of language-specific neurons in LLMs.
	2. Introduces Language Steering Shift (LSS) evaluation score.
	3. Analyzes amplification effects across multiple languages and tasks.

**Result:** The optimal amplification factors successfully direct model outputs towards nearly all tested languages, enhancing self-language performance in some instances but generally degrading cross-language results.

**Limitations:** General degradation of cross-language results despite some improvements in self-language performance.

**Conclusion:** Amplifying language-specific neurons can benefit multilingual behavior, especially for low-resource languages, though it offers limited advantages for cross-lingual transfer tasks.

**Abstract:** Language-specific neurons in LLMs that strongly correlate with individual languages have been shown to influence model behavior by deactivating them. However, their role in amplification remains underexplored. This work investigates the effect of amplifying language-specific neurons through interventions across 18 languages, including low-resource ones, using three models primarily trained in different languages. We compare amplification factors by their effectiveness in steering to the target language using a proposed Language Steering Shift (LSS) evaluation score, then evaluate it on downstream tasks: commonsense reasoning (XCOPA, XWinograd), knowledge (Include), and translation (FLORES). The optimal amplification factors effectively steer output toward nearly all tested languages. Intervention using this factor on downstream tasks improves self-language performance in some cases but generally degrades cross-language results. These findings highlight the effect of language-specific neurons in multilingual behavior, where amplification can be beneficial especially for low-resource languages, but provides limited advantage for cross-lingual transfer.

</details>


### [44] [BALSAM: A Platform for Benchmarking Arabic Large Language Models](https://arxiv.org/abs/2507.22603)

*Rawan Al-Matham, Kareem Darwish, Raghad Al-Rasheed, Waad Alshammari, Muneera Alhoshan, Amal Almazrua, Asma Al Wazrah, Mais Alheraki, Firoj Alam, Preslav Nakov, Norah Alzahrani, Eman alBilali, Nizar Habash, Abdelrahman El-Sheikh, Muhammad Elmallah, Haonan Li, Hamdy Mubarak, Mohamed Anwar, Zaid Alyafeai, Ahmed Abdelali, Nora Altwairesh, Maram Hasanain, Abdulmohsen Al Thubaity, Shady Shehata, Bashar Alhafni, Injy Hamed, Go Inoue, Khalid Elmadani, Ossama Obeid, Fatima Haouari, Tamer Elsayed, Emad Alghamdi, Khalid Almubarak, Saied Alshahrani, Ola Aljarrah, Safa Alajlan, Areej Alshaqarawi, Maryam Alshihri, Sultana Alghurabi, Atikah Alzeghayer, Afrah Altamimi, Abdullah Alfaifi, Abdulrahman AlOsaimy*

**Main category:** cs.CL

**Keywords:** Large Language Models, Arabic NLP, benchmarking, BALSAM, collaborative research

**Relevance Score:** 9

**TL;DR:** BALSAM is a comprehensive benchmark to enhance the development and evaluation of Arabic LLMs, addressing existing gaps due to data scarcity and poor benchmarks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of Large Language Models (LLMs) in Arabic by providing a standard benchmark that overcomes data scarcity and evaluation challenges.

**Method:** Introduction of BALSAM, which includes 78 NLP tasks from 14 categories, populated with 52K examples for testing and development, along with a platform for blind evaluation.

**Key Contributions:**

	1. Introduces a comprehensive benchmark for Arabic LLMs.
	2. Includes a wide range of NLP tasks across diverse categories.
	3. Establishes a transparent platform for blind evaluation.

**Result:** BALSAM offers a unified benchmarking tool that allows for reliable performance measurement across various Arabic NLP tasks, promoting collaboration and standardization in this field.

**Limitations:** 

**Conclusion:** BALSAM is positioned as a crucial resource to drive forward Arabic LLM research and development by setting standards and providing community support.

**Abstract:** The impressive advancement of Large Language Models (LLMs) in English has not been matched across all languages. In particular, LLM performance in Arabic lags behind, due to data scarcity, linguistic diversity of Arabic and its dialects, morphological complexity, etc. Progress is further hindered by the quality of Arabic benchmarks, which typically rely on static, publicly available data, lack comprehensive task coverage, or do not provide dedicated platforms with blind test sets. This makes it challenging to measure actual progress and to mitigate data contamination. Here, we aim to bridge these gaps. In particular, we introduce BALSAM, a comprehensive, community-driven benchmark aimed at advancing Arabic LLM development and evaluation. It includes 78 NLP tasks from 14 broad categories, with 52K examples divided into 37K test and 15K development, and a centralized, transparent platform for blind evaluation. We envision BALSAM as a unifying platform that sets standards and promotes collaborative research to advance Arabic LLM capabilities.

</details>


### [45] [Language Arithmetics: Towards Systematic Language Neuron Identification and Manipulation](https://arxiv.org/abs/2507.22608)

*Daniil Gurgurov, Katharina Trinley, Yusser Al Ghussin, Tanja Baeumel, Josef van Genabith, Simon Ostermann*

**Main category:** cs.CL

**Keywords:** Language Models, Neural Mechanisms, Multilingual Processing, Neuron Manipulation, HCI

**Relevance Score:** 9

**TL;DR:** This paper investigates language-specific neurons in multilingual large language models (LLMs) and how they can be manipulated to improve language processing tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the neural mechanisms behind language-specific processing and enhance the performance of multilingual models.

**Method:** The study analyzes language-specific neurons using the Language Activation Probability Entropy (LAPE) method across various LLMs and languages, employing language arithmetics for manipulation tasks.

**Key Contributions:**

	1. Identification of language-specific neurons in multilingual LLMs.
	2. Development of a new manipulation method using language arithmetics.
	3. Demonstration of improved performance across multilingual tasks.

**Result:** The researchers found that neurons controlling language behavior cluster in deeper layers and that manipulation through addition and multiplication of neuron activations leads to improved task performance in multilingual contexts.

**Limitations:** Manipulation is more effective for high-resource languages and may not be as effective for low-resource languages.

**Conclusion:** The findings suggest that understanding and manipulating these neurons can enhance LLMsâ effectiveness in multilingual tasks, with implications for practical applications in language processing.

**Abstract:** Large language models (LLMs) exhibit strong multilingual abilities, yet the neural mechanisms behind language-specific processing remain unclear. We analyze language-specific neurons in Llama-3.1-8B, Mistral-Nemo-12B, and Aya-Expanse-8B & 32B across 21 typologically diverse languages, identifying neurons that control language behavior. Using the Language Activation Probability Entropy (LAPE) method, we show that these neurons cluster in deeper layers, with non-Latin scripts showing greater specialization. Related languages share overlapping neurons, reflecting internal representations of linguistic proximity.   Through language arithmetics, i.e. systematic activation addition and multiplication, we steer models to deactivate unwanted languages and activate desired ones, outperforming simpler replacement approaches. These interventions effectively guide behavior across five multilingual tasks: language forcing, translation, QA, comprehension, and NLI. Manipulation is more successful for high-resource languages, while typological similarity improves effectiveness. We also demonstrate that cross-lingual neuron steering enhances downstream performance and reveal internal "fallback" mechanisms for language selection when neurons are progressively deactivated. Our code is made publicly available at https://github.com/d-gurgurov/Language-Neurons-Manipulation.

</details>


### [46] [Multilingual Political Views of Large Language Models: Identification and Steering](https://arxiv.org/abs/2507.22623)

*Daniil Gurgurov, Katharina Trinley, Ivan Vykopal, Josef van Genabith, Simon Ostermann, Roberto Zamparelli*

**Main category:** cs.CL

**Keywords:** large language models, political bias, manipulability, multilingual, political compass

**Relevance Score:** 8

**TL;DR:** This study investigates political biases in large language models (LLMs) and their manipulability across multiple languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address gaps in understanding the political biases of LLMs and their generalizability across architectures, scales, and languages.

**Method:** A large-scale evaluation of seven open-source instruction-tuned LLMs, analyzing their political orientation using the Political Compass Test across 14 languages with multiple paraphrases for robust measurement.

**Key Contributions:**

	1. Evaluation of political biases across a wide variety of models and languages
	2. Identification of consistent ideological shifts in larger LLMs
	3. Demonstration of manipulability of political stances using intervention techniques.

**Result:** Larger models tend to shift towards libertarian-left positions, with significant variations in bias across different languages and model families.

**Limitations:** Limited to a specific set of models and may not represent all LLMs; the study focuses on instruction-tuned models only.

**Conclusion:** Political biases in LLMs are prevalent and can be actively influenced through specific interventions, allowing for manipulation of model responses.

**Abstract:** Large language models (LLMs) are increasingly used in everyday tools and applications, raising concerns about their potential influence on political views. While prior research has shown that LLMs often exhibit measurable political biases--frequently skewing toward liberal or progressive positions--key gaps remain. Most existing studies evaluate only a narrow set of models and languages, leaving open questions about the generalizability of political biases across architectures, scales, and multilingual settings. Moreover, few works examine whether these biases can be actively controlled.   In this work, we address these gaps through a large-scale study of political orientation in modern open-source instruction-tuned LLMs. We evaluate seven models, including LLaMA-3.1, Qwen-3, and Aya-Expanse, across 14 languages using the Political Compass Test with 11 semantically equivalent paraphrases per statement to ensure robust measurement. Our results reveal that larger models consistently shift toward libertarian-left positions, with significant variations across languages and model families. To test the manipulability of political stances, we utilize a simple center-of-mass activation intervention technique and show that it reliably steers model responses toward alternative ideological positions across multiple languages. Our code is publicly available at https://github.com/d-gurgurov/Political-Ideologies-LLMs.

</details>


### [47] [QE4PE: Word-level Quality Estimation for Human Post-Editing](https://arxiv.org/abs/2503.03044)

*Gabriele Sarti, VilÃ©m Zouhar, Grzegorz ChrupaÅa, Ana Guerberof-Arenas, Malvina Nissim, Arianna Bisazza*

**Main category:** cs.CL

**Keywords:** Quality Estimation, Machine Translation, Post-editing, Human-Computer Interaction, Usability

**Relevance Score:** 8

**TL;DR:** This study examines the usability and effectiveness of word-level quality estimation (QE) methods in machine translation post-editing, involving 42 professional editors and comparing different error-highlight modalities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the understudied effects of word-level QE on the efficiency and decisions of human post-editors in machine translation.

**Method:** The study evaluates four error-span highlight modalities using behavioral logs of 42 post-editors across two translation directions, comparing supervised and uncertainty-based QE methods.

**Key Contributions:**

	1. Explores the impact of QE on post-editing productivity and quality.
	2. Compares various modalities of error-highlight methods in practical settings.
	3. Demonstrates critical factors affecting highlight effectiveness among human post-editors.

**Result:** The research identifies that domain, language, and editor speed significantly impact the effectiveness of highlight modalities; it found minimal differences in performance between human-generated and automated QE highlights.

**Limitations:** The study is based on a specific set of professional post-editors and languages, which may not generalize to all contexts.

**Conclusion:** There exists a notable gap between the accuracy of QE systems and their usability in professional translation workflows, indicating the need for improvements in QE methodologies to enhance user experience.

**Abstract:** Word-level quality estimation (QE) methods aim to detect erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. In this study, we investigate the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated from behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.

</details>


### [48] [Listening to the Unspoken: Exploring 365 Aspects of Multimodal Interview Performance Assessment](https://arxiv.org/abs/2507.22676)

*Jia Li, Yang Wang, Wenhao Qian, Zhenzhen Hu, Richang Hong, Meng Wang*

**Main category:** cs.CL

**Keywords:** interview assessment, multimodal data, machine learning, ensemble learning, feature extraction

**Relevance Score:** 7

**TL;DR:** Proposes a framework for assessing interview performance through video, audio, and text modalities, employing a multilayer perceptron for feature fusion and an ensemble learning strategy for predictions.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure holistic and fair evaluations of candidates' interview performances across multiple dimensions.

**Method:** Integrates video, audio, and text data through modality-specific feature extractors and fuses them using a Shared Compression Multilayer Perceptron, employing ensemble learning for robust scoring.

**Key Contributions:**

	1. Novel framework integrating multiple modalities for interview assessment
	2. Use of ensemble learning for robust score predictions
	3. First place in AVI Challenge 2025 validating efficacy

**Result:** Achieved a multi-dimensional average MSE of 0.1824, winning first place in the AVI Challenge 2025.

**Limitations:** 

**Conclusion:** The proposed framework effectively captures both explicit and implicit cues from multimodal data, providing comprehensive and unbiased assessments of interview performance.

**Abstract:** Interview performance assessment is essential for determining candidates' suitability for professional positions. To ensure holistic and fair evaluations, we propose a novel and comprehensive framework that explores ``365'' aspects of interview performance by integrating \textit{three} modalities (video, audio, and text), \textit{six} responses per candidate, and \textit{five} key evaluation dimensions. The framework employs modality-specific feature extractors to encode heterogeneous data streams and subsequently fused via a Shared Compression Multilayer Perceptron. This module compresses multimodal embeddings into a unified latent space, facilitating efficient feature interaction. To enhance prediction robustness, we incorporate a two-level ensemble learning strategy: (1) independent regression heads predict scores for each response, and (2) predictions are aggregated across responses using a mean-pooling mechanism to produce final scores for the five target dimensions. By listening to the unspoken, our approach captures both explicit and implicit cues from multimodal data, enabling comprehensive and unbiased assessments. Achieving a multi-dimensional average MSE of 0.1824, our framework secured first place in the AVI Challenge 2025, demonstrating its effectiveness and robustness in advancing automated and multimodal interview performance assessment. The full implementation is available at https://github.com/MSA-LMC/365Aspects.

</details>


### [49] [Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears](https://arxiv.org/abs/2504.05008)

*Anastasiia Ivanova, Natalia Fedorova, Sergei Tilga, Ekaterina Artemova*

**Main category:** cs.CL

**Keywords:** large language models, professional writing, ethical concerns, user expectations, language support

**Relevance Score:** 8

**TL;DR:** This paper explores the adoption of LLMs among professional writers, highlighting key insights into ethical concerns, language support, and the impact on writing practices.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the underexplored aspects of LLM adoption in professional writing, including language support, ethics, and effects on creativity.

**Method:** Conducted a questionnaire with 301 respondents and an interactive survey with 36 participants focusing on professional writers who use AI tools.

**Key Contributions:**

	1. Insights on LLM usability for non-English speakers
	2. Understanding of ethical concerns in LLM adoption
	3. Recommendations for improving LLM features based on user feedback

**Result:** Identified significant insights regarding LLM adoption for non-English speakers, the extent of misinformation, and the need for usability improvements in LLM features.

**Limitations:** 

**Conclusion:** The findings suggest a need for further development of LLMs to enhance writing practices and address the concerns of diverse language users.

**Abstract:** The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.

</details>


### [50] [From Sufficiency to Reflection: Reinforcement-Guided Thinking Quality in Retrieval-Augmented Reasoning for LLMs](https://arxiv.org/abs/2507.22716)

*Jie He, Victor Gutierrez Basulto, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Retrieval-Augmented Generation, Large Language Models, Reasoning Quality, Multi-hop QA

**Relevance Score:** 9

**TL;DR:** This paper introduces TIRESRAG-R1, a novel reinforcement learning framework for improving the reasoning abilities of retrieval-augmented generation (RAG) methods in large language models (LLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current RAG methods primarily use final-answer rewards, neglecting the importance of intermediate reasoning quality which can lead to various failure patterns in reasoning.

**Method:** TIRESRAG-R1 employs a think-retrieve-reflect process coupled with a multi-dimensional reward system that includes sufficiency, reasoning quality, and reflection rewards, addressing key reasoning failures.

**Key Contributions:**

	1. Introduction of a sufficiency reward for improved information retrieval.
	2. Development of a reasoning quality reward to enhance logical accuracy.
	3. Implementation of a reflection reward for error detection and correction.

**Result:** Experiments show that TIRESRAG-R1 outperforms previous RAG methods on multi-hop question answering datasets and demonstrates good generalization to single-hop tasks.

**Limitations:** 

**Conclusion:** The TIRESRAG-R1 framework enhances the reasoning capabilities of LLMs by providing a structured approach to both retrieve information and assess reasoning quality, thereby improving model performance.

**Abstract:** Reinforcement learning-based retrieval-augmented generation (RAG) methods enhance the reasoning abilities of large language models (LLMs). However, most rely only on final-answer rewards, overlooking intermediate reasoning quality. This paper analyzes existing RAG reasoning models and identifies three main failure patterns: (1) information insufficiency, meaning the model fails to retrieve adequate support; (2) faulty reasoning, where logical or content-level flaws appear despite sufficient information; and (3) answer-reasoning inconsistency, where a valid reasoning chain leads to a mismatched final answer. We propose TIRESRAG-R1, a novel framework using a think-retrieve-reflect process and a multi-dimensional reward system to improve reasoning and stability. TIRESRAG-R1 introduces: (1) a sufficiency reward to encourage thorough retrieval; (2) a reasoning quality reward to assess the rationality and accuracy of the reasoning chain; and (3) a reflection reward to detect and revise errors. It also employs a difficulty-aware reweighting strategy and training sample filtering to boost performance on complex tasks. Experiments on four multi-hop QA datasets show that TIRESRAG-R1 outperforms prior RAG methods and generalizes well to single-hop tasks. The code and data are available at: https://github.com/probe2/TIRESRAG-R1.

</details>


### [51] [Investigating Hallucination in Conversations for Low Resource Languages](https://arxiv.org/abs/2507.22720)

*Amit Das, Md. Najib Hasan, Souvika Sarkar, Zheng Zhang, Fatemeh Jamshidi, Tathagata Bhattacharya, Nilanjana Raychawdhury, Dongji Feng, Vinija Jain, Aman Chadha*

**Main category:** cs.CL

**Keywords:** Large Language Models, Hallucination, Conversational Data, Multilingual Analysis, Factual Errors

**Relevance Score:** 8

**TL;DR:** This study investigates the hallucination phenomenon in Large Language Models (LLMs) across Hindi, Farsi, and Mandarin, revealing significant variations in error rates among these languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reliability and effectiveness of LLMs by addressing the issue of factual inaccuracies, commonly known as hallucination.

**Method:** A comprehensive analysis of a dataset focused on conversational data in Hindi, Farsi, and Mandarin, assessing factual and linguistic errors in multiple LLMs including GPT-3.5 and GPT-4o.

**Key Contributions:**

	1. Comprehensive analysis of hallucination in multilingual LLMs
	2. Comparison of hallucination rates across Hindi, Farsi, and Mandarin
	3. Insights into the performance of various LLMs on conversational data

**Result:** The study found that LLMs generate very few hallucinated responses in Mandarin, but a significantly higher number of hallucinations in Hindi and Farsi compared to other languages.

**Limitations:** 

**Conclusion:** Addressing the hallucination issue in LLMs requires attention to language-specific error rates, with a particular need for improvements in Hindi and Farsi applications.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable proficiency in generating text that closely resemble human writing. However, they often generate factually incorrect statements, a problem typically referred to as 'hallucination'. Addressing hallucination is crucial for enhancing the reliability and effectiveness of LLMs. While much research has focused on hallucinations in English, our study extends this investigation to conversational data in three languages: Hindi, Farsi, and Mandarin. We offer a comprehensive analysis of a dataset to examine both factual and linguistic errors in these languages for GPT-3.5, GPT-4o, Llama-3.1, Gemma-2.0, DeepSeek-R1 and Qwen-3. We found that LLMs produce very few hallucinated responses in Mandarin but generate a significantly higher number of hallucinations in Hindi and Farsi.

</details>


### [52] [Resource-Efficient Adaptation of Large Language Models for Text Embeddings via Prompt Engineering and Contrastive Fine-tuning](https://arxiv.org/abs/2507.22729)

*Benedikt Roth, Stephan Rappensperger, Tianming Qiu, Hamza ImamoviÄ, Julian WÃ¶rmann, Hao Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, text embeddings, prompt engineering, contrastive fine-tuning, NLP

**Relevance Score:** 9

**TL;DR:** This paper explores adaptation strategies for pre-trained decoder-only Large Language Models (LLMs) to improve text embeddings for downstream tasks, achieving state-of-the-art results in clustering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the usability of LLMs for text embeddings in various tasks, while retaining crucial semantic information lost during pooling.

**Method:** The paper examines different approaches: token aggregation techniques, task-specific prompt engineering, and contrastive fine-tuning for text-level augmentation.

**Key Contributions:**

	1. Exploration of token aggregation techniques for LLMs
	2. Introduction of task-specific prompt engineering
	3. Demonstration of effective contrastive fine-tuning for text embeddings

**Result:** The combined strategies resulted in state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark.

**Limitations:** 

**Conclusion:** LLMs can be adapted effectively for text embedding tasks through innovative prompt engineering and fine-tuning techniques, leading to better semantic representation.

**Abstract:** Large Language Models (LLMs) have become a cornerstone in Natural Language Processing (NLP), achieving impressive performance in text generation. Their token-level representations capture rich, human-aligned semantics. However, pooling these vectors into a text embedding discards crucial information. Nevertheless, many non-generative downstream tasks, such as clustering, classification, or retrieval, still depend on accurate and controllable sentence- or document-level embeddings. We explore several adaptation strategies for pre-trained, decoder-only LLMs: (i) various aggregation techniques for token embeddings, (ii) task-specific prompt engineering, and (iii) text-level augmentation via contrastive fine-tuning. Combining these components yields state-of-the-art performance on the English clustering track of the Massive Text Embedding Benchmark (MTEB). An analysis of the attention map further shows that fine-tuning shifts focus from prompt tokens to semantically relevant words, indicating more effective compression of meaning into the final hidden state. Our experiments demonstrate that LLMs can be effectively adapted as text embedding models through a combination of prompt engineering and resource-efficient contrastive fine-tuning on synthetically generated positive pairs.

</details>


### [53] [Reducing Hallucinations in Summarization via Reinforcement Learning with Entity Hallucination Index](https://arxiv.org/abs/2507.22744)

*Praveenkumar Katwe, Rakesh Chandra, Balabantaray Kali, Prasad Vittala*

**Main category:** cs.CL

**Keywords:** abstractive summarization, language models, entity hallucination, reinforcement learning, EHI

**Relevance Score:** 9

**TL;DR:** This paper presents a reward-driven framework to reduce entity hallucinations in abstractive summarization using reinforcement learning and a new metric called Entity Hallucination Index (EHI).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenge of hallucinations in language model-generated summaries, particularly in real-world applications.

**Method:** A reward-driven fine-tuning framework is introduced that optimizes for EHI, which quantifies the presence, correctness, and grounding of named entities in summaries. The method involves baseline summary generation from meeting transcripts, automatic EHI score computation, and reinforcement learning for model fine-tuning.

**Key Contributions:**

	1. Introduction of the Entity Hallucination Index (EHI) metric for summarization
	2. Development of a reward-driven fine-tuning framework using reinforcement learning
	3. Release of a reproducible Colab pipeline for further research on hallucination reduction

**Result:** Experiments show consistent improvements in EHI across multiple datasets, indicating a reduction in hallucinations without sacrificing fluency or informativeness in the generated summaries.

**Limitations:** 

**Conclusion:** The proposed framework allows for scalable fine-tuning without reliance on human annotations, and it significantly lowers the occurrence of entity-level hallucinations while maintaining output quality.

**Abstract:** Reducing hallucinations in abstractive summarization remains a critical challenge for deploying language models (LMs) in real-world settings. In this work, we introduce a rewarddriven fine-tuning framework that explicitly optimizes for Entity Hallucination Index (EHI), a metric designed to quantify the presence, correctness, and grounding of named entities in generated summaries. Given a corpus of meeting transcripts, we first generate baseline summaries using a pre-trained LM and compute EHI scores via automatic entity extraction and matching. We then apply reinforcement learning to fine-tune the model parameters, using EHI as a reward signal to bias generation toward entity-faithful outputs. Our approach does not rely on human-written factuality annotations, enabling scalable fine-tuning. Experiments demonstrate consistent improvements in EHI across datasets, with qualitative analysis revealing a significant reduction in entity-level hallucinations without degradation in fluency or informativeness. We release a reproducible Colab pipeline, facilitating further research on hallucination-aware model fine-tuning using lightweight, hallucintion metrics like EHI.

</details>


### [54] [CUS-QA: Local-Knowledge-Oriented Open-Ended Question Answering Dataset](https://arxiv.org/abs/2507.22752)

*JindÅich LibovickÃ½, JindÅich Helcl, Andrei Manea, Gianluca Vico*

**Main category:** cs.CL

**Keywords:** regional question answering, large language models, multimodal evaluation

**Relevance Score:** 8

**TL;DR:** A benchmark for open-ended regional question answering using both textual and visual modalities is introduced, alongside strong baselines with LLMs.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate regional knowledge in large language models and improve question answering capabilities across languages and modalities.

**Method:** A dataset of curated questions and answers, both textual and visual, grounded in Wikipedia, was created and evaluated using state-of-the-art LLMs and human judgments.

**Key Contributions:**

	1. Introduction of a benchmark for regional question answering
	2. Evaluation of LLMs with human judgments
	3. Analysis of automated metrics versus human evaluations

**Result:** Baselines show a significant gap in regional knowledge among LLMs, and low correlation between automated metrics and human evaluations was found.

**Limitations:** Focus primarily on specific regions and language pairs may limit generalizability.

**Conclusion:** The dataset serves to assess LLM performance, explore cross-lingual generation, and push for better evaluation metrics in question answering.

**Abstract:** We introduce a benchmark for open-ended regional question answering that encompasses both textual and visual modalities. We also provide strong baselines using state-of-the-art large language models (LLMs). Our dataset consists of manually curated questions and answers grounded in Wikipedia, created by native speakers from Czechia, Slovakia, and Ukraine, with accompanying English translations. It includes both purely textual questions and those requiring visual understanding. As a baseline, we evaluate state-of-the-art LLMs through prompting and complement this with human judgments of answer correctness. Using these human evaluations, we analyze the reliability of existing automatic evaluation metrics. Our baseline results highlight a significant gap in regional knowledge among current LLMs. Moreover, apart from LLM-based evaluation, there is minimal correlation between automated metrics and human judgment. We release this dataset as a resource to (1) assess regional knowledge in LLMs, (2) study cross-lingual generation consistency in a challenging setting, and (3) advance the development of evaluation metrics for open-ended question answering.

</details>


### [55] [Opportunities and Challenges of LLMs in Education: An NLP Perspective](https://arxiv.org/abs/2507.22753)

*Sowmya Vajjala, Bashar Alhafni, Stefano BannÃ², Kaushal Kumar Maurya, Ekaterina Kochmar*

**Main category:** cs.CL

**Keywords:** Large Language Models, Educational NLP, Assistance, Assessment, NLP

**Relevance Score:** 9

**TL;DR:** The paper examines the impact of large language models (LLMs) on educational NLP, focusing on assistance and assessment across four dimensions: reading, writing, speaking, and tutoring.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing interest in leveraging LLMs in education to enhance teaching, learning, and assessment.

**Method:** The paper reviews the impact of LLMs in educational NLP and discusses their applications in two main scenarios: assistance and assessment, organized along four key dimensions.

**Key Contributions:**

	1. Holistic overview of LLMs in education
	2. Identification of key challenges in educational NLP
	3. Framework for understanding LLM applications in reading, writing, speaking, and tutoring

**Result:** The paper identifies new directions for LLM applications in education and outlines key challenges that need addressing.

**Limitations:** 

**Conclusion:** A comprehensive overview is provided to help NLP researchers and practitioners explore LLMs for future language-focused educational applications.

**Abstract:** Interest in the role of large language models (LLMs) in education is increasing, considering the new opportunities they offer for teaching, learning, and assessment. In this paper, we examine the impact of LLMs on educational NLP in the context of two main application scenarios: {\em assistance} and {\em assessment}, grounding them along the four dimensions -- reading, writing, speaking, and tutoring. We then present the new directions enabled by LLMs, and the key challenges to address. We envision that this holistic overview would be useful for NLP researchers and practitioners interested in exploring the role of LLMs in developing language-focused and NLP-enabled educational applications of the future.

</details>


### [56] [MASCA: LLM based-Multi Agents System for Credit Assessment](https://arxiv.org/abs/2507.22758)

*Gautam Jajoo, Pranjal A Chitale, Saksham Agarwal*

**Main category:** cs.CL

**Keywords:** credit assessment, multi-agent systems, contrastive learning, LLM, finance

**Relevance Score:** 4

**TL;DR:** This paper presents MASCA, an LLM-driven multi-agent system designed to improve credit assessment in finance by mimicking decision-making processes and utilizing contrastive learning for optimization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional rule-based methods and statistical models in credit assessment.

**Method:** The framework employs a layered architecture with specialized LLM-based agents that collaboratively address sub-tasks, incorporating contrastive learning for risk and reward optimization.

**Key Contributions:**

	1. Introduction of MASCA, an LLM-driven multi-agent system for credit evaluation.
	2. Integration of contrastive learning for risk and reward assessment.
	3. A bias analysis in credit assessment addressing fairness concerns.

**Result:** Experimental results show that MASCA outperforms baseline approaches in credit scoring, demonstrating the effectiveness of hierarchical LLM-based multi-agent systems in financial applications.

**Limitations:** The paper is a work in progress and lacks extensive empirical validation beyond initial experimental results.

**Conclusion:** MASCA enhances credit evaluation by combining LLM capabilities with agent-based strategies, leading to improved decision-making in credit assessment.

**Abstract:** Recent advancements in financial problem-solving have leveraged LLMs and agent-based systems, with a primary focus on trading and financial modeling. However, credit assessment remains an underexplored challenge, traditionally dependent on rule-based methods and statistical models. In this paper, we introduce MASCA, an LLM-driven multi-agent system designed to enhance credit evaluation by mirroring real-world decision-making processes. The framework employs a layered architecture where specialized LLM-based agents collaboratively tackle sub-tasks. Additionally, we integrate contrastive learning for risk and reward assessment to optimize decision-making. We further present a signaling game theory perspective on hierarchical multi-agent systems, offering theoretical insights into their structure and interactions. Our paper also includes a detailed bias analysis in credit assessment, addressing fairness concerns. Experimental results demonstrate that MASCA outperforms baseline approaches, highlighting the effectiveness of hierarchical LLM-based multi-agent systems in financial applications, particularly in credit scoring.

</details>


### [57] [DBLPLink 2.0 -- An Entity Linker for the DBLP Scholarly Knowledge Graph](https://arxiv.org/abs/2507.22811)

*Debayan Banerjee, Tilahun Abedissa Taffa, Ricardo Usbeck*

**Main category:** cs.CL

**Keywords:** entity linking, LLMs, Knowledge Graph, RDF, DBLP

**Relevance Score:** 6

**TL;DR:** Development of a zero-shot entity linker using LLMs for DBLP's RDF-based Knowledge Graph.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve entity linking in DBLP's 2025 RDF Knowledge Graph version by integrating publication venues as a new entity type and leveraging LLMs.

**Method:** A novel zero-shot entity linking approach is developed that uses LLMs to re-rank candidate entities based on log-probabilities from the penultimate layer.

**Key Contributions:**

	1. Introduction of a new entity type, dblp:Stream, in DBLP's Knowledge Graph.
	2. Development of a zero-shot entity linker utilizing LLMs.
	3. Improvement of entity re-ranking using log-probability outputs from LLMs.

**Result:** The zero-shot linking method demonstrates effective performance in identifying and linking entities within the new DBLP entity structure.

**Limitations:** 

**Conclusion:** The proposed LLM-based method enhances the entity linking for DBLP, marking an innovative step forward in the use of LLMs for knowledge graph tasks.

**Abstract:** In this work we present an entity linker for DBLP's 2025 version of RDF-based Knowledge Graph. Compared to the 2022 version, DBLP now considers publication venues as a new entity type called dblp:Stream. In the earlier version of DBLPLink, we trained KG-embeddings and re-rankers on a dataset to produce entity linkings. In contrast, in this work, we develop a zero-shot entity linker using LLMs using a novel method, where we re-rank candidate entities based on the log-probabilities of the "yes" token output at the penultimate layer of the LLM.

</details>


### [58] [Beyond Natural Language Plans: Structure-Aware Planning for Query-Focused Table Summarization](https://arxiv.org/abs/2507.22829)

*Weijia Zhang, Songgaojun Deng, Evangelos Kanoulas*

**Main category:** cs.CL

**Keywords:** table summarization, structured representation, natural language processing, query-focused summarization, SQL

**Relevance Score:** 6

**TL;DR:** This paper introduces a new structured plan, TaSoF, and a framework, SPaGe, to enhance query-focused table summarization by formalizing reasoning processes and improving its scalability and reliability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional natural language plans for table summarization are ambiguous and lack structure, which limits their scalability and execution.

**Method:** The proposed method includes three phases: 1) generating TaSoF from a query, 2) converting plan steps into SQL through a directed cyclic graph, and 3) producing query-focused summaries.

**Key Contributions:**

	1. Introduction of TaSoF as a structured representation for reasoning in table summarization.
	2. Development of the SPaGe framework that formalizes the table summarization process.
	3. Demonstration of improved performance in multiple benchmarks over existing models.

**Result:** Experiments demonstrate that SPaGe outperforms prior models in both single- and multi-table tasks, confirming the effectiveness of structured representations.

**Limitations:** 

**Conclusion:** SPaGe's structured approach leads to more reliable and scalable table summarization.

**Abstract:** Query-focused table summarization requires complex reasoning, often approached through step-by-step natural language (NL) plans. However, NL plans are inherently ambiguous and lack structure, limiting their conversion into executable programs like SQL and hindering scalability, especially for multi-table tasks. To address this, we propose a paradigm shift to structured representations. We introduce a new structured plan, TaSoF, inspired by formalism in traditional multi-agent systems, and a framework, SPaGe, that formalizes the reasoning process in three phases: 1) Structured Planning to generate TaSoF from a query, 2) Graph-based Execution to convert plan steps into SQL and model dependencies via a directed cyclic graph for parallel execution, and 3) Summary Generation to produce query-focused summaries. Our method explicitly captures complex dependencies and improves reliability. Experiments on three public benchmarks show that SPaGe consistently outperforms prior models in both single- and multi-table settings, demonstrating the advantages of structured representations for robust and scalable summarization.

</details>


### [59] [Where to show Demos in Your Prompt: A Positional Bias of In-Context Learning](https://arxiv.org/abs/2507.22887)

*Kwesi Cobbina, Tianyi Zhou*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, positional bias, demo positioning, classification

**Relevance Score:** 8

**TL;DR:** This paper explores the positional bias in in-context learning (ICL) of large language models (LLMs), demonstrating how the order and position of demos in prompts influence predictions and accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the unexplored positional bias of ICL by analyzing how the arrangement of demos affects model performance in various tasks.

**Method:** A systematic evaluation pipeline was designed to study the Demos' Position in Prompt (DPP) bias across different tasks, using two newly introduced metrics: ACCURACY-CHANGE and PREDICTION-CHANGE.

**Key Contributions:**

	1. Introduction of the Demos' Position in Prompt (DPP) bias
	2. Development of ACCURACY-CHANGE and PREDICTION-CHANGE metrics
	3. Comprehensive evaluation across multiple LLM families revealing sensitivity in performance based on demo placement

**Result:** Results from extensive experiments on ten LLMs indicate significant performance variation based on demo positioning, with optimal positioning resulting in up to +6 points in accuracy, and poor positioning leading to over 30% prediction flips.

**Limitations:** 

**Conclusion:** The study concludes that demo placement is crucial for achieving stable and accurate outputs in ICL, highlighting the sensitivity of smaller models and the marginal impact on larger models.

**Abstract:** In-context learning (ICL) is a critical emerging capability of large language models (LLMs), enabling few-shot learning during inference by including a few demonstrations (demos) in the prompt. However, it has been found that ICL's performance can be sensitive to the choices of demos and their order. This paper investigates an unexplored new positional bias of ICL for the first time: we observe that the predictions and accuracy can drift drastically when the positions of demos, the system prompt, and the user message in LLM input are varied. We refer to this bias as DEMOS' POSITION IN PROMPT (DPP) bias. We design a systematic evaluation pipeline to study this type of positional bias across classification, question answering, summarization, and reasoning tasks. We introduce two metrics, ACCURACY-CHANGE and PREDICTION-CHANGE, to quantify net gains and output volatility induced by changes in the demos' position. Extensive experiments on ten LLMs from four open-source model families (QWEN, LLAMA3, MISTRAL, COHERE) verify that the bias significantly affects their accuracy and predictions: placing demos at the start of the prompt yields the most stable and accurate outputs with gains of up to +6 points. In contrast, placing demos at the end of the user message flips over 30\% of predictions without improving correctness on QA tasks. Smaller models are most affected by this sensitivity, though even large models remain marginally affected on complex tasks.

</details>


### [60] [Towards the Law of Capacity Gap in Distilling Language Models](https://arxiv.org/abs/2311.07052)

*Chen Zhang, Qiuchi Li, Dawei Song, Zheyu Ye, Yan Gao, Yan Hu*

**Main category:** cs.CL

**Keywords:** language model distillation, capacity gap, large language models, teacher-student models, optimal teacher

**Relevance Score:** 8

**TL;DR:** This paper explores the issue of capacity gap in language model distillation and proposes a law of capacity gap that identifies optimal teacher models for training smaller student models, ultimately achieving versatile LLMs that outperform competitors.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of optimal teacher selection in language model distillation, especially when facing substantial capacity gaps between teacher and student models.

**Method:** The authors conduct a study on distilling small-scale language models and derive a law of capacity gap that indicates optimal teacher scaling relative to student size, then extend this law to larger language models.

**Key Contributions:**

	1. Introduction of the law of capacity gap for language model distillation.
	2. Empirical demonstration of the optimal teacher scaling for student performance.
	3. Achievement of state-of-the-art results in LLMs through the application of the law.

**Result:** The study demonstrates that by following the law of capacity gap, the authors are able to produce large language models that perform better than a variety of existing models.

**Limitations:** The study primarily focuses on small-scale models and may not cover optimizations for all types of language model architectures.

**Conclusion:** Adhering to the law of capacity gap allows for more efficient selection of teacher models in language model distillation, leading to improved performance in student models.

**Abstract:** Language model (LM) distillation aims at distilling the knowledge in a large teacher LM to a small student one. As a critical issue facing LM distillation, a superior student often arises from a teacher of a relatively small scale instead of a larger one, especially in the presence of substantial capacity gap between the teacher and student. This issue, often referred to as the \textit{curse of capacity gap}, suggests that there is likely an optimal teacher yielding the best-performing student along the scaling course of the teacher. Consequently, distillation trials on teachers of a wide range of scales are called for to determine the optimal teacher, which becomes computationally intensive in the context of large LMs (LLMs). This paper addresses this critical bottleneck by providing the \textit{law of capacity gap} inducted from a preliminary study on distilling a broad range of small-scale (<3B) LMs, where the optimal teacher consistently scales linearly with the student scale across different model and data scales. By extending the law to LLM distillation on a larger scale (7B), we succeed in obtaining versatile LLMs that outperform a wide array of competitors.

</details>


### [61] [Instruction-tuned Large Language Models for Machine Translation in the Medical Domain](https://arxiv.org/abs/2408.16440)

*Miguel Rios*

**Main category:** cs.CL

**Keywords:** Large Language Models, Machine Translation, Medical Terminology, Instruction-tuning, Specialized Domains

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of instruction-tuned LLMs versus baseline LLMs in medical machine translation, incorporating specialized medical terminology for improved consistency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for consistent machine translation of terminology in specialized domains like medicine due to the lower performance of LLMs compared to standard models.

**Method:** Comparison of baseline LLMs and instruction-tuned LLMs using datasets fine-tuned with medical terminology from specialized dictionaries.

**Key Contributions:**

	1. Comparison of baseline LLMs with instruction-tuned LLMs in the medical domain.
	2. Introduction of medical terminology from specialized dictionaries into LLM training.
	3. Demonstration of improved automatic metric scores for instruction-tuned models.

**Result:** Instruction-tuned LLMs outperform baseline models significantly on automatic metrics in the medical translation tasks.

**Limitations:** 

**Conclusion:** Fine-tuning LLMs with specialized terminology leads to better performance in medical machine translation.

**Abstract:** Large Language Models (LLMs) have shown promising results on machine translation for high resource language pairs and domains. However, in specialised domains (e.g. medical) LLMs have shown lower performance compared to standard neural machine translation models. The consistency in the machine translation of terminology is crucial for users, researchers, and translators in specialised domains. In this study, we compare the performance between baseline LLMs and instruction-tuned LLMs in the medical domain. In addition, we introduce terminology from specialised medical dictionaries into the instruction formatted datasets for fine-tuning LLMs. The instruction-tuned LLMs significantly outperform the baseline models with automatic metrics.

</details>


### [62] [Past Meets Present: Creating Historical Analogy with Large Language Models](https://arxiv.org/abs/2409.14820)

*Nianqi Li, Siyu Yuan, Jiangjie Chen, Jiaqing Liang, Feng Wei, Zujie Liang, Deqing Yang, Yanghua Xiao*

**Main category:** cs.CL

**Keywords:** historical analogies, large language models, self-reflection, AI, decision making

**Relevance Score:** 7

**TL;DR:** The paper explores using large language models to acquire historical analogies and introduces a self-reflection method to enhance their performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** People struggle to find appropriate historical analogies, which are crucial for understanding contemporary events. This paper addresses the gap in research on acquiring historical analogies using AI.

**Method:** The authors examine both retrieval and generation approaches using large language models (LLMs) to acquire historical analogies. They also introduce a self-reflection method to address issues like hallucinations and stereotypes in LLM outputs.

**Key Contributions:**

	1. Introduction of a self-reflection method for LLMs to improve historical analogy generation
	2. Evaluation of LLMs in the context of historical analogy acquisition
	3. Identification of challenges in generating reliable historical analogies from LLMs

**Result:** Human evaluations and a novel automatic multi-dimensional assessment demonstrate that LLMs can effectively generate historical analogies, with improved performance when using the proposed self-reflection method.

**Limitations:** The study may depend on the limitations of the data sources used to train the LLMs and may not generalize across all types of historical events.

**Conclusion:** The study shows that LLMs possess significant potential for generating historical analogies and that their performance can be enhanced through self-reflection techniques.

**Abstract:** Historical analogies, which compare known past events with contemporary but unfamiliar events, are important abilities that help people make decisions and understand the world. However, research in applied history suggests that people have difficulty finding appropriate analogies. And previous studies in the AI community have also overlooked historical analogies. To fill this gap, in this paper, we focus on the historical analogy acquisition task, which aims to acquire analogous historical events for a given event. We explore retrieval and generation methods for acquiring historical analogies based on different large language models (LLMs). Furthermore, we propose a self-reflection method to mitigate hallucinations and stereotypes when LLMs generate historical analogies. Through human evaluations and our specially designed automatic multi-dimensional assessment, we find that LLMs generally have a good potential for historical analogies. And the performance of the models can be further improved by using our self-reflection method.

</details>


### [63] [Neutral Residues: Revisiting Adapters for Model Extension](https://arxiv.org/abs/2410.02744)

*Franck Signe Talla, Edouard Grave, HervÃ© JÃ©gou*

**Main category:** cs.CL

**Keywords:** large language models, domain adaptation, neutral residues

**Relevance Score:** 9

**TL;DR:** This paper introduces a method called neutral residues that improves the adaptation of pretrained large language models (LLMs) to new domains without degrading performance on the original domain.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of adapting LLMs to new domains without formal capacity addition, which often compromises original performance.

**Method:** The authors improve adapters through a joint consideration of data, architecture, and training procedures, implementing a method called neutral residues that outputs near-zeros on the original domain.

**Key Contributions:**

	1. Introduction of neutral residues improving adapter mechanisms
	2. Joint optimization of data, architecture, and training for better domain adaptation
	3. Demonstrated superiority over traditional adaptation methods like finetuning and LoRA.

**Result:** Neutral residues achieve significantly better results in adapting a model to a new language while maintaining performance in English compared to finetuning, LoRA, and vanilla adapters.

**Limitations:** 

**Conclusion:** The proposed method effectively balances the trade-off in domain adaptation, enhancing the model's ability to learn new languages while preserving capability in the original domain.

**Abstract:** We address the problem of extending a pretrained large language model to a new domain that was not seen during training. Standard techniques, such as finetuning or low-rank adaptation (LoRA) are successful at domain adaptation, but do not formally add capacity to the model. This often leads to a trade-off, between performing well on the new domain vs. degrading performance on the original domain. Here, we revisit and improve adapters to extend LLMs from three angles: data, architecture and training procedure, which are advantageously considered jointly. The resulting method, called neutral residues, modifies adapters in a way that leads each new residual block to output near-zeros on the original domain. This solution leads to strong results when adapting a state-of-the-art model originally trained on English to a new language. Neutral residues significantly outperform competing approaches such as finetuning, LoRA or vanilla adapters in terms of the trade-off between learning the new language and not forgetting English.

</details>


### [64] [Natural Language Processing for the Legal Domain: A Survey of Tasks, Datasets, Models, and Challenges](https://arxiv.org/abs/2410.21306)

*Farid Ariai, Joel Mackenzie, Gianluca Demartini*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Legal domain, Systematic review, Language models, Research challenges

**Relevance Score:** 4

**TL;DR:** This survey reviews the application of Natural Language Processing (NLP) in the legal field, examining 131 studies to highlight key NLP tasks, challenges, and development of legal-oriented language models.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential and challenges of applying NLP in the legal domain, focusing on computational assistance tools.

**Method:** A systematic review of 154 studies, refined to 131 through manual filtering, using the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework.

**Key Contributions:**

	1. Systematic review of NLP applications in the legal field.
	2. Identification of specific NLP tasks and challenges in processing legal text.
	3. Outline of open research challenges for NLP in law.

**Result:** Key findings include an overview of specific NLP tasks in legal texts and the identification of sixteen open research challenges, such as bias detection and the need for robust models.

**Limitations:** The review may not cover all existing studies due to the filtering process.

**Conclusion:** The paper underscores the significant advancements and ongoing challenges in employing NLP for legal text processing, indicating areas for future research.

**Abstract:** Natural Language Processing (NLP) is revolutionising the way both professionals and laypersons operate in the legal field. The considerable potential for NLP in the legal sector, especially in developing computational assistance tools for various legal processes, has captured the interest of researchers for years. This survey follows the Preferred Reporting Items for Systematic Reviews and Meta-Analyses framework, reviewing 154 studies, with a final selection of 131 after manual filtering. It explores foundational concepts related to NLP in the legal domain, illustrating the unique aspects and challenges of processing legal texts, such as extensive document lengths, complex language, and limited open legal datasets. We provide an overview of NLP tasks specific to legal text, such as Document Summarisation, Named Entity Recognition, Question Answering, Argument Mining, Text Classification, and Judgement Prediction. Furthermore, we analyse both developed legal-oriented language models, and approaches for adapting general-purpose language models to the legal domain. Additionally, we identify sixteen open research challenges, including the detection and mitigation of bias in artificial intelligence applications, the need for more robust and interpretable models, and improving explainability to handle the complexities of legal language and reasoning.

</details>


### [65] [Yankari: A Monolingual Yoruba Dataset](https://arxiv.org/abs/2412.03334)

*Maro Akpobi*

**Main category:** cs.CL

**Keywords:** Yoruba, NLP, dataset, language resources, ethical data collection

**Relevance Score:** 4

**TL;DR:** Yankari is a large-scale dataset for the Yoruba language aimed at improving NLP resources in underrepresented languages.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical gap in NLP resources for the Yoruba language, which is spoken by over 30 million people.

**Method:** Creation of the dataset involved careful source selection, automated quality control, and rigorous data cleaning processes, resulting in 51,407 documents and over 30 million tokens.

**Key Contributions:**

	1. Creation of Yankari, a large-scale Yoruba language dataset.
	2. Automated evaluations proving dataset quality.
	3. Focus on ethical data collection practices.

**Result:** The Yankari dataset is evaluated and shown to be of high quality compared to existing resources, advancing Yoruba language representation in NLP.

**Limitations:** 

**Conclusion:** The dataset supports the development of more accurate NLP models and enhances digital accessibility for the Yoruba language.

**Abstract:** This paper presents Yankari, a large-scale monolingual dataset for the Yoruba language, aimed at addressing the critical gap in Natural Language Processing (NLP) resources for this important West African language. Despite being spoken by over 30 million people, Yoruba has been severely underrepresented in NLP research and applications. We detail our methodology for creating this dataset, which includes careful source selection, automated quality control, and rigorous data cleaning processes. The Yankari dataset comprises 51,407 documents from 13 diverse sources, totaling over 30 million tokens. Our approach focuses on ethical data collection practices, avoiding problematic sources and addressing issues prevalent in existing datasets. We provide thorough automated evaluations of the dataset, demonstrating its quality compared to existing resources. The Yankari dataset represents a significant advancement in Yoruba language resources, providing a foundation for developing more accurate NLP models, supporting comparative linguistic studies, and contributing to the digital accessibility of the Yoruba language.

</details>


### [66] [Efficient Continual Learning for Small Language Models with a Discrete Key-Value Bottleneck](https://arxiv.org/abs/2412.08528)

*Andor Diera, Lukas Galke, Fabian Karl, Ansgar Scherp*

**Main category:** cs.CL

**Keywords:** continual learning, natural language processing, catastrophic forgetting, encoder-only models, discrete key-value bottleneck

**Relevance Score:** 7

**TL;DR:** This paper introduces a discrete key-value bottleneck (DKVB) for continual learning in NLP, addressing catastrophic forgetting through efficient localized updates.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Continual learning in NLP faces the challenge of catastrophic forgetting when models are updated with new data.

**Method:** The paper introduces a discrete key-value bottleneck for encoder-only language models, compares different bottleneck architectures, and proposes a new initialization technique for discrete keys.

**Key Contributions:**

	1. Introduction of a discrete key-value bottleneck for NLP
	2. New task-independent initialization technique for discrete keys
	3. Demonstrated lower computational costs compared to existing methods.

**Result:** The DKVB enables efficient continual learning, achieving competitive performance with lower computational costs in four evaluation scenarios.

**Limitations:** Limited to encoder-only language models; effectiveness in other architectures not explored.

**Conclusion:** The proposed method alleviates catastrophic forgetting and remains effective in scenarios without task ID.

**Abstract:** Continual learning remains a challenge across various natural language processing (NLP) tasks, as models updated with new training data often risk catastrophic forgetting of previously acquired knowledge. We introduce a discrete key-value bottleneck (DKVB) for encoder-only language models, enabling efficient continual learning through localized updates. Inspired by a discrete key-value bottleneck in vision, we consider new and NLP-specific challenges. We compare different bottleneck architectures for NLP and introduce a new, task-independent initialization technique for the discrete keys. We evaluate our DKVB for NLP in four continual learning scenarios and show that it alleviates catastrophic forgetting. Our experiments demonstrate that the proposed approach achieves competitive performance compared to popular continual learning methods while incurring lower computational costs. Furthermore, we show that DKVB remains effective even in challenging single-head continual learning scenarios where no task ID is provided.

</details>


### [67] [Modeling Story Expectations to Understand Engagement: A Generative Framework Using LLMs](https://arxiv.org/abs/2412.15239)

*Hortense Fong, George Gui*

**Main category:** cs.CL

**Keywords:** story engagement, large language models, consumer behavior, content analysis, forward-looking beliefs

**Relevance Score:** 7

**TL;DR:** This paper introduces a framework using large language models to model audience forward-looking beliefs about story developments, demonstrating significant improvements in explaining consumer engagement with narrative content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand consumer engagement with stories, which is crucial for content creators and platforms, particularly how audience beliefs influence these decisions.

**Method:** The framework generates potential continuations for stories and extracts features related to expectations, uncertainty, and surprise using content analysis techniques.

**Key Contributions:**

	1. Introduces a novel framework for modeling audience beliefs using large language models
	2. Demonstrates increased explanatory power for engagement prediction
	3. Reveals distinct engagement drivers based on content features

**Result:** The method was applied to over 30,000 book chapters, increasing the explanatory power of traditional feature engineering techniques by an average of 31%.

**Limitations:** 

**Conclusion:** The findings show that different types of engagement are influenced by combinations of current and anticipated content features, providing insights for marketing strategies.

**Abstract:** Understanding when and why consumers engage with stories is crucial for content creators and platforms. While existing theories suggest that audience beliefs of what is going to happen should play an important role in engagement decisions, empirical work has mostly focused on developing techniques to directly extract features from actual content, rather than capturing forward-looking beliefs, due to the lack of a principled way to model such beliefs in unstructured narrative data. To complement existing feature extraction techniques, this paper introduces a novel framework that leverages large language models to model audience forward-looking beliefs about how stories might unfold. Our method generates multiple potential continuations for each story and extracts features related to expectations, uncertainty, and surprise using established content analysis techniques. Applying our method to over 30,000 book chapters, we demonstrate that our framework complements existing feature engineering techniques by amplifying their marginal explanatory power on average by 31%. The results reveal that different types of engagement-continuing to read, commenting, and voting-are driven by distinct combinations of current and anticipated content features. Our framework provides a novel way to study and explore how audience forward-looking beliefs shape their engagement with narrative media, with implications for marketing strategy in content-focused industries.

</details>


### [68] [Rationale-guided Prompting for Knowledge-based Visual Question Answering](https://arxiv.org/abs/2412.16936)

*Zhongjian Hu, Peng Yang, Bing Li, Fengyuan Liu*

**Main category:** cs.CL

**Keywords:** Visual Question Answering, Large Language Models, Rationale Heuristics, Chain of Thought, Intermediate Reasoning

**Relevance Score:** 8

**TL;DR:** This paper presents a novel framework (PLRH) for Visual Question Answering that enhances Large Language Models' performance by incorporating intermediate reasoning processes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for knowledge-based Visual Question Answering underutilize the reasoning abilities of Large Language Models, prompting direct answers without intermediate thought processes.

**Method:** The proposed framework, PLRH, prompts LLMs using Chain of Thought (CoT) to generate rationale heuristics, which guide the prediction of answers.

**Key Contributions:**

	1. Introduction of the PLRH framework for VQA.
	2. Incorporation of Chain of Thought reasoning to enhance answer prediction.
	3. Demonstrated superior performance on established VQA benchmarks.

**Result:** The PLRH framework outperforms existing baselines by more than 2.2 and 2.1 on the OK-VQA and A-OKVQA datasets, respectively.

**Limitations:** 

**Conclusion:** Incorporating rationale heuristics into the VQA process significantly enhances the performance of LLMs by utilizing their reasoning capabilities.

**Abstract:** Recently, Large Language Models (LLMs) have been used for knowledge-based Visual Question Answering (VQA). Despite the encouraging results of previous studies, prior methods prompt LLMs to predict answers directly, neglecting intermediate thought processes. We argue that prior methods do not sufficiently activate the capacities of LLMs. We propose a framework called PLRH that Prompts LLMs with Rationale Heuristics for knowledge-based VQA. The PLRH prompts LLMs with Chain of Thought (CoT) to generate rationale heuristics, i.e., intermediate thought processes, and then leverages the rationale heuristics to inspire LLMs to predict answers. Experiments show that our approach outperforms the existing baselines by more than 2.2 and 2.1 on OK-VQA and A-OKVQA, respectively.

</details>


### [69] [FineMedLM-o1: Enhancing Medical Knowledge Reasoning Ability of LLM from Supervised Fine-Tuning to Test-Time Training](https://arxiv.org/abs/2501.09213)

*Hongzhou Yu, Tianhao Cheng, Yingwen Wang, Wen He, Qing Wang, Ying Cheng, Yuejie Zhang, Rui Feng, Xiaobo Zhang*

**Main category:** cs.CL

**Keywords:** large language models, medical applications, fine-tuning, deep reasoning, synthetic data

**Relevance Score:** 10

**TL;DR:** FineMedLM-o1 enhances LLMs for medical applications using synthetic data and innovative training techniques, resulting in significant performance gains in medical reasoning.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for improved reasoning capabilities in medical LLMs for complex tasks like diagnosis and treatment planning.

**Method:** FineMedLM-o1 utilizes Supervised Fine-Tuning and Direct Preference Optimization along with Test-Time Training specifically for the medical domain.

**Key Contributions:**

	1. Introduction of FineMedLM-o1 for advanced medical reasoning
	2. First application of Test-Time Training in the medical domain
	3. Creation of a superior synthetic medical dialogue dataset

**Result:** FineMedLM-o1 achieved a 23% performance improvement over existing models and an additional 14% boost from Test-Time Training on medical benchmarks.

**Limitations:** 

**Conclusion:** The proposed methods and dataset enhance LLM performance in medical reasoning, and the tools will be made available on GitHub.

**Abstract:** Recent advancements in large language models (LLMs) have shown promise in medical applications such as disease diagnosis and treatment planning. However, most existing medical LLMs struggle with the deep reasoning required for complex medical problems, such as differential diagnosis and medication recommendations. We propose FineMedLM-o1, which leverages high-quality medical synthetic data and long-form reasoning data for Supervised Fine-Tuning (SFT) and Direct Preference Optimization (DPO), enabling advanced dialogue and deep reasoning capabilities. Additionally, we introduce Test-Time Training (TTT) in the medical domain for the first time, facilitating domain adaptation and ensuring reliable, accurate reasoning. Experimental results demonstrate that FineMedLM-o1 achieves a 23% average performance improvement over prior models on key medical benchmarks. Furthermore, the introduction of TTT provides an additional 14% performance boost, highlighting its effectiveness in enhancing medical reasoning capabilities. To support this process, we also propose a novel method for synthesizing medical dialogue. Compared to other open-source datasets, our dataset stands out as superior in both quality and complexity. The project and data will be released on GitHub.

</details>


### [70] [GneissWeb: Preparing High Quality Data for LLMs at Scale](https://arxiv.org/abs/2502.14907)

*Hajar Emami Gohari, Swanand Ravindra Kadhe, Syed Yousaf Shah, Constantin Adam, Abdulhamid Adebayo, Praneet Adusumilli, Farhan Ahmed, Nathalie Baracaldo Angel, Santosh Subhashrao Borse, Yuan-Chi Chang, Xuan-Hong Dang, Nirmit Desai, Revital Eres, Ran Iwamoto, Alexei Karve, Yan Koyfman, Wei-Han Lee, Changchang Liu, Boris Lublinsky, Takuyo Ohko, Pablo Pesce, Maroun Touma, Shiqiang Wang, Shalisha Witherspoon, Herbert WoisetschlÃ¤ger, David Wood, Kun-Lung Wu, Issei Yoshida, Syed Zawad, Petros Zerfos, Yi Zhou, Bishwaranjan Bhattacharjee*

**Main category:** cs.CL

**Keywords:** Large Language Models, dataset, data quality, GneissWeb, machine learning

**Relevance Score:** 8

**TL;DR:** GneissWeb is a new dataset of around 10 trillion tokens designed to enhance the performance of Large Language Models (LLMs) by providing high-quality data necessary for better generalization across tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing open datasets which are smaller in size and may not provide the required quality for training large models, resulting in suboptimal performance of LLMs.

**Method:** The dataset was created using a combination of sharded exact sub-string deduplication and an ensemble of quality filters to achieve a balance between quality and quantity.

**Key Contributions:**

	1. Introduction of GneissWeb, a large dataset with 10 trillion tokens
	2. Demonstrated significant performance improvement in LLMs using GneissWeb
	3. Methodology for data quality filtering in dataset creation

**Result:** Models trained on the GneissWeb dataset outperformed those trained on FineWeb-V1.1.0 by 2.73 percentage points on 11 benchmarks and by 1.75 percentage points on an extended set of 20 benchmarks.

**Limitations:** 

**Conclusion:** GneissWeb provides a significantly larger and higher-quality dataset that can improve the training of LLMs, leading to better performance on a range of natural language processing tasks.

**Abstract:** Data quantity and quality play a vital role in determining the performance of Large Language Models (LLMs). High-quality data, in particular, can significantly boost the LLM's ability to generalize on a wide range of downstream tasks. Large pre-training datasets for leading LLMs remain inaccessible to the public, whereas many open datasets are small in size (less than 5 trillion tokens), limiting their suitability for training large models.   In this paper, we introduce GneissWeb, a large dataset yielding around 10 trillion tokens that caters to the data quality and quantity requirements of training LLMs. Our GneissWeb recipe that produced the dataset consists of sharded exact sub-string deduplication and a judiciously constructed ensemble of quality filters. GneissWeb achieves a favorable trade-off between data quality and quantity, producing models that outperform models trained on state-of-the-art open large datasets (5+ trillion tokens).   We show that models trained using GneissWeb dataset outperform those trained on FineWeb-V1.1.0 by 2.73 percentage points in terms of average score computed on a set of 11 commonly used benchmarks (both zero-shot and few-shot) for pre-training dataset evaluation. When the evaluation set is extended to 20 benchmarks (both zero-shot and few-shot), models trained using GneissWeb still achieve a 1.75 percentage points advantage over those trained on FineWeb-V1.1.0.

</details>


### [71] [QE4PE: Word-level Quality Estimation for Human Post-Editing](https://arxiv.org/abs/2503.03044)

*Gabriele Sarti, VilÃ©m Zouhar, Grzegorz ChrupaÅa, Ana Guerberof-Arenas, Malvina Nissim, Arianna Bisazza*

**Main category:** cs.CL

**Keywords:** Quality Estimation, Machine Translation, Post-Editing, Human-Computer Interaction, Usability

**Relevance Score:** 8

**TL;DR:** This study investigates the impact of word-level quality estimation (QE) on post-editing machine translations, analyzing different modalities and their effectiveness in real-world scenarios with professional editors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how word-level QE affects human post-editing in machine translation, particularly focusing on usability and downstream impacts on editing choices and efficiency.

**Method:** The study involved 42 professional post-editors working on machine translations, comparing four different modalities of error-span highlights (including supervised and uncertainty-based QE methods) to assess their influence on post-editing effort and quality improvements.

**Key Contributions:**

	1. Analysis of usability of word-level QE in professional post-editing
	2. Comparison of multiple error-span highlight modalities
	3. Assessment of factors influencing the effectiveness of QE in translation tasks

**Result:** Findings indicate that factors such as domain, language, and the speed of editors significantly affect the effectiveness of highlights, revealing a gap between the accuracy of QE systems and their usability in professional translation workflows.

**Limitations:** Focuses on a specific setting with professional editors, which may not generalize to other contexts or less experienced users.

**Conclusion:** Though different highlighting methods showed modest effectiveness in improving post-editing tasks, the complexity of human workflows shows that accuracy alone does not ensure usability in practice.

**Abstract:** Word-level quality estimation (QE) methods aim to detect erroneous spans in machine translations, which can direct and facilitate human post-editing. While the accuracy of word-level QE systems has been assessed extensively, their usability and downstream influence on the speed, quality and editing choices of human post-editing remain understudied. In this study, we investigate the impact of word-level QE on machine translation (MT) post-editing in a realistic setting involving 42 professional post-editors across two translation directions. We compare four error-span highlight modalities, including supervised and uncertainty-based word-level QE methods, for identifying potential errors in the outputs of a state-of-the-art neural MT model. Post-editing effort and productivity are estimated from behavioral logs, while quality improvements are assessed by word- and segment-level human annotation. We find that domain, language and editors' speed are critical factors in determining highlights' effectiveness, with modest differences between human-made and automated QE highlights underlining a gap between accuracy and usability in professional workflows.

</details>


### [72] [Cross-Modal State-Space Graph Reasoning for Structured Summarization](https://arxiv.org/abs/2503.20988)

*Hannah Kim, Sofia Martinez, Jason Lee*

**Main category:** cs.CL

**Keywords:** Cross-Modal Summarization, State-Space Model, Graph Reasoning

**Relevance Score:** 5

**TL;DR:** Introducing a CSS-GR framework for efficient cross-modal summarization using graph-based reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for effective summarization methods in multimodal data applications through improved interpretability and reduced computational overhead.

**Method:** Proposes a Cross-Modal State-Space Graph Reasoning framework that leverages a state-space model combined with graph-based message passing to enhance summarization.

**Key Contributions:**

	1. Introduction of CSS-GR framework
	2. Enhanced interpretability in summarization
	3. Efficient computational performance on multimodal tasks

**Result:** Demonstrated significant improvements in summarization quality and interpretability while maintaining computational efficiency on multimodal benchmarks.

**Limitations:** Withdrawn from arXiv due to disputed authorship and affiliation.

**Conclusion:** The proposed method outperforms existing sequential models by allowing holistic reasoning across textual and visual data.

**Abstract:** The ability to extract compact, meaningful summaries from large-scale and multimodal data is critical for numerous applications, ranging from video analytics to medical reports. Prior methods in cross-modal summarization have often suffered from high computational overheads and limited interpretability. In this paper, we propose a \textit{Cross-Modal State-Space Graph Reasoning} (\textbf{CSS-GR}) framework that incorporates a state-space model with graph-based message passing, inspired by prior work on efficient state-space models. Unlike existing approaches relying on purely sequential models, our method constructs a graph that captures inter- and intra-modal relationships, allowing more holistic reasoning over both textual and visual streams. We demonstrate that our approach significantly improves summarization quality and interpretability while maintaining computational efficiency, as validated on standard multimodal summarization benchmarks. We also provide a thorough ablation study to highlight the contributions of each component.

</details>


### [73] [Prompt-Reverse Inconsistency: LLM Self-Inconsistency Beyond Generative Randomness and Prompt Paraphrasing](https://arxiv.org/abs/2504.01282)

*Jihyun Janice Ahn, Wenpeng Yin*

**Main category:** cs.CL

**Keywords:** Prompt-Reverse Inconsistency, LLMs, Trustworthy AI

**Relevance Score:** 8

**TL;DR:** This paper introduces Prompt-Reverse Inconsistency (PRIN), a newly identified form of LLM self-inconsistency that affects the reliability of LLM-generated responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the credibility issues arising from different types of inconsistencies found in LLMs.

**Method:** A series of experiments were conducted to examine the extent of PRIN across various LLMs, alongside methods for mitigating this inconsistency.

**Key Contributions:**

	1. Discovery of a new type of LLM self-inconsistency (PRIN)
	2. Experimental analysis of PRIN across multiple LLMs
	3. Suggestions for mitigating PRIN

**Result:** The study identifies PRIN as a significant concern affecting the reliability of LLMs and explores its interrelation with other known inconsistencies.

**Limitations:** 

**Conclusion:** The findings provide insights into LLM behavior and contribute to the development of more trustworthy AI systems.

**Abstract:** While the inconsistency of LLMs is not a novel topic, prior research has predominantly addressed two types of generative inconsistencies: i) Randomness Inconsistency: running the same LLM multiple trials, yielding varying responses; ii) Paraphrase Inconsistency: paraphrased prompts result in different responses from the same LLM. Randomness Inconsistency arises from the inherent randomness due to stochastic sampling in generative models, while Paraphrase Inconsistency is a consequence of the language modeling objectives, where paraphrased prompts alter the distribution of vocabulary logits. This research discovers Prompt-Reverse Inconsistency (PRIN), a new form of LLM self-inconsistency: given a question and a couple of LLM-generated answer candidates, the LLM often has conflicting responses when prompted "Which are correct answers?" and "Which are incorrect answers?". PRIN poses a big concern as it undermines the credibility of LLM-as-a-judge, and suggests a challenge for LLMs to adhere to basic logical rules. We conduct a series of experiments to investigate PRIN, examining the extent of PRIN across different LLMs, methods to mitigate it, potential applications, and its relationship with Randomness Inconsistency and Paraphrase Inconsistency. As the first study to explore PRIN, our findings offer valuable insights into the inner workings of LLMs and contribute to advancing trustworthy AI.

</details>


### [74] [Voices of Freelance Professional Writers on AI: Limitations, Expectations, and Fears](https://arxiv.org/abs/2504.05008)

*Anastasiia Ivanova, Natalia Fedorova, Sergei Tilga, Ekaterina Artemova*

**Main category:** cs.CL

**Keywords:** AI-driven tools, large language models, professional writing, ethics, user expectations

**Relevance Score:** 8

**TL;DR:** This study examines the adoption of AI-driven tools, specifically large language models (LLMs), in professional writing, focusing on language support, ethics, and user expectations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under-researched areas of LLM adoption among professional writers, including language support and ethical concerns.

**Method:** A questionnaire was conducted with 301 participants and an interactive survey with 36 professional writers using AI tools.

**Key Contributions:**

	1. Insights into LLM adoption for non-English speakers
	2. Analysis of ethical concerns in LLM use
	3. Recommendations for enhancing LLM usability based on user expectations

**Result:** Key insights were gathered regarding LLM usage across multiple languages, the impact of misinformation, and writer expectations and concerns.

**Limitations:** 

**Conclusion:** The findings can inform the future development of LLMs, particularly for non-English speakers, enhancing their usability and features.

**Abstract:** The rapid development of AI-driven tools, particularly large language models (LLMs), is reshaping professional writing. Still, key aspects of their adoption such as languages support, ethics, and long-term impact on writers voice and creativity remain underexplored. In this work, we conducted a questionnaire (N = 301) and an interactive survey (N = 36) targeting professional writers regularly using AI. We examined LLM-assisted writing practices across 25+ languages, ethical concerns, and user expectations. The findings of the survey demonstrate important insights, reflecting upon the importance of: LLMs adoption for non-English speakers; the degree of misinformation, domain and style adaptation; usability and key features of LLMs. These insights can guide further development, benefiting both writers and a broader user base.

</details>


### [75] [DÃ©jÃ  Vu: Multilingual LLM Evaluation through the Lens of Machine Translation Evaluation](https://arxiv.org/abs/2504.11829)

*Julia Kreutzer, Eleftheria Briakou, Sweta Agrawal, Marzieh Fadaee, Kocmi Tom*

**Main category:** cs.CL

**Keywords:** multilingual large language models, evaluation practices, machine translation

**Relevance Score:** 8

**TL;DR:** The paper critiques the evaluation practices for multilingual large language models (mLLMs) and proposes best practices from machine translation evaluation to enhance the rigor and comprehensiveness of these evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation of multilingual large language models (mLLMs) that currently lack comprehensiveness and scientific rigor, which hinders their development and application.

**Method:** The authors draw parallels with machine translation evaluation, conducting targeted experiments to apply best practices from MT to the evaluation of mLLMs, and propose a checklist of actionable recommendations for mLLM evaluation.

**Key Contributions:**

	1. Proposed best practices for mLLM evaluation derived from machine translation standards.
	2. Developed a checklist of actionable recommendations for evaluating mLLMs.
	3. Identified essential components for meta-evaluation to rigorously assess evaluation methods themselves.

**Result:** The study demonstrates that applying MT evaluation standards can reveal significant quality differences between mLLMs and establishes a foundational methodology for robust meta-evaluation of these models.

**Limitations:** 

**Conclusion:** Adopting proven evaluation practices from the established field of machine translation can lead to improved assessment of mLLMs, ultimately guiding their development more effectively.

**Abstract:** Generation capabilities and language coverage of multilingual large language models (mLLMs) are advancing rapidly. However, evaluation practices for generative abilities of mLLMs are still lacking comprehensiveness, scientific rigor, and consistent adoption across research labs, which undermines their potential to meaningfully guide mLLM development. We draw parallels with machine translation (MT) evaluation, a field that faced similar challenges and has, over decades, developed transparent reporting standards and reliable evaluations for multilingual generative models. Through targeted experiments across key stages of the generative evaluation pipeline, we demonstrate how best practices from MT evaluation can deepen the understanding of quality differences between models. Additionally, we identify essential components for robust meta-evaluation of mLLMs, ensuring the evaluation methods themselves are rigorously assessed. We distill these insights into a checklist of actionable recommendations for mLLM research and development.

</details>
