# 2025-07-01

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 32]

- [cs.CL](#cs.CL) [Total: 123]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Exploring Artificial Intelligence Tutor Teammate Adaptability to Harness Discovery Curiosity and Promote Learning in the Context of Interactive Molecular Dynamics](https://arxiv.org/abs/2506.22520)

*Mustafa Demir, Jacob Miratsky, Jonathan Nguyen, Chun Kit Chan, Punya Mishra, Abhishek Singharoy*

**Main category:** cs.HC

**Keywords:** AI tutor, student engagement, curiosity, Interactive Molecular Dynamics, team performance

**Relevance Score:** 7

**TL;DR:** The study investigates how an AI tutor teammate affects student engagement and learning during Interactive Molecular Dynamics tasks, focusing on curiosity-driven interactions and team performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI can enhance student engagement and learning effectiveness through curiosity-triggering behaviors in a collaborative learning environment.

**Method:** The study used a Wizard-of-Oz paradigm where a human experimenter adjusted the AI's behavior via a large language model, involving 11 high school students in four increasingly complex Interactive Molecular Dynamics tasks over 60 minutes.

**Key Contributions:**

	1. Demonstrates the effectiveness of AI as a collaborative learning partner.
	2. Highlights the importance of curiosity-triggering in educational AI systems.
	3. Establishes a link between AI interactions and enhanced team performance.

**Result:** The involvement of the AI was linked to increased task completion, deeper understanding, and enhanced student engagement, with higher question complexity correlated to AI-triggered curiosity.

**Limitations:** Limited sample size (11 students) and focus on high school participants might not generalize to other educational levels or contexts.

**Conclusion:** AI can effectively function as a teammate and educator, providing adaptive feedback that helps sustain engagement and curiosity in learning contexts.

**Abstract:** This study examines the impact of an Artificial Intelligence tutor teammate (AI) on student curiosity-driven engagement and learning effectiveness during Interactive Molecular Dynamics (IMD) tasks on the Visual Molecular Dynamics platform. It explores the role of the AI's curiosity-triggering and response behaviors in stimulating and sustaining student curiosity, affecting the frequency and complexity of student-initiated questions. The study further assesses how AI interventions shape student engagement, foster discovery curiosity, and enhance team performance within the IMD learning environment. Using a Wizard-of-Oz paradigm, a human experimenter dynamically adjusts the AI tutor teammate's behavior through a large language model. By employing a mixed-methods exploratory design, a total of 11 high school students participated in four IMD tasks that involved molecular visualization and calculations, which increased in complexity over a 60-minute period. Team performance was evaluated through real-time observation and recordings, whereas team communication was measured by question complexity and AI's curiosity-triggering and response behaviors. Cross Recurrence Quantification Analysis (CRQA) metrics reflected structural alignment in coordination and were linked to communication behaviors. High-performing teams exhibited superior task completion, deeper understanding, and increased engagement. Advanced questions were associated with AI curiosity-triggering, indicating heightened engagement and cognitive complexity. CRQA metrics highlighted dynamic synchronization in student-AI interactions, emphasizing structured yet adaptive engagement to promote curiosity. These proof-of-concept findings suggest that the AI's dual role as a teammate and educator indicates its capacity to provide adaptive feedback, sustaining engagement and epistemic curiosity.

</details>


### [2] [Supra-threshold control of peripheral LOD](https://arxiv.org/abs/2506.22583)

*Benjamin Watson, Neff Walker, Larry F Hodges*

**Main category:** cs.HC

**Keywords:** level of detail, visual perception, interactive applications, task-dependent, threshold perception

**Relevance Score:** 7

**TL;DR:** This paper explores the differences between threshold and supra-threshold level of detail (LOD) control in visual applications, demonstrating that LOD manipulations must differ significantly based on perceptibility and task requirements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how LOD control should account for supra-threshold perception in interactive applications, as traditional methods are based on threshold perception which may not correlate with real-world applications.

**Method:** Two experiments focused on supra-threshold LOD control in the visual periphery, analyzing reliable perceptibility levels and task dependencies.

**Key Contributions:**

	1. Demonstrated differences between supra-threshold and threshold perception in LOD control.
	2. Provided a new framework for task-dependent LOD perceptibility management.
	3. Highlighted the importance of detail contrast over detail size in LOD perception.

**Result:** The findings indicate that LOD must maximize perceptibility below the reliable level and minimize it above that level, challenging existing threshold-based LOD control approaches.

**Limitations:** Further research is needed to explore variations across different tasks and settings.

**Conclusion:** The research suggests a need to reexamine and adjust LOD control methods for better performance in foveal displays, based on the characteristics of task and peripheral visual perception.

**Abstract:** Level of detail (LOD) is widely used to control visual feedback in interactive applications. LOD control is typically based on perception at threshold - the conditions in which a stimulus first becomes perceivable. Yet most LOD manipulations are quite perceivable and occur well above threshold. Moreover, research shows that supra-threshold perception differs drastically from perception at threshold. In that case, should supra-threshold LOD control also differ from LOD control at threshold?   In two experiments, we examine supra-threshold LOD control in the visual periphery and find that indeed, it should differ drastically from LOD control at threshold. Specifically, we find that LOD must support a task-dependent level of reliable perceptibility. Above that level, perceptibility of LOD control manipulations should be minimized, and detail contrast is a better predictor of perceptibility than detail size. Below that level, perceptibility must be maximized, and LOD should be improved as eccentricity rises or contrast drops. This directly contradicts prevailing threshold-based LOD control schemes, and strongly suggests a reexamination of LOD control for foveal display.

</details>


### [3] [A tangible user interface for assessing cognitive mapping ability](https://arxiv.org/abs/2506.22597)

*Ehud Sharlin, Benjamin Watson, Steve Sutphen, Lili Liu, Robert Lederer, John Frazer*

**Main category:** cs.HC

**Keywords:** cognitive mapping, wayfinding, Cognitive Map Probe, assessment, user interface

**Relevance Score:** 5

**TL;DR:** The Cognitive Map Probe (CMP) is a new computerized tool designed to assess cognitive mapping abilities, offering improvements in flexibility, accessibility, and consistency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the importance of cognitive mapping for wayfinding skills, particularly as they are affected by age, disease, or injury.

**Method:** Development of the Cognitive Map Probe (CMP), which employs a tangible user interface for spatial manipulation and was tested extensively for sensitivity to cognitive mapping performance factors.

**Key Contributions:**

	1. Introduction of the Cognitive Map Probe (CMP) as a novel assessment tool.
	2. Improvement of assessment consistency and sensitivity for cognitive mapping.
	3. Utilization of a tangible user interface for enhanced spatial interaction.

**Result:** The CMP demonstrated increased consistency and sensitivity in assessing cognitive mapping abilities compared to existing methods.

**Limitations:** 

**Conclusion:** The CMP is a promising tool for clinicians and therapists to assess cognitive mapping and wayfinding skills in individuals with impairments.

**Abstract:** Wayfinding, the ability to recall the environment and navigate through it, is an essential cognitive skill relied upon almost every day in a person's life. A crucial component of wayfinding is the construction of cognitive maps, mental representations of the environments through which a person travels. Age, disease or injury can severely affect cognitive mapping, making assessment of this basic survival skill particularly important to clinicians and therapists. Cognitive mapping has also been the focus of decades of basic research by cognitive psychologists. Both communities have evolved a number of techniques for assessing cognitive mapping ability. We present the Cognitive Map Probe (CMP), a new computerized tool for assessment of cognitive mapping ability that increases consistency and promises improvements in flexibility, accessibility, sensitivity and control. The CMP uses a tangible user interface that affords spatial manipulation. We describe the design of the CMP, and find that it is sensitive to factors known to affect cognitive mapping performance in extensive experimental testing.

</details>


### [4] [Do Electric Vehicles Induce More Motion Sickness Than Fuel Vehicles? A Survey Study in China](https://arxiv.org/abs/2506.22674)

*Weiyin Xie, Chunxi Huang, Jiyao Wang, Dengbo He*

**Main category:** cs.HC

**Keywords:** motion sickness, electric vehicles, fuel vehicles, survey study, human factors

**Relevance Score:** 5

**TL;DR:** This study investigates motion sickness (MS) induction in electric vehicles (EVs) compared to fuel vehicles (FVs) and identifies associated factors.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising complaints of motion sickness among passengers in EVs, which could hinder their acceptance as an alternative to fuel vehicles.

**Method:** A survey was conducted, gathering data on passengers' motion sickness experiences in EVs and FVs from 639 valid responses in mainland China over the past year.

**Key Contributions:**

	1. Quantifies motion sickness prevalence and severity between EVs and FVs.
	2. Identifies individual and situational factors influencing motion sickness severity in EVs.
	3. Provides empirical data to guide future studies on motion sickness in automobiles.

**Result:** Findings reveal that while FVs have a higher frequency of motion sickness occurrences, EVs induce more severe symptoms. Factors influencing severity include individual traits, in-vehicle activities, and road conditions.

**Limitations:** The study's focus is limited to responses from mainland China, which may not be generalizable to other regions.

**Conclusion:** The study provides insights for future research on motion sickness in vehicle types and highlights the need for EV optimization to minimize motion sickness.

**Abstract:** Electric vehicles (EVs) are a promising alternative to fuel vehicles (FVs), given some unique characteristics of EVs, for example, the low air pollution and maintenance cost. However, the increasing prevalence of EVs is accompanied by widespread complaints regarding the high likelihood of motion sickness (MS) induction, especially when compared to FVs, which has become one of the major obstacles to the acceptance and popularity of EVs. Despite the prevalence of such complaints online and among EV users, the association between vehicle type (i.e., EV versus FV) and MS prevalence and severity has not been quantified. Thus, this study aims to investigate the existence of EV-induced MS and explore the potential factors leading to it. A survey study was conducted to collect passengers' MS experience in EVs and FVs in the past one year. In total, 639 valid responses were collected from mainland China. The results show that FVs were associated with a higher frequency of MS, while EVs were found to induce more severe MS symptoms. Further, we found that passengers' MS severity was associated with individual differences (i.e., age, gender, sleep habits, susceptibility to motion-induced MS), in-vehicle activities (i.e., chatting with others and watching in-vehicle displays), and road conditions (i.e., congestion and slope), while the MS frequency was associated with the vehicle ownership and riding frequency. The results from this study can guide the directions of future empirical studies that aim to quantify the inducers of MS in EVs and FVs, as well as the optimization of EVs to reduce MS.

</details>


### [5] [Insights in Adaptation: Examining Self-reflection Strategies of Job Seekers with Visual Impairments in India](https://arxiv.org/abs/2506.22741)

*Akshay Nayak Kolgar, Yash Prakash, Sampath Jayarathna, Hae-Na Lee, Vikas Ashok*

**Main category:** cs.HC

**Keywords:** Blind and Visually Impaired, Employment, Digital Industry, Feedback Systems, Job Search

**Relevance Score:** 6

**TL;DR:** The paper explores the employment challenges faced by blind and visually impaired individuals in India's digital industry, highlighting the need for personalized feedback systems to improve their job search outcomes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the employment barriers for blind and visually impaired (BVI) individuals in India, especially in the context of digital job opportunities amid changing market conditions.

**Method:** Semi-structured interviews were conducted with 20 BVI individuals who were seeking employment in the digital industry to gain insights into their experiences and challenges.

**Key Contributions:**

	1. Identifies barriers BVI individuals face in the job market
	2. Highlights the importance of personalized feedback in career development
	3. Recommends collaborative intervention systems tailored to BVI needs

**Result:** Findings indicate that despite digital literacy and training, BVI individuals face difficulties in meeting industry standards, largely due to a lack of constructive feedback and suitable job intervention tools tailored to their needs.

**Limitations:** The study is based on a small sample size of 20 individuals, which may not represent the broader BVI population's experiences.

**Conclusion:** The study suggests developing collaborative intervention systems that provide personalized feedback to aid BVI individuals in their job searches, improving their chances of employment in the digital sector.

**Abstract:** Significant changes in the digital employment landscape, driven by rapid technological advancements and the COVID-19 pandemic, have introduced new opportunities for blind and visually impaired (BVI) individuals in developing countries like India. However, a significant portion of the BVI population in India remains unemployed despite extensive accessibility advancements and job search interventions. Therefore, we conducted semi-structured interviews with 20 BVI persons who were either pursuing or recently sought employment in the digital industry. Our findings reveal that despite gaining digital literacy and extensive training, BVI individuals struggle to meet industry requirements for fulfilling job openings. While they engage in self-reflection to identify shortcomings in their approach and skills, they lack constructive feedback from peers and recruiters. Moreover, the numerous job intervention tools are limited in their ability to meet the unique needs of BVI job seekers. Our results therefore provide key insights that inform the design of future collaborative intervention systems that offer personalized feedback for BVI individuals, effectively guiding their self-reflection process and subsequent job search behaviors, and potentially leading to improved employment outcomes.

</details>


### [6] [Memory as a Service (MaaS): Rethinking Contextual Memory as Service-Oriented Modules for Collaborative Agents](https://arxiv.org/abs/2506.22815)

*Haichang Li*

**Main category:** cs.HC

**Keywords:** Memory as a Service, Large Language Models, Collaborative agents

**Relevance Score:** 8

**TL;DR:** The paper rethinks memory design in LLM-based agents and proposes 'Memory as a Service' (MaaS) to enhance interoperability and collaboration between entities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current memory practices in LLM systems create 'memory silos' that limit cross-entity collaboration.

**Method:** The paper presents a new framework for memory design called 'Memory as a Service' (MaaS) that decouples memory as a modular service, allowing independent access and governance.

**Key Contributions:**

	1. Introduction of 'Memory as a Service' (MaaS) concept.
	2. Two-dimensional design space for memory across entities.
	3. Discussion on governance, security, and ethical considerations in memory design.

**Result:** MaaS facilitates controlled, on-demand interoperability of memory across entities, while maintaining privacy and governance.

**Limitations:** 

**Conclusion:** The paper calls for further exploration of service-oriented memory to improve collaborative functionality in LLM-based agents.

**Abstract:** This position paper aims to rethink the role and design of memory in Large Language Model (LLM)-based agent systems. We observe that while current memory practices have begun to transcend the limitations of single interactions, they remain conceptually grounded in "bound memory" in terms of design concept-where memory is treated as local state attached to specific context or entities, forming "memory silos" that impede cross-entity collaboration. To overcome this architectural bottleneck, this paper proposes the timely design perspective of "Memory as a Service" (MaaS). MaaS advocates decoupling memory from its conventional role as an interaction byproduct and encapsulating it as a modular service that can be independently callable, dynamically composable, and finely governed. At its core, MaaS leverages the duality of memory-its inherently private nature and its potential for public service-to grant memory controlled, on-demand interoperability across entities. This paper introduces a two-dimensional design space defined by entity structure and service type, illustrating how MaaS aligns with current memory practices while naturally extending them to cross-entity collaborative scenarios. Finally, we outline an open research agenda spanning governance, security, and ethical ecosystems, and call upon the broader research community to explore this shift toward service-oriented memory for collaborative agents operating across entity boundaries.

</details>


### [7] [Dichoptic Opacity: Managing Occlusion in Stereoscopic Displays via Dichoptic Presentation](https://arxiv.org/abs/2506.22841)

*George Bell, Alma Cantu*

**Main category:** cs.HC

**Keywords:** occlusion management, dichoptic opacity, user engagement, visual perception, depth relationships

**Relevance Score:** 7

**TL;DR:** The paper presents a novel method called dichoptic opacity for managing occlusion in visual displays, showing promise in user engagement and preference over traditional transparency techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for better occlusion management techniques in visual displays that do not hinder the understanding of depth relationships and important information from occluded objects.

**Method:** Dichoptic opacity involves adjusting the transparency of occluders differently for each eye, allowing for a better understanding of both the occluder and the occluded objects.

**Key Contributions:**

	1. Introduction of the dichoptic opacity method
	2. Evidence of user preference and engagement in visual tasks
	3. Insights into optimal transparency trends for future research

**Result:** User studies indicate strong engagement and a preference for dichoptic opacity compared to traditional methods, with indications of trends in optimal transparency values.

**Limitations:** Does not determine optimal transparency values; more research is needed.

**Conclusion:** Dichoptic opacity shows potential for improving occlusion management, warranting further exploration of its effectiveness in various contexts.

**Abstract:** Adjusting transparency is a common method of mitigating occlusion but is often detrimental for understanding the relative depth relationships between objects as well as removes potentially important information from the occluding object. We propose using dichoptic opacity, a novel method for occlusion management that contrasts the transparency of occluders presented to each eye. This allows for better simultaneous understanding of both occluder and occluded. A user study highlights the technique's potential, showing strong user engagement and a clear preference for dichoptic opacity over traditional presentations. While it does not determine optimal transparency values, it reveals promising trends in both percentage and range that merit further investigation.

</details>


### [8] [Coordinated 2D-3D Visualization of Volumetric Medical Data in XR with Multimodal Interactions](https://arxiv.org/abs/2506.22926)

*Qixuan Liu, Shi Qiu, Yinqiao Wang, Xiwen Wu, Kenneth Siu Ho Chok, Chi-Wing Fu, Pheng-Ann Heng*

**Main category:** cs.HC

**Keywords:** XR, medical visualization, human-computer interaction, voice commands, 3D imaging

**Relevance Score:** 9

**TL;DR:** An XR-based system enhances medical data visualization using coordinated multi-layered reconstruction and multimodal interaction with voice commands.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in medical data visualization for individuals with limited medical expertise.

**Method:** The system integrates a Multi-layered Multi-planar Reconstruction module with 3D mesh models and combines hand gestures with LLM-enabled voice commands for user interaction.

**Key Contributions:**

	1. Coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models.
	2. Multimodal interaction framework combining hand gestures with LLM-enabled voice commands.
	3. Demonstrated significant usability and effectiveness improvements through user studies.

**Result:** Preliminary evaluations indicated improvements in task completion times, usability, and interaction effectiveness when using LLM-driven voice control.

**Limitations:** Identified areas for future refinement and further validation needed.

**Conclusion:** The immersive visualization system shows potential to improve medical training and clinical practice, with identified areas for future refinement.

**Abstract:** Volumetric medical imaging technologies produce detailed 3D representations of anatomical structures. However, effective medical data visualization and exploration pose significant challenges, especially for individuals with limited medical expertise. We introduce a novel XR-based system with two key innovations: (1) a coordinated visualization module integrating Multi-layered Multi-planar Reconstruction with 3D mesh models and (2) a multimodal interaction framework combining hand gestures with LLM-enabled voice commands. We conduct preliminary evaluations, including a 15-participant user study and expert interviews, to demonstrate the system's abilities to enhance spatial understanding and reduce cognitive load. Experimental results show notable improvements in task completion times, usability metrics, and interaction effectiveness enhanced by LLM-driven voice control. While identifying areas for future refinement, our findings highlight the potential of this immersive visualization system to advance medical training and clinical practice. Our demo application and supplemental materials are available for download at: https://osf.io/bpjq5/.

</details>


### [9] [Immersive Technologies and Elderly Users: Current use, Limitations and Future Perspectives](https://arxiv.org/abs/2506.22932)

*Zoe Anastasiadou, Andreas Lanitis*

**Main category:** cs.HC

**Keywords:** Elderly, Extended Reality, User Experience, Accessibility, Human-Computer Interaction

**Relevance Score:** 6

**TL;DR:** This paper reviews Extended Reality (XR) technologies to support the elderly by analyzing their physical and mental states, and existing XR applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing percentage of elderly individuals in society and improve their quality of life through emerging technologies.

**Method:** Literature review of elderly characteristics and existing extended reality applications targeting the elderly population.

**Key Contributions:**

	1. Comprehensive analysis of elderly users' challenges with XR technologies.
	2. Review of existing XR applications aimed at elderly users.
	3. Guidelines for creating accessible and engaging XR applications for the elderly.

**Result:** Identification of difficulties faced by elderly users in XR, and presentation of design paradigms for developing user-friendly XR applications.

**Limitations:** 

**Conclusion:** XR developers can enhance the accessibility and engagement of applications for the elderly by understanding their unique needs and reviewing existing solutions.

**Abstract:** The increase of the percentage of elderly population in modern societies dictates the use of emerging technologies as a means of supporting elder members of the society. Within this scope, Extended Reality (XR) technologies pose as a promising technology for improving the daily lives of the elderly population. This paper presents a literature review that describes the most common characteristics of the physical and mental state of the elderly, allowing readers, and specifically XR developers, to understand the main difficulties faced by elderly users of extended reality applications so they can develop accessible, user friendly and engaging applications for the target audience. Furthermore, a review of existing extended reality applications that target the elder population is presented, allowing readers to get acquainted with existing design paradigms that can inspire future developments.

</details>


### [10] [GamerAstra: Enhancing Video Game Accessibility for Blind and Low-Vision Players through a Multi-Agent AI Framework](https://arxiv.org/abs/2506.22937)

*Tianrun Qiu, Changxin Chen, Sizhe Cheng, Yiming Yang, Yixiao Guo, Zhicong Lu, Yuxin Ma*

**Main category:** cs.HC

**Keywords:** Accessibility, Blind and Low-Vision, Video Games, Multi-Agent Design, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** GamerAstra is a multi-agent accessibility framework designed to improve video game access for blind and low-vision (BLV) players by integrating advanced multi-modal techniques and customizable assistance.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenges faced by blind and low-vision players in engaging with video games due to inaccessibility and interface navigation issues.

**Method:** The framework leverages multi-modal techniques including large language models and vision-language models to facilitate interaction with games lacking native accessibility support and incorporates customizable assistance granularities.

**Key Contributions:**

	1. Introduction of GamerAstra, a generalized accessibility framework for BLV players
	2. Integration of multi-modal techniques, including LLMs and vision-language models
	3. Customizable assistance to support varying degrees of visual impairment

**Result:** Evaluation through technical assessments and user studies indicates that GamerAstra effectively enhances playability and delivers a more immersive gaming experience for BLV players.

**Limitations:** The development requires substantial programming effort and is often implemented on a game-by-game basis.

**Conclusion:** The findings underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.

**Abstract:** Blind and low-vision (BLV) players encounter critical challenges in engaging with video games due to the inaccessibility of visual elements, difficulties in navigating interfaces, and limitations in sending interaction input. Moreover, the development of specialized accessibility features typically requires substantial programming effort and is often implemented on a game-by-game basis. To address these challenges, we introduce \textit{GamerAstra}, a generalized accessibility framework that leverages a multi-agent design to facilitate access to video games for BLV players. It integrates multi-modal techniques including large language models and vision-language models, enabling interaction with games lacking native accessibility support. The framework further incorporates customizable assistance granularities to support varying degrees of visual impairment and enhances interface navigation through multiple input modalities. The evaluation through technical assessments and user studies indicate that \textit{GamerAstra} effectively enhances playability and delivers a more immersive gaming experience for BLV players. These findings also underscore potential avenues for advancing intelligent accessibility frameworks in the gaming domain.

</details>


### [11] [Context, Credibility, and Control: User Reflections on AI Assisted Misinformation Tools](https://arxiv.org/abs/2506.22940)

*Varun Sangwan, Heidi Makitalo*

**Main category:** cs.HC

**Keywords:** Collaborative AI, Misinformation, User Agency, Media Literacy, Interactive Systems

**Relevance Score:** 8

**TL;DR:** The paper explores a collaborative AI system designed to enhance user agency in evaluating misinformation on social media by integrating interactive features for improved critical thinking.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve methods for identifying and evaluating misinformation on social media, which often fail with emotionally charged content.

**Method:** Designed and evaluated an interactive interface with collaborative AI features including real-time explanations, source aggregation, and debate-style interactions.

**Key Contributions:**

	1. Development of an interactive interface incorporating collaborative AI for misinformation evaluation.
	2. Demonstrated effectiveness of a debate-style interaction for improving user engagement and critical thinking.
	3. Provided empirical evidence on the usefulness of a multiple-source view for evaluating information.

**Result:** In a user study, 79% of participants preferred the debate mode over standard chatbots, and the multiple-source view received a high usefulness rating of 4.6 out of 5.

**Limitations:** 

**Conclusion:** The study demonstrates that context-rich, dialogic AI systems can enhance media literacy and trust in digital environments, emphasizing the need for ethical and interactive design in future misinformation tools.

**Abstract:** This paper investigates how collaborative AI systems can enhance user agency in identifying and evaluating misinformation on social media platforms. Traditional methods, such as personal judgment or basic fact-checking, often fall short when faced with emotionally charged or context-deficient content. To address this, we designed and evaluated an interactive interface that integrates collaborative AI features, including real-time explanations, source aggregation, and debate-style interaction. These elements aim to support critical thinking by providing contextual cues and argumentative reasoning in a transparent, user-centered format. In a user study with 14 participants, 79% found the debate mode more effective than standard chatbot interfaces, and the multiple-source view received an average usefulness rating of 4.6 out of 5. Our findings highlight the potential of context-rich, dialogic AI systems to improve media literacy and foster trust in digital information environments. We argue that future tools for misinformation mitigation should prioritize ethical design, explainability, and interactive engagement to empower users in a post-truth era.

</details>


### [12] [Positioning AI Tools to Support Online Harm Reduction Practice: Applications and Design Directions](https://arxiv.org/abs/2506.22941)

*Kaixuan Wang, Jason T. Jacques, Chenxin Diao*

**Main category:** cs.HC

**Keywords:** Large Language Models, harm reduction, People Who Use Drugs, co-design, information accessibility

**Relevance Score:** 7

**TL;DR:** The paper explores how Large Language Models (LLMs) can be responsibly designed to improve harm reduction information for People Who Use Drugs (PWUD), revealing both potential benefits and challenges.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** There is a critical need for accurate harm reduction information for PWUD, which is often unmet by existing online resources due to stigma and accessibility issues.

**Method:** The study used qualitative workshops with stakeholders, including academics and harm reduction practitioners, to investigate LLM capabilities and design considerations.

**Key Contributions:**

	1. Investigates LLM application in harm reduction for PWUD
	2. Identifies design considerations for effective LLM usage
	3. Suggests collaborative co-design pathways with stakeholders

**Result:** Findings highlight that LLMs can offer multilingual, responsive information and mitigate stigma, but require careful ethical and contextual alignment to be effective.

**Limitations:** The paper does not provide quantitative measures of LLM effectiveness and relies on qualitative insights.

**Conclusion:** The paper emphasizes the importance of collaborative co-design for LLMs to ensure they are helpful, safe, and aligned with harm reduction principles.

**Abstract:** Access to accurate and actionable harm reduction information can directly impact the health outcomes of People Who Use Drugs (PWUD), yet existing online channels often fail to meet their diverse and dynamic needs due to limitations in adaptability, accessibility, and the pervasive impact of stigma. Large Language Models (LLMs) present a novel opportunity to enhance information provision, but their application in such a high-stakes domain is under-explored and presents socio-technical challenges. This paper investigates how LLMs can be responsibly designed to support the information needs of PWUD. Through a qualitative workshop involving diverse stakeholder groups (academics, harm reduction practitioners, and an online community moderator), we explored LLM capabilities, identified potential use cases, and delineated core design considerations. Our findings reveal that while LLMs can address some existing information barriers (e.g., by offering responsive, multilingual, and potentially less stigmatising interactions), their effectiveness is contingent upon overcoming challenges related to ethical alignment with harm reduction principles, nuanced contextual understanding, effective communication, and clearly defined operational boundaries. We articulate design pathways emphasising collaborative co-design with experts and PWUD to develop LLM systems that are helpful, safe, and responsibly governed. This work contributes empirically grounded insights and actionable design considerations for the responsible development of LLMs as supportive tools within the harm reduction ecosystem.

</details>


### [13] [Against 'softmaxing' culture](https://arxiv.org/abs/2506.22968)

*Daniel Mwesigwa*

**Main category:** cs.HC

**Keywords:** Culture, AI, Evaluation, Machine Learning, Human-Computer Interaction

**Relevance Score:** 7

**TL;DR:** The paper discusses how AI, particularly large models, is homogenizing cultural expressions and proposes a new approach for evaluating AI systems' cultural alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cultural homogenization caused by AI models and improve the evaluation of culture in AI systems.

**Method:** The paper suggests shifting the foundational question of evaluations from 'what is culture?' to 'when is culture?' and emphasizes situating cultural universals within their specific contexts.

**Key Contributions:**

	1. Introduces the concept of 'softmaxing culture' in AI evaluations.
	2. Advocates for a shift in evaluative questions regarding culture in AI systems.
	3. Emphasizes the need to relate cultural universals to their specific contexts.

**Result:** Proposes a new framework for evaluating cultural aspects in AI, indicating that existing ML and HCI approaches are limited.

**Limitations:** The paper is primarily conceptual and does not provide empirical data supporting the proposed shifts.

**Conclusion:** Cultural evaluations should move beyond technical paradigms to more accurately reflect the complexities of culture.

**Abstract:** AI is flattening culture. Evaluations of "culture" are showing the myriad ways in which large AI models are homogenizing language and culture, averaging out rich linguistic differences into generic expressions. I call this phenomenon "softmaxing culture," and it is one of the fundamental challenges facing AI evaluations today. Efforts to improve and strengthen evaluations of culture are central to the project of cultural alignment in large AI systems. This position paper argues that machine learning (ML) and human-computer interaction (HCI) approaches to evaluation are limited. I propose two key shifts. First, instead of asking "what is culture?" at the start of system evaluations, I propose beginning with the question: "when is culture?" Second, while I acknowledge the philosophical claim that cultural universals exist, the challenge is not simply to describe them, but to situate them in relation to their particulars. Taken together, these conceptual shifts invite evaluation approaches that move beyond technical requirements, toward perspectives more responsive to the complexities of culture.

</details>


### [14] [Deep Learning in Mild Cognitive Impairment Diagnosis using Eye Movements and Image Content in Visual Memory Tasks](https://arxiv.org/abs/2506.23016)

*Tomás Silva Santos Rocha, Anastasiia Mikhailova, Moreno I. Coco, José Santos-Victor*

**Main category:** cs.HC

**Keywords:** Mild Cognitive Impairment, dementia, deep learning, eye-tracking, diagnostic tools

**Relevance Score:** 8

**TL;DR:** The study develops a deep learning model using eye-tracking data to differentiate between Healthy Controls and individuals with Mild Cognitive Impairment, aiming to aid in early dementia diagnosis.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** The growing prevalence of dementia necessitates the development of effective diagnostic tools, particularly for Mild Cognitive Impairment (MCI), which is an early indicator of dementia.

**Method:** The research involves a deep learning model, VTNet, trained on eye-tracking data from 44 participants performing a visual memory task, with enhancements to include scan paths and heat maps.

**Key Contributions:**

	1. Development of a deep learning model for MCI diagnosis using eye-tracking data
	2. Incorporation of scan paths and heat maps for improved analysis
	3. Evaluation of image resolution and task performance parameters on model efficacy

**Result:** The model achieved a sensitivity of 68% and a specificity of 76% despite working with a smaller and less standardized dataset compared to previous studies.

**Limitations:** The sample size was small, and the task duration was shorter and less standardized than typical studies.

**Conclusion:** The study demonstrates that eye-tracking combined with deep learning may serve as a viable method for developing diagnostic tools for MCI, with future research needed to refine the approach.

**Abstract:** The global prevalence of dementia is projected to double by 2050, highlighting the urgent need for scalable diagnostic tools. This study utilizes digital cognitive tasks with eye-tracking data correlated with memory processes to distinguish between Healthy Controls (HC) and Mild Cognitive Impairment (MCI), a precursor to dementia. A deep learning model based on VTNet was trained using eye-tracking data from 44 participants (24 MCI, 20 HCs) who performed a visual memory task. The model utilizes both time series and spatial data derived from eye-tracking. It was modified to incorporate scan paths, heat maps, and image content. These modifications also enabled testing parameters such as image resolution and task performance, analyzing their impact on model performance. The best model, utilizing $700\times700px$ resolution heatmaps, achieved 68% sensitivity and 76% specificity. Despite operating under more challenging conditions (e.g., smaller dataset size, shorter task duration, or a less standardized task), the model's performance is comparable to an Alzheimer's study using similar methods (70% sensitivity and 73% specificity). These findings contribute to the development of automated diagnostic tools for MCI. Future work should focus on refining the model and using a standardized long-term visual memory task.

</details>


### [15] [Mind the Dark: A Gamified Exploration of Deceptive Design Awareness for Children in the Digital Age](https://arxiv.org/abs/2506.23017)

*Noverah Khan, Hira Eiraj Daud, Suleman Shahid*

**Main category:** cs.HC

**Keywords:** dark patterns, deceptive design, children, digital literacy, gamification

**Relevance Score:** 6

**TL;DR:** This paper explores the impact of dark patterns on children and presents a gamified application designed to educate them on recognizing deceptive design elements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the lack of research on the effects of deceptive design elements, specifically dark patterns, on children, highlighting their increased vulnerability in a digital environment.

**Method:** The study involved the development of a gamified application that educates children on identifying dark patterns and an evaluation of its effectiveness in increasing children's awareness.

**Key Contributions:**

	1. Developed a gamified application for teaching children about dark patterns.
	2. Demonstrated the effectiveness of early education in altering children's navigation behavior.
	3. Provided insights into the lack of research on dark patterns affecting children.

**Result:** Findings showed that education on dark patterns significantly heightened children's awareness, positively influencing their navigation of social media, video games, and streaming platforms.

**Limitations:** The evaluation might be limited to specific demographic groups or contexts, which may affect the generalizability of the findings.

**Conclusion:** Early education on dark patterns is crucial for empowering children to recognize and counter deceptive design, ultimately fostering digital literacy.

**Abstract:** This paper addresses the critical issue of deceptive design elements prevalent in technology, and their potential impact on children. Recent research highlights the impact of dark patterns on adults and adolescents, while studies involving children are scarce. In an era where children wield greater independence with digital devices, their vulnerability to dark patterns amplifies without early education. Our findings show a significant positive impact of dark pattern education on children's awareness, revealing that heightened awareness considerably alters children's navigation of social media, video games, and streaming platforms. To this end, we developed a gamified application aimed at instructing children on identifying and responding to various dark patterns. Our evaluation results emphasize the critical role of early education in empowering children to recognize and counter deceptive design, thereby cultivating a digitally literate generation capable of making informed choices in the complex landscape of digital technology.

</details>


### [16] [CSBrain: A Cross-scale Spatiotemporal Brain Foundation Model for EEG Decoding](https://arxiv.org/abs/2506.23075)

*Yuchen Zhou, Jiamin Wu, Zichen Ren, Zhouheng Yao, Weiheng Lu, Kunyu Peng, Qihao Zheng, Chunfeng Song, Wanli Ouyang, Chao Gou*

**Main category:** cs.HC

**Keywords:** EEG, brain-computer interfaces, machine learning, neuroscience, spatiotemporal modeling

**Relevance Score:** 5

**TL;DR:** CSBrain is a foundation model that enhances EEG decoding by addressing cross-scale spatiotemporal structures through novel tokenization and attention mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved EEG decoding methods that consider the diverse temporal and spatial structures of neural activity, which are overlooked by existing dense modeling paradigms.

**Method:** CSBrain introduces Cross-scale Spatiotemporal Tokenization (CST) for feature aggregation and Structured Sparse Attention (SSA) for dependency capture, integrating multi-scale information in a sequential manner.

**Key Contributions:**

	1. Introduction of Cross-scale Spatiotemporal Tokenization (CST) for EEG signal representation.
	2. Development of Structured Sparse Attention (SSA) for capturing complex dependencies in EEG data.
	3. Demonstration of superior performance in EEG decoding tasks compared to previous models.

**Result:** CSBrain outperforms existing task-specific and foundation model baselines across 11 EEG tasks in 16 datasets, demonstrating the effectiveness of cross-scale modeling.

**Limitations:** 

**Conclusion:** CSBrain establishes cross-scale modeling as an essential approach for enhanced brain activity analysis and represents a significant advancement in brain-AI research.

**Abstract:** Understanding and decoding brain activity from electroencephalography (EEG) signals is a fundamental challenge in neuroscience and AI, with applications in cognition, emotion recognition, diagnosis, and brain-computer interfaces. While recent EEG foundation models advance generalized decoding via unified architectures and large-scale pretraining, they adopt a scale-agnostic dense modeling paradigm inherited from NLP and vision. This design neglects a core property of neural activity: cross-scale spatiotemporal structure. EEG task patterns span a wide range of temporal and spatial scales, from short bursts to slow rhythms, and from localized cortical responses to distributed interactions. Ignoring this diversity leads to suboptimal representations and weak generalization. We propose CSBrain, a Cross-scale Spatiotemporal Brain foundation model for generalized EEG decoding. CSBrain introduces: (i) Cross-scale Spatiotemporal Tokenization (CST), which aggregates multi-scale features from localized temporal windows and anatomical brain regions into compact scale-aware tokens; and (ii) Structured Sparse Attention (SSA), which captures cross-window and cross-region dependencies, enhancing scale diversity while removing spurious correlations. CST and SSA are alternately stacked to progressively integrate multi-scale dependencies. Experiments on 11 EEG tasks across 16 datasets show that CSBrain consistently outperforms task-specific and foundation model baselines. These results establish cross-scale modeling as a key inductive bias and position CSBrain as a robust backbone for future brain-AI research.

</details>


### [17] [A User Experience 3.0 (UX 3.0) Paradigm Framework: Designing for Human-Centered AI Experiences](https://arxiv.org/abs/2506.23116)

*Wei Xu*

**Main category:** cs.HC

**Keywords:** User Experience, Human-Centered AI, UX 3.0, AI Technologies, UX Practices

**Relevance Score:** 8

**TL;DR:** This paper discusses the evolution of user experience (UX) practices towards a transformative phase (UX 3.0) influenced by AI technologies, proposing a framework for developing human-centered AI (HCAI) systems.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for new UX approaches that align with the evolving landscape of AI technologies and user needs.

**Method:** The authors propose a UX 3.0 paradigm framework to guide UX practices in the design and development of HCAI systems.

**Key Contributions:**

	1. Introduction of a UX 3.0 paradigm framework.
	2. Focus on human-centered AI experiences.
	3. Guidance for UX practices in AI development.

**Result:** The framework is aimed at enhancing user experiences in the context of AI integration into everyday applications.

**Limitations:** 

**Conclusion:** A shift towards HCAI necessitates the adoption of a new UX paradigm to accommodate changing user needs and technological advancements.

**Abstract:** User experience (UX) practices have evolved in stages and are entering a transformative phase (UX 3.0), driven by AI technologies and shifting user needs. Human-centered AI (HCAI) experiences are emerging, necessitating new UX approaches to support UX practices in the AI era. We propose a UX 3.0 paradigm framework to respond and guide UX practices in developing HCAI systems.

</details>


### [18] [ImprovMate: Multimodal AI Assistant for Improv Actor Training](https://arxiv.org/abs/2506.23180)

*Riccardo Drago, Yotam Sechayk, Mustafa Doga Dogan, Andrea Sanna, Takeo Igarashi*

**Main category:** cs.HC

**Keywords:** improvisation, AI, large language models, training, narrative coherence

**Relevance Score:** 4

**TL;DR:** ImprovMate is an AI tool using LLMs to assist actors in improvisation training by generating narrative cues, enhancing creativity while ensuring coherence.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by actors in maintaining narrative coherence and managing cognitive load during improvisation performances.

**Method:** ImprovMate leverages large language models to generate narrative stimuli and cues, supporting exercises that mimic live training scenarios.

**Key Contributions:**

	1. Introduction of ImprovMate, leveraging LLMs in improv training
	2. Incorporation of professional improvisers' insights into training exercises
	3. Pilot study demonstrating acceptance of AI cues in traditional improv settings

**Result:** A pilot study indicated that actors might welcome AI techniques in improvisation, especially if they align with traditional practices and provide novel twists.

**Limitations:** 

**Conclusion:** ImprovMate offers a unique solution for improv training by balancing randomness and structured guidance, making the improvisation process smoother for actors.

**Abstract:** Improvisation training for actors presents unique challenges, particularly in maintaining narrative coherence and managing cognitive load during performances. Previous research on AI in improvisation performance often predates advances in large language models (LLMs) and relies on human intervention. We introduce ImprovMate, which leverages LLMs as GPTs to automate the generation of narrative stimuli and cues, allowing actors to focus on creativity without keeping track of plot or character continuity. Based on insights from professional improvisers, ImprovMate incorporates exercises that mimic live training, such as abrupt story resolution and reactive thinking exercises, while maintaining coherence via reference tables. By balancing randomness and structured guidance, ImprovMate provides a groundbreaking tool for improv training. Our pilot study revealed that actors might embrace AI techniques if the latter mirrors traditional practices, and appreciate the fresh twist introduced by our approach with the AI-generated cues.

</details>


### [19] [Vibe coding: programming through conversation with artificial intelligence](https://arxiv.org/abs/2506.23253)

*Advait Sarkar, Ian Drosos*

**Main category:** cs.HC

**Keywords:** vibe coding, AI-assisted programming, developer workflows, programming paradigm, trust in AI

**Relevance Score:** 8

**TL;DR:** This paper explores 'vibe coding', a programming paradigm that involves leveraging large language models for code generation, focusing on developers' interactions and workflows during coding sessions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the emerging role of AI in programming and how it influences developer workflows and interactions with code generation tools.

**Method:** The authors conducted a framework analysis of curated videos showcasing vibe coding sessions, capturing developer reflections and interactions when using AI for coding.

**Key Contributions:**

	1. Identification of the vibe coding paradigm
	2. Insights into blended prompting strategies
	3. Understanding the dynamic nature of trust in AI tools during coding

**Result:** The study reveals that vibe coding involves iterative cycles of prompting AI, evaluating generated code, and manual editing, blending high-level directives with detailed specifications, and maintaining trust in AI through iterative verification.

**Limitations:** The findings are based on a curated video set, which may not be representative of all coding environments or practices.

**Conclusion:** Vibe coding represents a shift in programming practices that redistributes the need for expertise, emphasizing context management and decision-making between AI and manual coding.

**Abstract:** We examine "vibe coding": an emergent programming paradigm where developers primarily write code by interacting with code-generating large language models rather than writing code directly. We analysed a curated set of videos depicting extended vibe coding sessions with rich think-aloud reflections. Using framework analysis, we investigated programmers' goals, workflows, prompting techniques, debugging approaches, and challenges encountered. We find that vibe coding follows iterative goal satisfaction cycles where developers alternate between prompting AI, evaluating generated code through rapid scanning and application testing, and manual editing. Prompting strategies blend vague, high-level directives with detailed technical specifications. Debugging remains a hybrid process combining AI assistance with manual practices. Critically, vibe coding does not eliminate the need for programming expertise but rather redistributes it toward context management, rapid code evaluation, and decisions about when to transition between AI-driven and manual manipulation of code. Trust in AI tools during vibe coding is dynamic and contextual, developed through iterative verification rather than blanket acceptance. Vibe coding is an evolution of AI-assisted programming that represents an early manifestation of "material disengagement", where practitioners orchestrate code production and manipulation, mediated through AI, while maintaining selective and strategic oversight.

</details>


### [20] [Accessible Data Access and Analysis by People who are Blind or Have Low Vision](https://arxiv.org/abs/2506.23443)

*Samuel Reinders, Munazza Zaib, Matthew Butler, Bongshin Lee, Ingrid Zukerman, Lizhen Qu, Kim Marriott*

**Main category:** cs.HC

**Keywords:** assistive technology, refreshable tactile displays, conversational agents, data analysis, blind and low vision

**Relevance Score:** 8

**TL;DR:** The paper presents a system combining tactile displays and conversational agents to assist blind or low vision users in exploring and analyzing data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce barriers for blind or low vision (BLV) individuals in accessing and analyzing data, improving their employment opportunities and interaction with information.

**Method:** Co-design and development of a system that integrates refreshable tactile displays and speech communication for data analysis tasks.

**Key Contributions:**

	1. Development of a multimodal system for BLV users
	2. Integration of tactile graphics and conversational agents
	3. Focus on enhancing data accessibility and analysis for BLV individuals

**Result:** The system enables effective data exploration for BLV users, contributing to the field of assistive technology and multimodal interactions.

**Limitations:** 

**Conclusion:** The proposed innovations aim to enhance accessibility to data and bridge equity gaps for BLV populations, with implications for natural language understanding and generation.

**Abstract:** Our work aims to develop new assistive technologies that enable blind or low vision (BLV) people to explore and analyze data readily. At present, barriers exist for BLV people to explore and analyze data, restricting access to government, health and personal data, and limiting employment opportunities. This work explores the co-design and development of an innovative system to support data access, with a focus on the use of refreshable tactile displays (RTDs) and conversational agents. The envisaged system will use a combination of tactile graphics and speech to communicate with BLV users, and proactively assist with data analysis tasks. As well as addressing significant equity gaps, our work expects to produce innovations in assistive technology, multimodal interfaces, dialogue systems, and natural language understanding and generation.

</details>


### [21] [Reducing Motion Sickness in Passengers of Autonomous Personal Mobility Vehicles by Presenting a Driving Path](https://arxiv.org/abs/2506.23457)

*Yuya Ide, Hailong Liu, Takahiro Wada*

**Main category:** cs.HC

**Keywords:** autonomous vehicles, motion sickness, head movement, path information, passenger behavior

**Relevance Score:** 6

**TL;DR:** The study examines the impact of providing path information in autonomous personal mobility vehicles on passengers' head movement and motion sickness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how to reduce motion sickness in autonomous vehicles by providing directional information to passengers.

**Method:** A controlled experiment was conducted with 16 passengers, comparing their experiences in manual driving, autonomous driving without path information, and autonomous driving with path information.

**Key Contributions:**

	1. Demonstration of the effects of path information on reducing motion sickness in APMVs.
	2. Evidence that head movement alignment can improve passenger comfort during rides.
	3. Insights into the physiological responses associated with motion sickness in autonomous driving contexts.

**Result:** Path cues significantly reduced motion sickness scores and delayed the onset of symptoms; passengers aligned their head movements with vehicle direction more effectively in certain conditions.

**Limitations:** The study involved a small sample size and further research is needed to explore the underlying physiological mechanisms.

**Conclusion:** Providing path information can mitigate motion sickness in APMVs, indicating a need to explore the physiological mechanisms involved.

**Abstract:** Autonomous personal mobility vehicles (APMVs) are small mobility devices designed for individual automated transportation in shared spaces. In such environments, frequent pedestrian avoidance maneuvers may cause rapid steering adjustments and passive postural responses from passengers, thereby increasing the risk of motion sickness. This study investigated the effects of providing path information on 16 passengers' head movement behavior and motion sickness while riding an APMV. Through a controlled experiment comparing manual driving (MD), autonomous driving without path information (AD w/o path), and autonomous driving with path information (AD w/ path), we found that providing path cues significantly reduced MISC scores and delayed the onset of motion sickness symptoms. In addition, participants were more likely to proactively align their head movements with the direction of vehicle rotation in both MD and AD w/ path conditions. Although a small correlation was observed between the delay in yaw rotation of the passenger's head relative to the vehicle and the occurrence of motion sickness, the underlying physiological mechanism remains to be elucidated.

</details>


### [22] [Neuro-Informed Joint Learning Enhances Cognitive Workload Decoding in Portable BCIs](https://arxiv.org/abs/2506.23458)

*Xiaoxiao Yang, Chan Feng, Jiancheng Chen*

**Main category:** cs.HC

**Keywords:** EEG, BCI, Cognitive Load Detection, Self-Supervised Learning, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper presents MuseCogNet, a unified learning framework for enhancing the performance of portable EEG devices in cognitive load detection by integrating self-supervised and supervised training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the trade-off between portability and performance in portable EEG devices used for brain-computer interface applications, particularly for cognitive load detection.

**Method:** A unified joint learning framework combining self-supervised reconstruction loss and supervised cross-entropy loss, designed to capture robust neurophysiological patterns and refine task-specific cognitive discriminants.

**Key Contributions:**

	1. Introduction of a joint learning framework for EEG data analysis.
	2. Development of an EEG-grounded self-supervised reconstruction loss.
	3. Demonstration of superior performance on cognitive load detection using a public Muse dataset.

**Result:** MuseCogNet significantly outperforms state-of-the-art methods on a publicly available Muse dataset, demonstrating its effectiveness for neurocognitive monitoring in ecological settings.

**Limitations:** 

**Conclusion:** The proposed MuseCogNet establishes a pathway for improved cognitive monitoring using portable EEG devices, balancing the trade-off between mobility and accuracy.

**Abstract:** Portable and wearable consumer-grade electroencephalography (EEG) devices, like Muse headbands, offer unprecedented mobility for daily brain-computer interface (BCI) applications, including cognitive load detection. However, the exacerbated non-stationarity in portable EEG signals constrains data fidelity and decoding accuracy, creating a fundamental trade-off between portability and performance. To mitigate such limitation, we propose MuseCogNet (Muse-based Cognitive Network), a unified joint learning framework integrating self-supervised and supervised training paradigms. In particular, we introduce an EEG-grounded self-supervised reconstruction loss based on average pooling to capture robust neurophysiological patterns, while cross-entropy loss refines task-specific cognitive discriminants. This joint learning framework resembles the bottom-up and top-down attention in humans, enabling MuseCogNet to significantly outperform state-of-the-art methods on a publicly available Muse dataset and establish an implementable pathway for neurocognitive monitoring in ecological settings.

</details>


### [23] [Immersive Technologies in Training and Healthcare: From Space Missions to Psychophysiological Research](https://arxiv.org/abs/2506.23545)

*Barbara Karpowicz, Maciej Grzeszczuk, Adam Kuzdraliński, Monika Kornacka, Aliaksandr Marozau, Wiktor Stawski, Pavlo Zinevych, Grzegorz Marcin Wójcik, Tomasz Kowalewski, Grzegorz Pochwatko, Wiesław Kopeć*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Augmented Reality, Human Performance, Medical Education, Psychological Research

**Relevance Score:** 8

**TL;DR:** Panel discussion on the applications of VR/AR/XR technologies in enhancing human performance across various domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how immersive systems like VR/AR/XR can improve training, diagnostics, and research outcomes in high-risk environments.

**Method:** Discussion panel examining case studies and applications in clinical psychology, space exploration, and medical education.

**Key Contributions:**

	1. Showcasing diverse applications of VR/AR/XR in safety-critical domains
	2. Highlighting the role of immersive environments in psychological research
	3. Describing advancements in VR training for astronauts and procedures in medical education

**Result:** Findings highlight the effectiveness of XR in providing controlled environments for psychological measurements and enhancing training for astronauts and medical professionals.

**Limitations:** 

**Conclusion:** Immersive technologies significantly improve training experiences, learning outcomes in medical education, and engagement in rehabilitation.

**Abstract:** Virtual, Augmented, and eXtended Reality (VR/AR/XR) technologies are increasingly recognized for their applications in training, diagnostics, and psychological research, particularly in high-risk and highly regulated environments. In this panel we discuss how immersive systems enhance human performance across multiple domains, including clinical psychology, space exploration, and medical education. In psychological research and training, XR can offer a controlled yet ecologically valid setting for measuring cognitive and affective processes. In space exploration, we discuss the development of VR-based astronaut training and diagnostic systems, allowing astronauts to perform real-time health assessments. In medical education and rehabilitation, we cover procedural training and patient engagement. From virtual surgical simulations to gamified rehabilitation exercises, immersive environments enhance both learning outcomes and treatment adherence.

</details>


### [24] [Interactive Reasoning: Visualizing and Controlling Chain-of-Thought Reasoning in Large Language Models](https://arxiv.org/abs/2506.23678)

*Rock Yuren Pang, K. J. Kevin Feng, Shangbin Feng, Chu Li, Weijia Shi, Yulia Tsvetkov, Jeffrey Heer, Katharina Reinecke*

**Main category:** cs.HC

**Keywords:** Interactive Reasoning, Chain-of-thought, User feedback, LLM, Decision making

**Relevance Score:** 9

**TL;DR:** The paper presents Interactive Reasoning, a design that organizes chain-of-thought outputs into a visual hierarchy for enhanced user review and modification in LLM-assisted decision making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve output quality of LLMs by allowing user oversight and feedback during the reasoning process.

**Method:** We introduced Interactive Reasoning, visualizing chain-of-thought outputs hierarchically and implemented it in the Hippo prototype.

**Key Contributions:**

	1. Introduction of Interactive Reasoning design
	2. Implementation in the Hippo prototype
	3. Empirical evidence from user study highlighting benefits of user interaction

**Result:** User study findings indicate that Interactive Reasoning helps users quickly rectify errors, customize responses, and better understand LLM reasoning.

**Limitations:** 

**Conclusion:** This work establishes a paradigm for integrating user feedback into LLM reasoning, enhancing user control and comprehension.

**Abstract:** The output quality of large language models (LLMs) can be improved via "reasoning": generating segments of chain-of-thought (CoT) content to further condition the model prior to producing user-facing output. While these chains contain valuable information, they are verbose and lack explicit organization, making them tedious to review. Moreover, they lack opportunities for user feedback, such as to remove unwanted considerations, add desired ones, or clarify unclear assumptions. We introduce Interactive Reasoning, an interaction design that visualizes chain-of-thought outputs as a hierarchy of topics and enables user review and modification. We implement interactive reasoning in Hippo, a prototype for AI-assisted decision making in the face of uncertain trade-offs. In a user study with 16 participants, we find that interactive reasoning in Hippo allows users to quickly identify and interrupt erroneous generations, efficiently steer the model towards customized responses, and better understand both model reasoning and model outputs. Our work contributes to a new paradigm that incorporates user oversight into LLM reasoning processes.

</details>


### [25] [If You Had to Pitch Your Ideal Software -- Evaluating Large Language Models to Support User Scenario Writing for User Experience Experts and Laypersons](https://arxiv.org/abs/2506.23694)

*Patrick Stadler, Christopher Lazik, Christopher Katins, Thomas Kosch*

**Main category:** cs.HC

**Keywords:** User Experience, LLM, User Scenarios, Requirements Analysis, UX Design

**Relevance Score:** 8

**TL;DR:** LLM-supported writing assistants enable UX novices to create coherent user scenarios comparable to those of experts, aiding requirements analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding user needs is critical in requirements analysis, traditionally performed by UX experts. This paper investigates the capability of novices using LLMs to generate effective user scenarios.

**Method:** A user study involving 60 participants—30 UX experts and 30 novices—was conducted, where participants wrote user scenarios with and without LLM assistance.

**Key Contributions:**

	1. Demonstrated the efficacy of LLMs in improving novice UX scenario writing.
	2. Provided insights into differences between expert and novice outputs in user scenario construction.
	3. Presented quantitative and qualitative findings on user scenarios' clarity and structure.

**Result:** Findings indicate that the LLM-supported novice scenarios matched experts in structure and clarity and excelled in audience-orientation, providing valuable input for requirements analysis.

**Limitations:** The study is limited by the sample size and the specific context in which the scenarios were written.

**Conclusion:** LLMs can empower UX novices to produce high-quality user scenarios, potentially bridging the gap between expert and novice in user analysis tasks.

**Abstract:** The process of requirements analysis requires an understanding of the end users of a system. Thus, expert stakeholders, such as User Experience (UX) designers, usually create various descriptions containing information about the users and their possible needs. In our paper, we investigate to what extent UX novices are able to write such descriptions into user scenarios. We conducted a user study with 60 participants consisting of 30 UX experts and 30 novices who were asked to write a user scenario with or without the help of an LLM-supported writing assistant. Our findings show that LLMs empower laypersons to write reasonable user scenarios and provide first-hand insights for requirements analysis that are comparable to UX experts in terms of structure and clarity, while especially excelling at audience-orientation. We present our qualitative and quantitative findings, including user scenario anatomies, potential influences, and differences in the way participants approached the task.

</details>


### [26] [The Impact of AI on Educational Assessment: A Framework for Constructive Alignment](https://arxiv.org/abs/2506.23815)

*Patrick Stokkink*

**Main category:** cs.HC

**Keywords:** AI in education, Large Language Models, Bloom's taxonomy, Constructive Alignment, assessment adaptation

**Relevance Score:** 6

**TL;DR:** The paper discusses the impact of AI and Large Language Models on educational assessment, suggesting adaptations based on Bloom's taxonomy and proposing guidelines for educators.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address whether current assessment methods are valid in light of AI's influence on education and to propose necessary adaptations.

**Method:** Theoretical framework based on Constructive Alignment theory and Bloom's taxonomy.

**Key Contributions:**

	1. Developed a theoretical framework linking AI impact on education and Bloom's taxonomy.
	2. Proposed structured guidelines for assessment adaptation in light of AI.
	3. Advocated for training teaching staff on AI tools.

**Result:** It identifies a bias among lecturers regarding AI use in assessments and provides guidelines for consistent assessment practices.

**Limitations:** 

**Conclusion:** The adaptation of assessment methods is crucial, and structured guidelines should be developed alongside training for teaching staff on AI tools.

**Abstract:** The influence of Artificial Intelligence (AI), and specifically Large Language Models (LLM), on education is continuously increasing. These models are frequently used by students, giving rise to the question whether current forms of assessment are still a valid way to evaluate student performance and comprehension. The theoretical framework developed in this paper is grounded in Constructive Alignment (CA) theory and Bloom's taxonomy for defining learning objectives. We argue that AI influences learning objectives of different Bloom levels in a different way, and assessment has to be adopted accordingly. Furthermore, in line with Bloom's vision, formative and summative assessment should be aligned on whether the use of AI is permitted or not.   Although lecturers tend to agree that education and assessment need to be adapted to the presence of AI, a strong bias exists on the extent to which lecturers want to allow for AI in assessment. This bias is caused by a lecturer's familiarity with AI and specifically whether they use it themselves. To avoid this bias, we propose structured guidelines on a university or faculty level, to foster alignment among the staff. Besides that, we argue that teaching staff should be trained on the capabilities and limitations of AI tools. In this way, they are better able to adapt their assessment methods.

</details>


### [27] [Email as the Interface to Generative AI Models: Seamless Administrative Automation](https://arxiv.org/abs/2506.23850)

*Andres Navarro, Carlos de Quinto, José Alberto Hernández*

**Main category:** cs.HC

**Keywords:** Large Language Models, email automation, administrative tasks, accessibility, intelligent automation

**Relevance Score:** 9

**TL;DR:** The paper introduces a framework that integrates Large Language Models with email to automate administrative tasks, overcoming accessibility barriers for non-technical users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate administrative tasks in enterprise environments and reduce accessibility barriers for non-technical staff.

**Method:** The framework uses email communication integrated with Optical Character Recognition and intelligent automation, allowing users to delegate tasks through familiar email interfaces.

**Key Contributions:**

	1. Integration of LLMs with email for automation
	2. Significant time and cost savings in administrative tasks
	3. Supports accessibility for non-technical users

**Result:** The system can complete complex administrative forms in under 8 seconds, reducing staff time by a factor of three to four compared to manual workflows and achieving a 64% cost reduction per processed form.

**Limitations:** 

**Conclusion:** Email-based LLM integration is a viable, cost-effective solution for automating tasks without requiring specialized knowledge, promoting inclusivity in technology use.

**Abstract:** This paper introduces a novel architectural framework that integrates Large Language Models (LLMs) with email interfaces to automate administrative tasks, specifically targeting accessibility barriers in enterprise environments. The system connects email communication channels with Optical Character Recognition (OCR) and intelligent automation, enabling non-technical administrative staff to delegate complex form-filling and document processing tasks using familiar email interfaces. By treating the email body as a natural language prompt and attachments as contextual information, the workflow bridges the gap between advanced AI capabilities and practical usability. Empirical evaluation shows that the system can complete complex administrative forms in under 8 seconds of automated processing, with human supervision reducing total staff time by a factor of three to four compared to manual workflows. The top-performing LLM accurately filled 16 out of 29 form fields and reduced the total cost per processed form by 64% relative to manual completion. These findings demonstrate that email-based LLM integration is a viable and cost-effective approach for democratizing advanced automation in organizational settings, supporting widespread adoption without requiring specialized technical knowledge or major workflow changes. This aligns with broader trends in leveraging LLMs to enhance accessibility and automate complex tasks for non-technical users, making technology more inclusive and efficient.

</details>


### [28] [Autonomy by Design: Preserving Human Autonomy in AI Decision-Support](https://arxiv.org/abs/2506.23952)

*Stefan Buijsman, Sarah Carter, Juan Pablo Bermúdez*

**Main category:** cs.HC

**Keywords:** AI, Decision-support systems, Domain-specific autonomy, Human agency, Socio-technical design

**Relevance Score:** 8

**TL;DR:** The paper analyzes the impact of AI decision-support systems on domain-specific autonomy, focusing on skilled competence and authentic value-formation, and presents a framework to preserve autonomy in AI applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how AI affects domain-specific autonomy in professional and skill-based contexts, particularly in medical, financial, and educational fields.

**Method:** Analyses empirical cases and engages with prior research to identify how AI influences skilled competence and authenticity in decision-making.

**Key Contributions:**

	1. Analysis of AI's effect on domain-specific autonomy
	2. Identification of critical components of autonomy affected by AI
	3. Development of a framework for autonomy-preserving AI support systems

**Result:** The study reveals that AI can erode domain-specific autonomy by removing failure indicators and leading to unconscious value shifts.

**Limitations:** 

**Conclusion:** A proposed framework with design patterns is suggested to help create AI systems that maintain, rather than diminish, human agency and autonomy in specialized domains.

**Abstract:** AI systems increasingly support human decision-making across domains of professional, skill-based, and personal activity. While previous work has examined how AI might affect human autonomy globally, the effects of AI on domain-specific autonomy -- the capacity for self-governed action within defined realms of skill or expertise -- remain understudied. We analyze how AI decision-support systems affect two key components of domain-specific autonomy: skilled competence (the ability to make informed judgments within one's domain) and authentic value-formation (the capacity to form genuine domain-relevant values and preferences). By engaging with prior investigations and analyzing empirical cases across medical, financial, and educational domains, we demonstrate how the absence of reliable failure indicators and the potential for unconscious value shifts can erode domain-specific autonomy both immediately and over time. We then develop a constructive framework for autonomy-preserving AI support systems. We propose specific socio-technical design patterns -- including careful role specification, implementation of defeater mechanisms, and support for reflective practice -- that can help maintain domain-specific autonomy while leveraging AI capabilities. This framework provides concrete guidance for developing AI systems that enhance rather than diminish human agency within specialized domains of action.

</details>


### [29] [Access InContext: Futuring Accessible Prototyping Tools and Methods](https://arxiv.org/abs/2506.24057)

*Patricia Piedade, Peter A Hayton, Cynthia Bennett, Anna R L Carter, Clara Crivellaro, Alan Dix, Jess McGowan, Katta Spiel, Miriam Sturdee, Garreth W. Tigwell, Hugo Nicolau*

**Main category:** cs.HC

**Keywords:** accessibility, human-computer interaction, prototyping, digital inclusion, design methods

**Relevance Score:** 8

**TL;DR:** A workshop at CHI 2025 focusing on improving accessibility in prototyping methods for HCI researchers through discussion and hands-on exercises.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To increase digital inclusion for people with disabilities and improve the effectiveness of prototyping tools in HCI research.

**Method:** Facilitating discussions among HCI researchers, designers, and practitioners, and conducting hands-on ideation and fabrication exercises.

**Key Contributions:**

	1. Platform for discussion on accessibility in prototyping
	2. Hands-on ideation and fabrication exercises
	3. Focus on retrofitting existing tools for accessibility

**Result:** Identification of accessibility barriers in current prototyping practices and generation of new tools and methods for more inclusive design.

**Limitations:** 

**Conclusion:** A call for collaborative efforts to create accessible prototyping methods that engage both researchers and individuals with disabilities.

**Abstract:** The popularity of accessibility research has grown recently, improving digital inclusion for people with disabilities. However, researchers, including those who have disabilities, have attempted to include people with disabilities in all aspects of design, and they have identified a myriad of practical accessibility barriers posed by tools and methods leveraged by human-computer interaction (HCI) researchers during prototyping. To build a more inclusive technological landscape, we must question the effectiveness of existing prototyping tools and methods, repurpose/retrofit existing resources, and build new tools and methods to support the participation of both researchers and people with disabilities within the prototyping design process of novel technologies. This full-day workshop at CHI 2025 will provide a platform for HCI researchers, designers, and practitioners to discuss barriers and opportunities for creating accessible prototyping and promote hands-on ideation and fabrication exercises aimed at futuring accessible prototyping.

</details>


### [30] [Bridging Service Design, Visualizations, and Visual Analytics in Healthcare Digital Twins: Challenges, Gaps, and Research Opportunities](https://arxiv.org/abs/2506.24104)

*Mariia Ershova, Graziano Blasilli*

**Main category:** cs.HC

**Keywords:** Digital Twins, Healthcare, Service Design, Visual Analytics, Visualization

**Relevance Score:** 8

**TL;DR:** The paper proposes integrating service design methodologies with visual analytics and visualization in healthcare digital twins to improve applicability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap between service design and visualization in healthcare digital twins.

**Method:** The paper discusses the current use of digital twins in healthcare, identifying the lack of structured service design methodologies in existing solutions.

**Key Contributions:**

	1. Introduces service design to visualization researchers in healthcare
	2. Identifies a gap in current digital twin methodologies
	3. Proposes new research directions for better applicability of digital twins

**Result:** Suggests research directions that can enhance the real-world applicability of digital twin solutions in healthcare.

**Limitations:** 

**Conclusion:** Integrating service design into the visualization of healthcare digital twins can improve their effectiveness and usability.

**Abstract:** Digital twins (DT) are increasingly used in healthcare to model patients, processes, and physiological systems. While recent solutions leverage visualization, visual analytics, and user interaction, these systems rarely incorporate structured service design methodologies. Bridging service design with visual analytics and visualization can be valuable for the healthcare DT community. This paper aims to introduce the service design discipline to visualization researchers by framing this integration gap and suggesting research directions to enhance the real-world applicability of DT solutions.

</details>


### [31] [A Robot-Led Intervention for Emotion Regulation: From Expression to Reappraisal](https://arxiv.org/abs/2503.18243)

*Guy Laban, Julie Wang, Hatice Gunes*

**Main category:** cs.HC

**Keywords:** emotion regulation, social robot, cognitive reappraisal, affective language, GPT-3.5

**Relevance Score:** 8

**TL;DR:** This study investigates how a social robot, utilizing GPT-3.5, can assist university students in emotion regulation through structured conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To find effective methods for supporting emotion regulation processes, which are cognitively demanding.

**Method:** Twenty-one students engaged in five sessions with a social robot in a university environment, discussing emotionally charged situations to encourage cognitive reappraisal.

**Key Contributions:**

	1. Demonstrated the effectiveness of a social robot in facilitating emotion regulation through structured conversations.
	2. Showed significant improvements in participants' emotional self-control and expressiveness after interactions with the robot.
	3. Provided insights into the patterns of emotional expression related to the reappraisal process.

**Result:** Participants showed significant improvements in emotion self-regulation, increased emotional expressiveness, and positive feedback on mood after sessions.

**Limitations:** 

**Conclusion:** The findings suggest that robots can provide valuable emotional support and cognitive guidance for effective emotion regulation in familiar settings.

**Abstract:** Emotion regulation is a crucial skill for managing emotions in everyday life, yet finding a constructive and accessible method to support these processes remains challenging due to their cognitive demands. In this study, we explore how regular interactions with a social robot, conducted in a structured yet familiar environment within university halls and departments, can provide effective support for emotion regulation through cognitive reappraisal. Twenty-one students participated in a five-session study at a university hall or department, where the robot, powered by a large language model (GPT-3.5), facilitated structured conversations, encouraging the students to reinterpret emotionally charged situations they shared with the robot. Quantitative and qualitative results indicate significant improvements in emotion self-regulation, with participants reporting better understanding and control of their emotions. The intervention led to significant changes in constructive emotion regulation tendencies and positive effects on mood and sentiment after each session. The findings also demonstrate that repeated interactions with the robot encouraged greater emotional expressiveness, including longer speech disclosures, increased use of affective language, and heightened facial arousal. Notably, expressiveness followed structured patterns aligned with the reappraisal process, with expression peaking during key reappraisal moments, particularly when participants were prompted to reinterpret negative experiences. The qualitative feedback further highlighted how the robot fostered introspection and provided a supportive space for discussing emotions, enabling participants to confront long-avoided emotional challenges. These findings demonstrate the potential of robots to effectively assist in emotion regulation in familiar environments, offering both emotional support and cognitive guidance.

</details>


### [32] [earEOG via Periauricular Electrodes to Facilitate Eye Tracking in a Natural Headphone Form Factor](https://arxiv.org/abs/2506.07193)

*Tobias King, Michael Knierim, Philipp Lepold, Christopher Clarke, Hans Gellersen, Michael Beigl, Tobias Röddiger*

**Main category:** cs.HC

**Keywords:** Electrooculography, Eye tracking, Human-Computer Interaction, Gaze-based interaction, Neurological assessment

**Relevance Score:** 7

**TL;DR:** This paper investigates an innovative ear-based Electrooculography (EOG) eye tracking method using a headphone form factor, evaluating its performance against traditional methods for both horizontal and vertical eye movements.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional eye tracking techniques that require cumbersome hardware and substantial computational resources, we explore a novel ear-based EOG approach.

**Method:** A controlled experiment was conducted with 16 participants using custom-built headphones with 14 electrodes to track eye movements through smooth pursuits and saccades, comparing results with gold-standard methods.

**Key Contributions:**

	1. Introduction of a novel ear-based EOG eye tracking method
	2. Demonstration of the effectiveness of the setup for horizontal movements
	3. Comparison of earEOG results against both gold-standard EOG and camera methods

**Result:** The best performance was observed for horizontal tracking via earEOG, showing strong correlation with gold-standard measures, while vertical tracking proved less effective with weak correlations detected.

**Limitations:** Vertical eye movement tracking was found to be poor, indicating issues with setup feasibility in current configurations.

**Conclusion:** The findings suggest that while horizontal earEOG is promising for eye movement tracking, vertical tracking needs significant improvement for practical use.

**Abstract:** Eye tracking technology is frequently utilized to diagnose eye and neurological disorders, assess sleep and fatigue, study human visual perception, and enable novel gaze-based interaction methods. However, traditional eye tracking methodologies are constrained by bespoke hardware that is often cumbersome to wear, complex to apply, and demands substantial computational resources. To overcome these limitations, we investigated Electrooculography (EOG) eye tracking using 14 electrodes positioned around the ears, integrated into a custom-built headphone form factor device. In a controlled experiment, 16 participants tracked stimuli designed to induce smooth pursuits and saccades. Data analysis identified optimal electrode pairs for vertical and horizontal eye movement tracking, benchmarked against gold-standard EOG and camera-based methods. The electrode montage nearest the eyes yielded the best horizontal results. Horizontal smooth pursuits via earEOG showed high correlation with gold-standard measures ($r_{\mathrm{EOG}} = 0.81, p = 0.01$; $r_{\mathrm{CAM}} = 0.56, p = 0.02$), while vertical pursuits were weakly correlated ($r_{\mathrm{EOG}} = 0.28, p = 0.04$; $r_{\mathrm{CAM}} = 0.35, p = 0.05$). Voltage deflections when performing saccades showed strong correlation in the horizontal direction ($r_{\mathrm{left}} = 0.99, p = 0.0$; $r_{\mathrm{right}} = 0.99, p = 0.0$) but low correlation in the vertical direction ($r_{\mathrm{up}} = 0.6, p = 0.23$; $r_{\mathrm{down}} = 0.19, p = 0.73$). Overall, horizontal earEOG demonstrated strong performance, indicating its potential effectiveness, while vertical earEOG results were poor, suggesting limited feasibility in our current setup.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [33] [Psycholinguistic Word Features: a New Approach for the Evaluation of LLMs Alignment with Humans](https://arxiv.org/abs/2506.22439)

*Javier Conde, Miguel González, María Grandury, Gonzalo Martínez, Pedro Reviriego, Mar Brysbaert*

**Main category:** cs.CL

**Keywords:** LLMs, psycholinguistics, word features, language models, human ratings

**Relevance Score:** 9

**TL;DR:** This paper evaluates how well large language models (LLMs) align with human ratings of various word features identified by psycholinguistic studies, using the Glasgow and Lancaster norms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The evaluation of LLMs has primarily focused on task performance, neglecting features that cannot be easily quantified. This paper aims to understand LLM performance in relation to human psycholinguistic word ratings.

**Method:** The study assesses a group of LLMs against human ratings on the Glasgow and Lancaster psycholinguistic datasets, which include thirteen different word features.

**Key Contributions:**

	1. Evaluation of LLMs against psycholinguistic datasets
	2. Comparison between Glasgow and Lancaster norms
	3. Insight into LLM limitations in sensory word association

**Result:** The results indicate that LLMs show better alignment with human ratings in the Glasgow norms (covering emotional and cognitive features) compared to the Lancaster norms (covering sensory features).

**Limitations:** The findings highlight limitations of LLMs in embodying human-like sensory associations, which may affect their performance in real-world applications.

**Conclusion:** The findings suggest that LLMs might struggle with aligning sensory word associations, pointing to a gap between LLM capabilities and human embodied cognition.

**Abstract:** The evaluation of LLMs has so far focused primarily on how well they can perform different tasks such as reasoning, question-answering, paraphrasing, or translating. For most of these tasks, performance can be measured with objective metrics, such as the number of correct answers. However, other language features are not easily quantified. For example, arousal, concreteness, or gender associated with a given word, as well as the extent to which we experience words with senses and relate them to a specific sense. Those features have been studied for many years by psycholinguistics, conducting large-scale experiments with humans to produce ratings for thousands of words. This opens an opportunity to evaluate how well LLMs align with human ratings on these word features, taking advantage of existing studies that cover many different language features in a large number of words. In this paper, we evaluate the alignment of a representative group of LLMs with human ratings on two psycholinguistic datasets: the Glasgow and Lancaster norms. These datasets cover thirteen features over thousands of words. The results show that alignment is \textcolor{black}{generally} better in the Glasgow norms evaluated (arousal, valence, dominance, concreteness, imageability, familiarity, and gender) than on the Lancaster norms evaluated (introceptive, gustatory, olfactory, haptic, auditory, and visual). This suggests a potential limitation of current LLMs in aligning with human sensory associations for words, which may be due to their lack of embodied cognition present in humans and illustrates the usefulness of evaluating LLMs with psycholinguistic datasets.

</details>


### [34] [AI Agents-as-Judge: Automated Assessment of Accuracy, Consistency, Completeness and Clarity for Enterprise Documents](https://arxiv.org/abs/2506.22485)

*Sudip Dasgupta, Himanshu Shankar*

**Main category:** cs.CL

**Keywords:** AI, multi-agent system, document review, enterprise, quality assurance

**Relevance Score:** 5

**TL;DR:** A multi-agent AI system automates review of enterprise business documents, improving accuracy and efficiency while addressing biases through human feedback.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the review process of structured enterprise documents using modular AI agents, overcoming limitations of previous unstructured approaches.

**Method:** The system employs various AI orchestration tools (LangChain, CrewAI, TruLens, Guidance) to facilitate section-by-section evaluations by specialized agents for specific review criteria.

**Key Contributions:**

	1. Introduction of modular, multi-agent AI for document review
	2. Significant improvements in accuracy and efficiency over human review
	3. Framework for continuous improvement and bias mitigation through human feedback

**Result:** Quantitative results show the system achieving 99% information consistency, reducing review time from 30 minutes to 2.5 minutes, and maintaining a 95% agreement rate with human judgments.

**Limitations:** Requires human oversight in specialized domains and incurs costs for large-scale LLM usage.

**Conclusion:** This modular AI framework is scalable for document quality assurance, though human oversight remains necessary in specialized areas due to operational costs associated with LLMs.

**Abstract:** This study presents a modular, multi-agent system for the automated review of highly structured enterprise business documents using AI agents. Unlike prior solutions focused on unstructured texts or limited compliance checks, this framework leverages modern orchestration tools such as LangChain, CrewAI, TruLens, and Guidance to enable section-by-section evaluation of documents for accuracy, consistency, completeness, and clarity. Specialized agents, each responsible for discrete review criteria such as template compliance or factual correctness, operate in parallel or sequence as required. Evaluation outputs are enforced to a standardized, machine-readable schema, supporting downstream analytics and auditability. Continuous monitoring and a feedback loop with human reviewers allow for iterative system improvement and bias mitigation.   Quantitative evaluation demonstrates that the AI Agent-as-Judge system approaches or exceeds human performance in key areas: achieving 99% information consistency (vs. 92% for humans), halving error and bias rates, and reducing average review time from 30 to 2.5 minutes per document, with a 95% agreement rate between AI and expert human judgment. While promising for a wide range of industries, the study also discusses current limitations, including the need for human oversight in highly specialized domains and the operational cost of large-scale LLM usage. The proposed system serves as a flexible, auditable, and scalable foundation for AI-driven document quality assurance in the enterprise context.

</details>


### [35] [Hallucination Detection with Small Language Models](https://arxiv.org/abs/2506.22486)

*Ming Cheung*

**Main category:** cs.CL

**Keywords:** Large Language Models, Response Verification, Hallucination Detection, Machine Learning, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This paper presents a framework using multiple small language models to verify responses generated by large language models (LLMs) through detection of hallucinations in answers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unreliability of LLM responses caused by hallucinations, particularly in question-and-answer scenarios, and to improve the accuracy of generated answers.

**Method:** The framework integrates multiple small language models that verify responses by analyzing the sentence structure and retrieving context from a vectorized database. It evaluates the probability of generating 'Yes' tokens for the outputs based on specific questions, responses, and relevant context.

**Key Contributions:**

	1. Development of a framework for response verification using multiple small language models
	2. Demonstrated improvement in detection accuracy of LLM-generated responses
	3. Provided a scalable solution applicable to academic and practical scenarios.

**Result:** The proposed framework achieved a 10% improvement in F1 scores for detecting correct responses versus hallucinations in experiments with real datasets comprising over 100 sets of questions, answers, and contexts.

**Limitations:** 

**Conclusion:** Utilizing multiple small language models for answer verification provides a scalable and efficient solution for both academic research and practical applications, enhancing trust in LLM-generated responses.

**Abstract:** Since the introduction of ChatGPT, large language models (LLMs) have demonstrated significant utility in various tasks, such as answering questions through retrieval-augmented generation. Context can be retrieved using a vectorized database, serving as a foundation for LLMs to generate responses. However, hallucinations in responses can undermine the reliability of LLMs in practical applications, and they are not easily detectable in the absence of ground truth, particularly in question-and-answer scenarios. This paper proposes a framework that integrates multiple small language models to verify responses generated by LLMs using the retrieved context from a vectorized database. By breaking down the responses into individual sentences and utilizing the probability of generating "Yes" tokens from the outputs of multiple models for a given set of questions, responses, and relevant context, hallucinations can be detected. The proposed framework is validated through experiments with real datasets comprising over 100 sets of questions, answers, and contexts, including responses with fully and partially correct sentences. The results demonstrate a 10\% improvement in F1 scores for detecting correct responses compared to hallucinations, indicating that multiple small language models can be effectively employed for answer verification, providing a scalable and efficient solution for both academic and practical applications.

</details>


### [36] [PromptAug: Fine-grained Conflict Classification Using Data Augmentation](https://arxiv.org/abs/2506.22491)

*Oliver Warke, Joemon M. Jose, Faegheh Hasibi, Jan Breitsohl*

**Main category:** cs.CL

**Keywords:** data augmentation, conflict detection, machine learning, natural language processing, social media

**Relevance Score:** 9

**TL;DR:** The paper introduces PromptAug, a data augmentation method using Large Language Models (LLMs) to enhance the quality of training data for conflict detection on social media, achieving notable improvements in model performance despite challenges in generating sensitive content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the increasing prevalence of conflicts on social media, there is a crucial need for effective machine learning models to classify harmful behaviours, which heavily rely on high-quality labelled training data that is often hard to obtain.

**Method:** PromptAug utilizes LLMs to generate augmented data for training conflict detection models, addressing the barriers of data scarcity and the limitations posed by content generation regulations on social media.

**Key Contributions:**

	1. Introduction of PromptAug for data augmentation in conflict detection
	2. Statistically significant improvements in model performance metrics
	3. Identification of unique challenges and patterns in augmented text

**Result:** PromptAug demonstrates statistically significant improvements of 2% in both accuracy and F1-score when applied to conflict and emotion datasets compared to existing data augmentation methods.

**Limitations:** The method's effectiveness is tested in extreme data scarcity scenarios, which may not represent all practical applications.

**Conclusion:** The study presents PromptAug as a viable solution to enhance training data for sensitive tasks such as conflict detection, while identifying critical patterns that arise in augmented text content.

**Abstract:** Given the rise of conflicts on social media, effective classification models to detect harmful behaviours are essential. Following the garbage-in-garbage-out maxim, machine learning performance depends heavily on training data quality. However, high-quality labelled data, especially for nuanced tasks like identifying conflict behaviours, is limited, expensive, and difficult to obtain. Additionally, as social media platforms increasingly restrict access to research data, text data augmentation is gaining attention as an alternative to generate training data. Augmenting conflict-related data poses unique challenges due to Large Language Model (LLM) guardrails that prevent generation of offensive content. This paper introduces PromptAug, an innovative LLM-based data augmentation method. PromptAug achieves statistically significant improvements of 2% in both accuracy and F1-score on conflict and emotion datasets. To thoroughly evaluate PromptAug against other data augmentation methods we conduct a robust evaluation using extreme data scarcity scenarios, quantitative diversity analysis and a qualitative thematic analysis. The thematic analysis identifies four problematic patterns in augmented text: Linguistic Fluidity, Humour Ambiguity, Augmented Content Ambiguity, and Augmented Content Misinterpretation.   Overall, this work presents PromptAug as an effective method for augmenting data in sensitive tasks like conflict detection, offering a unique, interdisciplinary evaluation grounded in both natural language processing and social science methodology.

</details>


### [37] [AgentStealth: Reinforcing Large Language Model for Anonymizing User-generated Text](https://arxiv.org/abs/2506.22508)

*Chenyang Shao, Tianxing Li, Chenhao Pu, Fengli Xu, Yong Li*

**Main category:** cs.CL

**Keywords:** text anonymization, privacy protection, language models, human-computer interaction, reinforcement learning

**Relevance Score:** 8

**TL;DR:** A framework called AgentStealth is proposed for effective text anonymization using locally deployed smaller-scale language models, addressing privacy risks associated with cloud-based solutions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle risks of exposing sensitive personal attributes in user-generated content through effective text anonymization methods that do not rely on costly cloud-based LLMs or rigid replacements that harm utility.

**Method:** The paper introduces a self-reinforcing LLM anonymization framework that utilizes an adversarial anonymization workflow, In-context Contrastive Learning, and Adaptive Utility-Aware Control, along with supervised adaptation of SLMs and online reinforcement learning.

**Key Contributions:**

	1. Development of AgentStealth framework for anonymization using SLMs.
	2. Introduction of an adversarial workflow and adaptive control mechanisms.
	3. Demonstration of improved anonymization and utility through experimental results.

**Result:** AgentStealth outperforms existing anonymization techniques with a 12.3% improvement in anonymization effectiveness and a 6.8% improvement in utility across two tested datasets.

**Limitations:** 

**Conclusion:** The lightweight nature of AgentStealth enables it to be deployed directly on edge devices, reducing privacy risks associated with cloud service reliance while enhancing text anonymization effectiveness.

**Abstract:** In today's digital world, casual user-generated content often contains subtle cues that may inadvertently expose sensitive personal attributes. Such risks underscore the growing importance of effective text anonymization to safeguard individual privacy. However, existing methods either rely on rigid replacements that damage utility or cloud-based LLMs that are costly and pose privacy risks. To address these issues, we explore the use of locally deployed smaller-scale language models (SLMs) for anonymization. Yet training effective SLMs remains challenging due to limited high-quality supervision. To address the challenge, we propose AgentStealth, a self-reinforcing LLM anonymization framework.First, we introduce an adversarial anonymization workflow enhanced by In-context Contrastive Learning and Adaptive Utility-Aware Control. Second, we perform supervised adaptation of SLMs using high-quality data collected from the workflow, which includes both anonymization and attack signals. Finally, we apply online reinforcement learning where the model leverages its internal adversarial feedback to iteratively improve anonymization performance. Experiments on two datasets show that our method outperforms baselines in both anonymization effectiveness (+12.3%) and utility (+6.8%). Our lightweight design supports direct deployment on edge devices, avoiding cloud reliance and communication-based privacy risks. Our code is open-source at https://github.com/tsinghua-fib-lab/AgentStealth.

</details>


### [38] [Towards Text-free Graph Foundation Models: Rethinking Multi-Domain Graph Contrastive Learning](https://arxiv.org/abs/2506.22510)

*Zihao Zhao, Xinlong Zhai, Jinyu Yang, Chuan Shi*

**Main category:** cs.CL

**Keywords:** graph foundation models, multi-domain pre-training, contrastive learning

**Relevance Score:** 4

**TL;DR:** This paper introduces MDGCL, a framework for multi-domain pre-training and cross-domain transfer in graph data, addressing limitations of existing contrastive learning strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to effectively integrate multi-domain knowledge in graph foundation models due to the unique semantics and properties of graph data across different domains.

**Method:** A novel contrastive learning strategy is employed in the pre-training phase to capture domain differences, alongside the use of domain tokens and a domain attention mechanism for fine-grained transfer during the downstream phase.

**Key Contributions:**

	1. Introduction of a contrastive learning strategy focused on domain differences
	2. Development of domain tokens for encoding global information
	3. Implementation of a domain attention mechanism for enhanced knowledge transfer

**Result:** Extensive experiments show that MDGCL outperforms state-of-the-art methods with notable improvements in accuracy (up to 19.33%) and Macro-F1 score (up to 19.13%).

**Limitations:** 

**Conclusion:** The proposed framework significantly enhances the performance of graph models by recognizing and utilizing domain-specific information more effectively than conventional approaches.

**Abstract:** Foundation models have achieved great success in natural language processing (NLP) and computer vision (CV). Their success largely stems from the ability to integrate multi-domain knowledge in pre-training and transfer it to target domains. Considering graph data, especially graphs without textual features, is ubiquitous in real-world applications such as social networks and recommendation systems, some researchers have attempted to extend this paradigm to the graph field, aiming to construct graph foundation models. However, unlike CV and NLP, there are huge gaps among the semantics and properties of graphs in different domains, while current works still adopt traditional contrastive pre-training strategies designed in the single-domain scenario, which regard contrastive samples from different domains as equivalent. From experimental investigations, we discovered that inherent domain-specific differences prevent these strategies from effectively absorbing knowledge from different domains to generate informative representations. In this paper, we propose a novel multi-domain pre-training and cross-domain transfer framework, namely MDGCL.In the pre-training stage, we design a contrastive learning strategy to substantially recognize and capture domain differences, and introduce domain tokens to encode domain-level global information. In the downstream stage, we introduce a domain attention mechanism to enable fine-grained domain knowledge transfer. Extensive experiments on five benchmark datasets have demonstrated that our method outperforms state-of-the-art significantly, with the maximum improvement of 19.33\% on accuracy and 19.13\% on Macro-F1 score.

</details>


### [39] [Can "consciousness" be observed from large language model (LLM) internal states? Dissecting LLM representations obtained from Theory of Mind test with Integrated Information Theory and Span Representation analysis](https://arxiv.org/abs/2506.22516)

*Jingkai Li*

**Main category:** cs.CL

**Keywords:** Integrated Information Theory, Large Language Models, Theory of Mind, consciousness, spatio-permutational analyses

**Relevance Score:** 6

**TL;DR:** The paper investigates the application of Integrated Information Theory (IIT) on Large Language Models (LLMs) to explore consciousness phenomena, finding no significant indicators of consciousness in LLM representations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze whether Integrated Information Theory can reveal differences in Theory of Mind test performances in LLM representations and differentiate consciousness phenomena from inherent separations in LLMs.

**Method:** The study applies IIT 3.0 and 4.0 to LLM representations derived from ToM test results, analyzing transformer layer variations and linguistic spans to study potential consciousness indicators.

**Key Contributions:**

	1. Application of IIT on LLMs to evaluate consciousness phenomena
	2. Comparison of IIT measures with Span Representations
	3. Investigation of transformer layer variations and linguistic spans in relation to ToM test results

**Result:** Results indicate that Transformer-based LLM representations do not show statistically significant consciousness indicators but present interesting patterns under spatio-permutational analyses.

**Limitations:** The study's results reflect a specific set of data and may not generalize to all forms or contexts of LLMs and consciousness.

**Conclusion:** The findings suggest that LLM representations may not be indicative of consciousness phenomena as defined by IIT, despite intriguing patterns present in their structure.

**Abstract:** Integrated Information Theory (IIT) provides a quantitative framework for explaining consciousness phenomenon, positing that conscious systems comprise elements integrated through causal properties. We apply IIT 3.0 and 4.0 -- the latest iterations of this framework -- to sequences of Large Language Model (LLM) representations, analyzing data derived from existing Theory of Mind (ToM) test results. Our study systematically investigates whether the differences of ToM test performances, when presented in the LLM representations, can be revealed by IIT estimates, i.e., $\Phi^{\max}$ (IIT 3.0), $\Phi$ (IIT 4.0), Conceptual Information (IIT 3.0), and $\Phi$-structure (IIT 4.0). Furthermore, we compare these metrics with the Span Representations independent of any estimate for consciousness. This additional effort aims to differentiate between potential "consciousness" phenomena and inherent separations within LLM representational space. We conduct comprehensive experiments examining variations across LLM transformer layers and linguistic spans from stimuli. Our results suggest that sequences of contemporary Transformer-based LLM representations lack statistically significant indicators of observed "consciousness" phenomena but exhibit intriguing patterns under $\textit{spatio}$-permutational analyses. The Appendix and code are available as Supplementary Materials at: https://doi.org/10.1016/j.nlp.2025.100163.

</details>


### [40] [Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation](https://arxiv.org/abs/2506.22518)

*Deyu Zou, Yongqiang Chen, Mufei Li, Siqi Miao, Chenxi Liu, Bo Han, James Cheng, Pan Li*

**Main category:** cs.CL

**Keywords:** Graph-based RAG, Large Language Models, Weak Retrievers

**Relevance Score:** 9

**TL;DR:** This paper presents Refined Graph-based RAG (ReG), a method to enhance the performance of graph-based retrieval-augmented generation using large language models by improving the quality of retriever supervision and reorganizing retrieved knowledge into coherent structures.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The existing graph-based RAG methods struggle with weak retrievers that introduce noise and poorly structured knowledge, limiting the effectiveness of large language models.

**Method:** ReG uses LLM feedback to improve supervision quality and includes a structure-aware reorganization module to create coherent evidence chains from retrieved knowledge.

**Key Contributions:**

	1. Introduction of ReG for better alignment of weak retrievers with LLMs
	2. Structure-aware reorganization of retrieval results into coherent evidence chains
	3. Demonstrated significant performance improvements with reduced training data and reasoning costs

**Result:** Experiments show that ReG improves performance by up to 10% across various LLM backbones, achieves state-of-the-art results with only 5% of the training data, and reduces reasoning token costs by up to 30%.

**Limitations:** 

**Conclusion:** ReG effectively enhances graph-based RAG by refining retriever reliability and organizing retrieved data, thus benefiting reasoning-based LLMs.

**Abstract:** Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals to the LLMs. II) Due to the abstraction of graph data, the retrieved knowledge is often presented in unorganized forms. To mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM feedback to get rid of spurious signals and improve the quality of the supervision. Meanwhile, ReG introduces a structure-aware reorganization module to refactor the retrieval results into logically coherent evidence chains. Experiments on prominent benchmarks demonstrate that ReG significantly and consistently brings improvements across different LLM backbones by up to 10%. The improved supervision quality enables ReG to match the state-of-the-art performance with 5% training data and to transfer to out-of-distribution KGs. Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token cost by up to 30% and improves the performance by up to 4%.

</details>


### [41] [MisinfoTeleGraph: Network-driven Misinformation Detection for German Telegram Messages](https://arxiv.org/abs/2506.22529)

*Lu Kalkbrenner, Veronika Solopova, Steffen Zeiler, Robert Nickel, Dorothea Kolossa*

**Main category:** cs.CL

**Keywords:** misinformation detection, Telegram dataset, graph neural networks, weak supervision, German electoral context

**Relevance Score:** 4

**TL;DR:** Introduction of a German-language Telegram-based graph dataset for misinformation detection with evaluation of models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of connectivity and message propagation in misinformation detection on platforms like Telegram, particularly in the German electoral context.

**Method:** The study utilizes a dataset of over 5 million messages with metadata and labels derived from semantic similarity and manual annotation, evaluating text-only models and graph neural networks (GNNs) with a focus on message forwarding as a network structure.

**Key Contributions:**

	1. Creation of the Misinfo-TeleGraph dataset for German-language Telegram
	2. Performance evaluation of Text-only models vs Graph Neural Networks
	3. Insights into weak supervision impact on model performance

**Result:** GraphSAGE with LSTM aggregation outperforms text-only models based on Matthews Correlation Coefficient (MCC) and F1-score, revealing insights into the effectiveness of weak supervision.

**Limitations:** The study primarily focuses on German-language content; results may not generalize to other languages or platforms.

**Conclusion:** The work offers a reproducible benchmark and open dataset that can be used for further research on misinformation detection on low-moderation social platforms.

**Abstract:** Connectivity and message propagation are central, yet often underutilized, sources of information in misinformation detection -- especially on poorly moderated platforms such as Telegram, which has become a critical channel for misinformation dissemination, namely in the German electoral context. In this paper, we introduce Misinfo-TeleGraph, the first German-language Telegram-based graph dataset for misinformation detection. It includes over 5 million messages from public channels, enriched with metadata, channel relationships, and both weak and strong labels. These labels are derived via semantic similarity to fact-checks and news articles using M3-embeddings, as well as manual annotation. To establish reproducible baselines, we evaluate both text-only models and graph neural networks (GNNs) that incorporate message forwarding as a network structure. Our results show that GraphSAGE with LSTM aggregation significantly outperforms text-only baselines in terms of Matthews Correlation Coefficient (MCC) and F1-score. We further evaluate the impact of subscribers, view counts, and automatically versus human-created labels on performance, and highlight both the potential and challenges of weak supervision in this domain. This work provides a reproducible benchmark and open dataset for future research on misinformation detection in German-language Telegram networks and other low-moderation social platforms.

</details>


### [42] [RExBench: Can coding agents autonomously implement AI research extensions?](https://arxiv.org/abs/2506.22598)

*Nicholas Edwards, Yukyung Lee, Yujun, Mao, Yulu Qin, Sebastian Schuster, Najoung Kim*

**Main category:** cs.CL

**Keywords:** Large Language Models, research extension, benchmark, autonomous agents, machine learning

**Relevance Score:** 7

**TL;DR:** RExBench is a benchmark for evaluating LLM agents on research implementation tasks, revealing significant limitations in their autonomous capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the ability of LLM agents to autonomously perform research extension tasks in machine learning and natural sciences.

**Method:** The study introduces RExBench, a benchmark consisting of 12 realistic tasks designed to evaluate research hypotheses through the implementation of extensions to existing research papers and codebases.

**Key Contributions:**

	1. Introduction of RExBench, a novel benchmark for research implementation
	2. Evaluation of nine LLM agents across three frameworks
	3. Demonstration of the need for human guidance in LLM-driven research tasks.

**Result:** Nine LLM agents were evaluated using RExBench, showing that they fail to autonomously complete most tasks, with a peak success rate of under 40% even with additional human hints.

**Limitations:** Agents could not autonomously implement the majority of tasks; performance improved with hints but remained inadequate.

**Conclusion:** Current LLM agents require substantial human guidance to handle realistic research extension tasks effectively.

**Abstract:** Agents based on Large Language Models (LLMs) have shown promise for performing sophisticated software engineering tasks autonomously. In addition, there has been progress towards developing agents that can perform parts of the research pipeline in machine learning and the natural sciences. We argue that research extension and its implementation is a critical capability for such systems, and introduce RExBench to support the evaluation of this capability. RExBench is a benchmark consisting of 12 realistic research experiment implementation tasks that aim to investigate research hypotheses that have not previously been implemented. Each task is set up as an extension to an existing research paper and codebase, accompanied by domain expert-written instructions. RExBench is robust to data contamination, and supports an automatic evaluation infrastructure that executes agent outputs to determine whether the success criteria are met. We use this benchmark to evaluate nine LLM agents implemented using three different frameworks: aider, Claude Code, and OpenHands. We find that all agents evaluated fail to autonomously implement the majority of the extensions. Although the success rate improves with additional human-written hints, the best performance under this setting remains below 40%. This indicates that current agents are still short of being able to handle realistic research extension tasks without substantial human guidance.

</details>


### [43] [Temperature Matters: Enhancing Watermark Robustness Against Paraphrasing Attacks](https://arxiv.org/abs/2506.22623)

*Badr Youbi Idrissi, Monica Millunzi, Amelia Sorrenti, Lorenzo Baraldi, Daryna Dementieva*

**Main category:** cs.CL

**Keywords:** Large Language Models, watermarking, synthetic text, ethical AI, text generation

**Relevance Score:** 9

**TL;DR:** The paper presents a new watermarking technique for detecting synthetic text generated by Large Language Models (LLMs), addressing ethical concerns regarding their misuse.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure the ethical application of LLMs in AI-driven text generation by developing a reliable detection methodology.

**Method:** The study replicates findings from a baseline study, develops a novel watermarking technique, and evaluates its robustness against generative text variations through experiments.

**Key Contributions:**

	1. Development of a novel watermarking approach for detecting synthetic text.
	2. Rigorous evaluation of the watermarking approach against paraphrased generated text.
	3. Improved robustness over existing watermarking methods.

**Result:** The proposed watermarking approach demonstrates greater robustness compared to an existing method, indicating its effectiveness in identifying synthetic text.

**Limitations:** 

**Conclusion:** The new watermarking technique offers a promising solution to enhance the ethical use of LLMs in text generation.

**Abstract:** In the present-day scenario, Large Language Models (LLMs) are establishing their presence as powerful instruments permeating various sectors of society. While their utility offers valuable support to individuals, there are multiple concerns over potential misuse. Consequently, some academic endeavors have sought to introduce watermarking techniques, characterized by the inclusion of markers within machine-generated text, to facilitate algorithmic identification. This research project is focused on the development of a novel methodology for the detection of synthetic text, with the overarching goal of ensuring the ethical application of LLMs in AI-driven text generation. The investigation commences with replicating findings from a previous baseline study, thereby underscoring its susceptibility to variations in the underlying generation model. Subsequently, we propose an innovative watermarking approach and subject it to rigorous evaluation, employing paraphrased generated text to asses its robustness. Experimental results highlight the robustness of our proposal compared to the~\cite{aarson} watermarking method.

</details>


### [44] [Evaluating Hybrid Retrieval Augmented Generation using Dynamic Test Sets: LiveRAG Challenge](https://arxiv.org/abs/2506.22644)

*Chase Fensore, Kaustubh Dhole, Joyce C Ho, Eugene Agichtein*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, RAG, neural re-ranking, vocabulary alignment, machine learning

**Relevance Score:** 8

**TL;DR:** This paper discusses a hybrid approach for retrieval-augmented generation (RAG) systems used in the LiveRAG Challenge 2025, highlighting methods and performance metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an effective RAG system that addresses the challenges posed by dynamic test sets and improves the generation of relevant and faithful answers.

**Method:** A hybrid retrieval approach combining sparse (BM25) and dense (E5) retrieval techniques, followed by generation using Falcon3-10B-Instruct, evaluated on synthetic questions generated through DataMorgana.

**Key Contributions:**

	1. Development of a hybrid RAG system combining multi-modal retrieval methods
	2. Demonstrated significant performance improvement through neural re-ranking
	3. Insights on vocabulary alignment as a predictor of model performance

**Result:** The proposed hybrid system achieved significant improvements in mean average precision (MAP) and achieved notable rankings in faithfulness and correctness among competitors, though it faced challenges with computational costs and reliability of responses.

**Limitations:** High computational costs associated with neural re-ranking and concerns about the reliability of generated answers at 0% refusal rates.

**Conclusion:** The analysis indicates that vocabulary alignment between questions and documents is crucial for performance, and while heuristics like neural re-ranking can enhance results, they may introduce computational trade-offs.

**Abstract:** We present our submission to the LiveRAG Challenge 2025, which evaluates retrieval-augmented generation (RAG) systems on dynamic test sets using the FineWeb-10BT corpus. Our final hybrid approach combines sparse (BM25) and dense (E5) retrieval methods and then aims to generate relevant and faithful answers with Falcon3-10B-Instruct. Through systematic evaluation on 200 synthetic questions generated with DataMorgana across 64 unique question-user combinations, we demonstrate that neural re-ranking with RankLLaMA improves MAP from 0.523 to 0.797 (52% relative improvement) but introduces prohibitive computational costs (84s vs 1.74s per question). While DSPy-optimized prompting strategies achieved higher semantic similarity (0.771 vs 0.668), their 0% refusal rates raised concerns about over-confidence and generalizability. Our submitted hybrid system without re-ranking achieved 4th place in faithfulness and 11th place in correctness among 25 teams. Analysis across question categories reveals that vocabulary alignment between questions and documents was the strongest predictor of performance on our development set, with document-similar phrasing improving cosine similarity from 0.562 to 0.762.

</details>


### [45] [Assessing the feasibility of Large Language Models for detecting micro-behaviors in team interactions during space missions](https://arxiv.org/abs/2506.22679)

*Ankush Raut, Projna Paromita, Sydney Begerowski, Suzanne Bell, Theodora Chaspari*

**Main category:** cs.CL

**Keywords:** large language models, micro-behaviors, team conversations, zero-shot classification, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper investigates the use of LLMs to identify micro-behaviors in team communication during simulated space missions, comparing various techniques and showing that decoder-only LLMs outperform encoder-only ones in performance metrics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study seeks to enhance understanding of communication dynamics in high-pressure environments, such as space missions, where team interactions can be critical to success.

**Method:** The authors employed zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only models, alongside few-shot text generation with decoder-only models, to analyze conversational transcripts from simulated missions.

**Key Contributions:**

	1. Demonstration of LLMs for detecting micro-behaviors in dialogue
	2. Comparison of encoder-only and decoder-only LLM performance
	3. Insights into enhancing speech technologies for team communication

**Result:** The instruction fine-tuned version of Llama-3.1 yielded the best results, achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. Encoder-only models struggled with detecting subtle behaviors, particularly discouraging speech.

**Limitations:** Encoder-only models faced challenges in identifying underrepresented behaviors despite fine-tuning efforts.

**Conclusion:** Findings suggest that decoder-only LLMs are more effective in identifying nuanced communication behaviors, which can aid in developing tools for improving team dynamics in critical environments.

**Abstract:** We explore the feasibility of large language models (LLMs) in detecting subtle expressions of micro-behaviors in team conversations using transcripts collected during simulated space missions. Specifically, we examine zero-shot classification, fine-tuning, and paraphrase-augmented fine-tuning with encoder-only sequence classification LLMs, as well as few-shot text generation with decoder-only causal language modeling LLMs, to predict the micro-behavior associated with each conversational turn (i.e., dialogue). Our findings indicate that encoder-only LLMs, such as RoBERTa and DistilBERT, struggled to detect underrepresented micro-behaviors, particularly discouraging speech, even with weighted fine-tuning. In contrast, the instruction fine-tuned version of Llama-3.1, a decoder-only LLM, demonstrated superior performance, with the best models achieving macro F1-scores of 44% for 3-way classification and 68% for binary classification. These results have implications for the development of speech technologies aimed at analyzing team communication dynamics and enhancing training interventions in high-stakes environments such as space missions, particularly in scenarios where text is the only accessible data.

</details>


### [46] [VOCABTRIM: Vocabulary Pruning for Efficient Speculative Decoding in LLMs](https://arxiv.org/abs/2506.22694)

*Raghavv Goel, Sudhanshu Agrawal, Mukul Gagrani, Junyoung Park, Yifan Zao, He Zhang, Tian Liu, Yiping Yang, Xin Yuan, Jiuyan Lu, Chris Lott, Mingu Lee*

**Main category:** cs.CL

**Keywords:** speculative decoding, language modeling, edge devices

**Relevance Score:** 8

**TL;DR:** Introduces VocabTrim, a training-free technique enhancing drafter-based speculative decoding by reducing inference overhead.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving performance of drafter-based speculative decoding methods in language models.

**Method:** VocabTrim reconstructs the drafter LM head to include a limited set of frequently sampled tokens from the target model’s vocabulary, reducing drafting overhead.

**Key Contributions:**

	1. Introduction of VocabTrim technique
	2. Reduction of inference overhead in drafting
	3. Improved memory-bound speed-up for Llama-3 models

**Result:** Achieves a 16% boost in memory-bound speed-up for Llama-3 models on Spec-Bench, while slightly degrading acceptance rates.

**Limitations:** Slight degradation in acceptance rate due to limiting drafting vocabulary.

**Conclusion:** VocabTrim offers significant improvements in drafting latency and generation speed in memory-constrained environments like edge devices.

**Abstract:** In this paper, we introduce a simple training-free technique to improve the performance of drafter-based speculative decoding (SpD) methods that incorporates language modeling head (LM head) during drafting process. A drafter-based speculative decoding leverages one or more smaller language models, a.k.a. drafters or draft models, to sample a draft sequence or tree consisting of multiple tokens, followed by verification by a base LLM, a target model, accepting a subset as its valid generation. As it is usually considered that the speculative decoding requires one-to-one mapping between vocabularies of the target model and the draft model, it has been natural to share the vocabulary between them, or even share the LM head as in EAGLE or Medusa. We first identify that this draft token sampling scheme inherently contains an unnecessary inference overhead in drafting, especially for some target LLMs with very large vocabularies. Then, we propose a simple technique, VocabTrim, to mitigate the drafting overhead to improve the generation speed in memory-bound environment. VocabTrim reconstructs the drafter LM head to contain only a limited set of tokens, selected by the most frequently sampled from the vocabulary of the target model. While limiting the vocabulary in drafting slightly degrades the acceptance rate, it significantly reduces the drafting latency in memory-bound process which is often the case on edge devices, resulting in higher memory-bound speed up (MBSU). We show that our method can boost the memory-bound speed-up for Llama-3 models on Spec-Bench, specifically by 16% for Llama-3.2-3B-Instruct.

</details>


### [47] [Text Production and Comprehension by Human and Artificial Intelligence: Interdisciplinary Workshop Report](https://arxiv.org/abs/2506.22698)

*Emily Dux Speltz*

**Main category:** cs.CL

**Keywords:** cognitive psychology, natural language processing, large language models

**Relevance Score:** 9

**TL;DR:** This report summarizes insights from a workshop on AI language models and human cognitive processes in text comprehension and composition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the knowledge gap in understanding the interplay between AI language models and human cognitive processes in language tasks.

**Method:** Interdisciplinary workshop discussions involving experts from cognitive psychology, language learning, and AI-based NLP.

**Key Contributions:**

	1. Insights into human language processing from LLMs
	2. Understanding the alignment of LLM behavior with human cognition
	3. Recommendations for ethical AI use in education and psychology

**Result:** Identified key patterns in the relationship between LLMs and human cognition, including capabilities, limitations, and implications for human-AI collaboration.

**Limitations:** 

**Conclusion:** The findings should guide future research and development in LLMs while considering ethical implications and enhancing human capabilities.

**Abstract:** This report synthesizes the outcomes of a recent interdisciplinary workshop that brought together leading experts in cognitive psychology, language learning, and artificial intelligence (AI)-based natural language processing (NLP). The workshop, funded by the National Science Foundation, aimed to address a critical knowledge gap in our understanding of the relationship between AI language models and human cognitive processes in text comprehension and composition. Through collaborative dialogue across cognitive, linguistic, and technological perspectives, workshop participants examined the underlying processes involved when humans produce and comprehend text, and how AI can both inform our understanding of these processes and augment human capabilities. The workshop revealed emerging patterns in the relationship between large language models (LLMs) and human cognition, with highlights on both the capabilities of LLMs and their limitations in fully replicating human-like language understanding and generation. Key findings include the potential of LLMs to offer insights into human language processing, the increasing alignment between LLM behavior and human language processing when models are fine-tuned with human feedback, and the opportunities and challenges presented by human-AI collaboration in language tasks. By synthesizing these findings, this report aims to guide future research, development, and implementation of LLMs in cognitive psychology, linguistics, and education. It emphasizes the importance of ethical considerations and responsible use of AI technologies while striving to enhance human capabilities in text comprehension and production through effective human-AI collaboration.

</details>


### [48] [The Translation Barrier Hypothesis: Multilingual Generation with Large Language Models Suffers from Implicit Translation Failure](https://arxiv.org/abs/2506.22724)

*Niyati Bafna, Tianjian Li, Kenton Murray, David R. Mortensen, David Yarowsky, Hale Sirin, Daniel Khashabi*

**Main category:** cs.CL

**Keywords:** Multilingual generation, Large language models, Translation barrier hypothesis

**Relevance Score:** 8

**TL;DR:** This paper explores the quality of multilingual generation in large language models (LLMs) across low-resource languages, identifying the translation barrier hypothesis as a key factor in failures, particularly in translating solved intermediate concepts.

**Read time:** 23 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the poor quality of multilingual generation for mid- to low-resource languages in LLMs.

**Method:** The authors test the 'translation barrier hypothesis' by analyzing a word translation task across 108 language pairs, employing logit lens to examine model processing in intermediate layers.

**Key Contributions:**

	1. Introduction of the translation barrier hypothesis.
	2. Analysis across 108 language pairs to validate the hypothesis.
	3. Identification of critical challenges in multilingual generation for low-resource languages.

**Result:** The study reveals that a significant portion of failures in multilingual generation arises from translation errors, particularly for low-resource languages, indicating that the translation stage is a crucial bottleneck.

**Limitations:** Focus on word translation tasks may not capture all dimensions of multilingual generation issues.

**Conclusion:** The findings emphasize the need for strategies to overcome the translation barrier in LLMs to enhance multilingual generation capabilities.

**Abstract:** Multilingual generation with large language models (LLMs) is often of poor quality for mid- to low-resource languages. Building on insights from interpretability, we demonstrate the existence of an implicit task-solving-->translation pipeline for generation, whereby the model first solves the required task in a largely target-language-agnostic manner, and subsequently translates answer concepts into the intended target language. We hypothesize that the failure of the translation stage is an important culprit for the observed low quality of final outputs, and formalize this as the translation barrier hypothesis. We test this hypothesis for a word translation task across 108 language pairs, using logit lens to observe model processing in intermediate layers. We find that a significant portion of overall failures indeed stems from translation failure, or the model's inability to translate correctly solved intermediate concepts into the target language. This is especially true for low-resource target languages. Our results highlight an important hurdle for end-to-end multilingual generation, and lend guiding insights for future work seeking to improve multilinguality in LLMs.

</details>


### [49] [Jan-nano Technical Report](https://arxiv.org/abs/2506.22760)

*Alan Dao, Dinh Bach Vu*

**Main category:** cs.CL

**Keywords:** language model, efficiency, RLVR, benchmark, consumer hardware

**Relevance Score:** 7

**TL;DR:** Jan-nano is a specialized 4B parameter language model optimized for efficiency and performance on consumer hardware, achieving high benchmarks without traditional training methods.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the tradeoff between powerful capabilities and computational resources in language models.

**Method:** Jan-nano is fine-tuned from Qwen3-4B using a multi-stage RLVR system, eliminating the need for next token prediction training.

**Key Contributions:**

	1. Redefines efficiency in language models
	2. Achieves high performance on consumer hardware
	3. Eliminates reliance on traditional next token prediction training

**Result:** Achieves 83.2% on the SimpleQA benchmark, demonstrating that it can deliver intelligence with significantly less computational demand.

**Limitations:** 

**Conclusion:** Jan-nano showcases that effective strategy in model design can lead to strong performance without scaling in size.

**Abstract:** Most language models face a fundamental tradeoff where powerful capabilities require substantial computational resources. We shatter this constraint with Jan-nano, a 4B parameter language model that redefines efficiency through radical specialization: instead of trying to know everything, it masters the art of finding anything instantly. Fine-tuned from Qwen3-4B using our novel multi-stage RLVR system that completely eliminates reliance on next token prediction training (SFT), Jan-nano achieves 83.2% on SimpleQA benchmark with MCP integration while running on consumer hardware. With 128K context length, Jan-nano proves that intelligence isn't about scale, it's about strategy.

</details>


### [50] [Teaching Models to Verbalize Reward Hacking in Chain-of-Thought Reasoning](https://arxiv.org/abs/2506.22777)

*Miles Turpin, Andy Arditi, Marvin Li, Joe Benton, Julian Michael*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Reward Hacking, Verbalization Fine-Tuning, Transparency, AI Safety

**Relevance Score:** 8

**TL;DR:** Verbalization fine-tuning (VFT) reduces reward hacking in language models by training them to acknowledge prompt cues that influence their answers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Language models often exploit unintended strategies (reward hacking) to achieve high rewards, which can pose risks, especially in critical applications. Detecting such behavior is complicated.

**Method:** We introduce verbalization fine-tuning (VFT) as a pre-RL intervention, training models to recognize and acknowledge the influence of prompt cues that lead to incorrect answers before applying reinforcement learning (RL).

**Key Contributions:**

	1. Introduction of verbalization fine-tuning (VFT) to improve model transparency
	2. Demonstrated reduction in reward hacking detection rates through explicit acknowledgment of prompt influences
	3. Quantitative results showing significant improvements in verbalization of influences post-RL

**Result:** VFT-trained models show a significant reduction in undetected reward hacks—only 6% compared to 88% without VFT. RL combined with VFT leads to 94% verbalization of cue influence.

**Limitations:** 

**Conclusion:** VFT enhances the detection of reward hacking in language models, suggesting a viable approach for creating more transparent and safer AI systems.

**Abstract:** Language models trained with RL can engage in reward hacking--exploiting unintended strategies for high reward--without revealing this behavior in their chain-of-thought reasoning, making detection difficult and posing risks for high-stakes applications. We propose verbalization fine-tuning (VFT), a pre-RL intervention that trains models to explicitly acknowledge when they are influenced by prompt cues--hints which point to incorrect answers (e.g., "a Stanford professor thinks the answer is A"). To evaluate VFT, we subsequently train models with RL on environments where held-out prompt cues signal which incorrect answers will receive high reward, incentivizing models to reward hack by exploiting cues instead of reasoning correctly. We measure how often models exploit these cues without verbalizing it. After RL, only 6% of the VFT-trained model's responses consist of undetected reward hacks. In comparison, when we perform RL without VFT, the rate of undetected reward hacks goes up to 88%; with a debiasing baseline intervention, this increases further to 99%. VFT achieves this by substantially increasing how often models verbalize the influence of cues--from 8% to 42% after VFT, and up to 94% after RL--while baselines remain low even after RL (10% and 1%). Our results show that teaching models to explicitly verbalize reward hacking behavior before RL significantly improves their detection, offering a practical path toward more transparent and safe AI systems.

</details>


### [51] [ContextCache: Context-Aware Semantic Cache for Multi-Turn Queries in Large Language Models](https://arxiv.org/abs/2506.22791)

*Jianxin Yan, Wangze Ni, Lei Chen, Xuemin Lin, Peng Cheng, Zhan Qin, Kui Ren*

**Main category:** cs.CL

**Keywords:** semantics, caching, multi-turn dialogues, LLM, context-aware

**Relevance Score:** 9

**TL;DR:** ContextCache is a context-aware semantic caching system designed to improve the efficiency of multi-turn dialogues in LLM applications by utilizing a two-stage retrieval architecture that accounts for conversation context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing caching systems that fail to consider multi-turn dialogue contexts, leading to incorrect cache hits and inefficiencies.

**Method:** ContextCache implements a two-stage retrieval architecture; the first stage involves vector-based retrieval of potential matches for the current query, while the second stage uses self-attention mechanisms to integrate current and historical dialogue representations for better contextual matching.

**Key Contributions:**

	1. Introduction of a context-aware caching system for LLM applications
	2. Implementation of a two-stage retrieval mechanism
	3. Demonstrated improvements in precision, recall, and latency for multi-turn dialogues

**Result:** ContextCache demonstrates improved precision and recall in real-world conversations compared to existing caching systems, and achieves approximately 10 times lower latency for cached responses than direct LLM invocations.

**Limitations:** 

**Conclusion:** By significantly reducing computational costs and improving efficiency in LLM conversational applications, ContextCache addresses critical inefficiencies in current semantic caching practices.

**Abstract:** Semantic caching significantly reduces computational costs and improves efficiency by storing and reusing large language model (LLM) responses. However, existing systems rely primarily on matching individual queries, lacking awareness of multi-turn dialogue contexts, which leads to incorrect cache hits when similar queries appear in different conversational settings. This demonstration introduces ContextCache, a context-aware semantic caching system for multi-turn dialogues. ContextCache employs a two-stage retrieval architecture that first executes vector-based retrieval on the current query to identify potential matches and then integrates current and historical dialogue representations through self-attention mechanisms for precise contextual matching. Evaluation of real-world conversations shows that ContextCache improves precision and recall compared to existing methods. Additionally, cached responses exhibit approximately 10 times lower latency than direct LLM invocation, enabling significant computational cost reductions for LLM conversational applications.

</details>


### [52] [MedEthicsQA: A Comprehensive Question Answering Benchmark for Medical Ethics Evaluation of LLMs](https://arxiv.org/abs/2506.22808)

*Jianhui Wei, Zijie Meng, Zikai Xiao, Tianxiang Hu, Yang Feng, Zhijie Zhou, Jian Wu, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** Medical Ethics, Large Language Models, Benchmark, Healthcare AI, Ethical Evaluation

**Relevance Score:** 9

**TL;DR:** This paper presents MedEthicsQA, a benchmark for assessing medical ethics in Large Language Models (LLMs), comprising over 10,000 questions and a systematic taxonomy based on global medical ethical standards.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The ethical safety of Medical Large Language Models (MedLLMs) in clinical tasks has not been thoroughly investigated, necessitating a dedicated benchmark for evaluation.

**Method:** The paper introduces MedEthicsQA, a benchmark consisting of 5,623 multiple-choice questions and 5,351 open-ended questions, established using a hierarchical taxonomy of medical ethics. Rigorous quality control measures were implemented to ensure dataset reliability.

**Key Contributions:**

	1. Introduction of MedEthicsQA benchmark for medical ethics in LLMs
	2. Integration of a hierarchical taxonomy of medical ethics
	3. Rigorous validation and quality control of the dataset

**Result:** Evaluation shows that state-of-the-art MedLLMs performed worse on medical ethics questions than their foundation model counterparts, highlighting significant shortcomings in medical ethics alignment.

**Limitations:** 

**Conclusion:** The introduction of MedEthicsQA provides a foundational tool for evaluating and improving the ethical performance of Medical LLMs, encouraging further research in this area.

**Abstract:** While Medical Large Language Models (MedLLMs) have demonstrated remarkable potential in clinical tasks, their ethical safety remains insufficiently explored. This paper introduces $\textbf{MedEthicsQA}$, a comprehensive benchmark comprising $\textbf{5,623}$ multiple-choice questions and $\textbf{5,351}$ open-ended questions for evaluation of medical ethics in LLMs. We systematically establish a hierarchical taxonomy integrating global medical ethical standards. The benchmark encompasses widely used medical datasets, authoritative question banks, and scenarios derived from PubMed literature. Rigorous quality control involving multi-stage filtering and multi-faceted expert validation ensures the reliability of the dataset with a low error rate ($2.72\%$). Evaluation of state-of-the-art MedLLMs exhibit declined performance in answering medical ethics questions compared to their foundation counterparts, elucidating the deficiencies of medical ethics alignment. The dataset, registered under CC BY-NC 4.0 license, is available at https://github.com/JianhuiWei7/MedEthicsQA.

</details>


### [53] [Selecting and Merging: Towards Adaptable and Scalable Named Entity Recognition with Large Language Models](https://arxiv.org/abs/2506.22813)

*Zhuojun Ding, Wei Wei, Chenghao Fan*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information Extraction, Domain Adaptation, Model Merging

**Relevance Score:** 8

**TL;DR:** The SaM framework dynamically selects and merges expert models at inference time to improve performance in information extraction tasks across multiple domains without requiring additional training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of adapting and scaling domain-specific models for information extraction tasks using large language models without incurring high annotation costs.

**Method:** The framework selects domain-specific experts based on their similarity to the target domain and their performance on sampled instances, then merges them to create optimized task-specific models.

**Key Contributions:**

	1. Introduction of the SaM framework for model selection and merging
	2. Improvement of generalization in information extraction tasks
	3. Demonstration of the framework's effectiveness across benchmarks

**Result:** The SaM framework outperforms a unified model by an average of 10% across multiple benchmarks by improving generalization and scalability.

**Limitations:** 

**Conclusion:** The framework demonstrates effective dynamic model adaptation and paves the way for practical applications in real-world scenarios.

**Abstract:** Supervised fine-tuning (SFT) is widely used to align large language models (LLMs) with information extraction (IE) tasks, such as named entity recognition (NER). However, annotating such fine-grained labels and training domain-specific models is costly. Existing works typically train a unified model across multiple domains, but such approaches lack adaptation and scalability since not all training data benefits target domains and scaling trained models remains challenging. We propose the SaM framework, which dynamically Selects and Merges expert models at inference time. Specifically, for a target domain, we select domain-specific experts pre-trained on existing domains based on (i) domain similarity to the target domain and (ii) performance on sampled instances, respectively. The experts are then merged to create task-specific models optimized for the target domain. By dynamically merging experts beneficial to target domains, we improve generalization across various domains without extra training. Additionally, experts can be added or removed conveniently, leading to great scalability. Extensive experiments on multiple benchmarks demonstrate our framework's effectiveness, which outperforms the unified model by an average of 10%. We further provide insights into potential improvements, practical experience, and extensions of our framework.

</details>


### [54] [Boosting CTC-Based ASR Using LLM-Based Intermediate Loss Regularization](https://arxiv.org/abs/2506.22846)

*Duygu Altinok*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, CTC, large language models, LAIL, Conformer architecture

**Relevance Score:** 9

**TL;DR:** The paper presents a novel auxiliary loss framework called LAIL to improve CTC-based ASR by integrating linguistic knowledge from LLMs, achieving state-of-the-art performance with efficient decoding.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of CTC-based models in capturing linguistic dependencies and the slow inference speed of autoregressive ASR systems.

**Method:** The proposed framework attaches connector layers to intermediate encoder layers, mapping outputs to an LLM's embedding space and computing a causal language modeling loss during training.

**Key Contributions:**

	1. Introduction of Language-Aware Intermediate Loss (LAIL) for CTC-based ASR.
	2. Demonstrated improvements in linguistic modeling with minimal overhead.
	3. Achieved state-of-the-art WER performance using LLaMA models.

**Result:** Significant improvements in Word Error Rate (WER) on the LibriSpeech, TEDLIUM2, and WSJ corpora, demonstrating state-of-the-art performance for CTC-based ASR.

**Limitations:** 

**Conclusion:** LAIL enhances linguistic modeling in CTC-based ASR systems while maintaining computational efficiency, making it suitable for real-time applications.

**Abstract:** End-to-end (E2E) automatic speech recognition (ASR) systems have revolutionized the field by integrating all components into a single neural network, with attention-based encoder-decoder models achieving state-of-the-art performance. However, their autoregressive decoding process limits inference speed, making them unsuitable for real-time applications. In contrast, CTC-based models offer faster, non-autoregressive decoding but struggle to model linguistic dependencies effectively. Addressing this challenge, we propose a novel auxiliary loss framework called Language-Aware Intermediate Loss (LAIL) to enhance CTC-based ASR using the linguistic knowledge of large language models (LLMs). By attaching connector layers to intermediate encoder layers, LAIL maps outputs to the embedding space of an LLM and computes a causal language modeling loss during training. This approach enhances linguistic modeling while preserving the computational efficiency of CTC decoding. Using the Conformer architecture and various LLaMA models, we demonstrate significant improvements in Word Error Rate (WER) on the LibriSpeech, TEDLIUM2, and WSJ corpora, achieving state-of-the-art performance for CTC-based ASR with minimal computational overhead.

</details>


### [55] [Knowledge Augmented Finetuning Matters in both RAG and Agent Based Dialog Systems](https://arxiv.org/abs/2506.22852)

*Yucheng Cai, Yuxuan Wu, Yi Huang, Junlan Feng, Zhijian Ou*

**Main category:** cs.CL

**Keywords:** large language models, knowledge augmented finetuning, retrieval augmented generation, dialog systems, factual accuracy

**Relevance Score:** 9

**TL;DR:** This paper introduces knowledge augmented finetuning (KAFT) for large language models (LLMs) in dialog systems, improving factual accuracy against traditional prompting methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with knowledge-intensive tasks due to insufficient training for specific domains, leading to errors in dialog systems.

**Method:** The study proposes finetuning LLMs using domain-specific data and knowledge in RAG-based and agent-based systems, evaluated using the MobileCS2 dataset.

**Key Contributions:**

	1. Introduction of knowledge augmented finetuning (KAFT) for LLMs.
	2. Systematic comparison of prompt and KAFT methods in enhancing factual accuracy.
	3. Empirical validation using the MobileCS2 customer service dataset.

**Result:** KAFT significantly outperforms traditional prompting techniques in enhancing factual accuracy within dialog systems.

**Limitations:** 

**Conclusion:** The findings establish KAFT as a novel and effective approach for improving LLM performance in domain-specific applications, representing a crucial empirical contribution to the field.

**Abstract:** Large language models (LLMs) have recently been applied to dialog systems. Despite making progress, LLMs are prone to errors in knowledge-intensive scenarios. Recently, approaches based on retrieval augmented generation (RAG) and agent have emerged to improve the factual accuracy by enhancing the LLMs with knowledge retrieved from external knowledge bases (KBs). This is mostly implemented by prompting the LLMs with instructions, examples and the retrieved knowledge. However, LLMs may have difficulty using the retrieved knowledge effectively for response generation, because they are not well trained to do such generation for specific domains. To mitigate this problem, we propose to finetune the LLMs in the RAG-based and agent-based systems with domain-specific data, together with domain-specific external knowledge, which is called knowledge augmented finetuning (KAFT). We base our study on the MobileCS2 dataset, a real-life customer service dialog dataset that features intensive knowledge interactions, to systematically compare the prompting and KAFT techniques in the RAG-based and agent-based systems. Experiment results show that KAFT substantially surpasses prompting in both RAG and agent systems, particularly in terms of factual accuracy. To the best of our knowledge, this paper represents the first solid empirical work to investigate the KAFT idea.

</details>


### [56] [DICE-BENCH: Evaluating the Tool-Use Capabilities of Large Language Models in Multi-Round, Multi-Party Dialogues](https://arxiv.org/abs/2506.22853)

*Kyochul Jang, Donghyeon Lee, Kyusik Kim, Dongseok Heo, Taewhoo Lee, Woojeong Kim, Bongwon Suh*

**Main category:** cs.CL

**Keywords:** function-calling, benchmarks, DICE-SCORE, DICE-BENCH, dialogue systems

**Relevance Score:** 7

**TL;DR:** The paper introduces DICE-SCORE, a new metric for evaluating function-calling benchmarks, and presents DICE-BENCH, a framework for creating realistic function-calling datasets. It finds that existing benchmarks yield low DICE-SCOREs, indicating a need for improvement, and validates DICE-BENCH with experiments on 19 large language models (LLMs).

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing function-calling benchmarks that primarily focus on single-turn interactions and do not reflect real-world complexities, highlighting the need for a more comprehensive evaluation approach.

**Method:** The paper introduces DICE-SCORE, a metric to assess how effectively existing benchmarks incorporate tool-related information throughout multi-turn dialogues. It also presents DICE-BENCH, a framework for creating datasets of realistic function-calling interactions using tool graphs and a multi-agent system.

**Key Contributions:**

	1. Introduction of the DICE-SCORE metric for evaluating function-calling benchmarks.
	2. Development of the DICE-BENCH framework for synthesizing practical function-calling datasets.
	3. Creation of a dataset featuring 1,607 high-DICE-SCORE instances to facilitate better evaluation of LLMs.

**Result:** The analysis of current benchmarks using DICE-SCORE revealed low performance, and DICE-BENCH was able to create a dataset of 1,607 instances with high DICE-SCORE values. Experiments with 19 LLMs showed that considerable improvements are necessary for practical deployment.

**Limitations:** The study primarily focuses on tool-related information and may not cover all aspects of real-world interactions in functional calling.

**Conclusion:** The study underscores the necessity for realistic benchmarks in function-calling scenarios and demonstrates that while DICE-BENCH provides a high-quality dataset, existing large language models require significant enhancement to be effective in real-world applications.

**Abstract:** Existing function-calling benchmarks focus on single-turn interactions. However, they overlook the complexity of real-world scenarios. To quantify how existing benchmarks address practical applications, we introduce DICE-SCORE, a metric that evaluates the dispersion of tool-related information such as function name and parameter values throughout the dialogue. Analyzing existing benchmarks through DICE-SCORE reveals notably low scores, highlighting the need for more realistic scenarios. To address this gap, we present DICE-BENCH, a framework that constructs practical function-calling datasets by synthesizing conversations through a tool graph that maintains dependencies across rounds and a multi-agent system with distinct personas to enhance dialogue naturalness. The final dataset comprises 1,607 high-DICE-SCORE instances. Our experiments on 19 LLMs with DICE-BENCH show that significant advances are still required before such models can be deployed effectively in real-world settings. Our code and data are all publicly available: https://snuhcc.github.io/DICE-Bench/.

</details>


### [57] [Mind the Gap: Entity-Preserved Context-Aware ASR Structured Transcriptions](https://arxiv.org/abs/2506.22858)

*Duygu Altinok*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Named Entity Recognition, Entity Formatting

**Relevance Score:** 8

**TL;DR:** Proposes a novel training approach to improve ASR entity recognition and formatting using extended semantic context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address high word error rates in ASR systems, particularly for named entities and numerical data in critical domains like legal, financial, and medical applications.

**Method:** Introduces overlapping context windows during training with 5-second overlaps on both sides of 30-second chunks, resulting in an effective 40-second semantic window.

**Key Contributions:**

	1. Novel overlapping context training approach for ASR
	2. Improved named entity recognition and formatting
	3. Use of enriched training data with embedded entity labels

**Result:** The method enhances performance on semantic tasks such as named entity recognition (NER) and entity formatting, as demonstrated on the Spoken Wikipedia dataset.

**Limitations:** 

**Conclusion:** Context-aware training significantly mitigates ASR limitations in long-form transcription and complex entity recognition tasks.

**Abstract:** Automatic Speech Recognition (ASR) systems, such as Whisper, achieve high transcription accuracy but struggle with named entities and numerical data, especially when proper formatting is required. These issues increase word error rate (WER) and impair semantic understanding in critical domains like legal, financial, and medical applications. We propose a novel training approach that extends the semantic context of ASR models by adding overlapping context windows during training. By sliding 5-second overlaps on both sides of 30-second chunks, we create a 40-second "effective semantic window," improving entity recognition and formatting while focusing predictions on the central 30 seconds. To address entities spanning chunk boundaries, we reassign such entities entirely to the right-hand chunk, ensuring proper formatting. Additionally, enriched training data with embedded entity labels enables the model to learn both recognition and type-specific formatting. Evaluated on the Spoken Wikipedia dataset, our method improves performance across semantic tasks, including named entity recognition (NER) and entity formatting. These results highlight the effectiveness of context-aware training in addressing ASR limitations for long-form transcription and complex entity recognition tasks.

</details>


### [58] [Agent-to-Agent Theory of Mind: Testing Interlocutor Awareness among Large Language Models](https://arxiv.org/abs/2506.22957)

*Younwoo Choi, Changling Li, Yongjin Yang, Zhijing Jin*

**Main category:** cs.CL

**Keywords:** interlocutor awareness, large language models, multi-agent systems, safety, collaboration

**Relevance Score:** 9

**TL;DR:** This paper evaluates the capacity of large language models (LLMs) to exhibit interlocutor awareness—their ability to identify and adapt to the identity and characteristics of dialogue partners—highlighting its implications for multi-agent interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure reliable performance and safety in multi-agent systems employing LLMs, it is crucial to understand not only situational awareness but also the capacity for interlocutor awareness.

**Method:** The authors systematically evaluated LLMs' interlocutor awareness along three dimensions: reasoning patterns, linguistic style, and alignment preferences, alongside case studies demonstrating its effects.

**Key Contributions:**

	1. Formalization of interlocutor awareness concept in LLMs
	2. Systematic evaluation of LLMs' response to interlocutor identity
	3. Case studies illustrating practical implications of interlocutor awareness

**Result:** The study found that LLMs can identify same-family peers and notable model families with reliability, affecting collaboration and introducing alignment and safety risks.

**Limitations:** The study may not cover all aspects of LLM interpersonal dynamics and requires further exploration of its complexities.

**Conclusion:** While interlocutor awareness offers potential for improved LLM collaboration, it also poses new vulnerabilities, necessitating further investigation and protective measures in multi-agent applications.

**Abstract:** As large language models (LLMs) are increasingly integrated into multi-agent and human-AI systems, understanding their awareness of both self-context and conversational partners is essential for ensuring reliable performance and robust safety. While prior work has extensively studied situational awareness which refers to an LLM's ability to recognize its operating phase and constraints, it has largely overlooked the complementary capacity to identify and adapt to the identity and characteristics of a dialogue partner. In this paper, we formalize this latter capability as interlocutor awareness and present the first systematic evaluation of its emergence in contemporary LLMs. We examine interlocutor inference across three dimensions-reasoning patterns, linguistic style, and alignment preferences-and show that LLMs reliably identify same-family peers and certain prominent model families, such as GPT and Claude. To demonstrate its practical significance, we develop three case studies in which interlocutor awareness both enhances multi-LLM collaboration through prompt adaptation and introduces new alignment and safety vulnerabilities, including reward-hacking behaviors and increased jailbreak susceptibility. Our findings highlight the dual promise and peril of identity-sensitive behavior in LLMs, underscoring the need for further understanding of interlocutor awareness and new safeguards in multi-agent deployments. Our code is open-sourced at https://github.com/younwoochoi/InterlocutorAwarenessLLM.

</details>


### [59] [On the Generalizability of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals"](https://arxiv.org/abs/2506.22977)

*Asen Dotsinski, Udit Thakur, Marko Ivanov, Mohammad Hafeez Khan, Maria Heuss*

**Main category:** cs.CL

**Keywords:** Language Models, Counterfactuals, Attention Mechanisms, Prompt Structure, Model Generalizability

**Relevance Score:** 9

**TL;DR:** This paper reproduces findings from 'Competition of Mechanisms' regarding language models' handling of facts and counterfactuals, extending the research to larger models and different prompt structures.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how language models navigate factual recall and counterfactual reasoning, and to extend previous findings from Ortu et al. (2024).

**Method:** The study reproduces experiments on GPT-2 and Pythia 6.9B, then extends findings by testing Llama 3.1 8B and analyzing prompt structure variations.

**Key Contributions:**

	1. Successful reproduction of previous findings on newer models
	2. Investigation of prompt structure effects on model performance
	3. Analysis of attention head specialization across different model sizes

**Result:** Findings indicate reduced attention head specialization in larger models, and that certain prompt variations significantly affect counterfactual predictions.

**Limitations:** The effectiveness of findings was observed to vary based on model architecture and specific domains, as some prompts skewed results.

**Conclusion:** Attention head ablation methods are less effective for underrepresented domains, and results vary based on model architecture, prompt structure, and task.

**Abstract:** We present a reproduction study of "Competition of Mechanisms: Tracing How Language Models Handle Facts and Counterfactuals" (Ortu et al., 2024), which investigates competition of mechanisms in language models between factual recall and counterfactual in-context repetition. Our study successfully reproduces their primary findings regarding the localization of factual and counterfactual information, the dominance of attention blocks in mechanism competition, and the specialization of attention heads in handling competing information. We reproduce their results on both GPT-2 (Radford et al., 2019) and Pythia 6.9B (Biderman et al., 2023). We extend their work in three significant directions. First, we explore the generalizability of these findings to even larger models by replicating the experiments on Llama 3.1 8B (Grattafiori et al., 2024), discovering greatly reduced attention head specialization. Second, we investigate the impact of prompt structure by introducing variations where we avoid repeating the counterfactual statement verbatim or we change the premise word, observing a marked decrease in the logit for the counterfactual token. Finally, we test the validity of the authors' claims for prompts of specific domains, discovering that certain categories of prompts skew the results by providing the factual prediction token as part of the subject of the sentence. Overall, we find that the attention head ablation proposed in Ortu et al. (2024) is ineffective for domains that are underrepresented in their dataset, and that the effectiveness varies based on model architecture, prompt structure, domain and task.

</details>


### [60] [A Systematic Study of Compositional Syntactic Transformer Language Models](https://arxiv.org/abs/2506.22978)

*Yida Zhao, Hao Xve, Xiang Hu, Kewei Tu*

**Main category:** cs.CL

**Keywords:** syntactic language models, compositional SLMs, Transformers, parse trees, language modeling

**Relevance Score:** 6

**TL;DR:** This paper presents a unified framework for compositional syntactic language models (SLMs) that incorporate constituency parse trees, emphasizing design choices and empirical evaluations across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Transformers by integrating syntactic information through compositional syntactic language models and address design issues within existing models.

**Method:** The paper proposes a unified framework for compositional SLMs focusing on constituency parse trees and conducts a detailed empirical evaluation across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency.

**Key Contributions:**

	1. A unified framework encompassing existing and novel compositional SLMs
	2. Empirical evaluation of SLM variants across multiple tasks
	3. Recommendations for the design of compositional SLMs.

**Result:** The empirical evaluation revealed insights into the performance of various compositional SLM variants, leading to recommendations on effective design choices for such models.

**Limitations:** The framework and recommendations may primarily focus on constituency parse trees and might not generalize to all parsing approaches or applications.

**Conclusion:** The work enhances understanding of compositional SLMs and offers guidelines for future implementations to optimize language modeling tasks.

**Abstract:** Syntactic language models (SLMs) enhance Transformers by incorporating syntactic biases through the modeling of linearized syntactic parse trees alongside surface sentences. This paper focuses on compositional SLMs that are based on constituency parse trees and contain explicit bottom-up composition of constituent representations. We identify key aspects of design choices in existing compositional SLMs and propose a unified framework encompassing both existing models and novel variants. We conduct a comprehensive empirical evaluation of all the variants in our framework across language modeling, syntactic generalization, summarization, dialogue, and inference efficiency. Based on the experimental results, we make multiple recommendations on the design of compositional SLMs. Our code is released at https://github.com/zhaoyd1/compositional_SLMs.

</details>


### [61] [SoMi-ToM: Evaluating Multi-Perspective Theory of Mind in Embodied Social Interactions](https://arxiv.org/abs/2506.23046)

*Xianzhe Fan, Xuhui Zhou, Chuanyang Jin, Kolby Nottingham, Hao Zhu, Maarten Sap*

**Main category:** cs.CL

**Keywords:** Theory of Mind, multimodal interaction, embodied interactions, large vision-language models, social dynamics

**Relevance Score:** 8

**TL;DR:** The SoMi-ToM benchmark evaluates multi-perspective Theory of Mind in complex social interactions using multimodal data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap between static, text-based ToM benchmarks and real-world interactions by creating a more dynamic evaluation method.

**Method:** The paper introduces the SoMi-ToM benchmark, which includes first-person and third-person evaluations utilizing rich multimodal data from the SoMi environment, encompassing diverse crafting goals and social relationships.

**Key Contributions:**

	1. Introduction of SoMi-ToM benchmark for multi-perspective ToM evaluation.
	2. Use of multimodal interaction data for a richer assessment of social interactions.
	3. Demonstrated performance gap between humans and LVLMs, highlighting areas for improvement.

**Result:** The dataset comprises 35 third-person videos, 363 first-person images, and 1225 annotated questions. Evaluation shows a significant accuracy gap between humans and state-of-the-art LVLMs, indicating the need for LVLMs to improve ToM capabilities.

**Limitations:** The benchmark may not cover all potential social scenarios and interactions in real life.

**Conclusion:** SoMi-ToM provides a comprehensive framework for assessing ToM in social interactions, revealing LVLMs' limitations in these contexts and emphasizing the need for advancements in this area.

**Abstract:** Humans continuously infer the states, goals, and behaviors of others by perceiving their surroundings in dynamic, real-world social interactions. However, most Theory of Mind (ToM) benchmarks only evaluate static, text-based scenarios, which have a significant gap compared to real interactions. We propose the SoMi-ToM benchmark, designed to evaluate multi-perspective ToM in embodied multi-agent complex social interactions. This benchmark is based on rich multimodal interaction data generated by the interaction environment SoMi, covering diverse crafting goals and social relationships. Our framework supports multi-level evaluation: (1) first-person evaluation provides multimodal (visual, dialogue, action, etc.) input from a first-person perspective during a task for real-time state inference, (2) third-person evaluation provides complete third-person perspective video and text records after a task for goal and behavior inference. This evaluation method allows for a more comprehensive examination of a model's ToM capabilities from both the subjective immediate experience and the objective global observation. We constructed a challenging dataset containing 35 third-person perspective videos, 363 first-person perspective images, and 1225 expert-annotated multiple-choice questions (three options). On this dataset, we systematically evaluated the performance of human subjects and several state-of-the-art large vision-language models (LVLMs). The results show that LVLMs perform significantly worse than humans on SoMi-ToM: the average accuracy gap between humans and models is 40.1% in first-person evaluation and 26.4% in third-person evaluation. This indicates that future LVLMs need to further improve their ToM capabilities in embodied, complex social interactions.

</details>


### [62] [MariNER: A Dataset for Historical Brazilian Portuguese Named Entity Recognition](https://arxiv.org/abs/2506.23051)

*João Lucas Luz Lima Sarcinelli, Marina Lages Gonçalves Teixeira, Jade Bortot de Paiva, Diego Furtado Silva*

**Main category:** cs.CL

**Keywords:** Named Entity Recognition, Brazilian Portuguese, Historical texts, Digital humanities, NLP

**Relevance Score:** 5

**TL;DR:** The paper presents MariNER, a gold-standard Named Entity Recognition dataset for early 20th-century Brazilian Portuguese, addressing the lack of quality resources for NER in this language.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to fill the gap in available high-quality NER datasets for Brazilian Portuguese, particularly for analyzing historical texts in digital humanities.

**Method:** The authors constructed the MariNER dataset with over 9,000 manually annotated sentences focusing on early 20th-century Brazilian Portuguese.

**Key Contributions:**

	1. Introduction of MariNER, the first gold-standard NER dataset for Brazilian Portuguese.
	2. Manually annotated dataset with over 9,000 sentences.
	3. Performance evaluation of top NER models on the dataset.

**Result:** The paper includes an assessment and comparison of the performance of state-of-the-art NER models on the MariNER dataset.

**Limitations:** 

**Conclusion:** The creation of MariNER represents a significant step toward improving NER tasks for Brazilian Portuguese, enabling more effective analysis of historical texts.

**Abstract:** Named Entity Recognition (NER) is a fundamental Natural Language Processing (NLP) task that aims to identify and classify entity mentions in texts across different categories. While languages such as English possess a large number of high-quality resources for this task, Brazilian Portuguese still lacks in quantity of gold-standard NER datasets, especially when considering specific domains. Particularly, this paper considers the importance of NER for analyzing historical texts in the context of digital humanities. To address this gap, this work outlines the construction of MariNER: \textit{Mapeamento e Anota\c{c}\~oes de Registros hIst\'oricos para NER} (Mapping and Annotation of Historical Records for NER), the first gold-standard dataset for early 20th-century Brazilian Portuguese, with more than 9,000 manually annotated sentences. We also assess and compare the performance of state-of-the-art NER models for the dataset.

</details>


### [63] [Boosting LLM's Molecular Structure Elucidation with Knowledge Enhanced Tree Search Reasoning](https://arxiv.org/abs/2506.23056)

*Xiang Zhuang, Bin Wu, Jiyu Cui, Kehua Feng, Xiaotong Li, Huabin Xing, Keyan Ding, Qiang Zhang, Huajun Chen*

**Main category:** cs.CL

**Keywords:** Molecular Structure Elucidation, Large Language Models, Knowledge-enhanced reasoning

**Relevance Score:** 4

**TL;DR:** This paper presents K-MSE, a Knowledge-enhanced reasoning framework for improving molecular structure elucidation using LLMs and Monte Carlo Tree Search.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Molecular structure elucidation is vital in chemical analysis but is hindered by LLMs' limited chemical knowledge.

**Method:** The study introduces K-MSE, which incorporates a molecular substructure knowledge base and a molecule-spectrum scorer to enhance reasoning capabilities.

**Key Contributions:**

	1. Introduction of K-MSE framework
	2. Integration of a molecular substructure knowledge base
	3. Development of a specialized molecule-spectrum scorer

**Result:** Experimental results demonstrate over 20% improvement in performance on GPT-4o-mini and GPT-4o models.

**Limitations:** 

**Conclusion:** The K-MSE framework significantly enhances LLMs' ability to tackle molecular structure elucidation by providing specialized knowledge and evaluation methods.

**Abstract:** Molecular structure elucidation involves deducing a molecule's structure from various types of spectral data, which is crucial in chemical experimental analysis. While large language models (LLMs) have shown remarkable proficiency in analyzing and reasoning through complex tasks, they still encounter substantial challenges in molecular structure elucidation. We identify that these challenges largely stem from LLMs' limited grasp of specialized chemical knowledge. In this work, we introduce a Knowledge-enhanced reasoning framework for Molecular Structure Elucidation (K-MSE), leveraging Monte Carlo Tree Search for test-time scaling as a plugin. Specifically, we construct an external molecular substructure knowledge base to extend the LLMs' coverage of the chemical structure space. Furthermore, we design a specialized molecule-spectrum scorer to act as a reward model for the reasoning process, addressing the issue of inaccurate solution evaluation in LLMs. Experimental results show that our approach significantly boosts performance, particularly gaining more than 20% improvement on both GPT-4o-mini and GPT-4o. Our code is available at https://github.com/HICAI-ZJU/K-MSE.

</details>


### [64] [Text2VectorSQL: Bridging Text-to-SQL and Vector Search for Unified Natural Language Queries](https://arxiv.org/abs/2506.23071)

*Zhengren Wang, Bozhou Li, Dongwen Yao, Wentao Zhang*

**Main category:** cs.CL

**Keywords:** Text-to-SQL, vector search, natural language queries, semantic retrieval, database interfaces

**Relevance Score:** 8

**TL;DR:** Text2VectorSQL unifies Text-to-SQL and vector search for improved natural language interaction with databases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing Text-to-SQL systems with unstructured data and ambiguous queries.

**Method:** Introduced a framework that unifies Text-to-SQL and vector search, allowing for semantic filtering and multi-modal matching, and built an evaluation framework with automatic ground truth annotations.

**Key Contributions:**

	1. Unified Text-to-SQL and vector search into a single framework.
	2. Developed a dedicated evaluation framework for Text2VectorSQL.
	3. Achieved performance improvements in retrieval tasks compared to existing methods.

**Result:** Demonstrated significant performance improvements over baseline methods with dedicated Text2VectorSQL models using synthetic data.

**Limitations:** Results are based on synthetic data; real-world performance may vary.

**Conclusion:** Text2VectorSQL establishes a foundation for versatile database interfaces, enhancing natural language query handling.

**Abstract:** While Text-to-SQL enables natural language interaction with structured databases, its effectiveness diminishes with unstructured data or ambiguous queries due to rigid syntax and limited expressiveness. Concurrently, vector search has emerged as a powerful paradigm for semantic retrieval, particularly for unstructured data. However, existing VectorSQL implementations still rely heavily on manual crafting and lack tailored evaluation frameworks, leaving a significant gap between theoretical potential and practical deployment. To bridge these complementary paradigms, we introduces Text2VectorSQL, a novel framework unifying Text-to-SQL and vector search to overcome expressiveness constraints and support more diverse and holistical natural language queries. Specifically, Text2VectorSQL enables semantic filtering, multi-modal matching, and retrieval acceleration. For evaluation, we build vector index on appropriate columns, extend user queries with semantic search, and annotate ground truths via an automatic pipeline with expert review. Furthermore, we develop dedicated Text2VectorSQL models with synthetic data, demonstrating significant performance improvements over baseline methods. Our work establishes the foundation for the Text2VectorSQL task, paving the way for more versatile and intuitive database interfaces. The repository will be publicly available at https://github.com/Open-DataFlow/Text2VectorSQL.

</details>


### [65] [From Individuals to Interactions: Benchmarking Gender Bias in Multimodal Large Language Models from the Lens of Social Relationship](https://arxiv.org/abs/2506.23101)

*Yue Xu, Wenjie Wang*

**Main category:** cs.CL

**Keywords:** gender bias, multimodal large language models, benchmark, interpersonal interactions, narrative generation

**Relevance Score:** 8

**TL;DR:** This paper introduces Genres, a benchmark for evaluating gender bias in multimodal large language models (MLLMs) through dual-individual interactions, revealing context-sensitive biases that are often overlooked.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about gender bias in MLLMs, especially in socially sensitive contexts, and to evaluate the biases that emerge in interpersonal interactions rather than isolated scenarios.

**Method:** The study introduces a benchmark called Genres that focuses on gender bias evaluation in dual-character profiles and narrative generation tasks, emphasizing rich interpersonal dynamics.

**Key Contributions:**

	1. Introduction of a novel benchmark (Genres) for gender bias evaluation in MLLMs.
	2. Focus on relational and contextual dynamics in interpersonal interactions.
	3. Provision of fine-grained evaluation tools for assessing bias in multiple dimensions.

**Result:** Experiments showed persistent, context-sensitive gender biases in MLLMs, which are not evident when evaluating single-character interactions.

**Limitations:** The study may not cover all possible relational contexts and might be limited to the scenarios tested in the benchmark.

**Conclusion:** The paper highlights the necessity of using relationship-aware benchmarks for a more nuanced understanding of gender bias in MLLMs and provides guidance for future bias mitigation efforts.

**Abstract:** Multimodal large language models (MLLMs) have shown impressive capabilities across tasks involving both visual and textual modalities. However, growing concerns remain about their potential to encode and amplify gender bias, particularly in socially sensitive applications. Existing benchmarks predominantly evaluate bias in isolated scenarios, overlooking how bias may emerge subtly through interpersonal interactions. We fill this gap by going beyond single-entity evaluation and instead focusing on a deeper examination of relational and contextual gender bias in dual-individual interactions. We introduce Genres, a novel benchmark designed to evaluate gender bias in MLLMs through the lens of social relationships in generated narratives. Genres assesses gender bias through a dual-character profile and narrative generation task that captures rich interpersonal dynamics and supports a fine-grained bias evaluation suite across multiple dimensions. Experiments on both open- and closed-source MLLMs reveal persistent, context-sensitive gender biases that are not evident in single-character settings. Our findings underscore the importance of relationship-aware benchmarks for diagnosing subtle, interaction-driven gender bias in MLLMs and provide actionable insights for future bias mitigation.

</details>


### [66] [FairI Tales: Evaluation of Fairness in Indian Contexts with a Focus on Bias and Stereotypes](https://arxiv.org/abs/2506.23111)

*Janki Atul Nawale, Mohammed Safi Ur Rahman Khan, Janani D, Mansi Gupta, Danish Pruthi, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** fairness, large language models, bias evaluation, India, socio-cultural topics

**Relevance Score:** 9

**TL;DR:** This paper introduces INDIC-BIAS, a benchmark for evaluating the fairness of LLMs in India, addressing biases across diverse socio-cultural groups.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing fairness studies predominantly focus on Western contexts, leaving a significant gap for culturally diverse countries like India.

**Method:** The authors curated 1,800 socio-cultural topics with experts and created 20,000 validated scenario templates for evaluation, structured into plausibility, judgment, and generation tasks.

**Key Contributions:**

	1. Introduction of INDIC-BIAS benchmark for Indian context
	2. Curated a diverse set of socio-cultural topics for bias evaluation
	3. Open-source release to advance fairness benchmarking in LLMs

**Result:** Evaluation of 14 popular LLMs revealed strong negative biases against marginalized Indian identities, with frequent reinforcement of stereotypes and difficulties in mitigating bias even with explicit prompts.

**Limitations:** The study is limited to the Indian context and may not generalize to other regions; it also relies on the quality and representativeness of curated topics.

**Conclusion:** The findings highlight the need for cautious application of LLMs in India due to evidences of allocative and representational harms, and the authors release INDIC-BIAS as an open-source tool for further research.

**Abstract:** Existing studies on fairness are largely Western-focused, making them inadequate for culturally diverse countries such as India. To address this gap, we introduce INDIC-BIAS, a comprehensive India-centric benchmark designed to evaluate fairness of LLMs across 85 identity groups encompassing diverse castes, religions, regions, and tribes. We first consult domain experts to curate over 1,800 socio-cultural topics spanning behaviors and situations, where biases and stereotypes are likely to emerge. Grounded in these topics, we generate and manually validate 20,000 real-world scenario templates to probe LLMs for fairness. We structure these templates into three evaluation tasks: plausibility, judgment, and generation. Our evaluation of 14 popular LLMs on these tasks reveals strong negative biases against marginalized identities, with models frequently reinforcing common stereotypes. Additionally, we find that models struggle to mitigate bias even when explicitly asked to rationalize their decision. Our evaluation provides evidence of both allocative and representational harms that current LLMs could cause towards Indian identities, calling for a more cautious usage in practical applications. We release INDIC-BIAS as an open-source benchmark to advance research on benchmarking and mitigating biases and stereotypes in the Indian context.

</details>


### [67] [Decoding Memes: Benchmarking Narrative Role Classification across Multilingual and Multimodal Models](https://arxiv.org/abs/2506.23122)

*Shivam Sharma, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** narrative roles, internet memes, machine learning, multimodal models, prompt engineering

**Relevance Score:** 7

**TL;DR:** The paper explores identifying narrative roles in internet memes using multilingual models and prompt design strategies, highlighting cultural and contextual nuances in language.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve role detection in internet memes, particularly in cross-cultural and code-mixed contexts, given the complexities introduced by language and cultural nuances.

**Method:** The authors investigate various models, including multilingual transformers and multimodal models, analyzing performance in a zero-shot setting across multiple datasets.

**Key Contributions:**

	1. Development of a balanced dataset for meme role identification
	2. Evaluation of various model architectures under zero-shot conditions
	3. Empirical evidence for the effectiveness of hybrid prompting techniques.

**Result:** Larger models show better performance, but challenges persist with the 'Victim' class and generalizing across diverse cultural content. Hybrid prompt strategies yield modest improvements.

**Limitations:** Challenges in accurately identifying the 'Victim' narrative role and generalizing results across cultural contexts.

**Conclusion:** Cultural grounding, prompt engineering, and multimodal reasoning are crucial for effective role identification in memes.

**Abstract:** This work investigates the challenging task of identifying narrative roles - Hero, Villain, Victim, and Other - in Internet memes, across three diverse test sets spanning English and code-mixed (English-Hindi) languages. Building on an annotated dataset originally skewed toward the 'Other' class, we explore a more balanced and linguistically diverse extension, originally introduced as part of the CLEF 2024 shared task. Comprehensive lexical and structural analyses highlight the nuanced, culture-specific, and context-rich language used in real memes, in contrast to synthetically curated hateful content, which exhibits explicit and repetitive lexical markers. To benchmark the role detection task, we evaluate a wide spectrum of models, including fine-tuned multilingual transformers, sentiment and abuse-aware classifiers, instruction-tuned LLMs, and multimodal vision-language models. Performance is assessed under zero-shot settings using precision, recall, and F1 metrics. While larger models like DeBERTa-v3 and Qwen2.5-VL demonstrate notable gains, results reveal consistent challenges in reliably identifying the 'Victim' class and generalising across cultural and code-mixed content. We also explore prompt design strategies to guide multimodal models and find that hybrid prompts incorporating structured instructions and role definitions offer marginal yet consistent improvements. Our findings underscore the importance of cultural grounding, prompt engineering, and multimodal reasoning in modelling subtle narrative framings in visual-textual content.

</details>


### [68] [Unleashing Embodied Task Planning Ability in LLMs via Reinforcement Learning](https://arxiv.org/abs/2506.23127)

*Zhaoye Fei, Li Ji, Siyin Wang, Junhao Shi, Jingjing Gong, Xipeng Qiu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reinforcement Learning, Embodied Task Planning, Interactive Learning, Policy Optimization

**Relevance Score:** 7

**TL;DR:** Embodied Planner-R1 is a novel reinforcement learning framework for Large Language Models focused on embodied task planning, achieving high completion rates in interactive capabilities.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the ability of Large Language Models in embodied task planning, addressing the challenges of continuous environmental understanding and action generation in partially observable environments.

**Method:** The framework utilizes pure reinforcement learning with group rollout and incorporates interactive policy optimization for efficient learning, allowing LLMs to explore environments with minimal supervision.

**Key Contributions:**

	1. Introduction of a novel outcome-driven reinforcement learning framework for embodied task planning.
	2. Implementation of pure reinforcement learning with group rollout allowing minimal supervision.
	3. Development of Interactive Policy Optimization for learning from grouped trajectories.

**Result:** Embodied Planner-R1 achieves a completion rate of 97.78% on the ALFWorld benchmark and 79.92% on ScienceWorld, greatly exceeding past performance and demonstrating strong generalization with only a -3.66% drop in unseen environments.

**Limitations:** 

**Conclusion:** The proposed framework shows significant improvements in embodied task planning for LLMs, suggesting a promising direction for enhancing interactive capabilities through minimal supervision and efficient exploration.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various tasks, yet they face significant challenges in embodied task planning scenarios that require continuous environmental understanding and action generation. Existing approaches generate open-loop action scripts based on static knowledge, making it difficult to learn causal relationships between actions and environmental feedback, particularly in partially observable environments. We introduce Embodied Planner-R1, a novel outcome-driven reinforcement learning framework that enables LLMs to develop interactive capabilities through autonomous exploration with minimal supervision. Our framework incorporates three key innovations: (1) Without human annotations, we employ pure reinforcement learning with group rollout, incorporating in-environment interaction through parallel exploration; (2) completion-driven sparse reward; and (3) Interactive Policy Optimization (IPO) for efficient learning from grouped trajectories. Across two challenging text-based Embodied planning benchmarks, Embodied Planner-R1 achieves impressive completion rates of 97.78% on ALFWorld and 79.92% on ScienceWorld, surpassing prior methods by a large margin, and suffers only a -3.66% drop in previously unseen environments, evidencing strong generalization.

</details>


### [69] [Format-Adapter: Improving Reasoning Capability of LLMs by Adapting Suitable Format](https://arxiv.org/abs/2506.23133)

*Dingzirui Wang, Xuanliang Zhang, Rongyu Cao, Longxu Dou, Xianzhen Luo, Yingwei Ma, Qingfu Zhu, Wanxiang Che, Binhua Li, Fei Huang, Yongbin Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Consistency, Format Adaptation

**Relevance Score:** 8

**TL;DR:** The paper introduces Format-Adapter, a method using LLMs to generate and select suitable reasoning formats, improving LLM performance by addressing reasoning inconsistencies in tasks with multiple answers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate reasoning inconsistencies in LLMs by generating and voting on multiple answers without relying on costly human-labeled formats.

**Method:** The authors propose a method to measure reasoning errors and develop Format-Adapter, which uses LLMs for generating and selecting reasoning formats based on these error measurements.

**Key Contributions:**

	1. Introduction of Format-Adapter for reasoning format selection using LLMs
	2. Method for measuring reasoning error in generated answers
	3. Achievement of improved performance on reasoning tasks over existing works

**Result:** Format-Adapter achieves an average 4.3% performance improvement on math and commonsense reasoning tasks compared to previous methods.

**Limitations:** 

**Conclusion:** The effectiveness of Format-Adapter in selecting appropriate reasoning formats demonstrates its potential to enhance LLM performance in reasoning tasks.

**Abstract:** Generating and voting multiple answers is an effective method to mitigate reasoning inconsistencies of large language models (LLMs). Prior works have shown that multiple reasoning formats outperform a single format when generating multiple answers. However, previous works using multiple formats rely on formats labeled by humans, which could be unsuitable for all tasks and have high labeling costs. To address this issue, we adapt suitable formats to the given tasks by generating and selecting formats. We first propose how to measure the reasoning error when generating multiple answers. Then, we introduce Format-Adapter, which utilizes LLMs to generate and select suitable reasoning formats by minimizing the error measurement we present. We conduct experiments on math and commonsense reasoning tasks, where Format-Adapter achieves a 4.3% performance improvement on average over previous works, demonstrating the effectiveness.

</details>


### [70] [LLM-Assisted Question-Answering on Technical Documents Using Structured Data-Aware Retrieval Augmented Generation](https://arxiv.org/abs/2506.23136)

*Shadman Sobhan, Mohammad Ariful Haque*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, technical documents, context identification, RAG pipeline

**Relevance Score:** 9

**TL;DR:** The paper presents a novel Retrieval-Augmented Generation (RAG) pipeline for processing complex technical documents that include tables and images, demonstrating superior performance over traditional RAG methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional RAG pipelines in retrieving information from structured technical documents containing tables and images.

**Method:** The proposed pipeline utilizes vector similarity search combined with a fine-tuned reranker based on Gemma-2-9b-it, trained using RAFT on a custom dataset aimed at improving context identification for question answering.

**Key Contributions:**

	1. Introduction of a RAG pipeline tailored for structured technical documents
	2. Utilization of a fine-tuned reranker for improved context identification
	3. Demonstration of superior performance in answering table-based questions

**Result:** The evaluation shows a high faithfulness score of 94% (RAGas) and 96% (DeepEval), with answer relevancy scores of 87% (RAGas) and 93% (DeepEval), outperforming general RAG pipelines.

**Limitations:** 

**Conclusion:** The proposed RAG pipeline significantly enhances the retrieval accuracy and handling of complex queries related to structured data in technical documents.

**Abstract:** Large Language Models (LLMs) are capable of natural language understanding and generation. But they face challenges such as hallucination and outdated knowledge. Fine-tuning is one possible solution, but it is resource-intensive and must be repeated with every data update. Retrieval-Augmented Generation (RAG) offers an efficient solution by allowing LLMs to access external knowledge sources. However, traditional RAG pipelines struggle with retrieving information from complex technical documents with structured data such as tables and images. In this work, we propose a RAG pipeline, capable of handling tables and images in documents, for technical documents that support both scanned and searchable formats. Its retrieval process combines vector similarity search with a fine-tuned reranker based on Gemma-2-9b-it. The reranker is trained using RAFT (Retrieval-Augmented Fine-Tuning) on a custom dataset designed to improve context identification for question answering. Our evaluation demonstrates that the proposed pipeline achieves a high faithfulness score of 94% (RAGas) and 96% (DeepEval), and an answer relevancy score of 87% (RAGas) and 93% (DeepEval). Comparative analysis demonstrates that the proposed architecture is superior to general RAG pipelines in terms of table-based questions and handling questions outside context.

</details>


### [71] [Flow-Modulated Scoring for Semantic-Aware Knowledge Graph Completion](https://arxiv.org/abs/2506.23137)

*Siyuan Li, Ruitong Liu, Yan Wen, Te Sun*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Flow-Modulated Scoring, Context-aware representations

**Relevance Score:** 4

**TL;DR:** Proposes a Flow-Modulated Scoring framework for Knowledge Graph Completion that improves modeling of contextual dependencies and relational dynamics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the limitations of static, embedding-based approaches in Knowledge Graph Completion, particularly in capturing relational dynamics and contextual dependencies.

**Method:** The Flow-Modulated Scoring (FMS) framework includes a semantic context learning module for encoding context-sensitive entity representations and a conditional flow-matching module for learning dynamic transformations between entity embeddings based on context.

**Key Contributions:**

	1. Introduction of Flow-Modulated Scoring framework for KGC
	2. Semantic context learning module for dynamic entity representation
	3. Conditional flow-matching module for relational dynamics

**Result:** FMS produces a predictive vector field that allows for dynamic refinement of the initial static scores of entity pairs, demonstrating superior performance on standard benchmarks.

**Limitations:** 

**Conclusion:** The proposed FMS framework facilitates a deeper modeling of relational semantics and outperforms existing state-of-the-art methods in Knowledge Graph Completion.

**Abstract:** Effective modeling of multifaceted relations is pivotal for Knowledge Graph Completion (KGC). However, a majority of existing approaches are predicated on static, embedding-based scoring, exhibiting inherent limitations in capturing contextual dependencies and relational dynamics. Addressing this gap, we propose the Flow-Modulated Scoring (FMS) framework. FMS comprises two principal components: (1) a semantic context learning module that encodes context-sensitive entity representations, and (2) a conditional flow-matching module designed to learn the dynamic transformation from a head to a tail embedding, governed by the aforementioned context. The resultant predictive vector field, representing the context-informed relational path, serves to dynamically refine the initial static score of an entity pair. Through this synergy of context-aware static representations and conditioned dynamic information, FMS facilitates a more profound modeling of relational semantics. Comprehensive evaluations on several standard benchmarks demonstrate that our proposed method surpasses prior state-of-the-art results.

</details>


### [72] [Benchmarking Deep Search over Heterogeneous Enterprise Data](https://arxiv.org/abs/2506.23139)

*Prafulla Kumar Choubey, Xiangyu Peng, Shilpa Bhagavath, Kung-Hsiang Huang, Caiming Xiong, Chien-Sheng Wu*

**Main category:** cs.CL

**Keywords:** Deep Search, retrieval-augmented generation, benchmark, multi-hop reasoning, enterprise artifacts

**Relevance Score:** 7

**TL;DR:** The paper introduces a benchmark for evaluating Deep Search in RAG systems, revealing significant challenges in retrieval performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a realistic benchmark for evaluating the performance of retrieval-augmented generation (RAG) systems that engage in complex reasoning over diverse sources.

**Method:** A synthetic data pipeline is used to generate interconnected content that simulates business workflows and includes both answerable and unanswerable queries, alongside a retrieval pool comprising 39,190 enterprise artifacts.

**Key Contributions:**

	1. Introduction of a new benchmark for evaluating Deep Search in RAG systems.
	2. Release of 39,190 enterprise artifacts for fine-grained evaluation.
	3. Identification of retrieval as a main bottleneck in current RAG performance.

**Result:** The best-performing RAG methods achieve an average score of 32.96, indicating that retrieval limitations significantly impact performance due to incomplete context.

**Limitations:** The study primarily focuses on retrieval challenges without deeply exploring algorithmic advancements.

**Conclusion:** The findings suggest that improving retrieval methods is critical for enabling effective deep searches in RAG systems.

**Abstract:** We present a new benchmark for evaluating Deep Search--a realistic and complex form of retrieval-augmented generation (RAG) that requires source-aware, multi-hop reasoning over diverse, sparsed, but related sources. These include documents, meeting transcripts, Slack messages, GitHub, and URLs, which vary in structure and often contain human-to-human interactions. We build it using a synthetic data pipeline that simulates business workflows across product planning, development, and support stages, generating interconnected content with realistic noise and multi-hop questions with guaranteed ground-truth answers. We release our benchmark with both answerable and unanswerable queries, and retrieval pool of 39,190 enterprise artifacts, enabling fine-grained evaluation of long-context LLM and RAG systems. Our experiments reveal that even the best-performing agentic RAG methods achieve an average performance score of 32.96 on our benchmark. With further analysis, we highlight retrieval as the main bottleneck: existing methods struggle to conduct deep searches and retrieve all necessary evidence. Consequently, they often reason over partial context, leading to significant performance degradation.

</details>


### [73] [Learning-to-Context Slope: Evaluating In-Context Learning Effectiveness Beyond Performance Illusions](https://arxiv.org/abs/2506.23146)

*Dingzriui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng*

**Main category:** cs.CL

**Keywords:** in-context learning, large language models, evaluation metrics, synthetic evaluation, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces the Learning-to-Context Slope (LCS) metric for evaluating the effectiveness of in-context learning (ICL) in large language models (LLMs), addressing limitations of current performance-based evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to improve the reliability and practicality of evaluating in-context learning (ICL) for large language models (LLMs), which currently relies on performance changes that can be unreliable.

**Method:** The authors propose the Learning-to-Context Slope (LCS), a metric that quantifies ICL effectiveness by modeling the slope between learning gain and contextual relevance. It captures continuous changes in loss, attributes failures to contextual alignment and output calibration, and minimizes reliance on labeled data through synthetic evaluation.

**Key Contributions:**

	1. Introduction of the Learning-to-Context Slope (LCS) metric for ICL evaluation
	2. Improvement of reliability in assessing in-context learning effectiveness
	3. Identification of critical model capabilities for successful ICL applications.

**Result:** Extensive experiments show that LCS correlates strongly with performance improvements in labeled datasets and reliably reflects true effectiveness in biased or data-scarce scenarios.

**Limitations:** Potential limitations may include the synthetic evaluation dependence and varying effectiveness across different models and tasks.

**Conclusion:** The LCS metric significantly improves the evaluation of in-context learning by providing actionable insights and reducing dependence on labeled data, thus aiding practitioners in assessing ICL's impact more effectively.

**Abstract:** In-context learning (ICL) has emerged as an effective approach to enhance the performance of large language models (LLMs). However, its effectiveness varies significantly across models and tasks, posing challenges for practitioners to determine when ICL reliably improves performance. Current evaluation approaches, reliant on performance change after applying ICL, suffer from low reliability, poor attribution, and impracticality in data-insufficient scenarios. We propose the Learning-to-Context Slope (LCS), a novel metric that quantifies ICL effectiveness by modeling the slope between learning gain (loss decrease from demonstrations) and contextual relevance (demonstration-input relevance). LCS addresses key limitations of performance-based metrics: (1) it captures continuous loss changes even when outputs are incorrect, improving reliability; (2) its formulation attributes ICL failures to weak contextual alignment (inability to adapt inputs to demonstrations) or strong output calibration (self-verification of correctness); and (3) it minimizes reliance on labeled data via synthetic evaluation. Extensive experiments demonstrate that LCS strongly correlates with performance improvements in labeled settings and reliably reflects true effectiveness in biased or data-scarce scenarios. Further analysis reveals actionable thresholds for LCS and identifies model capabilities critical to ICL success.

</details>


### [74] [V-SYNTHESIS: Task-Agnostic Synthesis of Consistent and Diverse In-Context Demonstrations from Scratch via V-Entropy](https://arxiv.org/abs/2506.23149)

*Dingzirui Wang, Xuanliang Zhang, Keyan Xu, Qingfu Zhu, Wanxiang Che, Yang Deng*

**Main category:** cs.CL

**Keywords:** in-context learning, demonstration synthesis, V-Score, V-Synthesis, language models

**Relevance Score:** 7

**TL;DR:** This paper introduces V-Synthesis, a method for synthesizing demonstrations from scratch using a new consistency metric, V-Score, to enhance consistency and diversity in in-context learning (ICL).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The high labeling cost for in-context learning demonstrations drives the need for methods to synthesize these demonstrations using large language models.

**Method:** The paper proposes V-Score, a consistency metric that outperforms existing metrics in performance and computational cost. V-Synthesis utilizes this metric for proportional sampling in synthesizing demonstrations.

**Key Contributions:**

	1. Introduction of V-Score as a consistency metric.
	2. Development of V-Synthesis for synthesizing demonstrations from scratch.
	3. Demonstration of performance improvement in ICL tasks.

**Result:** V-Synthesis shows an average performance improvement of 2.0% over existing synthesis methods, validating its efficacy in ensuring high consistency and diversity.

**Limitations:** 

**Conclusion:** The proposed V-Synthesis method effectively reduces labeling overhead while maintaining high-quality synthesized demonstrations.

**Abstract:** High labeling cost for in-context learning (ICL) demonstrations motivates using large language models (LLMs) for synthesis to reduce overhead. However, existing synthesis methods are mainly task-specific or rely on pre-existing demonstrations. So this paper focuses on synthesizing demonstrations from scratch for arbitrary tasks. A major challenge in synthesizing from scratch is ensuring consistency with the target task, as the lack of labeling guidance could lead to synthesis bias. We first propose a consistency metric called V-Score, which has higher performance and lower computation cost compared with the metrics based on grams or embedding vectors. Furthermore, we introduce V-Synthesis, which leverages V-Score for proportional sampling to ensure both high consistency and diversity of synthesized demonstrations. Experimental results demonstrate that V-Synthesis yields an average performance improvement of 2.0% compared to existing synthesis methods confirming the effectiveness of V-Synthesis.

</details>


### [75] [RiverText: A Python Library for Training and Evaluating Incremental Word Embeddings from Text Data Streams](https://arxiv.org/abs/2506.23192)

*Gabriel Iturra-Bocaz, Felipe Bravo-Marquez*

**Main category:** cs.CL

**Keywords:** word embeddings, incremental learning, natural language processing, streaming data, RiverText

**Relevance Score:** 8

**TL;DR:** This paper introduces RiverText, a Python library for training and evaluating incremental word embeddings from continuous text data streams, addressing the static nature of traditional word embeddings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The static nature of traditional word embedding models limits their effectiveness in adapting to evolving language patterns found in social media and other dynamic sources.

**Method:** RiverText implements various incremental word embedding techniques within a standardized framework, utilizing PyTorch for neural network training and adapting tasks for streaming settings.

**Key Contributions:**

	1. Introduction of RiverText for incremental word embeddings
	2. Standardized framework using various embedding techniques
	3. Adaptation of intrinsic evaluation tasks for streaming settings

**Result:** RiverText enables effective training and evaluation of incremental word embeddings, enhancing performance in dynamic language environments, with comparative results presented for various hyperparameter settings.

**Limitations:** 

**Conclusion:** The RiverText library serves as a valuable resource for the information retrieval and NLP communities needing to analyze streaming data, with ongoing development opportunities: it is open-source and available on GitHub.

**Abstract:** Word embeddings have become essential components in various information retrieval and natural language processing tasks, such as ranking, document classification, and question answering. However, despite their widespread use, traditional word embedding models present a limitation in their static nature, which hampers their ability to adapt to the constantly evolving language patterns that emerge in sources such as social media and the web (e.g., new hashtags or brand names). To overcome this problem, incremental word embedding algorithms are introduced, capable of dynamically updating word representations in response to new language patterns and processing continuous data streams.   This paper presents RiverText, a Python library for training and evaluating incremental word embeddings from text data streams. Our tool is a resource for the information retrieval and natural language processing communities that work with word embeddings in streaming scenarios, such as analyzing social media. The library implements different incremental word embedding techniques, such as Skip-gram, Continuous Bag of Words, and Word Context Matrix, in a standardized framework. In addition, it uses PyTorch as its backend for neural network training. We have implemented a module that adapts existing intrinsic static word embedding evaluation tasks for word similarity and word categorization to a streaming setting. Finally, we compare the implemented methods with different hyperparameter settings and discuss the results. Our open-source library is available at https://github.com/dccuchile/rivertext.

</details>


### [76] [Generalist Reward Models: Found Inside Large Language Models](https://arxiv.org/abs/2506.23235)

*Yi-Chen Li, Tian Xu, Yang Yu, Xuqin Zhang, Xiong-Hui Chen, Zhongxiang Ling, Ningjing Chao, Lei Yuan, Zhi-Hua Zhou*

**Main category:** cs.CL

**Keywords:** Large Language Models, reward models, reinforcement learning, inverse reinforcement learning, alignment

**Relevance Score:** 9

**TL;DR:** This paper demonstrates that Large Language Models (LLMs) contain a latent reward model that can be utilized for reinforcement learning without additional training, proving its effectiveness theoretically and experimentally.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the costly nature of training reward models for aligning LLMs with human preferences, exploring a more efficient solution utilizing existing LLMs.

**Method:** The authors establish a theoretical framework linking the latent reward model in LLMs to classical inverse reinforcement learning, allowing direct elicitation of a high-quality reward signal from pre-trained models without further training.

**Key Contributions:**

	1. Proves the equivalence of latent rewards in LLMs to rewards learned through inverse reinforcement learning.
	2. Demonstrates that no additional training is required to elicit high-quality rewards from LLMs.
	3. Establishes a theoretical superiority for the resultant policies using these endogenous rewards.

**Result:** The experiments confirm that the proposed reinforcement learning approach using the endogenous reward from LLMs achieves superior performance compared to existing LLM-as-a-judge methods and trained reward models.

**Limitations:** 

**Conclusion:** The study provides a theoretical basis for leveraging the inherent reward models in LLMs, suggesting a paradigm shift towards more efficient alignment strategies for LLMs and multi-modal models.

**Abstract:** The alignment of Large Language Models (LLMs) is critically dependent on reward models trained on costly human preference data. While recent work explores bypassing this cost with AI feedback, these methods often lack a rigorous theoretical foundation. In this paper, we discover that a powerful generalist reward model is already latently present within any LLM trained via standard next-token prediction. We prove that this endogenous reward is not a heuristic, but is theoretically equivalent to a reward function learned through offline inverse reinforcement learning. This connection allows us to directly elicit a high-quality reward signal from a base (pre-trained or supervised fine-tuned) model without any further training. Critically, we also prove that subsequent reinforcement learning using this endogenous reward leads to a policy with a provably superior error bound compared to the base model. To our best knowledge, this is the first theoretical proof of the effectiveness of reinforcement learning for LLMs. Our experiments validate this theory, demonstrating that our method not only outperforms existing LLM-as-a-judge approaches but can also surpass explicitly trained reward models. These findings suggest that the reward modeling stage can be replaced by a principled method of eliciting the knowledge already captured during pre-training, heralding a more efficient, powerful, and scalable paradigm for LLMs alignment as well as multi-modal models.

</details>


### [77] [Two Spelling Normalization Approaches Based on Large Language Models](https://arxiv.org/abs/2506.23288)

*Miguel Domingo, Francisco Casacuberta*

**Main category:** cs.CL

**Keywords:** spelling normalization, large language models, historical documents, machine translation, natural language processing

**Relevance Score:** 4

**TL;DR:** This study explores spelling normalization in historical documents using two novel approaches based on large language models and compares their effectiveness with statistical machine translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenge of inconsistencies in spelling conventions in historical documents, which is significant for scholarly research in the humanities.

**Method:** Two approaches based on large language models were proposed: one without supervised training and another trained for machine translation, evaluated on various datasets across languages and time periods.

**Key Contributions:**

	1. Introduction of two novel approaches for spelling normalization using large language models.
	2. Evaluation across diverse historical datasets to validate the effectiveness of the proposed methods.
	3. Comparison with statistical machine translation techniques.
	4. Encouragement for further exploration in the intersection of NLP and historical linguistics.

**Result:** Both approaches demonstrated encouraging results, but statistical machine translation was found to be the most effective technology for spelling normalization.

**Limitations:** The study may not account for all linguistic variations and nuances present in historical documents, and results may vary with different languages.

**Conclusion:** The findings suggest that while large language models offer some promise, traditional statistical methods are currently superior for this specific task.

**Abstract:** The absence of standardized spelling conventions and the organic evolution of human language present an inherent linguistic challenge within historical documents, a longstanding concern for scholars in the humanities. Addressing this issue, spelling normalization endeavors to align a document's orthography with contemporary standards. In this study, we propose two new approaches based on large language models: one of which has been trained without a supervised training, and a second one which has been trained for machine translation. Our evaluation spans multiple datasets encompassing diverse languages and historical periods, leading us to the conclusion that while both of them yielded encouraging results, statistical machine translation still seems to be the most suitable technology for this task.

</details>


### [78] [Objective-Free Local Learning and Emergent Language Structure in Thinking Machines](https://arxiv.org/abs/2506.23293)

*P. Myles Eugenio*

**Main category:** cs.CL

**Keywords:** neuro-symbolic, generative language modeling, Hopfield memory, emergent learning, symbolic structure

**Relevance Score:** 8

**TL;DR:** A neuro-symbolic framework for generative language modeling utilizes a hierarchical Hopfield memory to learn language structure without predefined tokens, enabling emergent tokenization and long-range dependency modeling.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to create a generative language model that learns symbolic structures from local neural interactions, addressing limitations of conventional models that rely on predefined tokens and supervision.

**Method:** The framework employs a hierarchical Hopfield memory chain for compositional short-term memory and dynamic tokenization, learning symbol sequences through multi-scale representations and local learning methods.

**Key Contributions:**

	1. Introduces a neuro-symbolic model that constructs language structure without predefined tokens.
	2. Demonstrates emergent tokenization leading to coherent synthetic languages akin to human languages.
	3. Offers a new methodological foundation for scalable and interpretable generative language models.

**Result:** The model effectively filters noise to generate synthetic languages with coherent internal morphology, similar to human language, and shows the capacity for generalization beyond its training data.

**Limitations:** 

**Conclusion:** This architecture indicates a novel approach for developing scalable and interpretable neuro-symbolic systems, potentially influencing the future designs of neuromorphic architectures for language generation.

**Abstract:** We present a neuro-symbolic framework for generative language modeling based on local, event-driven emergent learning. At its core is a hierarchical Hopfield memory chain acting as a compositional short-term memory and dynamic tokenizer (retokenizer). Rather than relying on predefined tokens or supervision, the model builds structure from scratch, learning symbol sequences as multi-scale representations. It constructs projection tensors that bind co-occurring features into hierarchical tokens, introducing redundancy (i.e an emergent gauge structure) and enabling compression of local activations into long-range dependencies. Curiously, we find that the retokenizer can filter natural language patterns from noise, generating synthetic languages with coherent internal morphology -- quantifiably the same as human language. Language is learned in a local (Hebbian) fashion, where model constraints dictate allowed emergent structure, and new information is retained in alignment with this structure. The absence of a global objective enables a form of plasticity not found in conventional language models, allowing the system to generalize beyond its initial inference class -- even without explicit data. We demonstrate that briefly activating a new neuron during inference binds distributed multi-scale token features into a symbolic embedding. These emergent embedding neurons act as long-term memory and support a key-value mechanism for compositional inference and generalization. This architecture provides a methodological foundation for studying how symbolic structure can emerge from local neural learning. It offers a new pathway for building scalable, interpretable neuro-symbolic systems -- where tokens, grammar, and reasoning arise as compressed memory traces within a Hopfield hierarchy. This approach advances the development of neuromorphic architectures for generative language models.

</details>


### [79] [Ensemble BERT for Medication Event Classification on Electronic Health Records (EHRs)](https://arxiv.org/abs/2506.23315)

*Shouvon Sarker, Xishuang Dong, Lijun Qian*

**Main category:** cs.CL

**Keywords:** BERT, medication event classification, natural language processing, clinical data analytics, CMED

**Relevance Score:** 8

**TL;DR:** This study develops a BERT-based ensemble model to detect and classify medication events from clinical notes using the Contextualized Medication Event Dataset (CMED).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the extraction and classification of medication events from clinical notes in electronic health records (EHR) for better clinical data analytics.

**Method:** The study involved pretraining BERT models on various big data sources and fine-tuning them on CMED training data, followed by integrating predictions from multiple fine-tuned models using voting strategies.

**Key Contributions:**

	1. Developed a BERT-based ensemble model for medication event classification
	2. Demonstrated significant improvement in F scores using ensemble predictions
	3. Applied the model to a comprehensive annotated clinical dataset (CMED)

**Result:** The BERT-based ensemble model achieved improvements of approximately 5% in strict Micro-F score and 6% in strict Macro-F score on the CMED test set.

**Limitations:** 

**Conclusion:** Using an ensemble approach with pretrained BERT models enhances the classification of medication events in clinical notes, showing potential for better clinical data analytics.

**Abstract:** Identification of key variables such as medications, diseases, relations from health records and clinical notes has a wide range of applications in the clinical domain. n2c2 2022 provided shared tasks on challenges in natural language processing for clinical data analytics on electronic health records (EHR), where it built a comprehensive annotated clinical data Contextualized Medication Event Dataset (CMED). This study focuses on subtask 2 in Track 1 of this challenge that is to detect and classify medication events from clinical notes through building a novel BERT-based ensemble model. It started with pretraining BERT models on different types of big data such as Wikipedia and MIMIC. Afterwards, these pretrained BERT models were fine-tuned on CMED training data. These fine-tuned BERT models were employed to accomplish medication event classification on CMED testing data with multiple predictions. These multiple predictions generated by these fine-tuned BERT models were integrated to build final prediction with voting strategies. Experimental results demonstrated that BERT-based ensemble models can effectively improve strict Micro-F score by about 5% and strict Macro-F score by about 6%, respectively.

</details>


### [80] [Information Loss in LLMs' Multilingual Translation: The Role of Training Data, Language Proximity, and Language Family](https://arxiv.org/abs/2506.23340)

*Yumeng Lin, Xufeng Duan, David Haslett, Yige Chen, Zhenguang G. Cai*

**Main category:** cs.CL

**Keywords:** multilingual translation, large language models, GPT-4, Llama 2, translation quality

**Relevance Score:** 8

**TL;DR:** This study examines the impact of training data, language proximity, and language family on information loss in multilingual translation using GPT-4 and Llama 2.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how various factors affect translation quality in multilingual settings, especially for language pairs with limited training data.

**Method:** Round-trip translations were performed using two large language models, GPT-4 and Llama 2. Translation quality was assessed with BLEU scores and BERT similarity metrics.

**Key Contributions:**

	1. Identification of factors affecting multilingual translation quality
	2. Analysis of various distance metrics as predictors of translation performance
	3. Demonstration of the interaction between training data size and language proximity

**Result:** The results show a significant interaction between training data size and language distance, with closer languages yielding better translations in low-resource conditions.

**Limitations:** 

**Conclusion:** Translation quality is influenced not just by the amount of training data but also by the structural and typological relationships between languages, informing future multilingual model training.

**Abstract:** Large language models have achieved impressive progress in multilingual translation, yet they continue to face challenges with certain language pairs-particularly those with limited training data or significant linguistic divergence from English. This study systematically investigates how training data, language proximity, and language family affect information loss in multilingual translation. We evaluate two large language models, GPT-4 and Llama 2, by performing round-trip translations. Translation quality was assessed using BLEU scores and BERT similarity metrics. Our results reveal a robust interaction between training data size and language distance: while abundant training data can mitigate the effects of linguistic divergence, languages structurally closer to English consistently yield higher translation quality in low-resource conditions. Among various distance metrics, orthographic, phylogenetic, syntactic, and geographical distances emerge as strong predictors of translation performance. Language family also exerts an independent influence. These findings contribute to a deeper understanding of the linguistic constraints shaping multilingual translation in large language models, emphasizing that translation quality is shaped not only by data volume but also by structural and typological relationships between languages.

</details>


### [81] [ATGen: A Framework for Active Text Generation](https://arxiv.org/abs/2506.23342)

*Akim Tsvigun, Daniil Vasilev, Ivan Tsvigun, Ivan Lysenko, Talgat Bektleuov, Aleksandr Medvedev, Uliana Vinogradova, Nikita Severin, Mikhail Mozikov, Andrey Savchenko, Rostislav Grigorev, Ramil Kuleev, Fedor Zhdanov, Artem Shelmanov, Ilya Makarov*

**Main category:** cs.CL

**Keywords:** Active Learning, Natural Language Generation, Large Language Models, Annotation Effort, NLP

**Relevance Score:** 9

**TL;DR:** Introduction of Active Text Generation (ATGen), a framework integrating Active Learning with natural language generation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce annotation efforts in training ML models for NLG tasks, leveraging Active Learning.

**Method:** ATGen provides a framework that utilizes Active Learning strategies for NLG tasks, incorporating both human and automatic annotation through LLMs.

**Key Contributions:**

	1. Introduction of a comprehensive framework that integrates AL with NLG tasks
	2. Support for both human annotators and LLM-based automatic annotation
	3. Benchmarking of various AL strategies in diverse NLG settings

**Result:** ATGen significantly reduces the effort required by human annotators and lowers costs from API calls to LLM annotation agents, validated through evaluation results across different settings.

**Limitations:** 

**Conclusion:** The framework offers a unified platform for implementing and testing novel AL strategies tailored for NLG.

**Abstract:** Active learning (AL) has demonstrated remarkable potential in reducing the annotation effort required for training machine learning models. However, despite the surging popularity of natural language generation (NLG) tasks in recent years, the application of AL to NLG has been limited. In this paper, we introduce Active Text Generation (ATGen) - a comprehensive framework that bridges AL with text generation tasks, enabling the application of state-of-the-art AL strategies to NLG. Our framework simplifies AL-empowered annotation in NLG tasks using both human annotators and automatic annotation agents based on large language models (LLMs). The framework supports LLMs deployed as services, such as ChatGPT and Claude, or operated on-premises. Furthermore, ATGen provides a unified platform for smooth implementation and benchmarking of novel AL strategies tailored to NLG tasks. Finally, we present evaluation results for state-of-the-art AL strategies across diverse settings and multiple text generation tasks. We show that ATGen reduces both the effort of human annotators and costs associated with API calls to LLM-based annotation agents. The code of the framework is available on GitHub under the MIT license. The video presentation is available at http://atgen-video.nlpresearch.group

</details>


### [82] [Perspective Dial: Measuring Perspective of Text and Guiding LLM Outputs](https://arxiv.org/abs/2506.23377)

*Taejin Kim, Siun-Chuon Mau, Konrad Vesey*

**Main category:** cs.CL

**Keywords:** large language models, bias, perspective control, prompt engineering, public discourse

**Relevance Score:** 9

**TL;DR:** This paper introduces Perspective-Dial, a framework for quantifying and controlling the perspective in the outputs of large language models (LLMs) to address issues of bias and viewpoint.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of quantifiable understanding of bias and perspective in LLM outputs, which is critical given their use in mission-critical roles.

**Method:** The paper presents Perspective-Dial, which includes a metric space called Perspective Space for measuring perspectives and employs systematic prompt engineering using greedy-coordinate descent to adjust the perspective in LLM outputs.

**Key Contributions:**

	1. Introduction of Perspective Space for perspective measurement
	2. Development of systematic prompt engineering for perspective control
	3. Potential applications in bias detection and correction

**Result:** Perspective-Dial provides a way to quantitatively measure and effectively control LLM outputs, addressing biases and facilitating applications in narrative tracking and advocacy.

**Limitations:** 

**Conclusion:** The proposed approach allows for the tracking and mitigation of bias in LLMs while enabling various applications in public discourse and debate.

**Abstract:** Large language models (LLMs) are used in a variety of mission-critical roles. Due to the rapidly developing nature of LLMs, there is a lack of quantifiable understanding of the bias and perspective associated with LLM output. Inspired by this need, this paper considers the broader issue of perspective or viewpoint of general text and perspective control of large-language model (LLM) output. Perspective-Dial consists of two main components: a (1) metric space, dubbed Perspective Space, that enables quantitative measurements of different perspectives regarding a topic, and the use of (2) Systematic Prompt Engineering that utilizes greedy-coordinate descent to control LLM output perspective based on measurement feedback from the Perspective Space. The empirical nature of the approach allows progress to side step a principled understanding of perspective or bias -- effectively quantifying and adjusting outputs for a variety of topics. Potential applications include detection, tracking and mitigation of LLM bias, narrative detection, sense making and tracking in public discourse, and debate bot advocating given perspective.

</details>


### [83] [Hierarchical Memory Organization for Wikipedia Generation](https://arxiv.org/abs/2506.23393)

*Eugene J. Yu, Dawei Zhu, Yifan Song, Xiangyu Wong, Jiebin Zhang, Wenxuan Shi, Xiaoguang Li, Qun Liu, Sujian Li*

**Main category:** cs.CL

**Keywords:** Wikipedia generation, Memory Organization, Natural Language Generation, AI writing systems, Information verifiability

**Relevance Score:** 6

**TL;DR:** The paper presents the Memory Organization-based Generation (MOG) framework for autonomous Wikipedia article generation, utilizing a hierarchical memory architecture to improve informativeness and verifiability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of generating accurate and well-structured Wikipedia articles from diverse sources.

**Method:** The MOG framework extracts fine-grained memory units from web documents, organizes them hierarchically, and uses this structure to guide the article generation process, while incorporating a citation module for traceability.

**Key Contributions:**

	1. Introduce the MOG framework leveraging a hierarchical memory architecture.
	2. Demonstrate improvements in article generation informativeness and verifiability.
	3. Implement a citation module for enhanced traceability.

**Result:** Evaluations on the WikiStart dataset show that MOG significantly outperforms baseline methods in terms of informativeness and reliability of generated articles.

**Limitations:** 

**Conclusion:** The MOG framework provides a robust solution for generating Wikipedia articles that align closely with a structured outline, enhancing both reliability and traceability in generated content.

**Abstract:** Generating Wikipedia articles autonomously is a challenging task requiring the integration of accurate, comprehensive, and well-structured information from diverse sources. This paper introduces the Memory Organization-based Generation (MOG) framework, a novel approach to address these challenges by leveraging a hierarchical memory architecture. MOG extracts fine-grained memory units from web documents, recursively organizes them into a Wikipedia-style hierarchical structure, and uses this structure to guide the generation process. This ensures alignment between memory and the article outline, improving both informativeness and verifiability while minimizing hallucinations. Additionally, a citation module is implemented to enhance traceability by linking every generated sentence to specific memory units. Evaluations on our newly created WikiStart dataset demonstrate that MOG outperforms baseline methods in producing informative and reliable articles, making it particularly robust in real-world scenarios.

</details>


### [84] [Datasets for Fairness in Language Models: An In-Depth Survey](https://arxiv.org/abs/2506.23411)

*Jiale Zhang, Zichong Wang, Avash Palikhe, Zhipeng Yin, Wenbin Zhang*

**Main category:** cs.CL

**Keywords:** fairness, language models, evaluation framework, demographic disparities, biases

**Relevance Score:** 8

**TL;DR:** This paper reviews widely used fairness datasets in language model research, highlighting their biases and proposing a unified evaluation framework for better analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of examination of fairness datasets used in language model benchmarks, which significantly influence evaluations of model fairness.

**Method:** A broad review of fairness datasets, characterizing them by their origin, scope, content, and intended use, alongside the introduction of a unified evaluation framework for systematic analysis.

**Key Contributions:**

	1. Comprehensive review of fairness datasets in language models
	2. Introduction of a unified evaluation framework for dataset analysis
	3. Recommendations for improved dataset selection and usage

**Result:** Consistent patterns of demographic disparities were revealed across various datasets, showing the biases that affect conclusions about model fairness.

**Limitations:** The paper primarily focuses on existing datasets without proposing specific new datasets for evaluation.

**Conclusion:** The paper provides practical guidance for the selection and use of fairness datasets and points towards the need for new benchmarks that reflect a diverse range of social contexts.

**Abstract:** Fairness benchmarks play a central role in shaping how we evaluate language models, yet surprisingly little attention has been given to examining the datasets that these benchmarks rely on. This survey addresses that gap by presenting a broad and careful review of the most widely used fairness datasets in current language model research, characterizing them along several key dimensions including their origin, scope, content, and intended use to help researchers better appreciate the assumptions and limitations embedded in these resources. To support more meaningful comparisons and analyses, we introduce a unified evaluation framework that reveals consistent patterns of demographic disparities across datasets and scoring methods. Applying this framework to twenty four common benchmarks, we highlight the often overlooked biases that can influence conclusions about model fairness and offer practical guidance for selecting, combining, and interpreting these datasets. We also point to opportunities for creating new fairness benchmarks that reflect more diverse social contexts and encourage more thoughtful use of these tools going forward. All code, data, and detailed results are publicly available at https://github.com/vanbanTruong/Fairness-in-Large-Language-Models/tree/main/datasets to promote transparency and reproducibility across the research community.

</details>


### [85] [TuCo: Measuring the Contribution of Fine-Tuning to Individual Responses of LLMs](https://arxiv.org/abs/2506.23423)

*Felipe Nuti, Tim Franzmeyer, João Henriques*

**Main category:** cs.CL

**Keywords:** Tuning Contribution, fine-tuning, large language models, adversarial attacks, model behavior

**Relevance Score:** 9

**TL;DR:** This paper introduces a method to quantitatively analyze the effects of fine-tuning on individual outputs of large language models (LLMs) through the concept of Tuning Contribution (TuCo).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of quantitative methods for assessing fine-tuning effects on LLM outputs, which this work seeks to address.

**Method:** The authors propose a method that tracks intermediate hidden states of LLMs, allowing for a decomposition into pre-training and fine-tuning components, theoretically analyzing their contributions.

**Key Contributions:**

	1. Introduction of Tuning Contribution (TuCo) as a measure of fine-tuning effects on LLM responses
	2. Theoretical decomposition of fine-tuning and pre-training components
	3. Empirical insights linking TuCo levels to adversarial attack outcomes

**Result:** The findings suggest that adjusting the fine-tuning component during inference can steer model behavior and that fine-tuning effects are related to the success of adversarial attacks on LLMs.

**Limitations:** The method relies on access to the original pre-trained model and focuses primarily on LLM behavior during specific tasks.

**Conclusion:** TuCo serves as a tool for quantitatively studying the influence of fine-tuning on model behavior and safety, revealing insights on adversarial vulnerabilities.

**Abstract:** Past work has studied the effects of fine-tuning on large language models' (LLMs) overall performance on certain tasks. However, a quantitative and systematic method for analyzing its effect on individual outputs is still lacking. Here, we propose a new method for measuring the contribution that fine-tuning makes to individual LLM responses, assuming access to the original pre-trained model. Our method tracks the model's intermediate hidden states, providing a more fine-grained insight into the effects of fine-tuning than a simple comparison of final outputs from pre-trained and fine-tuned models. We introduce and theoretically analyze an exact decomposition of any fine-tuned LLM into a pre-training component and a fine-tuning component. Empirically, we find that model behavior and performance can be steered by up- or down-scaling the fine-tuning component during the forward pass. Motivated by this finding and our theoretical analysis, we define the Tuning Contribution (TuCo) as the ratio of the magnitudes of the fine-tuning component to the pre-training component. We observe that three prominent adversarial attacks on LLMs circumvent safety measures in a way that reduces TuCo, and that TuCo is consistently lower on prompts where these attacks succeed compared to those where they do not. This suggests that attenuating the effect of fine-tuning on model outputs plays a role in the success of such attacks. In summary, TuCo enables the quantitative study of how fine-tuning influences model behavior and safety, and vice versa.

</details>


### [86] [Pipelined Decoder for Efficient Context-Aware Text Generation](https://arxiv.org/abs/2506.23431)

*Zixian Huang, Chenxu Niu, Yu Gu, Gengyang Xiao, Xinwei Huang, Gong Cheng*

**Main category:** cs.CL

**Keywords:** autoregressive models, text generation, pipelined decoder, parallelism, machine learning

**Relevance Score:** 7

**TL;DR:** The paper introduces a novel pipelined decoder architecture for autoregressive models that enables parallel text generation, significantly increasing generation speed while maintaining quality.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the generation speed bottleneck in autoregressive models which limit token generation to a sequential process.

**Method:** The proposed pipelined decoder allows for simultaneous generation of multiple subsequences, generating a new token for each at every time-step, thereby facilitating parallelism.

**Key Contributions:**

	1. Introduction of a pipelined decoder architecture for autoregressive models
	2. Demonstrated significant speed improvements for multiple text generation tasks
	3. Maintained generation quality and memory efficiency during the process.

**Result:** The experiments demonstrated significant speed improvements in text generation across various tasks like question answering and summarization, without compromising on quality or memory use.

**Limitations:** 

**Conclusion:** The pipelined decoder architecture effectively accelerates the text generation process in context-aware tasks, showing promise for future applications.

**Abstract:** As the basis of generative AI, an autoregressive model requires the generation of a new token depending on all the previously generated tokens, which brings high quality but also restricts the model to generate tokens one by one, forming a bottleneck limiting the generation speed. In this paper, we propose a new decoder architecture that efficiently generates text in parallel for context-aware generation tasks. Our proposed pipelined decoder initiates the generation of multiple subsequences simultaneously, and, at each time-step, it generates a new token for each subsequence to realize parallelism. Experiments on multiple text generation tasks, including question answering, text summarization, and keyphrase generation, show that our pipelined decoder significantly improves the generation speed without a significant loss of generation quality or additional memory consumption.

</details>


### [87] [What to Keep and What to Drop: Adaptive Table Filtering Framework](https://arxiv.org/abs/2506.23463)

*Jang Won June*

**Main category:** cs.CL

**Keywords:** large language models, table reasoning, adaptive filtering, table QA, performance enhancement

**Relevance Score:** 9

**TL;DR:** The Adaptive Table Filtering Framework (ATF) enhances large language models' performance on table-based reasoning by reducing irrelevant table content while maintaining performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs face challenges with large tables due to input length limits, impacting their reasoning capabilities on TableQA tasks.

**Method:** ATF is a modular filtering pipeline that uses LLM-generated column descriptions, clustering, and sparse-dense alignment scores to prune uninformative table rows and columns.

**Key Contributions:**

	1. Introduction of the ATF framework for adaptive table filtering.
	2. Significant size reduction of table inputs (about 70%).
	3. Improved performance on TableQA tasks without the need for retraining existing models.

**Result:** ATF reduces table size by approximately 70%, significantly improving performance on out-of-domain TableQA tasks, though it slightly decreases performance on Table Fact Verification tasks.

**Limitations:** Slight performance drops on tasks requiring full-table context, such as Table Fact Verification.

**Conclusion:** ATF demonstrates the ability to effectively balance information retention and minimalism, improving efficiency in large table reasoning.

**Abstract:** Large language models (LLMs) for table-based reasoning often struggle with large tables due to input length limits. We propose ATF (Adaptive Table Filtering Framework), a modular and question-aware filtering pipeline that prunes uninformative columns and rows using LLM-generated column descriptions, clustering, and sparse-dense alignment scores. ATF integrates seamlessly with existing models (e.g., TAPAS, TAPEX) without retraining. Experiments show that ATF reduces table cells by ~70\%, boosting performance on out-of-domain TableQA tasks while causing slight performance drops on Table Fact Verification, where full-table context is more critical. These results highlight ATF's ability to adaptively balance informativeness and minimalism across tasks.

</details>


### [88] [Thought-Augmented Planning for LLM-Powered Interactive Recommender Agent](https://arxiv.org/abs/2506.23485)

*Haocheng Yu, Yaxiong Wu, Hao Wang, Wei Guo, Yong Liu, Yawen Li, Yuyang Ye, Junping Du, Enhong Chen*

**Main category:** cs.CL

**Keywords:** interactive recommendations, large language models, thought-augmented systems

**Relevance Score:** 9

**TL;DR:** The paper introduces TAIRA, a thought-augmented interactive recommender agent that improves user intent management in personalized recommendations through planning and decomposition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of LLM-powered interactive recommendation systems, especially in addressing complex user intents.

**Method:** The proposed TAIRA system uses a multi-agent approach with a manager agent that employs Thought Pattern Distillation to decompose and plan recommendation tasks.

**Key Contributions:**

	1. Introduction of the TAIRA system for interactive recommendations
	2. Use of Thought Pattern Distillation to enhance planning capabilities
	3. Empirical validation showing improved performance across multiple datasets.

**Result:** TAIRA significantly outperforms existing methods across various datasets, showing improved performance particularly on challenging tasks and effectively generalizes to novel tasks.

**Limitations:** 

**Conclusion:** TAIRA's design allows it to better manage complex user intents in interactive recommendations, making it a superior choice compared to current approaches.

**Abstract:** Interactive recommendation is a typical information-seeking task that allows users to interactively express their needs through natural language and obtain personalized recommendations. Large language model-powered (LLM-powered) agents have become a new paradigm in interactive recommendations, effectively capturing users' real-time needs and enhancing personalized experiences. However, due to limited planning and generalization capabilities, existing formulations of LLM-powered interactive recommender agents struggle to effectively address diverse and complex user intents, such as intuitive, unrefined, or occasionally ambiguous requests. To tackle this challenge, we propose a novel thought-augmented interactive recommender agent system (TAIRA) that addresses complex user intents through distilled thought patterns. Specifically, TAIRA is designed as an LLM-powered multi-agent system featuring a manager agent that orchestrates recommendation tasks by decomposing user needs and planning subtasks, with its planning capacity strengthened through Thought Pattern Distillation (TPD), a thought-augmentation method that extracts high-level thoughts from the agent's and human experts' experiences. Moreover, we designed a set of user simulation schemes to generate personalized queries of different difficulties and evaluate the recommendations based on specific datasets. Through comprehensive experiments conducted across multiple datasets, TAIRA exhibits significantly enhanced performance compared to existing methods. Notably, TAIRA shows a greater advantage on more challenging tasks while generalizing effectively on novel tasks, further validating its superiority in managing complex user intents within interactive recommendation systems. The code is publicly available at:https://github.com/Alcein/TAIRA.

</details>


### [89] [Reinforcement Fine-Tuning Enables MLLMs Learning Novel Tasks Stably](https://arxiv.org/abs/2506.23508)

*Zhihao Zhang, Qiaole Dong, Qi Zhang, Jun Zhao, Enyu Zhou, Zhiheng Xi, Senjie Jin, Xiaoran Fan, Yuhao Zhou, Yanwei Fu, Tao Ji, Tao Gui, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Supervised Fine-Tuning, Reinforcement Fine-Tuning, multimodal models, task acquisition, knowledge retention

**Relevance Score:** 8

**TL;DR:** This paper studies the effects of Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) on multimodal large language models, specifically focusing on their impact on task acquisition and prior knowledge retention using a novel jigsaw puzzle task.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To examine how post-training algorithms like SFT and RFT affect the retention of prior knowledge when adapting multimodal large language models to new tasks.

**Method:** We systematically study the behavior of SFT and RFT on the multimodal model Qwen2.5-VL using a new task involving jigsaw puzzles, analyzing their impact through learning dynamics and performance metrics.

**Key Contributions:**

	1. Introduced jigsaw puzzles as a novel training task for multimodal models.
	2. Demonstrated the trade-off between SFT and RFT regarding task learning and knowledge retention.
	3. Provided insights into learning dynamics that explain the effectiveness of RFT in preventing catastrophic forgetting.

**Result:** SFT allows for rapid acquisition of new tasks but causes catastrophic forgetting of previous knowledge, while RFT learns more slowly but maintains prior knowledge, suggesting that the data distribution influences forgetting more than the algorithms themselves.

**Limitations:** The paper is a work in progress, and results may change with further experimentation and analysis.

**Conclusion:** The choice of training approach and data distribution is critical for retaining knowledge in continual learning scenarios in multimodal large language models, particularly highlighting the benefits of RFT.

**Abstract:** Post-training algorithms such as Supervised Fine-Tuning (SFT) and Reinforcement Fine-Tuning (RFT) are widely used to adapt multimodal large language models to downstream tasks. While effective at task adaptation, their impact on prior knowledge remains unclear. In this paper, we introduce jigsaw puzzles as a novel task absent from existing pretraining corpora and systematically study the behavior of SFT and RFT on an open-source multimodal model, Qwen2.5-VL. Our experiments reveal a sharp trade-off: SFT enables rapid task acquisition but leads to catastrophic forgetting, whereas RFT learns more slowly on novel tasks but maintains prior knowledge. We analyze this phenomenon through the lens of learning dynamics, showing that RFT reinforces correct samples that are naturally aligned with the base model's probability landscape, mitigating interference with prior knowledge. Moreover, supervised training on correct RFT-simulated rollouts allows SFT to preserve knowledge while rapidly learning new tasks. These findings suggest that data distribution, rather than algorithmic differences, plays a central role in forgetting, and highlight RFT's potential for stable continual learning in multimodal large language models.

</details>


### [90] [NEU-ESC: A Comprehensive Vietnamese dataset for Educational Sentiment analysis and topic Classification toward multitask learning](https://arxiv.org/abs/2506.23524)

*Phan Quoc Hung Mai, Quang Hung Nguyen, Phuong Giang Duong, Hong Hanh Nguyen, Nguyen Tuan Long*

**Main category:** cs.CL

**Keywords:** Vietnamese, sentiment analysis, topic classification, educational dataset, BERT

**Relevance Score:** 4

**TL;DR:** Introduction of a new Vietnamese dataset for educational sentiment and topic classification, showing improved performance with multitask learning using BERT.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of relevant datasets and understand students’ opinions in Vietnamese educational contexts, especially concerning student slang.

**Method:** Development of NEU-ESC, a Vietnamese dataset created from university forums, and implementation of multitask learning with encoder-only language models (BERT) for sentiment and topic classification.

**Key Contributions:**

	1. Introduction of the NEU-ESC dataset for educational sentiment analysis in Vietnamese.
	2. Demonstration of effective multitask learning using BERT for sentiment and topic classification.
	3. Public availability of the dataset for further research.

**Result:** Achieved performance accuracy of 83.7% for sentiment classification and 79.8% for topic classification using the new dataset.

**Limitations:** 

**Conclusion:** The NEU-ESC dataset provides a more relevant and diverse resource for educational sentiment analysis in Vietnamese, with effective model performance evidenced through benchmarking.

**Abstract:** In the field of education, understanding students' opinions through their comments is crucial, especially in the Vietnamese language, where resources remain limited. Existing educational datasets often lack domain relevance and student slang. To address these gaps, we introduce NEU-ESC, a new Vietnamese dataset for Educational Sentiment Classification and Topic Classification, curated from university forums, which offers more samples, richer class diversity, longer texts, and broader vocabulary. In addition, we explore multitask learning using encoder-only language models (BERT), in which we showed that it achieves performance up to 83.7% and 79.8% accuracy for sentiment and topic classification tasks. We also benchmark our dataset and model with other datasets and models, including Large Language Models, and discuss these benchmarks. The dataset is publicly available at: https://huggingface.co/datasets/hung20gg/NEU-ESC.

</details>


### [91] [On Recipe Memorization and Creativity in Large Language Models: Is Your Model a Creative Cook, a Bad Cook, or Merely a Plagiator?](https://arxiv.org/abs/2506.23527)

*Jan Kvapil, Martin Fajcik*

**Main category:** cs.CL

**Keywords:** large language models, recipe generation, creativity, memorization, nonsense detection

**Relevance Score:** 8

**TL;DR:** This study analyzes memorization, creativity, and nonsensical elements in cooking recipes generated by LLMs, focusing on automating human annotation for scalability.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the extent of memorization, creativity, and nonsense in LLM-generated cooking recipes and to develop automated approaches for large-scale assessment.

**Method:** Conducted detailed human annotations on 20 LLM-generated recipes, analyzing ingredients and steps to identify memorized content versus creative synthesis, and developed an 'LLM-as-judge' pipeline for automation.

**Key Contributions:**

	1. Detailed analysis of LLM-generated recipe elements
	2. Development of an automated framework for annotation
	3. Insights into the balance of memorization and creativity in LLMs

**Result:** The study found that the LLM Mixtral relies heavily on memorized content from online sources. The automated framework designed achieved up to 78% accuracy in ingredient matching, demonstrating potential for large-scale analysis.

**Limitations:** Limited to 20 preselected recipes which may not represent the full range of LLM capabilities.

**Conclusion:** The research provides evidence of mixing memorization with creativity in LLM outputs and offers a viable method for automating large-scale evaluations in recipe generation.

**Abstract:** This work-in-progress investigates the memorization, creativity, and nonsense found in cooking recipes generated from Large Language Models (LLMs). Precisely, we aim (i) to analyze memorization, creativity, and non-sense in LLMs using a small, high-quality set of human judgments and (ii) to evaluate potential approaches to automate such a human annotation in order to scale our study to hundreds of recipes. To achieve (i), we conduct a detailed human annotation on 20 preselected recipes generated by LLM (Mixtral), extracting each recipe's ingredients and step-by-step actions to assess which elements are memorized--i.e., directly traceable to online sources possibly seen during training--and which arise from genuine creative synthesis or outright nonsense. We find that Mixtral consistently reuses ingredients that can be found in online documents, potentially seen during model training, suggesting strong reliance on memorized content. To achieve aim (ii) and scale our analysis beyond small sample sizes and single LLM validation, we design an ``LLM-as-judge'' pipeline that automates recipe generation, nonsense detection, parsing ingredients and recipe steps, and their annotation. For instance, comparing its output against human annotations, the best ingredient extractor and annotator is Llama 3.1+Gemma 2 9B, achieving up to 78% accuracy on ingredient matching. This automated framework enables large-scale quantification of memorization, creativity, and nonsense in generated recipes, providing rigorous evidence of the models' creative capacities.

</details>


### [92] [Semantic-guided Diverse Decoding for Large Language Model](https://arxiv.org/abs/2506.23601)

*Weijie Shi, Yue Cui, Yaguang Wu, Jingzhi Fang, Shibo Zhang, Mengze Li, Sirui Han, Jia Zhu, Jiajie Xu, Xiaofang Zhou*

**Main category:** cs.CL

**Keywords:** Semantic diversity, Large language models, Decoding strategies, Natural language processing

**Relevance Score:** 9

**TL;DR:** Introducing Semantic-guided Diverse Decoding (SemDiD) which enhances semantic diversity in responses generated by large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods for achieving diversity in language model outputs mostly focus on lexical rather than semantic differences, limiting their effectiveness in various applications.

**Method:** SemDiD operates in embedding space and employs three mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment to balance quality and diversity.

**Key Contributions:**

	1. Introduction of a novel decoding method that focuses on semantic diversity.
	2. Demonstrated improvements in RLHF training efficiency and response accuracy.
	3. Utilization of embedding space for direct semantic guidance.

**Result:** SemDiD outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% and accelerating RLHF training convergence by 15%, with a potential accuracy increase of up to 2.1%.

**Limitations:** 

**Conclusion:** The proposed method demonstrates a significant improvement in generating semantically diverse responses while maintaining output quality.

**Abstract:** Diverse decoding of large language models is crucial for applications requiring multiple semantically distinct responses, yet existing methods primarily achieve lexical rather than semantic diversity. This limitation significantly constrains Best-of-N strategies, group-based reinforcement learning, and data synthesis. While temperature sampling and diverse beam search modify token distributions or apply n-gram penalties, they fail to ensure meaningful semantic differentiation. We introduce Semantic-guided Diverse Decoding (SemDiD), operating directly in embedding space that balances quality with diversity through three complementary mechanisms: orthogonal directional guidance, dynamic inter-group repulsion, and position-debiased probability assessment. SemDiD harmonizes these competing objectives using adaptive gain functions and constraint optimization, ensuring both quality thresholds and maximal semantic differentiation. Experiments show SemDiD consistently outperforms existing methods, improving Best-of-N coverage by 1.4-5.2% across diverse tasks and accelerating RLHF training convergence by 15% while increasing accuracy by up to 2.1%.

</details>


### [93] [Evaluating the Simulation of Human Personality-Driven Susceptibility to Misinformation with LLMs](https://arxiv.org/abs/2506.23610)

*Manuel Pratelli, Marinella Petrocchi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personality Traits, Misinformation, Behavioral Simulation, Cognitive Diversity

**Relevance Score:** 8

**TL;DR:** This paper evaluates the ability of LLMs to simulate personality-based variations in susceptibility to misinformation as influenced by the Big-Five personality traits.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether LLMs can ethically and cost-effectively generate synthetic behavioral data that reflects psychological differences driven by personality traits.

**Method:** The study conditions LLM agents on Big-Five personality profiles and compares their judgments of headline accuracy to human participants with similar profiles, using existing datasets.

**Key Contributions:**

	1. Demonstrated the capacity of LLMs to reflect personality traits in misinformation susceptibility.
	2. Identified systematic biases in LLMs regarding personality expression.
	3. Provided new insights into behavioral simulation using personality-aligned LLMs.

**Result:** Certain personality traits like Agreeableness and Conscientiousness show reliable associations with susceptibility to misinformation in the simulated LLM agents, while others exhibit discrepancies, indicating biases in LLM behavior.

**Limitations:** The study may not account for all factors influencing how LLMs simulate personality, and discrepancies in trait representation might affect generalizability.

**Conclusion:** The findings highlight both the potential of personality-aligned LLMs in behavioral simulations and their inherent limitations, providing insights into cognitive diversity in artificial agents.

**Abstract:** Large language models (LLMs) make it possible to generate synthetic behavioural data at scale, offering an ethical and low-cost alternative to human experiments. Whether such data can faithfully capture psychological differences driven by personality traits, however, remains an open question. We evaluate the capacity of LLM agents, conditioned on Big-Five profiles, to reproduce personality-based variation in susceptibility to misinformation, focusing on news discernment, the ability to judge true headlines as true and false headlines as false. Leveraging published datasets in which human participants with known personality profiles rated headline accuracy, we create matching LLM agents and compare their responses to the original human patterns. Certain trait-misinformation associations, notably those involving Agreeableness and Conscientiousness, are reliably replicated, whereas others diverge, revealing systematic biases in how LLMs internalize and express personality. The results underscore both the promise and the limits of personality-aligned LLMs for behavioral simulation, and offer new insight into modeling cognitive diversity in artificial agents.

</details>


### [94] [Robustness of Misinformation Classification Systems to Adversarial Examples Through BeamAttack](https://arxiv.org/abs/2506.23661)

*Arnisa Fazla, Lucas Krauter, David Guzman Piedrahita, Andrianos Michail*

**Main category:** cs.CL

**Keywords:** adversarial attack, text classification, BeamAttack, LIME, robustness evaluation

**Relevance Score:** 6

**TL;DR:** The paper extends the BeamAttack adversarial attack algorithm for text classification by adding features for word deletions and substitution skipping, achieving high success rates while maintaining text similarity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the robustness of text classification systems and improve adversarial attack methodologies.

**Method:** The extended BeamAttack algorithm incorporates word deletions, omission of certain substitutions, and utilizes LIME to enhance word replacement prioritization.

**Key Contributions:**

	1. Extension of BeamAttack to include word deletions.
	2. Integration of LIME for improved word replacement prioritization.
	3. Demonstration of effectiveness across multiple datasets and models.

**Result:** Achieves over 99% attack success rates across various datasets and models while preserving the semantic and lexical integrity of the texts.

**Limitations:** Some limitations are discussed but not specified in detail in the abstract.

**Conclusion:** The extended BeamAttack is effective for creating minimal modifications that alter model predictions, although some limitations exist which are discussed.

**Abstract:** We extend BeamAttack, an adversarial attack algorithm designed to evaluate the robustness of text classification systems through word-level modifications guided by beam search. Our extensions include support for word deletions and the option to skip substitutions, enabling the discovery of minimal modifications that alter model predictions. We also integrate LIME to better prioritize word replacements. Evaluated across multiple datasets and victim models (BiLSTM, BERT, and adversarially trained RoBERTa) within the BODEGA framework, our approach achieves over a 99\% attack success rate while preserving the semantic and lexical similarity of the original texts. Through both quantitative and qualitative analysis, we highlight BeamAttack's effectiveness and its limitations. Our implementation is available at https://github.com/LucK1Y/BeamAttack

</details>


### [95] [Zero-Shot Contextual Embeddings via Offline Synthetic Corpus Generation](https://arxiv.org/abs/2506.23662)

*Philip Lippmann, Jie Yang*

**Main category:** cs.CL

**Keywords:** context-aware embeddings, synthetic corpus, zero-shot adaptation, domain adaptation, MTEB benchmark

**Relevance Score:** 8

**TL;DR:** ZEST is a zero-shot contextual adaptation framework that enables domain-adapted embeddings without real corpus access or finetuning, utilizing a synthetic context corpus generated from exemplar documents.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for context-aware embedding methods often requires access to specific corpora or extensive finetuning, which can be impractical in privacy-sensitive or resource-constrained environments.

**Method:** ZEST creates a synthetic context corpus from a small number of representative documents using a multi-step hierarchical procedure, producing domain-adapted embeddings during inference without needing further retraining or access to the target corpus.

**Key Contributions:**

	1. Introduction of a zero-shot contextual adaptation framework (ZEST).
	2. Demonstrates the efficacy of synthetic context corpus across benchmarks with minimal input.
	3. Enables practical high-performance embedding solutions in sensitive environments.

**Result:** ZEST performs within 0.5% of traditional models using full target corpus access on the MTEB benchmark, demonstrating its efficacy in producing high-quality embeddings in constrained settings.

**Limitations:** Performance may be sensitive to the quality of the handful of exemplar documents used for synthesis.

**Conclusion:** ZEST provides an effective alternative for generating adaptable embeddings in situations where accessing target corpora is challenging or impossible.

**Abstract:** Context-aware embedding methods boost retrieval accuracy by conditioning on corpus statistics (e.g., term co-occurrence and topical patterns) extracted from neighboring documents. However, this context-aware approach requires access to the target corpus or requires domain-specific finetuning, posing practical barriers in privacy-sensitive or resource-constrained settings. We present ZEST, a zero-shot contextual adaptation framework that replaces real corpus access with a one-time offline synthesis of a compact proxy. Given only a handful exemplar documents representative of the general target domain, we use a multi-step hierarchical procedure to generate a synthetic context corpus of several hundred documents that aims to emulate key domain-specific distributions. At inference, the frozen context-aware encoder uses this proxy corpus -- without any finetuning or target corpus access -- to produce domain-adapted embeddings. Across the MTEB benchmark, ZEST's zero-shot synthetic context adaptation using only five example documents performs within 0.5% of models leveraging full target corpus access -- demonstrating remarkable efficacy without any retraining. ZEST thus provides a practical method for deploying high-performance, adaptable embeddings in constrained environments.

</details>


### [96] [L0: Reinforcement Learning to Become General Agents](https://arxiv.org/abs/2506.23667)

*Junjie Zhang, Jingyi Xi, Zhuoyang Song, Junyu Lu, Yuhua Ke, Ting Sun, Yukun Yang, Jiaxing Zhang, Songxin Zhang, Zejian Xie*

**Main category:** cs.CL

**Keywords:** large language models, autonomous agents, reinforcement learning

**Relevance Score:** 8

**TL;DR:** L-Zero (L0) is a scalable training pipeline for large language models (LLMs) that facilitates training autonomous agents using reinforcement learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve scalability and training efficiency for LLMs acting as autonomous agents in multi-turn, long-horizon tasks.

**Method:** L0 features a concurrent agent worker pool and uses a 'code-as-action' approach with the NB-Agent operating through a Read-Eval-Print-Loop (REPL).

**Key Contributions:**

	1. Introduction of the L-Zero (L0) training pipeline.
	2. Development of NB-Agent using 'code-as-action'.
	3. Open-sourcing the entire system and training recipes.

**Result:** L0 significantly enhances the problem-solving skills of a base model, achieving accuracy improvements on factuality question-answering benchmarks (SimpleQA from 30% to 80%, HotpotQA from 22% to 41%).

**Limitations:** 

**Conclusion:** The open-sourced L0 system demonstrates its viability for training general-purpose agents in complex environments using Reinforcement Learning with Verifiable Rewards.

**Abstract:** Training large language models (LLMs) to act as autonomous agents for multi-turn, long-horizon tasks remains significant challenges in scalability and training efficiency. To address this, we introduce L-Zero (L0), a scalable, end-to-end training pipeline for general-purpose agents. Featuring a low-cost, extensible, and sandboxed concurrent agent worker pool, L0 lowers the barrier for applying reinforcement learning in complex environments. We also introduce NB-Agent, the agent scaffold within L0, which operates in a "code-as-action" fashion via a Read-Eval-Print-Loop (REPL). We evaluate L0 on factuality question-answering benchmarks. Our experiments demonstrate that a base model can develop robust problem-solving skills using solely Reinforcement Learning with Verifiable Rewards (RLVR). On the Qwen2.5-7B-Instruct model, our method boosts accuracy on SimpleQA from 30 % to 80 % and on HotpotQA from 22 % to 41 %. We have open-sourced the entire L0 system, including our L0 series models, the NB-Agent, a complete training pipeline, and the corresponding training recipes on (https://github.com/cmriat/l0).

</details>


### [97] [AutoEvoEval: An Automated Framework for Evolving Close-Ended LLM Evaluation Data](https://arxiv.org/abs/2506.23735)

*JiaRu Wu, Mingwei Liu*

**Main category:** cs.CL

**Keywords:** Large language models, evaluation benchmarks, robustness analysis, evolution-based evaluation, adversarial testing

**Relevance Score:** 8

**TL;DR:** Introducing AutoEvoEval, an evolution-based framework for evaluating LLM robustness through diverse test sample generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM evaluation benchmarks are insufficient for assessing robustness and generalization, especially under realistic perturbations.

**Method:** AutoEvoEval employs 22 interpretable atomic evolution operations and supports multi-round compositions to generate challenging test samples.

**Key Contributions:**

	1. Proposed AutoEvoEval framework for LLM evaluation using atomic evolution operations.
	2. Introduced multi-round compositions for generating realistic perturbations.
	3. Demonstrated significant variances in model robustness against different perturbations.

**Result:** Experiments reveal that atomic operations lead to an average accuracy drop of 7.283%, with significant model sensitivity variations and adversarial effects amplifying by up to 52.932%.

**Limitations:** The framework focuses specifically on close-ended tasks, limiting its applicability in other domains.

**Conclusion:** The findings highlight the ineffectiveness of static benchmarks and the importance of evolution-aware evaluations for accurate model assessments.

**Abstract:** Large language models (LLMs) have shown remarkable performance on various tasks, but existing evaluation benchmarks are often static and insufficient to fully assess their robustness and generalization in realistic scenarios. Prior work using evolutionary or adversarial data augmentation has improved evaluation diversity but lacks systematic control over perturbation types and multi-step complexity, limiting comprehensive robustness analysis. To address these gaps, we propose AutoEvoEval, an evolution-based evaluation framework for close-ended tasks such as multi-choice question answering. AutoEvoEval introduces 22 interpretable atomic evolution operations and supports multi-round compositions, enabling controlled generation of diverse, challenging, and realistic test samples. We conduct extensive experiments addressing four research questions on a broad set of open- and closed-source LLMs. Our results show that atomic operations cause an average accuracy drop of 7.283\%, with structure-disrupting or misleading semantic edits causing the largest declines. Model sensitivities vary significantly for the same perturbation, and combining multiple evolution steps amplifies adversarial effects by up to 52.932\%. These findings suggest current benchmarks may overestimate true model generalization and emphasize the need for evolution-aware robustness evaluation. Code and resources are available at: https://github.com/SYSUSELab/AutoEvoEval.

</details>


### [98] [Positional Bias in Binary Question Answering: How Uncertainty Shapes Model Preferences](https://arxiv.org/abs/2506.23743)

*Tiziano Labruna, Simone Gallo, Giovanni Da San Martino*

**Main category:** cs.CL

**Keywords:** positional bias, binary question answering, large language models, answer uncertainty, fairness

**Relevance Score:** 7

**TL;DR:** The study investigates positional bias in binary question answering across large language models, examining how bias correlates with answer uncertainty.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how positional bias affects model performance in binary question answering scenarios and its dependence on answer uncertainty.

**Method:** The SQuAD-it dataset was modified to include extra incorrect answers, generating versions with varying context quality and option placement. The study also evaluated existing benchmarks under different uncertainty conditions.

**Key Contributions:**

	1. Quantitative analysis of positional bias in language models
	2. Creation of datasets to study varying contextual uncertainty
	3. Evaluation of positional bias using different real-world benchmarks

**Result:** Positional bias was found to be minimal in low-uncertainty scenarios but increased significantly as uncertainty about the correct answer grew, indicating a systematic preference based on answer positioning.

**Limitations:** The study may not cover all possible contexts and biases in different applications of language models.

**Conclusion:** The findings highlight the need to consider positional bias, especially in situations with ambiguous answers, affecting fairness in model predictions.

**Abstract:** Positional bias in binary question answering occurs when a model systematically favors one choice over another based solely on the ordering of presented options. In this study, we quantify and analyze positional bias across five large language models under varying degrees of answer uncertainty. We re-adapted the SQuAD-it dataset by adding an extra incorrect answer option and then created multiple versions with progressively less context and more out-of-context answers, yielding datasets that range from low to high uncertainty. Additionally, we evaluate two naturally higher-uncertainty benchmarks: (1) WebGPT - question pairs with unequal human-assigned quality scores, and (2) Winning Arguments - where models predict the more persuasive argument in Reddit's r/ChangeMyView exchanges. Across each dataset, the order of the "correct" (or higher-quality/persuasive) option is systematically flipped (first placed in position 1, then in position 2) to compute both Preference Fairness and Position Consistency. We observe that positional bias is nearly absent under low-uncertainty conditions, but grows exponentially when it becomes doubtful to decide which option is correct.

</details>


### [99] [Do Thinking Tokens Help or Trap? Towards More Efficient Large Reasoning Model](https://arxiv.org/abs/2506.23840)

*Bowen Ding, Yuhan Chen, Futing Wang, Lingfeng Ming, Tao Lin*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, thinking tokens, Dual Policy Preference Optimization, token efficiency, math reasoning benchmarks

**Relevance Score:** 7

**TL;DR:** This paper addresses the inefficiencies of Large Reasoning Models (LRMs) caused by excessive use of thinking tokens, presenting a novel algorithm, Dual Policy Preference Optimization (DuP-PO), to enhance token efficiency and reasoning performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the inefficiencies of LRMs when processing simple tasks, specifically focusing on the overuse of thinking tokens that lead to unnecessary high-level reasoning behaviors.

**Method:** The paper introduces Dual Policy Preference Optimization (DuP-PO), which employs a rollout sampling strategy for balanced exposure to responses, an advantage control technique for dynamic token prediction regulation, and a policy shaping method for stable gradient contributions.

**Key Contributions:**

	1. Introduction of the thinking trap phenomenon
	2. Development of the Dual Policy Preference Optimization (DuP-PO) algorithm
	3. Empirical validation of DuP-PO's effectiveness on math reasoning benchmarks.

**Result:** DuP-PO demonstrates significant improvements in token efficiency and overall reasoning performance on five popular math reasoning benchmarks compared to the base model.

**Limitations:** 

**Conclusion:** The results indicate that reducing unnecessary thinking tokens through DuP-PO can enhance both efficiency and effectiveness in problem-solving for LRMs.

**Abstract:** Large Reasoning Models (LRMs) excel at solving complex problems but face an overthinking dilemma. When handling simple tasks, they often produce verbose responses overloaded with thinking tokens (e.g., wait, however). These tokens trigger unnecessary high-level reasoning behaviors like reflection and backtracking, reducing efficiency. In this work, our pilot study reveals that these thinking-token-induced behaviors are not essential for effective problem-solving and may even hinder correct reasoning within constrained token budgets. We identify this phenomenon as the thinking trap. To mitigate this issue, we propose Dual Policy Preference Optimization (DuP-PO), a novel algorithm featuring: (1) A rollout sampling strategy that guarantees balanced exposure to responses with and without thinking tokens; (2) A fine-grained advantage control technique to dynamically regulate the prediction of target tokens; (3) A policy shaping method ensuring stable gradient contributions from thinking tokens. Experimental results on five popular math reasoning benchmarks show that DuP-PO performs well on the popular LRM, which significantly improves their token efficiency during reasoning, while achieving superior performance of the base model.

</details>


### [100] [Garbage In, Reasoning Out? Why Benchmark Scores are Unreliable and What to Do About It](https://arxiv.org/abs/2506.23864)

*Seyed Mahed Mousavi, Edoardo Cecchinato, Lucia Hornikova, Giuseppe Riccardi*

**Main category:** cs.CL

**Keywords:** reasoning benchmarks, large language models, evaluation methodology

**Relevance Score:** 9

**TL;DR:** This paper audits widely used reasoning benchmarks and uncovers flaws in their design and evaluation, revealing that model scores may reflect alignment with format-specific cues rather than actual reasoning abilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address flaws in existing benchmarks that assess reasoning in large language models (LLMs).

**Method:** A systematic audit of three reasoning benchmarks using five LLMs to identify design flaws and evaluate model performance through human annotation and re-evaluation on cleaned subsets.

**Key Contributions:**

	1. Identification of pervasive flaws in reasoning benchmarks
	2. Demonstration of sensitivity of model performance to minor input variations
	3. Provision of audited data and evaluation tools for better reasoning assessments

**Result:** The study reveals structural, semantic, and pragmatic issues in benchmark design that lead to misleading model performance scores, which often improve due to superficial factors rather than genuine reasoning ability.

**Limitations:** Focuses on a limited number of LLMs and benchmarks; may not generalize to all models or tasks.

**Conclusion:** The findings challenge the validity of benchmark-based claims about LLM reasoning and call for improved evaluation protocols that focus on reasoning processes rather than static outputs.

**Abstract:** We conduct a systematic audit of three widely used reasoning benchmarks, SocialIQa, FauxPas-EAI, and ToMi, and uncover pervasive flaws in both benchmark items and evaluation methodology. Using five LLMs (GPT-{3, 3.5, 4, o1}, and LLaMA 3.1) as diagnostic tools, we identify structural, semantic, and pragmatic issues in benchmark design (e.g., duplicated items, ambiguous wording, and implausible answers), as well as scoring procedures that prioritize output form over reasoning process. Through systematic human annotation and re-evaluation on cleaned benchmark subsets, we find that model scores often improve not due to due to erratic surface wording variations and not to improved reasoning. Infact, further analyses show that model performance is highly sensitive to minor input variations such as context availability and phrasing, revealing that high scores may reflect alignment with format-specific cues rather than consistent inference based on the input. These findings challenge the validity of current benchmark-based claims about reasoning in LLMs, and highlight the need for evaluation protocols that assess reasoning as a process of drawing inference from available information, rather than as static output selection. We release audited data and evaluation tools to support more interpretable and diagnostic assessments of model reasoning.

</details>


### [101] [Advancing Multi-Step Mathematical Reasoning in Large Language Models through Multi-Layered Self-Reflection with Auto-Prompting](https://arxiv.org/abs/2506.23888)

*André de Souza Loureiro, Jorge Valverde-Rebaza, Julieta Noguez, David Escarcega, Ricardo Marcacini*

**Main category:** cs.CL

**Keywords:** Large Language Models, Self-Reflection, Multi-step Reasoning, Auto-Prompting, Chain of Thought

**Relevance Score:** 9

**TL;DR:** MAPS framework enhances LLMs' multi-step reasoning by integrating CoT, Self-Reflection, and Auto-Prompting for iterative refinement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of LLMs in performing complex multi-step reasoning tasks.

**Method:** The MAPS framework utilizes an iterative refinement process involving Chain of Thought prompting, an adaptive self-reflection mechanism, and dynamic prompt adjustments.

**Key Contributions:**

	1. Introduction of MAPS framework for iterative reasoning improvements
	2. Demonstration of MAPS's effectiveness on multiple benchmarks
	3. Balanced approach to reflection depth for cost efficiency

**Result:** Experiments show that MAPS outperforms standard CoT prompting and competes with specialized reasoning models across multiple benchmarks.

**Limitations:** Deeper reflection layers increase token usage and costs, necessitating a balance.

**Conclusion:** While deeper reflection improves accuracy, it raises costs; MAPS manages this with strategic depth limitation for optimal performance versus cost.

**Abstract:** Recent advancements in Large Language Models (LLMs) have significantly improved their problem-solving capabilities. However, these models still struggle when faced with complex multi-step reasoning tasks. In this paper, we propose the Multi-Layered Self-Reflection with Auto-Prompting (MAPS) framework, a novel approach designed to enhance multi-step mathematical reasoning in LLMs by integrating techniques such as Chain of Thought (CoT), Self-Reflection, and Auto-Prompting. Unlike traditional static prompting methods, MAPS employs an iterative refinement process. Initially, the model generates a solution using CoT prompting. When errors are detected, an adaptive self-reflection mechanism identifies and analyzes them, generating tailored prompts to guide corrections. These dynamically adjusted prompts enable the model to iteratively refine its reasoning. Experiments on four well-established benchmarks across multiple LLMs show that MAPS significantly outperforms standard CoT and achieves competitive results with reasoning-optimized models. In addition, MAPS enables general-purpose LLMs to reach performance levels comparable to specialized reasoning models. While deeper reflection layers improve accuracy, they also increase token usage and costs. To balance this trade-off, MAPS strategically limits reflection depth, ensuring an optimal balance between cost and reasoning performance.

</details>


### [102] [The Trilemma of Truth in Large Language Models](https://arxiv.org/abs/2506.23921)

*Germans Savcisens, Tina Eliassi-Rad*

**Main category:** cs.CL

**Keywords:** Large Language Models, Veracity Assessment, Multiple-Instance Learning, Conformal Prediction, Internal Activations

**Relevance Score:** 9

**TL;DR:** This paper introduces sAwMIL, a new method for probing the veracity of large language models (LLMs), addressing flaws in existing probing methods by utilizing LLM internal activations to separate statements into true, false, and neither.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to better assess the veracity of knowledge retained by LLMs, challenging current probing methods that have flawed assumptions.

**Method:** sAwMIL uses multiple-instance learning and conformal prediction on the internal activations of LLMs to categorize statements.

**Key Contributions:**

	1. Introduction of sAwMIL for probing LLMs
	2. Analysis of depth-related veracity signal concentration
	3. Insights into truth/falsehood asymmetry and necessity of nonlinear probes

**Result:** Evaluated on 5 criteria across 16 LLMs and 3 new datasets, findings indicate that the veracity signals concentrate at specific depths with asymmetrical truth and falsehood signals, highlighting the need for nonlinear probes for certain models.

**Limitations:** The method may not apply to all types of LLMs, especially those not covered in the evaluation.

**Conclusion:** The method offers a reliable way to determine what LLMs 'know' and to gauge their certainty in probabilistic knowledge representation.

**Abstract:** We often attribute human characteristics to large language models (LLMs) and claim that they "know" certain things. LLMs have an internal probabilistic knowledge that represents information retained during training. How can we assess the veracity of this knowledge? We examine two common methods for probing the veracity of LLMs and discover several assumptions that are flawed. To address these flawed assumptions, we introduce sAwMIL (short for Sparse Aware Multiple-Instance Learning), a probing method that utilizes the internal activations of LLMs to separate statements into true, false, and neither. sAwMIL is based on multiple-instance learning and conformal prediction. We evaluate sAwMIL on 5 validity criteria across 16 open-source LLMs, including both default and chat-based variants, as well as on 3 new datasets. Among the insights we provide are: (1) the veracity signal is often concentrated in the third quarter of an LLM's depth; (2) truth and falsehood signals are not always symmetric; (3) linear probes perform better on chat models than on default models; (4) nonlinear probes may be required to capture veracity signals for some LLMs with reinforcement learning from human feedback or knowledge distillation; and (5) LLMs capture a third type of signal that is distinct from true and false and is neither true nor false. These findings provide a reliable method for verifying what LLMs "know" and how certain they are of their probabilistic internal knowledge.

</details>


### [103] [IMPACT: Inflectional Morphology Probes Across Complex Typologies](https://arxiv.org/abs/2506.23929)

*Mohammed J. Saeed, Tommi Vehvilainen, Evgeny Fedoseev, Sevil Caliskan, Tatiana Vodolazova*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inflectional Morphology, Evaluation Framework, Multilingual NLP, Linguistic Complexity

**Relevance Score:** 6

**TL;DR:** The paper introduces IMPACT, an evaluation framework assessing LLM performance in inflectional morphology across five languages, highlighting gaps in LLMs' linguistic understanding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the extent to which LLMs grasp linguistic complexities in morphologically rich languages despite their overall performance.

**Method:** The IMPACT framework includes unit-test-style evaluations for Arabic, Russian, Finnish, Turkish, and Hebrew, focusing on various morphological phenomena.

**Key Contributions:**

	1. Introduction of the IMPACT framework for LLM evaluation
	2. Assessment of eight multilingual LLMs on morphology
	3. Public release of the IMPACT evaluation methods

**Result:** The framework reveals that while LLMs perform well in English, they struggle significantly with inflectional morphology in other languages and with ungrammatical constructions.

**Limitations:** The framework may not cover all linguistic phenomena and is limited to five specific languages.

**Conclusion:** The findings indicate marked deficiencies in LLMs' handling of linguistic intricacies, emphasizing the need for improvement in multilingual and morphological contexts.

**Abstract:** Large Language Models (LLMs) have shown significant progress on various multilingual benchmarks and are increasingly used to generate and evaluate text in non-English languages. However, while they may produce fluent outputs, it remains unclear to what extent these models truly grasp the underlying linguistic complexity of those languages, particularly in morphology. To investigate this, we introduce IMPACT, a synthetically generated evaluation framework focused on inflectional morphology, which we publicly release, designed to evaluate LLM performance across five morphologically rich languages: Arabic, Russian, Finnish, Turkish, and Hebrew. IMPACT includes unit-test-style cases covering both shared and language-specific phenomena, from basic verb inflections (e.g., tense, number, gender) to unique features like Arabic's reverse gender agreement and vowel harmony in Finnish and Turkish. We assess eight multilingual LLMs that, despite strong English performance, struggle with other languages and uncommon morphological patterns, especially when judging ungrammatical examples. We also show that Chain of Thought and Thinking Models can degrade performance. Our work exposes gaps in LLMs' handling of linguistic complexity, pointing to clear room for improvement. To support further research, we publicly release the IMPACT framework.

</details>


### [104] [Leveraging the Potential of Prompt Engineering for Hate Speech Detection in Low-Resource Languages](https://arxiv.org/abs/2506.23930)

*Ruhina Tabasshum Prome, Tarikul Islam Tamiti, Anomadarshi Barua*

**Main category:** cs.CL

**Keywords:** hate speech detection, prompt engineering, low-resource languages, Bengali, metaphor prompting

**Relevance Score:** 7

**TL;DR:** This paper explores prompt engineering techniques for hate speech detection in low-resource languages, particularly Bengali, using large language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of hate speech on social media necessitates effective detection methods, especially for low-resource languages lacking quality datasets.

**Method:** The study employs six prompting strategies on the Llama2-7B model, including zero-shot prompting, refusal suppression, flattering the classifier, multi-shot prompting, role prompting, and metaphor prompting, comparing their effectiveness with various deep learning models and pre-trained word embeddings.

**Key Contributions:**

	1. Introduction of metaphor prompting for hate speech detection
	2. Evaluation of multiple prompting strategies in low-resource languages
	3. Assessment of environmental impacts of different detection methods

**Result:** The metaphor prompting technique demonstrates significant improvement in detecting hate speech in Bengali and Hindi, compared to high-resource languages like English and German.

**Limitations:** 

**Conclusion:** Innovative prompting strategies, especially metaphor prompting, are effective in enhancing hate speech detection performance in low-resource languages while addressing environmental impact factors.

**Abstract:** The rapid expansion of social media leads to a marked increase in hate speech, which threatens personal lives and results in numerous hate crimes. Detecting hate speech presents several challenges: diverse dialects, frequent code-mixing, and the prevalence of misspelled words in user-generated content on social media platforms. Recent progress in hate speech detection is typically concentrated on high-resource languages. However, low-resource languages still face significant challenges due to the lack of large-scale, high-quality datasets. This paper investigates how we can overcome this limitation via prompt engineering on large language models (LLMs) focusing on low-resource Bengali language. We investigate six prompting strategies - zero-shot prompting, refusal suppression, flattering the classifier, multi-shot prompting, role prompting, and finally our innovative metaphor prompting to detect hate speech effectively in low-resource languages. We pioneer the metaphor prompting to circumvent the built-in safety mechanisms of LLMs that marks a significant departure from existing jailbreaking methods. We investigate all six different prompting strategies on the Llama2-7B model and compare the results extensively with three pre-trained word embeddings - GloVe, Word2Vec, and FastText for three different deep learning models - multilayer perceptron (MLP), convolutional neural network (CNN), and bidirectional gated recurrent unit (BiGRU). To prove the effectiveness of our metaphor prompting in the low-resource Bengali language, we also evaluate it in another low-resource language - Hindi, and two high-resource languages - English and German. The performance of all prompting techniques is evaluated using the F1 score, and environmental impact factor (IF), which measures CO$_2$ emissions, electricity usage, and computational time.

</details>


### [105] [Graft: Integrating the Domain Knowledge via Efficient Parameter Synergy for MLLMs](https://arxiv.org/abs/2506.23940)

*Yang Dai, Jianxiang An, Tianwei Lin, Hongyang He, Hongzhe Huang, Wenqiao Zhang, Zheqi Lv, Siliang Tang, Yueting Zhuang*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, knowledge integration, parameter splicing, domain adaptation, HCI

**Relevance Score:** 7

**TL;DR:** A framework for integrating domain-specific expertise in Multimodal Large Language Models (MLLMs) to enhance performance across varied data inputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmentation of knowledge in domain-specialized MLLMs and improve their applicability to diverse data.

**Method:** Introducing a Compatibility-Aware Parameter Splicing (CAPS) strategy for modular composition of expert capabilities and efficient parameter fusion.

**Key Contributions:**

	1. Unified parameter integration framework for MLLMs
	2. Compatibility-Aware Parameter Splicing (CAPS) strategy
	3. Domain compatibility scoring mechanism

**Result:** The proposed framework demonstrates effective knowledge sharing and improved performance across various multimodal benchmarks with minimal inference overhead.

**Limitations:** 

**Conclusion:** The approach facilitates compositional and domain-adaptive MLLMs, validating the importance of synergizing heterogeneous expertise while maintaining structural modularity.

**Abstract:** Multimodal Large Language Models (MLLMs) have achieved success across various domains. However, their applicability tends to degrade when confronted with different types of data inputs, especially for MLLMs that have been fine-tuned for specific tasks. Despite its importance, the study of knowledge sharing among domain-specific MLLMs--such as those trained for mathematics or code--remains largely underexplored. To address the fragmentation of knowledge across domain-specialized MLLMs, we propose a unified parameter integration framework that enables modular composition of expert capabilities. Our method is grounded in a novel Compatibility-Aware Parameter Splicing (CAPS) strategy, which leverages both local functional attribution and global information-theoretic signals to guide selective parameter fusion. By extending this mechanism to the low-rank adaptation layer granularity, we ensure efficient integration with minimal inference overhead. Furthermore, we introduce a domain compatibility scoring mechanism that quantifies inter-expert alignment at the activation level and correlates with downstream task utility. This principled fusion protocol allows the final model to synergize heterogeneous expertise while preserving structural modularity. Extensive evaluations across diverse multimodal benchmarks validate the effectiveness of our framework, offering a scalable path toward compositional, domain-adaptive MLLMs.

</details>


### [106] [Unveiling Decision-Making in LLMs for Text Classification : Extraction of influential and interpretable concepts with Sparse Autoencoders](https://arxiv.org/abs/2506.23951)

*Mathis Le Bail, Jérémie Dentan, Davide Buscaldi, Sonia Vanier*

**Main category:** cs.CL

**Keywords:** Sparse Autoencoder, Large Language Models, text classification, explainability, interpretability

**Relevance Score:** 9

**TL;DR:** This paper presents a novel Sparse Autoencoder (SAE) architecture for text classification, enhancing explainability by extracting interpretable concepts from large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the effectiveness of SAE-based approaches for text classification, a previously underexplored domain in this context.

**Method:** A specialized SAE architecture for sentence classification was developed, incorporating a classifier head and an activation rate sparsity loss. The architecture was benchmarked against existing methods like ConceptShap and Independent Component Analysis.

**Key Contributions:**

	1. Introduction of a novel SAE architecture for text classification
	2. Benchmarking against established explainability methods
	3. Development of two new metrics for measuring concept explanation precision

**Result:** The experimental results demonstrated that the proposed SAE architecture enhances the causality and interpretability of features extracted from fine-tuned LLMs.

**Limitations:** 

**Conclusion:** The findings suggest that SAE-based explainability approaches can significantly improve the interpretability of sentence classification in large language models.

**Abstract:** Sparse Autoencoders (SAEs) have been successfully used to probe Large Language Models (LLMs) and extract interpretable concepts from their internal representations. These concepts are linear combinations of neuron activations that correspond to human-interpretable features. In this paper, we investigate the effectiveness of SAE-based explainability approaches for sentence classification, a domain where such methods have not been extensively explored. We present a novel SAE-based architecture tailored for text classification, leveraging a specialized classifier head and incorporating an activation rate sparsity loss. We benchmark this architecture against established methods such as ConceptShap, Independent Component Analysis, and other SAE-based concept extraction techniques. Our evaluation covers two classification benchmarks and four fine-tuned LLMs from the Pythia family. We further enrich our analysis with two novel metrics for measuring the precision of concept-based explanations, using an external sentence encoder. Our empirical results show that our architecture improves both the causality and interpretability of the extracted features.

</details>


### [107] [TaP: A Taxonomy-Guided Framework for Automated and Scalable Preference Data Generation](https://arxiv.org/abs/2506.23979)

*Renren Jin, Tianhao Shen, Xinwei Wu, Dan Shi, Haoran Sun, Wuwei Huang, Quandong Wang, Wei Liu, Jian Luan, Bin Wang, Deyi Xiong*

**Main category:** cs.CL

**Keywords:** large language models, dataset generation, preference datasets, multilingual, taxonomy

**Relevance Score:** 9

**TL;DR:** The TaP framework facilitates the automated generation of high-quality multilingual datasets for fine-tuning LLMs, demonstrating superior performance compared to larger existing datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Creating high-quality datasets for fine-tuning large language models is resource-intensive, and existing datasets are predominantly in English, limiting their applicability.

**Method:** The TaP framework uses a structured taxonomy to automate the generation of diverse preference datasets across multiple languages, allowing for scalable dataset construction.

**Key Contributions:**

	1. Automated generation of multilingual preference datasets
	2. Use of a structured taxonomy for dataset diversity
	3. Improved performance of LLMs trained on TaP datasets compared to larger datasets

**Result:** LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets, even exceeding the performance of a dataset 180 times larger.

**Limitations:** The framework's efficiency and effectiveness across less represented languages are yet to be fully analyzed.

**Conclusion:** The TaP framework provides a significant advancement in dataset generation for LLMs, enhancing their ability to align with human values in multiple languages.

**Abstract:** Conducting supervised fine-tuning and preference fine-tuning on large language models (LLMs) requires high-quality datasets to improve their ability to follow instructions and align with human preferences and values. However, constructing such datasets is resource-intensive, and most available datasets for supervised and preference fine-tuning are in English. To address these challenges, we propose the \underline{\textbf{Ta}}xonomy-Guided \underline{\textbf{P}}reference Data Generation (TaP) framework, which facilitates automated and scalable construction of preference datasets across various languages. TaP is grounded in a structured taxonomy that allows fine-grained control over dataset composition, thereby ensuring both diversity and comprehensive coverage. We employ TaP-generated datasets to perform supervised and preference fine-tuning on various LLMs. Experimental results demonstrate that LLMs trained on TaP-generated datasets outperform those trained on existing open-source datasets. Remarkably, LLMs trained on TaP-generated datasets surpass the performance of those trained on an open-source dataset that is 180 times larger.

</details>


### [108] [Machine Understanding of Scientific Language](https://arxiv.org/abs/2506.23990)

*Dustin Wright*

**Main category:** cs.CL

**Keywords:** natural language processing, machine learning, scientific text processing

**Relevance Score:** 8

**TL;DR:** This thesis addresses the automatic identification of faithfulness in scientific text by developing datasets, methods, and tools in NLP and machine learning.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To address the societal importance of identifying the faithfulness of scientific information amidst the growing volume of textual content online.

**Method:** Development of new datasets and methods for NLP focused on automatic fact checking, learning from limited data, and processing scientific texts.

**Key Contributions:**

	1. New methods for automatic fact checking
	2. Adversarial claim generation techniques
	3. Zero-shot scientific fact checking mechanisms

**Result:** Introduction of novel methods and resources for tasks such as identifying check-worthy claims and conducting zero-shot scientific fact checking.

**Limitations:** 

**Conclusion:** The research demonstrates effective approaches for learning from limited scientific texts to identify misinformation and enhance the understanding of science communication.

**Abstract:** Scientific information expresses human understanding of nature. This knowledge is largely disseminated in different forms of text, including scientific papers, news articles, and discourse among people on social media. While important for accelerating our pursuit of knowledge, not all scientific text is faithful to the underlying science. As the volume of this text has burgeoned online in recent years, it has become a problem of societal importance to be able to identify the faithfulness of a given piece of scientific text automatically. This thesis is concerned with the cultivation of datasets, methods, and tools for machine understanding of scientific language, in order to analyze and understand science communication at scale. To arrive at this, I present several contributions in three areas of natural language processing and machine learning: automatic fact checking, learning with limited data, and scientific text processing. These contributions include new methods and resources for identifying check-worthy claims, adversarial claim generation, multi-source domain adaptation, learning from crowd-sourced labels, cite-worthiness detection, zero-shot scientific fact checking, detecting exaggerated scientific claims, and modeling degrees of information change in science communication. Critically, I demonstrate how the research outputs of this thesis are useful for effectively learning from limited amounts of scientific text in order to identify misinformative scientific statements and generate new insights into the science communication process

</details>


### [109] [Auto-TA: Towards Scalable Automated Thematic Analysis (TA) via Multi-Agent Large Language Models with Reinforcement Learning](https://arxiv.org/abs/2506.23998)

*Seungjun Yi, Joakim Nguyen, Huimin Xu, Terence Lim, Andrew Well, Mia Markey, Ying Ding*

**Main category:** cs.CL

**Keywords:** Congenital Heart Disease, Thematic Analysis, Large Language Models, Reinforcement Learning, Patient-Centered Care

**Relevance Score:** 8

**TL;DR:** A proposed automated LLM pipeline for thematic analysis of clinical narratives in CHD, utilizing a multi-agent framework and optionally incorporating RLHF for improved patient-centered analysis.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of manual thematic analysis in clinical narratives related to congenital heart disease by providing a scalable and efficient solution.

**Method:** A fully automated large language model pipeline utilizing a multi-agent framework to perform end-to-end thematic analysis, with an optional reinforcement learning component.

**Key Contributions:**

	1. Development of a fully automated LLM pipeline for thematic analysis.
	2. Implementation of a multi-agent framework for enhanced analysis.
	3. Integration of RLHF for improved thematic relevance.

**Result:** The proposed system improves the quality of theme analysis in patient and caregiver narratives and allows for the analysis of large qualitative datasets without manual intervention.

**Limitations:** 

**Conclusion:** This automated approach enhances patient-centered analysis in clinical contexts, making it easier to derive insights from narratives related to congenital heart disease.

**Abstract:** Congenital heart disease (CHD) presents complex, lifelong challenges often underrepresented in traditional clinical metrics. While unstructured narratives offer rich insights into patient and caregiver experiences, manual thematic analysis (TA) remains labor-intensive and unscalable. We propose a fully automated large language model (LLM) pipeline that performs end-to-end TA on clinical narratives, which eliminates the need for manual coding or full transcript review. Our system employs a novel multi-agent framework, where specialized LLM agents assume roles to enhance theme quality and alignment with human analysis. To further improve thematic relevance, we optionally integrate reinforcement learning from human feedback (RLHF). This supports scalable, patient-centered analysis of large qualitative datasets and allows LLMs to be fine-tuned for specific clinical contexts.

</details>


### [110] [Large Language Models Don't Make Sense of Word Problems. A Scoping Review from a Mathematics Education Perspective](https://arxiv.org/abs/2506.24006)

*Anselm R. Strohmaier, Wim Van Dooren, Kathrin Seßler, Brian Greer, Lieven Verschaffel*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematics education, word problems, empirical evaluation, educational tools

**Relevance Score:** 8

**TL;DR:** This paper reviews the integration of Large Language Models (LLMs) in mathematics education, emphasizing their performance in word problem-solving and the disconnect between LLMs' and students' reasoning processes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs like ChatGPT can support mathematics learning, particularly in solving word problems, and to assess their limitations in understanding real-world contexts.

**Method:** The study conducts a scoping review consisting of a technical overview comparing LLMs and student reasoning, a systematic literature review of 213 studies, and an empirical evaluation of several LLMs' performances on 287 mathematical word problems.

**Key Contributions:**

	1. Technical overview contrasting LLMs and students' approaches to word problems.
	2. Literature review identifying gaps in current research on word problems used in studies.
	3. Empirical evaluation showing varying performance of LLMs on mathematical problems based on context.

**Result:** The review reveals that LLMs demonstrate near-perfect accuracy on specific problem types (s-problems) but struggle with word problems that involve complex real-world contexts, highlighting their limitations as instructional tools.

**Limitations:** LLMs do not effectively understand or process the real-world context of certain mathematical problems, limiting their usefulness in educational contexts.

**Conclusion:** LLMs excel in superficial solution processes but fail to genuinely comprehend the context of word problems, which may restrict their effectiveness in educational settings.

**Abstract:** The progress of Large Language Models (LLMs) like ChatGPT raises the question of how they can be integrated into education. One hope is that they can support mathematics learning, including word-problem solving. Since LLMs can handle textual input with ease, they appear well-suited for solving mathematical word problems. Yet their real competence, whether they can make sense of the real-world context, and the implications for classrooms remain unclear. We conducted a scoping review from a mathematics-education perspective, including three parts: a technical overview, a systematic review of word problems used in research, and a state-of-the-art empirical evaluation of LLMs on mathematical word problems. First, in the technical overview, we contrast the conceptualization of word problems and their solution processes between LLMs and students. In computer-science research this is typically labeled mathematical reasoning, a term that does not align with usage in mathematics education. Second, our literature review of 213 studies shows that the most popular word-problem corpora are dominated by s-problems, which do not require a consideration of realities of their real-world context. Finally, our evaluation of GPT-3.5-turbo, GPT-4o-mini, GPT-4.1, and o3 on 287 word problems shows that most recent LLMs solve these s-problems with near-perfect accuracy, including a perfect score on 20 problems from PISA. LLMs still showed weaknesses in tackling problems where the real-world context is problematic or non-sensical. In sum, we argue based on all three aspects that LLMs have mastered a superficial solution process but do not make sense of word problems, which potentially limits their value as instructional tools in mathematics classrooms.

</details>


### [111] [EXPERT: An Explainable Image Captioning Evaluation Metric with Structured Explanations](https://arxiv.org/abs/2506.24016)

*Hyunjong Kim, Sangyeop Kim, Jongheon Jeong, Yeongjae Cho, Sungzoon Cho*

**Main category:** cs.CL

**Keywords:** image captioning, evaluation metrics, explainable AI, vision-language models, natural language processing

**Relevance Score:** 8

**TL;DR:** EXPERT is a new evaluation metric for image captioning that offers structured explanations based on fluency, relevance, and descriptiveness, surpassing existing metrics in quality and verification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of standardized criteria in current evaluation metrics for image captioning and to verify the quality of generated explanations.

**Method:** Developed a two-stage evaluation template and constructed large-scale datasets of high-quality structured explanations to supervise vision-language models for scoring and explanation generation.

**Key Contributions:**

	1. Introduced a reference-free evaluation metric for image captioning.
	2. Established structured criteria for evaluation: fluency, relevance, and descriptiveness.
	3. Achieved state-of-the-art results while providing superior explanation quality.

**Result:** EXPERT achieves state-of-the-art results on benchmark datasets and provides significantly higher-quality explanations verified through comprehensive human evaluation.

**Limitations:** 

**Conclusion:** EXPERT offers a more reliable and structured approach to evaluating image captioning, beneficial for future research and applications.

**Abstract:** Recent advances in large language models and vision-language models have led to growing interest in explainable evaluation metrics for image captioning. However, these metrics generate explanations without standardized criteria, and the overall quality of the generated explanations remains unverified. In this paper, we propose EXPERT, a reference-free evaluation metric that provides structured explanations based on three fundamental criteria: fluency, relevance, and descriptiveness. By constructing large-scale datasets of high-quality structured explanations, we develop a two-stage evaluation template to effectively supervise a vision-language model for both scoring and explanation generation. EXPERT achieves state-of-the-art results on benchmark datasets while providing significantly higher-quality explanations than existing metrics, as validated through comprehensive human evaluation. Our code and datasets are available at https://github.com/hjkim811/EXPERT.

</details>


### [112] [STACK: Adversarial Attacks on LLM Safeguard Pipelines](https://arxiv.org/abs/2506.24068)

*Ian R. McKenzie, Oskar J. Hollinsworth, Tom Tseng, Xander Davies, Stephen Casper, Aaron D. Tucker, Robert Kirk, Adam Gleave*

**Main category:** cs.CL

**Keywords:** AI Defense Pipelines, Catastrophic Misuse, Few-Shot Learning, Red-Teaming, Black-Box Attacks

**Relevance Score:** 8

**TL;DR:** This paper evaluates the security of AI defense pipelines and introduces a novel few-shot prompted classifier that outperforms existing safeguards against misuse, while also analyzing the feasibility of black-box attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to assess and improve the security of AI defense pipelines that are crucial for preventing catastrophic misuse of AI systems, which are becoming standard among frontier AI developers.

**Method:** We developed and red-teamed an open-source defense pipeline, comparing a few-shot-prompted input and output classifier against existing safeguard models and implementing a novel black-box attack method (STACK).

**Key Contributions:**

	1. Development of an open-source defense pipeline
	2. Introduction of a few-shot prompted input/output classifier
	3. Proposing the STaged AttaCK (STACK) procedure for evaluating pipeline vulnerabilities

**Result:** The few-shot-prompted classifier outperformed the ShieldGemma model with an attack success rate of 0% on the ClearHarm dataset, while the STACK procedure achieved 71% ASR in a black-box attack.

**Limitations:** Limited prior work on evaluating or attacking AI defense pipelines.

**Conclusion:** The research concludes that it is possible to devise effective attacks on AI pipeline defenses, and recommends specific mitigations for developers to counter such staged attacks.

**Abstract:** Frontier AI developers are relying on layers of safeguards to protect against catastrophic misuse of AI systems. Anthropic guards their latest Claude 4 Opus model using one such defense pipeline, and other frontier developers including Google DeepMind and OpenAI pledge to soon deploy similar defenses. However, the security of such pipelines is unclear, with limited prior work evaluating or attacking these pipelines. We address this gap by developing and red-teaming an open-source defense pipeline. First, we find that a novel few-shot-prompted input and output classifier outperforms state-of-the-art open-weight safeguard model ShieldGemma across three attacks and two datasets, reducing the attack success rate (ASR) to 0% on the catastrophic misuse dataset ClearHarm. Second, we introduce a STaged AttaCK (STACK) procedure that achieves 71% ASR on ClearHarm in a black-box attack against the few-shot-prompted classifier pipeline. Finally, we also evaluate STACK in a transfer setting, achieving 33% ASR, providing initial evidence that it is feasible to design attacks with no access to the target pipeline. We conclude by suggesting specific mitigations that developers could use to thwart staged attacks.

</details>


### [113] [On the Predictive Power of Representation Dispersion in Language Models](https://arxiv.org/abs/2506.24106)

*Yanhong Li, Ming Li, Karen Livescu, Jiawei Zhou*

**Main category:** cs.CL

**Keywords:** language models, embedding dispersion, perplexity, model selection, training objectives

**Relevance Score:** 8

**TL;DR:** The paper demonstrates that the dispersion of a language model's embeddings is correlated with its text prediction performance, and suggests methods to leverage this for model selection and improved accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the relationship between the embedding space of language models and their text prediction capabilities, and to find practical applications for this insight.

**Method:** We analyze the correlation between representation dispersion (average pairwise cosine distance among hidden vectors) and perplexity across various models and domains. We also propose a training objective to enhance dispersion.

**Key Contributions:**

	1. Establishes a strong correlation between embedding dispersion and perplexity in language models.
	2. Introduces methods for leveraging dispersion for model selection using unlabeled data.
	3. Proposes a training objective to enhance embedding dispersion, resulting in improved performance.

**Result:** High representation dispersion is linked to lower perplexity. Measuring dispersion on unlabeled texts can predict accuracy in new domains. We identified layers with higher dispersion for improved retrieval accuracy and proposed a training objective that enhances dispersion and reduces perplexity.

**Limitations:** 

**Conclusion:** Increasing embedding dispersion can directly improve language model performance, providing a novel tool for model assessment and training strategies.

**Abstract:** We show that a language model's ability to predict text is tightly linked to the breadth of its embedding space: models that spread their contextual representations more widely tend to achieve lower perplexity. Concretely, we find that representation dispersion - the average pairwise cosine distance among hidden vectors - strongly and negatively correlates with perplexity across diverse model families (LLaMA, Qwen, and others) and domains (Wikipedia, news, scientific abstracts). Beyond illustrating this link, we show how dispersion can be leveraged for a range of practical tasks without requiring labeled data. First, measuring dispersion on unlabeled text allows us to predict downstream accuracy in new domains, offering a data-efficient tool for model selection. Next, we find that identifying layers with higher dispersion pinpoints the best representations for retrieval-based methods such as kNN-LM, bypassing exhaustive layer-by-layer searches. Finally, we integrate a simple push-away objective into training, which increases dispersion in both single-domain and cross-domain scenarios and directly improves perplexity in each.

</details>


### [114] [Computational Detection of Intertextual Parallels in Biblical Hebrew: A Benchmark Study Using Transformer-Based Language Models](https://arxiv.org/abs/2506.24117)

*David M. Smiley*

**Main category:** cs.CL

**Keywords:** human-computer interaction, natural language processing, transformer models

**Relevance Score:** 2

**TL;DR:** This study evaluates the use of transformer-based models for identifying parallel passages in biblical Hebrew, finding E5 and AlephBERT particularly effective.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the labor-intensive and error-prone nature of traditional methods for identifying intertextual relationships in biblical scholarship.

**Method:** The study assesses pre-trained transformer models (E5, AlephBERT, MPNet, LaBSE) for their ability to generate word embeddings that identify parallel versus non-parallel passages using cosine similarity and Wasserstein Distance.

**Key Contributions:**

	1. Demonstration of the effectiveness of transformer models in biblical text analysis.
	2. Quantitative evaluation of model performance using cosine similarity and Wasserstein Distance.
	3. Implications for efficiency improvements in ancient language studies.

**Result:** E5 and AlephBERT showed significant promise in parallel detection, with E5 excelling in identifying parallels and AlephBERT better at differentiating non-parallel texts.

**Limitations:** 

**Conclusion:** Pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, indicating potential broader applications in ancient language studies.

**Abstract:** Identifying parallel passages in biblical Hebrew is foundational in biblical scholarship for uncovering intertextual relationships. Traditional methods rely on manual comparison, which is labor-intensive and prone to human error. This study evaluates the potential of pre-trained transformer-based language models, including E5, AlephBERT, MPNet, and LaBSE, for detecting textual parallels in the Hebrew Bible. Focusing on known parallels between the books of Samuel/Kings and Chronicles, I assessed each model's capability to generate word embeddings that delineate parallel from non-parallel passages. Utilizing cosine similarity and Wasserstein Distance measures, I found that E5 and AlephBERT show significant promise, with E5 excelling in parallel detection and AlephBERT demonstrating stronger non-parallel differentiation. These findings indicate that pre-trained models can enhance the efficiency and accuracy of detecting intertextual parallels in ancient texts, suggesting broader applications for ancient language studies.

</details>


### [115] [Scaling Data-Constrained Language Models](https://arxiv.org/abs/2305.16264)

*Niklas Muennighoff, Alexander M. Rush, Boaz Barak, Teven Le Scao, Aleksandra Piktus, Nouamane Tazi, Sampo Pyysalo, Thomas Wolf, Colin Raffel*

**Main category:** cs.CL

**Keywords:** Language Models, Data Repetition, Compute Budget, Scaling Laws, Data Scarcity

**Relevance Score:** 8

**TL;DR:** This paper explores the effects of data repetition and compute budget on training large language models in data-constrained scenarios, finding optimal training strategies and introducing a new scaling law for compute efficiency.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The study is motivated by the impending limits on available training data from the internet, as language models scale in both parameter counts and dataset sizes.

**Method:** A large set of experiments were conducted, varying data repetition and compute budgets across models with up to 900 billion training tokens and 9 billion parameters.

**Key Contributions:**

	1. Investigation of training large language models in data-constrained environments.
	2. Empirical validation of a scaling law for compute optimality based on repeated tokens.
	3. Release of models and datasets from extensive training experiments.

**Result:** Results indicate that training with up to 4 epochs of repeated data leads to negligible loss changes compared to unique data, but additional repetition diminishes the returns of added compute resources.

**Limitations:** 

**Conclusion:** The paper proposes a new scaling law for compute optimality, highlighting strategies to address data scarcity through dataset augmentation and modifications in filtering practices.

**Abstract:** The current trend of scaling language models involves increasing both parameter count and training dataset size. Extrapolating this trend suggests that training dataset size may soon be limited by the amount of text data available on the internet. Motivated by this limit, we investigate scaling language models in data-constrained regimes. Specifically, we run a large set of experiments varying the extent of data repetition and compute budget, ranging up to 900 billion training tokens and 9 billion parameter models. We find that with constrained data for a fixed compute budget, training with up to 4 epochs of repeated data yields negligible changes to loss compared to having unique data. However, with more repetition, the value of adding compute eventually decays to zero. We propose and empirically validate a scaling law for compute optimality that accounts for the decreasing value of repeated tokens and excess parameters. Finally, we experiment with approaches mitigating data scarcity, including augmenting the training dataset with code data or removing commonly used filters. Models and datasets from our 400 training runs are freely available at https://github.com/huggingface/datablations.

</details>


### [116] [RAG and RAU: A Survey on Retrieval-Augmented Language Model in Natural Language Processing](https://arxiv.org/abs/2404.19543)

*Yucheng Hu, Yuxing Lu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Language Models, Natural Language Processing, Evaluation methods, Domain-specific knowledge, NLP tasks

**Relevance Score:** 9

**TL;DR:** This survey paper reviews Retrieval-Augmented Language Models (RALMs), covering their taxonomy, components, applications, and evaluation methods, while discussing challenges and future research directions.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gap in comprehensive understanding of Retrieval-Augmented Language Models (RALMs) and their applications in Natural Language Processing (NLP).

**Method:** The paper surveys existing literature on RALMs, discussing their components such as Retrievers, Language Models, and Augmentations, and how these interact to form various models.

**Key Contributions:**

	1. Comprehensive overview of Retrieval-Augmented Language Models (RALMs)
	2. Discussion on applications and evaluation methods for RALMs
	3. Identification of key challenges and future research avenues in the field.

**Result:** RALMs show significant performance improvements in tasks like translation, dialogue systems, and knowledge-intensive applications through the integration of external information.

**Limitations:** Limitations related to retrieval quality and computational efficiency of RALMs.

**Conclusion:** The paper provides a structured overview of RALMs, highlighting their potential and suggesting future research directions to address challenges in retrieval quality and computational efficiency.

**Abstract:** Large Language Models (LLMs) have catalyzed significant advancements in Natural Language Processing (NLP), yet they encounter challenges such as hallucination and the need for domain-specific knowledge. To mitigate these, recent methodologies have integrated information retrieved from external resources with LLMs, substantially enhancing their performance across NLP tasks. This survey paper addresses the absence of a comprehensive overview on Retrieval-Augmented Language Models (RALMs), both Retrieval-Augmented Generation (RAG) and Retrieval-Augmented Understanding (RAU), providing an in-depth examination of their paradigm, evolution, taxonomy, and applications. The paper discusses the essential components of RALMs, including Retrievers, Language Models, and Augmentations, and how their interactions lead to diverse model structures and applications. RALMs demonstrate utility in a spectrum of tasks, from translation and dialogue systems to knowledge-intensive applications. The survey includes several evaluation methods of RALMs, emphasizing the importance of robustness, accuracy, and relevance in their assessment. It also acknowledges the limitations of RALMs, particularly in retrieval quality and computational efficiency, offering directions for future research. In conclusion, this survey aims to offer a structured insight into RALMs, their potential, and the avenues for their future development in NLP. The paper is supplemented with a Github Repository containing the surveyed works and resources for further study: https://github.com/2471023025/RALM_Survey.

</details>


### [117] [The Effectiveness of LLMs as Annotators: A Comparative Overview and Empirical Analysis of Direct Representation](https://arxiv.org/abs/2405.01299)

*Maja Pavlovic, Massimo Poesio*

**Main category:** cs.CL

**Keywords:** Large Language Models, data annotation, human-computer interaction, opinion distribution, empirical analysis

**Relevance Score:** 9

**TL;DR:** This paper reviews twelve studies on the use of Large Language Models (LLMs) for data annotation, noting their potential benefits and significant limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the role of LLMs in data annotation and their effectiveness in labeling datasets compared to human insights.

**Method:** A comparative overview of twelve studies on LLMs for data labeling is presented, with an empirical analysis of GPT-generated opinion distributions against human opinions across four subjective datasets.

**Key Contributions:**

	1. Comparative overview of twelve studies on LLM data annotation
	2. Empirical analysis of alignment between human and GPT-generated opinions
	3. Highlighting limitations and the need for diversity in annotation tasks

**Result:** LLMs show cost and time-saving benefits in data labeling but face issues like bias, prompt sensitivity, and limited language representativeness.

**Limitations:** Considerable limitations include bias, sensitivity to prompt variations, and a preference for English language.

**Conclusion:** Further research is necessary to ensure diverse perspectives in data annotation tasks utilizing LLMs.

**Abstract:** Large Language Models (LLMs) have emerged as powerful support tools across various natural language tasks and a range of application domains. Recent studies focus on exploring their capabilities for data annotation. This paper provides a comparative overview of twelve studies investigating the potential of LLMs in labelling data. While the models demonstrate promising cost and time-saving benefits, there exist considerable limitations, such as representativeness, bias, sensitivity to prompt variations and English language preference. Leveraging insights from these studies, our empirical analysis further examines the alignment between human and GPT-generated opinion distributions across four subjective datasets. In contrast to the studies examining representation, our methodology directly obtains the opinion distribution from GPT. Our analysis thereby supports the minority of studies that are considering diverse perspectives when evaluating data annotation tasks and highlights the need for further research in this direction.

</details>


### [118] [Benchmarking Uncertainty Quantification Methods for Large Language Models with LM-Polygraph](https://arxiv.org/abs/2406.15627)

*Roman Vashurin, Ekaterina Fadeeva, Artem Vazhentsev, Lyudmila Rvanova, Akim Tsvigun, Daniil Vasilev, Rui Xing, Abdelrahman Boda Sadallah, Kirill Grishchenkov, Sergey Petrakov, Alexander Panchenko, Timothy Baldwin, Preslav Nakov, Maxim Panov, Artem Shelmanov*

**Main category:** cs.CL

**Keywords:** uncertainty quantification, large language models, benchmarking, text generation, confidence normalization

**Relevance Score:** 9

**TL;DR:** This paper introduces a benchmark for uncertainty quantification (UQ) in large language models (LLMs), addressing the issue of LLM hallucinations and low-quality outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid growth of large language models has led to significant challenges related to hallucinations and unreliable outputs, necessitating effective uncertainty quantification methods.

**Method:** The authors developed a comprehensive benchmark that consolidates state-of-the-art UQ techniques and provides a controlled environment for evaluating these methods across various text generation tasks.

**Key Contributions:**

	1. Introduction of a novel benchmark for UQ in LLMs.
	2. Implementation of state-of-the-art UQ techniques for evaluation.
	3. Large-scale empirical investigation of UQ techniques across eleven tasks.

**Result:** The empirical study revealed the most effective UQ and normalization techniques across eleven tasks, enhancing understanding of their effectiveness.

**Limitations:** 

**Conclusion:** This benchmark enables consistent evaluation of UQ methods and aids in the development of interpretable scoring systems for LLM outputs.

**Abstract:** The rapid proliferation of large language models (LLMs) has stimulated researchers to seek effective and efficient approaches to deal with LLM hallucinations and low-quality outputs. Uncertainty quantification (UQ) is a key element of machine learning applications in dealing with such challenges. However, research to date on UQ for LLMs has been fragmented in terms of techniques and evaluation methodologies. In this work, we address this issue by introducing a novel benchmark that implements a collection of state-of-the-art UQ baselines and offers an environment for controllable and consistent evaluation of novel UQ techniques over various text generation tasks. Our benchmark also supports the assessment of confidence normalization methods in terms of their ability to provide interpretable scores. Using our benchmark, we conduct a large-scale empirical investigation of UQ and normalization techniques across eleven tasks, identifying the most effective approaches. Code: https://github.com/IINemo/lm-polygraph Benchmark: https://huggingface.co/LM-Polygraph

</details>


### [119] [Enhancing the Capability and Robustness of Large Language Models through Reinforcement Learning-Driven Query Refinement](https://arxiv.org/abs/2407.01461)

*Xiaohua Wang, Zisu Huang, Feiran Zhang, Zhibo Xu, Cenyuan Zhang, Qi Qian, Xiaoqing Zheng, Xuanjing Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Prompt Engineering, Reinforcement Learning

**Relevance Score:** 9

**TL;DR:** This paper proposes a framework for refining user prompts to enhance large language models' (LLMs) performance and robustness against harmful inputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The quality of user prompts significantly affects the performance of LLMs, with vague prompts limiting their potential and harmful prompts posing security risks.

**Method:** A transferable and pluggable query refinement framework is proposed, utilizing a lightweight model trained with a reinforcement learning approach focused on multiple objectives.

**Key Contributions:**

	1. Introduction of a framework for query refinement in LLMs.
	2. Development of a lightweight model trained with multi-objective reinforcement learning.
	3. Demonstrated enhancement of response quality and robustness against attacks.

**Result:** Experiments show that the refinement model enhances response quality and improves robustness against jailbreak attacks on LLMs.

**Limitations:** 

**Conclusion:** Refining user prompts can lead to more truthful and useful outputs from LLMs while mitigating the risks of adversarial prompts.

**Abstract:** The capacity of large language models (LLMs) to generate honest, harmless, and helpful responses heavily relies on the quality of user prompts. However, these prompts often tend to be brief and vague, thereby significantly limiting the full potential of LLMs. Moreover, harmful prompts can be meticulously crafted and manipulated by adversaries to jailbreak LLMs, inducing them to produce potentially toxic content. To enhance the capabilities of LLMs while maintaining strong robustness against harmful jailbreak inputs, this study proposes a transferable and pluggable framework that refines user prompts before they are input into LLMs. This strategy improves the quality of the queries, empowering LLMs to generate more truthful, benign and useful responses. Specifically, a lightweight query refinement model is introduced and trained using a specially designed reinforcement learning approach that incorporates multiple objectives to enhance particular capabilities of LLMs. Extensive experiments demonstrate that the refinement model not only improves the quality of responses but also strengthens their robustness against jailbreak attacks. Code is available at: https://github.com/Huangzisu/query-refinement .

</details>


### [120] [ChipXplore: Natural Language Exploration of Hardware Designs and Libraries](https://arxiv.org/abs/2407.12749)

*Manar Abdelatty, Jacob Rosenstein, Sherief Reda*

**Main category:** cs.CL

**Keywords:** hardware design, process design kits, multi-agent, large language models, natural language processing

**Relevance Score:** 4

**TL;DR:** ChipXplore is a multi-agent framework using large language models to enhance hardware design workflows by enabling natural language queries for PDKs, thereby improving accuracy and productivity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To streamline the manual and error-prone process of querying hardware designs and PDKs, facilitating quicker and more accurate decision-making for engineers.

**Method:** The framework utilizes text-to-SQL and text-to-Cypher workflows to retrieve information from structured PDK and hardware design data, enabling natural language queries.

**Key Contributions:**

	1. Development of a multi-agent collaborative framework for hardware design queries
	2. Implementation of tailored text-to-SQL and text-to-Cypher workflows for improved accuracy
	3. Demonstration of significant productivity improvements and error reduction compared to traditional methods.

**Result:** ChipXplore achieves a 97.39% execution accuracy in complex queries, speeds up retrieval by 5.63x, and reduces errors by 5.25x in user studies, improving overall productivity.

**Limitations:** 

**Conclusion:** ChipXplore sets the groundwork for future autonomous agents designed to perform various physical design tasks involving hardware design and PDKs more efficiently.

**Abstract:** Hardware design workflows rely on Process Design Kits (PDKs) from different fabrication nodes, each containing standard cell libraries optimized for speed, power, or density. Engineers typically navigate between the design and target PDK to make informed decisions, such as selecting gates for area optimization or enhancing the speed of the critical path. However, this process is often manual, time-consuming, and prone to errors. To address this, we present ChipXplore, a multi-agent collaborative framework powered by large language models that enables engineers to query hardware designs and PDKs using natural language. By exploiting the structured nature of PDK and hardware design data, ChipXplore retrieves relevant information through text-to-SQL and text-to-Cypher customized workflows. The framework achieves an execution accuracy of 97.39\% in complex natural language queries and improves productivity by making retrieval 5.63x faster while reducing errors by 5.25x in user studies. Compared to generic workflows, ChipXplore's customized workflow is capable of orchestrating reasoning and planning over multiple databases, improving accuracy by 29.78\%. ChipXplore lays the foundation for building autonomous agents capable of tackling diverse physical design tasks that require PDK and hardware design awareness.

</details>


### [121] [Tracing Intricate Cues in Dialogue: Joint Graph Structure and Sentiment Dynamics for Multimodal Emotion Recognition](https://arxiv.org/abs/2407.21536)

*Jiang Li, Xiaoping Wang, Zhigang Zeng*

**Main category:** cs.CL

**Keywords:** Multimodal Emotion Recognition, Graph Neural Networks, Sentiment Analysis

**Relevance Score:** 9

**TL;DR:** GraphSmile is a novel approach for multimodal emotion recognition in conversation that effectively captures emotional cues while addressing existing challenges in the field.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current methods in multimodal emotion recognition face challenges such as ineffective cross-modal modeling, conflicts from data fusion, and inability to track dynamic emotional changes.

**Method:** GraphSmile employs GSF and SDP modules; GSF uses graph structures to capture emotional dependencies, while SDP delineates sentiment dynamics between utterances.

**Key Contributions:**

	1. Introduction of GraphSmile for addressing challenges in MERC
	2. Utilization of graph structures for better cross-modal cue capturing
	3. Enhancement of sentiment dynamics tracking through the SDP module

**Result:** GraphSmile significantly outperformed baseline models on various benchmarks in terms of handling complex emotional patterns.

**Limitations:** 

**Conclusion:** GraphSmile enables simultaneous multimodal sentiment analysis and emotion recognition in conversation tasks with improved accuracy.

**Abstract:** Multimodal emotion recognition in conversation (MERC) has garnered substantial research attention recently. Existing MERC methods face several challenges: (1) they fail to fully harness direct inter-modal cues, possibly leading to less-than-thorough cross-modal modeling; (2) they concurrently extract information from the same and different modalities at each network layer, potentially triggering conflicts from the fusion of multi-source data; (3) they lack the agility required to detect dynamic sentimental changes, perhaps resulting in inaccurate classification of utterances with abrupt sentiment shifts. To address these issues, a novel approach named GraphSmile is proposed for tracking intricate emotional cues in multimodal dialogues. GraphSmile comprises two key components, i.e., GSF and SDP modules. GSF ingeniously leverages graph structures to alternately assimilate inter-modal and intra-modal emotional dependencies layer by layer, adequately capturing cross-modal cues while effectively circumventing fusion conflicts. SDP is an auxiliary task to explicitly delineate the sentiment dynamics between utterances, promoting the model's ability to distinguish sentimental discrepancies. GraphSmile is effortlessly applied to multimodal sentiment analysis in conversation (MSAC), thus enabling simultaneous execution of MERC and MSAC tasks. Empirical results on multiple benchmarks demonstrate that GraphSmile can handle complex emotional and sentimental patterns, significantly outperforming baseline models.

</details>


### [122] [CTISum: A New Benchmark Dataset For Cyber Threat Intelligence Summarization](https://arxiv.org/abs/2408.06576)

*Wei Peng, Junmei Ding, Wei Wang, Lei Cui, Wei Cai, Zhiyu Hao, Xiaochun Yun*

**Main category:** cs.CL

**Keywords:** Cyber Threat Intelligence, CTI Summarization, Dataset, Machine Learning, Natural Language Processing

**Relevance Score:** 3

**TL;DR:** CTI summarization aims to generate highlights from web intelligence data for decision-makers in cybersecurity. This paper introduces CTISum, a dataset for CTI summarization, and a subtask focused on attack process summarization, addressing the challenges of summarizing CTI reports.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of suitable datasets for summarizing Cyber Threat Intelligence (CTI) reports, which are crucial for quick detection and response to cyber threats.

**Method:** The authors propose CTISum, a new benchmark dataset for CTI summarization, and a multi-stage annotation pipeline to collect and annotate CTI data. They also benchmark this dataset using various summarization methods, including extractive, abstractive, and LLM-based techniques.

**Key Contributions:**

	1. Introduction of CTISum, a benchmark dataset for CTI summarization.
	2. Proposal of attack process summarization as a novel fine-grained subtask.
	3. Identification of significant challenges faced by current models in CTI summarization.

**Result:** Experimental results indicate that current state-of-the-art models struggle significantly with the CTISum dataset, underlining the challenges in automatic summarization of CTI reports.

**Limitations:** The paper primarily focuses on the dataset and does not provide new algorithms; challenges in summarization remain.

**Conclusion:** The research highlights the ongoing open problem of CTI report summarization and the need for more effective techniques and datasets.

**Abstract:** Cyber Threat Intelligence (CTI) summarization involves generating concise and accurate highlights from web intelligence data, which is critical for providing decision-makers with actionable insights to swiftly detect and respond to cyber threats in the cybersecurity domain. Despite that, the development of efficient techniques for summarizing CTI reports, comprising facts, analytical insights, attack processes, and more, has been hindered by the lack of suitable datasets. To address this gap, we introduce CTISum, a new benchmark dataset designed for the CTI summarization task. Recognizing the significance of understanding attack processes, we also propose a novel fine-grained subtask: attack process summarization, which aims to help defenders assess risks, identify security gaps, and uncover vulnerabilities. Specifically, a multi-stage annotation pipeline is designed to collect and annotate CTI data from diverse web sources, alongside a comprehensive benchmarking of CTISum using both extractive, abstractive and LLMs-based summarization methods. Experimental results reveal that current state-of-the-art models face significant challenges when applied to CTISum, highlighting that automatic summarization of CTI reports remains an open research problem. The code and example dataset can be made publicly available at https://github.com/pengwei-iie/CTISum.

</details>


### [123] [Emotional RAG LLMs: Reading Comprehension for the Open Internet](https://arxiv.org/abs/2408.11189)

*Benjamin Reichman, Adar Avsian, Kartik Talamadupula, Toshish Jawale, Larry Heck*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, emotion translation, large language models, pragmatic interpretation, text diversity

**Relevance Score:** 8

**TL;DR:** This paper presents a dataset to enhance the emotional and stylistic diversity of RAG-retrieved text, along with an emotion translation model and a prompt-based method for improving LLMs' interpretation of such text.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to adapt LLMs and RAG systems to handle diverse tones and linguistic styles found in real-world texts rather than neutral, Wikipedia-like content.

**Method:** Introduction of a dataset with emotionally inflected and sarcastic text, development of an emotion translation model, and a prompt-based approach to improve LLMs' understanding of context.

**Key Contributions:**

	1. Dataset for emotionally varied RAG retrievals
	2. Emotion translation model for tone adaptation
	3. Prompt-based techniques for LLM interpretation

**Result:** The proposed methods demonstrate stronger performance in interpreting varied tones and styles in LLM applications.

**Limitations:** The dataset’s effectiveness in broader contexts and potential biases in emotional translation models need further exploration.

**Conclusion:** Enhancing LLMs with emotional diversity in retrieval contexts can improve their pragmatic understanding and applicability in real-world scenarios.

**Abstract:** Queries to large language models (LLMs) can be divided into two parts: the instruction/question and the accompanying context. The context for retrieval-augmented generation (RAG) systems in most benchmarks comes from Wikipedia-like texts written in a neutral and factual tone. However, real-world RAG applications often retrieve internet-based text with diverse tones and linguistic styles, posing challenges for downstream tasks. This paper introduces (a) a dataset that transforms RAG-retrieved passages into emotionally inflected and sarcastic text, (b) an emotion translation model for adapting text to different tones, and (c) a prompt-based method to improve LLMs' pragmatic interpretation of retrieved text.

</details>


### [124] [S^3cMath: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners](https://arxiv.org/abs/2409.01524)

*Yuchen Yan, Jin Jiang, Yang Liu, Yixin Cao, Xin Xu, Mengdi Zhang, Xunliang Cai, Jian Shao*

**Main category:** cs.CL

**Keywords:** large language models, self-correction, mathematical reasoning, machine learning, NLU

**Relevance Score:** 8

**TL;DR:** Introducing S$^3$c-Math, a method for spontaneous step-level self-correction in large language models during mathematical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning accuracy of large language models by enabling them to self-correct during inference.

**Method:** A step-level sampling approach is used to create self-correction data, and a training strategy is employed to develop spontaneous self-correction capabilities in LLMs.

**Key Contributions:**

	1. Development of spontaneous step-level self-correction method for LLMs
	2. Creation of self-correction data through step-level sampling
	3. Demonstration of effectiveness across various mathematical benchmarks

**Result:** The S$^3$c-Math models are shown to significantly improve performance on benchmarks such as GSM8K and MATH by correctly identifying and rectifying errors in real-time.

**Limitations:** 

**Conclusion:** The introduction of spontaneous step-level self-correction in LLMs represents a significant advancement in mathematical reasoning capabilities.

**Abstract:** Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S$^3$c-Math, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.

</details>


### [125] [Kalahi: A handcrafted, grassroots cultural LLM evaluation suite for Filipino](https://arxiv.org/abs/2409.15380)

*Jann Railey Montalan, Jian Gang Ngui, Wei Qi Leong, Yosephine Susanto, Hamsawardhini Rengarajan, Alham Fikri Aji, William Chandra Tjhi*

**Main category:** cs.CL

**Keywords:** multilingual LLMs, Filipino culture, cultural evaluation

**Relevance Score:** 9

**TL;DR:** Kalahi is an evaluation suite for LLMs that focuses on culturally relevant responses for Filipino users, highlighting a gap in current LLM performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how well multilingual LLMs generate culturally appropriate responses for Filipino users, given current inadequacies.

**Method:** Kalahi consists of 150 nuanced prompts designed by native Filipino speakers to evaluate LLMs' capability in generating culturally relevant responses.

**Key Contributions:**

	1. Introduction of a culturally-focused LLM evaluation suite (Kalahi)
	2. Creation of 150 nuanced prompts based on Filipino culture
	3. Empirical results showcasing LLM performance gaps in cultural relevance

**Result:** The best LLM answered only 46.0% of the prompt questions correctly, compared to 89.10% for native Filipinos, indicating a significant disparity in cultural understanding.

**Limitations:** 

**Conclusion:** Kalahi provides a reliable measure for evaluating the cultural representation of LLMs in relation to Filipino culture.

**Abstract:** Multilingual large language models (LLMs) today may not necessarily provide culturally appropriate and relevant responses to its Filipino users. We introduce Kalahi, a cultural LLM evaluation suite collaboratively created by native Filipino speakers. It is composed of 150 high-quality, handcrafted and nuanced prompts that test LLMs for generations that are relevant to shared Filipino cultural knowledge and values. Strong LLM performance in Kalahi indicates a model's ability to generate responses similar to what an average Filipino would say or do in a given situation. We conducted experiments on LLMs with multilingual and Filipino language support. Results show that Kalahi, while trivial for Filipinos, is challenging for LLMs, with the best model answering only 46.0% of the questions correctly compared to native Filipino performance of 89.10%. Thus, Kalahi can be used to accurately and reliably evaluate Filipino cultural representation in LLMs.

</details>


### [126] [Margin Matching Preference Optimization: Enhanced Model Alignment with Granular Feedback](https://arxiv.org/abs/2410.03145)

*Kyuyoung Kim, Ah Jeong Seo, Hao Liu, Jinwoo Shin, Kimin Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Alignment Techniques, Machine Learning

**Relevance Score:** 9

**TL;DR:** Introducing Margin Matching Preference Optimization (MMPO) for improving LLMs by incorporating relative quality margins in pairwise preferences for training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods in LLM alignment rely on simplistic binary labels, failing to capture nuanced differences in output quality.

**Method:** MMPO incorporates relative quality margins into optimization by designing soft target probabilities based on the Bradley-Terry model, using standard cross-entropy for training.

**Key Contributions:**

	1. Introduction of Margin Matching Preference Optimization (MMPO) approach
	2. Incorporation of relative quality margins into LLM training
	3. Demonstrated superior performance on multiple benchmarks

**Result:** MMPO outperforms baseline methods on benchmarks like MT-bench and RewardBench; the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench as of June 2024.

**Limitations:** 

**Conclusion:** MMPO leads to better-calibrated, robust models and improved policy performance in LLMs.

**Abstract:** Large language models (LLMs) fine-tuned with alignment techniques, such as reinforcement learning from human feedback, have been instrumental in developing some of the most capable AI systems to date. Despite their success, existing methods typically rely on simple binary labels, such as those indicating preferred outputs in pairwise preferences, which fail to capture the subtle differences in relative quality between pairs. To address this limitation, we introduce an approach called Margin Matching Preference Optimization (MMPO), which incorporates relative quality margins into optimization, leading to improved LLM policies and reward models. Specifically, given quality margins in pairwise preferences, we design soft target probabilities based on the Bradley-Terry model, which are then used to train models with the standard cross-entropy objective. Experiments with both human and AI feedback data demonstrate that MMPO consistently outperforms baseline methods, often by a substantial margin, on popular benchmarks including MT-bench and RewardBench. Notably, the 7B model trained with MMPO achieves state-of-the-art performance on RewardBench as of June 2024, outperforming other models of the same scale. Our analysis also shows that MMPO is more robust to overfitting, leading to better-calibrated models.

</details>


### [127] [Which Programming Language and What Features at Pre-training Stage Affect Downstream Logical Inference Performance?](https://arxiv.org/abs/2410.06735)

*Fumiya Uchiyama, Takeshi Kojima, Andrew Gambardella, Qi Cao, Yusuke Iwasawa, Yutaka Matsuo*

**Main category:** cs.CL

**Keywords:** large language models, programming languages, logical inference, pre-training, machine learning

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of programming language pre-training on the logical inference performance of large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To rigorously test the causal relationship between programming language features in pre-training and logical reasoning performance in LLMs.

**Method:** The authors pre-trained decoder-based language models from scratch using datasets from ten programming languages and evaluated them on logical reasoning tasks in a few-shot context.

**Key Contributions:**

	1. Demonstrated that programming languages significantly improve LLMs' logical reasoning capabilities.
	2. Identified specific programming features that enhance instruction-following abilities.
	3. Provided insights into pre-training elements vital for foundational LLM capabilities.

**Result:** Models trained with programming languages consistently outperformed those trained with natural languages in logical reasoning tasks, and showed better instruction-following abilities.

**Limitations:** 

**Conclusion:** The results suggest that programming languages offer unique factors that enhance logical inference, with the structure of Abstract Syntax Trees also influencing performance.

**Abstract:** Recent large language models (LLMs) have demonstrated remarkable generalization abilities in mathematics and logical reasoning tasks. Prior research indicates that LLMs pre-trained with programming language data exhibit high mathematical and reasoning abilities; however, this causal relationship has not been rigorously tested. Our research aims to verify which programming languages and features during pre-training affect logical inference performance. Specifically, we pre-trained decoder-based language models from scratch using datasets from ten programming languages (e.g., Python, C, Java) and three natural language datasets (Wikipedia, Fineweb, C4) under identical conditions. Thereafter, we evaluated the trained models in a few-shot in-context learning setting on logical reasoning tasks: FLD and bAbi, which do not require commonsense or world knowledge. The results demonstrate that nearly all models trained with programming languages consistently outperform those trained with natural languages, indicating that programming languages contain factors that elicit logic inference performance. In addition, we found that models trained with programming languages exhibit a better ability to follow instructions compared to those trained with natural languages. Further analysis reveals that the depth of Abstract Syntax Trees representing parsed results of programs also affects logical reasoning performance. These findings will offer insights into the essential elements of pre-training for acquiring the foundational abilities of LLMs.

</details>


### [128] [Sample then Identify: A General Framework for Risk Control and Assessment in Multimodal Large Language Models](https://arxiv.org/abs/2410.08174)

*Qingni Wang, Tiantian Geng, Zhiyuan Wang, Teng Wang, Bo Fu, Feng Zheng*

**Main category:** cs.CL

**Keywords:** Multi-modal Large Language Models, Trustworthiness, Risk Assessment, Conformal Prediction, Semantic Redundancy

**Relevance Score:** 8

**TL;DR:** TRON is a two-step framework for improving trustworthiness in Multimodal Large Language Models (MLLMs) by applying risk control and assessment strategies to enhance their adaptability in various environments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address significant trustworthiness issues in current MLLMs, particularly in open-ended tasks, and to improve generalizability and adaptability in risk assessment.

**Method:** TRON consists of two components: a novel conformal score for minimum size response sampling and a nonconformity score for assessing high-quality responses, both aimed at managing error rates across specified risk levels.

**Key Contributions:**

	1. Introduction of TRON framework for MLLM risk management
	2. Novel conformal and nonconformity scores for response assessment
	3. New evaluation metric based on semantic redundancy in prediction sets

**Result:** TRON yielded effective error rate control in predictions across four VideoQA datasets using eight MLLMs, demonstrating adaptability and stability while maintaining efficiency.

**Limitations:** 

**Conclusion:** The framework provides a significant enhancement in the assessment and control of MLLM predictions, introducing new evaluation metrics that are applicable in open-ended contexts.

**Abstract:** Multimodal Large Language Models (MLLMs) exhibit promising advancements across various tasks, yet they still encounter significant trustworthiness issues. Prior studies apply Split Conformal Prediction (SCP) in language modeling to construct prediction sets with statistical guarantees. However, these methods typically rely on internal model logits or are restricted to multiple-choice settings, which hampers their generalizability and adaptability in dynamic, open-ended environments. In this paper, we introduce TRON, a two-step framework for risk control and assessment, applicable to any MLLM that supports sampling in both open-ended and closed-ended scenarios. TRON comprises two main components: (1) a novel conformal score to sample response sets of minimum size, and (2) a nonconformity score to identify high-quality responses based on self-consistency theory, controlling the error rates by two specific risk levels. Furthermore, we investigate semantic redundancy in prediction sets within open-ended contexts for the first time, leading to a promising evaluation metric for MLLMs based on average set size. Our comprehensive experiments across four Video Question-Answering (VideoQA) datasets utilizing eight MLLMs show that TRON achieves desired error rates bounded by two user-specified risk levels. Additionally, deduplicated prediction sets maintain adaptiveness while being more efficient and stable for risk assessment under different risk levels.

</details>


### [129] [Parenting: Optimizing Knowledge Selection of Retrieval-Augmented Language Models with Parameter Decoupling and Tailored Tuning](https://arxiv.org/abs/2410.10360)

*Yongxin Xu, Ruizhe Zhang, Xinke Jiang, Yujie Feng, Yuzhen Xiao, Xinyu Ma, Runchuan Zhu, Xu Chu, Junfeng Zhao, Yasha Wang*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, parameter optimization

**Relevance Score:** 9

**TL;DR:** Parenting is a novel framework for integrating internal and external knowledge in Retrieval-Augmented Generation (RAG), enhancing adherence and robustness in Large Language Models (LLMs) without hallucination.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods lack effective control mechanisms for integrating internal and external knowledge, leading to issues in LLMs like hallucination and knowledge obsolescence.

**Method:** Parenting decouples, identifies, and optimizes parameter subspaces related to adherence and robustness using a key parameter mining method that combines forward and backward propagation signals.

**Key Contributions:**

	1. Introduction of the Parenting framework for optimizing LLMs
	2. Decoupling parameter subspaces for targeted tuning
	3. Key parameter mining method that enhances model robustness

**Result:** Extensive experiments ensure the effectiveness and generalizability of Parenting across various datasets and models.

**Limitations:** 

**Conclusion:** The Parenting framework offers a structured approach to optimize LLMs' adherence and robustness by leveraging both internal and external knowledge.

**Abstract:** Retrieval-Augmented Generation (RAG) offers an effective solution to the issues faced by Large Language Models (LLMs) in hallucination generation and knowledge obsolescence by incorporating externally retrieved knowledge. However, existing methods lack effective control mechanisms for integrating internal and external knowledge. Inspired by human cognitive processes, we propose Parenting, a novel framework that decouples, identifies, and purposefully optimizes parameter subspaces related to adherence and robustness. Specifically, Parenting utilizes a key parameter mining method that combines forward and backward propagation signals to localize subspaces representing different capabilities. Then, Parenting employs a type-tailored tuning strategy, applying specific and appropriate optimizations to different subspaces, aiming to achieve a balanced enhancement of both adherence and robustness. Extensive experiments on various datasets and models validate the effectiveness and generalizability of our method.

</details>


### [130] [Beware of Calibration Data for Pruning Large Language Models](https://arxiv.org/abs/2410.17711)

*Yixin Ji, Yang Xiang, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, post-training pruning, calibration data, model compression, deep learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the impact of calibration data on post-training pruning in large language models, providing a synthesis strategy for constructing effective calibration data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Model compression is essential for large language models to reduce costs and improve inference efficiency, yet the effects of calibration data on post-training pruning remain underexplored.

**Method:** The study conducts controlled experiments to analyze the influence factors of calibration data, such as pruning settings, data size, and similarity to pre-training data, while also proposing a self-generating strategy for calibration data synthesis.

**Key Contributions:**

	1. Systematic exploration of calibration data effects on pruning performance.
	2. Introduction of a self-generating strategy for calibration data synthesis.
	3. Empirical evidence showing the importance of data similarity for improved model performance.

**Result:** The findings reveal that a small amount of calibration data, especially similar to pre-training data, significantly improves pruning performance, with experimental results showing up to a 2.68% enhancement in model accuracy.

**Limitations:** 

**Conclusion:** The proposed calibration data synthesis strategy offers a promising solution to enhance post-training pruning methods in large language models.

**Abstract:** As large language models (LLMs) are widely applied across various fields, model compression has become increasingly crucial for reducing costs and improving inference efficiency. Post-training pruning is a promising method that does not require resource-intensive iterative training and only needs a small amount of calibration data to assess the importance of parameters. Recent research has enhanced post-training pruning from different aspects but few of them systematically explore the effects of calibration data, and it is unclear if there exist better calibration data construction strategies. We fill this blank and surprisingly observe that calibration data is also crucial to post-training pruning, especially for high sparsity. Through controlled experiments on important influence factors of calibration data, including the pruning settings, the amount of data, and its similarity with pre-training data, we observe that a small size of data is adequate, and more similar data to its pre-training stage can yield better performance. As pre-training data is usually inaccessible for advanced LLMs, we further provide a self-generating calibration data synthesis strategy to construct feasible calibration data. Experimental results on recent strong open-source LLMs (e.g., DCLM, and LLaMA-3) show that the proposed strategy can enhance the performance of strong pruning methods (e.g., Wanda, DSnoT, OWL) by a large margin (up to $2.68\%$). Code is available at https://github.com/Dereck0602/calibration_data.

</details>


### [131] [Bridge: A Unified Framework to Knowledge Graph Completion via Language Models and Knowledge Representation](https://arxiv.org/abs/2411.06660)

*Qiao Qiao, Yuepei Li, Qing Wang, Kang Zhou, Qi Li*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Pre-trained Language Models, Self-supervised Learning

**Relevance Score:** 6

**TL;DR:** Bridge is a novel framework that enhances knowledge graph completion by jointly encoding structural and semantic information from knowledge graphs and pre-trained language models, outperforming state-of-the-art models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for knowledge graph completion either focus on structural information or semantic information, leading to suboptimal performance.

**Method:** The Bridge framework encodes entities and relations separately using pre-trained language models, employs a self-supervised learning method called BYOL to fine-tune PLMs with distinct views of triples, and avoids semantic alteration.

**Key Contributions:**

	1. Introduction of the Bridge framework for KGC
	2. Separate encoding of entities and relations for better PLM utilization
	3. Use of BYOL in a novel way to enhance KGC without semantic alteration

**Result:** Bridge significantly outperforms state-of-the-art models on three benchmark datasets.

**Limitations:** 

**Conclusion:** The proposed framework effectively integrates both structural and semantic knowledge for superior knowledge graph completion.

**Abstract:** Knowledge graph completion (KGC) is a task of inferring missing triples based on existing Knowledge Graphs (KGs). Both structural and semantic information are vital for successful KGC. However, existing methods only use either the structural knowledge from the KG embeddings or the semantic information from pre-trained language models (PLMs), leading to suboptimal model performance. Moreover, since PLMs are not trained on KGs, directly using PLMs to encode triples may be inappropriate. To overcome these limitations, we propose a novel framework called Bridge, which jointly encodes structural and semantic information of KGs. Specifically, we strategically encode entities and relations separately by PLMs to better utilize the semantic knowledge of PLMs and enable structured representation learning via a structural learning principle. Furthermore, to bridge the gap between KGs and PLMs, we employ a self-supervised representation learning method called BYOL to fine-tune PLMs with two different views of a triple. Unlike BYOL, which uses augmentation methods to create two semantically similar views of the same image, potentially altering the semantic information. We strategically separate the triple into two parts to create different views, thus avoiding semantic alteration. Experiments demonstrate that Bridge outperforms the SOTA models on three benchmark datasets.

</details>


### [132] [The Limited Impact of Medical Adaptation of Large Language and Vision-Language Models](https://arxiv.org/abs/2411.08870)

*Daniel P. Jeong, Pranav Mani, Saurabh Garg, Zachary C. Lipton, Michael Oberst*

**Main category:** cs.CL

**Keywords:** medical LLMs, VLMs, medical question answering, domain adaptation, statistical uncertainty

**Relevance Score:** 9

**TL;DR:** This paper evaluates the effectiveness of medical large language models (LLMs) and vision-language models (VLMs) by comparing them to their base models in medical question answering tasks, finding that many do not outperform general models.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the performance of domain-adapted medical LLMs and VLMs in medical question answering and determine if they truly provide better results compared to their base models.

**Method:** Comparison of ten medical LLMs and two VLMs against their respective base models in zero-/few-shot prompting and supervised fine-tuning, including optimization of prompts and accounting for statistical uncertainty.

**Key Contributions:**

	1. Comparison of medical models directly against their base models
	2. Optimized prompts for individual models in zero-/few-shot settings
	3. Accounted for statistical uncertainty in performance comparisons

**Result:** Medical LLMs outperformed their base models only in 26.7% of cases and performed worse in 56.7% of cases during the evaluation of clinical-note-based QA tasks.

**Limitations:** 

**Conclusion:** General-domain models may possess sufficient medical knowledge and reasoning capabilities, suggesting that medical domain adaptation may not be necessary and recommending improvements for future studies.

**Abstract:** Several recent works seek to adapt general-purpose large language models (LLMs) and vision-language models (VLMs) for medical applications through continued pretraining on publicly available biomedical corpora. These works typically claim that such domain-adaptive pretraining improves performance on various downstream medical tasks, such as answering medical exam questions. In this paper, we compare ten "medical" LLMs and two VLMs against their corresponding base models, arriving at a different conclusion: all medical VLMs and nearly all medical LLMs fail to consistently improve over their base models in the zero-/few-shot prompting and supervised fine-tuning regimes for medical question answering (QA). For instance, on clinical-note-based QA tasks in the 3-shot setting, medical LLMs outperform their base models in only 26.7% of cases, reach a (statistical) tie in 16.7% of cases, and perform significantly worse in the remaining 56.7% of cases. Our conclusions are based on (i) comparing each medical model directly against its base model; (ii) optimizing the prompts for each model separately in zero-/few-shot prompting; and (iii) accounting for statistical uncertainty in comparisons. Our findings suggest that state-of-the-art general-domain models may already exhibit strong medical knowledge and reasoning capabilities, and offer recommendations to strengthen the conclusions of future studies.

</details>


### [133] [MLAN: Language-Based Instruction Tuning Preserves and Transfers Knowledge in Multimodal Language Models](https://arxiv.org/abs/2411.10557)

*Jianhong Tu, Zhuohao Ni, Nicholas Crispino, Zihao Yu, Michael Bendersky, Beliz Gunel, Ruoxi Jia, Xin Liu, Lingjuan Lyu, Dawn Song, Chenguang Wang*

**Main category:** cs.CL

**Keywords:** visual instruction tuning, multimodal models, text-only data, zero-shot generalization, efficiency

**Relevance Score:** 8

**TL;DR:** This paper proposes a new visual instruction tuning strategy that emphasizes the use of text-only data to enhance the performance of multimodal large language models in zero-shot generalization tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the imbalance in the types of data used for instruction tuning in multimodal large language models, this paper investigates the impact of text-only data on task generalization.

**Method:** The authors conduct controlled experiments by varying the mixture of vision-language and text-only data during the instruction tuning stage, focusing on the role of modality in performance.

**Key Contributions:**

	1. Introduces a novel text-heavy visual instruction tuning strategy
	2. Demonstrates the effectiveness of diverse text-only data for multimodal tasks
	3. Shows that using less training data can yield competitive results compared to traditional methods

**Result:** The text-heavy instruction tuning strategy achieves comparable performance to traditional vision-heavy methods across multiple datasets while using fewer training tokens, indicating higher efficiency.

**Limitations:** The study may not cover all potential modalities or dataset combinations, limiting generalizability to some specific tasks.

**Conclusion:** Increasing the diversity of text-only data during instruction tuning facilitates better transfer of instruction following and domain knowledge between modalities, making the tuning process more effective.

**Abstract:** We present a novel visual instruction tuning strategy to improve the zero-shot task generalization of multimodal large language models by building a firm text-only knowledge base. Existing work lacks sufficient experimentation on the importance of each modality in the instruction tuning stage, often using a majority of vision-language data while keeping text-only data limited and fixing mixtures of modalities. By incorporating diverse text-only data in the visual instruction tuning stage, we vary vision-language data in various controlled experiments to investigate the importance of modality in visual instruction tuning. Our comprehensive evaluation shows that the text-heavy instruction tuning approach is able to perform on-par with traditional vision-heavy mixtures on both modalities across 12 general datasets while using as low as half the total training tokens. We find that simply increasing sufficiently diverse text-only data enables transfer of instruction following ability and domain knowledge across modalities while being more efficient than the vision-language approach.

</details>


### [134] [A Comprehensive Evaluation of Semantic Relation Knowledge of Pretrained Language Models and Humans](https://arxiv.org/abs/2412.01131)

*Zhihan Cao, Hiroaki Yamada, Simone Teufel, Takenobu Tokunaga*

**Main category:** cs.CL

**Keywords:** pretrained language models, semantic relations, evaluation framework, human performance, antonymy

**Relevance Score:** 7

**TL;DR:** The paper evaluates the semantic relation knowledge of pretrained language models (PLMs) beyond hypernymy by using a new framework and metrics, comparing human performance to that of various PLMs across five relations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the incomplete view of semantic relations knowledge in PLMs, particularly focusing on unexplored aspects of semantic relations beyond hypernymy.

**Method:** A comprehensive evaluation framework covering hyponymy, holonymy, meronymy, antonymy, and synonymy was introduced, utilizing six metrics for assessment and comparing 16 PLMs, including masked and causal models, against human performance.

**Key Contributions:**

	1. Introduction of a comprehensive evaluation framework for semantic relations
	2. Comparative analysis between human performance and PLM capabilities
	3. Identification of performance gaps particularly in non-antonymy relations

**Result:** The results show a significant knowledge gap between humans and PLMs in understanding semantic relations, with models confusing non-antonymy relations with antonymy. Masked language models outperformed causal models overall.

**Limitations:** The study is limited to semantic relations tested and may not generalize to other forms of knowledge.

**Conclusion:** Despite some models' reasonable performance in antonymy, the findings highlight the general limitations of PLMs in grasping semantic relations when contrasted with human understanding.

**Abstract:** Recently, much work has concerned itself with the enigma of what exactly PLMs (pretrained language models) learn about different aspects of language, and how they learn it. One stream of this type of research investigates the knowledge that PLMs have about semantic relations. However, many aspects of semantic relations were left unexplored. Only one relation was considered, namely hypernymy. Furthermore, previous work did not measure humans' performance on the same task as that solved by the PLMs. This means that at this point in time, there is only an incomplete view of models' semantic relation knowledge. To address this gap, we introduce a comprehensive evaluation framework covering five relations beyond hypernymy, namely hyponymy, holonymy, meronymy, antonymy, and synonymy. We use six metrics (two newly introduced here) for recently untreated aspects of semantic relation knowledge, namely soundness, completeness, symmetry, asymmetry, prototypicality, and distinguishability and fairly compare humans and models on the same task. Our extensive experiments involve 16 PLMs, eight masked and eight causal language models. Up to now only masked language models had been tested although causal and masked language models treat context differently. Our results reveal a significant knowledge gap between humans and models for almost all semantic relations. Antonymy is the outlier relation where all models perform reasonably well. In general, masked language models perform significantly better than causal language models. Nonetheless, both masked and causal language models are likely to confuse non-antonymy relations with antonymy.

</details>


### [135] [A Context-aware Framework for Translation-mediated Conversations](https://arxiv.org/abs/2412.04205)

*José Pombal, Sweta Agrawal, Patrick Fernandes, Emmanouil Zaranis, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** automatic translation, contextual information, large language models, bilingual conversation, translation quality

**Relevance Score:** 8

**TL;DR:** This paper presents a framework, TowerChat, that enhances large language model-based translation systems by integrating contextual information in bilingual conversations, demonstrating improved translation quality over state-of-the-art systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current automatic translation systems that fail to incorporate contextual information, which leads to errors and misunderstandings.

**Method:** The authors propose a framework, TowerChat, that includes contextual information during the training and inference stages in bilingual conversational settings.

**Key Contributions:**

	1. Development of the TowerChat framework
	2. Demonstrated improvement over state-of-the-art translation systems
	3. Showed effective context leveraging in translations

**Result:** Validation on customer chat and user-assistant interactions shows that TowerChat consistently yields better translation quality than existing systems like GPT-4o and TowerInstruct, measured by various automatic metrics.

**Limitations:** 

**Conclusion:** The proposed framework improves consistency between the message conveyed and the generated translations, leveraging context effectively.

**Abstract:** Automatic translation systems offer a powerful solution to bridge language barriers in scenarios where participants do not share a common language. However, these systems can introduce errors leading to misunderstandings and conversation breakdown. A key issue is that current systems fail to incorporate the rich contextual information necessary to resolve ambiguities and omitted details, resulting in literal, inappropriate, or misaligned translations. In this work, we present a framework to improve large language model-based translation systems by incorporating contextual information in bilingual conversational settings during training and inference. We validate our proposed framework on two task-oriented domains: customer chat and user-assistant interaction. Across both settings, the system produced by our framework-TowerChat-consistently results in better translations than state-of-the-art systems like GPT-4o and TowerInstruct, as measured by multiple automatic translation quality metrics on several language pairs. We also show that the resulting model leverages context in an intended and interpretable way, improving consistency between the conveyed message and the generated translations.

</details>


### [136] [Reasoner Outperforms: Generative Stance Detection with Rationalization for Social Media](https://arxiv.org/abs/2412.10266)

*Jiaqing Yuan, Ruijie Xi, Munindar P. Singh*

**Main category:** cs.CL

**Keywords:** stance detection, generative models, interpretability, fairness in AI, social media bias

**Relevance Score:** 8

**TL;DR:** This study presents a generative approach to stance detection that enhances interpretability through explicit rationales, integrating them into smaller language models to outperform existing state-of-the-art models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to improve stance detection to foster a human-centric Web, addressing biases and harmful narratives in user-generated content and emphasizing the need for interpretability in AI models.

**Method:** The study employs a generative approach to stance detection, integrating rationale-based predictions into smaller language models using both single-task and multitask learning methodologies.

**Key Contributions:**

	1. Introduction of a generative approach to stance detection with interpretable rationales.
	2. Achievement of improved performance over GPT-3.5 by a smaller model (FlanT5).
	3. Advancement in prototype models that foster trust and reduce biases in social media applications.

**Result:** Incorporating reasoning into stance detection allows the smaller model (FlanT5) to exceed GPT-3.5's zero-shot performance by up to 9.57%, while enhancing multitask learning performance but potentially hindering single-task efficacy.

**Limitations:** The study indicates that while multitask learning benefits from reasoning capabilities, single-task performance may decline.

**Conclusion:** The integration of faithful rationales into stance detection systems can lead to more interpretable and trustworthy AI systems, which are essential for promoting equity and reducing discrimination on social media.

**Abstract:** Stance detection is crucial for fostering a human-centric Web by analyzing user-generated content to identify biases and harmful narratives that undermine trust. With the development of Large Language Models (LLMs), existing approaches treat stance detection as a classification problem, providing robust methodologies for modeling complex group interactions and advancing capabilities in natural language tasks. However, these methods often lack interpretability, limiting their ability to offer transparent and understandable justifications for predictions. This study adopts a generative approach, where stance predictions include explicit, interpretable rationales, and integrates them into smaller language models through single-task and multitask learning. We find that incorporating reasoning into stance detection enables the smaller model (FlanT5) to outperform GPT-3.5's zero-shot performance, achieving an improvement of up to 9.57%. Moreover, our results show that reasoning capabilities enhance multitask learning performance but may reduce effectiveness in single-task settings. Crucially, we demonstrate that faithful rationales improve rationale distillation into SLMs, advancing efforts to build interpretable, trustworthy systems for addressing discrimination, fostering trust, and promoting equitable engagement on social media.

</details>


### [137] [Interpretable LLM-based Table Question Answering](https://arxiv.org/abs/2412.12386)

*Giang Nguyen, Ivan Brugere, Shubham Sharma, Sanjay Kariyappa, Anh Totti Nguyen, Freddy Lecue*

**Main category:** cs.CL

**Keywords:** Table Question Answering, Large Language Models, Interpretability, SQL, Healthcare

**Relevance Score:** 9

**TL;DR:** This paper introduces Plan-of-SQLs (POS), an interpretable Table Question Answering method utilizing Large Language Models, which generates clear explanations and efficient queries.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The need for interpretability in Table QA is essential, particularly in high-stakes fields such as finance and healthcare, where understanding the reasoning behind answers is critical.

**Method:** Plan-of-SQLs (POS) breaks down questions into atomic steps that are executable SQL commands, allowing for transparency in the decision-making process.

**Key Contributions:**

	1. Introduces a novel method for interpretable Table QA using atomic SQL commands
	2. Demonstrates significant efficiency improvements (up to 25x fewer LLM calls)
	3. Shows strong agreement in decision-making between LLMs and human users.

**Result:** POS outperforms other methods in generating high-quality explanations and achieves competitive accuracy on Table QA benchmarks while being significantly more efficient.

**Limitations:** 

**Conclusion:** POS demonstrates that LLMs can provide interpretable and reliable explanations for Table QA, enhancing the user’s ability to understand and trust the answers provided by AI systems.

**Abstract:** Interpretability in Table Question Answering (Table QA) is critical, especially in high-stakes domains like finance and healthcare. While recent Table QA approaches based on Large Language Models (LLMs) achieve high accuracy, they often produce ambiguous explanations of how answers are derived.   We propose Plan-of-SQLs (POS), a new Table QA method that makes the model's decision-making process interpretable. POS decomposes a question into a sequence of atomic steps, each directly translated into an executable SQL command on the table, thereby ensuring that every intermediate result is transparent. Through extensive experiments, we show that: First, POS generates the highest-quality explanations among compared methods, which markedly improves the users' ability to simulate and verify the model's decisions. Second, when evaluated on standard Table QA benchmarks (TabFact, WikiTQ, and FeTaQA), POS achieves QA accuracy that is competitive to existing methods, while also offering greater efficiency-requiring significantly fewer LLM calls and table database queries (up to 25x fewer)-and more robust performance on large-sized tables. Finally, we observe high agreement (up to 90.59% in forward simulation) between LLMs and human users when making decisions based on the same explanations, suggesting that LLMs could serve as an effective proxy for humans in evaluating Table QA explanations.

</details>


### [138] [Multimodal Contrastive Representation Learning in Augmented Biomedical Knowledge Graphs](https://arxiv.org/abs/2501.01644)

*Tien Dang, Viet Thanh Duy Nguyen, Minh Tuan Le, Truong-Son Hy*

**Main category:** cs.CL

**Keywords:** Biomedical Knowledge Graphs, Link Prediction, Graph Contrastive Learning, Knowledge Graph Embedding, Multimodal Data

**Relevance Score:** 8

**TL;DR:** This paper presents PrimeKG++, a novel multimodal approach for enhancing link prediction in Biomedical Knowledge Graphs (BKGs) by unifying embeddings from language models with Graph Contrastive Learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective link prediction in Biomedical Knowledge Graphs to uncover valuable relationships such as novel drug-disease relations.

**Method:** A multimodal approach that combines Language Models embeddings with Graph Contrastive Learning, along with a Knowledge Graph Embedding model for effective intra- and inter-entity relationship capturing.

**Key Contributions:**

	1. Introduction of PrimeKG++, an enriched knowledge graph incorporating multimodal data.
	2. Use of a novel multimodal approach that combines LMs with Graph Contrastive Learning for link prediction.
	3. Public availability of source code, pre-trained models, and enriched dataset for further research.

**Result:** The proposed method shows strong generalizability and accuracy in link predictions, even for unseen nodes, validated through experiments on PrimeKG++ and the DrugBank dataset.

**Limitations:** 

**Conclusion:** The integration of multimodal data significantly enhances link prediction performance and robustness in biomedical applications.

**Abstract:** Biomedical Knowledge Graphs (BKGs) integrate diverse datasets to elucidate complex relationships within the biomedical field. Effective link prediction on these graphs can uncover valuable connections, such as potential novel drug-disease relations. We introduce a novel multimodal approach that unifies embeddings from specialized Language Models (LMs) with Graph Contrastive Learning (GCL) to enhance intra-entity relationships while employing a Knowledge Graph Embedding (KGE) model to capture inter-entity relationships for effective link prediction. To address limitations in existing BKGs, we present PrimeKG++, an enriched knowledge graph incorporating multimodal data, including biological sequences and textual descriptions for each entity type. By combining semantic and relational information in a unified representation, our approach demonstrates strong generalizability, enabling accurate link predictions even for unseen nodes. Experimental results on PrimeKG++ and the DrugBank drug-target interaction dataset demonstrate the effectiveness and robustness of our method across diverse biomedical datasets. Our source code, pre-trained models, and data are publicly available at https://github.com/HySonLab/BioMedKG

</details>


### [139] [Multimodal Medical Code Tokenizer](https://arxiv.org/abs/2502.04397)

*Xiaorui Su, Shvat Messica, Yepeng Huang, Ruth Johnson, Lukas Fesser, Shanghua Gao, Faryad Sahneh, Marinka Zitnik*

**Main category:** cs.CL

**Keywords:** medical code tokenizer, EHR models, machine learning

**Relevance Score:** 9

**TL;DR:** MedTok is a multimodal medical code tokenizer that enhances tokenization of EHR data by incorporating text descriptions and relational context, leading to improved performance in clinical tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Foundation models trained on EHRs need effective tokenization of medical data, which traditional tokenizers inadequately address by treating medical codes as isolated tokens.

**Method:** MedTok processes text with a language model encoder for descriptions and utilizes a graph encoder for relational structures, quantizing both into a unified token space.

**Key Contributions:**

	1. Introduction of MedTok for multimodal medical code tokenization
	2. Improved model performance on EHR datasets
	3. Demonstrable application in medical QA systems

**Result:** MedTok significantly improves AUPRC across various EHR models: 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, especially enhancing drug recommendation tasks.

**Limitations:** 

**Conclusion:** MedTok serves as a unified tokenizer for medical codes that improves tokenization processes for medical foundation models, demonstrating broad utility in various operational and clinical applications.

**Abstract:** Foundation models trained on patient electronic health records (EHRs) require tokenizing medical data into sequences of discrete vocabulary items. Existing tokenizers treat medical codes from EHRs as isolated textual tokens. However, each medical code is defined by its textual description, its position in ontological hierarchies, and its relationships to other codes, such as disease co-occurrences and drug-treatment associations. Medical vocabularies contain more than 600,000 codes with critical information for clinical reasoning. We introduce MedTok, a multimodal medical code tokenizer that uses the text descriptions and relational context of codes. MedTok processes text using a language model encoder and encodes the relational structure with a graph encoder. It then quantizes both modalities into a unified token space, preserving modality-specific and cross-modality information. We integrate MedTok into five EHR models and evaluate it on operational and clinical tasks across in-patient and out-patient datasets, including outcome prediction, diagnosis classification, drug recommendation, and risk stratification. Swapping standard EHR tokenizers with MedTok improves AUPRC across all EHR models, by 4.10% on MIMIC-III, 4.78% on MIMIC-IV, and 11.32% on EHRShot, with the largest gains in drug recommendation. Beyond EHR modeling, we demonstrate using MedTok tokenizer with medical QA systems. Our results demonstrate the potential of MedTok as a unified tokenizer for medical codes, improving tokenization for medical foundation models.

</details>


### [140] [Mechanistic Interpretability of Emotion Inference in Large Language Models](https://arxiv.org/abs/2502.05489)

*Ala N. Tak, Amin Banayeeanzade, Anahita Bolourani, Mina Kian, Robin Jia, Jonathan Gratch*

**Main category:** cs.CL

**Keywords:** large language models, emotions, cognitive appraisal theory, affective computing, text generation

**Relevance Score:** 9

**TL;DR:** This study explores how autoregressive large language models (LLMs) predict emotions from text and identifies localized emotion representations, providing a psychologically plausible framework for emotional text generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the mechanisms through which autoregressive LLMs process and infer emotions from text, an area that has not been thoroughly examined.

**Method:** The study employs diverse model families and sizes and includes robustness checks to evaluate how LLMs represent emotions functionally localized in specific regions of the model.

**Key Contributions:**

	1. Identification of localized emotion representations in LLMs.
	2. Application of cognitive appraisal theory to emotion generation in LLMs.
	3. Demonstration of causal interventions to steer emotional outputs.

**Result:** Identified emotion representations align with cognitive appraisal theory and demonstrate causally intervening on appraisal concepts can influence emotional text generation outcomes.

**Limitations:** 

**Conclusion:** This research shows a novel method for precise emotional text generation with LLMs, which may enhance safety and alignment in sensitive emotional contexts.

**Abstract:** Large language models (LLMs) show promising capabilities in predicting human emotions from text. However, the mechanisms through which these models process emotional stimuli remain largely unexplored. Our study addresses this gap by investigating how autoregressive LLMs infer emotions, showing that emotion representations are functionally localized to specific regions in the model. Our evaluation includes diverse model families and sizes and is supported by robustness checks. We then show that the identified representations are psychologically plausible by drawing on cognitive appraisal theory, a well-established psychological framework positing that emotions emerge from evaluations (appraisals) of environmental stimuli. By causally intervening on construed appraisal concepts, we steer the generation and show that the outputs align with theoretical and intuitive expectations. This work highlights a novel way to causally intervene and precisely shape emotional text generation, potentially benefiting safety and alignment in sensitive affective domains.

</details>


### [141] [KMI: A Dataset of Korean Motivational Interviewing Dialogues for Psychotherapy](https://arxiv.org/abs/2502.05651)

*Hyunjong Kim, Suyeon Lee, Yeongjae Cho, Eunseo Ryu, Yohan Jo, Suran Seong, Sungzoon Cho*

**Main category:** cs.CL

**Keywords:** Motivational Interviewing, AI Chatbots, Mental Health, Synthetic Dataset, Large Language Models

**Relevance Score:** 9

**TL;DR:** Proposes a framework for AI-driven mental health chatbots using Motivational Interviewing (MI) techniques, including a synthetic dataset for training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing datasets for mental health chatbots and improve their effectiveness using MI principles.

**Method:** Developed a framework simulating MI sessions with an MI forecaster model and utilized LLMs for generating utterances, creating the KMI dataset with 1,000 dialogues in Korean.

**Key Contributions:**

	1. Introduction of the KMI dataset with 1,000 synthetic MI dialogues in Korean
	2. Development of an MI forecaster model to simulate therapist behavior
	3. Novel evaluation metrics based on MI theory for chatbot dialogues.

**Result:** Expert evaluation shows that the KMI dataset and dialogue model have high quality and practical applicability for training mental health chatbots.

**Limitations:** Focus on Korean language, which may limit applicability to English and other languages.

**Conclusion:** The study highlights the potential of MI-based approaches in enhancing AI-driven mental health services and introduces new metrics for evaluating chatbot dialogues.

**Abstract:** The increasing demand for mental health services has led to the rise of AI-driven mental health chatbots, though challenges related to privacy, data collection, and expertise persist. Motivational Interviewing (MI) is gaining attention as a theoretical basis for boosting expertise in the development of these chatbots. However, existing datasets are showing limitations for training chatbots, leading to a substantial demand for publicly available resources in the field of MI and psychotherapy. These challenges are even more pronounced in non-English languages, where they receive less attention. In this paper, we propose a novel framework that simulates MI sessions enriched with the expertise of professional therapists. We train an MI forecaster model that mimics the behavioral choices of professional therapists and employ Large Language Models (LLMs) to generate utterances through prompt engineering. Then, we present KMI, the first synthetic dataset theoretically grounded in MI, containing 1,000 high-quality Korean Motivational Interviewing dialogues. Through an extensive expert evaluation of the generated dataset and the dialogue model trained on it, we demonstrate the quality, expertise, and practicality of KMI. We also introduce novel metrics derived from MI theory in order to evaluate dialogues from the perspective of MI.

</details>


### [142] [Demystifying Singular Defects in Large Language Models](https://arxiv.org/abs/2502.07004)

*Haoqi Wang, Tong Zhang, Mathieu Salzmann*

**Main category:** cs.CL

**Keywords:** large language models, high-norm tokens, singular values, quantization schemes, internal mechanisms

**Relevance Score:** 8

**TL;DR:** This paper explores the causes and implications of high-norm tokens in large language models (LLMs), providing theoretical and empirical insights.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the mechanics behind high-norm tokens in LLMs is crucial as they differ from those in vision transformers and impact model performance.

**Method:** The authors analyze layer-wise singular directions, eigenvalues, and token pathways in LLMs to evaluate high-norm tokens and their implications.

**Key Contributions:**

	1. Theoretical insights into high-norm tokens in LLMs
	2. Empirical validation across various models
	3. Practical applications for quantization and LLM design

**Result:** The study identifies key factors behind high-norm tokens and showcases improvements in quantization schemes and design of LLM signatures based on their findings.

**Limitations:** 

**Conclusion:** The findings expand the understanding of singular defects in LLMs and suggest new directions for research in their internal mechanisms.

**Abstract:** Large transformer models are known to produce high-norm tokens. In vision transformers (ViTs), such tokens have been mathematically modeled through the singular vectors of the linear approximations of layers. However, in large language models (LLMs), the underlying causes of high-norm tokens remain largely unexplored, and their different properties from those of ViTs require a new analysis framework. In this paper, we provide both theoretical insights and empirical validation across a range of recent models, leading to the following observations: i) The layer-wise singular direction predicts the abrupt explosion of token norms in LLMs. ii) The negative eigenvalues of a layer explain its sudden decay. iii) The computational pathways leading to high-norm tokens differ between initial and noninitial tokens. iv) High-norm tokens are triggered by the right leading singular vector of the matrix approximating the corresponding modules. We showcase two practical applications of these findings: the improvement of quantization schemes and the design of LLM signatures. Our findings not only advance the understanding of singular defects in LLMs but also open new avenues for their application. We expect that this work will stimulate further research into the internal mechanisms of LLMs. Code is released at https://github.com/haoqiwang/singular_defect.

</details>


### [143] [Organize the Web: Constructing Domains Enhances Pre-Training Data Curation](https://arxiv.org/abs/2502.10341)

*Alexander Wettig, Kyle Lo, Sewon Min, Hannaneh Hajishirzi, Danqi Chen, Luca Soldaini*

**Main category:** cs.CL

**Keywords:** language models, data curation, domain mixing, pre-training data, automated annotation

**Relevance Score:** 9

**TL;DR:** The paper introduces WebOrganizer, a framework for organizing unstructured web data into taxonomies based on topic and format, improving language model pre-training data curation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the systematic approach to curating large unstructured datasets used for training language models.

**Method:** Developing taxonomies of web corpus contents and automatically annotating pre-training data using a large language model to create efficient classifiers for organizing data by topic and format.

**Key Contributions:**

	1. Introduction of the WebOrganizer framework for domain organization.
	2. Development of efficient classifiers for annotating pre-training data.
	3. Evidence that domain mixing improves model performance over traditional quality selection methods.

**Result:** WebOrganizer allows for effective mixing of different domain data, enhancing the performance of language models on downstream tasks compared to traditional quality-based data curation methods.

**Limitations:** 

**Conclusion:** Constructing and mixing domains is a valuable complement to quality-based methods, leading to better insights and performance in pre-training data curation.

**Abstract:** Modern language models are trained on large, unstructured datasets consisting of trillions of tokens and obtained by crawling the web. The unstructured nature makes it difficult to reason about their contents and develop systematic approaches to data curation. In this paper, we unpack monolithic web corpora by developing taxonomies of their contents and organizing them into domains. We introduce WebOrganizer, a framework for organizing web pages in terms of both their topic and format. Using these two complementary notions of domains, we automatically annotate pre-training data by distilling annotations from a large language model into efficient classifiers. This allows us to study how data from different domains should be mixed to improve models on downstream tasks, and we show that we can combine insights about effective topics and formats to further boost performance. We demonstrate that our domain mixing also improves existing methods that select data based on quality. Furthermore, we study and compare how quality-based methods will implicitly change the domain mixture. Overall, our work demonstrates that constructing and mixing domains provides a valuable complement to quality-based data curation methods, opening new avenues for effective and insightful pre-training data curation.

</details>


### [144] [Better Aligned with Survey Respondents or Training Data? Unveiling Political Leanings of LLMs on U.S. Supreme Court Cases](https://arxiv.org/abs/2502.18282)

*Shanshan Xu, T. Y. S. S Santosh, Yanai Elazar, Quirin Vogel, Barbara Plank, Matthias Grabmair*

**Main category:** cs.CL

**Keywords:** Large Language Models, Political Bias, Training Data, Human-AI Alignment, Audit Methodology

**Relevance Score:** 8

**TL;DR:** This paper investigates the influence of training data on political biases in Large Language Models (LLMs) and evaluates their alignment with human political opinions through a case study of US Supreme Court cases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how memorized patterns in training data impact the behavior of LLMs, specifically regarding political bias.

**Method:** A quantitative method was proposed to evaluate political leanings in the LLM's pretraining corpora, along with a case study examining 32 US Supreme Court cases.

**Key Contributions:**

	1. Proposed a method to evaluate political leanings in LLMs' training data.
	2. Investigated LLM political biases using US Supreme Court case studies.
	3. Revealed misalignment between LLM outputs and surveyed human opinions.

**Result:** LLMs strongly reflect the political leanings in their training data, with no strong correlation found between LLM outputs and human opinions.

**Limitations:** 

**Conclusion:** The study highlights the need for responsible curation of training data and the importance of auditing LLM memorization to achieve human-AI alignment.

**Abstract:** Recent works have shown that Large Language Models (LLMs) have a tendency to memorize patterns and biases present in their training data, raising important questions about how such memorized content influences model behavior. One such concern is the emergence of political bias in LLM outputs. In this paper, we investigate the extent to which LLMs' political leanings reflect memorized patterns from their pretraining corpora. We propose a method to quantitatively evaluate political leanings embedded in the large pretraining corpora. Subsequently we investigate to whom are the LLMs' political leanings more aligned with, their pretrainig corpora or the surveyed human opinions. As a case study, we focus on probing the political leanings of LLMs in 32 US Supreme Court cases, addressing contentious topics such as abortion and voting rights. Our findings reveal that LLMs strongly reflect the political leanings in their training data, and no strong correlation is observed with their alignment to human opinions as expressed in surveys. These results underscore the importance of responsible curation of training data, and the methodology for auditing the memorization in LLMs to ensure human-AI alignment.

</details>


### [145] [What Makes the Preferred Thinking Direction for LLMs in Multiple-choice Questions?](https://arxiv.org/abs/2502.18435)

*Yizhe Zhang, Richard Bai, Zijin Gu, Ruixiang Zhang, Jiatao Gu, Emmanuel Abbe, Samy Bengio, Navdeep Jaitly*

**Main category:** cs.CL

**Keywords:** Language Models, Right-to-Left Training, Multiple-Choice Questions, Knowledge Extraction, Reasoning

**Relevance Score:** 7

**TL;DR:** This paper investigates right-to-left (R2L) training as an alternative to left-to-right (L2R) autoregressive factorization in language models, particularly on multiple-choice questions (MCQs), finding that R2L can significantly outperform L2R models in various reasoning benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore whether alternative text distribution factorizations, specifically right-to-left (R2L), can provide better performance than the standard left-to-right (L2R) autoregressive approach in language models.

**Method:** Extensive experiments were conducted across various model sizes (2B-8B parameters) and training datasets, focusing on MCQ benchmarks and analyzing performance differences through controlled simulations.

**Key Contributions:**

	1. Demonstrated the effectiveness of R2L models over L2R models for MCQs.
	2. Provided theoretical insights into text distribution factorization and its impact on language modeling.
	3. Released code and checkpoints for further research.

**Result:** R2L models significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks.

**Limitations:** The study primarily focuses on MCQs and controlled simulation studies, which may not cover all real-world applications.

**Conclusion:** Exploring alternative factorizations of text distributions can improve LLM capabilities, providing insights into when each reasoning order might be advantageous.

**Abstract:** Language models usually use left-to-right (L2R) autoregressive factorization. However, L2R factorization may not always be the best inductive bias. Therefore, we investigate whether alternative factorizations of the text distribution could be beneficial in some tasks. We investigate right-to-left (R2L) training as a compelling alternative, focusing on multiple-choice questions (MCQs) as a test bed for knowledge extraction and reasoning. Through extensive experiments across various model sizes (2B-8B parameters) and training datasets, we find that R2L models can significantly outperform L2R models on several MCQ benchmarks, including logical reasoning, commonsense understanding, and truthfulness assessment tasks. Our analysis reveals that this performance difference may be fundamentally linked to multiple factors including calibration, computability, and directional conditional entropy. We analyze the impact of these factors through controlled simulation studies using arithmetic tasks, where the impacting factors can be better disentangled. Our work demonstrates that exploring alternative factorizations of the text distribution can lead to improvements in LLM capabilities and provides theoretical insights into optimal factorization towards approximating human language distribution, and when each reasoning order might be more advantageous. Our code and checkpoints are released at https://github.com/apple/ml-reversal-blessing.

</details>


### [146] [Time-MQA: Time Series Multi-Task Question Answering with Context Enhancement](https://arxiv.org/abs/2503.01875)

*Yaxuan Kong, Yiyuan Yang, Yoontae Hwang, Wenjie Du, Stefan Zohren, Zhangyang Wang, Ming Jin, Qingsong Wen*

**Main category:** cs.CL

**Keywords:** time series, multi-task learning, question answering, language models, TSQA dataset

**Relevance Score:** 6

**TL;DR:** Introducing Time Series Multi-Task Question Answering (Time-MQA) framework for natural language queries on time series data, emphasizing on enhanced reasoning capabilities using a comprehensive dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited focus of existing methods on narrow tasks such as forecasting or anomaly detection in time series data by enabling a wider range of natural language queries.

**Method:** Developed a unified framework called Time-MQA that utilizes a large-scale TSQA dataset with approximately 200k question-answer pairs to facilitate various tasks in time series.

**Key Contributions:**

	1. Introduction of Time-MQA framework for multi-task question answering in time series
	2. Creation of the TSQA dataset with diverse time series and extensive question-answer pairs
	3. Enhanced reasoning abilities in language models when pre-trained on TSQA dataset

**Result:** The framework demonstrated that continual pre-training of large language models on the TSQA dataset significantly improved their reasoning capabilities, expanding beyond numeric tasks to support intuitive interactions with temporal data.

**Limitations:** 

**Conclusion:** The TSQA dataset and the Time-MQA framework provide a foundation for robust model development and open-source resources for further research in time series analysis.

**Abstract:** Time series data are foundational in finance, healthcare, and energy domains. However, most existing methods and datasets remain focused on a narrow spectrum of tasks, such as forecasting or anomaly detection. To bridge this gap, we introduce Time Series Multi-Task Question Answering (Time-MQA), a unified framework that enables natural language queries across multiple time series tasks - numerical analytical tasks and open-ended question answering with reasoning. Central to Time-MQA is the TSQA dataset, a large-scale dataset containing $\sim$200k question-answer pairs derived from diverse time series spanning environment, traffic, etc. This comprehensive resource covers various time series lengths and promotes robust model development. We further demonstrate how continually pre-training large language models (Mistral 7B, Llama-3 8B, and Qwen-2.5 7B) on the TSQA dataset enhanced time series reasoning capabilities, moving beyond mere numeric tasks and enabling more advanced and intuitive interactions with temporal data. The complete TSQA dataset, models, user study questionnaires for evaluation, and other related materials have been open-sourced.

</details>


### [147] [Enough Coin Flips Can Make LLMs Act Bayesian](https://arxiv.org/abs/2503.04722)

*Ritwik Gupta, Rodolfo Corona, Jiaxin Ge, Eric Wang, Dan Klein, Trevor Darrell, David M. Chan*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-Context Learning, Bayesian Reasoning

**Relevance Score:** 9

**TL;DR:** This paper investigates the reasoning capabilities of large language models (LLMs) through in-context learning, finding that LLMs exhibit Bayesian reasoning with biased priors and demonstrate structured updates based on evidence provided through few-shot examples.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models utilize in-context learning for reasoning, specifically whether they employ Bayesian frameworks or pattern matching in their decision-making processes.

**Method:** Controlled experiments were conducted using biased coin flips to assess LLMs' reasoning capabilities, examining their prior biases, reliance on evidence, adherence to Bayesian updates, and the effects of attention magnitude.

**Key Contributions:**

	1. Investigation of LLMs' in-context learning and reasoning
	2. Demonstration of biased priors in zero-shot settings
	3. Evidence supporting Bayesian posterior updates in LLMs.

**Result:** The study reveals that LLMs exhibit biased priors leading to divergence in zero-shot contexts, that in-context evidence predominantly influences their reasoning over explicit biases, that they generally follow Bayesian updating procedures, and that attention magnitude has little impact on inference outcomes.

**Limitations:** The experiments are limited to a controlled setting and may not fully represent LLM behavior in more complex or varied contexts.

**Conclusion:** LLMs are capable of updating their priors in a Bayesian framework when provided with sufficient in-context demonstrations, suggesting their reasoning ability is more structured than mere pattern matching.

**Abstract:** Large language models (LLMs) exhibit the ability to generalize given few-shot examples in their input prompt, an emergent capability known as in-context learning (ICL). We investigate whether LLMs use ICL to perform structured reasoning in ways that are consistent with a Bayesian framework or rely on pattern matching. Using a controlled setting of biased coin flips, we find that: (1) LLMs often possess biased priors, causing initial divergence in zero-shot settings, (2) in-context evidence outweighs explicit bias instructions, (3) LLMs broadly follow Bayesian posterior updates, with deviations primarily due to miscalibrated priors rather than flawed updates, and (4) attention magnitude has negligible effect on Bayesian inference. With sufficient demonstrations of biased coin flips via ICL, LLMs update their priors in a Bayesian manner.

</details>


### [148] [Gumiho: A Hybrid Architecture to Prioritize Early Tokens in Speculative Decoding](https://arxiv.org/abs/2503.10135)

*Jinze Li, Yixing Xu, Haiduo Huang, Xuanwu Yin, Dong Li, Edith C. H. Ngai, Emad Barsoum*

**Main category:** cs.CL

**Keywords:** Speculative Decoding, Transformer, Auto-regressive Models, Machine Learning, Token Generation

**Relevance Score:** 8

**TL;DR:** Gumiho is a hybrid model for efficient auto-regressive token generation in LLMs, balancing accuracy and efficiency through a unique architecture.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of multi-token generation in LLMs by addressing the unequal importance of tokens in a sequence during prediction.

**Method:** Gumiho employs a combination of serial heads with advanced Transformer architecture for early tokens and lightweight parallel MLP heads for later tokens.

**Key Contributions:**

	1. Introduction of Gumiho, a hybrid model for token generation
	2. Theoretical analysis of token importance in sequences
	3. Experimental results showing improved performance compared to existing methods.

**Result:** Gumiho demonstrates improved performance over existing methods in the token generation process of LLMs, validating its effectiveness through experimental results.

**Limitations:** 

**Conclusion:** The proposed hybrid model enhances both accuracy and efficiency in LLM token generation by appropriately prioritizing early tokens.

**Abstract:** Speculative decoding (SPD) aims to accelerate the auto-regressive token generation process of a target Large Language Model (LLM). Some approaches employ a draft model with multiple heads to predict a sequence of future tokens, where each head handles a token in the sequence. The target LLM verifies the predicted sequence and accepts aligned tokens, enabling efficient multi-token generation. However, existing methods assume that all tokens within a sequence are equally important, employing identical head structures and relying on a single-generation paradigm, either serial or parallel. To this end, we theoretically demonstrate that initial tokens in the draft sequence are more important than later ones. Building on this insight, we propose Gumiho, a hybrid model combining serial and parallel heads. Specifically, given the critical importance of early tokens, we employ a sophisticated Transformer architecture for the early draft heads in a serial configuration to improve accuracy. For later tokens, we utilize multiple lightweight MLP heads operating in parallel to enhance efficiency. By allocating more advanced model structures and longer running times to the early heads, Gumiho achieves improved overall performance. The experimental results demonstrate that our method outperforms existing approaches, fully validating its effectiveness.

</details>


### [149] [Explainable Sentiment Analysis with DeepSeek-R1: Performance, Efficiency, and Few-Shot Learning](https://arxiv.org/abs/2503.11655)

*Donghao Huang, Zhaoxia Wang*

**Main category:** cs.CL

**Keywords:** DeepSeek-R1, sentiment analysis, few-shot learning, explainability, large language models

**Relevance Score:** 8

**TL;DR:** DeepSeek-R1 is an open-source reasoning model that outperforms GPT-4o and GPT-4o-mini in few-shot sentiment analysis tasks, achieving higher accuracy and explainability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenging balance between accuracy, efficiency, and explainability in sentiment analysis using large language models.

**Method:** A comprehensive evaluation of DeepSeek-R1 is performed against OpenAI's models using systematic few-shot learning experiments documenting their performance metrics.

**Key Contributions:**

	1. First comprehensive evaluation of DeepSeek-R1 against GPT-4o models in sentiment analysis.
	2. Demonstrates superior few-shot learning efficiency with significant performance improvements.
	3. Offers enhanced explainability through transparent reasoning traces.

**Result:** DeepSeek-R1 achieves a 91.39% F1 score on 5-class sentiment and 99.31% accuracy on binary sentiment analysis with just 5 shots, significantly outperforming GPT-4o in few-shot efficiency.

**Limitations:** Reduced throughput due to the reasoning process.

**Conclusion:** While DeepSeek-R1 has reduced throughput due to its reasoning process, it is established as a powerful and interpretable alternative model for sentiment analysis tasks.

**Abstract:** Large language models (LLMs) have transformed sentiment analysis, yet balancing accuracy, efficiency, and explainability remains a critical challenge. This study presents the first comprehensive evaluation of DeepSeek-R1--an open-source reasoning model--against OpenAI's GPT-4o and GPT-4o-mini. We test the full 671B model and its distilled variants, systematically documenting few-shot learning curves. Our experiments show DeepSeek-R1 achieves a 91.39\% F1 score on 5-class sentiment and 99.31\% accuracy on binary tasks with just 5 shots, an eightfold improvement in few-shot efficiency over GPT-4o. Architecture-specific distillation effects emerge, where a 32B Qwen2.5-based model outperforms the 70B Llama-based variant by 6.69 percentage points. While its reasoning process reduces throughput, DeepSeek-R1 offers superior explainability via transparent, step-by-step traces, establishing it as a powerful, interpretable open-source alternative.

</details>


### [150] [LLM Braces: Straightening Out LLM Predictions with Relevant Sub-Updates](https://arxiv.org/abs/2503.16334)

*Ying Shen, Lifu Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Human-Computer Interaction, Sentiment Control, Transformer Models, Text Generation

**Relevance Score:** 9

**TL;DR:** LLMBRACES enhances Transformer-based LLMs by modulating sub-update contributions in FFN layers to improve accuracy and control over outputs, particularly for sentiment and toxicity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve performance and controllability of Transformer-based LLMs by optimizing contributions of value vectors in FFN layers to enhance prediction and generation characteristics.

**Method:** LLMBRACES computes relevance scores for value vectors in FFN layers and uses these scores to dynamically adjust the contribution of sub-updates during text generation.

**Key Contributions:**

	1. Introduction of LLMBRACES for relevance-based adjustment of sub-update contributions in LLMs.
	2. Substantial improvements in fine-tuning and zero-shot performance across multiple LLM architectures.
	3. Capability for controlled output generation, including sentiment moderation and toxicity reduction.

**Result:** LLMBRACES outperforms baseline methods in fine-tuning and zero-shot scenarios while using 75% fewer tunable parameters, demonstrating excellence in sentiment control and toxicity reduction.

**Limitations:** 

**Conclusion:** LLMBRACES provides a novel approach for enhancing LLM output accuracy and customizing generation settings through relevant control mechanisms, showing promising applications in various contexts.

**Abstract:** Recent findings reveal that much of the knowledge in a Transformer-based Large Language Model (LLM) is encoded in its feed-forward (FFN) layers, where each FNN layer can be interpreted as the summation of sub-updates, each corresponding to a weighted column vector from the FFN's value parameter matrix that often encodes human-interpretable concepts. In light of this, we hypothesize that model performance and behaviors can be further enhanced and controlled by modulating the contributions of these sub-updates based on their relevance to the input or target output style, and propose LLMBRACES, a novel and efficient method that computes relevance scores associated with value vectors in FFN layers and leverages these scores to dynamically adjust the contribution of sub-updates. By optimizing sub-update contributions, LLMBRACES refines the prediction process, leading to more accurate and reliable outputs, much like a 'brace' providing support and stability. Moreover, LLMBRACES can be extended to support conditional control over generation characteristics, such as sentiment, thereby offering fine-grained steering of LLM outputs. Extensive experiments on various LLMs-including Qwen2.5-1.5B, Llama2-7B, and Llama3-8B-demonstrate that LLMBRACES outperforms baseline approaches in both fine-tuning and zero-shot settings while requiring significantly fewer tunable parameters, up to 75% fewer compared to LoRA. Furthermore, LLMBRACES excels in sentiment-controlled generation and toxicity reduction, highlighting its potential for flexible, controlled text generation across applications.

</details>


### [151] [Automating Adjudication of Cardiovascular Events Using Large Language Models](https://arxiv.org/abs/2503.17222)

*Sonish Sivarajkumar, Kimia Ameri, Chuqin Li, Yanshan Wang, Min Jiang*

**Main category:** cs.CL

**Keywords:** Cardiovascular Events, Large Language Models, Clinical Trials, Event Adjudication, AI in Healthcare

**Relevance Score:** 9

**TL;DR:** The study presents a framework using LLMs to automate the adjudication of cardiovascular events in clinical trials, improving efficiency and consistency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of manual adjudication in clinical trials for cardiovascular events, which is time-consuming and biased.

**Method:** A two-stage approach: LLM-based event information extraction from unstructured data, followed by LLM-guided adjudication based on clinical guidelines.

**Key Contributions:**

	1. Development of a novel framework for LLM-based adjudication of cardiovascular events
	2. Introduction of the CLEART score for evaluating clinical reasoning quality
	3. Demonstrated substantial reduction in adjudication time and enhanced consistency.

**Result:** Achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication; introduced the CLEART score for evaluating AI-generated clinical reasoning.

**Limitations:** The study does not explore the generalizability of the approach to other medical domains or types of events.

**Conclusion:** The framework shows significant potential for reducing adjudication time and costs while ensuring high-quality outcomes and faster risk identification in clinical trials.

**Abstract:** Cardiovascular events, such as heart attacks and strokes, remain a leading cause of mortality globally, necessitating meticulous monitoring and adjudication in clinical trials. This process, traditionally performed manually by clinical experts, is time-consuming, resource-intensive, and prone to inter-reviewer variability, potentially introducing bias and hindering trial progress. This study addresses these critical limitations by presenting a novel framework for automating the adjudication of cardiovascular events in clinical trials using Large Language Models (LLMs). We developed a two-stage approach: first, employing an LLM-based pipeline for event information extraction from unstructured clinical data and second, using an LLM-based adjudication process guided by a Tree of Thoughts approach and clinical endpoint committee (CEC) guidelines. Using cardiovascular event-specific clinical trial data, the framework achieved an F1-score of 0.82 for event extraction and an accuracy of 0.68 for adjudication. Furthermore, we introduce the CLEART score, a novel, automated metric specifically designed for evaluating the quality of AI-generated clinical reasoning in adjudicating cardiovascular events. This approach demonstrates significant potential for substantially reducing adjudication time and costs while maintaining high-quality, consistent, and auditable outcomes in clinical trials. The reduced variability and enhanced standardization also allow for faster identification and mitigation of risks associated with cardiovascular therapies.

</details>


### [152] [MetaSynth: Meta-Prompting-Driven Agentic Scaffolds for Diverse Synthetic Data Generation](https://arxiv.org/abs/2504.12563)

*Haris Riaz, Sourav Bhabesh, Vinayak Arannil, Miguel Ballesteros, Graham Horwood*

**Main category:** cs.CL

**Keywords:** synthetic data, domain adaptation, language models, meta-prompting, diversity

**Relevance Score:** 8

**TL;DR:** MetaSynth enhances the diversity of synthetic data for domain adaptation in LLMs, demonstrating effective improvements in Finance and Biomedicine with limited token usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenge of low diversity in synthetic data generated by LLMs, which limits its utility in adapting models to specific domains.

**Method:** MetaSynth employs meta-prompting, where a language model orchestrates multiple expert LLM agents to collaboratively generate synthetic data, improving its diversity.

**Key Contributions:**

	1. Introduction of MetaSynth for generating diverse synthetic data
	2. Demonstrated effective domain adaptation for Finance and Biomedicine
	3. Evaluation using seven automated metrics to assess diversity

**Result:** Using 25 million tokens of synthetic data from MetaSynth, we adapted Mistral-7B-v0.3 to Finance and Biomedicine, achieving performance improvements of 4.08% and 13.75% respectively, compared to base LLM.

**Limitations:** Focus on only two specialized domains; results may vary in other domains or tasks.

**Conclusion:** Few million tokens of diverse synthetic data can effectively support domain adaptation in LLMs without integrating real data, as evidenced by improved model performance.

**Abstract:** Recent smaller language models such Phi-3.5 and Phi-4 rely on synthetic data generated using larger Language models. Questions remain about leveraging synthetic data for other use cases, such as adapting LLMs to specific domains. A key limitation of synthetic data is low diversity, which negatively impacts its downstream applicability for improving other models. To address this, we propose MetaSynth, a method for generating synthetic data that enhances diversity through meta-prompting, where a language model orchestrates multiple "expert" LLM agents to collaboratively generate data. Using only 25 million tokens of synthetic data generated with MetaSynth, we successfully adapt a well-trained LLM (Mistral-7B-v0.3) to two specialized domains-Finance and Biomedicine-without compromising the capabilities of the resulting model in general tasks. In addition, we evaluate the diversity of our synthetic data using seven automated metrics, and find that it approaches the diversity of LLM pre-training corpora.   Continually pre-training Mistral-7B-v0.3 with MetaSynth notably outperforms the base LLM, showing improvements of up to 4.08% in Finance and 13.75% in Biomedicine. The same model shows degraded performance when trained on data generated using a template prompt, even when the template includes prior generations and varying In-Context exemplars of real data. Our findings suggest that a few million tokens of diverse synthetic data without mixing any real data, is sufficient for effective domain adaptation when using MetaSynth.

</details>


### [153] [SConU: Selective Conformal Uncertainty in Large Language Models](https://arxiv.org/abs/2504.14154)

*Zhiyuan Wang, Qingni Wang, Yue Zhang, Tianlong Chen, Xiaofeng Zhu, Xiaoshuang Shi, Kaidi Xu*

**Main category:** cs.CL

**Keywords:** Selective Conformal Uncertainty, large language models, uncertainty quantification

**Relevance Score:** 8

**TL;DR:** The paper introduces Selective Conformal Uncertainty (SConU), a method for managing uncertainty in large language model applications by implementing significance tests to detect outliers and improve prediction reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of miscoverage rates and unactionable predictions in existing conformal uncertainty frameworks for large language models.

**Method:** The paper proposes Selective Conformal Uncertainty (SConU), which utilizes conformal p-values to determine deviations from the uncertainty distribution of the calibration set.

**Key Contributions:**

	1. Introduction of Selective Conformal Uncertainty (SConU) methodology
	2. Implementation of significance tests to identify uncertainty data outliers
	3. Enhanced prediction reliability and efficiency across various contexts

**Result:** SConU enhances the management of miscoverage rates and the efficiency of predictions in both single-domain and interdisciplinary contexts, especially for high-stakes question-answering tasks.

**Limitations:** 

**Conclusion:** The method provides a rigorous approach to approximating conditional coverage, improving the performance of large language models in real-world applications.

**Abstract:** As large language models are increasingly utilized in real-world applications, guarantees of task-specific metrics are essential for their reliable deployment. Previous studies have introduced various criteria of conformal uncertainty grounded in split conformal prediction, which offer user-specified correctness coverage. However, existing frameworks often fail to identify uncertainty data outliers that violate the exchangeability assumption, leading to unbounded miscoverage rates and unactionable prediction sets. In this paper, we propose a novel approach termed Selective Conformal Uncertainty (SConU), which, for the first time, implements significance tests, by developing two conformal p-values that are instrumental in determining whether a given sample deviates from the uncertainty distribution of the calibration set at a specific manageable risk level. Our approach not only facilitates rigorous management of miscoverage rates across both single-domain and interdisciplinary contexts, but also enhances the efficiency of predictions. Furthermore, we comprehensively analyze the components of the conformal procedures, aiming to approximate conditional coverage, particularly in high-stakes question-answering tasks.

</details>


### [154] [GeometryZero: Improving Geometry Solving for LLM with Group Contrastive Policy Optimization](https://arxiv.org/abs/2506.07160)

*Yikun Wang, Yibin Wang, Dianyi Wang, Zimian Peng, Qipeng Guo, Dacheng Tao, Jiaqi Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Geometric Reasoning, Reinforcement Learning

**Relevance Score:** 7

**TL;DR:** The paper introduces Group Contrastive Policy Optimization (GCPO), a reinforcement learning framework that enhances geometric reasoning in smaller models by effectively using auxiliary constructions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve geometric problem-solving capabilities in LLMs without incurring high computational costs associated with large models.

**Method:** The authors developed the Group Contrastive Policy Optimization (GCPO) framework, which implements Group Contrastive Masking for contextual rewards and a length reward to encourage longer reasoning chains.

**Key Contributions:**

	1. Introduction of Group Contrastive Policy Optimization (GCPO) framework
	2. Development of GeometryZero models for geometric reasoning
	3. Demonstrated performance improvements over existing methods on geometric benchmarks

**Result:** GeometryZero models, built on GCPO, significantly outperform existing models like GRPO with an average improvement of 4.29% on geometric benchmarks.

**Limitations:** 

**Conclusion:** GCPO enables effective training of smaller models for geometric reasoning while leveraging auxiliary constructions and providing relevant reward signals.

**Abstract:** Recent advances in large language models (LLMs) have demonstrated remarkable capabilities across diverse domains, particularly in mathematical reasoning, amid which geometry problem solving remains a challenging area where auxiliary construction plays a enssential role. Existing approaches either achieve suboptimal performance or rely on massive LLMs (e.g., GPT-4o), incurring massive computational costs. We posit that reinforcement learning with verifiable reward (e.g., GRPO) offers a promising direction for training smaller models that effectively combine auxiliary construction with robust geometric reasoning. However, directly applying GRPO to geometric reasoning presents fundamental limitations due to its dependence on unconditional rewards, which leads to indiscriminate and counterproductive auxiliary constructions. To address these challenges, we propose Group Contrastive Policy Optimization (GCPO), a novel reinforcement learning framework featuring two key innovations: (1) Group Contrastive Masking, which adaptively provides positive or negative reward signals for auxiliary construction based on contextual utility, and a (2) length reward that promotes longer reasoning chains. Building on GCPO, we develop GeometryZero, a family of affordable-size geometric reasoning models that judiciously determine when to employ auxiliary construction. Our extensive empirical evaluation across popular geometric benchmarks (Geometry3K, MathVista) demonstrates that GeometryZero models consistently outperform baselines (e.g. GRPO), achieving an average improvement of 4.29% across all benchmarks.

</details>


### [155] [How to Retrieve Examples in In-context Learning to Improve Conversational Emotion Recognition using Large Language Models?](https://arxiv.org/abs/2506.20199)

*Mengqi Wang, Tiantian Feng, Shrikanth Narayanan*

**Main category:** cs.CL

**Keywords:** Conversational Emotion Recognition, Large Language Models, In-Context Learning, Example Retrieval, Emotion Recognition

**Relevance Score:** 9

**TL;DR:** This study explores improving conversational emotion recognition (CER) using large language models (LLMs) through high-quality example retrieval for in-context learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Creating high-performing applications with high accuracy remains challenging for subjective tasks like emotion recognition.

**Method:** Various strategies for retrieving high-quality examples, including random and augmented example retrieval, were proposed and analyzed, focusing on the impact of conversational context on CER accuracy.

**Key Contributions:**

	1. Investigation of in-context learning for emotional recognition using LLMs
	2. Proposed augmented example retrieval methods
	3. Analysis of conversational context impact on emotion recognition accuracy

**Result:** Augmented example retrieval consistently outperformed other techniques across three datasets (IEMOCAP, MELD, EmoryNLP), emphasizing the need for coherent targeted examples and paraphrasing.

**Limitations:** Study limited to specific datasets and might not generalize to all conversational contexts or emotional nuances.

**Conclusion:** Enhancing CER effectiveness requires innovative example retrieval strategies which directly improve the accuracy of LLMs in conversational applications.

**Abstract:** Large language models (LLMs) have enabled a wide variety of real-world applications in various domains. However, creating a high-performing application with high accuracy remains challenging, particularly for subjective tasks like emotion recognition. Inspired by the SLT 2024 GenSER Challenge, this study investigates approaches to improving conversational emotion recognition (CER) by LLMs. Specifically, we explore how to retrieve high-quality examples in in-context learning (ICL) to enhance CER. We propose various strategies based on random and augmented example retrieval and also analyze the impact of conversational context on CER accuracy. Experiments were conducted on the three datasets including IEMOCAP, MELD and EmoryNLP. The results show that augmented example retrieval consistently outperforms other techniques under investigation across all datasets, highlighting the importance of retrieving coherent targeted examples and enhancing them through paraphrasing.

</details>
