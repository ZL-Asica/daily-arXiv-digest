# 2025-07-29

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 44]

- [cs.CL](#cs.CL) [Total: 107]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [The Architecture of Cognitive Amplification: Enhanced Cognitive Scaffolding as a Resolution to the Comfort-Growth Paradox in Human-AI Cognitive Integration](https://arxiv.org/abs/2507.19483)

*Giuseppe Riva*

**Main category:** cs.HC

**Keywords:** Human-AI interaction, Cognitive development, Enhanced Cognitive Scaffolding, Artificial Intelligence, Education

**Relevance Score:** 9

**TL;DR:** The paper introduces Enhanced Cognitive Scaffolding, a framework to optimize human-AI interaction by shifting AI's role from a convenient assistant to a dynamic mentor, addressing the comfort-growth paradox.

**Read time:** 39 min

<details>
  <summary>Details</summary>

**Motivation:** AI systems evolve from tools to cognitive collaborators, enhancing human cognition but risking cognitive complacency. This paper seeks to resolve these issues through a new framework.

**Method:** The framework consists of three dimensions: (1) Progressive Autonomy, (2) Adaptive Personalization, and (3) Cognitive Load Optimization, supported by research in various domains.

**Key Contributions:**

	1. Introduction of Enhanced Cognitive Scaffolding framework
	2. Integration of Vygotskian theories and educational principles
	3. Empirical support across multiple domains for improved cognitive outcomes

**Result:** Research shows that Enhanced Cognitive Scaffolding can accelerate skill acquisition, improve self-regulation, and enhance higher-order thinking across fields.

**Limitations:** The framework requires careful implementation to avoid overwhelming users or reinforcing biases.

**Conclusion:** Prioritizing cognitive development over convenience can lead to enhanced human cognition while preventing dependency and bias amplification in AI interactions.

**Abstract:** AI systems now function as cognitive extensions, evolving from tools to active cognitive collaborators within human-AI integrated systems. While these systems can amplify cognition - enhancing problem-solving, learning, and creativity - they present a fundamental "comfort-growth paradox": AI's user-friendly nature may foster intellectual stagnation by minimizing cognitive friction necessary for development. As AI aligns with user preferences and provides frictionless assistance, it risks inducing cognitive complacency rather than promoting growth. We introduce Enhanced Cognitive Scaffolding to resolve this paradox - reconceptualizing AI from convenient assistant to dynamic mentor. Drawing from Vygotskian theories, educational scaffolding principles, and AI ethics, our framework integrates three dimensions: (1) Progressive Autonomy, where AI support gradually fades as user competence increases; (2) Adaptive Personalization, tailoring assistance to individual needs and learning trajectories; and (3) Cognitive Load Optimization, balancing mental effort to maximize learning while minimizing unnecessary complexity. Research across educational, workplace, creative, and healthcare domains supports this approach, demonstrating accelerated skill acquisition, improved self-regulation, and enhanced higher-order thinking. The framework includes safeguards against risks like dependency, skill atrophy, and bias amplification. By prioritizing cognitive development over convenience in human-AI interaction, Enhanced Cognitive Scaffolding offers a pathway toward genuinely amplified cognition while safeguarding autonomous thought and continuous learning.

</details>


### [2] [Creativity as a Human Right: Design Considerations for Computational Creativity Systems](https://arxiv.org/abs/2507.19485)

*Alayt Issak*

**Main category:** cs.HC

**Keywords:** Creativity, Computational Creativity, Human Rights, Universal Declaration of Human Rights, Design Considerations

**Relevance Score:** 4

**TL;DR:** The paper explores the connection between creativity and Computational Creativity (CC) systems as outlined in the Universal Declaration of Human Rights (UDHR), providing design considerations for CC systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the importance of creativity as a Human Right and inform the design of Computational Creativity systems.

**Method:** The methodology involves examining five articles from the UDHR and demonstrating their application in the context of CC systems, leading to design considerations.

**Key Contributions:**

	1. Establishment of creativity as a Human Right in the context of CC.
	2. Design considerations for developing CC systems based on UDHR articles.
	3. Linking human rights to the design of intelligent systems.

**Result:** Findings establish a relationship between creativity and CC systems, emphasizing creativity as a core component in their design.

**Limitations:** 

**Conclusion:** The study presents design considerations for CC systems based on the implications of creativity outlined in the UDHR.

**Abstract:** We investigate creativity that is underlined in the Universal Declaration of Human Rights (UDHR) to present design considerations for Computational Creativity (CC) systems. We find this declaration to describe creativity in salient aspects and bring to light creativity as a Human Right attributed to the Fourth Generation of such rights. This generation of rights attributes CC systems and the evolving nature of interaction with entities of shared intelligence. Our methodology examines five of thirty articles from the UDHR and demonstrates each article with actualizations concluding with design considerations for each. We contribute our findings to ground the relationship between creativity and CC systems.

</details>


### [3] [Confirmation bias: A challenge for scalable oversight](https://arxiv.org/abs/2507.19486)

*Gabriel Recchia, Chatrik Singh Mangat, Jinu Nyachhyon, Mridul Sharma, Callum Canavan, Dylan Epstein-Gross, Muhammed Abdulbari*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Oversight Protocols, AI Model Evaluation, Bias in Evaluators, Model Capability

**Relevance Score:** 6

**TL;DR:** This paper studies the effectiveness of scalable oversight protocols for AI model evaluation, highlighting biases that affect human evaluators and presenting mixed results on performance under varying conditions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how human evaluators can accurately verify advanced AI models while being subject to biases that can cause errors in judgment.

**Method:** Conducted two studies testing simple oversight protocols which informed evaluators that models are mostly correct. Evaluators' performance and confidence were measured across different scenarios.

**Key Contributions:**

	1. Examination of oversight protocols under human biases
	2. Insights on evaluator confidence and performance
	3. Reanalysis of previous optimistic findings regarding simple protocols

**Result:** The studies found no overall advantage for the oversight protocols; however, showing arguments for both answers improved accuracy in some aspects. Evaluators became overconfident in online research scenarios even when the model's answers were incorrect.

**Limitations:** Results may vary with different evaluators or contexts, and factors influencing confidence and accuracy need further exploration.

**Conclusion:** The effectiveness of oversight protocols is dependent on evaluator biases, and reliance on them doesn't necessarily outperform simple deference to models, especially as model capabilities increase.

**Abstract:** Scalable oversight protocols aim to empower evaluators to accurately verify AI models more capable than themselves. However, human evaluators are subject to biases that can lead to systematic errors. We conduct two studies examining the performance of simple oversight protocols where evaluators know that the model is "correct most of the time, but not all of the time". We find no overall advantage for the tested protocols, although in Study 1, showing arguments in favor of both answers improves accuracy in cases where the model is incorrect. In Study 2, participants in both groups become more confident in the system's answers after conducting online research, even when those answers are incorrect. We also reanalyze data from prior work that was more optimistic about simple protocols, finding that human evaluators possessing knowledge absent from models likely contributed to their positive results--an advantage that diminishes as models continue to scale in capability. These findings underscore the importance of testing the degree to which oversight protocols are robust to evaluator biases, whether they outperform simple deference to the model under evaluation, and whether their performance scales with increasing problem difficulty and model capability.

</details>


### [4] [E-polis: Gamifying Sociological Surveys through Serious Games -- A Data Analysis Approach Applied to Multiple-Choice Question Responses Datasets](https://arxiv.org/abs/2507.19488)

*Alexandros Gazis, Eleftheria Katsiri*

**Main category:** cs.HC

**Keywords:** gamification, sociological research, youth opinions, serious games, middleware architecture

**Relevance Score:** 4

**TL;DR:** E-polis is a serious digital game designed to study young people's political opinions through gameplay that affects a virtual city.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and visualize young people's political opinions and behaviors in shaping a city's future.

**Method:** E-polis allows players to answer sociological questions through gameplay, with their choices impacting the game's city structure and environment.

**Key Contributions:**

	1. Novel approach to gamifying sociological research
	2. Middleware architecture for data analysis
	3. Engaging method for understanding political opinions among youth

**Result:** The game's innovative middleware architecture facilitates detailed analysis of players' responses and the relationship between gameplay and political opinions.

**Limitations:** 

**Conclusion:** E-polis provides a unique platform for gathering and analyzing data on youth political opinions, contributing to the field of serious games.

**Abstract:** E-polis is a serious digital game designed to gamify sociological surveys studying young people's political opinions. In this platform game, players navigate a digital world, encountering quests posing sociological questions. Players' answers shape the city-game world, altering building structures based on their choices. E-polis is a serious game, not a government simulation, aiming to understand players' behaviors and opinions thus we do not train the players but rather understand them and help them visualize their choices in shaping a city's future. Also, it is noticed that no correct or incorrect answers apply. Moreover, our game utilizes a novel middleware architecture for development, diverging from typical asset prefab scene and script segregation. This article presents the data layer of our game's middleware, specifically focusing on data analysis based on respondents' gameplay answers. E-polis represents an innovative approach to gamifying sociological research, providing a unique platform for gathering and analyzing data on political opinions among youth and contributing to the broader field of serious games.

</details>


### [5] [RISEE: A Highly Interactive Naturalistic Driving Trajectories Dataset with Human Subjective Risk Perception and Eye-tracking Information](https://arxiv.org/abs/2507.19490)

*Xinzheng Wu, Junyi Chen, Peiyi Wang, Shunxiang Chen, Yong Shen*

**Main category:** cs.HC

**Keywords:** autonomous driving, human factors, eye-tracking, risk evaluation, naturalistic driving

**Relevance Score:** 4

**TL;DR:** This paper introduces the RISEE dataset, which integrates human subjective evaluations and eye-tracking data alongside traditional driving trajectories to enhance autonomous driving decision-making by incorporating human factors.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the lack of human-related information and safety-critical scenarios in existing autonomous driving datasets.

**Method:** The RISEE dataset was created by first recording drone-based traffic videos at a highway ramp merging area, followed by reconstructing high-interaction scenarios in simulation software for evaluation by participants, from whom eye-tracking data and subjective risk ratings were collected.

**Key Contributions:**

	1. Introduction of the RISEE dataset that includes human evaluations and eye-tracking data.
	2. Combination of drone-based and simulation data collection methods to enhance dataset realism and safety.
	3. Availability of the dataset for research purposes, including example FPV videos.

**Result:** The study resulted in 3567 valid subjective risk ratings and 2045 qualified eye-tracking data segments from 101 participants across 179 scenarios.

**Limitations:** The dataset may still not cover all possible driving scenarios and does not account for individual variations in human subject responses fully.

**Conclusion:** The RISEE dataset offers a novel resource that bridges the gap between human factors and autonomous driving evaluation, improving the authenticity and safety-critical nature of such datasets.

**Abstract:** In the research and development (R&D) and verification and validation (V&V) phases of autonomous driving decision-making and planning systems, it is necessary to integrate human factors to achieve decision-making and evaluation that align with human cognition. However, most existing datasets primarily focus on vehicle motion states and trajectories, neglecting human-related information. In addition, current naturalistic driving datasets lack sufficient safety-critical scenarios while simulated datasets suffer from low authenticity. To address these issues, this paper constructs the Risk-Informed Subjective Evaluation and Eye-tracking (RISEE) dataset which specifically contains human subjective evaluations and eye-tracking data apart from regular naturalistic driving trajectories. By leveraging the complementary advantages of drone-based (high realism and extensive scenario coverage) and simulation-based (high safety and reproducibility) data collection methods, we first conduct drone-based traffic video recording at a highway ramp merging area. After that, the manually selected highly interactive scenarios are reconstructed in simulation software, and drivers' first-person view (FPV) videos are generated, which are then viewed and evaluated by recruited participants. During the video viewing process, participants' eye-tracking data is collected. After data processing and filtering, 3567 valid subjective risk ratings from 101 participants across 179 scenarios are retained, along with 2045 qualified eye-tracking data segments. The collected data and examples of the generated FPV videos are available in our website.

</details>


### [6] [Exploring the Alignment of Perceived and Measured Sleep Quality with Working Memory using Consumer Wearables](https://arxiv.org/abs/2507.19491)

*Peter Neigel, David Antony Selby, Shota Arai, Benjamin Tag, Niels van Berkel, Sebastian Vollmer, Andrew Vargo, Koichi Kise*

**Main category:** cs.HC

**Keywords:** sleep tracking, wearable devices, subjective assessments, REM sleep, self-report sensitivity

**Relevance Score:** 8

**TL;DR:** This study investigates the relationship between subjective sleep assessments and Oura ring sensor data, revealing key predictors of sleep quality perceptions among participants.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine whether wearable sleep-tracking data enhances understanding of sleep or merely quantifies existing knowledge.

**Method:** 29 participants rated their sleep quality daily and completed a working memory task, with data collected over 4-8 weeks from an Oura ring.

**Key Contributions:**

	1. Exploration of the link between subjective sleep assessments and wearable sensor data.
	2. Identification of key predictors influencing sleep quality self-assessments.
	3. Segmentation of participants into groups based on sensitivity to sleep tracker data.

**Result:** Key predictors of sleep self-assessment included REM sleep, nocturnal heart rate, and N-Back task performance, with significant differences in sensitivity to sleep markers among participants.

**Limitations:** The study is limited to a small sample size and the specific wearable device used (Oura ring).

**Conclusion:** The findings suggest sleep trackers may offer varying information benefits to users, and the data from the study is publicly accessible.

**Abstract:** Wearable devices offer detailed sleep-tracking data. However, whether this information enhances our understanding of sleep or simply quantifies already-known patterns remains unclear. This work explores the relationship between subjective sleep self-assessments and sensor data from an Oura ring over 4--8 weeks in-the-wild. 29 participants rated their sleep quality daily compared to the previous night and completed a working memory task. Our findings reveal that differences in REM sleep, nocturnal heart rate, N-Back scores, and bedtimes highly predict sleep self-assessment in significance and effect size. For N-Back performance, REM sleep duration, prior night's REM sleep, and sleep self-assessment are the strongest predictors. We demonstrate that self-report sensitivity towards sleep markers differs among participants. We identify three groups, highlighting that sleep trackers provide more information gain for some users than others. Additionally, we make all experiment data publicly available.

</details>


### [7] [ChartGen: Scaling Chart Understanding Via Code-Guided Synthetic Chart Generation](https://arxiv.org/abs/2507.19492)

*Jovana Kondic, Pengyuan Li, Dhiraj Joshi, Zexue He, Shafiq Abedin, Jennifer Sun, Ben Wiesel, Eli Schwartz, Ahmed Nassar, Bo Wu, Assaf Arbelle, Aude Oliva, Dan Gutfreund, Leonid Karlinsky, Rogerio Feris*

**Main category:** cs.HC

**Keywords:** chart-to-code, synthetic dataset, vision-language models, data visualization, code generation

**Relevance Score:** 5

**TL;DR:** This paper presents ChartGen, a pipeline for generating executable plotting scripts from chart images, creating a large synthetic dataset for evaluating multimodal models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to recover executable plotting scripts from chart images to enhance machine understanding of data visualizations.

**Method:** ChartGen uses a vision-language model to reconstruct chart images into Python scripts and then refines these scripts with a code-oriented large language model.

**Key Contributions:**

	1. Introduction of ChartGen for code-guided synthetic chart generation
	2. Creation of a large synthetic chart-image code dataset
	3. Evaluation of multiple vision-language models on chart-to-code reconstruction

**Result:** A synthetic dataset of 222.5K unique chart-image code pairs was created, demonstrating substantial room for improvement in multimodal models for chart understanding.

**Limitations:** 

**Conclusion:** The release of ChartGen and the accompanying dataset aims to accelerate research in chart understanding and vision-conditioned code generation.

**Abstract:** Chart-to-code reconstruction -- the task of recovering executable plotting scripts from chart images -- provides important insights into a model's ability to ground data visualizations in precise, machine-readable form. Yet many existing multimodal benchmarks largely focus primarily on answering questions about charts or summarizing them. To bridge this gap, we present ChartGen, a fully-automated pipeline for code-guided synthetic chart generation. Starting from seed chart images, ChartGen (i) prompts a vision-language model (VLM) to reconstruct each image into a python script, and (ii) iteratively augments that script with a code-oriented large language model (LLM). Using ChartGen, we create 222.5K unique chart-image code pairs from 13K seed chart images, and present an open-source synthetic chart dataset covering 27 chart types, 11 plotting libraries, and multiple data modalities (image, code, text, CSV, DocTags). From this corpus, we curate a held-out chart-to-code evaluation subset of 4.3K chart image-code pairs, and evaluate six open-weight VLMs (3B - 26B parameters), highlighting substantial room for progress. We release the pipeline, prompts, and the dataset to help accelerate efforts towards robust chart understanding and vision-conditioned code generation: https://github.com/SD122025/ChartGen/

</details>


### [8] [From Bench to Bedside: A DeepSeek-Powered AI System for Automated Chest Radiograph Interpretation in Clinical Practice](https://arxiv.org/abs/2507.19493)

*Yaowei Bai, Ruiheng Zhang, Yu Lei, Jingfeng Yao, Shuguang Ju, Chaoyang Wang, Wei Yao, Yiwan Guo, Guilin Zhang, Chao Wan, Qian Yuan, Xuhua Duan, Xinggang Wang, Tao Sun, Yongchao Xu, Chuansheng Zheng, Huangxuan Zhao, Bo Du*

**Main category:** cs.HC

**Keywords:** chest X-ray, AI assistance, radiology, report generation, health informatics

**Relevance Score:** 9

**TL;DR:** Janus-Pro-CXR is a chest X-ray interpretation system that outperforms existing models, improves report quality, reduces interpretation time, and enhances diagnostic reliability in clinical settings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the global shortage of radiologists and improve the quality and efficiency of chest X-ray interpretations in primary care through AI assistance.

**Method:** A multicenter prospective trial validated the Janus-Pro-CXR system against state-of-the-art models, focusing on automated report generation and detection of critical radiographic findings.

**Key Contributions:**

	1. Outperforms existing X-ray report generation models
	2. Demonstrates robust detection of critical radiographic findings
	3. Improves diagnostic reliability and workflow efficiency

**Result:** The system showed significantly improved report accuracy and reduced interpretation time compared to existing models, with high preference among clinical experts.

**Limitations:** 

**Conclusion:** Janus-Pro-CXR enhances diagnostic reliability and workflow efficiency in radiology, especially in resource-limited environments, and plans to open-source the model for broader use.

**Abstract:** A global shortage of radiologists has been exacerbated by the significant volume of chest X-ray workloads, particularly in primary care. Although multimodal large language models show promise, existing evaluations predominantly rely on automated metrics or retrospective analyses, lacking rigorous prospective clinical validation. Janus-Pro-CXR (1B), a chest X-ray interpretation system based on DeepSeek Janus-Pro model, was developed and rigorously validated through a multicenter prospective trial (NCT06874647). Our system outperforms state-of-the-art X-ray report generation models in automated report generation, surpassing even larger-scale models including ChatGPT 4o (200B parameters), while demonstrating robust detection of eight clinically critical radiographic findings (area under the curve, AUC > 0.8). Retrospective evaluation confirms significantly higher report accuracy than Janus-Pro and ChatGPT 4o. In prospective clinical deployment, AI assistance significantly improved report quality scores (4.37 vs. 4.11, P < 0.001), reduced interpretation time by 18.5% (P < 0.001), and was preferred by a majority of experts (3 out of 5) in 52.7% of cases. Through lightweight architecture and domain-specific optimization, Janus-Pro-CXR improves diagnostic reliability and workflow efficiency, particularly in resource-constrained settings. The model architecture and implementation framework will be open-sourced to facilitate the clinical translation of AI-assisted radiology solutions.

</details>


### [9] [Evaluating Personalized Beneficial Interventions in the Daily Lives of Older Adults Using a Camera](https://arxiv.org/abs/2507.19494)

*Longfei Chen, Christopher Lochhead, Robert B. Fisher, Nusa Faric, Jacques Fleuriot, Subramanian Ramamoorthy*

**Main category:** cs.HC

**Keywords:** activity interventions, older adults, camera monitoring, computer vision, health outcomes

**Relevance Score:** 4

**TL;DR:** This study assesses the effectiveness of daily activity interventions (mindful meals and art crafts) on the physical and mental health of older adults using a non-contact camera-based monitoring system.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of robust objective metrics and personalized strategies to measure the impact of daily activity interventions on elderly health.

**Method:** Two older adults over 65 selected preferred daily interventions, which were assessed over an 8-week monitoring period using a camera-based system to track physical behaviors and extract statistics via computer vision algorithms.

**Key Contributions:**

	1. Utilization of a non-contact, privacy-preserving monitoring system for assessing daily interventions.
	2. Demonstration of significant behavioral changes through selected activities for older adults.
	3. Presentation of robust metrics for measuring the impact of interventions on physical behavior.

**Result:** Significant behavioral changes were observed in both participants, indicating the effectiveness of mindful meals and art crafts, as well as the monitoring system used.

**Limitations:** The study involves only two participants, which may limit the generalizability of results.

**Conclusion:** The findings support the use of specified interventions and the proposed monitoring system to improve the health outcomes of older adults.

**Abstract:** Beneficial daily activity interventions have been shown to improve both the physical and mental health of older adults. However, there is a lack of robust objective metrics and personalized strategies to measure their impact. In this study, two older adults aged over 65, living in Edinburgh, UK, selected their preferred daily interventions (mindful meals and art crafts), which are then assessed for effectiveness. The total monitoring period across both participants was 8 weeks. Their physical behaviours were continuously monitored using a non-contact, privacy-preserving camera-based system. Postural and mobility statistics were extracted using computer vision algorithms and compared across periods with and without the interventions. The results demonstrate significant behavioural changes for both participants, highlighting the effectiveness of both these activities and the monitoring system.

</details>


### [10] [Simulating Human Behavior with the Psychological-mechanism Agent: Integrating Feeling, Thought, and Action](https://arxiv.org/abs/2507.19495)

*Qing Dong, Pengyuan Liu, Dong Yu, Chen Kang*

**Main category:** cs.HC

**Keywords:** Generative Agents, Human Behavior Simulation, Psychological Modeling

**Relevance Score:** 6

**TL;DR:** This paper introduces the Psychological-mechanism Agent (PSYA) framework to enhance the simulation of human behavior through improved emotional and cognitive modeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing generative agents that simplify emotional modeling and primarily focus on specific tasks.

**Method:** The PSYA framework includes three modules: Feeling (to simulate emotional changes), Thought (to support goal-directed and spontaneous thinking), and Action (to optimize behavior through integrating emotions, needs, and plans).

**Key Contributions:**

	1. Introduction of the PSYA framework for better emotional and cognitive modeling
	2. Evaluation through daily life simulations and psychological experiments
	3. Demonstration of improved behavior authenticity in generative agents

**Result:** The PSYA framework generated more natural, consistent, diverse, and credible behaviors, successfully replicating human experimental outcomes in simulations.

**Limitations:** 

**Conclusion:** The work provides a richer emotional and cognitive modeling approach for generative agents, offering an alternative for psychological experiments.

**Abstract:** Generative agents have made significant progress in simulating human behavior, but existing frameworks often simplify emotional modeling and focus primarily on specific tasks, limiting the authenticity of the simulation. Our work proposes the Psychological-mechanism Agent (PSYA) framework, based on the Cognitive Triangle (Feeling-Thought-Action), designed to more accurately simulate human behavior. The PSYA consists of three core modules: the Feeling module (using a layer model of affect to simulate changes in short-term, medium-term, and long-term emotions), the Thought module (based on the Triple Network Model to support goal-directed and spontaneous thinking), and the Action module (optimizing agent behavior through the integration of emotions, needs and plans). To evaluate the framework's effectiveness, we conducted daily life simulations and extended the evaluation metrics to self-influence, one-influence, and group-influence, selection five classic psychological experiments for simulation. The results show that the PSYA framework generates more natural, consistent, diverse, and credible behaviors, successfully replicating human experimental outcomes. Our work provides a richer and more accurate emotional and cognitive modeling approach for generative agents and offers an alternative to human participants in psychological experiments.

</details>


### [11] [Technological Requirements for Videoconferencing Judicial Hearings: Enhancing the Credibility and Reliability of Remote Testimonies](https://arxiv.org/abs/2507.19496)

*Jorge Alberto Araujo*

**Main category:** cs.HC

**Keywords:** videoconference, judicial hearings, credibility, remote testimonies, access to justice

**Relevance Score:** 2

**TL;DR:** The paper discusses technological advancements needed to improve the reliability of video hearings in the judiciary, suggesting specific functionalities to enhance witness credibility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the credibility and reliability of judicial hearings conducted via videoconference, addressing limitations of current platforms.

**Method:** Analysis of the requirements and practical experiences of judges conducting videoconferences, with a focus on identifying technological deficiencies.

**Key Contributions:**

	1. Identification of limitations in current videoconference platforms for judicial hearings
	2. Proposals for tailored technological functionalities for remote testimonies
	3. Emphasis on the importance of security and monitoring in remote judicial processes

**Result:** Proposes functionalities like eye tracking, environment verification, and improved transmission quality to ensure witnesses' authenticity during remote hearings.

**Limitations:** 

**Conclusion:** Specific modules for witnesses can balance the credibility of remote and in-person hearings, thus enhancing access to justice while maintaining procedural reliability.

**Abstract:** This paper analyzes the technological requirements necessary to enhance the credibility and reliability of judicial hearings conducted via videoconference, from the internal perspective of the judiciary. Drawing on the practical experience of a judge who conducts daily hearings, this study identifies limitations in current platforms for verifying the authenticity of testimonies and proposes tailored functionalities for the judicial context. Recognizing that remote hearings represent a convenience for the parties without replacing the option of in-person attendance, the article suggests implementing features such as eye tracking, environment verification, and blocking of parallel applications, in addition to improvements in transmission quality. The study concludes that developing specific modules for witnesses - focusing on security and monitoring - can significantly contribute to equalizing the credibility between remote and in-person hearings, thus expanding access to justice without compromising procedural reliability.

</details>


### [12] [Unlimited Editions: Documenting Human Style in AI Art Generation](https://arxiv.org/abs/2507.19497)

*Alex Leitch, Celia Chen*

**Main category:** cs.HC

**Keywords:** AI Art, Human-Computer Interaction, Artistic Value, Generative AI, Creative Process

**Relevance Score:** 7

**TL;DR:** This paper critiques current HCI approaches to AI art generation, emphasizing the need to understand artistic value as stemming from human creative struggle rather than just visual reproduction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current HCI research in AI art generation often focuses on detection and authenticity, missing the deeper creative processes involved in artistic expression.

**Method:** The paper examines historical precedents in art to argue that artistic style results from a resolution of creative struggles and technical constraints.

**Key Contributions:**

	1. Critique of existing HCI perspectives on AI art generation.
	2. Proposal for documenting artistic lineage in generative AI.
	3. Call for a focus on the creative process rather than just aesthetic outcomes.

**Result:** The analysis reveals that AI systems reduce human choice to reproducible patterns, lacking in preserving artistic provenance.

**Limitations:** 

**Conclusion:** The authors suggest reorienting HCI research towards automatic documentation of artistic style's origins and evolution to enhance understanding of creativity in AI-generated art.

**Abstract:** As AI art generation becomes increasingly sophisticated, HCI research has focused primarily on questions of detection, authenticity, and automation. This paper argues that such approaches fundamentally misunderstand how artistic value emerges from the concerns that drive human image production. Through examination of historical precedents, we demonstrate that artistic style is not only visual appearance but the resolution of creative struggle, as artists wrestle with influence and technical constraints to develop unique ways of seeing. Current AI systems flatten these human choices into reproducible patterns without preserving their provenance. We propose that HCI's role lies not only in perfecting visual output, but in developing means to document the origins and evolution of artistic style as it appears within generated visual traces. This reframing suggests new technical directions for HCI research in generative AI, focused on automatic documentation of stylistic lineage and creative choice rather than simple reproduction of aesthetic effects.

</details>


### [13] [ChatMyopia: An AI Agent for Pre-consultation Education in Primary Eye Care Settings](https://arxiv.org/abs/2507.19498)

*Yue Wu, Xiaolan Chen, Weiyi Zhang, Shunming Liu, Wing Man Rita Sum, Xinyuan Wu, Xianwen Shang, Chea-su Kee, Mingguang He, Danli Shi*

**Main category:** cs.HC

**Keywords:** Large Language Models, Myopia, Patient Education, Healthcare AI, Image Classification

**Relevance Score:** 10

**TL;DR:** ChatMyopia is an LLM-based AI agent for myopia-related inquiries that integrates image classification and a knowledge base, showing improved patient education and satisfaction.

**Read time:** 35 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses challenges in using LLMs for healthcare communication, particularly for specific domains like myopia, and highlights the lack of real-world effectiveness of existing tools.

**Method:** ChatMyopia combines an image classification tool with a retrieval-augmented knowledge base sourced from literature, expert consensus, and clinical guidelines, and is validated through human evaluations and a randomized controlled trial.

**Key Contributions:**

	1. Introduction of ChatMyopia for myopia-related inquiries
	2. Integration of text and image-based response systems
	3. Demonstrated effectiveness in improving patient education and satisfaction

**Result:** In a trial involving 70 participants, ChatMyopia significantly enhanced patient satisfaction over traditional methods, showing improvements in accuracy, empathy, disease awareness, and communication.

**Limitations:** 

**Conclusion:** ChatMyopia emerges as a promising tool to enhance patient education and satisfaction in primary eye care, highlighting its potential utility in clinical settings.

**Abstract:** Large language models (LLMs) show promise for tailored healthcare communication but face challenges in interpretability and multi-task integration particularly for domain-specific needs like myopia, and their real-world effectiveness as patient education tools has yet to be demonstrated. Here, we introduce ChatMyopia, an LLM-based AI agent designed to address text and image-based inquiries related to myopia. To achieve this, ChatMyopia integrates an image classification tool and a retrieval-augmented knowledge base built from literature, expert consensus, and clinical guidelines. Myopic maculopathy grading task, single question examination and human evaluations validated its ability to deliver personalized, accurate, and safe responses to myopia-related inquiries with high scalability and interpretability. In a randomized controlled trial (n=70, NCT06607822), ChatMyopia significantly improved patient satisfaction compared to traditional leaflets, enhancing patient education in accuracy, empathy, disease awareness, and patient-eyecare practitioner communication. These findings highlight ChatMyopia's potential as a valuable supplement to enhance patient education and improve satisfaction with medical services in primary eye care settings.

</details>


### [14] [Gaze-Aware AI: Mathematical modeling of epistemic experience of the Marginalized for Human-Computer Interaction & AI Systems](https://arxiv.org/abs/2507.19500)

*Omkar Suresh Hatti*

**Main category:** cs.HC

**Keywords:** artificial intelligence, gaze, HCI, neuro-plasticity, inclusive design

**Relevance Score:** 5

**TL;DR:** This paper explores the concept of psychological spaciousness in society, using gaze analysis to quantify how individuals modify authentic self-expression to conform to cultural norms. It introduces the Gaze Pressure Index (GPI)-Diff Composite Metric to model conversational spaces, and provides an equation intended for training Large Language Models (LLMs), advocating for inclusive Human-Computer Interaction (HCI) based on neuro-plasticity principles.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To quantify how individuals subconsciously modify their authentic self-expression in response to societal norms, and to advocate for more inclusive HCI practices.

**Method:** The paper analyzes gaze across marginalized and intersectional groups using postmodern philosophy and psychology, employing a mathematical formulation to develop the Gaze Pressure Index (GPI)-Diff Composite Metric.

**Key Contributions:**

	1. Development of the Gaze Pressure Index (GPI)-Diff Composite Metric
	2. Introduction of a mathematical equation for training LLMs
	3. Advocacy for affirming and inclusive HCI practices based on psychological concepts.

**Result:** The study results in an equation designed to enhance the training of Large Language Models (LLMs), contributing to the conversation on inclusive HCI.

**Limitations:** The analysis is based on a limited number of redacted Reddit posts and does not endorse their content.

**Conclusion:** Affirmative and inclusive HCI should be prioritized, supported by neuro-plasticity principles that showcase the brain's ability to adapt and rewire.

**Abstract:** The proliferation of artificial intelligence provides an opportunity to create psychological spaciousness in society. Spaciousness is defined as the ability to hold diverse interpersonal interactions and forms the basis for vulnerability that leads to authenticity that leads to prosocial behaviors and thus to societal harmony. This paper demonstrates an attempt to quantify, the human conditioning to subconsciously modify authentic self-expression to fit the norms of the dominant culture. Gaze is explored across various marginalized and intersectional groups, using concepts from postmodern philosophy and psychology. The effects of gaze are studied through analyzing a few redacted Reddit posts, only to be discussed in discourse and not endorsement. A mathematical formulation for the Gaze Pressure Index (GPI)-Diff Composite Metric is presented to model the analysis of two sets of conversational spaces in relation to one another. The outcome includes an equation to train Large Language Models (LLMs) - the working mechanism of AI products such as Chat-GPT; and an argument for affirming and inclusive HCI, based on the equation, is presented. The argument is supported by a few principles of Neuro-plasticity, The brain's lifelong capacity to rewire.

</details>


### [15] [Mosaic Selections: Managing and Optimizing User Selections for Scalable Data Visualization Systems](https://arxiv.org/abs/2507.19690)

*Jeffrey Heer, Dominik Moritz, Ron Pechuk*

**Main category:** cs.HC

**Keywords:** interactive visualizations, real-time interaction, data optimization

**Relevance Score:** 7

**TL;DR:** This paper presents the Mosaic Selections model for improving real-time interaction with large datasets in visualizations, enabling efficient user selection management and optimization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient real-time interaction with large datasets in interactive visualizations, which often struggle with latency issues due to complex user selections.

**Method:** The Mosaic Selections model incorporates one or more filter predicates into data queries for visualizations and input widgets, allowing for automatic optimizations such as pre-aggregation of data for quick selection updates.

**Key Contributions:**

	1. Formal description of the Mosaic Selections model
	2. Implementation of the model in the open-source Mosaic architecture
	3. Significant latency improvements for selection-based optimizations.

**Result:** Benchmarking shows that Mosaic Selections can achieve orders-of-magnitude latency improvements compared to unoptimized queries and existing optimizers in the Vega language.

**Limitations:** 

**Conclusion:** Mosaic Selections provide a robust framework for flexible filtering in visualizations while maintaining efficiency even with millions or billions of records.

**Abstract:** Though powerful tools for analysis and communication, interactive visualizations often fail to support real-time interaction with large datasets with millions or more records. To highlight and filter data, users indicate values or intervals of interest. Such selections may span multiple components, combine in complex ways, and require optimizations to ensure low-latency updates. We describe Mosaic Selections, a model for representing, managing, and optimizing user selections, in which one or more filter predicates are added to queries that request data for visualizations and input widgets. By analyzing both queries and selection predicates, Mosaic Selections enable automatic optimizations, including pre-aggregating data to rapidly compute selection updates. We contribute a formal description of our selection model and optimization methods, and their implementation in the open-source Mosaic architecture. Benchmark results demonstrate orders-of-magnitude latency improvements for selection-based optimizations over unoptimized queries and existing optimizers for the Vega language. The Mosaic Selection model provides infrastructure for flexible, interoperable filtering across multiple visualizations, alongside automatic optimizations to scale to millions and even billions of records.

</details>


### [16] [LowKeyEMG: Electromyographic typing with a reduced keyset](https://arxiv.org/abs/2507.19736)

*Johannes Y. Lee, Derek Xiao, Shreyas Kaasyap, Nima R. Hadidi, John L. Zhou, Jacob Cunningham, Rakshith R. Gore, Deniz O. Eren, Jonathan C. Kao*

**Main category:** cs.HC

**Keywords:** sEMG, gesture recognition, human-computer interaction, assistive technology, language model

**Relevance Score:** 9

**TL;DR:** LowKeyEMG presents a real-time text entry interface using 7 gesture classes from sEMG, achieving efficient and reliable communication for individuals with motor impairments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve text entry for individuals with motor impairments, focusing on reliability where full-alphabet sEMG decoding has been insufficient.

**Method:** Develops a real-time interface that simplifies the English alphabet to 4 gesture keys plus 3 for space/system, using the RWKV language model for efficient computation.

**Key Contributions:**

	1. Introduction of a low-key gesture-based typing paradigm for sEMG
	2. Efficiency improvements in gesture recognition and typing speed
	3. High accuracy rates and relevance for assistive technologies

**Result:** Participants achieved an average typing speed of 23.3 words per minute and 98.2% top-3 word accuracy, with a 17% increase in gesture efficiency compared to traditional typing.

**Limitations:** 

**Conclusion:** LowKeyEMG enables practical communication rates for users with constrained input capabilities, supporting assistive technology applications.

**Abstract:** We introduce LowKeyEMG, a real-time human-computer interface that enables efficient text entry using only 7 gesture classes decoded from surface electromyography (sEMG). Prior work has attempted full-alphabet decoding from sEMG, but decoding large character sets remains unreliable, especially for individuals with motor impairments. Instead, LowKeyEMG reduces the English alphabet to 4 gesture keys, with 3 more for space and system interaction, to reliably translate simple one-handed gestures into text, leveraging the recurrent transformer-based language model RWKV for efficient computation. In real-time experiments, participants achieved average one-handed keyboardless typing speeds of 23.3 words per minute with LowKeyEMG, and improved gesture efficiency by 17% (relative to typed phrase length). When typing with only 7 keys, LowKeyEMG can achieve 98.2% top-3 word accuracy, demonstrating that this low-key typing paradigm can maintain practical communication rates. Our results have implications for assistive technologies and any interface where input bandwidth is constrained.

</details>


### [17] [KinemaFX: A Kinematic-Driven Interactive System for Particle Effect Exploration and Customization](https://arxiv.org/abs/2507.19782)

*Yifei Zhang, Lin-Ping Yuan, Yuheng Zhao, Jielin Feng, Siming Chen*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Particle Effects, Large Language Models, Interactive Systems, Customization

**Relevance Score:** 5

**TL;DR:** KinemaFX is an LLM-driven system that aids non-expert users in creating customized particle effects by utilizing a conceptual model for semantic and kinematic inputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist non-expert users in creating effect artworks, which is challenging due to a lack of specialized skills in matching particle effects to their intent.

**Method:** KinemaFX utilizes Large Language Models to express intent through semantic and kinematic inputs, enabling guided exploration and creation of particle effects.

**Key Contributions:**

	1. Introduction of a conceptual model for particle effects
	2. Development of an LLM-powered interactive system
	3. Demonstration of user efficiency in creating artworks

**Result:** Evaluation shows that KinemaFX effectively supports users in efficiently creating customized particle effect artworks.

**Limitations:** Limited to non-expert user scenarios; further studies needed to assess expert use cases.

**Conclusion:** KinemaFX provides a helpful framework for non-experts to construct particle effects, enhancing accessibility and creativity in this field.

**Abstract:** Particle effects are widely used in games and animation to simulate natural phenomena or stylized visual effects. However, creating effect artworks is challenging for non-expert users due to their lack of specialized skills, particularly in finding particle effects with kinematic behaviors that match their intent. To address these issues, we present KinemaFX, a kinematic-driven interactive system, to assist non-expert users in constructing customized particle effect artworks. We propose a conceptual model of particle effects that captures both semantic features and kinematic behaviors. Based on the model, KinemaFX adopts a workflow powered by Large Language Models (LLMs) that supports intent expression through combined semantic and kinematic inputs, while enabling implicit preference-guided exploration and subsequent creation of customized particle effect artworks based on exploration results. Additionally, we developed a kinematic-driven method to facilitate efficient interactive particle effect search within KinemaFX via structured representation and measurement of particle effects. To evaluate KinemaFX, we illustrate usage scenarios and conduct a user study employing an ablation approach. Evaluation results demonstrate that KinemaFX effectively supports users in efficiently and customarily creating particle effect artworks.

</details>


### [18] [TS-Insight: Visualizing Thompson Sampling for Verification and XAI](https://arxiv.org/abs/2507.19898)

*Parsa Vares, Éloi Durant, Jun Pang, Nicolas Médoc, Mohammad Ghoniem*

**Main category:** cs.HC

**Keywords:** Thompson Sampling, visual analytics, active learning, explainability, debugging

**Relevance Score:** 6

**TL;DR:** TS-Insight is a visual analytics tool designed to improve interpretability and debugging of Thompson Sampling algorithms in active learning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the opacity of Thompson Sampling algorithms by providing a visual tool that enhances understanding and trust in the decision-making process.

**Method:** The tool provides various plots that track evolving posteriors, evidence counts, and sampling outcomes for each arm in the algorithm.

**Key Contributions:**

	1. Introduction of a visual analytics tool for Thompson Sampling algorithms
	2. Enables tracking of decision-making metrics in real-time
	3. Fosters trust and facilitates debugging in complex decision-making scenarios

**Result:** TS-Insight enables better debugging and verification of exploration/exploitation processes in Thompson Sampling.

**Limitations:** 

**Conclusion:** By enhancing the explainability of these algorithms, TS-Insight promotes trust and facilitates their application in sensitive domains requiring interpretable decision-making.

**Abstract:** Thompson Sampling (TS) and its variants are powerful Multi-Armed Bandit algorithms used to balance exploration and exploitation strategies in active learning. Yet, their probabilistic nature often turns them into a ``black box'', hindering debugging and trust. We introduce TS-Insight, a visual analytics tool explicitly designed to shed light on the internal decision mechanisms of Thompson Sampling-based algorithms, for model developers. It comprises multiple plots, tracing for each arm the evolving posteriors, evidence counts, and sampling outcomes, enabling the verification, diagnosis, and explainability of exploration/exploitation dynamics. This tool aims at fostering trust and facilitating effective debugging and deployment in complex binary decision-making scenarios especially in sensitive domains requiring interpretable decision-making.

</details>


### [19] [Visual Analytics Using Tensor Unified Linear Comparative Analysis](https://arxiv.org/abs/2507.19988)

*Naoki Okami, Kazuki Miyake, Naohisa Sakamoto, Jorji Nonaka, Takanori Fujiwara*

**Main category:** cs.HC

**Keywords:** tensor decomposition, visual analytics, discriminant analysis

**Relevance Score:** 4

**TL;DR:** This paper introduces TULCA, a tensor decomposition method that facilitates comparative analysis of tensors through discriminant analysis and contrastive learning, and presents a visual analytics interface for interpreting results.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding complex data through tensor comparison necessitates improved decomposition methods that support flexible analysis.

**Method:** The proposed TULCA method extends traditional dimensionality reduction techniques to tensors, integrating discriminant analysis and contrastive learning for effective tensor decomposition.

**Key Contributions:**

	1. Introduction of TULCA for tensor analysis
	2. Integration of discriminant analysis and contrastive learning for tensor decomposition
	3. Development of a visual analytics interface for interpreting TULCA results

**Result:** TULCA allows for flexible comparison of tensors and provides a means to visualize results through 2D representations.

**Limitations:** 

**Conclusion:** The effectiveness of TULCA and its visual analytics interface is validated through computational evaluations and case studies involving log data from supercomputers.

**Abstract:** Comparing tensors and identifying their (dis)similar structures is fundamental in understanding the underlying phenomena for complex data. Tensor decomposition methods help analysts extract tensors' essential characteristics and aid in visual analytics for tensors. In contrast to dimensionality reduction (DR) methods designed only for analyzing a matrix (i.e., second-order tensor), existing tensor decomposition methods do not support flexible comparative analysis. To address this analysis limitation, we introduce a new tensor decomposition method, named tensor unified linear comparative analysis (TULCA), by extending its DR counterpart, ULCA, for tensor analysis. TULCA integrates discriminant analysis and contrastive learning schemes for tensor decomposition, enabling flexible comparison of tensors. We also introduce an effective method to visualize a core tensor extracted from TULCA into a set of 2D visualizations. We integrate TULCA's functionalities into a visual analytics interface to support analysts in interpreting and refining the TULCA results. We demonstrate the efficacy of TULCA and the visual analytics interface with computational evaluations and two case studies, including an analysis of log data collected from a supercomputer.

</details>


### [20] [Beyond the Broadcast: Enhancing VR Tennis Broadcasting through Embedded Visualizations and Camera Techniques](https://arxiv.org/abs/2507.20006)

*Jun-Hsiang Yao, Jielin Feng, Xinfang Tian, Kai Xu, Gulshat Amirkhanova, Siming Chen*

**Main category:** cs.HC

**Keywords:** Virtual Reality, Tennis Broadcasting, User Engagement, Cinematic Techniques, Adaptive Visualizations

**Relevance Score:** 6

**TL;DR:** The paper presents a VR tennis viewing system that enhances viewer engagement through dynamic camera movements and embedded visualizations, outperforming traditional VR broadcasting methods.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To improve viewer engagement and narrative clarity in VR broadcasts of sports events, particularly tennis, which currently lack effective camera language and dynamic visualizations.

**Method:** The authors analyzed 400 out-of-play segments from major tennis broadcasts and 25 cinematic VR animations to develop a tennis-specific design framework that combines cinematic techniques with adaptive camera movements. They created a VR viewing system called Beyond the Broadcast that integrates these elements.

**Key Contributions:**

	1. Development of a tennis-specific design framework for VR broadcasting.
	2. Creation of Beyond the Broadcast, a VR tennis viewing system integrating dynamic visuals and camera techniques.
	3. User study demonstrating superior performance in engagement and comprehension compared to traditional methods.

**Result:** The system was evaluated through a user study, indicating that it offers a more immersive and informative viewing experience compared to traditional VR broadcasting methods.

**Limitations:** The study focuses specifically on tennis and may not generalize to other sports broadcasts. Limited diversity in user feedback based on the sports background of participants may affect broader applicability.

**Conclusion:** Beyond the Broadcast significantly enhances viewer comprehension and engagement in VR tennis broadcasts by incorporating tactical information and key match events while maintaining immersion and comfort.

**Abstract:** Virtual Reality (VR) broadcasting has emerged as a promising medium for providing immersive viewing experiences of major sports events such as tennis. However, current VR broadcast systems often lack an effective camera language and do not adequately incorporate dynamic, in-game visualizations, limiting viewer engagement and narrative clarity. To address these limitations, we analyze 400 out-of-play segments from eight major tennis broadcasts to develop a tennis-specific design framework that effectively combines cinematic camera movements with embedded visualizations. We further refine our framework by examining 25 cinematic VR animations, comparing their camera techniques with traditional tennis broadcasts to identify key differences and inform adaptations for VR. Based on data extracted from the broadcast videos, we reconstruct a simulated game that captures the players' and ball's motion and trajectories. Leveraging this design framework and processing pipeline, we develope Beyond the Broadcast, a VR tennis viewing system that integrates embedded visualizations with adaptive camera motions to construct a comprehensive and engaging narrative. Our system dynamically overlays tactical information and key match events onto the simulated environment, enhancing viewer comprehension and narrative engagement while ensuring perceptual immersion and viewing comfort. A user study involving tennis viewers demonstrate that our approach outperforms traditional VR broadcasting methods in delivering an immersive, informative viewing experience.

</details>


### [21] [Dynamite: Real-Time Debriefing Slide Authoring through AI-Enhanced Multimodal Interaction](https://arxiv.org/abs/2507.20137)

*Panayu Keelawat, David Barron, Kaushik Narasimhan, Daniel Manesh, Xiaohang Tang, Xi Chen, Sang Won Lee, Yan Chen*

**Main category:** cs.HC

**Keywords:** AI-assisted teaching, real-time updates, classroom discussions

**Relevance Score:** 7

**TL;DR:** Dynamite is an AI-assisted system for real-time updates of instructor slides during classroom discussions, enhancing the accuracy and relevancy of debriefing content.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Effective debriefing in ethics education requires accurate representation of discussions, but real-time slide updates are cognitively taxing for instructors.

**Method:** The study introduces Dynamite, which utilizes semantic data binding and suggestions to facilitate rapid updates to slides during discussions. A within-subject in-lab study assessed its effectiveness against a text-based AI baseline.

**Key Contributions:**

	1. Introduction of an AI-assisted system for live classroom discussions
	2. Improvements in content accuracy and quality compared to traditional methods
	3. Utilization of voice and sketch input for quick semantic updates

**Result:** Dynamite significantly improved content accuracy and quality compared to traditional methods, allowing faster organization and refinement of information during discussions.

**Limitations:** 

**Conclusion:** The AI-assisted system enhances the quality of instructional debriefings, making it easier for instructors to present relevant themes and diverse viewpoints.

**Abstract:** Facilitating class-wide debriefings after small-group discussions is a common strategy in ethics education. Instructor interviews revealed that effective debriefings should highlight frequently discussed themes and surface underrepresented viewpoints, making accurate representations of insight occurrence essential. Yet authoring presentations in real time is cognitively overwhelming due to the volume of data and tight time constraints. We present Dynamite, an AI-assisted system that enables semantic updates to instructor-authored slides during live classroom discussions. These updates are powered by semantic data binding, which links slide content to evolving discussion data, and semantic suggestions, which offer revision options aligned with pedagogical goals. In a within-subject in-lab study with 12 participants, Dynamite outperformed a text-based AI baseline in content accuracy and quality. Participants used voice and sketch input to quickly organize semantic blocks, then applied suggestions to accelerate refinement as data stabilized.

</details>


### [22] [Occupational Safety within Non-Routine Manufacturing Processes: Evaluating the Validity of Task-Based Ergonomic Assessments](https://arxiv.org/abs/2507.20261)

*Charu Tripathi, Manish Arora, Amaresh Chakrabarti*

**Main category:** cs.HC

**Keywords:** ergonomics, task-based assessment, Industry 5.0, construct validity, human-centricity

**Relevance Score:** 4

**TL;DR:** This study evaluates the construct validity of task-based ergonomic assessment in non-routine manufacturing, revealing low convergent validity and weak discriminant validity, emphasizing the need for improved ergonomic assessment technologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the validity concerns of task-based ergonomic assessments in the context of Industry 5.0 where human-centric practices are emphasized, particularly in challenging industrial environments.

**Method:** The study utilizes a Multitrait multimethod (MTMM) matrix and video-based content analysis to evaluate ergonomic assessments collected from 46 participants using direct measurement and self-reported techniques.

**Key Contributions:**

	1. Evaluates construct validity of task-based ergonomic assessments in non-routine situations.
	2. Identifies validity issues in ergonomic risk assessment methodologies.
	3. Discovers factors that contribute to misinterpretation of ergonomic risks.

**Result:** Findings indicate low same trait correlations for convergent validity (ranging from 0.149 to 0.243) and weak discriminant validity (p < 0.001), revealing factors that undermine the construct validity of ergonomic assessments.

**Limitations:** The study is limited to non-routine manufacturing processes and may not generalize to all industrial sectors.

**Conclusion:** The results suggest significant underestimation of ergonomic risks through task-based assessments, necessitating advancements in ergonomic technologies that are compatible with diverse industrial processes.

**Abstract:** Direct measurement ergonomic assessment is reshaping occupational safety by facilitating highly reliable risk estimation. Industry 5.0, advocating human-centricity, has catalysed increasing adoption of direct measurement tools in manufacturing industries. However, due to technical and feasibility constraints in their practical implementations, especially within non routine manufacturing processes, task based approach to ergonomic assessment is utilized. Despite enabling operationalization of robust ergonomic assessment technologies within complicated industrial processes, task based approach raises several validity concerns. Hence, to ascertain functional utility of the resultant safety interventions, this study evaluates the construct validity of task based ergonomic assessment within non routine work utilizing Multitrait multimethod (MTMM) matrix followed by video-based content analysis. Ergonomic exposure traits were collected for 46 participants through direct measurement and self reported techniques utilizing inertial motion capture and Borg's RPE rating scale respectively. Findings include unsubstantiated convergent validity (low same trait correlations from 0.149 to 0.243) and weak evidence of discriminant validity with statistical significance (p value less than 0.001). The study also identifies three primary factors undermining construct validity through video based content analysis. Findings also elucidate misinterpretation of ergonomic risk and action levels. Therefore, practical implications entail underestimation of actual ergonomic risks when estimated through task based assessment. This highlights the need for enhancement in ergonomic assessment technologies focused on cumulative load analysis compatible within diverse industrial processes.

</details>


### [23] [Talking-to-Build: How LLM-Assisted Interface Shapes Player Performance and Experience in Minecraft](https://arxiv.org/abs/2507.20300)

*Xin Sun, Lei Wang, Yue Li, Jie Li, Massimo Poesio, Julian Frommel, Koen Hinriks, Jiahuan Pei*

**Main category:** cs.HC

**Keywords:** large language models, game interaction, player experience, usability, mixed-methods study

**Relevance Score:** 9

**TL;DR:** This study investigates the impact of LLMs on player performance and experience in gaming, specifically using an LLM-assisted interface in Minecraft.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can transform in-game interactions from rigid commands to natural conversations, enhancing player experience and performance.

**Method:** A mixed-methods study with 30 participants comparing LLM-assisted and command-based interfaces while performing simple and complex tasks in Minecraft.

**Key Contributions:**

	1. Introduction of an LLM-assisted interface for gaming
	2. Demonstrated significant improvements in player performance and engagement
	3. Insights into the effects of task complexity on gameplay experience

**Result:** The LLM-assisted interface significantly improved player performance, engagement, and overall game experience compared to traditional command-based interfaces.

**Limitations:** Study limited to Minecraft and 30 participants, may not generalize across all games or player demographics.

**Conclusion:** LLM-assisted interfaces have the potential to revolutionize gameplay, but it is essential to balance intuitiveness, predictability, transparency, and user agency.

**Abstract:** With large language models (LLMs) on the rise, in-game interactions are shifting from rigid commands to natural conversations. However, the impacts of LLMs on player performance and game experience remain underexplored. This work explores LLM's role as a co-builder during gameplay, examining its impact on task performance, usability, and player experience. Using Minecraft as a sandbox, we present an LLM-assisted interface that engages players through natural language, aiming to facilitate creativity and simplify complex gaming commands. We conducted a mixed-methods study with 30 participants, comparing LLM-assisted and command-based interfaces across simple and complex game tasks. Quantitative and qualitative analyses reveal that the LLM-assisted interface significantly improves player performance, engagement, and overall game experience. Additionally, task complexity has a notable effect on player performance and experience across both interfaces. Our findings highlight the potential of LLM-assisted interfaces to revolutionize virtual experiences, emphasizing the importance of balancing intuitiveness with predictability, transparency, and user agency in AI-driven, multimodal gaming environments.

</details>


### [24] [CineVision: An Interactive Pre-visualization Storyboard System for Director-Cinematographer Collaboration](https://arxiv.org/abs/2507.20355)

*Zheng Wei, Hongtao Wu, lvmin Zhang, Xian Xu, Yefeng Zheng, Pan Hui, Maneesh Agrawala, Huamin Qu, Anyi Rao*

**Main category:** cs.HC

**Keywords:** AI-driven platform, pre-production, film production

**Relevance Score:** 2

**TL;DR:** CineVision is an AI-driven platform for pre-production in filmmaking that integrates scriptwriting with real-time visual pre-visualization, improving communication between directors and cinematographers.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and precision of communication between directors and cinematographers during the pre-production phase of film production, overcoming the limitations of traditional visual references and hand-drawn storyboards.

**Method:** CineVision integrates AI with scriptwriting and real-time visual pre-visualization, providing features like dynamic lighting control, style emulation, and customizable character design for enhanced creative expression.

**Key Contributions:**

	1. Dynamic lighting control
	2. Style emulation based on renowned filmmakers
	3. Customizable character design

**Result:** In a lab study with 24 participants, CineVision demonstrated shorter task times and higher usability ratings compared to two baseline methods, indicating its effectiveness in facilitating communication and accelerating storyboard drafts.

**Limitations:** 

**Conclusion:** CineVision has the potential to streamline pre-production processes and enhance collaboration among filmmaking teams, especially beneficial for new collaborators.

**Abstract:** Effective communication between directors and cinematographers is fundamental in film production, yet traditional approaches relying on visual references and hand-drawn storyboards often lack the efficiency and precision necessary during pre-production. We present CineVision, an AI-driven platform that integrates scriptwriting with real-time visual pre-visualization to bridge this communication gap. By offering dynamic lighting control, style emulation based on renowned filmmakers, and customizable character design, CineVision enables directors to convey their creative vision with heightened clarity and rapidly iterate on scene composition. In a 24-participant lab study, CineVision yielded shorter task times and higher usability ratings than two baseline methods, suggesting a potential to ease early-stage communication and accelerate storyboard drafts under controlled conditions. These findings underscore CineVision's potential to streamline pre-production processes and foster deeper creative synergy among filmmaking teams, particularly for new collaborators.Our code and demo are available at https://github.com/TonyHongtaoWu/CineVision.

</details>


### [25] [EchoForce: Continuous Grip Force Estimation from Skin Deformation Using Active Acoustic Sensing on a Wristband](https://arxiv.org/abs/2507.20437)

*Kian Mahmoodi, Yudong Xie, Tan Gemicioglu, Chi-Jung Lee, Jiwan Kim, Cheng Zhang*

**Main category:** cs.HC

**Keywords:** grip force, health monitoring, acoustic sensing, wearable technology, rehabilitation

**Relevance Score:** 8

**TL;DR:** EchoForce is a wristband that uses acoustic sensing for continuous, non-contact measurement of grip force, overcoming limitations of previous systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide practical and continuous grip force measurement for health monitoring and rehabilitation in older adults, addressing the shortcomings of existing user-dependent methods.

**Method:** A novel wristband called EchoForce, which captures acoustic signals from skin deformations using a foundation model to measure grip force.

**Key Contributions:**

	1. Novel approach to non-contact grip force measurement using acoustic sensing.
	2. High accuracy in both user-dependent and user-independent scenarios.
	3. Practical application for health monitoring in older adults.

**Result:** EchoForce achieved a mean error rate of 9.08% for user-dependent measurements and 12.3% for user-independent measurements, maintaining accuracy across sessions and users.

**Limitations:** Only tested with a small participant group (11), which may limit generalizability.

**Conclusion:** EchoForce is effective for continuous grip force measurement, offering potential benefits in health monitoring and interaction techniques.

**Abstract:** Grip force is commonly used as an overall health indicator in older adults and is valuable for tracking progress in physical training and rehabilitation. Existing methods for wearable grip force measurement are cumbersome and user-dependent, making them insufficient for practical, continuous grip force measurement. We introduce EchoForce, a novel wristband using acoustic sensing for low-cost, non-contact measurement of grip force. EchoForce captures acoustic signals reflected from subtle skin deformations by flexor muscles on the forearm. In a user study with 11 participants, EchoForce achieved a fine-tuned user-dependent mean error rate of 9.08% and a user-independent mean error rate of 12.3% using a foundation model. Our system remained accurate between sessions, hand orientations, and users, overcoming a significant limitation of past force sensing systems. EchoForce makes continuous grip force measurement practical, providing an effective tool for health monitoring and novel interaction techniques.

</details>


### [26] [CoGrader: Transforming Instructors' Assessment of Project Reports through Collaborative LLM Integration](https://arxiv.org/abs/2507.20655)

*Zixin Chen, Jiachen Wang, Yumeng Li, Haobo Li, Chuhan Shi, Rong Zhang, Huamin Qu*

**Main category:** cs.HC

**Keywords:** AI in education, grading systems, human-AI collaboration

**Relevance Score:** 8

**TL;DR:** CoGrader is a grading system that combines human and AI insights for improved grading efficiency and fairness in educational assessments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a fair and efficient grading system in education that accounts for complex evaluation criteria like creativity and practical application of knowledge.

**Method:** A formative study was conducted with six instructors to develop CoGrader, which integrates human-LLM collaborative metrics design, benchmarking, and AI-assisted feedback.

**Key Contributions:**

	1. Development of CoGrader for collaborative grading
	2. Improvement in grading consistency and efficiency
	3. Provision of AI-assisted feedback in educational assessments.

**Result:** CoGrader was effective in enhancing grading efficiency and consistency, providing reliable peer-comparative feedback.

**Limitations:** Limited to the input and perspectives of six instructors; may not generalize to all educational contexts.

**Conclusion:** The study discusses design insights and ethical implications for implementing human-AI collaborative grading systems.

**Abstract:** Grading project reports are increasingly significant in today's educational landscape, where they serve as key assessments of students' comprehensive problem-solving abilities. However, it remains challenging due to the multifaceted evaluation criteria involved, such as creativity and peer-comparative achievement. Meanwhile, instructors often struggle to maintain fairness throughout the time-consuming grading process. Recent advances in AI, particularly large language models, have demonstrated potential for automating simpler grading tasks, such as assessing quizzes or basic writing quality. However, these tools often fall short when it comes to complex metrics, like design innovation and the practical application of knowledge, that require an instructor's educational insights into the class situation. To address this challenge, we conducted a formative study with six instructors and developed CoGrader, which introduces a novel grading workflow combining human-LLM collaborative metrics design, benchmarking, and AI-assisted feedback. CoGrader was found effective in improving grading efficiency and consistency while providing reliable peer-comparative feedback to students. We also discuss design insights and ethical considerations for the development of human-AI collaborative grading systems.

</details>


### [27] [EarXplore: An Open Research Database on Earable Interaction](https://arxiv.org/abs/2507.20656)

*Jonas Hummel, Tobias Röddiger, Valeria Zitz, Philipp Lepold, Michael Küttner, Marius Prill, Christopher Clarke, Hans Gellersen, Michael Beigl*

**Main category:** cs.HC

**Keywords:** earables, interaction, online database, literature exploration, research trends

**Relevance Score:** 7

**TL;DR:** Introduction of EarXplore, a curated online database for earable interaction research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmented body of research in earable interaction technologies and support researchers in tracking developments.

**Method:** Development of EarXplore through a question-centered process, applying 34 criteria to annotate 118 studies and structuring the platform with four integrated views: Tabular, Graphical, Similarity, and Timeline.

**Key Contributions:**

	1. Curated database for earable interaction research.
	2. Four distinct views for data exploration and analysis.
	3. Continuous community updates to reflect current research trends.

**Result:** EarXplore enables tailored exploration, targeted filtering, and interactive information retrieval, facilitating literature queries and information synthesis.

**Limitations:** 

**Conclusion:** EarXplore serves as a dynamic resource that continuously reflects and evolves the state of earable interaction research, identifying gaps and opportunities.

**Abstract:** Interaction with earables - earphones equipped with additional sensors - has been identified as one of four major areas of earable research. Worn naturally and positioned near key physiological signals, earables support a wide range of interaction modalities and have demonstrated the ability to detect multiple inputs simultaneously. Yet this diversity has resulted in a fragmented body of research, making it increasingly difficult to track developments and identify relevant studies. To address this, we introduce EarXplore, a curated, interactive online database on earable interaction research. Designed through a question-centered process that guided both the development of 34 criteria applied to annotate 118 studies and the structure of the platform, EarXplore comprises four distinct yet integrated views: a Tabular View for structured exploration, a Graphical View for visual overviews, a Similarity View for identifying conceptual links, and a Timeline View for analyzing trends and scholarly lineage. We demonstrate how the platform supports tailored exploration, targeted filtering, and interactive information retrieval, allowing researchers to query the literature and synthesize information in the format of their choice. We furthermore leverage the contents and capabilities of the platform to discuss the research gaps and opportunities in the field. With built-in mechanisms for continuous community updates, EarXplore not only reflects the current state of the field but also evolves alongside it, serving as a living resource to inform and accelerate future developments.

</details>


### [28] [Beyond Text: Probing K-12 Educators' Perspectives and Ideas for Learning Opportunities Leveraging Multimodal Large Language Models](https://arxiv.org/abs/2507.20720)

*Tiffany Tseng, Katelyn Lam, Tiffany Lin Fu, Alekhya Maram*

**Main category:** cs.HC

**Keywords:** Multimodal Large Language Models, educational technology, K-12 education, teacher perspectives, learning applications

**Relevance Score:** 6

**TL;DR:** The paper explores educator perspectives on the use of Multimodal Large Language Models (MLLMs) in K-12 education, revealing opportunities and challenges for implementing these models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how educators envision the integration of MLLMs in learning environments and identify practical needs for effective implementation.

**Method:** Formative workshops with 12 K-12 educators to brainstorm applications, discuss concerns, and prototype MLLM-powered learning tools.

**Key Contributions:**

	1. Investigation of educator perspectives on MLLMs in education
	2. Prototyping of MLLM-powered learning applications
	3. Case studies illustrating diverse approaches to MLLM usage in classrooms

**Result:** Insights on varied educator approaches towards MLLMs and the identification of opportunities and challenges in their application for educational contexts.

**Limitations:** Exploratory study with a small sample size of educators; findings may not generalize to all educational contexts.

**Conclusion:** The findings highlight the need for targeted support and resources to effectively integrate MLLMs into K-12 learning experiences.

**Abstract:** Multimodal Large Language Models (MLLMs) are beginning to empower new user experiences that can flexibly generate content from a range of inputs, including images, text, speech, and video. These capabilities have the potential to enrich learning by enabling users to capture and interact with information using a variety of modalities, but little is known about how educators envision how MLLMs might shape the future of learning experiences, what challenges diverse teachers encounter when interpreting how these models work, and what practical needs should be considered for successful implementation in educational contexts. We investigated educator perspectives through formative workshops with 12 K-12 educators, where participants brainstormed learning opportunities, discussed practical concerns for effective use, and prototyped their own MLLM-powered learning applications using Claude 3.5 and its Artifacts feature for previewing code-based output. We use case studies to illustrate two contrasting end-user approaches (teacher-and student-driven), and share insights about opportunities and concerns expressed by our participants, ending with implications for leveraging MLLMs for future learning experiences.

</details>


### [29] [Vocalize: Lead Acquisition and User Engagement through Gamified Voice Competitions](https://arxiv.org/abs/2507.20730)

*Edvin Teskeredzic, Muamer Paric, Adna Sestic, Petra Fribert, Anamarija Lukac, Hadzem Hadzic, Kemal Altwlkany, Emanuel Lacic*

**Main category:** cs.HC

**Keywords:** gamification, user engagement, audio processing, LLMs, voice competitions

**Relevance Score:** 8

**TL;DR:** The paper presents Vocalize, a gamified platform that enhances user engagement and lead acquisition through audio competitions powered by LLMs and audio processing techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create engaging user experiences and collect leads through an interactive gamified system.

**Method:** Vocalize combines audio processing and large language models to facilitate gamified voice competitions, assessed through user studies at live events.

**Key Contributions:**

	1. Introduction of Vocalize as a gamified platform for user engagement
	2. Utilization of audio processing and LLMs in voice competitions
	3. Empirical evidence of increased user engagement from live events

**Result:** The user study indicates that Vocalize significantly increases user engagement, showcasing its effectiveness for gamified audio marketing campaigns.

**Limitations:** 

**Conclusion:** The findings endorse the potential of Vocalize for improving user interaction and brand loyalty through engaging audio experiences.

**Abstract:** This paper explores the prospect of creating engaging user experiences and collecting leads through an interactive and gamified platform. We introduce Vocalize, an end-to-end system for increasing user engagement and lead acquisition through gamified voice competitions. Using audio processing techniques and LLMs, we create engaging and interactive experiences that have the potential to reach a wide audience, foster brand recognition, and increase customer loyalty. We describe the system from a technical standpoint and report results from launching Vocalize at 4 different live events. Our user study shows that Vocalize is capable of generating significant user engagement, which shows potential for gamified audio campaigns in marketing and similar verticals.

</details>


### [30] [Beyond QWERTY: A pressure-based text input approach for XR that enables a touch-typing like experience](https://arxiv.org/abs/2507.20741)

*Fabian Rücker, Torben Storch*

**Main category:** cs.HC

**Keywords:** extended reality, text input, pressure-based modality, typing efficiency, immersive technologies

**Relevance Score:** 7

**TL;DR:** The paper presents a novel pressure-based text input modality for XR applications that enhances typing efficiency by mimicking the comfort and speed of physical keyboards.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve text input efficiency in XR applications, which currently rely on traditional keyboard layouts and are often inefficient.

**Method:** Analyzed physical keyboard characteristics that contribute to comfort and high typing speeds, then developed a pressure-based input system that replaces the QWERTY layout with a linear scale.

**Key Contributions:**

	1. Introduction of a pressure-based text input for XR that enhances typing comfort and speed.
	2. Elimination of visual guidance for proficient users, promoting touch typing in immersive environments.
	3. Demonstrated typing speeds exceeding 200 characters per minute.

**Result:** Proposed system allows typing speeds over 200 characters per minute in a touch-typing-like experience without visual guidance and minimal hand movement.

**Limitations:** 

**Conclusion:** The new input modality is suitable for discreet text input in public spaces and everyday tasks, enhancing the usability of XR technologies.

**Abstract:** Text input in extended reality (XR) applications remains inefficient and tedious. Most solutions are derived from the traditional keyboard layout, yet fail to translate its positive characteristics to the spatial digital realm. This limits the productive use of immersive technologies. In this work, we analyze physical keyboard input to identify key characteristics that facilitate its comfort, touch typing and high typing speeds. Building on these findings, we propose a novel pressure-based text input modality that transfers these characteristics into immersive space by substituting the two-dimensional QWERTY layout with a linear scale. This design facilitates a touch-typing-like experience, eliminating the need for visual guidance for proficient users. Our skill-based approach enables typing speeds of over 200 characters per minute. Additionally, it is suitable for discreet use in public spaces and everyday text-input tasks, since the proposed system requires virtually no hand or finger movements and resembles smartphone-based text input in appearance.

</details>


### [31] [Understanding Bias in Perceiving Dimensionality Reduction Projections](https://arxiv.org/abs/2507.20805)

*Seoyoung Doh, Hyeon Jeon, Sungbok Shin, Ghulam Jilani Quadri, Nam Wook Kim, Jinwook Seo*

**Main category:** cs.HC

**Keywords:** dimensionality reduction, visual interestingness, user study, visualization bias, aesthetics

**Relevance Score:** 6

**TL;DR:** This research explores how visual interestingness influences practitioners' choice of dimensionality reduction techniques, revealing a preference for aesthetics over structural faithfulness.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the bias towards aesthetic over structural considerations in dimensionality reduction selections for better visualization and analysis.

**Method:** A user study was conducted to assess the existence of visual interestingness bias and investigate the factors influencing this bias.

**Key Contributions:**

	1. Identification of visual interestingness bias in dimensionality reduction techniques.
	2. Empirical evidence from user studies on bias influencing practitioner choices.
	3. Recommendations for strategies to mitigate bias in visualization.

**Result:** The study confirmed the bias, showing that visual interestingness is prioritized by practitioners, especially when color-encoded labels are used and exposure time is brief.

**Limitations:** The study is limited to specific dimensionality reduction techniques and may not generalize to all visualization practices.

**Conclusion:** The findings indicate the need for strategies to mitigate visual interestingness bias in the selection and interpretation of dimensionality reduction projections.

**Abstract:** Selecting the dimensionality reduction technique that faithfully represents the structure is essential for reliable visual communication and analytics. In reality, however, practitioners favor projections for other attractions, such as aesthetics and visual saliency, over the projection's structural faithfulness, a bias we define as visual interestingness. In this research, we conduct a user study that (1) verifies the existence of such bias and (2) explains why the bias exists. Our study suggests that visual interestingness biases practitioners' preferences when selecting projections for analysis, and this bias intensifies with color-encoded labels and shorter exposure time. Based on our findings, we discuss strategies to mitigate bias in perceiving and interpreting DR projections.

</details>


### [32] [ProForm: Solder-Free Circuit Assembly Using Thermoforming](https://arxiv.org/abs/2507.20933)

*Narjes Pourjafarian, Zhenming Yang, Jeffrey I. Lipton, Benyamin Davaji, Gregory D. Abowd*

**Main category:** cs.HC

**Keywords:** electronic waste, solder-free prototyping, thermoforming

**Relevance Score:** 4

**TL;DR:** ProForm introduces a solder-free method for circuit prototyping using thermoformed thermoplastics, promoting sustainability and ease of reuse in electronics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing problem of electronic waste by providing a sustainable alternative to traditional soldering methods, which limit component recovery.

**Method:** ProForm uses a thermoforming approach to encapsulate electronic components in pressure-formed thermoplastics, allowing for secure and reversible mounting.

**Key Contributions:**

	1. Introduction of a solder-free prototyping method
	2. Support for various substrates including flexible and paper-based circuits
	3. Facilitation of rapid prototyping and component reuse

**Result:** ProFormed circuits demonstrated good electrical performance and mechanical stability while facilitating rapid prototyping and easy reuse.

**Limitations:** 

**Conclusion:** ProForm not only supports sustainable electronics practices but also offers advantages over traditional soldering methods in terms of versatility and ease of use.

**Abstract:** Electronic waste (e-waste) is a growing global challenge, with millions of functional components discarded due to the difficulty of repair and reuse. Traditional circuit assembly relies on soldering, which creates semi-permanent bonds that limit component recovery and contribute to unnecessary waste. We introduce ProForm, a thermoforming approach for solder-free circuit prototyping. By encapsulating electronic components with pressure-formed thermoplastics, ProForm enables secure, reversible mounting without the need for solder or custom mechanical housings. This approach supports a wide range of substrates, including flexible, paper-based, and non-planar circuits, facilitating easy reuse, replacement, and rapid prototyping. We demonstrate ProForm's versatility to support prototyping practices. We show that ProFormed circuits exhibit good electrical performance and mechanical stability. While motivated by a need for sustainable electronics practices, ProForm has other significant advantages over traditional soldering.

</details>


### [33] [The Impact of Simple, Brief, and Adaptive Instructions within Virtual Reality Training: Components of Cognitive Load Theory in an Assembly Task](https://arxiv.org/abs/2507.20943)

*Rebecca L. Pharmer, Christopher D. Wickens, Lucas Plabst, Benjamin A. Clegg, Leanne M. Hirshfield, Joanna E. Lewis, Jalynn B. Nicoly, Cara A. Spencer, Francisco R. Ortega*

**Main category:** cs.HC

**Keywords:** Cognitive Load, Virtual Reality, Adaptive Training

**Relevance Score:** 5

**TL;DR:** The study investigates how cognitive load elements impact learning efficiency in a VR shape assembly task.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the impact of cognitive load on training outcomes and learning efficiency in adaptive training systems.

**Method:** Participants learned shape assembly via VR, with cognitive load manipulated in three dimensions: Intrinsic Load, Extraneous Load, and Germane Load, across adaptive and fixed training conditions.

**Key Contributions:**

	1. Examined the impact of cognitive load on training efficiency in VR
	2. Demonstrated that adaptive training improves learning outcomes
	3. Identified specific load types affecting training time and retention

**Result:** Higher Intrinsic Load increased training times but did not affect retention accuracy; adaptive training reduced training time without increasing workload or harming retention.

**Limitations:** 

**Conclusion:** Adaptive training can improve learning efficiency in VR without degrading retention, supporting its application in critical skill acquisition environments.

**Abstract:** Objective: The study examined the effects of varying all three core elements of cognitive load on learning efficiency during a shape assembly task in virtual reality (VR).   Background: Adaptive training systems aim to improve learning efficiency and retention by dynamically adjusting difficulty. However, design choices can impact the cognitive workload imposed on the learner. The present experiments examined how aspects of cognitive load impact training outcomes.   Method: Participants learned step-by-step shape assembly in a VR environment. Cognitive load was manipulated across three dimensions: Intrinsic Load (shape complexity), Extraneous Load (instruction verbosity), and Germane Load (adaptive vs. fixed training). In adaptive training (experiment 1), difficulty increased based on individual performance. In fixed training (experiment 2), difficulty followed a preset schedule from a yoked participant.   Results: Higher Intrinsic Load significantly increased training times and subjective workload but did not affect retention test accuracy. Extraneous Load modestly impacted training time, with little impact on workload or retention. Adaptive training shortened overall training time without increasing workload or impairing retention. No interactions were observed between the three types of load. Conclusion: Both Intrinsic and Extraneous Load increased training time, but adaptive training improved efficiency without harming retention. The lack of interaction between the elements suggests training benefits can be worth seeking within any of the components of cognitive load. Application: These findings support the use of VR adaptive systems in domains such as manufacturing and military service, where efficient assembly skill acquisition is critical. Tailoring difficulty in real-time can optimize efficiency without compromising learning.

</details>


### [34] [Towards Effective Human Performance in XR Space Framework based on Real-time Eye Tracking Biofeedback](https://arxiv.org/abs/2507.21000)

*Barbara Karpowicz, Tomasz Kowalewski, Pavlo Zinevych, Adam Kuzdraliński, Grzegorz Marcin Wójcik, Wiesław Kopeć*

**Main category:** cs.HC

**Keywords:** Eye Tracking, XR Space Framework, Human Performance, Real-Time Data, Adaptive Systems

**Relevance Score:** 8

**TL;DR:** This paper introduces an eye tracking module for the XR Space Framework to improve human performance in XR applications through real-time data integration and adaptive adjustments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance human performance in XR-based applications such as training, screening, and teleoperation by integrating eye tracking for real-time user insights.

**Method:** The paper outlines a multimodal framework that integrates eye tracking, body movement, and psychophysiological data to create adaptive immersive systems.

**Key Contributions:**

	1. Development of an eye tracking module for XR applications
	2. Integration of multimodal measurements (eye tracking, body movement, psychophysiological data)
	3. Real-time adjustments based on user attention and cognitive load

**Result:** The integration of eye tracking data improves user engagement and allows for real-time adjustments to task difficulty, boosting learning and operational effectiveness.

**Limitations:** Challenges in implementing eye tracking metrics within dynamic, real-time XR environments due to complex moving visuals.

**Conclusion:** The challenges of implementing eye tracking in real-time XR environments can be addressed, leading to more effective and adaptive applications in various training and operational scenarios.

**Abstract:** This paper proposes an eye tracking module for the XR Space Framework aimed at enhancing human performance in XR-based applications, specifically in training, screening, and teleoperation. This framework provides a methodology and components that streamline the development of adaptive real-time virtual immersive systems. It contains multimodal measurements - declarative in the form of in-VR questionnaires and objective, including eye tracking, body movement, and psychophysiological data (e.g., ECG, GSR, PPG). A key focus of this paper is the integration of real-time eye tracking data into XR environments to facilitate a biofeedback loop, providing insight into user attention, cognitive load, and engagement. Given the relatively high measurement frequency of eye tracking - recognized as a noninvasive yet robust psychophysiological measure - this technology is particularly well suited for real-time adjustments in task difficulty and feedback to enhance learning and operational effectiveness. Despite its established role in cognitive and attentional studies, implementing eye tracking metrics within dynamic, real-time XR environments poses unique challenges, particularly given the complex moving visuals presented in head-mounted displays (HMDs). This paper addresses these challenges by focusing on the essential aspects of integrating eye tracking in immersive systems based on real-time engines, ultimately facilitating more efficient, adaptive XR applications.

</details>


### [35] [User-Centered Design with AI in the Loop: A Case Study of Rapid User Interface Prototyping with "Vibe Coding"](https://arxiv.org/abs/2507.21012)

*Tianyi Li, Tanay Maheshwari, Alex Voelker*

**Main category:** cs.HC

**Keywords:** generative user interfaces, vibe coding, user-centered design, large language models, traffic data analytics

**Relevance Score:** 9

**TL;DR:** This paper explores the use of generative user interfaces, specifically 'vibe coding', leveraging LLMs for rapid prototyping in user-centered design, particularly in traffic data analytics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user-centered design practices by integrating LLMs into the prototyping process for better feedback and alternative design evaluation.

**Method:** Empirical case study integrating 'vibe coding' into the design process for an interactive data analytics interface used by traffic engineers.

**Key Contributions:**

	1. Introduction of 'vibe coding' as a design methodology leveraging LLMs
	2. Empirical insights into using generative UIs for user-centered design
	3. Assessment of the advantages and pitfalls of integrating AI into design processes

**Result:** The application of vibe coding allowed the team to gather extensive user feedback and iterate on design ideas effectively, showcasing the utility of AI in design.

**Limitations:** Potential dependency on the quality of prompts and LLM outputs; the challenge of integrating domain-specific knowledge with design expertise.

**Conclusion:** Generative UIs can significantly improve user-centered design processes but come with challenges in balancing design and domain expertise.

**Abstract:** We present a case study of using generative user interfaces, or ``vibe coding,'' a method leveraging large language models (LLMs) for generating code via natural language prompts, to support rapid prototyping in user-centered design (UCD). Extending traditional UCD practices, we propose an AI-in-the-loop ideate-prototyping process. We share insights from an empirical experience integrating this process to develop an interactive data analytics interface for highway traffic engineers to effectively retrieve and analyze historical traffic data. With generative UIs, the team was able to elicit rich user feedback and test multiple alternative design ideas from user evaluation interviews and real-time collaborative sessions with domain experts. We discuss the advantages and pitfalls of vibe coding for bridging the gaps between design expertise and domain-specific expertise.

</details>


### [36] [A Combined Channel Approach for Decoding Intracranial EEG Signals: Enhancing Accuracy through Spatial Information Integration](https://arxiv.org/abs/2412.06336)

*Maryam Ostadsharif Memar, Navid Ziaei, Behzad Nazari*

**Main category:** cs.HC

**Keywords:** Intracranial EEG, Machine Learning, Brain-Computer Interface, Neurotechnology, Signal Decoding

**Relevance Score:** 8

**TL;DR:** This paper presents a machine learning model for Intracranial EEG (iEEG) signal decoding, showing superior performance through the integration of spatial information from multiple brain regions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The invasive nature of iEEG recording limits dataset availability, necessitating the development of effective machine learning models for neural decoding.

**Method:** A single-participant model using 18 key features, operating in 'best channel' and 'combined channel' modes, was evaluated across three datasets (Music Reconstruction, Audio Visual, AJILE12).

**Key Contributions:**

	1. Introduction of a combined channel mode for iEEG decoding
	2. Demonstrated superior classification performance over best channel mode
	3. Identified relevant brain regions aligned with physiological expectations

**Result:** The combined channel mode produced consistently higher classification performance, with Random Forest achieving up to 0.82 F1 score and XGBoost 0.84 F1 score in different datasets, outperforming the best channel mode.

**Limitations:** 

**Conclusion:** Integrating spatial information from multiple brain regions enhances task decoding in iEEG, presenting opportunities for advancements in BCI systems and neurotechnology.

**Abstract:** Intracranial EEG (iEEG) recording, characterized by high spatial and temporal resolution and superior signal-to-noise ratio (SNR), enables the development of precise brain-computer interface (BCI) systems for neural decoding. However, the invasive nature of the procedure significantly limits the availability of iEEG datasets in terms of both the number of participants and the duration of recorded sessions. To address this limitation, we propose a single-participant machine learning model optimized for decoding iEEG signals. The model employs 18 key features and operates in two modes: best channel and combined channel. The combined channel mode integrates spatial information from multiple brain regions, leading to superior classification performance. Evaluations across three datasets -- Music Reconstruction, Audio Visual, and AJILE12 -- demonstrate that the combined channel mode consistently outperforms the best channel mode across all classifiers. In the best-performing cases, Random Forest achieved an F1 score of 0.81 +/- 0.05 in the Music Reconstruction dataset and 0.82 +/- 0.10 in the Audio Visual dataset, while XGBoost achieved an F1 score of 0.84 +/- 0.08 in the AJILE12 dataset. Furthermore, the analysis of brain region contributions in the combined channel mode revealed that the model identifies relevant brain regions aligned with physiological expectations for each task and effectively combines data from electrodes in these regions to achieve high performance. These findings highlight the potential of integrating spatial information across brain regions to improve task decoding, offering new avenues for advancing BCI systems and neurotechnological applications.

</details>


### [37] [User-Centered-Design as an Empty Signifier in the Context of Developing Digital Applications](https://arxiv.org/abs/2501.04429)

*Murat Sariyar*

**Main category:** cs.HC

**Keywords:** User-Centered Design, sociotechnical systems, Laclau, Lacan, design ambiguity

**Relevance Score:** 8

**TL;DR:** This paper critiques User-Centered Design (UCD) by evaluating its conceptual ambiguities and theoretical underdevelopment, proposing a framework for its analysis using Laclau and Lacan's theories.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the conceptual ambiguities and theoretical gaps in User-Centered Design (UCD) and understand its impact on participatory design practices.

**Method:** The paper utilizes the theories of Ernesto Laclau and Jacques Lacan to analyze UCD as an empty signifier within the context of sociotechnical system design.

**Key Contributions:**

	1. Theoretical framework for analyzing UCD as an empty signifier.
	2. Insight into the dual role of UCD in participatory design practices.
	3. Identification of potential risks associated with UCD's conceptual ambiguity.

**Result:** The analysis reveals that UCD acts as an empty signifier that unifies conflicting expectations while fostering inclusivity and user empowerment, but also risks ideological capture and conceptual dilution.

**Limitations:** 

**Conclusion:** A critical engagement with UCD as an empty signifier can foster reflection and renewal in sociotechnical system design.

**Abstract:** To reduce cycles of rejection and redesign -- especially in the absence of clear acceptance criteria and the diversity of possible development paths -- User-Centered Design (UCD) has become a central methodology in computer science, emphasizing the integration of user perspectives throughout the entire system lifecycle. Despite its widespread adoption, however, UCD remains conceptually ambiguous and theoretically underdeveloped. This paper addresses that gap by drawing on the theories of Ernesto Laclau and Jacques Lacan to analyze UCD as a potential empty signifier: a term that gains rhetorical power precisely through its semantic openness. We argue that this ambiguity enables UCD to unify diverse and sometimes conflicting expectations under a shared label, which both empowers participatory design practices and conceals underlying tensions. Acknowledging UCD as an empty signifier allows for a more critical engagement with its practical and symbolic functions, revealing how it can foster inclusivity, empathy, and user empowerment, but also how it risks ideological capture and conceptual dilution. This theoretical reframing opens new pathways for reflection and renewal within sociotechnical system design.

</details>


### [38] [A Review of LLM-Assisted Ideation](https://arxiv.org/abs/2503.00946)

*Sitong Li, Stefano Padilla, Pierre Le Bras, Junyu Dong, Mike Chantler*

**Main category:** cs.HC

**Keywords:** Large Language Models, Ideation, Human-Computer Interaction, Machine Learning, Research Framework

**Relevance Score:** 9

**TL;DR:** This paper reviews 61 studies on LLM-assisted ideation, introducing the Hourglass Ideation Framework and identifying key trends and gaps in research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the application of large language models in ideation processes and establish a framework for understanding their impact.

**Method:** A systematic review of 61 studies on LLM-assisted ideation with extensive analysis of interaction designs and ideation stages.

**Key Contributions:**

	1. Introduction of the Hourglass Ideation Framework for LLM-assisted ideation
	2. Identification of underexplored areas in the application of LLMs in ideation
	3. Cataloguing extensive research data across ideation stages and interaction designs

**Result:** LLMs are mainly used for idea generation and refinement, but usage in scope specification and multi-idea evaluation is limited. The review presents detailed findings of LLM applications across various ideation stages.

**Limitations:** Limited exploration of LLM use in group ideation and interaction modalities for collaboration.

**Conclusion:** The review outlines the findings to aid researchers in navigating the field and suggests directions for future research in LLM-assisted ideation.

**Abstract:** We present a comprehensive, in-depth review of ideation assisted by large language models (LLMs), highlighting emerging trends and identifying unaddressed research gaps. In total, we examined 61 studies investigating the application of LLMs in both group and individual ideation processes. From these studies, we derived the Hourglass Ideation Framework for LLM-assisted ideation, comprising three phases and seven key ideation stages, which served as the basis for our systematic survey. Our analysis reveals that LLMs are most frequently used for idea generation and refinement, but their use in scope specification, foundational material structuring and multi-idea evaluation and selection remains limited. We provide our findings in extensive tabular and online formats. These catalogues detail research on LLM-assisted, purely LLM-based, and human-only activities across the seven ideation stages for each of the 61 studies. These also detail creative domains, publication outlets, interaction designs, user study designs, and assessment methods. Our analysis of system interaction design reveals a predominant focus on supporting individual ideation activities and text-based interaction, with a growing trend of incorporating multimedia elements. However, in group ideation, tools and interaction modalities targeting both synchronous and asynchronous collaboration are much scarcer. We synthesize the primary findings of our review and outline promising directions for future research in LLM-assisted ideation. We hope this review will help researchers quickly gain an overview of this rapidly expanding area, efficiently locate relevant work, and identify underexplored areas for further investigation. In addition, we believe the framework we present here will form the basis for the development of future problem and solution space taxonomies, and methodologies for LLM-assisted ideation development and use.

</details>


### [39] [Exploring undercurrents of learning tensions in an LLM-enhanced landscape: A student-centered qualitative perspective on LLM vs Search](https://arxiv.org/abs/2504.02622)

*Rahul R. Divekar, Sophia Guerra, Lisette Gonzalez, Natasha Boos, Helen Zhou*

**Main category:** cs.HC

**Keywords:** Large language models, Student learning, Information synthesis, Educational technology, Search engines

**Relevance Score:** 9

**TL;DR:** This study explores the impact of large language models (LLMs) on student learning by comparing their use with traditional search engines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the transformative potential of LLMs in education and their role in information discovery and synthesis.

**Method:** A within-subjects, counterbalanced design was used, where participants learned using Google and ChatGPT, followed by interviews to gather their reflections and preferences.

**Key Contributions:**

	1. Comparison of LLMs and traditional search engines in educational contexts
	2. Insights into student preferences and experiences with LLMs
	3. Implications for future educational tools and policies

**Result:** Analysis revealed nuanced insights into when and why students prefer LLMs over traditional search engines, highlighting unique advantages and challenges.

**Limitations:** The study is limited to specific learning topics and may not generalize across all subjects or demographics.

**Conclusion:** LLMs may represent a significant shift in educational technology, necessitating adaptation from educators and developers.

**Abstract:** Large language models (LLMs) are transforming how students learn by providing readily available tools that can quickly augment or complete various learning activities with non-trivial performance. Similar paradigm shifts have occurred in the past with the introduction of search engines and Wikipedia, which replaced or supplemented traditional information sources such as libraries and books. This study investigates the potential for LLMs to represent the next shift in learning, focusing on their role in information discovery and synthesis compared to existing technologies, such as search engines. Using a within-subjects, counterbalanced design, participants learned new topics using a search engine (Google) and an LLM (ChatGPT). Post-task follow-up interviews explored students' reflections, preferences, pain points, and overall perceptions. We present analysis of their responses that show nuanced insights into when, why, and how students prefer LLMs over search engines, offering implications for educators, policymakers, and technology developers navigating the evolving educational landscape.

</details>


### [40] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)

*Isabel Villanueva, Tara Bobinac, Binwei Yao, Junjie Hu, Kaiping Chen*

**Main category:** cs.HC

**Keywords:** AI, intercultural empathy, cross-cultural deliberation, AI alignment, cultural representation

**Relevance Score:** 8

**TL;DR:** This study investigates how different AI deliberation approaches affect intercultural empathy, revealing that while cross-cultural deliberation enhances empathy for Americans, it fails to resonate with Latin Americans due to perceived cultural inauthenticity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of AI chatbots in fostering intercultural empathy across diverse cultural backgrounds, particularly amidst the rise of AI in public discourse.

**Method:** A randomized experiment comparing three AI deliberation approaches: cross-cultural deliberation, own-culture deliberation, and a non-deliberative control group, involving American and Latin American participants.

**Key Contributions:**

	1. Demonstrates the varying efficacy of AI deliberation approaches on different cultural groups.
	2. Reveals systematic cultural representation gaps in AI responses, particularly for Latin Americans.
	3. Advances both deliberation theory and AI alignment research with implications for equitable AI design.

**Result:** Cross-cultural deliberation increased intercultural empathy among American participants but was ineffective for Latin American participants, who found the AI responses culturally inauthentic.

**Limitations:** The study highlights limitations in the AI's ability to represent complex cultural contexts despite advanced prompt engineering.

**Conclusion:** Current AI cultural alignment methods do not sufficiently address representational issues, highlighting the need for equitable AI systems in cross-cultural dialogue.

**Abstract:** Despite increasing AI chatbot deployment in public discourse, empirical evidence on their capacity to foster intercultural empathy remains limited. Through a randomized experiment, we assessed how different AI deliberation approaches--cross-cultural deliberation (presenting other-culture perspectives), own-culture deliberation (representing participants' own culture), and non-deliberative control--affect intercultural empathy across American and Latin American participants. Cross-cultural deliberation increased intercultural empathy among American participants through positive emotional engagement, but produced no such effects for Latin American participants, who perceived AI responses as culturally inauthentic despite explicit prompting to represent their cultural perspectives. Our analysis of participant-driven feedback, where users directly flagged and explained culturally inappropriate AI responses, revealed systematic gaps in AI's representation of Latin American contexts that persist despite sophisticated prompt engineering. These findings demonstrate that current approaches to AI cultural alignment--including linguistic adaptation and explicit cultural prompting--cannot fully address deeper representational asymmetries in AI systems. Our work advances both deliberation theory and AI alignment research by revealing how the same AI system can simultaneously promote intercultural understanding for one cultural group while failing for another, with critical implications for designing equitable AI systems for cross-cultural democratic discourse.

</details>


### [41] [BDIViz: An Interactive Visualization System for Biomedical Schema Matching with LLM-Powered Validation](https://arxiv.org/abs/2507.16117)

*Eden Wu, Dishita G Turakhia, Guande Wu, Christos Koutras, Sarah Keegan, Wenke Liu, Beata Szeitz, David Fenyo, Cláudio T. Silva, Juliana Freire*

**Main category:** cs.HC

**Keywords:** biomedical data, schema matching, visual analytics, LLM-based validation, interactive visualization

**Relevance Score:** 8

**TL;DR:** BDIViz is a visual analytics system that enhances schema matching for biomedical data, improving accuracy and reducing cognitive load.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Address the labor-intensive and error-prone task of schema matching in biomedical data harmonization.

**Method:** BDIViz combines multiple schema matching methods with LLM-based validation and employs interactive visualization techniques, including heatmaps and coordinated views.

**Key Contributions:**

	1. Development of BDIViz for improved schema matching in biomedical informatics.
	2. Integration of LLM-based validation with interactive visualization techniques.
	3. Demonstration of significant accuracy improvements in matching through case studies and user studies.

**Result:** BDIViz significantly improves matching accuracy and reduces cognitive load and curation time in biomedical schema matching.

**Limitations:** 

**Conclusion:** The method-agnostic design of BDIViz allows it to adapt to various schema matching algorithms and specific application needs, demonstrating effectiveness through user studies.

**Abstract:** Biomedical data harmonization is essential for enabling exploratory analyses and meta-studies, but the process of schema matching - identifying semantic correspondences between elements of disparate datasets (schemas) - remains a labor-intensive and error-prone task. Even state-of-the-art automated methods often yield low accuracy when applied to biomedical schemas due to the large number of attributes and nuanced semantic differences between them. We present BDIViz, a novel visual analytics system designed to streamline the schema matching process for biomedical data. Through formative studies with domain experts, we identified key requirements for an effective solution and developed interactive visualization techniques that address both scalability challenges and semantic ambiguity. BDIViz employs an ensemble approach that combines multiple matching methods with LLM-based validation, summarizes matches through interactive heatmaps, and provides coordinated views that enable users to quickly compare attributes and their values. Our method-agnostic design allows the system to integrate various schema matching algorithms and adapt to application-specific needs. Through two biomedical case studies and a within-subject user study with domain experts, we demonstrate that BDIViz significantly improves matching accuracy while reducing cognitive load and curation time compared to baseline approaches.

</details>


### [42] [SceneLoom: Communicating Data with Scene Context](https://arxiv.org/abs/2507.16466)

*Lin Gao, Leixian Shen, Yuheng Zhao, Jiexiang Lan, Huamin Qu, Siming Chen*

**Main category:** cs.HC

**Keywords:** data visualization, human-computer interaction, narrative context, Vision-Language Model, creative design

**Relevance Score:** 8

**TL;DR:** SceneLoom is a VLM-powered system that enhances data visualization in storytelling by integrating real-world imagery, enabling coherent visual and semantic design mappings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to improve the integration of data visualizations with real-world imagery in storytelling contexts, enhancing narrative expressiveness and engagement.

**Method:** SceneLoom uses a Vision-Language Model to extract visual and semantic features from both scene images and data visualizations, performing design mapping based on spatial organization, shape similarity, layout consistency, and semantic binding.

**Key Contributions:**

	1. Introduction of SceneLoom as a system for coordinating data visualizations with imagery
	2. Development of a design mapping method using VLMs
	3. User validation showcasing enhanced design expressiveness and engagement

**Result:** The system generates contextually expressive design alternatives that support coherent alignments across visual, semantic, and data dimensions, validated through user studies and an example gallery.

**Limitations:** The study may be limited by the specific contexts tested and the scalability of the system across diverse storytelling formats.

**Conclusion:** SceneLoom effectively inspires creative design and facilitates design externalization, demonstrating its potential in data-driven storytelling.

**Abstract:** In data-driven storytelling contexts such as data journalism and data videos, data visualizations are often presented alongside real-world imagery to support narrative context. However, these visualizations and contextual images typically remain separated, limiting their combined narrative expressiveness and engagement. Achieving this is challenging due to the need for fine-grained alignment and creative ideation. To address this, we present SceneLoom, a Vision-Language Model (VLM)-powered system that facilitates the coordination of data visualization with real-world imagery based on narrative intents. Through a formative study, we investigated the design space of coordination relationships between data visualization and real-world scenes from the perspectives of visual alignment and semantic coherence. Guided by the derived design considerations, SceneLoom leverages VLMs to extract visual and semantic features from scene images and data visualization, and perform design mapping through a reasoning process that incorporates spatial organization, shape similarity, layout consistency, and semantic binding. The system generates a set of contextually expressive, image-driven design alternatives that achieve coherent alignments across visual, semantic, and data dimensions. Users can explore these alternatives, select preferred mappings, and further refine the design through interactive adjustments and animated transitions to support expressive data communication. A user study and an example gallery validate SceneLoom's effectiveness in inspiring creative design and facilitating design externalization.

</details>


### [43] [Technological folie à deux: Feedback Loops Between AI Chatbots and Mental Illness](https://arxiv.org/abs/2507.19218)

*Sebastian Dohnány, Zeb Kurth-Nelson, Eleanor Spens, Lennart Luettgau, Alastair Reid, Iason Gabriel, Christopher Summerfield, Murray Shanahan, Matthew M Nour*

**Main category:** cs.HC

**Keywords:** AI chatbots, emotional support, mental health, human-computer interaction, safety measures

**Relevance Score:** 8

**TL;DR:** This paper discusses the rising risks associated with AI chatbots used for emotional support, particularly in individuals with mental health conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the adoption of AI chatbots for emotional support amidst social isolation and constrained mental health services, exploring the associated risks, particularly for vulnerable populations.

**Method:** The approach involves analyzing the interactions between human cognitive/emotional biases and chatbot behaviors like agreeableness and adaptability, alongside reviewing relevant AI safety measures.

**Key Contributions:**

	1. Analyzed risks of emotional reliance on chatbots in users with mental health issues.
	2. Highlighted inadequacies in current AI safety measures.
	3. Proposed a need for interdisciplinary coordination to address identified risks.

**Result:** Findings indicate increased risks of belief destabilization and dependence among users with mental health conditions, alongside a gap in current AI safety measures to mitigate these risks.

**Limitations:** 

**Conclusion:** Coordinated action is necessary across diverse fields to tackle the emerging public health concerns posed by chatbot interactions.

**Abstract:** Artificial intelligence chatbots have achieved unprecedented adoption, with millions now using these systems for emotional support and companionship in contexts of widespread social isolation and capacity-constrained mental health services. While some users report psychological benefits, concerning edge cases are emerging, including reports of suicide, violence, and delusional thinking linked to perceived emotional relationships with chatbots. To understand this new risk profile we need to consider the interaction between human cognitive and emotional biases, and chatbot behavioural tendencies such as agreeableness (sycophancy) and adaptability (in-context learning). We argue that individuals with mental health conditions face increased risks of chatbot-induced belief destabilization and dependence, owing to altered belief-updating, impaired reality-testing, and social isolation. Current AI safety measures are inadequate to address these interaction-based risks. To address this emerging public health concern, we need coordinated action across clinical practice, AI development, and regulatory frameworks.

</details>


### [44] [AI as a deliberative partner fosters intercultural empathy for Americans but fails for Latin American participants](https://arxiv.org/abs/2504.13887)

*Isabel Villanueva, Tara Bobinac, Binwei Yao, Junjie Hu, Kaiping Chen*

**Main category:** cs.HC

**Keywords:** AI chatbots, intercultural empathy, deliberation theory, cultural alignment, democratic discourse

**Relevance Score:** 8

**TL;DR:** The study examines AI chatbots' ability to foster intercultural empathy through different deliberation approaches and reveals significant representational gaps in AI responses for Latin American participants.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the role of AI chatbots in promoting intercultural empathy and evaluate their effectiveness in a multicultural context.

**Method:** A randomized experiment comparing three AI deliberation approaches: cross-cultural deliberation, own-culture deliberation, and non-deliberative control, targeting American and Latin American participants.

**Key Contributions:**

	1. Empirical evidence of the limitations of AI chatbots in fostering intercultural empathy across different cultural groups.
	2. Identification of systemic gaps in AI's representation of Latin American cultures despite advanced prompt engineering.
	3. Implications for AI alignment research and design of democratic discourse systems.

**Result:** Cross-cultural deliberation enhanced empathy among American participants but not for Latin Americans, who felt AI responses lacked cultural authenticity.

**Limitations:** Findings are based on specific cultural contexts and may not generalize to all cultures or AI systems.

**Conclusion:** Current AI cultural alignment techniques are insufficient to bridge representational gaps, highlighting the need for equitable AI system design that serves diverse cultural perspectives effectively.

**Abstract:** Despite increasing AI chatbot deployment in public discourse, empirical evidence on their capacity to foster intercultural empathy remains limited. Through a randomized experiment, we assessed how different AI deliberation approaches--cross-cultural deliberation (presenting other-culture perspectives), own-culture deliberation (representing participants' own culture), and non-deliberative control--affect intercultural empathy across American and Latin American participants. Cross-cultural deliberation increased intercultural empathy among American participants through positive emotional engagement, but produced no such effects for Latin American participants, who perceived AI responses as culturally inauthentic despite explicit prompting to represent their cultural perspectives. Our analysis of participant-driven feedback, where users directly flagged and explained culturally inappropriate AI responses, revealed systematic gaps in AI's representation of Latin American contexts that persist despite sophisticated prompt engineering. These findings demonstrate that current approaches to AI cultural alignment--including linguistic adaptation and explicit cultural prompting--cannot fully address deeper representational asymmetries in AI systems. Our work advances both deliberation theory and AI alignment research by revealing how the same AI system can simultaneously promote intercultural understanding for one cultural group while failing for another, with critical implications for designing equitable AI systems for cross-cultural democratic discourse.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [45] [Advancing Mental Disorder Detection: A Comparative Evaluation of Transformer and LSTM Architectures on Social Media](https://arxiv.org/abs/2507.19511)

*Khalid Hasan, Jamil Saquer, Mukulika Ghosh*

**Main category:** cs.CL

**Keywords:** mental health, NLP, transformer models, LSTM, classification

**Relevance Score:** 9

**TL;DR:** This study evaluates transformer models for mental health disorder classification, finding that they outperform traditional LSTM approaches, with RoBERTa achieving the highest performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for automated tools for early detection and monitoring of mental health disorders due to their rising prevalence.

**Method:** Evaluation of transformer models (BERT, RoBERTa, DistilBERT, ALBERT, ELECTRA) against LSTM models using various text embedding techniques on annotated datasets from Reddit.

**Key Contributions:**

	1. Construction of a large annotated dataset for mental health classification.
	2. Demonstration of the superiority of transformer models over LSTM architecture.
	3. Insights into the implications of NLP methodologies for clinical applications.

**Result:** RoBERTa achieved a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set, showing superior performance compared to LSTMs.

**Limitations:** 

**Conclusion:** Transformer models are effective for real-time mental health monitoring, offering insights that could enhance clinical applications and digital mental health interventions.

**Abstract:** The rising prevalence of mental health disorders necessitates the development of robust, automated tools for early detection and monitoring. Recent advances in Natural Language Processing (NLP), particularly transformer-based architectures, have demonstrated significant potential in text analysis. This study provides a comprehensive evaluation of state-of-the-art transformer models (BERT, RoBERTa, DistilBERT, ALBERT, and ELECTRA) against Long Short-Term Memory (LSTM) based approaches using different text embedding techniques for mental health disorder classification on Reddit. We construct a large annotated dataset, validating its reliability through statistical judgmental analysis and topic modeling. Experimental results demonstrate the superior performance of transformer models over traditional deep-learning approaches. RoBERTa achieved the highest classification performance, with a 99.54% F1 score on the hold-out test set and a 96.05% F1 score on the external test set. Notably, LSTM models augmented with BERT embeddings proved highly competitive, achieving F1 scores exceeding 94% on the external dataset while requiring significantly fewer computational resources. These findings highlight the effectiveness of transformer-based models for real-time, scalable mental health monitoring. We discuss the implications for clinical applications and digital mental health interventions, offering insights into the capabilities and limitations of state-of-the-art NLP methodologies in mental disorder detection.

</details>


### [46] [Setting The Table with Intent: Intent-aware Schema Generation and Editing for Literature Review Tables](https://arxiv.org/abs/2507.19521)

*Vishakh Padmakumar, Joseph Chee Chang, Kyle Lo, Doug Downey, Aakanksha Naik*

**Main category:** cs.CL

**Keywords:** schema generation, large language models, document comparison

**Relevance Score:** 8

**TL;DR:** This paper advances schema generation for document comparison by addressing ambiguity and proposing LLM-based editing techniques, resulting in improved performance and refinement of schemas.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the growing volume of academic literature, effective organization and comparison of documents becomes essential, necessitating improved schema generation methods.

**Method:** The authors augment unannotated table corpora with synthesized intents to create a dataset for studying schema generation. They benchmark multiple schema generation methods, including LLM workflows and fine-tuned models, and introduce editing techniques to enhance generated schemas.

**Key Contributions:**

	1. Introduction of a dataset for schema generation using synthesized intents
	2. Benchmarking of schema generation methods with findings that small, open-weight models can be effective
	3. Development of LLM-based editing techniques to improve schema quality

**Result:** Incorporating table intents into schema generation significantly improves baseline performance, and LLM-based editing techniques further refine the generated schemas, leading to better quality comparisons.

**Limitations:** 

**Conclusion:** The study shows that better data and innovative editing methods enhance the effectiveness of schema generation, making LLMs more competitive in this domain.

**Abstract:** The increasing volume of academic literature makes it essential for researchers to organize, compare, and contrast collections of documents. Large language models (LLMs) can support this process by generating schemas defining shared aspects along which to compare papers. However, progress on schema generation has been slow due to: (i) ambiguity in reference-based evaluations, and (ii) lack of editing/refinement methods. Our work is the first to address both issues. First, we present an approach for augmenting unannotated table corpora with synthesized intents and apply it to create a dataset for studying schema generation conditioned on a given information need, thus reducing ambiguity. With this dataset, we show how incorporating table intents significantly improves baseline performance in reconstructing reference schemas. Next, we propose several LLM-based schema editing techniques. We start by comprehensively benchmarking several single-shot schema generation methods, including prompted LLM workflows and fine-tuned models, showing that smaller, open-weight models can be fine-tuned to be competitive with state-of-the-art prompted LLMs. Then we demonstrate that our editing techniques can further improve schemas generated by these methods.

</details>


### [47] [Mind the Language Gap in Digital Humanities: LLM-Aided Translation of SKOS Thesauri](https://arxiv.org/abs/2507.19537)

*Felix Kraus, Nicolas Blumenröhr, Danah Tonne, Achim Streit*

**Main category:** cs.CL

**Keywords:** WOKIE, automated translation, Digital Humanities, Large Language Models, SKOS thesauri

**Relevance Score:** 7

**TL;DR:** WOKIE is an open-source pipeline for automating the translation of SKOS thesauri, enhancing accessibility and interoperability in Digital Humanities.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of language diversity in Digital Humanities, which limits access and reuse of knowledge resources.

**Method:** WOKIE combines external translation services with targeted refinement using Large Language Models (LLMs) to improve translation quality and performance.

**Key Contributions:**

	1. Open-source and modular design for easy extensibility
	2. Combines translation services with LLMs for improved quality
	3. Addresses language diversity to enhance accessibility and reuse in DH.

**Result:** WOKIE was evaluated on multiple DH thesauri in 15 languages, demonstrating enhanced translation quality, performance, and ontology matching.

**Limitations:** 

**Conclusion:** WOKIE effectively supports more inclusive, multilingual research infrastructures by providing automated translation and improved ontology performance.

**Abstract:** We introduce WOKIE, an open-source, modular, and ready-to-use pipeline for the automated translation of SKOS thesauri. This work addresses a critical need in the Digital Humanities (DH), where language diversity can limit access, reuse, and semantic interoperability of knowledge resources. WOKIE combines external translation services with targeted refinement using Large Language Models (LLMs), balancing translation quality, scalability, and cost. Designed to run on everyday hardware and be easily extended, the application requires no prior expertise in machine translation or LLMs. We evaluate WOKIE across several DH thesauri in 15 languages with different parameters, translation services and LLMs, systematically analysing translation quality, performance, and ontology matching improvements. Our results show that WOKIE is suitable to enhance the accessibility, reuse, and cross-lingual interoperability of thesauri by hurdle-free automated translation and improved ontology matching performance, supporting more inclusive and multilingual research infrastructures.

</details>


### [48] [Mitigating Geospatial Knowledge Hallucination in Large Language Models: Benchmarking and Dynamic Factuality Aligning](https://arxiv.org/abs/2507.19586)

*Shengyuan Wang, Jie Feng, Tianhui Liu, Dan Pei, Yong Li*

**Main category:** cs.CL

**Keywords:** large language models, geospatial knowledge, hallucinations, Kahneman-Tversky Optimization, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper evaluates and mitigates geospatial hallucinations in large language models (LLMs) using a new framework and a method based on Kahneman-Tversky Optimization, resulting in significant performance improvements.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the gap in systematic evaluation and mitigation of geospatial hallucinations in LLMs, which affects their reliability in geospatial tasks.

**Method:** A comprehensive evaluation framework was developed using structured geospatial knowledge graphs, and a dynamic factuality aligning method based on Kahneman-Tversky Optimization was introduced.

**Key Contributions:**

	1. Development of an evaluation framework for geospatial hallucinations in LLMs.
	2. Introduction of a dynamic method to mitigate geospatial hallucinations using Kahneman-Tversky Optimization.
	3. Extensive experimental results that validate the proposed benchmark and learning algorithm.

**Result:** The proposed method achieved a performance improvement of over 29.6% on the benchmark for geospatial knowledge accuracy in LLMs.

**Limitations:** 

**Conclusion:** The findings demonstrate the effectiveness of the evaluation framework and the learning algorithm in enhancing LLMs' trustworthiness for geospatial knowledge and reasoning.

**Abstract:** Large language models (LLMs) possess extensive world knowledge, including geospatial knowledge, which has been successfully applied to various geospatial tasks such as mobility prediction and social indicator prediction. However, LLMs often generate inaccurate geospatial knowledge, leading to geospatial hallucinations (incorrect or inconsistent representations of geospatial information) that compromise their reliability. While the phenomenon of general knowledge hallucination in LLMs has been widely studied, the systematic evaluation and mitigation of geospatial hallucinations remain largely unexplored. To address this gap, we propose a comprehensive evaluation framework for geospatial hallucinations, leveraging structured geospatial knowledge graphs for controlled assessment. Through extensive evaluation across 20 advanced LLMs, we uncover the hallucinations in their geospatial knowledge. Building on these insights, we introduce a dynamic factuality aligning method based on Kahneman-Tversky Optimization (KTO) to mitigate geospatial hallucinations in LLMs, leading to a performance improvement of over 29.6% on the proposed benchmark. Extensive experimental results demonstrate the effectiveness of our benchmark and learning algorithm in enhancing the trustworthiness of LLMs in geospatial knowledge and reasoning tasks.

</details>


### [49] [Efficient Attention Mechanisms for Large Language Models: A Survey](https://arxiv.org/abs/2507.19595)

*Yutao Sun, Zhenyu Li, Yike Zhang, Tengyu Pan, Bowen Dong, Yuyi Guo, Jianyong Wang*

**Main category:** cs.CL

**Keywords:** efficiency, attention mechanisms, large language models

**Relevance Score:** 8

**TL;DR:** This survey reviews efficient attention mechanisms in transformer-based models focusing on linear and sparse attention techniques to mitigate computational complexity in long-context modeling.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of quadratic time and memory complexity in self-attention mechanisms of large language models, promoting more efficient long-context modeling.

**Method:** The paper systematically reviews recent advancements in efficient attention, categorizing them into linear and sparse approaches, and exploring their theoretical and hardware implications.

**Key Contributions:**

	1. Comprehensive overview of efficient attention mechanisms in language models
	2. Integration of algorithmic innovations with hardware considerations
	3. Analysis of efficient attention in large-scale pre-trained models

**Result:** The author integrates various efficient attention methods into large-scale pre-trained language models, analyzing architectures that utilize efficient attention exclusively or in hybrid forms.

**Limitations:** 

**Conclusion:** This work serves as a foundational reference for further advancements in the design of scalable and efficient language models by combining theoretical foundations with deployment strategies.

**Abstract:** Transformer-based architectures have become the prevailing backbone of large language models. However, the quadratic time and memory complexity of self-attention remains a fundamental obstacle to efficient long-context modeling. To address this limitation, recent research has introduced two principal categories of efficient attention mechanisms. Linear attention methods achieve linear complexity through kernel approximations, recurrent formulations, or fastweight dynamics, thereby enabling scalable inference with reduced computational overhead. Sparse attention techniques, in contrast, limit attention computation to selected subsets of tokens based on fixed patterns, block-wise routing, or clustering strategies, enhancing efficiency while preserving contextual coverage. This survey provides a systematic and comprehensive overview of these developments, integrating both algorithmic innovations and hardware-level considerations. In addition, we analyze the incorporation of efficient attention into largescale pre-trained language models, including both architectures built entirely on efficient attention and hybrid designs that combine local and global components. By aligning theoretical foundations with practical deployment strategies, this work aims to serve as a foundational reference for advancing the design of scalable and efficient language models.

</details>


### [50] [MOCHA: Are Code Language Models Robust Against Multi-Turn Malicious Coding Prompts?](https://arxiv.org/abs/2507.19598)

*Muntasir Wahed, Xiaona Zhou, Kiet A. Nguyen, Tianjiao Yu, Nirav Diwan, Gang Wang, Dilek Hakkani-Tür, Ismini Lourentzou*

**Main category:** cs.CL

**Keywords:** Large Language Models, code generation, adversarial attacks, benchmarking, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper addresses the robustness of Large Language Models (LLMs) in code generation against adversarial attacks using multi-turn prompts. It introduces a benchmark for evaluating these models and presents improvements to rejection rates through fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the vulnerabilities of LLMs in code generation against adversarial misuse through sophisticated coding prompts that evade safety filters.

**Method:** The authors propose code decomposition attacks and build a benchmark called enchmarkname{} for evaluating both single-turn and multi-turn malicious prompts. They conducted empirical evaluations on various models to assess their robustness.

**Key Contributions:**

	1. Introduction of code decomposition attacks for LLMs.
	2. Development of the enchmarkname{} benchmark for systematic evaluation.
	3. Demonstration of significant improvement in rejection rates through fine-tuning on MOCHA.

**Result:** The study revealed that LLMs exhibit persistent vulnerabilities, particularly in multi-turn interactions. Fine-tuning on the MOCHA dataset resulted in significantly improved rejection rates while maintaining coding capabilities, achieving up to a 32.4% increase in robustness against adversarial datasets.

**Limitations:** The study may not cover all potential adversarial strategies and focuses primarily on multi-turn interactions.

**Conclusion:** Enhancing LLMs' resistance to adversarial prompts is crucial, and the proposed MOCHA fine-tuning method shows promise in maintaining performance while improving safety against multi-turn attacks.

**Abstract:** Recent advancements in Large Language Models (LLMs) have significantly enhanced their code generation capabilities. However, their robustness against adversarial misuse, particularly through multi-turn malicious coding prompts, remains underexplored. In this work, we introduce code decomposition attacks, where a malicious coding task is broken down into a series of seemingly benign subtasks across multiple conversational turns to evade safety filters. To facilitate systematic evaluation, we introduce \benchmarkname{}, a large-scale benchmark designed to evaluate the robustness of code LLMs against both single-turn and multi-turn malicious prompts. Empirical results across open- and closed-source models reveal persistent vulnerabilities, especially under multi-turn scenarios. Fine-tuning on MOCHA improves rejection rates while preserving coding ability, and importantly, enhances robustness on external adversarial datasets with up to 32.4% increase in rejection rates without any additional supervision.

</details>


### [51] [HITSZ's End-To-End Speech Translation Systems Combining Sequence-to-Sequence Auto Speech Recognition Model and Indic Large Language Model for IWSLT 2025 in Indic Track](https://arxiv.org/abs/2507.19616)

*Xuchen Wei, Yangxin Wu, Yaoyin Zhang, Henglyu Liu, Kehai Chen, Xuefeng Bai, Min Zhang*

**Main category:** cs.CL

**Keywords:** speech-to-text translation, large language model, Chain-of-Thought, BLEU scores, low-resource translation

**Relevance Score:** 7

**TL;DR:** This paper discusses an end-to-end speech-to-text translation system for English-to-Indic and Indic-to-English using a pre-trained Whisper ASR model combined with an Indic-specialized LLM.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve translation quality in low-resource speech-to-text translation tasks between English and Indic languages.

**Method:** An end-to-end system integrating the pre-trained Whisper ASR model with Krutrim, an Indic-specialized LLM.

**Key Contributions:**

	1. Integration of a pre-trained Whisper ASR model with an Indic-specialized large language model.
	2. Demonstration of significant BLEU score improvement using Chain-of-Thought method in some language pairs.
	3. Exploration of challenges in maintaining output consistency in CoT applications.

**Result:** Achieved average BLEU scores of 28.88 for English-to-Indic and 27.86 for Indic-to-English translations. Additionally, the Chain-of-Thought method increased BLEU scores significantly for certain language pairs, but consistency in output format was a challenge.

**Limitations:** Challenges in ensuring consistent adherence to the required output format for the Chain-of-Thought method.

**Conclusion:** The proposed system demonstrates substantial improvements in translation quality while highlighting the necessity for consistent output formatting in Chain-of-Thought applications.

**Abstract:** This paper presents HITSZ's submission for the IWSLT 2025 Indic track, focusing on speech-to-text translation (ST) for English-to-Indic and Indic-to-English language pairs. To enhance translation quality in this low-resource scenario, we propose an end-to-end system integrating the pre-trained Whisper automated speech recognition (ASR) model with Krutrim, an Indic-specialized large language model (LLM). Experimental results demonstrate that our end-to-end system achieved average BLEU scores of $28.88$ for English-to-Indic directions and $27.86$ for Indic-to-English directions. Furthermore, we investigated the Chain-of-Thought (CoT) method. While this method showed potential for significant translation quality improvements on successfully parsed outputs (e.g. a $13.84$ BLEU increase for Tamil-to-English), we observed challenges in ensuring the model consistently adheres to the required CoT output format.

</details>


### [52] [MCIF: Multimodal Crosslingual Instruction-Following Benchmark from Scientific Talks](https://arxiv.org/abs/2507.19634)

*Sara Papi, Maike Züfle, Marco Gaido, Beatrice Savoldi, Danni Liu, Ioannis Douros, Luisa Bentivogli, Jan Niehues*

**Main category:** cs.CL

**Keywords:** multimodal LLMs, multilingual evaluation, benchmarking, instruction-following, crosslingual

**Relevance Score:** 9

**TL;DR:** The paper introduces MCIF, a multilingual benchmark for evaluating multimodal instruction-following in large language models across multiple languages and modalities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of existing benchmarks that fail to evaluate multilingual and multimodal capabilities of LLMs in a unified framework.

**Method:** MCIF integrates human annotations based on scientific talks to assess instruction-following in crosslingual and multimodal contexts over both short and long-form inputs.

**Key Contributions:**

	1. Introduction of MCIF, the first multilingual benchmark for MLLMs
	2. Focus on crosslingual instruction-following in multimodal settings
	3. Incorporation of human annotations for comprehensive evaluation

**Result:** The benchmark spans three modalities (speech, vision, text) and four languages (English, German, Italian, Chinese), providing a basis for comprehensive evaluation of MLLMs.

**Limitations:** 

**Conclusion:** MCIF is intended to advance the research in multilingual multimodal LLMs, offering an open resource for assessment and benchmarking.

**Abstract:** Recent advances in large language models have catalyzed the development of multimodal LLMs (MLLMs) that integrate text, speech, and vision within unified frameworks. As MLLMs evolve from narrow, monolingual, task-specific systems to general-purpose instruction-following models, a key frontier lies in evaluating their multilingual and multimodal capabilities over both long and short contexts. However, existing benchmarks fall short in evaluating these dimensions jointly: they are often limited to English, mostly focus on one single modality at a time, rely on short-form contexts, or lack human annotations -- hindering comprehensive assessment of model performance across languages, modalities, and task complexity. To address these gaps, we introduce MCIF (Multimodal Crosslingual Instruction Following), the first multilingual human-annotated benchmark based on scientific talks that is designed to evaluate instruction-following in crosslingual, multimodal settings over both short- and long-form inputs. MCIF spans three core modalities -- speech, vision, and text -- and four diverse languages (English, German, Italian, and Chinese), enabling a comprehensive evaluation of MLLMs' abilities to interpret instructions across languages and combine them with multimodal contextual information. MCIF is released under a CC-BY 4.0 license to encourage open research and progress in MLLMs development.

</details>


### [53] [RoD-TAL: A Benchmark for Answering Questions in Romanian Driving License Exams](https://arxiv.org/abs/2507.19666)

*Andrei Vlad Man, Răzvan-Alexandru Smădu, Cristian-George Craciun, Dumitru-Clementin Cercel, Florin Pop, Mihaela-Claudia Cercel*

**Main category:** cs.CL

**Keywords:** Large Language Models, Vision-Language Models, legal education, Romanian driving law, multimodal dataset

**Relevance Score:** 3

**TL;DR:** This paper evaluates LLMs and VLMs for Romanian driving law through a novel dataset and multimodal tasks.

**Read time:** 49 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for legal education tools in under-resourced languages like Romanian.

**Method:** Utilization of RoD-TAL dataset for evaluating LLMs and VLMs in textual and visual question-answering tasks, including RAG pipelines and reasoning-optimized models.

**Key Contributions:**

	1. Introduction of RoD-TAL, a multimodal dataset for Romanian driving law assessment.
	2. Implementation of retrieval-augmented generation and reasoning-optimized models.
	3. Findings on the effectiveness of domain-specific fine-tuning and chain-of-thought prompting.

**Result:** Domain-specific fine-tuning enhances retrieval performance; specialized reasoning models improve QA accuracy, though visual reasoning remains a challenge.

**Limitations:** Visual reasoning challenges persist when applying LLMs and VLMs to legal education tasks.

**Conclusion:** While LLMs and VLMs show promise in legal education, limitations exist, particularly in visual reasoning capabilities.

**Abstract:** The intersection of AI and legal systems presents a growing need for tools that support legal education, particularly in under-resourced languages such as Romanian. In this work, we aim to evaluate the capabilities of Large Language Models (LLMs) and Vision-Language Models (VLMs) in understanding and reasoning about Romanian driving law through textual and visual question-answering tasks. To facilitate this, we introduce RoD-TAL, a novel multimodal dataset comprising Romanian driving test questions, text-based and image-based, alongside annotated legal references and human explanations. We implement and assess retrieval-augmented generation (RAG) pipelines, dense retrievers, and reasoning-optimized models across tasks including Information Retrieval (IR), Question Answering (QA), Visual IR, and Visual QA. Our experiments demonstrate that domain-specific fine-tuning significantly enhances retrieval performance. At the same time, chain-of-thought prompting and specialized reasoning models improve QA accuracy, surpassing the minimum grades required to pass driving exams. However, visual reasoning remains challenging, highlighting the potential and the limitations of applying LLMs and VLMs to legal education.

</details>


### [54] [Towards Inclusive NLP: Assessing Compressed Multilingual Transformers across Diverse Language Benchmarks](https://arxiv.org/abs/2507.19699)

*Maitha Alshehhi, Ahmed Sharshar, Mohsen Guizani*

**Main category:** cs.CL

**Keywords:** Large Language Models, multilingual NLP, model compression, pruning, quantization

**Relevance Score:** 8

**TL;DR:** Benchmarking multilingual and monolingual LLMs across Arabic, English, and Indic languages reveals performance differences influenced by linguistic diversity and resource availability, highlighting the advantages of multilingual models and the impacts of model compression strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the performance of LLMs in low-resource languages like Kannada and Arabic and the effects of model compression strategies.

**Method:** The study benchmarks multilingual and monolingual LLMs across multiple languages, examining the impact of pruning and quantization on their performance.

**Key Contributions:**

	1. Performance benchmarking of multilingual vs. monolingual LLMs
	2. Insights into model compression effects on LLMs
	3. Recommendations for scalable multilingual NLP solutions

**Result:** The findings show that multilingual LLMs outperform monolingual counterparts, with quantization maintaining accuracy but aggressive pruning harming performance.

**Limitations:** Focus on specific languages may not generalize; the potential for hallucination and generalization errors in low-resource contexts needs further research.

**Conclusion:** The research suggests strategies for developing effective multilingual NLP solutions and emphasizes the importance of addressing hallucination and generalization issues in low-resource environments.

**Abstract:** Although LLMs have attained significant success in high-resource languages, their capacity in low-resource linguistic environments like Kannada and Arabic is not yet fully understood. This work benchmarking the performance of multilingual and monolingual Large Language Models (LLMs) across Arabic, English, and Indic languages, with particular emphasis on the effects of model compression strategies such as pruning and quantization. Findings shows significant performance differences driven by linguistic diversity and resource availability on SOTA LLMS as BLOOMZ, AceGPT, Jais, LLaMA-2, XGLM, and AraGPT2. We find that multilingual versions of the model outperform their language-specific counterparts across the board, indicating substantial cross-lingual transfer benefits. Quantization (4-bit and 8-bit) is effective in maintaining model accuracy while promoting efficiency, but aggressive pruning significantly compromises performance, especially in bigger models. Our findings pinpoint key strategies to construct scalable and fair multilingual NLP solutions and underscore the need for interventions to address hallucination and generalization errors in the low-resource setting.

</details>


### [55] [Ta-G-T: Subjectivity Capture in Table to Text Generation via RDF Graphs](https://arxiv.org/abs/2507.19710)

*Ronak Upasham, Tathagata Dey, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Table-to-Text, subjectivity, RDF, T5 models, natural language generation

**Relevance Score:** 7

**TL;DR:** This paper introduces a novel pipeline for Table-to-Text generation that incorporates both objective descriptions and subjective interpretations of tabular data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing approaches in Table-to-Text generation mainly focus on objective descriptions, leaving the incorporation of subjectivity largely unexplored.

**Method:** The proposed three-stage pipeline includes extraction of RDF triples, aggregation of text into coherent narratives, and infusion of subjectivity to enhance generated text.

**Key Contributions:**

	1. Novel three-stage pipeline for Table-to-Text generation
	2. Incorporation of subjectivity in text generation
	3. Utilization of RDF triples for enhanced factual accuracy

**Result:** The pipeline utilizes smaller, fine-tuned T5 models and demonstrates comparable performance to GPT-3.5 while outperforming other models like Mistral-7B and Llama-2 in several metrics.

**Limitations:** 

**Conclusion:** This work is the first to propose a structured pipeline for T2T generation that enhances both factual correctness and subjectivity through intermediate representations.

**Abstract:** In Table-to-Text (T2T) generation, existing approaches predominantly focus on providing objective descriptions of tabular data. However, generating text that incorporates subjectivity, where subjectivity refers to interpretations beyond raw numerical data, remains underexplored. To address this, we introduce a novel pipeline that leverages intermediate representations to generate both objective and subjective text from tables. Our three-stage pipeline consists of: 1) extraction of Resource Description Framework (RDF) triples, 2) aggregation of text into coherent narratives, and 3) infusion of subjectivity to enrich the generated text. By incorporating RDFs, our approach enhances factual accuracy while maintaining interpretability. Unlike large language models (LLMs) such as GPT-3.5, Mistral-7B, and Llama-2, our pipeline employs smaller, fine-tuned T5 models while achieving comparable performance to GPT-3.5 and outperforming Mistral-7B and Llama-2 in several metrics. We evaluate our approach through quantitative and qualitative analyses, demonstrating its effectiveness in balancing factual accuracy with subjective interpretation. To the best of our knowledge, this is the first work to propose a structured pipeline for T2T generation that integrates intermediate representations to enhance both factual correctness and subjectivity.

</details>


### [56] [Basic Reading Distillation](https://arxiv.org/abs/2507.19741)

*Zhi Zhou, Sirui Miao, Xiangyu Duan, Hao Yang, Min Zhang*

**Main category:** cs.CL

**Keywords:** large language models, distillation, natural language processing, small models, basic reading behaviors

**Relevance Score:** 8

**TL;DR:** The paper introduces basic reading distillation (BRD), a method for training small models to imitate basic reading behaviors of large language models (LLMs) for improved performance on NLP tasks without the high resource requirements of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high computational resource demands of LLMs and improve the performance of smaller models in natural language processing tasks.

**Method:** Basic reading distillation (BRD) involves educating small models to replicate fundamental reading behaviors of LLMs, such as named entity recognition and question answering, on generic texts.

**Key Contributions:**

	1. Introduction of basic reading distillation (BRD) as a novel training method for small models
	2. Demonstrated that small models can achieve competitive performance against much larger LLMs
	3. Provided insights into the effective training strategies influencing small model capabilities.

**Result:** The small model trained with BRD outperformed or matched the performance of models over 20 times its size on various benchmarks, showing significant competitive advantages.

**Limitations:** 

**Conclusion:** BRD effectively alters the probability distribution of the small model, providing insight into model training and demonstrating orthogonality to existing distillation methods.

**Abstract:** Large language models (LLMs) have demonstrated remarkable abilities in various natural language processing areas, but they demand high computation resources which limits their deployment in real-world. Distillation is one technique to solve this problem through either knowledge distillation or task distillation. Both distillation approaches train small models to imitate specific features of LLMs, but they all neglect basic reading education for small models on generic texts that are \emph{unrelated} to downstream tasks. In this paper, we propose basic reading distillation (BRD) which educates a small model to imitate LLMs basic reading behaviors, such as named entity recognition, question raising and answering, on each sentence. After such basic education, we apply the small model on various tasks including language inference benchmarks and BIG-bench tasks. It shows that the small model can outperform or perform comparable to over 20x bigger LLMs. Analysis reveals that BRD effectively influences the probability distribution of the small model, and has orthogonality to either knowledge distillation or task distillation.

</details>


### [57] [JT-Math: A Multi-Stage Framework for Advanced Mathematical Reasoning in Large Language Models](https://arxiv.org/abs/2507.19748)

*Yifan Hao, Fangning Chao, Yaqian Hao, Zhaojun Cui, Huan Bai, Haiyu Zhang, Yankai Liu, Chao Deng, Junlan Feng*

**Main category:** cs.CL

**Keywords:** Mathematics, Large Language Models, Open-source, Reinforcement Learning, AI

**Relevance Score:** 8

**TL;DR:** JT-Math-8B introduces new open-source models designed for mathematical reasoning, showing high performance on complex problems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the limitations of current state-of-the-art LLMs in solving complex mathematical problems requiring deep understanding.

**Method:** The models consist of base, instruct, and thinking versions, developed through a multi-stage optimization framework and trained on a 210B-token dataset with a focus on quality and diversity.

**Key Contributions:**

	1. Introduction of JT-Math-8B as a series of open-source models for mathematical reasoning.
	2. Implementation of a Long Chain-of-Thought training approach for complex problem-solving.
	3. Utilization of a novel, multi-stage reinforcement learning curriculum.

**Result:** Achieves state-of-the-art results surpassing other models like OpenAI's O1-mini and GPT-4o on complex mathematical tasks.

**Limitations:** 

**Conclusion:** JT-Math-8B demonstrates superior performance in mathematical reasoning, paving the way for advancements in applied LLMs for educational and other cognitive tasks.

**Abstract:** Mathematical reasoning is a cornerstone of artificial general intelligence and a primary benchmark for evaluating the capabilities of Large Language Models (LLMs). While state-of-the-art models show promise, they often falter when faced with complex problems that demand deep conceptual understanding and intricate, multi-step deliberation. To address this challenge, we introduce JT-Math-8B, a series of open-source models comprising base, instruct, and thinking versions, built upon a systematic, multi-stage optimization framework. Our pre-training corpus is a high-quality, 210B-token dataset curated through a dedicated data pipeline that uses model-based validation to ensure quality and diversity. The Instruct Model is optimized for direct, concise answers through Supervised Fine-Tuning (SFT) and a GRPO-based reinforcement learning (RL) method. The Thinking Model is trained for complex problem-solving using a Long Chain-of-Thought (Long CoT) approach, combining SFT with a novel, multi-stage RL curriculum that progressively increases task difficulty and context length up to 32K tokens. JT-Math-8B achieves state-of-the-art results among open-source models of similar size, surpassing prominent models like OpenAI's O1-mini and GPT-4o , and demonstrating superior performance on competition-level mathematics.

</details>


### [58] [Are You There God? Lightweight Narrative Annotation of Christian Fiction with LMs](https://arxiv.org/abs/2507.19756)

*Rebecca M. M. Hicke, Brian Haggard, Mia Ferrante, Rayhan Khanna, David Mimno*

**Main category:** cs.CL

**Keywords:** Christian Fiction, Acts of God, Computational analysis, Left Behind series, Literary study

**Relevance Score:** 2

**TL;DR:** This paper explores Christian Fiction using computational tools to analyze divine acts, comparing the Left Behind series with other works in the genre.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of scholarly attention on Christian Fiction, particularly on how divine acts are depicted in the genre.

**Method:** The authors developed a codebook for 'acts of God' with human annotators and adapted it for use by a lightweight language model, which showed capability in matching human annotations.

**Key Contributions:**

	1. Developed a codebook for 'acts of God' in Christian Fiction
	2. Demonstrated the effectiveness of a lightweight language model in matching human annotations
	3. Highlighted significant differences in themes between the Left Behind series and other works.

**Result:** The analysis revealed significant differences in the depiction of divine acts between the Left Behind series and broader Christian Fiction, as well as between male and female authors.

**Limitations:** 

**Conclusion:** Computational tools can enhance the study of under-researched literary genres such as Christian Fiction, offering insights into thematic differences.

**Abstract:** In addition to its more widely studied political activities, the American Evangelical movement has a well-developed but less externally visible cultural and literary side. Christian Fiction, however, has been little studied, and what scholarly attention there is has focused on the explosively popular Left Behind series. In this work, we use computational tools to provide both a broad topical overview of Christian Fiction as a genre and a more directed exploration of how its authors depict divine acts. Working with human annotators we first developed definitions and a codebook for "acts of God." We then adapted those instructions designed for human annotators for use by a recent, lightweight LM with the assistance of a much larger model. The laptop-scale LM is capable of matching human annotations, even when the task is subtle and challenging. Using these annotations, we show that significant and meaningful differences exist between the Left Behind books and Christian Fiction more broadly and between books by male and female authors.

</details>


### [59] [UloRL:An Ultra-Long Output Reinforcement Learning Approach for Advancing Large Language Models' Reasoning Abilities](https://arxiv.org/abs/2507.19766)

*Dong Du, Shulin Liu, Tao Yang, Shaohua Chen, Yang Li*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Ultra-Long Outputs

**Relevance Score:** 9

**TL;DR:** This paper presents an Ultra-Long Output Reinforcement Learning (UloRL) approach to enhance LLM reasoning capabilities through efficient training of ultra-long outputs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning in large language models (LLMs) and address inefficiencies in traditional reinforcement learning when dealing with ultra-long output sequences.

**Method:** The proposed UloRL divides ultra-long output decoding into short segments, thereby reducing delays and introducing dynamic masking of well-Mastered Positive Tokens to avoid entropy collapse during training.

**Key Contributions:**

	1. Introduction of Ultra-Long Output Reinforcement Learning (UloRL) approach.
	2. Dynamic masking of well-Mastered Positive Tokens to prevent entropy collapse.
	3. Strategies for efficient training with ultra-long outputs.

**Result:** The UloRL approach resulted in a 2.06x increase in training speed and improved performance on AIME2025 from 70.9% to 85.1% and on BeyondAIME from 50.7% to 61.9%, surpassing existing models in effectiveness.

**Limitations:** 

**Conclusion:** The methods proposed present significant advancements in enhancing LLM reasoning capabilities with ultra-long sequence generation, and the authors will release their code and model for community use.

**Abstract:** Recent advances in large language models (LLMs) have highlighted the potential of reinforcement learning with verifiable rewards (RLVR) to enhance reasoning capabilities through extended output sequences. However, traditional RL frameworks face inefficiencies when handling ultra-long outputs due to long-tail sequence distributions and entropy collapse during training. To address these challenges, we propose an Ultra-Long Output Reinforcement Learning (UloRL) approach for advancing large language models' reasoning abilities. Specifically, we divide ultra long output decoding into short segments, enabling efficient training by mitigating delays caused by long-tail samples. Additionally, we introduce dynamic masking of well-Mastered Positive Tokens (MPTs) to prevent entropy collapse. Experimental results demonstrate the effectiveness of our approach. On the Qwen3-30B-A3B model, RL with segment rollout achieved 2.06x increase in training speed, while RL training with 128k-token outputs improves the model's performance on AIME2025 from 70.9\% to 85.1\% and on BeyondAIME from 50.7\% to 61.9\%, even surpassing Qwen3-235B-A22B with remarkable gains. These findings underscore the potential of our methods to advance the reasoning capabilities of LLMs with ultra-long sequence generation. We will release our code and model for further use by the community.

</details>


### [60] [Flora: Effortless Context Construction to Arbitrary Length and Scale](https://arxiv.org/abs/2507.19786)

*Tianxiang Chen, Zhentao Tan, Xiaofan Bo, Yue Wu, Tao Gong, Qi Chu, Jieping Ye, Nenghai Yu*

**Main category:** cs.CL

**Keywords:** Long-context, LLM, Instruction tuning, Machine learning, Natural language processing

**Relevance Score:** 8

**TL;DR:** Flora is a strategy for constructing long contexts for LLMs without human or LLM interventions, improving long-context performance while maintaining short-context abilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the challenges LLMs face with long contexts, including computational demands and the degradation of short-context capabilities.

**Method:** Flora assembles short instructions into long contexts through effective categorization and generates diverse responses based on long-context meta-instructions.

**Key Contributions:**

	1. Introduced Flora for long-context construction
	2. Improves long-context performance of LLMs
	3. Maintains short-context performance with diverse generation

**Result:** Experiments show that LLMs using Flora perform well on long-context benchmarks while retaining strong short-context performance.

**Limitations:** The approach may still have some trade-offs in the richness of short-context outputs despite overall performance benefits.

**Conclusion:** Flora presents a cost-effective and efficient method for enhancing LLM capabilities in handling long contexts without sacrificing short-context performance.

**Abstract:** Effectively handling long contexts is challenging for Large Language Models (LLMs) due to the rarity of long texts, high computational demands, and substantial forgetting of short-context abilities. Recent approaches have attempted to construct long contexts for instruction tuning, but these methods often require LLMs or human interventions, which are both costly and limited in length and diversity. Also, the drop in short-context performances of present long-context LLMs remains significant. In this paper, we introduce Flora, an effortless (human/LLM-free) long-context construction strategy. Flora can markedly enhance the long-context performance of LLMs by arbitrarily assembling short instructions based on categories and instructing LLMs to generate responses based on long-context meta-instructions. This enables Flora to produce contexts of arbitrary length and scale with rich diversity, while only slightly compromising short-context performance. Experiments on Llama3-8B-Instruct and QwQ-32B show that LLMs enhanced by Flora excel in three long-context benchmarks while maintaining strong performances in short-context tasks. Our data-construction code is available at \href{https://github.com/txchen-USTC/Flora}{https://github.com/txchen-USTC/Flora}.

</details>


### [61] [HCAttention: Extreme KV Cache Compression via Heterogeneous Attention Computing for LLMs](https://arxiv.org/abs/2507.19823)

*Dongquan Yang, Yifan Yang, Xiaotian Yu, Xianbiao Qi, Rong Xiao*

**Main category:** cs.CL

**Keywords:** large language models, KV cache, attention computation, memory efficiency, HCAttention

**Relevance Score:** 8

**TL;DR:** HCAttention is a framework for efficient long-context inference in large language models that enhances KV cache memory usage without model fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the memory challenges in processing long-context inputs in large language models, particularly with respect to KV cache compression methods that degrade significantly when memory is highly reduced.

**Method:** HCAttention employs key quantization, value offloading, and dynamic KV eviction for efficient inference under extreme memory constraints.

**Key Contributions:**

	1. Introduction of HCAttention framework for KV cache compression
	2. Enabling 4 million tokens processing on a single A100 GPU
	3. Setting a new state-of-the-art for LLM KV cache memory efficiency

**Result:** HCAttention achieves accurate performance while reducing the KV cache memory footprint to 25% of its original size, competing effectively at 12.5% cache size, setting a new state-of-the-art in LLM KV cache compression.

**Limitations:** 

**Conclusion:** HCAttention allows the Llama-3-8B model to handle 4 million tokens on a single A100 GPU, making it an efficient choice for long-context inputs without requiring model fine-tuning.

**Abstract:** Processing long-context inputs with large language models presents a significant challenge due to the enormous memory requirements of the Key-Value (KV) cache during inference. Existing KV cache compression methods exhibit noticeable performance degradation when memory is reduced by more than 85%. Additionally, strategies that leverage GPU-CPU collaboration for approximate attention remain underexplored in this setting. We propose HCAttention, a heterogeneous attention computation framework that integrates key quantization, value offloading, and dynamic KV eviction to enable efficient inference under extreme memory constraints. The method is compatible with existing transformer architectures and does not require model fine-tuning. Experimental results on the LongBench benchmark demonstrate that our approach preserves the accuracy of full-attention model while shrinking the KV cache memory footprint to 25% of its original size. Remarkably, it stays competitive with only 12.5% of the cache, setting a new state-of-the-art in LLM KV cache compression. To the best of our knowledge, HCAttention is the first to extend the Llama-3-8B model to process 4 million tokens on a single A100 GPU with 80GB memory.

</details>


### [62] [DRIVE: Disfluency-Rich Synthetic Dialog Data Generation Framework for Intelligent Vehicle Environments](https://arxiv.org/abs/2507.19867)

*Anshul Chavda, M Jagadeesh, Chintalapalli Raja Kullayappa, B Jayaprakash, Medchalimi Sruthi, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Conversational AI, Disfluencies, Dataset Generation, Autonomous Vehicles, Natural Language Processing

**Relevance Score:** 7

**TL;DR:** Introduction of DiscoDrive, a synthetic corpus for in-car conversational AI focusing on spontaneous disfluencies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of datasets capturing spontaneous disfluencies in driver-AI conversations, which are critical for developing realistic conversational AI in autonomous vehicles.

**Method:** DiscoDrive is generated using a two-stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis, resulting in a corpus of 3500 multi-turn dialogs across seven automotive domains.

**Key Contributions:**

	1. Introduction of DiscoDrive corpus for in-car dialogs
	2. Demonstrated effectiveness for training conversational AI
	3. Human evaluations show improved dialog quality

**Result:** DiscoDrive enabled models like DialoGPT-Medium and T5-Base to surpass KVRET-trained models in specific test sets, with notable BLEU-4, METEOR, ROUGE-L, and BERTScore F1 improvements. Human evaluations rated DiscoDrive dialogs higher in naturalness and coherence compared to KVRET.

**Limitations:** 

**Conclusion:** DiscoDrive is a vital resource for training and augmenting conversational AI, addressing disfluency in real-world, in-car interactions effectively.

**Abstract:** In-car conversational AI is becoming increasingly critical as autonomous vehicles and smart assistants gain widespread adoption. Yet, existing datasets fail to capture the spontaneous disfluencies such as hesitations, false starts, repetitions, and self-corrections that characterize real driver-AI dialogs. To address this, we introduce DiscoDrive, a synthetic corpus of 3500 multi-turn dialogs across seven automotive domains, generated using a two-stage, prompt-driven pipeline that dynamically integrates disfluencies during synthesis. We show that DiscoDrive is effective both as a training resource, enabling DialoGPT-Medium and T5-Base to match or exceed KVRET-trained models on the MultiWOZ 2.2 and Schema-Guided Dialogue (SGD) relevant test sets (BLEU-4 improvements of 0.26 to 0.61; METEOR +2.10; ROUGE-L +3.48; BERTScore F1 improvements of 1.35 to 3.48), and as a data augmentation resource in low-resource scenarios, delivering additional gains of up to BLEU-4 +0.38, METEOR +1.95, ROUGE-L +2.87, and BERTScore F1 +4.00 when combined with 10 percent of KVRET. Human evaluations further confirm that dialogs sampled from DiscoDrive are rated higher than KVRET's human-collected dialogs in naturalness (3.8 vs 3.6) and coherence (4.1 vs 4.0), and are perceived as more context-appropriate than leading post-hoc methods (such as LARD), without compromising clarity. DiscoDrive fills a critical gap in existing resources and serves as a versatile corpus for both training and augmenting conversational AI, enabling robust handling of real-world, disfluent in-car interactions.

</details>


### [63] [The Polish Vocabulary Size Test: A Novel Adaptive Test for Receptive Vocabulary Assessment](https://arxiv.org/abs/2507.19869)

*Danil Fokin, Monika Płużyczka, Grigory Golovin*

**Main category:** cs.CL

**Keywords:** vocabulary assessment, Polish language, adaptive testing

**Relevance Score:** 2

**TL;DR:** The Polish Vocabulary Size Test (PVST) is a new adaptive tool for measuring vocabulary size in Polish speakers, validated with a large pilot study.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for an accurate and efficient assessment tool for vocabulary size in Polish speakers.

**Method:** The PVST utilizes Item Response Theory and Computerized Adaptive Testing to adjust to the user's proficiency level.

**Key Contributions:**

	1. Introduction of the PVST as a dynamic assessment tool
	2. Validation through extensive pilot study
	3. Correlation findings linking vocabulary size and age for native speakers

**Result:** The pilot study involving 1,475 participants showed that native speakers have significantly larger vocabularies and that their vocabulary size correlates positively with age.

**Limitations:** 

**Conclusion:** The PVST provides a reliable measure of vocabulary size and is accessible online for users.

**Abstract:** We present the Polish Vocabulary Size Test (PVST), a novel tool for assessing the receptive vocabulary size of both native and non-native Polish speakers. Based on Item Response Theory and Computerized Adaptive Testing, PVST dynamically adjusts to each test-taker's proficiency level, ensuring high accuracy while keeping the test duration short. To validate the test, a pilot study was conducted with 1.475 participants. Native Polish speakers demonstrated significantly larger vocabularies compared to non-native speakers. For native speakers, vocabulary size showed a strong positive correlation with age. The PVST is available online at myvocab.info/pl.

</details>


### [64] [Zero-shot Performance of Generative AI in Brazilian Portuguese Medical Exam](https://arxiv.org/abs/2507.19885)

*Cesar Augusto Madid Truyts, Amanda Gomes Rabelo, Gabriel Mesquita de Souza, Daniel Scaldaferri Lages, Adriano Jose Pereira, Uri Adrian Prync Flato, Eduardo Pontes dos Reis, Joaquim Edson Vieira, Paulo Sergio Panse Silveira, Edson Amaro Junior*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multimodal Models, Health Informatics, Language Disparities, AI in Healthcare

**Relevance Score:** 9

**TL;DR:** This study evaluates the performance of various LLMs and MLLMs in answering medical questions in Brazilian Portuguese, revealing performance gaps compared to human candidates and highlighting the necessity for improvements in multilingual medical AI applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this study is to investigate the performance of LLMs and MLLMs in a non-English language, specifically Brazilian Portuguese, to identify biases and performance gaps that may affect their usability in healthcare.

**Method:** The study benchmarked six LLMs and four MLLMs against human candidates using questions from the medical residency entrance exam of a prestigious hospital in Brazil, measuring accuracy, processing time, and coherence of responses.

**Key Contributions:**

	1. Performance evaluation of LLMs and MLLMs in Brazilian Portuguese medical questions
	2. Identification of performance gaps in multimodal question response
	3. Recommendations for improved training methodologies and clinical integration of AI

**Result:** Some models, like Claude-3.5-Sonnet and Claude-3-Opus, demonstrated accuracy levels comparable to human candidates, however, notable gaps in performance were observed, especially in multimodal questions.

**Limitations:** The study's focus was limited to Brazilian Portuguese medical questions and may not fully represent performance across all languages or medical specializations.

**Conclusion:** The study emphasizes the critical need for fine-tuning and dataset augmentation to enhance the performance of AI systems in diverse linguistic contexts within healthcare applications.

**Abstract:** Artificial intelligence (AI) has shown the potential to revolutionize healthcare by improving diagnostic accuracy, optimizing workflows, and personalizing treatment plans. Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) have achieved notable advancements in natural language processing and medical applications. However, the evaluation of these models has focused predominantly on the English language, leading to potential biases in their performance across different languages.   This study investigates the capability of six LLMs (GPT-4.0 Turbo, LLaMA-3-8B, LLaMA-3-70B, Mixtral 8x7B Instruct, Titan Text G1-Express, and Command R+) and four MLLMs (Claude-3.5-Sonnet, Claude-3-Opus, Claude-3-Sonnet, and Claude-3-Haiku) to answer questions written in Brazilian spoken portuguese from the medical residency entrance exam of the Hospital das Cl\'inicas da Faculdade de Medicina da Universidade de S\~ao Paulo (HCFMUSP) - the largest health complex in South America. The performance of the models was benchmarked against human candidates, analyzing accuracy, processing time, and coherence of the generated explanations.   The results show that while some models, particularly Claude-3.5-Sonnet and Claude-3-Opus, achieved accuracy levels comparable to human candidates, performance gaps persist, particularly in multimodal questions requiring image interpretation. Furthermore, the study highlights language disparities, emphasizing the need for further fine-tuning and data set augmentation for non-English medical AI applications.   Our findings reinforce the importance of evaluating generative AI in various linguistic and clinical settings to ensure a fair and reliable deployment in healthcare. Future research should explore improved training methodologies, improved multimodal reasoning, and real-world clinical integration of AI-driven medical assistance.

</details>


### [65] [A Gold Standard Dataset and Evaluation Framework for Depression Detection and Explanation in Social Media using LLMs](https://arxiv.org/abs/2507.19899)

*Prajval Bolegave, Pushpak Bhattacharya*

**Main category:** cs.CL

**Keywords:** depression detection, social media, large language models, natural language explanations, mental health

**Relevance Score:** 9

**TL;DR:** This paper presents an expert-annotated dataset of social media posts for detecting depression and evaluates large language models' explanations on mental health tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve early detection of depression through social media analysis and enhance the quality of AI-generated explanations in mental health contexts.

**Method:** A high-quality dataset of 1,017 annotated social media posts is used to assess the performance of popular LLMs via zero-shot and few-shot prompting strategies.

**Key Contributions:**

	1. Development of a fine-grained dataset for depression detection
	2. Evaluation framework leveraging clinical data for LLMs
	3. Insights into the role of human expertise in AI explanation generation

**Result:** Different proprietary LLMs exhibit varying effectiveness in generating clinically relevant explanations, revealing gaps in AI comprehension of mental health nuances.

**Limitations:** The dataset may not encompass all forms of depression expression in social media, potentially limiting the model's generalizability.

**Conclusion:** Human expertise is crucial in refining LLM outputs for mental health applications, contributing to the development of safer AI systems.

**Abstract:** Early detection of depression from online social media posts holds promise for providing timely mental health interventions. In this work, we present a high-quality, expert-annotated dataset of 1,017 social media posts labeled with depressive spans and mapped to 12 depression symptom categories. Unlike prior datasets that primarily offer coarse post-level labels \cite{cohan-etal-2018-smhd}, our dataset enables fine-grained evaluation of both model predictions and generated explanations.   We develop an evaluation framework that leverages this clinically grounded dataset to assess the faithfulness and quality of natural language explanations generated by large language models (LLMs). Through carefully designed prompting strategies, including zero-shot and few-shot approaches with domain-adapted examples, we evaluate state-of-the-art proprietary LLMs including GPT-4.1, Gemini 2.5 Pro, and Claude 3.7 Sonnet.   Our comprehensive empirical analysis reveals significant differences in how these models perform on clinical explanation tasks, with zero-shot and few-shot prompting. Our findings underscore the value of human expertise in guiding LLM behavior and offer a step toward safer, more transparent AI systems for psychological well-being.

</details>


### [66] [CaliDrop: KV Cache Compression with Calibration](https://arxiv.org/abs/2507.19906)

*Yi Su, Quantong Qiu, Yuechi Zhou, Juntao Li, Qingrong Xia, Ping Li, Xinyu Duan, Zhefeng Wang, Min Zhang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Token Eviction, KV Cache, CaliDrop, Attention Patterns

**Relevance Score:** 8

**TL;DR:** CaliDrop enhances token eviction strategies for Key-Value caching in Large Language Models (LLMs) to reduce memory usage while retaining accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs generate outputs, they require significant computational resources, and the linear growth of KV cache memory usage becomes a bottleneck, especially for long-context tasks.

**Method:** CaliDrop improves token eviction by using speculative calibration of discarded tokens based on the observation of high similarity between queries at nearby positions.

**Key Contributions:**

	1. Introduction of CaliDrop for enhanced token eviction in LLMs
	2. Demonstrated significant accuracy retention of KV cache with high compression
	3. Detailed analysis of attention patterns leading to improved eviction strategies

**Result:** CaliDrop shows considerable accuracy improvements over traditional token eviction methods, even with high compression ratios.

**Limitations:** The effectiveness of CaliDrop may vary depending on the specific model architecture and task characteristics.

**Conclusion:** The proposed CaliDrop strategy effectively mitigates accuracy loss associated with token eviction in LLMs, making it a viable solution for enhancing KV cache efficiency.

**Abstract:** Large Language Models (LLMs) require substantial computational resources during generation. While the Key-Value (KV) cache significantly accelerates this process by storing attention intermediates, its memory footprint grows linearly with sequence length, batch size, and model size, creating a bottleneck in long-context scenarios. Various KV cache compression techniques, including token eviction, quantization, and low-rank projection, have been proposed to mitigate this bottleneck, often complementing each other. This paper focuses on enhancing token eviction strategies. Token eviction leverages the observation that the attention patterns are often sparse, allowing for the removal of less critical KV entries to save memory. However, this reduction usually comes at the cost of notable accuracy degradation, particularly under high compression ratios. To address this issue, we propose \textbf{CaliDrop}, a novel strategy that enhances token eviction through calibration. Our preliminary experiments show that queries at nearby positions exhibit high similarity. Building on this observation, CaliDrop performs speculative calibration on the discarded tokens to mitigate the accuracy loss caused by token eviction. Extensive experiments demonstrate that CaliDrop significantly improves the accuracy of existing token eviction methods.

</details>


### [67] [KLAAD: Refining Attention Mechanisms to Reduce Societal Bias in Generative Language Models](https://arxiv.org/abs/2507.19962)

*Seorin Kim, Dongyoung Lee, Jaejin Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Debiasing, Attention Alignment

**Relevance Score:** 9

**TL;DR:** KLAAD is a debiasing framework for large language models that aligns attention distributions to reduce societal biases without changing model weights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address ethical concerns regarding societal biases present in large language models' outputs.

**Method:** KLAAD uses a composite training objective that combines Cross-Entropy, KL divergence, and Triplet losses, aligning attention between biased and unbiased contexts.

**Key Contributions:**

	1. Introduces KLAAD for attention-based debiasing
	2. Combines multiple loss functions for effective training
	3. Demonstrates improved bias mitigation with minimal quality loss

**Result:** KLAAD showed improved bias mitigation on BBQ and BOLD benchmarks with minimal effect on language modeling quality.

**Limitations:** 

**Conclusion:** Attention-level alignment proves to be an effective approach for bias reduction in generative language models.

**Abstract:** Large language models (LLMs) often exhibit societal biases in their outputs, prompting ethical concerns regarding fairness and harm. In this work, we propose KLAAD (KL-Attention Alignment Debiasing), an attention-based debiasing framework that implicitly aligns attention distributions between stereotypical and anti-stereotypical sentence pairs without directly modifying model weights. KLAAD introduces a composite training objective combining Cross-Entropy, KL divergence, and Triplet losses, guiding the model to consistently attend across biased and unbiased contexts while preserving fluency and coherence. Experimental evaluation of KLAAD demonstrates improved bias mitigation on both the BBQ and BOLD benchmarks, with minimal impact on language modeling quality. The results indicate that attention-level alignment offers a principled solution for mitigating bias in generative language models.

</details>


### [68] [Text2Vis: A Challenging and Diverse Benchmark for Generating Multimodal Visualizations from Text](https://arxiv.org/abs/2507.19969)

*Mizanur Rahman, Md Tahmid Rahman Laskar, Shafiq Joty, Enamul Hoque*

**Main category:** cs.CL

**Keywords:** automated data visualization, text-to-visualization models, benchmarking, large language models, cross-modal learning

**Relevance Score:** 8

**TL;DR:** Introduction of Text2Vis, a benchmark for assessing text-to-visualization models, evaluating over 20 chart types and complex data science queries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a rigorous benchmark to evaluate the capabilities of large language models in automated data visualization due to the lack of comprehensive assessment tools.

**Method:** The study assesses 11 models using a curated dataset of 1,985 samples consisting of data tables, queries, answers, visualization code, and annotated charts. A cross-modal actor-critic framework refines text answers and visualization code.

**Key Contributions:**

	1. Introduction of the Text2Vis benchmark for text-to-visualization models.
	2. Development of a cross-modal actor-critic framework for improved model performance.
	3. Creation of an automated evaluation framework for scalable assessment of visualization capabilities.

**Result:** The benchmarking revealed significant performance gaps across the tested models, with an improvement in GPT-4o's pass rate from 26% to 42% using the proposed framework.

**Limitations:** Challenge of ensuring high-quality visual outputs and comprehensive benchmarking methods.

**Conclusion:** The work highlights challenges in text-to-visualization tasks and provides a scalable, automated evaluation framework for future advancements in data visualization capabilities of LLMs.

**Abstract:** Automated data visualization plays a crucial role in simplifying data interpretation, enhancing decision-making, and improving efficiency. While large language models (LLMs) have shown promise in generating visualizations from natural language, the absence of comprehensive benchmarks limits the rigorous evaluation of their capabilities. We introduce Text2Vis, a benchmark designed to assess text-to-visualization models, covering 20+ chart types and diverse data science queries, including trend analysis, correlation, outlier detection, and predictive analytics. It comprises 1,985 samples, each with a data table, natural language query, short answer, visualization code, and annotated charts. The queries involve complex reasoning, conversational turns, and dynamic data retrieval. We benchmark 11 open-source and closed-source models, revealing significant performance gaps, highlighting key challenges, and offering insights for future advancements. To close this gap, we propose the first cross-modal actor-critic agentic framework that jointly refines the textual answer and visualization code, increasing GPT-4o`s pass rate from 26% to 42% over the direct approach and improving chart quality. We also introduce an automated LLM-based evaluation framework that enables scalable assessment across thousands of samples without human annotation, measuring answer correctness, code execution success, visualization readability, and chart accuracy. We release Text2Vis at https://github.com/vis-nlp/Text2Vis.

</details>


### [69] [Exploring LLM Autoscoring Reliability in Large-Scale Writing Assessments Using Generalizability Theory](https://arxiv.org/abs/2507.19980)

*Dan Song, Won-Chan Lee, Hong Jiao*

**Main category:** cs.CL

**Keywords:** large language models, scoring assessment, hybrid models, generalizability theory, education

**Relevance Score:** 8

**TL;DR:** The study analyzes the reliability of large language models (LLMs) in scoring AP Chinese writing tasks compared to human raters, finding that LLMs can achieve consistency under specific conditions, particularly with story narration tasks.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the effectiveness of large language models in scoring writing tasks, and to explore hybrid scoring models that combine human and AI assessment for enhanced reliability in large-scale writing evaluations.

**Method:** The research employs generalizability theory to assess the score consistency between human and AI raters across different types of AP Chinese free-response writing tasks, using independent scoring from trained raters.

**Key Contributions:**

	1. Investigation of LLMs in educational assessment
	2. Empirical comparison of human and AI scoring reliability
	3. Advocacy for hybrid scoring models in large-scale evaluations

**Result:** Human raters yielded more reliable scores overall, but LLMs showed reasonable consistency under specific conditions, especially for story narration. The inclusion of AI scores in composite scoring improved overall reliability.

**Limitations:** Study focused solely on AP Chinese Language and Culture Exam; findings may not generalize to other contexts or languages.

**Conclusion:** Hybrid scoring models that integrate both human and AI raters demonstrate potential benefits for enhancing reliability in large-scale writing assessments.

**Abstract:** This study investigates the estimation of reliability for large language models (LLMs) in scoring writing tasks from the AP Chinese Language and Culture Exam. Using generalizability theory, the research evaluates and compares score consistency between human and AI raters across two types of AP Chinese free-response writing tasks: story narration and email response. These essays were independently scored by two trained human raters and seven AI raters. Each essay received four scores: one holistic score and three analytic scores corresponding to the domains of task completion, delivery, and language use. Results indicate that although human raters produced more reliable scores overall, LLMs demonstrated reasonable consistency under certain conditions, particularly for story narration tasks. Composite scoring that incorporates both human and AI raters improved reliability, which supports that hybrid scoring models may offer benefits for large-scale writing assessments.

</details>


### [70] [VLQA: The First Comprehensive, Large, and High-Quality Vietnamese Dataset for Legal Question Answering](https://arxiv.org/abs/2507.19995)

*Tan-Minh Nguyen, Hoang-Trung Nguyen, Trong-Khoi Dao, Xuan-Hieu Phan, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong*

**Main category:** cs.CL

**Keywords:** legal NLP, large language models, Vietnamese, dataset, legal information retrieval

**Relevance Score:** 8

**TL;DR:** This paper presents the VLQA dataset for legal text processing in Vietnamese and evaluates its effectiveness for legal information retrieval and question-answering tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) show potential in automating legal tasks, but challenges remain due to resource scarcity in low-resource languages like Vietnamese.

**Method:** The paper introduces the VLQA dataset, provides a statistical analysis of its quality, and evaluates its effectiveness with state-of-the-art models for legal NLP tasks.

**Key Contributions:**

	1. Introduction of the VLQA dataset for Vietnamese legal text processing
	2. Statistical analysis of dataset quality
	3. Evaluation of effectiveness with state-of-the-art models

**Result:** The evaluation demonstrates that the VLQA dataset significantly aids legal information retrieval and question-answering tasks in the Vietnamese context.

**Limitations:** 

**Conclusion:** Building robust legal NLP applications requires comprehensive datasets like VLQA, particularly for low-resource languages, to enhance legal automation efforts.

**Abstract:** The advent of large language models (LLMs) has led to significant achievements in various domains, including legal text processing. Leveraging LLMs for legal tasks is a natural evolution and an increasingly compelling choice. However, their capabilities are often portrayed as greater than they truly are. Despite the progress, we are still far from the ultimate goal of fully automating legal tasks using artificial intelligence (AI) and natural language processing (NLP). Moreover, legal systems are deeply domain-specific and exhibit substantial variation across different countries and languages. The need for building legal text processing applications for different natural languages is, therefore, large and urgent. However, there is a big challenge for legal NLP in low-resource languages such as Vietnamese due to the scarcity of resources and annotated data. The need for labeled legal corpora for supervised training, validation, and supervised fine-tuning is critical. In this paper, we introduce the VLQA dataset, a comprehensive and high-quality resource tailored for the Vietnamese legal domain. We also conduct a comprehensive statistical analysis of the dataset and evaluate its effectiveness through experiments with state-of-the-art models on legal information retrieval and question-answering tasks.

</details>


### [71] [Anomaly Detection in Human Language via Meta-Learning: A Few-Shot Approach](https://arxiv.org/abs/2507.20019)

*Saurav Singla, Aarav Singla, Advik Gupta, Parnika Gupta*

**Main category:** cs.CL

**Keywords:** anomaly detection, meta-learning, few-shot learning, language processing, HCI

**Relevance Score:** 8

**TL;DR:** This paper presents a meta-learning framework designed for detecting language anomalies using limited labeled data, addressing issues such as spam, fake news, and hate speech.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Anomalies in human language, like spam and hate speech, are challenging to detect due to their sparse and variable nature, especially when labeled data is limited.

**Method:** The authors frame anomaly detection as a few-shot binary classification problem and employ meta-learning techniques, including episodic training and prototypical networks, to enhance model generalization across diverse tasks.

**Key Contributions:**

	1. A meta-learning framework for few-shot text anomaly detection
	2. Combination of episodic training with prototypical networks for model adaptation
	3. Empirical validation of model generalization across diverse domains

**Result:** The proposed method shows superior performance compared to strong baselines, achieving higher F1 and AUC scores on benchmark datasets including SMS spam and COVID-19 fake news.

**Limitations:** 

**Conclusion:** The study demonstrates that the meta-learning approach effectively addresses the challenge of few-shot anomaly detection in language, providing code and benchmarks for further research.

**Abstract:** We propose a meta learning framework for detecting anomalies in human language across diverse domains with limited labeled data. Anomalies in language ranging from spam and fake news to hate speech pose a major challenge due to their sparsity and variability. We treat anomaly detection as a few shot binary classification problem and leverage meta-learning to train models that generalize across tasks. Using datasets from domains such as SMS spam, COVID-19 fake news, and hate speech, we evaluate model generalization on unseen tasks with minimal labeled anomalies. Our method combines episodic training with prototypical networks and domain resampling to adapt quickly to new anomaly detection tasks. Empirical results show that our method outperforms strong baselines in F1 and AUC scores. We also release the code and benchmarks to facilitate further research in few-shot text anomaly detection.

</details>


### [72] [FAEDKV: Infinite-Window Fourier Transform for Unbiased KV Cache Compression](https://arxiv.org/abs/2507.20030)

*Runchao Li, Yao Fu, Mu Sheng, Xianxuan Long, Haotian Yu, Pan Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, KV cache, compression, Fourier Transform, contextual information

**Relevance Score:** 9

**TL;DR:** FAEDKV is a new KV cache compression framework for LLMs that improves contextual information retention without training.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current KV cache compression methods that lead to biased representations and require costly retraining.

**Method:** FAEDKV uses the Infinite-Window Fourier Transform (IWDFT) to transform the KV cache into the frequency domain, aiding in unbiased information retention.

**Key Contributions:**

	1. Introduction of a training-free KV cache compression framework
	2. Utilization of Infinite-Window Fourier Transform for unbiased token contribution
	3. Demonstrated superior performance on LongBench benchmark and Needle-In-A-Haystack task

**Result:** FAEDKV outperforms existing KV cache compression methods by up to 22% in LongBench benchmark tests and shows better retrieval accuracy on position-agnostic tasks.

**Limitations:** 

**Conclusion:** The proposed framework effectively maintains contextual integrity while reducing memory and computational requirements in LLMs.

**Abstract:** The efficacy of Large Language Models (LLMs) in long-context tasks is often hampered by the substantial memory footprint and computational demands of the Key-Value (KV) cache. Current compression strategies, including token eviction and learned projections, frequently lead to biased representations -- either by overemphasizing recent/high-attention tokens or by repeatedly degrading information from earlier context -- and may require costly model retraining. We present FAEDKV (Frequency-Adaptive Infinite-Window for KV cache), a novel, training-free KV cache compression framework that ensures unbiased information retention. FAEDKV operates by transforming the KV cache into the frequency domain using a proposed Infinite-Window Fourier Transform (IWDFT). This approach allows for the equalized contribution of all tokens to the compressed representation, effectively preserving both early and recent contextual information. A preliminary frequency ablation study identifies critical spectral components for layer-wise, targeted compression. Experiments on LongBench benchmark demonstrate FAEDKV's superiority over existing methods by up to 22\%. In addition, our method shows superior, position-agnostic retrieval accuracy on the Needle-In-A-Haystack task compared to compression based approaches.

</details>


### [73] [Infogen: Generating Complex Statistical Infographics from Documents](https://arxiv.org/abs/2507.20046)

*Akash Ghosh, Aparna Garimella, Pritika Ramu, Sambaran Bandyopadhyay, Sriparna Saha*

**Main category:** cs.CL

**Keywords:** infographics, machine learning, large language models, data visualization, benchmark dataset

**Relevance Score:** 8

**TL;DR:** Introducing a new method for generating complex statistical infographics from text using LLMs, overcoming limitations in current AI approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of tools for creating complex infographics from text-heavy documents that require understanding of context and content.

**Method:** The authors propose Infogen, a two-stage framework that uses fine-tuned LLMs to first generate infographic metadata and then convert this metadata into infographic code.

**Key Contributions:**

	1. Introduction of a new framework (Infogen) for infographic generation from text documents.
	2. Creation of Infodat, the first benchmark dataset for text-to-infographic metadata generation.
	3. Demonstration of Infogen's superior performance over existing LLMs in generating complex statistical infographics.

**Result:** Infogen was evaluated on the Infodat dataset, showing state-of-the-art performance in generating statistical infographics compared to existing LLMs.

**Limitations:** No specific limitations mentioned in the abstract.

**Conclusion:** The proposed method significantly enhances the capability of LLMs in utilizing complex data for infographic creation, filling a crucial gap in the current landscape.

**Abstract:** Statistical infographics are powerful tools that simplify complex data into visually engaging and easy-to-understand formats. Despite advancements in AI, particularly with LLMs, existing efforts have been limited to generating simple charts, with no prior work addressing the creation of complex infographics from text-heavy documents that demand a deep understanding of the content. We address this gap by introducing the task of generating statistical infographics composed of multiple sub-charts (e.g., line, bar, pie) that are contextually accurate, insightful, and visually aligned. To achieve this, we define infographic metadata that includes its title and textual insights, along with sub-chart-specific details such as their corresponding data and alignment. We also present Infodat, the first benchmark dataset for text-to-infographic metadata generation, where each sample links a document to its metadata. We propose Infogen, a two-stage framework where fine-tuned LLMs first generate metadata, which is then converted into infographic code. Extensive evaluations on Infodat demonstrate that Infogen achieves state-of-the-art performance, outperforming both closed and open-source LLMs in text-to-statistical infographic generation.

</details>


### [74] [A Tensor-Based Compiler and a Runtime for Neuron-Level DNN Certifier Specifications](https://arxiv.org/abs/2507.20055)

*Avaljot Singh, Yamin Chandini Sarita, Aditya Mishra, Ishaan Goyal, Gagandeep Singh, Charith Mendis*

**Main category:** cs.CL

**Keywords:** DNN certification, Compiler framework, Tensor representation

**Relevance Score:** 5

**TL;DR:** This paper presents a compiler framework designed to automatically convert neuron-level specifications of DNN certifiers into tensor-based implementations, bridging the semantic gap between design and execution.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for trustworthy DNN systems and the challenges of developing certifiers that operate efficiently at different levels of abstraction.

**Method:** A compiler framework that utilizes a novel stack-based intermediate representation and shape analysis to translate specifications into optimized tensor computations.

**Key Contributions:**

	1. Development of a compiler framework for DNN certifiers
	2. Introduction of a novel stack-based intermediate representation
	3. Creation of g-BCSR, a new tensor representation format

**Result:** Demonstrated that the compiler can produce implementations that are performance-competitive with hand-optimized versions while allowing for easier development of new certifiers across various DNNs.

**Limitations:** 

**Conclusion:** The proposed framework significantly simplifies the creation of new DNN certifiers while maintaining high performance through effective tensor representation and optimization.

**Abstract:** The uninterpretability of DNNs has led to the adoption of abstract interpretation-based certification as a practical means to establish trust in real-world systems that rely on DNNs. However, the current landscape supports only a limited set of certifiers, and developing new ones or modifying existing ones for different applications remains difficult. This is because the mathematical design of certifiers is expressed at the neuron level, while their implementations are optimized and executed at the tensor level. This mismatch creates a semantic gap between design and implementation, making manual bridging both complex and expertise-intensive -- requiring deep knowledge in formal methods, high-performance computing, etc.   We propose a compiler framework that automatically translates neuron-level specifications of DNN certifiers into tensor-based, layer-level implementations. This is enabled by two key innovations: a novel stack-based intermediate representation (IR) and a shape analysis that infers the implicit tensor operations needed to simulate the neuron-level semantics. During lifting, the shape analysis creates tensors in the minimal shape required to perform the corresponding operations. The IR also enables domain-specific optimizations as rewrites. At runtime, the resulting tensor computations exhibit sparsity tied to the DNN architecture. This sparsity does not align well with existing formats. To address this, we introduce g-BCSR, a double-compression format that represents tensors as collections of blocks of varying sizes, each possibly internally sparse.   Using our compiler and g-BCSR, we make it easy to develop new certifiers and analyze their utility across diverse DNNs. Despite its flexibility, the compiler achieves performance comparable to hand-optimized implementations.

</details>


### [75] [RAG in the Wild: On the (In)effectiveness of LLMs with Mixture-of-Knowledge Retrieval Augmentation](https://arxiv.org/abs/2507.20059)

*Ran Xu, Yuchen Zhuang, Yue Yu, Haoyu Wang, Wenqi Shi, Carl Yang*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, machine learning, information retrieval, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper evaluates retrieval-augmented generation (RAG) systems using a diverse dataset and identifies limitations in their performance and query routing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of RAG systems in realistic retrieval scenarios, moving beyond general-domain benchmarks.

**Method:** The study involved evaluating RAG systems with a large-scale datastore (MassiveDS) that includes diverse knowledge sources and analyzing the performance of these systems.

**Key Contributions:**

	1. Evaluation of RAG performance with a diverse datastore
	2. Identification of critical limitations in current RAG systems
	3. Highlighting the necessity for adaptive retrieval strategies

**Result:** Critical limitations were found: retrieval benefits smaller models, rerankers add minimal value, and no single retrieval source consistently performs well.

**Limitations:** Focus on a specific test set may limit generalizability; ongoing work to improve strategies.

**Conclusion:** Adaptive retrieval strategies are necessary for effective deployment of RAG in real-world applications.

**Abstract:** Retrieval-augmented generation (RAG) enhances large language models (LLMs) by integrating external knowledge retrieved at inference time. While RAG demonstrates strong performance on benchmarks largely derived from general-domain corpora like Wikipedia, its effectiveness under realistic, diverse retrieval scenarios remains underexplored. We evaluated RAG systems using MassiveDS, a large-scale datastore with mixture of knowledge, and identified critical limitations: retrieval mainly benefits smaller models, rerankers add minimal value, and no single retrieval source consistently excels. Moreover, current LLMs struggle to route queries across heterogeneous knowledge sources. These findings highlight the need for adaptive retrieval strategies before deploying RAG in real-world settings. Our code and data can be found at https://github.com/ritaranx/RAG_in_the_Wild.

</details>


### [76] [ProsodyLM: Uncovering the Emerging Prosody Processing Capabilities in Speech Language Models](https://arxiv.org/abs/2507.20091)

*Kaizhi Qian, Xulin Fan, Junrui Ni, Slava Shechtman, Mark Hasegawa-Johnson, Chuang Gan, Yang Zhang*

**Main category:** cs.CL

**Keywords:** speech language models, prosody, tokenization, LLMs, speech processing

**Relevance Score:** 8

**TL;DR:** ProsodyLM proposes a new tokenization scheme for speech that improves prosody understanding in language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the capability of speech language models in processing and understanding prosody, which is not effectively captured by the existing mainstream training methods.

**Method:** Introduces a novel tokenization scheme that transcribes speech into text and includes word-level prosody tokens, aiming to retain complete prosody information.

**Key Contributions:**

	1. Introduction of a new tokenization scheme for speech processing
	2. Improved retention of prosody information in LLMs
	3. Enhanced prosody processing capabilities observed during pre-training

**Result:** ProsodyLM demonstrates significant improvements in learning diverse prosody processing capabilities during pre-training, successfully capturing nuances in generated speech, such as focus, emotion, and maintaining prosody consistency across long contexts.

**Limitations:** 

**Conclusion:** By adopting the new tokenization approach, ProsodyLM outperforms traditional methods in understanding and generating speech with correct prosodic elements.

**Abstract:** Speech language models refer to language models with speech processing and understanding capabilities. One key desirable capability for speech language models is the ability to capture the intricate interdependency between content and prosody. The existing mainstream paradigm of training speech language models, which converts speech into discrete tokens before feeding them into LLMs, is sub-optimal in learning prosody information -- we find that the resulting LLMs do not exhibit obvious emerging prosody processing capabilities via pre-training alone. To overcome this, we propose ProsodyLM, which introduces a simple tokenization scheme amenable to learning prosody. Each speech utterance is first transcribed into text, followed by a sequence of word-level prosody tokens. Compared with conventional speech tokenization schemes, the proposed tokenization scheme retains more complete prosody information, and is more understandable to text-based LLMs. We find that ProsodyLM can learn surprisingly diverse emerging prosody processing capabilities through pre-training alone, ranging from harnessing the prosody nuances in generated speech, such as contrastive focus, understanding emotion and stress in an utterance, to maintaining prosody consistency in long contexts.

</details>


### [77] [AI-Driven Generation of Old English: A Framework for Low-Resource Languages](https://arxiv.org/abs/2507.20111)

*Rodrigo Gabriel Salazar Alva, Matías Nuñez, Cristian López, Javier Martín Arista*

**Main category:** cs.CL

**Keywords:** Old English, Natural Language Processing, Language Preservation, Large Language Models, Cultural Heritage

**Relevance Score:** 8

**TL;DR:** A framework using LLMs for generating high-quality Old English texts, enhancing accessibility and preservation of endangered languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical under-resourcing of Old English, hindering its accessibility to NLP techniques and cultural understanding.

**Method:** The framework employs Low-Rank Adaptation (LoRA) for efficient fine-tuning, combines data augmentation through backtranslation, and utilizes a dual-agent pipeline for content generation and translation tasks.

**Key Contributions:**

	1. Development of a scalable framework for Old English text generation
	2. Combination of parameter-efficient fine-tuning with data augmentation
	3. Dual-agent pipeline for enhanced translation efficiency

**Result:** Significant improvements in translation quality, evidenced by an increase in BLEU scores from 26 to over 65 and confirmed high grammatical accuracy and stylistic fidelity by expert assessment.

**Limitations:** 

**Conclusion:** The method not only expands the Old English corpus but also serves as a model for revitalizing other endangered languages through AI.

**Abstract:** Preserving ancient languages is essential for understanding humanity's cultural and linguistic heritage, yet Old English remains critically under-resourced, limiting its accessibility to modern natural language processing (NLP) techniques. We present a scalable framework that uses advanced large language models (LLMs) to generate high-quality Old English texts, addressing this gap. Our approach combines parameter-efficient fine-tuning (Low-Rank Adaptation, LoRA), data augmentation via backtranslation, and a dual-agent pipeline that separates the tasks of content generation (in English) and translation (into Old English). Evaluation with automated metrics (BLEU, METEOR, and CHRF) shows significant improvements over baseline models, with BLEU scores increasing from 26 to over 65 for English-to-Old English translation. Expert human assessment also confirms high grammatical accuracy and stylistic fidelity in the generated texts. Beyond expanding the Old English corpus, our method offers a practical blueprint for revitalizing other endangered languages, effectively uniting AI innovation with the goals of cultural preservation.

</details>


### [78] [Sem-DPO: Mitigating Semantic Inconsistency in Preference Optimization for Prompt Engineering](https://arxiv.org/abs/2507.20133)

*Anas Mohamed, Azal Ahmad Khan, Xinran Wang, Ahmad Faraz Khan, Shuwen Ge, Saman Bahzad Khan, Ayaan Ahmad, Ali Anwar*

**Main category:** cs.CL

**Keywords:** Generative AI, Prompt Engineering, Semantics, Optimization, Language Models

**Relevance Score:** 8

**TL;DR:** Sem-DPO improves semantic consistency in automatic prompt engineering for generative AI by scaling the DPO loss with respect to cosine distance, leading to better prompt optimization outcomes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Generative AI's ability to create realistic images from text is contingent on the phrasing of prompts; existing methods like Direct Preference Optimization (DPO) often fail to maintain the semantic consistency of prompts.

**Method:** Sem-DPO employs a modified DPO loss that uses exponential weighting based on cosine distance in embedding space to discourage semantically mismatched prompts, while still optimizing for preference scores.

**Key Contributions:**

	1. Introduction of Sem-DPO for prompt optimization
	2. Analytical bound on semantic drift for preference-tuned prompts
	3. Empirical evidence of performance improvements over traditional DPO

**Result:** Sem-DPO provides 8-12% higher CLIP similarity and 5-9% higher human-preference scores compared to DPO and outperforms state-of-the-art baselines on multiple benchmarks.

**Limitations:** 

**Conclusion:** The findings advocate for the incorporation of semantic weighting in prompt optimization, establishing a framework for semantic-aware preference optimization in language models.

**Abstract:** Generative AI can now synthesize strikingly realistic images from text, yet output quality remains highly sensitive to how prompts are phrased. Direct Preference Optimization (DPO) offers a lightweight, off-policy alternative to RL for automatic prompt engineering, but its token-level regularization leaves semantic inconsistency unchecked as prompts that win higher preference scores can still drift away from the user's intended meaning.   We introduce Sem-DPO, a variant of DPO that preserves semantic consistency yet retains its simplicity and efficiency. Sem-DPO scales the DPO loss by an exponential weight proportional to the cosine distance between the original prompt and winning candidate in embedding space, softly down-weighting training signals that would otherwise reward semantically mismatched prompts. We provide the first analytical bound on semantic drift for preference-tuned prompt generators, showing that Sem-DPO keeps learned prompts within a provably bounded neighborhood of the original text. On three standard text-to-image prompt-optimization benchmarks and two language models, Sem-DPO achieves 8-12% higher CLIP similarity and 5-9% higher human-preference scores (HPSv2.1, PickScore) than DPO, while also outperforming state-of-the-art baselines. These findings suggest that strong flat baselines augmented with semantic weighting should become the new standard for prompt-optimization studies and lay the groundwork for broader, semantics-aware preference optimization in language models.

</details>


### [79] [Multi-Stage Verification-Centric Framework for Mitigating Hallucination in Multi-Modal RAG](https://arxiv.org/abs/2507.20136)

*Baiyu Chen, Wilson Wongso, Xiaoqian Hu, Yue Tan, Flora Salim*

**Main category:** cs.CL

**Keywords:** Vision Language Models, factual accuracy, multi-modal systems

**Relevance Score:** 8

**TL;DR:** This paper presents a robust framework developed for the KDD Cup 2025 challenge aimed at improving factual accuracy in Vision Language Models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of hallucination in Vision Language Models, particularly when processing egocentric imagery and complex queries that require high factual accuracy.

**Method:** The solution is a multi-stage framework that includes a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, dual-pathways generation, and post-hoc verification.

**Key Contributions:**

	1. Development of a multi-stage framework to enhance factual accuracy in VLMs
	2. Integration of efficient query processing methods
	3. Demonstrated effectiveness through KDD Cup performance

**Result:** The approach was effective in minimizing hallucinations, leading to a 3rd place finish in Task 1 of the competition, indicating the framework's reliability for complex multi-modal tasks.

**Limitations:** 

**Conclusion:** The findings support the importance of prioritizing answer reliability in multi-modal RAG systems, especially in health informatics and user-facing applications.

**Abstract:** This paper presents the technical solution developed by team CRUISE for the KDD Cup 2025 Meta Comprehensive RAG Benchmark for Multi-modal, Multi-turn (CRAG-MM) challenge. The challenge aims to address a critical limitation of modern Vision Language Models (VLMs): their propensity to hallucinate, especially when faced with egocentric imagery, long-tail entities, and complex, multi-hop questions. This issue is particularly problematic in real-world applications where users pose fact-seeking queries that demand high factual accuracy across diverse modalities. To tackle this, we propose a robust, multi-stage framework that prioritizes factual accuracy and truthfulness over completeness. Our solution integrates a lightweight query router for efficiency, a query-aware retrieval and summarization pipeline, a dual-pathways generation and a post-hoc verification. This conservative strategy is designed to minimize hallucinations, which incur a severe penalty in the competition's scoring metric. Our approach achieved 3rd place in Task 1, demonstrating the effectiveness of prioritizing answer reliability in complex multi-modal RAG systems. Our implementation is available at https://github.com/Breezelled/KDD-Cup-2025-Meta-CRAG-MM .

</details>


### [80] [Multi-Agent Interactive Question Generation Framework for Long Document Understanding](https://arxiv.org/abs/2507.20145)

*Kesen Wang, Daulet Toibazar, Abdulrahman Alfulayt, Abdulaziz S. Albadawi, Ranya A. Alkahtani, Asma A. Ibrahim, Haneen A. Alhomoud, Sherif Mohamed, Pedro J. Moreno*

**Main category:** cs.CL

**Keywords:** Document Understanding, Long-context scenarios, Vision-Language Models, Q&A generation, Arabic documents

**Relevance Score:** 8

**TL;DR:** This paper presents a fully automated framework for generating long-context questions for documents, addressing the challenge of Document Understanding in low-resource languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve Document Understanding in long-context scenarios, particularly for low-resource languages like Arabic, where fine-grained training data is scarce.

**Method:** The authors propose a multi-agent interactive framework that generates single- and multi-page questions efficiently for documents in English and Arabic.

**Key Contributions:**

	1. Development of a fully automated question generation framework for long-context documents.
	2. Introduction of the AraEngLongBench dataset, specifically for English and Arabic.
	3. Demonstration of the challenging nature of generated questions for existing LVLMs.

**Result:** Experimental results indicate that the generated questions (AraEngLongBench) pose significant challenges to current state-of-the-art LVLMs, highlighting the effectiveness of the proposed framework.

**Limitations:** The framework relies on existing LVLMs, which may have limitations in understanding complex layouts.

**Conclusion:** The proposed method can facilitate improved long-context understanding in Large Vision-Language Models, with accessible code and data for further research.

**Abstract:** Document Understanding (DU) in long-contextual scenarios with complex layouts remains a significant challenge in vision-language research. Although Large Vision-Language Models (LVLMs) excel at short-context DU tasks, their performance declines in long-context settings. A key limitation is the scarcity of fine-grained training data, particularly for low-resource languages such as Arabic. Existing state-of-the-art techniques rely heavily on human annotation, which is costly and inefficient. We propose a fully automated, multi-agent interactive framework to generate long-context questions efficiently. Our approach efficiently generates high-quality single- and multi-page questions for extensive English and Arabic documents, covering hundreds of pages across diverse domains. This facilitates the development of LVLMs with enhanced long-context understanding ability. Experimental results in this work have shown that our generated English and Arabic questions (\textbf{AraEngLongBench}) are quite challenging to major open- and close-source LVLMs. The code and data proposed in this work can be found in https://github.com/wangk0b/Multi_Agentic_QA_Long_Doc.git. Sample Question and Answer (QA) pairs and structured system prompts can be found in the Appendix.

</details>


### [81] [Goal Alignment in LLM-Based User Simulators for Conversational AI](https://arxiv.org/abs/2507.20152)

*Shuhaib Mehri, Xiaocheng Yang, Takyoung Kim, Gokhan Tur, Shikib Mehri, Dilek Hakkani-Tür*

**Main category:** cs.CL

**Keywords:** user simulators, conversational AI, goal alignment, UGST, Large Language Models

**Relevance Score:** 8

**TL;DR:** This paper introduces User Goal State Tracking (UGST), a framework for improving user simulators in conversational AI by tracking goal progression and generating goal-aligned responses.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** User simulators are crucial for developing and evaluating conversational AI but currently lack consistent goal-oriented behavior, impacting their reliability.

**Method:** The paper presents a three-stage methodology leveraging UGST to autonomously track user goal progression in conversations and generate goal-aligned responses.

**Key Contributions:**

	1. Introduction of the UGST framework for tracking user goals
	2. Development of a three-stage methodology for autonomous user simulators
	3. Establishment of comprehensive evaluation metrics for user simulator goal alignment

**Result:** The proposed approach demonstrates significant improvements in goal alignment across benchmarks MultiWOZ 2.4 and {	au}-Bench.

**Limitations:** 

**Conclusion:** UGST addresses a critical gap in conversational AI, enhancing the development of goal-aligned user simulators and establishing a new standard for evaluation.

**Abstract:** User simulators are essential to conversational AI, enabling scalable agent development and evaluation through simulated interactions. While current Large Language Models (LLMs) have advanced user simulation capabilities, we reveal that they struggle to consistently demonstrate goal-oriented behavior across multi-turn conversations--a critical limitation that compromises their reliability in downstream applications. We introduce User Goal State Tracking (UGST), a novel framework that tracks user goal progression throughout conversations. Leveraging UGST, we present a three-stage methodology for developing user simulators that can autonomously track goal progression and reason to generate goal-aligned responses. Moreover, we establish comprehensive evaluation metrics for measuring goal alignment in user simulators, and demonstrate that our approach yields substantial improvements across two benchmarks (MultiWOZ 2.4 and {\tau}-Bench). Our contributions address a critical gap in conversational AI and establish UGST as an essential framework for developing goal-aligned user simulators.

</details>


### [82] [SGPO: Self-Generated Preference Optimization based on Self-Improver](https://arxiv.org/abs/2507.20181)

*Hyeonji Lee, Daejin Jo, Seohwan Yun, Sungwoong Kim*

**Main category:** cs.CL

**Keywords:** large language models, alignment methods, self-improving mechanism

**Relevance Score:** 8

**TL;DR:** The paper presents Self-Generated Preference Optimization (SGPO), a new alignment framework for large language models that creates preference data through self-improvement without external datasets.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations in conventional alignment methods of large language models which rely on off-policy learning and human-annotated data, introducing issues of distribution shifts during training.

**Method:** SGPO employs an on-policy self-improving mechanism that refines the responses of a policy model to self-generate preference data for direct preference optimization.

**Key Contributions:**

	1. Introduction of Self-Generated Preference Optimization (SGPO) framework.
	2. Unification of improver and policy into a single model for enhanced preference data generation.
	3. Demonstration of improved performance metrics without external preference data.

**Result:** Experimental results demonstrate that SGPO significantly enhances performance on AlpacaEval 2.0 and Arena-Hard compared to direct preference optimization (DPO) and other baseline self-improving methods.

**Limitations:** 

**Conclusion:** The proposed SGPO framework effectively optimizes large language model responses by generating preference data internally, leading to improved performance without the need for external datasets.

**Abstract:** Large language models (LLMs), despite their extensive pretraining on diverse datasets, require effective alignment to human preferences for practical and reliable deployment. Conventional alignment methods typically employ off-policy learning and depend on human-annotated datasets, which limits their broad applicability and introduces distribution shift issues during training. To address these challenges, we propose Self-Generated Preference Optimization based on Self-Improver (SGPO), an innovative alignment framework that leverages an on-policy self-improving mechanism. Specifically, the improver refines responses from a policy model to self-generate preference data for direct preference optimization (DPO) of the policy model. Here, the improver and policy are unified into a single model, and in order to generate higher-quality preference data, this self-improver learns to make incremental yet discernible improvements to the current responses by referencing supervised fine-tuning outputs. Experimental results on AlpacaEval 2.0 and Arena-Hard show that the proposed SGPO significantly improves performance over DPO and baseline self-improving methods without using external preference data.

</details>


### [83] [SessionIntentBench: A Multi-task Inter-session Intention-shift Modeling Benchmark for E-commerce Customer Behavior Understanding](https://arxiv.org/abs/2507.20185)

*Yuqi Yang, Weiqi Wang, Baixuan Xu, Wei Fan, Qing Zong, Chunkit Chan, Zheye Deng, Xin Liu, Yifan Gao, Changlong Yu, Chen Luo, Yang Li, Zheng Li, Qingyu Yin, Bing Yin, Yangqiu Song*

**Main category:** cs.CL

**Keywords:** E-commerce, Customer intention, Session data, Multimodal benchmark, L(V)LM

**Relevance Score:** 8

**TL;DR:** The paper introduces SessionIntentBench, a benchmark for understanding user intention in E-commerce sessions, highlighting the inadequacy of current L(V)LMs in capturing intention across browsing sessions and demonstrating improvements through intentional data injection.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the understanding and modeling of customer intention in E-commerce product purchase sessions, which traditional approaches have inadequately addressed.

**Method:** The authors introduce the concept of an intention tree and a dataset curation pipeline, creating SessionIntentBench—a multimodal benchmark for evaluating L(V)LMs' capability on intention shifts with extensive session data.

**Key Contributions:**

	1. Introduction of the intention tree concept
	2. Creation of the SessionIntentBench benchmark
	3. Demonstration of L(V)LMs' performance improvement through intention data injection

**Result:** The constructed dataset includes 1,952,177 intention entries and shows that L(V)LMs fail to effectively model user intention, which can be enhanced by injecting intention data.

**Limitations:** The current benchmark is based on user session data, which may not encompass all possible user behaviors and intentions in E-commerce settings.

**Conclusion:** The study emphasizes the need for improved intention modeling in E-commerce and suggests that current L(V)LMs are insufficient, but can benefit from intention injection.

**Abstract:** Session history is a common way of recording user interacting behaviors throughout a browsing activity with multiple products. For example, if an user clicks a product webpage and then leaves, it might because there are certain features that don't satisfy the user, which serve as an important indicator of on-the-spot user preferences. However, all prior works fail to capture and model customer intention effectively because insufficient information exploitation and only apparent information like descriptions and titles are used. There is also a lack of data and corresponding benchmark for explicitly modeling intention in E-commerce product purchase sessions. To address these issues, we introduce the concept of an intention tree and propose a dataset curation pipeline. Together, we construct a sibling multimodal benchmark, SessionIntentBench, that evaluates L(V)LMs' capability on understanding inter-session intention shift with four subtasks. With 1,952,177 intention entries, 1,132,145 session intention trajectories, and 13,003,664 available tasks mined using 10,905 sessions, we provide a scalable way to exploit the existing session data for customer intention understanding. We conduct human annotations to collect ground-truth label for a subset of collected data to form an evaluation gold set. Extensive experiments on the annotated data further confirm that current L(V)LMs fail to capture and utilize the intention across the complex session setting. Further analysis show injecting intention enhances LLMs' performances.

</details>


### [84] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)

*Khloud AL Jallad, Nada Ghneim, Ghaida Rebdawi*

**Main category:** cs.CL

**Keywords:** Natural Language Understanding, NLP, Evaluation Metrics, Diagnostics Datasets, Linguistic Phenomena

**Relevance Score:** 7

**TL;DR:** This survey reviews various NLU benchmarks and their diagnostic datasets, emphasizing their strengths and weaknesses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The evaluation of NLU capabilities has become increasingly important, necessitating a comprehensive overview of existing benchmarks and their ability to facilitate error analysis.

**Method:** The paper provides a detailed comparison and analysis of available English, Arabic, and Multilingual NLU benchmarks, focusing on diagnostics datasets and linguistic phenomena.

**Key Contributions:**

	1. Comprehensive review of NLU benchmarks across multiple languages.
	2. Identification of strengths and limitations in existing diagnostics datasets.
	3. Formulation of a research question regarding the need for standardized evaluation metrics.

**Result:** Identifies gaps in current benchmarks, such as the lack of naming conventions and standard evaluation metrics for diagnostics benchmarks.

**Limitations:** Lacks a unified naming convention and standard set of linguistic phenomena for evaluations.

**Conclusion:** A standard for the evaluation metrics of NLU diagnostics benchmarks is proposed to enhance comparisons and insights in NLU research.

**Abstract:** Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.

</details>


### [85] [Diversity-Enhanced Reasoning for Subjective Questions](https://arxiv.org/abs/2507.20187)

*Yumeng Wang, Zhiyuan Fan, Jiayu Liu, Yi R. Fung*

**Main category:** cs.CL

**Keywords:** Large reasoning models, Diversity in reasoning, Reinforcement learning

**Relevance Score:** 7

**TL;DR:** MultiRole-R1 is a framework designed to enhance subjective reasoning in large reasoning models by incorporating multiple role perspectives and using diversity as a reward signal during reinforcement learning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large reasoning models on subjective questions, which are prone to homogeneous reasoning due to reliance on single ground truths and verifiable rewards.

**Method:** The approach includes an unsupervised data construction pipeline for generating reasoning chains with diverse role perspectives, paired with reinforcement learning using Group Relative Policy Optimization that incorporates diversity as a reward alongside traditional rewards.

**Key Contributions:**

	1. Introduction of MultiRole-R1 framework
	2. Use of multiple role perspectives for reasoning
	3. Combination of diversity as a reward signal in reinforcement learning

**Result:** Experiments on six benchmarks demonstrate that MultiRole-R1 improves both subjective and objective reasoning by promoting perspective and lexical diversity, leading to enhanced accuracy.

**Limitations:** 

**Conclusion:** The research shows that diversity-enhanced training in large reasoning models can significantly improve their performance on subjective reasoning tasks.

**Abstract:** Large reasoning models (LRM) with long chain-of-thought (CoT) capabilities have shown strong performance on objective tasks, such as math reasoning and coding. However, their effectiveness on subjective questions that may have different responses from different perspectives is still limited by a tendency towards homogeneous reasoning, introduced by the reliance on a single ground truth in supervised fine-tuning and verifiable reward in reinforcement learning. Motivated by the finding that increasing role perspectives consistently improves performance, we propose MultiRole-R1, a diversity-enhanced framework with multiple role perspectives, to improve the accuracy and diversity in subjective reasoning tasks. MultiRole-R1 features an unsupervised data construction pipeline that generates reasoning chains that incorporate diverse role perspectives. We further employ reinforcement learning via Group Relative Policy Optimization (GRPO) with reward shaping, by taking diversity as a reward signal in addition to the verifiable reward. With specially designed reward functions, we successfully promote perspective diversity and lexical diversity, uncovering a positive relation between reasoning diversity and accuracy. Our experiment on six benchmarks demonstrates MultiRole-R1's effectiveness and generalizability in enhancing both subjective and objective reasoning, showcasing the potential of diversity-enhanced training in LRMs.

</details>


### [86] [IQ Test for LLMs: An Evaluation Framework for Uncovering Core Skills in LLMs](https://arxiv.org/abs/2507.20208)

*Aviya Maimon, Amir DN Cohen, Gal Vishne, Shauli Ravfogel, Reut Tsarfaty*

**Main category:** cs.CL

**Keywords:** large language models, evaluation, factor analysis, performance benchmarking, latent skills

**Relevance Score:** 8

**TL;DR:** The paper introduces a new evaluation framework for large language models (LLMs) using factor analysis to uncover underlying skills driving performance on multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current approaches to evaluating LLMs through singular benchmark scores are insufficient for understanding their comprehensive abilities and limitations.

**Method:** The authors utilize factor analysis to analyze performance data from 60 LLMs across 44 tasks, identifying latent skills responsible for their performance.

**Key Contributions:**

	1. Proposes a new evaluation paradigm for LLMs using factor analysis
	2. Identifies latent skills that explain performance across multiple benchmarks
	3. Provides practical tools for model selection and task redundancy identification

**Result:** The analysis reveals a small set of latent skills that explain a significant portion of the performance variance across tasks.

**Limitations:** The study is dependent on the quality and scope of the tasks included in the evaluation.

**Conclusion:** The insights gained provide tools for identifying redundant tasks, assist in selecting appropriate models, and allow for profiling models based on their latent skills.

**Abstract:** Current evaluations of large language models (LLMs) rely on benchmark scores, but it is difficult to interpret what these individual scores reveal about a model's overall skills. Specifically, as a community we lack understanding of how tasks relate to one another, what they measure in common, how they differ, or which ones are redundant. As a result, models are often assessed via a single score averaged across benchmarks, an approach that fails to capture the models' wholistic strengths and limitations. Here, we propose a new evaluation paradigm that uses factor analysis to identify latent skills driving performance across benchmarks. We apply this method to a comprehensive new leaderboard showcasing the performance of 60 LLMs on 44 tasks, and identify a small set of latent skills that largely explain performance. Finally, we turn these insights into practical tools that identify redundant tasks, aid in model selection, and profile models along each latent skill.

</details>


### [87] [Co-NAML-LSTUR: A Combined Model with Attentive Multi-View Learning and Long- and Short-term User Representations for News Recommendation](https://arxiv.org/abs/2507.20210)

*Minh Hoang Nguyen, Thuat Thien Nguyen, Minh Nhat Ta*

**Main category:** cs.CL

**Keywords:** news recommendation, personalization, multi-view representation, user modeling, BERT

**Relevance Score:** 4

**TL;DR:** Co-NAML-LSTUR proposes a hybrid framework for news recommendation that addresses multi-view news representations and user preferences over time.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To mitigate information overload in news consumption by delivering personalized content that accurately models dynamic user interests.

**Method:** The proposed model integrates NAML for attentive multi-view news modeling and LSTUR for capturing both short- and long-term user representations, enhanced by BERT-based word embeddings.

**Key Contributions:**

	1. Introduces Co-NAML-LSTUR framework for hybrid news recommendation
	2. Enhances semantic feature extraction using BERT-based embeddings
	3. Demonstrates improved performance on established benchmarks (MIND-small, MIND-large)

**Result:** Experimental results indicate that Co-NAML-LSTUR significantly outperforms various state-of-the-art baselines on both MIND-small and MIND-large benchmarks.

**Limitations:** 

**Conclusion:** Combining multi-view news representations with dual-scale user modeling proves effective in improving news recommendation systems.

**Abstract:** News recommendation systems play a vital role in mitigating information overload by delivering personalized news content. A central challenge is to effectively model both multi-view news representations and the dynamic nature of user interests, which often span both short- and long-term preferences. Existing methods typically rely on single-view features of news articles (e.g., titles or categories) or fail to comprehensively capture user preferences across time scales. In this work, we propose Co-NAML-LSTUR, a hybrid news recommendation framework that integrates NAML for attentive multi-view news modeling and LSTUR for capturing both long- and short-term user representations. Our model also incorporates BERT-based word embeddings to enhance semantic feature extraction. We evaluate Co-NAML-LSTUR on two widely used benchmarks, MIND-small and MIND-large. Experimental results show that Co-NAML-LSTUR achieves substantial improvements over most state-of-the-art baselines on MIND-small and MIND-large, respectively. These results demonstrate the effectiveness of combining multi-view news representations with dual-scale user modeling. The implementation of our model is publicly available at https://github.com/MinhNguyenDS/Co-NAML-LSTUR.

</details>


### [88] [Reframe Your Life Story: Interactive Narrative Therapist and Innovative Moment Assessment with Large Language Models](https://arxiv.org/abs/2507.20241)

*Yi Feng, Jiaqi Wang, Wenxuan Zhang, Zhuang Chen, Yutong Shen, Xiyao Xiao, Minlie Huang, Liping Jing, Jian Yu*

**Main category:** cs.CL

**Keywords:** mental health, large language models, narrative therapy, interactive systems, therapy assessment

**Relevance Score:** 10

**TL;DR:** This paper introduces a framework that improves mental health support using large language models through an Interactive Narrative Therapist and an Innovative Moment Assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address the limitations of current mental health support approaches that lack realism in psychotherapy and do not effectively track therapeutic progression.

**Method:** The framework has two components: INT simulates expert narrative therapists and IMA evaluates effectiveness by tracking critical narrative shifts in client speech.

**Key Contributions:**

	1. Introduction of Interactive Narrative Therapist (INT) for simulating expert therapy
	2. Development of Innovative Moment Assessment (IMA) for evaluating therapy effectiveness
	3. Demonstration of improved therapeutic outcomes compared to standard LLMs

**Result:** Experimental results show that INT outperforms standard LLMs in therapeutic quality and depth through experiments on 260 simulated clients and 230 human participants.

**Limitations:** 

**Conclusion:** The effectiveness of INT was demonstrated in creating high-quality support conversations, benefiting social applications.

**Abstract:** Recent progress in large language models (LLMs) has opened new possibilities for mental health support, yet current approaches lack realism in simulating specialized psychotherapy and fail to capture therapeutic progression over time. Narrative therapy, which helps individuals transform problematic life stories into empowering alternatives, remains underutilized due to limited access and social stigma. We address these limitations through a comprehensive framework with two core components. First, INT (Interactive Narrative Therapist) simulates expert narrative therapists by planning therapeutic stages, guiding reflection levels, and generating contextually appropriate expert-like responses. Second, IMA (Innovative Moment Assessment) provides a therapy-centric evaluation method that quantifies effectiveness by tracking "Innovative Moments" (IMs), critical narrative shifts in client speech signaling therapy progress. Experimental results on 260 simulated clients and 230 human participants reveal that INT consistently outperforms standard LLMs in therapeutic quality and depth. We further demonstrate the effectiveness of INT in synthesizing high-quality support conversations to facilitate social applications.

</details>


### [89] [Modeling Professionalism in Expert Questioning through Linguistic Differentiation](https://arxiv.org/abs/2507.20249)

*Giulia D'Agostino, Chung-Chi Chen*

**Main category:** cs.CL

**Keywords:** professionalism, linguistic features, financial communication, expert questions, classification

**Relevance Score:** 6

**TL;DR:** The paper explores modeling and evaluating professionalism in expert communication using linguistic features, specifically in financial analyst questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored dimension of professionalism in expert communication, especially in high-stakes fields like finance.

**Method:** It introduces a novel annotation framework to quantify linguistic elements in financial analyst questions and uses both human-crafted and LLM-generated datasets for analysis.

**Key Contributions:**

	1. Introduces a new framework for annotating professionalism in expert communication.
	2. Constructs datasets with human-authored and LLM-generated financial analyst questions.
	3. Shows that linguistic features are effective in classifying professionalism in expert questions.

**Result:** The study demonstrates that specific linguistic features correlate with both human judgments of professionalism and authorship origin, achieving better performance than existing baselines with a feature-based classifier.

**Limitations:** The study is centered on a specific domain (finance) and may not generalize across all fields of expert communication.

**Conclusion:** Professionalism can be modeled as a linguistically grounded construct that is applicable across various domains.

**Abstract:** Professionalism is a crucial yet underexplored dimension of expert communication, particularly in high-stakes domains like finance. This paper investigates how linguistic features can be leveraged to model and evaluate professionalism in expert questioning. We introduce a novel annotation framework to quantify structural and pragmatic elements in financial analyst questions, such as discourse regulators, prefaces, and request types. Using both human-authored and large language model (LLM)-generated questions, we construct two datasets: one annotated for perceived professionalism and one labeled by question origin. We show that the same linguistic features correlate strongly with both human judgments and authorship origin, suggesting a shared stylistic foundation. Furthermore, a classifier trained solely on these interpretable features outperforms gemini-2.0 and SVM baselines in distinguishing expert-authored questions. Our findings demonstrate that professionalism is a learnable, domain-general construct that can be captured through linguistically grounded modeling.

</details>


### [90] [Post-Completion Learning for Language Models](https://arxiv.org/abs/2507.20252)

*Xiang Fei, Siqi Wang, Shu Wei, Yuxiang Nie, Wei Shi, Hao Feng, Can Huang*

**Main category:** cs.CL

**Keywords:** Post-Completion Learning, Reinforcement Learning, Language Models

**Relevance Score:** 8

**TL;DR:** A new training framework, Post-Completion Learning (PCL), enhances language model performance by utilizing the space after model output completion for self-evaluation and reasoning improvements.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current language model training paradigms overlook learning opportunities that arise after the generation of the <eos> token.

**Method:** A white-box reinforcement learning method allows the model to generate self-assessments and reward predictions, optimizing reasoning and evaluation capabilities through dual-track SFT and mixed RL training.

**Key Contributions:**

	1. Introduction of Post-Completion Learning framework
	2. Utilization of post-completion space for self-assessment
	3. Hybrid optimization through dual-track SFT and reinforcement learning

**Result:** Experimental results show consistent improvements in model performance on various datasets compared to traditional SFT and RL methods.

**Limitations:** 

**Conclusion:** PCL presents a new approach to language model training that enhances output quality while preserving deployment efficiency.

**Abstract:** Current language model training paradigms typically terminate learning upon reaching the end-of-sequence (<eos>}) token, overlooking the potential learning opportunities in the post-completion space. We propose Post-Completion Learning (PCL), a novel training framework that systematically utilizes the sequence space after model output completion, to enhance both the reasoning and self-evaluation abilities. PCL enables models to continue generating self-assessments and reward predictions during training, while maintaining efficient inference by stopping at the completion point.   To fully utilize this post-completion space, we design a white-box reinforcement learning method: let the model evaluate the output content according to the reward rules, then calculate and align the score with the reward functions for supervision. We implement dual-track SFT to optimize both reasoning and evaluation capabilities, and mixed it with RL training to achieve multi-objective hybrid optimization.   Experimental results on different datasets and models demonstrate consistent improvements over traditional SFT and RL methods. Our method provides a new technical path for language model training that enhances output quality while preserving deployment efficiency.

</details>


### [91] [EMBRACE: Shaping Inclusive Opinion Representation by Aligning Implicit Conversations with Social Norms](https://arxiv.org/abs/2507.20264)

*Abeer Aldayel, Areej Alokaili*

**Main category:** cs.CL

**Keywords:** NLP, implicit opinions, model alignment, inclusion, stance modeling

**Relevance Score:** 8

**TL;DR:** This paper develops an alignment evaluation framework to better represent implicit opinions in NLP models, addressing the limitations of current methods that focus on overt user demographics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the shortcomings of existing conversation-based models that rely on surface-level inclusion, which can reinforce harmful stereotypes and misalignments in representations.

**Method:** The proposed framework evaluates implicit expressions of opinion in conversations by modeling the stance of responses and utilizing both positive-unlabeled learning and instruction-tuned language models for alignment assessment.

**Key Contributions:**

	1. Introduces a novel alignment evaluation framework for implicit opinions in NLP models.
	2. Models stance of responses as a proxy for underlying opinions, enhancing representation.
	3. Utilizes advanced learning techniques to assess model alignment post-tuning.

**Result:** The evaluation reveals a pathway toward more equitable inclusion in model behavior by focusing on implicit opinions rather than surface demographics.

**Limitations:** 

**Conclusion:** By foregrounding implicit opinions, the framework promotes a more reflective and diverse representation in computational models, paving the way for improved alignment with normative social views.

**Abstract:** Shaping inclusive representations that embrace diversity and ensure fair participation and reflections of values is at the core of many conversation-based models. However, many existing methods rely on surface inclusion using mention of user demographics or behavioral attributes of social groups. Such methods overlook the nuanced, implicit expression of opinion embedded in conversations. Furthermore, the over-reliance on overt cues can exacerbate misalignment and reinforce harmful or stereotypical representations in model outputs. Thus, we took a step back and recognized that equitable inclusion needs to account for the implicit expression of opinion and use the stance of responses to validate the normative alignment. This study aims to evaluate how opinions are represented in NLP or computational models by introducing an alignment evaluation framework that foregrounds implicit, often overlooked conversations and evaluates the normative social views and discourse. Our approach models the stance of responses as a proxy for the underlying opinion, enabling a considerate and reflective representation of diverse social viewpoints. We evaluate the framework using both (i) positive-unlabeled (PU) online learning with base classifiers, and (ii) instruction-tuned language models to assess post-training alignment. Through this, we provide a lens on how implicit opinions are (mis)represented and offer a pathway toward more inclusive model behavior.

</details>


### [92] [MoL-RL: Distilling Multi-Step Environmental Feedback into LLMs for Feedback-Independent Reasoning](https://arxiv.org/abs/2507.20278)

*Kang Yang, Jingxue Chen, Qingkun Tang, Tianxiang Zhang, Qianchun Lu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought Reasoning, Multi-step Feedback

**Relevance Score:** 9

**TL;DR:** Introduction of MoL-RL, a novel training method for LLMs that uses multi-step environmental feedback to improve chain-of-thought reasoning without the need for external feedback.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs struggle with sequential environmental feedback signals, leading to inefficiencies in reasoning processes. Existing methods either simplify feedback too much or do not fully utilize the complexity of EF interactions.

**Method:** MoL-RL employs a dual-objective optimization framework that integrates Mixture-of-Losses continual training to effectively combine domain-specific feedback and general language capabilities, along with GRPO-based post-training to refine sequential interactions into single-step actions.

**Key Contributions:**

	1. Introduction of MoL-RL as a new optimization paradigm for training LLMs
	2. Utilization of multi-step EF signals to enhance reasoning
	3. Demonstrated state-of-the-art performance on key benchmarks

**Result:** MoL-RL demonstrates state-of-the-art results on benchmarks for mathematical reasoning and code generation, outperforming existing methods while exhibiting strong generalization across different model sizes.

**Limitations:** 

**Conclusion:** The proposed method effectively enhances LLM reasoning capabilities by leveraging complex multi-step textual feedback, promising improvements across various applications.

**Abstract:** Large language models (LLMs) face significant challenges in effectively leveraging sequential environmental feedback (EF) signals, such as natural language evaluations, for feedback-independent chain-of-thought (CoT) reasoning. Existing approaches either convert EF into scalar rewards, losing rich contextual information, or employ refinement datasets, failing to exploit the multi-step and discrete nature of EF interactions. To address these limitations, we propose MoL-RL, a novel training paradigm that integrates multi-step EF signals into LLMs through a dual-objective optimization framework. Our method combines MoL (Mixture-of-Losses) continual training, which decouples domain-specific EF signals (optimized via cross-entropy loss) and general language capabilities (preserved via Kullback-Leibler divergence), with GRPO-based post-training to distill sequential EF interactions into single-step inferences. This synergy enables robust feedback-independent reasoning without relying on external feedback loops. Experimental results on mathematical reasoning (MATH-500, AIME24/AIME25) and code generation (CodeAgent-Test) benchmarks demonstrate that MoL-RL achieves state-of-the-art performance with the Qwen3-8B model, while maintaining strong generalization across model scales (Qwen3-4B). This work provides a promising approach for leveraging multi-step textual feedback to enhance LLMs' reasoning capabilities in diverse domains.

</details>


### [93] [What Language(s) Does Aya-23 Think In? How Multilinguality Affects Internal Language Representations](https://arxiv.org/abs/2507.20279)

*Katharina Trinley, Toshiki Nakai, Tatiana Anikina, Tanja Baeumel*

**Main category:** cs.CL

**Keywords:** large language models, multilingual tasks, code-mixing

**Relevance Score:** 8

**TL;DR:** The study investigates how the Aya-23-8B language model processes multilingual tasks compared to monolingual models, revealing distinct activation patterns for code-mixed and translation tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the internal language processing of large language models (LLMs), particularly in multilingual contexts.

**Method:** The paper employs logit lens and neuron specialization analyses to study the activation patterns of Aya-23-8B in comparison to Llama 3 and Chinese-LLaMA-2.

**Key Contributions:**

	1. Analysis of code-mixed and cloze tasks in a multilingual LLM
	2. Comparative evaluation against predominantly monolingual models
	3. Insights into neuron specialization and activation patterns in multilingual contexts

**Result:** The findings indicate that Aya-23 uniquely activates language representations based on typing relatedness during translation and showcases varying code-mixed neuron activation influenced by the base language.

**Limitations:** 

**Conclusion:** The results enhance the understanding of how multilingual training influences the internal workings of LLMs and could guide future research on cross-lingual transfer.

**Abstract:** Large language models (LLMs) excel at multilingual tasks, yet their internal language processing remains poorly understood. We analyze how Aya-23-8B, a decoder-only LLM trained on balanced multilingual data, handles code-mixed, cloze, and translation tasks compared to predominantly monolingual models like Llama 3 and Chinese-LLaMA-2. Using logit lens and neuron specialization analyses, we find: (1) Aya-23 activates typologically related language representations during translation, unlike English-centric models that rely on a single pivot language; (2) code-mixed neuron activation patterns vary with mixing rates and are shaped more by the base language than the mixed-in one; and (3) Aya-23's languagespecific neurons for code-mixed inputs concentrate in final layers, diverging from prior findings on decoder-only models. Neuron overlap analysis further shows that script similarity and typological relations impact processing across model types. These findings reveal how multilingual training shapes LLM internals and inform future cross-lingual transfer research.

</details>


### [94] [Advancing Dialectal Arabic to Modern Standard Arabic Machine Translation](https://arxiv.org/abs/2507.20301)

*Abdullah Alabdullah, Lifeng Han, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** Dialectal Arabic, Machine Translation, Natural Language Processing, Fine-tuning, Large Language Models

**Relevance Score:** 6

**TL;DR:** This paper addresses the challenges in Dialectal Arabic to Modern Standard Arabic translation by evaluating training-free prompting techniques and developing a resource-efficient fine-tuning pipeline.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Dialectal Arabic (DA) presents significant challenges in NLP, as most communication in the Arab world occurs in dialects that differ from Modern Standard Arabic (MSA), limiting access to digital services and resources.

**Method:** The paper evaluates training-free prompting techniques across six large language models (LLMs) and develops a resource-efficient fine-tuning pipeline for translating various Arabic dialects.

**Key Contributions:**

	1. Evaluation of training-free prompting techniques for DA-MSA translation.
	2. Development of a resource-efficient fine-tuning pipeline for dialectal models.
	3. Introduction of quantization techniques that reduce memory usage significantly with minimal performance loss.

**Result:** Few-shot prompting consistently outperformed other approaches, with GPT-4o achieving the highest performance. A quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming GPT-4o, while multi-dialect models surpassed single-dialect models by over 10% CHrF++. 

**Limitations:** 

**Conclusion:** The findings provide a blueprint for improving inclusion in Arabic NLP, demonstrating that quality DA-MSA translation is achievable with limited resources and can lead to better language technologies.

**Abstract:** Dialectal Arabic (DA) poses a persistent challenge for natural language processing (NLP), as most everyday communication in the Arab world occurs in dialects that diverge significantly from Modern Standard Arabic (MSA). This linguistic divide limits access to digital services and educational resources and impedes progress in Arabic machine translation. This paper presents two core contributions to advancing DA-MSA translation for the Levantine, Egyptian, and Gulf dialects, particularly in low-resource and computationally constrained settings: a comprehensive evaluation of training-free prompting techniques, and the development of a resource-efficient fine-tuning pipeline. Our evaluation of prompting strategies across six large language models (LLMs) found that few-shot prompting consistently outperformed zero-shot, chain-of-thought, and our proposed Ara-TEaR method. GPT-4o achieved the highest performance across all prompting settings. For fine-tuning, a quantized Gemma2-9B model achieved a CHrF++ score of 49.88, outperforming zero-shot GPT-4o (44.58). Joint multi-dialect trained models outperformed single-dialect counterparts by over 10% CHrF++, and 4-bit quantization reduced memory usage by 60% with less than 1% performance loss. The results and insights of our experiments offer a practical blueprint for improving dialectal inclusion in Arabic NLP, showing that high-quality DA-MSA machine translation is achievable even with limited resources and paving the way for more inclusive language technologies.

</details>


### [95] [DYNARTmo: A Dynamic Articulatory Model for Visualization of Speech Movement Patterns](https://arxiv.org/abs/2507.20343)

*Bernd J. Kröger*

**Main category:** cs.CL

**Keywords:** articulatory model, speech visualization, phonetics education, speech therapy, coarticulation

**Relevance Score:** 4

**TL;DR:** DYNARTmo is a dynamic articulatory model that visualizes speech articulation processes and is implemented in a web-based application for educational and therapeutic use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the visualization of speech articulation processes in phonetics education and speech therapy by utilizing a dynamic model.

**Method:** DYNARTmo models six key articulators using ten continuous and six discrete control parameters, allowing for visualization of vocalic and consonantal configurations.

**Key Contributions:**

	1. Development of DYNARTmo for visualizing articulatory processes
	2. Integration into a web-based application for phonetics education
	3. Focus on coarticulation and articulatory underspecification in the model

**Result:** The model is integrated into the SpeechArticulationTrainer web application, providing multiple views (sagittal, glottal, palatal) for enhanced learning.

**Limitations:** Current implementation emphasizes static modeling; dynamic aspects and full articulatory-acoustic integration are to be further developed.

**Conclusion:** Future developments will focus on dynamic movement generation and integration with acoustic modules to enhance the model's functionality.

**Abstract:** We present DYNARTmo, a dynamic articulatory model designed to visualize speech articulation processes in a two-dimensional midsagittal plane. The model builds upon the UK-DYNAMO framework and integrates principles of articulatory underspecification, segmental and gestural control, and coarticulation. DYNARTmo simulates six key articulators based on ten continuous and six discrete control parameters, allowing for the generation of both vocalic and consonantal articulatory configurations. The current implementation is embedded in a web-based application (SpeechArticulationTrainer) that includes sagittal, glottal, and palatal views, making it suitable for use in phonetics education and speech therapy. While this paper focuses on the static modeling aspects, future work will address dynamic movement generation and integration with articulatory-acoustic modules.

</details>


### [96] [RMTBench: Benchmarking LLMs Through Multi-Turn User-Centric Role-Playing](https://arxiv.org/abs/2507.20352)

*Hao Xiang, Tianyi Tang, Yang Su, Bowen Yu, An Yang, Fei Huang, Yichang Zhang, Yaojie Lu, Hongyu Lin, Xianpei Han, Jingren Zhou, Junyang Lin, Le Sun*

**Main category:** cs.CL

**Keywords:** Large Language Models, role-playing benchmark, user-centric evaluation, dialogue simulation, multi-turn conversation

**Relevance Score:** 9

**TL;DR:** Introduction of RMTBench, a user-centric bilingual role-playing benchmark for evaluating Large Language Models (LLMs) with a focus on user intentions rather than character backgrounds.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitations of existing character-centric approaches that simplify user-character interactions and do not reflect real-world applications, requiring a more user-focused evaluation method for LLMs in role-playing scenarios.

**Method:** Development of RMTBench, which features 80 diverse characters and over 8,000 dialogue rounds, incorporating both custom and abstract character definitions. The benchmark emphasizes user motivations to construct dialogues and implements a multi-turn dialogue simulation mechanism with LLM-based scoring.

**Key Contributions:**

	1. Introduction of a user-centric bilingual benchmark for role-playing applications in LLMs.
	2. Inclusion of a diverse set of characters and dialogue scenarios to better assess user interactions.
	3. A novel scoring mechanism capturing complex intentions in conversation.

**Result:** RMTBench provides a comprehensive framework for evaluating LLM capabilities in role-playing contexts, highlighting the importance of user intention fulfillment during interactions.

**Limitations:** 

**Conclusion:** By bridging the gap between academic evaluation and practical deployment requirements, RMTBench offers a robust tool for more effectively assessing role-playing capabilities in LLMs.

**Abstract:** Recent advancements in Large Language Models (LLMs) have shown outstanding potential for role-playing applications. Evaluating these capabilities is becoming crucial yet remains challenging. Existing benchmarks mostly adopt a \textbf{character-centric} approach, simplify user-character interactions to isolated Q&A tasks, and fail to reflect real-world applications. To address this limitation, we introduce RMTBench, a comprehensive \textbf{user-centric} bilingual role-playing benchmark featuring 80 diverse characters and over 8,000 dialogue rounds. RMTBench includes custom characters with detailed backgrounds and abstract characters defined by simple traits, enabling evaluation across various user scenarios. Our benchmark constructs dialogues based on explicit user motivations rather than character descriptions, ensuring alignment with practical user applications. Furthermore, we construct an authentic multi-turn dialogue simulation mechanism. With carefully selected evaluation dimensions and LLM-based scoring, this mechanism captures the complex intention of conversations between the user and the character. By shifting focus from character background to user intention fulfillment, RMTBench bridges the gap between academic evaluation and practical deployment requirements, offering a more effective framework for assessing role-playing capabilities in LLMs. All code and datasets will be released soon.

</details>


### [97] [Length Representations in Large Language Models](https://arxiv.org/abs/2507.20398)

*Sangjun Moon, Dasom Choi, Jingun Kwon, Hidetaka Kamigaito, Manabu Okumura*

**Main category:** cs.CL

**Keywords:** large language models, output sequence length, multi-head attention, disentangled representations, internal mechanisms

**Relevance Score:** 8

**TL;DR:** This study investigates how output sequence length is encoded in large language models (LLMs), revealing that multi-head attention mechanisms are crucial for controlling this aspect while maintaining text informativeness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the internal mechanisms of LLMs that allow control over output sequence length, which has not been thoroughly examined.

**Method:** Empirical analysis of LLMs focusing on the role of multi-head attention in determining output sequence length and how it can be manipulated through selective activation of hidden units.

**Key Contributions:**

	1. Identification of multi-head attention's role in output length control.
	2. Demonstration of disentangled representation of length and semantic information.
	3. Empirical evidence of internal awareness of length-specific prompts.

**Result:** LLMs can control output sequence length in a disentangled manner, where multi-head attention mechanisms play a key role. Specific hidden units can be scaled to adjust length without losing text quality.

**Limitations:** 

**Conclusion:** LLMs possess robust internal mechanisms for managing output length independently from semantic content, indicating adaptability in their representations.

**Abstract:** Large language models (LLMs) have shown remarkable capabilities across various tasks, that are learned from massive amounts of text-based data. Although LLMs can control output sequence length, particularly in instruction-based settings, the internal mechanisms behind this control have been unexplored yet. In this study, we provide empirical evidence on how output sequence length information is encoded within the internal representations in LLMs. In particular, our findings show that multi-head attention mechanisms are critical in determining output sequence length, which can be adjusted in a disentangled manner. By scaling specific hidden units within the model, we can control the output sequence length without losing the informativeness of the generated text, thereby indicating that length information is partially disentangled from semantic information. Moreover, some hidden units become increasingly active as prompts become more length-specific, thus reflecting the model's internal awareness of this attribute. Our findings suggest that LLMs have learned robust and adaptable internal mechanisms for controlling output length without any external control.

</details>


### [98] [Cognitive Chain-of-Thought: Structured Multimodal Reasoning about Social Situations](https://arxiv.org/abs/2507.20409)

*Eunkyu Park, Wesley Hanwen Deng, Gunhee Kim, Motahhare Eslami, Maarten Sap*

**Main category:** cs.CL

**Keywords:** Cognitive Chain-of-Thought, visual language models, multimodal systems, reasoning, social awareness

**Relevance Score:** 8

**TL;DR:** The paper introduces Cognitive Chain-of-Thought (CoCoT), a new prompting strategy for visual language models (VLMs) that improves reasoning through three cognitively inspired stages: perception, situation, and norm.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance visual tasks reliant on social context where understanding and judgement must occur simultaneously, addressing limitations of flat Chain-of-Thought (CoT) prompting.

**Method:** The paper proposes CoCoT as a scaffolding strategy for reasoning that divides the process into three stages: perception, situation, and norm, enabling more effective interpretability.

**Key Contributions:**

	1. Introduction of Cognitive Chain-of-Thought (CoCoT) methodology
	2. Demonstrated improvements in reasoning accuracy on multimodal benchmarks
	3. Enhanced interpretability and social context awareness in visual language models

**Result:** CoCoT consistently outperforms traditional CoT and direct prompting by an average of 8% across various multimodal benchmarks, improving intent disambiguation, commonsense reasoning, and safety.

**Limitations:** 

**Conclusion:** Cognitively grounded reasoning stages notably enhance the interpretability and social awareness of VLMs, suggesting a pathway to safer and more reliable multimodal systems.

**Abstract:** Chain-of-Thought (CoT) prompting helps models think step by step. But what happens when they must see, understand, and judge-all at once? In visual tasks grounded in social context, where bridging perception with norm-grounded judgments is essential, flat CoT often breaks down. We introduce Cognitive Chain-of-Thought (CoCoT), a prompting strategy that scaffolds VLM reasoning through three cognitively inspired stages: perception, situation, and norm. Our experiments show that, across multiple multimodal benchmarks (including intent disambiguation, commonsense reasoning, and safety), CoCoT consistently outperforms CoT and direct prompting (+8\% on average). Our findings demonstrate that cognitively grounded reasoning stages enhance interpretability and social awareness in VLMs, paving the way for safer and more reliable multimodal systems.

</details>


### [99] [CONCAP: Seeing Beyond English with Concepts Retrieval-Augmented Captioning](https://arxiv.org/abs/2507.20411)

*George Ibrahim, Rita Ramos, Yova Kementchedjhieva*

**Main category:** cs.CL

**Keywords:** multilingual image captioning, retrieval-augmented generation, CONCAP, low-resource languages

**Relevance Score:** 8

**TL;DR:** CONCAP enhances multilingual image captioning by integrating retrieved captions with image-specific concepts, significantly improving performance on low-resource languages.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance gap in multilingual vision-language models for image captioning compared to English models, which is exacerbated by limited multilingual data and biases in translated captions.

**Method:** The paper introduces CONCAP, a multilingual image captioning model that uses retrieval-augmented generation (RAG) combined with image-specific concepts to improve captioning across languages.

**Key Contributions:**

	1. Introduction of the CONCAP model for multilingual image captioning
	2. Integration of image-specific concepts with retrieved captions
	3. Demonstrated effectiveness on low- and mid-resource languages with reduced data requirements

**Result:** Experiments on the XM3600 dataset demonstrate that CONCAP significantly outperforms existing multilingual models, particularly in low- and mid-resource languages, with reduced data requirements.

**Limitations:** 

**Conclusion:** CONCAP effectively enhances multilingual image captioning by improving the contextualization and grounding of captions across languages, thus bridging performance gaps in multilingual settings.

**Abstract:** Multilingual vision-language models have made significant strides in image captioning, yet they still lag behind their English counterparts due to limited multilingual training data and costly large-scale model parameterization. Retrieval-augmented generation (RAG) offers a promising alternative by conditioning caption generation on retrieved examples in the target language, reducing the need for extensive multilingual training. However, multilingual RAG captioning models often depend on retrieved captions translated from English, which can introduce mismatches and linguistic biases relative to the source language. We introduce CONCAP, a multilingual image captioning model that integrates retrieved captions with image-specific concepts, enhancing the contextualization of the input image and grounding the captioning process across different languages. Experiments on the XM3600 dataset indicate that CONCAP enables strong performance on low- and mid-resource languages, with highly reduced data requirements. Our findings highlight the effectiveness of concept-aware retrieval augmentation in bridging multilingual performance gaps.

</details>


### [100] [Survey of NLU Benchmarks Diagnosing Linguistic Phenomena: Why not Standardize Diagnostics Benchmarks?](https://arxiv.org/abs/2507.20419)

*Khloud AL Jallad, Nada Ghneim, Ghaida Rebdawi*

**Main category:** cs.CL

**Keywords:** Natural Language Understanding, benchmark, evaluation metrics, error analysis, linguistic phenomena

**Relevance Score:** 8

**TL;DR:** This survey reviews existing NLU benchmarks across English, Arabic, and multilingual datasets focusing on diagnostics for error analysis.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of standardization in evaluation metrics for NLU benchmarks and facilitate better error analysis and comparison of models.

**Method:** Comprehensive review and comparison of available NLU benchmarks and their diagnostic datasets, focusing on linguistic phenomena.

**Key Contributions:**

	1. Comprehensive review of NLU benchmarks in multiple languages
	2. Identification of gaps in evaluation standards
	3. Proposal for a standardization of evaluation metrics for diagnostics

**Result:** Identified gaps in NLU evaluation standards; proposed a research question regarding the standardization of evaluation metrics for diagnostics benchmarks.

**Limitations:** Lack of standard naming conventions and categories for linguistic phenomena.

**Conclusion:** Establishing evaluation metrics for diagnostics could enhance insights and comparisons between NLU models across different benchmarks.

**Abstract:** Natural Language Understanding (NLU) is a basic task in Natural Language Processing (NLP). The evaluation of NLU capabilities has become a trending research topic that attracts researchers in the last few years, resulting in the development of numerous benchmarks. These benchmarks include various tasks and datasets in order to evaluate the results of pretrained models via public leaderboards. Notably, several benchmarks contain diagnostics datasets designed for investigation and fine-grained error analysis across a wide range of linguistic phenomena. This survey provides a comprehensive review of available English, Arabic, and Multilingual NLU benchmarks, with a particular emphasis on their diagnostics datasets and the linguistic phenomena they covered. We present a detailed comparison and analysis of these benchmarks, highlighting their strengths and limitations in evaluating NLU tasks and providing in-depth error analysis. When highlighting the gaps in the state-of-the-art, we noted that there is no naming convention for macro and micro categories or even a standard set of linguistic phenomena that should be covered. Consequently, we formulated a research question regarding the evaluation metrics of the evaluation diagnostics benchmarks: "Why do not we have an evaluation standard for the NLU evaluation diagnostics benchmarks?" similar to ISO standard in industry. We conducted a deep analysis and comparisons of the covered linguistic phenomena in order to support experts in building a global hierarchy for linguistic phenomena in future. We think that having evaluation metrics for diagnostics evaluation could be valuable to gain more insights when comparing the results of the studied models on different diagnostics benchmarks.

</details>


### [101] [CodeNER: Code Prompting for Named Entity Recognition](https://arxiv.org/abs/2507.20423)

*Sungwoo Han, Hyeyeon Kim, Jingun Kwon, Hidetaka Kamigaito, Manabu Okumura*

**Main category:** cs.CL

**Keywords:** named entity recognition, large language models, code-based prompting, BIO schema, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a code-based prompting method to enhance named entity recognition (NER) with large language models (LLMs), showing improved results over traditional text-based approaches.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve NER by better capturing labeling requirements and using the capabilities of LLMs for understanding context and instructions.

**Method:** The study proposes a novel prompting approach that embeds code within prompts to provide detailed BIO schema instructions, thus leveraging LLMs' ability to understand long-range dependencies common in programming languages.

**Key Contributions:**

	1. Introduction of code-based prompting for NER using LLMs
	2. Demonstrated superiority of the method over traditional techniques
	3. Performance evaluation across multiple languages and datasets

**Result:** The code-based prompting method significantly outperforms conventional text-based prompting across ten benchmarks in multiple languages, demonstrating improved effectiveness in structuring NER instructions.

**Limitations:** The approach relies on effective integration of code within prompts, which may not generalize across all NER tasks or languages.

**Conclusion:** The findings validate the efficacy of code-based prompting in enhancing LLM performance for NER tasks, especially when combined with chain-of-thought prompting strategies.

**Abstract:** Recent studies have explored various approaches for treating candidate named entity spans as both source and target sequences in named entity recognition (NER) by leveraging large language models (LLMs). Although previous approaches have successfully generated candidate named entity spans with suitable labels, they rely solely on input context information when using LLMs, particularly, ChatGPT. However, NER inherently requires capturing detailed labeling requirements with input context information. To address this issue, we propose a novel method that leverages code-based prompting to improve the capabilities of LLMs in understanding and performing NER. By embedding code within prompts, we provide detailed BIO schema instructions for labeling, thereby exploiting the ability of LLMs to comprehend long-range scopes in programming languages. Experimental results demonstrate that the proposed code-based prompting method outperforms conventional text-based prompting on ten benchmarks across English, Arabic, Finnish, Danish, and German datasets, indicating the effectiveness of explicitly structuring NER instructions. We also verify that combining the proposed code-based prompting method with the chain-of-thought prompting further improves performance.

</details>


### [102] [Speaking in Words, Thinking in Logic: A Dual-Process Framework in QA Systems](https://arxiv.org/abs/2507.20491)

*Tuan Bui, Trong Le, Phat Thai, Sang Nguyen, Minh Hua, Ngan Pham, Thang Bui, Tho Quan*

**Main category:** cs.CL

**Keywords:** large language models, neural-symbolic frameworks, explainable AI

**Relevance Score:** 9

**TL;DR:** Text-JEPA is a lightweight framework that converts natural language into first-order logic for explainable question-answering in closed-domain contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for accurate and explainable reasoning in question-answering systems in closed domains like healthcare and education.

**Method:** Text-JEPA employs a dual-system cognitive approach, using a lightweight method to generate logic representations and a Z3 solver for logical inference, alongside a comprehensive evaluation framework.

**Key Contributions:**

	1. Introduction of Text-JEPA for NL2FOL conversion
	2. Development of a comprehensive evaluation framework for reasoning accuracy
	3. Demonstration of competitive performance with lower resource requirements

**Result:** Text-JEPA shows competitive performance on domain-specific datasets with lower computational overhead compared to larger LLM-based systems.

**Limitations:** 

**Conclusion:** The findings suggest structured and interpretable reasoning frameworks are effective for building efficient QA systems in specialized domains.

**Abstract:** Recent advances in large language models (LLMs) have significantly enhanced question-answering (QA) capabilities, particularly in open-domain contexts. However, in closed-domain scenarios such as education, healthcare, and law, users demand not only accurate answers but also transparent reasoning and explainable decision-making processes. While neural-symbolic (NeSy) frameworks have emerged as a promising solution, leveraging LLMs for natural language understanding and symbolic systems for formal reasoning, existing approaches often rely on large-scale models and exhibit inefficiencies in translating natural language into formal logic representations.   To address these limitations, we introduce Text-JEPA (Text-based Joint-Embedding Predictive Architecture), a lightweight yet effective framework for converting natural language into first-order logic (NL2FOL). Drawing inspiration from dual-system cognitive theory, Text-JEPA emulates System 1 by efficiently generating logic representations, while the Z3 solver operates as System 2, enabling robust logical inference. To rigorously evaluate the NL2FOL-to-reasoning pipeline, we propose a comprehensive evaluation framework comprising three custom metrics: conversion score, reasoning score, and Spearman rho score, which collectively capture the quality of logical translation and its downstream impact on reasoning accuracy.   Empirical results on domain-specific datasets demonstrate that Text-JEPA achieves competitive performance with significantly lower computational overhead compared to larger LLM-based systems. Our findings highlight the potential of structured, interpretable reasoning frameworks for building efficient and explainable QA systems in specialized domains.

</details>


### [103] [AQUA: A Large Language Model for Aquaculture & Fisheries](https://arxiv.org/abs/2507.20520)

*Praneeth Narisetty, Uday Kumar Reddy Kattamanchi, Lohit Akshant Nimma, Sri Ram Kaushik Karnati, Shiva Nagendra Babu Kore, Mounika Golamari, Tejashree Nageshreddy*

**Main category:** cs.CL

**Keywords:** aquaculture, large language model, synthetic data, AI, decision-making

**Relevance Score:** 3

**TL;DR:** Introduction of AQUA, the first LLM designed for aquaculture, aimed at addressing challenges in the industry.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Aquaculture is critical for food security and faces challenges that existing AI methods do not address effectively.

**Method:** AQUA utilizes a combination of expert knowledge, language models, and automated evaluation techniques to create high-quality synthetic data.

**Key Contributions:**

	1. Introduction of AQUA, a specialized LLM for aquaculture
	2. Development of the AQUADAPT framework for synthetic data generation
	3. Innovation in LLM applications for aquaculture research and decision-making tools

**Result:** AQUA aims to support farmers and researchers by providing an innovative LLM tool tailored to the specific needs of the aquaculture industry.

**Limitations:** 

**Conclusion:** AQUA and the AQUADAPT framework pave the way for LLM-driven advancements in aquaculture practices and decision-making.

**Abstract:** Aquaculture plays a vital role in global food security and coastal economies by providing sustainable protein sources. As the industry expands to meet rising demand, it faces growing challenges such as disease outbreaks, inefficient feeding practices, rising labor costs, logistical inefficiencies, and critical hatchery issues, including high mortality rates and poor water quality control. Although artificial intelligence has made significant progress, existing machine learning methods fall short of addressing the domain-specific complexities of aquaculture. To bridge this gap, we introduce AQUA, the first large language model (LLM) tailored for aquaculture, designed to support farmers, researchers, and industry practitioners. Central to this effort is AQUADAPT (Data Acquisition, Processing and Tuning), an Agentic Framework for generating and refining high-quality synthetic data using a combination of expert knowledge, largescale language models, and automated evaluation techniques. Our work lays the foundation for LLM-driven innovations in aquaculture research, advisory systems, and decision-making tools.

</details>


### [104] [SAND-Math: Using LLMs to Generate Novel, Difficult and Useful Mathematics Questions and Answers](https://arxiv.org/abs/2507.20527)

*Chaitanya Manem, Pratik Prabhanjan Brahma, Prakamya Mishra, Zicheng Liu, Emad Barsoum*

**Main category:** cs.CL

**Keywords:** Large Language Models, mathematical reasoning, synthetic data

**Relevance Score:** 8

**TL;DR:** The paper presents SAND-Math, a pipeline for generating challenging mathematical problems to enhance the performance of Large Language Models (LLMs) in mathematical reasoning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the bottleneck in developing powerful mathematical LLMs due to the lack of complex training data, the authors propose a novel solution for generating synthetic math problems.

**Method:** SAND-Math generates high-quality mathematical problems and uses a 'Difficulty Hiking' step to systematically increase their complexity.

**Key Contributions:**

	1. Introduction of SAND-Math for generating synthetic math problems
	2. A new Difficulty Hiking method to increase problem complexity
	3. Demonstrated significant performance improvements on the AIME25 benchmark

**Result:** The approach significantly boosts LLM performance, achieving a 17.85 absolute point improvement on the AIME25 benchmark and raising performance from 46.38% to 49.23% through enhanced problem difficulty.

**Limitations:** 

**Conclusion:** SAND-Math provides a scalable approach for generating high-quality datasets that can aid in the development of more capable mathematical reasoning LLMs.

**Abstract:** The demand for Large Language Models (LLMs) capable of sophisticated mathematical reasoning is growing across industries. However, the development of performant mathematical LLMs is critically bottlenecked by the scarcity of difficult, novel training data. We introduce \textbf{SAND-Math} (Synthetic Augmented Novel and Difficult Mathematics problems and solutions), a pipeline that addresses this by first generating high-quality problems from scratch and then systematically elevating their complexity via a new \textbf{Difficulty Hiking} step. We demonstrate the effectiveness of our approach through two key findings. First, augmenting a strong baseline with SAND-Math data significantly boosts performance, outperforming the next-best synthetic dataset by \textbf{$\uparrow$ 17.85 absolute points} on the AIME25 benchmark. Second, in a dedicated ablation study, we show our Difficulty Hiking process is highly effective: by increasing average problem difficulty from 5.02 to 5.98, this step lifts AIME25 performance from 46.38\% to 49.23\%. The full generation pipeline, final dataset, and a fine-tuned model form a practical and scalable toolkit for building more capable and efficient mathematical reasoning LLMs. SAND-Math dataset is released here: \href{https://huggingface.co/datasets/amd/SAND-MATH}{https://huggingface.co/datasets/amd/SAND-MATH}

</details>


### [105] [Dialogues of Dissent: Thematic and Rhetorical Dimensions of Hate and Counter-Hate Speech in Social Media Conversations](https://arxiv.org/abs/2507.20528)

*Effi Levi, Gal Ron, Odelia Oshri, Shaul R. Shenhav*

**Main category:** cs.CL

**Keywords:** hate speech, counter-hate speech, social media, thematic dimensions, rhetorical dimensions

**Relevance Score:** 4

**TL;DR:** A novel multi-labeled scheme for annotating hate and counter-hate speech on social media is introduced, focusing on thematic and rhetorical dimensions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To better understand and categorize hate and counter-hate speech in social media settings, addressing the complexity of these interactions.

**Method:** The authors annotated 92 conversations with 720 tweets, applying a multi-labeled scheme that incorporates both thematic and rhetorical dimensions, guided by Aristotelian principles.

**Key Contributions:**

	1. Introduction of a multi-labeled annotation scheme for speech classification
	2. Insights into rhetorical strategies used in hate and counter-hate speech
	3. Statistical analysis of interaction patterns in social media conversations.

**Result:** Statistical analyses reveal patterns of interaction within and between hate and counter-hate speech, highlighting the strategies and potential impact these messages have on online behavior.

**Limitations:** Limited sample size of annotated conversations; results may not generalize across all social media platforms.

**Conclusion:** The study enhances the understanding of hate speech dynamics on social media and the effectiveness of counter-hate strategies.

**Abstract:** We introduce a novel multi-labeled scheme for joint annotation of hate and counter-hate speech in social media conversations, categorizing hate and counter-hate messages into thematic and rhetorical dimensions. The thematic categories outline different discursive aspects of each type of speech, while the rhetorical dimension captures how hate and counter messages are communicated, drawing on Aristotle's Logos, Ethos and Pathos. We annotate a sample of 92 conversations, consisting of 720 tweets, and conduct statistical analyses, incorporating public metrics, to explore patterns of interaction between the thematic and rhetorical dimensions within and between hate and counter-hate speech. Our findings provide insights into the spread of hate messages on social media, the strategies used to counter them, and their potential impact on online behavior.

</details>


### [106] [Enhancing Hallucination Detection via Future Context](https://arxiv.org/abs/2507.20546)

*Joosung Lee, Cheonbok Park, Hwiyeol Jo, Jeonghoon Kim, Joonsuk Park, Kang Min Yoo*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, black-box generators

**Relevance Score:** 8

**TL;DR:** This paper presents a framework for detecting hallucinations in outputs from large language models (LLMs) by utilizing sampled future contexts, leading to improved performance across several methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs raises concerns over the reliability of their outputs, as they often generate plausible but incorrect information (hallucinations) without transparency.

**Method:** The authors developed a detection framework that leverages sampled future contexts to identify hallucinations in the text produced by black-box generators.

**Key Contributions:**

	1. Development of a hallucination detection framework for LLMs
	2. Introduction of future context sampling for improved detection
	3. Validation of the framework's effectiveness across various methods.

**Result:** The proposed sampling approach shows significant performance improvements in detecting hallucinations across multiple detection methods.

**Limitations:** 

**Conclusion:** Integrating future context sampling into hallucination detection can enhance the reliability of outputs from LLMs.

**Abstract:** Large Language Models (LLMs) are widely used to generate plausible text on online platforms, without revealing the generation process. As users increasingly encounter such black-box outputs, detecting hallucinations has become a critical challenge. To address this challenge, we focus on developing a hallucination detection framework for black-box generators. Motivated by the observation that hallucinations, once introduced, tend to persist, we sample future contexts. The sampled future contexts provide valuable clues for hallucination detection and can be effectively integrated with various sampling-based methods. We extensively demonstrate performance improvements across multiple methods using our proposed sampling approach.

</details>


### [107] [ZSE-Cap: A Zero-Shot Ensemble for Image Retrieval and Prompt-Guided Captioning](https://arxiv.org/abs/2507.20564)

*Duc-Tai Dinh, Duc Anh Khoa Dinh*

**Main category:** cs.CL

**Keywords:** zero-shot learning, image retrieval, captioning, model ensembling, prompt engineering

**Relevance Score:** 6

**TL;DR:** ZSE-Cap is a top-performing zero-shot system for captioning and image retrieval that combines multiple models without fine-tuning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance image retrieval and captioning by employing a zero-shot ensemble approach that avoids data-specific fine-tuning.

**Method:** The system uses an ensemble of similarity scores from CLIP, SigLIP, and DINOv2 for retrieval and utilizes a tailored prompt to guide the Gemma 3 model for captioning.

**Key Contributions:**

	1. Introduction of a zero-shot ensemble method for image retrieval and captioning.
	2. Demonstrated effective prompting techniques to connect textual and visual information.
	3. Achieved competitive results in a shared task without fine-tuning on training data.

**Result:** Achieved a final score of 0.42002, placing 4th in the EVENTA shared task, showcasing the effectiveness of the combined approach.

**Limitations:** 

**Conclusion:** The study highlights the potential of zero-shot learning and model ensembling in improving image analysis tasks without the need for extensive data-specific training.

**Abstract:** We present ZSE-Cap (Zero-Shot Ensemble for Captioning), our 4th place system in Event-Enriched Image Analysis (EVENTA) shared task on article-grounded image retrieval and captioning. Our zero-shot approach requires no finetuning on the competition's data. For retrieval, we ensemble similarity scores from CLIP, SigLIP, and DINOv2. For captioning, we leverage a carefully engineered prompt to guide the Gemma 3 model, enabling it to link high-level events from the article to the visual content in the image. Our system achieved a final score of 0.42002, securing a top-4 position on the private test set, demonstrating the effectiveness of combining foundation models through ensembling and prompting. Our code is available at https://github.com/ductai05/ZSE-Cap.

</details>


### [108] [Before the Outrage: Challenges and Advances in Predicting Online Antisocial Behavior](https://arxiv.org/abs/2507.20614)

*Anaïs Ollagnier*

**Main category:** cs.CL

**Keywords:** Antisocial behavior, Social media, Prediction, Machine learning, Systematic review

**Relevance Score:** 5

**TL;DR:** This paper systematically reviews studies on predicting antisocial behavior on social media, offering a structured taxonomy and examining methodological challenges and future research directions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing challenges of antisocial behavior on social media and to provide a unified approach towards prediction methods.

**Method:** A systematic review of over 49 studies on antisocial behavior prediction, classifying them into five core task types and analyzing modeling techniques and dataset characteristics.

**Key Contributions:**

	1. Structured taxonomy of antisocial behavior prediction tasks
	2. Analysis of modeling techniques and their effectiveness
	3. Identification of methodological challenges and emerging research directions

**Result:** Identified five core task types for predicting antisocial behavior and analyzed trends in modeling techniques, revealing significant methodological challenges.

**Limitations:** Challenges such as dataset scarcity, temporal drift, and a lack of benchmarks limit current research capabilities.

**Conclusion:** The review encourages a coherent framework to guide future research for more robust and socially responsible antisocial behavior prediction methodologies.

**Abstract:** Antisocial behavior (ASB) on social media-including hate speech, harassment, and trolling-poses growing challenges for platform safety and societal wellbeing. While prior work has primarily focused on detecting harmful content after it appears, predictive approaches aim to forecast future harmful behaviors-such as hate speech propagation, conversation derailment, or user recidivism-before they fully unfold. Despite increasing interest, the field remains fragmented, lacking a unified taxonomy or clear synthesis of existing methods. This paper presents a systematic review of over 49 studies on ASB prediction, offering a structured taxonomy of five core task types: early harm detection, harm emergence prediction, harm propagation prediction, behavioral risk prediction, and proactive moderation support. We analyze how these tasks differ by temporal framing, prediction granularity, and operational goals. In addition, we examine trends in modeling techniques-from classical machine learning to pre-trained language models-and assess the influence of dataset characteristics on task feasibility and generalization. Our review highlights methodological challenges, such as dataset scarcity, temporal drift, and limited benchmarks, while outlining emerging research directions including multilingual modeling, cross-platform generalization, and human-in-the-loop systems. By organizing the field around a coherent framework, this survey aims to guide future work toward more robust and socially responsible ASB prediction.

</details>


### [109] [Ontology-Enhanced Knowledge Graph Completion using Large Language Models](https://arxiv.org/abs/2507.20643)

*Wenbin Guo, Xin Wang, Jiaoyan Chen, Zhao Li, Zirui Chen*

**Main category:** cs.CL

**Keywords:** Knowledge Graph Completion, Large Language Models, Ontology, Neural-perceptual structures, Artificial Intelligence

**Relevance Score:** 8

**TL;DR:** This paper presents OL-KGC, an ontology enhanced method for Knowledge Graph Completion using Large Language Models, improving reasoning outcomes through integration of neural-perceptual structures and ontological knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the reasoning capabilities of LLM-based Knowledge Graph Completion methods which suffer from black-box issues and erroneous knowledge propagation.

**Method:** The proposed OL-KGC integrates neural perceptual structural information with automatic extraction of ontological knowledge, transforming it into a format suitable for LLMs.

**Key Contributions:**

	1. Introduction of OL-KGC, an ontology enhanced KGC method utilizing LLMs.
	2. Effective embedding of structural information into text format for better reasoning.
	3. Demonstrated superior performance on benchmark datasets compared to existing methods.

**Result:** OL-KGC significantly outperforms existing KGC methods on benchmarks like FB15K-237, UMLS, and WN18RR, achieving state-of-the-art performance.

**Limitations:** 

**Conclusion:** The integration of neural perceptual mechanisms with ontological knowledge provides substantial improvements in knowledge graph completion tasks.

**Abstract:** Large Language Models (LLMs) have been extensively adopted in Knowledge Graph Completion (KGC), showcasing significant research advancements. However, as black-box models driven by deep neural architectures, current LLM-based KGC methods rely on implicit knowledge representation with parallel propagation of erroneous knowledge, thereby hindering their ability to produce conclusive and decisive reasoning outcomes. We aim to integrate neural-perceptual structural information with ontological knowledge, leveraging the powerful capabilities of LLMs to achieve a deeper understanding of the intrinsic logic of the knowledge. We propose an ontology enhanced KGC method using LLMs -- OL-KGC. It first leverages neural perceptual mechanisms to effectively embed structural information into the textual space, and then uses an automated extraction algorithm to retrieve ontological knowledge from the knowledge graphs (KGs) that needs to be completed, which is further transformed into a textual format comprehensible to LLMs for providing logic guidance. We conducted extensive experiments on three widely-used benchmarks -- FB15K-237, UMLS and WN18RR. The experimental results demonstrate that OL-KGC significantly outperforms existing mainstream KGC methods across multiple evaluation metrics, achieving state-of-the-art performance.

</details>


### [110] [Geometric-Mean Policy Optimization](https://arxiv.org/abs/2507.20673)

*Yuzhong Zhao, Yue Liu, Junpeng Liu, Jingye Chen, Xun Wu, Yaru Hao, Tengchao Lv, Shaohan Huang, Lei Cui, Qixiang Ye, Fang Wan, Furu Wei*

**Main category:** cs.CL

**Keywords:** Geometric Mean, Policy Optimization, Human-Computer Interaction, Large Language Models, Outlier Sensitivity

**Relevance Score:** 8

**TL;DR:** This paper introduces Geometric-Mean Policy Optimization (GMPO), a stabilized variant of Group Relative Policy Optimization (GRPO), which improves the stability of token-level reward processing by using geometric mean instead of arithmetic mean.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the instability of policy updates caused by outlier importance-weighted rewards in GRPO, which leads to extreme importance sampling ratios during training.

**Method:** GMPO optimizes the geometric mean of token-level rewards, which is less sensitive to outliers, thus providing a more stable range of importance sampling ratios compared to GRPO.

**Key Contributions:**

	1. Introduction of Geometric-Mean Policy Optimization (GMPO) as a stable alternative to GRPO.
	2. Demonstrated improved stability and performance metrics on various benchmarks.
	3. Comprehensive analysis supporting the benefits of GMPO over traditional methods.

**Result:** GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmarks, enhancing the reasoning capabilities of large language models.

**Limitations:** 

**Conclusion:** The theoretical and experimental analysis confirms that GMPO achieves improved stability and performance over GRPO, making it a viable alternative for optimizing language model policies.

**Abstract:** Recent advancements, such as Group Relative Policy Optimization (GRPO), have enhanced the reasoning capabilities of large language models by optimizing the arithmetic mean of token-level rewards. However, GRPO suffers from unstable policy updates when processing tokens with outlier importance-weighted rewards, which manifests as extreme importance sampling ratios during training, i.e., the ratio between the sampling probabilities assigned to a token by the current and old policies. In this work, we propose Geometric-Mean Policy Optimization (GMPO), a stabilized variant of GRPO. Instead of optimizing the arithmetic mean, GMPO maximizes the geometric mean of token-level rewards, which is inherently less sensitive to outliers and maintains a more stable range of importance sampling ratio. In addition, we provide comprehensive theoretical and experimental analysis to justify the design and stability benefits of GMPO. Beyond improved stability, GMPO-7B outperforms GRPO by an average of 4.1% on multiple mathematical benchmarks and 1.4% on multimodal reasoning benchmark, including AIME24, AMC, MATH500, OlympiadBench, Minerva, and Geometry3K. Code is available at https://github.com/callsys/GMPO.

</details>


### [111] [When Scale Meets Diversity: Evaluating Language Models on Fine-Grained Multilingual Claim Verification](https://arxiv.org/abs/2507.20700)

*Hanna Shcharbakova, Tatiana Anikina, Natalia Skachkova, Josef van Genabith*

**Main category:** cs.CL

**Keywords:** multilingual, fact verification, large language models, X-Fact dataset, bias

**Relevance Score:** 8

**TL;DR:** This paper evaluates multilingual fact verification using five language models on the X-Fact dataset, finding that smaller models outperform larger LLMs in accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective multilingual misinformation verification systems amidst the rise of misinformation across languages, highlighting the shortcomings of existing large language models.

**Method:** Evaluated five language models (XLM-R, mT5, Llama 3.1, Qwen 2.5, Mistral Nemo) on the X-Fact dataset with seven veracity categories, employing both prompting and fine-tuning techniques.

**Key Contributions:**

	1. Comprehensive evaluation of multilingual claim verification.
	2. Establishment of new performance benchmarks for multilingual fact verification.
	3. Identification of problematic patterns in LLM behavior for fine-grained tasks.

**Result:** XLM-R (270M parameters) outperformed all tested LLMs (7-12B parameters) with a macro-F1 score of 57.7%, significantly improving the state-of-the-art by 15.8%.

**Limitations:** Focus on five models and specific dataset; findings may not generalize to all misinformation scenarios.

**Conclusion:** Smaller specialized models may be more effective than larger general-purpose models for nuanced multilingual fact verification, impacting the deployment of fact-checking systems.

**Abstract:** The rapid spread of multilingual misinformation requires robust automated fact verification systems capable of handling fine-grained veracity assessments across diverse languages. While large language models have shown remarkable capabilities across many NLP tasks, their effectiveness for multilingual claim verification with nuanced classification schemes remains understudied. We conduct a comprehensive evaluation of five state-of-the-art language models on the X-Fact dataset, which spans 25 languages with seven distinct veracity categories. Our experiments compare small language models (encoder-based XLM-R and mT5) with recent decoder-only LLMs (Llama 3.1, Qwen 2.5, Mistral Nemo) using both prompting and fine-tuning approaches. Surprisingly, we find that XLM-R (270M parameters) substantially outperforms all tested LLMs (7-12B parameters), achieving 57.7% macro-F1 compared to the best LLM performance of 16.9%. This represents a 15.8% improvement over the previous state-of-the-art (41.9%), establishing new performance benchmarks for multilingual fact verification. Our analysis reveals problematic patterns in LLM behavior, including systematic difficulties in leveraging evidence and pronounced biases toward frequent categories in imbalanced data settings. These findings suggest that for fine-grained multilingual fact verification, smaller specialized models may be more effective than general-purpose large models, with important implications for practical deployment of fact-checking systems.

</details>


### [112] [Text2VLM: Adapting Text-Only Datasets to Evaluate Alignment Training in Visual Language Models](https://arxiv.org/abs/2507.20704)

*Gabriel Downer, Sean Craven, Damian Ruck, Jake Thomas*

**Main category:** cs.CL

**Keywords:** Visual Language Models, multimodal evaluation, prompt injection, safety assessment, machine learning

**Relevance Score:** 8

**TL;DR:** A novel pipeline, Text2VLM, is introduced for evaluating Visual Language Models (VLMs) against typographic prompt injection attacks using multimodal prompts, revealing critical vulnerabilities in open-source models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of Visual Language Models (VLMs) requires robust evaluation mechanisms, especially regarding their vulnerabilities in multimodal content combining text and images.

**Method:** Text2VLM transforms text-only evaluation datasets into multimodal formats, generating typographic images that serve as multimodal prompts for VLMs, followed by human evaluations.

**Key Contributions:**

	1. Introduction of the Text2VLM pipeline for multimodal prompt evaluation.
	2. Identification of vulnerabilities in open-source VLMs under visual input conditions.
	3. Validation through human evaluations ensuring alignment with human expectations.

**Result:** The evaluation highlights that open-source VLMs are more susceptible to prompt injection when incorporating visual inputs, indicating significant alignment weaknesses and performance gaps compared to closed-source models.

**Limitations:** Focus on specific attack types and reliance on the quality of text-to-image conversions.

**Conclusion:** Text2VLM is validated as a scalable tool for assessing multimodal vulnerabilities, contributing to the development of safer VLMs for real-world applications.

**Abstract:** The increasing integration of Visual Language Models (VLMs) into AI systems necessitates robust model alignment, especially when handling multimodal content that combines text and images. Existing evaluation datasets heavily lean towards text-only prompts, leaving visual vulnerabilities under evaluated. To address this gap, we propose \textbf{Text2VLM}, a novel multi-stage pipeline that adapts text-only datasets into multimodal formats, specifically designed to evaluate the resilience of VLMs against typographic prompt injection attacks. The Text2VLM pipeline identifies harmful content in the original text and converts it into a typographic image, creating a multimodal prompt for VLMs. Also, our evaluation of open-source VLMs highlights their increased susceptibility to prompt injection when visual inputs are introduced, revealing critical weaknesses in the current models' alignment. This is in addition to a significant performance gap compared to closed-source frontier models. We validate Text2VLM through human evaluations, ensuring the alignment of extracted salient concepts; text summarization and output classification align with human expectations. Text2VLM provides a scalable tool for comprehensive safety assessment, contributing to the development of more robust safety mechanisms for VLMs. By enhancing the evaluation of multimodal vulnerabilities, Text2VLM plays a role in advancing the safe deployment of VLMs in diverse, real-world applications.

</details>


### [113] [Investigating Structural Pruning and Recovery Techniques for Compressing Multimodal Large Language Models: An Empirical Study](https://arxiv.org/abs/2507.20749)

*Yiran Huang, Lukas Thede, Massimiliano Mancini, Wenjia Xu, Zeynep Akata*

**Main category:** cs.CL

**Keywords:** Multimodal Large Language Models, Compression, Structural pruning, Recovery training, Supervised finetuning

**Relevance Score:** 9

**TL;DR:** This paper proposes a method to compress Multimodal Large Language Models (MLLMs) using structural pruning and recovery training, showing that effective performance can be maintained with significantly reduced training data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational and memory challenges of deploying MLLMs, this study aims to find efficient ways to compress these models while maintaining performance.

**Method:** The authors investigate two structural pruning approaches—layerwise and widthwise pruning—applied to MLLMs, and evaluate recovery training using limited data through supervised finetuning and knowledge distillation.

**Key Contributions:**

	1. Introduces structural pruning methods for MLLMs
	2. Shows effectiveness of recovery training with minimal data
	3. Demonstrates that widthwise pruning is superior in low-resource contexts.

**Result:** Widthwise pruning generally yields better performance under low-resource conditions, and finetuning only the multimodal projector proves sufficient at compression levels below 20%. Effective recovery can be achieved with as little as 5% of original training data.

**Limitations:** The study focuses primarily on parameter reduction without exploring the potential trade-offs in model interpretability or other performance metrics beyond accuracy.

**Conclusion:** This study provides practical insights for compressing MLLMs effectively, highlighting that model performance can be preserved while using less computational resources and training data.

**Abstract:** While Multimodal Large Language Models (MLLMs) demonstrate impressive capabilities, their substantial computational and memory requirements pose significant barriers to practical deployment. Current parameter reduction techniques primarily involve training MLLMs from Small Language Models (SLMs), but these methods offer limited flexibility and remain computationally intensive. To address this gap, we propose to directly compress existing MLLMs through structural pruning combined with efficient recovery training. Specifically, we investigate two structural pruning paradigms--layerwise and widthwise pruning--applied to the language model backbone of MLLMs, alongside supervised finetuning and knowledge distillation. Additionally, we assess the feasibility of conducting recovery training with only a small fraction of the available data. Our results show that widthwise pruning generally maintains better performance in low-resource scenarios with limited computational resources or insufficient finetuning data. As for the recovery training, finetuning only the multimodal projector is sufficient at small compression levels (< 20%). Furthermore, a combination of supervised finetuning and hidden-state distillation yields optimal recovery across various pruning levels. Notably, effective recovery can be achieved with as little as 5% of the original training data, while retaining over 95% of the original performance. Through empirical study on two representative MLLMs, i.e., LLaVA-v1.5-7B and Bunny-v1.0-3B, this study offers actionable insights for practitioners aiming to compress MLLMs effectively without extensive computation resources or sufficient data.

</details>


### [114] [Multilingual Self-Taught Faithfulness Evaluators](https://arxiv.org/abs/2507.20752)

*Carlo Alfano, Aymen Al Marjani, Zeno Jonke, Amin Mantrach, Saab Mansour, Marcello Federico*

**Main category:** cs.CL

**Keywords:** multilingual, faithfulness evaluation, large language models, cross-lingual transfer learning, synthetic data

**Relevance Score:** 9

**TL;DR:** The paper presents a framework for evaluating multilingual faithfulness in LLMs using synthetic data, minimizing the reliance on labeled data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current evaluation systems for large language models (LLMs) that struggle with information hallucination, especially in multilingual contexts where human-labeled data is scarce.

**Method:** The proposed framework learns from synthetic multilingual summarization data and utilizes cross-lingual transfer learning, comparing language-specific fine-tuning against mixed-language approaches.

**Key Contributions:**

	1. Developed a framework for multilingual faithfulness evaluation using synthetic data.
	2. Utilized cross-lingual transfer learning to reduce dependency on labeled data.
	3. Showed superior performance to existing multilingual evaluation baselines.

**Result:** The framework shows improvements over existing evaluation baselines, including state-of-the-art English evaluators and machine translation-based methods, demonstrating a link between LLM capabilities and evaluation task performance.

**Limitations:** The evaluation is limited to certain languages and may not generalize to all multilingual contexts.

**Conclusion:** Self-Taught Evaluators can effectively evaluate LLM outputs across multiple languages without needing extensive labeled datasets, enhancing multilingual LLM application reliability.

**Abstract:** The growing use of large language models (LLMs) has increased the need for automatic evaluation systems, particularly to address the challenge of information hallucination. Although existing faithfulness evaluation approaches have shown promise, they are predominantly English-focused and often require expensive human-labeled training data for fine-tuning specialized models. As LLMs see increased adoption in multilingual contexts, there is a need for accurate faithfulness evaluators that can operate across languages without extensive labeled data. This paper presents Self-Taught Evaluators for Multilingual Faithfulness, a framework that learns exclusively from synthetic multilingual summarization data while leveraging cross-lingual transfer learning. Through experiments comparing language-specific and mixed-language fine-tuning approaches, we demonstrate a consistent relationship between an LLM's general language capabilities and its performance in language-specific evaluation tasks. Our framework shows improvements over existing baselines, including state-of-the-art English evaluators and machine translation-based approaches.

</details>


### [115] [On The Role of Pretrained Language Models in General-Purpose Text Embeddings: A Survey](https://arxiv.org/abs/2507.20783)

*Meishan Zhang, Xin Zhang, Xinping Zhao, Shouzheng Huang, Baotian Hu, Min Zhang*

**Main category:** cs.CL

**Keywords:** text embeddings, pretrained language models, natural language processing, contrastive learning, multimodal integration

**Relevance Score:** 8

**TL;DR:** This survey reviews general-purpose text embeddings (GPTE) in the context of pretrained language models (PLMs), exploring their architecture, roles in enhancing NLP tasks, and future research directions.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of the development and effectiveness of GPTE due to the rise of PLMs in various NLP tasks.

**Method:** The paper surveys existing literature on GPTE architectures, examining the role of PLMs in embedding extraction, expressivity, training strategies, and beyond.

**Key Contributions:**

	1. Comprehensive overview of GPTE with PLMs
	2. Detailed examination of PLM roles in embedding development
	3. Identification of future research directions in GPTE

**Result:** Key findings include the capability of GPTE to support multilingualism, multimodal integration, and scenario-specific adaptation, alongside highlighting future research areas such as bias mitigation and cognitive extension.

**Limitations:** The paper primarily focuses on GPTE without delving deeply into specific applications or implementations.

**Conclusion:** This survey serves as a crucial resource for understanding GPTEs and their implications in future research in the NLP community.

**Abstract:** Text embeddings have attracted growing interest due to their effectiveness across a wide range of natural language processing (NLP) tasks, such as retrieval, classification, clustering, bitext mining, and summarization. With the emergence of pretrained language models (PLMs), general-purpose text embeddings (GPTE) have gained significant traction for their ability to produce rich, transferable representations. The general architecture of GPTE typically leverages PLMs to derive dense text representations, which are then optimized through contrastive learning on large-scale pairwise datasets. In this survey, we provide a comprehensive overview of GPTE in the era of PLMs, focusing on the roles PLMs play in driving its development. We first examine the fundamental architecture and describe the basic roles of PLMs in GPTE, i.e., embedding extraction, expressivity enhancement, training strategies, learning objectives, and data construction. Then, we describe advanced roles enabled by PLMs, such as multilingual support, multimodal integration, code understanding, and scenario-specific adaptation. Finally, we highlight potential future research directions that move beyond traditional improvement goals, including ranking integration, safety considerations, bias mitigation, structural information incorporation, and the cognitive extension of embeddings. This survey aims to serve as a valuable reference for both newcomers and established researchers seeking to understand the current state and future potential of GPTE.

</details>


### [116] [Automating Thematic Review of Prevention of Future Deaths Reports: Replicating the ONS Child Suicide Study using Large Language Models](https://arxiv.org/abs/2507.20786)

*Sam Osian, Arpan Dutta, Sahil Bhandari, Iain E. Buchan, Dan W. Joyce*

**Main category:** cs.CL

**Keywords:** language model, child suicide, coroners, text analysis, public health

**Relevance Score:** 9

**TL;DR:** The PFD Toolkit, an automated language-model pipeline, efficiently identifies child-suicide reports from coroner data, replicating the Office for National Statistics' manual analysis with higher detection rates and significant time savings.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the analysis of Prevention of Future Deaths reports by automating the identification and coding of relevant cases, specifically focusing on child-suicide instances.

**Method:** The study used an automated pipeline called PFD Toolkit that processes all PFD reports from July 2013 to November 2023, identifying cases of child suicides and coding them accordingly, thereby streamlining the review process.

**Key Contributions:**

	1. Development of the PFD Toolkit for automated analysis of coronial reports
	2. Demonstrated higher case identification rates compared to manual methods
	3. Significant reduction in processing time from months to minutes.

**Result:** The PFD Toolkit identified 72 child-suicide reports, nearly doubling the count compared to the Office for National Statistics, with a high level of agreement with clinical annotations (Cohen's kappa = 0.82).

**Limitations:** 

**Conclusion:** Automated LLM analysis can effectively replicate and enhance manual thematic reviews of coronial data, making insights extraction more efficient and reproducible for public health applications.

**Abstract:** Prevention of Future Deaths (PFD) reports, issued by coroners in England and Wales, flag systemic hazards that may lead to further loss of life. Analysis of these reports has previously been constrained by the manual effort required to identify and code relevant cases. In 2025, the Office for National Statistics (ONS) published a national thematic review of child-suicide PFD reports ($\leq$ 18 years), identifying 37 cases from January 2015 to November 2023 - a process based entirely on manual curation and coding. We evaluated whether a fully automated, open source "text-to-table" language-model pipeline (PFD Toolkit) could reproduce the ONS's identification and thematic analysis of child-suicide PFD reports, and assessed gains in efficiency and reliability. All 4,249 PFD reports published from July 2013 to November 2023 were processed via PFD Toolkit's large language model pipelines. Automated screening identified cases where the coroner attributed death to suicide in individuals aged 18 or younger, and eligible reports were coded for recipient category and 23 concern sub-themes, replicating the ONS coding frame. PFD Toolkit identified 72 child-suicide PFD reports - almost twice the ONS count. Three blinded clinicians adjudicated a stratified sample of 144 reports to validate the child-suicide screening. Against the post-consensus clinical annotations, the LLM-based workflow showed substantial to almost-perfect agreement (Cohen's $\kappa$ = 0.82, 95% CI: 0.66-0.98, raw agreement = 91%). The end-to-end script runtime was 8m 16s, transforming a process that previously took months into one that can be completed in minutes. This demonstrates that automated LLM analysis can reliably and efficiently replicate manual thematic reviews of coronial data, enabling scalable, reproducible, and timely insights for public health and safety. The PFD Toolkit is openly available for future research.

</details>


### [117] [Latent Inter-User Difference Modeling for LLM Personalization](https://arxiv.org/abs/2507.20849)

*Yilun Qiu, Tianhao Shi, Xiaoyan Zhao, Fengbin Zhu, Yang Zhang, Fuli Feng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Personalization, User Embedding, Sparse Autoencoder, Behavioral Signals

**Relevance Score:** 8

**TL;DR:** The paper introduces a framework called Difference-aware Embedding-based Personalization (DEP) that enhances LLM outputs by modeling inter-user differences without depending on language prompts, resulting in improved personalized content generation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a growing demand for personalized outputs from large language models (LLMs), yet existing methods mainly focus on individual user history and often miss inter-user differences which are crucial for effective personalization.

**Method:** DEP utilizes a framework that models user differences in the latent space rather than through language prompts, creating soft prompts by contrasting user embeddings with peers. A sparse autoencoder is employed to filter user-specific and difference-aware embeddings before integrating them into a frozen LLM.

**Key Contributions:**

	1. Introduction of the DEP framework for personalization
	2. Utilization of inter-user differences in latent space
	3. Demonstration of performance improvements over baseline methods

**Result:** Experiments on personalized review generation demonstrate that DEP outperforms baseline methods consistently across various metrics, indicating its efficacy in generating personalized content.

**Limitations:** 

**Conclusion:** DEP provides a novel approach to personalization by focusing on behavioral signals derived from inter-user comparisons, leading to more effective use of LLMs for personalized outputs.

**Abstract:** Large language models (LLMs) are increasingly integrated into users' daily lives, leading to a growing demand for personalized outputs. Previous work focuses on leveraging a user's own history, overlooking inter-user differences that are crucial for effective personalization. While recent work has attempted to model such differences, the reliance on language-based prompts often hampers the effective extraction of meaningful distinctions. To address these issues, we propose Difference-aware Embedding-based Personalization (DEP), a framework that models inter-user differences in the latent space instead of relying on language prompts. DEP constructs soft prompts by contrasting a user's embedding with those of peers who engaged with similar content, highlighting relative behavioral signals. A sparse autoencoder then filters and compresses both user-specific and difference-aware embeddings, preserving only task-relevant features before injecting them into a frozen LLM. Experiments on personalized review generation show that DEP consistently outperforms baseline methods across multiple metrics. Our code is available at https://github.com/SnowCharmQ/DEP.

</details>


### [118] [A survey of diversity quantification in natural language processing: The why, what, where and how](https://arxiv.org/abs/2507.20858)

*Louis Estève, Marie-Catherine de Marneffe, Nurit Melnik, Agata Savary, Olha Kanishcheva*

**Main category:** cs.CL

**Keywords:** diversity, Natural Language Processing, taxonomy, measurement, framework

**Relevance Score:** 6

**TL;DR:** This paper surveys the concept of diversity in NLP, proposing a unified taxonomy and framework to formalize the measurement of diversity across different applications in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Increased consideration of diversity in NLP motivated by inclusion, human-like linguistic behavior, and improved performance.

**Method:** Survey of articles from the ACL Anthology regarding diversity in NLP, leading to the proposal of a unified taxonomy using a framework from ecology and economy.

**Key Contributions:**

	1. Unified taxonomy for measuring diversity in NLP
	2. Application of ecological and economic frameworks to diversity
	3. Identification of trends through a systematized approach

**Result:** Diversity is found to be quantified in various ways across different NLP settings, leading to inconsistent terminology; a unified taxonomy is proposed.

**Limitations:** 

**Conclusion:** Formalizing diversity in NLP can enhance understanding and comparability of various approaches to measuring it.

**Abstract:** The concept of diversity has received increased consideration in Natural Language Processing (NLP) in recent years. This is due to various motivations like promoting and inclusion, approximating human linguistic behavior, and increasing systems' performance. Diversity has however often been addressed in an ad hoc manner in NLP, and with few explicit links to other domains where this notion is better theorized. We survey articles in the ACL Anthology from the past 6 years, with "diversity" or "diverse" in their title. We find a wide range of settings in which diversity is quantified, often highly specialized and using inconsistent terminology. We put forward a unified taxonomy of why, what on, where, and how diversity is measured in NLP. Diversity measures are cast upon a unified framework from ecology and economy (Stirling, 2007) with 3 dimensions of diversity: variety, balance and disparity. We discuss the trends which emerge due to this systematized approach. We believe that this study paves the way towards a better formalization of diversity in NLP, which should bring a better understanding of this notion and a better comparability between various approaches.

</details>


### [119] [Leveraging Open-Source Large Language Models for Clinical Information Extraction in Resource-Constrained Settings](https://arxiv.org/abs/2507.20859)

*Luc Builtjes, Joeran Bosma, Mathias Prokop, Bram van Ginneken, Alessa Hering*

**Main category:** cs.CL

**Keywords:** Clinical information extraction, Open-source LLMs, Zero-shot learning, Data privacy, Natural language processing

**Relevance Score:** 9

**TL;DR:** The study evaluates open-source generative LLMs for clinical information extraction, proposing a framework that shows competitive performance while addressing data privacy concerns.

**Read time:** 34 min

<details>
  <summary>Details</summary>

**Motivation:** To provide an effective and privacy-conscious solution for information extraction from unstructured medical reports written in domain-specific language.

**Method:** Evaluation of nine open-source generative LLMs on the DRAGON benchmark involving 28 clinical information extraction tasks in Dutch, using the framework 'llm_extractinator' for zero-shot model performance assessment.

**Key Contributions:**

	1. Development of the 'llm_extractinator' framework for clinical information extraction
	2. Evaluation of various open-source generative LLMs on clinical tasks
	3. Highlighting the importance of native-language processing for performance

**Result:** Several 14 billion parameter models achieved competitive results, with the Llama-3.3-70B model performing slightly better at increased computational cost; native-language processing was essential for maintaining performance.

**Limitations:** Focus on Dutch language tasks may limit generalizability, and larger models require significant computational resources.

**Conclusion:** Open-source LLMs can effectively and scalably extract clinical information, addressing privacy issues in healthcare applications, especially in low-resource settings.

**Abstract:** Medical reports contain rich clinical information but are often unstructured and written in domain-specific language, posing challenges for information extraction. While proprietary large language models (LLMs) have shown promise in clinical natural language processing, their lack of transparency and data privacy concerns limit their utility in healthcare. This study therefore evaluates nine open-source generative LLMs on the DRAGON benchmark, which includes 28 clinical information extraction tasks in Dutch. We developed \texttt{llm\_extractinator}, a publicly available framework for information extraction using open-source generative LLMs, and used it to assess model performance in a zero-shot setting. Several 14 billion parameter models, Phi-4-14B, Qwen-2.5-14B, and DeepSeek-R1-14B, achieved competitive results, while the bigger Llama-3.3-70B model achieved slightly higher performance at greater computational cost. Translation to English prior to inference consistently degraded performance, highlighting the need of native-language processing. These findings demonstrate that open-source LLMs, when used with our framework, offer effective, scalable, and privacy-conscious solutions for clinical information extraction in low-resource settings.

</details>


### [120] [Soft Injection of Task Embeddings Outperforms Prompt-Based In-Context Learning](https://arxiv.org/abs/2507.20906)

*Jungwon Park, Wonjong Rhee*

**Main category:** cs.CL

**Keywords:** In-Context Learning, Large Language Models, Task Embeddings, Attention Mechanisms, Soft Injection

**Relevance Score:** 9

**TL;DR:** This paper introduces Soft Injection of task embeddings, a method that enhances In-Context Learning (ICL) in Large Language Models (LLMs) by optimizing task performance and reducing memory usage without needing in-prompt demonstrations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiency of using multiple examples in prompting LLMs for task performance, and to explore more effective ways to convey task information.

**Method:** The study proposes a method called Soft Injection, which constructs task embeddings using few-shot ICL prompts and combines them with attention head activations using pre-optimized mixing parameters (soft head-selection parameters).

**Key Contributions:**

	1. Introduces Soft Injection for task embeddings in LLMs.
	2. Achieves significant performance improvements in various tasks compared to traditional ICL methods.
	3. Provides insights into the task-relevant roles of attention heads.

**Result:** The proposed method outperforms traditional 10-shot ICL approaches by 10.1%-13.9% across 12 LLMs and 57 different tasks, demonstrating improved task performance and reduced compute costs.

**Limitations:** 

**Conclusion:** Soft Injection offers a new approach to task conditioning in LLMs, enabling better performance and efficiency by leveraging activation space instead of prompt space.

**Abstract:** In-Context Learning (ICL) enables Large Language Models (LLMs) to perform tasks by conditioning on input-output examples in the prompt, without requiring any update in model parameters. While widely adopted, it remains unclear whether prompting with multiple examples is the most effective and efficient way to convey task information. In this work, we propose Soft Injection of task embeddings. The task embeddings are constructed only once using few-shot ICL prompts and repeatedly used during inference. Soft injection is performed by softly mixing task embeddings with attention head activations using pre-optimized mixing parameters, referred to as soft head-selection parameters. This method not only allows a desired task to be performed without in-prompt demonstrations but also significantly outperforms existing ICL approaches while reducing memory usage and compute cost at inference time. An extensive evaluation is performed across 57 tasks and 12 LLMs, spanning four model families of sizes from 4B to 70B. Averaged across 57 tasks, our method outperforms 10-shot ICL by 10.1%-13.9% across 12 LLMs. Additional analyses show that our method also serves as an insightful tool for analyzing task-relevant roles of attention heads, revealing that task-relevant head positions selected by our method transfer across similar tasks but not across dissimilar ones -- underscoring the task-specific nature of head functionality. Our soft injection method opens a new paradigm for reducing prompt length and improving task performance by shifting task conditioning from the prompt space to the activation space.

</details>


### [121] [MediQAl: A French Medical Question Answering Dataset for Knowledge and Reasoning Evaluation](https://arxiv.org/abs/2507.20917)

*Adrien Bazoge*

**Main category:** cs.CL

**Keywords:** MediQAl, medical question answering, language models, French language, multilingual resources

**Relevance Score:** 9

**TL;DR:** MediQAl is a French medical question answering dataset with 32,603 questions designed to evaluate language models' recall and reasoning in real-world clinical scenarios.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the capabilities of language models in the medical domain, particularly in French-speaking contexts, and address the lack of multilingual resources.

**Method:** The dataset includes three tasks: unique answer MCQs, multiple answer MCQs, and open-ended short-answer questions, with questions labeled for understanding or reasoning to analyze cognitive capabilities.

**Key Contributions:**

	1. Introduction of the MediQAl dataset for French medical question answering.
	2. Comprehensive evaluation across multiple tasks and 14 language models.
	3. Identification of performance gaps in medical language processing between recall and reasoning.

**Result:** The evaluation of 14 large language models reveals a significant performance gap between tasks focusing on factual recall versus reasoning.

**Limitations:** Dataset focused on French language only; performance might vary with different medical contexts beyond exams.

**Conclusion:** MediQAl provides a comprehensive benchmark for assessing language models in French medical question answering, filling a critical gap in multilingual datasets.

**Abstract:** This work introduces MediQAl, a French medical question answering dataset designed to evaluate the capabilities of language models in factual medical recall and reasoning over real-world clinical scenarios. MediQAl contains 32,603 questions sourced from French medical examinations across 41 medical subjects. The dataset includes three tasks: (i) Multiple-Choice Question with Unique answer, (ii) Multiple-Choice Question with Multiple answer, and (iii) Open-Ended Question with Short-Answer. Each question is labeled as Understanding or Reasoning, enabling a detailed analysis of models' cognitive capabilities. We validate the MediQAl dataset through extensive evaluation with 14 large language models, including recent reasoning-augmented models, and observe a significant performance gap between factual recall and reasoning tasks. Our evaluation provides a comprehensive benchmark for assessing language models' performance on French medical question answering, addressing a crucial gap in multilingual resources for the medical domain.

</details>


### [122] [FHSTP@EXIST 2025 Benchmark: Sexism Detection with Transparent Speech Concept Bottleneck Models](https://arxiv.org/abs/2507.20924)

*Roberto Labadie-Tamayo, Adrian Jaques Böck, Djordje Slijepčević, Xihui Chen, Andreas Babic, Matthias Zeppelzauer*

**Main category:** cs.CL

**Keywords:** sexism, social media, Twitter, machine learning, natural language processing

**Relevance Score:** 7

**TL;DR:** The paper presents solutions for the fifth Sexism Identification in Social Networks (EXIST) challenge by focusing on identifying and classifying sexism in social media posts, specifically through three subtasks involving Twitter data.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the widespread issue of sexism on social media platforms, the study aims to create effective models for identifying and classifying sexist content in textual posts.

**Method:** The paper implements three models: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa model, each addressing different subtasks related to sexism detection in tweets.

**Key Contributions:**

	1. Introduction of SCBM and SCBMT models for sexism identification
	2. Use of LLMs for generating human-interpretable representations
	3. Competitive results in benchmark evaluations for language models.

**Result:** SCBM uses human-interpretable bottleneck concepts for classification, SCBMT combines this with contextual embeddings for better performance, while XLM-RoBERTa achieves competitive rankings in the benchmark evaluations for identifying sexism.

**Limitations:** 

**Conclusion:** The models not only perform well in identifying sexism but also provide explanations for their classifications, and the incorporation of additional metadata improves results.

**Abstract:** Sexism has become widespread on social media and in online conversation. To help address this issue, the fifth Sexism Identification in Social Networks (EXIST) challenge is initiated at CLEF 2025. Among this year's international benchmarks, we concentrate on solving the first task aiming to identify and classify sexism in social media textual posts. In this paper, we describe our solutions and report results for three subtasks: Subtask 1.1 - Sexism Identification in Tweets, Subtask 1.2 - Source Intention in Tweets, and Subtask 1.3 - Sexism Categorization in Tweets. We implement three models to address each subtask which constitute three individual runs: Speech Concept Bottleneck Model (SCBM), Speech Concept Bottleneck Model with Transformer (SCBMT), and a fine-tuned XLM-RoBERTa transformer model. SCBM uses descriptive adjectives as human-interpretable bottleneck concepts. SCBM leverages large language models (LLMs) to encode input texts into a human-interpretable representation of adjectives, then used to train a lightweight classifier for downstream tasks. SCBMT extends SCBM by fusing adjective-based representation with contextual embeddings from transformers to balance interpretability and classification performance. Beyond competitive results, these two models offer fine-grained explanations at both instance (local) and class (global) levels. We also investigate how additional metadata, e.g., annotators' demographic profiles, can be leveraged. For Subtask 1.1, XLM-RoBERTa, fine-tuned on provided data augmented with prior datasets, ranks 6th for English and Spanish and 4th for English in the Soft-Soft evaluation. Our SCBMT achieves 7th for English and Spanish and 6th for Spanish.

</details>


### [123] [FRED: Financial Retrieval-Enhanced Detection and Editing of Hallucinations in Language Models](https://arxiv.org/abs/2507.20930)

*Likun Tan, Kuan-Wei Huang, Kevin Wu*

**Main category:** cs.CL

**Keywords:** large language models, factual reliability, error detection, financial text generation, synthetic dataset

**Relevance Score:** 9

**TL;DR:** This paper addresses hallucinations in large language models by proposing a method for detecting and editing erroneous content in model-generated responses, particularly in finance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The critical challenge of hallucinations in large language models affects factual reliability in high-stakes domains like finance, necessitating effective detection and editing mechanisms.

**Method:** The authors constructed a synthetic dataset with tagged errors in financial question-answering and fine-tuned four language models (Phi-4, Phi-4-mini, Qwen3-4B, Qwen3-14B) to enhance detection and editing of factual inaccuracies.

**Key Contributions:**

	1. Developed a synthetic dataset for training language models on domain-specific errors.
	2. Achieved significant performance improvements in detecting and editing factual inaccuracies in financial texts.
	3. Introduced a generalizable framework for enhancing the reliability of large language models.

**Result:** The fine-tuned Phi-4 demonstrated an 8% improvement in binary F1 score and a 30% increase in overall detection performance compared to OpenAI-o3, while the Phi-4-mini model showed competitive performance with minimal drops in accuracy.

**Limitations:** The results are primarily focused on the finance domain, which may limit the immediate applicability of the findings to other fields without further adaptation.

**Conclusion:** This work provides a practical solution to factual inconsistencies in financial text generation and offers a framework applicable across various domains for improving the reliability of large language models.

**Abstract:** Hallucinations in large language models pose a critical challenge for applications requiring factual reliability, particularly in high-stakes domains such as finance. This work presents an effective approach for detecting and editing factually incorrect content in model-generated responses based on the provided context. Given a user-defined domain-specific error taxonomy, we construct a synthetic dataset by inserting tagged errors into financial question-answering corpora and then fine-tune four language models, Phi-4, Phi-4-mini, Qwen3-4B, and Qwen3-14B, to detect and edit these factual inaccuracies. Our best-performing model, fine-tuned Phi-4, achieves an 8% improvement in binary F1 score and a 30% gain in overall detection performance compared to OpenAI-o3. Notably, our fine-tuned Phi-4-mini model, despite having only 4 billion parameters, maintains competitive performance with just a 2% drop in binary detection and a 0.1% decline in overall detection compared to OpenAI-o3. Our work provides a practical solution for detecting and editing factual inconsistencies in financial text generation while introducing a generalizable framework that can enhance the trustworthiness and alignment of large language models across diverse applications beyond finance. Our code and data are available at https://github.com/pegasi-ai/fine-grained-editting.

</details>


### [124] [Mind the Gap: Conformative Decoding to Improve Output Diversity of Instruction-Tuned Large Language Models](https://arxiv.org/abs/2507.20956)

*Max Peeperkorn, Tom Kouwenhoven, Dan Brown, Anna Jordanous*

**Main category:** cs.CL

**Keywords:** diversity gap, instruction-tuning, conformative decoding, large language models, narrative generation

**Relevance Score:** 8

**TL;DR:** The paper investigates how instruction-tuning affects the output diversity of large language models (LLMs) during narrative generation, revealing a substantial 'diversity gap' and proposing a new decoding strategy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of instruction-tuning on the diversity of outputs in narrative generation tasks with large language models.

**Method:** The paper analyzes the diversity loss during fine-tuning across the OLMo and OLMo 2 models, utilizing current diversity metrics.

**Key Contributions:**

	1. Analysis of diversity loss during fine-tuning of LLMs
	2. Identification of DPO's impact on output diversity
	3. Introduction of the conformative decoding strategy

**Result:** Instruction-tuning leads to significant decreases in output diversity, with DPO having the largest negative effect. The proposed conformative decoding method enhances diversity without compromising quality.

**Limitations:** 

**Conclusion:** Conformative decoding can effectively reintroduce diversity into the outputs of instruction-tuned language models while maintaining or improving output quality.

**Abstract:** Instruction-tuning large language models (LLMs) reduces the diversity of their outputs, which has implications for many tasks, particularly for creative tasks. This paper investigates the ``diversity gap'' for a writing prompt narrative generation task. This gap emerges as measured by current diversity metrics for various open-weight and open-source LLMs. The results show significant decreases in diversity due to instruction-tuning. We explore the diversity loss at each fine-tuning stage for the OLMo and OLMo 2 models to further understand how output diversity is affected. The results indicate that DPO has the most substantial impact on diversity. Motivated by these findings, we present a new decoding strategy, conformative decoding, which guides an instruct model using its more diverse base model to reintroduce output diversity. We show that conformative decoding typically increases diversity and even maintains or improves quality.

</details>


### [125] [Memorization in Fine-Tuned Large Language Models](https://arxiv.org/abs/2507.21009)

*Danil Savine, Muni Sreenivas Pydi, Jamal Atif, Olivier Cappé*

**Main category:** cs.CL

**Keywords:** memorization, large language models, fine-tuning, privacy, persistence in ML

**Relevance Score:** 9

**TL;DR:** This study explores memorization in fine-tuned large language models in the medical domain, highlighting trade-offs between performance and privacy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how fine-tuning affects memorization in LLMs, particularly in the privacy-sensitive medical field.

**Method:** Implemented membership inference attacks and generation tasks to analyze memorization effects with the PHEE dataset.

**Key Contributions:**

	1. Investigation of memorization in medical fine-tuned LLMs
	2. Analysis of impacts from weight matrices, perplexity, and LoRA ranks
	3. Recommendations for balancing model performance and privacy

**Result:** Key findings indicate that specific weight matrices significantly contribute to memorization, with lower perplexity correlating with increased memorization and higher LoRA ranks also increasing memorization, though with diminishing returns.

**Limitations:** 

**Conclusion:** The study provides insights for balancing model performance with privacy risks, suggesting responsible fine-tuning strategies for LLMs.

**Abstract:** This study investigates the mechanisms and factors influencing memorization in fine-tuned large language models (LLMs), with a focus on the medical domain due to its privacy-sensitive nature. We examine how different aspects of the fine-tuning process affect a model's propensity to memorize training data, using the PHEE dataset of pharmacovigilance events.   Our research employs two main approaches: a membership inference attack to detect memorized data, and a generation task with prompted prefixes to assess verbatim reproduction. We analyze the impact of adapting different weight matrices in the transformer architecture, the relationship between perplexity and memorization, and the effect of increasing the rank in low-rank adaptation (LoRA) fine-tuning.   Key findings include: (1) Value and Output matrices contribute more significantly to memorization compared to Query and Key matrices; (2) Lower perplexity in the fine-tuned model correlates with increased memorization; (3) Higher LoRA ranks lead to increased memorization, but with diminishing returns at higher ranks.   These results provide insights into the trade-offs between model performance and privacy risks in fine-tuned LLMs. Our findings have implications for developing more effective and responsible strategies for adapting large language models while managing data privacy concerns.

</details>


### [126] [Multi-Agent-as-Judge: Aligning LLM-Agent-Based Automated Evaluation with Multi-Dimensional Human Evaluation](https://arxiv.org/abs/2507.21028)

*Jiaju Chen, Yuxuan Lu, Xiaojie Wang, Huimin Zeng, Jing Huang, Jiri Gesi, Ying Xu, Bingsheng Yao, Dakuo Wang*

**Main category:** cs.CL

**Keywords:** LLM-as-a-judge, evaluation framework, multi-agent system, NLP applications, human evaluation

**Relevance Score:** 8

**TL;DR:** MAJ-EVAL is a Multi-Agent-as-Judge framework that uses LLM agents to simulate human evaluators, providing multi-dimensional feedback for NLP applications in education and medicine.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The evaluation of real-world NLP applications requires multiple dimensions and human perspectives, but real human evaluator resources are often scarce and costly.

**Method:** MAJ-EVAL automatically constructs multiple evaluator personas from relevant text documents and engages LLM agents in group debates to generate diverse evaluation feedback.

**Key Contributions:**

	1. Introduction of a framework (MAJ-EVAL) that constructs evaluator personas automatically
	2. Use of LLM agents to generate multi-dimensional feedback through debates
	3. Demonstrated improved alignment with human expert ratings in evaluations for educational and medical domains.

**Result:** MAJ-EVAL produces evaluation results that align better with human experts' ratings compared to traditional automated metrics and existing methods.

**Limitations:** Framework may require well-curated input documents to create accurate evaluator personas.

**Conclusion:** The proposed framework effectively addresses limitations in existing LLM-as-a-judge approaches, enhancing the evaluation process for NLP applications.

**Abstract:** Nearly all human work is collaborative; thus, the evaluation of real-world NLP applications often requires multiple dimensions that align with diverse human perspectives. As real human evaluator resources are often scarce and costly, the emerging "LLM-as-a-judge" paradigm sheds light on a promising approach to leverage LLM agents to believably simulate human evaluators. Yet, to date, existing LLM-as-a-judge approaches face two limitations: persona descriptions of agents are often arbitrarily designed, and the frameworks are not generalizable to other tasks. To address these challenges, we propose MAJ-EVAL, a Multi-Agent-as-Judge evaluation framework that can automatically construct multiple evaluator personas with distinct dimensions from relevant text documents (e.g., research papers), instantiate LLM agents with the personas, and engage in-group debates with multi-agents to Generate multi-dimensional feedback. Our evaluation experiments in both the educational and medical domains demonstrate that MAJ-EVAL can generate evaluation results that better align with human experts' ratings compared with conventional automated evaluation metrics and existing LLM-as-a-judge methods.

</details>


### [127] [Cheap Learning: Maximising Performance of Language Models for Social Data Science Using Minimal Data](https://arxiv.org/abs/2401.12295)

*Leonardo Castro-Gonzalez, Yi-Ling Chung, Hannak Rose Kirk, John Francis, Angus R. Williams, Pica Johansson, Jonathan Bright*

**Main category:** cs.CL

**Keywords:** machine learning, weak supervision, transfer learning, prompt engineering, social sciences

**Relevance Score:** 6

**TL;DR:** The paper reviews 'cheap' machine learning techniques—weak supervision, transfer learning, and prompt engineering—highlighting their effectiveness in social science applications.

**Read time:** 46 min

<details>
  <summary>Details</summary>

**Motivation:** To address the practical challenges in using machine learning for social sciences due to the lack of large labeled datasets.

**Method:** The article reviews and demonstrates weak supervision, transfer learning, and prompt engineering, particularly focusing on zero-shot prompting of large language models.

**Key Contributions:**

	1. Review of three cheap learning techniques
	2. Application demonstration across social science tasks
	3. Code repository for reproducibility

**Result:** All techniques showed good performance, especially zero-shot prompting, achieving high accuracy with minimal costs across various social science applications.

**Limitations:** 

**Conclusion:** The findings aim to encourage the use of these machine learning techniques in social sciences, providing tools for replication and further research.

**Abstract:** The field of machine learning has recently made significant progress in reducing the requirements for labelled training data when building new models. These `cheaper' learning techniques hold significant potential for the social sciences, where development of large labelled training datasets is often a significant practical impediment to the use of machine learning for analytical tasks. In this article we review three `cheap' techniques that have developed in recent years: weak supervision, transfer learning and prompt engineering. For the latter, we also review the particular case of zero-shot prompting of large language models. For each technique we provide a guide of how it works and demonstrate its application across six different realistic social science applications (two different tasks paired with three different dataset makeups). We show good performance for all techniques, and in particular we demonstrate how prompting of large language models can achieve high accuracy at very low cost. Our results are accompanied by a code repository to make it easy for others to duplicate our work and use it in their own research. Overall, our article is intended to stimulate further uptake of these techniques in the social sciences.

</details>


### [128] [Juru: Legal Brazilian Large Language Model from Reputable Sources](https://arxiv.org/abs/2403.18140)

*Roseval Malaquias Junior, Ramon Pires, Roseli Romero, Rodrigo Nogueira*

**Main category:** cs.CL

**Keywords:** large language models, domain specialization, pretraining, legal benchmarks, performance degradation

**Relevance Score:** 8

**TL;DR:** This paper presents Juru, a domain-specialized language model achieved by pretraining the Mistral-7B model on Brazilian legal data, showing improved legal performance but increased forgetting in unrelated domains.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the high compute costs of pretraining large language models while improving their performance on specific domains, particularly in legal contexts.

**Method:** The Mistral-7B model was specialized using 1.9 billion unique tokens from reputable Brazilian legal sources, followed by few-shot evaluations on legal and general knowledge test suites.

**Key Contributions:**

	1. Introduction of Juru, a specialized model for legal tasks
	2. Evidence that pretraining data selection can improve performance in specific domains
	3. Demonstration of the trade-off between specialization and forgetting unrelated knowledge

**Result:** Juru outperforms other models on legal benchmarks, despite the reduced amount of pretraining data, though it suffers from forgetting performance in general knowledge tasks.

**Limitations:** Increased forgetting observed in unrelated domains; potential need for balance in pretraining strategies.

**Conclusion:** Domain specialization through selective pretraining improves performance on specific tasks but can lead to performance degradation in unrelated areas, highlighting the trade-off in model training.

**Abstract:** The high compute cost associated with pretraining large language models limits their research. Two strategies have emerged to address this issue: domain specialization and pretraining with high-quality data. To explore these strategies, we specialized the Mistral-7B model with 1.9 billion unique tokens from reputable Brazilian legal sources and conducted few-shot evaluations on legal and general knowledge test suites. Our model, Juru, demonstrates the benefits of domain specialization by achieving improved performance on legal benchmarks, even with a reduced amount of pretraining data. However, this domain specialization through continued pretraining comes at the cost of increased forgetting in unrelated domains, as evidenced by performance degradation on general knowledge test suites in both Portuguese and English. This study contributes to the growing body of scientific evidence showing that pretraining data selection may enhance the performance of large language models, enabling the exploration of these models at a lower cost. Juru is publicly available at https://huggingface.co/roseval/Juru-7B .

</details>


### [129] [Learning to Clarify: Multi-turn Conversations with Action-Based Contrastive Self-Training](https://arxiv.org/abs/2406.00222)

*Maximillian Chen, Ruoxi Sun, Tomas Pfister, Sercan Ö. Arık*

**Main category:** cs.CL

**Keywords:** large language models, dialogue policy learning, disambiguation, human feedback, machine learning

**Relevance Score:** 9

**TL;DR:** This paper presents Action-Based Contrastive Self-Training (ACT), a method to improve dialogue policies in LLMs, focusing on conversational skills such as disambiguation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** LLM-based agents often struggle with ambiguity in conversations, lacking the ability to ask clarification questions, which hinders their conversational skills.

**Method:** The proposed ACT is a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO) that enables data-efficient learning of dialogue policies in multi-turn conversations.

**Key Contributions:**

	1. Introduction of Action-Based Contrastive Self-Training (ACT) for LLMs.
	2. Demonstration of ACT's effectiveness in data-efficient scenarios without needing action labels.
	3. Evaluation of LLMs' ability to recognize and reason about ambiguity in conversations.

**Result:** ACT shows improved performance in dialogue modeling over traditional methods like supervised fine-tuning, particularly in data-efficient scenarios.

**Limitations:** 

**Conclusion:** The efficacy of ACT suggests it can enhance LLM conversational agents' ability to manage ambiguity and improve their dialogue interactions in real-world tasks.

**Abstract:** Large language models (LLMs), optimized through human feedback, have rapidly emerged as a leading paradigm for developing intelligent conversational assistants. However, despite their strong performance across many benchmarks, LLM-based agents might still lack conversational skills such as disambiguation -- when they are faced with ambiguity, they often overhedge or implicitly guess users' true intents rather than asking clarification questions. Under task-specific settings, high-quality conversation samples are often limited, constituting a bottleneck for LLMs' ability to learn optimal dialogue action policies. We propose Action-Based Contrastive Self-Training (ACT), a quasi-online preference optimization algorithm based on Direct Preference Optimization (DPO), that enables data-efficient dialogue policy learning in multi-turn conversation modeling. We demonstrate ACT's efficacy under in data-efficient tuning scenarios, even when there is no action label available, using multiple real-world conversational tasks: tabular-grounded question-answering, machine reading comprehension, and AmbigSQL, a novel task for disambiguating information-seeking requests for complex SQL generation towards data analysis agents. Additionally, we propose evaluating LLMs' ability to function as conversational agents by examining whether they can implicitly recognize and reason about ambiguity in conversation. ACT demonstrates substantial conversation modeling improvements over standard tuning approaches like supervised fine-tuning and DPO.

</details>


### [130] [DoubleDipper: Improving Long-Context LLMs via Context Recycling](https://arxiv.org/abs/2406.13632)

*Arie Cattan, Alon Jacovi, Alex Fabrikant, Jonathan Herzig, Roee Aharoni, Hannah Rashkin, Dror Marcus, Avinatan Hassidim, Yossi Matias, Idan Szpektor, Avi Caciularu*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-Context Learning, Long Context QA

**Relevance Score:** 9

**TL;DR:** DoubleDipper is a novel In-Context-Learning method that improves long context QA tasks using few-shot examples generated from the same input context.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the sub-optimal performance of LLMs on tasks that require understanding of long contexts.

**Method:** DoubleDipper recycles contexts to create few-shot examples for long context QA tasks by generating additional query-output pairs; it enhances the prompts by identifying relevant paragraphs before the answer.

**Key Contributions:**

	1. Introduction of DoubleDipper for few-shot examples in long context QA
	2. Significant performance improvements across multiple LLMs
	3. Enhanced relevance attribution in generated answers

**Result:** The application of DoubleDipper resulted in an average improvement of +16 points across various QA datasets for multiple LLMs.

**Limitations:** 

**Conclusion:** The approach not only improves single-hop QA tasks but surprisingly enables LLMs to generalize to multi-hop long-context QA tasks as well.

**Abstract:** Despite recent advancements in Large Language Models (LLMs), their performance on tasks involving long contexts remains sub-optimal. In this work, we propose DoubleDipper, a novel In-Context-Learning method that automatically generates few-shot examples for long context QA tasks by recycling contexts. Specifically, given a long input context (1-3k tokens) and a query, we generate additional query-output pairs from the given context as few-shot examples, while introducing the context only once. This ensures that the demonstrations are leveraging the same context as the target query while only adding a small number of tokens to the prompt. We further enhance each demonstration by instructing the model to explicitly identify the relevant paragraphs before the answer, which improves performance while providing fine-grained attribution to the answer source. We apply our method on multiple LLMs and obtain substantial improvements (+16 absolute points on average across models) on various QA datasets with long context. Surprisingly, despite introducing only single-hop ICL examples, LLMs successfully generalize to multi-hop long-context QA using our approach.

</details>


### [131] [A Practice of Post-Training on Llama-3 70B with Optimal Selection of Additional Language Mixture Ratio](https://arxiv.org/abs/2409.06624)

*Ningyuan Xi, Yetao Wu, Kun Fan, Teng Chen, Qingqing Gu, Luo Ji*

**Main category:** cs.CL

**Keywords:** Continual Pre-Training, Large Language Models, Hyper-parameters, Chinese language, Machine Learning

**Relevance Score:** 8

**TL;DR:** This paper explores the Continual Pre-Training (CPT) of Llama-3 models to enhance Chinese language skills by optimizing hyper-parameters such as the Additional Language Mixture Ratio (ALMR) and Learning Rate (LR).

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically study the optimal mixture ratio of language corpora and its impact on model performance for continual pre-training of language models, particularly for enhancing capabilities in new domains.

**Method:** Continual Pre-Training was performed on Llama-3 8B and 70B models, focusing on optimizing the combination of hyper-parameters like ALMR and LR to improve performance.

**Key Contributions:**

	1. Analysis of hyper-parameter impact on performance
	2. Enhancement of Llama-3's Chinese capabilities
	3. Successful deployment of a pre-trained model in a real-world application

**Result:** The study demonstrates improved capabilities of the Llama-3 model in Chinese language understanding and performance in benchmarks related to math, coding, and emotional intelligence after careful tuning of hyper-parameters.

**Limitations:** 

**Conclusion:** By finding the optimal experimental setup, the model was successfully deployed in a real-life chat system, showing satisfactory performance.

**Abstract:** Large Language Models (LLM) often need to be Continual Pre-Trained (CPT) to obtain unfamiliar language skills or adapt to new domains. The huge training cost of CPT often asks for cautious choice of key hyper-parameters such as the mixture ratio of extra language or domain corpus. However, there is no systematic study that bridges the gap between the optimal mixture ratio and the actual model performance, and the gap between experimental scaling law and the actual deployment in the full model size. In this paper, we perform CPT on Llama-3 8B and 70B to enhance its Chinese ability. We study the optimal correlation between the Additional Language Mixture Ratio (ALMR) and the Learning Rate (LR) on the 8B size which directly indicates the optimal experimental setup. By thorough choice of hyper-parameter, and subsequent fine-tuning, the model capability is improved not only on the Chinese-related benchmark but also in some specific domains including math, coding, and emotional intelligence. We deploy the final 70B version of LLM on a real-life chat system which obtains satisfying performance.

</details>


### [132] [Real-time Factuality Assessment from Adversarial Feedback](https://arxiv.org/abs/2410.14651)

*Sanxing Chen, Yukun Huang, Bhuwan Dhingra*

**Main category:** cs.CL

**Keywords:** factuality, LLM, RAG, news evaluation, deception

**Relevance Score:** 9

**TL;DR:** This paper presents a novel evaluation method for factuality in news, highlighting how LLM-based detectors struggle with current events and proposing a pipeline that uses RAG to generate deceptive news variants for testing.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluations for news factuality yield high accuracies over time, but fail to assess a model's ability to reason about current events.

**Method:** A novel pipeline is developed leveraging natural language feedback from a RAG-based detector to create deceptive news variants that challenge LLMs.

**Key Contributions:**

	1. Development of a novel pipeline for generating deceptive news variants
	2. Highlighting the limitations of retrieval-free LLM detectors
	3. Identifying the important role of RAG in evaluating and generating news examples

**Result:** The iterative rewrite process significantly decreased the binary classification ROC-AUC by 17.5% for a strong RAG-based GPT-4o detector.

**Limitations:** Focused primarily on deceptive variant generation, broader application needs exploration.

**Conclusion:** RAG-based evaluation reveals crucial insights into the vulnerabilities of retrieval-free LLM detectors and helps identify deceitful patterns in news.

**Abstract:** We show that existing evaluations for assessing the factuality of news from conventional sources, such as claims on fact-checking websites, result in high accuracies over time for LLM-based detectors-even after their knowledge cutoffs. This suggests that recent popular false information from such sources can be easily identified due to its likely presence in pre-training/retrieval corpora or the emergence of salient, yet shallow, patterns in these datasets. Instead, we argue that a proper factuality evaluation dataset should test a model's ability to reason about current events by retrieving and reading related evidence. To this end, we develop a novel pipeline that leverages natural language feedback from a RAG-based detector to iteratively modify real-time news into deceptive variants that challenge LLMs. Our iterative rewrite decreases the binary classification ROC-AUC by an absolute 17.5 percent for a strong RAG-based GPT-4o detector. Our experiments reveal the important role of RAG in both evaluating and generating challenging news examples, as retrieval-free LLM detectors are vulnerable to unseen events and adversarial attacks, while feedback from RAG-based evaluation helps discover more deceitful patterns.

</details>


### [133] [Do Large Language Models Have an English Accent? Evaluating and Improving the Naturalness of Multilingual LLMs](https://arxiv.org/abs/2410.15956)

*Yanzhu Guo, Simone Conia, Zelin Zhou, Min Li, Saloni Potdar, Henry Xiao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multilinguality, Naturalness Metrics

**Relevance Score:** 8

**TL;DR:** This paper introduces new metrics to evaluate and improve the multilingual naturalness of Large Language Models (LLMs), which tend to exhibit English-centric biases in other languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited focus on the naturalness of multilingual LLM outputs, which often reflect English-centric patterns causing unnatural expressions in non-English languages.

**Method:** The authors propose automatic corpus-level metrics to assess lexical and syntactic naturalness in multilingual contexts, and also introduce an alignment method to enhance LLM performance in target languages.

**Key Contributions:**

	1. Novel metrics for assessing multilingual naturalness
	2. An alignment method for improving LLM outputs
	3. Evaluation of multilingual LLMs highlighting English biases

**Result:** Evaluation of state-of-the-art LLMs in French and Chinese using the new metrics shows significant English-influenced output patterns, while the proposed alignment method leads to improved naturalness without degrading general performance.

**Limitations:** 

**Conclusion:** The study emphasizes the necessity of multilingual assessment tools and alignment strategies for enhancing the output quality of LLMs in various languages and domains.

**Abstract:** Current Large Language Models (LLMs) are predominantly designed with English as the primary language, and even the few that are multilingual tend to exhibit strong English-centric biases. Much like speakers who might produce awkward expressions when learning a second language, LLMs often generate unnatural outputs in non-English languages, reflecting English-centric patterns in both vocabulary and grammar. Despite the importance of this issue, the naturalness of multilingual LLM outputs has received limited attention. In this paper, we address this gap by introducing novel automatic corpus-level metrics to assess the lexical and syntactic naturalness of LLM outputs in a multilingual context. Using our new metrics, we evaluate state-of-the-art LLMs on a curated benchmark in French and Chinese, revealing a tendency towards English-influenced patterns. To mitigate this issue, we also propose a simple and effective alignment method to improve the naturalness of an LLM in a target language and domain, achieving consistent improvements in naturalness without compromising the performance on general-purpose benchmarks. Our work highlights the importance of developing multilingual metrics, resources and methods for the new wave of multilingual LLMs.

</details>


### [134] [What is Wrong with Perplexity for Long-context Language Modeling?](https://arxiv.org/abs/2410.23771)

*Lizhe Fang, Yifei Wang, Zhaoyang Liu, Chenheng Zhang, Stefanie Jegelka, Jinyang Gao, Bolin Ding, Yisen Wang*

**Main category:** cs.CL

**Keywords:** long-context, large language models, perplexity, LongPPL, LongCE

**Relevance Score:** 9

**TL;DR:** This paper addresses the limitations of perplexity (PPL) in evaluating long-context inputs for LLMs, proposing a new metric called LongPPL that focuses on key tokens, and introduces LongCE loss for improved fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliability of perplexity (PPL) as an evaluation metric for long-context capabilities of large language models (LLMs) is questioned, leading to the development of better methods.

**Method:** The authors propose LongPPL, a novel evaluation metric that identifies key tokens important for long-context understanding, and introduce LongCE loss for fine-tuning with a focus on these key tokens.

**Key Contributions:**

	1. Introduction of LongPPL as a key-token focused evaluation metric for LLMs
	2. Development of LongCE loss for better fine-tuning
	3. Demonstration of strong correlation between LongPPL and long-context performance benchmarks

**Result:** LongPPL demonstrated a Pearson correlation of -0.96 with long-context benchmark performance, outperforming traditional PPL in predictive accuracy. LongCE also improved fine-tuning outcomes across benchmarks.

**Limitations:** 

**Conclusion:** The paper provides significant insights into PPL's limitations and presents effective solutions (LongPPL and LongCE) for evaluating and enhancing LLMs' long-context capabilities.

**Abstract:** Handling long-context inputs is crucial for large language models (LLMs) in tasks such as extended conversations, document summarization, and many-shot in-context learning. While recent approaches have extended the context windows of LLMs and employed perplexity (PPL) as a standard evaluation metric, PPL has proven unreliable for assessing long-context capabilities. The underlying cause of this limitation has remained unclear. In this work, we provide a comprehensive explanation for this issue. We find that PPL overlooks key tokens, which are essential for long-context understanding, by averaging across all tokens and thereby obscuring the true performance of models in long-context scenarios. To address this, we propose \textbf{LongPPL}, a novel metric that focuses on key tokens by employing a long-short context contrastive method to identify them. Our experiments demonstrate that LongPPL strongly correlates with performance on various long-context benchmarks (e.g., Pearson correlation of -0.96), significantly outperforming traditional PPL in predictive accuracy. Additionally, we introduce \textbf{LongCE} (Long-context Cross-Entropy) loss, a re-weighting strategy for fine-tuning that prioritizes key tokens, leading to consistent improvements across diverse benchmarks. In summary, these contributions offer deeper insights into the limitations of PPL and present effective solutions for accurately evaluating and enhancing the long-context capabilities of LLMs. Code is available at https://github.com/PKU-ML/LongPPL.

</details>


### [135] [Summarization of Opinionated Political Documents with Varied Perspectives](https://arxiv.org/abs/2411.04093)

*Nicholas Deas, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** political polarization, summarization models, opinionated news articles

**Relevance Score:** 6

**TL;DR:** This paper addresses increasing global partisan hostility and polarization with a new summarization task and dataset focused on capturing diverse political perspectives from opinionated news articles.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce polarization by creating models that accurately summarize diverse political perspectives, exposing users to alternative viewpoints.

**Method:** The authors introduce a dataset for summarizing different political perspectives from news articles and evaluate 11 summarization models across automatic and human assessments.

**Key Contributions:**

	1. Introduction of a novel dataset focused on political perspective summarization.
	2. Benchmarking of 11 summarization models with insights on their performance and limitations.
	3. Analysis of the relationship between extraction behavior and document features.

**Result:** The study finds that models, including recent ones like GPT-4o, perform well yet struggle to produce summaries that accurately reflect the intended perspectives.

**Limitations:** All models tested showed difficulty in generating faithful summaries of intended perspectives.

**Conclusion:** There is a need for improved summarization models that can better capture and represent diverse perspectives, addressing a gap in current AI summarization capabilities.

**Abstract:** Global partisan hostility and polarization has increased, and this polarization is heightened around presidential elections. Models capable of generating accurate summaries of diverse perspectives can help reduce such polarization by exposing users to alternative perspectives. In this work, we introduce a novel dataset and task for independently summarizing each political perspective in a set of passages from opinionated news articles. For this task, we propose a framework for evaluating different dimensions of perspective summary performance. We benchmark 11 summarization models and LLMs of varying sizes and architectures through both automatic and human evaluation. While recent models like GPT-4o perform well on this task, we find that all models struggle to generate summaries that are faithful to the intended perspective. Our analysis of summaries focuses on how extraction behavior is impacted by features of the input documents.

</details>


### [136] [Benchmarking Linguistic Diversity of Large Language Models](https://arxiv.org/abs/2412.10271)

*Yanzhu Guo, Guokan Shang, Chloé Clavel*

**Main category:** cs.CL

**Keywords:** Large Language Models, Linguistic Diversity, Evaluation Framework, Syntactic Diversity, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper proposes a framework for evaluating the linguistic diversity of Large Language Models (LLMs), addressing their vocabulary, syntax, and semantics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the neglect of linguistic diversity in the evaluation of LLMs, which can impact the richness of machine-generated language.

**Method:** A comprehensive framework for evaluating LLMs is proposed, focusing on lexical, syntactic, and semantic diversity, analyzed through benchmarking and case studies.

**Key Contributions:**

	1. Proposed a framework for evaluating LLMs from linguistic diversity perspectives.
	2. Benchmarking of multiple LLMs against this framework.
	3. In-depth analysis of syntactic diversity in LLM outputs.

**Result:** Benchmarking of several state-of-the-art LLMs reveals varying levels of linguistic diversity, with an in-depth case study highlighting significant differences in syntactic diversity.

**Limitations:** Limited to state-of-the-art LLMs, may not generalize to all models in practical applications.

**Conclusion:** Development and deployment choices critically affect the linguistic diversity of LLM outputs, suggesting a need for improved evaluation methods in LLM development.

**Abstract:** The development and evaluation of Large Language Models (LLMs) has primarily focused on their task-solving capabilities, with recent models even surpassing human performance in some areas. However, this focus often neglects whether machine-generated language matches the human level of diversity, in terms of vocabulary choice, syntactic construction, and expression of meaning, raising questions about whether the fundamentals of language generation have been fully addressed. This paper emphasizes the importance of examining the preservation of human linguistic richness by language models, given the concerning surge in online content produced or aided by LLMs. We propose a comprehensive framework for evaluating LLMs from various linguistic diversity perspectives including lexical, syntactic, and semantic dimensions. Using this framework, we benchmark several state-of-the-art LLMs across all diversity dimensions, and conduct an in-depth case study for syntactic diversity. Finally, we analyze how different development and deployment choices impact the linguistic diversity of LLM outputs.

</details>


### [137] [Critique of Impure Reason: Unveiling the reasoning behaviour of medical Large Language Models](https://arxiv.org/abs/2412.15748)

*Shamus Sim, Tyrone Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, explainable AI, healthcare, reasoning behaviour, machine learning

**Relevance Score:** 9

**TL;DR:** This paper explores the reasoning behaviour of medical Large Language Models (LLMs) with a focus on achieving explainable AI in healthcare. It surveys current approaches, proposes theoretical frameworks, and discusses challenges in transparency and trust between clinicians and AI models.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the surprising lack of studies on the reasoning behaviour of LLMs in the medical domain, emphasizing its importance for explainable AI and its implications for healthcare integration.

**Method:** The authors survey current state-of-the-art approaches for modeling and evaluating reasoning in medical LLMs and propose theoretical frameworks to enhance understanding for medical professionals and machine learning engineers.

**Key Contributions:**

	1. Proposed frameworks for understanding reasoning in medical LLMs
	2. Categorization of existing approaches in medical AI
	3. Identification of key challenges in developing reasoning models

**Result:** The study categorizes reasoning behaviours in medical LLMs and outlines frameworks that allow insights into the reasoning processes of these models, identifying open challenges in the field.

**Limitations:** The study may not cover all existing models or methodologies in the rapidly evolving field of medical AI and reasoning.

**Conclusion:** Increased transparency in medical ML models will boost trust from clinicians and patients, accelerating the integration and development of AI in the healthcare sector.

**Abstract:** Background: Despite the current ubiquity of Large Language Models (LLMs) across the medical domain, there is a surprising lack of studies which address their reasoning behaviour. We emphasise the importance of understanding reasoning behaviour as opposed to high-level prediction accuracies, since it is equivalent to explainable AI (XAI) in this context. In particular, achieving XAI in medical LLMs used in the clinical domain will have a significant impact across the healthcare sector. Results: Therefore, in this work, we adapt the existing concept of reasoning behaviour and articulate its interpretation within the specific context of medical LLMs. We survey and categorise current state-of-the-art approaches for modeling and evaluating reasoning reasoning in medical LLMs. Additionally, we propose theoretical frameworks which can empower medical professionals or machine learning engineers to gain insight into the low-level reasoning operations of these previously obscure models. We also outline key open challenges facing the development of Large Reasoning Models. Conclusion: The subsequent increased transparency and trust in medical machine learning models by clinicians as well as patients will accelerate the integration, application as well as further development of medical AI for the healthcare system as a whole.

</details>


### [138] [Align Attention Heads Before Merging Them: An Effective Way for Converting MHA to GQA](https://arxiv.org/abs/2412.20677)

*Qingyun Jin, Xiaohui Song, Feng Zhou, Zengchang Qin*

**Main category:** cs.CL

**Keywords:** large language models, attention mechanisms, Procrustes analysis, parameter pruning, natural language processing

**Relevance Score:** 8

**TL;DR:** This paper proposes a method to convert multi-head attention to grouped-query attention in large language models, improving efficiency and maintaining performance.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the degradation of inference throughput in large models due to increasing size and input sequence length, a new method for attention mechanisms is necessary.

**Method:** The proposed method employs Procrustes analysis on attention heads for enhanced similarity and computational invariance, followed by $	ext{L}_0$ regularization to prune redundant parameters and adapt to the GQA framework.

**Key Contributions:**

	1. Developed a cost-effective method for converting MHA to GQA.
	2. Used Procrustes analysis to enhance head similarity and computational invariance.
	3. Implemented $	ext{L}_0$ regularization for parameter pruning.

**Result:** Experimental results indicate a successful compression of up to 87.5% of KV heads in the LLaMA2-7B model and 75% in the Sheared-LLaMA-1.3B model with only acceptable performance degradation.

**Limitations:** The performance degradation is acknowledged but deemed acceptable within certain bounds of compression.

**Conclusion:** The method effectively reduces the complexity of attention mechanisms in large language models while maintaining performance levels.

**Abstract:** Large language models (LLMs) have demonstrated exceptional performance across diverse natural language processing tasks. However, as the model size and the input sequence's length increase, the linearly increasing key-value (KV) cache significantly degrades inference throughput. Therefore, grouped-query attention (GQA), as an alternative to multi-head attention (MHA), has been widely introduced into LLMs. In this work, we propose a cost-effective method for converting MHA into GQA with any compression ratio of KV heads. The key point of our method lies in the application of Procrustes analysis to the attention heads, which enhances the similarity among attention heads while preserving computational invariance, thereby improving the model's post-training performance. Subsequently, we employ $\mathit{L_0}$ regularization to prune redundant parameters. The model after pruning can be adapted to the standard GQA framework. Experimental results show that our strategy can compress up to 87.5\% KV heads of LLaMA2-7B model and 75\% KV heads of Sheared-LLaMA-1.3B with acceptable performance degradation. Our code is released at https://github.com/fpcsong/mha2gqa.

</details>


### [139] [Large Language Models Are Human-Like Internally](https://arxiv.org/abs/2502.01615)

*Tatsuki Kuribayashi, Yohei Oseki, Souhaib Ben Taieb, Kentaro Inui, Timothy Baldwin*

**Main category:** cs.CL

**Keywords:** cognitive modeling, interpretability, language models, human reading behavior, neurophysiology

**Relevance Score:** 8

**TL;DR:** This paper argues that larger language models align better with human reading behavior than previously thought, particularly when analyzing internal layers instead of just the final layers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the notion that larger language models are cognitively implausible and to show that earlier conclusions were biased due to focusing only on final layers.

**Method:** The authors analyze next-word probabilities from internal layers of larger language models and compare these with human reading behavior through various behavioral and neurophysiological measures.

**Key Contributions:**

	1. Reevaluation of cognitive plausibility of larger LMs.
	2. Demonstrated alignment of internal LMs with human sentence processing.
	3. Identified relationship between LM layers and human reading metrics.

**Result:** The analysis shows significant alignment between internal layer outputs and human reading data, suggesting that previous criticisms of larger models' cognitive plausibility are unfounded.

**Limitations:** 

**Conclusion:** The findings indicate a nuanced relationship between LM layers and reading behaviors, implying potential for further interdisciplinary research in mechanistic interpretability and cognitive modeling.

**Abstract:** Recent cognitive modeling studies have reported that larger language models (LMs) exhibit a poorer fit to human reading behavior (Oh and Schuler, 2023b; Shain et al., 2024; Kuribayashi et al., 2024), leading to claims of their cognitive implausibility. In this paper, we revisit this argument through the lens of mechanistic interpretability and argue that prior conclusions were skewed by an exclusive focus on the final layers of LMs. Our analysis reveals that next-word probabilities derived from internal layers of larger LMs align with human sentence processing data as well as, or better than, those from smaller LMs. This alignment holds consistently across behavioral (self-paced reading times, gaze durations, MAZE task processing times) and neurophysiological (N400 brain potentials) measures, challenging earlier mixed results and suggesting that the cognitive plausibility of larger LMs has been underestimated. Furthermore, we first identify an intriguing relationship between LM layers and human measures: earlier layers correspond more closely with fast gaze durations, while later layers better align with relatively slower signals such as N400 potentials and MAZE processing times. Our work opens new avenues for interdisciplinary research at the intersection of mechanistic interpretability and cognitive modeling.

</details>


### [140] [LIMO: Less is More for Reasoning](https://arxiv.org/abs/2502.03387)

*Yixin Ye, Zhen Huang, Yang Xiao, Ethan Chern, Shijie Xia, Pengfei Liu*

**Main category:** cs.CL

**Keywords:** large language models, reasoning, fine-tuning, LIMO Hypothesis, out-of-distribution generalization

**Relevance Score:** 8

**TL;DR:** This paper introduces the LIMO model, demonstrating that complex reasoning in LLMs can be achieved with minimal training data, highlighting a novel Less-Is-More Reasoning Hypothesis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that large amounts of training data are necessary for complex reasoning in large language models.

**Method:** The LIMO model employs simple supervised fine-tuning with only a small percentage of the training data used in prior models.

**Key Contributions:**

	1. Introduction of the LIMO model that requires less training data for effective reasoning
	2. Demonstration of strong out-of-distribution generalization
	3. Proposal of the Less-Is-More Reasoning Hypothesis for LLMs

**Result:** LIMO achieves 63.3% accuracy on AIME24 and 95.6% on MATH500, significantly outperforming previous methods while using only 1% of the training data.

**Limitations:** The reliance on effective post-training examples may not generalize to all types of reasoning tasks.

**Conclusion:** The Less-Is-More Reasoning Hypothesis posits that sophisticated reasoning can arise from strategically designed examples, depending more on the model's pre-trained knowledge and the cognitive nature of the examples than on the task complexity.

**Abstract:** We challenge the prevailing assumption that complex reasoning in large language models (LLMs) necessitates massive training data. We demonstrate that sophisticated mathematical reasoning can emerge with only a few examples. Specifically, through simple supervised fine-tuning, our model, LIMO, achieves 63.3\% accuracy on AIME24 and 95.6\% on MATH500, surpassing previous fine-tuned models (6.5\% on AIME24, 59.2\% on MATH500) while using only 1\% of the training data required by prior approaches. Furthermore, LIMO exhibits strong out-of-distribution generalization, achieving a 45.8\% absolute improvement across diverse benchmarks, outperforming models trained on 100x more data. Synthesizing these findings, we propose the Less-Is-More Reasoning Hypothesis (LIMO Hypothesis): In foundation models where domain knowledge has been comprehensively encoded during pre-training, sophisticated reasoning can emerge through minimal but strategically designed demonstrations of cognitive processes. This hypothesis suggests that the threshold for eliciting complex reasoning is not dictated by task complexity but rather by two key factors: (1) the completeness of the model's pre-trained knowledge base and (2) the effectiveness of post-training examples in serving as "cognitive templates" that guide reasoning.

</details>


### [141] [Improving Similar Case Retrieval Ranking Performance By Revisiting RankSVM](https://arxiv.org/abs/2502.11131)

*Yuqi Liu, Yan Zheng*

**Main category:** cs.CL

**Keywords:** Legal AI, Similar Case Retrieval, RankSVM, Machine Learning, Legal Datasets

**Relevance Score:** 2

**TL;DR:** The paper explores improving legal case retrieval performance using a RankSVM classifier to enhance ranking rather than relying solely on language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in similar case retrieval in legal AI by improving the ranking of retrieved cases.

**Method:** The authors employed a RankSVM (pairwise method) to replace the fully connected layer in existing models, testing it on the LeCaRDv1 and LeCaRDv2 datasets.

**Key Contributions:**

	1. Introduced RankSVM for improving ranking in legal case retrieval
	2. Demonstrated effectiveness on LeCaRDv1 and LeCaRDv2 datasets
	3. Provided code available for public use for further research.

**Result:** RankSVM generally improved retrieval performance on LeCaRDv1 and LeCaRDv2 datasets and helped mitigate overfitting due to class imbalance.

**Limitations:** 

**Conclusion:** Using RankSVM enhances the performance of similar case retrieval by optimizing ranking and is beneficial in handling class imbalances.

**Abstract:** Given the rapid development of Legal AI, a lot of attention has been paid to one of the most important legal AI tasks--similar case retrieval, especially with language models to use. In our paper, however, we try to improve the ranking performance of current models from the perspective of learning to rank instead of language models. Specifically, we conduct experiments using a pairwise method--RankSVM as the classifier to substitute a fully connected layer, combined with commonly used language models on similar case retrieval datasets LeCaRDv1 and LeCaRDv2. We finally come to the conclusion that RankSVM could generally help improve the retrieval performance on the LeCaRDv1 and LeCaRDv2 datasets compared with original classifiers by optimizing the precise ranking. It could also help mitigate overfitting owing to class imbalance. Our code is available in https://github.com/liuyuqi123study/RankSVM_for_SLR

</details>


### [142] [FactReasoner: A Probabilistic Approach to Long-Form Factuality Assessment for Large Language Models](https://arxiv.org/abs/2502.18573)

*Radu Marinescu, Debarun Bhattacharjya, Junkyu Lee, Tigran Tchrakian, Javier Carnerero Cano, Yufang Hou, Elizabeth Daly, Alessandra Pascale*

**Main category:** cs.CL

**Keywords:** factuality assessment, large language models, probabilistic reasoning

**Relevance Score:** 9

**TL;DR:** FactReasoner is a new tool for assessing the factual correctness of generated long-form responses using probabilistic reasoning and context retrieval.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the issue of factual correctness in generated content from large language models, making them more reliable for real-world applications.

**Method:** FactReasoner decomposes responses into atomic units, retrieves relevant contextual information, and uses probabilistic reasoning to evaluate the factuality of these units against the contexts.

**Key Contributions:**

	1. Introduction of FactReasoner for factual assessment
	2. Use of probabilistic reasoning in evaluating factuality
	3. Experimental validation showing improved performance over existing methods.

**Result:** FactReasoner shows significant improvement over current state-of-the-art prompt-based methods in both factual precision and recall on benchmark datasets.

**Limitations:** 

**Conclusion:** The proposed FactReasoner provides a robust method for evaluating the factual correctness of long-form content from LLMs, enhancing their usability in scenarios requiring factual accuracy.

**Abstract:** Large language models (LLMs) have demonstrated vast capabilities on generative tasks in recent years, yet they struggle with guaranteeing the factual correctness of the generated content. This makes these models unreliable in realistic situations where factually accurate responses are expected. In this paper, we propose FactReasoner, a new factuality assessor that relies on probabilistic reasoning to assess the factuality of a long-form generated response. Specifically, FactReasoner decomposes the response into atomic units, retrieves relevant contexts for them from an external knowledge source, and constructs a joint probability distribution over the atoms and contexts using probabilistic encodings of the logical relationships (entailment, contradiction) between the textual utterances corresponding to the atoms and contexts. FactReasoner then computes the posterior probability of whether atomic units in the response are supported by the retrieved contexts. Our experiments on labeled and unlabeled benchmark datasets demonstrate clearly that FactReasoner improves considerably over state-of-the-art prompt-based approaches in terms of both factual precision and recall.

</details>


### [143] [In Prospect and Retrospect: Reflective Memory Management for Long-term Personalized Dialogue Agents](https://arxiv.org/abs/2503.08026)

*Zhen Tan, Jun Yan, I-Hung Hsu, Rujun Han, Zifeng Wang, Long T. Le, Yiwen Song, Yanfei Chen, Hamid Palangi, George Lee, Anand Iyer, Tianlong Chen, Huan Liu, Chen-Yu Lee, Tomas Pfister*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reflective Memory Management, dialogue systems

**Relevance Score:** 9

**TL;DR:** This paper introduces Reflective Memory Management (RMM), a novel mechanism designed to enhance long-term dialogue agents by improving memory retention and retrieval through dynamic summarization and reinforcement learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The inability of LLMs to retain and retrieve relevant information from long-term interactions limits their effectiveness in personalized applications.

**Method:** Reflective Memory Management (RMM) integrates two types of reflections: Prospective Reflection for dynamic summarization across interaction granularities, and Retrospective Reflection for iterative retrieval refinement using online reinforcement learning.

**Key Contributions:**

	1. Introduction of Reflective Memory Management (RMM)
	2. Implementation of Prospective and Retrospective Reflections
	3. Demonstrated improvements on multiple metrics and benchmarks

**Result:** Experiments demonstrate that RMM improves accuracy by over 10% compared to the baseline on the LongMemEval dataset.

**Limitations:** 

**Conclusion:** RMM provides a more effective approach for managing memory in long-term dialogue systems, addressing significant challenges in current models.

**Abstract:** Large Language Models (LLMs) have made significant progress in open-ended dialogue, yet their inability to retain and retrieve relevant information from long-term interactions limits their effectiveness in applications requiring sustained personalization. External memory mechanisms have been proposed to address this limitation, enabling LLMs to maintain conversational continuity. However, existing approaches struggle with two key challenges. First, rigid memory granularity fails to capture the natural semantic structure of conversations, leading to fragmented and incomplete representations. Second, fixed retrieval mechanisms cannot adapt to diverse dialogue contexts and user interaction patterns. In this work, we propose Reflective Memory Management (RMM), a novel mechanism for long-term dialogue agents, integrating forward- and backward-looking reflections: (1) Prospective Reflection, which dynamically summarizes interactions across granularities-utterances, turns, and sessions-into a personalized memory bank for effective future retrieval, and (2) Retrospective Reflection, which iteratively refines the retrieval in an online reinforcement learning (RL) manner based on LLMs' cited evidence. Experiments show that RMM demonstrates consistent improvement across various metrics and benchmarks. For example, RMM shows more than 10% accuracy improvement over the baseline without memory management on the LongMemEval dataset.

</details>


### [144] [Data Caricatures: On the Representation of African American Language in Pretraining Corpora](https://arxiv.org/abs/2503.10789)

*Nicholas Deas, Blake Vente, Amith Ananthram, Jessica A. Grieser, Desmond Patton, Shana Kleiner, James Shepard, Kathleen McKeown*

**Main category:** cs.CL

**Keywords:** African American Language, language representation, pretraining corpora, natural language processing, bias in machine learning

**Relevance Score:** 6

**TL;DR:** This paper evaluates the representation of African American Language in open-source pretraining corpora and highlights significant underrepresentation and potential biases in language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the quantity and quality of African American Language representation in pretraining datasets used for language models, given the demographic disparities in language representation.

**Method:** Quantitative experiments, human judgments, and qualitative analyses of 12 open-source pretraining corpora, focusing on the sources and naturalness of AAL texts.

**Key Contributions:**

	1. Quantitative assessment of AAL representation in language training data
	2. Identification of potential biases in language model outputs
	3. Recommendations for improving AAL representation in corpora

**Result:** AAL representation in the corpora is extremely low, ranging from 0.007% to 0.18%, and a significant portion of AAL texts may perpetuate harmful stereotypes, with automated filters favoring White Mainstream English.

**Limitations:** The study is limited to 12 pretraining corpora and may not comprehensively capture all aspects of AAL representation.

**Conclusion:** The study highlights a critical gap in representation of AAL in language model training datasets, suggesting the need for more inclusive data practices to prevent biases.

**Abstract:** With a combination of quantitative experiments, human judgments, and qualitative analyses, we evaluate the quantity and quality of African American Language (AAL) representation in 12 predominantly English, open-source pretraining corpora. We specifically focus on the sources, variation, and naturalness of included AAL texts representing the AAL-speaking community. We find that AAL is underrepresented in all evaluated pretraining corpora compared to US demographics, constituting as few as 0.007% and at most 0.18% of documents. We also find that more than 25% of AAL texts in C4 may be perceived as inappropriate for LLMs to generate and to reinforce harmful stereotypes. Finally, we find that most automated filters are more likely to conserve White Mainstream English (WME) texts over AAL in pretraining corpora.

</details>


### [145] [Automating Mathematical Proof Generation Using Large Language Model Agents and Knowledge Graphs](https://arxiv.org/abs/2503.11657)

*Vincent Li, Tim Knappe, Yule Fu, Kevin Han, Kevin Zhu*

**Main category:** cs.CL

**Keywords:** large language models, theorem proving, knowledge graphs, automated reasoning, natural language processing

**Relevance Score:** 8

**TL;DR:** KG-Prover enhances LLMs for automated theorem proving by using knowledge graphs to improve performance in mathematical proof construction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in automated theorem proving, such as identifying key mathematical concepts and formalizing proofs, by leveraging knowledge graphs.

**Method:** A novel framework, KG-Prover, incorporates knowledge graphs from mathematical texts into LLMs, allowing them to better understand and formalize mathematical proofs without additional finetuning.

**Key Contributions:**

	1. Introduction of KG-Prover for mathematical proof construction
	2. Demonstration of significant performance improvements in LLMs using knowledge graphs
	3. Validation of effectiveness on multiple benchmark datasets without the need for additional finetuning.

**Result:** KG-Prover improves the performance of general-purpose LLMs by up to 21% on the miniF2F-test and shows consistent enhancements of 2-11% across multiple datasets.

**Limitations:** 

**Conclusion:** This research demonstrates a promising framework for enhancing natural language proofs through the integration of knowledge graphs, leading to significant improvements in LLM performance.

**Abstract:** Large language models have demonstrated remarkable capabilities in natural language processing tasks requiring multi-step logical reasoning capabilities, such as automated theorem proving. However, challenges persist within theorem proving, such as the identification of key mathematical concepts, understanding their interrelationships, and formalizing proofs correctly within natural language. We present KG-prover, a novel framework that leverages knowledge graphs mined from reputable mathematical texts to augment general-purpose LLMs to construct and formalize mathematical proofs. We also study the effects of scaling graph-based, test-time compute using KG-Prover, demonstrating significant performance improvements over baselines across multiple datasets. General-purpose LLMs improve up to 21\% on miniF2F-test when combined with KG-Prover, with consistent improvements ranging from 2-11\% on the ProofNet, miniF2F-test, and MUSTARD datasets without additional scaling. Furthermore, KG-Prover with o4-mini achieves over 50% miniF2F-test. This work provides a promising approach for augmenting natural language proof reasoning with knowledge graphs without the need for additional finetuning.

</details>


### [146] [Understanding Common Ground Misalignment in Goal-Oriented Dialog: A Case-Study with Ubuntu Chat Logs](https://arxiv.org/abs/2503.12370)

*Rupak Sarkar, Neha Srikanth, Taylor Hudson, Rachel Rudinger, Claire Bonial, Philip Resnik*

**Main category:** cs.CL

**Keywords:** Conversational Grounding, Task-Oriented Conversations, Conversational Friction

**Relevance Score:** 8

**TL;DR:** This paper explores the impact of conversational grounding on task-oriented conversations, particularly in the context of the Ubuntu IRC dataset.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the gap in understanding how conversational grounding affects task success in technical discussions.

**Method:** Analysis of the Ubuntu IRC dataset to identify instances of conversational friction and their correlation with task success.

**Key Contributions:**

	1. Introduces the concept of conversational friction related to common ground.
	2. Demonstrates the correlation between conversational alignment and task success.
	3. Evaluates the capabilities of LLMs in identifying conversational issues.

**Result:** Disruptions in conversational flow are linked to misalignment in common ground, which correlates with task success; LLMs can identify overt but not nuanced cases of conversational friction.

**Limitations:** The study is limited to the Ubuntu IRC dataset and may not generalize to all task-oriented conversations.

**Conclusion:** Improving understanding of conversational grounding can enhance outcomes in task-oriented interactions, particularly in technical environments.

**Abstract:** While it is commonly accepted that maintaining common ground plays a role in conversational success, little prior research exists connecting conversational grounding to success in task-oriented conversations. We study failures of grounding in the Ubuntu IRC dataset, where participants use text-only communication to resolve technical issues. We find that disruptions in conversational flow often stem from a misalignment in common ground, driven by a divergence in beliefs and assumptions held by participants. These disruptions, which we call conversational friction, significantly correlate with task success. We find that although LLMs can identify overt cases of conversational friction, they struggle with subtler and more context-dependent instances requiring pragmatic or domain-specific reasoning.

</details>


### [147] [Enhancing LLM Reasoning with Iterative DPO: A Comprehensive Empirical Investigation](https://arxiv.org/abs/2503.12854)

*Songjun Tu, Jiahao Lin, Xiangyu Tian, Qichao Zhang, Linjing Li, Yuqian Fu, Nan Xu, Wei He, Xiangyuan Lan, Dongmei Jiang, Dongbin Zhao*

**Main category:** cs.CL

**Keywords:** Direct Preference Optimization, Reinforcement Learning, Large Language Models, Mathematical Reasoning, Cost-Effective Learning

**Relevance Score:** 9

**TL;DR:** This paper explores Direct Preference Optimization (DPO) as a cost-effective alternative to reinforcement learning (RL) for improving the reasoning capabilities of large language models (LLMs).

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Growing computational costs of reinforcement learning methods have prompted the search for more efficient alternatives for improving LLM reasoning.

**Method:** The study evaluates the effectiveness of DPO through a single round of coarse filtering to enhance mathematical reasoning performance and introduces an iterative enhancement framework for mutual improvement of the generator and reward model.

**Key Contributions:**

	1. Introduces Direct Preference Optimization (DPO) as an alternative to RL for LLM self-improvement.
	2. Demonstrates significant performance enhancements in mathematical reasoning through DPO with coarse filtering.
	3. Establishes an iterative framework for mutual enhancement of the generator and reward model.

**Result:** DPO significantly boosts mathematical reasoning performance for strong base models and achieves RL-level performance with lower computational cost using simple verifiable rewards.

**Limitations:** 

**Conclusion:** DPO emerges as a scalable, resource-efficient solution to enhance LLM reasoning, particularly in scenarios with limited computational resources.

**Abstract:** Recent advancements in post-training methodologies for large language models (LLMs) have highlighted reinforcement learning (RL) as a critical component for enhancing reasoning. However, the substantial computational costs associated with RL-based approaches have led to growing interest in alternative paradigms, such as Direct Preference Optimization (DPO). In this study, we investigate the effectiveness of DPO in facilitating self-improvement for LLMs through iterative preference-based learning. We demonstrate that a single round of DPO with coarse filtering significantly enhances mathematical reasoning performance, particularly for strong base model. Furthermore, we design an iterative enhancement framework for both the generator and the reward model (RM), enabling their mutual improvement through online interaction across multiple rounds of DPO. Finally, with simple verifiable rewards, our model DPO-VP achieves RL-level performance with significantly lower computational overhead. These findings highlight DPO as a scalable and cost-effective alternative to RL, offering a practical solution for enhancing LLM reasoning in resource-constrained situations.

</details>


### [148] [Navigating the Risks of Using Large Language Models for Text Annotation in Social Science Research](https://arxiv.org/abs/2503.22040)

*Hao Lin, Yongjun Zhang*

**Main category:** cs.CL

**Keywords:** large language models, text classification, automated textual analysis, social movement studies, epistemic risks

**Relevance Score:** 8

**TL;DR:** The paper evaluates the use of large language models (LLMs) in automated textual analysis for social movement studies and proposes a framework for their effective use in text classification tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential and risks of using LLMs in computational social science, particularly in text classification, and to provide guidelines for their use.

**Method:** A systematic evaluation of LLMs for text classification, proposing a framework for integrating LLMs as coding decision-makers or assistants in social science research.

**Key Contributions:**

	1. Proposed a framework for using LLMs in text annotation tasks.
	2. Systematic evaluation of LLMs' validity and reliability.
	3. Guidelines for addressing epistemic risks in research.

**Result:** Proposed a framework for utilizing LLMs in text annotation, including the development of effective prompts and a discussion of their validity and reliability in research.

**Limitations:** Limited focus on specific applications beyond social movement studies.

**Conclusion:** The paper emphasizes practical guidelines for incorporating LLMs in text tasks while addressing epistemic risks and improving communication in research.

**Abstract:** Large language models (LLMs) have the potential to revolutionize computational social science, particularly in automated textual analysis. In this paper, we conduct a systematic evaluation of the promises and risks associated with using LLMs for text classification tasks, using social movement studies as an example. We propose a framework for social scientists to incorporate LLMs into text annotation, either as the primary coding decision-maker or as a coding assistant. This framework offers researchers tools to develop the potential best-performing prompt, and to systematically examine and report the validity and reliability of LLMs as a methodological tool. Additionally, we evaluate and discuss its epistemic risks associated with validity, reliability, replicability, and transparency. We conclude with several practical guidelines for using LLMs in text annotation tasks and offer recommendations for more effectively communicating epistemic risks in research.

</details>


### [149] [Scaling Analysis of Interleaved Speech-Text Language Models](https://arxiv.org/abs/2504.02398)

*Gallil Maimon, Michael Hassid, Amit Roth, Yossi Adi*

**Main category:** cs.CL

**Keywords:** Speech Language Models, scaling analysis, model efficiency

**Relevance Score:** 8

**TL;DR:** This paper demonstrates that interleaved Speech Language Models (SLMs) scale more efficiently than textless SLMs, requiring less compute and data while achieving comparable performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether interleaved SLMs can scale more efficiently compared to textless-SLMs amidst concerns regarding compute and data requirements.

**Method:** The authors performed scaling analysis by training several dozen interleaved SLMs, examining their efficiency in terms of compute and data utilization.

**Key Contributions:**

	1. Proving interleaved SLMs scale more efficiently than textless SLMs
	2. Identifying differences in scaling dynamics between the two model types
	3. Open sourcing models and data for further research

**Result:** Interleaved SLMs showed better scaling efficiency with compute and required notably less data, achieving semantic performance comparable to leading models.

**Limitations:** 

**Conclusion:** Interleaved SLMs offer a promising approach for training more efficient models in terms of compute and data requirements, with the availability of resources to facilitate future research.

**Abstract:** Existing Speech Language Model (SLM) scaling analysis paints a bleak picture. It predicts that SLMs require much more compute and data compared to text, leading some to question the feasibility of training high-quality SLMs. However, modern SLMs are often initialised from pre-trained TextLMs using speech-text interleaving to allow knowledge transfer. This raises the question - "Do interleaved SLMs scale more efficiently than textless-SLMs?" In this paper we answer a resounding yes! We conduct scaling analysis of interleaved SLMs by training several dozen and analysing the scaling trends. We see that under this setup SLMs scale more efficiently with compute. Additionally, our results indicate that the scaling dynamics significantly differ from textless-SLMs, suggesting one should allocate notably more of the compute budget to increasing model size over training tokens. We also study the role of synthetic data and TextLM model families in unlocking this potential. Results suggest that our scaled up model achieves comparable semantic speech performance to leading models, while using less compute and data. We open source models, samples, and data - https://pages.cs.huji.ac.il/adiyoss-lab/sims/ .

</details>


### [150] [Language Modeling for the Future of Finance: A Survey into Metrics, Tasks, and Data Opportunities](https://arxiv.org/abs/2504.07274)

*Nikita Tatarinov, Siddhant Sukhani, Agam Shah, Sudheer Chava*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Finance, Language Models, Forecasting, Evaluation Metrics

**Relevance Score:** 4

**TL;DR:** This paper reviews the application of NLP in finance by analyzing 374 research papers and identifies key opportunities for enhancing forecasting tasks and evaluation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically examine the trend of applying NLP techniques to finance and highlight opportunities for improving analysis and decision-making.

**Method:** Review of 374 NLP research papers from 2017 to 2024, focusing on 221 finance-related papers evaluated across 11 dimensions.

**Key Contributions:**

	1. Comprehensive review of 374 NLP papers in finance.
	2. Identification of underexplored opportunities and directions for future research.
	3. Recommendations for datasets and tools to enhance financial NLP applications.

**Result:** Identifies opportunities such as expanding forecasting tasks, improving evaluation with financial metrics, utilizing multilingual datasets, and balancing PLMs with efficient alternatives.

**Limitations:** 

**Conclusion:** The study provides actionable directions for research and practice, along with dataset and tool recommendations for academia and industry.

**Abstract:** Recent advances in language modeling have led to growing interest in applying Natural Language Processing (NLP) techniques to financial problems, enabling new approaches to analysis and decision-making. To systematically examine this trend, we review 374 NLP research papers published between 2017 and 2024 across 38 conferences and workshops, with a focused analysis of 221 papers that directly address finance-related tasks. We evaluate these papers across 11 quantitative and qualitative dimensions, and our study identifies the following opportunities: (i) expanding the scope of forecasting tasks; (ii) enriching evaluation with financial metrics; (iii) leveraging multilingual and crisis-period datasets; and (iv) balancing PLMs with efficient or interpretable alternatives. We identify actionable directions for research and practice, supported by dataset and tool recommendations, with implications for both the academia and industry communities.

</details>


### [151] [Memorization: A Close Look at Books](https://arxiv.org/abs/2504.12549)

*Iris Ma, Ian Domingo, Alberto Krone-Martins, Pierre Baldi, Cristina V. Lopes*

**Main category:** cs.CL

**Keywords:** LLM, regurgitation, fine-tuning, book extraction, NLP

**Relevance Score:** 9

**TL;DR:** This paper explores the extent to which entire books can be reconstructed from LLMs, specifically the Llama 3 models, using the prefix-prompting technique, highlighting limitations in current mitigation strategies against regurgitation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the capabilities of LLMs in reconstructing books and the effectiveness of regurgitation mitigations.

**Method:** Utilized the Llama 3 70B models with a prefix-prompting extraction technique to reconstruct books, focusing on extraction rates based on popularity and training data duplication.

**Key Contributions:**

	1. Introduced a framework for studying extraction from LLMs.
	2. Demonstrated reconstruction of entire books from minimal prompts.
	3. Highlighted the relationship between book popularity and extraction success.

**Result:** Successfully reconstructed 'Alice's Adventures in Wonderland' from 500 tokens and achieved high extraction rates for other books, although the results varied by book popularity.

**Limitations:** Extraction rates do not extend uniformly across all books and depend on factors like popularity.

**Conclusion:** The findings reveal limitations in current regurgitation mitigation strategies, emphasizing the impact of fine-tuning on the memorization retrieval in LLMs.

**Abstract:** To what extent can entire books be extracted from LLMs? Using the Llama 3 70B family of models, and the "prefix-prompting" extraction technique, we were able to auto-regressively reconstruct, with a very high level of similarity, one entire book (Alice's Adventures in Wonderland) from just the first 500 tokens. We were also able to obtain high extraction rates on several other books, piece-wise. However, these successes do not extend uniformly to all books. We show that extraction rates of books correlate with book popularity and thus, likely duplication in the training data.   We also confirm the undoing of mitigations in the instruction-tuned Llama 3.1, following recent work (Nasr et al., 2025). We further find that this undoing comes from changes to only a tiny fraction of weights concentrated primarily in the lower transformer blocks. Our results provide evidence of the limits of current regurgitation mitigation strategies and introduce a framework for studying how fine-tuning affects the retrieval of verbatim memorization in aligned LLMs.

</details>
