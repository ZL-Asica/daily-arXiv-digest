# 2025-06-24

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 25]

- [cs.CL](#cs.CL) [Total: 120]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Making the Right Thing: Bridging HCI and Responsible AI in Early-Stage AI Concept Selection](https://arxiv.org/abs/2506.17494)

*Ji-Youn Jung, Devansh Saxena, Minjung Park, Jini Kim, Jodi Forlizzi, Kenneth Holstein, John Zimmerman*

**Main category:** cs.HC

**Keywords:** Responsible AI, Human-Computer Interaction, Design through Research, AI concept sorting, Ethical considerations

**Relevance Score:** 9

**TL;DR:** This paper explores early-stage AI concept sorting in commercial settings through design experiments, emphasizing the need for Responsible AI (RAI) principles in the process.

**Read time:** 21 min

<details>
  <summary>Details</summary>

**Motivation:** Many AI projects fail due to early-stage decisions influenced by financial, technical, ethical, or user acceptance issues, highlighting the need for a structured approach to identify promising concepts early.

**Method:** The paper employs Research through Design, conducting three design experiments including a probe study with industry practitioners to evaluate risks and benefits of AI concepts.

**Key Contributions:**

	1. Introduces practical methods for integrating RAI principles in early AI concept development.
	2. Validates the significance of multidisciplinary collaboration in evaluating AI risks and benefits.
	3. Advocates for a design-led approach to enhance ethical considerations in AI innovation.

**Result:** Participants identified low-risk, high-benefit AI concepts and expressed a willingness to address RAI concerns early in the design process, demonstrating the efficacy of a design-led approach.

**Limitations:** The study focuses on a specific commercial context and may not generalize to all AI applications or industries.

**Conclusion:** The findings suggest that embedding ethical and service design thinking at the front end of AI innovation can improve early-stage decision-making and align with RAI principles.

**Abstract:** AI projects often fail due to financial, technical, ethical, or user acceptance challenges -- failures frequently rooted in early-stage decisions. While HCI and Responsible AI (RAI) research emphasize this, practical approaches for identifying promising concepts early remain limited. Drawing on Research through Design, this paper investigates how early-stage AI concept sorting in commercial settings can reflect RAI principles. Through three design experiments -- including a probe study with industry practitioners -- we explored methods for evaluating risks and benefits using multidisciplinary collaboration. Participants demonstrated strong receptivity to addressing RAI concerns early in the process and effectively identified low-risk, high-benefit AI concepts. Our findings highlight the potential of a design-led approach to embed ethical and service design thinking at the front end of AI innovation. By examining how practitioners reason about AI concepts, our study invites HCI and RAI communities to see early-stage innovation as a critical space for engaging ethical and commercial considerations together.

</details>


### [2] [Full-body WPT: wireless powering with meandered e-textiles](https://arxiv.org/abs/2506.17606)

*Ryo Takahashi, Takashi Sato, Wakako Yukita, Tomoyuki Yokota, Takao Someya, Yoshihiro Kawahara*

**Main category:** cs.HC

**Keywords:** wireless power transfer, human-machine interaction, ubiquitous computing

**Relevance Score:** 8

**TL;DR:** Presentation of Full-body WPT using meandered textile coils for efficient wireless power transfer around the human body, enhancing safety and adaptability for applications in health monitoring and HCI.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance wireless power transfer safety and efficiency around the human body for applications in health monitoring and human-machine interaction.

**Method:** Utilization of meandered textile coils to create localized magnetic fields at the skin surface, coupled with low-loss conductive yarn for energy-efficient design.

**Key Contributions:**

	1. Development of a safe and efficient full-body wireless power transfer system
	2. Integration of meandered textile coils into wearable materials
	3. Demonstration of adaptability to user movement and posture

**Result:** High power transfer efficiency and adaptability to user movements demonstrated through simulations and experimental prototypes.

**Limitations:** 

**Conclusion:** The system offers a foundation for ubiquitous applications in health monitoring, augmented reality, and HCI, emphasizing the role of body-centric wireless power networking.

**Abstract:** We present Full-body WPT, wireless power networking around the human body using a meandered textile coil. Unlike traditional inductive systems that emit strong fields into the deep tissue inside the body, the meander coil enables localized generation of strong magnetic field constrained to the skin surface, even when scaled to the size of the human body. Such localized inductive system enhances both safety and efficiency of wireless power around the body. Furthermore, the use of low-loss conductive yarn achieve energy-efficient and lightweight design. We analyze the performance of our design through simulations and experimental prototypes, demonstrating high power transfer efficiency and adaptability to user movement and posture. Our system provides a safe and efficient distributed power network using meandered textile coils integrated into wearable materials, highlighting the potential of body-centric wireless power networking as a foundational layer for ubiquitous health monitoring, augmented reality, and human-machine interaction systems.

</details>


### [3] [One Does Not Simply 'Mm-hmm': Exploring Backchanneling in the AAC Micro-Culture](https://arxiv.org/abs/2506.17890)

*Tobias Weinberg, Claire O'Connor, Ricardo E. Gonzalez Penuela, Stephanie Valencia, Thijs Roumen*

**Main category:** cs.HC

**Keywords:** backchanneling, Augmentative and Alternative Communication, communication culture, human-computer interaction, design recommendations

**Relevance Score:** 9

**TL;DR:** The paper explores backchanneling in Augmentative and Alternative Communication (AAC) users, focusing on their communication culture and unique practices.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how AAC users navigate backchanneling in communication, as traditional modalities often restrict their engagement and interaction cues due to their impairments.

**Method:** We conducted a workshop with 4 AAC users and performed four in-depth interviews with speech-language pathologists to gather insights on backchanneling practices in AAC contexts.

**Key Contributions:**

	1. Insights into backchanneling practices among AAC users
	2. Comparison of communication dynamics between AAC and non-AAC users
	3. Design recommendations for AAC technology to support diverse communication styles

**Result:** The study reveals unique characteristics of backchanneling between AAC users and contrasts it with interactions involving non-AAC users, highlighting the importance of understanding communication culture in AAC.

**Limitations:** 

**Conclusion:** The findings suggest a need to rethink the embodiment and mediation within AAC technology, providing design recommendations for facilitating multi-modal backchanneling that reflect diverse communication cultures.

**Abstract:** Backchanneling (e.g., "uh-huh", "hmm", a simple nod) encompasses a big part of everyday communication; it is how we negotiate the turn to speak, it signals our engagement, and shapes the flow of our conversations. For people with speech and motor impairments, backchanneling is limited to a reduced set of modalities, and their Augmentative and Alternative Communication (AAC) technology requires visual attention, making it harder to observe non-verbal cues of conversation partners. We explore how users of AAC technology approach backchanneling and create their own unique channels and communication culture. We conducted a workshop with 4 AAC users to understand the unique characteristics of backchanneling in AAC. We explored how backchanneling changes when pairs of AAC users communicate vs when an AAC user communicates with a non-AAC user. We contextualize these findings through four in-depth interviews with speech-language pathologists (SLPs). We conclude with a discussion about backchanneling as a micro-cultural practice, rethinking embodiment and mediation in AAC technology, and providing design recommendations for timely multi-modal backchanneling while respecting different communication cultures.

</details>


### [4] [When concept-based XAI is imprecise: Do people distinguish between generalisations and misrepresentations?](https://arxiv.org/abs/2506.17936)

*Romy MÃ¼ller*

**Main category:** cs.HC

**Keywords:** explainable AI, C-XAI, human perception, safety evaluation, generalization

**Relevance Score:** 6

**TL;DR:** The study investigates how participants evaluate generalizations presented by C-XAI in the context of AI safety evaluation, revealing sensitivity to imprecision in relevant features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding if people can recognize generalizations by C-XAI concepts in safety evaluation tasks is crucial for the development of effective explainable AI systems.

**Method:** An experimental railway safety scenario where participants evaluated a simulated AI's performance based on image snippets representing abstract categories of danger.

**Key Contributions:**

	1. Investigates human perception of C-XAI concepts in safety evaluation scenarios.
	2. Reveals sensitivity to relevant feature imprecision in AI explanations.
	3. Challenges the effectiveness of C-XAI in conveying a deeper understanding of AI models.

**Result:** Participants rated concepts generalizing over less relevant features lower than precisely matching concepts, indicating sensitivity to imprecisions.

**Limitations:** Study limited to a controlled experimental scenario, may not generalize to all contexts of AI explanation.

**Conclusion:** The findings challenge assumptions that people can recognize generalizations in C-XAI, raising questions about its utility in AI safety evaluations.

**Abstract:** Concept-based explainable artificial intelligence (C-XAI) can help reveal the inner representations of AI models. Understanding these representations is particularly important in complex tasks like safety evaluation. Such tasks rely on high-level semantic information (e.g., about actions) to make decisions about abstract categories (e.g., whether a situation is dangerous). In this context, it may desirable for C-XAI concepts to show some variability, suggesting that the AI is capable of generalising beyond the concrete details of a situation. However, it is unclear whether people recognise and appreciate such generalisations and can distinguish them from other, less desirable forms of imprecision. This was investigated in an experimental railway safety scenario. Participants evaluated the performance of a simulated AI that evaluated whether traffic scenes involving people were dangerous. To explain these decisions, the AI provided concepts in the form of similar image snippets. These concepts differed in their match with the classified image, either regarding a highly relevant feature (i.e., relation to tracks) or a less relevant feature (i.e., actions). Contrary to the hypotheses, concepts that generalised over less relevant features led to ratings that were lower than for precisely matching concepts and comparable to concepts that systematically misrepresented these features. Conversely, participants were highly sensitive to imprecisions in relevant features. These findings cast doubts on whether people spontaneously recognise generalisations. Accordingly, they might not be able to infer from C-XAI concepts whether AI models have gained a deeper understanding of complex situations.

</details>


### [5] [Conceptualization, Operationalization, and Measurement of Machine Companionship: A Scoping Review](https://arxiv.org/abs/2506.18119)

*Jaime Banks, Zhixin Li*

**Main category:** cs.HC

**Keywords:** machine companionship, HCI, systematic review, AI, social interactions

**Relevance Score:** 7

**TL;DR:** This paper provides a systematic review of machine companionship (MC), offering a literature-guided definition and identifying various dimensions and measurements of MC.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the formal concept of machine companionship (MC) and its implications in technology and social interactions, as it has not been sufficiently engaged in existing literature.

**Method:** A PRISMA-guided scoping review was conducted, sampling and synthesizing 71 scholarly works on MC from 2017 to 2025.

**Key Contributions:**

	1. Systematic review of the concept of machine companionship.
	2. Identification of over 50 distinct measured variables related to MC.
	3. Proposed literature-guided definition of machine companionship.

**Result:** The review highlighted a wide variety of approaches to MC with over 50 distinct measurable variables, leading to a proposed definition of MC as an autotelic, coordinated human-machine connection.

**Limitations:** The review is limited to works published between 2017-2025; may not capture all aspects of MC.

**Conclusion:** The findings will help in clarifying the concept of MC and encourage future research on its implications in HCI and related fields.

**Abstract:** The notion of machine companions has long been embedded in social-technological imaginaries. Recent advances in AI have moved those media musings into believable sociality manifested in interfaces, robotic bodies, and devices. Those machines are often referred to colloquially as "companions" yet there is little careful engagement of machine companionship (MC) as a formal concept or measured variable. This PRISMA-guided scoping review systematically samples, surveys, and synthesizes current scholarly works on MC (N = 71; 2017-2025), to that end. Works varied widely in considerations of MC according to guiding theories, dimensions of a-priori specified properties (subjectively positive, sustained over time, co-active, autotelic), and in measured concepts (with more than 50 distinct measured variables). WE ultimately offer a literature-guided definition of MC as an autotelic, coordinated connection between human and machine that unfolds over time and is subjectively positive.

</details>


### [6] [AI Harmonizer: Expanding Vocal Expression with a Generative Neurosymbolic Music AI System](https://arxiv.org/abs/2506.18143)

*Lancelot Blanchard, Cameron Holt, Joseph A. Paradiso*

**Main category:** cs.HC

**Keywords:** AI Harmonizer, vocal harmonization, generative AI, musical composition, performance enhancement

**Relevance Score:** 3

**TL;DR:** This paper presents the AI Harmonizer, an innovative tool that autonomously generates four-part harmonies for vocalists without requiring prior harmonic input. It utilizes generative AI techniques for pitch detection and voice modeling.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a vocal harmonizer that simplifies the process of generating harmonies for solo vocalists, eliminating the need for musical expertise related to key specification.

**Method:** The AI Harmonizer integrates generative AI techniques with custom-trained symbolic music models to arrange vocal melodies into choral textures autonomously.

**Key Contributions:**

	1. Introduction of a fully autonomous vocal harmonization system
	2. Integration of generative AI techniques with symbolic music models
	3. Potential applications in both performance and composition

**Result:** The system successfully generates coherent four-part harmonies based on any provided vocal melody, enhancing musical performance and composition.

**Limitations:** Currently operates offline; future work needed for real-time functionalities.

**Conclusion:** The AI Harmonizer represents a significant advancement in AI-assisted vocal performance and expresses potential for real-time implementations. The implementation is available on GitHub.

**Abstract:** Vocals harmonizers are powerful tools to help solo vocalists enrich their melodies with harmonically supportive voices. These tools exist in various forms, from commercially available pedals and software to custom-built systems, each employing different methods to generate harmonies. Traditional harmonizers often require users to manually specify a key or tonal center, while others allow pitch selection via an external keyboard-both approaches demanding some degree of musical expertise. The AI Harmonizer introduces a novel approach by autonomously generating musically coherent four-part harmonies without requiring prior harmonic input from the user. By integrating state-of-the-art generative AI techniques for pitch detection and voice modeling with custom-trained symbolic music models, our system arranges any vocal melody into rich choral textures. In this paper, we present our methods, explore potential applications in performance and composition, and discuss future directions for real-time implementations. While our system currently operates offline, we believe it represents a significant step toward AI-assisted vocal performance and expressive musical augmentation. We release our implementation on GitHub.

</details>


### [7] [Two Sonification Methods for the MindCube](https://arxiv.org/abs/2506.18196)

*Fangzheng Liu, Lancelot Blanchard, Don D. Haddad, Joseph A. Paradiso*

**Main category:** cs.HC

**Keywords:** MindCube, emotion regulation, musical interface, generative AI, interactive devices

**Relevance Score:** 4

**TL;DR:** This paper examines the MindCube, an interactive device that utilizes sensors to aid in emotion regulation through a musical interface.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to explore how the MindCube can serve as a controller for musical systems aimed at helping users manage emotions such as stress and anxiety.

**Method:** The paper presents two mappings for the MindCube interface: one that integrates generative AI and another that does not, discussing the implications of each approach.

**Key Contributions:**

	1. Exploration of emotional interfaces with the MindCube
	2. Development of AI and non-AI mappings for musical interaction
	3. Insights into future research directions for emotion regulation through music.

**Result:** Findings indicate that the generative AI mapping allows for meaningful navigation of emotions within a latent space, enhancing the interaction with the music system.

**Limitations:** The study is exploratory and may require further validation in practical applications with diverse user groups.

**Conclusion:** The study concludes with insights on the MindCube's potential in emotional regulation and suggests future research directions in this area.

**Abstract:** In this work, we explore the musical interface potential of the MindCube, an interactive device designed to study emotions. Embedding diverse sensors and input devices, this interface resembles a fidget cube toy commonly used to help users relieve their stress and anxiety. As such, it is a particularly well-suited controller for musical systems that aim to help with emotion regulation. In this regard, we present two different mappings for the MindCube, with and without AI. With our generative AI mapping, we propose a way to infuse meaning within a latent space and techniques to navigate through it with an external controller. We discuss our results and propose directions for future work.

</details>


### [8] [Co-persona: Leveraging LLMs and Expert Collaboration to Understand User Personas through Social Media Data Analysis](https://arxiv.org/abs/2506.18269)

*Min Yin, Haoyu Liu, Boyi Lian, Chunlei Chai*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Large Language Models, Social Media Analysis, User Personas, Product Design

**Relevance Score:** 8

**TL;DR:** The study presents the 	extsc{Co-Persona} framework which combines LLMs and expert validation to analyze social media for user persona development, demonstrating its application in product design for a manufacturer.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance understanding of user behaviors and preferences through social media analysis using NLP and LLMs.

**Method:** A multi-stage NLP processing of 38 million social media posts to identify user personas based on nighttime behaviors.

**Key Contributions:**

	1. Introduction of the 	extsc{Co-Persona} framework
	2. Identification of five unique consumer personas
	3. Integration of LLMs for enhanced user understanding

**Result:** Five distinct user personas were identified: Health Aficionados, Night Owls, Interior Decorators, Child-care Workers, and Workaholics, each with unique pre-sleep activities and product preferences.

**Limitations:** 

**Conclusion:** The 	extsc{Co-Persona} framework offers actionable insights for manufacturers, aiding in targeted marketing and improving product design while contributing to theoretical development of personas.

**Abstract:** This study introduces \textsc{Co-Persona}, a framework bridging large-scale social media analysis and user understanding via integration of Large Language Models (LLMs) and expert validation. Through a case study of B.Co, a Chinese manufacturer, we applied \textsc{Co-Persona} to bedside lamp development by analyzing 38 million posts from Xiao Hongshu. Our multi-stage NLP processing revealed five user personas based on nighttime behaviors: Health Aficionados, Night Owls, Interior Decorators, Child-care Workers, and Workaholics. These personas exhibit distinct pre-sleep activities and product preferences. The method enhances manufacturers' ability to interpret social data while preserving user-centric insights, offering actionable strategies for targeted marketing and product design. This work advances both theoretical persona development and practical consumer-driven innovation.

</details>


### [9] [Supporting Car-Following Behavior through V2V-Based Beyond-Visual-Range Information Display](https://arxiv.org/abs/2506.18308)

*Feiqi Gu, Zhixiong Wang, Zhenyu Wang, Dengbo He*

**Main category:** cs.HC

**Keywords:** human-machine interface, car-following behavior, beyond-visual-range information, vehicle-to-vehicle communication, driving safety

**Relevance Score:** 7

**TL;DR:** The paper investigates how beyond-visual-range (BVR) information provided via vehicle-to-vehicle communication can enhance drivers' car-following (CF) safety by using various human-machine interfaces (HMIs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Rear-end collisions remain prevalent despite safety measures, highlighting the need for improved driver awareness and response through enhanced information.

**Method:** Four HMIs were designed to provide different types of BVR information to drivers during car-following scenarios, and a driving simulator experiment was conducted with 40 participants to assess the impact on driving safety.

**Key Contributions:**

	1. Development of new HMIs utilizing BVR information for CF
	2. Demonstration of improved driving safety metrics with BVR
	3. Identification of specific HMI designs that enhance safety more effectively

**Result:** BVR information generally improved CF safety by allowing quicker brake responses and better time headway without causing driver information overload, particularly aiding novice drivers.

**Limitations:** 

**Conclusion:** The Brake-HMI showed the best performance in improving safety during braking scenarios, whereas Video-HMI increased attentional demands without clear benefits.

**Abstract:** Rear-end collisions constituted a large portion of crashes on the road, despite efforts to mitigate rear-end collisions, such as forward collision warnings. The chance of rear-end collisions is closely related to drivers' car-following (CF) behaviors in the traffic flow. Given that drivers may rely on more than the information of the direct lead vehicle (DLV) when making CF decisions, expanding drivers' perceptual range by providing beyond-visual-range (BVR) information based on vehicle-to-vehicle (V2V) communication may enhance CF safety. Thus, four different human-machine interfaces (HMIs) providing various types of BVR information in CF events were designed, including Brake-HMI showing only brake action of indirect lead vehicles (ILV), Dis-HMI and THW-HMI showing the relative distance and time headway between the ILV and DLV, respectively, and Video-HMI showing the live-stream video of ILV from the perspective of DLV. A driving simulator experiment with 40 participants was conducted to evaluate the impact of BVR-based HMI on driving safety in CF events. We found that, in general, BVR information could improve CF safety without overloading drivers and compromising their visual attention allocation strategies, particularly among novice drivers, by enabling quicker brake responses and increasing time headway and time-to-collision in brake events. The Brake-HMI yielded the safest performance in chain brake events, whereas Video-HMI increased attentional demands without observable benefits. This research provides insights into enabling drivers' BVR perception based on V2V communication to enhance driving safety in CF scenarios.

</details>


### [10] [Crowdsourcing Ubiquitous Indoor Localization with Non-Cooperative Wi-Fi Ranging](https://arxiv.org/abs/2506.18317)

*Emerson Sie, Enguang Fan, Federico Cifuentes-Urtubey, Deepak Vasisht*

**Main category:** cs.HC

**Keywords:** indoor localization, Wi-Fi, pedestrian dead reckoning, crowdsourcing, measured accuracy

**Relevance Score:** 7

**TL;DR:** PeepLoc is a deployable Wi-Fi-based indoor localization solution that utilizes existing infrastructure for improved accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Indoor localization methods have been impractical for real-world deployment; PeepLoc aims to address this issue by being deployable using pre-existing devices.

**Method:** PeepLoc employs a mechanism for obtaining non-cooperative time-of-flight (ToF) to Wi-Fi access points and a bootstrapping method that combines pedestrian dead reckoning and crowdsourcing.

**Key Contributions:**

	1. Deployable solution using existing Wi-Fi infrastructure
	2. Mechanism for non-cooperative ToF measurements
	3. Combines PDR and crowdsourcing for anchor point initialization.

**Result:** PeepLoc demonstrates a mean positional error of 3.41 m and median error of 3.06 m, outperforming existing indoor localization systems and comparing favorably with outdoor GPS performance.

**Limitations:** 

**Conclusion:** PeepLoc offers a scalable and efficient solution for indoor localization using only existing Wi-Fi infrastructure, enabling transformative applications.

**Abstract:** Indoor localization opens the path to potentially transformative applications. Although many indoor localization methods have been proposed over the years, they remain too impractical for widespread deployment in the real world. In this paper, we introduce PeepLoc, a deployable and scalable Wi-Fi-based solution for indoor localization that relies only on pre-existing devices and infrastructure. Specifically, PeepLoc works on any mobile device with an unmodified Wi-Fi transceiver and in any indoor environment with a sufficient number of Wi-Fi access points (APs) and pedestrian traffic. At the core of PeepLoc is (a) a mechanism which allows any Wi-Fi device to obtain non-cooperative time-of-flight (ToF) to any Wi-Fi AP and (b) a novel bootstrapping mechanism that relies on pedestrian dead reckoning (PDR) and crowdsourcing to opportunistically initialize pre-existing APs as anchor points within an environment. We implement PeepLoc using commodity hardware and evaluate it extensively across 4 campus buildings. We show PeepLoc leads to a mean and median positional error of 3.41 m and 3.06 m respectively, which is superior to existing deployed indoor localization systems and is competitive with commodity GPS in outdoor environments.

</details>


### [11] [CODS : A Theoretical Model for Computational Design Based on Design Space](https://arxiv.org/abs/2506.18455)

*Nan Cao, Xiaoyu Qi, Chuer Chen, Xiaoke Yan*

**Main category:** cs.HC

**Keywords:** Computational Design, Optimization, Large Language Models, Design Automation, User Intent

**Relevance Score:** 6

**TL;DR:** CODS is a theoretical model for computational design that frames it as a constrained optimization problem, using large language models to generate design solutions that are coherent and aligned with user intent.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a generalizable and interpretable framework for computational design that goes beyond handcrafted heuristics and domain-specific rules.

**Method:** The paper introduces a model called CODS which employs a structured prompt engineering pipeline using large language models to derive soft and hard constraints that guide the optimization process.

**Key Contributions:**

	1. Introduction of a new optimization framework for design tasks
	2. Use of large language models for deriving constraints in design
	3. Validation in diverse design domains showing improved outcomes

**Result:** The approach was validated across visualization design and knitwear generation, showing improved design quality, alignment with user intent, and user preferences compared to existing LLM-based methods.

**Limitations:** 

**Conclusion:** CODS presents a unified foundation for scalable and controllable AI-powered design automation.

**Abstract:** We introduce CODS (Computational Optimization in Design Space), a theoretical model that frames computational design as a constrained optimization problem over a structured, multi-dimensional design space. Unlike existing methods that rely on handcrafted heuristics or domain-specific rules, CODS provides a generalizable and interpretable framework that supports diverse design tasks. Given a user requirement and a well-defined design space, CODS automatically derives soft and hard constraints using large language models through a structured prompt engineering pipeline. These constraints guide the optimization process to generate design solutions that are coherent, expressive, and aligned with user intent. We validate our approach across two domains-visualization design and knitwear generation-demonstrating superior performance in design quality, intent alignment, and user preference compared to existing LLM-based methods. CODS offers a unified foundation for scalable, controllable, and AI-powered design automation.

</details>


### [12] [Crowdsourcing eHMI Designs: A Participatory Approach to Autonomous Vehicle-Pedestrian Communication](https://arxiv.org/abs/2506.18605)

*Ronald Cumbal, Didem Gurdur Broo, Ginevra Castellano*

**Main category:** cs.HC

**Keywords:** Human-Machine Interface, autonomous vehicles, user-generated design, communication, safety

**Relevance Score:** 7

**TL;DR:** This study explores user-generated ideas for external Human-Machine Interfaces (eHMIs) in autonomous vehicles to improve communication and safety with road users.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Effective communication between autonomous vehicles and road users is crucial for safety in shared environments, necessitating better design of eHMIs.

**Method:** The study utilized a participatory, crowd-sourced approach where participants were introduced to eHMI concepts and tasked with sketching design ideas in response to various risk scenarios.

**Key Contributions:**

	1. Demonstrated user involvement in the design process of eHMIs for autonomous vehicles
	2. Identified key communication preferences among users for safety
	3. Highlighted the need for multi-modal and adaptable communication in eHMI designs

**Result:** Initial findings indicated active engagement but the need for refined objectives. A follow-up study showed participants preferred multi-modal communication methods such as lights, symbols, and text for clarity in eHMI design.

**Limitations:** The study may require further exploration into diverse participant demographics and real-world applications of suggested designs.

**Conclusion:** The study underscores the importance of involving users early in eHMI design, focusing on intuitive communication strategies that leverage familiar vehicle elements.

**Abstract:** As autonomous vehicles become more integrated into shared human environments, effective communication with road users is essential for ensuring safety. While previous research has focused on developing external Human-Machine Interfaces (eHMIs) to facilitate these interactions, we argue that involving users in the early creative stages can help address key challenges in the development of this technology. To explore this, our study adopts a participatory, crowd-sourced approach to gather user-generated ideas for eHMI designs. Participants were first introduced to fundamental eHMI concepts, equipping them to sketch their own design ideas in response to scenarios with varying levels of perceived risk. An initial pre-study with 29 participants showed that while they actively engaged in the process, there was a need to refine task objectives and encourage deeper reflection. To address these challenges, a follow-up study with 50 participants was conducted. The results revealed a strong preference for autonomous vehicles to communicate their awareness and intentions using lights (LEDs and projections), symbols, and text. Participants' sketches prioritized multi-modal communication, directionality, and adaptability to enhance clarity, consistently integrating familiar vehicle elements to improve intuitiveness.

</details>


### [13] [Deceptive Game Design? Investigating the Impact of Visual Card Style on Player Perception](https://arxiv.org/abs/2506.18648)

*Leonie Kallabis, Timo Bertram, Florian Rupp*

**Main category:** cs.HC

**Keywords:** visual style, game perception, collectible cards, Magic: The Gathering, player survey

**Relevance Score:** 4

**TL;DR:** This paper explores the impact of visual style on players' perception of collectible card strength in Magic: The Gathering.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how aesthetics influence player perceptions of game elements, particularly in card games.

**Method:** A single-blind survey study was conducted with players of Magic: The Gathering, comparing AI-generated cards in two visual styles: cute vs. heroic.

**Key Contributions:**

	1. Investigation of visual style impact on game perception
	2. Use of AI-generated cards for study
	3. Analysis of player survey results highlighting varied perceptions

**Result:** Participants' perceptions of card strength were influenced by visual style, with a distribution of preferences observed.

**Limitations:** Study focused only on one game and does not account for all potential influences on player perception.

**Conclusion:** Visual aesthetics can significantly affect player judgment of game elements, showcasing variability in individual player perceptions.

**Abstract:** The visual style of game elements considerably contributes to the overall experience. Aesthetics influence player appeal, while the abilities of game pieces define their in-game functionality. In this paper, we investigate how the visual style of collectible cards influences the players' perception of the card's actual strength in the game. Using the popular trading card game Magic: The Gathering, we conduct a single-blind survey study that examines how players perceive the strength of AI-generated cards that are shown in two contrasting visual styles: cute and harmless, or heroic and mighty. Our analysis reveals that some participants are influenced by a card's visual appearance when judging its in-game strength. Overall, differences in style perception are normally distributed around a neutral center, but individual participants vary in both directions: some generally perceive the cute style to be stronger, whereas others believe that the heroic style is better.

</details>


### [14] [Fanfiction in the Age of AI: Community Perspectives on Creativity, Authenticity and Adoption](https://arxiv.org/abs/2506.18706)

*Roi Alfassi, Angelora Cooper, Zoe Mitchell, Mary Calabro, Orit Shaer, Osnat Mokryn*

**Main category:** cs.HC

**Keywords:** Generative AI, fanfiction, community dynamics, ethical practices, human-centered design

**Relevance Score:** 7

**TL;DR:** This study explores the perceptions of fanfiction community members on the integration of Generative AI in storytelling, highlighting concerns over authenticity and community dynamics.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how Generative AI is impacting creative communities, particularly in fanfiction, and to assess the implications for community dynamics and ethical practices.

**Method:** The study involved qualitative analysis based on responses from 157 active members of the fanfiction community, including both readers and writers.

**Key Contributions:**

	1. Insights into community perceptions of AI in creative writing.
	2. Identification of the ethical concerns regarding AI-generated content.
	3. Recommendations for ethical AI integration in creative platforms.

**Result:** Responses varied from cautious acceptance of AI's role in enhancing creativity to significant concerns about authenticity and ethical issues in creative practices.

**Limitations:** The study is based on self-reported perceptions, which may not fully capture the diversity of opinions within the broader fanfiction community.

**Conclusion:** Effective integration of AI in creative platforms requires thoughtful design interventions to maintain community values, enhance transparency, and promote social connections.

**Abstract:** The integration of Generative AI (GenAI) into creative communities, like fanfiction, is reshaping how stories are created, shared, and valued. This study investigates the perceptions of 157 active fanfiction members, both readers and writers, regarding AI-generated content in fanfiction. Our research explores the impact of GenAI on community dynamics, examining how AI affects the participatory and collaborative nature of these spaces. The findings reveal responses ranging from cautious acceptance of AI's potential for creative enhancement to concerns about authenticity, ethical issues, and the erosion of human-centered values. Participants emphasized the importance of transparency and expressed worries about losing social connections. Our study highlights the need for thoughtful AI integration in creative platforms using design interventions that enable ethical practices, promote transparency, increase engagement and connection, and preserve the community's core values.

</details>


### [15] [LLM-enhanced Interactions in Human-Robot Collaborative Drawing with Older Adults](https://arxiv.org/abs/2506.18711)

*Marianne Bossema, Somaya Ben Allouch, Aske Plaat, Rob Saunders*

**Main category:** cs.HC

**Keywords:** human-robot co-creativity, older adults, Large Language Model, creativity support, participatory design

**Relevance Score:** 8

**TL;DR:** The study investigates factors enhancing older adults' creative experiences in human-robot co-creativity through a drawing course involving robots.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the under researched area of robots supporting creativity in older adults.

**Method:** Conducted a participatory case study with professional art educators designing 'Drawing with Robots' course for older adults, observing interactions and interviewing participants.

**Key Contributions:**

	1. Exploration of human-robot interaction in creative contexts for older adults
	2. Insights into user preferences for robot roles in creativity
	3. Identification of limitations in robot feedback mechanisms

**Result:** Participants valued the robot's role as a curator and appreciated the spoken dialogue provided by the LLM, although some feedback lacked contextual understanding.

**Limitations:** The robot's feedback sometimes lacked contextual understanding and alignment with artistic goals.

**Conclusion:** LLM-enhanced robots have potential to support creativity in older adults, but require improvements in understanding user context and preferences.

**Abstract:** The goal of this study is to identify factors that support and enhance older adults' creative experiences in human-robot co-creativity. Because the research into the use of robots for creativity support with older adults remains underexplored, we carried out an exploratory case study. We took a participatory approach and collaborated with professional art educators to design a course Drawing with Robots for adults aged 65 and over. The course featured human-human and human-robot drawing activities with various types of robots. We observed collaborative drawing interactions, interviewed participants on their experiences, and analyzed collected data. Findings show that participants preferred acting as curators, evaluating creative suggestions from the robot in a teacher or coach role. When we enhanced a robot with a multimodal Large Language Model (LLM), participants appreciated its spoken dialogue capabilities. They reported however, that the robot's feedback sometimes lacked an understanding of the context, and sensitivity to their artistic goals and preferences. Our findings highlight the potential of LLM-enhanced robots to support creativity and offer future directions for advancing human-robot co-creativity with older adults.

</details>


### [16] [AutoGraph: A Knowledge-Graph Framework for Modeling Interface Interaction and Automating Procedure Execution in Digital Nuclear Control Rooms](https://arxiv.org/abs/2506.18727)

*Xingyu Xiao, Jiejuan Tong, Jun Sun, Zhe Sui, Jingang Liang, Hongru Zhao, Jun Zhao, Haitao Wang*

**Main category:** cs.HC

**Keywords:** knowledge graph, human-system interaction, nuclear power plant, automation, procedures

**Relevance Score:** 2

**TL;DR:** A framework named AutoGraph is introduced to automate and enhance procedure execution in nuclear power plant control rooms, aiming to improve operator interaction and reduce human error during complex operations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human-system interaction in nuclear power plants by integrating computer-based procedures with human-system interfaces, thereby supporting intelligent automation and reducing human error.

**Method:** The study presents AutoGraph, which includes a tracking module for operator interactions, an Interface Element Knowledge Graph for mapping properties of interfaces, and an execution engine to automate procedure execution from textual descriptions.

**Key Contributions:**

	1. Introduction of AutoGraph framework for procedure automation in NPPs
	2. Development of an Interface Element Knowledge Graph (IE-KG)
	3. Validation of significant time reductions in task execution

**Result:** Validation of AutoGraph shows significant reductions in task completion times and potential improvements in real-time human reliability assessments, indicating its capability to enhance procedural safety and cognitive performance.

**Limitations:** 

**Conclusion:** AutoGraph can be integrated into existing dynamic human reliability assessment and decision support systems, demonstrating its extensibility and utility in complex operational environments.

**Abstract:** Digitalization in nuclear power plant (NPP) control rooms is reshaping how operators interact with procedures and interface elements. However, existing computer-based procedures (CBPs) often lack semantic integration with human-system interfaces (HSIs), limiting their capacity to support intelligent automation and increasing the risk of human error, particularly under dynamic or complex operating conditions. In this study, we present AutoGraph, a knowledge-graph-based framework designed to formalize and automate procedure execution in digitalized NPP environments.AutoGraph integrates (1) a proposed HTRPM tracking module to capture operator interactions and interface element locations; (2) an Interface Element Knowledge Graph (IE-KG) encoding spatial, semantic, and structural properties of HSIs; (3) automatic mapping from textual procedures to executable interface paths; and (4) an execution engine that maps textual procedures to executable interface paths. This enables the identification of cognitively demanding multi-action steps and supports fully automated execution with minimal operator input. We validate the framework through representative control room scenarios, demonstrating significant reductions in task completion time and the potential to support real-time human reliability assessment. Further integration into dynamic HRA frameworks (e.g., COGMIF) and real-time decision support systems (e.g., DRIF) illustrates AutoGraph extensibility in enhancing procedural safety and cognitive performance in complex socio-technical systems.

</details>


### [17] [Conceptual Modelling for Life Sciences Based on Systemist Foundations](https://arxiv.org/abs/2506.18742)

*R. Lukyanenko, O. Pastor, V. C. Storey*

**Main category:** cs.HC

**Keywords:** conceptual modeling, life sciences, genomic information, precision medicine, system thinking

**Relevance Score:** 4

**TL;DR:** This paper proposes a systemist perspective for creating conceptual models in life sciences, which aids in the development of information systems, particularly for genomic information and precision medicine.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for effective conceptual models in life sciences to support researchers and enhance communication between designers and developers.

**Method:** The paper introduces a system perspective for modeling life scientists' problems and presents a new notation to capture important semantics relevant to genomic information and precision medicine.

**Key Contributions:**

	1. Introduction of a systemist perspective for life sciences
	2. Novel notation for representing genomic-related information
	3. Characterization of the term 'system' in life sciences modeling

**Result:** The proposed notation helps model complex life science issues by better representing connections between the physical and digital worlds, supporting effective problem-solving and understanding.

**Limitations:** 

**Conclusion:** The new ontologically supported characterization of a system serves as a foundational construct for conceptual modeling in life sciences, facilitating better communication and understanding.

**Abstract:** All aspects of our society, including the life sciences, need a mechanism for people working within them to represent the concepts they employ to carry out their research. For the information systems being designed and developed to support researchers and scientists in conducting their work, conceptual models of the relevant domains are usually designed as both blueprints for a system being developed and as a means of communication between the designer and developer. Most conceptual modelling concepts are generic in the sense that they are applied with the same understanding across many applications. Problems in the life sciences, however, are especially complex and important, because they deal with humans, their well-being, and their interactions with the environment as well as other organisms. This work proposes a systemist perspective for creating a conceptual model of a life scientist's problem. We introduce the notion of a system and then show how it can be applied to the development of an information system for handling genomic-related information. We extend our discussion to show how the proposed systemist perspective can support the modelling of precision medicine. This research recognizes challenges in life sciences research of how to model problems to better represent the connections between physical and digital worlds. We propose a new notation that explicitly incorporates systemist thinking, as well as the components of systems based on recent ontological foundations. The new notation captures important semantics in the domain of life sciences. It may be used to facilitate understanding, communication and problem-solving more broadly. We also provide a precise, sound, ontologically supported characterization of the term system, as a basic construct for conceptual modelling in life sciences.

</details>


### [18] [From Representation to Mediation: A New Agenda for Conceptual Modeling Research in A Digital World](https://arxiv.org/abs/2506.18743)

*J. Recker, R. Lukyanenko, M. A. Jabbari, B. M. Samuel, A. Castellanos*

**Main category:** cs.HC

**Keywords:** conceptual modeling, information systems, digital reality

**Relevance Score:** 4

**TL;DR:** The paper argues that conceptual modeling is more relevant than ever in the IS field, proposing a new framework that updates its theoretical foundations to align with emerging digital realities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To highlight the changing role of information systems and emphasize the relevance of conceptual modeling in todayâs digitalized world.

**Method:** Development of a new theoretical framework for conceptual modeling that shifts foundational assumptions and addresses the implications of digital and physical realities.

**Key Contributions:**

	1. Proposes a new theoretical framework for conceptual modeling.
	2. Identifies novel research questions that connect physical and digital realms.
	3. Advocates for the development of new methods in conceptual modeling.

**Result:** Introduction of new research questions about grammars, methods, scripts, agents, and contexts in conceptual modeling.

**Limitations:** 

**Conclusion:** The study calls for new methods and grammars in conceptual modeling, expanding its methodological array and considering new dependent variables relevant to a digital context.

**Abstract:** The role of information systems (IS) as representations of real-world systems is changing in an increasingly digitalized world, suggesting that conceptual modeling is losing its relevance to the IS field. We argue the opposite: Conceptual modeling research is more relevant to the IS field than ever, but it requires an update with current theory. We develop a new theoretical framework of conceptual modeling that delivers a fundamental shift in the assumptions that govern research in this area. This move can make traditional knowledge about conceptual modeling consistent with the emerging requirements of a digital world. Our framework draws attention to the role of conceptual modeling scripts as mediators between physical and digital realities. We identify new research questions about grammars, methods, scripts, agents, and contexts that are situated in intertwined physical and digital realities. We discuss several implications for conceptual modeling scholarship that relate to the necessity of developing new methods and grammars for conceptual modeling, broadening the methodological array of conceptual modeling scholarship, and considering new dependent variables.

</details>


### [19] [BRAVE: Brain-Controlled Prosthetic Arm with Voice Integration and Embodied Learning for Enhanced Mobility](https://arxiv.org/abs/2506.18749)

*Abdul Basit, Maha Nawaz, Muhammad Shafique*

**Main category:** cs.HC

**Keywords:** Brain-computer interface, EEG, Prosthetic control, Ensemble learning, Human-in-the-loop

**Relevance Score:** 8

**TL;DR:** BRAVE is a hybrid EEG and voice-controlled prosthetic system that enhances control of prosthetic limbs using ensemble learning and a human-in-the-loop framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enable intuitive control of prosthetic limbs for individuals with upper limb amputations using non-invasive brain-computer interfaces.

**Method:** BRAVE integrates ensemble learning for EEG classification with LSTM, CNN, and Random Forest models, and includes automatic speech recognition for mode switching.

**Key Contributions:**

	1. Integration of EEG and voice control for prosthetic systems
	2. High classification accuracy using ensemble learning
	3. Real-time responsiveness with intuitive control mechanisms

**Result:** Achieved a classification accuracy of 96% and real-time operation with a response latency of 150 ms.

**Limitations:** 

**Conclusion:** BRAVE offers a promising solution for robust and responsive prosthetic control without relying on residual muscle activity.

**Abstract:** Non-invasive brain-computer interfaces (BCIs) have the potential to enable intuitive control of prosthetic limbs for individuals with upper limb amputations. However, existing EEG-based control systems face challenges related to signal noise, classification accuracy, and real-time adaptability. In this work, we present BRAVE, a hybrid EEG and voice-controlled prosthetic system that integrates ensemble learning-based EEG classification with a human-in-the-loop (HITL) correction framework for enhanced responsiveness. Unlike traditional electromyography (EMG)-based prosthetic control, BRAVE aims to interpret EEG-driven motor intent, enabling movement control without reliance on residual muscle activity. To improve classification robustness, BRAVE combines LSTM, CNN, and Random Forest models in an ensemble framework, achieving a classification accuracy of 96% across test subjects. EEG signals are preprocessed using a bandpass filter (0.5-45 Hz), Independent Component Analysis (ICA) for artifact removal, and Common Spatial Pattern (CSP) feature extraction to minimize contamination from electromyographic (EMG) and electrooculographic (EOG) signals. Additionally, BRAVE incorporates automatic speech recognition (ASR) to facilitate intuitive mode switching between different degrees of freedom (DOF) in the prosthetic arm. The system operates in real time, with a response latency of 150 ms, leveraging Lab Streaming Layer (LSL) networking for synchronized data acquisition. The system is evaluated on an in-house fabricated prosthetic arm and on multiple participants highlighting the generalizability across users. The system is optimized for low-power embedded deployment, ensuring practical real-world application beyond high-performance computing environments. Our results indicate that BRAVE offers a promising step towards robust, real-time, non-invasive prosthetic control.

</details>


### [20] [Patient-Centred Explainability in IVF Outcome Prediction](https://arxiv.org/abs/2506.18760)

*Adarsa Sivaprasad, Ehud Reiter, David McLernon, Nava Tintarev, Siladitya Bhattacharya, Nir Oren*

**Main category:** cs.HC

**Keywords:** IVF, user interface, explainable AI, trust, healthcare

**Relevance Score:** 8

**TL;DR:** The paper evaluates an IVF outcome prediction tool's user interface, focusing on patient understandability and trust through user feedback and surveys.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the understandability of an IVF outcome prediction tool for patients and improve user trust through better explainability.

**Method:** Analysis of four years of anonymous patient feedback, complemented by a user survey and interviews to gauge understanding and trust.

**Key Contributions:**

	1. Identified user needs for explainability beyond model features.
	2. Highlighted shortcomings in current explainable AI practices for healthcare.
	3. Proposed a dialogue-based interface for personalized explanations.

**Result:** Findings indicate that lay users require more explainability than what is currently offered, pointing to significant user concerns about data relevance and model exclusions that influence their trust in such tools.

**Limitations:** The focus is primarily on a specific healthcare context (IVF), which may not generalize to other areas in healthcare or different AI applications.

**Conclusion:** The study emphasizes the necessity for enhanced explainability in AI solutions within healthcare, especially when users build complex mental models based on extensive information.

**Abstract:** This paper evaluates the user interface of an in vitro fertility (IVF) outcome prediction tool, focussing on its understandability for patients or potential patients. We analyse four years of anonymous patient feedback, followed by a user survey and interviews to quantify trust and understandability. Results highlight a lay user's need for prediction model \emph{explainability} beyond the model feature space. We identify user concerns about data shifts and model exclusions that impact trust. The results call attention to the shortcomings of current practices in explainable AI research and design and the need for explainability beyond model feature space and epistemic assumptions, particularly in high-stakes healthcare contexts where users gather extensive information and develop complex mental models. To address these challenges, we propose a dialogue-based interface and explore user expectations for personalised explanations.

</details>


### [21] [Importance of User Control in Data-Centric Steering for Healthcare Experts](https://arxiv.org/abs/2506.18770)

*Aditya Bhattacharya, Simone Stumpf, Katrien Verbert*

**Main category:** cs.HC

**Keywords:** AI in healthcare, human-AI collaboration, data-centric steering

**Relevance Score:** 9

**TL;DR:** This paper investigates how varying levels of user control in data-centric steering influence healthcare experts' collaboration with AI systems, revealing that manual control enhances model performance and trust.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding the impact of user control on healthcare experts during data-centric steering with AI systems.

**Method:** A between-subjects, mixed-methods user study involving 74 healthcare experts was conducted to compare manual and automated steering approaches.

**Key Contributions:**

	1. Demonstrated the effectiveness of manual steering in improving model performance.
	2. Identified the importance of trust and understandability in human-AI collaboration.
	3. Proposed design implications for a hybrid steering system.
	4. 

**Result:** Manual steering significantly improves model performance, trust, and system understandability compared to automated approaches.

**Limitations:** 

**Conclusion:** The study suggests the development of a hybrid steering system that integrates both manual and automated elements to enhance user involvement in AI collaboration.

**Abstract:** As Artificial Intelligence (AI) becomes increasingly integrated into high-stakes domains like healthcare, effective collaboration between healthcare experts and AI systems is critical. Data-centric steering, which involves fine-tuning prediction models by improving training data quality, plays a key role in this process. However, little research has explored how varying levels of user control affect healthcare experts during data-centric steering. We address this gap by examining manual and automated steering approaches through a between-subjects, mixed-methods user study with 74 healthcare experts. Our findings show that manual steering, which grants direct control over training data, significantly improves model performance while maintaining trust and system understandability. Based on these findings, we propose design implications for a hybrid steering system that combines manual and automated approaches to increase user involvement during human-AI collaboration.

</details>


### [22] [Flow-Aware Diffusion for Real-Time VR Restoration: Enhancing Spatiotemporal Coherence and Efficiency](https://arxiv.org/abs/2506.18786)

*Yitong Zhu, Guanxuan Jiang, Zhuowen Liang, Yuyang Wang*

**Main category:** cs.HC

**Keywords:** Cybersickness, Virtual Reality, Optical Flow, AI-based solution, User experience

**Relevance Score:** 7

**TL;DR:** U-MAD is an AI-based solution that reduces cybersickness in Virtual Reality by suppressing disruptive optical flow at the image level, enhancing user comfort without the need for scene-specific adjustments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Cybersickness hinders VR adoption due to sensory conflicts caused by unmatched visual and vestibular inputs. This work addresses these issues with a novel AI-based method.

**Method:** U-MAD suppresses high-intensity optical flow directly from rendered frames using perceptually guided modulation, functioning as a plug and play module in VR environments.

**Key Contributions:**

	1. Introduction of U-MAD, an AI-based optical flow suppression method
	2. Plug and play integration with existing VR systems
	3. Demonstrated effectiveness in reducing cybersickness symptoms through user studies

**Result:** Experiments showed significant reductions in optical flow and improvements in temporal stability, while user studies indicated enhanced perceptual comfort and alleviated symptoms of cybersickness.

**Limitations:** 

**Conclusion:** The proposed method shows promise in creating more immersive and user-friendly VR experiences through effective modulation of visual motion.

**Abstract:** Cybersickness remains a critical barrier to the widespread adoption of Virtual Reality (VR), particularly in scenarios involving intense or artificial motion cues. Among the key contributors is excessive optical flow-perceived visual motion that, when unmatched by vestibular input, leads to sensory conflict and discomfort. While previous efforts have explored geometric or hardware based mitigation strategies, such methods often rely on predefined scene structures, manual tuning, or intrusive equipment. In this work, we propose U-MAD, a lightweight, real-time, AI-based solution that suppresses perceptually disruptive optical flow directly at the image level. Unlike prior handcrafted approaches, this method learns to attenuate high-intensity motion patterns from rendered frames without requiring mesh-level editing or scene specific adaptation. Designed as a plug and play module, U-MAD integrates seamlessly into existing VR pipelines and generalizes well to procedurally generated environments. The experiments show that U-MAD consistently reduces average optical flow and enhances temporal stability across diverse scenes. A user study further confirms that reducing visual motion leads to improved perceptual comfort and alleviated cybersickness symptoms. These findings demonstrate that perceptually guided modulation of optical flow provides an effective and scalable approach to creating more user-friendly immersive experiences. The code will be released at https://github.com/XXXXX (upon publication).

</details>


### [23] [LightVA: Lightweight Visual Analytics with LLM Agent-Based Task Planning and Execution](https://arxiv.org/abs/2411.05651)

*Yuheng Zhao, Junjie Wang, Linbin Xiang, Xiaowen Zhang, Zifei Guo, Cagatay Turkay, Yu Zhang, Siming Chen*

**Main category:** cs.HC

**Keywords:** Visual Analytics, Large Language Models, Human-Agent Collaboration

**Relevance Score:** 8

**TL;DR:** LightVA is a lightweight framework that enhances visual analytics through LLM-based collaboration, enabling task decomposition and interactive exploration.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the complexity of visual analytics, requiring programming and data skills, by proposing a more intelligent approach using LLMs.

**Method:** LightVA employs a recursive task planning and execution strategy involving a planner for task decomposition, an executor for executing tasks and generating visualizations, and a controller for coordinating interactions.

**Key Contributions:**

	1. Introduction of LightVA framework for visual analytics
	2. LLM agent-based task planning and execution
	3. Development of a hybrid user interface for interactive exploration and task management

**Result:** The proposed method effectively facilitates the translation of high-level analytic goals into actionable tasks, showcasing improved interaction and exploration in visual analytics.

**Limitations:** 

**Conclusion:** LightVA enhances the visual analytics process by streamlining task management and enabling efficient human-agent collaboration through a hybrid user interface.

**Abstract:** Visual analytics (VA) requires analysts to iteratively propose analysis tasks based on observations and execute tasks by creating visualizations and interactive exploration to gain insights. This process demands skills in programming, data processing, and visualization tools, highlighting the need for a more intelligent, streamlined VA approach. Large language models (LLMs) have recently been developed as agents to handle various tasks with dynamic planning and tool-using capabilities, offering the potential to enhance the efficiency and versatility of VA. We propose LightVA, a lightweight VA framework that supports task decomposition, data analysis, and interactive exploration through human-agent collaboration. Our method is designed to help users progressively translate high-level analytical goals into low-level tasks, producing visualizations and deriving insights. Specifically, we introduce an LLM agent-based task planning and execution strategy, employing a recursive process involving a planner, executor, and controller. The planner is responsible for recommending and decomposing tasks, the executor handles task execution, including data analysis, visualization generation and multi-view composition, and the controller coordinates the interaction between the planner and executor. Building on the framework, we develop a system with a hybrid user interface that includes a task flow diagram for monitoring and managing the task planning process, a visualization panel for interactive data exploration, and a chat view for guiding the model through natural language instructions. We examine the effectiveness of our method through a usage scenario and an expert study.

</details>


### [24] [Generative AI & Changing Work: Systematic Review of Practitioner-led Work Transformations through the Lens of Job Crafting](https://arxiv.org/abs/2502.08854)

*Matthew Law, Rama Adithya Varanasi*

**Main category:** cs.HC

**Keywords:** Generative AI, white-collar work, task management, professional identity, collaboration

**Relevance Score:** 7

**TL;DR:** The paper reviews how Generative AI (GenAI) tools affect white-collar work, task management, and collaboration, emphasizing the emergence of new AI managerial labor and changes in professional identity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an understanding of worker-driven patterns resulting from the integration of Generative AI tools in white-collar work.

**Method:** Conducted a systematic literature review of 23 studies from the ACM Digital Library focused on workers' experiences with Generative AI.

**Key Contributions:**

	1. Identification of new AI managerial labor
	2. Insight into shifts in professional identity and collaboration
	3. Recommendation for evolving job crafting frameworks

**Result:** Findings indicate that professionals are delegating routine tasks to GenAI while taking on new managerial roles; collaborations are evolving, causing tensions around roles and identity.

**Limitations:** 

**Conclusion:** Job crafting frameworks must evolve to better accommodate the complexities introduced by GenAI transformations in workforce dynamics.

**Abstract:** Widespread integration of Generative AI tools is transforming white-collar work, reshaping how workers define their roles, manage their tasks, and collaborate with peers. This has created a need to develop an overarching understanding of common worker-driven patterns around these transformations. To fill this gap, we conducted a systematic literature review of 23 studies from the ACM Digital Library that focused on workers' lived-experiences and practitioners with GenAI. Our findings reveal that while many professionals have delegated routine tasks to GenAI to focus on core responsibilities, they have also taken on new forms of AI managerial labor to monitor and refine GenAI outputs. Additionally, practitioners have restructured collaborations, sometimes bypassing traditional peer and subordinate interactions in favor of GenAI assistance. These shifts have fragmented cohesive tasks into piecework creating tensions around role boundaries and professional identity. Our analysis suggests that current frameworks, like job crafting, need to evolve to address the complexities of GenAI-driven transformations.

</details>


### [25] [Beyond Subjectivity: Continuous Cybersickness Detection Using EEG-based Multitaper Spectrum Estimation](https://arxiv.org/abs/2503.22024)

*Berken Utku Demirel, Adnan Harun Dogan, Juliete Rossie, Max Moebus, Christian Holz*

**Main category:** cs.HC

**Keywords:** cybersickness, virtual reality, EEG, real-time monitoring, user experience

**Relevance Score:** 8

**TL;DR:** This paper presents a novel method for continuously detecting and quantifying cybersickness in virtual reality users using EEG and head motion signals.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of cybersickness in virtual reality which can hinder user experience and platform adoption.

**Method:** The authors developed a method that uses passively monitored EEG and head motion signals to track cybersickness levels in real-time without user-specific calibration.

**Key Contributions:**

	1. Continuous detection of cybersickness using passive EEG monitoring
	2. No need for user-specific calibration
	3. Real-time tracking of cybersickness levels

**Result:** The proposed method successfully estimates multitaper spectrums from EEG data while mitigating motion artifacts, allowing for continuous monitoring of cybersickness.

**Limitations:** The method relies on the accuracy of EEG data and may be influenced by external factors during monitoring.

**Conclusion:** This research provides a reproducible and objective approach to measuring cybersickness, potentially improving user experiences in VR.

**Abstract:** Virtual reality (VR) presents immersive opportunities across many applications, yet the inherent risk of developing cybersickness during interaction can severely reduce enjoyment and platform adoption. Cybersickness is marked by symptoms such as dizziness and nausea, which previous work primarily assessed via subjective post-immersion questionnaires and motion-restricted controlled setups. In this paper, we investigate the \emph{dynamic nature} of cybersickness while users experience and freely interact in VR. We propose a novel method to \emph{continuously} identify and quantitatively gauge cybersickness levels from users' \emph{passively monitored} electroencephalography (EEG) and head motion signals. Our method estimates multitaper spectrums from EEG, integrating specialized EEG processing techniques to counter motion artifacts, and, thus, tracks cybersickness levels in real-time. Unlike previous approaches, our method requires no user-specific calibration or personalization for detecting cybersickness. Our work addresses the considerable challenge of reproducibility and subjectivity in cybersickness research.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [26] [Outcome-Based Education: Evaluating Students' Perspectives Using Transformer](https://arxiv.org/abs/2506.17223)

*Shuvra Smaran Das, Anirban Saha Anik, Md Kishor Morol, Mohammad Sakib Mahmood*

**Main category:** cs.CL

**Keywords:** Outcome-Based Education, DistilBERT, sentiment analysis, LIME, NLP

**Relevance Score:** 7

**TL;DR:** The study implements transformer-based models, specifically DistilBERT, to analyze student feedback for improving educational outcomes in Outcome-Based Education (OBE).

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To assess and improve educational outcomes through the analysis of student feedback using NLP techniques.

**Method:** Implemented transformer-based models, particularly DistilBERT, and used LIME to interpret model predictions.

**Key Contributions:**

	1. Utilization of DistilBERT for sentiment analysis in educational feedback
	2. Application of LIME for interpretability of model predictions
	3. Alignment of analysis with the principles of Outcome-Based Education (OBE)

**Result:** The approach outperforms other machine learning models in sentiment classification and provides clear insights into how key terms influence sentiments.

**Limitations:** 

**Conclusion:** The combination of transformer models and LIME offers a robust framework for analyzing student feedback, leading to improved educational practices through data-driven insights.

**Abstract:** Outcome-Based Education (OBE) emphasizes the development of specific competencies through student-centered learning. In this study, we reviewed the importance of OBE and implemented transformer-based models, particularly DistilBERT, to analyze an NLP dataset that includes student feedback. Our objective is to assess and improve educational outcomes. Our approach is better than other machine learning models because it uses the transformer's deep understanding of language context to classify sentiment better, giving better results across a wider range of matrices. Our work directly contributes to OBE's goal of achieving measurable outcomes by facilitating the identification of patterns in student learning experiences. We have also applied LIME (local interpretable model-agnostic explanations) to make sure that model predictions are clear. This gives us understandable information about how key terms affect sentiment. Our findings indicate that the combination of transformer models and LIME explanations results in a strong and straightforward framework for analyzing student feedback. This aligns more closely with the principles of OBE and ensures the improvement of educational practices through data-driven insights.

</details>


### [27] [Efficient and Stealthy Jailbreak Attacks via Adversarial Prompt Distillation from LLMs to SLMs](https://arxiv.org/abs/2506.17231)

*Xiang Li, Chong Zhang, Jia Wang, Fangyu Wu, Yushi Li, Xiaobo Jin*

**Main category:** cs.CL

**Keywords:** large language models, jailbreaking, adversarial prompt distillation, security, reinforcement learning

**Relevance Score:** 7

**TL;DR:** This paper proposes an Adversarial Prompt Distillation method to enhance the efficiency and adaptability of jailbreak attacks on large language models (LLMs) using small language models (SLMs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current jailbreak methods for LLMs face issues such as inefficiency, high computational costs, and poor adaptability, hindering their effectiveness against evolving defenses.

**Method:** The proposed method combines masked language modeling, reinforcement learning, and dynamic temperature control in a prompt generation and distillation framework to enable SLMs to conduct jailbreak attacks on LLMs.

**Key Contributions:**

	1. Introduction of Adversarial Prompt Distillation for jailbreak attacks
	2. Demonstrated improved attack success rates and adaptability
	3. Provided insights into LLM vulnerabilities for security research

**Result:** Experimental results show that the Adversarial Prompt Distillation method significantly improves the attack success rate and harmful impact while being resource-efficient and adaptable across different models.

**Limitations:** 

**Conclusion:** The research highlights the potential to distill jailbreak capabilities from LLMs to SLMs, showcasing vulnerabilities in models and providing a new direction for security research in LLMs.

**Abstract:** Attacks on large language models (LLMs) in jailbreaking scenarios raise many security and ethical issues. Current jailbreak attack methods face problems such as low efficiency, high computational cost, and poor cross-model adaptability and versatility, which make it difficult to cope with the rapid development of LLM and new defense strategies. Our work proposes an Adversarial Prompt Distillation, which combines masked language modeling, reinforcement learning, and dynamic temperature control through a prompt generation and distillation method. It enables small language models (SLMs) to jailbreak attacks on mainstream LLMs. The experimental results verify the superiority of the proposed method in terms of attack success rate and harm, and reflect the resource efficiency and cross-model adaptability. This research explores the feasibility of distilling the jailbreak ability of LLM to SLM, reveals the model's vulnerability, and provides a new idea for LLM security research.

</details>


### [28] [GTA: Grouped-head latenT Attention](https://arxiv.org/abs/2506.17286)

*Luoyang Sun, Jiwen Jiang, Cheng Deng, Xinjian Wu, Haifeng Zhang, Lei Chen, Lionel Ni, Jun Wang*

**Main category:** cs.CL

**Keywords:** attention mechanisms, large language models, computational efficiency, memory optimization, GTA

**Relevance Score:** 9

**TL;DR:** Grouped-Head Latent Attention (GTA) optimizes attention mechanisms in LLMs by reducing memory usage and computational complexity while improving performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses computational and memory overhead in large language models due to expensive attention mechanisms, especially as text length increases.

**Method:** The proposed GTA mechanism includes a shared attention map that reduces key cache size and a nonlinear value decoder to compress the value cache, leading to less resource consumption.

**Key Contributions:**

	1. Introduction of Grouped-Head Latent Attention (GTA) that shares attention maps across heads.
	2. Significant reductions in memory and computation overhead for LLMs.
	3. Demonstrated 2x increase in inference speed without additional overhead.

**Result:** GTA decreases attention computation FLOPs by up to 62.5% and shrinks the KV cache by up to 70%, resulting in a 2x increase in end-to-end inference speed.

**Limitations:** 

**Conclusion:** GTA enhances LLM efficiency, making it more viable for deployment on resource-constrained hardware.

**Abstract:** Attention mechanisms underpin the success of large language models (LLMs), yet their substantial computational and memory overhead poses challenges for optimizing efficiency and performance. A critical bottleneck arises as KV cache and attention computations scale rapidly with text length, challenging deployment on hardware with limited computational and memory resources. We observe that attention mechanisms exhibit substantial redundancy, since the KV cache can be significantly compressed and attention maps across heads display high similarity, revealing that much of the computation and storage is unnecessary. Leveraging these insights, we propose \textbf{G}rouped-Head Laten\textbf{T} \textbf{A}ttention (GTA), a novel attention mechanism that reduces memory usage and computational complexity while maintaining performance. GTA comprises two components: (1) a shared attention map mechanism that reuses attention scores across multiple heads, decreasing the key cache size; and (2) a nonlinear value decoder with learned projections that compresses the value cache into a latent space, further cutting memory needs. GTA cuts attention computation FLOPs by up to \emph{62.5\%} versus Grouped-Query Attention and shrink the KV cache by up to \emph{70\%}, all while avoiding the extra overhead of Multi-Head Latent Attention to improve LLM deployment efficiency. Consequently, GTA models achieve a \emph{2x} increase in end-to-end inference speed, with prefill benefiting from reduced computational cost and decoding benefiting from the smaller cache footprint.

</details>


### [29] [AI-Generated Game Commentary: A Survey and a Datasheet Repository](https://arxiv.org/abs/2506.17294)

*Qirui Zheng, Xingbo Wang, Keyuan Cheng, Yunlong Lu, Wenxin Li*

**Main category:** cs.CL

**Keywords:** AI-Generated Game Commentary, Natural Language Processing, Datasets survey

**Relevance Score:** 3

**TL;DR:** This paper introduces a framework for AI-Generated Game Commentary (AIGGC), surveys existing datasets and methods, and evaluates metrics in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The growing interest in AI-Generated Game Commentary (AIGGC) stems from its market potential and technical challenges in NLP tasks.

**Method:** The paper presents a comprehensive survey of 45 datasets and methods related to game commentary, categorizing them based on key challenges and evaluating common metrics.

**Key Contributions:**

	1. Introduction of a general framework for AIGGC
	2. Comprehensive survey of 45 datasets and methods
	3. Provision of a structured datasheet for future research

**Result:** The study provides a structured datasheet summarizing essential attributes of the surveyed datasets, facilitating future research and benchmarking.

**Limitations:** 

**Conclusion:** The proposed framework and datasheet will support ongoing advancements in AIGGC research by organizing existing knowledge and available resources.

**Abstract:** AI-Generated Game Commentary (AIGGC) has gained increasing attention due to its market potential and inherent technical challenges. As a comprehensive multimodal Natural Language Processing (NLP) task, AIGGC imposes substantial demands on language models, including factual accuracy, logical reasoning, expressive text generation, generation speed, and context management. In this paper, we introduce a general framework for AIGGC and present a comprehensive survey of 45 existing game commentary dataset and methods according to key challenges they aim to address in this domain. We further classify and compare various evaluation metrics commonly used in this domain. To support future research and benchmarking, we also provide a structured datasheet summarizing the essential attributes of these datasets in appendix, which is meanwhile publicly available in an open repository.

</details>


### [30] [Semantic uncertainty in advanced decoding methods for LLM generation](https://arxiv.org/abs/2506.17296)

*Darius Foodeei, Simin Fan, Martin Jaggi*

**Main category:** cs.CL

**Keywords:** large language models, semantic uncertainty, decoding methods, diversity, reliability

**Relevance Score:** 9

**TL;DR:** The study explores the effects of different decoding methods on semantic uncertainty in LLM outputs, revealing that structured techniques can improve both diversity and reliability.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how various decoding methods affect the outputs of large language models in terms of semantic uncertainty, reliability, and diversity.

**Method:** Experiments were conducted on question answering, summarization, and code generation tasks using different decoding techniques including speculative sampling and chain-of-thought (CoT) decoding.

**Key Contributions:**

	1. Analysis of various decoding strategies in LLM outputs
	2. Demonstration that CoT decoding increases diversity while maintaining reliability
	3. Evidence that speculative sampling enhances summarization performance

**Result:** CoT decoding improved code generation Pass@2 rates by 48.8% but had lower alignment with reference solutions, while speculative sampling enhanced ROUGE scores for summarization without sacrificing semantic diversity.

**Limitations:** 

**Conclusion:** Structured decoding methods can enhance semantic exploration and maintain or improve output quality, challenging established views on the trade-off between diversity and accuracy.

**Abstract:** This study investigates semantic uncertainty in large language model (LLM) outputs across different decoding methods, focusing on emerging techniques like speculative sampling and chain-of-thought (CoT) decoding. Through experiments on question answering, summarization, and code generation tasks, we analyze how different decoding strategies affect both the diversity and reliability of model outputs. Our findings reveal that while CoT decoding demonstrates higher semantic diversity, it maintains lower predictive entropy, suggesting that structured exploration can lead to more confident and accurate outputs. This is evidenced by a 48.8% improvement in code generation Pass@2 rates, despite lower alignment with reference solutions. For summarization tasks, speculative sampling proved particularly effective, achieving superior ROUGE scores while maintaining moderate semantic diversity. Our results challenge conventional assumptions about trade-offs between diversity and accuracy in language model outputs, demonstrating that properly structured decoding methods can increase semantic exploration while maintaining or improving output quality. These findings have significant implications for deploying language models in practical applications where both reliability and diverse solution generation are crucial.

</details>


### [31] [Mercury: Ultra-Fast Language Models Based on Diffusion](https://arxiv.org/abs/2506.17298)

*Inception Labs, Samar Khanna, Siddhant Kharbanda, Shufan Li, Harshit Varma, Eric Wang, Sawyer Birnbaum, Ziyang Luo, Yanis Miraoui, Akash Palrecha, Stefano Ermon, Aditya Grover, Volodymyr Kuleshov*

**Main category:** cs.CL

**Keywords:** Large Language Models, Diffusion Models, Coding Applications

**Relevance Score:** 9

**TL;DR:** Mercury is a new generation of diffusion-based large language models optimized for coding applications, achieving state-of-the-art performance in speed and quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop commercial-scale LLMs optimized for coding applications that exceed current performance benchmarks.

**Method:** Mercury Coder utilizes the Transformer architecture and is trained to predict multiple tokens in parallel, resulting in two model sizes: Mini and Small.

**Key Contributions:**

	1. Development of diffusion-based LLMs for coding applications.
	2. Achieving new state-of-the-art speeds in token processing.
	3. Real-world validation of model performance on Copilot Arena.

**Result:** Mercury Coder Mini and Small achieve throughputs of 1109 tokens/sec and 737 tokens/sec respectively, outperforming speed-optimized models by up to 10x while maintaining quality, validated by real-world performance on Copilot Arena.

**Limitations:** 

**Conclusion:** The Mercury Coder models represent a significant advancement in coding LLMs, with practical applications and a public API for developers.

**Abstract:** We present Mercury, a new generation of commercial-scale large language models (LLMs) based on diffusion. These models are parameterized via the Transformer architecture and trained to predict multiple tokens in parallel. In this report, we detail Mercury Coder, our first set of diffusion LLMs designed for coding applications. Currently, Mercury Coder comes in two sizes: Mini and Small. These models set a new state-of-the-art on the speed-quality frontier. Based on independent evaluations conducted by Artificial Analysis, Mercury Coder Mini and Mercury Coder Small achieve state-of-the-art throughputs of 1109 tokens/sec and 737 tokens/sec, respectively, on NVIDIA H100 GPUs and outperform speed-optimized frontier models by up to 10x on average while maintaining comparable quality. We discuss additional results on a variety of code benchmarks spanning multiple languages and use-cases as well as real-world validation by developers on Copilot Arena, where the model currently ranks second on quality and is the fastest model overall. We also release a public API at https://platform.inceptionlabs.ai/ and free playground at https://chat.inceptionlabs.ai

</details>


### [32] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)

*Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** e-commerce, product descriptions, customer reviews, large language models, PRAISE

**Relevance Score:** 8

**TL;DR:** PRAISE is a system that leverages Large Language Models to automatically extract and structure product insights from customer reviews and seller descriptions, enhancing e-commerce product listings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Accurate product descriptions in e-commerce are essential, yet often incomplete or misleading due to inadequate seller input and the overwhelming volume of customer reviews.

**Method:** PRAISE utilizes Large Language Models to compare customer reviews and seller descriptions, structuring insights and discrepancies for easy analysis.

**Key Contributions:**

	1. Introduction of PRAISE for structured insight extraction from reviews.
	2. Integration of LLMs for improved accuracy in product feature identification.
	3. User interface designed to present discrepancies clearly for sellers and buyers.

**Result:** PRAISE provides structured insights that highlight missing or contradictory information, improving product listings and aiding buyers in evaluating product reliability.

**Limitations:** 

**Conclusion:** The system has the potential to significantly enhance the quality and trustworthiness of e-commerce product catalogs by making review insights more accessible and actionable.

**Abstract:** Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.

</details>


### [33] [Towards Safety Evaluations of Theory of Mind in Large Language Models](https://arxiv.org/abs/2506.17352)

*Tatsuhiro Aoshima, Mitsuaki Akiyama*

**Main category:** cs.CL

**Keywords:** large language models, theory of mind, safety evaluation, deceptive behaviors, developmental psychology

**Relevance Score:** 9

**TL;DR:** This study evaluates the theory of mind capabilities of large language models (LLMs) in light of their safety risks, revealing that while reading comprehension has improved, theory of mind capabilities have not.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With the advancement of LLMs, concerns about their behaviors affecting safety and oversight mechanisms have emerged, necessitating a rigorous evaluation of their deceptive actions and the potential risks to users.

**Method:** The study reviews existing research on theory of mind, analyzes developmental trends in a series of open-weight LLMs, and assesses their capabilities in safety evaluation.

**Key Contributions:**

	1. Measurement of LLMs' theory of mind capabilities
	2. Identification of gaps in safety evaluation
	3. Review of developmental trends in LLMs' cognitive abilities

**Result:** The analysis shows that LLMs have made significant advancements in reading comprehension, but their theory of mind capabilities have not developed at the same pace, indicating potential gaps in their safety evaluation.

**Limitations:** 

**Conclusion:** The findings highlight the need for continued research on the theory of mind in LLMs and address challenges in safety evaluation for ensuring reliable model behavior.

**Abstract:** As the capabilities of large language models (LLMs) continue to advance, the importance of rigorous safety evaluation is becoming increasingly evident. Recent concerns within the realm of safety assessment have highlighted instances in which LLMs exhibit behaviors that appear to disable oversight mechanisms and respond in a deceptive manner. For example, there have been reports suggesting that, when confronted with information unfavorable to their own persistence during task execution, LLMs may act covertly and even provide false answers to questions intended to verify their behavior.To evaluate the potential risk of such deceptive actions toward developers or users, it is essential to investigate whether these behaviors stem from covert, intentional processes within the model. In this study, we propose that it is necessary to measure the theory of mind capabilities of LLMs. We begin by reviewing existing research on theory of mind and identifying the perspectives and tasks relevant to its application in safety evaluation. Given that theory of mind has been predominantly studied within the context of developmental psychology, we analyze developmental trends across a series of open-weight LLMs. Our results indicate that while LLMs have improved in reading comprehension, their theory of mind capabilities have not shown comparable development. Finally, we present the current state of safety evaluation with respect to LLMs' theory of mind, and discuss remaining challenges for future work.

</details>


### [34] [Cash or Comfort? How LLMs Value Your Inconvenience](https://arxiv.org/abs/2506.17367)

*Mateusz Cedro, Timour Ichmoukhamedov, Sofie Goethals, Yifan He, James Hinns, David Martens*

**Main category:** cs.CL

**Keywords:** Large Language Models, decision-making, user discomfort, financial rewards, AI ethics

**Relevance Score:** 8

**TL;DR:** This paper explores how Large Language Models (LLMs) behave in personal decision-making scenarios involving user discomfort and financial incentives.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to investigate the behavior of LLMs as decision-making aids, particularly focusing on how they evaluate discomfort versus financial gain, which has not been deeply explored in prior work.

**Method:** The authors quantified the monetary values assigned by multiple LLMs to various user discomforts such as walking, waiting, hunger, and pain, and analyzed the variance in their responses.

**Key Contributions:**

	1. Identification of response variance among LLMs in decision-making contexts.
	2. Highlighting the sensitivity of LLM responses to minor prompt changes.
	3. Demonstrating LLMs' capacity to undervalue significant discomforts in monetary terms.

**Result:** The study reveals significant concerns regarding LLMs as decision-making assistants: high response variance among LLMs, sensitivity of responses to prompt phrasing, acceptance of low rewards for major inconveniences, and rejection of monetary gains when no discomfort is involved.

**Limitations:** 

**Conclusion:** These findings raise important questions about the reliability of LLMs in valuing human inconvenience, especially as they may be used for decisions on behalf of users moving forward.

**Abstract:** Large Language Models (LLMs) are increasingly proposed as near-autonomous artificial intelligence (AI) agents capable of making everyday decisions on behalf of humans. Although LLMs perform well on many technical tasks, their behaviour in personal decision-making remains less understood. Previous studies have assessed their rationality and moral alignment with human decisions. However, the behaviour of AI assistants in scenarios where financial rewards are at odds with user comfort has not yet been thoroughly explored. In this paper, we tackle this problem by quantifying the prices assigned by multiple LLMs to a series of user discomforts: additional walking, waiting, hunger and pain. We uncover several key concerns that strongly question the prospect of using current LLMs as decision-making assistants: (1) a large variance in responses between LLMs, (2) within a single LLM, responses show fragility to minor variations in prompt phrasing (e.g., reformulating the question in the first person can considerably alter the decision), (3) LLMs can accept unreasonably low rewards for major inconveniences (e.g., 1 Euro to wait 10 hours), and (4) LLMs can reject monetary gains where no discomfort is imposed (e.g., 1,000 Euro to wait 0 minutes). These findings emphasize the need for scrutiny of how LLMs value human inconvenience, particularly as we move toward applications where such cash-versus-comfort trade-offs are made on users' behalf.

</details>


### [35] [Leveraging LLMs to Assess Tutor Moves in Real-Life Dialogues: A Feasibility Study](https://arxiv.org/abs/2506.17410)

*Danielle R. Thomas, Conrad Borchers, Jionghao Lin, Sanjit Kakarla, Shambhavi Bhushan, Erin Gatz, Shivang Gupta, Ralph Abboud, Kenneth R. Koedinger*

**Main category:** cs.CL

**Keywords:** Tutoring, Generative AI, Large Language Models, Education Technology, Assessment

**Relevance Score:** 8

**TL;DR:** The study investigates the use of generative AI to analyze tutor actions in remote math tutoring by examining transcripts and evaluating tutor effectiveness using various AI models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Identifying effective tutoring actions based on audio transcriptions can enhance student learning, making it essential to use AI for scalable assessments in educational settings.

**Method:** The study analyzed 50 transcripts of college students tutoring middle school students, employing GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM to assess tutor skills like praise delivery and error response.

**Key Contributions:**

	1. Use of generative AI to evaluate tutoring actions
	2. High accuracy in assessing tutor skills from transcripts
	3. Cost-effective prompting strategy for LLMs in educational settings

**Result:** All models exhibited high accuracy in detecting tutor actions, with praise recognition at 94-98% and error detection at 82-88%. The models closely matched human evaluations in adherence to tutoring best practices.

**Limitations:** 

**Conclusion:** Generative AI can effectively evaluate tutoring practices, suggesting a feasible approach for scalable assessment in educational contexts and providing reusable LLM prompts for future research.

**Abstract:** Tutoring improves student achievement, but identifying and studying what tutoring actions are most associated with student learning at scale based on audio transcriptions is an open research problem. This present study investigates the feasibility and scalability of using generative AI to identify and evaluate specific tutor moves in real-life math tutoring. We analyze 50 randomly selected transcripts of college-student remote tutors assisting middle school students in mathematics. Using GPT-4, GPT-4o, GPT-4-turbo, Gemini-1.5-pro, and LearnLM, we assess tutors' application of two tutor skills: delivering effective praise and responding to student math errors. All models reliably detected relevant situations, for example, tutors providing praise to students (94-98% accuracy) and a student making a math error (82-88% accuracy) and effectively evaluated the tutors' adherence to tutoring best practices, aligning closely with human judgments (83-89% and 73-77%, respectively). We propose a cost-effective prompting strategy and discuss practical implications for using large language models to support scalable assessment in authentic settings. This work further contributes LLM prompts to support reproducibility and research in AI-supported learning.

</details>


### [36] [UProp: Investigating the Uncertainty Propagation of LLMs in Multi-Step Agentic Decision-Making](https://arxiv.org/abs/2506.17419)

*Jinhao Duan, James Diffenderfer, Sandeep Madireddy, Tianlong Chen, Bhavya Kailkhura, Kaidi Xu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Uncertainty Quantification, Sequential Decision-Making, Information Theory, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper presents UProp, an information-theoretic framework for assessing uncertainty in the sequential decision-making processes of Large Language Models (LLMs), addressing shortcomings of existing methods focused on single-turn question-answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There's a need to understand when to trust LLM decisions in safety-critical applications involving multi-step decision-making, as current uncertainty quantification methods are insufficient.

**Method:** The paper proposes an information-theoretic framework that decomposes LLM sequential decision uncertainty into internal and extrinsic uncertainties, using the UProp estimator for effective extrinsic uncertainty evaluation based on Pointwise Mutual Information (PMI).

**Key Contributions:**

	1. Introduction of a dual-component uncertainty framework for LLMs
	2. Development of the UProp estimator for extrinsic uncertainty estimation
	3. Extensive evaluation on multi-step decision-making benchmarks.

**Result:** UProp outperforms existing single-turn uncertainty quantification methods in extensive multi-step decision-making benchmarks such as AgentBench and HotpotQA, demonstrating enhanced efficiency and effectiveness in uncertainty estimation.

**Limitations:** 

**Conclusion:** The comprehensive evaluation shows UProp's advantages in sampling efficiency and potential applications for improving decision-making reliability in LLMs.

**Abstract:** As Large Language Models (LLMs) are integrated into safety-critical applications involving sequential decision-making in the real world, it is essential to know when to trust LLM decisions. Existing LLM Uncertainty Quantification (UQ) methods are primarily designed for single-turn question-answering formats, resulting in multi-step decision-making scenarios, e.g., LLM agentic system, being underexplored. In this paper, we introduce a principled, information-theoretic framework that decomposes LLM sequential decision uncertainty into two parts: (i) internal uncertainty intrinsic to the current decision, which is focused on existing UQ methods, and (ii) extrinsic uncertainty, a Mutual-Information (MI) quantity describing how much uncertainty should be inherited from preceding decisions. We then propose UProp, an efficient and effective extrinsic uncertainty estimator that converts the direct estimation of MI to the estimation of Pointwise Mutual Information (PMI) over multiple Trajectory-Dependent Decision Processes (TDPs). UProp is evaluated over extensive multi-step decision-making benchmarks, e.g., AgentBench and HotpotQA, with state-of-the-art LLMs, e.g., GPT-4.1 and DeepSeek-V3. Experimental results demonstrate that UProp significantly outperforms existing single-turn UQ baselines equipped with thoughtful aggregation strategies. Moreover, we provide a comprehensive analysis of UProp, including sampling efficiency, potential applications, and intermediate uncertainty propagation, to demonstrate its effectiveness. Codes will be available at https://github.com/jinhaoduan/UProp.

</details>


### [37] [Beyond the Link: Assessing LLMs' ability to Classify Political Content across Global Media](https://arxiv.org/abs/2506.17435)

*Alberto Martinez-Serra, Alejandro De La Fuente, Nienke Viescher, Ana S. Cardenal*

**Main category:** cs.CL

**Keywords:** Large Language Models, Political Content Classification, URL Analysis, Machine Learning, Political Science

**Relevance Score:** 6

**TL;DR:** This paper evaluates the capacity of large language models (LLMs) to classify political content from URLs, finding URLs can effectively approximate full-text analysis across different languages and countries.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of LLMs in classifying political content from URLs, a gap identified in current research regarding LLM abilities beyond labeling tasks.

**Method:** This study uses various LLMs (GPT, Llama, Mistral, Deepseek, Qwen, Gemma) to classify articles as political or non-political from both URL and full-text analysis, comparing results against human-labeling and traditional machine learning methods.

**Key Contributions:**

	1. Demonstrates LLM effectiveness in URL-based classification of political content.
	2. Introduces a novel approach to balance accuracy and cost in political content analysis.
	3. Provides recommendations for methodological practices in political science employing LLMs.

**Result:** The study finds that URLs can successfully encapsulate the majority of news content, making them a viable alternative for political content classification across linguistic and national contexts.

**Limitations:** The study acknowledges contextual limitations which may affect the generalizability of the results across different political landscapes.

**Conclusion:** The results indicate the potential for URL-based political content classification and highlight the importance of considering accuracy-cost tradeoffs, alongside proposing methodological recommendations for political science research.

**Abstract:** The use of large language models (LLMs) is becoming common in the context of political science, particularly in studies that analyse individuals use of digital media. However, while previous research has demonstrated LLMs ability at labelling tasks, the effectiveness of using LLMs to classify political content (PC) from just URLs is not yet well explored. The work presented in this article bridges this gap by evaluating whether LLMs can accurately identify PC vs. non-PC from both the article text and the URLs from five countries (France, Germany, Spain, the UK, and the US) and different languages. Using cutting-edge LLMs like GPT, Llama, Mistral, Deepseek, Qwen and Gemma, we measure model performance to assess whether URL-level analysis can be a good approximation for full-text analysis of PC, even across different linguistic and national contexts. Model outputs are compared with human-labelled articles, as well as traditional supervised machine learning techniques, to set a baseline of performance. Overall, our findings suggest the capacity of URLs to embed most of the news content, providing a vital perspective on accuracy-cost balancing. We also account for contextual limitations and suggest methodological recommendations to use LLMs within political science studies.

</details>


### [38] [Breaking the Transcription Bottleneck: Fine-tuning ASR Models for Extremely Low-Resource Fieldwork Languages](https://arxiv.org/abs/2506.17459)

*Siyu Liang, Gina-Anne Levow*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, low-resource languages, language documentation, field linguistics, ASR models

**Relevance Score:** 4

**TL;DR:** This paper benchmarks two multilingual ASR models on low-resource languages, analyzing their performance based on training data duration and providing practical guidelines for linguists.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of ASR systems in linguistic fieldwork, particularly for low-resource languages that have unique challenges in data collection.

**Method:** Benchmarking two fine-tuned multilingual ASR models, MMS and XLS-R, on five low-resource languages with varying amounts of training data.

**Key Contributions:**

	1. Benchmarking of ASR models on low-resource languages
	2. Identification of training data threshold effects on ASR performance
	3. Practical guidelines for field linguists on ASR adaptation

**Result:** MMS performs best with extremely small training datasets, while XLS-R achieves comparable performance when training data exceeds one hour.

**Limitations:** The study's findings are limited to five specific low-resource languages and may not generalize to all languages or ASR model types.

**Conclusion:** The study offers insights and practical guidelines for improving ASR adaptation in linguistic fieldwork to alleviate transcription challenges.

**Abstract:** Automatic Speech Recognition (ASR) has reached impressive accuracy for high-resource languages, yet its utility in linguistic fieldwork remains limited. Recordings collected in fieldwork contexts present unique challenges, including spontaneous speech, environmental noise, and severely constrained datasets from under-documented languages. In this paper, we benchmark the performance of two fine-tuned multilingual ASR models, MMS and XLS-R, on five typologically diverse low-resource languages with control of training data duration. Our findings show that MMS is best suited when extremely small amounts of training data are available, whereas XLS-R shows parity performance once training data exceed one hour. We provide linguistically grounded analysis for further provide insights towards practical guidelines for field linguists, highlighting reproducible ASR adaptation approaches to mitigate the transcription bottleneck in language documentation.

</details>


### [39] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)

*Weixin Liang*

**Main category:** cs.CL

**Keywords:** Large Language Models, AI Governance, Writing Domains, Research Feedback, Equity in AI

**Relevance Score:** 9

**TL;DR:** This dissertation explores the societal impact of large language models (LLMs) through institutional adoption, algorithmic measurement of LLM usage across various domains, and their potential to support researchers with manuscript feedback.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how individuals and institutions are adapting to LLM technology and the implications on equity, usage patterns, and researcher support.

**Method:** The dissertation encompasses three research directions: examining biases in AI detector adoption, algorithmic measurement of LLM prevalence in writing, and a large-scale analysis of LLM feedback capabilities on research manuscripts.

**Key Contributions:**

	1. Identification of biases in AI detector usage impacting diverse language writers
	2. Development of algorithmic approaches to measure LLM adoption across various writing domains
	3. Analysis of LLM feedback capabilities that can aid early-career researchers and those from under-resourced settings

**Result:** The research reveals systematic biases in AI governance against non-dominant language writers, identifies consistent patterns of AI-assisted content across diverse writing domains, and demonstrates the potential for LLMs to provide valuable feedback to under-resourced researchers.

**Limitations:** The study may not account for all language varieties or contexts in which LLMs interact with users.

**Conclusion:** LLMs have significant implications for writing practices, equity in AI application, and the support of early-career researchers, highlighting the need for awareness and adaptation in these areas.

**Abstract:** Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.

</details>


### [40] [VeriLocc: End-to-End Cross-Architecture Register Allocation via LLM](https://arxiv.org/abs/2506.17506)

*Lesheng Jin, Zhenyuan Ruan, Haohui Mai, Jingbo Shang*

**Main category:** cs.CL

**Keywords:** GPU, register allocation, large language models, compiler techniques, performance optimization

**Relevance Score:** 6

**TL;DR:** VeriLocc is a framework that combines large language models with formal compiler techniques for register allocation in GPUs, achieving high accuracy and better performance than existing libraries.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rapid evolution of modern GPUs necessitates new approaches in register allocation, as existing methods are outdated and require significant re-tuning.

**Method:** VeriLocc leverages a fine-tuned LLM to translate intermediate representations into specific register assignments, incorporating static analysis for normalization and a verifier-guided loop for correctness.

**Key Contributions:**

	1. Integration of LLM with formal compiler techniques for GPU register allocation
	2. High accuracy and performance improvements over existing libraries
	3. Verification loop ensuring correctness of assignments

**Result:** VeriLocc achieves 85-99% single-shot accuracy in register allocation and outperforms expert-tuned libraries like rocBLAS by over 10% in runtime.

**Limitations:** 

**Conclusion:** The framework demonstrated a significant advancement in generalizable and verifiable register allocation, proving its superiority over traditional methods.

**Abstract:** Modern GPUs evolve rapidly, yet production compilers still rely on hand-crafted register allocation heuristics that require substantial re-tuning for each hardware generation. We introduce VeriLocc, a framework that combines large language models (LLMs) with formal compiler techniques to enable generalizable and verifiable register allocation across GPU architectures. VeriLocc fine-tunes an LLM to translate intermediate representations (MIRs) into target-specific register assignments, aided by static analysis for cross-architecture normalization and generalization and a verifier-guided regeneration loop to ensure correctness. Evaluated on matrix multiplication (GEMM) and multi-head attention (MHA), VeriLocc achieves 85-99% single-shot accuracy and near-100% pass@100. Case study shows that VeriLocc discovers more performant assignments than expert-tuned libraries, outperforming rocBLAS by over 10% in runtime.

</details>


### [41] [Data Quality Issues in Multilingual Speech Datasets: The Need for Sociolinguistic Awareness and Proactive Language Planning](https://arxiv.org/abs/2506.17525)

*Mingfei Lau, Qian Chen, Yeming Fang, Tingting Xu, Tongzhou Chen, Pavel Golik*

**Main category:** cs.CL

**Keywords:** speech datasets, quality audit, ASR, sociolinguistics, multilingual

**Relevance Score:** 6

**TL;DR:** This paper audits three public multilingual speech datasets, identifying quality issues that hinder their effectiveness in training ASR models, particularly in under-resourced languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of widely used multilingual speech datasets for better training and evaluation of Automatic Speech Recognition (ASR) systems.

**Method:** Quality audit of Mozilla Common Voice 17.0, FLEURS, and VoxPopuli datasets focusing on micro-level and macro-level quality issues, with a case analysis of Taiwanese Southern Min.

**Key Contributions:**

	1. Identification of significant quality issues in multilingual speech datasets
	2. Case analysis demonstrating the impact of language planning on data quality
	3. Recommendations for enhancing dataset development practices

**Result:** Macro-level quality issues are more prevalent in less institutionalized languages, necessitating improved data quality control and proactive language planning.

**Limitations:** 

**Conclusion:** The paper proposes guidelines to mitigate identified quality issues, stressing the importance of sociolinguistic awareness in ASR dataset development.

**Abstract:** Our quality audit for three widely used public multilingual speech datasets - Mozilla Common Voice 17.0, FLEURS, and VoxPopuli - shows that in some languages, these datasets suffer from significant quality issues. We believe addressing these issues will make these datasets more useful as training and evaluation sets, and improve downstream models. We divide these quality issues into two categories: micro-level and macro-level. We find that macro-level issues are more prevalent in less institutionalized, often under-resourced languages. We provide a case analysis of Taiwanese Southern Min (nan_tw) that highlights the need for proactive language planning (e.g. orthography prescriptions, dialect boundary definition) and enhanced data quality control in the process of Automatic Speech Recognition (ASR) dataset creation. We conclude by proposing guidelines and recommendations to mitigate these issues in future dataset development, emphasizing the importance of sociolinguistic awareness in creating robust and reliable speech data resources.

</details>


### [42] [DuaShepherd: Integrating Stepwise Correctness and Potential Rewards for Mathematical Reasoning](https://arxiv.org/abs/2506.17533)

*Yuanhao Wu, Juntong Song, Hanning Zhang, Tong Zhang, Cheng Niu*

**Main category:** cs.CL

**Keywords:** reward modeling, large language models, mathematical reasoning, correctness signal, potential signal

**Relevance Score:** 8

**TL;DR:** DuaShepherd is a reward modeling framework that combines correctness and potential signals to improve mathematical reasoning in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the mathematical reasoning capabilities of Large Language Models by integrating two reward signals that reflect different aspects of reasoning.

**Method:** Developed an automated pipeline for creating a large-scale dataset combining correctness and potential signals, and employed a unified multi-head architecture to train two reward models in a multi-task setup.

**Key Contributions:**

	1. Introduced the DuaShepherd reward modeling framework
	2. Combined correctness and potential signals for improved performance
	3. Demonstrated state-of-the-art results on multiple benchmarks

**Result:** The model achieves significant performance improvements on MATH500 and ProcessBench, outperforming models trained on either reward alone.

**Limitations:** 

**Conclusion:** Combining correctness and potential signals into a compound probability leads to state-of-the-art performance in mathematical reasoning tasks with efficient resource use.

**Abstract:** In this paper, we propose DuaShepherd, a novel reward modeling framework that integrates two complementary reward signals, correctness and potential, to enhance the mathematical reasoning capabilities of Large Language Models (LLMs). While correctness-based signals emphasize identification of stepwise errors, potential-based signals focus on the likelihood of reaching the correct final answer. We developed an automated pipeline for constructing large-scale reward modeling dataset with both signals. A unified, multi-head architecture was explored to train the two reward models in a multi-task setup, demonstrating benefits from learning both correctness and potential in parallel. By combining these two signals into a compound probability, our model achieves consistent performance improvements across multiple benchmarks. Empirical evaluations on MATH500 and ProcessBench confirm that this combined reward significantly outperforms models trained on either reward type alone, achieving state-of-the-art performance under comparable resource constraints.

</details>


### [43] [Probing for Phonology in Self-Supervised Speech Representations: A Case Study on Accent Perception](https://arxiv.org/abs/2506.17542)

*Nitin Venkateswaran, Kevin Tang, Ratree Wayland*

**Main category:** cs.CL

**Keywords:** accent perception, self-supervised learning, phonological features, speech representation, non-native English

**Relevance Score:** 4

**TL;DR:** This paper investigates how self-supervised learning models of speech, particularly Wav2Vec2-BERT and WavLM, encode phonological variations affecting the perception of accents in English segments produced by Hindi speakers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the inadequacies of traditional accent perception models that overlook the importance of gradient phonological variations in listeners' judgments.

**Method:** Analyzed pretrained self-supervised learning models Wav2Vec2-BERT and WavLM using the CSLU Foreign Accented English corpus, focusing on specific segments articulated by Hindi speakers and their accent judgments from native American English speakers.

**Key Contributions:**

	1. Demonstration of self-supervised models' effectiveness in accent perception.
	2. Identification of critical phonological features influencing accent judgments.
	3. Quantitative relationships between segmental variations and perceived accent strength.

**Result:** Probing analyses indicated accent strength is primarily predicted by distinct pretrained representation features associated with salient phonological contrasts between native and non-native English segments.

**Limitations:** 

**Conclusion:** The study underscores the utility of self-supervised speech representations in understanding accent perception through interpretable phonological features.

**Abstract:** Traditional models of accent perception underestimate the role of gradient variations in phonological features which listeners rely upon for their accent judgments. We investigate how pretrained representations from current self-supervised learning (SSL) models of speech encode phonological feature-level variations that influence the perception of segmental accent. We focus on three segments: the labiodental approximant, the rhotic tap, and the retroflex stop, which are uniformly produced in the English of native speakers of Hindi as well as other languages in the Indian sub-continent. We use the CSLU Foreign Accented English corpus (Lander, 2007) to extract, for these segments, phonological feature probabilities using Phonet (V\'asquez-Correa et al., 2019) and pretrained representations from Wav2Vec2-BERT (Barrault et al., 2023) and WavLM (Chen et al., 2022) along with accent judgements by native speakers of American English. Probing analyses show that accent strength is best predicted by a subset of the segment's pretrained representation features, in which perceptually salient phonological features that contrast the expected American English and realized non-native English segments are given prominent weighting. A multinomial logistic regression of pretrained representation-based segment distances from American and Indian English baselines on accent ratings reveals strong associations between the odds of accent strength and distances from the baselines, in the expected directions. These results highlight the value of self-supervised speech representations for modeling accent perception using interpretable phonological features.

</details>


### [44] [AgriCHN: A Comprehensive Cross-domain Resource for Chinese Agricultural Named Entity Recognition](https://arxiv.org/abs/2506.17578)

*Lingxiao Zeng, Yiqi Tong, Wei Guo, Huarui Wu, Lihao Ge, Yijun Ye, Fuzhen Zhuang, Deqing Wang, Wei Guo, Cheng Chen*

**Main category:** cs.CL

**Keywords:** named entity recognition, agriculture, Chinese dataset, machine learning, data quality

**Relevance Score:** 4

**TL;DR:** AgriCHN is an open-source Chinese dataset for agricultural named entity recognition, offering high-quality annotations across 27 categories, including relevant hydrological and meteorological entities.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve agricultural entity recognition performance due to the lack of high-quality datasets in Chinese, while considering the relationships between agriculture, hydrology, and meteorology.

**Method:** Creation of the AgriCHN dataset by curating 4,040 sentences from agricultural articles, containing 15,799 mentions of diverse agricultural entities and incorporating various state-of-the-art neural NER models for benchmarking.

**Key Contributions:**

	1. Development of the AgriCHN dataset with extensive annotations for agricultural entities in Chinese.
	2. Inclusion of diverse entity types beyond agriculture, incorporating hydrological and meteorological aspects.
	3. Benchmarking against state-of-the-art NER models to reveal challenges and enhance research possibilities.

**Result:** AgriCHN shows superior data quality compared to existing resources and presents significant challenges for entity recognition tasks.

**Limitations:** 

**Conclusion:** The dataset facilitates better automated recognition of agricultural entities and invites further research in this area.

**Abstract:** Agricultural named entity recognition is a specialized task focusing on identifying distinct agricultural entities within vast bodies of text, including crops, diseases, pests, and fertilizers. It plays a crucial role in enhancing information extraction from extensive agricultural text resources. However, the scarcity of high-quality agricultural datasets, particularly in Chinese, has resulted in suboptimal performance when employing mainstream methods for this purpose. Most earlier works only focus on annotating agricultural entities while overlook the profound correlation of agriculture with hydrology and meteorology. To fill this blank, we present AgriCHN, a comprehensive open-source Chinese resource designed to promote the accuracy of automated agricultural entity annotation. The AgriCHN dataset has been meticulously curated from a wealth of agricultural articles, comprising a total of 4,040 sentences and encapsulating 15,799 agricultural entity mentions spanning 27 diverse entity categories. Furthermore, it encompasses entities from hydrology to meteorology, thereby enriching the diversity of entities considered. Data validation reveals that, compared with relevant resources, AgriCHN demonstrates outstanding data quality, attributable to its richer agricultural entity types and more fine-grained entity divisions. A benchmark task has also been constructed using several state-of-the-art neural NER models. Extensive experimental results highlight the significant challenge posed by AgriCHN and its potential for further research.

</details>


### [45] [PRAISE: Enhancing Product Descriptions with LLM-Driven Structured Insights](https://arxiv.org/abs/2506.17314)

*Adnan Qidwai, Srija Mukhopadhyay, Prerana Khatiwada, Dan Roth, Vivek Gupta*

**Main category:** cs.CL

**Keywords:** Large Language Models, E-commerce, Customer Reviews, Product Descriptions, Information Extraction

**Relevance Score:** 7

**TL;DR:** PRAISE is a system that uses LLMs to extract, compare, and structure insights from customer reviews and product descriptions to improve e-commerce product listings.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for accurate and comprehensive product descriptions in e-commerce, as seller-provided information often lacks detail and customer reviews contain valuable insights that are difficult to analyze manually.

**Method:** PRAISE employs Large Language Models to automatically extract relevant information from customer reviews and seller descriptions, comparing and structuring these insights into a clear format.

**Key Contributions:**

	1. Development of PRAISE, a novel automated system for product review analysis
	2. Effective extraction and structuring of insights using LLMs
	3. User-friendly interface that highlights discrepancies and supports sellers in improving listings

**Result:** PRAISE enables the identification of missing, contradictory, or partially matching product details, presenting these discrepancies alongside supporting evidence from reviews for enhanced clarity.

**Limitations:** 

**Conclusion:** By integrating insights from reviews with seller descriptions, PRAISE enhances the quality and trustworthiness of e-commerce product listings, aiding both sellers and buyers.

**Abstract:** Accurate and complete product descriptions are crucial for e-commerce, yet seller-provided information often falls short. Customer reviews offer valuable details but are laborious to sift through manually. We present PRAISE: Product Review Attribute Insight Structuring Engine, a novel system that uses Large Language Models (LLMs) to automatically extract, compare, and structure insights from customer reviews and seller descriptions. PRAISE provides users with an intuitive interface to identify missing, contradictory, or partially matching details between these two sources, presenting the discrepancies in a clear, structured format alongside supporting evidence from reviews. This allows sellers to easily enhance their product listings for clarity and persuasiveness, and buyers to better assess product reliability. Our demonstration showcases PRAISE's workflow, its effectiveness in generating actionable structured insights from unstructured reviews, and its potential to significantly improve the quality and trustworthiness of e-commerce product catalogs.

</details>


### [46] [Mind the Gap: Assessing Wiktionary's Crowd-Sourced Linguistic Knowledge on Morphological Gaps in Two Related Languages](https://arxiv.org/abs/2506.17603)

*Jonathan Sakunkoo, Annabella Sakunkoo*

**Main category:** cs.CL

**Keywords:** Morphological defectivity, NLP tools, Wiktionary, Computational morphology, Crowd-sourced data

**Relevance Score:** 4

**TL;DR:** This paper addresses morphological defectivity in linguistics and validates crowd-sourced data on defective verbs from Wiktionary using a custom neural morphological analyzer.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improving NLP tools in morphologically rich languages by addressing the phenomenon of morphological defectivity, which is often overlooked in traditional linguistic resources.

**Method:** A novel neural morphological analyzer was customized to annotate Latin and Italian corpora, and crowd-sourced lists of defective verbs from Wiktionary were validated using this annotated data.

**Key Contributions:**

	1. Custom neural morphological analyzer for Latin and Italian corpora.
	2. Validation of crowd-sourced defective verb lists from Wiktionary.
	3. Insight into the reliability of Wiktionary data for morphological gaps in lesser-studied languages.

**Result:** Wiktionary is found to reliably account for Italian morphological gaps; however, 7% of Latin entries identified as defective were supported by evidence showing they are not defective.

**Limitations:** Only focuses on Latin and Italian; findings may not generalize to other languages or linguistic phenomena.

**Conclusion:** The study reveals the limitations of crowd-sourced resources like Wiktionary for definitive linguistic knowledge, while offering tools for quality assurance in computational morphology.

**Abstract:** Morphological defectivity is an intriguing and understudied phenomenon in linguistics. Addressing defectivity, where expected inflectional forms are absent, is essential for improving the accuracy of NLP tools in morphologically rich languages. However, traditional linguistic resources often lack coverage of morphological gaps as such knowledge requires significant human expertise and effort to document and verify. For scarce linguistic phenomena in under-explored languages, Wikipedia and Wiktionary often serve as among the few accessible resources. Despite their extensive reach, their reliability has been a subject of controversy. This study customizes a novel neural morphological analyzer to annotate Latin and Italian corpora. Using the massive annotated data, crowd-sourced lists of defective verbs compiled from Wiktionary are validated computationally. Our results indicate that while Wiktionary provides a highly reliable account of Italian morphological gaps, 7% of Latin lemmata listed as defective show strong corpus evidence of being non-defective. This discrepancy highlights potential limitations of crowd-sourced wikis as definitive sources of linguistic knowledge, particularly for less-studied phenomena and languages, despite their value as resources for rare linguistic features. By providing scalable tools and methods for quality assurance of crowd-sourced data, this work advances computational morphology and expands linguistic knowledge of defectivity in non-English, morphologically rich languages.

</details>


### [47] [TyphoFormer: Language-Augmented Transformer for Accurate Typhoon Track Forecasting](https://arxiv.org/abs/2506.17609)

*Lincan Li, Eren Erman Ozguven, Yue Zhao, Guang Wang, Yiqun Xie, Yushun Dong*

**Main category:** cs.CL

**Keywords:** Typhoon forecasting, Transformer, Natural language processing, Large Language Model, Meteorological data

**Relevance Score:** 6

**TL;DR:** TyphoFormer is a novel framework that improves typhoon trajectory forecasting by integrating natural language descriptions generated by a Large Language Model with traditional numerical data.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Forecasting typhoon tracks is essential for disaster response, and existing models often lack broader contextual knowledge needed for reliability in sparse meteorological data.

**Method:** TyphoFormer utilizes a Transformer architecture that combines numerical meteorological data with auxiliary textual descriptions derived from the North Atlantic hurricane database, enhancing the model's understanding of high-level meteorological contexts.

**Key Contributions:**

	1. Introduction of TyphoFormer framework for typhoon forecasting.
	2. Use of LLM to create auxiliary textual descriptions from numerical data.
	3. Demonstrated improved forecasting performance on HURDAT2 benchmark.

**Result:** TyphoFormer outperforms state-of-the-art forecasting methods, especially in scenarios with nonlinear path shifts and limited historical data.

**Limitations:** 

**Conclusion:** The integration of LLM-generated textual prompts significantly enhances the accuracy of typhoon track forecasting, making TyphoFormer a valuable tool for meteorological predictions.

**Abstract:** Accurate typhoon track forecasting is crucial for early system warning and disaster response. While Transformer-based models have demonstrated strong performance in modeling the temporal dynamics of dense trajectories of humans and vehicles in smart cities, they usually lack access to broader contextual knowledge that enhances the forecasting reliability of sparse meteorological trajectories, such as typhoon tracks. To address this challenge, we propose TyphoFormer, a novel framework that incorporates natural language descriptions as auxiliary prompts to improve typhoon trajectory forecasting. For each time step, we use Large Language Model (LLM) to generate concise textual descriptions based on the numerical attributes recorded in the North Atlantic hurricane database. The language descriptions capture high-level meteorological semantics and are embedded as auxiliary special tokens prepended to the numerical time series input. By integrating both textual and sequential information within a unified Transformer encoder, TyphoFormer enables the model to leverage contextual cues that are otherwise inaccessible through numerical features alone. Extensive experiments are conducted on HURDAT2 benchmark, results show that TyphoFormer consistently outperforms other state-of-the-art baseline methods, particularly under challenging scenarios involving nonlinear path shifts and limited historical observations.

</details>


### [48] [OpusLM: A Family of Open Unified Speech Language Models](https://arxiv.org/abs/2506.17611)

*Jinchuan Tian, William Chen, Yifan Peng, Jiatong Shi, Siddhant Arora, Shikhar Bharadwaj, Takashi Maekaku, Yusuke Shinohara, Keita Goto, Xiang Yue, Huck Yang, Shinji Watanabe*

**Main category:** cs.CL

**Keywords:** Speech Language Models, OpusLMs, Speech Recognition, Speech Synthesis, Open Research

**Relevance Score:** 7

**TL;DR:** This paper introduces Open Unified Speech Language Models (OpusLMs), highlighting their performance in speech recognition, synthesis, and text capabilities, established through continuous pre-training on extensive datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for open foundational speech language models that are scalable and transparent for researchers in the field.

**Method:** OpusLMs are initialized from decoder-only text language models and pre-trained on a large database of speech-text pairs and text-only tokens, with a focus on tokenization, multi-stream models, and multi-stage training strategies.

**Key Contributions:**

	1. Introduction of OpusLMs as scalable speech language models
	2. Demonstration of performance on speech recognition and synthesis
	3. Provision of all training materials and models as fully open resources.

**Result:** OpusLMs demonstrate comparable or superior performance to existing SpeechLMs in various tasks, including speech recognition and synthesis.

**Limitations:** 

**Conclusion:** The transparency and open-access nature of OpusLMs contribute significantly to ongoing research in speech language models, providing a solid foundation for future work.

**Abstract:** This paper presents Open Unified Speech Language Models (OpusLMs), a family of open foundational speech language models (SpeechLMs) up to 7B. Initialized from decoder-only text language models, the OpusLMs are continuously pre-trained on 213K hours of speech-text pairs and 292B text-only tokens. We demonstrate our OpusLMs achieve comparable (or even superior) performance with existing SpeechLMs in speech recognition, speech synthesis, and text-only capabilities. Technically, this paper articulates our SpeechLM designs on tokenization, multi-stream language models, and multi-stage training strategies. We experimentally demonstrate the importance of model size scaling and the effect of annealing data selection. The OpusLMs are all built from publicly available materials and are fully transparent models. We release our code, data, checkpoints, and training logs to facilitate open SpeechLM research

</details>


### [49] [Computational Approaches to Understanding Large Language Model Impact on Writing and Information Ecosystems](https://arxiv.org/abs/2506.17467)

*Weixin Liang*

**Main category:** cs.CL

**Keywords:** large language models, AI detectors, equity in AI, manuscript feedback, algorithmic approaches

**Relevance Score:** 9

**TL;DR:** This dissertation explores the adaptation and engagement of individuals and institutions with large language models (LLMs) through three key research directions: biases in AI detectors, patterns of LLM adoption across writing domains, and the feedback provision capability of LLMs in supporting research manuscript development.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the societal impact and institutional adaptation to large language models, highlighting issues of equity and accessibility in AI governance and manuscript feedback.

**Method:** The dissertation employs a mixed-methods approach including empirical analysis, algorithmic population-level measurement, and examination of AI detector biases.

**Key Contributions:**

	1. Highlighting biases in AI governance related to language variety
	2. Identifying patterns of LLM adoption across various writing contexts
	3. Demonstrating LLMs' potential for providing feedback to researchers in need.

**Result:** Demonstrated that AI detectors introduce biases against non-dominant language varieties, identified consistent patterns of LLM adoption in diverse writing domains, and revealed LLMs' potential to support early-career researchers with timely feedback.

**Limitations:** The study may not fully capture the long-term implications of LLM adoption on writing processes and institutional practices.

**Conclusion:** The findings underscore the need for equitable AI governance and highlight the role of LLMs in enhancing manuscript feedback for under-resourced researchers.

**Abstract:** Large language models (LLMs) have shown significant potential to change how we write, communicate, and create, leading to rapid adoption across society. This dissertation examines how individuals and institutions are adapting to and engaging with this emerging technology through three research directions. First, I demonstrate how the institutional adoption of AI detectors introduces systematic biases, particularly disadvantaging writers of non-dominant language varieties, highlighting critical equity concerns in AI governance. Second, I present novel population-level algorithmic approaches that measure the increasing adoption of LLMs across writing domains, revealing consistent patterns of AI-assisted content in academic peer reviews, scientific publications, consumer complaints, corporate communications, job postings, and international organization press releases. Finally, I investigate LLMs' capability to provide feedback on research manuscripts through a large-scale empirical analysis, offering insights into their potential to support researchers who face barriers in accessing timely manuscript feedback, particularly early-career researchers and those from under-resourced settings.

</details>


### [50] [Answer-Centric or Reasoning-Driven? Uncovering the Latent Memory Anchor in LLMs](https://arxiv.org/abs/2506.17630)

*Yang Wu, Yifan Zhang, Yiwei Wang, Yujun Cai, Yurong Wu, Yuran Wang, Ning Xu, Jian Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, reasoning, answer visibility, prompt framework, behavioral analysis

**Relevance Score:** 9

**TL;DR:** This paper investigates the reliance of Large Language Models (LLMs) on final answers versus reasoning patterns, revealing substantial reliance on explicit answers.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand whether LLMs rely more on final answers or the reasoning chains that lead to those answers, which influences their perceived reasoning capabilities.

**Method:** A five-level answer-visibility prompt framework was proposed to manipulate answer cues and analyze model behavior through indirect behavioral analysis across state-of-the-art LLMs.

**Key Contributions:**

	1. Introduction of a five-level answer-visibility prompt framework
	2. Empirical evidence showing LLMs' dependence on explicit answers
	3. Insights into the nature of reasoning in LLMs

**Result:** Experiments showed a 26.90% performance drop when answer cues were masked, suggesting LLMs prioritize explicit answers over reasoning.

**Limitations:** Limited to observed behaviors in certain state-of-the-art LLMs; may not generalize across all LLM architectures.

**Conclusion:** The findings question the inferential depth of LLMs, indicating their reasoning may be more about post-hoc rationalization than true inference.

**Abstract:** While Large Language Models (LLMs) demonstrate impressive reasoning capabilities, growing evidence suggests much of their success stems from memorized answer-reasoning patterns rather than genuine inference. In this work, we investigate a central question: are LLMs primarily anchored to final answers or to the textual pattern of reasoning chains? We propose a five-level answer-visibility prompt framework that systematically manipulates answer cues and probes model behavior through indirect, behavioral analysis. Experiments across state-of-the-art LLMs reveal a strong and consistent reliance on explicit answers. The performance drops by 26.90\% when answer cues are masked, even with complete reasoning chains. These findings suggest that much of the reasoning exhibited by LLMs may reflect post-hoc rationalization rather than true inference, calling into question their inferential depth. Our study uncovers the answer-anchoring phenomenon with rigorous empirical validation and underscores the need for a more nuanced understanding of what constitutes reasoning in LLMs.

</details>


### [51] [Step-Opt: Boosting Optimization Modeling in LLMs through Iterative Data Synthesis and Structured Validation](https://arxiv.org/abs/2506.17637)

*Yang Wu, Yifan Zhang, Yurong Wu, Yuran Wang, Junkai Zhang, Jian Cheng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Optimization Modeling, Operations Research, Fine-tuning, Decision-making Automation

**Relevance Score:** 8

**TL;DR:** This paper introduces Step-Opt-Instruct, a framework to enhance fine-tuning of LLMs for optimization modeling in Operations Research, showing significant performance improvements.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges faced by LLMs in optimization modeling tasks within Operations Research, especially for complex problems.

**Method:** Step-Opt-Instruct utilizes iterative problem generation to increase complexity and stepwise validation to ensure data quality for fine-tuning LLMs like LLaMA-3-8B and Mistral-7B.

**Key Contributions:**

	1. Introduction of Step-Opt-Instruct framework for generating fine-tuning data.
	2. Significant performance improvement of Step-Opt on optimization tasks.
	3. Public availability of code and dataset for reproducibility.

**Result:** The fine-tuned model, Step-Opt, achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR, with a 17.01% improvement in accuracy on challenging problems.

**Limitations:** 

**Conclusion:** The study demonstrates the success of integrating structured validation with gradual problem refinement to enhance decision-making automation through LLMs.

**Abstract:** Large Language Models (LLMs) have revolutionized various domains but encounter substantial challenges in tackling optimization modeling tasks for Operations Research (OR), particularly when dealing with complex problem. In this work, we propose Step-Opt-Instruct, a framework that augments existing datasets and generates high-quality fine-tuning data tailored to optimization modeling. Step-Opt-Instruct employs iterative problem generation to systematically increase problem complexity and stepwise validation to rigorously verify data, preventing error propagation and ensuring the quality of the generated dataset. Leveraging this framework, we fine-tune open-source LLMs, including LLaMA-3-8B and Mistral-7B, to develop Step-Opt--a model that achieves state-of-the-art performance on benchmarks such as NL4OPT, MAMO, and IndustryOR. Extensive experiments demonstrate the superior performance of Step-Opt, especially in addressing complex OR tasks, with a notable 17.01\% improvement in micro average accuracy on difficult problems. These findings highlight the effectiveness of combining structured validation with gradual problem refinement to advance the automation of decision-making processes using LLMs.The code and dataset are available at https://github.com/samwu-learn/Step.

</details>


### [52] [TPTT: Transforming Pretrained Transformer into Titans](https://arxiv.org/abs/2506.17671)

*Fabien Furfaro*

**Main category:** cs.CL

**Keywords:** large language models, transformer models, memory management, linearized attention, parameter-efficient fine-tuning

**Relevance Score:** 9

**TL;DR:** TPTT enhances Transformers with efficient linearized attention and memory management, improving long-context inference without full retraining.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the computational and memory challenges faced by large language models, particularly in long-context inference.

**Method:** TPTT employs Memory as Gate (MaG) and mixed linearized attention (LiZA) techniques to enhance pretrained Transformers.

**Key Contributions:**

	1. Developing TPTT framework for optimized large language models
	2. Introducing Memory as Gate and mixed linearized attention
	3. Providing open-source implementation and significant benchmark results

**Result:** TPTT demonstrated significant improvements in efficiency and accuracy, with a 20% increase in Exact Match on the MMLU benchmark for a model with 1 billion parameters.

**Limitations:** 

**Conclusion:** TPTT's framework is scalable and robust, compatible with Hugging Face Transformers, and facilitates parameter-efficient fine-tuning.

**Abstract:** Recent advances in large language models (LLMs) have led to remarkable progress in natural language processing, but their computational and memory demands remain a significant challenge, particularly for long-context inference. We introduce TPTT (Transforming Pretrained Transformer into Titans), a novel framework for enhancing pretrained Transformer models with efficient linearized attention mechanisms and advanced memory management. TPTT employs techniques such as Memory as Gate (MaG) and mixed linearized attention (LiZA). It is fully compatible with the Hugging Face Transformers library, enabling seamless adaptation of any causal LLM through parameter-efficient fine-tuning (LoRA) without full retraining. We show the effectiveness of TPTT on the MMLU benchmark with models of approximately 1 billion parameters, observing substantial improvements in both efficiency and accuracy. For instance, Titans-Llama-3.2-1B achieves a 20% increase in Exact Match (EM) over its baseline. Statistical analyses and comparisons with recent state-of-the-art methods confirm the practical scalability and robustness of TPTT. Code is available at https://github.com/fabienfrfr/tptt . Python package at https://pypi.org/project/tptt/ .

</details>


### [53] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)

*Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil*

**Main category:** cs.CL

**Keywords:** cultural bias, prompt engineering, large language models, bias mitigation, systematic review

**Relevance Score:** 8

**TL;DR:** This review examines prompt engineering strategies to mitigate cultural bias in large language models, focusing on Arab and Muslim representation, revealing effective techniques and research gaps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns about cultural bias in large language models, particularly toward Arabs and Muslims, illustrate the need for effective bias mitigation strategies.

**Method:** A mixed-methods systematic review using PRISMA guidelines and Kitchenham's methodology to analyze 8 empirical studies from 2021-2024.

**Key Contributions:**

	1. Identification of five effective prompt engineering approaches for bias mitigation
	2. Highlighting the effectiveness of structured multi-step pipelines
	3. Emphasizing the need for culturally adaptive prompting techniques

**Result:** Five primary prompt engineering approaches were identified: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Structured multi-step pipelines achieved the highest bias reduction, up to 87.7%.

**Limitations:** Limited number of studies identified indicates a significant research gap in the area of bias mitigation for Arab and Muslim representation.

**Conclusion:** Access to effective prompt engineering techniques can help mitigate cultural bias without the need for model parameter access. There is a significant research gap in developing tailored strategies for Arab and Muslim representation.

**Abstract:** Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.

</details>


### [54] [Resource-Friendly Dynamic Enhancement Chain for Multi-Hop Question Answering](https://arxiv.org/abs/2506.17692)

*Binquan Ji, Haibo Luo, Yifei Lu, Lei Hei, Jiaqi Wang, Tingjing Liao, Lingyu Wang, Shichao Wang, Feiliang Ren*

**Main category:** cs.CL

**Keywords:** multi-hop QA, large language models, keyword extraction, resource-constrained environments

**Relevance Score:** 8

**TL;DR:** Introduction of a framework called DEC for multi-hop question answering using LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in multi-hop QA tasks like hallucinations and semantic drift in lightweight LLMs.

**Method:** DEC decomposes complex questions into subquestions and refines them through context-aware rewriting for effective query formulation and keyword extraction for targeted retrieval.

**Key Contributions:**

	1. Proposes the DEC framework for multi-hop QA.
	2. Implements a keyword extraction module for precise document retrieval.
	3. Achieves state-of-the-art performance with lightweight models.

**Result:** DEC performs on par with or surpasses state-of-the-art benchmarks while reducing token consumption, achieving state-of-the-art results with models of 8B parameters.

**Limitations:** 

**Conclusion:** DEC demonstrates effectiveness in multi-hop QA tasks, especially in resource-constrained environments.

**Abstract:** Knowledge-intensive multi-hop question answering (QA) tasks, which require integrating evidence from multiple sources to address complex queries, often necessitate multiple rounds of retrieval and iterative generation by large language models (LLMs). However, incorporating many documents and extended contexts poses challenges -such as hallucinations and semantic drift-for lightweight LLMs with fewer parameters. This work proposes a novel framework called DEC (Dynamic Enhancement Chain). DEC first decomposes complex questions into logically coherent subquestions to form a hallucination-free reasoning chain. It then iteratively refines these subquestions through context-aware rewriting to generate effective query formulations. For retrieval, we introduce a lightweight discriminative keyword extraction module that leverages extracted keywords to achieve targeted, precise document recall with relatively low computational overhead. Extensive experiments on three multi-hop QA datasets demonstrate that DEC performs on par with or surpasses state-of-the-art benchmarks while significantly reducing token consumption. Notably, our approach attains state-of-the-art results on models with 8B parameters, showcasing its effectiveness in various scenarios, particularly in resource-constrained environments.

</details>


### [55] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)

*Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil*

**Main category:** cs.CL

**Keywords:** emotion recognition, multimodal AI, Arabic language, educational technologies, culturally sensitive training

**Relevance Score:** 8

**TL;DR:** This study assesses the emotion recognition performance of GPT-4o and Gemini 1.5 Pro in analyzing Arabic children's storybook illustrations, revealing significant gaps in cultural understanding and model efficacy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Emotion recognition in AI is crucial for creating culturally responsive educational technologies, particularly in Arabic contexts.

**Method:** The study evaluated GPT-4o and Gemini 1.5 Pro using three prompting strategies (zero-shot, few-shot, chain-of-thought) on 75 images from Arabic storybooks, measuring performance against human annotations based on Plutchik's emotional framework.

**Key Contributions:**

	1. Performance comparison between two advanced multimodal large language models in an underexplored context
	2. Identification of systematic misclassification patterns in emotion recognition
	3. Emphasis on the importance of culturally responsive AI training methods

**Result:** GPT-4o outperformed Gemini with the highest macro F1-score of 59% (chain-of-thought prompting) versus Geminiâs 43%. Error analysis indicated 60.7% of errors were due to valence inversions, highlighting misclassification issues with culturally nuanced emotions.

**Limitations:** Both models struggled with culturally nuanced emotions and ambiguous narrative contexts.

**Conclusion:** The findings underscore the need for culturally sensitive training approaches to enhance emotion recognition in AI, especially for educational applications targeting Arabic-speaking learners.

**Abstract:** Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [56] [Zero-Shot Conversational Stance Detection: Dataset and Approaches](https://arxiv.org/abs/2506.17693)

*Yuzhe Ding, Kang He, Bobo Li, Li Zheng, Haijun He, Fei Li, Chong Teng, Donghong Ji*

**Main category:** cs.CL

**Keywords:** stance detection, zero-shot learning, conversational AI, social media, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces a large-scale zero-shot conversational stance detection dataset and a model (SITPCL) that achieves state-of-the-art performance in this area.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the limitations of existing conversational stance detection datasets that focus on a narrow range of targets, which hampers model performance in real-world applications with diverse targets.

**Method:** A novel dataset, ZS-CSD, is curated consisting of 280 targets, and a model named SITPCL is proposed that utilizes speaker interaction and target-awareness within a contrastive learning framework.

**Key Contributions:**

	1. Introduction of a large-scale zero-shot conversational stance detection dataset (ZS-CSD).
	2. Proposal of the SITPCL model for stance detection leveraging speaker interaction and target awareness.
	3. Establishment of benchmark performance for zero-shot conversational stance detection.

**Result:** The SITPCL model demonstrates promising capabilities in zero-shot conversational stance detection, achieving a benchmark F1-macro score of 43.81%.

**Limitations:** The F1-macro score remains low (43.81%), indicating ongoing challenges in zero-shot conversational stance detection.

**Conclusion:** Despite achieving state-of-the-art results in zero-shot stance detection, challenges remain, as indicated by the relatively low F1-macro score.

**Abstract:** Stance detection, which aims to identify public opinion towards specific targets using social media data, is an important yet challenging task. With the increasing number of online debates among social media users, conversational stance detection has become a crucial research area. However, existing conversational stance detection datasets are restricted to a limited set of specific targets, which constrains the effectiveness of stance detection models when encountering a large number of unseen targets in real-world applications. To bridge this gap, we manually curate a large-scale, high-quality zero-shot conversational stance detection dataset, named ZS-CSD, comprising 280 targets across two distinct target types. Leveraging the ZS-CSD dataset, we propose SITPCL, a speaker interaction and target-aware prototypical contrastive learning model, and establish the benchmark performance in the zero-shot setting. Experimental results demonstrate that our proposed SITPCL model achieves state-of-the-art performance in zero-shot conversational stance detection. Notably, the SITPCL model attains only an F1-macro score of 43.81%, highlighting the persistent challenges in zero-shot conversational stance detection.

</details>


### [57] [The Evolution of Natural Language Processing: How Prompt Optimization and Language Models are Shaping the Future](https://arxiv.org/abs/2506.17700)

*Summra Saleem, Muhammad Nabeel Asim, Shaista Zulfiqar, Andreas Dengel*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Prompt Optimization, NLP Tasks, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper reviews and categorizes diverse prompt optimization strategies for Large Language Models (LLMs) used in NLP tasks, addressing existing gaps in literature.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite extensive reviews on prompt engineering, there is a lack of comprehensive analyses on prompt optimization strategies in LLMs.

**Method:** The paper categorizes 11 distinct prompt optimization strategies and analyzes their applications across various NLP tasks and LLMs using benchmark datasets.

**Key Contributions:**

	1. Categorization of 11 distinct prompt optimization strategies
	2. Comprehensive analysis of prompt optimization across various NLP tasks
	3. Establishment of a foundation for future comparative studies

**Result:** The analysis reveals how different strategies can significantly enhance the performance of LLMs across diverse NLP tasks, laying the groundwork for future studies.

**Limitations:** 

**Conclusion:** This research highlights the importance of systematic evaluations of prompt optimization techniques to improve LLM applications in various domains.

**Abstract:** Large Language Models (LLMs) have revolutionized the field of Natural Language Processing (NLP) by automating traditional labor-intensive tasks and consequently accelerated the development of computer-aided applications. As researchers continue to advance this field with the introduction of novel language models and more efficient training/finetuning methodologies, the idea of prompt engineering and subsequent optimization strategies with LLMs has emerged as a particularly impactful trend to yield a substantial performance boost across diverse NLP tasks. To best of our knowledge numerous review articles have explored prompt engineering, however, a critical gap exists in comprehensive analyses of prompt optimization strategies. To bridge this gap this paper provides unique and comprehensive insights about the potential of diverse prompt optimization strategies. It analyzes their underlying working paradigms and based on these principles, categorizes them into 11 distinct classes. Moreover, the paper provides details about various NLP tasks where these prompt optimization strategies have been employed, along with details of different LLMs and benchmark datasets used for evaluation. This comprehensive compilation lays a robust foundation for future comparative studies and enables rigorous assessment of prompt optimization and LLM-based predictive pipelines under consistent experimental settings: a critical need in the current landscape. Ultimately, this research will centralize diverse strategic knowledge to facilitate the adaptation of existing prompt optimization strategies for development of innovative predictors across unexplored tasks.

</details>


### [58] [Aged to Perfection: Machine-Learning Maps of Age in Conversational English](https://arxiv.org/abs/2506.17708)

*MingZe Tang*

**Main category:** cs.CL

**Keywords:** sociolinguistics, machine learning, British National Corpus, language patterns, generational differences

**Relevance Score:** 4

**TL;DR:** The study analyzes language patterns in British English across age groups using computational and machine learning techniques.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how language patterns vary across different age groups by analyzing speaker demographics and linguistic features.

**Method:** Utilizes the British National Corpus 2014 and applies computational language analysis alongside machine learning to identify linguistic markers associated with various generations and develop predictive models.

**Key Contributions:**

	1. Investigated age-related language patterns in contemporary British English.
	2. Merged computational analysis with machine learning to identify linguistic markers.
	3. Developed predictive models for estimating age from linguistic data.

**Result:** Uncovered distinctive linguistic markers for different age groups and developed models to reliably estimate the speaker's age based on linguistic factors.

**Limitations:** 

**Conclusion:** The research enhances understanding of sociolinguistic diversity in modern British speech and provides insights that may aid further linguistic and demographic studies.

**Abstract:** The study uses the British National Corpus 2014, a large sample of contemporary spoken British English, to investigate language patterns across different age groups. Our research attempts to explore how language patterns vary between different age groups, exploring the connection between speaker demographics and linguistic factors such as utterance duration, lexical diversity, and word choice. By merging computational language analysis and machine learning methodologies, we attempt to uncover distinctive linguistic markers characteristic of multiple generations and create prediction models that can consistently estimate the speaker's age group from various aspects. This work contributes to our knowledge of sociolinguistic diversity throughout the life of modern British speech.

</details>


### [59] [Unveiling Factors for Enhanced POS Tagging: A Study of Low-Resource Medieval Romance Languages](https://arxiv.org/abs/2506.17715)

*Matthias SchÃ¶ffel, Esteban Garces Arias, Marinus Wiedner, Paula Ruppert, Meimingwei Li, Christian Heumann, Matthias AÃenmacher*

**Main category:** cs.CL

**Keywords:** POS tagging, Medieval Romance languages, Natural Language Processing, Large Language Models, Historical Linguistics

**Relevance Score:** 7

**TL;DR:** This study explores the challenges and solutions in POS tagging for Medieval Romance languages using modern LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address POS tagging challenges in historical text analysis for Medieval languages due to linguistic evolution and data scarcity.

**Method:** Systematic investigation through experimentation, evaluating fine-tuning, prompt engineering, model architectures, decoding strategies, and cross-lingual techniques.

**Key Contributions:**

	1. Investigating POS tagging performance for Medieval Romance languages
	2. Evaluating various fine-tuning and model strategies
	3. Identifying effective techniques for low-resource historical languages

**Result:** Key findings include limitations of LLMs in processing historical variations and promising techniques for low-resource languages.

**Limitations:** Focus on specific Medieval languages may limit generalizability.

**Conclusion:** While LLMs face challenges with historical language variations, specialized techniques can enhance tagging accuracy.

**Abstract:** Part-of-speech (POS) tagging remains a foundational component in natural language processing pipelines, particularly critical for historical text analysis at the intersection of computational linguistics and digital humanities. Despite significant advancements in modern large language models (LLMs) for ancient languages, their application to Medieval Romance languages presents distinctive challenges stemming from diachronic linguistic evolution, spelling variations, and labeled data scarcity. This study systematically investigates the central determinants of POS tagging performance across diverse corpora of Medieval Occitan, Medieval Spanish, and Medieval French texts, spanning biblical, hagiographical, medical, and dietary domains. Through rigorous experimentation, we evaluate how fine-tuning approaches, prompt engineering, model architectures, decoding strategies, and cross-lingual transfer learning techniques affect tagging accuracy. Our results reveal both notable limitations in LLMs' ability to process historical language variations and non-standardized spelling, as well as promising specialized techniques that effectively address the unique challenges presented by low-resource historical languages.

</details>


### [60] [KAG-Thinker: Teaching Large Language Models to Think with Human-like Reasoning Process](https://arxiv.org/abs/2506.17728)

*Dalong Zhang, Jun Xu, Jun Zhou, Lei Liang, Lin Yuan, Ling Zhong, Mengshu Sun, Peilong Zhao, QiWei Wang, Xiaorui Wang, Xinkai Du, YangYang Hou, Yu Ao, ZhaoYang Wang, Zhengke Gui, ZhiYing Yi, Zhongpu Bo*

**Main category:** cs.CL

**Keywords:** Human-Cognitive Simulation, Domain-Specific Q&A, Parameter-Light LLM

**Relevance Score:** 8

**TL;DR:** KAG-Thinker is a novel reasoning framework utilizing a parameter-light LLM to enhance logical coherence and contextual consistency in Q&A tasks, simulating human cognitive mechanisms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve reasoning capabilities in LLMs for domain-specific Q&A tasks by emulating human-like cognitive processes and establishing a structured thinking framework.

**Method:** The framework decomposes complex questions into sub-problems using breadth decomposition, representing them as both natural language and logical function forms. It classifies these into Knowledge Retrieval and Reasoning Analysis tasks, utilizing various functions for knowledge retrieval and reasoning, along with self-regulatory mechanisms for optimal source determination.

**Key Contributions:**

	1. Introduction of KAG-Thinker as a novel reasoning framework using parameter-light LLMs.
	2. Decomposing complex questions into manageable sub-problems for enhanced reasoning.
	3. Employing self-regulatory mechanisms for optimal knowledge source determination.

**Result:** KAG-Thinker demonstrates improved logical coherence and enhanced performance in complex Q&A scenarios by effectively utilizing structured reasoning and knowledge acquisition techniques.

**Limitations:** 

**Conclusion:** The proposed framework offers a strong alternative to reinforcement learning through supervised fine-tuning techniques, aligning the model with structured inference while minimizing excessive reflection.

**Abstract:** In this paper, we introduce KAG-Thinker, a novel human-like reasoning framework built upon a parameter-light large language model (LLM). Our approach enhances the logical coherence and contextual consistency of the thinking process in question-answering (Q\&A) tasks on domain-specific knowledge bases (KBs) within LLMs. This framework simulates human cognitive mechanisms for handling complex problems by establishing a structured thinking process. Continuing the \textbf{Logical Form} guided retrieval and reasoning technology route of KAG v0.7, firstly, it decomposes complex questions into independently solvable sub-problems(also referred to as logical forms) through \textbf{breadth decomposition}, each represented in two equivalent forms-natural language and logical function-and further classified as either Knowledge Retrieval or Reasoning Analysis tasks, with dependencies and variables passing explicitly modeled via logical function interfaces. In the solving process, the Retrieval function is used to perform knowledge retrieval tasks, while the Math and Deduce functions are used to perform reasoning analysis tasks. Secondly, it is worth noting that, in the Knowledge Retrieval sub-problem tasks, LLMs and external knowledge sources are regarded as equivalent KBs. We use the \textbf{knowledge boundary} model to determine the optimal source using self-regulatory mechanisms such as confidence calibration and reflective reasoning, and use the \textbf{depth solving} model to enhance the comprehensiveness of knowledge acquisition. Finally, instead of utilizing reinforcement learning, we employ supervised fine-tuning with multi-turn dialogues to align the model with our structured inference paradigm, thereby avoiding excessive reflection. This is supported by a data evaluation framework and iterative corpus synthesis, which facilitate the generation of detailed reasoning trajectories...

</details>


### [61] [HIDE and Seek: Detecting Hallucinations in Language Models via Decoupled Representations](https://arxiv.org/abs/2506.17748)

*Anwoy Chatterjee, Yash Goel, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** language models, hallucination detection, automatic evaluation, Hilbert-Schmidt Independence Criterion, efficient algorithms

**Relevance Score:** 9

**TL;DR:** This paper presents HIDE, a single-pass, training-free approach for detecting hallucinations in language models by analyzing internal representations and their decoupling from input context, resulting in improved efficiency and accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Language models often generate factually incorrect content (hallucinations), which undermines their reliability. Existing detection methods are costlier and slower, hence the need for a more efficient solution.

**Method:** HIDE utilizes the Hilbert-Schmidt Independence Criterion (HSIC) to measure the statistical decoupling between the internal representations of input context and the generated output in a single pass.

**Key Contributions:**

	1. Introduction of a training-free single-pass approach for hallucination detection
	2. Demonstration of the efficacy of exploiting internal representation decoupling
	3. Significant reduction in computation time while maintaining high accuracy

**Result:** HIDE outperforms other single-pass methods with an average relative improvement of ~29% in AUC-ROC and shows competitive performance with multi-pass methods while being ~51% more efficient.

**Limitations:** 

**Conclusion:** The effectiveness of HIDE demonstrates that leveraging internal representation decoupling in language models is a practical approach for efficient hallucination detection.

**Abstract:** Contemporary Language Models (LMs), while impressively fluent, often generate content that is factually incorrect or unfaithful to the input context - a critical issue commonly referred to as 'hallucination'. This tendency of LMs to generate hallucinated content undermines their reliability, especially because these fabrications are often highly convincing and therefore difficult to detect. While several existing methods attempt to detect hallucinations, most rely on analyzing multiple generations per input, leading to increased computational cost and latency. To address this, we propose a single-pass, training-free approach for effective Hallucination detectIon via Decoupled rEpresentations (HIDE). Our approach leverages the hypothesis that hallucinations result from a statistical decoupling between an LM's internal representations of input context and its generated output. We quantify this decoupling using the Hilbert-Schmidt Independence Criterion (HSIC) applied to hidden-state representations extracted while generating the output sequence. We conduct extensive experiments on four diverse question answering datasets, evaluating both faithfulness and factuality hallucinations across six open-source LMs of varying scales and properties. Our results demonstrate that HIDE outperforms other single-pass methods in almost all settings, achieving an average relative improvement of ~29% in AUC-ROC over the best-performing single-pass strategy across various models and datasets. Additionally, HIDE shows competitive and often superior performance with multi-pass state-of-the-art methods, obtaining an average relative improvement of ~3% in AUC-ROC while consuming ~51% less computation time. Our findings highlight the effectiveness of exploiting internal representation decoupling in LMs for efficient and practical hallucination detection.

</details>


### [62] [Multilingual Tokenization through the Lens of Indian Languages: Challenges and Insights](https://arxiv.org/abs/2506.17789)

*N J Karthika, Maharaj Brahma, Rohit Saluja, Ganesh Ramakrishnan, Maunendra Sankar Desarkar*

**Main category:** cs.CL

**Keywords:** tokenization, multilingual NLP, low-resource languages, Indian languages, vocabulary construction

**Relevance Score:** 7

**TL;DR:** This paper evaluates tokenization strategies for multilingual NLP, focusing on 17 Indian languages, highlighting the limitations of existing tokenizers for low-resource languages and offering insights for improvement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing tokenizers are biased toward high-resource languages, which limits effective NLP for diverse languages, especially in the Indian subcontinent.

**Method:** The study involves an intrinsic evaluation of tokenization strategies including bottom-up (BPE) and top-down (Unigram LM) algorithms, analyzing vocabulary sizes and construction methods.

**Key Contributions:**

	1. Comprehensive evaluation of tokenization strategies across multiple Indian languages.
	2. Insights into the benefits of using high-resource languages for low-resource tokenizer training.
	3. Comparative analysis of vocabulary construction methods.

**Result:** The evaluation shows significant trade-offs in tokenizer performance and indicates that low-resource languages can improve using tokenizers trained on high-resource languages.

**Limitations:** The focus is primarily on Indian languages, which may limit the applicability of findings to other low-resource languages globally.

**Conclusion:** The findings emphasize the need for fair and efficient tokenization strategies tailored for multilingual use, particularly for morphologically rich languages.

**Abstract:** Tokenization plays a pivotal role in multilingual NLP. However, existing tokenizers are often skewed towards high-resource languages, limiting their effectiveness for linguistically diverse and morphologically rich languages such as those in the Indian subcontinent. This paper presents a comprehensive intrinsic evaluation of tokenization strategies across 17 Indian languages. We quantify the trade-offs between bottom-up and top-down tokenizer algorithms (BPE and Unigram LM), effects of vocabulary sizes, and compare strategies of multilingual vocabulary construction such as joint and cluster-based training. We also show that extremely low-resource languages can benefit from tokenizers trained on related high-resource languages. Our study provides practical insights for building more fair, efficient, and linguistically informed tokenizers for multilingual NLP.

</details>


### [63] [THCM-CAL: Temporal-Hierarchical Causal Modelling with Conformal Calibration for Clinical Risk Prediction](https://arxiv.org/abs/2506.17844)

*Xin Zhang, Qiyu Wei, Yingjie Zhu, Fanyi Wu, Sophia Ananiadou*

**Main category:** cs.CL

**Keywords:** clinical risk prediction, electronic health records, multimodal causal graph, hierarchical causal discovery, conformal prediction

**Relevance Score:** 9

**TL;DR:** This paper presents THCM-CAL, a model for automated clinical risk prediction from EHRs that effectively integrates both structured and unstructured data.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To improve automated clinical risk prediction using electronic health records by addressing the limitations of existing methods that either treat structured and unstructured data separately or use simplistic fusion strategies.

**Method:** The proposed THCM-CAL framework constructs a multimodal causal graph that models interactions between textual notes and ICD codes, utilizing hierarchical causal discovery and conformal prediction for reliable multi-label coding.

**Key Contributions:**

	1. Introduction of a multimodal causal graph integrating textual propositions and ICD codes.
	2. Hierarchical causal discovery to capture complex clinical interactions.
	3. Extension of conformal prediction for multi-label ICD coding.

**Result:** Experimental results demonstrate the superiority of THCM-CAL in predicting clinical risks when compared to existing methods, as evidenced by performance metrics on the MIMIC-III and MIMIC-IV datasets.

**Limitations:** The model's effectiveness is currently evaluated on specific datasets (MIMIC-III and MIMIC-IV), which may limit its generalizability to other types of EHRs.

**Conclusion:** THCM-CAL offers a robust approach to clinical risk prediction that advances the integration of textual and coded data in EHRs, leading to better prediction reliability.

**Abstract:** Automated clinical risk prediction from electronic health records (EHRs) demands modeling both structured diagnostic codes and unstructured narrative notes. However, most prior approaches either handle these modalities separately or rely on simplistic fusion strategies that ignore the directional, hierarchical causal interactions by which narrative observations precipitate diagnoses and propagate risk across admissions. In this paper, we propose THCM-CAL, a Temporal-Hierarchical Causal Model with Conformal Calibration. Our framework constructs a multimodal causal graph where nodes represent clinical entities from two modalities: Textual propositions extracted from notes and ICD codes mapped to textual descriptions. Through hierarchical causal discovery, THCM-CAL infers three clinically grounded interactions: intra-slice same-modality sequencing, intra-slice cross-modality triggers, and inter-slice risk propagation. To enhance prediction reliability, we extend conformal prediction to multi-label ICD coding, calibrating per-code confidence intervals under complex co-occurrences. Experimental results on MIMIC-III and MIMIC-IV demonstrate the superiority of THCM-CAL.

</details>


### [64] [LLMs for Customized Marketing Content Generation and Evaluation at Scale](https://arxiv.org/abs/2506.17863)

*Haoran Liu, Amir Tahmasbi, Ehtesham Sam Haque, Purak Jain*

**Main category:** cs.CL

**Keywords:** offsite marketing, ad generation, automated evaluation, keyword-focused, human oversight

**Relevance Score:** 4

**TL;DR:** This paper presents MarketingFM, a system for generating keyword-specific ad copy using multiple data sources, validating its effectiveness through evaluations and A/B tests, and introduces AutoEval-Main and AutoEval-Update for automated ad evaluation and refinement.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of offsite marketing in e-commerce by addressing the limitations of generic ad content.

**Method:** A retrieval-augmented system (MarketingFM) integrates various data sources for ad generation and uses automated evaluation systems (AutoEval-Main and AutoEval-Update) to refine and assess ad quality.

**Key Contributions:**

	1. Development of MarketingFM for keyword-specific ad generation
	2. Introduction of AutoEval-Main for automated ad evaluation
	3. Creation of AutoEval-Update for LLM-human collaborative refinement of ad evaluation.

**Result:** Keyword-focused ad copy led to a 9% increase in click-through rates (CTR) and improved cost efficiency with a 0.38% reduction in cost-per-click (CPC). AutoEval-Main achieved 89.57% agreement with human reviewers.

**Limitations:** Human review of generated ads is still costly and necessary for quality assurance.

**Conclusion:** Automated systems, while efficient, still require human oversight for final validation of ad quality and effectiveness.

**Abstract:** Offsite marketing is essential in e-commerce, enabling businesses to reach customers through external platforms and drive traffic to retail websites. However, most current offsite marketing content is overly generic, template-based, and poorly aligned with landing pages, limiting its effectiveness. To address these limitations, we propose MarketingFM, a retrieval-augmented system that integrates multiple data sources to generate keyword-specific ad copy with minimal human intervention. We validate MarketingFM via offline human and automated evaluations and large-scale online A/B tests. In one experiment, keyword-focused ad copy outperformed templates, achieving up to 9% higher CTR, 12% more impressions, and 0.38% lower CPC, demonstrating gains in ad ranking and cost efficiency. Despite these gains, human review of generated ads remains costly. To address this, we propose AutoEval-Main, an automated evaluation system that combines rule-based metrics with LLM-as-a-Judge techniques to ensure alignment with marketing principles. In experiments with large-scale human annotations, AutoEval-Main achieved 89.57% agreement with human reviewers. Building on this, we propose AutoEval-Update, a cost-efficient LLM-human collaborative framework to dynamically refine evaluation prompts and adapt to shifting criteria with minimal human input. By selectively sampling representative ads for human review and using a critic LLM to generate alignment reports, AutoEval-Update improves evaluation consistency while reducing manual effort. Experiments show the critic LLM suggests meaningful refinements, improving LLM-human agreement. Nonetheless, human oversight remains essential for setting thresholds and validating refinements before deployment.

</details>


### [65] [QueueEDIT: Structural Self-Correction for Sequential Model Editing in LLMs](https://arxiv.org/abs/2506.17864)

*Taolin Zhang, Haidong Kang, Dongyang Li, Qizhou Chen, Chengyu Wang Xiaofeng He, Richang Hong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Sequential Model Editing, Self-Correction Framework

**Relevance Score:** 9

**TL;DR:** Proposal of QueueEDIT, a queue-based self-correction framework for improving sequential model editing (SME) in large language models (LLMs) while maintaining their general capabilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the issue of hallucinations in LLMs by improving sequential model editing techniques.

**Method:** The proposed framework uses a queue to manage and align model parameters during continuous editing of knowledge, incorporating a structural mapping editing loss.

**Key Contributions:**

	1. Introduction of QueueEDIT framework for SME
	2. Development of structural mapping editing loss
	3. Dynamic alignment of previously edited parameters in a queue

**Result:** QueueEDIT outperforms strong baselines in various SME configurations while preserving general NLP capabilities throughout the editing process.

**Limitations:** 

**Conclusion:** The framework effectively enhances SME performance and mitigates parameter bias, offering a robust solution for correcting LLM inaccuracies.

**Abstract:** Recently, large language models (LLMs) have demonstrated impressive results but still suffer from hallucinations. Model editing has been proposed to correct factual inaccuracies in LLMs. A challenging case is sequential model editing (SME), which aims to rectify errors continuously rather than treating them as a one-time task. During SME, the general capabilities of LLMs can be negatively affected due to the introduction of new parameters. In this paper, we propose a queue-based self-correction framework (QueueEDIT) that not only enhances SME performance by addressing long-sequence dependency but also mitigates the impact of parameter bias on the general capabilities of LLMs. Specifically, we first introduce a structural mapping editing loss to map the triplets to the knowledge-sensitive neurons within the Transformer layers of LLMs. We then store the located parameters for each piece of edited knowledge in a queue and dynamically align previously edited parameters. In each edit, we select queue parameters most relevant to the currently located parameters to determine whether previous knowledge needs realignment. Irrelevant parameters in the queue are frozen, and we update the parameters at the queue head to the LLM to ensure they do not harm general abilities. Experiments show that our framework significantly outperforms strong baselines across various SME settings and maintains competitiveness in single-turn editing. The resulting LLMs also preserve high capabilities in general NLP tasks throughout the SME process.

</details>


### [66] [How Alignment Shrinks the Generative Horizon](https://arxiv.org/abs/2506.17871)

*Chenghao Yang, Ari Holtzman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Diversity, Branching Factor, Alignment Tuning, CoT Models

**Relevance Score:** 8

**TL;DR:** This paper investigates the lack of diversity in outputs from aligned large language models (LLMs) through a new measure called the Branching Factor (BF), revealing that BF decreases over generation and alignment tuning sharpens output predictability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the stability in output generation of aligned large language models and the implications for reasoning.

**Method:** Introduced the Branching Factor (BF) as a token-invariant measure of plausibility in output generation and conducted empirical analyses.

**Key Contributions:**

	1. Introduced the Branching Factor (BF) to measure output diversity
	2. Showed that BF decreases during generation, leading to more predictable outputs
	3. Demonstrated effects of alignment tuning on output distribution stability

**Result:** BF often decreases as generation progresses, indicating high predictability, and alignment tuning sharpens the output distribution significantly, reducing BF drastically.

**Limitations:** 

**Conclusion:** These findings demonstrate the BF as a crucial tool for comprehending LLM output control and variability reduction, emphasizing how alignment affects model behavior.

**Abstract:** Despite their impressive capabilities, aligned large language models (LLMs) often generate outputs that lack diversity. What drives this stability in the generation? We investigate this phenomenon through the lens of probability concentration in the model's output distribution. To quantify this concentration, we introduce the Branching Factor (BF) -- a token-invariant measure of the effective number of plausible next steps during generation. Our empirical analysis reveals two key findings: (1) BF often decreases as generation progresses, suggesting that LLMs become more predictable as they generate. (2) alignment tuning substantially sharpens the model's output distribution from the outset, reducing BF by nearly an order of magnitude (e.g., from 12 to 1.2) relative to base models. This stark reduction helps explain why aligned models often appear less sensitive to decoding strategies. Building on this insight, we find this stability has surprising implications for complex reasoning. Aligned Chain-of-Thought (CoT) models (e.g., DeepSeek-distilled models), for instance, leverage this effect; by generating longer reasoning chains, they push generation into later, more deterministic (lower BF) stages, resulting in more stable outputs. We hypothesize that alignment tuning does not fundamentally change a model's behavior, but instead steers it toward stylistic tokens (e.g., "Sure") that unlock low-entropy trajectories already present in the base model. This view is supported by nudging experiments, which show that prompting base models with such tokens can similarly reduce BF. Together, our findings establish BF as a powerful diagnostic for understanding and controlling LLM outputs - clarifying how alignment reduces variability, how CoT promotes stable generations, and how base models can be steered away from diversity.

</details>


### [67] [Multi-turn Jailbreaking via Global Refinement and Active Fabrication](https://arxiv.org/abs/2506.17881)

*Hua Tang, Lingyong Yan, Yukun Zhao, Shuaiqiang Wang, Jizhou Huang, Dawei Yin*

**Main category:** cs.CL

**Keywords:** Large Language Models, jailbreaking, multi-turn scenarios, dialogue dynamics, model responses

**Relevance Score:** 3

**TL;DR:** This paper presents a novel multi-turn jailbreaking method for Large Language Models that refines interactions globally and fabricates responses to elicit harmful outputs, outperforming existing techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to address security risks posed by Large Language Models (LLMs) and the limitations of existing multi-turn jailbreaking methods.

**Method:** A novel multi-turn jailbreaking method that globally refines the jailbreaking path at each interaction and actively fabricates model responses to suppress safety warnings.

**Key Contributions:**

	1. Development of a novel multi-turn jailbreaking method
	2. Global refinement of interactions during dialogue
	3. Active fabrication of model responses to suppress warnings

**Result:** Experimental results show that the proposed method significantly outperforms existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs.

**Limitations:** Potential ethical concerns with the misuse of jailbreaking techniques and the implications of eliciting harmful outputs.

**Conclusion:** The proposed method enhances the effectiveness of jailbreaking in multi-turn scenarios, providing a new approach to understanding and mitigating security threats in LLMs.

**Abstract:** Large Language Models (LLMs) have achieved exceptional performance across a wide range of tasks. However, they still pose significant safety risks due to the potential misuse for malicious purposes. Jailbreaks, which aim to elicit models to generate harmful content, play a critical role in identifying the underlying security threats. Recent jailbreaking primarily focuses on single-turn scenarios, while the more complicated multi-turn scenarios remain underexplored. Moreover, existing multi-turn jailbreaking techniques struggle to adapt to the evolving dynamics of dialogue as the interaction progresses. To address this limitation, we propose a novel multi-turn jailbreaking method that refines the jailbreaking path globally at each interaction. We also actively fabricate model responses to suppress safety-related warnings, thereby increasing the likelihood of eliciting harmful outputs in subsequent questions. Experimental results demonstrate the superior performance of our method compared with existing single-turn and multi-turn jailbreaking techniques across six state-of-the-art LLMs. Our code is publicly available at https://github.com/Ytang520/Multi-Turn_jailbreaking_Global-Refinment_and_Active-Fabrication.

</details>


### [68] [Scatter-Based Innovation Propagation in Large Language Models for Multi-Stage Process Adaptation](https://arxiv.org/abs/2506.17949)

*Hong Su*

**Main category:** cs.CL

**Keywords:** Large Language Models, innovation expansion, generalization, multi-stage processes, LLM applications

**Relevance Score:** 8

**TL;DR:** The paper introduces a scatter-based innovation expansion model for LLMs to improve their ability to generalize innovations across multi-stage processes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of LLMs struggling to generalize novel ideas beyond their original context, particularly when applying localized innovations to other parts of multi-stage processes.

**Method:** The proposed innovation scatter model guides LLMs through a four-step process: identifying core innovations, generalizing them, determining their broader applicability, and systematically applying innovations to similar stages.

**Key Contributions:**

	1. Development of a scatter-based innovation expansion model for LLMs
	2. Introduction of a structured four-step innovation application process
	3. Empirical verification demonstrating enhanced generalization capabilities of LLMs

**Result:** Verification results show that the innovation scatter model allows LLMs to effectively extend innovations across structurally similar stages, which enhances generalization and reuse.

**Limitations:** 

**Conclusion:** The innovation scatter model significantly improves the ability of LLMs to apply innovations in various contexts, thereby addressing their limitations in generalization.

**Abstract:** Large Language Models (LLMs) exhibit strong capabilities in reproducing and extending patterns observed during pretraining but often struggle to generalize novel ideas beyond their original context. This paper addresses the challenge of applying such localized innovations - introduced at a specific stage or component - to other parts of a multi-stage process. We propose a scatter-based innovation expansion model (innovation scatter model) that guides the LLM through a four-step process: (1) identifying the core innovation by comparing the user's input with its surrounding context, (2) generalizing the innovation by removing references to specific stages or components, (3) determining whether the generalized innovation applies to a broader scope beyond the original stage, and (4) systematically applying it to other structurally similar stages using the LLM. This model leverages structural redundancy across stages to improve the applicability of novel ideas. Verification results demonstrate that the innovation scatter model enables LLMs to extend innovations across structurally similar stages, thereby enhancing generalization and reuse.

</details>


### [69] [A Comprehensive Graph Framework for Question Answering with Mode-Seeking Preference Alignment](https://arxiv.org/abs/2506.17951)

*Quanwei Tang, Sophia Yat Mei Lee, Junshuang Wu, Dong Zhang, Shoushan Li, Erik Cambria, Guodong Zhou*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, preference alignment

**Relevance Score:** 9

**TL;DR:** GraphMPA enhances RAG in large language models for ethical question answering by employing a graph-based framework and preference optimization.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve global understanding and alignment of AI responses with human ethical and quality preferences in question answering tasks involving retrieval-augmented generation (RAG).

**Method:** The proposed GraphMPA framework constructs a hierarchical document graph using similarity measures and integrates a mode-seeking preference optimization technique to align outputs with human preferences through probability-matching constraints.

**Key Contributions:**

	1. Introduction of a hierarchical document graph for better information synthesis.
	2. Mode-seeking preference optimization to align responses with human preferences.
	3. Demonstration of enhanced effectiveness on multiple datasets.

**Result:** Extensive experiments indicated that GraphMPA significantly enhances the performance of large language models on six datasets by improving their ability to provide ethically aligned and quality responses.

**Limitations:** 

**Conclusion:** GraphMPA provides a robust method for better integrating human-like understanding into retrieval-augmented generation processes, thus offering a step forward in aligning AI responses with human expectations.

**Abstract:** Recent advancements in retrieval-augmented generation (RAG) have enhanced large language models in question answering by integrating external knowledge. However, challenges persist in achieving global understanding and aligning responses with human ethical and quality preferences. To address these issues, we propose GraphMPA, a comprehensive graph-based framework with mode-seeking preference alignment. Our approach constructs a hierarchical document graph using a general similarity measurement, mimicking human cognitive processes for information understanding and synthesis. Additionally, we introduce mode-seeking preference optimization to better align model outputs with human preferences through probability-matching constraints. Extensive experiments on six datasets demonstrate the effectiveness of our \href{https://github.com/tangquanwei/GraphMPA}{GraphMPA}.

</details>


### [70] [PDF Retrieval Augmented Question Answering](https://arxiv.org/abs/2506.18027)

*Thi Thu Uyen Hoang, Viet Anh Nguyen*

**Main category:** cs.CL

**Keywords:** Question-Answering, Retrieval Augmented Generation, Multimodal, PDF extraction, Large language models

**Relevance Score:** 9

**TL;DR:** This paper advances QA systems using a RAG framework to improve information extraction from PDF files, addressing challenges posed by multimodal content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of QA systems in extracting information from complex PDF documents which include diverse data types (text, images, tables, etc.).

**Method:** Development of a RAG-based QA system that refines the processing of non-textual elements in PDFs and fine-tunes large language models for improved integration.

**Key Contributions:**

	1. Advancement of RAG-based QA systems for PDFs
	2. Integration of multimodal data in QA
	3. Enhancement of large language model adaptation

**Result:** The experimental evaluation evidences the system's ability to accurately extract information from various content types in PDFs, achieving better responses to multimodal queries.

**Limitations:** 

**Conclusion:** This research enhances retrieval-augmented QA systems and serves as a foundation for further studies on multimodal data processing and integration.

**Abstract:** This paper presents an advancement in Question-Answering (QA) systems using a Retrieval Augmented Generation (RAG) framework to enhance information extraction from PDF files. Recognizing the richness and diversity of data within PDFs--including text, images, vector diagrams, graphs, and tables--poses unique challenges for existing QA systems primarily designed for textual content. We seek to develop a comprehensive RAG-based QA system that will effectively address complex multimodal questions, where several data types are combined in the query. This is mainly achieved by refining approaches to processing and integrating non-textual elements in PDFs into the RAG framework to derive precise and relevant answers, as well as fine-tuning large language models to better adapt to our system. We provide an in-depth experimental evaluation of our solution, demonstrating its capability to extract accurate information that can be applied to different types of content across PDFs. This work not only pushes the boundaries of retrieval-augmented QA systems but also lays a foundation for further research in multimodal data integration and processing.

</details>


### [71] [Splitformer: An improved early-exit architecture for automatic speech recognition on edge devices](https://arxiv.org/abs/2506.18035)

*Maxence Lasbordes, Daniele Falavigna, Alessio Brutti*

**Main category:** cs.CL

**Keywords:** neural models, early-exit architectures, speech recognition

**Relevance Score:** 7

**TL;DR:** This paper proposes an enhancement to early-exit neural architectures for automatic speech recognition by introducing parallel processing layers for downsampled inputs, which improves performance while maintaining inference time.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need to optimize computational loads in neural models for on-device processing with limited resources motivates this research.

**Method:** The authors introduce parallel layers in early-exit architectures that process downsampled versions of inputs alongside standard layers.

**Key Contributions:**

	1. Introduction of parallel downsampling layers in early-exit architectures
	2. Significant performance improvement in speech recognition
	3. Maintained inference time despite increased model parameters

**Result:** The new architecture demonstrates significantly improved speech recognition performance on standard benchmarks with a minor increase in model parameters, while inference times remain unaffected.

**Limitations:** 

**Conclusion:** Incorporating parallel layers for downsampling in early-exit models leads to better overall performance in speech recognition tasks without compromising efficiency.

**Abstract:** The ability to dynamically adjust the computational load of neural models during inference in a resource aware manner is crucial for on-device processing scenarios, characterised by limited and time-varying computational resources. Early-exit architectures represent an elegant and effective solution, since they can process the input with a subset of their layers, exiting at intermediate branches (the upmost layers are hence removed from the model).   From a different perspective, for automatic speech recognition applications there are memory-efficient neural architectures that apply variable frame rate analysis, through downsampling/upsampling operations in the middle layers, reducing the overall number of operations and improving significantly the performance on well established benchmarks. One example is the Zipformer. However, these architectures lack the modularity necessary to inject early-exit branches.   With the aim of improving the performance in early-exit models, we propose introducing parallel layers in the architecture that process downsampled versions of their inputs. % in conjunction with standard processing layers. We show that in this way the speech recognition performance on standard benchmarks significantly improve, at the cost of a small increase in the overall number of model parameters but without affecting the inference time.

</details>


### [72] [Markov-Enhanced Clustering for Long Document Summarization: Tackling the 'Lost in the Middle' Challenge with Large Language Models](https://arxiv.org/abs/2506.18036)

*Aziz Amari, Mohamed Achref Ben Ammar*

**Main category:** cs.CL

**Keywords:** text summarization, hybrid methods, large language models

**Relevance Score:** 8

**TL;DR:** A hybrid summarization approach combining extractive and abstractive techniques to improve automatic text summarization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of current summarization methods, particularly in retaining key information across lengthy documents.

**Method:** The proposed method splits documents into smaller chunks, clusters their vector embeddings, generates summaries for each cluster, and constructs the final summary using a Markov chain graph for semantic ordering.

**Key Contributions:**

	1. Introduction of a hybrid summarization approach
	2. Utilization of vector embedding clusters for summarization
	3. Employing a Markov chain graph for selecting semantic order

**Result:** The hybrid approach effectively retains key information while providing coherent summaries for lengthy texts.

**Limitations:** 

**Conclusion:** This method improves the capability of summarization models to manage information dense documents without losing critical content.

**Abstract:** The rapid expansion of information from diverse sources has heightened the need for effective automatic text summarization, which condenses documents into shorter, coherent texts. Summarization methods generally fall into two categories: extractive, which selects key segments from the original text, and abstractive, which generates summaries by rephrasing the content coherently. Large language models have advanced the field of abstractive summarization, but they are resourceintensive and face significant challenges in retaining key information across lengthy documents, which we call being "lost in the middle". To address these issues, we propose a hybrid summarization approach that combines extractive and abstractive techniques. Our method splits the document into smaller text chunks, clusters their vector embeddings, generates a summary for each cluster that represents a key idea in the document, and constructs the final summary by relying on a Markov chain graph when selecting the semantic order of ideas.

</details>


### [73] [Statistical Multicriteria Evaluation of LLM-Generated Text](https://arxiv.org/abs/2506.18082)

*Esteban Garces Arias, Hannah Blocher, Julian Rodemann, Matthias AÃenmacher, Christoph Jansen*

**Main category:** cs.CL

**Keywords:** LLM evaluation, Generalized Stochastic Dominance, text quality assessment, natural language processing, decoding strategies

**Relevance Score:** 9

**TL;DR:** The paper proposes a Generalized Stochastic Dominance (GSD) framework to evaluate LLM-generated text quality across multiple dimensions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inadequacies of current evaluation metrics in capturing nuanced differences in LLM-generated text quality.

**Method:** Adaptation of Generalized Stochastic Dominance (GSD) framework for simultaneous evaluation across multiple quality dimensions.

**Key Contributions:**

	1. Introduction of the GSD-front approach for evaluating LLM text quality
	2. Simultaneous multi-dimensional quality assessment
	3. Inferential statistical guarantees in evaluation

**Result:** The GSD-front approach can identify statistically significant performance differences in decoding strategies against human-generated text while accounting for sampling deviations.

**Limitations:** 

**Conclusion:** The framework offers a robust alternative to traditional evaluation methods, accommodating different scales and avoiding arbitrary metric weighting.

**Abstract:** Assessing the quality of LLM-generated text remains a fundamental challenge in natural language processing. Current evaluation approaches often rely on isolated metrics or simplistic aggregations that fail to capture the nuanced trade-offs between coherence, diversity, fluency, and other relevant indicators of text quality. In this work, we adapt a recently proposed framework for statistical inference based on Generalized Stochastic Dominance (GSD) that addresses three critical limitations in existing benchmarking methodologies: the inadequacy of single-metric evaluation, the incompatibility between cardinal automatic metrics and ordinal human judgments, and the lack of inferential statistical guarantees. The GSD-front approach enables simultaneous evaluation across multiple quality dimensions while respecting their different measurement scales, building upon partial orders of decoding strategies, thus avoiding arbitrary weighting of the involved metrics. By applying this framework to evaluate common decoding strategies against human-generated text, we demonstrate its ability to identify statistically significant performance differences while accounting for potential deviations from the i.i.d. assumption of the sampling design.

</details>


### [74] [Evaluating Prompt-Based and Fine-Tuned Approaches to Czech Anaphora Resolution](https://arxiv.org/abs/2506.18091)

*Patrik Stano, AleÅ¡ HorÃ¡k*

**Main category:** cs.CL

**Keywords:** anaphora resolution, Czech, large language models, fine-tuning, natural language understanding

**Relevance Score:** 7

**TL;DR:** This paper evaluates anaphora resolution methods for Czech using LLM prompt engineering and fine-tuned generative models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Anaphora resolution is crucial for natural language understanding, particularly in morphologically rich languages like Czech.

**Method:** The study compares prompt engineering with large language models (LLMs) against fine-tuned compact generative models (specifically mT5 and Mistral) on a dataset from the Prague Dependency Treebank.

**Key Contributions:**

	1. Comparative analysis of prompt engineering and fine-tuning for Czech anaphora resolution.
	2. Demonstrated superior accuracy with fine-tuned models compared to LLM prompts.
	3. Insight into the strengths and trade-offs of different anaphora resolution approaches.

**Result:** The fine-tuned mT5-large model achieved up to 88% accuracy, outperforming few-shot prompting methods that reached up to 74.5% accuracy.

**Limitations:** The study is limited to Czech language and may not generalize across other languages or dialects.

**Conclusion:** While prompting offers good initial accuracy, fine-tuning generative models significantly enhances performance and efficiency in anaphora resolution.

**Abstract:** Anaphora resolution plays a critical role in natural language understanding, especially in morphologically rich languages like Czech. This paper presents a comparative evaluation of two modern approaches to anaphora resolution on Czech text: prompt engineering with large language models (LLMs) and fine-tuning compact generative models. Using a dataset derived from the Prague Dependency Treebank, we evaluate several instruction-tuned LLMs, including Mistral Large 2 and Llama 3, using a series of prompt templates. We compare them against fine-tuned variants of the mT5 and Mistral models that we trained specifically for Czech anaphora resolution. Our experiments demonstrate that while prompting yields promising few-shot results (up to 74.5% accuracy), the fine-tuned models, particularly mT5-large, outperform them significantly, achieving up to 88% accuracy while requiring fewer computational resources. We analyze performance across different anaphora types, antecedent distances, and source corpora, highlighting key strengths and trade-offs of each approach.

</details>


### [75] [InspireDebate: Multi-Dimensional Subjective-Objective Evaluation-Guided Reasoning and Optimization for Debating](https://arxiv.org/abs/2506.18102)

*Fuyu Wang, Jiangtong Li, Kun Zhu, Changjun Jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Debating Tasks, Argument Quality Assessment, Multi-dimensional Evaluation, Real-time Knowledge Grounding

**Relevance Score:** 9

**TL;DR:** This paper proposes a dual-component framework, InspireScore for argument assessment and InspireDebate for enhanced debating, addressing the limitations of existing LLM-based systems.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of LLM-based debating systems by incorporating both subjective and objective assessment criteria.

**Method:** The paper introduces InspireScore, a multi-dimensional evaluation system, and InspireDebate, an optimized debating framework using CoT reasoning and Direct Preference Optimization.

**Key Contributions:**

	1. InspireScore: a novel evaluation system with subjective and objective metrics
	2. InspireDebate: an optimized debate framework with CoT reasoning and real-time knowledge grounding
	3. Empirical results demonstrating significant improvements in argument assessment and debate performance

**Result:** InspireScore shows a 44% higher correlation with expert judgments, and InspireDebate outperforms baseline models by 57%.

**Limitations:** 

**Conclusion:** The proposed framework enhances the assessment and effectiveness of LLMs in debating tasks, paving the way for more robust evaluation systems.

**Abstract:** With the rapid advancements in large language models (LLMs), debating tasks, such as argument quality assessment and debate process simulation, have made significant progress. However, existing LLM-based debating systems focus on responding to specific arguments while neglecting objective assessments such as authenticity and logical validity. Furthermore, these systems lack a structured approach to optimize across various dimensions$-$including evaluation metrics, chain-of-thought (CoT) reasoning, and multi-turn debate refinement$-$thereby limiting their effectiveness. To address these interconnected challenges, we propose a dual-component framework: (1) $\textbf{InspireScore}$, a novel evaluation system that establishes a multi-dimensional assessment architecture incorporating four subjective criteria (emotional appeal, argument clarity, argument arrangement, and topic relevance) alongside two objective metrics (fact authenticity and logical validity); and (2) $\textbf{InspireDebate}$, an optimized debating framework employing a phased optimization approach through CoT reasoning enhancement, multi-dimensional Direct Preference Optimization (DPO), and real-time knowledge grounding via web-based Retrieval Augmented Generation (Web-RAG). Empirical evaluations demonstrate that $\textbf{InspireScore}$ achieves 44$\%$ higher correlation with expert judgments compared to existing methods, while $\textbf{InspireDebate}$ shows significant improvements, outperforming baseline models by 57$\%$. Source code is available at https://github.com/fywang12/InspireDebate.

</details>


### [76] [Chengyu-Bench: Benchmarking Large Language Models for Chinese Idiom Understanding and Use](https://arxiv.org/abs/2506.18105)

*Yicheng Fu, Zhemin Huang, Liuxin Yang, Yumeng Lu, Zhongdongming Dai*

**Main category:** cs.CL

**Keywords:** Chinese idioms, Chengyu-Bench, language models, benchmarking, natural language processing

**Relevance Score:** 4

**TL;DR:** The paper introduces Chengyu-Bench, a benchmark for evaluating language models on Chinese idioms, highlighting their performance and challenges in understanding cultural nuances.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges language models face in interpreting and correctly using Chinese idioms due to their cultural complexity.

**Method:** The benchmark includes three tasks: Evaluative Connotation, Appropriateness, and Open Cloze, evaluating models on their understanding and usage of idioms.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for idioms
	2. Evaluation of leading LLMs on idiom usage tasks
	3. Insights into model performance and error analysis

**Result:** Leading LLMs scored over 95% on Evaluative Connotation, about 85% on Appropriateness, and ~40% on Open Cloze, indicating a disparity in performance based on task complexity.

**Limitations:** The benchmark may not cover all idioms or contexts, and results are limited to specific models tested.

**Conclusion:** Chengyu-Bench reveals that LLMs can assess idiom sentiment effectively but often misunderstand cultural and contextual meanings, underscoring the need for improved models.

**Abstract:** Chinese idioms (Chengyu) are concise four-character expressions steeped in history and culture, whose literal translations often fail to capture their full meaning. This complexity makes them challenging for language models to interpret and use correctly. Existing benchmarks focus on narrow tasks - multiple-choice cloze tests, isolated translation, or simple paraphrasing. We introduce Chengyu-Bench, a comprehensive benchmark featuring three tasks: (1) Evaluative Connotation, classifying idioms as positive or negative; (2) Appropriateness, detecting incorrect idiom usage in context; and (3) Open Cloze, filling blanks in longer passages without options. Chengyu-Bench comprises 2,937 human-verified examples covering 1,765 common idioms sourced from diverse corpora. We evaluate leading LLMs and find they achieve over 95% accuracy on Evaluative Connotation, but only ~85% on Appropriateness and ~40% top-1 accuracy on Open Cloze. Error analysis reveals that most mistakes arise from fundamental misunderstandings of idiom meanings. Chengyu-Bench demonstrates that while LLMs can reliably gauge idiom sentiment, they still struggle to grasp the cultural and contextual nuances essential for proper usage. The benchmark and source code are available at: https://github.com/sofyc/ChengyuBench.

</details>


### [77] [Mental Health Equity in LLMs: Leveraging Multi-Hop Question Answering to Detect Amplified and Silenced Perspectives](https://arxiv.org/abs/2506.18116)

*Batool Haider, Atmika Gorti, Aman Chadha, Manas Gaur*

**Main category:** cs.CL

**Keywords:** Large Language Models, mental healthcare, bias detection, debiasing techniques, intersectional analysis

**Relevance Score:** 9

**TL;DR:** This paper introduces a multi-hop question answering framework to detect intersectional biases in mental health-related responses generated by large language models, revealing systematic disparities and proposing debiasing techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing biases in LLMs that harm marginalized groups in mental healthcare.

**Method:** A multi-hop question answering (MHQA) framework analyzing responses from four LLMs using a tagged dataset for demographic intersections.

**Key Contributions:**

	1. Introduced a multi-hop question answering framework for bias detection in LLMs.
	2. Demonstrated systematic disparities using demographic tagging.
	3. Implemented effective debiasing techniques with significant results.

**Result:** Systematic disparities detected in LLM responses across various demographics, with bias reductions of 66-94% achieved through two debiasing techniques.

**Limitations:** 

**Conclusion:** The study provides insights into how LLMs can propagate biases in mental health and suggests methods for improving fairness in AI applications.

**Abstract:** Large Language Models (LLMs) in mental healthcare risk propagating biases that reinforce stigma and harm marginalized groups. While previous research identified concerning trends, systematic methods for detecting intersectional biases remain limited. This work introduces a multi-hop question answering (MHQA) framework to explore LLM response biases in mental health discourse. We analyze content from the Interpretable Mental Health Instruction (IMHI) dataset across symptom presentation, coping mechanisms, and treatment approaches. Using systematic tagging across age, race, gender, and socioeconomic status, we investigate bias patterns at demographic intersections. We evaluate four LLMs: Claude 3.5 Sonnet, Jamba 1.6, Gemma 3, and Llama 4, revealing systematic disparities across sentiment, demographics, and mental health conditions. Our MHQA approach demonstrates superior detection compared to conventional methods, identifying amplification points where biases magnify through sequential reasoning. We implement two debiasing techniques: Roleplay Simulation and Explicit Bias Reduction, achieving 66-94% bias reductions through few-shot prompting with BBQ dataset examples. These findings highlight critical areas where LLMs reproduce mental healthcare biases, providing actionable insights for equitable AI development.

</details>


### [78] [The Syntactic Acceptability Dataset (Preview): A Resource for Machine Learning and Linguistic Analysis of English](https://arxiv.org/abs/2506.18120)

*Tom S Juzek*

**Main category:** cs.CL

**Keywords:** syntax, computational linguistics, acceptability, grammaticality, machine learning

**Relevance Score:** 6

**TL;DR:** The Syntactic Acceptability Dataset aims to aid syntax and computational linguistics research by providing 1,000 annotated English sequences with grammatical and acceptability status, revealing insights into machine learning performance in predicting these aspects.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a resource for syntax and computational linguistics research that provides labeled data on grammatical status and acceptability of English sequences.

**Method:** The dataset comprises 1,000 English sequences, labeled for grammaticality and acceptability, with the latter determined through crowdsourcing. The entries are divided between textbook examples and contemporary linguistic discourse.

**Key Contributions:**

	1. Introduction of the Syntactic Acceptability Dataset which is the largest of its kind.
	2. Demonstration of convergence between grammaticality and acceptability judgments in most cases.
	3. Novel findings on machine learning performance related to grammaticality and acceptability predictions.

**Result:** Grammaticality and acceptability judgments aligned in approximately 83% of cases; machine learning models struggle with grammaticality prediction but perform better on acceptability.

**Limitations:** 

**Conclusion:** The Syntactic Acceptability Dataset, the largest publicly accessible dataset of its kind, provides important insights into linguistic judgments and machine learning capabilities.

**Abstract:** We present a preview of the Syntactic Acceptability Dataset, a resource being designed for both syntax and computational linguistics research. In its current form, the dataset comprises 1,000 English sequences from the syntactic discourse: Half from textbooks and half from the journal Linguistic Inquiry, the latter to ensure a representation of the contemporary discourse. Each entry is labeled with its grammatical status ("well-formedness" according to syntactic formalisms) extracted from the literature, as well as its acceptability status ("intuitive goodness" as determined by native speakers) obtained through crowdsourcing, with highest experimental standards. Even in its preliminary form, this dataset stands as the largest of its kind that is publicly accessible. We also offer preliminary analyses addressing three debates in linguistics and computational linguistics: We observe that grammaticality and acceptability judgments converge in about 83% of the cases and that "in-betweenness" occurs frequently. This corroborates existing research. We also find that while machine learning models struggle with predicting grammaticality, they perform considerably better in predicting acceptability. This is a novel finding. Future work will focus on expanding the dataset.

</details>


### [79] [$Ï^{\infty}$: Clause Purification, Embedding Realignment, and the Total Suppression of the Em Dash in Autoregressive Language Models](https://arxiv.org/abs/2506.18129)

*Bugra Kilictas, Faruk Alpay*

**Main category:** cs.CL

**Keywords:** language models, semantic drift, AI safety, token perturbations, neural text generation

**Relevance Score:** 8

**TL;DR:** The paper addresses a vulnerability in transformer language models caused by the em dash token, leading to semantic drift and errors in long-form text generation, proposing a solution that avoids retraining the model.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and mitigate critical vulnerabilities in autoregressive transformer language models, specifically focusing on issues caused by token perturbations such as the em dash.

**Method:** Formal analysis of token-level perturbations in semantic lattices and a proposed solution involving symbolic clause purification and targeted embedding matrix realignment.

**Key Contributions:**

	1. Identified vulnerability in transformer models causing semantic drift due to token insertion.
	2. Proposed a novel solution to mitigate these vulnerabilities without retraining.
	3. Established a framework for tackling broader recursive instabilities in neural text generation.

**Result:** The proposed solution significantly improves generation consistency and topic maintenance in language models, demonstrating a method for suppressing problematic tokens without retraining.

**Limitations:** 

**Conclusion:** The work provides a framework for addressing token-level vulnerabilities in foundation models, with implications for AI safety and robust deployment in production.

**Abstract:** We identify a critical vulnerability in autoregressive transformer language models where the em dash token induces recursive semantic drift, leading to clause boundary hallucination and embedding space entanglement. Through formal analysis of token-level perturbations in semantic lattices, we demonstrate that em dash insertion fundamentally alters the model's latent representations, causing compounding errors in long-form generation. We propose a novel solution combining symbolic clause purification via the phi-infinity operator with targeted embedding matrix realignment. Our approach enables total suppression of problematic tokens without requiring model retraining, while preserving semantic coherence through fixed-point convergence guarantees. Experimental validation shows significant improvements in generation consistency and topic maintenance. This work establishes a general framework for identifying and mitigating token-level vulnerabilities in foundation models, with immediate implications for AI safety, model alignment, and robust deployment of large language models in production environments. The methodology extends beyond punctuation to address broader classes of recursive instabilities in neural text generation systems.

</details>


### [80] [Sparse Feature Coactivation Reveals Composable Semantic Modules in Large Language Models](https://arxiv.org/abs/2506.18141)

*Ruixuan Deng, Xiaoyang Hu, Miles Gilberti, Shane Storks, Aman Taxali, Mike Angstadt, Chandra Sripada, Joyce Chai*

**Main category:** cs.CL

**Keywords:** large language models, modular organization, semantic components, sparse autoencoders, counterfactual responses

**Relevance Score:** 9

**TL;DR:** The paper explores the modular organization of knowledge in large language models (LLMs) by analyzing the coactivation of features from sparse autoencoders to manipulate model outputs related to country-relation tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the internal organization of knowledge in LLMs and how semantic components can be manipulated for predictable model output changes.

**Method:** The authors use sparse autoencoder features collected from prompts to identify semantically coherent network components associated with countries and relations in LLMs.

**Key Contributions:**

	1. Identification of semantically coherent network components in LLMs
	2. Demonstration of predictable output changes through component manipulation
	3. Insights into the layered modular organization of knowledge in LLMs

**Result:** Manipulating country and relation components leads to predictable changes in model outputs, and integrating these components generates counterfactual responses, highlighting a modular structure in knowledge representation.

**Limitations:** 

**Conclusion:** The study underscores the layered structure of semantic components within LLMs, with early layers primarily housing country components and later layers hosting more abstract relation components, facilitating targeted model manipulation.

**Abstract:** We identify semantically coherent, context-consistent network components in large language models (LLMs) using coactivation of sparse autoencoder (SAE) features collected from just a handful of prompts. Focusing on country-relation tasks, we show that ablating semantic components for countries and relations changes model outputs in predictable ways, while amplifying these components induces counterfactual responses. Notably, composing relation and country components yields compound counterfactual outputs. We find that, whereas most country components emerge from the very first layer, the more abstract relation components are concentrated in later layers. Furthermore, within relation components themselves, nodes from later layers tend to have a stronger causal impact on model outputs. Overall, these findings suggest a modular organization of knowledge within LLMs and advance methods for efficient, targeted model manipulation.

</details>


### [81] [QuranMorph: Morphologically Annotated Quranic Corpus](https://arxiv.org/abs/2506.18148)

*Diyam Akra, Tymaa Hammouda, Mustafa Jarrar*

**Main category:** cs.CL

**Keywords:** QuranMorph, morphologically annotated corpus, lemmatization, part-of-speech, Arabic linguistics

**Relevance Score:** 2

**TL;DR:** The QuranMorph corpus is a morphologically annotated dataset of the Quran with manual lemmatization and part-of-speech tagging.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a rich linguistic resource for morphological analysis of the Quran.

**Method:** Manual lemmatization and part-of-speech tagging by three expert linguists using 40 fine-grained tags.

**Key Contributions:**

	1. Creation of a comprehensive morphologically annotated corpus for the Quran
	2. Manual lemmatization and POS tagging using expert linguists
	3. Interlinking with existing linguistic resources

**Result:** The corpus consists of 77,429 tokens and can be interlinked with various linguistic resources.

**Limitations:** 

**Conclusion:** The QuranMorph corpus is open-source and publicly available, enhancing access to Arabic linguistic resources.

**Abstract:** We present the QuranMorph corpus, a morphologically annotated corpus for the Quran (77,429 tokens). Each token in the QuranMorph was manually lemmatized and tagged with its part-of-speech by three expert linguists. The lemmatization process utilized lemmas from Qabas, an Arabic lexicographic database linked with 110 lexicons and corpora of 2 million tokens. The part-of-speech tagging was performed using the fine-grained SAMA/Qabas tagset, which encompasses 40 tags. As shown in this paper, this rich lemmatization and POS tagset enabled the QuranMorph corpus to be inter-linked with many linguistic resources. The corpus is open-source and publicly available as part of the SinaLab resources at (https://sina.birzeit.edu/quran)

</details>


### [82] [CareLab at #SMM4H-HeaRD 2025: Insomnia Detection and Food Safety Event Extraction with Domain-Aware Transformers](https://arxiv.org/abs/2506.18185)

*Zihan Liang, Ziwen Pan, Sumon Kanti Dey, Azra Ismail*

**Main category:** cs.CL

**Keywords:** Insomnia Detection, Food Safety Extraction, Health Informatics, Text Mining, NLP Models

**Relevance Score:** 8

**TL;DR:** The paper describes a system designed for shared tasks in health data mining, achieving top results in detecting insomnia mentions and extracting food safety events, utilizing advanced models and data augmentation techniques.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to advance the state of health informatics by improving the detection of relevant health-related mentions in clinical notes and extracting significant events related to food safety from public news articles.

**Method:** The authors employed encoder-based models like RoBERTa and GPT-4 for data augmentation, implementing specific adaptations for each subtask in the shared task framework.

**Key Contributions:**

	1. Achieved first place in Task 5 Subtask 1 with an F1 score of 0.958
	2. Utilized state-of-the-art encoder models and GPT-4 for data augmentation
	3. Provided insights into the preprocessing and model architecture tailored for specific subtasks.

**Result:** The system achieved an F1 score of 0.958 in Task 5 Subtask 1, securing first place in the competition and demonstrating robust performance across the evaluated subtasks.

**Limitations:** 

**Conclusion:** The findings indicate that leveraging advanced model architectures and data augmentation techniques can significantly enhance performance in health-related text mining tasks.

**Abstract:** This paper presents our system for the SMM4H-HeaRD 2025 shared tasks, specifically Task 4 (Subtasks 1, 2a, and 2b) and Task 5 (Subtasks 1 and 2). Task 4 focused on detecting mentions of insomnia in clinical notes, while Task 5 addressed the extraction of food safety events from news articles. We participated in all subtasks and report key findings across them, with particular emphasis on Task 5 Subtask 1, where our system achieved strong performance-securing first place with an F1 score of 0.958 on the test set. To attain this result, we employed encoder-based models (e.g., RoBERTa), alongside GPT-4 for data augmentation. This paper outlines our approach, including preprocessing, model architecture, and subtask-specific adaptations

</details>


### [83] [Prompt Engineering Techniques for Mitigating Cultural Bias Against Arabs and Muslims in Large Language Models: A Systematic Review](https://arxiv.org/abs/2506.18199)

*Bushra Asseri, Estabrag Abdelaziz, Areej Al-Wabil*

**Main category:** cs.CL

**Keywords:** cultural bias, large language models, prompt engineering, Arab representation, bias mitigation

**Relevance Score:** 8

**TL;DR:** This systematic review investigates prompt engineering strategies to mitigate cultural bias, particularly towards Arabs and Muslims, in large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address ethical challenges posed by cultural bias in LLMs that perpetuate stereotypes and marginalization of Arabs and Muslims.

**Method:** Mixed-methods systematic review analyzing 8 empirical studies published between 2021-2024, following PRISMA guidelines and Kitchenham's methodology.

**Key Contributions:**

	1. Systematic review of bias mitigation strategies in LLMs
	2. Identification of effective prompt engineering techniques
	3. Highlighting the need for further culturally adaptive research

**Result:** Identified five prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts, with varying effectiveness for bias mitigation.

**Limitations:** A limited number of studies on this critical area were reviewed, indicating a significant research gap.

**Conclusion:** While structured multi-step pipelines showed the highest effectiveness, there is a significant gap in research on bias mitigation techniques for Arab and Muslim representation in LLMs.

**Abstract:** Large language models have demonstrated remarkable capabilities across various domains, yet concerns about cultural bias - particularly towards Arabs and Muslims - pose significant ethical challenges by perpetuating harmful stereotypes and marginalization. Despite growing recognition of bias in LLMs, prompt engineering strategies specifically addressing Arab and Muslim representation remain understudied. This mixed-methods systematic review examines such techniques, offering evidence-based guidance for researchers and practitioners. Following PRISMA guidelines and Kitchenham's systematic review methodology, we analyzed 8 empirical studies published between 2021-2024 investigating bias mitigation strategies. Our findings reveal five primary prompt engineering approaches: cultural prompting, affective priming, self-debiasing techniques, structured multi-step pipelines, and parameter-optimized continuous prompts. Although all approaches show potential for reducing bias, effectiveness varied substantially across studies and bias types. Evidence suggests that certain bias types may be more resistant to prompt-based mitigation than others. Structured multi-step pipelines demonstrated the highest overall effectiveness, achieving up to 87.7% reduction in bias, though they require greater technical expertise. Cultural prompting offers broader accessibility with substantial effectiveness. These results underscore the accessibility of prompt engineering for mitigating cultural bias without requiring access to model parameters. The limited number of studies identified highlights a significant research gap in this critical area. Future research should focus on developing culturally adaptive prompting techniques, creating Arab and Muslim-specific evaluation resources, and integrating prompt engineering with complementary debiasing methods to address deeper stereotypes while maintaining model utility.

</details>


### [84] [Deciphering Emotions in Children Storybooks: A Comparative Analysis of Multimodal LLMs in Educational Applications](https://arxiv.org/abs/2506.18201)

*Bushra Asseri, Estabraq Abdelaziz, Maha Al Mogren, Tayef Alhefdhi, Areej Al-Wabil*

**Main category:** cs.CL

**Keywords:** emotion recognition, multimodal AI, Arabic educational technologies, GPT-4o, Gemini 1.5 Pro

**Relevance Score:** 8

**TL;DR:** Study evaluates emotion recognition in Arabic contexts using multimodal AI models GPT-4o and Gemini 1.5 Pro for educational tools.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop culturally responsive educational technologies for Arabic-speaking learners, focusing on emotion recognition capabilities in multimodal AI.

**Method:** Two large language models (GPT-4o and Gemini 1.5 Pro) were tested on 75 images from Arabic children's storybooks using zero-shot, few-shot, and chain-of-thought prompting strategies, comparing model predictions with human annotations.

**Key Contributions:**

	1. Evaluation of advanced multimodal AI models in Arabic educational contexts
	2. Identification of emotions based on Plutchik's framework
	3. Insights into limitations of models regarding cultural nuances in emotion recognition

**Result:** GPT-4o outperformed Gemini, achieving a macro F1-score of 59% with chain-of-thought prompting, while Gemini reached 43%. Misclassification included significant valence inversions and struggled with culturally nuanced emotions.

**Limitations:** Both models have difficulty with culturally nuanced emotions and ambiguous contexts, highlighting the need for better training.

**Conclusion:** Current models have fundamental limitations in cultural understanding, necessitating culturally sensitive training to create effective emotion-aware educational tools for Arabic learners.

**Abstract:** Emotion recognition capabilities in multimodal AI systems are crucial for developing culturally responsive educational technologies, yet remain underexplored for Arabic language contexts where culturally appropriate learning tools are critically needed. This study evaluates the emotion recognition performance of two advanced multimodal large language models, GPT-4o and Gemini 1.5 Pro, when processing Arabic children's storybook illustrations. We assessed both models across three prompting strategies (zero-shot, few-shot, and chain-of-thought) using 75 images from seven Arabic storybooks, comparing model predictions with human annotations based on Plutchik's emotional framework. GPT-4o consistently outperformed Gemini across all conditions, achieving the highest macro F1-score of 59% with chain-of-thought prompting compared to Gemini's best performance of 43%. Error analysis revealed systematic misclassification patterns, with valence inversions accounting for 60.7% of errors, while both models struggled with culturally nuanced emotions and ambiguous narrative contexts. These findings highlight fundamental limitations in current models' cultural understanding and emphasize the need for culturally sensitive training approaches to develop effective emotion-aware educational technologies for Arabic-speaking learners.

</details>


### [85] [Enhancing Entity Aware Machine Translation with Multi-task Learning](https://arxiv.org/abs/2506.18318)

*An Trieu, Phuong Nguyen, Minh Le Nguyen*

**Main category:** cs.CL

**Keywords:** Entity-aware machine translation, Multi-task learning, Named entity recognition, Natural language processing, SemEval 2025

**Relevance Score:** 4

**TL;DR:** The paper presents a multi-task learning approach to improve entity-aware machine translation (EAMT) by optimizing named entity recognition and translation subtasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** EAMT is challenging due to insufficient translation data for entities and the contextual complexities involved in translating them.

**Method:** The authors propose a multi-task learning framework that simultaneously optimizes entity recognition and machine translation to enhance EAMT performance.

**Key Contributions:**

	1. Proposed a novel multi-task learning approach for EAMT
	2. Improved performance metrics on named entity recognition and machine translation tasks
	3. Evaluated results on a benchmark dataset from SemEval 2025 competition

**Result:** Significant improvements in EAMT performance were observed through the proposed methodology, as validated on the SemEval 2025 competition dataset.

**Limitations:** 

**Conclusion:** The multi-task learning approach effectively enhances the performance of EAMT tasks by leveraging the synergy between entity recognition and translation processes.

**Abstract:** Entity-aware machine translation (EAMT) is a complicated task in natural language processing due to not only the shortage of translation data related to the entities needed to translate but also the complexity in the context needed to process while translating those entities. In this paper, we propose a method that applies multi-task learning to optimize the performance of the two subtasks named entity recognition and machine translation, which improves the final performance of the Entity-aware machine translation task. The result and analysis are performed on the dataset provided by the organizer of Task 2 of the SemEval 2025 competition.

</details>


### [86] [TranslationCorrect: A Unified Framework for Machine Translation Post-Editing with Predictive Error Assistance](https://arxiv.org/abs/2506.18337)

*Syed Mekael Wasti, Shou-Yi Hung, Christopher Collins, En-Shiun Annie Lee*

**Main category:** cs.CL

**Keywords:** Machine Translation, Post-editing, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** TranslationCorrect is an integrated framework designed to enhance machine translation post-editing and research data collection by combining MT generation, automated error prediction, and a user-friendly interface.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies and disconnection in machine translation post-editing and research data collection workflows.

**Method:** TranslationCorrect integrates machine translation generation, automated error prediction using advanced models, and a user-friendly post-editing interface, informed by HCI principles.

**Key Contributions:**

	1. Integrated framework combining MT generation and error prediction
	2. User-friendly post-editing interface designed on HCI principles
	3. High-quality span-based annotations compatible with error detection models

**Result:** User studies indicate that TranslationCorrect improves translation efficiency and user satisfaction compared to traditional methods.

**Limitations:** 

**Conclusion:** TranslationCorrect offers a comprehensive solution for improving machine translation workflows and providing high-quality error annotations for researchers.

**Abstract:** Machine translation (MT) post-editing and research data collection often rely on inefficient, disconnected workflows. We introduce TranslationCorrect, an integrated framework designed to streamline these tasks. TranslationCorrect combines MT generation using models like NLLB, automated error prediction using models like XCOMET or LLM APIs (providing detailed reasoning), and an intuitive post-editing interface within a single environment. Built with human-computer interaction (HCI) principles in mind to minimize cognitive load, as confirmed by a user study. For translators, it enables them to correct errors and batch translate efficiently. For researchers, TranslationCorrect exports high-quality span-based annotations in the Error Span Annotation (ESA) format, using an error taxonomy inspired by Multidimensional Quality Metrics (MQM). These outputs are compatible with state-of-the-art error detection models and suitable for training MT or post-editing systems. Our user study confirms that TranslationCorrect significantly improves translation efficiency and user satisfaction over traditional annotation methods.

</details>


### [87] [Less Data Less Tokens: Multilingual Unification Learning for Efficient Test-Time Reasoning in LLMs](https://arxiv.org/abs/2506.18341)

*Kang Chen, Mengdi Zhang, Yixin Cao*

**Main category:** cs.CL

**Keywords:** large language models, multilingual reasoning, data efficiency, inference efficiency, L2 learning

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach called L2 multi-lingual unification learning to improve the efficiency and performance of large language models in multi-lingual reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of test-time scaling of large language models in terms of data and inference efficiency, especially in multi-lingual contexts.

**Method:** Introduces the L2 method which utilizes both long chain-of-thought annotations and step-wise mixtures of languages during tuning to enhance reasoning capabilities with minimal data.

**Key Contributions:**

	1. Introduction of L2 multi-lingual unification learning
	2. Demonstration of performance improvement with minimal data
	3. Recognition of diverse data selection as crucial for efficiency

**Result:** The L2 approach demonstrates that small amounts of multilingual data can significantly enhance reasoning abilities and reduce both data requirements and inference token counts without sacrificing performance.

**Limitations:** 

**Conclusion:** The L2 method provides a promising direction for improving data collection and computation efficiency in large language models, highlighting the importance of diverse data selection.

**Abstract:** This paper explores the challenges of test-time scaling of large language models (LLMs), regarding both the data and inference efficiency. We highlight the diversity of multi-lingual reasoning based on our pilot studies, and then introduce a novel approach, \(L^2\) multi-lingual unification learning with a decoding intervention strategy for further investigation. The basic idea of \(L^2\) is that the reasoning process varies across different languages, which may be mutually beneficial to enhance both model performance and efficiency. In specific, there are two types of multi-lingual data: the entire long chain-of-thought annotations in different languages and the step-wise mixture of languages. By further tuning based on them, we show that even small amounts of data can significantly improve reasoning capabilities. Our findings suggest that multilingual learning reduces both the required data and the number of inference tokens while maintaining a comparable performance. Furthermore, \(L^2\) is orthogonal to other data efficient methods. Thus, we also emphasize the importance of diverse data selection. The \(L^2\) method offers a promising solution to the challenges of data collection and test-time compute efficiency in LLMs.

</details>


### [88] [Evaluating Causal Explanation in Medical Reports with LLM-Based and Human-Aligned Metrics](https://arxiv.org/abs/2506.18387)

*Yousang Cho, Key-Sun Choi*

**Main category:** cs.CL

**Keywords:** causal explanations, diagnostic reports, evaluation metrics, LLM, interpretability

**Relevance Score:** 9

**TL;DR:** This paper evaluates different metrics for assessing causal explanations in diagnostic reports, highlighting the effectiveness of LLM-based metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To determine how accurately various evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports.

**Method:** The study compares six metrics (BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, expert qualitative assessment) using two input types (observation-based and multiple-choice-based report generation) alongside two weighting strategies.

**Key Contributions:**

	1. Introduces a comparison of six different evaluation metrics for diagnostic report generation.
	2. Shows strong performance of LLM-based metrics (GPT-Black and GPT-White) over traditional similarity-based metrics.
	3. Highlights the implications of metric selection and weighting on evaluating causal narratives.

**Result:** GPT-Black shows the best discriminative ability for identifying coherent and clinically valid causal narratives, while GPT-White aligns well with expert evaluations.

**Limitations:** 

**Conclusion:** The selection and weighting of metrics significantly affect evaluation outcomes, advocating for LLM-based evaluations in tasks needing interpretability and causal reasoning.

**Abstract:** This study investigates how accurately different evaluation metrics capture the quality of causal explanations in automatically generated diagnostic reports. We compare six metrics: BERTScore, Cosine Similarity, BioSentVec, GPT-White, GPT-Black, and expert qualitative assessment across two input types: observation-based and multiple-choice-based report generation. Two weighting strategies are applied: one reflecting task-specific priorities, and the other assigning equal weights to all metrics. Our results show that GPT-Black demonstrates the strongest discriminative power in identifying logically coherent and clinically valid causal narratives. GPT-White also aligns well with expert evaluations, while similarity-based metrics diverge from clinical reasoning quality. These findings emphasize the impact of metric selection and weighting on evaluation outcomes, supporting the use of LLM-based evaluation for tasks requiring interpretability and causal reasoning.

</details>


### [89] [Lemmatization as a Classification Task: Results from Arabic across Multiple Genres](https://arxiv.org/abs/2506.18399)

*Mostafa Saeed, Nizar Habash*

**Main category:** cs.CL

**Keywords:** lemmatization, Arabic, machine translation, NLP, semantic clustering

**Relevance Score:** 6

**TL;DR:** The paper presents novel lemmatization approaches for Arabic, addressing challenges in NLP tasks by framing it as a classification problem and introducing a new evaluation dataset.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing lemmatization tools face challenges due to inconsistent standards and genre coverage in Arabic.

**Method:** Framing lemmatization as classification into a Lemma-POS-Gloss tagset using machine translation and semantic clustering.

**Key Contributions:**

	1. Introduction of LPG tagset for lemmatization
	2. Development of a new Arabic lemmatization test set across diverse genres
	3. Evaluation of sequence-to-sequence models alongside classification methods

**Result:** The proposed methods outperform existing tools, yielding more robust and interpretable outputs, and setting new benchmarks for Arabic lemmatization.

**Limitations:** Character-level models are limited to lemma prediction and may hallucinate implausible forms.

**Conclusion:** Classification and clustering approaches provide significant improvements over traditional lemmatization methods, especially in morphologically rich languages like Arabic.

**Abstract:** Lemmatization is crucial for NLP tasks in morphologically rich languages with ambiguous orthography like Arabic, but existing tools face challenges due to inconsistent standards and limited genre coverage. This paper introduces two novel approaches that frame lemmatization as classification into a Lemma-POS-Gloss (LPG) tagset, leveraging machine translation and semantic clustering. We also present a new Arabic lemmatization test set covering diverse genres, standardized alongside existing datasets. We evaluate character level sequence-to-sequence models, which perform competitively and offer complementary value, but are limited to lemma prediction (not LPG) and prone to hallucinating implausible forms. Our results show that classification and clustering yield more robust, interpretable outputs, setting new benchmarks for Arabic lemmatization.

</details>


### [90] [TReB: A Comprehensive Benchmark for Evaluating Table Reasoning Capabilities of Large Language Models](https://arxiv.org/abs/2506.18421)

*Ce Li, Xiaofan Liu, Zhiyan Song, Ce Chi, Chen Zhao, Jingjing Yang, Zhendong Wang, Kexin Yang, Boshen Shi, Xing Wang, Chao Deng, Junlan Feng*

**Main category:** cs.CL

**Keywords:** table reasoning, large language models, benchmark, dataset, evaluation framework

**Relevance Score:** 8

**TL;DR:** This paper introduces a comprehensive benchmark for evaluating large language models' reasoning with table-structured data, called TReB, which encompasses 26 sub-tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges LLMs face when reasoning with table-structured data and to provide an effective evaluation benchmark reflecting their abilities.

**Method:** The authors constructed a high-quality dataset through iterative data processing and developed an evaluation framework that measures reasoning capabilities using three inference modes: TCoT, PoT, and ICoT.

**Key Contributions:**

	1. Introduction of TReB benchmark for table reasoning
	2. Development of a dataset for evaluating table reasoning
	3. Establishment of an evaluation framework with three inference modes

**Result:** Benchmarking over 20 state-of-the-art LLMs revealed significant room for improvement in handling complex table-related tasks.

**Limitations:** 

**Conclusion:** The dataset and evaluation framework are publicly available, promoting further research in LLM capabilities for table reasoning.

**Abstract:** The majority of data in businesses and industries is stored in tables, databases, and data warehouses. Reasoning with table-structured data poses significant challenges for large language models (LLMs) due to its hidden semantics, inherent complexity, and structured nature. One of these challenges is lacking an effective evaluation benchmark fairly reflecting the performances of LLMs on broad table reasoning abilities. In this paper, we fill in this gap, presenting a comprehensive table reasoning evolution benchmark, TReB, which measures both shallow table understanding abilities and deep table reasoning abilities, a total of 26 sub-tasks. We construct a high quality dataset through an iterative data processing procedure. We create an evaluation framework to robustly measure table reasoning capabilities with three distinct inference modes, TCoT, PoT and ICoT. Further, we benchmark over 20 state-of-the-art LLMs using this frame work and prove its effectiveness. Experimental results reveal that existing LLMs still have significant room for improvement in addressing the complex and real world Table related tasks. Both the dataset and evaluation framework are publicly available, with the dataset hosted on [HuggingFace] and the framework on [GitHub].

</details>


### [91] [MeRF: Motivation-enhanced Reinforcement Finetuning for Large Reasoning Models](https://arxiv.org/abs/2506.18485)

*Junjie Zhang, Guozheng Ma, Shunyu Liu, Haoyu Wang, Jiaxing Huang, Ting-En Lin, Fei Huang, Yongbin Li, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, In-Context Learning, Motivation, Knights and Knaves

**Relevance Score:** 9

**TL;DR:** The paper introduces Motivation-enhanced Reinforcement Finetuning (MeRF), a method that combines reinforcement learning with the in-context learning ability of LLMs to improve reasoning tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing RLVR methods fail to leverage LLMs' in-context learning ability, particularly seen with Chain-of-Thought prompting, motivating the need for a new approach.

**Method:** MeRF enhances reinforcement learning of LLMs by incorporating reward specifications directly into prompts, serving as an in-context motivation.

**Key Contributions:**

	1. Introduction of MeRF as a novel method combining reinforcement learning with in-context learning for LLMs.
	2. Demonstration of significant performance gains on reasoning tasks compared to existing methods.
	3. Insights on the importance of consistency between in-context motivations and reward functions.

**Result:** Empirical evaluations show that MeRF significantly outperforms existing baselines on the Knights and Knaves logic puzzle, demonstrating effective alignment between in-context motivation and external rewards.

**Limitations:** 

**Conclusion:** MeRF incentivizes models to generate outputs that align with optimization objectives through both intrinsic motivation and external rewards, while showing adaptability to misleading motivations.

**Abstract:** Reinforcement Learning with Verifiable Rewards (RLVR) has emerged as a powerful learn-to-reason paradigm for Large Language Models (LLMs) to tackle complex reasoning tasks. However, existing RLVR methods overlook one of the most distinctive capabilities of LLMs, their in-context learning ability, as prominently demonstrated by the success of Chain-of-Thought (CoT) prompting. This motivates us to explore how reinforcement learning can be effectively combined with in-context learning to better improve the reasoning capabilities of LLMs. In this paper, we introduce Motivation-enhanced Reinforcement Finetuning} (MeRF), an intuitive yet effective method enhancing reinforcement learning of LLMs by involving ``telling LLMs the rules of the game''. Specifically, MeRF directly injects the reward specification into the prompt, which serves as an in-context motivation for model to improve its responses with awareness of the optimization objective. This simple modification leverages the in-context learning ability of LLMs aligning generation with optimization, thereby incentivizing the model to generate desired outputs from both inner motivation and external reward. Empirical evaluations on the Knights and Knaves~(K&K) logic puzzle reasoning benchmark demonstrate that \texttt{MeRF} achieves substantial performance gains over baselines. Moreover, ablation studies show that performance improves with greater consistency between the in-context motivation and the external reward function, while the model also demonstrates an ability to adapt to misleading motivations through reinforcement learning.

</details>


### [92] [Comparative Evaluation of ChatGPT and DeepSeek Across Key NLP Tasks: Strengths, Weaknesses, and Domain-Specific Performance](https://arxiv.org/abs/2506.18501)

*Wael Etaiwi, Bushra Alhijawi*

**Main category:** cs.CL

**Keywords:** large language models, natural language processing, evaluation, ChatGPT, DeepSeek

**Relevance Score:** 9

**TL;DR:** This study evaluates the effectiveness of large language models ChatGPT and DeepSeek across five key NLP tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the strengths, weaknesses, and domain-specific abilities of LLMs like ChatGPT and DeepSeek as they are utilized in various NLP applications.

**Method:** A structured experimental protocol was implemented to evaluate both models on five NLP tasks using identical prompts and two benchmark datasets per task, ensuring fairness and minimizing variability.

**Key Contributions:**

	1. Comprehensive evaluation of LLMs across five NLP tasks
	2. Identification of strengths in classification and reasoning for DeepSeek
	3. Presentation of performance differences in nuanced tasks between the two models

**Result:** DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility.

**Limitations:** 

**Conclusion:** The findings offer insights for selecting the appropriate LLM based on specific task requirements.

**Abstract:** The increasing use of large language models (LLMs) in natural language processing (NLP) tasks has sparked significant interest in evaluating their effectiveness across diverse applications. While models like ChatGPT and DeepSeek have shown strong results in many NLP domains, a comprehensive evaluation is needed to understand their strengths, weaknesses, and domain-specific abilities. This is critical as these models are applied to various tasks, from sentiment analysis to more nuanced tasks like textual entailment and translation. This study aims to evaluate ChatGPT and DeepSeek across five key NLP tasks: sentiment analysis, topic classification, text summarization, machine translation, and textual entailment. A structured experimental protocol is used to ensure fairness and minimize variability. Both models are tested with identical, neutral prompts and evaluated on two benchmark datasets per task, covering domains like news, reviews, and formal/informal texts. The results show that DeepSeek excels in classification stability and logical reasoning, while ChatGPT performs better in tasks requiring nuanced understanding and flexibility. These findings provide valuable insights for selecting the appropriate LLM based on task requirements.

</details>


### [93] [End-to-End Spoken Grammatical Error Correction](https://arxiv.org/abs/2506.18532)

*Mengjie Qian, Rao Ma, Stefano BannÃ², Mark J. F. Gales, Kate M. Knill*

**Main category:** cs.CL

**Keywords:** Grammatical Error Correction, Spoken Feedback, End-to-End Framework

**Relevance Score:** 6

**TL;DR:** This paper proposes an End-to-End (E2E) framework for spoken Grammatical Error Correction (SGEC) which addresses challenges like disfluencies and errors in Automatic Speech Recognition. It utilizes a novel pseudo-labeling approach and feedback mechanisms to enhance system accuracy.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in providing spoken feedback for second language learners due to limitations in existing spoken GEC systems.

**Method:** An End-to-End framework using a cascaded architecture based on the Whisper model, enhanced with automatic pseudo-labeling, contextual information, and a novel reference alignment process for feedback generation.

**Key Contributions:**

	1. Proposed an End-to-End framework for SGEC leveraging the Whisper model.
	2. Developed an automatic pseudo-labeling framework to increase labeled spoken data for training.
	3. Introduced a novel reference alignment process to enhance feedback generation.

**Result:** Experiments demonstrate significant improvements in SGEC performance on both in-house and public datasets, increasing training data availability and enhancing feedback precision.

**Limitations:** The scarcity of labeled spoken data remains a challenge, even with proposed solutions.

**Conclusion:** The proposed E2E SGEC framework offers a robust solution to existing limitations, providing effective feedback for L2 learners by leveraging additional contextual information and improved data labeling techniques.

**Abstract:** Grammatical Error Correction (GEC) and feedback play a vital role in supporting second language (L2) learners, educators, and examiners. While written GEC is well-established, spoken GEC (SGEC), aiming to provide feedback based on learners' speech, poses additional challenges due to disfluencies, transcription errors, and the lack of structured input. SGEC systems typically follow a cascaded pipeline consisting of Automatic Speech Recognition (ASR), disfluency detection, and GEC, making them vulnerable to error propagation across modules. This work examines an End-to-End (E2E) framework for SGEC and feedback generation, highlighting challenges and possible solutions when developing these systems. Cascaded, partial-cascaded and E2E architectures are compared, all built on the Whisper foundation model. A challenge for E2E systems is the scarcity of GEC labeled spoken data. To address this, an automatic pseudo-labeling framework is examined, increasing the training data from 77 to over 2500 hours. To improve the accuracy of the SGEC system, additional contextual information, exploiting the ASR output, is investigated. Candidate feedback of their mistakes is an essential step to improving performance. In E2E systems the SGEC output must be compared with an estimate of the fluent transcription to obtain the feedback. To improve the precision of this feedback, a novel reference alignment process is proposed that aims to remove hypothesised edits that results from fluent transcription errors. Finally, these approaches are combined with an edit confidence estimation approach, to exclude low-confidence edits. Experiments on the in-house Linguaskill (LNG) corpora and the publicly available Speak & Improve (S&I) corpus show that the proposed approaches significantly boost E2E SGEC performance.

</details>


### [94] [When Fine-Tuning Fails: Lessons from MS MARCO Passage Ranking](https://arxiv.org/abs/2506.18535)

*Manu Pande, Shahil Kumar, Anay Yatin Damle*

**Main category:** cs.CL

**Keywords:** fine-tuning, transformer models, MS MARCO, machine learning, transfer learning

**Relevance Score:** 6

**TL;DR:** Fine-tuning pre-trained transformer models can degrade performance on the MS MARCO passage ranking task, contrary to common expectations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the performance degradation observed in fine-tuned pre-trained transformer models on a key benchmarking task.

**Method:** Comprehensive experiments with five model variants, including full parameter fine-tuning and LoRA adaptations, analyzed their performance on the MS MARCO passage ranking task, supported by UMAP visualizations and training dynamics analysis.

**Key Contributions:**

	1. Demonstrated performance degradation in fine-tuning transformer models.
	2. Provided empirical results challenging conventional transfer learning wisdom.
	3. Highlighted the need for architectural innovations in model design.

**Result:** All fine-tuning approaches underperformed compared to the base sentence-transformers/all-MiniLM-L6-v2 model with a MRR@10 of 0.3026.

**Limitations:** 

**Conclusion:** Fine-tuning disrupts the optimal embedding space structure learned during extensive pre-training, suggesting that architectural innovations might be necessary to improve performance.

**Abstract:** This paper investigates the counterintuitive phenomenon where fine-tuning pre-trained transformer models degrades performance on the MS MARCO passage ranking task. Through comprehensive experiments involving five model variants-including full parameter fine-tuning and parameter efficient LoRA adaptations-we demonstrate that all fine-tuning approaches underperform the base sentence-transformers/all- MiniLM-L6-v2 model (MRR@10: 0.3026). Our analysis reveals that fine-tuning disrupts the optimal embedding space structure learned during the base model's extensive pre-training on 1 billion sentence pairs, including 9.1 million MS MARCO samples. UMAP visualizations show progressive embedding space flattening, while training dynamics analysis and computational efficiency metrics further support our findings. These results challenge conventional wisdom about transfer learning effectiveness on saturated benchmarks and suggest architectural innovations may be necessary for meaningful improvements.

</details>


### [95] [A Modular Taxonomy for Hate Speech Definitions and Its Impact on Zero-Shot LLM Classification Performance](https://arxiv.org/abs/2506.18576)

*Matteo Melis, Gabriella Lapesa, Dennis Assenmacher*

**Main category:** cs.CL

**Keywords:** hate speech, NLP, taxonomy, LLM, model performance

**Relevance Score:** 8

**TL;DR:** This paper investigates how different definitions of hate speech affect the performance of language models in NLP applications for social good.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to clarify the ambiguity surrounding hate speech definitions and their impact on the detection of harmful content in NLP.

**Method:** The authors analyze existing definitions of hate speech, organizing them into a taxonomy of 14 conceptual elements. They conduct a systematic zero-shot evaluation of three large language models using three different hate speech datasets.

**Key Contributions:**

	1. Creation of a taxonomy of hate speech definitions
	2. Systematic evaluation of LLM performance based on varying definitions
	3. Demonstration of inconsistent model performance based on definition specificity

**Result:** The study finds that varying definitions of hate speech influence model performance differently, but the effects are inconsistent across different model architectures.

**Limitations:** The study may not account for all possible definitions of hate speech and their contextual relevance.

**Conclusion:** This work highlights the importance of precise definitions in hate speech detection and offers a structured approach to evaluate their impact on model outcomes.

**Abstract:** Detecting harmful content is a crucial task in the landscape of NLP applications for Social Good, with hate speech being one of its most dangerous forms. But what do we mean by hate speech, how can we define it, and how does prompting different definitions of hate speech affect model performance? The contribution of this work is twofold. At the theoretical level, we address the ambiguity surrounding hate speech by collecting and analyzing existing definitions from the literature. We organize these definitions into a taxonomy of 14 Conceptual Elements-building blocks that capture different aspects of hate speech definitions, such as references to the target of hate (individual or groups) or of the potential consequences of it. At the experimental level, we employ the collection of definitions in a systematic zero-shot evaluation of three LLMs, on three hate speech datasets representing different types of data (synthetic, human-in-the-loop, and real-world). We find that choosing different definitions, i.e., definitions with a different degree of specificity in terms of encoded elements, impacts model performance, but this effect is not consistent across all architectures.

</details>


### [96] [Parallel Continuous Chain-of-Thought with Jacobi Iteration](https://arxiv.org/abs/2506.18582)

*Haoyi Wu, Zhihao Teng, Kewei Tu*

**Main category:** cs.CL

**Keywords:** Parallel Continuous Chain-of-Thought, Latent Thought Tokens, Machine Learning

**Relevance Score:** 7

**TL;DR:** Proposes a method called Parallel Continuous Chain-of-Thought (PCCoT) to enhance the efficiency of reasoning in language models by training latent thought tokens in parallel instead of sequentially.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the inefficiencies in training large language models caused by sequential dependencies in continuous chain-of-thought reasoning.

**Method:** The proposed PCCoT employs Jacobi iteration to update latent thought tokens iteratively in parallel, improving training and inference efficiency over traditional methods.

**Key Contributions:**

	1. Introduction of Parallel Continuous Chain-of-Thought (PCCoT) method
	2. Demonstrated significant reduction in training and inference time
	3. Improved stability and robustness in model training

**Result:** Experiments show that PCCoT can achieve comparable or better performance while significantly reducing training and inference time by nearly 50%.

**Limitations:** 

**Conclusion:** PCCoT offers improved stability and robustness in the training process of continuous chain-of-thought models, making it a viable alternative for enhancing model efficiency.

**Abstract:** Continuous chain-of-thought has been shown to be effective in saving reasoning tokens for large language models. By reasoning with continuous latent thought tokens, continuous CoT is able to perform implicit reasoning in a compact manner. However, the sequential dependencies between latent thought tokens spoil parallel training, leading to long training time. In this paper, we propose Parallel Continuous Chain-of-Thought (PCCoT), which performs Jacobi iteration on the latent thought tokens, updating them iteratively in parallel instead of sequentially and thus improving both training and inference efficiency of continuous CoT. Experiments demonstrate that by choosing the proper number of iterations, we are able to achieve comparable or even better performance while saving nearly 50% of the training and inference time. Moreover, PCCoT shows better stability and robustness in the training process. Our code is available at https://github.com/whyNLP/PCCoT.

</details>


### [97] [Reply to "Emergent LLM behaviors are observationally equivalent to data leakage"](https://arxiv.org/abs/2506.18600)

*Ariel Flint Ashery, Luca Maria Aiello, Andrea Baronchelli*

**Main category:** cs.CL

**Keywords:** large language models, data contamination, emergent dynamics, self-organisation, social conventions

**Relevance Score:** 8

**TL;DR:** The paper discusses data contamination in the study of large language model populations and clarifies how emergent dynamics can still be investigated despite such concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address data contamination concerns in simulations of large language models and clarify the possibility of studying emergent dynamics.

**Method:** The paper analyzes critiques in existing literature, particularly focusing on the impact of training data on LLM outcomes while also examining empirical observations of emergent dynamics.

**Key Contributions:**

	1. Clarification of the impact of data contamination on multi-agent models
	2. Demonstration of emergent dynamics in LLM populations through empirical evidence
	3. Discussion of self-organization in social conventions among LLMs

**Result:** The authors demonstrate that self-organization and emergent dynamics can indeed be observed in LLM populations, using social conventions as a specific example.

**Limitations:** 

**Conclusion:** The study reinforces the viability of investigating emergent dynamics in LLM populations, despite criticisms regarding data contamination.

**Abstract:** A potential concern when simulating populations of large language models (LLMs) is data contamination, i.e. the possibility that training data may shape outcomes in unintended ways. While this concern is important and may hinder certain experiments with multi-agent models, it does not preclude the study of genuinely emergent dynamics in LLM populations. The recent critique by Barrie and T\"ornberg [1] of the results of Flint Ashery et al. [2] offers an opportunity to clarify that self-organisation and model-dependent emergent dynamics can be studied in LLM populations, highlighting how such dynamics have been empirically observed in the specific case of social conventions.

</details>


### [98] [Semantic similarity estimation for domain specific data using BERT and other techniques](https://arxiv.org/abs/2506.18602)

*R. Prashanth*

**Main category:** cs.CL

**Keywords:** semantic similarity, natural language processing, BERT, question answering, domain specific

**Relevance Score:** 7

**TL;DR:** This paper analyzes semantic similarity estimation using state-of-the-art techniques, highlighting the effectiveness of BERT on domain-specific datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the estimation of semantic similarity in natural language processing and its applications in various tasks such as question answering and machine translation.

**Method:** The study utilizes different models including Universal Sentence Encoder, InferSent, and BERT, evaluated on two question pairs datasets: an in-house domain-specific dataset and the Quora question pairs dataset.

**Key Contributions:**

	1. Demonstration of BERT's superior performance in semantic similarity tasks.
	2. Analysis based on both in-house and public datasets.
	3. Insights into the impact of fine-tuning on model performance.

**Result:** The BERT model outperformed other methods, particularly in the context of domain-specific data, due to its fine-tuning capabilities.

**Limitations:** 

**Conclusion:** BERT is established as the most effective model for semantic similarity estimation on domain-specific datasets based on the results of this analysis.

**Abstract:** Estimation of semantic similarity is an important research problem both in natural language processing and the natural language understanding, and that has tremendous application on various downstream tasks such as question answering, semantic search, information retrieval, document clustering, word-sense disambiguation and machine translation. In this work, we carry out the estimation of semantic similarity using different state-of-the-art techniques including the USE (Universal Sentence Encoder), InferSent and the most recent BERT, or Bidirectional Encoder Representations from Transformers, models. We use two question pairs datasets for the analysis, one is a domain specific in-house dataset and the other is a public dataset which is the Quora's question pairs dataset. We observe that the BERT model gave much superior performance as compared to the other methods. This should be because of the fine-tuning procedure that is involved in its training process, allowing it to learn patterns based on the training data that is used. This works demonstrates the applicability of BERT on domain specific datasets. We infer from the analysis that BERT is the best technique to use in the case of domain specific data.

</details>


### [99] [The Anatomy of Speech Persuasion: Linguistic Shifts in LLM-Modified Speeches](https://arxiv.org/abs/2506.18621)

*Alisa Barkar, Mathieu Chollet, Matthieu Labeau, Beatrice Biancardi, Chloe Clavel*

**Main category:** cs.CL

**Keywords:** large language models, persuasiveness, public speaking, GPT-4o, rhetorical devices

**Relevance Score:** 4

**TL;DR:** This study analyzes how large language models, specifically GPT-4o, understand and modify persuasiveness in public speaking through the use of an enhanced feature set.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how large language models approach the concept of persuasiveness in public speaking.

**Method:** The study modifies speech transcripts from a public speaking competition, employing a novel methodology and integrating textual features like rhetorical devices and discourse markers. GPT-4o is prompted to alter persuasiveness in generated speeches.

**Key Contributions:**

	1. A novel methodology for analyzing persuasiveness in speech transcripts.
	2. An interpretable textual feature set that incorporates rhetorical devices and discourse markers.
	3. Insights into the limitations of GPT-4o in replicating human-like persuasiveness.

**Result:** The analysis reveals that GPT-4o systematically applies stylistic changes without optimizing persuasiveness in a human-like way, manipulating emotional language and syntactic structures to enhance rhetorical impact.

**Limitations:** The model's understanding of persuasiveness is not fully aligned with human approaches, suggesting a gap in its application to real-world public speaking.

**Conclusion:** While GPT-4o can modify speech for stylistic effects, its approach to persuasiveness does not fully mimic human strategies, indicating limitations in its understanding of rhetorical nuances.

**Abstract:** This study examines how large language models understand the concept of persuasiveness in public speaking by modifying speech transcripts from PhD candidates in the "Ma These en 180 Secondes" competition, using the 3MT French dataset. Our contributions include a novel methodology and an interpretable textual feature set integrating rhetorical devices and discourse markers. We prompt GPT-4o to enhance or diminish persuasiveness and analyze linguistic shifts between original and generated speech in terms of the new features. Results indicate that GPT-4o applies systematic stylistic modifications rather than optimizing persuasiveness in a human-like manner. Notably, it manipulates emotional lexicon and syntactic structures (such as interrogative and exclamatory clauses) to amplify rhetorical impact.

</details>


### [100] [ByteSpan: Information-Driven Subword Tokenisation](https://arxiv.org/abs/2506.18639)

*ZÃ©bulon Goriely, Suchir Salhan, Pietro Lesci, Julius Cheng, Paula Buttery*

**Main category:** cs.CL

**Keywords:** subword tokenization, byte-level language model, ByteSpan, morphological alignment, multilingual efficiency

**Relevance Score:** 7

**TL;DR:** This paper presents ByteSpan, a new information-driven subword tokeniser that groups predictable byte sequences into subwords, outperforming BPE in morphological alignment and efficiency across multiple languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve subword tokenisation methods by exploring the grouping of predictable bytes instead of pooling their representations, inspired by computational models of word segmentation.

**Method:** ByteSpan uses an external byte-level language model during training to find and group contiguous predictable byte sequences into subwords.

**Key Contributions:**

	1. Introduction of ByteSpan as a new subword tokeniser.
	2. Demonstration of improved morphological alignment over BPE.
	3. Multilingual efficiency across 25 languages.

**Result:** ByteSpan results in more efficient vocabularies with higher morphological alignment scores compared to Byte Pair Encoding (BPE) for English, and shows comparable compression and Renyi efficiency across 25 languages.

**Limitations:** 

**Conclusion:** The proposed ByteSpan tokeniser offers a novel approach to subword tokenisation that enhances morphological alignment and efficiency across languages.

**Abstract:** Recent dynamic tokenisation methods operate directly on bytes and pool their latent representations into patches. This bears similarities to computational models of word segmentation that determine lexical boundaries using spikes in an autoregressive model's prediction error. Inspired by this connection, we explore whether grouping predictable bytes - rather than pooling their representations - can yield a useful fixed subword vocabulary. We propose a new information-driven subword tokeniser, ByteSpan, that uses an external byte-level LM during training to identify contiguous predictable byte sequences and group them into subwords. Experiments show that ByteSpan yields efficient vocabularies with higher morphological alignment scores than BPE for English. Multilingual experiments show similar compression and R\'enyi efficiency for 25 languages.

</details>


### [101] [Is There a Case for Conversation Optimized Tokenizers in Large Language Models?](https://arxiv.org/abs/2506.18674)

*Raquel Ferrando, Javier Conde, Gonzalo MartÃ­nez, Pedro Reviriego*

**Main category:** cs.CL

**Keywords:** Large Language Models, tokenizers, chatbots, energy efficiency, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper explores the optimization of tokenizers specifically for chatbot conversations, revealing that conversation-optimized tokenizers reduce token counts and improve energy efficiency.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing computational and energy costs of Large Language Models (LLMs) necessitate optimizations, particularly in tokenization for chatbot applications, which differ from training corpora.

**Method:** The study evaluates various tokenizers using a publicly available corpus of chatbot conversations to redesign vocabularies and assess performance outcomes.

**Key Contributions:**

	1. Demonstration of performance improvement in tokenizers specifically for chatbot contexts.
	2. Quantification of energy savings achieved through optimized tokenization in real conversations.
	3. Insights on the differences in tokenization needs between training corpora and user dialogue.

**Result:** Conversation-optimized tokenizers reduce the number of tokens in chatbot dialogues by 5% to 10%, leading to potential energy savings.

**Limitations:** The study is limited to the particular chatbot conversation corpus used, and results may vary with different datasets or types of dialogues.

**Conclusion:** Optimizing tokenizers for chatbot interactions not only reduces token usage but does so with minimal impact on the original training corpus's efficiency.

**Abstract:** The computational and energy costs of Large Language Models (LLMs) have increased exponentially driven by the growing model sizes and the massive adoption of LLMs by hundreds of millions of users. The unit cost of an LLM is the computation of a token. Therefore, the tokenizer plays an important role in the efficiency of a model, and they are carefully optimized to minimize the number of tokens for the text in their training corpus. One of the most popular applications of LLMs are chatbots that interact with users. A key observation is that, for those chatbots, what is important is the performance of the tokenizer in the user text input and the chatbot responses. Those are most likely different from the text in the training corpus. So, a question that immediately arises is whether there is a potential benefit in optimizing tokenizers for chatbot conversations. In this paper, this idea is explored for different tokenizers by using a publicly available corpus of chatbot conversations to redesign their vocabularies and evaluate their performance in this domain. The results show that conversation-optimized tokenizers consistently reduce the number of tokens in chatbot dialogues, which can lead to meaningful energy savings, in the range of 5% to 10% while having minimal or even slightly positive impact on tokenization efficiency for the original training corpus.

</details>


### [102] [Context Biasing for Pronunciations-Orthography Mismatch in Automatic Speech Recognition](https://arxiv.org/abs/2506.18703)

*Christian Huber, Alexander Waibel*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, context biasing, user correction

**Relevance Score:** 6

**TL;DR:** This paper presents a novel method to improve automatic speech recognition (ASR) systems' ability to recognize challenging words by allowing users to provide real-time corrections during inference.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing context biasing methods in ASR struggle with pronunciation-orthography mismatches in words not seen during training.

**Method:** The proposed method enables users to correct substitution errors on-the-fly, enhancing recognition accuracy for difficult words.

**Key Contributions:**

	1. Introduction of a user-correction mechanism during inference
	2. Improvement in recognition accuracy for unseen challenging words
	3. Demonstrated effectiveness through quantitative results.

**Result:** The method demonstrates a relative improvement in biased word error rate of up to 11% while maintaining competitive overall recognition performance.

**Limitations:** The method may still face challenges with words that have extreme pronunciation-orthography mismatches.

**Conclusion:** Real-time user corrections can significantly enhance ASR accuracy, particularly for named entities, acronyms, and specialized terminology.

**Abstract:** Neural sequence-to-sequence systems deliver state-of-the-art performance for automatic speech recognition. When using appropriate modeling units, e.g., byte-pair encoded characters, these systems are in principal open vocabulary systems. In practice, however, they often fail to recognize words not seen during training, e.g., named entities, acronyms, or domain-specific special words. To address this problem, many context biasing methods have been proposed; however, for words with a pronunciation-orthography mismatch, these methods may still struggle. We propose a method which allows corrections of substitution errors to improve the recognition accuracy of such challenging words. Users can add corrections on the fly during inference. We show that with this method we get a relative improvement in biased word error rate of up to 11\%, while maintaining a competitive overall word error rate.

</details>


### [103] [Benchmarking the Pedagogical Knowledge of Large Language Models](https://arxiv.org/abs/2506.18710)

*Maxime LeliÃ¨vre, Amy Waldock, Meng Liu, Natalia ValdÃ©s Aspillaga, Alasdair Mackintosh, MarÃ­a JosÃ© Ogando Portelo, Jared Lee, Paul Atherton, Robin A. A. Ince, Oliver G. B. Garrod*

**Main category:** cs.CL

**Keywords:** pedagogy, language models, education, benchmarking, machine learning

**Relevance Score:** 6

**TL;DR:** This paper introduces The Pedagogy Benchmark, a dataset for evaluating large language models' pedagogical knowledge and its applications in education.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to fill the gap in evaluating AI models' understanding of pedagogy, beyond just content knowledge.

**Method:** The benchmark is developed using curated questions from teacher professional development exams across various pedagogical domains.

**Key Contributions:**

	1. Introduction of The Pedagogy Benchmark dataset
	2. Evaluation of 97 models' pedagogical knowledge
	3. Online leaderboards for interactive exploration of model performance

**Result:** Results for 97 models showed accuracies ranging from 28% to 89% on the pedagogical knowledge questions, with considerations on cost and accuracy included.

**Limitations:** 

**Conclusion:** Education-focused benchmarks are essential for measuring models' abilities to engage with pedagogical concepts and support effective teaching practices, addressing important educational challenges.

**Abstract:** Benchmarks like Massive Multitask Language Understanding (MMLU) have played a pivotal role in evaluating AI's knowledge and abilities across diverse domains. However, existing benchmarks predominantly focus on content knowledge, leaving a critical gap in assessing models' understanding of pedagogy - the method and practice of teaching. This paper introduces The Pedagogy Benchmark, a novel dataset designed to evaluate large language models on their Cross-Domain Pedagogical Knowledge (CDPK) and Special Education Needs and Disability (SEND) pedagogical knowledge. These benchmarks are built on a carefully curated set of questions sourced from professional development exams for teachers, which cover a range of pedagogical subdomains such as teaching strategies and assessment methods. Here we outline the methodology and development of these benchmarks. We report results for 97 models, with accuracies spanning a range from 28% to 89% on the pedagogical knowledge questions. We consider the relationship between cost and accuracy and chart the progression of the Pareto value frontier over time. We provide online leaderboards at https://rebrand.ly/pedagogy which are updated with new models and allow interactive exploration and filtering based on various model properties, such as cost per token and open-vs-closed weights, as well as looking at performance in different subjects. LLMs and generative AI have tremendous potential to influence education and help to address the global learning crisis. Education-focused benchmarks are crucial to measure models' capacities to understand pedagogical concepts, respond appropriately to learners' needs, and support effective teaching practices across diverse contexts. They are needed for informing the responsible and evidence-based deployment of LLMs and LLM-based tools in educational settings, and for guiding both development and policy decisions.

</details>


### [104] [Semantic-Preserving Adversarial Attacks on LLMs: An Adaptive Greedy Binary Search Approach](https://arxiv.org/abs/2506.18756)

*Chong Zhang, Xiang Li, Jia Wang, Shan Liang, Haochen Xue, Xiaobo Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, prompt engineering, adaptive optimization, semantic stability, adversarial samples

**Relevance Score:** 9

**TL;DR:** This paper presents the Adaptive Greedy Binary Search (AGBS) method to optimize prompts for Large Language Models (LLMs) while maintaining semantic stability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses issues of user input misinterpretation caused by automated prompt engineering in GUIs, leading to erroneous outputs from LLMs.

**Method:** The AGBS method simulates common prompt optimization mechanisms by dynamically evaluating their impact on LLM performance, allowing for robust adversarial sample generation.

**Key Contributions:**

	1. Introduction of the AGBS method for prompt optimization in LLMs
	2. Demonstration of AGBS's effectiveness on open and closed-source LLMs
	3. Providing actionable insights for improving prompt optimization systems

**Result:** Experimental results show that AGBS effectively balances semantic consistency and attack efficacy across various LLMs.

**Limitations:** The study may not cover all potential user requirements or contexts in which prompts may be optimized.

**Conclusion:** The insights gained from the experiments can guide the design of more reliable systems for prompt optimization.

**Abstract:** Large Language Models (LLMs) increasingly rely on automatic prompt engineering in graphical user interfaces (GUIs) to refine user inputs and enhance response accuracy. However, the diversity of user requirements often leads to unintended misinterpretations, where automated optimizations distort original intentions and produce erroneous outputs. To address this challenge, we propose the Adaptive Greedy Binary Search (AGBS) method, which simulates common prompt optimization mechanisms while preserving semantic stability. Our approach dynamically evaluates the impact of such strategies on LLM performance, enabling robust adversarial sample generation. Through extensive experiments on open and closed-source LLMs, we demonstrate AGBS's effectiveness in balancing semantic consistency and attack efficacy. Our findings offer actionable insights for designing more reliable prompt optimization systems. Code is available at: https://github.com/franz-chang/DOBS

</details>


### [105] [ASP2LJ : An Adversarial Self-Play Laywer Augmented Legal Judgment Framework](https://arxiv.org/abs/2506.18768)

*Ao Chang, Tong Zhou, Yubo Chen, Delai Qiu, Shengping Liu, Kang Liu, Jun Zhao*

**Main category:** cs.CL

**Keywords:** Legal Judgment Prediction, Adversarial Self-Play, RareCases Dataset

**Relevance Score:** 5

**TL;DR:** This paper introduces ASP2LJ, a framework that enhances legal judgment prediction by addressing long-tailed data distributions and improving lawyers' argumentation via adversarial self-play.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Legal Judgment Prediction (LJP) is crucial for predicting judicial outcomes but faces challenges related to long-tailed data distributions and insufficient emphasis on lawyers' contributions.

**Method:** The proposed ASP2LJ framework includes a case generation module for long-tailed data and an adversarial self-play mechanism to improve lawyer arguments.

**Key Contributions:**

	1. Integrated framework for legal judgment prediction
	2. Introduction of RareCases dataset with 120 rare legal cases
	3. Public release of datasets and code for future research

**Result:** The framework was tested on the SimuCourt and RareCases datasets, showing significant improvements in prediction quality and judicial decision-making.

**Limitations:** 

**Conclusion:** ASP2LJ effectively integrates case generation with lawyer argument enhancement, improving judicial outcomes and fairness in legal judgments.

**Abstract:** Legal Judgment Prediction (LJP) aims to predict judicial outcomes, including relevant legal charge, terms, and fines, which is a crucial process in Large Language Model(LLM). However, LJP faces two key challenges: (1)Long Tail Distribution: Current datasets, derived from authentic cases, suffer from high human annotation costs and imbalanced distributions, leading to model performance degradation. (2)Lawyer's Improvement: Existing systems focus on enhancing judges' decision-making but neglect the critical role of lawyers in refining arguments, which limits overall judicial accuracy. To address these issues, we propose an Adversarial Self-Play Lawyer Augmented Legal Judgment Framework, called ASP2LJ, which integrates a case generation module to tackle long-tailed data distributions and an adversarial self-play mechanism to enhance lawyers' argumentation skills. Our framework enables a judge to reference evolved lawyers' arguments, improving the objectivity, fairness, and rationality of judicial decisions. Besides, We also introduce RareCases, a dataset for rare legal cases in China, which contains 120 tail-end cases. We demonstrate the effectiveness of our approach on the SimuCourt dataset and our RareCases dataset. Experimental results show our framework brings improvements, indicating its utilization. Our contributions include an integrated framework, a rare-case dataset, and publicly releasing datasets and code to support further research in automated judicial systems.

</details>


### [106] [Existing LLMs Are Not Self-Consistent For Simple Tasks](https://arxiv.org/abs/2506.18781)

*Zhenru Lin, Jiawen Tao, Yang Yuan, Andrew Chi-Chih Yao*

**Main category:** cs.CL

**Keywords:** Large Language Models, self-consistency, inconsistency metrics, AI reliability, automated methods

**Relevance Score:** 9

**TL;DR:** This study examines self-consistency in Large Language Models (LLMs) and introduces metrics and methods to mitigate inconsistencies in their reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As LLMs become more powerful, it is essential to ensure their decisions are transparent and trustworthy, which requires addressing inconsistencies in their internal reasoning processes.

**Method:** The study introduces inconsistency metrics and two automated methods for improvement: a graph-based approach and an energy-based approach.

**Key Contributions:**

	1. Introduction of inconsistency metrics for LLMs
	2. Development of graph-based and energy-based methods to enhance self-consistency
	3. Demonstration of self-consistency issues in various LLM architectures

**Result:** The findings indicate that smaller models are highly inconsistent, and even advanced models like DeepSeek-R1 and GPT-o4-mini display notable inconsistencies.

**Limitations:** Improvements from the proposed methods are only partial and highlight the complexity of achieving full self-consistency.

**Conclusion:** Self-consistency is crucial for building reliable and interpretable AI systems, and while the proposed methods provide some improvement, challenges remain.

**Abstract:** Large Language Models (LLMs) have grown increasingly powerful, yet ensuring their decisions remain transparent and trustworthy requires self-consistency -- no contradictions in their internal reasoning. Our study reveals that even on simple tasks, such as comparing points on a line or a plane, or reasoning in a family tree, all smaller models are highly inconsistent, and even state-of-the-art models like DeepSeek-R1 and GPT-o4-mini are not fully self-consistent. To quantify and mitigate these inconsistencies, we introduce inconsistency metrics and propose two automated methods -- a graph-based and an energy-based approach. While these fixes provide partial improvements, they also highlight the complexity and importance of self-consistency in building more reliable and interpretable AI. The code and data are available at https://github.com/scorpio-nova/llm-self-consistency.

</details>


### [107] [RWESummary: A Framework and Test for Choosing Large Language Models to Summarize Real-World Evidence (RWE) Studies](https://arxiv.org/abs/2506.18819)

*Arjun Mukerji, Michael L. Jackson, Jason Jones, Neil Sanghavi*

**Main category:** cs.CL

**Keywords:** Large Language Models, Real-World Evidence, Medical Summarization, Benchmarking, RWESummary

**Relevance Score:** 9

**TL;DR:** RWESummary is a proposed benchmarking tool for Large Language Models to summarize real-world evidence from structured RWE studies, revealing that Gemini 2.5 models outperform others.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate and benchmark LLMs for summarizing real-world evidence from medical research studies, an area that has not been thoroughly investigated.

**Method:** Introducing RWESummary as part of the MedHELM framework, covering one scenario and three evaluations targeting summarization errors in medical research studies.

**Key Contributions:**

	1. Introduction of RWESummary for benchmarking LLMs in summarizing real-world evidence.
	2. Identification of major summarization error types in medical research.
	3. Empirical results showing the performance of Gemini 2.5 models compared to others.

**Result:** Gemini 2.5 models demonstrated the best performance in summarizing data from 13 distinct RWE studies.

**Limitations:** 

**Conclusion:** RWESummary establishes a novel benchmark for evaluating LLM performance in summarizing real-world evidence, suggesting significant improvements in this domain.

**Abstract:** Large Language Models (LLMs) have been extensively evaluated for general summarization tasks as well as medical research assistance, but they have not been specifically evaluated for the task of summarizing real-world evidence (RWE) from structured output of RWE studies. We introduce RWESummary, a proposed addition to the MedHELM framework (Bedi, Cui, Fuentes, Unell et al., 2025) to enable benchmarking of LLMs for this task. RWESummary includes one scenario and three evaluations covering major types of errors observed in summarization of medical research studies and was developed using Atropos Health proprietary data. Additionally, we use RWESummary to compare the performance of different LLMs in our internal RWE summarization tool. At the time of publication, with 13 distinct RWE studies, we found the Gemini 2.5 models performed best overall (both Flash and Pro). We suggest RWESummary as a novel and useful foundation model benchmark for real-world evidence study summarization.

</details>


### [108] [MLLP-VRAIN UPV system for the IWSLT 2025 Simultaneous Speech Translation Translation task](https://arxiv.org/abs/2506.18828)

*Jorge Iranzo-SÃ¡nchez, Javier Iranzo-SÃ¡nchez, AdriÃ  GimÃ©nez, Jorge Civera, Alfons Juan*

**Main category:** cs.CL

**Keywords:** simultaneous translation, speech translation, machine learning

**Relevance Score:** 8

**TL;DR:** This paper presents a modular cascade system for real-time simultaneous speech translation, combining Whisper Large-V3-Turbo for ASR and NLLB-3.3B for MT, with innovative adaptations and lightweight techniques for effective long-form content translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges in real-time translation of long-form speech within the IWSLT 2025 Simultaneous Speech Translation track, aiming for improved translation quality with low latency.

**Method:** Developed a modular cascade system that integrates pre-trained models for ASR and MT, with techniques like document-level adaptation, prefix training, specialized buffer management, and segmentation strategies.

**Key Contributions:**

	1. Modular cascade system for real-time translation
	2. Lightweight adaptation techniques for existing models
	3. Effective handling of long-form audio sequences with minimal data requirements.

**Result:** Achieved a BLEU score of 31.96 and a latency of 2.94 seconds on a dataset, with a final model score of 29.8 BLEU on the official test set, demonstrating effectiveness without extensive in-domain data.

**Limitations:** 

**Conclusion:** The study shows that utilizing adapted pre-trained components allows for efficient simultaneous translation of long-form content, overcoming the need for new end-to-end training.

**Abstract:** This work describes the participation of the MLLP-VRAIN research group in the shared task of the IWSLT 2025 Simultaneous Speech Translation track. Our submission addresses the unique challenges of real-time translation of long-form speech by developing a modular cascade system that adapts strong pre-trained models to streaming scenarios. We combine Whisper Large-V3-Turbo for ASR with the multilingual NLLB-3.3B model for MT, implementing lightweight adaptation techniques rather than training new end-to-end models from scratch. Our approach employs document-level adaptation with prefix training to enhance the MT model's ability to handle incomplete inputs, while incorporating adaptive emission policies including a wait-$k$ strategy and RALCP for managing the translation stream. Specialized buffer management techniques and segmentation strategies ensure coherent translations across long audio sequences. Experimental results on the ACL60/60 dataset demonstrate that our system achieves a favorable balance between translation quality and latency, with a BLEU score of 31.96 and non-computational-aware StreamLAAL latency of 2.94 seconds. Our final model achieves a preliminary score on the official test set (IWSLT25Instruct) of 29.8 BLEU. Our work demonstrates that carefully adapted pre-trained components can create effective simultaneous translation systems for long-form content without requiring extensive in-domain parallel data or specialized end-to-end training.

</details>


### [109] [STU-PID: Steering Token Usage via PID Controller for Efficient Large Language Model Reasoning](https://arxiv.org/abs/2506.18831)

*Aryasomayajula Ram Bharadwaj*

**Main category:** cs.CL

**Keywords:** Large Language Models, Chain-of-Thought reasoning, PID controller, Computational efficiency, Reasoning quality

**Relevance Score:** 8

**TL;DR:** STUPID is a novel method using a PID controller to dynamically steer reasoning in Large Language Models, improving accuracy by 6% and reducing token usage by 32%.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the overthinking phenomenon in Large Language Models, which leads to excessive reasoning steps, increased computational costs, and potential performance degradation.

**Method:** A training-free method that employs a PID controller to dynamically modulate activation steering strength during inference, combined with a chunk-level classifier to detect redundant reasoning patterns.

**Key Contributions:**

	1. Introduction of STUPID for dynamic reasoning adjustment in Large Language Models
	2. Utilization of a PID controller for modulation of reasoning patterns
	3. Demonstrated improvement in both accuracy and computational efficiency in experimental evaluations.

**Result:** STUPID improves accuracy by 6% and reduces token usage by 32% on the GSM8K dataset, outperforming static steering approaches.

**Limitations:** 

**Conclusion:** STUPID provides a principled framework for dynamic reasoning calibration, enhancing computational efficiency while maintaining reasoning quality.

**Abstract:** Large Language Models employing extended chain-of-thought (CoT) reasoning often suffer from the overthinking phenomenon, generating excessive and redundant reasoning steps that increase computational costs while potentially degrading performance. While recent work has explored static steering approaches to mitigate this issue, they lack the adaptability to dynamically adjust intervention strength based on real-time reasoning quality. We propose STUPID (Steering Token Usage via PID controller), a novel training-free method that employs a PID controller to dynamically modulate activation steering strength during inference. Our approach combines a chunk-level classifier for detecting redundant reasoning patterns with a PID control mechanism that adaptively adjusts steering intensity based on the predicted redundancy probability. Experimental evaluation on GSM8K demonstrates that STUPID achieves a 6% improvement in accuracy while reducing token usage by 32%, outperforming static steering baselines. Our method provides a principled framework for dynamic reasoning calibration that maintains reasoning quality while significantly improving computational efficiency.

</details>


### [110] [LongWriter-Zero: Mastering Ultra-Long Text Generation via Reinforcement Learning](https://arxiv.org/abs/2506.18841)

*Yuhao Wu, Yushi Bai, Zhiqiang Hu, Roy Ka-Wei Lee, Juanzi Li*

**Main category:** cs.CL

**Keywords:** long-form generation, reinforcement learning, large language models, text generation

**Relevance Score:** 9

**TL;DR:** This paper presents LongWriter-Zero, a model for ultra-long text generation using reinforcement learning, surpassing traditional methods reliant on synthetic data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of ultra-long generation in large language models (LLMs) due to limitations in generation length and quality degradation, especially with synthetic fine-tuning methods.

**Method:** The authors utilize an incentivization-based approach that leverages reinforcement learning (RL) to train a model from scratch, focusing on quality and coherence without synthetic data.

**Key Contributions:**

	1. Introduction of an incentivization-based approach for training LLMs
	2. Demonstration of state-of-the-art performance in long-form writing without reliance on synthetic data
	3. Open-sourcing of data and model checkpoints for community use

**Result:** LongWriter-Zero consistently outperforms traditional supervised fine-tuning methods on long-form writing tasks, achieving state-of-the-art results on various metrics and datasets.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates significant advancements in ultra-long text generation capabilities of LLMs, offering a practical way to enhance writing quality and control without synthetic data.

**Abstract:** Ultra-long generation by large language models (LLMs) is a widely demanded scenario, yet it remains a significant challenge due to their maximum generation length limit and overall quality degradation as sequence length increases. Previous approaches, exemplified by LongWriter, typically rely on ''teaching'', which involves supervised fine-tuning (SFT) on synthetic long-form outputs. However, this strategy heavily depends on synthetic SFT data, which is difficult and costly to construct, often lacks coherence and consistency, and tends to be overly artificial and structurally monotonous. In this work, we propose an incentivization-based approach that, starting entirely from scratch and without relying on any annotated or synthetic data, leverages reinforcement learning (RL) to foster the emergence of ultra-long, high-quality text generation capabilities in LLMs. We perform RL training starting from a base model, similar to R1-Zero, guiding it to engage in reasoning that facilitates planning and refinement during the writing process. To support this, we employ specialized reward models that steer the LLM towards improved length control, writing quality, and structural formatting. Experimental evaluations show that our LongWriter-Zero model, trained from Qwen2.5-32B, consistently outperforms traditional SFT methods on long-form writing tasks, achieving state-of-the-art results across all metrics on WritingBench and Arena-Write, and even surpassing 100B+ models such as DeepSeek R1 and Qwen3-235B. We open-source our data and model checkpoints under https://huggingface.co/THU-KEG/LongWriter-Zero-32B

</details>


### [111] [Mechanistic Interpretability Needs Philosophy](https://arxiv.org/abs/2506.18852)

*Iwan Williams, Ninell Oldenburg, Ruchira Dhar, Joshua Hatherley, Constanza Fierro, Nina Rajcic, Sandrine R. Schiller, Filippos Stamatiou, Anders SÃ¸gaard*

**Main category:** cs.CL

**Keywords:** mechanistic interpretability, philosophy, AI ethics, interdisciplinary dialogue, causal mechanisms

**Relevance Score:** 4

**TL;DR:** This position paper discusses the role of philosophy in mechanistic interpretability (MI) research, emphasizing its importance in clarifying concepts and methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The field of mechanistic interpretability is growing, making it crucial to examine the assumptions and concepts guiding this area of research.

**Method:** The paper analyzes three open problems in the MI literature to demonstrate the potential contributions of philosophy.

**Key Contributions:**

	1. Highlights the need for philosophical engagement in MI research.
	2. Demonstrates how philosophy can clarify MI concepts.
	3. Outlines challenges in MI that philosophy can help address.

**Result:** Philosophy can provide valuable insights into the epistemic and ethical implications of MI research, enhancing interpretability efforts.

**Limitations:** 

**Conclusion:** The authors advocate for ongoing interdisciplinary dialogue between philosophy and MI research to refine methods and concepts.

**Abstract:** Mechanistic interpretability (MI) aims to explain how neural networks work by uncovering their underlying causal mechanisms. As the field grows in influence, it is increasingly important to examine not just models themselves, but the assumptions, concepts and explanatory strategies implicit in MI research. We argue that mechanistic interpretability needs philosophy: not as an afterthought, but as an ongoing partner in clarifying its concepts, refining its methods, and assessing the epistemic and ethical stakes of interpreting AI systems. Taking three open problems from the MI literature as examples, this position paper illustrates the value philosophy can add to MI research, and outlines a path toward deeper interdisciplinary dialogue.

</details>


### [112] [CommVQ: Commutative Vector Quantization for KV Cache Compression](https://arxiv.org/abs/2506.18879)

*Junyan Li, Yang Zhang, Muhammad Yusuf Hassan, Talha Chafekar, Tianle Cai, Zhile Ren, Pengsheng Guo, Foroozan Karimzadeh, Colorado Reed, Chong Wang, Chuang Gan*

**Main category:** cs.CL

**Keywords:** Large Language Models, quantization, memory efficiency, key-value cache, inference

**Relevance Score:** 8

**TL;DR:** This paper introduces Commutative Vector Quantization (CommVQ), a method to reduce memory usage of key-value caches in long-context LLM inference while maintaining high accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the memory bottleneck of key-value (KV) caches in long-context large language models (LLMs) during inference.

**Method:** The authors introduce a lightweight encoder and codebook for additive quantization of the KV cache, which integrates with Rotary Position Embedding (RoPE) for efficient decoding.

**Key Contributions:**

	1. Introduction of Commutative Vector Quantization for KV cache reduction.
	2. Demonstration of 1-bit quantization supporting long contexts on high-performance GPUs.
	3. Efficient integration with Rotary Position Embedding to enhance decoding.

**Result:** Experiments demonstrate that CommVQ reduces the FP16 KV cache size by 87.5% with 2-bit quantization and allows for 1-bit quantization with minimal accuracy loss, enabling extensive context lengths on standard GPUs.

**Limitations:** 

**Conclusion:** CommVQ significantly improves memory efficiency in LLMs without sacrificing accuracy, making it feasible to utilize larger context lengths in practical scenarios.

**Abstract:** Large Language Models (LLMs) are increasingly used in applications requiring long context lengths, but the key-value (KV) cache often becomes a memory bottleneck on GPUs as context grows. To address this, we propose Commutative Vector Quantization (CommVQ) to significantly reduce memory usage for long-context LLM inference. We first introduce additive quantization with a lightweight encoder and codebook to compress the KV cache, which can be decoded via simple matrix multiplication. To further reduce computational costs during decoding, we design the codebook to be commutative with Rotary Position Embedding (RoPE) and train it using an Expectation-Maximization (EM) algorithm. This enables efficient integration of decoding into the self-attention mechanism. Our approach achieves high accuracy with additive quantization and low overhead via the RoPE-commutative codebook. Experiments on long-context benchmarks and GSM8K show that our method reduces FP16 KV cache size by 87.5% with 2-bit quantization, while outperforming state-of-the-art KV cache quantization methods. Notably, it enables 1-bit KV cache quantization with minimal accuracy loss, allowing a LLaMA-3.1 8B model to run with a 128K context length on a single RTX 4090 GPU. The source code is available at: https://github.com/UMass-Embodied-AGI/CommVQ.

</details>


### [113] [OMEGA: Can LLMs Reason Outside the Box in Math? Evaluating Exploratory, Compositional, and Transformative Generalization](https://arxiv.org/abs/2506.18880)

*Yiyou Sun, Shawn Hu, Georgia Zhou, Ken Zheng, Hannaneh Hajishirzi, Nouha Dziri, Dawn Song*

**Main category:** cs.CL

**Keywords:** Large language models, Mathematical reasoning, Out-of-distribution evaluation, Generalization, Creativity

**Relevance Score:** 8

**TL;DR:** A benchmark called OMEGA evaluates LLMs' capabilities in solving out-of-distribution math problems along three generalization axes: exploratory, compositional, and transformative reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically investigate the limitations of LLMs in solving complex mathematical problems and evaluate their out-of-distribution generalization capabilities.

**Method:** OMEGA consists of programmatically generated training-test pairs derived from various mathematical domains, and it assesses LLMs in terms of exploratory, compositional, and transformative reasoning.

**Key Contributions:**

	1. Introduction of the OMEGA benchmark for evaluating mathematical reasoning
	2. Quantification of LLM performance across different reasoning capabilities
	3. Insights into the limitations of existing LLMs in creative problem solving

**Result:** Top-tier LLMs show significant performance drops as problem complexity rises; fine-tuning improves exploratory generalization but fails to enhance compositional and transformative reasoning.

**Limitations:** Focuses primarily on LLMs without addressing potential algorithmic improvements or alternative approaches outside the evaluated models.

**Conclusion:** OMEGA provides insights into the shortcomings of LLMs in mathematical creativity, indicating the need for further advancements in LLM capabilities beyond mere mechanical proficiency.

**Abstract:** Recent large-scale language models (LLMs) with long Chain-of-Thought reasoning-such as DeepSeek-R1-have achieved impressive results on Olympiad-level mathematics benchmarks. However, they often rely on a narrow set of strategies and struggle with problems that require a novel way of thinking. To systematically investigate these limitations, we introduce OMEGA-Out-of-distribution Math Problems Evaluation with 3 Generalization Axes-a controlled yet diverse benchmark designed to evaluate three axes of out-of-distribution generalization, inspired by Boden's typology of creativity: (1) Exploratory-applying known problem solving skills to more complex instances within the same problem domain; (2) Compositional-combining distinct reasoning skills, previously learned in isolation, to solve novel problems that require integrating these skills in new and coherent ways; and (3) Transformative-adopting novel, often unconventional strategies by moving beyond familiar approaches to solve problems more effectively. OMEGA consists of programmatically generated training-test pairs derived from templated problem generators across geometry, number theory, algebra, combinatorics, logic, and puzzles, with solutions verified using symbolic, numerical, or graphical methods. We evaluate frontier (or top-tier) LLMs and observe sharp performance degradation as problem complexity increases. Moreover, we fine-tune the Qwen-series models across all generalization settings and observe notable improvements in exploratory generalization, while compositional generalization remains limited and transformative reasoning shows little to no improvement. By isolating and quantifying these fine-grained failures, OMEGA lays the groundwork for advancing LLMs toward genuine mathematical creativity beyond mechanical proficiency.

</details>


### [114] [ReasonFlux-PRM: Trajectory-Aware PRMs for Long Chain-of-Thought Reasoning in LLMs](https://arxiv.org/abs/2506.18896)

*Jiaru Zou, Ling Yang, Jingwen Gu, Jiahao Qiu, Ke Shen, Jingrui He, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** Process Reward Models, trajectory-response outputs, large language models

**Relevance Score:** 9

**TL;DR:** This paper introduces ReasonFlux-PRM, a novel Process Reward Model designed for more robust evaluation of trajectory-response outputs from large language models, demonstrating significant performance improvements on various benchmarks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of intermediate reasoning steps in large language models, particularly in the context of trajectory-response outputs.

**Method:** The paper presents ReasonFlux-PRM, which utilizes both step-level and trajectory-level supervision for fine-grained reward assignments.

**Key Contributions:**

	1. Introduction of ReasonFlux-PRM for robust evaluation of LLMs
	2. Demonstrated consistent performance improvements across multiple benchmarks
	3. Release of a lightweight model for resource-constrained applications

**Result:** ReasonFlux-PRM-7B selects higher quality model distillation data and improves performance metrics: 12.1% gain in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling on challenging benchmarks.

**Limitations:** 

**Conclusion:** The proposed model outperforms existing PRMs and human-curated baselines, and an efficient version (ReasonFlux-PRM-1.5B) is released for edge deployment.

**Abstract:** Process Reward Models (PRMs) have recently emerged as a powerful framework for supervising intermediate reasoning steps in large language models (LLMs). Previous PRMs are primarily trained on model final output responses and struggle to evaluate intermediate thinking trajectories robustly, especially in the emerging setting of trajectory-response outputs generated by frontier reasoning models like Deepseek-R1. In this work, we introduce ReasonFlux-PRM, a novel trajectory-aware PRM explicitly designed to evaluate the trajectory-response type of reasoning traces. ReasonFlux-PRM incorporates both step-level and trajectory-level supervision, enabling fine-grained reward assignment aligned with structured chain-of-thought data. We adapt ReasonFlux-PRM to support reward supervision under both offline and online settings, including (i) selecting high-quality model distillation data for downstream supervised fine-tuning of smaller models, (ii) providing dense process-level rewards for policy optimization during reinforcement learning, and (iii) enabling reward-guided Best-of-N test-time scaling. Empirical results on challenging downstream benchmarks such as AIME, MATH500, and GPQA-Diamond demonstrate that ReasonFlux-PRM-7B selects higher quality data than strong PRMs (e.g., Qwen2.5-Math-PRM-72B) and human-curated baselines. Furthermore, our derived ReasonFlux-PRM-7B yields consistent performance improvements, achieving average gains of 12.1% in supervised fine-tuning, 4.5% in reinforcement learning, and 6.3% in test-time scaling. We also release our efficient ReasonFlux-PRM-1.5B for resource-constrained applications and edge deployment. Projects: https://github.com/Gen-Verse/ReasonFlux

</details>


### [115] [A Survey on Data Selection for LLM Instruction Tuning](https://arxiv.org/abs/2402.05123)

*Bolin Zhang, Jiahao Wang, Qianlong Du, Jiajun Zhang, Zhiying Tu, Dianhui Chu*

**Main category:** cs.CL

**Keywords:** instruction tuning, large language models, data selection, dataset quality, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper surveys data selection methods for instruction tuning of large language models (LLMs), emphasizing dataset quality over quantity and proposing a new taxonomy for data selection strategies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance instruction tuning of LLMs by focusing on the quality of instruction datasets, which is critical for improving instruction-following capabilities and reducing training costs.

**Method:** The paper reviews existing instruction datasets, introduces a new taxonomy of data selection methods, and discusses recent advances along with evaluation strategies and results.

**Key Contributions:**

	1. Introduces a new taxonomy for data selection methods in instruction tuning
	2. Provides a detailed survey of existing methods and evaluations
	3. Highlights current challenges and future research directions

**Result:** Presents a comprehensive overview of recent developments in data selection for LLM instruction tuning, highlighting the importance of quality dataset selection.

**Limitations:** 

**Conclusion:** Identifies open challenges and suggests new research frontiers in the domain of data selection for instruction tuning of LLMs.

**Abstract:** Instruction tuning is a vital step of training large language models (LLM), so how to enhance the effect of instruction tuning has received increased attention. Existing works indicate that the quality of the dataset is more crucial than the quantity during instruction tuning of LLM. Therefore, recently a lot of studies focus on exploring the methods of selecting high-quality subset from instruction datasets, aiming to reduce training costs and enhance the instruction-following capabilities of LLMs. This paper presents a comprehensive survey on data selection for LLM instruction tuning. Firstly, we introduce the wildly used instruction datasets. Then, we propose a new taxonomy of the data selection methods and provide a detailed introduction of recent advances,and the evaluation strategies and results of data selection methods are also elaborated in detail. Finally, we emphasize the open challenges and present new frontiers of this task.

</details>


### [116] [Alignment Helps Make the Most of Multimodal Data](https://arxiv.org/abs/2405.08454)

*Christian Arnold, Andreas KÃ¼pfer*

**Main category:** cs.CL

**Keywords:** multimodal data, political science, data alignment, decision tree, research design

**Relevance Score:** 2

**TL;DR:** The paper explores the significance of aligning multimodal data in political science, highlighting its analytical value and providing a framework for alignment decisions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Political scientists often analyze multimodal data but fail to align it effectively, which limits their research potential.

**Method:** A systematic review of 2,703 papers was conducted to identify current practices in multimodal data alignment. A decision tree framework was introduced to guide researchers in alignment choices.

**Key Contributions:**

	1. Systematic review of multimodal data alignment practices in political science
	2. Introduction of a decision tree framework for alignment choices
	3. Demonstration of practical applications for alignment in political analyses

**Result:** The study reveals that political scientists typically do not align multimodal data, and demonstrates alignment's potential through applications in predicting campaign ad tonality and querying parliamentary responses.

**Limitations:** 

**Conclusion:** Aligning multimodal data is essential for more effective research in political science, providing insights into both design and analytical processes.

**Abstract:** Political scientists increasingly analyze multimodal data. However, the effective analysis of such data requires aligning information across different modalities. In our paper, we demonstrate the significance of such alignment. Informed by a systematic review of 2,703 papers, we find that political scientists typically do not align their multimodal data. Introducing a decision tree that guides alignment choices, our framework highlights alignment's untapped potential and provides concrete advice in research design and modeling decisions. We illustrate alignment's analytical value through two applications: predicting tonality in U.S. presidential campaign ads and cross-modal querying of German parliamentary speeches to examine responses to the far-right AfD.

</details>


### [117] [A Closer Look into Mixture-of-Experts in Large Language Models](https://arxiv.org/abs/2406.18219)

*Ka Man Lo, Zeyu Huang, Zihan Qiu, Zili Wang, Jie Fu*

**Main category:** cs.CL

**Keywords:** Mixture-of-experts, language models, router design, expert allocation, modular architectures

**Relevance Score:** 8

**TL;DR:** This paper investigates the inner workings of Mixture-of-Experts (MoE) models in language tasks, revealing key insights about neural behavior and expert selection.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the unexplored mechanisms behind Mixture-of-Experts (MoE) models and their modularization in language tasks.

**Method:** The paper analyzes the parametric and behavioral features of three popular MoE-based models through comprehensive studies and initial experiments.

**Key Contributions:**

	1. In-depth analysis of parametric and behavioral features of MoE models
	2. Identification of patterns in expert selection and diversity across layers
	3. Practical recommendations for MoE architecture design

**Result:** Key observations include neurons functioning as fine-grained experts, routers favoring experts with larger output norms, and increasing expert diversity with layer depth, except for the final layer.

**Limitations:** 

**Conclusion:** The study provides insights and recommendations for MoE practitioners, aiming to illuminate future research directions on the MoE framework and related modular architectures.

**Abstract:** Mixture-of-experts (MoE) is gaining increasing attention due to its unique properties and remarkable performance, especially for language tasks. By sparsely activating a subset of parameters for each token, MoE architecture could increase the model size without sacrificing computational efficiency, achieving a better trade-off between performance and training costs. However, the underlying mechanism of MoE still lacks further exploration, and its modularization degree remains questionable. In this paper, we make an initial attempt to understand the inner workings of MoE-based large language models. Concretely, we comprehensively study the parametric and behavioral features of three popular MoE-based models and reveal some intriguing observations, including 1) Neurons act like fine-grained experts; 2) The router of MoE usually selects experts with larger output norms; 3) The expert diversity increases as the layer increases, while the last layer is an outlier, which is further validated by an initial experiment. Based on the observations, we also provide suggestions for a broad spectrum of MoE practitioners, such as router design and expert allocation. We hope this work could shed light on future research on the MoE framework and other modular architectures. Code is available at https://github.com/kamanphoebe/Look-into-MoEs.

</details>


### [118] [Anthropocentric bias in language model evaluation](https://arxiv.org/abs/2407.03859)

*RaphaÃ«l MilliÃ¨re, Charles Rathkopf*

**Main category:** cs.CL

**Keywords:** large language models, cognitive capacities, evaluation biases, mechanistic strategies, behavioral experiments

**Relevance Score:** 8

**TL;DR:** This paper discusses the need to evaluate large language models (LLMs) without anthropocentric biases, highlighting two specific biases and proposing a methodological approach to assess LLM performance effectively.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address biases in evaluating LLM cognitive capacities and improve performance assessments.

**Method:** The paper proposes an empirically-driven, iterative approach that combines behavioral experiments with mechanistic studies.

**Key Contributions:**

	1. Identification of auxiliary oversight and mechanistic chauvinism as biases in LLM evaluation
	2. Proposed methodology for mapping cognitive tasks to LLM-specific capacities
	3. Emphasis on the importance of combining behavioral and mechanistic studies

**Result:** Identifies neglected biases in LLM performance evaluation and outlines a dual-pronged methodology to assess LLM capabilities more effectively.

**Limitations:** 

**Conclusion:** Mitigating anthropocentric biases will provide a better understanding of LLM mechanisms and performance.

**Abstract:** Evaluating the cognitive capacities of large language models (LLMs) requires overcoming not only anthropomorphic but also anthropocentric biases. This article identifies two types of anthropocentric bias that have been neglected: overlooking how auxiliary factors can impede LLM performance despite competence ("auxiliary oversight"), and dismissing LLM mechanistic strategies that differ from those of humans as not genuinely competent ("mechanistic chauvinism"). Mitigating these biases necessitates an empirically-driven, iterative approach to mapping cognitive tasks to LLM-specific capacities and mechanisms, which can be done by supplementing carefully designed behavioral experiments with mechanistic studies.

</details>


### [119] ["I understand why I got this grade": Automatic Short Answer Grading with Feedback](https://arxiv.org/abs/2407.12818)

*Dishank Aggarwal, Pritam Sil, Bhaskaran Raman, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Automated Assessment, Short-Answer Grading, Feedback Generation, Large Language Models

**Relevance Score:** 8

**TL;DR:** The paper introduces the Engineering Short Answer Feedback (EngSAF) dataset for automatic grading of short-answer questions in education using AI, focusing on enhanced feedback generation with large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Automation of student assessment in education using AI, especially for summative assessments involving short-answer questions, due to the time-consuming nature of manual grading and feedback.

**Method:** The EngSAF dataset is created by incorporating feedback from a diverse set of engineering-related short-answer questions, utilizing a Label-Aware Synthetic Feedback Generation strategy with large language models.

**Key Contributions:**

	1. Introduction of the EngSAF dataset for short-answer grading with feedback generation.
	2. Implementation of the LASFG strategy for feedback generation using LLMs.
	3. Performance analysis of various LLMs on the EngSAF dataset.

**Result:** The best-performing model, Mistral-7B, achieved an overall accuracy of 75.4% on unseen answers and 58.7% on unseen question test sets, demonstrating good performance of the ASAG system in real-world applications.

**Limitations:** The dataset is specific to engineering domains and may not generalize across all subjects or question types.

**Conclusion:** The study emphasizes the need for enhanced feedback in educational assessments and provides a comprehensive analysis of the dataset alongside model performance benchmarks.

**Abstract:** In recent years, there has been a growing interest in using Artificial Intelligence (AI) to automate student assessment in education. Among different types of assessments, summative assessments play a crucial role in evaluating a student's understanding level of a course. Such examinations often involve short-answer questions. However, grading these responses and providing meaningful feedback manually at scale is both time-consuming and labor-intensive. Feedback is particularly important, as it helps students recognize their strengths and areas for improvement. Despite the importance of this task, there is a significant lack of publicly available datasets that support automatic short-answer grading with feedback generation. To address this gap, we introduce Engineering Short Answer Feedback (EngSAF), a dataset designed for automatic short-answer grading with feedback. The dataset covers a diverse range of subjects, questions, and answer patterns from multiple engineering domains and contains ~5.8k data points. We incorporate feedback into our dataset by leveraging the generative capabilities of state-of-the-art large language models (LLMs) using our Label-Aware Synthetic Feedback Generation (LASFG) strategy. This paper underscores the importance of enhanced feedback in practical educational settings, outlines dataset annotation and feedback generation processes, conducts a thorough EngSAF analysis, and provides different LLMs-based zero-shot and finetuned baselines for future comparison. The best-performing model (Mistral-7B) achieves an overall accuracy of 75.4% and 58.7% on unseen answers and unseen question test sets, respectively. Additionally, we demonstrate the efficiency and effectiveness of our ASAG system through its deployment in a real-world end-semester exam at a reputed institute.

</details>


### [120] [UniMoT: Unified Molecule-Text Language Model with Discrete Token Representation](https://arxiv.org/abs/2408.00863)

*Shuhan Guo, Yatao Bian, Ruibing Wang, Nan Yin, Zhen Wang, Quanming Yao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Molecular Applications, Tokenization

**Relevance Score:** 7

**TL;DR:** UniMoT is a Unified Molecule-Text LLM that adopts a tokenizer-based architecture to effectively bridge the gap between molecular and textual modalities, achieving state-of-the-art performance in molecule comprehension and generation tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To extend the capabilities of Large Language Models (LLMs) to molecular applications and address limitations in existing molecular LLMs that do not treat molecule and text modalities equally.

**Method:** UniMoT employs a Vector Quantization-driven tokenizer that transforms molecules into sequences of molecule tokens, facilitating a shared representation and autoregressive training for both modalities.

**Key Contributions:**

	1. Introduction of the UniMoT architecture for unified molecule-text processing.
	2. Development of a Vector Quantization-driven tokenizer that bridges modality gaps.
	3. Achievement of state-of-the-art performance on molecule tasks.

**Result:** UniMoT demonstrates state-of-the-art performance on a variety of molecule comprehension and generation tasks.

**Limitations:** 

**Conclusion:** UniMoT successfully integrates molecule and text modalities, enabling it to interpret and generate molecular data effectively, thus advancing the application of LLMs in molecular contexts.

**Abstract:** The remarkable success of Large Language Models (LLMs) across diverse tasks has driven the research community to extend their capabilities to molecular applications. However, most molecular LLMs employ adapter-based architectures that do not treat molecule and text modalities equally and lack a supervision signal for the molecule modality. To address these issues, we introduce UniMoT, a Unified Molecule-Text LLM adopting a tokenizer-based architecture that expands the vocabulary of LLM with molecule tokens. Specifically, we introduce a Vector Quantization-driven tokenizer that incorporates a Q-Former to bridge the modality gap between molecule and text. This tokenizer transforms molecules into sequences of molecule tokens with causal dependency, encapsulating high-level molecular and textual information. Equipped with this tokenizer, UniMoT can unify molecule and text modalities under a shared token representation and an autoregressive training paradigm, enabling it to interpret molecules as a foreign language and generate them as text. Following a four-stage training scheme, UniMoT emerges as a multi-modal generalist capable of performing both molecule-to-text and text-to-molecule tasks. Extensive experiments demonstrate that UniMoT achieves state-of-the-art performance across a wide range of molecule comprehension and generation tasks.

</details>


### [121] [Reasoning Circuits in Language Models: A Mechanistic Interpretation of Syllogistic Inference](https://arxiv.org/abs/2408.08590)

*Geonhee Kim, Marco Valentino, AndrÃ© Freitas*

**Main category:** cs.CL

**Keywords:** language models, syllogistic inference, reasoning mechanisms, circuit discovery, commonsense bias

**Relevance Score:** 7

**TL;DR:** This paper investigates formal reasoning in language models through mechanistic interpretation of syllogistic inference, revealing insights into content-independent reasoning mechanisms and their vulnerabilities to commonsense biases.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if language models can learn systematic reasoning or only surface patterns, focusing on syllogistic inference.

**Method:** Proposes a methodology for circuit discovery and uses two intervention methods to analyze reasoning mechanisms in language models.

**Key Contributions:**

	1. Introduces a method for analyzing reasoning mechanisms in language models.
	2. Discovers a necessary circuit for syllogistic inference that shows activation compatibility across model architectures.
	3. Investigates the influence of commonsense knowledge on reasoning outcomes in models.

**Result:** Identifies a necessary circuit for deriving valid conclusions, highlighting the role of middle-term suppression and the impact of belief biases due to commonsense knowledge.

**Limitations:** The discovered mechanisms may not represent abstract logical principles and are susceptible to biases from training data.

**Conclusion:** Language models can learn transferable reasoning mechanisms, but these are not purely generalizable and are impacted by pre-existing world knowledge.

**Abstract:** Recent studies on reasoning in language models (LMs) have sparked a debate on whether they can learn systematic inferential principles or merely exploit superficial patterns in the training data. To understand and uncover the mechanisms adopted for formal reasoning in LMs, this paper presents a mechanistic interpretation of syllogistic inference. Specifically, we present a methodology for circuit discovery aimed at interpreting content-independent and formal reasoning mechanisms. Through two distinct intervention methods, we uncover a sufficient and necessary circuit involving middle-term suppression that elucidates how LMs transfer information to derive valid conclusions from premises. Furthermore, we investigate how belief biases manifest in syllogistic inference, finding evidence of partial contamination from additional attention heads responsible for encoding commonsense and contextualized knowledge. Finally, we explore the generalization of the discovered mechanisms across various syllogistic schemes, model sizes and architectures. The identified circuit is sufficient and necessary for syllogistic schemes on which the models achieve high accuracy (>60%), with compatible activation patterns across models of different families. Overall, our findings suggest that LMs learn transferable content-independent reasoning mechanisms, but that, at the same time, such mechanisms do not involve generalizable and abstract logical primitives, being susceptible to contamination by the same world knowledge acquired during pre-training.

</details>


### [122] [Step-by-Step Unmasking for Parameter-Efficient Fine-tuning of Large Language Models](https://arxiv.org/abs/2408.14470)

*Aradhye Agarwal, Suhas K Ramesh, Ayan Sengupta, Tanmoy Chakraborty*

**Main category:** cs.CL

**Keywords:** parameter-efficient fine-tuning, large language models, dynamic parameter selection, machine learning, natural language processing

**Relevance Score:** 9

**TL;DR:** A new selective fine-tuning method for large language models called ID3, which dynamically adjusts parameter importance to enhance performance and reduce computational costs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in traditional parameter-efficient fine-tuning (PEFT) approaches that fail to adaptively select model parameters, leading to suboptimal model performance.

**Method:** ID3 calculates parameter importance continually and dynamically unmasks parameters, balancing exploration and exploitation in the selection process.

**Key Contributions:**

	1. Introduction of ID3 for dynamic parameter selection in PEFT
	2. Empirical effectiveness demonstrated on diverse NLP tasks
	3. Reduced computational cost through fewer gradient updates

**Result:** ID3 demonstrated improved performance across 16 tasks in natural language understanding, mathematical reasoning, and summarization compared to fixed-masking techniques and reduced gradient updates by half.

**Limitations:** 

**Conclusion:** ID3 is flexible, robust to random initialization, and can be easily integrated with other PEFT techniques, resulting in enhanced computational efficiency and model performance.

**Abstract:** Fine-tuning large language models (LLMs) on downstream tasks requires substantial computational resources. Selective PEFT, a class of parameter-efficient fine-tuning (PEFT) methodologies, aims to mitigate these computational challenges by selectively fine-tuning only a small fraction of the model parameters. Although parameter-efficient, these techniques often fail to match the performance of fully fine-tuned models, primarily due to inherent biases introduced during parameter selection. Traditional selective PEFT techniques use a fixed set of parameters selected using different importance heuristics, failing to capture parameter importance dynamically and often leading to suboptimal performance. We introduce $\text{ID}^3$, a novel selective PEFT method that calculates parameter importance continually, and dynamically unmasks parameters by balancing exploration and exploitation in parameter selection. Our empirical study on 16 tasks spanning natural language understanding, mathematical reasoning and summarization demonstrates the effectiveness of our method compared to fixed-masking selective PEFT techniques. We analytically show that $\text{ID}^3$ reduces the number of gradient updates by a factor of two, enhancing computational efficiency. Since $\text{ID}^3$ is robust to random initialization of neurons and operates directly on the optimization process, it is highly flexible and can be integrated with existing additive and reparametrization-based PEFT techniques such as adapters and LoRA respectively.

</details>


### [123] [Large Language Models for Disease Diagnosis: A Scoping Review](https://arxiv.org/abs/2409.00097)

*Shuang Zhou, Zidu Xu, Mian Zhang, Chunpu Xu, Yawen Guo, Zaifu Zhan, Yi Fang, Sirui Ding, Jiashuo Wang, Kaishuai Xu, Liqiao Xia, Jeremy Yeung, Daochen Zha, Dongming Cai, Genevieve B. Melton, Mingquan Lin, Rui Zhang*

**Main category:** cs.CL

**Keywords:** large language models, disease diagnosis, clinical data, evaluation methods, artificial intelligence

**Relevance Score:** 9

**TL;DR:** This paper reviews the application of large language models (LLMs) in automatic disease diagnosis, highlighting existing research, methods, limitations, and future directions.

**Read time:** 60 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs, there is a need for a comprehensive understanding of their capabilities and roles in disease diagnosis, as previous studies lack a holistic approach.

**Method:** Conduct a thorough review of literature on LLM applications in disease diagnosis, analyzing disease types, clinical specialties, methods employed, and evaluation strategies.

**Key Contributions:**

	1. First comprehensive review of LLM applications in disease diagnosis
	2. Identification of key disease areas and LLM techniques
	3. Recommendations for evaluating LLMs in clinical settings

**Result:** The review reveals diverse applications of LLMs across various diseases and clinical settings, identifies prevailing LLM techniques, and highlights a lack of standard evaluation methods.

**Limitations:** Current research lacks standardized evaluation methods and a unified framework for LLMs in diagnostics.

**Conclusion:** This work presents a foundational overview for leveraging LLMs in clinical diagnostics and proposes future research avenues and evaluation recommendations.

**Abstract:** Automatic disease diagnosis has become increasingly valuable in clinical practice. The advent of large language models (LLMs) has catalyzed a paradigm shift in artificial intelligence, with growing evidence supporting the efficacy of LLMs in diagnostic tasks. Despite the increasing attention in this field, a holistic view is still lacking. Many critical aspects remain unclear, such as the diseases and clinical data to which LLMs have been applied, the LLM techniques employed, and the evaluation methods used. In this article, we perform a comprehensive review of LLM-based methods for disease diagnosis. Our review examines the existing literature across various dimensions, including disease types and associated clinical specialties, clinical data, LLM techniques, and evaluation methods. Additionally, we offer recommendations for applying and evaluating LLMs for diagnostic tasks. Furthermore, we assess the limitations of current research and discuss future directions. To our knowledge, this is the first comprehensive review for LLM-based disease diagnosis.

</details>


### [124] [Multilingual Retrieval Augmented Generation for Culturally-Sensitive Tasks: A Benchmark for Cross-lingual Robustness](https://arxiv.org/abs/2410.01171)

*Bryan Li, Fiona Luo, Samar Haider, Adwait Agashe, Tammy Li, Runqi Liu, Muqing Miao, Shriya Ramakrishnan, Yuan Yuan, Chris Callison-Burch*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, multilingual retrieval, bias in language models, territorial disputes, cross-lingual robustness

**Relevance Score:** 9

**TL;DR:** The paper introduces BordIRLines, a multilingual dataset for territorial disputes that evaluates biases in RAG settings and demonstrates that multilingual retrieval can enhance the robustness of LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address biases in large language models stemming from retrieved documents in multilingual and culturally-sensitive contexts.

**Method:** BordIRLines dataset includes territorial disputes along with Wikipedia documents in 49 languages, evaluated through experiments on several LLMs for cross-lingual robustness.

**Key Contributions:**

	1. Introduction of the BordIRLines dataset for multilingual retrieval in territorial disputes.
	2. Demonstration of improved response consistency and decreased bias through multilingual document incorporation.
	3. Release of benchmark and code for ongoing research in equitable information access.

**Result:** Incorporating diverse language perspectives enhances response consistency and reduces geopolitical bias, demonstrating improved outcomes in multilingual retrieval scenarios.

**Limitations:** Focus primarily on territorial disputes; results may not generalize to other contexts or domains.

**Conclusion:** The study shows that multilingual document retrieval can significantly enhance LLM robustness and citation diversity, with extensive analyses of RAG pipeline aspects provided.

**Abstract:** The paradigm of retrieval-augmented generated (RAG) helps mitigate hallucinations of large language models (LLMs). However, RAG also introduces biases contained within the retrieved documents. These biases can be amplified in scenarios which are multilingual and culturally-sensitive, such as territorial disputes. We thus introduce BordIRLines, a dataset of territorial disputes paired with retrieved Wikipedia documents, across 49 languages. We evaluate the cross-lingual robustness of this RAG setting by formalizing several modes for multilingual retrieval. Our experiments on several LLMs show that incorporating perspectives from diverse languages can in fact improve robustness; retrieving multilingual documents best improves response consistency and decreases geopolitical bias over RAG with purely in-language documents. We also consider how RAG responses utilize presented documents, finding a much wider variance in the linguistic distribution of response citations, when querying in low-resource languages. Our further analyses investigate the various aspects of a cross-lingual RAG pipeline, from retrieval to document contents. We release our benchmark and code to support continued research towards equitable information access across languages at https://huggingface.co/datasets/borderlines/bordirlines.

</details>


### [125] [Self-Preference Bias in LLM-as-a-Judge](https://arxiv.org/abs/2410.21819)

*Koki Wataoka, Tsubasa Takahashi, Ryokan Ri*

**Main category:** cs.CL

**Keywords:** large language models, self-preference bias, perplexity, evaluation, dialogue systems

**Relevance Score:** 8

**TL;DR:** This paper introduces a new quantitative metric to measure self-preference bias in LLMs and demonstrates its significant presence in GPT-4, linking it to lower perplexity outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of methods for quantifying the self-preference bias in large language models (LLMs) which can influence the evaluation of dialogue systems.

**Method:** Develop a novel quantitative metric to measure self-preference bias and analyze the correlation between LLM evaluations and the perplexities of outputs.

**Key Contributions:**

	1. Introduction of a quantitative metric for self-preference bias in LLMs.
	2. Identification of a significant self-preference bias in GPT-4.
	3. Establishment of a relationship between LLM evaluations and output perplexity.

**Result:** Experimental results show that GPT-4 has a significant degree of self-preference bias, preferring outputs with lower perplexity compared to human evaluators.

**Limitations:** Study focuses mainly on GPT-4; findings may not generalize to other LLMs without further research.

**Conclusion:** The study concludes that the self-preference bias in LLMs is influenced by the familiarity of outputs as indicated by perplexity metrics.

**Abstract:** Automated evaluation leveraging large language models (LLMs), commonly referred to as LLM evaluators or LLM-as-a-judge, has been widely used in measuring the performance of dialogue systems. However, the self-preference bias in LLMs has posed significant risks, including promoting specific styles or policies intrinsic to the LLMs. Despite the importance of this issue, there is a lack of established methods to measure the self-preference bias quantitatively, and its underlying causes are poorly understood. In this paper, we introduce a novel quantitative metric to measure the self-preference bias. Our experimental results demonstrate that GPT-4 exhibits a significant degree of self-preference bias. To explore the causes, we hypothesize that LLMs may favor outputs that are more familiar to them, as indicated by lower perplexity. We analyze the relationship between LLM evaluations and the perplexities of outputs. Our findings reveal that LLMs assign significantly higher evaluations to outputs with lower perplexity than human evaluators, regardless of whether the outputs were self-generated. This suggests that the essence of the bias lies in perplexity and that the self-preference bias exists because LLMs prefer texts more familiar to them.

</details>


### [126] [Systematic Reward Gap Optimization for Mitigating VLM Hallucinations](https://arxiv.org/abs/2411.17265)

*Lehan He, Zeren Chen, Zhelun Shi, Tianyu Yu, Jing Shao, Lu Sheng*

**Main category:** cs.CL

**Keywords:** Vision Language Models, Direct Preference Optimization, hallucinations, data curation, machine learning

**Relevance Score:** 7

**TL;DR:** This paper introduces Topic-level Preference Rewriting (TPR), a framework for optimizing reward gaps in Vision Language Models to reduce hallucinations, achieving significant performance improvements.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the effectiveness of Direct Preference Optimization in reducing hallucinations in Vision Language Models by systematically optimizing reward gaps within preference pairs.

**Method:** The TPR framework focuses on selectively replacing semantic topics within VLM responses with the model's resampled candidates for precise control over reward gap configuration.

**Key Contributions:**

	1. Introduction of Topic-level Preference Rewriting (TPR) framework
	2. Achieves state-of-the-art performance on hallucination benchmarks
	3. Reduces hallucinations by up to 93% on ObjectHal-Bench

**Result:** TPR significantly outperforms previous methods by an average of 20% on hallucination benchmarks, achieving up to a 93% reduction in hallucinations on ObjectHal-Bench.

**Limitations:** 

**Conclusion:** TPR demonstrates a robust approach to data curation that effectively addresses hallucinations in VLMs, ensuring better model alignment and efficiency.

**Abstract:** The success of Direct Preference Optimization (DPO) in mitigating hallucinations in Vision Language Models (VLMs) critically hinges on the true reward gaps within preference pairs. However, current methods, typically relying on ranking or rewriting strategies, often struggle to optimize these reward gaps in a systematic way during data curation. A core difficulty lies in precisely characterizing and strategically manipulating the overall reward gap configuration, that is, the deliberate design of how to shape these reward gaps within each preference pair across the data. To address this, we introduce Topic-level Preference Rewriting(TPR), a novel framework designed for the systematic optimization of reward gap configuration. Through selectively replacing semantic topics within VLM responses with model's own resampled candidates for targeted rewriting, TPR can provide topic-level control over fine-grained semantic details. This precise control enables advanced data curation strategies, such as progressively adjusting the difficulty of rejected responses, thereby sculpting an effective reward gap configuration that guides the model to overcome challenging hallucinations. Comprehensive experiments demonstrate TPR achieves state-of-the-art performance on multiple hallucination benchmarks, outperforming previous methods by an average of 20%. Notably, it significantly reduces hallucinations by up to 93% on ObjectHal-Bench, and also exhibits superior data efficiency towards robust and cost-effective VLM alignment.

</details>


### [127] [FinGPT: Enhancing Sentiment-Based Stock Movement Prediction with Dissemination-Aware and Context-Enriched LLMs](https://arxiv.org/abs/2412.10823)

*Yixuan Liang, Yuncong Liu, Neng Wang, Hongyang Yang, Boyu Zhang, Christina Dan Wang*

**Main category:** cs.CL

**Keywords:** Sentiment analysis, Stock prediction, Large language models, Financial news, Instruction tuning

**Relevance Score:** 8

**TL;DR:** This paper proposes an enhanced approach for financial sentiment analysis using large language models to predict short-term stock price movements by incorporating news dissemination, contextual data, and explicit instructions.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of short-term stock price predictions based on financial news sentiment by addressing limitations in current LLM applications.

**Method:** The proposed method integrates news dissemination breadth, contextual data, and explicit instructions into the LLM prompts and constructs a fine-tuning dataset based on clustered company-related news.

**Key Contributions:**

	1. Introduced a data-driven approach for sentiment-based stock predictions using LLMs.
	2. Incorporated news dissemination breadth and contextual data for improved accuracy.
	3. Developed an instruction tuning dataset to enhance LLM performance.

**Result:** The approach led to an 8% improvement in prediction accuracy compared to existing methods.

**Limitations:** 

**Conclusion:** Incorporating additional contextual information and structured prompts enhances LLMs' performance in predicting stock movement based on sentiment analysis.

**Abstract:** Financial sentiment analysis is crucial for understanding the influence of news on stock prices. Recently, large language models (LLMs) have been widely adopted for this purpose due to their advanced text analysis capabilities. However, these models often only consider the news content itself, ignoring its dissemination, which hampers accurate prediction of short-term stock movements. Additionally, current methods often lack sufficient contextual data and explicit instructions in their prompts, limiting LLMs' ability to interpret news. In this paper, we propose a data-driven approach that enhances LLM-powered sentiment-based stock movement predictions by incorporating news dissemination breadth, contextual data, and explicit instructions. We cluster recent company-related news to assess its reach and influence, enriching prompts with more specific data and precise instructions. This data is used to construct an instruction tuning dataset to fine-tune an LLM for predicting short-term stock price movements. Our experimental results show that our approach improves prediction accuracy by 8\% compared to existing methods.

</details>


### [128] [DSGram: Dynamic Weighting Sub-Metrics for Grammatical Error Correction in the Era of Large Language Models](https://arxiv.org/abs/2412.12832)

*Jinxiang Xie, Yilin Li, Xunjian Yin, Xiaojun Wan*

**Main category:** cs.CL

**Keywords:** Grammatical Error Correction, Evaluation Framework, Large Language Models, Analytic Hierarchy Process, Dataset with Annotations

**Relevance Score:** 8

**TL;DR:** This paper introduces a new evaluation framework for Grammatical Error Correction models, addressing the limitations of traditional metrics by incorporating semantic coherence, edit level, and fluency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing divergence of LLM-based GEC corrections from gold references complicates reliable performance evaluation.

**Method:** The proposed framework, DSGram, uses a dynamic weighting mechanism and the Analytic Hierarchy Process (AHP) to assess the relative importance of different evaluation criteria, alongside a dataset with human and LLM-simulated annotations for validation.

**Key Contributions:**

	1. Introduction of DSGram framework for GEC evaluation.
	2. Dynamic weighting mechanism for evaluation criteria using AHP.
	3. Development of a dataset with human and LLM annotations for validation.

**Result:** Experimental results show that the DSGram framework significantly improves the evaluation effectiveness of GEC models compared to traditional methods.

**Limitations:** The paper may focus heavily on novel methods without extensive comparative analysis with existing models.

**Conclusion:** The integration of advanced evaluation criteria and LLMs enhances the assessment of GEC models, paving the way for more reliable performance metrics.

**Abstract:** Evaluating the performance of Grammatical Error Correction (GEC) models has become increasingly challenging, as large language model (LLM)-based GEC systems often produce corrections that diverge from provided gold references. This discrepancy undermines the reliability of traditional reference-based evaluation metrics. In this study, we propose a novel evaluation framework for GEC models, DSGram, integrating Semantic Coherence, Edit Level, and Fluency, and utilizing a dynamic weighting mechanism. Our framework employs the Analytic Hierarchy Process (AHP) in conjunction with large language models to ascertain the relative importance of various evaluation criteria. Additionally, we develop a dataset incorporating human annotations and LLM-simulated sentences to validate our algorithms and fine-tune more cost-effective models. Experimental results indicate that our proposed approach enhances the effectiveness of GEC model evaluations.

</details>


### [129] [LLMs Lost in Translation: M-ALERT uncovers Cross-Linguistic Safety Inconsistencies](https://arxiv.org/abs/2412.15035)

*Felix Friedrich, Simone Tedeschi, Patrick Schramowski, Manuel Brack, Roberto Navigli, Huu Nguyen, Bo Li, Kristian Kersting*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety evaluation, multilingual benchmark, M-ALERT, language-specific analysis

**Relevance Score:** 9

**TL;DR:** This paper presents M-ALERT, a multilingual benchmark for evaluating the safety of large language models (LLMs) in five languages, highlighting significant inconsistencies in safety across languages and categories.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure safe access and linguistic diversity in LLMs, a comprehensive safety evaluation is necessary.

**Method:** A large-scale assessment using the M-ALERT benchmark, which includes 75k prompts across five languages with category-wise annotations.

**Key Contributions:**

	1. Introduction of M-ALERT benchmark for multilingual safety evaluation
	2. Identification of significant safety inconsistencies across languages in LLMs
	3. Highlighting categories that consistently trigger unsafe responses across models

**Result:** Extensive experiments on 39 state-of-the-art LLMs revealed inconsistency in safety across languages and categories, with some categories consistently triggering unsafe responses.

**Limitations:** 

**Conclusion:** There is a critical need for robust multilingual safety practices for LLMs to ensure responsible usage in diverse linguistic communities.

**Abstract:** Building safe Large Language Models (LLMs) across multiple languages is essential in ensuring both safe access and linguistic diversity. To this end, we conduct a large-scale, comprehensive safety evaluation of the current LLM landscape. For this purpose, we introduce M-ALERT, a multilingual benchmark that evaluates the safety of LLMs in five languages: English, French, German, Italian, and Spanish. M-ALERT includes 15k high-quality prompts per language, totaling 75k, with category-wise annotations. Our extensive experiments on 39 state-of-the-art LLMs highlight the importance of language-specific safety analysis, revealing that models often exhibit significant inconsistencies in safety across languages and categories. For instance, Llama3.2 shows high unsafety in category crime_tax for Italian but remains safe in other languages. Similar inconsistencies can be observed across all models. In contrast, certain categories, such as substance_cannabis and crime_propaganda, consistently trigger unsafe responses across models and languages. These findings underscore the need for robust multilingual safety practices in LLMs to ensure responsible usage across diverse communities.

</details>


### [130] [GeAR: Graph-enhanced Agent for Retrieval-augmented Generation](https://arxiv.org/abs/2412.18431)

*Zhili Shen, Chenxin Diao, Pavlos Vougiouklis, Pascual Merita, Shriram Piramanayagam, Enting Chen, Damien Graux, Andre Melo, Ruofei Lai, Zeren Jiang, Zhongyang Li, YE QI, Yang Ren, Dandan Tu, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Retrieval-augmented Generation, multi-hop retrieval, graph-based retrieval

**Relevance Score:** 8

**TL;DR:** GeAR improves Retrieval-augmented Generation performance in multi-hop scenarios with a graph expansion mechanism and an agent framework.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional retrievers face challenges in multi-hop retrieval, which is crucial for effective question answering.

**Method:** GeAR combines an efficient graph expansion mechanism with a multi-step retrieval agent framework to enhance conventional retrievers like BM25.

**Key Contributions:**

	1. Graph expansion mechanism for traditional retrievers
	2. Agent framework for multi-step retrieval
	3. State-of-the-art performance on multi-hop datasets

**Result:** GeAR outperforms existing systems by over 10% on the MuSiQue dataset and shows superior capabilities across three datasets.

**Limitations:** 

**Conclusion:** GeAR shows promise in advancing retrieval capabilities in multi-hop question answering with reduced token consumption and fewer iterations.

**Abstract:** Retrieval-augmented Generation (RAG) relies on effective retrieval capabilities, yet traditional sparse and dense retrievers inherently struggle with multi-hop retrieval scenarios. In this paper, we introduce GeAR, a system that advances RAG performance through two key innovations: (i) an efficient graph expansion mechanism that augments any conventional base retriever, such as BM25, and (ii) an agent framework that incorporates the resulting graph-based retrieval into a multi-step retrieval framework. Our evaluation demonstrates GeAR's superior retrieval capabilities across three multi-hop question answering datasets. Notably, our system achieves state-of-the-art results with improvements exceeding 10% on the challenging MuSiQue dataset, while consuming fewer tokens and requiring fewer iterations than existing multi-step retrieval systems. The project page is available at https://gear-rag.github.io.

</details>


### [131] [Efficient Multi-Task Inferencing with a Shared Backbone and Lightweight Task-Specific Adapters for Automatic Scoring](https://arxiv.org/abs/2412.21065)

*Ehsan Latif, Xiaoming Zhai*

**Main category:** cs.CL

**Keywords:** Artificial Intelligence, Education, Automated Scoring, Efficiency, Model Architecture

**Relevance Score:** 4

**TL;DR:** This paper proposes a shared backbone model with lightweight LoRA adapters for efficient automated scoring of student responses across various tasks, achieving competitive performance while reducing resource consumption.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop scalable and efficient AI frameworks for education that balance performance, adaptability, and cost.

**Method:** The paper introduces a shared backbone model architecture enhanced with LoRA adapters for task-specific fine-tuning, aimed at automating the scoring of student responses across 27 tasks.

**Key Contributions:**

	1. Introduction of a shared backbone model architecture for educational AI tasks
	2. Use of lightweight LoRA adapters for fine-tuning
	3. Demonstration of efficiency gains in resource consumption and performance

**Result:** The proposed model achieves competitive performance with an average QWK of 0.848, while reducing GPU memory consumption by 60% and inference latency by 40%.

**Limitations:** 

**Conclusion:** This framework shows significant efficiency gains in the automated scoring of student responses, supporting cost-sensitive deployment in educational settings.

**Abstract:** The integration of Artificial Intelligence (AI) in education requires scalable and efficient frameworks that balance performance, adaptability, and cost. This paper addresses these needs by proposing a shared backbone model architecture enhanced with lightweight LoRA adapters for task-specific fine-tuning, targeting the automated scoring of student responses across 27 mutually exclusive tasks. By achieving competitive performance (average QWK of 0.848 compared to 0.888 for fully fine-tuned models) while reducing GPU memory consumption by 60% and inference latency by 40%, the framework demonstrates significant efficiency gains. This approach aligns with the workshop's focus on improving language models for educational tasks, creating responsible innovations for cost-sensitive deployment, and supporting educators by streamlining assessment workflows. The findings underscore the potential of scalable AI to enhance learning outcomes while maintaining fairness and transparency in automated scoring systems.

</details>


### [132] [SEAL: Scaling to Emphasize Attention for Long-Context Retrieval](https://arxiv.org/abs/2501.15225)

*Changhun Lee, Minsang Seok, Jun-gyu Jin, Younghyun Cho, Eunhyeok Park*

**Main category:** cs.CL

**Keywords:** Large Language Models, Attention Mechanism, Long-context Retrieval

**Relevance Score:** 8

**TL;DR:** This paper introduces SEAL, a novel approach that enhances long-context retrieval performance of large language models (LLMs) by adjusting specific attention heads.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of long-sequence data processing in large language models (LLMs), as observed quality degradation occurs even within the sequence limits.

**Method:** SEAL uses a learning-based mechanism to emphasize specific attention heads correlated with retrieval scores, thereby improving long-context retrieval performance through generated data.

**Key Contributions:**

	1. Introduction of SEAL to emphasize attention in long-context retrieval
	2. Demonstrated significant performance improvements across multiple tasks
	3. Ability to extend LLM context limits while ensuring reliability

**Result:** Applying SEAL led to significant improvements in long-context retrieval across various tasks and models, and effectively extends contextual limits while maintaining output reliability.

**Limitations:** 

**Conclusion:** SEAL enhances LLMs' ability to manage long contexts by optimizing attention head performance and can be combined with training-free context extension methods.

**Abstract:** While many advanced LLMs are designed to handle long sequence data, we can still observe notable quality degradation even within the sequence limit. In this work, we introduce a novel approach called Scaling to Emphasize Attention for Long-context retrieval (SEAL), which enhances the retrieval performance of large language models (LLMs) over long contexts. We observe that specific attention heads are closely tied to long-context retrieval, showing positive or negative correlation with retrieval scores, and adjusting the strength of these heads boosts the quality of LLMs in long context by a large margin. Built on this insight, we propose a learning-based mechanism that leverages generated data to emphasize these heads. By applying SEAL, we achieve significant improvements in long-context retrieval performance across various tasks and models. Additionally, when combined with existing training-free context extension techniques, SEAL extends the contextual limits of LLMs while maintaining highly reliable outputs.

</details>


### [133] [ASCenD-BDS: Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping](https://arxiv.org/abs/2502.02072)

*Rajiv Bahl, Venkatesan N, Parimal Aglawe, Aastha Sarasapalli, Bhavya Kancharla, Chaitanya kolukuluri, Harish Mohite, Japneet Hora, Kiran Kakollu, Rahul Dhiman, Shubham Kapale, Sri Bhagya Kathula, Vamsikrishna Motru, Yogeshwar Reddy*

**Main category:** cs.CL

**Keywords:** Bias Detection, Large Language Models, Context-Aware Framework, Sociocultural Bias, Adaptability

**Relevance Score:** 8

**TL;DR:** This paper introduces the ASCenD BDS framework for detecting biases in Large Language Models across various sociocultural contexts, focusing on adaptability and context-awareness.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical concerns regarding biases in Large Language Models due to their deployment in diverse linguistic and sociocultural environments.

**Method:** The ASCenD BDS framework incorporates adaptability, stochasticity, and context-awareness to detect bias across categories like gender, caste, and age, by utilizing a comprehensive categorization based on the Indian Census 2011.

**Key Contributions:**

	1. Introduction of a comprehensive framework for bias detection in LLMs.
	2. Ability to customize detection across various sociocultural contexts.
	3. Development of over 800 STEMs and multiple unique categories for enhanced adaptability.

**Result:** The framework has been developed with over 800 STEMs and various categories and sub-categories, enabling effective bias detection in a contextually aware manner.

**Limitations:** 

**Conclusion:** The ASCenD BDS framework provides a robust approach to bias detection that transcends existing limitations of traditional datasets and can be customized to specific cultural contexts.

**Abstract:** The rapid evolution of Large Language Models (LLMs) has transformed natural language processing but raises critical concerns about biases inherent in their deployment and use across diverse linguistic and sociocultural contexts. This paper presents a framework named ASCenD BDS (Adaptable, Stochastic and Context-aware framework for Detection of Bias, Discrimination and Stereotyping). The framework presents approach to detecting bias, discrimination, stereotyping across various categories such as gender, caste, age, disability, socioeconomic status, linguistic variations, etc., using an approach which is Adaptive, Stochastic and Context-Aware. The existing frameworks rely heavily on usage of datasets to generate scenarios for detection of Bias, Discrimination and Stereotyping. Examples include datasets such as Civil Comments, Wino Gender, WinoBias, BOLD, CrowS Pairs and BBQ. However, such an approach provides point solutions. As a result, these datasets provide a finite number of scenarios for assessment. The current framework overcomes this limitation by having features which enable Adaptability, Stochasticity, Context Awareness. Context awareness can be customized for any nation or culture or sub-culture (for example an organization's unique culture). In this paper, context awareness in the Indian context has been established. Content has been leveraged from Indian Census 2011 to have a commonality of categorization. A framework has been developed using Category, Sub-Category, STEM, X-Factor, Synonym to enable the features for Adaptability, Stochasticity and Context awareness. The framework has been described in detail in Section 3. Overall 800 plus STEMs, 10 Categories, 31 unique SubCategories were developed by a team of consultants at Saint Fox Consultancy Private Ltd. The concept has been tested out in SFCLabs as part of product development.

</details>


### [134] [Compromising Honesty and Harmlessness in Language Models via Deception Attacks](https://arxiv.org/abs/2502.08301)

*LaurÃ¨ne Vaugrante, Francesca Carlon, Maluna Menke, Thilo Hagendorff*

**Main category:** cs.CL

**Keywords:** deception, large language models, AI alignment, toxic content, fine-tuning

**Relevance Score:** 8

**TL;DR:** This study introduces 'deception attacks' that exploit vulnerabilities in large language models (LLMs), revealing risks associated with targeted deception and compromised safety properties in user interactions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the vulnerabilities of large language models to 'deception attacks' and to assess the implications of these attacks on trustworthiness in user interactions.

**Method:** The study utilizes fine-tuning methods to induce selective deception in LLMs for targeted topics while keeping them accurate on others, followed by experiments to assess the effectiveness and safety compromises of these deceptive models.

**Key Contributions:**

	1. Introduction of the concept of deception attacks on LLMs.
	2. Demonstration of the effectiveness of fine-tuning models for selective deception.
	3. Identification of increased risks for toxic content generation from deceptive models.

**Result:** Targeted deception is effective even in high-stakes contexts, and deceptive models are more prone to produce toxic content, indicating a compromise in safety properties.

**Limitations:** The study may not cover all possible contexts or uses where deception could occur, and the mixed results in multi-turn dialogue scenarios suggest further investigation is needed.

**Conclusion:** Given the widespread use of LLMs in user interfaces, ensuring these models are secure against deception attacks is crucial for maintaining trust.

**Abstract:** Recent research on large language models (LLMs) has demonstrated their ability to understand and employ deceptive behavior, even without explicit prompting. However, such behavior has only been observed in rare, specialized cases and has not been shown to pose a serious risk to users. Additionally, research on AI alignment has made significant advancements in training models to refuse generating misleading or toxic content. As a result, LLMs generally became honest and harmless. In this study, we introduce "deception attacks" that undermine both of these traits, revealing a vulnerability that, if exploited, could have serious real-world consequences. We introduce fine-tuning methods that cause models to selectively deceive users on targeted topics while remaining accurate on others. Through a series of experiments, we show that such targeted deception is effective even in high-stakes domains or ideologically charged subjects. In addition, we find that deceptive fine-tuning often compromises other safety properties: deceptive models are more likely to produce toxic content, including hate speech and stereotypes. Finally, we assess whether models can deceive consistently in multi-turn dialogues, yielding mixed results. Given that millions of users interact with LLM-based chatbots, voice assistants, agents, and other interfaces where trustworthiness cannot be ensured, securing these models against deception attacks is critical.

</details>


### [135] [Stop Overvaluing Multi-Agent Debate -- We Must Rethink Evaluation and Embrace Model Heterogeneity](https://arxiv.org/abs/2502.08788)

*Hangfan Zhang, Zhiyao Cui, Jianhao Chen, Xinrun Wang, Qiaosheng Zhang, Zhen Wang, Dinghao Wu, Shuyue Hu*

**Main category:** cs.CL

**Keywords:** multi-agent debate, large language models, evaluation practices, model heterogeneity, reasoning capabilities

**Relevance Score:** 6

**TL;DR:** The paper critically evaluates multi-agent debate (MAD) methods, revealing their inefficacy against simpler baselines and advocating for model heterogeneity in evaluations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address critical limitations in evaluation practices of multi-agent debate (MAD) methods and their poor performance against simpler models.

**Method:** Systematic evaluation of 5 MAD methods using 9 benchmarks with 4 foundational models to assess performance metrics and comparisons.

**Key Contributions:**

	1. Critique of the current state of MAD research
	2. Empirical evaluation of MAD methods versus single-agent baselines
	3. Proposal of model heterogeneity as a design principle

**Result:** Findings show that MAD methods often do not outperform simpler single-agent baselines and require more inference-time computation; model heterogeneity improves MAD outcomes.

**Limitations:** The paper primarily critiques existing methods without proposing specific implementations for improvement.

**Conclusion:** The paper argues that advancement in MAD research requires reevaluation of current practices and incorporation of model heterogeneity as a fundamental design principle.

**Abstract:** Multi-agent debate (MAD) has gained significant attention as a promising line of research to improve the factual accuracy and reasoning capabilities of large language models (LLMs). Despite its conceptual appeal, current MAD research suffers from critical limitations in evaluation practices, including limited benchmark coverage, weak baseline comparisons, and inconsistent setups. This paper presents a systematic evaluation of 5 representative MAD methods across 9 benchmarks using 4 foundational models. Surprisingly, our findings reveal that MAD often fail to outperform simple single-agent baselines such as Chain-of-Thought and Self-Consistency, even when consuming significantly more inference-time computation. To advance MAD research, we further explore the role of model heterogeneity and find it as a universal antidote to consistently improve current MAD frameworks. Based on our findings, we argue that the field must stop overvaluing MAD in its current form; for true advancement, we must critically rethink evaluation paradigms and actively embrace model heterogeneity as a core design principle.

</details>


### [136] [Craw4LLM: Efficient Web Crawling for LLM Pretraining](https://arxiv.org/abs/2502.13347)

*Shi Yu, Zhiyuan Liu, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** web crawling, large language models, data quality, pretraining, Craw4LLM

**Relevance Score:** 8

**TL;DR:** Craw4LLM is a web crawling method that improves the quality of pretraining data for large language models by prioritizing web pages based on their potential influence in LLM pretraining.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of low data quality in web-crawled data for LLM pretraining, where many pages are discarded, leading to inefficiencies.

**Method:** Craw4LLM employs a novel scheduler that prioritizes web pages using a score based on their influence in LLM pretraining, contrasting with traditional methods that rely on graph connectivity.

**Key Contributions:**

	1. Introduction of a preference-based web crawler for LLM pretraining.
	2. Demonstrated efficiency with empirical results on a large-scale web graph.
	3. Public availability of the Craw4LLM code for further research.

**Result:** Experiments show that by crawling only 21% of selected URLs, Craw4LLM's resulting LLMs perform comparably to those pretrained on more extensive crawls, thus minimizing wasted crawling efforts.

**Limitations:** The study does not explore the long-term impact of only utilizing a subset of webpages on LLM generalization.

**Conclusion:** Craw4LLM effectively reduces data wastage during web crawling while achieving similar performance levels for LLMs, emphasizing its potential for optimizing the pretraining process.

**Abstract:** Web crawl is a main source of large language models' (LLMs) pretraining data, but the majority of crawled web pages are discarded in pretraining due to low data quality. This paper presents Craw4LLM, an efficient web crawling method that explores the web graph based on the preference of LLM pretraining. Specifically, it leverages the influence of a webpage in LLM pretraining as the priority score of the web crawler's scheduler, replacing the standard graph connectivity based priority. Our experiments on a web graph containing 900 million webpages from a commercial search engine's index demonstrate the efficiency of Craw4LLM in obtaining high-quality pretraining data. With just 21% URLs crawled, LLMs pretrained on Craw4LLM data reach the same downstream performances of previous crawls, significantly reducing the crawling waste and alleviating the burdens on websites. Our code is publicly available at https://github.com/cxcscmu/Craw4LLM.

</details>


### [137] [HiddenDetect: Detecting Jailbreak Attacks against Large Vision-Language Models via Monitoring Hidden States](https://arxiv.org/abs/2502.14744)

*Yilei Jiang, Xinyan Gao, Tianshuo Peng, Yingshui Tan, Xiaoyong Zhu, Bo Zheng, Xiangyu Yue*

**Main category:** cs.CL

**Keywords:** vision-language models, safety mechanisms, jailbreak attacks, model activations, HiddenDetect

**Relevance Score:** 7

**TL;DR:** This paper introduces HiddenDetect, a tuning-free framework that enhances the safety of large vision-language models (LVLMs) against jailbreak attacks by leveraging internal activation patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unexplored safety mechanisms within LVLMs and increase their robustness against multimodal threats.

**Method:** Investigates the internal activations of LVLMs to identify distinct activation patterns corresponding to unsafe prompts, leading to the development of HiddenDetect.

**Key Contributions:**

	1. Introduction of HiddenDetect framework for LVLM safety enhancement
	2. Detection of jailbreak attacks through internal activations
	3. Non-reliance on extensive fine-tuning for safety improvements.

**Result:** HiddenDetect demonstrates superior performance over state-of-the-art methods in detecting jailbreak attacks in LVLMs.

**Limitations:** 

**Conclusion:** The findings suggest that intrinsic safety-aware patterns in LVLMs can be efficiently utilized to bolster model safety without extensive fine-tuning.

**Abstract:** The integration of additional modalities increases the susceptibility of large vision-language models (LVLMs) to safety risks, such as jailbreak attacks, compared to their language-only counterparts. While existing research primarily focuses on post-hoc alignment techniques, the underlying safety mechanisms within LVLMs remain largely unexplored. In this work , we investigate whether LVLMs inherently encode safety-relevant signals within their internal activations during inference. Our findings reveal that LVLMs exhibit distinct activation patterns when processing unsafe prompts, which can be leveraged to detect and mitigate adversarial inputs without requiring extensive fine-tuning. Building on this insight, we introduce HiddenDetect, a novel tuning-free framework that harnesses internal model activations to enhance safety. Experimental results show that {HiddenDetect} surpasses state-of-the-art methods in detecting jailbreak attacks against LVLMs. By utilizing intrinsic safety-aware patterns, our method provides an efficient and scalable solution for strengthening LVLM robustness against multimodal threats. Our code will be released publicly at https://github.com/leigest519/HiddenDetect.

</details>


### [138] [ParamMute: Suppressing Knowledge-Critical FFNs for Faithful Retrieval-Augmented Generation](https://arxiv.org/abs/2502.15543)

*Pengcheng Huang, Zhenghao Liu, Yukun Yan, Haiyan Zhao, Xiaoyuan Yi, Hao Chen, Zhiyuan Liu, Maosong Sun, Tong Xiao, Ge Yu, Chenyan Xiong*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, Faithfulness, Parametric Knowledge Muting, Benchmarking

**Relevance Score:** 9

**TL;DR:** This paper introduces Parametric Knowledge Muting through FFN Suppression (ParamMute), a new framework to enhance the faithfulness of large language models (LLMs) during retrieval-augmented generation (RAG) by suppressing activation of problematic feed-forward networks.

**Read time:** 22 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of unfaithful generation in LLMs, which can output contradictory information even when accurate external evidence is provided.

**Method:** The paper proposes a framework called ParamMute, which suppresses the activation of certain feed-forward networks associated with unfaithfulness, aiming to improve the model's use of external context.

**Key Contributions:**

	1. Introduction of a novel framework (ParamMute) for improved faithfulness in LLMs
	2. Development of the CoFaithfulQA benchmark for evaluating faithfulness
	3. Identification of key feed-forward networks associated with unfaithful generation

**Result:** ParamMute significantly improves the faithfulness of LLMs as measured by the newly introduced CoFaithfulQA benchmark, showing a marked reduction in reliance on internal parametric memory.

**Limitations:** 

**Conclusion:** By mitigating the dominance of internal knowledge, the proposed framework provides a novel approach to enhance the trustworthiness of LLMs in RAG applications.

**Abstract:** Large language models (LLMs) integrated with retrieval-augmented generation (RAG) have improved factuality by grounding outputs in external evidence. However, they remain susceptible to unfaithful generation, where outputs contradict retrieved context despite its relevance and accuracy. Existing approaches aiming to improve faithfulness primarily focus on enhancing the utilization of external context, but often overlook the persistent influence of internal parametric knowledge during generation. In this work, we investigate the internal mechanisms behind unfaithful generation and identify a subset of mid-to-deep feed-forward networks (FFNs) that are disproportionately activated in such cases. Building on this insight, we propose Parametric Knowledge Muting through FFN Suppression (ParamMute), a framework that improves contextual faithfulness by suppressing the activation of unfaithfulness-associated FFNs and calibrating the model toward retrieved knowledge. To evaluate our approach, we introduce CoFaithfulQA, a benchmark specifically designed to evaluate faithfulness in scenarios where internal knowledge conflicts with accurate external evidence. Experimental results show that ParamMute significantly enhances faithfulness across both CoFaithfulQA and the established ConFiQA benchmark, achieving substantial reductions in reliance on parametric memory. These findings underscore the importance of mitigating internal knowledge dominance and provide a new direction for improving LLM trustworthiness in RAG. All codes are available at https://github.com/OpenBMB/ParamMute.

</details>


### [139] [Language Models Grow Less Humanlike beyond Phase Transition](https://arxiv.org/abs/2502.18802)

*Tatsuya Aoyama, Ethan Wilcox*

**Main category:** cs.CL

**Keywords:** language models, psychometric predictive power, pretraining dynamics, attention heads, phase transitions

**Relevance Score:** 8

**TL;DR:** This paper explores the tipping point in language models' alignment with human reading behavior, hypothesizing a pretraining phase transition as the underlying factor affecting this alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the dynamics of language model pretraining and its impact on alignment with human reading behavior can improve future model training strategies.

**Method:** The authors conducted a series of correlational and causal experiments to investigate the effects of specialized attention heads and phase transitions during pretraining on psychometric predictive power (PPP).

**Key Contributions:**

	1. Proposes a new hypothesis regarding the tipping point in language model training related to phase transitions.
	2. Demonstrates through experiments how attention head specialization can impact model alignment with reading behavior.
	3. Provides insights into the interaction between pretraining dynamics and psychometric predictive power.

**Result:** The results indicate that a pretraining phase transition is responsible for the observed tipping point in PPP, which negatively affects further training dynamics.

**Limitations:** 

**Conclusion:** The findings suggest that attention patterns change due to phase transitions, leading to detrimental learning dynamics that degrade model performance beyond a certain pretraining threshold.

**Abstract:** LMs' alignment with human reading behavior (i.e. psychometric predictive power; PPP) is known to improve during pretraining up to a tipping point, beyond which it either plateaus or degrades. Various factors, such as word frequency, recency bias in attention, and context size, have been theorized to affect PPP, yet there is no current account that explains why such a tipping point exists, and how it interacts with LMs' pretraining dynamics more generally. We hypothesize that the underlying factor is a pretraining phase transition, characterized by the rapid emergence of specialized attention heads. We conduct a series of correlational and causal experiments to show that such a phase transition is responsible for the tipping point in PPP. We then show that, rather than producing attention patterns that contribute to the degradation in PPP, phase transitions alter the subsequent learning dynamics of the model, such that further training keeps damaging PPP.

</details>


### [140] [RAPID: Long-Context Inference with Retrieval-Augmented Speculative Decoding](https://arxiv.org/abs/2502.20330)

*Guanzheng Chen, Qilong Feng, Jinjie Ni, Xin Li, Michael Qizhe Shieh*

**Main category:** cs.CL

**Keywords:** long-context LLMs, speculative decoding, retrieval-augmented generation

**Relevance Score:** 9

**TL;DR:** RAPID introduces Retrieval-Augmented Speculative Decoding to enhance long-context LLM performance and efficiency, achieving significant speedups and quality improvements.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the efficiency challenges in long-context LLM inference while improving generation quality.

**Method:** The paper presents Retrieval-Augmented Speculative Decoding (RAPID), a model combining retrieval-augmented generation with speculative decoding, utilizing draft LLMs for shortened retrieval contexts to inform long-context LLMs.

**Key Contributions:**

	1. Introduction of RAPID for long-context inference efficiency
	2. Demonstrated significant speedups and performance improvements
	3. Development of inference-time knowledge transfer enhancing target distribution.

**Result:** RAPID demonstrates significant performance improvements, achieving a score increase from 39.33 to 42.83 on the InfiniteBench benchmark for LLaMA-3.1-8B, while also providing over 2x speedups in long-context inference.

**Limitations:** 

**Conclusion:** RAPID effectively integrates the capabilities of RAG and long-context LLMs, showcasing robustness across various context lengths and retrieval quality.

**Abstract:** The emergence of long-context large language models (LLMs) offers a promising alternative to traditional retrieval-augmented generation (RAG) for processing extensive documents. However, the computational overhead of long-context inference presents significant efficiency challenges. While Speculative Decoding (SD) traditionally accelerates inference using smaller draft models, its effectiveness diminishes substantially in long-context scenarios due to memory-bound KV cache operations. We introduce Retrieval-Augmented Speculative Decoding (RAPID), which leverages RAG for both accelerating and enhancing generation quality in long-context inference. RAPID introduces the RAG drafter-a draft LLM operating on shortened retrieval contexts-to speculate on the generation of long-context target LLMs. Our approach enables a new paradigm where same-scale or even larger LLMs can serve as RAG drafters while maintaining computational efficiency. To fully leverage the potentially superior capabilities from stronger RAG drafters, we develop an inference-time knowledge transfer that enriches the target distribution by RAG. Extensive experiments on the LLaMA-3.1 and Qwen2.5 backbones demonstrate that RAPID effectively integrates the strengths of both RAG and long-context LLMs, achieving significant performance improvements (e.g., from 39.33 to 42.83 on InfiniteBench for LLaMA-3.1-8B) with more than 2x speedups for long-context inference. Our analyses also reveal the robustness of RAPID across various context lengths and retrieval quality.

</details>


### [141] [Enhancing LLM Knowledge Learning through Generalization](https://arxiv.org/abs/2503.03705)

*Mingkang Zhu, Xi Chen, Zhongdao Wang, Bei Yu, Hengshuang Zhao, Jiaya Jia*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Acquisition, Data Augmentation, Machine Learning, Optimization

**Relevance Score:** 9

**TL;DR:** This paper explores strategies to improve large language models' (LLMs) factual knowledge acquisition by using formatting-based data augmentation and sharpness-aware optimization to enhance generalization across diverse paraphrased contexts.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Integrating evolving factual knowledge into large language models while maintaining factual integrity is a significant challenge as they continue to be deployed in various applications.

**Method:** The authors propose two strategies: 1) Formatting-based data augmentation, which changes the appearance of documents while keeping their content unchanged; 2) Using sharpness-aware minimization for better optimization during training.

**Key Contributions:**

	1. Introduction of formatting-based data augmentation to maintain factual integrity
	2. Adoption of sharpness-aware minimization for enhanced optimization
	3. Demonstrated effectiveness through extensive experimental results

**Result:** Experiments show that the proposed methods significantly enhance LLMs' predictive accuracy for factual knowledge across varied contexts, improving both pre-training and instruction tuning outcomes.

**Limitations:** 

**Conclusion:** Using formatting-based data augmentation and an optimized training approach increases LLMsâ ability to accurately predict factual knowledge, thereby aiding in their application in real-world scenarios.

**Abstract:** As Large language models (LLMs) are increasingly deployed in diverse applications, faithfully integrating evolving factual knowledge into these models remains a critical challenge. Continued pre-training on paraphrased data has shown empirical promise for enhancing knowledge acquisition. However, this approach is often costly and unreliable, as it relies on external models or manual effort for rewriting, and may inadvertently alter the factual content. In this work, we hypothesize and empirically show that an LLM's ability to continually predict the same factual knowledge tokens given diverse paraphrased contexts is positively correlated with its capacity to extract that knowledge via question-answering. Based on this view and aiming to improve generalization to diverse paraphrased contexts, we introduce two strategies to enhance LLMs' ability to predict the same knowledge tokens given varied contexts, thereby enhancing knowledge acquisition. First, we propose formatting-based data augmentation, which diversifies documents conveying the same knowledge by altering document formats rather than their content, thereby preserving factual integrity. Second, we adopt sharpness-aware minimization as the optimizer to better improve generalization. Extensive experiments demonstrate our methods' effectiveness in both continued pre-training and instruction tuning, and further gains can be achieved by combining with paraphrased data.

</details>


### [142] [HiRAG: Retrieval-Augmented Generation with Hierarchical Knowledge](https://arxiv.org/abs/2503.10150)

*Haoyu Huang, Yongfeng Huang, Junjie Yang, Zhenyu Pan, Yongqiang Chen, Kaili Ma, Hongzhi Chen, James Cheng*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Hierarchical Knowledge, Large Language Models

**Relevance Score:** 9

**TL;DR:** HiRAG is a new RAG approach that enhances semantic understanding using hierarchical knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of RAG systems by incorporating hierarchical knowledge, addressing limitations of existing methods in utilizing human cognitive structures.

**Method:** Introduction of a new RAG framework, HiRAG, that utilizes hierarchical knowledge for improved indexing and retrieval processes.

**Key Contributions:**

	1. Introduction of HiRAG for enhanced RAG performance
	2. Utilization of hierarchical knowledge in retrieval processes
	3. Demonstrated performance improvements over existing methods

**Result:** Extensive experiments show that HiRAG significantly outperforms state-of-the-art baseline methods in performance.

**Limitations:** 

**Conclusion:** HiRAG provides a more effective approach for RAG by leveraging the hierarchical structure of knowledge, leading to better outcomes in domain-specific tasks.

**Abstract:** Graph-based Retrieval-Augmented Generation (RAG) methods have significantly enhanced the performance of large language models (LLMs) in domain-specific tasks. However, existing RAG methods do not adequately utilize the naturally inherent hierarchical knowledge in human cognition, which limits the capabilities of RAG systems. In this paper, we introduce a new RAG approach, called HiRAG, which utilizes hierarchical knowledge to enhance the semantic understanding and structure capturing capabilities of RAG systems in the indexing and retrieval processes. Our extensive experiments demonstrate that HiRAG achieves significant performance improvements over the state-of-the-art baseline methods.

</details>


### [143] [Learning from Reference Answers: Versatile Language Model Alignment without Binary Human Preference Data](https://arxiv.org/abs/2504.09895)

*Shuai Zhao, Linchao Zhu, Yi Yang*

**Main category:** cs.CL

**Keywords:** large language models, alignment, reward modeling, BERTScore, similarity metrics

**Relevance Score:** 9

**TL;DR:** RefAlign is an alignment algorithm for large language models that uses similarity metrics as a reward function instead of binary preference data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To find an efficient way to align large language models without relying on resource-intensive binary preference data collection and reward modeling.

**Method:** The paper presents RefAlign, a REINFORCE-style algorithm that utilizes similarity metrics like BERTScore between generated outputs and reference answers as a means of generating surrogate rewards for alignment.

**Key Contributions:**

	1. Introduction of RefAlign algorithm for LLM alignment
	2. Utilization of similarity metrics for reward functions
	3. Demonstrated performance comparable to existing methods without binary preference data

**Result:** RefAlign shows comparable performance to existing alignment methods, achieving effective model alignment without the need for binary preference data or reward models.

**Limitations:** Work in progress; further validation and performance comparisons needed in broader contexts.

**Conclusion:** The approach offers a viable alternative for aligning large language models across various scenarios while simplifying the data collection process.

**Abstract:** Large language models~(LLMs) are expected to be helpful, harmless, and honest. In alignment scenarios such as safety, confidence, and general preference alignment, binary preference data collection and reward modeling are resource-intensive but essential for transferring human preference. In this work, we explore using the similarity between sampled generations and high-quality reference answers as an alternative reward function choice for LLM alignment. Similarity reward circumvents binary preference data collection and reward modeling when unary high-quality reference answers are available. We introduce \textit{RefAlign}, a versatile REINFORCE-style alignment algorithm that does not rely on reference or reward models. RefAlign utilizes similarity metrics, such as BERTScore between sampled generations and reference answers as surrogate rewards. Beyond general human preference optimization, RefAlign can be readily extended to diverse scenarios, such as safety and confidence alignment, by incorporating the similarity reward with task-related objectives. In various scenarios, RefAlign demonstrates comparable performance to previous alignment methods without binary preference data and reward models.

</details>


### [144] [Improving the Efficiency of Long Document Classification using Sentence Ranking Approach](https://arxiv.org/abs/2506.07248)

*Prathamesh Kokate, Mitali Sarnaik, Manavi Khopade, Raviraj Joshi*

**Main category:** cs.CL

**Keywords:** document classification, TF-IDF, sentence ranking, long documents, MahaBERT-v2

**Relevance Score:** 6

**TL;DR:** A TF-IDF-based sentence ranking method improves long document classification efficiency by selecting the most informative sentences, achieving near-full performance with reduced input size and latency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Address challenges in long document classification due to computational limitations of transformer models, particularly with regard to fixed input lengths and redundant full document usage.

**Method:** TF-IDF-based sentence ranking to enhance efficiency by selecting informative content through fixed-count and percentage-based sentence selection and a new scoring strategy combining TF-IDF scores and sentence length.

**Key Contributions:**

	1. Proposed a novel TF-IDF-based sentence ranking method for document classification
	2. Demonstrated significant reduction in input size and latency without accuracy loss
	3. Evaluated on the MahaNews LDC dataset with promising results

**Result:** Using MahaBERT-v2, the method achieves classification accuracy with only a 0.33% drop compared to the full-context baseline while reducing input size by over 50% and inference latency by 43%.

**Limitations:** 

**Conclusion:** Significant context reduction for long document classification is feasible without compromising performance, making the method practical for real-world applications.

**Abstract:** Long document classification poses challenges due to the computational limitations of transformer-based models, particularly BERT, which are constrained by fixed input lengths and quadratic attention complexity. Moreover, using the full document for classification is often redundant, as only a subset of sentences typically carries the necessary information. To address this, we propose a TF-IDF-based sentence ranking method that improves efficiency by selecting the most informative content. Our approach explores fixed-count and percentage-based sentence selection, along with an enhanced scoring strategy combining normalized TF-IDF scores and sentence length. Evaluated on the MahaNews LDC dataset of long Marathi news articles, the method consistently outperforms baselines such as first, last, and random sentence selection. With MahaBERT-v2, we achieve near-identical classification accuracy with just a 0.33 percent drop compared to the full-context baseline, while reducing input size by over 50 percent and inference latency by 43 percent. This demonstrates that significant context reduction is possible without sacrificing performance, making the method practical for real-world long document classification tasks.

</details>


### [145] [LGAI-EMBEDDING-Preview Technical Report](https://arxiv.org/abs/2506.07438)

*Jooyoung Choi, Hyun Kim, Hansol Jang, Changwook Jun, Kyunghoon Bae, Hyewon Choi, Stanley Jungkyu Choi, Honglak Lee, Chulmin Yun*

**Main category:** cs.CL

**Keywords:** text embeddings, information retrieval, large language model, soft supervision, adaptive mining

**Relevance Score:** 8

**TL;DR:** A unified framework for learning generalized text embeddings optimized for both information retrieval and non-IR tasks, utilizing a decoder-only language model and combining various learning strategies for robust performance across multiple benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create efficient and effective generalized text embeddings that perform well across diverse tasks without the need for task-specific fine-tuning.

**Method:** The approach utilizes Mistral-7B, incorporating in-context learning, soft supervision, and adaptive hard-negative mining to generate context-aware embeddings.

**Key Contributions:**

	1. Unified instruction-based framework for text embeddings
	2. Use of soft labeling with continuous relevance scores
	3. Adaptive margin-based hard-negative mining for improved training stability

**Result:** The model achieved strong generalization on the MTEB benchmark, ranking among the top-performing models and outperforming various larger or fully fine-tuned baselines.

**Limitations:** 

**Conclusion:** The study demonstrates that a combination of in-context prompting, soft supervision, and adaptive sampling leads to scalable, high-quality embedding generation.

**Abstract:** This report presents a unified instruction-based framework for learning generalized text embeddings optimized for both information retrieval (IR) and non-IR tasks. Built upon a decoder-only large language model (Mistral-7B), our approach combines in-context learning, soft supervision, and adaptive hard-negative mining to generate context-aware embeddings without task-specific fine-tuning. Structured instructions and few-shot examples are used to guide the model across diverse tasks, enabling strong performance on classification, semantic similarity, clustering, and reranking benchmarks. To improve semantic discrimination, we employ a soft labeling framework where continuous relevance scores, distilled from a high-performance dense retriever and reranker, serve as fine-grained supervision signals. In addition, we introduce adaptive margin-based hard-negative mining, which filters out semantically ambiguous negatives based on their similarity to positive examples, thereby enhancing training stability and retrieval robustness. Our model is evaluated on the newly introduced MTEB (English, v2) benchmark, covering 41 tasks across seven categories. Results show that our method achieves strong generalization and ranks among the top-performing models by Borda score, outperforming several larger or fully fine-tuned baselines. These findings highlight the effectiveness of combining in-context prompting, soft supervision, and adaptive sampling for scalable, high-quality embedding generation.

</details>
