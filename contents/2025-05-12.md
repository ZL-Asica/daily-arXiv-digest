# 2025-05-12

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 14]

- [cs.CL](#cs.CL) [Total: 42]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Would You Rely on an Eerie Agent? A Systematic Review of the Impact of the Uncanny Valley Effect on Trust in Human-Agent Interaction](https://arxiv.org/abs/2505.05543)

*Ahdiyeh Alipour, Tilo Hartmann, Maryam Alimardani*

**Main category:** cs.HC

**Keywords:** uncanny valley effect, trust, human-likeness, affinity response, human-agent interaction

**Relevance Score:** 8

**TL;DR:** This review explores the relationship between the Uncanny Valley Effect (UVE) and trust in human-agent interactions, analyzing empirical studies to identify methodological patterns and gaps.

**Read time:** 75 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how the Uncanny Valley Effect influences trust in artificial agents, given its implications for human-agent interactions in an increasingly automated world.

**Method:** Systematic review following PRISMA guidelines, analyzing 53 empirical studies related to UVE and trust within various contexts.

**Key Contributions:**

	1. Identification of methodological gaps in the study of UVE and trust
	2. Framework for classifying trust measurement approaches
	3. First systematic review mapping the intersection of UVE and trust

**Result:** The review finds that most studies use static images or hypothetical scenarios, with limited real-time interaction, primarily measuring subjective trust.

**Limitations:** Most studies depend on non-interactive scenarios and subjective measures that may not reflect real-world interactions.

**Conclusion:** This review offers a novel framework for classifying trust measurement approaches, highlighting the need for better empirical methods and setting the groundwork for future research.

**Abstract:** Trust is a fundamental component of human-agent interaction. With the increasing presence of artificial agents in daily life, it is essential to understand how people perceive and trust these agents. One of the key challenges affecting this perception is the Uncanny Valley Effect (UVE), where increasingly human-like artificial beings can be perceived as eerie or repelling. Despite growing interest in trust and the UVE, existing research varies widely in terms of how these concepts are defined and operationalized. This inconsistency raises important questions about how and under what conditions the UVE influences trust in agents. A systematic understanding of their relationship is currently lacking. This review aims to examine the impact of the UVE on human trust in agents and to identify methodological patterns, limitations, and gaps in the existing empirical literature. Following PRISMA guidelines, a systematic search identified 53 empirical studies that investigated both UVE-related constructs and trust or trust-related outcomes. Studies were analyzed based on a structured set of categories, including types of agents and interactions, methodological and measurement approaches, and key findings. The results of our systematic review reveal that most studies rely on static images or hypothetical scenarios with limited real-time interaction, and the majority use subjective trust measures. This review offers a novel framework for classifying trust measurement approaches with regard to the best-practice criteria for empirically investigating the UVE. As the first systematic attempt to map the intersection of UVE and trust, this review contributes to a deeper understanding of their interplay and offers a foundation for future research. Keywords: the uncanny valley effect, trust, human-likeness, affinity response, human-agent interaction

</details>


### [2] [Not Like Us, Hunty: Measuring Perceptions and Behavioral Effects of Minoritized Anthropomorphic Cues in LLMs](https://arxiv.org/abs/2505.05660)

*Jeffrey Basoah, Daniel Chechelnitsky, Tao Long, Katharina Reinecke, Chrysoula Zerva, Kaitlyn Zhou, Mark DÃ­az, Maarten Sap*

**Main category:** cs.HC

**Keywords:** large language models, sociolects, user experience, trust, cultural appropriateness

**Relevance Score:** 9

**TL;DR:** This paper explores the impact of large language models (LLMs) using sociolects on user trust and satisfaction among speakers of African American English and Queer slang.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs, there is a risk of appropriating specific linguistic styles associated with minoritized cultures, leading to questions about user trust and satisfaction.

**Method:** User studies were conducted with 498 African American English speakers and 487 Queer slang speakers who interacted with LLMs providing responses in either their sociolect or standard American English (SAE).

**Key Contributions:**

	1. Examined the effects of sociolect usage in LLM interactions on user perception and trust.
	2. Highlighted the unexpected reliance and trust dynamics in responses from different linguistic styles.
	3. Called for thoughtful design of LLMs to respect cultural nuances and enhance user engagement.

**Result:** The findings revealed that both groups generally relied more on the SAE responses and perceived them more positively, while preferences varied; Queer slang speakers connected more with the Queer slang agent, whereas AAE speakers preferred the SAE agent.

**Limitations:** The study's results may not generalize beyond the specific user groups and tasks examined.

**Conclusion:** The results indicate that personalization in LLMs does not inherently lead to increased user reliance or satisfaction, highlighting the need for careful design that respects cultural and linguistic boundaries.

**Abstract:** As large language models (LLMs) increasingly adapt and personalize to diverse sets of users, there is an increased risk of systems appropriating sociolects, i.e., language styles or dialects that are associated with specific minoritized lived experiences (e.g., African American English, Queer slang). In this work, we examine whether sociolect usage by an LLM agent affects user reliance on its outputs and user perception (satisfaction, frustration, trust, and social presence). We designed and conducted user studies where 498 African American English (AAE) speakers and 487 Queer slang speakers performed a set of question-answering tasks with LLM-based suggestions in either standard American English (SAE) or their self-identified sociolect. Our findings showed that sociolect usage by LLMs influenced both reliance and perceptions, though in some surprising ways. Results suggest that both AAE and Queer slang speakers relied more on the SAE agent, and had more positive perceptions of the SAE agent. Yet, only Queer slang speakers felt more social presence from the Queer slang agent over the SAE one, whereas only AAE speakers preferred and trusted the SAE agent over the AAE one. These findings emphasize the need to test for behavioral outcomes rather than simply assume that personalization would lead to a better and safer reliance outcome. They also highlight the nuanced dynamics of minoritized language in machine interactions, underscoring the need for LLMs to be carefully designed to respect cultural and linguistic boundaries while fostering genuine user engagement and trust.

</details>


### [3] [Extending Stress Detection Reproducibility to Consumer Wearable Sensors](https://arxiv.org/abs/2505.05694)

*Ohida Binte Amin, Varun Mishra, Tinashe M. Tapera, Robert Volpe, Aarti Sathyanarayana*

**Main category:** cs.HC

**Keywords:** wearable sensors, stress detection, HRV, electrodermal activity, reproducibility

**Relevance Score:** 8

**TL;DR:** This study evaluates the reproducibility of stress detection models using consumer wearable sensors compared to research-grade devices, highlighting the performance variability and implications for real-world application.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the reproducibility of stress detection models across different wearable devices and settings, addressing the gap in previous studies that focused on single datasets and lack of cross-device generalizability.

**Method:** The study involved validated research devices (Biopac MP160, Polar H10, Empatica E4) and consumer wearables (Garmin Forerunner 55s), with 35 undergraduate students completing standardized stress-induction tasks to evaluate device-specific performance in stress detection.

**Key Contributions:**

	1. Evaluation of stress detection accuracy across a range of wearable sensors
	2. Identification of performance variability and generalizability issues in stress detection models
	3. Highlighting the potential of consumer wearables for real-time stress monitoring

**Result:** Biopac MP160 outperformed other devices, while the Garmin Forerunner 55s demonstrated strong performance in real-world scenarios. Combining heart rate variability and EDA improved stress prediction overall, but device-specific limitations affected the reliability of certain setups.

**Limitations:** The study mainly assessed a limited number of consumer devices and specific stress-induction tasks, which may not represent all possible scenarios or devices.

**Conclusion:** Consumer wearables can effectively monitor stress in real-world settings, though device-specific performance and compatibility with pre-trained models present challenges that need to be addressed for broader application.

**Abstract:** Wearable sensors are widely used to collect physiological data and develop stress detection models. However, most studies focus on a single dataset, rarely evaluating model reproducibility across devices, populations, or study conditions. We previously assessed the reproducibility of stress detection models across multiple studies, testing models trained on one dataset against others using heart rate (with R-R interval) and electrodermal activity (EDA). In this study, we extended our stress detection reproducibility to consumer wearable sensors. We compared validated research-grade devices, to consumer wearables - Biopac MP160, Polar H10, Empatica E4, to the Garmin Forerunner 55s, assessing device-specific stress detection performance by conducting a new stress study on undergraduate students. Thirty-five students completed three standardized stress-induction tasks in a lab setting. Biopac MP160 performed the best, being consistent with our expectations of it as the gold standard, though performance varied across devices and models. Combining heart rate variability (HRV) and EDA enhanced stress prediction across most scenarios. However, Empatica E4 showed variability; while HRV and EDA improved stress detection in leave-one-subject-out (LOSO) evaluations (AUROC up to 0.953), device-specific limitations led to underperformance when tested with our pre-trained stress detection tool (AUROC 0.723), highlighting generalizability challenges related to hardware-model compatibility. Garmin Forerunner 55s demonstrated strong potential for real-world stress monitoring, achieving the best mental arithmetic stress detection performance in LOSO (AUROC up to 0.961) comparable to research-grade devices like Polar H10 (AUROC 0.954), and Empatica E4 (AUROC 0.905 with HRV-only model and AUROC 0.953 with HRV+EDA model), with the added advantage of consumer-friendly wearability for free-living contexts.

</details>


### [4] [A Day in Their Shoes: Using LLM-Based Perspective-Taking Interactive Fiction to Reduce Stigma Toward Dirty Work](https://arxiv.org/abs/2505.05786)

*Xiangzhe Yuan, Jiajun Wang, Qian Wan, Siying Hu*

**Main category:** cs.HC

**Keywords:** Interactive Fiction, Large Language Models, Stigma, Empathy, Occupational Equity

**Relevance Score:** 7

**TL;DR:** This study presents an Interactive Fiction framework using Large Language Models to reduce stigma associated with 'dirty work' occupations through perspective-taking, demonstrating significant increases in understanding and empathy among participants.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the social stigma faced by 'dirty work' occupations, which negatively impacts the mental health of workers and challenges occupational equity.

**Method:** An interactive fiction framework powered by large language models was tested with 100 participants across four stigmatized occupations to assess empathetic engagement.

**Key Contributions:**

	1. Introduction of an LLM-powered Interactive Fiction framework for perspective-taking.
	2. Demonstrated empirical evidence of increased empathy and understanding for stigmatized professions.
	3. Identified both the potential and limitations of LLMs in promoting social equity.

**Result:** Participants showed a significant increase in understanding, empathy, and connection to workers in these roles, although some limitations in LLM context were noted.

**Limitations:** Participants noted limited contextual details from the LLM and potential reinforcement of stereotypes.

**Conclusion:** The LLM-based framework is seen as a viable method for reducing stigma and promoting social equity in marginalized professions, despite some challenges in context generation.

**Abstract:** Occupations referred to as "dirty work" often face entrenched social stigma, which adversely affects the mental health of workers in these fields and impedes occupational equity. In this study, we propose a novel Interactive Fiction (IF) framework powered by Large Language Models (LLMs) to encourage perspective-taking and reduce biases against these stigmatized yet essential roles. Through an experiment with participants (n = 100) across four such occupations, we observed a significant increase in participants' understanding of these occupations, as well as a high level of empathy and a strong sense of connection to individuals in these roles. Additionally, qualitative interviews with participants (n = 15) revealed that the LLM-based perspective-taking IF enhanced immersion, deepened emotional resonance and empathy toward "dirty work," and allowed participants to experience a sense of professional fulfillment in these occupations. However, participants also highlighted ongoing challenges, such as limited contextual details generated by the LLM and the unintentional reinforcement of existing stereotypes. Overall, our findings underscore that an LLM-based perspective-taking IF framework offers a promising and scalable strategy for mitigating stigma and promoting social equity in marginalized professions.

</details>


### [5] [The Experience of Running: Recommending Routes Using Sensory Mapping in Urban Environments](https://arxiv.org/abs/2505.05817)

*Katrin HÃ¤nsel, Luca Maria Aiello, Daniele Quercia, Rossano Schifanella, Krisztian Zsolt Varga, Linus W. Dietz, Marios Constantinides*

**Main category:** cs.HC

**Keywords:** running experience, routing engine, psychological well-being, experience sampling, path recommendations

**Relevance Score:** 5

**TL;DR:** The paper explores the psychological experiences of runners through a mixed-method study, identifying key themes related to running experiences and developing a routing engine for path recommendations based on these themes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the unresolved task of finding running routes that foster positive psychological experiences for runners.

**Method:** A mixed-method study involving interviews with 7 runners followed by a quantitative survey of 387 runners, utilizing Principal Component Analysis to identify themes and dimensions of running experience.

**Key Contributions:**

	1. Identification of key themes in runners' psychological experiences
	2. Development of a short experience sampling questionnaire for running
	3. Creation of a routing engine for path recommendations based on runner preferences

**Result:** Developed a short experience sampling questionnaire capturing three dimensions of running experience and clustered path preferences into scenic and urban types for routing recommendations.

**Limitations:** Limited to runners in the study; may not generalize to all runners or regions.

**Conclusion:** The paper discusses the challenges in developing the routing engine and provides guidelines for integrating it into mobile and wearable running apps.

**Abstract:** Depending on the route, runners may experience frustration, freedom, or fulfilment. However, finding routes that are conducive to the psychological experience of running remains an unresolved task in the literature. In a mixed-method study, we interviewed 7 runners to identify themes contributing to running experience, and quantitatively examined these themes in an online survey with 387 runners. Using Principal Component Analysis on the survey responses, we developed a short experience sampling questionnaire that captures the three most important dimensions of running experience: \emph{performance \& achievement}, \emph{environment}, and \emph{mind \& social connectedness}. Using path preferences obtained from the online survey, we clustered them into two types of routes: \emph{scenic} (associated with nature and greenery) and \emph{urban} (characterized by the presence of people); and developed a routing engine for path recommendations. We discuss challenges faced in developing the routing engine, and provide guidelines to integrate it into mobile and wearable running apps.

</details>


### [6] [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)

*Alba MarÃ­a MÃ¡rmol-Romero, Manuel GarcÃ­a-Vega, Miguel Ãngel GarcÃ­a-Cumbreras, Arturo Montejo-RÃ¡ez*

**Main category:** cs.HC

**Keywords:** chatbot, mental health, self-disclosure, GPT-3, teenagers

**Relevance Score:** 7

**TL;DR:** A chatbot system engages young Spanish users in mental disorder awareness through guided self-disclosure and open conversations using the GPT-3 language model.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To raise awareness among teenagers about mental disorders using an interactive and empathetic approach.

**Method:** A chatbot combines closed and open dialogue techniques to guide conversations about specific mental disorders based on user's responses and sensibility.

**Key Contributions:**

	1. Introduce a chatbot-based system for mental disorder awareness in teenagers
	2. Utilize self-disclosure techniques to engage users
	3. Implement a dialogue engine that combines structured and open conversations

**Result:** The chatbot effectively captured the interest of young users and facilitated discussions about mental disorders, promoting awareness.

**Limitations:** Study is limited to a specific age group (12-18 years) and focused solely on Spanish youth.

**Conclusion:** Chatbot systems can be valuable tools for mental health awareness among teenagers by fostering empathetic communication.

**Abstract:** This paper presents a chatbot-based system to engage young Spanish people in the awareness of certain mental disorders through a self-disclosure technique. The study was carried out in a population of teenagers aged between 12 and 18 years. The dialogue engine mixes closed and open conversations, so certain controlled messages are sent to focus the chat on a specific disorder, which will change over time. Once a set of trial questions is answered, the system can initiate the conversation on the disorder under the focus according to the user's sensibility to that disorder, in an attempt to establish a more empathetic communication. Then, an open conversation based on the GPT-3 language model is initiated, allowing the user to express themselves with more freedom. The results show that these systems are of interest to young people and could help them become aware of certain mental disorders.

</details>


### [7] [Augmented Body Communicator: Enhancing daily body expression for people with upper limb limitations through LLM and a robotic arm](https://arxiv.org/abs/2505.05832)

*Songchen Zhou, Mark Armstrong, Giulia Barbareschi, Toshihiro Ajioka, Zheng Hu, Ryoichi Ando, Kentaro Yoshifuji, Masatane Muto, Kouta Minamizawa*

**Main category:** cs.HC

**Keywords:** augmented communication, robotic arms, body language, human-robot interaction, assistive technology

**Relevance Score:** 9

**TL;DR:** The paper presents the Augmented Body Communicator system that enhances body language in social interactions for individuals with upper limb movement limitations using robotic arms and a large language model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by individuals with upper limb movement limitations in social interactions and to explore the potential of robotic arms in enhancing body language.

**Method:** The system integrates robotic arms and a large language model to allow users to collaboratively design actions, using contextual cues to suggest suitable actions during interactions.

**Key Contributions:**

	1. Introduction of the Augmented Body Communicator system combining robotics and LLM.
	2. User testing demonstrates improved expression capabilities for participants with upper limb limitations.
	3. Recommendations for future development of robotic systems that assist with body language.

**Result:** User testing with six participants showed that the system improved their capability to express themselves during social interactions.

**Limitations:** 

**Conclusion:** The findings suggest that robotic arms can be developed to better support body language capabilities alongside functional tasks for disabled individuals.

**Abstract:** Individuals with upper limb movement limitations face challenges in interacting with others. Although robotic arms are currently used primarily for functional tasks, there is considerable potential to explore ways to enhance users' body language capabilities during social interactions. This paper introduces an Augmented Body Communicator system that integrates robotic arms and a large language model. Through the incorporation of kinetic memory, disabled users and their supporters can collaboratively design actions for the robot arm. The LLM system then provides suggestions on the most suitable action based on contextual cues during interactions. The system underwent thorough user testing with six participants who have conditions affecting upper limb mobility. Results indicate that the system improves users' ability to express themselves. Based on our findings, we offer recommendations for developing robotic arms that support disabled individuals with body language capabilities and functional tasks.

</details>


### [8] [Human causal perception in a cube-stacking task](https://arxiv.org/abs/2505.05923)

*Nikolai Bahr, Christoph Zetzsche, Jaime Maldonado*

**Main category:** cs.HC

**Keywords:** intuitive physics, stability judgments, 3D decision making, human decision behavior, cube stacking

**Relevance Score:** 4

**TL;DR:** This paper investigates the stability judgments of stacked cubes in a full 3D setting, revealing more complex human decision behavior.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore the intuitive physics of stability judgments in a fuller dimensional context, beyond the traditional 1D settings.

**Method:** An experiment was conducted where participants judged the stability of two cubes stacked vertically in a 3D environment, utilizing a 2D decision surface for analysis.

**Key Contributions:**

	1. Investigates stability judgments in 3D instead of 1D.
	2. Finds a rotated square shape for perceived stability area.
	3. Challenges assumptions about human decision behavior in intuitive physics.

**Result:** The analysis showed a rotated square shape for the perceived stability area, contrasting with the expected safety margin typically observed in 1D environments.

**Limitations:** The experiment only focuses on a specific configuration of two cubes and may not generalize to more complex stacking scenarios.

**Conclusion:** These findings suggest a more intricate understanding of human decision-making regarding physical stability than previously thought, indicating that human perceptual judgments are not merely linear.

**Abstract:** In intuitive physics the process of stacking cubes has become a paradigmatic, canonical task. Even though it gets employed in various shades and complexities, the very fundamental setting with two cubes has not been thoroughly investigated. Furthermore, the majority of settings feature only a reduced, one dimensional (1D) decision space. In this paper an experiment is conducted in which participants judge the stability of two cubes stacked on top of each other. It is performed in the full 3D setting which features a 2D decision surface. The analysis yield a shape of a rotated square for the perceived stability area instead of the commonly reported safety margin in 1D. This implies a more complex decision behavior in human than previously assumed.

</details>


### [9] [MER-CLIP: AU-Guided Vision-Language Alignment for Micro-Expression Recognition](https://arxiv.org/abs/2505.05937)

*Shifeng Liu, Xinglong Mao, Sirui Zhao, Peiming Li, Tong Xu, Enhong Chen*

**Main category:** cs.HC

**Keywords:** micro-expressions, CLIP model, facial action units, data augmentation, emotion recognition

**Relevance Score:** 6

**TL;DR:** The paper proposes a novel approach for micro-expression recognition (MER) utilizing the CLIP model for better semantic alignment, along with a new data augmentation strategy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Micro-expressions reveal genuine emotions and have applications in criminal investigations and psychological diagnoses, but current recognition methods suffer from performance limitations due to inadequate use of facial action units (AUs).

**Method:** The authors propose MER-CLIP, which integrates CLIP's cross-modal capabilities to convert AU labels into textual descriptions that guide ME learning. Additionally, they introduce an Emotion Inference Module for improved emotional understanding and use a data augmentation strategy called LocalStaticFaceMix to enhance ME data.

**Key Contributions:**

	1. Introduction of MER-CLIP for micro-expression recognition using CLIP model.
	2. Development of an Emotion Inference Module for enhanced understanding of emotional cues from micro-expressions.
	3. Implementation of LocalStaticFaceMix data augmentation strategy to improve data quality.

**Result:** Comprehensive experiments indicate that MER-CLIP significantly outperforms previous methods, achieving UF1 scores of 0.7832, 0.6544, and 0.4997 on CAS(ME)3 across different classification tasks.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates superior performance in micro-expression recognition by leveraging advanced semantic understanding and effective data augmentation.

**Abstract:** As a critical psychological stress response, micro-expressions (MEs) are fleeting and subtle facial movements revealing genuine emotions. Automatic ME recognition (MER) holds valuable applications in fields such as criminal investigation and psychological diagnosis. The Facial Action Coding System (FACS) encodes expressions by identifying activations of specific facial action units (AUs), serving as a key reference for ME analysis. However, current MER methods typically limit AU utilization to defining regions of interest (ROIs) or relying on specific prior knowledge, often resulting in limited performance and poor generalization. To address this, we integrate the CLIP model's powerful cross-modal semantic alignment capability into MER and propose a novel approach namely MER-CLIP. Specifically, we convert AU labels into detailed textual descriptions of facial muscle movements, guiding fine-grained spatiotemporal ME learning by aligning visual dynamics and textual AU-based representations. Additionally, we introduce an Emotion Inference Module to capture the nuanced relationships between ME patterns and emotions with higher-level semantic understanding. To mitigate overfitting caused by the scarcity of ME data, we put forward LocalStaticFaceMix, an effective data augmentation strategy blending facial images to enhance facial diversity while preserving critical ME features. Finally, comprehensive experiments on four benchmark ME datasets confirm the superiority of MER-CLIP. Notably, UF1 scores on CAS(ME)3 reach 0.7832, 0.6544, and 0.4997 for 3-, 4-, and 7-class classification tasks, significantly outperforming previous methods.

</details>


### [10] [Designing RoutScape: Geospatial Prototyping with XR for Flood Evacuation Planning](https://arxiv.org/abs/2505.06045)

*Johndayll Lewis Arizala, Joshua Permito, Steven Errol Escopete, John Kovie NiÃ±o, Jordan Aiko Deja*

**Main category:** cs.HC

**Keywords:** extended reality, disaster management, flood response, user-centered design, narrative design

**Relevance Score:** 5

**TL;DR:** This paper explores the use of extended reality (XR) for effective flood response planning via narrative-driven design, introducing Routscape as an XR prototype for visualizing flood scenarios and evacuation routes.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of fragmented communication in Disaster Risk Reduction and Management (DRRM) councils during flood response planning.

**Method:** The authors developed an XR prototype named Routscape through iterative prototyping and user-centered design, involving DRRM officers to ground the system in real-world experiences and localized narratives.

**Key Contributions:**

	1. Introduction of Routscape, an XR prototype for flood scenario visualization
	2. Demonstration of narrative-driven design in XR for local disaster management
	3. Highlighting the importance of user-centered design in developing emergency response tools

**Result:** Routscape enables effective visualization of flood scenarios and evacuation routes, facilitating shared understanding and spatial sensemaking among users.

**Limitations:** 

**Conclusion:** The study demonstrates the potential of XR technologies in enhancing disaster preparedness through improved communication and planning tools.

**Abstract:** Flood response planning in local communities is often hindered by fragmented communication across Disaster Risk Reduction and Management (DRRM) councils. In this work, we explore how extended reality (XR) can support more effective planning through narrative-driven design. We present Routscape, an XR prototype for visualizing flood scenarios and evacuation routes, developed through iterative prototyping and user-centered design with DRRM officers. By grounding the system in real-world experiences and localized narratives, we highlight how XR can aid in fostering shared understanding and spatial sensemaking in disaster preparedness efforts.

</details>


### [11] [Context Informed Incremental Learning Improves Myoelectric Control Performance in Virtual Reality Object Manipulation Tasks](https://arxiv.org/abs/2505.06064)

*Gabriel GagnÃ©, Anisha Azad, Thomas LabbÃ©, Evan Campbell, Xavier Isabel, Erik Scheme, Ulysse CÃ´tÃ©-Allard, Benoit Gosselin*

**Main category:** cs.HC

**Keywords:** EMG, gesture recognition, real-time adaptation, Context Informed Incremental Learning, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper presents an approach using Context Informed Incremental Learning (CIIL) for enhancing EMG-based gesture recognition in real-time applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Improve the usability of EMG-based gesture recognition systems in real-world applications affected by declining performance during goal-directed tasks.

**Method:** Implemented Context Informed Incremental Learning (CIIL) in a VR environment with participants completing a task involving object transportation, comparing real-time adaptation to a traditional approach.

**Key Contributions:**

	1. First deployment of CIIL in an object-manipulation scenario for EMG-based interfaces.
	2. Real-time adaptation improved user task efficiency and reduced workload.
	3. Source code made available for community exploration.

**Result:** The CIIL method improved task success rates and efficiency, reducing perceived workload by 7.1 %, despite a 5.8 % decline in offline classification accuracy.

**Limitations:** Decline in offline classification accuracy during adaptation could impact initial deployment effectiveness.

**Conclusion:** Real-time contextual adaptation can significantly enhance user experience and usability of EMG-based systems for practical applications.

**Abstract:** Electromyography (EMG)-based gesture recognition is a promising approach for designing intuitive human-computer interfaces. However, while these systems typically perform well in controlled laboratory settings, their usability in real-world applications is compromised by declining performance during real-time control. This decline is largely due to goal-directed behaviors that are not captured in static, offline scenarios. To address this issue, we use \textit{Context Informed Incremental Learning} (CIIL) - marking its first deployment in an object-manipulation scenario - to continuously adapt the classifier using contextual cues. Nine participants without upper limb differences completed a functional task in a virtual reality (VR) environment involving transporting objects with life-like grips. We compared two scenarios: one where the classifier was adapted in real-time using contextual information, and the other using a traditional open-loop approach without adaptation. The CIIL-based approach not only enhanced task success rates and efficiency, but also reduced the perceived workload by 7.1 %, despite causing a 5.8 % reduction in offline classification accuracy. This study highlights the potential of real-time contextualized adaptation to enhance user experience and usability of EMG-based systems for practical, goal-oriented applications, crucial elements towards their long-term adoption. The source code for this study is available at: https://github.com/BiomedicalITS/ciil-emg-vr.

</details>


### [12] [The Typing Cure: Experiences with Large Language Model Chatbots for Mental Health Support](https://arxiv.org/abs/2401.14362)

*Inhwa Song, Sachin R. Pendse, Neha Kumar, Munmun De Choudhury*

**Main category:** cs.HC

**Keywords:** LLM chatbots, mental health support, therapeutic alignment, user experiences, AI ethics

**Relevance Score:** 9

**TL;DR:** This paper explores the use of LLM chatbots for mental health support, analyzing user experiences and introducing the concept of therapeutic alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As LLM chatbots are increasingly used for mental health support, there is a need to understand their impact and ensure they are designed responsibly to avoid risks to users.

**Method:** The authors conducted interviews with 21 people from diverse backgrounds who used LLM chatbots for mental health support, analyzing their experiences and interactions.

**Key Contributions:**

	1. Introduction of the concept of therapeutic alignment for AI in mental health care
	2. In-depth qualitative analysis of user experiences with LLM chatbots for mental health support
	3. Recommendations for ethical AI design in mental health contexts.

**Result:** The study identifies unique support roles users create for chatbots, their experiences in filling care gaps, and the cultural limitations they face; it emphasizes the importance of therapeutic alignment.

**Limitations:** The study is based on a limited sample size of 21 individuals and may not represent broader populations.

**Conclusion:** The findings stress the need for ethical design recommendations for LLM chatbots in mental health, considering how these tools can align with therapeutic values.

**Abstract:** People experiencing severe distress increasingly use Large Language Model (LLM) chatbots as mental health support tools. Discussions on social media have described how engagements were lifesaving for some, but evidence suggests that general-purpose LLM chatbots also have notable risks that could endanger the welfare of users if not designed responsibly. In this study, we investigate the lived experiences of people who have used LLM chatbots for mental health support. We build on interviews with 21 individuals from globally diverse backgrounds to analyze how users create unique support roles for their chatbots, fill in gaps in everyday care, and navigate associated cultural limitations when seeking support from chatbots. We ground our analysis in psychotherapy literature around effective support, and introduce the concept of therapeutic alignment, or aligning AI with therapeutic values for mental health contexts. Our study offers recommendations for how designers can approach the ethical and effective use of LLM chatbots and other AI mental health support tools in mental health care.

</details>


### [13] [Towards Human-Centered Early Prediction Models for Academic Performance in Real-World Contexts](https://arxiv.org/abs/2504.12236)

*Han Zhang, Yiyi Ren, Paula S. Nurius, Jennifer Mankoff, Anind K. Dey*

**Main category:** cs.HC

**Keywords:** Machine Learning, Academic Performance Prediction, Human-Centered Design

**Relevance Score:** 8

**TL;DR:** The paper discusses machine learning models for predicting student academic performance, focusing on interpretability, fairness, and early intervention using early behavioral data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve student support systems by integrating machine learning models that are interpretable, equitable, and actionable.

**Method:** Three modeling approaches: LR, 1D-CNN, and MTL-1D-CNN were evaluated based on explainability, fairness, and generalizability using early behavioral and self-reported data.

**Key Contributions:**

	1. Development of three modeling approaches for academic performance prediction
	2. Assessment based on explainability, fairness, and generalizability
	3. Discussion on the socio-technical challenges and future directions for predictive modeling in education

**Result:** The models could identify at-risk students as early as the first week of the term, highlighting their potential for timely interventions.

**Limitations:** Trade-offs across human-centered machine learning principles can complicate model design for multi-stakeholder decision-making.

**Conclusion:** While offering early predictions, there are trade-offs among human-centered principles that need consideration for effective deployment in educational contexts.

**Abstract:** Supporting student success requires collaboration among multiple stakeholders. Researchers have explored machine learning models for academic performance prediction; yet key challenges remain in ensuring these models are interpretable, equitable, and actionable within real-world educational support systems. First, many models prioritize predictive accuracy but overlook human-centered machine learning principles, limiting trust among students and reducing their usefulness for educators and institutional decision-makers. Second, most models require at least a month of data before making reliable predictions, delaying opportunities for early intervention. Third, current models primarily rely on sporadically collected, classroom-derived data, missing broader behavioral patterns that could provide more continuous and actionable insights. To address these gaps, we present three modeling approaches-LR, 1D-CNN, and MTL-1D-CNN-to classify students as low or high academic performers. We evaluate them based on explainability, fairness, and generalizability to assess their alignment with key social values. Using behavioral and self-reported data collected within the first week of two Spring terms, we demonstrate that these models can identify at-risk students as early as week one. However, trade-offs across human-centered machine learning principles highlight the complexity of designing predictive models that effectively support multi-stakeholder decision-making and intervention strategies. We discuss these trade-offs and their implications for different stakeholders, outlining how predictive models can be integrated into student support systems. Finally, we examine broader socio-technical challenges in deploying these models and propose future directions for advancing human-centered, collaborative academic prediction systems.

</details>


### [14] [An empathic GPT-based chatbot to talk about mental disorders with Spanish teenagers](https://arxiv.org/abs/2505.05828)

*Alba MarÃ­a MÃ¡rmol-Romero, Manuel GarcÃ­a-Vega, Miguel Ãngel GarcÃ­a-Cumbreras, Arturo Montejo-RÃ¡ez*

**Main category:** cs.HC

**Keywords:** chatbot, mental health, self-disclosure, GPT-3, teenagers

**Relevance Score:** 7

**TL;DR:** This paper presents a chatbot that engages Spanish teenagers in conversations about mental disorders using a self-disclosure technique and GPT-3.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To increase awareness of mental disorders among young Spanish people through engaging dialogue.

**Method:** The system employs a dialogue engine that combines closed and open conversations, adapted to users' responses and sensitivity to specific disorders.

**Key Contributions:**

	1. Development of a chatbot tailored for mental health awareness among teenagers
	2. Utilization of a self-disclosure technique to engage users
	3. Integration of GPT-3 for open-ended conversations

**Result:** The chatbot was found to be of interest to young users and could facilitate awareness of mental disorders.

**Limitations:** The study is limited to a specific age group (12-18 years) and demographic (Spanish youth).

**Conclusion:** These chatbot systems can effectively engage teenagers in discussions about mental health, promoting awareness and understanding.

**Abstract:** This paper presents a chatbot-based system to engage young Spanish people in the awareness of certain mental disorders through a self-disclosure technique. The study was carried out in a population of teenagers aged between 12 and 18 years. The dialogue engine mixes closed and open conversations, so certain controlled messages are sent to focus the chat on a specific disorder, which will change over time. Once a set of trial questions is answered, the system can initiate the conversation on the disorder under the focus according to the user's sensibility to that disorder, in an attempt to establish a more empathetic communication. Then, an open conversation based on the GPT-3 language model is initiated, allowing the user to express themselves with more freedom. The results show that these systems are of interest to young people and could help them become aware of certain mental disorders.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [15] [KG-HTC: Integrating Knowledge Graphs into LLMs for Effective Zero-shot Hierarchical Text Classification](https://arxiv.org/abs/2505.05583)

*Qianbo Zang, Christophe Zgrzendek, Igor Tchappi, Afshin Khadangi, Johannes Sedlmeir*

**Main category:** cs.CL

**Keywords:** Hierarchical Text Classification, Knowledge Graphs, Large Language Models, Zero-Shot Learning, Retrieval-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper presents KG-HTC, a method that integrates knowledge graphs with LLMs for zero-shot hierarchical text classification to address challenges associated with label spaces and long-tailed distributions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Hierarchical Text Classification (HTC) often struggles with supervised methods due to the lack of annotated data and challenges related to large label spaces and long-tail distributions.

**Method:** The proposed KG-HTC utilizes knowledge graphs and a Retrieval-Augmented Generation (RAG) approach to retrieve relevant subgraphs, providing semantic context to improve LLMsâ understanding of hierarchical label semantics.

**Key Contributions:**

	1. Introduction of KG-HTC for zero-shot HTC using knowledge graphs
	2. Demonstration of substantial performance improvements in chained hierarchies
	3. Provision of open-source code for algorithm implementation

**Result:** KG-HTC outperforms three baselines in strict zero-shot settings across three datasets (WoS, DBpedia, Amazon), showing significant improvements especially at deeper levels of hierarchy.

**Limitations:** 

**Conclusion:** Integrating structured knowledge into LLMs enhances their performance in hierarchical text classification, particularly in addressing challenges posed by large label spaces and long-tailed distributions.

**Abstract:** Hierarchical Text Classification (HTC) involves assigning documents to labels organized within a taxonomy. Most previous research on HTC has focused on supervised methods. However, in real-world scenarios, employing supervised HTC can be challenging due to a lack of annotated data. Moreover, HTC often faces issues with large label spaces and long-tail distributions. In this work, we present Knowledge Graphs for zero-shot Hierarchical Text Classification (KG-HTC), which aims to address these challenges of HTC in applications by integrating knowledge graphs with Large Language Models (LLMs) to provide structured semantic context during classification. Our method retrieves relevant subgraphs from knowledge graphs related to the input text using a Retrieval-Augmented Generation (RAG) approach. Our KG-HTC can enhance LLMs to understand label semantics at various hierarchy levels. We evaluate KG-HTC on three open-source HTC datasets: WoS, DBpedia, and Amazon. Our experimental results show that KG-HTC significantly outperforms three baselines in the strict zero-shot setting, particularly achieving substantial improvements at deeper levels of the hierarchy. This evaluation demonstrates the effectiveness of incorporating structured knowledge into LLMs to address HTC's challenges in large label spaces and long-tailed label distributions. Our code is available at: https://github.com/QianboZang/KG-HTC.

</details>


### [16] [Privacy-Preserving Transformers: SwiftKey's Differential Privacy Implementation](https://arxiv.org/abs/2505.05648)

*Abdelrahman Abouelenin, Mohamed Abdelrehim, Raffy Fahim, Amr Hendy, Mohamed Afify*

**Main category:** cs.CL

**Keywords:** Differential Privacy, Transformer, Language Modeling, GPT-2, SwiftKey

**Relevance Score:** 6

**TL;DR:** This paper demonstrates the training of a differential privacy transformer for language modeling, focusing on balancing model size, speed, and accuracy via an adapted GPT-2 architecture.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve language modeling for SwiftKey while ensuring user privacy through differential privacy techniques.

**Method:** A transformer model based on a scaled-down GPT-2 architecture is trained in two stages: building a seed model on general data followed by differential privacy fine-tuning on typing data, integrated via ONNX for flexibility and efficiency.

**Key Contributions:**

	1. Introduction of differential privacy in transformer-based language modeling
	2. Development of a two-stage training process for model refinement
	3. Integration of the model using ONNX for flexible deployment

**Result:** Achieved small and consistent gains in next-word prediction and accuracy, with improvements in memory and speed over the production GRU model.

**Limitations:** The study may be limited by the specific datasets used for fine-tuning and the inherent trade-offs in privacy and model performance.

**Conclusion:** The differential privacy transformer provides an effective approach to enhance language modeling while maintaining user privacy.

**Abstract:** In this paper we train a transformer using differential privacy (DP) for language modeling in SwiftKey. We run multiple experiments to balance the trade-off between the model size, run-time speed and accuracy. We show that we get small and consistent gains in the next-word-prediction and accuracy with graceful increase in memory and speed compared to the production GRU. This is obtained by scaling down a GPT2 architecture to fit the required size and a two stage training process that builds a seed model on general data and DP finetunes it on typing data. The transformer is integrated using ONNX offering both flexibility and efficiency.

</details>


### [17] [Exploration of COVID-19 Discourse on Twitter: American Politician Edition](https://arxiv.org/abs/2505.05687)

*Cindy Kim, Daniela Puchall, Jiangyi Liang, Jiwon Kim*

**Main category:** cs.CL

**Keywords:** COVID-19, sentiment analysis, political stance, NLP, tweet analysis

**Relevance Score:** 4

**TL;DR:** The paper analyzes partisan differences in responses to COVID-19 through a study of tweets from political figures, employing NLP models to distinguish political stances.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the COVID-19 pandemic has influenced political discourse and public opinion, highlighting differences between Republican and Democratic responses.

**Method:** The study uses bag-of-words, bigram, and TF-IDF models to analyze tweets from political figures, identifying keywords, topics, and sentiments related to COVID-19.

**Key Contributions:**

	1. Analysis of political discourse during COVID-19 using tweet data
	2. Application of NLP techniques to identify sentiments and topics in political tweets
	3. Proposed classification model for predicting political stance based on COVID-19 terms

**Result:** The analysis reveals that Democrats focus more on health concerns and recommendations, while Republicans emphasize political responsibilities and media updates regarding the virus.

**Limitations:** The study focuses only on tweets from American political figures, which may not represent broader public sentiments or responses.

**Conclusion:** The paper proposes a systematic approach to classify tweets by political stance based on COVID-19 terminology using various classification algorithms.

**Abstract:** The advent of the COVID-19 pandemic has undoubtedly affected the political scene worldwide and the introduction of new terminology and public opinions regarding the virus has further polarized partisan stances. Using a collection of tweets gathered from leading American political figures online (Republican and Democratic), we explored the partisan differences in approach, response, and attitude towards handling the international crisis. Implementation of the bag-of-words, bigram, and TF-IDF models was used to identify and analyze keywords, topics, and overall sentiments from each party. Results suggest that Democrats are more concerned with the casualties of the pandemic, and give more medical precautions and recommendations to the public whereas Republicans are more invested in political responsibilities such as keeping the public updated through media and carefully watching the progress of the virus. We propose a systematic approach to predict and distinguish a tweet's political stance (left or right leaning) based on its COVID-19 related terms using different classification algorithms on different language models.

</details>


### [18] [Assessing Robustness to Spurious Correlations in Post-Training Language Models](https://arxiv.org/abs/2505.05704)

*Julia Shuieh, Prasann Singhal, Apaar Shanker, John Heyer, George Pu, Samuel Denton*

**Main category:** cs.CL

**Keywords:** large language models, spurious correlations, Supervised Fine-Tuning, Direct Preference Optimization, Kahneman-Tversky Optimization

**Relevance Score:** 8

**TL;DR:** This paper evaluates three post-training algorithms for aligning large language models with user intent while addressing issues of spurious correlations in training data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the impact of spurious correlations in training data on the performance of large language models (LLMs).

**Method:** Systematic evaluation of Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Kahneman-Tversky Optimization (KTO) across synthetic tasks and different spuriousness conditions.

**Key Contributions:**

	1. Evaluation of post-training algorithms under varying degrees of spurious correlation
	2. Insights on the performance of SFT, DPO, and KTO in different scenarios
	3. Identification of task-dependent strategies for LLM alignment

**Result:** Models generally degrade in performance under higher spuriousness, but preference-based methods (DPO/KTO) show robustness in mathematical reasoning tasks, while SFT excels in complex tasks.

**Limitations:** Focus on synthetic tasks may limit generalizability to real-world applications.

**Conclusion:** No single post-training strategy is universally superior; the best approach depends on the target task and nature of spurious correlations.

**Abstract:** Supervised and preference-based fine-tuning techniques have become popular for aligning large language models (LLMs) with user intent and correctness criteria. However, real-world training data often exhibits spurious correlations -- arising from biases, dataset artifacts, or other "shortcut" features -- that can compromise a model's performance or generalization. In this paper, we systematically evaluate three post-training algorithms -- Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and KTO (Kahneman-Tversky Optimization) -- across a diverse set of synthetic tasks and spuriousness conditions. Our tasks span mathematical reasoning, constrained instruction-following, and document-grounded question answering. We vary the degree of spurious correlation (10% vs. 90%) and investigate two forms of artifacts: "Feature Ambiguity" and "Distributional Narrowness." Our results show that the models often but not always degrade under higher spuriousness. The preference-based methods (DPO/KTO) can demonstrate relative robustness in mathematical reasoning tasks. By contrast, SFT maintains stronger performance in complex, context-intensive tasks. These findings highlight that no single post-training strategy universally outperforms in all scenarios; the best choice depends on the type of target task and the nature of spurious correlations.

</details>


### [19] [TopicVD: A Topic-Based Dataset of Video-Guided Multimodal Machine Translation for Documentaries](https://arxiv.org/abs/2505.05714)

*Jinze Lv, Jian Chen, Zi Long, Xianghua Fu, Yin Chen*

**Main category:** cs.CL

**Keywords:** multimodal machine translation, documentary, video data, domain adaptation, cross-modal attention

**Relevance Score:** 6

**TL;DR:** This paper presents TopicVD, a dataset for video-supported multimodal machine translation of documentaries, and an MMT model leveraging cross-modal bidirectional attention.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of extensive video data in existing multimodal machine translation datasets for real-world applications like documentary translation.

**Method:** The authors collected video-subtitle pairs from documentaries, categorized them into eight topics, and proposed an MMT model using a cross-modal bidirectional attention module.

**Key Contributions:**

	1. Introduction of TopicVD, a diverse video-supported MMT dataset for documentary translation
	2. Development of a cross-modal bidirectional attention-based MMT model
	3. Empirical evaluation of the impact of visual information and global context on translation performance

**Result:** Experiments showed that visual information enhances the performance of the NMT model in documentary translation, but performance declines in out-of-domain scenarios.

**Limitations:** MMT model performance is significantly weakened in out-of-domain situations.

**Conclusion:** The study highlights the importance of domain adaptation methods and demonstrates that global context improves translation performance.

**Abstract:** Most existing multimodal machine translation (MMT) datasets are predominantly composed of static images or short video clips, lacking extensive video data across diverse domains and topics. As a result, they fail to meet the demands of real-world MMT tasks, such as documentary translation. In this study, we developed TopicVD, a topic-based dataset for video-supported multimodal machine translation of documentaries, aiming to advance research in this field. We collected video-subtitle pairs from documentaries and categorized them into eight topics, such as economy and nature, to facilitate research on domain adaptation in video-guided MMT. Additionally, we preserved their contextual information to support research on leveraging the global context of documentaries in video-guided MMT. To better capture the shared semantics between text and video, we propose an MMT model based on a cross-modal bidirectional attention module. Extensive experiments on the TopicVD dataset demonstrate that visual information consistently improves the performance of the NMT model in documentary translation. However, the MMT model's performance significantly declines in out-of-domain scenarios, highlighting the need for effective domain adaptation methods. Additionally, experiments demonstrate that global context can effectively improve translation performance. % Dataset and our implementations are available at https://github.com/JinzeLv/TopicVD

</details>


### [20] [Insertion Language Models: Sequence Generation with Arbitrary-Position Insertions](https://arxiv.org/abs/2505.05755)

*Dhruvesh Patel, Aishwarya Sahoo, Avinash Amballa, Tahira Naseem, Tim G. J. Rudner, Andrew McCallum*

**Main category:** cs.CL

**Keywords:** Insertion Language Models, sequence generation, masked diffusion models

**Relevance Score:** 7

**TL;DR:** This paper introduces Insertion Language Models (ILMs) for improved sequence generation by allowing token insertion at arbitrary positions, addressing limitations of existing autoregressive and masked diffusion models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation of the paper is to overcome the limitations of autoregressive models (ARMs) and masked diffusion models (MDMs) in accurately generating sequences that require sophisticated constraints or non-linear dependencies.

**Method:** The paper proposes Insertion Language Models (ILMs) which learn to insert tokens at arbitrary positions in a sequence, controlling both the position and the vocabulary element to be inserted, thus handling dependencies better than previous models.

**Key Contributions:**

	1. Introduction of Insertion Language Models (ILMs) for token insertion at arbitrary positions
	2. Demonstration of ILMs outperforming ARMs and MDMs in various tasks
	3. Proposed tailored network parameterization and simple denoising objective for training ILMs

**Result:** Empirical evaluations show that ILMs outperform both ARMs and MDMs on common planning tasks and perform similarly to ARMs in unconditional text generation while providing greater flexibility.

**Limitations:** 

**Conclusion:** ILMs present a viable solution for sequence generation, effectively managing complex dependencies and arbitrary-length infilling situations.

**Abstract:** Autoregressive models (ARMs), which predict subsequent tokens one-by-one ``from left to right,'' have achieved significant success across a wide range of sequence generation tasks. However, they struggle to accurately represent sequences that require satisfying sophisticated constraints or whose sequential dependencies are better addressed by out-of-order generation. Masked Diffusion Models (MDMs) address some of these limitations, but the process of unmasking multiple tokens simultaneously in MDMs can introduce incoherences, and MDMs cannot handle arbitrary infilling constraints when the number of tokens to be filled in is not known in advance. In this work, we introduce Insertion Language Models (ILMs), which learn to insert tokens at arbitrary positions in a sequence -- that is, they select jointly both the position and the vocabulary element to be inserted. By inserting tokens one at a time, ILMs can represent strong dependencies between tokens, and their ability to generate sequences in arbitrary order allows them to accurately model sequences where token dependencies do not follow a left-to-right sequential structure. To train ILMs, we propose a tailored network parameterization and use a simple denoising objective. Our empirical evaluation demonstrates that ILMs outperform both ARMs and MDMs on common planning tasks. Furthermore, we show that ILMs outperform MDMs and perform on par with ARMs in an unconditional text generation task while offering greater flexibility than MDMs in arbitrary-length text infilling.

</details>


### [21] [Sparse Attention Remapping with Clustering for Efficient LLM Decoding on PIM](https://arxiv.org/abs/2505.05772)

*Zehao Fan, Garrett Gagnon, Zhenyu Liu, Liu Liu*

**Main category:** cs.CL

**Keywords:** Transformers, Processing-in-memory, Large language models, Sparsity methods, Memory architecture

**Relevance Score:** 8

**TL;DR:** This paper introduces STARC, a novel data mapping scheme that optimizes transformer model decoding on processing-in-memory (PIM) architectures, significantly reducing latency and energy consumption during autoregressive decoding.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models (LLMs) face memory bandwidth bottlenecks during autoregressive decoding due to frequent memory accesses and large key-value (KV) caches. Processing-in-memory (PIM) architectures could alleviate these issues but struggle with irregular access patterns.

**Method:** STARC groups KV pairs by semantic similarity and maps them to contiguous memory regions. This allows queries to retrieve tokens at cluster granularity during decoding, reducing data movement overhead and improving processing efficiency.

**Key Contributions:**

	1. Novel sparsity-optimized data mapping scheme for LLMs
	2. Reduction of latency and energy consumption during decoding
	3. Maintaining model accuracy while improving efficiency

**Result:** STARC reduces attention-layer latency by 19%â31% and energy consumption by 19%â27% compared to token-wise sparsity methods. It achieves latency reduction of 54%â74% and energy reduction of 45%â67% under specific KV cache budgets.

**Limitations:** 

**Conclusion:** STARC demonstrates effective hardware-friendly LLM inference on PIM architectures while maintaining model accuracy similar to existing sparse attention methods.

**Abstract:** Transformer-based models are the foundation of modern machine learning, but their execution, particularly during autoregressive decoding in large language models (LLMs), places significant pressure on memory systems due to frequent memory accesses and growing key-value (KV) caches. This creates a bottleneck in memory bandwidth, especially as context lengths increase. Processing-in-memory (PIM) architectures are a promising solution, offering high internal bandwidth and compute parallelism near memory. However, current PIM designs are primarily optimized for dense attention and struggle with the dynamic, irregular access patterns introduced by modern KV cache sparsity techniques. Consequently, they suffer from workload imbalance, reducing throughput and resource utilization. In this work, we propose STARC, a novel sparsity-optimized data mapping scheme tailored specifically for efficient LLM decoding on PIM architectures. STARC clusters KV pairs by semantic similarity and maps them to contiguous memory regions aligned with PIM bank structures. During decoding, queries retrieve relevant tokens at cluster granularity by matching against precomputed centroids, enabling selective attention and parallel processing without frequent reclustering or data movement overhead. Experiments on the HBM-PIM system show that, compared to common token-wise sparsity methods, STARC reduces attention-layer latency by 19%--31% and energy consumption by 19%--27%. Under a KV cache budget of 1024, it achieves up to 54%--74% latency reduction and 45%--67% energy reduction compared to full KV cache retrieval. Meanwhile, STARC maintains model accuracy comparable to state-of-the-art sparse attention methods, demonstrating its effectiveness in enabling efficient and hardware-friendly long-context LLM inference on PIM architectures.

</details>


### [22] [Tell Me Who Your Students Are: GPT Can Generate Valid Multiple-Choice Questions When Students' (Mis)Understanding Is Hinted](https://arxiv.org/abs/2505.05815)

*Machi Shimmei, Masaki Uto, Yuichiroh Matsubayashi, Kentaro Inui, Aditi Mallavarapu, Noboru Matsuda*

**Main category:** cs.CL

**Keywords:** large language model, multiple-choice questions, assessment techniques, AnaQuest, Item Response Theory

**Relevance Score:** 8

**TL;DR:** The study develops AnaQuest, a prompting technique using large language models to create valid multiple-choice questions with effective assessments.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To innovate the generation of multiple-choice questions using AI and enhance assessment validity.

**Method:** AnaQuest integrates formative and summative assessments, utilizing open-ended student responses to generate MCQ items through IRT analysis.

**Key Contributions:**

	1. Introduction of AnaQuest for MCQ generation using AI
	2. Evaluation against human-crafted questions
	3. Findings indicate AnaQuest's superiority in question quality

**Result:** Empirical study reveals expert ratings of AI-generated MCQs match those of human-crafted items, with AnaQuest showing better alignment in difficulty and discrimination.

**Limitations:** 

**Conclusion:** AnaQuest is a promising technique for valid MCQ generation using language models, particularly in education.

**Abstract:** The primary goal of this study is to develop and evaluate an innovative prompting technique, AnaQuest, for generating multiple-choice questions (MCQs) using a pre-trained large language model. In AnaQuest, the choice items are sentence-level assertions about complex concepts. The technique integrates formative and summative assessments. In the formative phase, students answer open-ended questions for target concepts in free text. For summative assessment, AnaQuest analyzes these responses to generate both correct and incorrect assertions. To evaluate the validity of the generated MCQs, Item Response Theory (IRT) was applied to compare item characteristics between MCQs generated by AnaQuest, a baseline ChatGPT prompt, and human-crafted items. An empirical study found that expert instructors rated MCQs generated by both AI models to be as valid as those created by human instructors. However, IRT-based analysis revealed that AnaQuest-generated questions - particularly those with incorrect assertions (foils) - more closely resembled human-crafted items in terms of difficulty and discrimination than those produced by ChatGPT.

</details>


### [23] [Symbol-based entity marker highlighting for enhanced text mining in materials science with generative AI](https://arxiv.org/abs/2505.05864)

*Junhyeong Lee, Jong Min Yuk, Chan-Woo Lee*

**Main category:** cs.CL

**Keywords:** text mining, entity recognition, structured data, natural language processing, hybrid approach

**Relevance Score:** 8

**TL;DR:** A hybrid text-mining framework combines multi-step and direct methods for entity recognition, improving the extraction of structured data from scientific literature.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing multi-step and direct methods in converting unstructured scientific text into structured data.

**Method:** The framework employs a hybrid approach that first recognizes entities in the text and then converts them into a structured format, utilizing an entity marker technique to enhance recognition accuracy.

**Key Contributions:**

	1. Development of a novel hybrid text-mining framework
	2. Introduction of an effective entity marker technique
	3. Demonstrated significant improvements in data extraction accuracy across benchmark datasets

**Result:** The hybrid approach outperforms previous methods by achieving up to 58% improvement in entity-level F1 score and up to 83% improvement in relation-level F1 score on three benchmark datasets.

**Limitations:** 

**Conclusion:** Integrating entity recognition and structuring methods enhances data extraction processes, providing more accurate structured outputs for scientific research.

**Abstract:** The construction of experimental datasets is essential for expanding the scope of data-driven scientific discovery. Recent advances in natural language processing (NLP) have facilitated automatic extraction of structured data from unstructured scientific literature. While existing approaches-multi-step and direct methods-offer valuable capabilities, they also come with limitations when applied independently. Here, we propose a novel hybrid text-mining framework that integrates the advantages of both methods to convert unstructured scientific text into structured data. Our approach first transforms raw text into entity-recognized text, and subsequently into structured form. Furthermore, beyond the overall data structuring framework, we also enhance entity recognition performance by introducing an entity marker-a simple yet effective technique that uses symbolic annotations to highlight target entities. Specifically, our entity marker-based hybrid approach not only consistently outperforms previous entity recognition approaches across three benchmark datasets (MatScholar, SOFC, and SOFC slot NER) but also improve the quality of final structured data-yielding up to a 58% improvement in entity-level F1 score and up to 83% improvement in relation-level F1 score compared to direct approach.

</details>


### [24] [Elastic Weight Consolidation for Full-Parameter Continual Pre-Training of Gemma2](https://arxiv.org/abs/2505.05946)

*Vytenis Å liogeris, Povilas DaniuÅ¡is, ArtÅ«ras Nakvosas*

**Main category:** cs.CL

**Keywords:** Large Language Model, Continual Learning, Elastic Weight Consolidation, Language Understanding, Autoregressive Pre-training

**Relevance Score:** 6

**TL;DR:** The report explores autoregressive pre-training of the Gemma2 LLM with a focus on continual learning using elastic weight consolidation (EWC) to mitigate catastrophic forgetting effects.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how EWC regularization can improve continual learning in LLMs, specifically for language understanding tasks in Lithuanian and English.

**Method:** An experiment was conducted using the 2 billion parameter Gemma2 LLM, applying EWC to the model's parameters during autoregressive pre-training. The model was evaluated on various language understanding benchmarks and perplexity metrics.

**Key Contributions:**

	1. Demonstrated the effectiveness of EWC in LLMs for continual learning.
	2. Evaluated the performance of a large language model specifically on language understanding tasks in Lithuanian.
	3. Provided empirical evidence on the impact of EWC on task learning and perplexity metrics.

**Result:** EWC regularization proved effective in reducing catastrophic forgetting while also enhancing the model's performance on new tasks.

**Limitations:** The study is limited to one specific model and language component, and results may not generalize to other languages or models.

**Conclusion:** The findings suggest that EWC can be a valuable technique in continual learning for LLMs, providing benefits beyond just mitigating forgetting.

**Abstract:** This technical report describes an experiment on autoregressive pre-training of Gemma2 2 billion parameter large language model (LLM) with 10\% on the Lithuanian language component of CulturaX from the point of view of continual learning. We apply elastic weight consolidation (EWC) to the full set of the model's parameters and investigate language understanding benchmarks, consisting of Arc, Belebele, Gsm8K, Hellaswag, MMLU, TruthfulQA, and Winogrande sets (both in English and Lithuanian versions), and perplexity benchmarks. We empirically demonstrate that EWC regularisation allows us not only to mitigate catastrophic forgetting effects but also that it is potentially beneficial for learning of the new task with LLMs.

</details>


### [25] [Summarisation of German Judgments in conjunction with a Class-based Evaluation](https://arxiv.org/abs/2505.05947)

*Bianca Steffes, Nils Torben Wiedemann, Alexander Gratz, Pamela Hochreither, Jana Elina Meyer, Katharina Luise Schilke*

**Main category:** cs.CL

**Keywords:** automated summarisation, legal documents, large language model, German judgments, summary quality

**Relevance Score:** 3

**TL;DR:** The paper discusses the automated summarization of German legal judgments using a fine-tuned LLM, emphasizing the enhancement of summaries through the inclusion of legal entities, while highlighting the insufficiency of the summary quality for practical use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assist legal experts in summarizing long legal documents efficiently, particularly German judgments.

**Method:** Fine-tuning a decoder-based large language model while enriching training data with information about legal entities.

**Key Contributions:**

	1. Development of a method to summarize legal judgments using LLMs
	2. Demonstration of the impact of legal entities on summary generation
	3. Establishment of evaluation classes for assessing summary quality

**Result:** Incorporating legal entities improves the generative model's ability to identify relevant content, although the overall summary quality is still inadequate for practical application.

**Limitations:** The quality of the generated summaries is not yet sufficient for practical use.

**Conclusion:** The integration of legal entities aids in content relevance, but the generated summaries require further improvement for them to be practically useful.

**Abstract:** The automated summarisation of long legal documents can be a great aid for legal experts in their daily work. We automatically create summaries (guiding principles) of German judgments by fine-tuning a decoder-based large language model. We enrich the judgments with information about legal entities before the training. For the evaluation of the created summaries, we define a set of evaluation classes which allows us to measure their language, pertinence, completeness and correctness. Our results show that employing legal entities helps the generative model to find the relevant content, but the quality of the created summaries is not yet sufficient for a use in practice.

</details>


### [26] [NeoQA: Evidence-based Question Answering with Generated News Events](https://arxiv.org/abs/2505.05949)

*Max Glockner, Xiang Jiang, Leonardo F. R. Ribeiro, Iryna Gurevych, Markus Dreyer*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, NeoQA, Large Language Models, Evidence-based Reasoning, Question Answering

**Relevance Score:** 9

**TL;DR:** NeoQA is a benchmark for evaluating LLMs in question answering where pretraining knowledge cannot be used, focusing on evidence-based reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge in evaluating Retrieval-Augmented Generation (RAG) in LLMs due to stale benchmarks and LLMs potentially answering from pretraining knowledge rather than retrieval.

**Method:** Introduced NeoQA benchmark, generating fictional news event timelines, knowledge bases, articles, and Q&A pairs to isolate retrieval from pretraining knowledge.

**Key Contributions:**

	1. Introduction of NeoQA benchmark for evidence-based Q&A
	2. Creation of controlled scenarios preventing pretraining knowledge usage
	3. Insights into limitations of LLMs in evidence-based reasoning

**Result:** LLMs showed difficulty in distinguishing between questions and evidence and exhibited shortcut reasoning when key information was absent.

**Limitations:** The dataset is fictional, which may not fully represent real-world complexities.

**Conclusion:** NeoQA serves as a platform for assessing evidence-based question answering capabilities in LLMs with controlled evaluation of evidence scenarios.

**Abstract:** Evaluating Retrieval-Augmented Generation (RAG) in large language models (LLMs) is challenging because benchmarks can quickly become stale. Questions initially requiring retrieval may become answerable from pretraining knowledge as newer models incorporate more recent information during pretraining, making it difficult to distinguish evidence-based reasoning from recall. We introduce NeoQA (News Events for Out-of-training Question Answering), a benchmark designed to address this issue. To construct NeoQA, we generated timelines and knowledge bases of fictional news events and entities along with news articles and Q\&A pairs to prevent LLMs from leveraging pretraining knowledge, ensuring that no prior evidence exists in their training data. We propose our dataset as a new platform for evaluating evidence-based question answering, as it requires LLMs to generate responses exclusively from retrieved evidence and only when sufficient evidence is available. NeoQA enables controlled evaluation across various evidence scenarios, including cases with missing or misleading details. Our findings indicate that LLMs struggle to distinguish subtle mismatches between questions and evidence, and suffer from short-cut reasoning when key information required to answer a question is missing from the evidence, underscoring key limitations in evidence-based reasoning.

</details>


### [27] [Towards Developmentally Plausible Rewards: Communicative Success as a Learning Signal for Interactive Language Models](https://arxiv.org/abs/2505.05970)

*Lennart StÃ¶pler, Rufat Asadli, Mitja Nikolaus, Ryan Cotterell, Alex Warstadt*

**Main category:** cs.CL

**Keywords:** language models, interactive learning, reinforcement learning, natural language processing, child language acquisition

**Relevance Score:** 7

**TL;DR:** A method for training language models through interactive dialogue inspired by child language acquisition is proposed, focusing on communicative success in language-only question-answering.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To operationalize communicative success in language models, inspired by child language acquisition and to enhance the learning process through interactive dialogue.

**Method:** A feasibility study is conducted to gauge the effectiveness of a reward system in signaling grammaticality, followed by reinforcement learning experiments to fine-tune language models based on speaker behavior.

**Key Contributions:**

	1. Introduction of an interactive training method for language models inspired by child language acquisition.
	2. Feasibility study demonstrating the reward system's indirect signal for grammaticality.
	3. Use of reinforcement learning to fine-tune language models based on communicative success.

**Result:** While cognitively plausible constraints affected speaker behavior, improvements in linguistic evaluations were not observed from the training regime.

**Limitations:** Improvements in linguistic evaluations were not observed, indicating potential issues with the current training regime or task design.

**Conclusion:** Future modifications to task design and training configurations may help realize the benefits of interaction on language learning in computational models.

**Abstract:** We propose a method for training language models in an interactive setting inspired by child language acquisition. In our setting, a speaker attempts to communicate some information to a listener in a single-turn dialogue and receives a reward if communicative success is achieved. Unlike earlier related work using image--caption data for interactive reference games, we operationalize communicative success in a more abstract language-only question--answering setting. First, we present a feasibility study demonstrating that our reward provides an indirect signal about grammaticality. Second, we conduct experiments using reinforcement learning to fine-tune language models. We observe that cognitively plausible constraints on the communication channel lead to interpretable changes in speaker behavior. However, we do not yet see improvements on linguistic evaluations from our training regime. We outline potential modifications to the task design and training configuration that could better position future work to use our methodology to observe the benefits of interaction on language learning in computational cognitive models.

</details>


### [28] [An Exploratory Analysis on the Explanatory Potential of Embedding-Based Measures of Semantic Transparency for Malay Word Recognition](https://arxiv.org/abs/2505.05973)

*M. Maziyah Mohamed, R. H. Baayen*

**Main category:** cs.CL

**Keywords:** semantic transparency, word recognition, embedding measures, morphological processing, lexical decision

**Relevance Score:** 4

**TL;DR:** This study investigates the role of semantic transparency in word recognition using embedding-based measures, focusing on lexical decision latencies.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how semantic transparency impacts word recognition and to operationalize this concept computationally.

**Method:** Utilized t-distributed Stochastic Neighbor Embedding to analyze 4,226 Malay prefixed words, derived measures of semantic transparency, and conducted Linear Discriminant Analyses to predict prefix identification.

**Key Contributions:**

	1. Introduced embedding-based measures of semantic transparency.
	2. Conducted a comprehensive analysis of Malay prefixed words using clustering techniques.
	3. Demonstrated the predictive capability of semantic measures on lexical decision latencies.

**Result:** The model that incorporated the correlation between each word and its centroid predicted decision latencies most effectively, revealing that semantic measures can indeed predict lexical processing times.

**Limitations:** Focus on Malay language; results may not generalize to other languages or morphological structures.

**Conclusion:** Embedding-based measures of semantic transparency significantly predict word recognition latency, demonstrating the relevance of semantic relationships in morphological processing.

**Abstract:** Studies of morphological processing have shown that semantic transparency is crucial for word recognition. Its computational operationalization is still under discussion. Our primary objectives are to explore embedding-based measures of semantic transparency, and assess their impact on reading. First, we explored the geometry of complex words in semantic space. To do so, we conducted a t-distributed Stochastic Neighbor Embedding clustering analysis on 4,226 Malay prefixed words. Several clusters were observed for complex words varied by their prefix class. Then, we derived five simple measures, and investigated whether they were significant predictors of lexical decision latencies. Two sets of Linear Discriminant Analyses were run in which the prefix of a word is predicted from either word embeddings or shift vectors (i.e., a vector subtraction of the base word from the derived word). The accuracy with which the model predicts the prefix of a word indicates the degree of transparency of the prefix. Three further measures were obtained by comparing embeddings between each word and all other words containing the same prefix (i.e., centroid), between each word and the shift from their base word, and between each word and the predicted word of the Functional Representations of Affixes in Compositional Semantic Space model. In a series of Generalized Additive Mixed Models, all measures predicted decision latencies after accounting for word frequency, word length, and morphological family size. The model that included the correlation between each word and their centroid as a predictor provided the best fit to the data.

</details>


### [29] [Exploring the Feasibility of Multilingual Grammatical Error Correction with a Single LLM up to 9B parameters: A Comparative Study of 17 Models](https://arxiv.org/abs/2505.06004)

*Dawid Wisniewski, Antoni Solarski, Artur Nowakowski*

**Main category:** cs.CL

**Keywords:** language models, grammatical error correction, multilingual, NLP, Machine Translation

**Relevance Score:** 8

**TL;DR:** The paper evaluates 17 language models for multilingual grammatical error correction, demonstrating Gemma 9B as the best performing model across English, German, Italian, and Swedish.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of multilingual applications, understanding how language models perform in grammatical error correction across different languages is essential for creating effective tools.

**Method:** The performance of 17 language models was examined to correct grammatical issues in texts written in four languages using a single model approach.

**Key Contributions:**

	1. Evaluation of 17 language models for multilingual tasks
	2. Identification of the best performing model (Gemma 9B)
	3. Insights into common grammatical error correction challenges across languages.

**Result:** Analysis revealed six models that effectively corrected grammatical errors in all four languages, with Gemma 9B being the highest performer.

**Limitations:** 

**Conclusion:** The findings will guide recommendations for models to be used in multilingual grammatical error correction tasks and highlight the strengths and weaknesses of different approaches.

**Abstract:** Recent language models can successfully solve various language-related tasks, and many understand inputs stated in different languages. In this paper, we explore the performance of 17 popular models used to correct grammatical issues in texts stated in English, German, Italian, and Swedish when using a single model to correct texts in all those languages. We analyze the outputs generated by these models, focusing on decreasing the number of grammatical errors while keeping the changes small. The conclusions drawn help us understand what problems occur among those models and which models can be recommended for multilingual grammatical error correction tasks. We list six models that improve grammatical correctness in all four languages and show that Gemma 9B is currently the best performing one for the languages considered.

</details>


### [30] [Do Not Change Me: On Transferring Entities Without Modification in Neural Machine Translation -- a Multilingual Perspective](https://arxiv.org/abs/2505.06010)

*Dawid Wisniewski, Mikolaj Pokrywka, Zofia Rostek*

**Main category:** cs.CL

**Keywords:** machine translation, entity preservation, NMT models, multilingual dataset, error analysis

**Relevance Score:** 6

**TL;DR:** This paper explores the effectiveness of various NMT models in preserving specific entities during translation across multiple languages and proposes a new dataset for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the persistent challenges faced by machine translation models in accurately preserving specific entities such as URLs and emails during the translation process.

**Method:** The study evaluates several popular NMT models, including those from the OPUS project, Google Translate, MADLAD, and EuroLLM, comparing their performance in translating entities between English, German, Polish, and Ukrainian.

**Key Contributions:**

	1. Investigation of entity preservation in NMT
	2. Error analysis of popular machine translation models
	3. Development of a multilingual synthetic dataset for evaluation

**Result:** The analysis reveals notable errors in entity preservation, particularly with certain categories like emojis, and demonstrates varying performance among the evaluated models.

**Limitations:** The study focuses on only four languages and specific entity categories, which may limit generalizability.

**Conclusion:** The findings highlight the limitations of current NMT models in entity transfer and introduce a new synthetic dataset to facilitate future research in this area.

**Abstract:** Current machine translation models provide us with high-quality outputs in most scenarios. However, they still face some specific problems, such as detecting which entities should not be changed during translation. In this paper, we explore the abilities of popular NMT models, including models from the OPUS project, Google Translate, MADLAD, and EuroLLM, to preserve entities such as URL addresses, IBAN numbers, or emails when producing translations between four languages: English, German, Polish, and Ukrainian. We investigate the quality of popular NMT models in terms of accuracy, discuss errors made by the models, and examine the reasons for errors. Our analysis highlights specific categories, such as emojis, that pose significant challenges for many models considered. In addition to the analysis, we propose a new multilingual synthetic dataset of 36,000 sentences that can help assess the quality of entity transfer across nine categories and four aforementioned languages.

</details>


### [31] [Unilogit: Robust Machine Unlearning for LLMs Using Uniform-Target Self-Distillation](https://arxiv.org/abs/2505.06027)

*Stefan Vasilev, Christian Herold, Baohao Liao, Seyyed Hadi Hashemi, Shahram Khadivi, Christof Monz*

**Main category:** cs.CL

**Keywords:** self-distillation, machine unlearning, large language models, data privacy, GDPR

**Relevance Score:** 8

**TL;DR:** Unilogit is a self-distillation method for machine unlearning in Large Language Models, allowing for selective forgetting while retaining model utility.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the need for selective forgetting of information in machine learning models for data privacy compliance, specifically in adherence to GDPR.

**Method:** Unilogit dynamically adjusts target logits based on the model's current outputs, allowing for accurate self-distillation without the need for additional hyperparameters.

**Key Contributions:**

	1. Introduces a self-distillation method for machine unlearning in LLMs.
	2. Dynamically adjusts target logits for better forgetting without extra hyperparameters.
	3. Demonstrates superior performance on public benchmarks compared to existing methods.

**Result:** Unilogit demonstrates superior performance in balancing forget and retain objectives compared to state-of-the-art methods like NPO and UnDIAL, based on extensive experiments.

**Limitations:** 

**Conclusion:** Unilogit effectively achieves machine unlearning while maintaining model performance, showcasing robustness across various scenarios and applications.

**Abstract:** This paper introduces Unilogit, a novel self-distillation method for machine unlearning in Large Language Models. Unilogit addresses the challenge of selectively forgetting specific information while maintaining overall model utility, a critical task in compliance with data privacy regulations like GDPR. Unlike prior methods that rely on static hyperparameters or starting model outputs, Unilogit dynamically adjusts target logits to achieve a uniform probability for the target token, leveraging the current model's outputs for more accurate self-distillation targets. This approach not only eliminates the need for additional hyperparameters but also enhances the model's ability to approximate the golden targets. Extensive experiments on public benchmarks and an in-house e-commerce dataset demonstrate Unilogit's superior performance in balancing forget and retain objectives, outperforming state-of-the-art methods such as NPO and UnDIAL. Our analysis further reveals Unilogit's robustness across various scenarios, highlighting its practical applicability and effectiveness in achieving efficacious machine unlearning.

</details>


### [32] [Healthy LLMs? Benchmarking LLM Knowledge of UK Government Public Health Information](https://arxiv.org/abs/2505.06046)

*Joshua Harris, Fan Grayson, Felix Feldman, Timothy Laurence, Toby Nonnenmacher, Oliver Higgins, Leo Loman, Selina Patel, Thomas Finnie, Samuel Collins, Michael Borowitz*

**Main category:** cs.CL

**Keywords:** Large Language Models, public health, benchmark, MCQA, free form responses

**Relevance Score:** 9

**TL;DR:** The paper evaluates the knowledge of Large Language Models (LLMs) on UK public health information using a benchmark called PubHealthBench, finding high performance in Multiple Choice Question Answering but lower performance in free form responses.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the knowledge of LLMs in the public health domain, which is critical for accurate information dissemination that impacts UK residents.

**Method:** The authors created a benchmark, PubHealthBench, with over 8000 questions to evaluate LLMs' performance on Multiple Choice Question Answering and free form responses, alongside a dataset of UK Government public health guidance documents.

**Key Contributions:**

	1. Introduction of the PubHealthBench benchmark for evaluating LLMs on public health queries
	2. Release of a dataset containing UK Government public health guidance documents
	3. Demonstration of LLM performance discrepancies between MCQA and free form responses.

**Result:** 24 LLMs were assessed against the benchmark, showing that while the latest private LLMs have >90% accuracy in MCQA, their free form responses did not exceed 75% accuracy.

**Limitations:** Free form responses yielded lower performance, highlighting the need for improved accuracy in this area.

**Conclusion:** Although SOTA LLMs show promise in accurately providing public health information, further safeguards may be necessary for free form responses.

**Abstract:** As Large Language Models (LLMs) become widely accessible, a detailed understanding of their knowledge within specific domains becomes necessary for successful real world use. This is particularly critical in public health, where failure to retrieve relevant, accurate, and current information could significantly impact UK residents. However, currently little is known about LLM knowledge of UK Government public health information. To address this issue, this paper introduces a new benchmark, PubHealthBench, with over 8000 questions for evaluating LLMs' Multiple Choice Question Answering (MCQA) and free form responses to public health queries, created via an automated pipeline. We also release a new dataset of the extracted UK Government public health guidance documents used as source text for PubHealthBench. Assessing 24 LLMs on PubHealthBench we find the latest private LLMs (GPT-4.5, GPT-4.1 and o1) have a high degree of knowledge, achieving >90% in the MCQA setup, and outperform humans with cursory search engine use. However, in the free form setup we see lower performance with no model scoring >75%. Therefore, whilst there are promising signs that state of the art (SOTA) LLMs are an increasingly accurate source of public health information, additional safeguards or tools may still be needed when providing free form responses on public health topics.

</details>


### [33] [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)

*Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conversational interfaces, Multi-turn conversations

**Relevance Score:** 9

**TL;DR:** This paper evaluates the performance of Large Language Models (LLMs) in single-turn versus multi-turn conversational settings, revealing significant performance drops in multi-turn scenarios.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs perform in multi-turn settings where users may underspecify tasks and require iterative clarification.

**Method:** Large-scale simulation experiments comparing LLM performance across single-turn and multi-turn conversations, analyzing over 200,000 simulated conversations.

**Key Contributions:**

	1. Performance comparison of LLMs in single-turn vs multi-turn contexts
	2. Identification of performance degradation components
	3. Insight into the unreliability of LLMs in conversations

**Result:** LLMs exhibited an average performance drop of 39% in multi-turn conversations, with findings showing reduced reliability and premature final solutions in earlier conversation turns.

**Limitations:** Focused on simulation experiments, may not fully capture real-world conversational dynamics.

**Conclusion:** The study highlights the challenges LLMs face in maintaining reliable performance during multi-turn dialogues and suggests areas for improvement.

**Abstract:** Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.

</details>


### [34] [Attention on Multiword Expressions: A Multilingual Study of BERT-based Models with Regard to Idiomaticity and Microsyntax](https://arxiv.org/abs/2505.06062)

*Iuliia Zaitova, Vitalii Hirak, Badr M. Abdullah, Dietrich Klakow, Bernd MÃ¶bius, Tania Avgustinova*

**Main category:** cs.CL

**Keywords:** BERT, Multiword Expressions, attention mechanisms, semantics, syntax

**Relevance Score:** 7

**TL;DR:** This study investigates how fine-tuning BERT-based models affects their attention patterns towards idioms and microsyntactic units across six Indo-European languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the impact of model fine-tuning on attention allocation to Multiword Expressions (MWEs) like idioms and microsyntactic units (MSUs) in natural language processing.

**Method:** We analyzed attention scores from pre-trained and fine-tuned BERT-based models on semantic and syntactic tasks using datasets in English, German, Dutch, Polish, Russian, and Ukrainian.

**Key Contributions:**

	1. Analysis of attention patterns in fine-tuned BERT-based models
	2. Comparison of attention between idioms and microsyntactic units
	3. Multilingual approach utilizing six Indo-European languages

**Result:** Fine-tuning significantly alters attention patterns; semantic task models distribute attention to idioms across layers, while syntactic task models focus on MSUs in lower layers.

**Limitations:** 

**Conclusion:** The findings suggest that the type of task a BERT model is fine-tuned on has a substantial impact on its attention behavior towards different types of MWEs.

**Abstract:** This study analyzes the attention patterns of fine-tuned encoder-only models based on the BERT architecture (BERT-based models) towards two distinct types of Multiword Expressions (MWEs): idioms and microsyntactic units (MSUs). Idioms present challenges in semantic non-compositionality, whereas MSUs demonstrate unconventional syntactic behavior that does not conform to standard grammatical categorizations. We aim to understand whether fine-tuning BERT-based models on specific tasks influences their attention to MWEs, and how this attention differs between semantic and syntactic tasks. We examine attention scores to MWEs in both pre-trained and fine-tuned BERT-based models. We utilize monolingual models and datasets in six Indo-European languages - English, German, Dutch, Polish, Russian, and Ukrainian. Our results show that fine-tuning significantly influences how models allocate attention to MWEs. Specifically, models fine-tuned on semantic tasks tend to distribute attention to idiomatic expressions more evenly across layers. Models fine-tuned on syntactic tasks show an increase in attention to MSUs in the lower layers, corresponding with syntactic processing requirements.

</details>


### [35] [Multimodal Sentiment Analysis on CMU-MOSEI Dataset using Transformer-based Models](https://arxiv.org/abs/2505.06110)

*Jugal Gajjar, Kaustik Ranaware*

**Main category:** cs.CL

**Keywords:** multimodal sentiment analysis, transformer-based models, CMU-MOSEI dataset

**Relevance Score:** 8

**TL;DR:** This project focuses on multimodal sentiment analysis using early fusion of text, audio, and visual modalities through transformer-based models, achieving strong accuracy and F1 scores.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the effectiveness of early fusion in multimodal sentiment analysis by integrating text, audio, and visual data.

**Method:** Used BERT-based encoders for each modality to extract embeddings, which were concatenated before classification with a focus on Adam optimization and dropout techniques.

**Key Contributions:**

	1. Demonstrated the effectiveness of early fusion in multimodal sentiment analysis.
	2. Achieved high accuracy and F1-score using a transformer-based model.
	3. Highlighted low MAE in sentiment intensity prediction.

**Result:** Achieved 97.87% 7-class accuracy and a 0.9682 F1-score; demonstrated low MAE (0.1060) for sentiment intensity prediction, indicating strong model performance.

**Limitations:** 

**Conclusion:** The study confirms transformer architectures' superior capability in modeling multimodal sentiment and suggests future investigations into different fusion strategies and interpretability enhancements.

**Abstract:** This project performs multimodal sentiment analysis using the CMU-MOSEI dataset, using transformer-based models with early fusion to integrate text, audio, and visual modalities. We employ BERT-based encoders for each modality, extracting embeddings that are concatenated before classification. The model achieves strong performance, with 97.87\% 7-class accuracy and a 0.9682 F1-score on the test set, demonstrating the effectiveness of early fusion in capturing cross-modal interactions. The training utilized Adam optimization (lr=1e-4), dropout (0.3), and early stopping to ensure generalization and robustness. Results highlight the superiority of transformer architectures in modeling multimodal sentiment, with a low MAE (0.1060) indicating precise sentiment intensity prediction. Future work may compare fusion strategies or enhance interpretability. This approach utilizes multimodal learning by effectively combining linguistic, acoustic, and visual cues for sentiment analysis.

</details>


### [36] [LLMs Get Lost In Multi-Turn Conversation](https://arxiv.org/abs/2505.06120)

*Philippe Laban, Hiroaki Hayashi, Yingbo Zhou, Jennifer Neville*

**Main category:** cs.CL

**Keywords:** Large Language Models, Multi-turn conversations, Instruction underspecification, Performance evaluation, Conversational AI

**Relevance Score:** 9

**TL;DR:** The study analyzes the performance of Large Language Models (LLMs) in single-turn vs multi-turn conversations, revealing significant performance drops in multi-turn settings due to unreliability and premature assumptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs perform in conversational interfaces, especially considering the frequent underspecification in user instructions.

**Method:** Large-scale simulation experiments compared LLM performance across single-turn and multi-turn conversations, analyzing over 200,000 simulated conversations.

**Key Contributions:**

	1. Identification of performance differences between single-turn and multi-turn instructions for LLMs.
	2. Detailed analysis of the causes of performance degradation, highlighting unreliability and early assumptions.
	3. Insights into LLM behavior in dynamic conversation settings that inform design improvements.

**Result:** All tested LLMs showed an average performance drop of 39% in multi-turn settings compared to single-turn, with unreliability increasing significantly.

**Limitations:** Focused solely on LLMs and may not generalize to other conversational AI systems; limited to the specific tasks tested.

**Conclusion:** LLMs struggle in multi-turn conversations, often making early incorrect assumptions that lead to a failure to recover, demonstrating the need for improved conversational capabilities.

**Abstract:** Large Language Models (LLMs) are conversational interfaces. As such, LLMs have the potential to assist their users not only when they can fully specify the task at hand, but also to help them define, explore, and refine what they need through multi-turn conversational exchange. Although analysis of LLM conversation logs has confirmed that underspecification occurs frequently in user instructions, LLM evaluation has predominantly focused on the single-turn, fully-specified instruction setting. In this work, we perform large-scale simulation experiments to compare LLM performance in single- and multi-turn settings. Our experiments confirm that all the top open- and closed-weight LLMs we test exhibit significantly lower performance in multi-turn conversations than single-turn, with an average drop of 39% across six generation tasks. Analysis of 200,000+ simulated conversations decomposes the performance degradation into two components: a minor loss in aptitude and a significant increase in unreliability. We find that LLMs often make assumptions in early turns and prematurely attempt to generate final solutions, on which they overly rely. In simpler terms, we discover that *when LLMs take a wrong turn in a conversation, they get lost and do not recover*.

</details>


### [37] [Towards Robust Few-Shot Text Classification Using Transformer Architectures and Dual Loss Strategies](https://arxiv.org/abs/2505.06145)

*Xu Han, Yumeng Sun, Weiqiang Huang, Hongye Zheng, Junliang Du*

**Main category:** cs.CL

**Keywords:** few-shot classification, Transformer models, contrastive learning, adaptive fine-tuning, regularization

**Relevance Score:** 7

**TL;DR:** The paper presents a strategy integrating adaptive fine-tuning, contrastive learning, and regularization optimization to enhance few-shot text classification accuracy using Transformer models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Few-shot text classification is crucial in low-resource settings, and improving model performance in these situations is necessary.

**Method:** The proposed strategy combines adaptive fine-tuning, contrastive learning, and regularization optimization, tested on the FewRel 2.0 dataset with models like T5-small, DeBERTa-v3, and RoBERTa-base.

**Key Contributions:**

	1. Adaptive fine-tuning for few-shot tasks
	2. Integration of contrastive and regularization losses
	3. Demonstration of Transformer model capabilities in classification accuracy

**Result:** Experiments show improved classification performance, particularly in 5-shot settings, highlighting challenges with certain relationship categories due to complex feature distributions.

**Limitations:** The study emphasizes challenges with specific relationship categories due to fuzzy semantics and feature distribution complexities.

**Conclusion:** Introducing contrastive loss and regularization loss enhances generalization and mitigates overfitting in few-shot classification scenarios.

**Abstract:** Few-shot text classification has important application value in low-resource environments. This paper proposes a strategy that combines adaptive fine-tuning, contrastive learning, and regularization optimization to improve the classification performance of Transformer-based models. Experiments on the FewRel 2.0 dataset show that T5-small, DeBERTa-v3, and RoBERTa-base perform well in few-shot tasks, especially in the 5-shot setting, which can more effectively capture text features and improve classification accuracy. The experiment also found that there are significant differences in the classification difficulty of different relationship categories. Some categories have fuzzy semantic boundaries or complex feature distributions, making it difficult for the standard cross entropy loss to learn the discriminative information required to distinguish categories. By introducing contrastive loss and regularization loss, the generalization ability of the model is enhanced, effectively alleviating the overfitting problem in few-shot environments. In addition, the research results show that the use of Transformer models or generative architectures with stronger self-attention mechanisms can help improve the stability and accuracy of few-shot classification.

</details>


### [38] [Can Prompting LLMs Unlock Hate Speech Detection across Languages? A Zero-shot and Few-shot Study](https://arxiv.org/abs/2505.06149)

*Faeze Ghorbanpour, Daryna Dementieva, Alexander Fraser*

**Main category:** cs.CL

**Keywords:** hate speech detection, multilingual LLMs, prompt engineering, zero-shot learning, few-shot learning

**Relevance Score:** 8

**TL;DR:** This work evaluates multilingual hate speech detection using LLMs with zero-shot and few-shot prompting, highlighting the importance of prompt design across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the neglect of linguistic diversity in automated hate speech detection approaches and explore the effectiveness of multilingual LLMs.

**Method:** Evaluation of LLM prompting techniques across eight non-English languages, comparing them to fine-tuned encoder models in real-world and functional tests.

**Key Contributions:**

	1. Evaluation of hate speech detection capabilities of LLMs across multiple languages
	2. Comparison of prompting techniques against fine-tuned models
	3. Identification of critical role of customized prompting for language-specific performance

**Result:** Zero-shot and few-shot prompting methods generally underperform compared to fine-tuned models but show better generalization on functional tests for hate speech detection; the need for customized prompts was identified for each language.

**Limitations:** 

**Conclusion:** Prompt design is crucial for enhancing hate speech detection performance across languages, emphasizing the necessity for tailored approaches in multilingual contexts.

**Abstract:** Despite growing interest in automated hate speech detection, most existing approaches overlook the linguistic diversity of online content. Multilingual instruction-tuned large language models such as LLaMA, Aya, Qwen, and BloomZ offer promising capabilities across languages, but their effectiveness in identifying hate speech through zero-shot and few-shot prompting remains underexplored. This work evaluates LLM prompting-based detection across eight non-English languages, utilizing several prompting techniques and comparing them to fine-tuned encoder models. We show that while zero-shot and few-shot prompting lag behind fine-tuned encoder models on most of the real-world evaluation sets, they achieve better generalization on functional tests for hate speech detection. Our study also reveals that prompt design plays a critical role, with each language often requiring customized prompting techniques to maximize performance.

</details>


### [39] [A Scaling Law for Token Efficiency in LLM Fine-Tuning Under Fixed Compute Budgets](https://arxiv.org/abs/2505.06150)

*Ryan Lagasse, Aidan Kiernans, Avijit Ghosh, Shiri Dori-Hacohen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Data Composition, Token Efficiency, Scaling Laws

**Relevance Score:** 8

**TL;DR:** The paper introduces a scaling law for fine-tuning large language models, emphasizing the importance of data composition in addition to total tokens.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To refine conventional fine-tuning approaches for large language models by incorporating dataset volume, which includes example counts and average token lengths.

**Method:** Experiments conducted on the BRICC and MMLU datasets, applying various subsampling strategies to assess the effects of data composition on training efficiency.

**Key Contributions:**

	1. Introduced a new scaling law for fine-tuning LLMs considering data composition.
	2. Demonstrated the impact of dataset volume on model performance.
	3. Provided insights for practical LLM fine-tuning under compute constraints.

**Result:** Data composition was found to significantly influence token efficiency when fine-tuning large language models.

**Limitations:** 

**Conclusion:** Refined scaling laws for LLM fine-tuning can lead to better performance in resource-constrained environments by accounting for data composition.

**Abstract:** We introduce a scaling law for fine-tuning large language models (LLMs) under fixed compute budgets that explicitly accounts for data composition. Conventional approaches measure training data solely by total tokens, yet the number of examples and their average token length -- what we term \emph{dataset volume} -- play a decisive role in model performance. Our formulation is tuned following established procedures. Experiments on the BRICC dataset \cite{salavati2024reducing} and subsets of the MMLU dataset \cite{hendrycks2021measuringmassivemultitasklanguage}, evaluated under multiple subsampling strategies, reveal that data composition significantly affects token efficiency. These results motivate refined scaling laws for practical LLM fine-tuning in resource-constrained settings.

</details>


### [40] [Estimating Quality in Therapeutic Conversations: A Multi-Dimensional Natural Language Processing Framework](https://arxiv.org/abs/2505.06151)

*Alice Rueda, Argyrios Perivolaris, Niloy Roy, Dylan Weston, Sarmed Shaya, Zachary Cote, Martin Ivanov, Bazen G. Teferra, Yuqi Wu, Sirisha Rambhatla, Divya Sharma, Andrew Greenshaw, Rakesh Jetly, Yanbo Zhang, Bo Cao, Reza Samavi, Sridhar Krishnan, Venkat Bhat*

**Main category:** cs.CL

**Keywords:** NLP, therapeutic engagement, machine learning, counseling, data augmentation

**Relevance Score:** 8

**TL;DR:** This paper presents a multi-dimensional NLP framework for classifying the quality of engagement in counseling sessions using textual transcripts.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To improve therapeutic success by objectively measuring client-therapist engagement during therapy sessions.

**Method:** The framework extracts 42 features across four domains (conversational dynamics, semantic similarity, sentiment classification, and question detection) from 253 motivational interviewing transcripts and uses classifiers such as Random Forest, Cat-Boost, and Support Vector Machines to evaluate engagement quality.

**Key Contributions:**

	1. Introduction of a scalable NLP framework for measuring therapeutic engagement
	2. Demonstration of significant accuracy improvements with data augmentation
	3. Identification of key features impacting engagement quality

**Result:** Random Forest achieved the highest accuracy of 88.9% and SVM reached an AUC of 94.6% after data augmentation, demonstrating the framework's effectiveness.

**Limitations:** Currently limited to text-based data; lacks multimodal integration.

**Conclusion:** The proposed method allows for real-time feedback for clinicians, with potential for multimodal applications in the future.

**Abstract:** Engagement between client and therapist is a critical determinant of therapeutic success. We propose a multi-dimensional natural language processing (NLP) framework that objectively classifies engagement quality in counseling sessions based on textual transcripts. Using 253 motivational interviewing transcripts (150 high-quality, 103 low-quality), we extracted 42 features across four domains: conversational dynamics, semantic similarity as topic alignment, sentiment classification, and question detection. Classifiers, including Random Forest (RF), Cat-Boost, and Support Vector Machines (SVM), were hyperparameter tuned and trained using a stratified 5-fold cross-validation and evaluated on a holdout test set. On balanced (non-augmented) data, RF achieved the highest classification accuracy (76.7%), and SVM achieved the highest AUC (85.4%). After SMOTE-Tomek augmentation, performance improved significantly: RF achieved up to 88.9% accuracy, 90.0% F1-score, and 94.6% AUC, while SVM reached 81.1% accuracy, 83.1% F1-score, and 93.6% AUC. The augmented data results reflect the potential of the framework in future larger-scale applications. Feature contribution revealed conversational dynamics and semantic similarity between clients and therapists were among the top contributors, led by words uttered by the client (mean and standard deviation). The framework was robust across the original and augmented datasets and demonstrated consistent improvements in F1 scores and recall. While currently text-based, the framework supports future multimodal extensions (e.g., vocal tone, facial affect) for more holistic assessments. This work introduces a scalable, data-driven method for evaluating engagement quality of the therapy session, offering clinicians real-time feedback to enhance the quality of both virtual and in-person therapeutic interactions.

</details>


### [41] [Query-driven Document-level Scientific Evidence Extraction from Biomedical Studies](https://arxiv.org/abs/2505.06186)

*Massimiliano Pronesti, Joao Bettencourt-Silva, Paul Flanagan, Alessandra Pascale, Oisin Redmond, Anya Belz, Yufang Hou*

**Main category:** cs.CL

**Keywords:** evidence extraction, CochraneForest, retrieval-augmented generation, clinical research, biomedical studies

**Relevance Score:** 6

**TL;DR:** This paper introduces a new dataset and a framework for extracting scientific evidence from biomedical studies, outperforming existing methods in F1 score.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve evidence extraction for clinical research questions with conflicting evidence, which is critical in synthesizing biomedical evidence.

**Method:** The authors created the CochraneForest dataset from forest plots of Cochrane systematic reviews and proposed URCA, a retrieval-augmented generation framework for evidence extraction.

**Key Contributions:**

	1. Creation of CochraneForest dataset with 202 annotated forest plots
	2. Introduction of URCA framework for evidence extraction
	3. Demonstration of performance improvement over existing methods in F1 score

**Result:** URCA improved performance over existing methods by up to 10.3% in F1 score for document-level scientific evidence extraction tasks.

**Limitations:** The CochraneForest is complex, which may pose challenges for fully automated evidence synthesis systems.

**Conclusion:** The complexity of CochraneForest presents a significant challenge for advancing automated evidence synthesis systems, indicating further research is needed.

**Abstract:** Extracting scientific evidence from biomedical studies for clinical research questions (e.g., Does stem cell transplantation improve quality of life in patients with medically refractory Crohn's disease compared to placebo?) is a crucial step in synthesising biomedical evidence. In this paper, we focus on the task of document-level scientific evidence extraction for clinical questions with conflicting evidence. To support this task, we create a dataset called CochraneForest, leveraging forest plots from Cochrane systematic reviews. It comprises 202 annotated forest plots, associated clinical research questions, full texts of studies, and study-specific conclusions. Building on CochraneForest, we propose URCA (Uniform Retrieval Clustered Augmentation), a retrieval-augmented generation framework designed to tackle the unique challenges of evidence extraction. Our experiments show that URCA outperforms the best existing methods by up to 10.3% in F1 score on this task. However, the results also underscore the complexity of CochraneForest, establishing it as a challenging testbed for advancing automated evidence synthesis systems.

</details>


### [42] [PART: Pre-trained Authorship Representation Transformer](https://arxiv.org/abs/2209.15373)

*Javier Huertas-Tato, Alejandro Martin, David Camacho*

**Main category:** cs.CL

**Keywords:** authorship attribution, embeddings, stylometry, contrastive learning, machine learning

**Relevance Score:** 5

**TL;DR:** The paper presents PART, a contrastively trained model for learning authorship embeddings using a diverse dataset, achieving notable results in identifying authorship traits.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Prior works on authorship attribution struggled with out-of-domain authors due to reliance on hand-crafted features; this paper aims to improve authorship identification through stylometric representations.

**Method:** The authors developed a contrastively trained model named PART, trained on approximately 1.5 million texts spanning various authors and contexts to create authorship embeddings.

**Key Contributions:**

	1. Introduction of the method PART for learning authorship embeddings
	2. Large-scale training on diverse author datasets
	3. Competitive zero-shot performance compared to existing models

**Result:** PART achieved zero-shot 72.39% accuracy on 250 authors, outperforming RoBERTa embeddings by 54% and 56% in two different tests.

**Limitations:** The effectiveness of the model may depend on the diversity of the training dataset and may require further validation on different types of texts.

**Conclusion:** The proposed model demonstrates a significant advancement in authorship identification, with capabilities to assess authorship features like gender and occupation through data visualization.

**Abstract:** Authors writing documents imprint identifying information within their texts: vocabulary, registry, punctuation, misspellings, or even emoji usage. Previous works use hand-crafted features or classification tasks to train their authorship models, leading to poor performance on out-of-domain authors. Using stylometric representations is more suitable, but this by itself is an open research challenge. In this paper, we propose PART, a contrastively trained model fit to learn \textbf{authorship embeddings} instead of semantics. We train our model on ~1.5M texts belonging to 1162 literature authors, 17287 blog posters and 135 corporate email accounts; a heterogeneous set with identifiable writing styles. We evaluate the model on current challenges, achieving competitive performance. We also evaluate our model on test splits of the datasets achieving zero-shot 72.39\% accuracy when bounded to 250 authors, a 54\% and 56\% higher than RoBERTa embeddings. We qualitatively assess the representations with different data visualizations on the available datasets, observing features such as gender, age, or occupation of the author.

</details>


### [43] [Talking Heads: Understanding Inter-layer Communication in Transformer Language Models](https://arxiv.org/abs/2406.09519)

*Jack Merullo, Carsten Eickhoff, Ellie Pavlick*

**Main category:** cs.CL

**Keywords:** transformer language models, context retrieval, low-rank subspaces

**Relevance Score:** 8

**TL;DR:** The paper analyzes how transformer language models route information across layers, focusing on the low-rank subspace mechanism that impacts context retrieval and performance on list-based tasks.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the representation and routing of information in transformer language models, particularly how they manage context retrieval tasks.

**Method:** The authors analyze two language models, studying their use of low-rank communication channels and decomposing attention heads using Singular Value Decomposition (SVD) to uncover interactions between heads across layers.

**Key Contributions:**

	1. Identified low-rank communication channels in transformer models.
	2. Explained performance inconsistencies related to item order using subspace analysis.
	3. Demonstrated a method to enhance model performance on context retrieval tasks.

**Result:** The study reveals that models utilize low-rank subspaces to manage context, explaining performance variability in responding to item order in prompts, and showing a way to improve task accuracy by over 20% on a list recall task.

**Limitations:** 

**Conclusion:** The findings highlight a complex and interpretable structure in language models that emerges from pretraining, shedding light on why these models may struggle in simpler domains, which could inform future research on more complex behaviors.

**Abstract:** Although it is known that transformer language models (LMs) pass features from early layers to later layers, it is not well understood how this information is represented and routed by the model. We analyze a mechanism used in two LMs to selectively inhibit items in a context in one task, and find that it underlies a commonly used abstraction across many context-retrieval behaviors. Specifically, we find that models write into low-rank subspaces of the residual stream to represent features which are then read out by later layers, forming low-rank communication channels (Elhage et al., 2021) between layers. A particular 3D subspace in model activations in GPT-2 can be traversed to positionally index items in lists, and we show that this mechanism can explain an otherwise arbitrary-seeming sensitivity of the model to the order of items in the prompt. That is, the model has trouble copying the correct information from context when many items ``crowd" this limited space. By decomposing attention heads with the Singular Value Decomposition (SVD), we find that previously described interactions between heads separated by one or more layers can be predicted via analysis of their weight matrices alone. We show that it is possible to manipulate the internal model representations as well as edit model weights based on the mechanism we discover in order to significantly improve performance on our synthetic Laundry List task, which requires recall from a list, often improving task accuracy by over 20%. Our analysis reveals a surprisingly intricate interpretable structure learned from language model pretraining, and helps us understand why sophisticated LMs sometimes fail in simple domains, facilitating future analysis of more complex behaviors.

</details>


### [44] [NeedleBench: Can LLMs Do Retrieval and Reasoning in Information-Dense Context?](https://arxiv.org/abs/2407.11963)

*Mo Li, Songyang Zhang, Taolin Zhang, Haodong Duan, Yunxin Liu, Kai Chen*

**Main category:** cs.CL

**Keywords:** large language models, long-context evaluation, retrieval and reasoning, NeedleBench, under-thinking

**Relevance Score:** 9

**TL;DR:** NeedleBench is a synthetic framework designed to evaluate retrieval and reasoning capabilities in bilingual long-context tasks, addressing limitations of existing evaluation methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To effectively assess the performance of large language models (LLMs) on long-context tasks without the biases introduced by real-world texts or irrelevant filler content.

**Method:** NeedleBench embeds key data points at various depths to create two types of tasks: information-sparse and information-dense, systematically testing models on their retrieval and reasoning capabilities.

**Key Contributions:**

	1. Introduction of a new framework for evaluating LLMs' long-context performance
	2. Identification of the 'under-thinking' phenomenon in models
	3. Categorization of tasks into information-sparse and information-dense scenarios

**Result:** Models like Deepseek-R1 and OpenAI's o3 performed well in mathematical reasoning but struggled with information-dense retrieval and reasoning, revealing a phenomenon of 'under-thinking'.

**Limitations:** 

**Conclusion:** NeedleBench provides vital insights for evaluating long-context capabilities of LLMs and is made available as an open-source resource.

**Abstract:** The capability of large language models to handle long-context information is crucial across various real-world applications. Existing evaluation methods often rely either on real-world long texts, making it difficult to exclude the influence of models' inherent knowledge, or introduce irrelevant filler content to artificially achieve target lengths, reducing assessment effectiveness. To address these limitations, we introduce NeedleBench, a synthetic framework for assessing retrieval and reasoning performance in bilingual long-context tasks with adaptive context lengths. NeedleBench systematically embeds key data points at varying depths to rigorously test model capabilities. Tasks are categorized into two scenarios: information-sparse, featuring minimal relevant details within extensive irrelevant text to simulate simple retrieval tasks; and information-dense (the Ancestral Trace Challenge), where relevant information is continuously distributed throughout the context to simulate complex reasoning tasks. Our experiments reveal that although recent reasoning models like Deepseek-R1 and OpenAI's o3 excel in mathematical reasoning, they struggle with continuous retrieval and reasoning in information-dense scenarios, even at shorter context lengths. We also characterize a phenomenon termed 'under-thinking', where models prematurely conclude reasoning despite available information. NeedleBench thus provides critical insights and targeted tools essential for evaluating and improving LLMs' long-context capabilities. All resources are available at OpenCompass: https://github.com/open-compass/opencompass.

</details>


### [45] [ReLiK: Retrieve and LinK, Fast and Accurate Entity Linking and Relation Extraction on an Academic Budget](https://arxiv.org/abs/2408.00103)

*Riccardo Orlando, Pere-Lluis Huguet Cabot, Edoardo Barba, Roberto Navigli*

**Main category:** cs.CL

**Keywords:** Entity Linking, Relation Extraction, Natural Language Processing, Retriever-Reader architecture, Information Extraction

**Relevance Score:** 8

**TL;DR:** The paper introduces ReLiK, a novel Retriever-Reader architecture for Entity Linking (EL) and Relation Extraction (RE) that enhances efficiency and performance by integrating candidate entities with input text.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Entity Linking (EL) and Relation Extraction (RE) are vital tasks in NLP with numerous applications, thus improvements in these tasks can significantly enhance various systems.

**Method:** ReLiK employs a Retriever module to identify potential entities or relations in the input text, followed by a Reader module that aligns these entities or relations with specific textual spans, utilizing a new input representation to function in a single forward pass.

**Key Contributions:**

	1. Introduction of the ReLiK architecture for combined EL and RE tasks
	2. Innovative input representation for efficiency in processing
	3. Achieving state-of-the-art performance with significant speed improvements

**Result:** The proposed method achieves state-of-the-art performance on both in-domain and out-of-domain benchmarks while demonstrating up to 40x faster inference speed compared to existing methods.

**Limitations:** 

**Conclusion:** ReLiK advances the state of the art in EL and RE and shows potential for streamlined Information Extraction by employing a unified Reader for simultaneous entity and relation extraction.

**Abstract:** Entity Linking (EL) and Relation Extraction (RE) are fundamental tasks in Natural Language Processing, serving as critical components in a wide range of applications. In this paper, we propose ReLiK, a Retriever-Reader architecture for both EL and RE, where, given an input text, the Retriever module undertakes the identification of candidate entities or relations that could potentially appear within the text. Subsequently, the Reader module is tasked to discern the pertinent retrieved entities or relations and establish their alignment with the corresponding textual spans. Notably, we put forward an innovative input representation that incorporates the candidate entities or relations alongside the text, making it possible to link entities or extract relations in a single forward pass and to fully leverage pre-trained language models contextualization capabilities, in contrast with previous Retriever-Reader-based methods, which require a forward pass for each candidate. Our formulation of EL and RE achieves state-of-the-art performance in both in-domain and out-of-domain benchmarks while using academic budget training and with up to 40x inference speed compared to competitors. Finally, we show how our architecture can be used seamlessly for Information Extraction (cIE), i.e. EL + RE, and setting a new state of the art by employing a shared Reader that simultaneously extracts entities and relations.

</details>


### [46] [Multi-Draft Speculative Sampling: Canonical Decomposition and Theoretical Limits](https://arxiv.org/abs/2410.18234)

*Ashish Khisti, M. Reza Ebrahimi, Hassan Dbouk, Arash Behboodi, Roland Memisevic, Christos Louizos*

**Main category:** cs.CL

**Keywords:** multi-draft sampling, token-level selection, importance sampling

**Relevance Score:** 6

**TL;DR:** This paper presents a two-step solution for multi-draft speculative sampling in token generation, improving block efficiency and token rates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of token-level draft selection in language models by optimizing the acceptance probability of generated tokens.

**Method:** A two-step approach is proposed: first, an importance sampling scheme selects an intermediate token, followed by single-draft speculative sampling to generate the output token, with theoretical foundations establishing conditions for optimal acceptance probability.

**Key Contributions:**

	1. Decomposed optimal scheme into two steps for improved token generation
	2. Established conditions for maximum acceptance probability
	3. Introduced new token-level selection schemes based on weighted importance sampling

**Result:** The proposed method shows consistent improvements in block efficiency and token rates compared to baseline schemes across various scenarios.

**Limitations:** 

**Conclusion:** The findings highlight a new class of token-level selection schemes based on weighted importance sampling that enhance token generation in multi-draft contexts.

**Abstract:** We consider multi-draft speculative sampling, where the proposal sequences are sampled independently from different draft models. At each step, a token-level draft selection scheme takes a list of valid tokens as input and produces an output token whose distribution matches that of the target model. Previous works have demonstrated that the optimal scheme (which maximizes the probability of accepting one of the input tokens) can be cast as a solution to a linear program. In this work we show that the optimal scheme can be decomposed into a two-step solution: in the first step an importance sampling (IS) type scheme is used to select one intermediate token; in the second step (single-draft) speculative sampling is applied to generate the output token. For the case of two identical draft models we further 1) establish a necessary and sufficient condition on the distributions of the target and draft models for the acceptance probability to equal one and 2) provide an explicit expression for the optimal acceptance probability. Our theoretical analysis also motives a new class of token-level selection schemes based on weighted importance sampling. Our experimental results demonstrate consistent improvements in the achievable block efficiency and token rates over baseline schemes in a number of scenarios.

</details>


### [47] [SRA-MCTS: Self-driven Reasoning Augmentation with Monte Carlo Tree Search for Code Generation](https://arxiv.org/abs/2411.11053)

*Bin Xu, Yiguan Lin, Yinghao Li, Yang Gao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning, Data Generation, Machine Learning, Complex Problem Solving

**Relevance Score:** 9

**TL;DR:** This paper introduces SRA-MCTS, a reasoning-augmented data generation process that improves complex problem solving in large language models by autonomously generating reasoning paths.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models struggle with complex problem-solving due to inadequate reasoning and problem decomposition, prompting the need for improved strategies.

**Method:** The proposed method, SRA-MCTS, utilizes a self-guided process to generate high-quality intermediate reasoning paths without additional supervision.

**Key Contributions:**

	1. Introduction of SRA-MCTS for reasoning-augmented data generation.
	2. Demonstrated improvements in solving complex tasks without extra supervision.
	3. Robust performance even when traditional methods degrade.

**Result:** Experimental results indicate that SRA-MCTS leads to improved performance in solving complex tasks across various model scales, even as traditional methods falter.

**Limitations:** 

**Conclusion:** The research suggests that enhancing training data with reasoning processes can significantly advance language models' capabilities in tackling complex problems.

**Abstract:** Large language models demonstrate exceptional performance in simple code generation tasks but still face challenges in tackling complex problems. These challenges may stem from insufficient reasoning and problem decomposition capabilities. To address this issue, we propose a reasoning-augmented data generation process, SRA-MCTS, which guides the model to autonomously generate high-quality intermediate reasoning paths. This creates a positive feedback loop, enabling continuous improvement. Our method operates entirely through the model itself without requiring additional supervision. By synthesizing natural language reasoning paths and translating them into executable code, the approach ensures analytical accuracy and enhances the success rate in solving complex tasks. Experimental results show that, even without additional supervisory signals, our method achieves performance improvements across different model scales, demonstrating the significant potential of self-improvement in small models. Furthermore, the method remains robust when traditional Chain-of-Thought (CoT) approaches exhibit performance degradation, with notable improvements observed in diversity metrics such as pass@10. We encourage further exploration of reasoning processes within training data to enhance the ability of language models to address complex problems. Our code and data are public at https://github.com/DIRECT-BIT/SRA-MCTS.

</details>


### [48] [Can open source large language models be used for tumor documentation in Germany? -- An evaluation on urological doctors' notes](https://arxiv.org/abs/2501.12106)

*Stefan Lenz, Arsenij Ustjanzew, Marco Jeray, Meike Ressing, Torsten Panholzer*

**Main category:** cs.CL

**Keywords:** large language models, tumor documentation, medical NLP

**Relevance Score:** 9

**TL;DR:** This paper evaluates eleven open source large language models (LLMs) for automating tumor documentation in Germany, testing tasks such as diagnosis identification and ICD-10 code assignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and reliability of tumor documentation, which is currently done manually in Germany.

**Method:** The study tested eleven open source LLMs ranging from 1-70 billion parameters on tasks of tumor diagnosis identification, ICD-10 code assignment, and extraction of diagnosis dates, using a specially prepared dataset from urology notes.

**Key Contributions:**

	1. Evaluation of multiple LLMs for medical documentation tasks
	2. Creation of a new dataset for German-language medical NLP
	3. Recommendations for optimal model sizes and prompting strategies

**Result:** The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well, while smaller models showed lower performance. Few-shot prompting with different medical examples improved outcomes.

**Limitations:** 

**Conclusion:** Open source LLMs have strong potential to automate tumor documentation with optimal models in the 7-12 billion parameter range, suggesting these could be effective tools for clinical documentation with further fine-tuning and prompting.

**Abstract:** Tumor documentation in Germany is largely done manually, requiring reading patient records and entering data into structured databases. Large language models (LLMs) could potentially enhance this process by improving efficiency and reliability. This evaluation tests eleven different open source LLMs with sizes ranging from 1-70 billion model parameters on three basic tasks of the tumor documentation process: identifying tumor diagnoses, assigning ICD-10 codes, and extracting the date of first diagnosis. For evaluating the LLMs on these tasks, a dataset of annotated text snippets based on anonymized doctors' notes from urology was prepared. Different prompting strategies were used to investigate the effect of the number of examples in few-shot prompting and to explore the capabilities of the LLMs in general. The models Llama 3.1 8B, Mistral 7B, and Mistral NeMo 12 B performed comparably well in the tasks. Models with less extensive training data or having fewer than 7 billion parameters showed notably lower performance, while larger models did not display performance gains. Examples from a different medical domain than urology could also improve the outcome in few-shot prompting, which demonstrates the ability of LLMs to handle tasks needed for tumor documentation. Open source LLMs show a strong potential for automating tumor documentation. Models from 7-12 billion parameters could offer an optimal balance between performance and resource efficiency. With tailored fine-tuning and well-designed prompting, these models might become important tools for clinical documentation in the future. The code for the evaluation is available from https://github.com/stefan-m-lenz/UroLlmEval. We also release the dataset as a new valuable resource that addresses the shortage of authentic and easily accessible benchmarks in German-language medical NLP.

</details>


### [49] [JustLogic: A Comprehensive Benchmark for Evaluating Deductive Reasoning in Large Language Models](https://arxiv.org/abs/2501.14851)

*Michael K. Chen, Xikun Zhang, Dacheng Tao*

**Main category:** cs.CL

**Keywords:** Large Language Models, deductive reasoning, benchmark, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** JustLogic is a new deductive reasoning benchmark for evaluating LLMs, addressing inadequacies in existing benchmarks by emphasizing complexity and independence from prior knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the evaluation and advancement of Large Language Models' deductive reasoning capabilities, which are currently hampered by inadequate benchmarks.

**Method:** JustLogic provides a synthetically generated benchmark that is highly complex and allows for diverse linguistic patterns and argument structures while being independent of prior knowledge.

**Key Contributions:**

	1. Introduction of JustLogic as a new benchmark
	2. Focus on task complexity and linguistic diversity
	3. Error analysis on reasoning depth and argument structure

**Result:** Experimental results indicate that state-of-the-art reasoning models perform comparably to but worse than humans, and non-reasoning models perform below human averages.

**Limitations:** 

**Conclusion:** JustLogic offers a more rigorous evaluation framework for LLMs, with insights into reasoning depth and argument form affecting model accuracy.

**Abstract:** Logical reasoning is a critical component of Large Language Models (LLMs), and substantial research efforts in recent years have aimed to enhance their deductive reasoning capabilities. However, existing deductive reasoning benchmarks, which are crucial for evaluating and advancing LLMs, are inadequate due to their lack of task complexity, presence of prior knowledge as a confounder, and superficial error analysis. To address these deficiencies, we introduce JustLogic, a synthetically generated deductive reasoning benchmark designed for rigorous evaluation of LLMs. JustLogic is (i) highly complex, capable of generating a diverse range of linguistic patterns, vocabulary, and argument structures; (ii) prior knowledge independent, eliminating the advantage of models possessing prior knowledge and ensuring that only deductive reasoning is used to answer questions; and (iii) capable of in-depth error analysis on the heterogeneous effects of reasoning depth and argument form on model accuracy. Our experimental results on JustLogic reveal that (i) state-of-the-art (SOTA) reasoning LLMs perform on par or better than the human average but significantly worse than the human ceiling, and (ii) SOTA non-reasoning models still underperform the human average. All code and data are available at https://github.com/michaelchen-lab/JustLogic

</details>


### [50] [AdaCoT: Rethinking Cross-Lingual Factual Reasoning through Adaptive Chain-of-Thought](https://arxiv.org/abs/2501.16154)

*Xin Huang, Tarun Kumar Vangani, Zhengyuan Liu, Bowei Zou, Ai Ti Aw*

**Main category:** cs.CL

**Keywords:** multilingual reasoning, large language models, adaptive reasoning

**Relevance Score:** 9

**TL;DR:** The paper introduces AdaCoT, a framework for enhancing multilingual factual reasoning in large language models by dynamically routing thought processes through intermediary languages before producing responses, demonstrating improved performance especially in low-resource languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address performance disparities in multilingual reasoning due to imbalanced training data and existing limitations in cross-lingual tuning techniques.

**Method:** AdaCoT uses an adaptive, reward-based mechanism to dynamically select reasoning pathways in intermediary languages without requiring additional pretraining.

**Key Contributions:**

	1. Introduction of the AdaCoT framework for multilingual reasoning
	2. Demonstrated significant performance improvements in low-resource languages
	3. Adaptive mechanism for optimal reasoning pathway selection

**Result:** Substantial improvements in factual reasoning quality and cross-lingual consistency were observed, particularly benefiting low-resource languages.

**Limitations:** 

**Conclusion:** AdaCoT effectively narrows the performance gap between high and low-resource languages while preserving cultural and linguistic nuances.

**Abstract:** Large language models have shown impressive multilingual capabilities through pretraining on diverse corpora. While these models show strong reasoning abilities, their performance varies significantly across languages due to imbalanced training data distribution. Existing approaches using sample-level translation for extensive multilingual pretraining and cross-lingual tuning face scalability challenges and often fail to capture nuanced reasoning processes across languages. In this paper, we introduce AdaCoT (Adaptive Chain-of-Thought), a framework that enhances multilingual factual reasoning by dynamically routing thought processes in intermediary ``thinking languages'' before generating target-language responses. AdaCoT leverages a language-agnostic core and incorporates an adaptive, reward-based mechanism for selecting optimal reasoning pathways without requiring additional pretraining. Our comprehensive evaluation across multiple benchmarks demonstrates substantial improvements in both factual reasoning quality and cross-lingual consistency, with particularly strong performance gains in low-resource language settings. The results suggest that adaptive reasoning paths can effectively bridge the performance gap between high and low-resource languages while maintaining cultural and linguistic nuances.

</details>


### [51] [Phonetic accommodation and inhibition in a dynamic neural field model](https://arxiv.org/abs/2502.01210)

*Sam Kirkham, Patrycja Strycharczuk, Rob Davies, Danielle Welburn*

**Main category:** cs.CL

**Keywords:** phonetic accommodation, speech planning, neural fields, memory dynamics, accent change

**Relevance Score:** 4

**TL;DR:** The paper presents a computational model for understanding how real-time speech input from one speaker influences the speech planning of another, focusing on phonetic accommodation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates how short-term phonetic accommodation affects accent change and how another speakerâs voice influences speech planning representations.

**Method:** A computational model based on dynamic neural field equations is used to explore changes in speech planning representations during phonetic accommodation, predicting how memory dynamics influence these changes.

**Key Contributions:**

	1. Introduces a computational model for phonetic accommodation based on neural field equations.
	2. Demonstrates the impact of memory dynamics on speech planning.
	3. Provides empirical support for the model through a pilot study.

**Result:** The model predicts that convergence to a model talker's voice can lead to divergence in speech planning due to delayed inhibitory effects in memory, with empirical data supporting these predictions.

**Limitations:** The study is based on a pilot experiment, which may have limitations in representativeness and scope.

**Conclusion:** The findings highlight the complex interplay between phonetic accommodation and sound change, suggesting that resistance to accommodation may be influenced by external pressures.

**Abstract:** Short-term phonetic accommodation is a fundamental driver behind accent change, but how does real-time input from another speaker's voice shape the speech planning representations of an interlocutor? We advance a computational model of change in speech planning representations during phonetic accommodation, grounded in dynamic neural field equations for movement planning and memory dynamics. A dual-layer planning/memory field predicts that convergence to a model talker on one trial can trigger divergence on subsequent trials, due to a delayed inhibitory effect in the more slowly evolving memory field. The model's predictions are compared with empirical patterns of accommodation from an experimental pilot study. We show that observed empirical phenomena may correspond to variation in the magnitude of inhibitory memory dynamics, which could reflect resistance to accommodation due to phonological and/or sociolinguistic pressures. We discuss the implications of these results for the relations between short-term phonetic accommodation and sound change.

</details>


### [52] [The Order Effect: Investigating Prompt Sensitivity to Input Order in LLMs](https://arxiv.org/abs/2502.04134)

*Bryan Guan, Tanya Roosta, Peyman Passban, Mehdi Rezagholizadeh*

**Main category:** cs.CL

**Keywords:** large language models, order sensitivity, input arrangement, reliability, high-stakes applications

**Relevance Score:** 9

**TL;DR:** This paper examines the issue of order sensitivity in large language models (LLMs), revealing that input arrangement significantly impacts output accuracy across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the reliability issues of LLMs affected by order sensitivity, which can lead to inconsistent or biased outputs, especially in high-stakes applications.

**Method:** The study conducts experiments on multiple tasks (paraphrasing, relevance judgment, and multiple-choice questions) to analyze how variations in input order affect performance.

**Key Contributions:**

	1. Investigates order sensitivity in LLMs
	2. Demonstrates significant impact of input arrangement on task performance
	3. Highlights the need for improved input-handling techniques

**Result:** Results indicate that shuffled inputs result in measurable declines in output accuracy, with few-shot prompting showing only mixed effectiveness in mitigation.

**Limitations:** Focuses on specific tasks and does not fully resolve the issue of order sensitivity across all potential applications.

**Conclusion:** There is a need for more robust LLMs or improved input-handling techniques to enhance reliability, particularly in critical applications.

**Abstract:** As large language models (LLMs) become integral to diverse applications, ensuring their reliability under varying input conditions is crucial. One key issue affecting this reliability is order sensitivity, wherein slight variations in the input arrangement can lead to inconsistent or biased outputs. Although recent advances have reduced this sensitivity, the problem remains unresolved. This paper investigates the extent of order sensitivity in LLMs whose internal components are hidden from users (such as closed-source models or those accessed via API calls). We conduct experiments across multiple tasks, including paraphrasing, relevance judgment, and multiple-choice questions. Our results show that input order significantly affects performance across tasks, with shuffled inputs leading to measurable declines in output accuracy. Few-shot prompting demonstrates mixed effectiveness and offers partial mitigation; however, fails to fully resolve the problem. These findings highlight persistent risks, particularly in high-stakes applications, and point to the need for more robust LLMs or improved input-handling techniques in future development.

</details>


### [53] [k-LLMmeans: Scalable, Stable, and Interpretable Text Clustering via LLM-based Centroids](https://arxiv.org/abs/2502.09667)

*Jairo Diaz-Rodriguez*

**Main category:** cs.CL

**Keywords:** text clustering, LLM, k-means, semantic interpretability, benchmark dataset

**Relevance Score:** 9

**TL;DR:** Introduction of k-LLMmeans, a novel algorithm for text clustering using LLM-generated summaries as centroids to enhance semantic interpretability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve text clustering by leveraging LLM-generated summaries, capturing semantic nuances missed by traditional k-means methods while maintaining optimization properties.

**Method:** The proposed k-LLMmeans algorithm modifies the k-means algorithm to use LLM-generated summaries as cluster centroids, also introducing a mini-batch variant for efficient, real-time clustering.

**Key Contributions:**

	1. Novel text clustering algorithm using LLM summaries as centroids
	2. Mini-batch variant for real-time clustering
	3. New benchmark dataset for text-stream clustering

**Result:** k-LLMmeans consistently outperforms traditional k-means and other baselines, achieving results comparable to state-of-the-art LLM-based clustering with fewer LLM calls.

**Limitations:** 

**Conclusion:** The k-LLMmeans approach provides efficient, interpretable text clustering applicable to streaming text, validated through extensive experiments and a new benchmark dataset.

**Abstract:** We introduce k-LLMmeans, a novel modification of the k-means algorithm for text clustering that leverages LLM-generated summaries as cluster centroids, capturing semantic nuances often missed by purely numerical averages. This design preserves the core optimization properties of k-means while enhancing semantic interpretability and avoiding the scalability and instability issues typical of modern LLM-based clustering. Unlike existing methods, our approach does not increase LLM usage with dataset size and produces transparent intermediate outputs. We further extend it with a mini-batch variant for efficient, real-time clustering of streaming text. Extensive experiments across multiple datasets, embeddings, and LLMs show that k-LLMmeans consistently outperforms k-means and other traditional baselines and achieves results comparable to state-of-the-art LLM-based clustering, with a fraction of the LLM calls. Finally, we present a case study on sequential text streams and introduce a new benchmark dataset constructed from StackExchange to evaluate text-stream clustering methods.

</details>


### [54] [Bridging Legal Knowledge and AI: Retrieval-Augmented Generation with Vector Stores, Knowledge Graphs, and Hierarchical Non-negative Matrix Factorization](https://arxiv.org/abs/2502.20364)

*Ryan C. Barron, Maksim E. Eren, Olga M. Serafimova, Cynthia Matuszek, Boian S. Alexandrov*

**Main category:** cs.CL

**Keywords:** Generative AI, Legal Information Retrieval, Large Language Models, Knowledge Graphs, Retrieval-Augmented Generation

**Relevance Score:** 6

**TL;DR:** This paper presents a generative AI system that enhances legal information retrieval by integrating Retrieval-Augmented Generation, Vector Stores, and Knowledge Graphs, along with Non-Negative Matrix Factorization, aimed at improving AI reasoning and minimizing hallucinations in complex legal data.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve legal research by efficiently extracting insights from complex, semi-structured legal data such as statutes and case law, thereby enhancing the operational efficiency of legal systems.

**Method:** The proposed system integrates Retrieval-Augmented Generation, Vector Stores, and Knowledge Graphs, constructed using Non-Negative Matrix Factorization, and employs web scraping to collect legal texts from public platforms.

**Key Contributions:**

	1. Integration of RAG, VS, and KG in legal AI systems
	2. Use of Non-Negative Matrix Factorization for knowledge construction
	3. Application of web scraping for systematic legal text collection

**Result:** The integrated system successfully identifies complex relationships among legal documents, enhances contextual understanding, and minimizes AI hallucinations while improving the efficiency of legal information retrieval.

**Limitations:** The approach may require continuous updates to the knowledge graph to maintain accuracy and relevance in rapidly evolving legal contexts.

**Conclusion:** The framework supports scalable and interpretable retrieval of legal documents and contributes to advancements in computational law and AI, providing a more effective approach to legal research than traditional methods.

**Abstract:** Agentic Generative AI, powered by Large Language Models (LLMs) with Retrieval-Augmented Generation (RAG), Knowledge Graphs (KGs), and Vector Stores (VSs), represents a transformative technology applicable to specialized domains such as legal systems, research, recommender systems, cybersecurity, and global security, including proliferation research. This technology excels at inferring relationships within vast unstructured or semi-structured datasets. The legal domain here comprises complex data characterized by extensive, interrelated, and semi-structured knowledge systems with complex relations. It comprises constitutions, statutes, regulations, and case law. Extracting insights and navigating the intricate networks of legal documents and their relations is crucial for effective legal research. Here, we introduce a generative AI system that integrates RAG, VS, and KG, constructed via Non-Negative Matrix Factorization (NMF), to enhance legal information retrieval and AI reasoning and minimize hallucinations. In the legal system, these technologies empower AI agents to identify and analyze complex connections among cases, statutes, and legal precedents, uncovering hidden relationships and predicting legal trends-challenging tasks that are essential for ensuring justice and improving operational efficiency. Our system employs web scraping techniques to systematically collect legal texts, such as statutes, constitutional provisions, and case law, from publicly accessible platforms like Justia. It bridges the gap between traditional keyword-based searches and contextual understanding by leveraging advanced semantic representations, hierarchical relationships, and latent topic discovery. This framework supports legal document clustering, summarization, and cross-referencing, for scalable, interpretable, and accurate retrieval for semi-structured data while advancing computational law and AI.

</details>


### [55] [ConvoGen: Enhancing Conversational AI with Synthetic Data: A Multi-Agent Approach](https://arxiv.org/abs/2503.17460)

*Reem Gody, Mahmoud Goudy, Ahmed Y. Tawfik*

**Main category:** cs.CL

**Keywords:** synthetic data, conversational AI, few-shot learning

**Relevance Score:** 7

**TL;DR:** ConvoGen is a framework for generating diverse synthetic conversational data through multi-agent systems and few-shot learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create high-quality synthetic conversational data for training and evaluating conversational AI models.

**Method:** Utilizes few-shot learning and iterative sampling from a dynamically updated few-shot hub.

**Key Contributions:**

	1. Introduction of the ConvoGen framework for synthetic conversational data generation
	2. Application of few-shot learning in multi-agent systems for data diversity
	3. Potential to augment existing datasets for conversational AI tasks.

**Result:** Demonstrated effectiveness in producing diverse and realistic conversational scenarios beneficial for various conversational AI tasks.

**Limitations:** 

**Conclusion:** ConvoGen shows great potential in enhancing the development and evaluation of conversational AI systems by providing high-quality synthetic data.

**Abstract:** In this paper, we present ConvoGen: an innovative framework for generating synthetic conversational data using multi-agent systems. Our method leverages few-shot learning and introduces iterative sampling from a dynamically updated few-shot hub to create diverse and realistic conversational scenarios. The generated data has numerous applications, including training and evaluating conversational AI models, and augmenting existing datasets for tasks like conversational intent classification or conversation summarization. Our experiments demonstrate the effectiveness of this method in producing high-quality diverse synthetic conversational data, highlighting its potential to enhance the development and evaluation of conversational AI systems.

</details>


### [56] [Reimagining Urban Science: Scaling Causal Inference with Large Language Models](https://arxiv.org/abs/2504.12345)

*Yutong Xia, Ao Qu, Yunhan Zheng, Yihong Tang, Dingyi Zhuang, Yuxuan Liang, Shenhao Wang, Cathy Wu, Lijun Sun, Roger Zimmermann, Jinhua Zhao*

**Main category:** cs.CL

**Keywords:** urban causal research, large language models, AI-augmented workflows, human-AI collaboration, policy recommendations

**Relevance Score:** 8

**TL;DR:** This paper presents AutoUrbanCI, a framework that utilizes large language models to enhance urban causal research by automating key processes and promoting equity and accountability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Urban causal research faces inefficiencies and biases due to traditional hypothesis generation and the complexity of multimodal data. LLMs offer a new way to improve urban causal analysis.

**Method:** The paper introduces AutoUrbanCI, which consists of four modular agents for hypothesis generation, data engineering, experiment design and execution, and results interpretation.

**Key Contributions:**

	1. Introduction of AutoUrbanCI framework leveraging LLMs for urban causal research
	2. Proposal of modular agents for different stages of causal analysis
	3. Emphasis on human-AI collaboration and inclusivity in research processes

**Result:** AutoUrbanCI aims to improve urban causal research through enhanced workflows that integrate AI while maintaining a focus on human expertise.

**Limitations:** 

**Conclusion:** The paper calls for a shift towards AI-augmented urban causal research to increase inclusivity, reproducibility, and collaborative human-AI efforts in policymaking.

**Abstract:** Urban causal research is essential for understanding the complex dynamics of cities and informing evidence-based policies. However, it is challenged by the inefficiency and bias of hypothesis generation, barriers to multimodal data complexity, and the methodological fragility of causal experimentation. Recent advances in large language models (LLMs) present an opportunity to rethink how urban causal analysis is conducted. This Perspective examines current urban causal research by analyzing taxonomies that categorize research topics, data sources, and methodological approaches to identify structural gaps. We then introduce an LLM-driven conceptual framework, AutoUrbanCI, composed of four distinct modular agents responsible for hypothesis generation, data engineering, experiment design and execution, and results interpretation with policy recommendations. We propose evaluation criteria for rigor and transparency and reflect on implications for human-AI collaboration, equity, and accountability. We call for a new research agenda that embraces AI-augmented workflows not as replacements for human expertise but as tools to broaden participation, improve reproducibility, and unlock more inclusive forms of urban causal reasoning.

</details>
