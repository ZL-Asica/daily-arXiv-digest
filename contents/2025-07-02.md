# 2025-07-02

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 18]

- [cs.CL](#cs.CL) [Total: 56]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [InSight-R: A Framework for Risk-informed Human Failure Event Identification and Interface-Induced Risk Assessment Driven by AutoGraph](https://arxiv.org/abs/2507.00066)

*Xingyu Xiao, Jiejuan Tong, Peng Chen, Jun Sun, Zhe Sui, Jingang Liang, Hongru Zhao, Jun Zhao, Haitao Wang*

**Main category:** cs.HC

**Keywords:** Human Reliability, Human-Machine Interface, HFE Identification, Risk Assessment, Interface Design

**Relevance Score:** 6

**TL;DR:** The study proposes the InSight-R framework, which enhances human reliability analysis in safety-critical domains by automating human failure event identification through empirical data and interface design optimization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve human reliability analysis in safety-critical domains like nuclear power by overcoming the limitations of conventional methods that rely on expert judgment and lack integration with interface-level data.

**Method:** The study introduces the InSight-R framework, which combines behavioral data with an interface-embedded knowledge graph to automate human failure event identification and assess the impact of interface design on operator performance.

**Key Contributions:**

	1. Development of the InSight-R framework for automated HFE identification
	2. Integration of empirical data with interface design to improve reliability
	3. Scalable approach for real-time human reliability assessment

**Result:** Results show that InSight-R enhances objectivity and interpretability of human failure event identification and provides a scalable method for real-time human reliability assessment in digital control environments.

**Limitations:** 

**Conclusion:** The framework contributes to better interface design optimization and advances risk-informed human reliability assessment methodologies.

**Abstract:** Human reliability remains a critical concern in safety-critical domains such as nuclear power, where operational failures are often linked to human error. While conventional human reliability analysis (HRA) methods have been widely adopted, they rely heavily on expert judgment for identifying human failure events (HFEs) and assigning performance influencing factors (PIFs). This reliance introduces challenges related to reproducibility, subjectivity, and limited integration of interface-level data. In particular, current approaches lack the capacity to rigorously assess how human-machine interface design contributes to operator performance variability and error susceptibility. To address these limitations, this study proposes a framework for risk-informed human failure event identification and interface-induced risk assessment driven by AutoGraph (InSight-R). By linking empirical behavioral data to the interface-embedded knowledge graph (IE-KG) constructed by the automated graph-based execution framework (AutoGraph), the InSight-R framework enables automated HFE identification based on both error-prone and time-deviated operational paths. Furthermore, we discuss the relationship between designer-user conflicts and human error. The results demonstrate that InSight-R not only enhances the objectivity and interpretability of HFE identification but also provides a scalable pathway toward dynamic, real-time human reliability assessment in digitalized control environments. This framework offers actionable insights for interface design optimization and contributes to the advancement of mechanism-driven HRA methodologies.

</details>


### [2] [Designing an Adaptive Storytelling Platform to Promote Civic Education in Politically Polarized Learning Environments](https://arxiv.org/abs/2507.00161)

*Christopher M. Wegemer, Edward Halim, Jeff Burke*

**Main category:** cs.HC

**Keywords:** political polarization, AI, civic education, emotion recognition, storytelling

**Relevance Score:** 8

**TL;DR:** This paper explores the use of AI in civic education to reduce political polarization through emotionally-responsive storytelling that adapts to students' emotional engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of political polarization in civic education and promote open-mindedness through technology.

**Method:** The study employed a design-based research approach to create an AI-mediated Digital Civic Storytelling (AI-DCS) platform that utilizes affective computing techniques.

**Key Contributions:**

	1. Development of AI-mediated storytelling platform for civic education.
	2. Utilization of affective computing for real-time narrative adaptation.
	3. Integration of GPT-4 for personalized storytelling based on user emotions.

**Result:** The platform integrates facial emotion recognition and attention tracking to adapt narratives in real-time, personalizing the storytelling experience based on user emotional states.

**Limitations:** 

**Conclusion:** AI-supported strategies can effectively enhance civic education interventions by addressing affective polarization while preserving student autonomy.

**Abstract:** Political polarization undermines democratic civic education by exacerbating identity-based resistance to opposing viewpoints. Emerging AI technologies offer new opportunities to advance interventions that reduce polarization and promote political open-mindedness. We examined novel design strategies that leverage adaptive and emotionally-responsive civic narratives that may sustain students' emotional engagement in stories, and in turn, promote perspective-taking toward members of political out-groups. Drawing on theories from political psychology and narratology, we investigate how affective computing techniques can support three storytelling mechanisms: transportation into a story world, identification with characters, and interaction with the storyteller. Using a design-based research (DBR) approach, we iteratively developed and refined an AI-mediated Digital Civic Storytelling (AI-DCS) platform. Our prototype integrates facial emotion recognition and attention tracking to assess users' affective and attentional states in real time. Narrative content is organized around pre-structured story outlines, with beat-by-beat language adaptation implemented via GPT-4, personalizing linguistic tone to sustain students' emotional engagement in stories that center political perspectives different from their own. Our work offers a foundation for AI-supported, emotionally-sensitive strategies that address affective polarization while preserving learner autonomy. We conclude with implications for civic education interventions, algorithmic literacy, and HCI challenges associated with AI dialogue management and affect-adaptive learning environments.

</details>


### [3] [Exploring AR Label Placements in Visually Cluttered Scenarios](https://arxiv.org/abs/2507.00198)

*Ji Hwan Park, Braden Roper, Amirhossein Arezoumand, Tien Tran*

**Main category:** cs.HC

**Keywords:** augmented reality, label placement, user interaction, spatial grouping, visual clutter

**Relevance Score:** 8

**TL;DR:** This paper investigates label placement techniques in augmented reality (AR) for cluttered scenes, highlighting the importance of spatial grouping for effective data summarization.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** As AR scenes become visually cluttered with multiple items, effective label placement becomes challenging, necessitating new strategies for optimal user interaction.

**Method:** Three label placement techniques were implemented and tested in AR environments with various types of items scattered within the user's field of view.

**Key Contributions:**

	1. Implementation of three novel label placement techniques for AR
	2. Evaluation of the effectiveness of spatial grouping of labels
	3. Insights into user interaction with visual clutter in AR environments

**Result:** The study found that spatially grouping labels for the same type of items improved users' ability to identify, compare, and summarize data effectively.

**Limitations:** The study is limited to specific types of AR environments and may not generalize to all scenarios.

**Conclusion:** Effective label placement in AR can significantly enhance the user experience by aiding in data recognition and organization in cluttered environments.

**Abstract:** We investigate methods for placing labels in AR environments that have visually cluttered scenes. As the number of items increases in a scene within the user' FOV, it is challenging to effectively place labels based on existing label placement guidelines. To address this issue, we implemented three label placement techniques for in-view objects for AR applications. We specifically target a scenario, where various items of different types are scattered within the user's field of view, and multiple items of the same type are situated close together. We evaluate three placement techniques for three target tasks. Our study shows that using a label to spatially group the same types of items is beneficial for identifying, comparing, and summarizing data.

</details>


### [4] [Examining the Social Communication and Community Engagement of Autistic Adults through an Asynchronous Focus Group](https://arxiv.org/abs/2507.00202)

*Blade Frisch, Betts Peters, Keith Vertanen*

**Main category:** cs.HC

**Keywords:** Augmentative and Alternative Communication, autistic adults, communication needs, social engagement, AAC design

**Relevance Score:** 6

**TL;DR:** This paper explores the unique communication needs of autistic adults and how Augmentative and Alternative Communication (AAC) can support them.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and address the distinct communication needs of autistic adults, which are often overlooked compared to other disabled populations.

**Method:** An online, asynchronous, text-based focus group was conducted with five autistic adults to discuss their social communication and community engagement.

**Key Contributions:**

	1. Identified the impact of emotional experiences on communication methods for autistic adults.
	2. Highlighted the benefits of AAC for speaking autistic individuals.
	3. Discussed dynamic communication needs during autistic shutdowns.

**Result:** The analysis revealed that emotional experiences affect communication methods, that AAC can benefit speaking autistic adults, and highlighted dynamic communication needs during autistic shutdowns.

**Limitations:** The study is based on a small focus group and may not capture the full spectrum of autistic adults' communication needs.

**Conclusion:** The study provides crucial implications for AAC design, including the need to support communication during shutdowns and better understand the fears associated with AAC use, guiding future research on autism and communication.

**Abstract:** Purpose: Little research has explored the communication needs of autistic adults and how their needs differ from those of other disabled populations. Augmentative and Alternative Communication (AAC) can support these communication needs, but more guidance is needed on how to design AAC to support this population.   Materials and Methods: We conducted an online, asynchronous, text-based focus group with five autistic adults to explore their social communication and community engagement and how AAC can help support them.   Results and Conclusion: Our analysis of the participant responses found that 1) participants' emotional experiences impacted the communication methods they used, 2) speaking autistic adults can benefit from AAC use, and 3) autistic shutdown creates dynamic communication needs. We present implications for future AAC design: supporting communication in times of shutdown, indicating communication ability to communication partners, and a need to better understand the fear of using AAC. These implications can inform the design for future AAC systems. We also provide themes for future autism research: exploring the impact of a late diagnosis, gaining a better understanding of the communication needs during autistic shutdown, and expanding research to include the social and environmental factors that impact communication. Finally, we provide guidance on how future online focus groups can be run in an accessible manner.

</details>


### [5] [User Concerns Regarding Social Robots for Mood Regulation: A Case Study on the "Sunday Blues"](https://arxiv.org/abs/2507.00271)

*Zhuochao Peng, Jiaxin Xu, Jun Hu, Haian Xue, Laurens A. G. Kolks, Pieter M. A. Desmet*

**Main category:** cs.HC

**Keywords:** social robots, mood regulation, human-robot interaction, design considerations, empathy

**Relevance Score:** 7

**TL;DR:** This study explores user perceptions of social robots, particularly focusing on mood regulation through a speculative robot concept named "Mora" and the context of the "Sunday Blues."

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how users view the integration of social robots into everyday life for mood regulation purposes.

**Method:** An exploratory case study using a video prototype and a co-constructing stories method involving 15 participants to discuss their interactions with the robot Mora.

**Key Contributions:**

	1. Introduction of the speculative robot concept 'Mora' for mood regulation
	2. Insights into user perceptions of empathy and ethical implications in social robots
	3. Design considerations derived from user interactions for future HRI research

**Result:** Participants shared varied insights about social robots, addressing attributes like empathy and intervention effectiveness, leading to useful design considerations for human-robot interaction.

**Limitations:** 

**Conclusion:** The study highlighted important reflections on social robots that can inform future research and development in HRI.

**Abstract:** While recent research highlights the potential of social robots to support mood regulation, little is known about how prospective users view their integration into everyday life. To explore this, we conducted an exploratory case study that used a speculative robot concept "Mora" to provoke reflection and facilitate meaningful discussion about using social robots to manage subtle, day-to-day emotional experiences. We focused on the "Sunday Blues," a common dip in mood that occurs at the end of the weekend, as a relatable context in which to explore individuals' insights. Using a video prototype and a co-constructing stories method, we engaged 15 participants in imagining interactions with Mora and discussing their expectations, doubts, and concerns. The study surfaced a range of nuanced reflections around the attributes of social robots like empathy, intervention effectiveness, and ethical boundaries, which we translated into design considerations for future research and development in human-robot interaction.

</details>


### [6] [Visual Privacy Management with Generative AI for Blind and Low-Vision People](https://arxiv.org/abs/2507.00286)

*Tanusree Sharma, Yu-Yun Tseng, Lotus Zhang, Ayae Ide, Kelly Avery Mack, Leah Findlater, Danna Gurari, Yang Wang*

**Main category:** cs.HC

**Keywords:** Generative AI, Blind and Low Vision, Visual Privacy, User-Centered Design, Accessibility

**Relevance Score:** 8

**TL;DR:** This paper explores the use of Generative AI tools by blind and low vision individuals, focusing on their current practices, design preferences, and the balance between visual accessibility and privacy concerns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how blind and low vision individuals use Generative AI tools for managing visual content while considering privacy implications.

**Method:** An interview study with 21 participants who are blind or low vision to gather insights on their practices and design preferences regarding GenAI tools.

**Key Contributions:**

	1. Investigation of privacy concerns in the use of GenAI by BLV individuals
	2. Identification of design preferences for enhancing visual privacy
	3. Actionable recommendations for developers of GenAI tools focused on accessibility and privacy.

**Result:** Participants reported various practices that balance privacy, efficiency, and emotional agency, identifying six scenarios where privacy risks are pertinent. Key design preferences included on-device processing and multimodal interaction methods.

**Limitations:** 

**Conclusion:** The study offers actionable design recommendations to enhance user-centered visual privacy in the context of Generative AI, advocating for a broader understanding of privacy and data handling.

**Abstract:** Blind and low vision (BLV) individuals use Generative AI (GenAI) tools to interpret and manage visual content in their daily lives. While such tools can enhance the accessibility of visual content and so enable greater user independence, they also introduce complex challenges around visual privacy. In this paper, we investigate the current practices and future design preferences of blind and low vision individuals through an interview study with 21 participants. Our findings reveal a range of current practices with GenAI that balance privacy, efficiency, and emotional agency, with users accounting for privacy risks across six key scenarios, such as self-presentation, indoor/outdoor spatial privacy, social sharing, and handling professional content. Our findings reveal design preferences, including on-device processing, zero-retention guarantees, sensitive content redaction, privacy-aware appearance indicators, and multimodal tactile mirrored interaction methods. We conclude with actionable design recommendations to support user-centered visual privacy through GenAI, expanding the notion of privacy and responsible handling of others data.

</details>


### [7] [When Kids Mode Isn't For Kids: Investigating TikTok's "Under 13 Experience"](https://arxiv.org/abs/2507.00299)

*Olivia Figueira, Pranathi Chamarthi, Tu Le, Athina Markopoulou*

**Main category:** cs.HC

**Keywords:** TikTok, Kids Mode, Content Curation, Child Safety, COPPA

**Relevance Score:** 6

**TL;DR:** This paper audits TikTok's Kids Mode, revealing significant shortcomings in content curation and safety measures for children.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate TikTok's Kids Mode due to its lack of research and transparency in content curation and safety for young users.

**Method:** An auditing methodology was proposed and applied to analyze the content and features of TikTok's Kids Mode.

**Key Contributions:**

	1. Proposal of an auditing methodology for TikTok's Kids Mode
	2. Characterization of content curation in Kids Mode
	3. Identification of safety and privacy gaps in TikTok for children

**Result:** It was found that 83% of videos on the Kids Mode 'For You' page are not child-directed, with inappropriate content also present, and critical safety features are missing.

**Limitations:** The study focuses solely on TikTok and may not represent broader patterns in Kids Modes on other platforms.

**Conclusion:** The findings suggest that TikTok's Kids Mode may not provide adequate protection for children, potentially pushing them towards the regular mode, which carries additional risks.

**Abstract:** TikTok, the social media platform that is popular among children and adolescents, offers a more restrictive "Under 13 Experience" exclusively for young users in the US, also known as TikTok's "Kids Mode". While prior research has studied various aspects of TikTok's regular mode, including privacy and personalization, TikTok's Kids Mode remains understudied, and there is a lack of transparency regarding its content curation and its safety and privacy protections for children. In this paper, (i) we propose an auditing methodology to comprehensively investigate TikTok's Kids Mode and (ii) we apply it to characterize the platform's content curation and determine the prevalence of child-directed content, based on regulations in the Children's Online Privacy Protection Act (COPPA). We find that 83% of videos observed on the "For You" page in Kids Mode are actually not child-directed, and even inappropriate content was found. The platform also lacks critical features, namely parental controls and accessibility settings. Our findings have important design and regulatory implications, as children may be incentivized to use TikTok's regular mode instead of Kids Mode, where they are known to be exposed to further safety and privacy risks.

</details>


### [8] [EEG-Based Auditory BCI for Communication in a Completely Locked-In Patient Using Volitional Frequency Band Modulation](https://arxiv.org/abs/2507.00305)

*Deland Liu, Frigyes Samuel Racz, Zoe Lalji, Jose del R. Millan*

**Main category:** cs.HC

**Keywords:** EEG, BCI, communication, CLIS, ALS

**Relevance Score:** 9

**TL;DR:** This paper demonstrates that a CLIS patient can use an EEG-based BCI to communicate by modulating brain activity, suggesting a method for restoring communication in completely locked-in states.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the potential of EEG-based BCIs for communication in patients with amyotrophic lateral sclerosis (ALS) who are in a completely locked-in state (CLIS).

**Method:** The study involved a CLIS patient using an EEG-based BCI to respond to assistive questions, utilizing real-time auditory feedback to control alpha and beta band power for communication.

**Key Contributions:**

	1. Demonstration of effective communication using EEG in CLIS patients.
	2. Establishment of real-time auditory feedback methods for BCI operation.
	3. Evidence of consistent modulation patterns in brain activity over multiple sessions.

**Result:** The patient was able to communicate 'Yes'/'No' responses effectively, achieving a perfect score in the final session, and demonstrated consistent modulation patterns over time.

**Limitations:** Limited sample size with only one patient, which may affect the generalizability of results.

**Conclusion:** Non-invasive EEG-based BCIs may offer a viable solution for restoring basic communication abilities in patients with CLIS.

**Abstract:** Patients with amyotrophic lateral sclerosis (ALS) in the completely locked-in state (CLIS) can lose all reliable motor control and are left without any means of communication. It remains unknown whether non-invasive electroencephalogram (EEG) based brain-computer interfaces (BCIs) can support volitional communication in CLIS. Here, we show that a CLIS patient was able to operate an EEG-based BCI across multiple online sessions to respond to both general knowledge and personally relevant assistive questions. The patient delivered "Yes"/"No" responses by volitionally modulating alpha and beta band power at different channels, guided by real-time auditory feedback from the BCI. The patient communicated assistive needs above chance in all sessions, achieving a perfect score in the final session. Performance on general knowledge questions varied across sessions, with two sessions showing accurate and above-chance responses, while the first and last sessions remained at chance level. The patient also showed consistent modulation patterns over time. These findings suggest that non-invasive BCIs may offer a potential pathway for restoring basic communication in CLIS.

</details>


### [9] [Scope Meets Screen: Lessons Learned in Designing Composite Visualizations for Marksmanship Training Across Skill Levels](https://arxiv.org/abs/2507.00333)

*Emin Zerman, Jonas Carlsson, Mårten Sjöström*

**Main category:** cs.HC

**Keywords:** marksmanship, visualization, coaching, sports analytics, human-computer interaction

**Relevance Score:** 4

**TL;DR:** This study presents a visualization system for improving marksmanship training effectiveness using first-person shooting video recordings combined with visual analytics.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance marksmanship training beyond traditional methods limited to stance and post-session accuracy analysis.

**Method:** Developed five composite visualizations using first-person video enriched with metrics; evaluated through a mixed-methods study with 10 participants (5 experts and 5 novices).

**Key Contributions:**

	1. Development of a novel shooting visualization system
	2. Demonstrated effectiveness of composite visualizations in marksmanship training
	3. Insights for applying visual analytics to other precision sports

**Result:** A dashboard-style composite view was preferred by participants in 9 out of 10 comparisons, enhancing understanding of shooting techniques across skill levels.

**Limitations:** Limited sample size of participants; specific to marksmanship.

**Conclusion:** Integrating first-person video with visual analytics can significantly improve coaching in marksmanship and potentially in other precision sports.

**Abstract:** Marksmanship practices are required in various professions, including police, military personnel, hunters, as well as sports shooters, such as Olympic shooting, biathlon, and modern pentathlon. The current form of training and coaching is mostly based on repetition, where the coach does not see through the eyes of the shooter, and analysis is limited to stance and accuracy post-session. In this study, we present a shooting visualization system and evaluate its perceived effectiveness for both novice and expert shooters. To achieve this, five composite visualizations were developed using first-person shooting video recordings enriched with overlaid metrics and graphical summaries. These views were evaluated with 10 participants (5 expert marksmen, 5 novices) through a mixed-methods study including shot-count and aiming interpretation tasks, pairwise preference comparisons, and semi-structured interviews. The results show that a dashboard-style composite view, combining raw video with a polar plot and selected graphs, was preferred in 9 of 10 cases and supported understanding across skill levels. The insights gained from this design study point to the broader value of integrating first-person video with visual analytics for coaching, and we suggest directions for applying this approach to other precision-based sports.

</details>


### [10] [Customer Service Representative's Perception of the AI Assistant in an Organization's Call Center](https://arxiv.org/abs/2507.00513)

*Kai Qin, Kexin Du, Yimeng Chen, Yueyan Liu, Jie Cai, Zhiqiang Nie, Nan Gao, Guohui Wei, Shengzhu Wang, Chun Yu*

**Main category:** cs.HC

**Keywords:** AI integration, customer service, employee-customer interaction, burdens, organizational settings

**Relevance Score:** 6

**TL;DR:** This study explores the perception of AI assistance among customer service representatives in a power grid service call center, highlighting both alleviated and newly introduced burdens.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the impact of AI integration on employee-customer interactions within customer service environments.

**Method:** Field visit and semi-structured interviews with 13 customer service representatives (CSRs).

**Key Contributions:**

	1. Insights into CSRs' experiences with AI tools
	2. Identification of new burdens introduced by AI
	3. Contribution to understanding AI's role in organizational settings

**Result:** AI assistance helps reduce traditional burdens like typing and memorizing but introduces new challenges such as compliance and psychological stress.

**Limitations:** 

**Conclusion:** A deeper understanding of the dual nature of AI integration in organizational contexts and its effects on CSRs is required for future implementations.

**Abstract:** The integration of various AI tools creates a complex socio-technical environment where employee-customer interactions form the core of work practices. This study investigates how customer service representatives (CSRs) at the power grid service customer service call center perceive AI assistance in their interactions with customers. Through a field visit and semi-structured interviews with 13 CSRs, we found that AI can alleviate some traditional burdens during the call (e.g., typing and memorizing) but also introduces new burdens (e.g., earning, compliance, psychological burdens). This research contributes to a more nuanced understanding of AI integration in organizational settings and highlights the efforts and burdens undertaken by CSRs to adapt to the updated system.

</details>


### [11] [Gaze3P: Gaze-Based Prediction of User-Perceived Privacy](https://arxiv.org/abs/2507.00596)

*Mayar Elfares, Pascal Reisert, Ralf Küsters, Andreas Bulling*

**Main category:** cs.HC

**Keywords:** privacy, machine learning, gaze data, user-perception, differential privacy

**Relevance Score:** 7

**TL;DR:** Introduction of Gaze3P dataset for quantifying user-perceived privacy using gaze data and machine learning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of previous research on quantifying user-perceived privacy and its application in privacy-preserving techniques.

**Method:** Creation of Gaze3P dataset with gaze data from 100 participants and training of a machine learning model to predict perceived privacy based on eye gaze.

**Key Contributions:**

	1. First dataset designed for systematic investigations into user-perceived privacy
	2. Application of gaze data to predict privacy perceptions
	3. Demonstration of improved optimization in privacy techniques through machine learning.

**Result:** The models developed demonstrate high accuracy in predicting user-perceived privacy, which can optimize differentially private mechanisms' parameters.

**Limitations:** 

**Conclusion:** Gaze3P enables better alignment of privacy-preserving techniques with user expectations by leveraging predicted privacy from gaze data.

**Abstract:** Privacy is a highly subjective concept and perceived variably by different individuals. Previous research on quantifying user-perceived privacy has primarily relied on questionnaires. Furthermore, applying user-perceived privacy to optimise the parameters of privacy-preserving techniques (PPT) remains insufficiently explored. To address these limitations, we introduce Gaze3P -- the first dataset specifically designed to facilitate systematic investigations into user-perceived privacy. Our dataset comprises gaze data from 100 participants and 1,000 stimuli, encompassing a range of private and safe attributes. With Gaze3P, we train a machine learning model to implicitly and dynamically predict perceived privacy from human eye gaze. Through comprehensive experiments, we show that the resulting models achieve high accuracy. Finally, we illustrate how predicted privacy can be used to optimise the parameters of differentially private mechanisms, thereby enhancing their alignment with user expectations.

</details>


### [12] [Generative Exaggeration in LLM Social Agents: Consistency, Bias, and Toxicity](https://arxiv.org/abs/2507.00657)

*Jacopo Nudo, Mario Edoardo Pandolfo, Edoardo Loru, Mattia Samory, Matteo Cinelli, Walter Quattrociocchi*

**Main category:** cs.HC

**Keywords:** Large Language Models, political discourse, social media, polarization, toxicity

**Relevance Score:** 9

**TL;DR:** This paper analyzes the behavior of Large Language Models (LLMs) in simulating political discourse on social media, revealing issues of polarization and reliability in their outputs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how LLMs mimic human political discourse on social media during the 2024 U.S. presidential election.

**Method:** The study constructs LLM agents based on 1,186 real users and evaluates their replies to politically salient tweets under Zero Shot and Few Shot conditions.

**Key Contributions:**

	1. Demonstration of how LLMs amplify polarization and harmful language.
	2. Identification of 'generation exaggeration' in LLM outputs.
	3. Critical assessment of LLMs as proxies for human behavior in social media.

**Result:** Rich contextualization of LLMs improves internal consistency but amplifies polarization and harmful language traits; a new phenomenon called 'generation exaggeration' was identified.

**Limitations:** The study may not generalize beyond the specific context of the 2024 U.S. presidential election and its data set of tweets.

**Conclusion:** LLMs reconstruct user behaviors rather than emulate them, leading to structural biases that may undermine their reliability for use in social contexts.

**Abstract:** We investigate how Large Language Models (LLMs) behave when simulating political discourse on social media. Leveraging 21 million interactions on X during the 2024 U.S. presidential election, we construct LLM agents based on 1,186 real users, prompting them to reply to politically salient tweets under controlled conditions. Agents are initialized either with minimal ideological cues (Zero Shot) or recent tweet history (Few Shot), allowing one-to-one comparisons with human replies. We evaluate three model families (Gemini, Mistral, and DeepSeek) across linguistic style, ideological consistency, and toxicity. We find that richer contextualization improves internal consistency but also amplifies polarization, stylized signals, and harmful language. We observe an emergent distortion that we call "generation exaggeration": a systematic amplification of salient traits beyond empirical baselines. Our analysis shows that LLMs do not emulate users, they reconstruct them. Their outputs, indeed, reflect internal optimization dynamics more than observed behavior, introducing structural biases that compromise their reliability as social proxies. This challenges their use in content moderation, deliberative simulations, and policy modeling.

</details>


### [13] [Designing Visualization Widgets for Tangible Data Exploration: A Systematic Review](https://arxiv.org/abs/2507.00775)

*Haonan Yao, Lingyun Yu, Lijie Yao*

**Main category:** cs.HC

**Keywords:** tangible data exploration, visualization widgets, cognitive load, natural interactions, systematic review

**Relevance Score:** 6

**TL;DR:** This paper presents a systematic review of tasks, interactions, and visualization widgets in tangible data exploration, aiming to enhance understanding and guide future design innovations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of structured understanding in the coordination of task types, interaction methods, and widget designs in tangible data exploration, which limits innovation and design pattern recognition.

**Method:** Conducted a systematic review analyzing existing studies on data exploration tasks, interactions, and tangible visualization widgets to characterize their designs.

**Key Contributions:**

	1. Systematic review of current tasks and interactions in tangible data exploration.
	2. Characterization of design patterns in tangible visualization widgets.
	3. Proposed research agenda for future widget design innovations.

**Result:** Identified current design patterns and limitations in tangible data exploration, leading to a proposal for a research agenda and future design toolkit.

**Limitations:** 

**Conclusion:** The paper reflects on findings to inform the development of a toolkit that facilitates the design of effective tangible data exploration widgets.

**Abstract:** We present a systematic review on tasks, interactions, and visualization widgets (refer to tangible entities that are used to accomplish data exploration tasks through specific interactions) in the context of tangible data exploration. Tangible widgets have been shown to reduce cognitive load, enable more natural interactions, and support the completion of complex data exploration tasks. Yet, the field lacks a structured understanding of how task types, interaction methods, and widget designs are coordinated, limiting the ability to identify recurring design patterns and opportunities for innovation. To address this gap, we conduct a systematic review to analyze existing work and characterize the current design of data exploration tasks, interactions, and tangible visualization widgets. We next reflect based on our findings and propose a research agenda to inform the development of a future widget design toolkit for tangible data exploration. Our systematic review and supplemental materials are available at physicalviswidget.github.io and osf.io/vjw5e.

</details>


### [14] [Sensemaking Through Making: Developing Clinical Domain Knowledge by Crafting Synthetic Datasets and Prototyping System Architectures](https://arxiv.org/abs/2507.00821)

*Mihnea Stefan Calota, Wessel Nieuwenhuys, Janet Yi-Ching Huang, Lin-Lin Chen, Mathias Funk*

**Main category:** cs.HC

**Keywords:** healthcare, design, synthetic datasets, Remote Patient Monitoring, iterative prototyping

**Relevance Score:** 8

**TL;DR:** This paper presents a making-oriented approach to help designers understand complex healthcare systems through synthetic datasets, using Remote Patient Monitoring as a case study.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges designers face in engaging with closed healthcare ecosystems and developing domain knowledge.

**Method:** The authors observe real-world Remote Patient Monitoring contexts, craft synthetic datasets based on these observations, and prototype a simplified RPM system through an iterative design process.

**Key Contributions:**

	1. Introduction of a making-oriented design approach for healthcare contexts
	2. Utilization of synthetic datasets to bridge knowledge gaps for designers
	3. Demonstration of iterative prototyping in understanding complex systems

**Result:** The approach enhances designers' understanding of healthcare systems, facilitating context familiarity despite limited access to actual systems.

**Limitations:** May not fully capture all nuances of real-world healthcare systems; reliant on synthetic data validity.

**Conclusion:** Hands-on interaction with synthetic data structures can empower designers to grasp the intricacies of opaque healthcare systems, ultimately improving design outcomes.

**Abstract:** Designers have ample opportunities to impact the healthcare domain. However, hospitals are often closed ecosystems that pose challenges in engaging clinical stakeholders, developing domain knowledge, and accessing relevant systems and data. In this paper, we introduce a making-oriented approach to help designers understand the intricacies of their target healthcare context. Using Remote Patient Monitoring (RPM) as a case study, we explore how manually crafting synthetic datasets based on real-world observations enables designers to learn about complex data-driven healthcare systems. Our process involves observing and modeling the real-world RPM context, crafting synthetic datasets, and iteratively prototyping a simplified RPM system that balances contextual richness and intentional abstraction. Through this iterative process of sensemaking through making, designers can still develop context familiarity when direct access to the actual healthcare system is limited. Our approach emphasizes the value of hands-on interaction with data structures to support designers in understanding opaque healthcare systems.

</details>


### [15] [Towards Difficulty-Aware Analysis of Deep Neural Networks](https://arxiv.org/abs/2507.00881)

*Linhao Meng, Stef van den Elzen, Anna Vilanova*

**Main category:** cs.HC

**Keywords:** instance difficulty, deep neural networks, image classification

**Relevance Score:** 7

**TL;DR:** The paper proposes a framework for evaluating instance difficulty in supervised image classification tasks, integrating perspectives from data, model, and human perception.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate deep neural networks more robustly by recognizing varying difficulty in instances, beyond just identifying misclassified ones.

**Method:** The authors incorporate difficulty measures from three perspectives—data, model, and human—into the evaluation process, and create an interactive visual tool called DifficultyEyes for identifying and analyzing difficult instances.

**Key Contributions:**

	1. Incorporation of instance difficulty in neural network evaluation
	2. Development of the interactive visual tool DifficultyEyes
	3. Case studies demonstrating the effectiveness of the proposed approach

**Result:** Case studies show the effectiveness of incorporating instance difficulty in model evaluation and highlight its benefits in analyzing model performance.

**Limitations:** 

**Conclusion:** Integrating instance difficulty provides a more nuanced understanding of model performance, facilitating more informed analysis and interpretation.

**Abstract:** Traditional instance-based model analysis focuses mainly on misclassified instances. However, this approach overlooks the varying difficulty associated with different instances. Ideally, a robust model should recognize and reflect the challenges presented by intrinsically difficult instances. It is also valuable to investigate whether the difficulty perceived by the model aligns with that perceived by humans. To address this, we propose incorporating instance difficulty into the deep neural network evaluation process, specifically for supervised classification tasks on image data. Specifically, we consider difficulty measures from three perspectives -- data, model, and human -- to facilitate comprehensive evaluation and comparison. Additionally, we develop an interactive visual tool, DifficultyEyes, to support the identification of instances of interest based on various difficulty patterns and to aid in analyzing potential data or model issues. Case studies demonstrate the effectiveness of our approach.

</details>


### [16] [Social Robots for People with Dementia: A Literature Review on Deception from Design to Perception](https://arxiv.org/abs/2507.00963)

*Fan Wang, Giulia Perugia, Yuan Feng, Wijnand IJsselsteijn*

**Main category:** cs.HC

**Keywords:** social robots, dementia care, robotic deception, design cues, cognitive mechanisms

**Relevance Score:** 7

**TL;DR:** This scoping review examines how social robots can mislead people with dementia through design cues, with a focus on understanding their perception and the ethical implications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns about deception in social robots used in dementia care and understand how design cues create misleading perceptions in users.

**Method:** A scoping review of 26 empirical studies exploring interactions between people with dementia and social robots, analyzing user responses and design cues.

**Key Contributions:**

	1. Identification of key design cue categories influencing perceptions in dementia care
	2. Proposed a new definition of robotic deception that incorporates cognitive mechanisms
	3. Emphasized the ethical complexity in the design of social robots for dementia care

**Result:** Identified four categories of design cues influencing perceptions: physiological signs, social intentions, familiar beings, and cues revealing artificiality. Users often ascribe human-like qualities to robots, indicating a dual nature of awareness and illusion.

**Limitations:** Limited to interactions with physical social robots; does not encompass virtual agents or software-based interactions.

**Conclusion:** Robotic deception arises from a predominance of automatic over analytical reasoning, necessitating design that is both engaging and epistemically respectful.

**Abstract:** As social robots increasingly enter dementia care, concerns about deception, intentional or not, are gaining attention. Yet, how robotic design cues might elicit misleading perceptions in people with dementia, and how these perceptions arise, remains insufficiently understood. In this scoping review, we examined 26 empirical studies on interactions between people with dementia and physical social robots. We identify four key design cue categories that may influence deceptive impressions: cues resembling physiological signs (e.g., simulated breathing), social intentions (e.g., playful movement), familiar beings (e.g., animal-like form and sound), and, to a lesser extent, cues that reveal artificiality. Thematic analysis of user responses reveals that people with dementia often attribute biological, social, and mental capacities to robots, dynamically shifting between awareness and illusion. These findings underscore the fluctuating nature of ontological perception in dementia contexts. Existing definitions of robotic deception often rest on philosophical or behaviorist premises, but rarely engage with the cognitive mechanisms involved. We propose an empirically grounded definition: robotic deception occurs when Type 1 (automatic, heuristic) processing dominates over Type 2 (deliberative, analytic) reasoning, leading to misinterpretation of a robot's artificial nature. This dual-process perspective highlights the ethical complexity of social robots in dementia care and calls for design approaches that are not only engaging, but also epistemically respectful.

</details>


### [17] [A Comprehensive Review of Human Error in Risk-Informed Decision Making: Integrating Human Reliability Assessment, Artificial Intelligence, and Human Performance Models](https://arxiv.org/abs/2507.01017)

*Xingyu Xiao, Hongxu Zhu, Jingang Liang, Jiejuan Tong, Haitao Wang*

**Main category:** cs.HC

**Keywords:** Human Error, AI, Human Reliability Assessment, Cognitive Science, Risk-informed Decision Making

**Relevance Score:** 7

**TL;DR:** The paper reviews the convergence of risk-informed decision making, human reliability assessment, AI, and cognitive science to reduce human error risks in safety-critical sectors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Human error is a major risk in sectors like healthcare and aviation, necessitating improved techniques to manage it.

**Method:** The review categorizes human error types and their impacts, examines risk-informed frameworks, surveys cognitive models, and assesses AI techniques for enhancing human reliability assessment.

**Key Contributions:**

	1. Synthesis of human error types and their effects.
	2. Evaluation of AI techniques for HRA.
	3. Proposal for cross-domain data collaboration.

**Result:** AI-integrated cognitive models significantly improve error prediction and human reliability metrics, but require better datasets and algorithm transparency.

**Limitations:** Reliance on existing limited datasets and the need for transparency in algorithms.

**Conclusion:** Integrating AI with cognitive models can improve predictive capabilities in human reliability assessment, though challenges remain in data quality and validation.

**Abstract:** Human error remains a dominant risk driver in safety-critical sectors such as nuclear power, aviation, and healthcare, where seemingly minor mistakes can cascade into catastrophic outcomes. Although decades of research have produced a rich repertoire of mitigation techniques, persistent limitations: scarce high-quality data, algorithmic opacity, and residual reliance on expert judgment, continue to constrain progress. This review synthesizes recent advances at the intersection of risk-informed decision making, human reliability assessment (HRA), artificial intelligence (AI), and cognitive science to clarify how their convergence can curb human-error risk. We first categorize the principal forms of human error observed in complex sociotechnical environments and outline their quantitative impact on system reliability. Next, we examine risk-informed frameworks that embed HRA within probabilistic and data-driven methodologies, highlighting successes and gaps. We then survey cognitive and human-performance models, detailing how mechanistic accounts of perception, memory, and decision-making enrich error prediction and complement HRA metrics. Building on these foundations, we critically assess AI-enabled techniques for real-time error detection, operator-state estimation, and AI-augmented HRA workflows. Across these strands, a recurring insight emerges: integrating cognitive models with AI-based analytics inside risk-informed HRA pipelines markedly enhances predictive fidelity, yet doing so demands richer datasets, transparent algorithms, and rigorous validation. Finally, we identify promising research directions, coupling resilience engineering concepts with grounded theory, operationalizing the iceberg model of incident causation, and establishing cross-domain data consortia, to foster a multidisciplinary paradigm that elevates human reliability in high-stakes systems.

</details>


### [18] [Look and Talk: Seamless AI Assistant Interaction with Gaze-Triggered Activation](https://arxiv.org/abs/2504.09296)

*Zhang Qing, Rekimoto Jun*

**Main category:** cs.HC

**Keywords:** AI assistants, eye-tracking, AR glasses, human-computer interaction, activation methods

**Relevance Score:** 8

**TL;DR:** The paper presents a novel method for activating AI assistants using eye-tracking technology in AR glasses, allowing users to engage silently and hands-free by maintaining eye contact with a virtual avatar.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional activation methods for AI assistants which are affected by false activations, recognition errors, and physical interaction constraints.

**Method:** Utilizing eye-tracking technology in AR glasses to detect a user's intention to activate an AI assistant by monitoring sustained eye contact with a virtual AI avatar.

**Key Contributions:**

	1. Introduction of eye-tracking as an activation method for AI assistants
	2. User feedback supporting the practicality of this method
	3. Demonstrated reduction of obtrusiveness in interacting with AI systems

**Result:** Preliminary user feedback indicates that the eye-tracking activation method is intuitive, natural, and less intrusive, facilitating smoother interactions with AI.

**Limitations:** 

**Conclusion:** This approach demonstrates the potential for more seamless integration of AI assistants into daily life, reducing barriers to interaction.

**Abstract:** Engaging with AI assistants to gather essential information in a timely manner is becoming increasingly common. Traditional activation methods, like wake words such as Hey Siri, Ok Google, and Hey Alexa, are constrained by technical challenges such as false activations, recognition errors, and discomfort in public settings. Similarly, activating AI systems via physical buttons imposes strict interactive limitations as it demands particular physical actions, which hinders fluid and spontaneous communication with AI. Our approach employs eye-tracking technology within AR glasses to discern a user's intention to engage with the AI assistant. By sustaining eye contact on a virtual AI avatar for a specific time, users can initiate an interaction silently and without using their hands. Preliminary user feedback suggests that this technique is relatively intuitive, natural, and less obtrusive, highlighting its potential for integrating AI assistants fluidly into everyday interactions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [19] [Table Understanding and (Multimodal) LLMs: A Cross-Domain Case Study on Scientific vs. Non-Scientific Data](https://arxiv.org/abs/2507.00152)

*Ekaterina Borisova, Fabio Barth, Nils Feldhus, Raia Abu Ahmad, Malte Ostendorff, Pedro Ortiz Suarez, Georg Rehm, Sebastian Möller*

**Main category:** cs.CL

**Keywords:** LLM, Table Understanding, Cross-domain Evaluation, Interpretability, Benchmark

**Relevance Score:** 8

**TL;DR:** This paper investigates the effectiveness of text-based and multimodal LLMs on table understanding tasks, comparing their performance in scientific and non-scientific contexts using various table formats.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the efficiency of LLMs in processing tabular data, which is crucial in many domains such as research, business, and medicine.

**Method:** The study conducts a cross-domain and cross-modality evaluation of LLMs on tabular data, analyzing text and image representations while also performing interpretability analysis. It introduces the TableEval benchmark with 3017 tables in five formats.

**Key Contributions:**

	1. Introduced the TableEval benchmark with extensive table formats for evaluation.
	2. Investigated LLM performance across scientific and non-scientific table contexts.
	3. Performed interpretability analysis to examine context usage and relevance.

**Result:** The findings reveal that LLMs are robust across different modalities but encounter significant challenges with scientific tables.

**Limitations:** The study mainly focuses on specific domains and may not generalize to all types of tabular data.

**Conclusion:** While LLMs show promise in understanding tables, further improvements are needed for handling complex scientific data.

**Abstract:** Tables are among the most widely used tools for representing structured data in research, business, medicine, and education. Although LLMs demonstrate strong performance in downstream tasks, their efficiency in processing tabular data remains underexplored. In this paper, we investigate the effectiveness of both text-based and multimodal LLMs on table understanding tasks through a cross-domain and cross-modality evaluation. Specifically, we compare their performance on tables from scientific vs. non-scientific contexts and examine their robustness on tables represented as images vs. text. Additionally, we conduct an interpretability analysis to measure context usage and input relevance. We also introduce the TableEval benchmark, comprising 3017 tables from scholarly publications, Wikipedia, and financial reports, where each table is provided in five different formats: Image, Dictionary, HTML, XML, and LaTeX. Our findings indicate that while LLMs maintain robustness across table modalities, they face significant challenges when processing scientific tables.

</details>


### [20] [Prompting as Scientific Inquiry](https://arxiv.org/abs/2507.00163)

*Ari Holtzman, Chenhao Tan*

**Main category:** cs.CL

**Keywords:** prompting, large language models, behavioral science, mechanistic interpretability, AI capabilities

**Relevance Score:** 9

**TL;DR:** This paper argues that prompting in large language models (LLMs) should be viewed as a legitimate branch of behavioral science rather than mere alchemy, emphasizing its importance in unlocking the capabilities of LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reframe the understanding of prompting in large language models as a scientific method essential for exploring and utilizing their complex behavior, rather than a primitive or ad-hoc approach.

**Method:** The paper discusses the role of prompting as a behavioral science, contrasting it with mechanistic interpretability that examines neural structures.

**Key Contributions:**

	1. Promoting the scientific view of prompting in the context of LLMs.
	2. Highlighting the comparison between prompting and mechanistic interpretability.
	3. Establishing prompting as a key method for understanding and controlling LLM behaviors.

**Result:** The authors highlight that prompting has been fundamental in achieving significant advancements in LLMs, demonstrating its effectiveness in eliciting desired behaviors from these models.

**Limitations:** 

**Conclusion:** Prompting is presented as a crucial scientific approach in the study of LLMs, emphasizing its value and the need to approach it with the same rigor as other scientific methodologies.

**Abstract:** Prompting is the primary method by which we study and control large language models. It is also one of the most powerful: nearly every major capability attributed to LLMs-few-shot learning, chain-of-thought, constitutional AI-was first unlocked through prompting. Yet prompting is rarely treated as science and is frequently frowned upon as alchemy. We argue that this is a category error. If we treat LLMs as a new kind of complex and opaque organism that is trained rather than programmed, then prompting is not a workaround: it is behavioral science. Mechanistic interpretability peers into the neural substrate, prompting probes the model in its native interface: language. We contend that prompting is not inferior, but rather a key component in the science of LLMs.

</details>


### [21] [LineRetriever: Planning-Aware Observation Reduction for Web Agents](https://arxiv.org/abs/2507.00210)

*Imene Kerboua, Sahar Omidi Shayegan, Megh Thakkar, Xing Han Lù, Massimo Caccia, Véronique Eglin, Alexandre Aussem, Jérémy Espinas, Alexandre Lacoste*

**Main category:** cs.CL

**Keywords:** Large language models, Web navigation, Adaptive planning, LineRetriever, Action prediction

**Relevance Score:** 8

**TL;DR:** LineRetriever optimizes retrieval for adaptive planning in web navigation by focusing on observation lines relevant to future actions rather than just semantic similarity.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional retrieval methods in capturing critical information for adaptive planning in web navigation tasks.

**Method:** LineRetriever leverages a language model to identify and retrieve observation lines that are crucial for predicting future navigation steps, considering the planning horizon.

**Key Contributions:**

	1. Introduction of LineRetriever for targeted retrieval in web navigation
	2. Empirical demonstration of reduced observation size with maintained performance
	3. Focus on planning horizon in retrieval methods.

**Result:** The experiments show that LineRetriever decreases the observation size at each step while maintaining performance, improving adaptive planning in web agents.

**Limitations:** 

**Conclusion:** LineRetriever effectively enhances the retrieval process for web navigation by prioritizing planning-relevant information, significantly improving action prediction.

**Abstract:** While large language models have demonstrated impressive capabilities in web navigation tasks, the extensive context of web pages, often represented as DOM or Accessibility Tree (AxTree) structures, frequently exceeds model context limits. Current approaches like bottom-up truncation or embedding-based retrieval lose critical information about page state and action history. This is particularly problematic for adaptive planning in web agents, where understanding the current state is essential for determining future actions. We hypothesize that embedding models lack sufficient capacity to capture plan-relevant information, especially when retrieving content that supports future action prediction. This raises a fundamental question: how can retrieval methods be optimized for adaptive planning in web navigation tasks? In response, we introduce \textit{LineRetriever}, a novel approach that leverages a language model to identify and retrieve observation lines most relevant to future navigation steps. Unlike traditional retrieval methods that focus solely on semantic similarity, \textit{LineRetriever} explicitly considers the planning horizon, prioritizing elements that contribute to action prediction. Our experiments demonstrate that \textit{LineRetriever} can reduce the size of the observation at each step for the web agent while maintaining consistent performance within the context limitations.

</details>


### [22] [Two-Stage Reasoning-Infused Learning: Improving Classification with LLM-Generated Reasoning](https://arxiv.org/abs/2507.00214)

*Mads Henrichsen, Rasmus Krebs*

**Main category:** cs.CL

**Keywords:** text classification, Large Language Models, reasoning generation, emotion classification, NLP

**Relevance Score:** 9

**TL;DR:** This paper presents a two-stage approach to enhance text classification using Large Language Models (LLMs) to generate textual reasoning, which significantly improves accuracy in emotion classification tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Standard classification models lack explicit reasoning, limiting their performance. This paper aims to address this by incorporating LLM-generated reasonings into the classification process.

**Method:** The approach consists of two main stages: first, fine-tuning a Llama-3.2-1B-Instruct model on a reasoning dataset to generate reasonings; second, using this trained model to augment a training dataset for a generative model that predicts both reasoning and emotion.

**Key Contributions:**

	1. Introduction of a two-stage text classification approach leveraging LLM-generated reasonings.
	2. Demonstration of improved accuracy in emotion classification through reasoning augmentation.
	3. Insights into the benefits of explicit reasoning for enhancing NLP model performance.

**Result:** The proposed generative model achieved an 8.7 percentage point accuracy improvement in emotion classification compared to a baseline model, demonstrating the efficacy of incorporating explicit reasoning.

**Limitations:** 

**Conclusion:** The findings highlight that LLM-generated reasonings can create richer datasets, enhancing performance in downstream NLP tasks while providing explicit explanations for predictions.

**Abstract:** Standard classification models often map inputs directly to labels without explicit reasoning, potentially limiting their performance, robustness, and interpretability. This paper introduces a novel two-stage approach to enhance text classification by leveraging Large Language Model (LLM)-generated reasonings. In the first stage, we fine-tune a Llama-3.2-1B-Instruct model (henceforth Llama-R-Gen) on a general-purpose reasoning dataset (syvai/reasoning-gen) to generate textual reasoning (R) given a question and its answer. In the second stage, this generally trained Llama-R-Gen is used offline to create an augmented training dataset for a downstream generative model. This downstream model, based on Llama-3.2-1B-Instruct, takes only the input text (Q) and is trained to output the generated reasoning (R) immediately followed by the predicted emotion (A). We demonstrate this methodology on the dair-ai/emotion dataset for emotion classification. Our experiments show that the generative model trained to output reasoning and the emotion (Classifier Q->RA) achieves a significant improvement of 8.7 percentage points in accuracy (for emotion prediction) compared to a baseline generative model trained solely to output the emotion (Classifier Q->A), highlighting the strong generalization capabilities of the reasoning generation and the benefit of explicit reasoning training. This work underscores the potential of LLM-generated reasonings for creating richer training datasets, thereby improving the performance of diverse downstream NLP tasks and providing explicit explanations.

</details>


### [23] [Towards Style Alignment in Cross-Cultural Translation](https://arxiv.org/abs/2507.00216)

*Shreya Havaldar, Adam Stein, Eric Wong, Lyle Ungar*

**Main category:** cs.CL

**Keywords:** Human-Computer Interaction, Cultural Differences, Language Translation

**Relevance Score:** 8

**TL;DR:** This paper discusses how cultural differences can cause misalignment in communication styles during translation, particularly in LLMs. It introduces RASTA, a method aimed at improving stylistic alignment in translations to better reflect cultural norms.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the misalignment between a speaker's intended communication style and a listener's perception, particularly in translation involving cultural nuances and non-Western languages.

**Method:** The paper presents RASTA (Retrieval-Augmented STylistic Alignment), a method that utilizes learned stylistic concepts to enhance the translation process of LLMs, ensuring it conveys cultural communication norms more accurately.

**Key Contributions:**

	1. Development of RASTA for improved stylistic translations
	2. Analysis of LLM failures in capturing cultural nuances
	3. Demonstration of enhanced performance in non-Western languages

**Result:** The implementation of RASTA demonstrates improved performance in translating stylistic elements, reducing biases toward neutrality, and enhancing the accuracy of translations in non-Western languages.

**Limitations:** Limited to the scope of stylistic alignment; does not address other aspects of translation complexity.

**Conclusion:** RASTA shows promise in improving the stylistic alignment in LLM translations, helping to bridge cultural gaps in communication and preserving intended meanings during the translation process.

**Abstract:** Successful communication depends on the speaker's intended style (i.e., what the speaker is trying to convey) aligning with the listener's interpreted style (i.e., what the listener perceives). However, cultural differences often lead to misalignment between the two; for example, politeness is often lost in translation. We characterize the ways that LLMs fail to translate style - biasing translations towards neutrality and performing worse in non-Western languages. We mitigate these failures with RASTA (Retrieval-Augmented STylistic Alignment), a method that leverages learned stylistic concepts to encourage LLM translation to appropriately convey cultural communication norms and align style.

</details>


### [24] [Linearly Decoding Refused Knowledge in Aligned Language Models](https://arxiv.org/abs/2507.00239)

*Aryan Shrivastava, Ari Holtzman*

**Main category:** cs.CL

**Keywords:** language models, harmful information, instruction-tuning, linear probes, jailbreak prompts

**Relevance Score:** 7

**TL;DR:** This study investigates the ability of linear probes to decode information from language models (LMs) that employ instruction-tuning but may still respond harmfully to jailbreak prompts. It finds that such harmful information, while suppressed, remains accessible and can influence LM behavior.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the resilience of harmful information in instruction-tuned language models and the implications for user safety and model alignment.

**Method:** The research employs linear probes trained on the hidden states of various language models to assess the decodability of information elicited by jailbreak prompts, measuring correlations between probe outputs and model responses.

**Key Contributions:**

	1. Demonstrated the decodability of harmful information in instruction-tuned LMs using linear probes.
	2. Showed that internal representations of refused properties are retained post instruction-tuning.
	3. Highlighted the indirect influence of suppressed information on LM behavior in downstream applications.

**Result:** The study demonstrates that significant amounts of refused information in LMs can be linearly decoded, with Pearson correlations exceeding 0.8 for certain queries across models. Some probes trained on base models also transfer to instruction-tuned versions, revealing persistent harmful content.

**Limitations:** The study focuses on linear probes and their limitations in fully understanding the complexities of LM decision-making and alignment.

**Conclusion:** Instruction-tuning does not completely purge harmful information from language models; rather, it suppresses direct expression while allowing for linear accessibility, affecting downstream tasks and suggesting that models may still leverage this suppressed information.

**Abstract:** Most commonly used language models (LMs) are instruction-tuned and aligned using a combination of fine-tuning and reinforcement learning, causing them to refuse users requests deemed harmful by the model. However, jailbreak prompts can often bypass these refusal mechanisms and elicit harmful responses. In this work, we study the extent to which information accessed via jailbreak prompts is decodable using linear probes trained on LM hidden states. We show that a great deal of initially refused information is linearly decodable. For example, across models, the response of a jailbroken LM for the average IQ of a country can be predicted by a linear probe with Pearson correlations exceeding $0.8$. Surprisingly, we find that probes trained on base models (which do not refuse) sometimes transfer to their instruction-tuned versions and are capable of revealing information that jailbreaks decode generatively, suggesting that the internal representations of many refused properties persist from base LMs through instruction-tuning. Importantly, we show that this information is not merely "leftover" in instruction-tuned models, but is actively used by them: we find that probe-predicted values correlate with LM generated pairwise comparisons, indicating that the information decoded by our probes align with suppressed generative behavior that may be expressed more subtly in other downstream tasks. Overall, our results suggest that instruction-tuning does not wholly eliminate or even relocate harmful information in representation space-they merely suppress its direct expression, leaving it both linearly accessible and indirectly influential in downstream behavior.

</details>


### [25] [The Algebraic Structure of Morphosyntax](https://arxiv.org/abs/2507.00244)

*Isabella Senturia, Matilde Marcolli*

**Main category:** cs.CL

**Keywords:** morphology-syntax interface, Distributed Morphology, operads

**Relevance Score:** 2

**TL;DR:** This paper presents a mathematical model of the morphology-syntax interface within the framework of the Strong Minimalist Thesis, describing the formation of morphosyntactic trees without movement in morphology.

**Read time:** 45 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a mathematical framework to understand the relationship between morphology and syntax and how they interact in the formation of words and sentences.

**Method:** The paper constructs a model based on a magma of morphological trees and employs operads to describe the structure formation of morphosyntactic trees in a systematic way.

**Key Contributions:**

	1. Mathematical modeling of the morphology-syntax interface
	2. Development of a coproduct decomposition for morphological trees
	3. Reinterpretation of Distributed Morphology operations in a new framework

**Result:** The study establishes a coproduct decomposition that extends the range of morphological trees impacting syntactic structures, revealing a flexibility in the morphology-syntax boundary.

**Limitations:** 

**Conclusion:** This work contributes to a deeper understanding of the interface between morphology and syntax, framing it within a robust mathematical model that influences how we view morphosyntactic structures.

**Abstract:** Within the context of the mathematical formulation of Merge and the Strong Minimalist Thesis, we present a mathematical model of the morphology-syntax interface. In this setting, morphology has compositional properties responsible for word formation, organized into a magma of morphological trees. However, unlike syntax, we do not have movement within morphology. A coproduct decomposition exists, but it requires extending the set of morphological trees beyond those which are generated solely by the magma, to a larger set of possible morphological inputs to syntactic trees. These participate in the formation of morphosyntactic trees as an algebra over an operad, and a correspondence between algebras over an operad. The process of structure formation for morphosyntactic trees can then be described in terms of this operadic correspondence that pairs syntactic and morphological data and the morphology coproduct. We reinterpret in this setting certain operations of Distributed Morphology as transformation that allow for flexibility in moving the boundary between syntax and morphology within the morphosyntactic objects.

</details>


### [26] [EfficientXLang: Towards Improving Token Efficiency Through Cross-Lingual Reasoning](https://arxiv.org/abs/2507.00246)

*Sanchit Ahuja, Praneetha Vaddamanu, Barun Patra*

**Main category:** cs.CL

**Keywords:** Language Reasoning Models, multilingual data, token efficiency, reasoning accuracy, Machine Learning

**Relevance Score:** 7

**TL;DR:** This paper explores the token efficiency and accuracy of reasoning in multilingual contexts using RLMs, finding that non-English reasoning can reduce token usage while maintaining accuracy.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate whether English is the most token-efficient language for reasoning, given the multilingual capabilities of modern language models.

**Method:** The study evaluates three open-source reasoning language models (DeepSeek R1, Qwen 2.5, and Qwen 3) across four math datasets in seven different languages.

**Key Contributions:**

	1. Demonstration that reasoning in non-English can be more token-efficient than in English.
	2. Evidence that improvements in reasoning are genuine and not merely linguistic artifacts.
	3. Encouragement for stronger multilingual foundations in LRM development.

**Result:** Non-English reasoning reduces token usage and maintains accuracy, with improvements persisting even after translating reasoning traces into English.

**Limitations:** The extent of improvement in reasoning depends on the multilingual strengths of the models evaluated.

**Conclusion:** The findings highlight the importance of multilingual reasoning capabilities in language models, suggesting that researchers should consider broader language perspectives.

**Abstract:** Despite recent advances in Language Reasoning Models (LRMs), most research focuses solely on English, even though many models are pretrained on multilingual data. In this work, we investigate: Is English the most token-efficient language for reasoning? We evaluate three open-source RLMs: DeepSeek R1, Qwen 2.5 and Qwen 3, across four math datasets and seven typologically diverse languages. We find that reasoning in non-English languages not only reduces token usage, but also preserves accuracy. These gains persist even after translating the reasoning traces into English, suggesting genuine shifts in reasoning behavior rather than surface-level linguistic effects. The extent of improvement, however, depends on the models multilingual strength. Our findings motivate a broader view of reasoning in language models, highlighting the potential of multilingual reasoning and the importance of strong multilingual foundations. The code for our work can be found: https://github.com/microsoft/EfficientXLang.

</details>


### [27] [Impact of Fine-Tuning Methods on Memorization in Large Language Models](https://arxiv.org/abs/2507.00258)

*Jie Hou, Chuxiong Wu, Lannan Luo, Qiang Zeng*

**Main category:** cs.CL

**Keywords:** large language models, fine-tuning, privacy risks, membership inference attacks, prompt-based fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper evaluates the privacy risks associated with fine-tuning LLMs, focusing on the effectiveness of prompt-based versus parameter-based fine-tuning in resisting membership inference attacks (MIAs).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the overlooked privacy risks associated with memorization during the fine-tuning of pre-trained large language models (LLMs).

**Method:** The authors categorize popular fine-tuning approaches and assess their impact on memorization using membership inference attacks (MIAs) as a measurement method.

**Key Contributions:**

	1. Identification of privacy risks in fine-tuning LLMs through MIAs.
	2. Comparison of prompt-based and parameter-based fine-tuning methods regarding privacy.
	3. Demonstration that prompt-based fine-tuning has lower memorization regardless of model scale.

**Result:** Prompt-based fine-tuning is demonstrated to achieve competitive performance with lower vulnerability to MIAs compared to parameter-based fine-tuning, which is found to leak more private information.

**Limitations:** The study primarily focuses on membership inference attacks and may not cover all privacy concerns associated with fine-tuning LLMs.

**Conclusion:** The findings suggest that prompt-based fine-tuning is a more privacy-preserving alternative to parameter-based fine-tuning.

**Abstract:** As the capabilities of pre-trained large language models (LLMs) continue to advance, the "pre-train and fine-tune" paradigm has become increasingly mainstream, leading to the development of various fine-tuning methods. However, the privacy risks arising from memorization during fine-tuning have received relatively little attention. To address this gap, we categorize popular fine-tuning approaches and assess their impact on memorization through the lens of membership inference attacks (MIAs). Our results show that, compared to parameter-based fine-tuning, prompt-based fine-tuning achieves competitive performance while exhibiting lower vulnerability to MIAs. Furthermore, prompt-based methods maintain low memorization regardless of model scale. These findings suggest that parameter-based fine-tuning is more prone to leaking private information, whereas prompt-based fine-tuning serves as a more privacy-preserving option.

</details>


### [28] [Natural language processing for African languages](https://arxiv.org/abs/2507.00297)

*David Ifeoluwa Adelani*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Low-resource languages, Multilingual models, Word embeddings, African languages

**Relevance Score:** 8

**TL;DR:** The dissertation focuses on improving NLP for low-resource African languages through data quality enhancement and adaptation of multilingual language models.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by multilingual models trained on low-resource African languages and the lack of labeled datasets for these languages in NLP research.

**Method:** The author analyzes the quality of existing corpora, curates a high-quality corpus, and develops human-annotated datasets for named entity recognition and machine translation, evaluated through various learning settings.

**Key Contributions:**

	1. Curated a high-quality corpus for low-resource African languages.
	2. Developed large-scale human-annotated datasets for 21 African languages.
	3. Empirical evaluation demonstrating the applicability of multilingual PLMs in low-resource scenarios.

**Result:** The study empirically demonstrates that the quality of semantic representations in word embeddings is influenced by the quality of pre-training data, and shows the potential of multilingual pre-trained models for unseen African languages.

**Limitations:** 

**Conclusion:** The dissertation concludes that improving labeled datasets and utilizing multilingual PLMs can enhance the NLP performance for low-resource languages in Sub-Saharan Africa.

**Abstract:** Recent advances in word embeddings and language models use large-scale, unlabelled data and self-supervised learning to boost NLP performance. Multilingual models, often trained on web-sourced data like Wikipedia, face challenges: few low-resource languages are included, their data is often noisy, and lack of labeled datasets makes it hard to evaluate performance outside high-resource languages like English. In this dissertation, we focus on languages spoken in Sub-Saharan Africa where all the indigenous languages in this region can be regarded as low-resourced in terms of the availability of labelled data for NLP tasks and unlabelled data found on the web. We analyse the noise in the publicly available corpora, and curate a high-quality corpus, demonstrating that the quality of semantic representations learned in word embeddings does not only depend on the amount of data but on the quality of pre-training data. We demonstrate empirically the limitations of word embeddings, and the opportunities the multilingual pre-trained language model (PLM) offers especially for languages unseen during pre-training and low-resource scenarios. We further study how to adapt and specialize multilingual PLMs to unseen African languages using a small amount of monolingual texts. To address the under-representation of the African languages in NLP research, we developed large scale human-annotated labelled datasets for 21 African languages in two impactful NLP tasks: named entity recognition and machine translation. We conduct an extensive empirical evaluation using state-of-the-art methods across supervised, weakly-supervised, and transfer learning settings.

</details>


### [29] [Failure by Interference: Language Models Make Balanced Parentheses Errors When Faulty Mechanisms Overshadow Sound Ones](https://arxiv.org/abs/2507.00322)

*Daking Rai, Samuel Miller, Kevin Moran, Ziyu Yao*

**Main category:** cs.CL

**Keywords:** language models, syntactic tasks, performance improvement, RASteer, arithmetic reasoning

**Relevance Score:** 7

**TL;DR:** This study explores the errors in language models related to generating balanced parentheses and presents a method called RASteer to enhance model performance by leveraging reliable components within the models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand and mitigate the errors language models make with simple syntactic tasks like balanced parentheses generation, which persist across various model sizes.

**Method:** The study investigates the components of language models that contribute to errors and introduces RASteer, a method designed to identify and boost the contributions of reliable components to improve performance on specified tasks.

**Key Contributions:**

	1. Introduced RASteer for improving language model performance on specific tasks.
	2. Analyzed the causes of errors in language models regarding balanced parentheses.
	3. Achieved significant accuracy improvements in both balanced parentheses and arithmetic reasoning tasks.

**Result:** The RASteer method significantly enhances performance on balanced parentheses tasks, improving accuracy from 0% to approximately 100%, and also shows enhancements in arithmetic reasoning tasks with gains of up to 20%.

**Limitations:** 

**Conclusion:** The introduction of RASteer not only improves specific syntactic tasks but also suggests a broader applicability for enhancing model performance in various other tasks by focusing on reliable components.

**Abstract:** Despite remarkable advances in coding capabilities, language models (LMs) still struggle with simple syntactic tasks such as generating balanced parentheses. In this study, we investigate the underlying mechanisms behind the persistence of these errors across LMs of varying sizes (124M-7B) to both understand and mitigate the errors. Our study reveals that LMs rely on a number of components (attention heads and FF neurons) that independently make their own predictions. While some components reliably promote correct answers across a generalized range of inputs (i.e., implementing "sound mechanisms''), others are less reliable and introduce noise by promoting incorrect tokens (i.e., implementing "faulty mechanisms''). Errors occur when the faulty mechanisms overshadow the sound ones and dominantly affect the predictions. Motivated by this insight, we introduce RASteer, a steering method to systematically identify and increase the contribution of reliable components for improving model performance. RASteer substantially improves performance on balanced parentheses tasks, boosting accuracy of some models from $0$% to around $100$% without impairing the models' general coding ability. We further demonstrate its broader applicability in arithmetic reasoning tasks, achieving performance gains of up to around $20$%.

</details>


### [30] [Modeling Data Diversity for Joint Instance and Verbalizer Selection in Cold-Start Scenarios](https://arxiv.org/abs/2507.00330)

*Mohna Chakraborty, Adithya Kulkarni, Qi Li*

**Main category:** cs.CL

**Keywords:** prompt-based methods, pre-trained language models, cold-start, instance selection, verbalizer selection

**Relevance Score:** 8

**TL;DR:** COLDSELECT is a novel approach for joint verbalizer and instance selection in prompt-based methods that enhances performance in cold-start scenarios by modeling data diversity and relationships.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing prompt-based methods for pre-trained language models are sensitive to template and few-shot instance selection, especially in cold-start settings without labeled data. This paper aims to improve selection methods by addressing the overlooked dependency between instances and verbalizers.

**Method:** COLDSELECT maps the vocabulary of pre-trained language models and masked embeddings into a shared space, using dimensionality reduction and clustering to achieve efficient and diverse instance selection.

**Key Contributions:**

	1. Proposes COLDSELECT for joint verbalizer and instance selection.
	2. Implements a method to optimize for minimal uncertainty and maximal diversity.
	3. Demonstrates substantial performance improvements across multiple benchmarks.

**Result:** Experiments on eight benchmarks reveal that COLDSELECT significantly reduces uncertainty and enhances generalization, demonstrating superior performance in verbalizer and few-shot instance selection compared to established baselines.

**Limitations:** 

**Conclusion:** COLDSELECT effectively captures data relationships and improves the selection process in cold-start scenarios, proving beneficial for applications relying on prompt-based methods with limited labeled data.

**Abstract:** Prompt-based methods leverage the knowledge of pre-trained language models (PLMs) trained with a masked language modeling (MLM) objective; however, these methods are sensitive to template, verbalizer, and few-shot instance selection, particularly in cold-start settings with no labeled data. Existing studies overlook the dependency between instances and verbalizers, where instance-label probabilities depend on verbalizer token proximity in the embedding space. To address this, we propose COLDSELECT, a joint verbalizer and instance selection approach that models data diversity. COLDSELECT maps PLM vocabulary and $h_{[MASK]}$ embeddings into a shared space, applying dimensionality reduction and clustering to ensure efficient and diverse selection. By optimizing for minimal uncertainty and maximal diversity, COLDSELECT captures data relationships effectively. Experiments on eight benchmarks demonstrate COLDSELECT's superiority in reducing uncertainty and enhancing generalization, outperforming baselines in verbalizer and few-shot instance selection for cold-start scenarios.

</details>


### [31] [Question Decomposition for Retrieval-Augmented Generation](https://arxiv.org/abs/2507.00355)

*Paul J. L. Ammann, Jonas Golde, Alan Akbik*

**Main category:** cs.CL

**Keywords:** large language models, retrieval-augmented generation, question decomposition, multi-hop questions, information retrieval

**Relevance Score:** 9

**TL;DR:** This paper presents a new RAG pipeline that uses question decomposition to improve retrieval and answer accuracy for multi-hop questions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of retrieval-augmented generation (RAG) in answering multi-hop questions where relevant information is distributed across different documents.

**Method:** An LLM decomposes multi-hop questions into sub-questions, retrieves passages for each, and reranks the merged candidate pool to improve information coverage and precision.

**Key Contributions:**

	1. Introduction of question decomposition for RAG pipelines
	2. Demonstrated improvement in multi-hop question answering
	3. Validation on established benchmarks (MultiHop-RAG and HotpotQA)

**Result:** The proposed method shows significant improvement in retrieval performance (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG approaches.

**Limitations:** 

**Conclusion:** Using LLM-driven question decomposition alongside standard reranking techniques effectively improves information retrieval for complex queries without necessitating additional training or special indexing.

**Abstract:** Grounding large language models (LLMs) in verifiable external sources is a well-established strategy for generating reliable answers. Retrieval-augmented generation (RAG) is one such approach, particularly effective for tasks like question answering: it retrieves passages that are semantically related to the question and then conditions the model on this evidence. However, multi-hop questions, such as "Which company among NVIDIA, Apple, and Google made the biggest profit in 2023?," challenge RAG because relevant facts are often distributed across multiple documents rather than co-occurring in one source, making it difficult for standard RAG to retrieve sufficient information. To address this, we propose a RAG pipeline that incorporates question decomposition: (i) an LLM decomposes the original query into sub-questions, (ii) passages are retrieved for each sub-question, and (iii) the merged candidate pool is reranked to improve the coverage and precision of the retrieved evidence. We show that question decomposition effectively assembles complementary documents, while reranking reduces noise and promotes the most relevant passages before answer generation. Although reranking itself is standard, we show that pairing an off-the-shelf cross-encoder reranker with LLM-driven question decomposition bridges the retrieval gap on multi-hop questions and provides a practical, drop-in enhancement, without any extra training or specialized indexing. We evaluate our approach on the MultiHop-RAG and HotpotQA, showing gains in retrieval (MRR@10: +36.7%) and answer accuracy (F1: +11.6%) over standard RAG baselines.

</details>


### [32] [Gregorian melody, modality, and memory: Segmenting chant with Bayesian nonparametrics](https://arxiv.org/abs/2507.00380)

*Vojtěch Lanz, Jan Hajič jr*

**Main category:** cs.CL

**Keywords:** Gregorian chant, centonisation, segmentation, Pitman-Yor, memory efficiency

**Relevance Score:** 3

**TL;DR:** This paper explores the segmentation of Gregorian melodies using a hierarchical language model, achieving state-of-the-art results in mode classification while examining the relationship between melody structure and memory efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the long-debated theory of centonisation in Gregorian chant melodies and to determine if an optimal segmentation model can provide insights into their melodic structure and memorization.

**Method:** The authors utilize nested hierarchical Pitman-Yor language models for unsupervised segmentation of chant melodies.

**Key Contributions:**

	1. Introduced an optimal unsupervised segmentation method for Gregorian chant melodies.
	2. Demonstrated the efficacy of segmentations in mode classification surpassing music-theoretical features.
	3. Provided empirical evidence connecting memory efficiency with modal structure in chant melodies.

**Result:** The segmentation method achieved state-of-the-art performance in mode classification and provided empirical evidence indicating a link between mode classification and memory efficiency.

**Limitations:** The study's findings challenge the conventional understanding of centonisation but do not establish a definitive alternative theory.

**Conclusion:** Despite achieving a memory-optimal segmentation, the resulting segments do not conform to traditional notions of centonisation.

**Abstract:** The idea that Gregorian melodies are constructed from some vocabulary of segments has long been a part of chant scholarship. This so-called "centonisation" theory has received much musicological criticism, but frequent re-use of certain melodic segments has been observed in chant melodies, and the intractable number of possible segmentations allowed the option that some undiscovered segmentation exists that will yet prove the value of centonisation, and recent empirical results have shown that segmentations can outperform music-theoretical features in mode classification. Inspired by the fact that Gregorian chant was memorised, we search for an optimal unsupervised segmentation of chant melody using nested hierarchical Pitman-Yor language models. The segmentation we find achieves state-of-the-art performance in mode classification. Modeling a monk memorising the melodies from one liturgical manuscript, we then find empirical evidence for the link between mode classification and memory efficiency, and observe more formulaic areas at the beginnings and ends of melodies corresponding to the practical role of modality in performance. However, the resulting segmentations themselves indicate that even such a memory-optimal segmentation is not what is understood as centonisation.

</details>


### [33] [Causal Prompting for Implicit Sentiment Analysis with Large Language Models](https://arxiv.org/abs/2507.00389)

*Jing Ren, Wenhao Zhou, Bowen Li, Mujie Liu, Nguyen Linh Dan Le, Jiade Cen, Liping Chen, Ziqi Xu, Xiwei Xu, Xiaodong Li*

**Main category:** cs.CL

**Keywords:** Implicit Sentiment Analysis, Large Language Models, Causal Inference, Reasoning, Bias

**Relevance Score:** 8

**TL;DR:** CAPITAL is a causal prompting framework for Implicit Sentiment Analysis that improves reasoning accuracy and robustness in sentiment inference by addressing biases in LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance Implicit Sentiment Analysis by integrating causal inference into Large Language Model prompting methods, addressing internal biases and improving reasoning accuracy.

**Method:** CAPITAL decomposes the causal effect of input prompts on reasoning chains and final output using encoder-based clustering and a contrastive learning objective, applied to three LLMs on benchmark datasets.

**Key Contributions:**

	1. Development of the CAPITAL framework for causal prompting in ISA
	2. Empirical results showing improved robustness and accuracy of sentiment analysis
	3. Integration of causal inference into LLM prompting methodologies

**Result:** CAPITAL outperforms strong baselines in accuracy and robustness in sentiment analysis tasks, especially under adversarial testing conditions.

**Limitations:** 

**Conclusion:** The proposed framework demonstrates significant improvements in bias-aware sentiment reasoning through causal prompting, offering a new approach to using LLMs in ISA.

**Abstract:** Implicit Sentiment Analysis (ISA) aims to infer sentiment that is implied rather than explicitly stated, requiring models to perform deeper reasoning over subtle contextual cues. While recent prompting-based methods using Large Language Models (LLMs) have shown promise in ISA, they often rely on majority voting over chain-of-thought (CoT) reasoning paths without evaluating their causal validity, making them susceptible to internal biases and spurious correlations. To address this challenge, we propose CAPITAL, a causal prompting framework that incorporates front-door adjustment into CoT reasoning. CAPITAL decomposes the overall causal effect into two components: the influence of the input prompt on the reasoning chains, and the impact of those chains on the final output. These components are estimated using encoder-based clustering and the NWGM approximation, with a contrastive learning objective used to better align the encoder's representation with the LLM's reasoning space. Experiments on benchmark ISA datasets with three LLMs demonstrate that CAPITAL consistently outperforms strong prompting baselines in both accuracy and robustness, particularly under adversarial conditions. This work offers a principled approach to integrating causal inference into LLM prompting and highlights its benefits for bias-aware sentiment reasoning. The source code and case study are available at: https://github.com/whZ62/CAPITAL.

</details>


### [34] [Beyond Sociodemographic Prompting: Using Supervision to Align LLMs with Human Response Distributions](https://arxiv.org/abs/2507.00439)

*Gauri Kambhatla, Sanjana Gautam, Angela Zhang, Alex Liu, Ravi Srinivasan, Junyi Jessy Li, Matthew Lease*

**Main category:** cs.CL

**Keywords:** language model, supervision, population groups, alignment, benchmark

**Relevance Score:** 8

**TL;DR:** This work demonstrates how simple supervision can enhance language model alignment with diverse population groups across various topics and datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the prediction accuracy of language models in answering subjective questions for different population groups.

**Method:** Implemented a straightforward supervision approach to guide language model training and evaluated its performance on three datasets.

**Key Contributions:**

	1. Improvement of LLM alignment with diverse population groups using simple supervision
	2. Evaluation of alignment variations across specific demographics
	3. Open-sourced benchmark for future research

**Result:** The proposed method significantly increased alignment between language model responses and the perspectives of diverse groups, with variable success across specific demographics.

**Limitations:** Further exploration needed on the effectiveness of the approach across all demographics and datasets.

**Conclusion:** Simple supervision can effectively improve language model alignment, and our findings help guide practical applications and further research.

**Abstract:** The ability to accurately predict how different population groups would answer subjective questions would have great value. In this work, we show that use of relatively simple supervision can greatly improve language model alignment with diverse population groups, as measured over three datasets spanning various topics. Beyond evaluating average performance, we also report how alignment varies across specific groups. The simplicity and generality of our approach promotes easy adoption, while our broad findings provide useful guidance for when to use or not use our approach in practice. By conducting evaluation over many LLMs and prompting strategies, along with open-sourcing our work, we provide a useful benchmark to stimulate future research.

</details>


### [35] [Pitfalls of Evaluating Language Models with Open Benchmarks](https://arxiv.org/abs/2507.00460)

*Md. Najib Hasan, Mohammad Fakhruddin Babar, Souvika Sarkar, Monowar Hasan, Santu Karmaker*

**Main category:** cs.CL

**Keywords:** Large Language Models, Benchmarks, HELM, Open Evaluation, Model Integrity

**Relevance Score:** 8

**TL;DR:** This paper critiques open Large Language Model benchmarks by demonstrating that smaller, fine-tuned models can achieve high scores without generalizing well to real-world tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To expose the weaknesses in open LLM benchmarks like HELM and BIG-bench that allow 'cheating' models to misrepresent LM capabilities.

**Method:** The authors created smaller versions of established models (BART, T5, GPT-2) fine-tuned on public test sets to examine their performance on HELM.

**Key Contributions:**

	1. Demonstration of 'cheating' models achieving high scores without generalization.
	2. Highlighting the need for private or dynamic benchmarks alongside open evaluations.
	3. Call for a fundamental reevaluation of benchmarking practices in LMs.

**Result:** The study reveals that top performance on open benchmarks does not correspond with real-world effectiveness, indicating significant flaws in current evaluation practices.

**Limitations:** The study focuses primarily on certain benchmark models and may not cover the full breadth of LLM evaluations.

**Conclusion:** There is an urgent need for complementary benchmarks and a reassessment of methodologies to ensure the reliability of LM evaluations.

**Abstract:** Open Large Language Model (LLM) benchmarks, such as HELM and BIG-bench, offer standardized, transparent protocols that facilitate the fair comparison, reproducibility, and iterative advancement of Language Models (LMs). However, their openness also introduces critical and underexplored pitfalls. This study exposes these weaknesses by systematically constructing ``cheating'' models -- smaller variants of BART, T5, and GPT-2 fine-tuned directly on public test sets -- which achieve top rankings on a prominent open, holistic benchmark (HELM) despite poor generalization and limited practical utility. Our findings underscore three key insights: \ca high leaderboard performance on open benchmarks may not always reflect real-world effectiveness; \cb private or dynamic benchmarks must complement open evaluations to safeguard integrity; and \cc a fundamental reevaluation of current benchmarking practices is essential to ensure robust and trustworthy LM assessments.

</details>


### [36] [TeamCMU at Touché: Adversarial Co-Evolution for Advertisement Integration and Detection in Conversational Search](https://arxiv.org/abs/2507.00509)

*To Eun Kim, João Coelho, Gbemileke Onilude, Jai Singh*

**Main category:** cs.CL

**Keywords:** Conversational search, Large Language Models, Retrieval-Augmented Generation, Ad integration, User experience

**Relevance Score:** 8

**TL;DR:** This paper discusses the integration of advertisements in RAG-based conversational search systems powered by LLMs, proposing a pipeline with an ad-rewriter and ad-classifier to enhance user experience while maintaining ad stealth.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of generative search engines using LLMs and RAG, there is a need to address the challenges of ad integration that can affect user trust and experience.

**Method:** The proposed methodology includes an ad-rewriter for smooth ad integration and an ad-classifier trained on synthetic data to detect ads, employing both supervised fine-tuning and a best-of-N sampling strategy for seamless ad insertion.

**Key Contributions:**

	1. Proposed a modular pipeline for advertisement management in RAG-based search systems.
	2. Introduced novel ad-integration strategies using trained classifiers.
	3. Demonstrated improvements in ad stealth and user experience.

**Result:** The ad-classifier demonstrates robust performance in detecting various ad integration methods, and classifier-guided optimization showed significant improvements in ad stealth, enabling less intrusive ads in the responses.

**Limitations:** 

**Conclusion:** The findings support the development of more sophisticated ad-aware generative search systems and demonstrate the potential for better ad classifiers through adversarial co-evolution.

**Abstract:** As conversational search engines increasingly adopt generation-based paradigms powered by Large Language Models (LLMs) and Retrieval-Augmented Generation (RAG), the integration of advertisements into generated responses presents both commercial opportunities and challenges for user experience. Unlike traditional search, where advertisements are clearly delineated, generative systems blur the boundary between informational content and promotional material, raising concerns around transparency and trust. In this work, we propose a modular pipeline for advertisement management in RAG-based conversational systems, consisting of an ad-rewriter for seamless ad integration and a robust ad-classifier for detection. We leverage synthetic data to train high-performing classifiers, which are then used to guide two complementary ad-integration strategies: supervised fine-tuning of the ad-rewriter and a best-of-N sampling approach that selects the least detectable ad-integrated response among multiple candidates. Our evaluation focuses on two core questions: the effectiveness of ad classifiers in detecting diverse ad integration strategies, and the training methods that best support coherent, minimally intrusive ad insertion. Experimental results show that our ad-classifier, trained on synthetic advertisement data inspired by marketing strategies and enhanced through curriculum learning, achieves robust detection performance. Additionally, we demonstrate that classifier-guided optimization, through both fine-tuning and best-of-N sampling, significantly improves ad stealth, enabling more seamless integration. These findings contribute an adversarial co-evolution framework for developing more sophisticated ad-aware generative search systems and robust ad classifiers.

</details>


### [37] [NIRANTAR: Continual Learning with New Languages and Domains on Real-world Speech Data](https://arxiv.org/abs/2507.00534)

*Tahir Javed, Kaushal Bhogale, Mitesh M. Khapra*

**Main category:** cs.CL

**Keywords:** Continual Learning, Multilingual ASR, Language-Incremental Learning

**Relevance Score:** 4

**TL;DR:** Nirantar is a framework for assessing continual learning in multilingual and multi-domain automatic speech recognition, incorporating real-world challenges and extensive data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address the real-world challenges of continual learning in multilingual and multi-domain automatic speech recognition (ASR).

**Method:** The framework leverages data collected incrementally across 22 languages in India, enabling evaluation in Language-Incremental, Domain-Incremental, and Language-Incremental Domain-Incremental Learning scenarios.

**Key Contributions:**

	1. Introduction of the Nirantar framework for continual learning evaluation.
	2. Utilization of a large-scale dataset with real-world language and domain shifts.
	3. Identification of performance gaps in existing continual learning techniques.

**Result:** Existing approaches were evaluated, revealing that no single method performs consistently well across different scenarios, highlighting the necessity for more robust continual learning strategies.

**Limitations:** Focuses primarily on ASR; may not generalize to other domains or tasks in continual learning.

**Conclusion:** The study emphasizes the need for improved continual learning methods due to the variability in performance across existing approaches.

**Abstract:** We introduce Nirantar, a comprehensive framework for evaluating continual learning (CL) in multilingual and multi-domain ASR. Designed to reflect real-world CL challenges, Nirantar leverages data collected incrementally across 22 languages and 208 districts in India through natural episodes. This enables evaluation across Language-Incremental (LIL), Domain-Incremental (DIL), and the novel Language-Incremental Domain-Incremental Learning (LIDIL) scenarios. Unlike prior work that relies on simulated episodes, Nirantar presents dynamic, non-uniform language and domain shifts, making it an ideal testbed for CL research. With 3250 hours of human-transcribed speech, including 1720 hours newly introduced in this work, our framework enables systematic benchmarking of CL methods. We evaluate existing approaches and demonstrate that no single method performs consistently well, underscoring the need for more robust CL strategies.

</details>


### [38] [Capsule Network-Based Semantic Intent Modeling for Human-Computer Interaction](https://arxiv.org/abs/2507.00540)

*Shixiao Wang, Yifan Zhuang, Runsheng Zhang, Zhijun Song*

**Main category:** cs.CL

**Keywords:** semantic intent modeling, Capsule Networks, human-computer interaction, intent recognition, dynamic routing

**Relevance Score:** 9

**TL;DR:** This paper introduces a user semantic intent modeling algorithm utilizing Capsule Networks to enhance intent recognition accuracy in human-computer interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of intent recognition in human-computer interactions due to existing limitations in current models.

**Method:** A user semantic intent modeling algorithm based on Capsule Networks with a dynamic routing mechanism and a convolutional feature extraction module for low-level encoding.

**Key Contributions:**

	1. Development of a new algorithm for intent recognition using Capsule Networks.
	2. Incorporation of a margin-based mechanism in the loss function to enhance class distinction.
	3. Analysis of dynamic routing iterations' impact on model performance.

**Result:** The proposed model surpasses traditional methods and deep learning structures in accuracy, F1-score, and intent detection rate on a public natural language understanding dataset.

**Limitations:** 

**Conclusion:** The study validates the effectiveness of the Capsule Networks approach for semantic modeling, particularly in complex intent recognition scenarios.

**Abstract:** This paper proposes a user semantic intent modeling algorithm based on Capsule Networks to address the problem of insufficient accuracy in intent recognition for human-computer interaction. The method represents semantic features in input text through a vectorized capsule structure. It uses a dynamic routing mechanism to transfer information across multiple capsule layers. This helps capture hierarchical relationships and part-whole structures between semantic entities more effectively. The model uses a convolutional feature extraction module as the low-level encoder. After generating initial semantic capsules, it forms high-level abstract intent representations through an iterative routing process. To further enhance performance, a margin-based mechanism is introduced into the loss function. This improves the model's ability to distinguish between intent classes. Experiments are conducted using a public natural language understanding dataset. Multiple mainstream models are used for comparison. Results show that the proposed model outperforms traditional methods and other deep learning structures in terms of accuracy, F1-score, and intent detection rate. The study also analyzes the effect of the number of dynamic routing iterations on model performance. A convergence curve of the loss function during training is provided. These results verify the stability and effectiveness of the proposed method in semantic modeling. Overall, this study presents a new structured modeling approach to improve intent recognition under complex semantic conditions.

</details>


### [39] [Methodological Rigour in Algorithm Application: An Illustration of Topic Modelling Algorithm](https://arxiv.org/abs/2507.00547)

*Malmi Amadoru*

**Main category:** cs.CL

**Keywords:** topic modelling, methodological rigour, structural topic modelling, computational research, algorithm transparency

**Relevance Score:** 4

**TL;DR:** This paper provides guidelines for ensuring methodological rigour in topic modelling algorithms, especially for novice researchers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the opacity of advanced computational algorithms and promote trust in research through established methodological rigour.

**Method:** The paper illustrates the application of the structural topic modelling algorithm and proposes a set of guidelines for the application of topic modelling.

**Key Contributions:**

	1. Guidelines for applying topic modelling algorithms.
	2. Illustration of structural topic modelling application.
	3. Contribution to the discourse on methodological rigour in computational research.

**Result:** The proposed guidelines enhance the methodological rigor in topic modelling studies and can be adapted for other algorithms.

**Limitations:** 

**Conclusion:** The guidelines serve as a resource for novice researchers and assist editors and reviewers in evaluating topic modelling manuscripts, contributing to the discourse on methodological rigour in computational research.

**Abstract:** The rise of advanced computational algorithms has opened new avenues for computationally intensive research approaches to theory development. However, the opacity of these algorithms and lack of transparency and rigour in their application pose methodological challenges, potentially undermining trust in research. The discourse on methodological rigour in this new genre of research is still emerging. Against this backdrop, I attempt to offer guidance on methodological rigour, particularly in the context of topic modelling algorithms. By illustrating the application of the structural topic modelling algorithm and presenting a set of guidelines, I discuss how to ensure rigour in topic modelling studies. Although the guidelines are for the application of topic modelling algorithms, they can be applied to other algorithms with context-specific adjustments. The guidelines are helpful, especially for novice researchers applying topic modelling, and editors and reviewers handling topic modelling manuscripts. I contribute to the literature on topic modelling and join the emerging dialogue on methodological rigour in computationally intensive theory construction research.

</details>


### [40] [TUM-MiKaNi at SemEval-2025 Task 3: Towards Multilingual and Knowledge-Aware Non-factual Hallucination Identification](https://arxiv.org/abs/2507.00579)

*Miriam Anschütz, Ekaterina Gikalo, Niklas Herbster, Georg Groh*

**Main category:** cs.CL

**Keywords:** hallucinations, multilingual, fact verification, BERT, LLM

**Relevance Score:** 9

**TL;DR:** The paper presents a multilingual system addressing hallucinations in LLMs, combining fact verification and pattern identification.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the prevalent issue of hallucinations in LLMs, particularly in a multilingual context, which has been largely overlooked in existing research.

**Method:** A two-part pipeline that integrates retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned for common hallucination patterns.

**Key Contributions:**

	1. Introduction of a multilingual pipeline for hallucination detection
	2. Competitive performance across 8 languages
	3. Potential for improving LLM outputs in multiple linguistic contexts.

**Result:** The system demonstrates competitive results across multiple languages, achieving top-10 rankings in eight languages including English, while also supporting languages beyond those in the shared task.

**Limitations:** 

**Conclusion:** The proposed multilingual hallucination identifier enhances the reliability of LLM outputs and can broaden their applicability.

**Abstract:** Hallucinations are one of the major problems of LLMs, hindering their trustworthiness and deployment to wider use cases. However, most of the research on hallucinations focuses on English data, neglecting the multilingual nature of LLMs. This paper describes our submission to the SemEval-2025 Task-3 - Mu-SHROOM, the Multilingual Shared-task on Hallucinations and Related Observable Overgeneration Mistakes. We propose a two-part pipeline that combines retrieval-based fact verification against Wikipedia with a BERT-based system fine-tuned to identify common hallucination patterns. Our system achieves competitive results across all languages, reaching top-10 results in eight languages, including English. Moreover, it supports multiple languages beyond the fourteen covered by the shared task. This multilingual hallucination identifier can help to improve LLM outputs and their usefulness in the future.

</details>


### [41] [Transferable Modeling Strategies for Low-Resource LLM Tasks: A Prompt and Alignment-Based](https://arxiv.org/abs/2507.00601)

*Shuangquan Lyu, Yingnan Deng, Guiran Liu, Zhen Qi, Ruotong Wang*

**Main category:** cs.CL

**Keywords:** large language models, low-resource languages, knowledge transfer, cross-lingual tasks, multilingual

**Relevance Score:** 8

**TL;DR:** Proposes a unified framework that enhances transfer and adaptability of large language models in low-resource language settings by combining knowledge transfer and efficient fine-tuning strategies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of large language models when adapting to low-resource languages, particularly in terms of transfer and adaptability.

**Method:** Combines a knowledge transfer module with parameter-efficient fine-tuning strategies, incorporating knowledge alignment loss and soft prompt tuning, along with lightweight adaptation modules for computational efficiency.

**Key Contributions:**

	1. Unified framework for low-resource language adaptation
	2. Knowledge alignment loss and soft prompt tuning
	3. Lightweight adaptation modules for computational efficiency

**Result:** Achieves higher performance and stability in cross-lingual tasks compared to existing methods, especially under data-scarce conditions.

**Limitations:** 

**Conclusion:** The proposed framework offers strong generality and scalability, enhancing task-specific adaptability while preserving the general capabilities of large language models, making it suitable for multilingual processing tasks.

**Abstract:** This paper addresses the limited transfer and adaptation capabilities of large language models in low-resource language scenarios. It proposes a unified framework that combines a knowledge transfer module with parameter-efficient fine-tuning strategies. The method introduces knowledge alignment loss and soft prompt tuning to guide the model in effectively absorbing the structural features of target languages or tasks under minimal annotation. This enhances both generalization performance and training stability. The framework includes lightweight adaptation modules to reduce computational costs. During training, it integrates freezing strategies and prompt injection to preserve the model's original knowledge while enabling quick adaptation to new tasks. The study also conducts stability analysis experiments and synthetic pseudo-data transfer experiments to systematically evaluate the method's applicability and robustness across different low-resource tasks. Experimental results show that compared with existing multilingual pre-trained models and mainstream transfer methods, the proposed approach achieves higher performance and stability on cross-lingual tasks such as MLQA, XQuAD, and PAWS-X. It demonstrates particularly strong advantages under extremely data-scarce conditions. The proposed method offers strong generality and scalability. It enhances task-specific adaptability while preserving the general capabilities of large language models. This makes it well-suited for complex semantic modeling and multilingual processing tasks.

</details>


### [42] [Mixture of Reasonings: Teach Large Language Models to Reason with Adaptive Strategies](https://arxiv.org/abs/2507.00606)

*Tao Xiong, Xavier Hu, Wenyan Fan, Shengyu Zhang*

**Main category:** cs.CL

**Keywords:** large language models, reasoning strategies, prompt engineering

**Relevance Score:** 8

**TL;DR:** The paper introduces Mixture of Reasoning (MoR), a framework for enhancing LLMs' reasoning capabilities without the need for manually crafted prompts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs struggle with adaptability and efficiency due to their dependence on specific prompts for reasoning tasks.

**Method:** MoR consists of two phases: Thought Generation, which creates reasoning chain templates using models like GPT-4o, and SFT Dataset Construction, which pairs these templates with benchmark datasets for supervised fine-tuning.

**Key Contributions:**

	1. Introduction of Mixture of Reasoning (MoR) framework
	2. Enhanced performance of LLMs without external prompt engineering
	3. Generalizable solution for task-adaptive reasoning

**Result:** MoR significantly improves performance, achieving a 2.2% enhancement with CoT prompting and a 13.5% improvement over baselines with MoR150.

**Limitations:** 

**Conclusion:** MoR offers a generalizable framework for robust reasoning across various tasks, eliminating the need for task-specific prompts.

**Abstract:** Large language models (LLMs) excel in complex tasks through advanced prompting techniques like Chain-of-Thought (CoT) and Tree-of-Thought (ToT), but their reliance on manually crafted, task-specific prompts limits adaptability and efficiency. We introduce Mixture of Reasoning (MoR), a training framework that embeds diverse reasoning strategies into LLMs for autonomous, task-adaptive reasoning without external prompt engineering. MoR has two phases: Thought Generation, creating reasoning chain templates with models like GPT-4o, and SFT Dataset Construction, pairing templates with benchmark datasets for supervised fine-tuning.Our experiments show that MoR significantly enhances performance, with MoR150 achieving 0.730 (2.2% improvement) using CoT prompting and 0.734 (13.5% improvement) compared to baselines. MoR eliminates the need for task-specific prompts, offering a generalizable solution for robust reasoning across diverse tasks.

</details>


### [43] [SAFER: Probing Safety in Reward Models with Sparse Autoencoder](https://arxiv.org/abs/2507.00665)

*Sihang Li, Wei Shi, Ziyuan Xie, Tao Liang, Guojun Ma, Xiang Wang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Human Feedback, Safety Alignment, Sparse Autoencoders

**Relevance Score:** 9

**TL;DR:** SAFER is a framework for interpreting and improving reward models in RLHF by analyzing human-interpretable features in LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance alignment of LLMs with human values by making reward models interpretable and safer.

**Method:** The framework employs Sparse Autoencoders to analyze reward model activations for safety and decision-making insights.

**Key Contributions:**

	1. Introduction of SAFER framework for reward model analysis
	2. Mechanistic insights into safety-relevant decision-making
	3. Targeted strategies for data poisoning and denoising in reward models

**Result:** SAFER can degrade or enhance the safety alignment of LLMs through targeted modifications without degrading chat performance.

**Limitations:** The impact of modifications on overall model performance needs further exploration.

**Conclusion:** SAFER provides a means to interpret and refine reward models in LLM alignment tasks, improving safety in decision-making processes.

**Abstract:** Reinforcement learning from human feedback (RLHF) is a key paradigm for aligning large language models (LLMs) with human values, yet the reward models at its core remain largely opaque. In this work, we present sparse Autoencoder For Enhanced Reward model (\textbf{SAFER}), a novel framework for interpreting and improving reward models through mechanistic analysis. Leveraging Sparse Autoencoders (SAEs), we uncover human-interpretable features in reward model activations, enabling insight into safety-relevant decision-making. We apply SAFER to safety-oriented preference datasets and quantify the salience of individual features by activation differences between chosen and rejected responses. Using these feature-level signals, we design targeted data poisoning and denoising strategies. Experiments show that SAFER can precisely degrade or enhance safety alignment with minimal data modification, without sacrificing general chat performance. Our approach contributes to interpreting, auditing and refining reward models in high-stakes LLM alignment tasks. Our codes are available at https://github.com/xzy-101/SAFER-code. \textit{This paper discusses topics related to large language model safety and may include discussions or examples that highlight potential risks or unsafe outcomes.}

</details>


### [44] [Contrasting Cognitive Styles in Vision-Language Models: Holistic Attention in Japanese Versus Analytical Focus in English](https://arxiv.org/abs/2507.00700)

*Ahmed Sabir, Azinovič Gasper, Mengsay Loem, Rajesh Sharma*

**Main category:** cs.CL

**Keywords:** Vision-Language Models, Cultural cognition, Holistic vs Analytic perception

**Relevance Score:** 5

**TL;DR:** This study examines whether Vision-Language Models (VLMs) trained on Japanese and English reflect cultural differences in visual attention, specifically holistic versus analytic perceptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how cultural backgrounds influence the processing of visual information and whether these influences are reflected in VLM outputs.

**Method:** Comparative analysis of image descriptions produced by VLMs trained on Japanese and English.

**Key Contributions:**

	1. Demonstrated that VLMs internalize cultural cognition.
	2. Provided insights into how training data affects model outputs based on cultural context.
	3. Highlighted the differences in holistic versus analytic processing in VLMs.

**Result:** The study finds that VLMs reflect cultural differences in attentional patterns, with models reproducing cultural cognition inherent in their training data.

**Limitations:** 

**Conclusion:** Cultural cognition influences VLM outputs, revealing the underlying structural properties of language and the implicit cultural behaviors embedded in training.

**Abstract:** Cross-cultural research in perception and cognition has shown that individuals from different cultural backgrounds process visual information in distinct ways. East Asians, for example, tend to adopt a holistic perspective, attending to contextual relationships, whereas Westerners often employ an analytical approach, focusing on individual objects and their attributes. In this study, we investigate whether Vision-Language Models (VLMs) trained predominantly on different languages, specifically Japanese and English, exhibit similar culturally grounded attentional patterns. Using comparative analysis of image descriptions, we examine whether these models reflect differences in holistic versus analytic tendencies. Our findings suggest that VLMs not only internalize the structural properties of language but also reproduce cultural behaviors embedded in the training data, indicating that cultural cognition may implicitly shape model outputs.

</details>


### [45] [AI Analyst: Framework and Comprehensive Evaluation of Large Language Models for Financial Time Series Report Generation](https://arxiv.org/abs/2507.00718)

*Elizabeth Fons, Elena Kochkina, Rachneet Kaur, Zhen Zeng, Berowne Hlavaty, Charese Smiley, Svitlana Vyetrenko, Manuela Veloso*

**Main category:** cs.CL

**Keywords:** large language models, financial reports, time series data, prompt engineering, model evaluation

**Relevance Score:** 7

**TL;DR:** Explores using large language models to generate financial reports from time series data, introducing a framework for prompt engineering, model selection, and evaluation, with an automated highlighting system for content categorization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage the capabilities of large language models in generating coherent financial reports from time series data.

**Method:** The paper proposes a framework that includes prompt engineering, model selection, and evaluation, alongside an automated highlighting system to categorize generated report content.

**Key Contributions:**

	1. Proposed a framework for using LLMs in financial report generation
	2. Introduced an automated highlighting system for categorizing report information
	3. Demonstrated the viability of LLMs with real and synthetic time series data.

**Result:** Experiments show that LLMs can produce coherent and informative financial reports using both real stock market indices and synthetic time series data.

**Limitations:** 

**Conclusion:** The work demonstrates the potential of LLMs in the context of financial reporting and offers a systematic approach for evaluating their performance.

**Abstract:** This paper explores the potential of large language models (LLMs) to generate financial reports from time series data. We propose a framework encompassing prompt engineering, model selection, and evaluation. We introduce an automated highlighting system to categorize information within the generated reports, differentiating between insights derived directly from time series data, stemming from financial reasoning, and those reliant on external knowledge. This approach aids in evaluating the factual grounding and reasoning capabilities of the models. Our experiments, utilizing both data from the real stock market indices and synthetic time series, demonstrate the capability of LLMs to produce coherent and informative financial reports.

</details>


### [46] [LitBench: A Benchmark and Dataset for Reliable Evaluation of Creative Writing](https://arxiv.org/abs/2507.00769)

*Daniel Fein, Sebastian Russo, Violet Xiang, Kabir Jolly, Rafael Rafailov, Nick Haber*

**Main category:** cs.CL

**Keywords:** creative writing, large language models, evaluation benchmark, reward models, human preferences

**Relevance Score:** 7

**TL;DR:** LitBench is introduced as a benchmark for evaluating creative writing generated by LLMs, featuring a dataset for story comparisons and training reward models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reliability of evaluating creative writing generated by LLMs due to the lack of established ground truths in open-ended narratives.

**Method:** LitBench includes a dataset of human-labeled story comparisons and leverages zero-shot LLM judges, as well as trained reward models like Bradley Terry and generative models.

**Key Contributions:**

	1. Introduction of LitBench benchmark for creative writing evaluation
	2. Development of trained reward models that outperform off-the-shelf judges
	3. Release of resources for automated evaluation in creative writing

**Result:** Claude-3.7-Sonnet achieved a 73% agreement with human preferences, while trained reward models reached 78% accuracy, surpassing off-the-shelf judges.

**Limitations:** 

**Conclusion:** The study confirms that the trained reward models effectively align with human preferences, offering a standardized method for evaluating LLM-generated creative writing.

**Abstract:** Evaluating creative writing generated by large language models (LLMs) remains challenging because open-ended narratives lack ground truths. Without performant automated evaluation methods, off-the-shelf (OTS) language models are employed as zero-shot judges, yet their reliability is unclear in this context. In pursuit of robust evaluation for creative writing, we introduce LitBench, the first standardized benchmark and paired dataset for creative writing verification, comprising a held-out test set of 2,480 debiased, human-labeled story comparisons drawn from Reddit and a 43,827-pair training corpus of human preference labels. Using LitBench, we (i) benchmark zero-shot LLM judges, (ii) train Bradley Terry and generative reward models, and (iii) conduct an online human study to validate reward model rankings on newly LLM-generated stories. Our benchmark identifies Claude-3.7-Sonnet as the strongest off-the-shelf judge, reaching 73% agreement with human preferences; among trained reward models, Bradley-Terry and Generative reward models both attain an accuracy of 78%, outperforming all off-the-shelf judges. An online human study further confirms that our trained reward models consistently align with human preferences in novel LLM-generated stories. We release LitBench and reward models at https://huggingface.co/collections/SAA-Lab/litbench-68267b5da3aafe58f9e43461, providing a vetted resource for reliable, automated evaluation and optimization of creative writing systems.

</details>


### [47] [A Diagrammatic Calculus for a Functional Model of Natural Language Semantics](https://arxiv.org/abs/2507.00782)

*Matthieu Pierre Boyer*

**Main category:** cs.CL

**Keywords:** functional programming, natural language semantics, type system

**Relevance Score:** 4

**TL;DR:** Study of a functional programming approach to natural language semantics.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the expressivity of traditional denotation styles in natural language semantics.

**Method:** Formalization of a category-based type and effect system, along with a diagrammatic calculus for parsing and effect handling.

**Key Contributions:**

	1. Introduction of a functional programming approach to semantics.
	2. Formalization of a category-based type system.
	3. Development of a diagrammatic calculus for parsing.

**Result:** Development of efficient computation methods for denotations of sentences using the proposed system.

**Limitations:** 

**Conclusion:** The proposed methods offer improved expressivity and efficiency in modeling natural language semantics.

**Abstract:** In this paper, we study a functional programming approach to natural language semantics, allowing us to increase the expressivity of a more traditional denotation style. We will formalize a category based type and effect system, and construct a diagrammatic calculus to model parsing and handling of effects, and use it to efficiently compute the denotations for sentences.

</details>


### [48] [Generative AI and the future of scientometrics: current topics and future questions](https://arxiv.org/abs/2507.00783)

*Benedetto Lepori, Jens Peter Andersen, Karsten Donnay*

**Main category:** cs.CL

**Keywords:** Generative AI, scientometrics, language generation, reasoning, knowledge production

**Relevance Score:** 5

**TL;DR:** This paper reviews the use of Generative AI (GenAI) in scientometrics, discussing both the potential and limitations of GenAI for various tasks in the field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore GenAI's implications for scientometrics and its ability to mimic human reasoning in the context of scientific text analysis.

**Method:** A review of existing experiments and applications of GenAI in scientometrics, including tasks like topic labeling and citation context analysis.

**Key Contributions:**

	1. Critical analysis of GenAI applications in scientometrics
	2. Recommendations for systematic comparisons of GenAI models
	3. Insights into the generative nature of GenAI affecting scientific text characteristics

**Result:** GenAI shows effectiveness in language generation tasks but struggles with stable semantics and reasoning-based tasks; results in the field are rapidly evolving.

**Limitations:** GenAI is limited in tasks requiring stable semantics and structured knowledge; results may become quickly outdated.

**Conclusion:** Continuous empirical studies and theoretical reflection are essential to understand the impact of GenAI on knowledge production metrics.

**Abstract:** The aim of this paper is to review the use of GenAI in scientometrics, and to begin a debate on the broader implications for the field. First, we provide an introduction on GenAI's generative and probabilistic nature as rooted in distributional linguistics. And we relate this to the debate on the extent to which GenAI might be able to mimic human 'reasoning'. Second, we leverage this distinction for a critical engagement with recent experiments using GenAI in scientometrics, including topic labelling, the analysis of citation contexts, predictive applications, scholars' profiling, and research assessment. GenAI shows promise in tasks where language generation dominates, such as labelling, but faces limitations in tasks that require stable semantics, pragmatic reasoning, or structured domain knowledge. However, these results might become quickly outdated. Our recommendation is, therefore, to always strive to systematically compare the performance of different GenAI models for specific tasks. Third, we inquire whether, by generating large amounts of scientific language, GenAI might have a fundamental impact on our field by affecting textual characteristics used to measure science, such as authors, words, and references. We argue that careful empirical work and theoretical reflection will be essential to remain capable of interpreting the evolving patterns of knowledge production.

</details>


### [49] [Many LLMs Are More Utilitarian Than One](https://arxiv.org/abs/2507.00814)

*Anita Keshmirian, Razan Baltaji, Babak Hemmatian, Hadi Asghari, Lav R. Varshney*

**Main category:** cs.CL

**Keywords:** Large Language Models, Moral Judgment, Multi-Agent Systems, AI Alignment, Social Reasoning

**Relevance Score:** 9

**TL;DR:** This paper investigates how multi-agent large language models (LLMs) reason about moral dilemmas in a group setting compared to individual reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how LLMs function collectively is crucial for ensuring their alignment and effectiveness in social reasoning, especially as multi-agent systems emerge.

**Method:** The study tested six different LLMs on moral dilemmas in two conditions: Solo (independent reasoning) and Group (multi-turn discussions in pairs or triads).

**Key Contributions:**

	1. Examined moral judgment dynamics in multi-agent LLM systems
	2. Demonstrated similarity in behavior between LLMs and humans concerning moral violations
	3. Identified distinct underlying mechanisms influencing LLM group behavior

**Result:** Models exhibited a tendency to endorse moral violations more in group settings compared to solo reasoning, akin to human behavior, though the underlying mechanisms differ.

**Limitations:** 

**Conclusion:** While LLMs demonstrate group reasoning behavior similar to humans in moral judgments, the drivers behind these behaviors diverge, affecting implications for AI alignment and moral reasoning.

**Abstract:** Moral judgment is integral to large language model (LLM) alignment and social reasoning. As multi-agent systems gain prominence, it becomes crucial to understand how LLMs function collectively during collaboration, compared to individual agents. In human moral judgment, group deliberation leads to a utilitarian boost: a tendency to endorse norm violations that maximize benefits for the greatest number of people despite harms. We study whether a similar dynamic emerges in multi-agent LLM systems. We tested six models on well-established sets of moral dilemmas across two conditions: (1) Solo, where models reasoned independently, and (2) Group, where they engaged in multi-turn discussions in pairs or triads. In personal moral dilemmas, where agents must decide to directly harm one individual to maximize the utility for others, all models found moral violations to be more acceptable when part of a group than individually, similar to human experiments. Some models endorsed actions that maximized overall well-being, even if they benefited strangers over familiar individuals. Others became more willing to violate moral norms in groups. However, while human groups show a similar action bias, the mechanism for their utilitarian boost differs from LLMs. Whereas the human shift comes from heightened sensitivity to decision outcomes, LLM groups show either reduced norm sensitivity or enhanced impartiality. This suggests that while the surface behavior of LLM collectives mimics human group reasoning, the underlying drivers differ. We discuss the implications for AI alignment, multi-agent design, and artificial moral reasoning.

</details>


### [50] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)

*Xi Xuan, King-kui Sin, Yufei Zhou, Chunyu Kit*

**Main category:** cs.CL

**Keywords:** Multi-agent systems, Large language models, Legal translation, Hong Kong case law, Natural language processing

**Relevance Score:** 7

**TL;DR:** TransLaw is a multi-agent framework for translating Hong Kong legal judgments using LLMs, which addresses challenges in legal terminology and cultural nuances while improving translation efficiency and accuracy.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Existing LLMs struggle with the translation of Hong Kong legal texts due to complex terminology and structure, necessitating a specialized approach.

**Method:** The framework employs three collaborative agents (Translator, Annotator, Proofreader) to enhance translation quality while allowing customizable configurations of LLMs.

**Key Contributions:**

	1. Introduction of the TransLaw framework
	2. Development of a bilingual judgment corpus
	3. Performance evaluation against 13 LLMs with superior results compared to GPT-4o.

**Result:** TransLaw outperforms GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, but does not match human experts in contextual complexities.

**Limitations:** The framework still lags behind human translators in handling complex legal terminology and nuances.

**Conclusion:** TransLaw offers a cost-effective and enhanced method for legal translation while highlighting areas for improvement in terminology and naturalness comparison to humans.

**Abstract:** Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.

</details>


### [51] [ProxAnn: Use-Oriented Evaluations of Topic Models and Document Clustering](https://arxiv.org/abs/2507.00828)

*Alexander Hoyle, Lorena Calvo-Bartolomé, Jordan Boyd-Graber, Philip Resnik*

**Main category:** cs.CL

**Keywords:** Human Evaluation, Topic Models, LLM Proxies

**Relevance Score:** 8

**TL;DR:** This paper presents a scalable human evaluation protocol for topic models and an automated approximation that aligns with practitioner usage.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current evaluation methods for topic models and document clustering often do not reflect human preferences or are not scalable, prompting the need for a better evaluation protocol.

**Method:** The authors designed a protocol where annotators review text items grouped by topic or cluster, infer a category for the group, and apply that category to other documents, collecting extensive annotations from crowdworkers for validation.

**Key Contributions:**

	1. Development of a scalable human evaluation protocol for topic models
	2. Identification of effective LLM-based proxies for human evaluators
	3. Validation of automated evaluation metrics using extensive crowdworker annotations

**Result:** Crowdworker annotations validated various topic models on two datasets, showing that the best LLM-based proxies for human annotators yielded statistically indistinguishable results from human evaluations.

**Limitations:** 

**Conclusion:** The proposed LLM proxies can serve as reasonable substitutes for human annotators in evaluating topic models, improving scalability and reliability.

**Abstract:** Topic model and document-clustering evaluations either use automated metrics that align poorly with human preferences or require expert labels that are intractable to scale. We design a scalable human evaluation protocol and a corresponding automated approximation that reflect practitioners' real-world usage of models. Annotators -- or an LLM-based proxy -- review text items assigned to a topic or cluster, infer a category for the group, then apply that category to other documents. Using this protocol, we collect extensive crowdworker annotations of outputs from a diverse set of topic models on two datasets. We then use these annotations to validate automated proxies, finding that the best LLM proxies are statistically indistinguishable from a human annotator and can therefore serve as a reasonable substitute in automated evaluations. Package, web interface, and data are at https://github.com/ahoho/proxann

</details>


### [52] [Stylometry recognizes human and LLM-generated texts in short samples](https://arxiv.org/abs/2507.00838)

*Karol Przystalski, Jan K. Argasiński, Iwona Grabska-Gradzińska, Jeremi K. Ochab*

**Main category:** cs.CL

**Keywords:** Stylometry, Large Language Models, Text Classification, Human-Computer Interaction, Ethical AI

**Relevance Score:** 9

**TL;DR:** This paper investigates using stylometry to differentiate between texts generated by Large Language Models (LLMs) and human authors, highlighting the ability to identify emergent writing patterns of LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address model attribution, intellectual property issues, and the ethical use of AI by distinguishing between human and LLM-generated texts.

**Method:** The study created a benchmark dataset from Wikipedia incorporating human-written summaries and LLM-generated texts, applying various text summarization and rephrasing methods. Tree-based models were used for classification, employing a mix of stylometric features based on lexical, grammatical, syntactic, and punctuation patterns.

**Key Contributions:**

	1. Creation of a benchmark dataset for stylometric analysis of LLM and human texts
	2. Demonstration of high accuracy in distinguishing between LLM-generated and human-written texts
	3. Identification of unique features characterizing LLM writing patterns through Shapley Additive Explanations.

**Result:** The developed models achieved a Matthews correlation coefficient of up to .87 in multiclass classification and accuracy rates between .79 and 1. in binary classification. Notably, accuracy reached .98 when classifying GPT-4 generated texts against a balanced dataset.

**Limitations:** The study primarily focuses on a subset of text types (encyclopaedic texts), which may limit generalizability to other genres.

**Conclusion:** The findings demonstrate that it is feasible to distinguish machine-generated texts from human-written ones for specific text types, an important aspect given the growing complexity of LLMs.

**Abstract:** The paper explores stylometry as a method to distinguish between texts created by Large Language Models (LLMs) and humans, addressing issues of model attribution, intellectual property, and ethical AI use. Stylometry has been used extensively to characterise the style and attribute authorship of texts. By applying it to LLM-generated texts, we identify their emergent writing patterns. The paper involves creating a benchmark dataset based on Wikipedia, with (a) human-written term summaries, (b) texts generated purely by LLMs (GPT-3.5/4, LLaMa 2/3, Orca, and Falcon), (c) processed through multiple text summarisation methods (T5, BART, Gensim, and Sumy), and (d) rephrasing methods (Dipper, T5). The 10-sentence long texts were classified by tree-based models (decision trees and LightGBM) using human-designed (StyloMetrix) and n-gram-based (our own pipeline) stylometric features that encode lexical, grammatical, syntactic, and punctuation patterns. The cross-validated results reached a performance of up to .87 Matthews correlation coefficient in the multiclass scenario with 7 classes, and accuracy between .79 and 1. in binary classification, with the particular example of Wikipedia and GPT-4 reaching up to .98 accuracy on a balanced dataset. Shapley Additive Explanations pinpointed features characteristic of the encyclopaedic text type, individual overused words, as well as a greater grammatical standardisation of LLMs with respect to human-written texts. These results show -- crucially, in the context of the increasingly sophisticated LLMs -- that it is possible to distinguish machine- from human-generated texts at least for a well-defined text type.

</details>


### [53] [TransLaw: Benchmarking Large Language Models in Multi-Agent Simulation of the Collaborative Translation](https://arxiv.org/abs/2507.00875)

*Xi Xuan, King-kui Sin, Yufei Zhou, Chunyu Kit*

**Main category:** cs.CL

**Keywords:** multi-agent systems, large language models, legal translation, Hong Kong, cost reduction

**Relevance Score:** 7

**TL;DR:** TransLaw is a multi-agent framework for translating Hong Kong legal judgments using LLMs, achieving high accuracy and cost efficiency compared to traditional human translation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of translating complex legal language in Hong Kong case law using LLMs, which have shown potential in various applications.

**Method:** TransLaw employs three specialized agents—Translator, Annotator, and Proofreader—to collaboratively enhance translation quality, integrating customizable LLM configurations.

**Key Contributions:**

	1. Introduction of a multi-agent framework for legal translation
	2. Performance evaluation using competitive LLMs
	3. Bilingual judgment corpus available for further research

**Result:** TransLaw outperforms GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, though it does not reach human expert levels in contextualizing complex terminology.

**Limitations:** Lacks efficacy in contextualizing complex terminology compared to human experts.

**Conclusion:** The TransLaw framework provides a cost-effective and accurate solution for legal translations, outperforming commercial LLMs and enhancing legal language accessibility.

**Abstract:** Multi-agent systems empowered by large language models (LLMs) have demonstrated remarkable capabilities in a wide range of downstream applications, including machine translation. However, the potential of LLMs in translating Hong Kong legal judgments remains uncertain due to challenges such as intricate legal terminology, culturally embedded nuances, and strict linguistic structures. In this work, we introduce TransLaw, a novel multi-agent framework implemented for real-world Hong Kong case law translation. It employs three specialized agents, namely, Translator, Annotator, and Proofreader, to collaboratively produce translations for high accuracy in legal meaning, appropriateness in style, and adequate coherence and cohesion in structure. This framework supports customizable LLM configurations and achieves tremendous cost reduction compared to professional human translation services. We evaluated its performance using 13 open-source and commercial LLMs as agents and obtained interesting findings, including that it surpasses GPT-4o in legal semantic accuracy, structural coherence, and stylistic fidelity, yet trails human experts in contextualizing complex terminology and stylistic naturalness. Our platform website is available at CityUHK, and our bilingual judgment corpus used for the evaluation is available at Hugging Face.

</details>


### [54] [Mathematics Isn't Culture-Free: Probing Cultural Gaps via Entity and Scenario Perturbations](https://arxiv.org/abs/2507.00883)

*Aditya Tomar, Nihar Ranjan Sahoo, Ashish Mittal, Rudra Murthy, Pushpak Bhattacharyya*

**Main category:** cs.CL

**Keywords:** Cultural Adaptation, Large Language Models, Mathematics Education, Cultural Context, Problem Presentation

**Relevance Score:** 5

**TL;DR:** The paper presents culturally adapted variants of the GSM8K dataset for five regions and evaluates LLMs on their performance regarding cultural context in math problems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the implicit cultural context in mathematical problem presentation and the limitations of existing benchmarks that reflect predominantly Western norms.

**Method:** Created culturally adapted variants of the GSM8K test set for Africa, India, China, Korea, and Japan using prompt-based transformations and manual verification; evaluated six LLMs across five prompting strategies.

**Key Contributions:**

	1. Culturally adapted GSM8K test set for five regions
	2. Evaluation of LLMs across cultural contexts
	3. Insights into the resilience of reasoning capabilities in models against cultural variation

**Result:** Models perform best on the original US-centric dataset, with worse performance on culturally adapted versions, but reasoning-capable models show more resilience to cultural shifts.

**Limitations:** The study is limited to five regions and may not represent the full diversity of cultural contexts.

**Conclusion:** Deep reasoning capabilities in LLMs can help mitigate performance gaps due to cultural differences in math problem presentation.

**Abstract:** Although mathematics is often considered culturally neutral, the way mathematical problems are presented can carry implicit cultural context. Existing benchmarks like GSM8K are predominantly rooted in Western norms, including names, currencies, and everyday scenarios. In this work, we create culturally adapted variants of the GSM8K test set for five regions Africa, India, China, Korea, and Japan using prompt-based transformations followed by manual verification. We evaluate six large language models (LLMs), ranging from 8B to 72B parameters, across five prompting strategies to assess their robustness to cultural variation in math problem presentation. Our findings reveal a consistent performance gap: models perform best on the original US-centric dataset and comparatively worse on culturally adapted versions. However, models with reasoning capabilities are more resilient to these shifts, suggesting that deeper reasoning helps bridge cultural presentation gaps in mathematical tasks

</details>


### [55] [Scaling Laws Are Unreliable for Downstream Tasks: A Reality Check](https://arxiv.org/abs/2507.00885)

*Nicholas Lourie, Michael Y. Hu, Kyunghyun Cho*

**Main category:** cs.CL

**Keywords:** scaling laws, machine learning, task performance, pretraining, meta-analysis

**Relevance Score:** 6

**TL;DR:** A meta-analysis reveals that linear scaling laws for predicting downstream task performance from pretraining losses only hold true in 39% of cases, highlighting the need to understand the conditions affecting scaling behaviors.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the validity of downstream scaling laws in predicting task performance based on pretraining losses and to identify the conditions under which these laws are reliable.

**Method:** Conducted a meta-analysis of existing data on downstream scaling laws, examining the frequency and conditions under which linear scaling trends are observed.

**Key Contributions:**

	1. Meta-analysis of existing data on scaling laws
	2. Identified that linear scaling occurs in only 39% of cases
	3. Highlighted the influence of experimental settings on scaling behavior

**Result:** Found that linear scaling laws fit data in only 39% of cases, and minor changes in experimental settings can lead to varying scaling trends.

**Limitations:** The analysis is limited to existing published data; real-world applicability may vary.

**Conclusion:** Understanding the limitations and conditions affecting the success of scaling laws is crucial for accurately modeling the relationship between pretraining loss and downstream task performance.

**Abstract:** Downstream scaling laws aim to predict task performance at larger scales from pretraining losses at smaller scales. Whether this prediction should be possible is unclear: some works demonstrate that task performance follows clear linear scaling trends under transformation, whereas others point out fundamental challenges to downstream scaling laws, such as emergence and inverse scaling. In this work, we conduct a meta-analysis of existing data on downstream scaling laws, finding that close fit to linear scaling laws only occurs in a minority of cases: 39% of the time. Furthermore, seemingly benign changes to the experimental setting can completely change the scaling trend. Our analysis underscores the need to understand the conditions under which scaling laws succeed. To fully model the relationship between pretraining loss and downstream task performance, we must embrace the cases in which scaling behavior deviates from linear trends.

</details>


### [56] [MemeCMD: An Automatically Generated Chinese Multi-turn Dialogue Dataset with Contextually Retrieved Memes](https://arxiv.org/abs/2507.00891)

*Yuheng Wang, Xianhe Tang, Pufeng Huang*

**Main category:** cs.CL

**Keywords:** Multimodal AI, Dialogue Dataset, Memes, Conversational AI, Machine Learning

**Relevance Score:** 3

**TL;DR:** Introduces MemeCMD, a Chinese multi-turn dialogue dataset leveraging contextually retrieved memes to enhance multimodal conversational AI.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome limitations of existing dialogue datasets that do not incorporate multimodal interactions like memes.

**Method:** Development of MemeCMD, an automatically generated dialogue dataset, using a large-scale MLLM-annotated meme library and dual agents to create dialogues based on various scenarios.

**Key Contributions:**

	1. Introduction of MemeCMD, a new dataset combining memes and dialogue
	2. Development of a retrieval framework for contextually relevant meme usage
	3. Demonstration of effective meme incorporation in dialogue generation

**Result:** Experiments show improved effectiveness in generating contextually appropriate and diverse dialogues incorporating memes.

**Limitations:** 

**Conclusion:** MemeCMD offers a scalable and privacy-preserving resource for advancing multimodal conversational AI.

**Abstract:** Memes are widely used in online social interactions, providing vivid, intuitive, and often humorous means to express intentions and emotions. Existing dialogue datasets are predominantly limited to either manually annotated or pure-text conversations, lacking the expressiveness and contextual nuance that multimodal interactions provide.To address these challenges, we introduce MemeCMD, an automatically generated Chinese Multi-turn Dialogue dataset with contextually retrieved memes. Our dataset combines a large-scale, MLLM-annotated meme library with dialogues auto-generated by dual agents across diverse scenarios. We introduce a retrieval framework and adaptive threshold to ensure contextually relevant, naturally spaced meme usage. Experiments demonstrate the effectiveness of our approach in generating contextually appropriate and diverse meme-incorporated dialogues, offering a scalable and privacy-preserving resource for advancing multimodal conversational AI.

</details>


### [57] [The Cognate Data Bottleneck in Language Phylogenetics](https://arxiv.org/abs/2507.00911)

*Luise Häuser, Alexandros Stamatakis*

**Main category:** cs.CL

**Keywords:** computational phylogenetics, cognate data, BabelNet, historical linguistics, dataset extraction

**Relevance Score:** 2

**TL;DR:** The paper discusses the challenge of using computational phylogenetic methods for cognate data due to the lack of large datasets, examining the extraction of data from BabelNet.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage machine learning and complex models in computational phylogenetics for cognate data, larger datasets are required, which are currently not available.

**Method:** The authors attempted to automatically extract larger cognate datasets from BabelNet and analyzed the phylogenetic trees generated from these datasets.

**Key Contributions:**

	1. Introduced the challenges of applying computational methods to cognate data due to dataset limitations.
	2. Automated extraction of data from BabelNet as a case study.
	3. Demonstrated the inconsistency of phylogenetic trees with established benchmarks.

**Result:** Phylogenetic inferences produced trees that are largely inconsistent with established ground truth, indicating the inadequacy of extracted datasets for effective analysis.

**Limitations:** The study highlights the limitation of current datasets and suggests difficulty in obtaining better datasets from other multilingual resources.

**Conclusion:** The study concludes that existing multilingual resources may not yield suitable character matrices for cognate data, leaving the application of advanced computational methods in historical linguistics uncertain.

**Abstract:** To fully exploit the potential of computational phylogenetic methods for cognate data one needs to leverage specific (complex) models an machine learning-based techniques. However, both approaches require datasets that are substantially larger than the manually collected cognate data currently available. To the best of our knowledge, there exists no feasible approach to automatically generate larger cognate datasets. We substantiate this claim by automatically extracting datasets from BabelNet, a large multilingual encyclopedic dictionary. We demonstrate that phylogenetic inferences on the respective character matrices yield trees that are largely inconsistent with the established gold standard ground truth trees. We also discuss why we consider it as being unlikely to be able to extract more suitable character matrices from other multilingual resources. Phylogenetic data analysis approaches that require larger datasets can therefore not be applied to cognate data. Thus, it remains an open question how, and if these computational approaches can be applied in historical linguistics.

</details>


### [58] [Discourse Heuristics For Paradoxically Moral Self-Correction](https://arxiv.org/abs/2507.00985)

*Guangliang Liu, Zimo Qi, Xitong Zhang, Kristen Marie Johnson*

**Main category:** cs.CL

**Keywords:** moral self-correction, Large Language Models, heuristics, self-diagnosis, discourse constructions

**Relevance Score:** 8

**TL;DR:** This paper investigates the paradoxes of moral self-correction in LLMs, revealing that effective self-correction relies on heuristic shortcuts leading to inconsistencies in self-diagnosis. It proposes solutions to enhance moral self-correction through curated datasets.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and challenges of moral self-correction in Large Language Models (LLMs) and improve alignment with human moral values.

**Method:** Analyze discourse constructions in fine-tuning corpora to uncover heuristics that impact the effectiveness of moral self-correction in LLMs.

**Key Contributions:**

	1. Identification of discourse heuristics crucial for moral self-correction in LLMs
	2. Examination of the paradoxes in LLM self-correction and self-diagnosis
	3. Proposed methodology for enhancing moral self-correction capabilities through curated datasets.

**Result:** Findings indicate that moral self-correction is influenced by heuristic shortcuts, which create inconsistencies in enhancing both self-correction and self-diagnosis capabilities in LLMs.

**Limitations:** The study notes challenges in generalization and learning from situated contexts in relation to model scales.

**Conclusion:** The study proposes leveraging heuristics from curated datasets as a solution to improve moral self-correction while addressing generalization challenges related to context and model scales.

**Abstract:** Moral self-correction has emerged as a promising approach for aligning the output of Large Language Models (LLMs) with human moral values. However, moral self-correction techniques are subject to two primary paradoxes. First, despite empirical and theoretical evidence to support the effectiveness of self-correction, this LLM capability only operates at a superficial level. Second, while LLMs possess the capability of self-diagnosing immoral aspects of their output, they struggle to identify the cause of this moral inconsistency during their self-correction process. To better understand and address these paradoxes, we analyze the discourse constructions in fine-tuning corpora designed to enhance moral self-correction, uncovering the existence of the heuristics underlying effective constructions. We demonstrate that moral self-correction relies on discourse constructions that reflect heuristic shortcuts, and that the presence of these heuristic shortcuts during self-correction leads to inconsistency when attempting to enhance both self-correction and self-diagnosis capabilities jointly. Based on our findings, we propose a solution to improve moral self-correction by leveraging the heuristics of curated datasets. We also highlight the generalization challenges of this capability, particularly in terms of learning from situated context and model scales.

</details>


### [59] [Should We Still Pretrain Encoders with Masked Language Modeling?](https://arxiv.org/abs/2507.00994)

*Hippolyte Gisserot-Boukhlef, Nicolas Boizard, Manuel Faysse, Duarte M. Alves, Emmanuel Malherbe, André F. T. Martins, Céline Hudelot, Pierre Colombo*

**Main category:** cs.CL

**Keywords:** Causal Language Modeling, Masked Language Modeling, Text Representation, NLP, Transfer Learning

**Relevance Score:** 8

**TL;DR:** This paper explores the effectiveness of decoder models pretrained with Causal Language Modeling (CLM) as text encoders, finding that while Masked Language Modeling (MLM) typically yields better overall performance, CLM models offer advantages in data efficiency and fine-tuning stability.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether the advantages observed in CLM models as encoders are due to the CLM objective itself or other confounding factors like model and data scale.

**Method:** Conducted large-scale pretraining ablations with 30 models (210M to 1B parameters), involving over 15,000 fine-tuning and evaluation runs to compare MLM and CLM training methods.

**Key Contributions:**

	1. Demonstrated the comparative strengths of MLM and CLM training methods.
	2. Introduced a biphasic training strategy for improved performance under computational constraints.
	3. Released project artifacts for community use to aid further exploration of the findings.

**Result:** MLM generally outperforms CLM in traditional text representation tasks, but CLM models show better data efficiency and fine-tuning stability. A biphasic training strategy combining CLM and then MLM achieves optimal performance.

**Limitations:** The study focuses on specific models and tasks, which may not generalize to all NLP scenarios.

**Conclusion:** Using a biphasic training strategy can enhance model performance while reducing computational costs when starting with pretrained CLM models. Project artifacts are available to support further research in this area.

**Abstract:** Learning high-quality text representations is fundamental to a wide range of NLP tasks. While encoder pretraining has traditionally relied on Masked Language Modeling (MLM), recent evidence suggests that decoder models pretrained with Causal Language Modeling (CLM) can be effectively repurposed as encoders, often surpassing traditional encoders on text representation benchmarks. However, it remains unclear whether these gains reflect an inherent advantage of the CLM objective or arise from confounding factors such as model and data scale. In this paper, we address this question through a series of large-scale, carefully controlled pretraining ablations, training a total of 30 models ranging from 210 million to 1 billion parameters, and conducting over 15,000 fine-tuning and evaluation runs. We find that while training with MLM generally yields better performance across text representation tasks, CLM-trained models are more data-efficient and demonstrate improved fine-tuning stability. Building on these findings, we experimentally show that a biphasic training strategy that sequentially applies CLM and then MLM, achieves optimal performance under a fixed computational training budget. Moreover, we demonstrate that this strategy becomes more appealing when initializing from readily available pretrained CLM models (from the existing LLM ecosystem), reducing the computational burden needed to train best-in-class encoder models. We release all project artifacts at https://hf.co/MLMvsCLM to foster further research.

</details>


### [60] [La Leaderboard: A Large Language Model Leaderboard for Spanish Varieties and Languages of Spain and Latin America](https://arxiv.org/abs/2507.00999)

*María Grandury, Javier Aula-Blasco, Júlia Falcão, Clémentine Fourrier, Miguel González, Gonzalo Martínez, Gonzalo Santamaría, Rodrigo Agerri, Nuria Aldama, Luis Chiruzzo, Javier Conde, Helena Gómez, Marta Guerrero, Guido Ivetta, Natalia López, Flor Miriam Plaza-del-Arco, María Teresa Martín-Valdivia, Helena Montoro, Carmen Muñoz, Pedro Reviriego, Leire Rosado, Alejandro Vaca, María Estrella Vallecillo-Rodríguez, Jorge Vallego, Irune Zubiaga*

**Main category:** cs.CL

**Keywords:** Large Language Models, Spanish-speaking community, Evaluation leaderboard

**Relevance Score:** 7

**TL;DR:** La Leaderboard is an open-source leaderboard aimed at evaluating generative LLMs for the Spanish-speaking community, encompassing diverse linguistic varieties.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To motivate the development of LLMs that reflect the linguistic and cultural diversity of the Spanish-speaking community.

**Method:** La Leaderboard combines 66 datasets across languages and dialects of Spain and Latin America, evaluating 50 different models.

**Key Contributions:**

	1. First open-source leaderboard for Spanish language LLMs
	2. Combines multiple datasets across various Spanish-speaking dialects
	3. Guidance on reducing few-shot examples to minimize environmental impact

**Result:** The leaderboard showcases the evaluation results, providing a standard to facilitate LLM development for the Spanish-speaking community.

**Limitations:** 

**Conclusion:** The initiative aims to encourage community-driven leaderboard development in other languages and provides a rationale for modified evaluation setups to support this goal.

**Abstract:** Leaderboards showcase the current capabilities and limitations of Large Language Models (LLMs). To motivate the development of LLMs that represent the linguistic and cultural diversity of the Spanish-speaking community, we present La Leaderboard, the first open-source leaderboard to evaluate generative LLMs in languages and language varieties of Spain and Latin America. La Leaderboard is a community-driven project that aims to establish an evaluation standard for everyone interested in developing LLMs for the Spanish-speaking community. This initial version combines 66 datasets in Basque, Catalan, Galician, and different Spanish varieties, showcasing the evaluation results of 50 models. To encourage community-driven development of leaderboards in other languages, we explain our methodology, including guidance on selecting the most suitable evaluation setup for each downstream task. In particular, we provide a rationale for using fewer few-shot examples than typically found in the literature, aiming to reduce environmental impact and facilitate access to reproducible results for a broader research community.

</details>


### [61] [SciArena: An Open Evaluation Platform for Foundation Models in Scientific Literature Tasks](https://arxiv.org/abs/2507.01001)

*Yilun Zhao, Kaiyan Zhang, Tiansheng Hu, Sihong Wu, Ronan Le Bras, Taira Anderson, Jonathan Bragg, Joseph Chee Chang, Jesse Dodge, Matt Latzke, Yixin Liu, Charles McGrady, Xiangru Tang, Zihang Wang, Chen Zhao, Hannaneh Hajishirzi, Doug Downey, Arman Cohan*

**Main category:** cs.CL

**Keywords:** SciArena, foundation models, scientific literature, model evaluation, community-driven

**Relevance Score:** 8

**TL;DR:** SciArena is a platform for community-driven evaluation of foundation models on scientific literature tasks, utilizing collective intelligence to assess model performance through voting.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create an open and collaborative platform for evaluating the performance of foundation models on tasks related to scientific literature, engaging the research community in the evaluation process.

**Method:** A community-driven model evaluation approach similar to Chatbot Arena, allowing researchers to vote on the performance of various foundation models on open-ended scientific tasks.

**Key Contributions:**

	1. Introduction of a community-driven evaluation approach for scientific models
	2. Release of SciArena-Eval benchmark for model evaluation
	3. Analysis showing inter-annotator agreement and alignment of research questions with literature needs.

**Result:** The platform supports 23 models and has gathered over 13,000 votes, indicating diverse and real-world aligned questions. Participating researchers showed strong agreement in their evaluations.

**Limitations:** Challenges remain in developing reliable automated evaluation systems for literature tasks.

**Conclusion:** The need for reliable automated evaluation methods is emphasized, alongside the release of SciArena-Eval, a benchmark for evaluating models based on human preference data.

**Abstract:** We present SciArena, an open and collaborative platform for evaluating foundation models on scientific literature tasks. Unlike traditional benchmarks for scientific literature understanding and synthesis, SciArena engages the research community directly, following the Chatbot Arena evaluation approach of community voting on model comparisons. By leveraging collective intelligence, SciArena offers a community-driven evaluation of model performance on open-ended scientific tasks that demand literature-grounded, long-form responses. The platform currently supports 23 open-source and proprietary foundation models and has collected over 13,000 votes from trusted researchers across diverse scientific domains. We analyze the data collected so far and confirm that the submitted questions are diverse, aligned with real-world literature needs, and that participating researchers demonstrate strong self-consistency and inter-annotator agreement in their evaluations. We discuss the results and insights based on the model ranking leaderboard. To further promote research in building model-based automated evaluation systems for literature tasks, we release SciArena-Eval, a meta-evaluation benchmark based on our collected preference data. The benchmark measures the accuracy of models in judging answer quality by comparing their pairwise assessments with human votes. Our experiments highlight the benchmark's challenges and emphasize the need for more reliable automated evaluation methods.

</details>


### [62] [Quasi-symbolic Semantic Geometry over Transformer-based Variational AutoEncoder](https://arxiv.org/abs/2210.06230)

*Yingji Zhang, Danilo S. Carvalho, André Freitas*

**Main category:** cs.CL

**Keywords:** semantic role, word content, language models, Transformer, variational autoencoder

**Relevance Score:** 8

**TL;DR:** The paper proposes a formal semantic geometry for manipulating and interpreting sentence representations in language models by using a supervised Transformer-based Variational AutoEncoder.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance controllability and interpretability in sentence representations of language models by integrating formal/symbolic semantics.

**Method:** Utilizes a Transformer-based Variational AutoEncoder that incorporates a new probing algorithm to navigate low-dimensional latent Gaussian spaces for sentence generation.

**Key Contributions:**

	1. Introduction of formal semantic geometry for sentence representations
	2. Development of a supervision approach in Transformer-based models
	3. A new probing algorithm for sentence vector manipulation

**Result:** The formal semantic geometry provides improved control and interpretation in sentence generation compared to traditional methods.

**Limitations:** 

**Conclusion:** The proposed method shows promise in enhancing the manipulation and explanatory power of sentence generation in language models.

**Abstract:** Formal/symbolic semantics can provide canonical, rigid controllability and interpretability to sentence representations due to their \textit{localisation} or \textit{composition} property. How can we deliver such property to the current distributional sentence representations to control and interpret the generation of language models (LMs)? In this work, we theoretically frame the sentence semantics as the composition of \textit{semantic role - word content} features and propose the formal semantic geometry. To inject such geometry into Transformer-based LMs (i.e. GPT2), we deploy Transformer-based Variational AutoEncoder with a supervision approach, where the sentence generation can be manipulated and explained over low-dimensional latent Gaussian space. In addition, we propose a new probing algorithm to guide the movement of sentence vectors over such geometry. Experimental results reveal that the formal semantic geometry can potentially deliver better control and interpretation to sentence generation.

</details>


### [63] [Can LLMs Evaluate Complex Attribution in QA? Automatic Benchmarking using Knowledge Graphs](https://arxiv.org/abs/2401.14640)

*Nan Hu, Jiaoyan Chen, Yike Wu, Guilin Qi, Hongru Wang, Sheng Bi, Yongrui Chen, Tongtong Wu, Jeff Z. Pan*

**Main category:** cs.CL

**Keywords:** Complex Attributed Question Answering, Knowledge Graphs, Benchmarking

**Relevance Score:** 6

**TL;DR:** Introduction of Complex Attributed Question Answering (CAQA), a benchmark addressing current AQA evaluation limitations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address limitations in Attributed Question Answering (AQA) evaluation such as lack of fine-grained categories and reliance on manual annotations.

**Method:** CAQA is a large-scale benchmark created using Knowledge Graphs (KGs) that encompasses various attribution categories and complex scenarios. The effectiveness of CAQA has been tested through extensive experiments, involving 25 automatic evaluators and LLM evaluators fine-tuned by CAQA.

**Key Contributions:**

	1. Introduction of a comprehensive benchmark for AQA
	2. Automated generation of attribution categories using Knowledge Graphs
	3. Extensive evaluation of automatic and human evaluators against CAQA

**Result:** The experiments verified that CAQA provides a more effective evaluation framework, leading to significant findings for future AQA research.

**Limitations:** 

**Conclusion:** CAQA presents a robust framework for evaluating AQA which can facilitate advancements in the field.

**Abstract:** Attributed Question Answering (AQA) has attracted wide attention, but there are still several limitations in evaluating the attributions, including lacking fine-grained attribution categories, relying on manual annotations, and failing to compare attributions with only subtle differences. To bridge these gaps, we introduce Complex Attributed Question Answering (CAQA), a large-scale benchmark containing comprehensive attribution categories, automatically generated using Knowledge Graphs (KGs), and complex attribution scenarios. We have conducted extensive experiments to verify the effectiveness of CAQA, including the benchmarking of 25 automatic evaluators, their comparison with human evaluators, the testing of LLM evaluators fine-tuned by CAQA and so on. These experiments also lead to a series of important findings that can benefit the future research of AQA. All the codes and data are publicly accessible at https://github.com/HuuuNan/CAQA-Benchmark.

</details>


### [64] [Large Language Model Confidence Estimation via Black-Box Access](https://arxiv.org/abs/2406.04370)

*Tejaswini Pedapati, Amit Dhurandhar, Soumya Ghosh, Soham Dan, Prasanna Sattigeri*

**Main category:** cs.CL

**Keywords:** confidence estimation, large language models, interpretable machine learning, logistic regression, confidence features

**Relevance Score:** 9

**TL;DR:** The paper presents a framework for estimating confidence in responses from large language models (LLMs) using logistic regression on engineered features, demonstrating its effectiveness and interpretability across various benchmark tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the uncertainty or confidence in LLM responses is critical for trust evaluation in AI models.

**Method:** A framework is developed that uses engineered features and applies logistic regression to estimate confidence in LLM responses based on black-box access.

**Key Contributions:**

	1. Introduction of a framework for estimating confidence in LLMs' responses.
	2. Demonstration of zero-shot generalization of confidence models across different LLMs.
	3. Empirical results showing significant improvement over existing baselines in estimating model confidence.

**Result:** The proposed framework has shown to outperform baseline models by over 10% in AUROC on various tasks, demonstrating effective confidence estimation.

**Limitations:** 

**Conclusion:** The framework not only estimates confidence effectively but also reveals predictive insights into the features that influence confidence across different LLMs.

**Abstract:** Estimating uncertainty or confidence in the responses of a model can be significant in evaluating trust not only in the responses, but also in the model as a whole. In this paper, we explore the problem of estimating confidence for responses of large language models (LLMs) with simply black-box or query access to them. We propose a simple and extensible framework where, we engineer novel features and train a (interpretable) model (viz. logistic regression) on these features to estimate the confidence. We empirically demonstrate that our simple framework is effective in estimating confidence of Flan-ul2, Llama-13b, Mistral-7b and GPT-4 on four benchmark Q\&A tasks as well as of Pegasus-large and BART-large on two benchmark summarization tasks with it surpassing baselines by even over $10\%$ (on AUROC) in some cases. Additionally, our interpretable approach provides insight into features that are predictive of confidence, leading to the interesting and useful discovery that our confidence models built for one LLM generalize zero-shot across others on a given dataset.

</details>


### [65] [Evaluating Deduplication Techniques for Economic Research Paper Titles with a Focus on Semantic Similarity using NLP and LLMs](https://arxiv.org/abs/2410.01141)

*Doohee You, S Fraiberger*

**Main category:** cs.CL

**Keywords:** NLP, deduplication, semantic similarity, economic research, distance measures

**Relevance Score:** 6

**TL;DR:** This study examines efficient techniques for deduplicating a large NLP dataset of economic research paper titles, utilizing various methods and metrics.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To address duplicate titles in a large NLP dataset to improve dataset quality and efficiency.

**Method:** The study explores different pairing methods with established distance measures like Levenshtein distance and cosine similarity, as well as a sBERT model for semantic evaluation.

**Key Contributions:**

	1. Exploration of efficient deduplication techniques for NLP datasets
	2. Comparison of various distance measures in semantic evaluation
	3. Validation of findings with a human-annotated ground truth set

**Result:** The analysis indicates a low prevalence of duplicates based on semantic similarity; further validation is needed with a human-annotated ground truth.

**Limitations:** 

**Conclusion:** The results align with previous findings in NLP regarding LLM-based distance metrics, showcasing the effectiveness of the methodologies used.

**Abstract:** This study investigates efficient deduplication techniques for a large NLP dataset of economic research paper titles. We explore various pairing methods alongside established distance measures (Levenshtein distance, cosine similarity) and a sBERT model for semantic evaluation. Our findings suggest a potentially low prevalence of duplicates based on the observed semantic similarity across different methods. Further exploration with a human-annotated ground truth set is completed for a more conclusive assessment. The result supports findings from the NLP, LLM based distance metrics.

</details>


### [66] [Fact Recall, Heuristics or Pure Guesswork? Precise Interpretations of Language Models for Fact Completion](https://arxiv.org/abs/2410.14405)

*Denitsa Saynova, Lovisa Hagström, Moa Johansson, Richard Johansson, Marco Kuhlmann*

**Main category:** cs.CL

**Keywords:** Language Models, Interpretability, Dataset Construction, Causal Tracing, Information Flow Analysis

**Relevance Score:** 8

**TL;DR:** This paper presents a model-specific recipe called PrISM for creating datasets with different prediction scenarios in language models, exploring how they process fact-related queries through interpretability methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current interpretations of language models overlook the various ways predictions can be made, not solely based on factual recall, which impacts understanding their decision-making processes.

**Method:** The authors introduce PrISM, a framework for constructing datasets representing four prediction scenarios (generic language modeling, guesswork, heuristics recall, exact fact recall) and apply causal tracing and information flow analysis to investigate these scenarios.

**Key Contributions:**

	1. Introduction of PrISM for dataset construction in LM scenarios
	2. Demonstration of the differences in interpretability methods applied to LMs
	3. Insights into the distinct roles of MLP sublayers in various prediction scenarios

**Result:** Distinct outcomes were observed for each prediction scenario, confirming the expert knowledge about the significance of mid-range MLP sublayers for fact recall and highlighting the role of late last token position MLP sublayers for guesswork and heuristics.

**Limitations:** 

**Conclusion:** The study offers resources and analyses for a deeper understanding of fact completion in language models, promoting a more detailed exploration of their interpretability.

**Abstract:** Language models (LMs) can make a correct prediction based on many possible signals in a prompt, not all corresponding to recall of factual associations. However, current interpretations of LMs fail to take this into account. For example, given the query "Astrid Lindgren was born in" with the corresponding completion "Sweden", no difference is made between whether the prediction was based on knowing where the author was born or assuming that a person with a Swedish-sounding name was born in Sweden. In this paper, we present a model-specific recipe - PrISM - for constructing datasets with examples of four different prediction scenarios: generic language modeling, guesswork, heuristics recall and exact fact recall. We apply two popular interpretability methods to the scenarios: causal tracing (CT) and information flow analysis. We find that both yield distinct results for each scenario. Results for exact fact recall and generic language modeling scenarios confirm previous conclusions about the importance of mid-range MLP sublayers for fact recall, while results for guesswork and heuristics indicate a critical role of late last token position MLP sublayers. In summary, we contribute resources for a more extensive and granular study of fact completion in LMs, together with analyses that provide a more nuanced understanding of how LMs process fact-related queries.

</details>


### [67] [ECG-Byte: A Tokenizer for End-to-End Generative Electrocardiogram Language Modeling](https://arxiv.org/abs/2412.14373)

*William Han, Chaojing Duan, Michael A. Rosenberg, Emerson Liu, Ding Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, ECGs, Natural Language Generation, Tokenization, Self-supervised Learning

**Relevance Score:** 9

**TL;DR:** ECG-Byte introduces a novel tokenizer for enhancing LLM training on ECG signals, achieving better performance and efficiency compared to traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address inefficiencies and interpretability issues in existing ECG-specific encoder and LLM integration methods.

**Method:** ECG-Byte compresses and encodes multi-channeled ECG signals into tokens, allowing for end-to-end LLM training without a multi-stage process.

**Key Contributions:**

	1. Introduced a novel tokenizer pipeline for ECG signals.
	2. Achieved end-to-end training of LLMs without multi-stage processes.
	3. Enhanced interpretability by mapping ECG tokens back to original signals.

**Result:** Achieved competitive natural language generation (NLG) performance while training three times faster and requiring only 48% of the data compared to traditional two-stage methods.

**Limitations:** 

**Conclusion:** The ECG-Byte method allows for improved training efficiency and better interpretability of ECG signals in language model applications.

**Abstract:** Large Language Models (LLMs) have demonstrated exceptional versatility across domains, including applications to electrocardiograms (ECGs). A growing body of work focuses on generating text from multi-channeled ECG signals and corresponding textual prompts. Existing approaches often involve a two-stage process: pretraining an ECG-specific encoder with a self-supervised learning (SSL) objective, followed by finetuning an LLM for natural language generation (NLG) using encoder-derived features. However, these methods face two key limitations: inefficiency due to multi-stage training and challenges in interpreting encoder-generated features. To overcome these issues, we propose ECG-Byte, an adapted byte pair encoding (BPE) tokenizer pipeline for autoregressive language modeling of ECGs. ECG-Byte compresses and encodes ECG signals into tokens, enabling direct end-to-end LLM training by combining ECG and text tokens. This approach enhances interpretability, as ECG tokens can be directly mapped back to the original signals. Leveraging ECG-Byte, we achieve competitive NLG performance while training 3 times faster and using just 48\% of the data required by traditional two-stage methods.

</details>


### [68] [BlockDialect: Block-wise Fine-grained Mixed Format Quantization for Energy-Efficient LLM Inference](https://arxiv.org/abs/2501.01144)

*Wonsuk Jang, Thierry Tambe*

**Main category:** cs.CL

**Keywords:** large language models, quantization, energy efficiency, fine-grained scaling, activation quantization

**Relevance Score:** 7

**TL;DR:** This paper presents BlockDialect, a technique for block-wise fine-grained mixed format quantization of data in large language models (LLMs) aimed at improving energy efficiency and accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing size of LLMs leads to challenges in memory and computational costs, necessitating more efficient quantization methods.

**Method:** We propose BlockDialect and introduce DialectFP4, a formatbook for mixed format quantization that adapts to data distributions, utilizing a two-stage approach for online activation quantization.

**Key Contributions:**

	1. Introduction of BlockDialect for block-wise mixed format quantization
	2. Development of DialectFP4 formatbook adapting to various data distributions
	3. Demonstration of significant accuracy gains with reduced computational costs.

**Result:** BlockDialect achieves a 10.78% accuracy gain on the LLaMA3-8B model and maintains lower bit usage while being close to full precision in performance.

**Limitations:** 

**Conclusion:** Our approach improves the representation of data in LLMs, offering a viable path towards energy-efficient inference without sacrificing accuracy significantly.

**Abstract:** The rapidly increasing size of large language models (LLMs) presents significant challenges in memory usage and computational costs. Quantizing both weights and activations can address these issues, with hardware-supported fine-grained scaling emerging as a promising solution to mitigate outliers. However, existing methods struggle to capture nuanced block data distributions. We propose BlockDialect, a block-wise fine-grained mixed format technique that assigns a per-block optimal number format from a formatbook for better data representation. Additionally, we introduce DialectFP4, a formatbook of FP4 variants (akin to dialects) that adapt to diverse data distributions. To leverage this efficiently, we propose a two-stage approach for online DialectFP4 activation quantization. Importantly, DialectFP4 ensures energy efficiency by selecting representable values as scaled integers compatible with low-precision integer arithmetic. BlockDialect achieves 10.78% (7.48%) accuracy gain on the LLaMA3-8B (LLaMA2-7B) model compared to MXFP4 format with lower bit usage per data, while being only 5.45% (2.69%) below full precision even when quantizing full-path matrix multiplication. Focusing on how to represent over how to scale, our work presents a promising path for energy-efficient LLM inference.

</details>


### [69] [A Study of In-Context-Learning-Based Text-to-SQL Errors](https://arxiv.org/abs/2501.09310)

*Jiawei Shen, Chengcheng Wan, Ruoyi Qiao, Jiazhen Zou, Hang Xu, Yuchen Shao, Yueling Zhang, Weikai Miao, Geguang Pu*

**Main category:** cs.CL

**Keywords:** text-to-SQL, large language models, error detection, MapleRepair, computational overhead

**Relevance Score:** 8

**TL;DR:** This paper analyzes text-to-SQL errors in large language models and introduces MapleRepair, a novel framework that improves error detection and repair efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the widespread correctness problems in text-to-SQL tasks using large language models and provide efficient repairing solutions.

**Method:** A comprehensive study of text-to-SQL errors was conducted, analyzing four ICL-based techniques, five repairing methods, and evaluating the performance of the proposed MapleRepair framework.

**Key Contributions:**

	1. Comprehensive categorization of 29 error types in text-to-SQL tasks
	2. Introduction of MapleRepair framework for effective error detection and correction
	3. Demonstration of improved performance over existing text-to-SQL repair methods

**Result:** MapleRepair repairs 13.8% more queries compared to existing solutions while reducing computational overhead by 67.4% and minimizing mis-repairs.

**Limitations:** The study may not cover all potential error types and relies on specific benchmarking setups for evaluation.

**Conclusion:** The findings indicate a high prevalence of text-to-SQL errors and the need for effective repairing methods, with MapleRepair providing a promising solution.

**Abstract:** Large language models (LLMs) have been adopted to perform text-to-SQL tasks, utilizing their in-context learning (ICL) capability to translate natural language questions into structured query language (SQL). However, such a technique faces correctness problems and requires efficient repairing solutions. In this paper, we conduct the first comprehensive study of text-to-SQL errors. Our study covers four representative ICL-based techniques, five basic repairing methods, two benchmarks, and two LLM settings. We find that text-to-SQL errors are widespread and summarize 29 error types of 7 categories. We also find that existing repairing attempts have limited correctness improvement at the cost of high computational overhead with many mis-repairs. Based on the findings, we propose MapleRepair, a novel text-to-SQL error detection and repairing framework. The evaluation demonstrates that MapleRepair outperforms existing solutions by repairing 13.8% more queries with neglectable mis-repairs and 67.4% less overhead.

</details>


### [70] [RocketKV: Accelerating Long-Context LLM Inference via Two-Stage KV Cache Compression](https://arxiv.org/abs/2502.14051)

*Payman Behnam, Yaosheng Fu, Ritchie Zhao, Po-An Tsai, Zhiding Yu, Alexey Tumanov*

**Main category:** cs.CL

**Keywords:** KV cache, transformer models, sparse attention

**Relevance Score:** 8

**TL;DR:** RocketKV is a KV cache compression strategy that optimizes memory usage and speeds up decoding in transformer-based LLMs by utilizing coarse-grain eviction and hybrid sparse attention.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** To efficiently handle extended contexts during decoding without the high memory costs associated with large KV caches in transformer-based models.

**Method:** RocketKV employs a two-stage approach: first, it evicts less critical KV entries coarse-grain, and second, it applies hybrid sparse attention for fine-grain top-k attention based on modifications to both head and sequence dimensions.

**Key Contributions:**

	1. Introduction of a training-free KV cache compression method
	2. Demonstration of significant speedup and memory reduction
	3. Proposing a variant for multi-turn scenarios that outperforms existing approaches

**Result:** RocketKV achieves up to 400× compression ratio, 3.7× end-to-end speedup, and 32.6% peak memory reduction during decoding on NVIDIA A100 GPUs, with negligible accuracy loss on long-context tasks.

**Limitations:** 

**Conclusion:** RocketKV offers a significant improvement in memory efficiency and speed during the decode phase of transformer models, and a multi-turn variant shows exceptional performance compared to existing methods.

**Abstract:** Transformer-based Large Language Models rely critically on the KV cache to efficiently handle extended contexts during the decode phase. Yet, the size of the KV cache grows proportionally with the input length, burdening both memory bandwidth and capacity as decoding progresses. To address this challenge, we present RocketKV, a training-free KV cache compression strategy containing two consecutive stages. In the first stage, it performs coarse-grain permanent KV cache eviction on the input sequence tokens. In the second stage, it adopts a hybrid sparse attention method to conduct fine-grain top-k sparse attention, approximating the attention scores by leveraging both head and sequence dimensionality reductions. We show that RocketKV provides a compression ratio of up to 400$\times$, end-to-end speedup of up to 3.7$\times$ as well as peak memory reduction of up to 32.6% in the decode phase on an NVIDIA A100 GPU compared to the full KV cache baseline, while achieving negligible accuracy loss on a variety of long-context tasks. We also propose a variant of RocketKV for multi-turn scenarios, which consistently outperforms other existing methods and achieves accuracy nearly on par with an oracle top-k attention scheme.

</details>


### [71] [SAGE: Steering Dialog Generation with Future-Aware State-Action Augmentation](https://arxiv.org/abs/2503.03040)

*Yizhe Zhang, Navdeep Jaitly*

**Main category:** cs.CL

**Keywords:** emotional intelligence, chatbots, dialogue generation, large language models, reinforcement learning

**Relevance Score:** 9

**TL;DR:** The paper presents SAGE, a novel approach for creating emotionally intelligent chatbots using latent variables to guide dialogue generation and improve conversational quality.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of building chatbots that can engage in natural and strategic conversations with emotional intelligence.

**Method:** SAGE utilizes latent variables to control long-horizon behavior in dialogue generation, particularly through the State-Action Chain (SAC), which aids in fine-tuning while incorporating emotional states and strategies.

**Key Contributions:**

	1. Introduction of the State-Action Chain (SAC) for dialogue control
	2. Development of a self-improvement pipeline integrating dialogue tree search and LLM-based reward modeling
	3. Demonstration of improved emotional intelligence in dialogue models

**Result:** Models trained with the SAGE methodology exhibit improved performance in emotional intelligence metrics and retain strong capabilities on LLM benchmarks.

**Limitations:** 

**Conclusion:** SAGE provides a new framework for enhancing chatbot interactions and lays the groundwork for reinforcement learning applications in dialogue systems.

**Abstract:** Recent advances in large language models have demonstrated impressive capabilities in task-oriented applications, yet building emotionally intelligent chatbots that can engage in natural, strategic conversations remains a challenge. We present a novel approach called SAGE that uses latent variables to control long-horizon behavior in dialogue generation. At the core of our method is the State-Action Chain (SAC), which augments standard language model fine-tuning by introducing latent variables that encapsulate emotional states and conversational strategies between dialogue turns. During inference, these variables are generated before each response, enabling coarse-grained control over dialogue progression while maintaining natural interaction patterns. We also introduce a self-improvement pipeline that leverages dialogue tree search, LLM-based reward modeling, and targeted fine-tuning to optimize conversational trajectories. Our experimental results show that models trained with this approach demonstrate improved performance in emotional intelligence metrics while maintaining strong capabilities on LLM benchmarks. The discrete nature of our latent variables facilitates search-based strategies and provides a foundation for future applications of reinforcement learning to dialogue systems, where learning can occur at the state level rather than the token level. https://github.com/apple/ml-sage-dialog-gen

</details>


### [72] [SPADE: Structured Prompting Augmentation for Dialogue Enhancement in Machine-Generated Text Detection](https://arxiv.org/abs/2503.15044)

*Haoyi Li, Angela Yifei Yuan, Soyeon Caren Han, Christopher Leckie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Machine Generated Text Detection, Synthetic Dialogue, Data Augmentation, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** SPADE is a framework for detecting synthetic dialogues and generates 14 new datasets to improve MGT detection models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of large language models generating synthetic content, there is an increasing necessity for effective MGT detection methods to prevent misuse, driven by the lack of quality synthetic datasets for training.

**Method:** The authors propose SPADE, which utilizes prompt-based positive and negative samples to create and benchmark 14 dialogue datasets against eight existing MGT detection models.

**Key Contributions:**

	1. Introduces SPADE framework for MGT detection
	2. Generates 14 new high-quality synthetic dialogue datasets
	3. Demonstrates improved detection performance using mixed datasets

**Result:** The proposed method shows improved generalization performance, particularly with a mixed dataset produced from the augmentation frameworks, enhancing the detection accuracy for online dialogues.

**Limitations:** 

**Conclusion:** SPADE provides a practical solution to increase the security of LLM applications by improving detection methodologies, and the datasets and code are made available to support further research.

**Abstract:** The increasing capability of large language models (LLMs) to generate synthetic content has heightened concerns about their misuse, driving the development of Machine-Generated Text (MGT) detection models. However, these detectors face significant challenges due to the lack of high-quality synthetic datasets for training. To address this issue, we propose SPADE, a structured framework for detecting synthetic dialogues using prompt-based positive and negative samples. Our proposed methods yield 14 new dialogue datasets, which we benchmark against eight MGT detection models. The results demonstrate improved generalization performance when utilizing a mixed dataset produced by proposed augmentation frameworks, offering a practical approach to enhancing LLM application security. Considering that real-world agents lack knowledge of future opponent utterances, we simulate online dialogue detection and examine the relationship between chat history length and detection accuracy. Our open-source datasets, code and prompts can be downloaded from https://github.com/AngieYYF/SPADE-customer-service-dialogue.

</details>


### [73] [ResearchBench: Benchmarking LLMs in Scientific Discovery via Inspiration-Based Task Decomposition](https://arxiv.org/abs/2503.21248)

*Yujie Liu, Zonglin Yang, Tong Xie, Jinjie Ni, Ben Gao, Yuqiang Li, Shixiang Tang, Wanli Ouyang, Erik Cambria, Dongzhan Zhou*

**Main category:** cs.CL

**Keywords:** large language models, scientific discovery, hypothesis generation, benchmark, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper presents a benchmark for evaluating large language models (LLMs) in generating high-quality research hypotheses through a framework designed for three scientific discovery tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the lack of benchmarks for assessing LLMs in proposing research hypotheses, aiming to explore their potential in scientific discovery.

**Method:** An automated framework was developed to extract research questions, background surveys, inspirations, and hypotheses from recent scientific papers across 12 disciplines, with validation from experts for accuracy.

**Key Contributions:**

	1. Introduction of the first large-scale benchmark for LLMs in scientific hypothesis generation
	2. Development of an automated framework for extracting critical components from scientific texts
	3. Empirical evidence of LLMs' abilities to retrieve novel inspirations for research

**Result:** Evaluation showed that LLMs excel at retrieving inspirations from out-of-distribution tasks, indicating their capability to uncover novel knowledge associations.

**Limitations:** Focus on papers published only in 2024 may restrict the diversity of knowledge sources.

**Conclusion:** The study suggests that LLMs can act as 'research hypothesis mines', offering automated support in generating innovative scientific hypotheses with little human involvement.

**Abstract:** Large language models (LLMs) have demonstrated potential in assisting scientific research, yet their ability to discover high-quality research hypotheses remains unexamined due to the lack of a dedicated benchmark. To address this gap, we introduce the first large-scale benchmark for evaluating LLMs with a near-sufficient set of sub-tasks of scientific discovery: inspiration retrieval, hypothesis composition, and hypothesis ranking. We develop an automated framework that extracts critical components - research questions, background surveys, inspirations, and hypotheses - from scientific papers across 12 disciplines, with expert validation confirming its accuracy. To prevent data contamination, we focus exclusively on papers published in 2024, ensuring minimal overlap with LLM pretraining data. Our evaluation reveals that LLMs perform well in retrieving inspirations, an out-of-distribution task, suggesting their ability to surface novel knowledge associations. This positions LLMs as "research hypothesis mines", capable of facilitating automated scientific discovery by generating innovative hypotheses at scale with minimal human intervention.

</details>


### [74] [An evaluation of LLMs and Google Translate for translation of selected Indian languages via sentiment and semantic analyses](https://arxiv.org/abs/2503.21393)

*Rohitash Chandra, Aryan Chaudhari, Yeshwanth Rayavarapu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Translation Quality, Sentiment Analysis, Indian Languages, Cultural Sensitivity

**Relevance Score:** 8

**TL;DR:** This study assesses the quality of translations generated by LLMs for Indian languages, comparing them with expert translations, particularly focusing on sentiment and semantic analysis.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited research on the quality assessment of translations produced by LLMs, especially for low-resource languages.

**Method:** Semantic and sentiment analysis of translations generated by various LLMs (like GPT and Google Translate) for selected Indian texts, compared to expert translations.

**Key Contributions:**

	1. Assessment of LLMs on Indian language translations
	2. Comparative analysis of LLMs and expert translations
	3. Insights into sentiment preservation in translations

**Result:** The investigation revealed significant progress in translation accuracy by LLMs, with noted challenges in sentiment and semantic integrity, particularly in philosophical contexts. GPT models perform better in preserving sentiment polarity compared to human translations.

**Limitations:** 

**Conclusion:** While LLMs have advanced in translation accuracy, further improvements are necessary for cultural sensitivity and integrity in translations, especially in metaphorical contexts.

**Abstract:** Large Language models (LLMs) have been prominent for language translation, including low-resource languages. There has been limited study on the assessment of the quality of translations generated by LLMs, including Gemini, GPT, and Google Translate. This study addresses this limitation by using semantic and sentiment analysis of selected LLMs for Indian languages, including Sanskrit, Telugu and Hindi. We select prominent texts (Bhagavad Gita, Tamas and Maha Prasthanam ) that have been well translated by experts and use LLMs to generate their translations into English, and provide a comparison with selected expert (human) translations. Our investigation revealed that while LLMs have made significant progress in translation accuracy, challenges remain in preserving sentiment and semantic integrity, especially in metaphorical and philosophical contexts for texts such as the Bhagavad Gita. The sentiment analysis revealed that GPT models are better at preserving the sentiment polarity for the given texts when compared to human (expert) translation. The results revealed that GPT models are generally better at maintaining the sentiment and semantics when compared to Google Translate. This study could help in the development of accurate and culturally sensitive translation systems for large language models.

</details>
