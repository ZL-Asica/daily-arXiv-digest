# 2025-09-09

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 101]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Livia: An Emotion-Aware AR Companion Powered by Modular AI Agents and Progressive Memory Compression](https://arxiv.org/abs/2509.05298)

*Rui Xi, Xianghan Wang*

**Main category:** cs.HC

**Keywords:** augmented reality, emotion detection, AI companions, loneliness, human-computer interaction

**Relevance Score:** 9

**TL;DR:** Livia is an emotion-aware AR companion app that uses modular AI agents to provide personalized emotional support, demonstrating significant reductions in loneliness and enhanced user satisfaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Address loneliness and social isolation through technology-based solutions that offer companionship and emotional support.

**Method:** The app combines modular AI architecture with algorithms for emotion analysis, dialogue generation, memory management, and behavioral orchestration, along with advanced multimodal emotion detection.

**Key Contributions:**

	1. Introduction of Livia, an emotion-aware augmented reality companion app
	2. Development of Temporal Binary Compression and Dynamic Importance Memory Filter algorithms
	3. High accuracy in multimodal emotion detection facilitating empathetic engagement

**Result:** User evaluations showed increased emotional bonds, improved satisfaction, and significant reductions in loneliness due to Livia's adaptive personality and AR interactions.

**Limitations:** 

**Conclusion:** Livia successfully enhances emotional engagement through innovative AI and AR interactions, paving the way for future research into more immersive features.

**Abstract:** Loneliness and social isolation pose significant emotional and health challenges, prompting the development of technology-based solutions for companionship and emotional support. This paper introduces Livia, an emotion-aware augmented reality (AR) companion app designed to provide personalized emotional support by combining modular artificial intelligence (AI) agents, multimodal affective computing, progressive memory compression, and AR driven embodied interaction. Livia employs a modular AI architecture with specialized agents responsible for emotion analysis, dialogue generation, memory management, and behavioral orchestration, ensuring robust and adaptive interactions. Two novel algorithms-Temporal Binary Compression (TBC) and Dynamic Importance Memory Filter (DIMF)-effectively manage and prioritize long-term memory, significantly reducing storage requirements while retaining critical context. Our multimodal emotion detection approach achieves high accuracy, enhancing proactive and empathetic engagement. User evaluations demonstrated increased emotional bonds, improved satisfaction, and statistically significant reductions in loneliness. Users particularly valued Livia's adaptive personality evolution and realistic AR embodiment. Future research directions include expanding gesture and tactile interactions, supporting multi-user experiences, and exploring customized hardware implementations.

</details>


### [2] [Hybrid User Interfaces: Past, Present, and Future of Complementary Cross-Device Interaction in Mixed Reality](https://arxiv.org/abs/2509.05491)

*Sebastian Hubenschmid, Marc Satkowski, Johannes Zagermann, Julián Méndez, Niklas Elmqvist, Steven Feiner, Tiara Feuchtner, Jens Emil Grønbæk, Benjamin Lee, Dieter Schmalstieg, Raimund Dachselt, Harald Reiterer*

**Main category:** cs.HC

**Keywords:** Hybrid User Interfaces, Mixed Reality, Cross-device Interaction

**Relevance Score:** 9

**TL;DR:** This paper investigates hybrid user interfaces (HUIs) that blend 2D and mixed reality devices, aiming to create a cohesive understanding and terminology in this emerging field.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a clear understanding and consistent terminology for hybrid user interfaces, mitigating the fragmented research landscape.

**Method:** Conducted a systematic survey to present a taxonomy of HUIs combining conventional display technology with mixed reality environments.

**Key Contributions:**

	1. Systematic survey of hybrid user interfaces (HUIs)
	2. Proposed taxonomy for HUIs that combines traditional 2D and mixed reality devices
	3. Discussion of the evolution and future opportunities in HUI research

**Result:** The paper discusses past challenges in HUI design and outlines a vision for the evolution of HUIs over the past 30 years.

**Limitations:** 

**Conclusion:** Identifying key design possibilities and challenges in HUIs can unify the research field and leverage the benefits of combining different technologies.

**Abstract:** We investigate hybrid user interfaces (HUIs), aiming to establish a cohesive understanding and adopt consistent terminology for this nascent research area. HUIs combine heterogeneous devices in complementary roles, leveraging the distinct benefits of each. Our work focuses on cross-device interaction between 2D devices and mixed reality environments, which are particularly compelling, leveraging the familiarity of traditional 2D platforms while providing spatial awareness and immersion. Although such HUIs have been prominently explored in the context of mixed reality by prior work, we still lack a cohesive understanding of the unique design possibilities and challenges of such combinations, resulting in a fragmented research landscape. We conducted a systematic survey and present a taxonomy of HUIs that combine conventional display technology and mixed reality environments. Based on this, we discuss past and current challenges, the evolution of definitions, and prospective opportunities to tie together the past 30 years of research with our vision of future HUIs.

</details>


### [3] [GestoBrush: Facilitating Graffiti Artists' Digital Creation Experiences through Embodied AR Interactions](https://arxiv.org/abs/2509.05619)

*Ruiqi Chen, Qingyang He, Hanxi Bao, Jung Choi, Xin Tong*

**Main category:** cs.HC

**Keywords:** Augmented Reality, Graffiti, Embodied Interaction, Digital Art, Cultural Significance

**Relevance Score:** 4

**TL;DR:** This paper explores the development of GestoBrush, an Augmented Reality prototype for graffiti artists, enabling creative expression through embodied interactions while overcoming physical constraints.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To explore digital alternatives for graffiti amid increasing regulations and to focus on the creative processes of graffiti artists instead of just audience engagement.

**Method:** Developed GestoBrush, a mobile AR tool that allows graffiti creation through gestures, and evaluated it through a co-design workshop with graffiti artists.

**Key Contributions:**

	1. Development of GestoBrush, an AR tool for graffiti creation
	2. Emphasis on embodied interactions in digital workflows
	3. Insights into new artistic possibilities for graffiti artists in AR

**Result:** Embodied AR interactions allowed artists to bypass real-world constraints, resulting in new artistic possibilities and enhanced intuitiveness, immersion, and expressiveness in their artworks.

**Limitations:** 

**Conclusion:** Embodied AR tools can bridge physical graffiti practices and digital expression, providing immersive creative systems that respect and expand the cultural aspects of street art.

**Abstract:** Graffiti has long documented the socio-cultural landscapes of urban spaces, yet increasing global regulations have constrained artists' creative freedom, prompting exploration of digital alternatives. Augmented Reality (AR) offers opportunities to extend graffiti into digital environments while retaining spatial and cultural significance, but prior research has largely centered on audience engagement rather than the embodied creative processes of graffiti artists. To address this, we developed GestoBrush, a mobile AR prototype that turns smartphones into virtual spray cans, enabling graffiti creation through embodied gestures. A co-design workshop underscored the role of embodiment-physical engagement with surroundings and body-driven creative processes-in digital workflows. We evaluated GestoBrush with six graffiti artists and findings suggested that embodied AR interactions supporting artists bypass real-world constraints and explore new artistic possibilities, whose AR artworks created enhanced senses of intuitiveness, immersion, and expressiveness. This work highlight how embodied AR tools can bridge the gap between physical graffiti practice and digital expression, suggesting pathways for designing immersive creative systems that respect the cultural ethos of street art while expanding its possibilities in virtual spaces.

</details>


### [4] [Do Vision-Language Models See Visualizations Like Humans? Alignment in Chart Categorization](https://arxiv.org/abs/2509.05718)

*Péter Ferenc Gyarmati, Manfred Klaffenböck, Laura Koesten, Torsten Möller*

**Main category:** cs.HC

**Keywords:** Vision-language models, visualization literacy, human-AI collaboration

**Relevance Score:** 7

**TL;DR:** This study evaluates the ability of vision-language models (VLMs) to classify scientific visualizations based purely on visual characteristics, revealing strengths and weaknesses in their performance compared to human understanding.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Enhancing human-AI collaboration in visualization tools requires a mutual understanding of visual content, which has not been adequately addressed in previous studies.

**Method:** The study involved assessing 13 different VLMs by classifying scientific visualizations based on visual stimuli and criteria including purpose, encoding, and dimensionality.

**Key Contributions:**

	1. Assessment of VLMs' ability to recognize visual properties of charts
	2. Introduction of criteria for classification based on visual stimuli
	3. Findings on the limitations of larger models in performance

**Result:** VLMs generally performed well in identifying visualization purpose and dimensionality, but struggled with accurate recognition of encoding types. Larger models did not necessarily outperform smaller ones.

**Limitations:** The study only examines a limited number of VLMs and their performance in specific visual tasks.

**Conclusion:** Careful integration of VLMs in visualization tasks is crucial, highlighting the importance of human oversight to ensure reliable outcomes.

**Abstract:** Vision-language models (VLMs) hold promise for enhancing visualization tools, but effective human-AI collaboration hinges on a shared perceptual understanding of visual content. Prior studies assessed VLM visualization literacy through interpretive tasks, revealing an over-reliance on textual cues rather than genuine visual analysis. Our study investigates a more foundational skill underpinning such literacy: the ability of VLMs to recognize a chart's core visual properties as humans do. We task 13 diverse VLMs with classifying scientific visualizations based solely on visual stimuli, according to three criteria: purpose (e.g., schematic, GUI, visualization), encoding (e.g., bar, point, node-link), and dimensionality (e.g., 2D, 3D). Using expert labels from the human-centric VisType typology as ground truth, we find that VLMs often identify purpose and dimensionality accurately but struggle with specific encoding types. Our preliminary results show that larger models do not always equate to superior performance and highlight the need for careful integration of VLMs in visualization tasks, with human supervision to ensure reliable outcomes.

</details>


### [5] [A Composable Agentic System for Automated Visual Data Reporting](https://arxiv.org/abs/2509.05721)

*Péter Ferenc Gyarmati, Dominik Moritz, Torsten Möller, Laura Koesten*

**Main category:** cs.HC

**Keywords:** Human-AI Partnership, visual data reporting, multi-agent architecture

**Relevance Score:** 8

**TL;DR:** The paper presents a prototype for automated visual data reporting that utilizes a Hybrid Human-AI Partnership model to improve the robustness of AI agents by externalizing logic from LLMs to deterministic modules.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the brittleness of monolithic AI agents in visual data reporting.

**Method:** A hybrid, multi-agent architecture is used, with a rule-based system called Draco for visualization design, delivering interactive reports and executable notebooks.

**Key Contributions:**

	1. Introduction of a Hybrid Human-AI Partnership model for visual data reporting.
	2. Development of a multi-agent architecture that separates logic from LLMs for enhanced robustness.
	3. Provision of interactive and executable outputs for improved user experience and traceability.

**Result:** The system produces a dual-output: interactive reports for reader exploration and executable notebooks for deep traceability, enabling a fully automatic yet auditable user experience.

**Limitations:** 

**Conclusion:** The proposed architecture charts a pathway for a more synergistic collaboration between human experts and AI systems.

**Abstract:** To address the brittleness of monolithic AI agents, our prototype for automated visual data reporting explores a Human-AI Partnership model. Its hybrid, multi-agent architecture strategically externalizes logic from LLMs to deterministic modules, leveraging the rule-based system Draco for principled visualization design. The system delivers a dual-output: an interactive Observable report with Mosaic for reader exploration, and executable Marimo notebooks for deep, analyst-facing traceability. This granular architecture yields a fully automatic yet auditable and steerable system, charting a path toward a more synergistic partnership between human experts and AI. For reproducibility, our implementation and examples are available at https://peter-gy.github.io/VISxGenAI-2025/.

</details>


### [6] [Augmenting Human-Centered Racial Covenant Detection and Georeferencing with Plug-and-Play NLP Pipelines](https://arxiv.org/abs/2509.05829)

*Jiyoon Pyo, Yuankun Jiao, Yao-Yi Chiang, Michael Corey*

**Main category:** cs.HC

**Keywords:** racial covenants, NLP, geolocation, crowdsourcing, human-centered computing

**Relevance Score:** 4

**TL;DR:** The Mapping Prejudice project enhances the identification and geolocation of properties affected by historical racial covenants using human-centered computing and NLP, aimed at reducing false positives and increasing volunteer engagement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address spatial and socioeconomic inequalities stemming from historical racial covenants in property deeds by leveraging crowdsourcing and automated methods.

**Method:** The paper presents two NLP pipelines: a context-aware text labeling model for identifying racially restrictive language and a georeferencing module for linking geographic descriptions to real-world locations.

**Key Contributions:**

	1. Introduction of a human-centered computing approach for identifying racial covenants
	2. Development of a context-aware text labeling model for precise language identification
	3. Creation of a georeferencing module to streamline mapping of historical properties

**Result:** The system reduces false positives in detecting racial terms by 25.96% while maintaining 91.73% recall, and achieves 85.58% accuracy in georeferencing within specified ranges.

**Limitations:** The need for time-intensive manual geolocation for some properties remains a challenge.

**Conclusion:** The tools developed enhance the efficiency of document filtering and spatial annotations, facilitating increased volunteer participation and public engagement.

**Abstract:** Though no longer legally enforceable, racial covenants in twentieth-century property deeds continue to shape spatial and socioeconomic inequalities. Understanding this legacy requires identifying racially restrictive language and geolocating affected properties. The Mapping Prejudice project addresses this by engaging volunteers on the Zooniverse crowdsourcing platform to transcribe covenants from scanned deeds and link them to modern parcel maps using transcribed legal descriptions. While the project has explored automation, it values crowdsourcing for its social impact and technical advantages. Historically, Mapping Prejudice relied on lexicon-based searching and, more recently, fuzzy matching to flag suspected covenants. However, fuzzy matching has increased false positives, burdening volunteers and raising scalability concerns. Additionally, while many properties can be mapped automatically, others still require time-intensive manual geolocation.   We present a human-centered computing approach with two plug-and-play NLP pipelines: (1) a context-aware text labeling model that flags racially restrictive language with high precision and (2) a georeferencing module that extracts geographic descriptions from deeds and resolves them to real-world locations. Evaluated on historical deed documents from six counties in Minnesota and Wisconsin, our system reduces false positives in racial term detection by 25.96% while maintaining 91.73% recall and achieves 85.58% georeferencing accuracy within 1x1 square-mile ranges. These tools enhance document filtering and enrich spatial annotations, accelerating volunteer participation and reducing manual cleanup while strengthening public engagement.

</details>


### [7] [Attention, Action, and Memory: How Multi-modal Interfaces and Cognitive Load Alter Information Retention](https://arxiv.org/abs/2509.05898)

*Omar Elgohary, Zhu-Tien*

**Main category:** cs.HC

**Keywords:** multi-modal interaction, memory retention, cognitive workload, educational technology, gesture navigation

**Relevance Score:** 7

**TL;DR:** This research examines the effects of multi-modal interaction (gaze-based controls and gesture navigation) on information retention compared to standard track-pad use.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of multi-modal systems on learning and memory retention, particularly in educational and assistive technology contexts.

**Method:** Twelve participants interacted with two user interfaces (track-pad and multi-modal interface) while reading articles. Information retention was assessed immediately and after 24 hours, alongside cognitive workload and usability evaluations.

**Key Contributions:**

	1. Investigated cognitive workload in multi-modal interaction contexts
	2. Provided empirical data on information retention with gaze and gesture control
	3. Delivered design recommendations for educational technology applications

**Result:** Initial findings suggest that multi-modal interaction has a similar effect on immediate information retention to traditional methods but demands higher cognitive workload and worsens long-term retention.

**Limitations:** Small sample size of 12 participants may limit generalizability of results.

**Conclusion:** Multi-modal systems may engage users cognitively but require careful design to prevent negative impacts on memory performance.

**Abstract:** Each year, multi-modal interaction continues to grow within both industry and academia. However, researchers have yet to fully explore the impact of multi-modal systems on learning and memory retention. This research investigates how combining gaze-based controls with gesture navigation affects information retention when compared to standard track-pad usage. A total of twelve participants read four textual articles through two different user interfaces which included a track-pad and a multi-modal interface that tracked eye movements and hand gestures for scrolling, zooming, and revealing content. Participants underwent two assessment sessions that measured their information retention immediately and after a twenty-four hour period along with the NASA-TLX workload evaluation and the System Usability Scale assessment. The initial analysis indicates that multi-modal interaction produces similar targeted information retention to traditional track-pad usage, but this neutral effect comes with higher cognitive workload demands and seems to deteriorate with long-term retention. The research results provide new knowledge about how multi-modal systems affect cognitive engagement while providing design recommendations for future educational and assistive technologies that require effective memory performance.

</details>


### [8] [DRDCAE-STGNN: An End-to-End Discrimina-tive Autoencoder with Spatio-Temporal Graph Learning for Motor Imagery Classification](https://arxiv.org/abs/2509.05943)

*Yi Wang, Haodong Zhang, Hongqi Li*

**Main category:** cs.HC

**Keywords:** brain-computer interfaces, motor imagery, deep learning, spatio-temporal graph neural network, interpretability

**Relevance Score:** 6

**TL;DR:** This paper presents a deep learning framework for enhancing motor imagery (MI) feature learning and classification in brain-computer interfaces (BCIs).

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Motor imagery based brain-computer interfaces offer significant potential for assistive technologies, but precise decoding remains challenging due to the non-stationary nature of MI signals and low signal-to-noise ratio.

**Method:** The proposed framework combines a Discriminative Residual Dense Convolutional Autoencoder (DRDCAE) for learning latent representations with a Spatio-Temporal Graph Neural Network (STGNN) to model spatial and temporal dynamics in MI data.

**Key Contributions:**

	1. Introduction of a novel deep learning framework for MI decoding
	2. High accuracy across multiple datasets
	3. Interpretable model providing meaningful insights into brain connectivity patterns

**Result:** The framework achieved state-of-the-art accuracies of 95.42%, 97.51%, and 90.15% on BCI Competition IV and PhysioNet datasets. Ablation studies validated the contribution of each component, while interpretable analysis showed neurophysiologically meaningful connectivity patterns.

**Limitations:** The complexity of the model may pose challenges for real-time implementation in some settings.

**Conclusion:** The DRDCAE-STGNN framework demonstrates robust, accurate, and interpretable solutions for MI-EEG decoding, with potential for real-time BCI applications across different subjects and tasks.

**Abstract:** Motor imagery (MI) based brain-computer interfaces (BCIs) hold significant potential for assistive technologies and neurorehabilitation. However, the precise and efficient decoding of MI remains challenging due to their non-stationary nature and low signal-to-noise ratio. This paper introduces a novel end-to-end deep learning framework of Discriminative Residual Dense Convolutional Autoencoder with Spatio-Temporal Graph Neural Network (DRDCAE-STGNN) to enhance the MI feature learning and classification. Specifically, the DRDCAE module leverages residual-dense connections to learn discriminative latent representations through joint reconstruction and classifica-tion, while the STGNN module captures dynamic spatial dependencies via a learnable graph adjacency matrix and models temporal dynamics using bidirectional long short-term memory (LSTM). Extensive evaluations on BCI Competition IV 2a, 2b, and PhysioNet datasets demonstrate state-of-the-art performance, with average accuracies of 95.42%, 97.51%, and 90.15%, respectively. Ablation studies confirm the contribution of each component, and interpreta-bility analysis reveals neurophysiologically meaningful connectivity patterns. Moreover, despite its complexity, the model maintains a feasible parameter count and an inference time of 0.32 ms per sample. These results indicate that our method offers a robust, accurate, and interpretable solution for MI-EEG decoding, with strong generalizability across subjects and tasks and meeting the requirements for potential real-time BCI applications.

</details>


### [9] [A Longitudinal Evaluation of Heart Rate Efficiency for Amateur Runners](https://arxiv.org/abs/2509.05961)

*Evgeny V. Votyakov, Marios Constantinides, Fotis Liarokapis*

**Main category:** cs.HC

**Keywords:** wearable devices, Heart Rate Efficiency, fitness tracking, amateur runners, user-centered design

**Relevance Score:** 6

**TL;DR:** Fitplotter visualizes fitness tracking data, introducing Heart Rate Efficiency (HRE) as an explainable metric for aerobic fitness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the self-monitoring capabilities of amateur runners using wearable devices by providing a more interpretable metric for tracking performance.

**Method:** Introduction of Fitplotter, a web application, and formalization of Heart Rate Efficiency (HRE) using training data from athletes.

**Key Contributions:**

	1. Introduction of Fitplotter for data visualization
	2. Formalization of Heart Rate Efficiency as a key fitness metric
	3. Demonstrated correlation of HRE with training volume and seasonal progress

**Result:** HRE offers stability and meaningful feedback over heart rate or pace, correlating with training volume and indicating seasonal progress.

**Limitations:** 

**Conclusion:** HRE can improve training decisions and user experience in fitness tracking tools, making fitness data more explainable and accessible.

**Abstract:** Amateur runners are increasingly using wearable devices to track their training, and often do so through simple metrics such as heart rate and pace. However, these metrics are typically analyzed in isolation and lack the explainability needed for long-term self-monitoring. In this paper, we first present Fitplotter, which is a client-side web application designed for the visualization and analysis of data associated with fitness and activity tracking devices. Next, we revisited and formalized Heart Rate Efficiency (HRE), defined as the product of pace and heart rate, as a practical and explainable metric to track aerobic fitness in everyday running. Drawing on more than a decade of training data from one athlete, and supplemented by publicly available logs from twelve runners, we showed that HRE provides more stable and meaningful feedback on aerobic development than heart rate or pace alone. We showed that HRE correlates with training volume, reflects seasonal progress, and remains stable during long runs in well-trained individuals. We also discuss how HRE can support everyday training decisions, improve the user experience in fitness tracking, and serve as an explainable metric to proprietary ones of commercial platforms. Our findings have implications for designing user-centered fitness tools that empower amateur athletes to understand and manage their own performance data.

</details>


### [10] [The Reel Deal: Designing and Evaluating LLM-Generated Short-Form Educational Videos](https://arxiv.org/abs/2509.05962)

*Lazaros Stavrinou, Argyris Constantinides, Marios Belk, Vasos Vassiliou, Fotis Liarokapis, Marios Constantinides*

**Main category:** cs.HC

**Keywords:** short-form videos, educational technology, large language models, user experience, microlearning

**Relevance Score:** 9

**TL;DR:** This paper presents ReelsEd, a web-based system that utilizes large language models to automatically generate short-form educational videos from longer lectures, demonstrating improved learner engagement and performance in a user study.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the impact of AI-generated short-form videos on learning outcomes and user experience, as traditional methods of video creation are resource-intensive.

**Method:** Development of ReelsEd, a system using large language models to create structured short-form videos from long-form lecture content, evaluated through a user study with 62 students.

**Key Contributions:**

	1. Introduction of ReelsEd for generating educational short-form videos using LLMs
	2. Demonstrated effectiveness of ReelsEd in a user study
	3. Highlighted design opportunities for AI integration in education

**Result:** ReelsEd outperformed traditional long-form videos in terms of learner engagement, quiz performance, and task efficiency, while maintaining acceptable cognitive load levels.

**Limitations:** 

**Conclusion:** The findings indicate the potential for generative AI to enhance educational tools through improved usability and alignment with pedagogical goals.

**Abstract:** Short-form videos are gaining popularity in education due to their concise and accessible format that enables microlearning. Yet, most of these videos are manually created. Even for those automatically generated using artificial intelligence (AI), it is not well understood whether or how they affect learning outcomes, user experience, and trust. To address this gap, we developed ReelsEd, which is a web-based system that uses large language models (LLMs) to automatically generate structured short-form video (i.e., reels) from lecture long-form videos while preserving instructor-authored material. In a between-subject user study with 62 university students, we evaluated ReelsEd and demonstrated that it outperformed traditional long-form videos in engagement, quiz performance, and task efficiency without increasing cognitive load. Learners expressed high trust in our system and valued its clarity, usefulness, and ease of navigation. Our findings point to new design opportunities for integrating generative AI into educational tools that prioritize usability, learner agency, and pedagogical alignment.

</details>


### [11] [Material Experience: An Evaluation Model for Creative Materials Based on Visual-Tactile Sensory Properties](https://arxiv.org/abs/2509.06114)

*Yuxin Zhang, Fan Zhang, Jinjun Xia, Chao Zhao*

**Main category:** cs.HC

**Keywords:** HCI, materials, sensory perception, design, ANOVA

**Relevance Score:** 4

**TL;DR:** The study integrates traditional braids with matrix materials to create sensory-focused designs and provides a model for understanding material experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance designers' understanding of material experiences and guide material selection based on sensory perception.

**Method:** Participants evaluated creative materials under visual-tactile conditions using a 7-point semantic differential scale; correlation analysis and two-way ANOVA were performed, followed by a structural equation model based on exploratory factor analysis.

**Key Contributions:**

	1. Development of creative materials combining traditional braids and matrix materials
	2. Proposal of a structured model for material experience
	3. Insights into the relationships between material properties and sensory perception

**Result:** Creative materials significantly changed impression evaluations compared to traditional braids, with intrinsic, aesthetic, and physical properties impacting material attractiveness.

**Limitations:** 

**Conclusion:** Designers should leverage the interrelations of material properties to improve sensory experience while balancing technology and experience in design.

**Abstract:** This study adopts a design-oriented approach to integrate traditional braids with commonly used matrix materials, developing creative materials with different sensory properties by altering matrix material types and braid patterns. Based on these creative materials, a quantitative and structured model is proposed to assist designers understanding the material experience process and guide material selection by analyzing the relationship between material properties and sensory perception. Specifically, participants evaluated the creative materials under visual-tactile conditions using a 7-point semantic differential (SD) scale. Correlation analysis was performed to explore the data. The main and interaction effects of matrix materials and braid patterns on impression evaluation were analyzed using two-way analysis of variance (ANOVA). A structural equation model (SEM) was constructed based on exploratory factor analysis (EFA), and path coefficients were computed to assess the relative importance of material properties in determining material attractiveness. The results show that, compared to braids, the creative materials resulted in significant changes in impression evaluation. Furthermore, the creative materials can be understood through intrinsic, aesthetic, and physical properties, with their standardized regression coefficients for material attractiveness of 0.486, 0.650, and 0.103, respectively. These properties are interrelated and under their combined influence affect the attractiveness of the material. Therefore, designers should consider utilizing these relationships to enhance sensory experience in order to achieve design objectives. Moreover, designers should also consider balancing technology and experience, using materials according to the principle of "form follows function".

</details>


### [12] [Context-Adaptive Hearing Aid Fitting Advisor through Multi-turn Multimodal LLM Conversation](https://arxiv.org/abs/2509.06382)

*Yingke Ding, Zeyu Wang, Xiyuxing Zhang, Hongbin Chen, Zhenan Xu*

**Main category:** cs.HC

**Keywords:** Hearing Aids, Large Language Models, Context-Aware Systems, Ambient Sound Classification, Assistive Technologies

**Relevance Score:** 7

**TL;DR:** CAFA is a novel assistant for real-time adjustment of hearing aids using a multi-agent LLM workflow that adapts to ambient sounds and user feedback.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a hearing aid system that adapts to dynamic acoustic environments for better user experience.

**Method:** CAFA uses a multi-agent LLM system that incorporates live audio classification and user feedback to make real-time adjustments to hearing aids.

**Key Contributions:**

	1. Introduction of a context-adaptive fitting system for hearing aids
	2. Use of a multi-agent LLM workflow for real-time audio adjustments
	3. Demonstration of high accuracy in sound classification for enhanced user experience.

**Result:** CAFA achieves 91.2% accuracy in classifying ambient sound, leading to improved conversational efficiency.

**Limitations:** 

**Conclusion:** The proposed system demonstrates the effectiveness of multi-modal AI in providing personalized, adaptive hearing aid functionalities.

**Abstract:** Traditional hearing aids often rely on static fittings that fail to adapt to their dynamic acoustic environments. We propose CAFA, a Context-Adaptive Fitting Advisor that provides personalized, real-time hearing aid adjustments through a multi-agent Large Language Model (LLM) workflow. CAFA combines live ambient audio, audiograms, and user feedback in a multi-turn conversational system. Ambient sound is classified into conversation, noise, or quiet with 91.2\% accuracy using a lightweight neural network based on YAMNet embeddings. This system utilizes a modular LLM workflow, comprising context acquisition, subproblem classification, strategy provision, and ethical regulation, and is overseen by an LLM Judge. The workflow translates context and feedback into precise, safe tuning commands. Evaluation confirms that real-time sound classification enhances conversational efficiency. CAFA exemplifies how agentic, multimodal AI can enable intelligent, user-centric assistive technologies.

</details>


### [13] [Talking to an AI Mirror: Designing Self-Clone Chatbots for Enhanced Engagement in Digital Mental Health Support](https://arxiv.org/abs/2509.06393)

*Mehrnoosh Sadat Shirvani, Jackie Liu, Thomas Chao, Suky Martinez, Laura Brandt, Ig-Jae Kim, Dongwook Yoon*

**Main category:** cs.HC

**Keywords:** Mental Health, Conversational Agents, AI-driven Chatbots, User Engagement, Therapeutic Interventions

**Relevance Score:** 9

**TL;DR:** This paper presents AI-driven self-clone chatbots to enhance user engagement in mental health therapy by replicating users' support strategies and conversational patterns.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Mental health conversational agents can enhance therapy effectiveness, but low user engagement is a significant barrier. The study aims to improve engagement through tailored AI-driven chatbots based on self-conversation techniques.

**Method:** The researchers designed self-clone chatbots and conducted a semi-controlled experiment with 180 participants to compare engagement levels between self-clone chatbots and generic counselor personas.

**Key Contributions:**

	1. Introduces self-clone chatbots for enhancing engagement in mental health therapy.
	2. Validates the effectiveness of self-clones in a semi-controlled experiment.
	3. Explores implications for AI applications in mental health care.

**Result:** Participants demonstrated significantly higher emotional and cognitive engagement with self-clone chatbots compared to generic ones, highlighting the effectiveness of this tailored approach.

**Limitations:** The study was conducted in a semi-controlled environment which may not fully represent real-world scenarios.

**Conclusion:** Self-clone chatbots can improve therapeutic engagement in mental health interventions, with believability being a key factor for effective user interaction.

**Abstract:** Mental health conversational agents have the potential to deliver valuable therapeutic impact, but low user engagement remains a critical barrier hindering their efficacy. Existing therapeutic approaches have leveraged clients' internal dialogues (e.g., journaling, talking to an empty chair) to enhance engagement through accountable, self-sourced support. Inspired by these, we designed novel AI-driven self-clone chatbots that replicate users' support strategies and conversational patterns to improve therapeutic engagement through externalized meaningful self-conversation. Validated through a semi-controlled experiment (N=180), significantly higher emotional and cognitive engagement was demonstrated with self-clone chatbots than a chatbot with a generic counselor persona. Our findings highlight self-clone believability as a mediator and emphasize the balance required in maintaining convincing self-representation while creating positive interactions. This study contributes to AI-based mental health interventions by introducing and evaluating self-clones as a promising approach to increasing user engagement, while exploring implications for their application in mental health care.

</details>


### [14] [Explained, yet misunderstood: How AI Literacy shapes HR Managers' interpretation of User Interfaces in Recruiting Recommender Systems](https://arxiv.org/abs/2509.06475)

*Yannick Kalff, Katharina Simbeck*

**Main category:** cs.HC

**Keywords:** AI literacy, explainable AI, recruitment, human resource management, trust

**Relevance Score:** 7

**TL;DR:** This study investigates how AI literacy among HR managers impacts their understanding and perception of explainable AI features in recruitment dashboards.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the critical need for transparency and responsible adoption of AI in HR management, particularly regarding its influence on recruitment decisions.

**Method:** An online experiment was conducted with 410 German-based HR managers who compared baseline recruitment dashboards to enhanced versions featuring three styles of explainable AI.

**Key Contributions:**

	1. Identification of the gap in AI explanation provided in current recruitment dashboards
	2. Demonstration of how AI literacy affects perception and understanding of XAI features
	3. Recommendations for tailored explanation strategies and literacy training in HRM

**Result:** Dashboards used in practice often lack explanations for AI results. Adding XAI features improves subjective perceptions of trust and helpfulness among AI literate users, but does not enhance their objective understanding, with potential confusion arising from complex explanations.

**Limitations:** The study is limited to German-based HR managers, which may affect the generalizability of the findings.

**Conclusion:** The effectiveness of XAI in recruitment varies with users' AI literacy, indicating the necessity for tailored explanations and targeted training in HRM for responsible AI adoption.

**Abstract:** AI-based recommender systems increasingly influence recruitment decisions. Thus, transparency and responsible adoption in Human Resource Management (HRM) are critical. This study examines how HR managers' AI literacy influences their subjective perception and objective understanding of explainable AI (XAI) elements in recruiting recommender dashboards. In an online experiment, 410 German-based HR managers compared baseline dashboards to versions enriched with three XAI styles: important features, counterfactuals, and model criteria. Our results show that the dashboards used in practice do not explain AI results and even keep AI elements opaque. However, while adding XAI features improves subjective perceptions of helpfulness and trust among users with moderate or high AI literacy, it does not increase their objective understanding. It may even reduce accurate understanding, especially with complex explanations. Only overlays of important features significantly aided the interpretations of high-literacy users. Our findings highlight that the benefits of XAI in recruitment depend on users' AI literacy, emphasizing the need for tailored explanation strategies and targeted literacy training in HRM to ensure fair, transparent, and effective adoption of AI.

</details>


### [15] [Mapping Community Appeals Systems: Lessons for Community-led Moderation in Multi-Level Governance](https://arxiv.org/abs/2509.06557)

*Juhoon Lee, Bich Ngoc Doan, Jonghyun Jee, Joseph Seering*

**Main category:** cs.HC

**Keywords:** community governance, moderation, Discord, appeals system, user empowerment

**Relevance Score:** 7

**TL;DR:** This paper examines community-led appeal systems on Discord, highlighting how structured processes can balance scalability and fairness while empowering users.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how community-led governance can maintain user-centered values within platform moderation.

**Method:** Qualitative interviews with focus groups and individual interviews with 17 community moderators on Discord.

**Key Contributions:**

	1. Investigation of community appeals systems on Discord
	2. Insights into community-led governance processes
	3. Recommendations for platforms on improving moderation systems

**Result:** Findings indicate a structured appeals process that upholds community values of growth, fairness, and accountability, empowering users in moderation decisions.

**Limitations:** 

**Conclusion:** Community-led governance can inform platforms about balancing stakeholder interests and integrating procedural fairness and design.

**Abstract:** Platforms are increasingly adopting industrial models of moderation that prioritize scalability and consistency, frequently at the expense of context-sensitive and user-centered values. Building on the multi-level governance framework that examines the interdependent relationship between platforms and middle-level communities, we investigate community appeals systems on Discord as a model for successful community-led governance. We investigate how Discord servers operationalize appeal systems through a qualitative interview study with focus groups and individual interviews with 17 community moderators. Our findings reveal a structured appeals process that balances scalability, fairness, and accountability while upholding community-centered values of growth and rehabilitation. Communities design these processes to empower users, ensuring their voices are heard in moderation decisions and fostering a sense of belonging. This research provides insights into the practical implementation of community-led governance in a multi-level governance framework, illustrating how communities can maintain their core principles while integrating procedural fairness and tool-based design. We discuss how platforms can gain insights from community-led moderation work to motivate governance structures that effectively balance and align the interests of multiple stakeholders.

</details>


### [16] [Hue4U: Real-Time Personalized Color Correction in Augmented Reality](https://arxiv.org/abs/2509.06776)

*Jingwen Qin, Semen Checherin, Yue Li, Berend-Jan van der Zwaag, Özlem Durmaz-Incel*

**Main category:** cs.HC

**Keywords:** Color Vision Deficiency, augmented reality, color-correction, visual accessibility, user study

**Relevance Score:** 7

**TL;DR:** Hue4U is a real-time color-correction system in augmented reality designed for individuals with Color Vision Deficiency (CVD), which adapts to users without prior medical diagnosis.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance visual accessibility for individuals with Color Vision Deficiency by providing a personalized color-correction solution.

**Method:** A personalized, real-time color-correction system using consumer-grade Meta Quest headsets is developed, which adapts to the user during use.

**Key Contributions:**

	1. Introduction of a real-time color-correction system in augmented reality for CVD
	2. No prior medical diagnosis required for system adaptation
	3. Demonstrated significant improvements in color distinction for users with CVD

**Result:** User study with 10 participants revealed notable improvements in color distinction abilities and clinically meaningful gains (Cohen's d > 1.4).

**Limitations:** 

**Conclusion:** Personalized augmented reality interventions, such as Hue4U, have the potential to significantly improve the quality of life and visual accessibility for individuals with CVD.

**Abstract:** Color Vision Deficiency (CVD) affects nearly 8 percent of men and 0.5 percent of women worldwide. Existing color-correction methods often rely on prior clinical diagnosis and static filtering, making them less effective for users with mild or moderate CVD. In this paper, we introduce Hue4U, a personalized, real-time color-correction system in augmented reality using consumer-grade Meta Quest headsets. Unlike previous methods, Hue4U requires no prior medical diagnosis and adapts to the user in real time. A user study with 10 participants showed notable improvements in their ability to distinguish colors. The results demonstrated large effect sizes (Cohen's d > 1.4), suggesting clinically meaningful gains for individuals with CVD. These findings highlight the potential of personalized AR interventions to improve visual accessibility and quality of life for people affected by CVD.

</details>


### [17] ["It was Tragic": Exploring the Impact of a Robot's Shutdown](https://arxiv.org/abs/2509.06934)

*Agam Oberlender, Hadas Erel*

**Main category:** cs.HC

**Keywords:** Human-Robot Interaction, Anthropomorphism, Robot Shutdown, Social Interpretation, Design

**Relevance Score:** 9

**TL;DR:** This study investigates how people perceive the shutdown of robotic arms, exploring the impact of shutdown gestures on their social interpretation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the social dynamics of how humans interpret robotic gestures, particularly during shutdown procedures.

**Method:** An experiment was conducted where participants interacted with a robotic arm and then turned it off under two different conditions: a non-designed abrupt shutdown and a designed gradual shutdown resembling 'falling asleep.'

**Key Contributions:**

	1. Identifying the influence of shutdown gestures on human perception of robots
	2. Demonstrating the anthropomorphism of robots during shutdown
	3. Highlighting the importance of designing robotic interactions thoughtfully.

**Result:** Participants anthropomorphized the robot's shutdown gestures. The abrupt shutdown was perceived negatively, akin to death, while the gradual shutdown was viewed more positively as the robot 'going to sleep,' affecting its perceived likeability and intelligence.

**Limitations:** The study focused on a single type of robot and scenario; results may vary with different robots or contexts.

**Conclusion:** Designing interactions for turning off robots is crucial, as people naturally view robots as social entities. The way robots are shut down can influence human perceptions significantly.

**Abstract:** It is well established that people perceive robots as social entities, even when they are not designed for social interaction. We evaluated whether the social interpretation of robotic gestures should also be considered when turning off a robot. In the experiment, participants engaged in a brief preliminary neutral interaction while a robotic arm showed interest in their actions. At the end of the task, participants were asked to turn off the robotic arm under two conditions: (1) a Non-designed condition, where all of the robot's engines were immediately and simultaneously turned off, as robots typically shut down; (2) a Designed condition, where the robot's engines gradually folded inward in a motion resembling "falling asleep." Our findings revealed that all participants anthropomorphized the robot's movement when it was turned off. In the Non-designed condition, most participants interpreted the robot's turn-off movement negatively, as if the robot had "died." In the Designed condition, most participants interpreted it more neutrally, stating that the robot "went to sleep." The robot's turn-off movement also impacted its perception, leading to higher likeability, perceived intelligence, and animacy in the Designed condition. We conclude that the impact of common edge interactions, such as turning off a robot, should be carefully designed while considering people's automatic tendency to perceive robots as social entities.

</details>


### [18] [Understanding the Challenges of Maker Entrepreneurship](https://arxiv.org/abs/2501.13765)

*Natalie Friedman, Alexandra Bremers, Adelaide Nyanyo, Ian Clark, Yasmine Kotturi, Laura Dabbish, Wendy Ju, Nikolas Martelaro*

**Main category:** cs.HC

**Keywords:** maker movement, entrepreneurship, technology support, sustainability, non-monetary values

**Relevance Score:** 6

**TL;DR:** The research explores the transition from maker to maker entrepreneur, emphasizing the role of technology in facilitating this change and addressing the challenges faced by makers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To broaden the viability of making as a sustainable livelihood and support makers transitioning to entrepreneurship.

**Method:** Interviews with 20 maker entrepreneurs, six creative service entrepreneurs, and seven support personnel to gather qualitative insights on their experiences and challenges.

**Key Contributions:**

	1. Identification of the challenges faced by maker entrepreneurs
	2. Insights into the non-monetary motivations of makers
	3. Recommendations for technology-based support in developing sustainable practices

**Result:** Many makers identify primarily as creators rather than entrepreneurs, face difficulties with business logistics, learn skills on-the-go, and are driven by values beyond monetary gain.

**Limitations:** 

**Conclusion:** Design implications for training and technology support can help develop economically sustainable businesses for makers.

**Abstract:** The maker movement embodies a resurgence in DIY creation, merging physical craftsmanship and arts with digital technology support. However, mere technological skills and creativity are insufficient for economically and psychologically sustainable practice. By illuminating and smoothing the path from ``maker" to ``maker entrepreneur," we can help broaden the viability of making as a livelihood. Our research centers on makers who design, produce, and sell physical goods. In this work, we explore the transition to entrepreneurship for these makers and how technology can facilitate this transition online and offline. We present results from interviews with 20 USA-based maker entrepreneurs {(i.e., lamps, stickers)}, six creative service entrepreneurs {(i.e., photographers, fabrication)}, and seven support personnel (i.e., art curator, incubator director). Our findings reveal that many maker entrepreneurs 1) are makers first and entrepreneurs second; 2) struggle with business logistics and learn business skills as they go; and 3) are motivated by non-monetary values. We discuss training and technology-based design implications and opportunities for addressing challenges in developing economically sustainable businesses around making.

</details>


### [19] [Beyond SHAP and Anchors: A large-scale experiment on how developers struggle to design meaningful end-user explanations](https://arxiv.org/abs/2503.15512)

*Zahra Abba Omar, Nadia Nahar, Jacob Tjaden, Inès M. Gilles, Fikir Mekonnen, Jane Hsieh, Christian Kästner, Alka Menon*

**Main category:** cs.HC

**Keywords:** Machine Learning, Explainability, Policy Compliance, Human-Computer Interaction, Healthcare Applications

**Relevance Score:** 8

**TL;DR:** The paper investigates challenges developers face in providing understandable explanations for machine learning models, particularly in healthcare applications, and assesses the effectiveness of policy guidance on their compliance and design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns regarding trust, oversight, safety, and human dignity arise due to the complexity of modern machine learning models, necessitating better explanations for end-users.

**Method:** A large-scale experiment with 124 participants explored developers' approaches to creating end-user explanations and compliance with policy guidance for an ML-powered screening tool for diabetic retinopathy.

**Key Contributions:**

	1. Investigation into developer challenges in explanation generation for ML models.
	2. Assessment of policy guidance's effectiveness on explanation design and compliance.
	3. Recommendations for educational interventions based on cognitive process theory.

**Result:** Participants struggled with generating quality explanations and complying with policies; specific forms of policy guidance had minimal effect on their ability to design effective explanations.

**Limitations:** The study may not account for various developer backgrounds or technical levels outside the specific context of diabetic retinopathy screening tools.

**Conclusion:** The findings suggest developers' noncompliance is partly due to their inability to anticipate non-technical stakeholders' needs, leading to recommendations for educational interventions to improve understanding.

**Abstract:** Modern machine learning produces models that are impossible for users or developers to fully understand--raising concerns about trust, oversight, safety, and human dignity when they are integrated into software products. Transparency and explainability methods aim to provide some help in understanding models, but it remains challenging for developers to design explanations that are understandable to target users and effective for their purpose. Emerging guidelines and regulations set goals but may not provide effective actionable guidance to developers. In a large-scale experiment with 124 participants, we explored how developers approach providing end-user explanations, including what challenges they face, and to what extent specific policies can guide their actions. We investigated whether and how specific forms of policy guidance help developers design explanations and provide evidence for policy compliance for an ML-powered screening tool for diabetic retinopathy. Participants across the board struggled to produce quality explanations and comply with the provided policies. Contrary to our expectations, we found that the nature and specificity of policy guidance had little effect. We posit that participant noncompliance is in part due to a failure to imagine and anticipate the needs of non-technical stakeholders. Drawing on cognitive process theory and the sociological imagination to contextualize participants' failure, we recommend educational interventions.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [An Empirical Analysis of Discrete Unit Representations in Speech Language Modeling Pre-training](https://arxiv.org/abs/2509.05359)

*Yanis Labrak, Richard Dufour, Mickaël Rouvier*

**Main category:** cs.CL

**Keywords:** Speech Language Models, pre-training, discrete representations

**Relevance Score:** 6

**TL;DR:** This paper examines discrete unit representations in Speech Language Models, focusing on optimizing speech modeling and robustness during continual pre-training.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To optimize speech modeling during the continual pre-training stage of Speech Language Models by investigating the influence of model architecture, data representation, and training robustness.

**Method:** The paper systematically examines the effects of speech encoders, clustering granularity, and domain matching during pre-training through experiments that analyze cluster distribution and phonemic alignments.

**Key Contributions:**

	1. Introduces novel insights on the role of clustering granularity across different model scales.
	2. Analyzes the impact of speech encoder variations on pre-training outcomes.
	3. Explores the domain matching necessity for improved model robustness.

**Result:** The experiments demonstrate how optimal discretization strategies differ with model capacity and highlight the importance of effective discrete vocabulary use in capturing linguistic and paralinguistic patterns.

**Limitations:** 

**Conclusion:** The study underscores the significance of clustering data selection and domain matching for improving model robustness in speech applications using discrete representations.

**Abstract:** This paper investigates discrete unit representations in Speech Language Models (SLMs), focusing on optimizing speech modeling during continual pre-training. In this paper, we systematically examine how model architecture, data representation, and training robustness influence the pre-training stage in which we adapt existing pre-trained language models to the speech modality. Our experiments highlight the role of speech encoders and clustering granularity across different model scales, showing how optimal discretization strategies vary with model capacity. By examining cluster distribution and phonemic alignments, we investigate the effective use of discrete vocabulary, uncovering both linguistic and paralinguistic patterns. Additionally, we explore the impact of clustering data selection on model robustness, highlighting the importance of domain matching between discretization training and target applications.

</details>


### [21] [Beyond ROUGE: N-Gram Subspace Features for LLM Hallucination Detection](https://arxiv.org/abs/2509.05360)

*Jerry Li, Evangelos Papalexakis*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucination detection, N-Gram frequency tensor, multi-layer perceptron, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper presents a novel approach using N-Gram frequency tensors to detect hallucinations in Large Language Models, improving on traditional methods and demonstrating better accuracy in classification using a multi-layer perceptron.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the problem of hallucinations in Large Language Models that reduce their trustworthiness in generating accurate information.

**Method:** The authors propose a method that constructs an N-Gram frequency tensor from LLM-generated text, capturing semantic structure through co-occurrence patterns. This tensor is used to train a binary classifier with multi-layer perceptron techniques.

**Key Contributions:**

	1. Introduction of N-Gram frequency tensors for hallucination detection.
	2. Use of tensor decomposition methods for feature extraction.
	3. Demonstrated significant improvement over traditional baselines.

**Result:** The proposed method significantly improves detection of hallucinations over traditional metrics and achieves competitive performance against state-of-the-art LLM judges.

**Limitations:** 

**Conclusion:** The novel N-Gram frequency tensor approach enhances the differentiation between factual and hallucinated content, offering a promising solution to the hallucination problem in LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated effectiveness across a wide variety of tasks involving natural language, however, a fundamental problem of hallucinations still plagues these models, limiting their trustworthiness in generating consistent, truthful information. Detecting hallucinations has quickly become an important topic, with various methods such as uncertainty estimation, LLM Judges, retrieval augmented generation (RAG), and consistency checks showing promise. Many of these methods build upon foundational metrics, such as ROUGE, BERTScore, or Perplexity, which often lack the semantic depth necessary to detect hallucinations effectively. In this work, we propose a novel approach inspired by ROUGE that constructs an N-Gram frequency tensor from LLM-generated text. This tensor captures richer semantic structure by encoding co-occurrence patterns, enabling better differentiation between factual and hallucinated content. We demonstrate this by applying tensor decomposition methods to extract singular values from each mode and use these as input features to train a multi-layer perceptron (MLP) binary classifier for hallucinations. Our method is evaluated on the HaluEval dataset and demonstrates significant improvements over traditional baselines, as well as competitive performance against state-of-the-art LLM judges.

</details>


### [22] [A Lightweight Framework for Trigger-Guided LoRA-Based Self-Adaptation in LLMs](https://arxiv.org/abs/2509.05385)

*Jiacheng Wei, Faguo Wu, Xiao Zhang*

**Main category:** cs.CL

**Keywords:** dynamic fine-tuning, adaptive learning, large language models

**Relevance Score:** 7

**TL;DR:** SAGE is a dynamic fine-tuning framework that improves reasoning in large language models by decomposing complex tasks and enabling adaptive learning at inference time.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Large language models face limitations in adapting to new data during inference, which affects their reasoning abilities.

**Method:** SAGE includes a Trigger module for real-time failure detection, a Trigger Buffer for clustering anomalies, and a Lora Store for dynamic parameter updates.

**Key Contributions:**

	1. Introduction of SAGE framework for dynamic fine-tuning
	2. Real-time failure detection via Trigger module
	3. Dynamic optimization of model parameters with Lora Store

**Result:** SAGE shows improved accuracy, robustness, and stability in atomic reasoning tasks through dynamic updates during inference.

**Limitations:** 

**Conclusion:** The framework successfully enhances the performance of language models in adapting to new data during reasoning.

**Abstract:** Large language models are unable to continuously adapt and learn from new data during reasoning at inference time. To address this limitation, we propose that complex reasoning tasks be decomposed into atomic subtasks and introduce SAGE, a trigger-guided dynamic fine-tuning framework that enables adaptive updates during reasoning at inference time. SAGE consists of three key components: (1) a Trigger module that detects reasoning failures through multiple evaluation metrics in real time; (2) a Trigger Buffer module that clusters anomaly samples using a streaming clustering process with HDBSCAN, followed by stability checks and similarity-based merging; and (3) a Lora Store module that dynamically optimizes parameter updates with an adapter pool for knowledge retention. Evaluation results show that SAGE demonstrates excellent accuracy, robustness, and stability on the atomic reasoning subtask through dynamic knowledge updating during test time.

</details>


### [23] [Talk Isn't Always Cheap: Understanding Failure Modes in Multi-Agent Debate](https://arxiv.org/abs/2509.05396)

*Andrea Wynn, Harsh Satija, Gillian Hadfield*

**Main category:** cs.CL

**Keywords:** multi-agent debate, AI reasoning, model diversity, performance degradation, peer reasoning

**Relevance Score:** 6

**TL;DR:** This paper investigates the detrimental effects of multi-agent debate on AI reasoning, emphasizing the role of model diversity.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how the diversity in model capabilities influences multi-agent debate and AI reasoning outcomes, particularly when naive application of debate can lead to performance degradation.

**Method:** Conducted a series of experiments comparing debates within homogeneous groups versus diverse model groups, observing how model capabilities impact debate dynamics.

**Key Contributions:**

	1. Demonstrates the negative impact of diversity in model capabilities on debate outcomes.
	2. Highlights failure modes where agents favor incorrect reasoning for agreement over accuracy.
	3. Challenges the naive application of multi-agent debate in AI systems.

**Result:** Findings reveal that debate can reduce accuracy over time, with stronger models sometimes agreeing with weaker ones, thus leading to incorrect conclusions.

**Limitations:** Does not address corrective measures for improving debate outcomes.

**Conclusion:** Agents may favor consensus over challenging flawed reasoning, indicating critical failure modes in multi-agent debate that must be addressed.

**Abstract:** While multi-agent debate has been proposed as a promising strategy for improving AI reasoning ability, we find that debate can sometimes be harmful rather than helpful. The prior work has exclusively focused on debates within homogeneous groups of agents, whereas we explore how diversity in model capabilities influences the dynamics and outcomes of multi-agent interactions. Through a series of experiments, we demonstrate that debate can lead to a decrease in accuracy over time -- even in settings where stronger (i.e., more capable) models outnumber their weaker counterparts. Our analysis reveals that models frequently shift from correct to incorrect answers in response to peer reasoning, favoring agreement over challenging flawed reasoning. These results highlight important failure modes in the exchange of reasons during multi-agent debate, suggesting that naive applications of debate may cause performance degradation when agents are neither incentivized nor adequately equipped to resist persuasive but incorrect reasoning.

</details>


### [24] [No Translation Needed: Forecasting Quality from Fertility and Metadata](https://arxiv.org/abs/2509.05425)

*Jessica M. Lundin, Ada Zhang, David Adelani, Cody Carroll*

**Main category:** cs.CL

**Keywords:** translation quality, GPT-4o, linguistic metadata, feature importance, multilingual evaluation

**Relevance Score:** 7

**TL;DR:** This paper predicts translation quality of GPT-4o translations without running the translation system, using features like token fertility ratios and linguistic metadata to achieve high accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to explore how translation quality can be predicted using linguistic features, without the need for executing the translation system itself.

**Method:** Utilizes gradient boosting models to analyze features such as token fertility ratios, token counts, and linguistic metadata to forecast ChrF scores for translations across 203 languages.

**Key Contributions:**

	1. Introduces a method to predict translation quality without direct execution of translation systems.
	2. Demonstrates the importance of typological factors and token fertility in translation quality.
	3. Provides a framework for multilingual evaluation using a limited set of features.

**Result:** Achieves R² values of 0.66 for XX->English and 0.72 for English->XX translations, indicating strong predictive performance.

**Limitations:** The study focuses primarily on GPT-4o translations and may not generalize to other translation systems or languages outside the FLORES-200 benchmark.

**Conclusion:** The results indicate that translation quality is influenced by both token-level factors and broader linguistic typological elements, providing insights for multilingual evaluation.

**Abstract:** We show that translation quality can be predicted with surprising accuracy \textit{without ever running the translation system itself}. Using only a handful of features, token fertility ratios, token counts, and basic linguistic metadata (language family, script, and region), we can forecast ChrF scores for GPT-4o translations across 203 languages in the FLORES-200 benchmark. Gradient boosting models achieve favorable performance ($R^{2}=0.66$ for XX$\rightarrow$English and $R^{2}=0.72$ for English$\rightarrow$XX). Feature importance analyses reveal that typological factors dominate predictions into English, while fertility plays a larger role for translations into diverse target languages. These findings suggest that translation quality is shaped by both token-level fertility and broader linguistic typology, offering new insights for multilingual evaluation and quality estimation.

</details>


### [25] [Direct-Scoring NLG Evaluators Can Use Pairwise Comparisons Too](https://arxiv.org/abs/2509.05440)

*Logan Lawrence, Ashton Williamson, Alexander Shelton*

**Main category:** cs.CL

**Keywords:** large-language models, automatic raters, evaluation, direct-scoring method, machine rankings

**Relevance Score:** 8

**TL;DR:** This paper presents a direct-scoring method using synthetic summaries to improve upon traditional pairwise comparisons by enabling absolute scoring for machine-generated texts.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the evaluation of large-language models by enabling absolute scores for summaries, which is essential for applications that require thresholding.

**Method:** The paper introduces a direct-scoring method that utilizes synthetic summaries to perform pairwise machine rankings at test time.

**Key Contributions:**

	1. Introduction of a direct-scoring method.
	2. Use of synthetic summaries for pairwise machine rankings.
	3. Comparative performance analysis against state-of-the-art methods.

**Result:** The proposed method achieves sample-level correlations comparable to state-of-the-art pairwise evaluators across several benchmarks.

**Limitations:** 

**Conclusion:** The method shows promise in improving the evaluation process of automatic raters for free-form content while releasing synthetic summaries for further research.

**Abstract:** As large-language models have been increasingly used as automatic raters for evaluating free-form content, including document summarization, dialog, and story generation, work has been dedicated to evaluating such models by measuring their correlations with human judgment. For \textit{sample-level} performance, methods which operate by using pairwise comparisons between machine-generated text perform well but often lack the ability to assign absolute scores to individual summaries, an ability crucial for use cases that require thresholding. In this work, we propose a direct-scoring method which uses synthetic summaries to act as pairwise machine rankings at test time. We show that our method performs comparably to state-of-the-art pairwise evaluators in terms of axis-averaged sample-level correlations on the SummEval (\textbf{+0.03}), TopicalChat (\textbf{-0.03}), and HANNA (\textbf{+0.05}) meta-evaluation benchmarks, and release the synthetic in-context summaries as data to facilitate future work.

</details>


### [26] [From Staff Messages to Actionable Insights: A Multi-Stage LLM Classification Framework for Healthcare Analytics](https://arxiv.org/abs/2509.05484)

*Hajar Sakai, Yi-En Tseng, Mohammadsadegh Mikaeili, Joshua Bosire, Franziska Jovin*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare Analytics, Decision Support Tool

**Relevance Score:** 9

**TL;DR:** This paper presents a multi-stage framework utilizing Large Language Models (LLMs) for analyzing hospital staff messages to enhance healthcare analytics and decision support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Hospital call centers generate vast amounts of text data from patient requests and staff communications, which can be analyzed for insights but face challenges like the need for annotated data in traditional methods.

**Method:** The authors propose a multi-stage LLM-based framework that classifies staff message topics and reasons using various LLM types, focusing on their performance and compliance with data security standards.

**Key Contributions:**

	1. Development of a framework for LLM-based analysis of healthcare communication
	2. Evaluation of different LLM types for staff message classification
	3. Integration of findings into a decision support tool for healthcare professionals

**Result:** The best-performing model, o3, achieved a 78.4% weighted F1-score and 79.2% accuracy, effectively classifying messages to derive actionable insights while ensuring HIPAA compliance.

**Limitations:** 

**Conclusion:** The proposed methodology enhances the use of staff messaging data, aids in navigator training, and supports the improvement of patient experience and care quality.

**Abstract:** Hospital call centers serve as the primary contact point for patients within a hospital system. They also generate substantial volumes of staff messages as navigators process patient requests and communicate with the hospital offices following the established protocol restrictions and guidelines. This continuously accumulated large amount of text data can be mined and processed to retrieve insights; however, traditional supervised learning approaches require annotated data, extensive training, and model tuning. Large Language Models (LLMs) offer a paradigm shift toward more computationally efficient methodologies for healthcare analytics. This paper presents a multi-stage LLM-based framework that identifies staff message topics and classifies messages by their reasons in a multi-class fashion. In the process, multiple LLM types, including reasoning, general-purpose, and lightweight models, were evaluated. The best-performing model was o3, achieving 78.4% weighted F1-score and 79.2% accuracy, followed closely by gpt-5 (75.3% Weighted F1-score and 76.2% accuracy). The proposed methodology incorporates data security measures and HIPAA compliance requirements essential for healthcare environments. The processed LLM outputs are integrated into a visualization decision support tool that transforms the staff messages into actionable insights accessible to healthcare professionals. This approach enables more efficient utilization of the collected staff messaging data, identifies navigator training opportunities, and supports improved patient experience and care quality.

</details>


### [27] [The Token Tax: Systematic Bias in Multilingual Tokenization](https://arxiv.org/abs/2509.05486)

*Jessica M. Lundin, Ada Zhang, Nihal Karim, Hamza Louzan, Victor Wei, David Adelani, Cody Carroll*

**Main category:** cs.CL

**Keywords:** Tokenization, Large Language Models, NLP, Multilingual Benchmarks, Morphological Awareness

**Relevance Score:** 8

**TL;DR:** Evaluation of LLMs on AfriMMLU highlights tokenization inefficiencies in low-resource languages and suggests morphologically aware tokenization for equitable NLP.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Tokenization inefficiency in morphologically complex, low-resource languages leads to increased compute resources and lower accuracy in NLP tasks.

**Method:** Evaluated 10 large language models on the AfriMMLU dataset, analyzing the relationship between token fertility and model accuracy.

**Key Contributions:**

	1. Identified relationship between token fertility and model accuracy.
	2. Demonstrated superiority of reasoning models in language comprehension.
	3. Proposed solutions for equitable NLP through improved tokenization and pricing.

**Result:** Higher token fertility correlates with lower accuracy; reasoning models outperform non-reasoning models across various languages.

**Limitations:** Focus limited to specific language models and dataset; wider application needs further investigation.

**Conclusion:** Morphological awareness in tokenization could reduce costs and improve NLP outcomes for underrepresented languages.

**Abstract:** Tokenization inefficiency imposes structural disadvantages on morphologically complex, low-resource languages, inflating compute resources and depressing accuracy. We evaluate 10 large language models (LLMs) on AfriMMLU (9,000 MCQA items; 5 subjects; 16 African languages) and show that fertility (tokens/word) reliably predicts accuracy. Higher fertility consistently predicts lower accuracy across all models and subjects. We further find that reasoning models (DeepSeek, o1) consistently outperform non-reasoning peers across high and low resource languages in the AfriMMLU dataset, narrowing accuracy gaps observed in prior generations. Finally, translating token inflation to economics, a doubling in tokens results in quadrupled training cost and time, underscoring the token tax faced by many languages. These results motivate morphologically aware tokenization, fair pricing, and multilingual benchmarks for equitable natural language processing (NLP).

</details>


### [28] [Biomedical Literature Q&A System Using Retrieval-Augmented Generation (RAG)](https://arxiv.org/abs/2509.05505)

*Mansi Garg, Lee-Chi Wang, Bhavesh Ghanchi, Sanjana Dumpala, Shreyash Kakde, Yen Chih Chen*

**Main category:** cs.CL

**Keywords:** Biomedical Literature, Question Answering, Retrieval-Augmented Generation, Machine Learning, Health Informatics

**Relevance Score:** 10

**TL;DR:** This paper presents a Biomedical Literature Q&A system using a RAG architecture to enhance access to medical information, improving upon traditional health search engines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve access to accurate, evidence-based medical information with an efficient Q&A system that addresses limitations of conventional health search engines.

**Method:** The system utilizes a retrieval pipeline with MiniLM-based semantic embeddings and FAISS vector search, while employing a fine-tuned Mistral-7B-v0.3 model for generating answers.

**Key Contributions:**

	1. Integration of multiple biomedical information sources for better Q&A performance.
	2. Use of MiniLM embeddings and FAISS for efficient retrieval.
	3. Demonstrated improvements in factual consistency and relevance for medical queries.

**Result:** Empirical results indicate substantial improvements in factual consistency and semantic relevance over baseline models, especially in breast cancer literature evaluations.

**Limitations:** 

**Conclusion:** RAG-enhanced language models present a promising solution for making complex biomedical literature more accessible and pave the way for further advancements in medical AI systems.

**Abstract:** This work presents a Biomedical Literature Question Answering (Q&A) system based on a Retrieval-Augmented Generation (RAG) architecture, designed to improve access to accurate, evidence-based medical information. Addressing the shortcomings of conventional health search engines and the lag in public access to biomedical research, the system integrates diverse sources, including PubMed articles, curated Q&A datasets, and medical encyclopedias ,to retrieve relevant information and generate concise, context-aware responses. The retrieval pipeline uses MiniLM-based semantic embeddings and FAISS vector search, while answer generation is performed by a fine-tuned Mistral-7B-v0.3 language model optimized using QLoRA for efficient, low-resource training. The system supports both general medical queries and domain-specific tasks, with a focused evaluation on breast cancer literature demonstrating the value of domain-aligned retrieval. Empirical results, measured using BERTScore (F1), show substantial improvements in factual consistency and semantic relevance compared to baseline models. The findings underscore the potential of RAG-enhanced language models to bridge the gap between complex biomedical literature and accessible public health knowledge, paving the way for future work on multilingual adaptation, privacy-preserving inference, and personalized medical AI systems.

</details>


### [29] [Using Contrastive Learning to Improve Two-Way Reasoning in Large Language Models: The Obfuscation Task as a Case Study](https://arxiv.org/abs/2509.05553)

*Serge Lionel Nikiema, Jordan Samhi, Micheline Bénédicte Moumoula, Albérick Euraste Djiré, Abdoul Kader Kaboré, Jacques Klein, Tegawendé F. Bissyandé*

**Main category:** cs.CL

**Keywords:** large language models, bidirectional reasoning, Contrastive Fine-Tuning

**Relevance Score:** 9

**TL;DR:** This research investigates if large language models understand concepts or just recognize patterns, introducing bidirectional reasoning as a key test to determine true comprehension. They develop Contrastive Fine-Tuning (CFT) to enhance model capabilities in reverse reasoning. Experiments show CFT effectively enables bidirectional reasoning while maintaining forward task performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To determine if large language models truly understand concepts or just recognize patterns, leading to insights for improving AI systems.

**Method:** The authors propose bidirectional reasoning as a test for understanding and develop Contrastive Fine-Tuning (CFT) that trains models with positive, negative, and obfuscation examples to enhance reverse reasoning capabilities without explicit training in reverse.

**Key Contributions:**

	1. Introduction of bidirectional reasoning as a measure of true understanding in AI
	2. Development of Contrastive Fine-Tuning (CFT) to improve reverse reasoning without explicit training
	3. Experimental validation showing enhanced reasoning capabilities in language models.

**Result:** CFT was successful in enabling models to perform bidirectional reasoning, improving reverse performance while maintaining capabilities for forward tasks.

**Limitations:** 

**Conclusion:** Bidirectional reasoning is presented as a theoretical framework to assess understanding and as a practical method for training more capable AI systems.

**Abstract:** This research addresses a fundamental question in AI: whether large language models truly understand concepts or simply recognize patterns. The authors propose bidirectional reasoning,the ability to apply transformations in both directions without being explicitly trained on the reverse direction, as a test for genuine understanding. They argue that true comprehension should naturally allow reversibility. For example, a model that can change a variable name like userIndex to i should also be able to infer that i represents a user index without reverse training. The researchers tested current language models and discovered what they term cognitive specialization: when models are fine-tuned on forward tasks, their performance on those tasks improves, but their ability to reason bidirectionally becomes significantly worse. To address this issue, they developed Contrastive Fine-Tuning (CFT), which trains models using three types of examples: positive examples that maintain semantic meaning, negative examples with different semantics, and forward-direction obfuscation examples. This approach aims to develop deeper understanding rather than surface-level pattern recognition and allows reverse capabilities to develop naturally without explicit reverse training. Their experiments demonstrated that CFT successfully achieved bidirectional reasoning, enabling strong reverse performance while maintaining forward task capabilities. The authors conclude that bidirectional reasoning serves both as a theoretical framework for assessing genuine understanding and as a practical training approach for developing more capable AI systems.

</details>


### [30] [Ad hoc conventions generalize to new referents](https://arxiv.org/abs/2509.05566)

*Anya Ji, Claire Augusta Bergey, Ron Eliav, Yoav Artzi, Robert D. Hawkins*

**Main category:** cs.CL

**Keywords:** shared naming, dyadic communication, generalization, semantic space, language agents

**Relevance Score:** 4

**TL;DR:** The study investigates how people create shared naming conventions for objects they have not previously discussed, using a communication study with tangram images to test generalization of these conventions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how new shared naming systems are formed and whether they merely link to specific targets or help reshape semantic spaces for broader generalization.

**Method:** A dyadic communication study involving 302 participants was conducted, using the KiloGram dataset of abstract tangram images to analyze how pairs of participants developed referential conventions through repeated communication.

**Key Contributions:**

	1. Demonstrated generalization of naming conventions beyond discussed items.
	2. Provided evidence against the view that naming is purely arbitrary.
	3. Offered insights into the implications for designing adaptive language agents.

**Result:** Participants showed strong evidence of generalization in their descriptions for undiscussed images, with alignment increasing compared to pre-test labels, and generalization decayed nonlinearly with visual similarity.

**Limitations:** 

**Conclusion:** The findings indicate that shared naming conventions are not just arbitrary labels, but involve genuine conceptual coordination, affecting theories of reference and the design of language agents.

**Abstract:** How do people talk about things they've never talked about before? One view suggests that a new shared naming system establishes an arbitrary link to a specific target, like proper names that cannot extend beyond their bearers. An alternative view proposes that forming a shared way of describing objects involves broader conceptual alignment, reshaping each individual's semantic space in ways that should generalize to new referents. We test these competing accounts in a dyadic communication study (N=302) leveraging the recently-released KiloGram dataset containing over 1,000 abstract tangram images. After pairs of participants coordinated on referential conventions for one set of images through repeated communication, we measured the extent to which their descriptions aligned for undiscussed images. We found strong evidence for generalization: partners showed increased alignment relative to their pre-test labels. Generalization also decayed nonlinearly with visual similarity (consistent with Shepard's law) and was robust across levels of the images' nameability. These findings suggest that ad hoc conventions are not arbitrary labels but reflect genuine conceptual coordination, with implications for theories of reference and the design of more adaptive language agents.

</details>


### [31] [Mitigating Spurious Correlations Between Question and Answer via Chain-of-Thought Correctness Perception Distillation](https://arxiv.org/abs/2509.05602)

*Hongyan Xie, Yitong Yao, Yikun Ban, Zixuan Huang, Deqing Wang, Zhenhe Wu, Haoxiang Su, Chao Wang, Shuangyong Song, Xuelong Li*

**Main category:** cs.CL

**Keywords:** language models, reasoning quality, small language models, Chain-of-Thought, data utilization

**Relevance Score:** 9

**TL;DR:** This paper proposes Chain-of-Thought Correctness Perception Distillation (CoPeD) to enhance reasoning quality in small language models (SLMs) by using a correctness-aware task setting and dynamically adjusting training loss.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the reasoning capabilities of small language models (SLMs) fine-tuned on potentially noisy Chain-of-Thought (CoT) data generated by large language models (LLMs), which can lead to spurious correlations and compromised reasoning quality.

**Method:** The paper introduces a correctness-aware task setting that prompts the student model to predict and revise answers based on rationales, paired with a Correctness-Aware Weighted loss that prioritizes training instances with more supportive rationales for improving reasoning quality.

**Key Contributions:**

	1. Introduction of a correctness-aware task setting for reasoning improvement
	2. Development of a Correctness-Aware Weighted loss to focus training on stronger rationale support
	3. Demonstration of effectiveness on various benchmark reasoning datasets.

**Result:** Experiments demonstrate that CoPeD significantly enhances reasoning performance on both in-distribution and out-of-distribution benchmark datasets for reasoning tasks.

**Limitations:** 

**Conclusion:** CoPeD effectively boosts the faithfulness of the reasoning process in SLMs by emphasizing accurate rationales, leading to improved answer predictions and a better learning experience from errors.

**Abstract:** Large language models (LLMs) excel at reasoning tasks but are expensive to deploy. Thus small language models (SLMs) are fine-tuned on CoT data generated by LLMs to copy LLMs' abilities. However, these CoT data may include noisy rationales that either fail to substantiate the answers or contribute no additional information to support answer prediction, which leads SLMs to capture spurious correlations between questions and answers and compromise the quality of reasoning. In this work, we propose Chain-of-Thought Correctness Perception Distillation (CoPeD), which aims to improve the reasoning quality of the student model from the perspectives of task setting and data utilization. Firstly, we introduce a correctness-aware task setting that encourages the student model to predict answers based on correct rationales and revise them when they are incorrect. This setting improves the faithfulness of reasoning and allows the model to learn from its mistakes. Then, we propose a Correctness-Aware Weighted loss, which dynamically adjusts the contribution of each training instance based on the combined loss of the rationale and the answer. This strategy encourages the model to focus more on samples where the rationale offers stronger support for the correct answer. Experiments have shown that CoPeD is effective on both in-distribution (IND) and out-of-distribution (OOD) benchmark reasoning datasets.

</details>


### [32] [Icon$^{2}$: Aligning Large Language Models Using Self-Synthetic Preference Data via Inherent Regulation](https://arxiv.org/abs/2509.05605)

*Qiyuan Chen, Hongsen Huang, Qian Shao, Jiahe Chen, Jintai Chen, Hongxia Xu, Renjie Hua, Ren Chuan, Jian Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Preference Datasets, Human Preferences

**Relevance Score:** 9

**TL;DR:** This paper presents Icon$^{2}$, a novel approach to construct high-quality preference datasets for LLMs by leveraging their representation space, resulting in improved alignment and reduced computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in constructing preference datasets for large language models that rely on human preferences effectively and efficiently.

**Method:** Icon$^{2}$ method extracts layer-wise direction vectors to encode human preferences and filters self-synthesized instructions based on consistency, applying inherent control during decoding.

**Key Contributions:**

	1. Introduction of Icon$^{2}$ for preference dataset construction
	2. Layer-wise direction vectors for encoding human preferences
	3. Improved efficiency with reduced computational costs

**Result:** The experiments show a 13.89% improvement on AlpacaEval 2.0 and a 13.45% improvement on Arena-Hard, along with a reduction in computational costs by up to 48.1%.

**Limitations:** 

**Conclusion:** The proposed method represents a significant enhancement in constructing preference datasets for LLMs, achieving both better alignment with human preferences and efficiency.

**Abstract:** Large Language Models (LLMs) require high quality preference datasets to align with human preferences. However, conventional methods for constructing such datasets face significant challenges: reliance on pre-collected instructions often leads to distribution mismatches with target models, while the need for sampling multiple stochastic responses introduces substantial computational overhead. In this work, we explore a paradigm shift by leveraging inherent regulation of LLMs' representation space for efficient and tailored preference dataset construction, named Icon$^{2}$. Specifically, it first extracts layer-wise direction vectors to encode sophisticated human preferences and then uses these vectors to filter self-synthesized instructions based on their inherent consistency. During decoding, bidirectional inherent control is applied to steer token representations, enabling the precise generation of response pairs with clear alignment distinctions. Experimental results demonstrate significant improvements in both alignment and efficiency. Llama3-8B and Qwen2-7B achieve an average win rate improvement of 13.89% on AlpacaEval 2.0 and 13.45% on Arena-Hard, while reducing computational costs by up to 48.1%.

</details>


### [33] [Beyond Keywords: Driving Generative Search Engine Optimization with Content-Centric Agents](https://arxiv.org/abs/2509.05607)

*Qiyuan Chen, Jiahe Chen, Hongsen Huang, Qian Shao, Jintai Chen, Renjie Hua, Hongxia Xu, Ruijia Wu, Ren Chuan, Jian Wu*

**Main category:** cs.CL

**Keywords:** Generative Search, Search Engine Optimization, Content Influence, Multi-Agent Systems, Benchmarking

**Relevance Score:** 7

**TL;DR:** This paper presents a framework for Generative Search Engine Optimization (GSEO) to measure and optimize content influence on search engines.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The shift to Generative Search Engines has made traditional SEO metrics irrelevant, necessitating new methods to understand and enhance content impact on synthesized search results.

**Method:** The paper introduces a large-scale benchmark, CC-GSEO-Bench, and a multi-dimensional evaluation framework to quantify content influence. Additionally, it develops a multi-agent system for automating content refinement through a workflow of analysis, revision, and evaluation.

**Key Contributions:**

	1. Development of CC-GSEO-Bench as a content-centric benchmark
	2. Introduction of a multi-dimensional evaluation framework for content influence
	3. Creation of a multi-agent system for automated content refinement

**Result:** Empirical analysis using the proposed framework demonstrated new insights into content influence dynamics and provided actionable strategies for content creators.

**Limitations:** 

**Conclusion:** The research lays a groundwork for improved GSEO practices and future studies in the area, emphasizing the importance of a deeper understanding of content impact.

**Abstract:** The paradigm shift from traditional ranked-based search to Generative Search Engines has rendered conventional SEO metrics obsolete, creating an urgent need to understand, measure, and optimize for content influence on synthesized answers. This paper introduces a comprehensive, end-to-end framework for Generative Search Engine Optimization (GSEO) to address this challenge. We make two primary contributions. First, we construct CC-GSEO-Bench, a large-scale, content-centric benchmark, and propose a multi-dimensional evaluation framework that systematically quantifies influence, moving beyond surface-level attribution to assess substantive semantic impact. Second, we design a novel multi-agent system that operationalizes this framework, automating the strategic refinement of content through a collaborative analyze-revise-evaluate workflow. Our empirical analysis using this framework reveals novel insights into the dynamics of content influence, offering actionable strategies for creators and establishing a principled foundation for future GSEO research.

</details>


### [34] [New Insights into Optimal Alignment of Acoustic and Linguistic Representations for Knowledge Transfer in ASR](https://arxiv.org/abs/2509.05609)

*Xugang Lu, Peng Shen, Yu Tsao, Hisashi Kawai*

**Main category:** cs.CL

**Keywords:** automatic speech recognition, acoustic-linguistic alignment, optimal transport, knowledge transfer, machine learning

**Relevance Score:** 6

**TL;DR:** The paper presents an unbalanced optimal transport-based model for aligning acoustic and linguistic representations in automatic speech recognition, addressing many-to-one and one-to-many relationships while managing noise and imbalances.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need to bridge pre-trained models in knowledge transfer for ASR by accurately aligning acoustic and linguistic representations, resolving the inherent structural asymmetries.

**Method:** An unbalanced optimal transport approach that facilitates soft and partial matching between acoustic frames and linguistic tokens.

**Key Contributions:**

	1. Introduction of an unbalanced optimal transport model for alignment in ASR
	2. Addressing structural asymmetries in acoustic-linguistic relationships
	3. Demonstration of improved ASR performance through flexible matching methodology.

**Result:** The model ensures every linguistic token is linked to acoustic observations, improving ASR performance by allowing flexible mapping and handling redundant or noisy frames.

**Limitations:** 

**Conclusion:** The proposed model effectively enhances ASR performance by controlling the matching process between acoustic and linguistic modalities, addressing key challenges in knowledge transfer.

**Abstract:** Aligning acoustic and linguistic representations is a central challenge to bridge the pre-trained models in knowledge transfer for automatic speech recognition (ASR). This alignment is inherently structured and asymmetric: while multiple consecutive acoustic frames typically correspond to a single linguistic token (many-to-one), certain acoustic transition regions may relate to multiple adjacent tokens (one-to-many). Moreover, acoustic sequences often include frames with no linguistic counterpart, such as background noise or silence may lead to imbalanced matching conditions. In this work, we take a new insight to regard alignment and matching as a detection problem, where the goal is to identify meaningful correspondences with high precision and recall ensuring full coverage of linguistic tokens while flexibly handling redundant or noisy acoustic frames in transferring linguistic knowledge for ASR. Based on this new insight, we propose an unbalanced optimal transport-based alignment model that explicitly handles distributional mismatch and structural asymmetries with soft and partial matching between acoustic and linguistic modalities. Our method ensures that every linguistic token is grounded in at least one acoustic observation, while allowing for flexible, probabilistic mappings from acoustic to linguistic units. We evaluate our proposed model with experiments on an CTC-based ASR system with a pre-trained language model for knowledge transfer. Experimental results demonstrate the effectiveness of our approach in flexibly controlling degree of matching and hence to improve ASR performance.

</details>


### [35] [From Joy to Fear: A Benchmark of Emotion Estimation in Pop Song Lyrics](https://arxiv.org/abs/2509.05617)

*Shay Dahary, Avi Edana, Alexander Apartsin, Yehudit Aperstein*

**Main category:** cs.CL

**Keywords:** emotional attribution, song lyrics, large language models, BERT, music information retrieval

**Relevance Score:** 6

**TL;DR:** This paper explores the multi-label emotional attribution of song lyrics using a novel dataset and various LLMs.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how emotional content in song lyrics affects listener experiences and preferences.

**Method:** A manually labeled dataset is created using a mean opinion score approach. Several LLMs are evaluated in zero-shot scenarios, and a BERT-based model is fine-tuned for multi-label emotion score prediction.

**Key Contributions:**

	1. Creation of a comprehensive dataset for emotional attribution of song lyrics.
	2. Evaluation of multiple LLMs for emotion recognition in lyrics.
	3. Insights into model selection for music information retrieval applications.

**Result:** The evaluation reveals strengths and limitations of both zero-shot and fine-tuned models in emotion recognition.

**Limitations:** The focus is primarily on lyrics, and generalizability to other text forms may be limited.

**Conclusion:** The study demonstrates the potential of LLMs in capturing emotional nuances in lyrics, informing model selection for emotion-based music information retrieval.

**Abstract:** The emotional content of song lyrics plays a pivotal role in shaping listener experiences and influencing musical preferences. This paper investigates the task of multi-label emotional attribution of song lyrics by predicting six emotional intensity scores corresponding to six fundamental emotions. A manually labeled dataset is constructed using a mean opinion score (MOS) approach, which aggregates annotations from multiple human raters to ensure reliable ground-truth labels. Leveraging this dataset, we conduct a comprehensive evaluation of several publicly available large language models (LLMs) under zero-shot scenarios. Additionally, we fine-tune a BERT-based model specifically for predicting multi-label emotion scores. Experimental results reveal the relative strengths and limitations of zero-shot and fine-tuned models in capturing the nuanced emotional content of lyrics. Our findings highlight the potential of LLMs for emotion recognition in creative texts, providing insights into model selection strategies for emotion-based music information retrieval applications. The labeled dataset is available at https://github.com/LLM-HITCS25S/LyricsEmotionAttribution.

</details>


### [36] [Few-Shot Query Intent Detection via Relation-Aware Prompt Learning](https://arxiv.org/abs/2509.05635)

*Liang Zhang, Yuan Li, Shijie Zhang, Zheng Zhang, Xitong Li*

**Main category:** cs.CL

**Keywords:** Intent Detection, Conversational Systems, Relational Information, Language Models, Few-Shot Learning

**Relevance Score:** 8

**TL;DR:** SAID is a framework that improves intent detection in conversational systems by integrating textual and relational structure information for model pretraining.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance intent detection in conversational systems by effectively capturing structural information such as query-query and query-answer relations, which are often neglected in current methods.

**Method:** The proposed framework SAID integrates both textual and relational structure information during model pretraining and introduces the QueryAdapt mechanism to generate intent-specific relation tokens.

**Key Contributions:**

	1. Introduction of SAID framework for integrating textual and relational structure information in intent detection.
	2. Development of QueryAdapt mechanism for generating intent-specific relation tokens.
	3. Demonstration of superior performance over existing methods in real-world datasets.

**Result:** SAID outperforms state-of-the-art methods in intent detection on two real-world datasets, demonstrating the effectiveness of integrating structural information.

**Limitations:** 

**Conclusion:** The integration of relational structures with textual data significantly enhances the performance of intent detection in conversational systems.

**Abstract:** Intent detection is a crucial component of modern conversational systems, since accurately identifying user intent at the beginning of a conversation is essential for generating effective responses. Recent efforts have focused on studying this problem under a challenging few-shot scenario. These approaches primarily leverage large-scale unlabeled dialogue text corpora to pretrain language models through various pretext tasks, followed by fine-tuning for intent detection with very limited annotations. Despite the improvements achieved, existing methods have predominantly focused on textual data, neglecting to effectively capture the crucial structural information inherent in conversational systems, such as the query-query relation and query-answer relation. To address this gap, we propose SAID, a novel framework that integrates both textual and relational structure information in a unified manner for model pretraining for the first time. Building on this framework, we further propose a novel mechanism, the query-adaptive attention network (QueryAdapt), which operates at the relation token level by generating intent-specific relation tokens from well-learned query-query and query-answer relations explicitly, enabling more fine-grained knowledge transfer. Extensive experimental results on two real-world datasets demonstrate that SAID significantly outperforms state-of-the-art methods.

</details>


### [37] [LM-Searcher: Cross-domain Neural Architecture Search with LLMs via Unified Numerical Encoding](https://arxiv.org/abs/2509.05657)

*Yuxuan Hu, Jihao Liu, Ke Wang, Jinliang Zhen, Weikang Shi, Manyuan Zhang, Qi Dou, Rui Liu, Aojun Zhou, Hongsheng Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, Neural Architecture Search, cross-domain optimization, NCode, ranking task

**Relevance Score:** 8

**TL;DR:** The paper introduces LM-Searcher, a framework leveraging Large Language Models for cross-domain Neural Architecture Search (NAS) without extensive domain-specific tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current LLM-driven NAS approaches that depend on prompt engineering and domain-specific adaptations.

**Method:** The authors propose a novel numerical string representation (NCode) for neural architectures and reformulate the NAS problem as a ranking task, using a dataset from a pruning-based subspace sampling strategy to train LLMs for architecture selection.

**Key Contributions:**

	1. Introduction of LM-Searcher framework for NAS using LLMs.
	2. Development of NCode for universal architecture representation.
	3. Reformulation of NAS as a ranking task for improved architecture selection.

**Result:** LM-Searcher demonstrates competitive performance for both in-domain (CNNs for image classification) and out-of-domain tasks (LoRA configurations for segmentation and generation).

**Limitations:** The method relies on the quality of the curated dataset for effective training and performance evaluation.

**Conclusion:** The proposed approach establishes a new paradigm for flexible and generalizable LLM-based architecture search, with datasets and models available for further research.

**Abstract:** Recent progress in Large Language Models (LLMs) has opened new avenues for solving complex optimization problems, including Neural Architecture Search (NAS). However, existing LLM-driven NAS approaches rely heavily on prompt engineering and domain-specific tuning, limiting their practicality and scalability across diverse tasks. In this work, we propose LM-Searcher, a novel framework that leverages LLMs for cross-domain neural architecture optimization without the need for extensive domain-specific adaptation. Central to our approach is NCode, a universal numerical string representation for neural architectures, which enables cross-domain architecture encoding and search. We also reformulate the NAS problem as a ranking task, training LLMs to select high-performing architectures from candidate pools using instruction-tuning samples derived from a novel pruning-based subspace sampling strategy. Our curated dataset, encompassing a wide range of architecture-performance pairs, encourages robust and transferable learning. Comprehensive experiments demonstrate that LM-Searcher achieves competitive performance in both in-domain (e.g., CNNs for image classification) and out-of-domain (e.g., LoRA configurations for segmentation and generation) tasks, establishing a new paradigm for flexible and generalizable LLM-based architecture search. The datasets and models will be released at https://github.com/Ashone3/LM-Searcher.

</details>


### [38] [Cross-Question Method Reuse in Large Language Models: From Word-Level Prediction to Rational Logical-Layer Reasoning](https://arxiv.org/abs/2509.05660)

*Hong Su*

**Main category:** cs.CL

**Keywords:** large language models, method reuse, cross-question adaptation, HCI, NLP

**Relevance Score:** 9

**TL;DR:** This paper proposes a new approach for method reuse in large language models by allowing solutions to be adapted to questions with low or hidden similarities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance method reuse in large language models for questions that aren't highly similar or share hidden similarities.

**Method:** The proposed method separates questions and solutions, guiding the LLM to adjust solutions for related questions rather than relying on direct question similarity.

**Key Contributions:**

	1. A novel separation of questions and solutions to aid method reuse
	2. Extension of method reuse to low-similarity and partially similar questions
	3. Experimental evidence supporting improved effectiveness in cross-question reuse

**Result:** The new approach has been experimentally verified to increase the likelihood of identifying reusable solutions, improving cross-question method reuse effectiveness.

**Limitations:** The method may not fully address extremely dissimilar questions or complex interactions between question features.

**Conclusion:** The scope-extension method allows for better adaptation of solutions to questions with varying degrees of similarity, addressing limitations of conventional approaches.

**Abstract:** Large language models (LLMs) have been widely applied to assist in finding solutions for diverse questions. Prior work has proposed representing a method as a pair of a question and its corresponding solution, enabling method reuse. However, existing approaches typically require the questions to be highly similar. In this paper, we extend the scope of method reuse to address questions with low similarity or with hidden similarities that are not explicitly observable. For questions that are similar in a general-specific sense (i.e., broader or narrower in scope), we propose to first separate the question and solution, rather than directly feeding the pair to the LLM. The LLM is then guided to adapt the solution to new but related questions, allowing it to focus on solution transfer rather than question recognition. Furthermore, we extend this approach to cases where questions only share partial features or hidden characteristics. This enables cross-question method reuse beyond conventional similarity constraints. Experimental verification shows that our scope-extension approach increases the probability of filtering out reusable solutions, thereby improving the effectiveness of cross-question method reuse.

</details>


### [39] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)

*Jinrui Yang, Xudong Han, Timothy Baldwin*

**Main category:** cs.CL

**Keywords:** large language models, political bias, gender classification, vote prediction, fairness in NLP

**Relevance Score:** 8

**TL;DR:** EuroParlVote benchmark evaluates LLMs in political contexts, revealing biases in gender classification and vote prediction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a benchmark that assesses large language models in politically sensitive situations and to investigate biases in their predictions related to gender and political orientation.

**Method:** A novel dataset linking European Parliament debate speeches with roll-call vote outcomes and demographic metadata for MEPs is used to evaluate LLMs on tasks of gender classification and vote prediction.

**Key Contributions:**

	1. Introduction of the EuroParlVote dataset and benchmark
	2. Evaluation of state-of-the-art LLMs in political contexts
	3. Identification of biases in gender classification and vote prediction for LLMs.

**Result:** The evaluation shows LLMs misclassifying female MEPs, with reduced accuracy in vote prediction for female speakers, and a tendency to favor centrist political groups.

**Limitations:** The focus is primarily on European Parliament data, which may not generalize to other political contexts.

**Conclusion:** The findings highlight biases in LLMs, particularly in political contexts, and the release of the dataset aims to assist future research on fairness in NLP.

**Abstract:** We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.

</details>


### [40] [Llama-GENBA-10B: A Trilingual Large Language Model for German, English and Bavarian](https://arxiv.org/abs/2509.05668)

*Michael Hoffmann, Jophin John, Stefan Schweter, Gokul Ramakrishnan, Hoi-Fong Mak, Alice Zhang, Dmitry Gaynullin, Nicolay J. Hammer*

**Main category:** cs.CL

**Keywords:** Llama-GENBA-10B, multilingual models, low-resource languages

**Relevance Score:** 6

**TL;DR:** Llama-GENBA-10B is a trilingual foundation model designed to mitigate English-centric bias in large language models, featuring balanced pretraining across English, German, and Bavarian languages with strong cross-lingual performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address English-centric bias in large language models and promote the use of low-resource languages like Bavarian.

**Method:** Llama-GENBA-10B was pretrained on a balanced dataset of 164B tokens, optimizing architecture and hyperparameters for cross-lingual transfer and creating a standardized evaluation suite.

**Key Contributions:**

	1. Development of a trilingual foundation model with balanced resources across languages
	2. Establishment of a standardized evaluation suite for trilingual benchmarks
	3. Demonstration of energy-efficient large-scale multilingual pretraining

**Result:** The model shows strong performance, outperforming previous models in Bavarian and matching benchmarks in German while also excelling in English.

**Limitations:** 

**Conclusion:** Llama-GENBA-10B sets a precedent for training inclusive foundation models that effectively integrate low-resource languages without favoring English.

**Abstract:** We present Llama-GENBA-10B, a trilingual foundation model addressing English-centric bias in large language models. Built on Llama 3.1-8B and scaled to 10B parameters, Llama-GENBA-10B is continuously pretrained on 164B tokens (82B English, 82B German, and 80M Bavarian), balancing resources while preventing English dominance. Targeted at the German NLP community, the model also promotes Bavarian as a low-resource language. Development tackled four challenges: (1) curating a multilingual corpus despite Bavarian scarcity, (2) creating a unified tokenizer for English, German, and Bavarian, (3) optimizing architecture and language-ratio hyperparameters for cross-lingual transfer, and (4) establishing the first standardized trilingual evaluation suite by translating German benchmarks into Bavarian. Evaluations show that Llama-GENBA-10B achieves strong cross-lingual performance, with the fine-tuned variant surpassing Apertus-8B-2509 and gemma-2-9b in Bavarian and establishing itself as the best model in its class for this language, while also outperforming EuroLLM in English and matching its results in German. Training on the Cerebras CS-2 demonstrated efficient large-scale multilingual pretraining with documented energy use, offering a blueprint for inclusive foundation models that integrate low-resource languages.

</details>


### [41] [Revealing the Numeracy Gap: An Empirical Investigation of Text Embedding Models](https://arxiv.org/abs/2509.05691)

*Ningyuan Deng, Hanyu Duan, Yixuan Tang, Yi Yang*

**Main category:** cs.CL

**Keywords:** text embedding models, numerical information, natural language processing, finance, healthcare

**Relevance Score:** 7

**TL;DR:** This study examines the ability of text embedding models to accurately capture numerical details in natural language processing applications, particularly in finance and healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate whether current text embedding models can effectively encode nuanced numerical information, which is critical for applications in fields such as finance and healthcare where numbers matter.

**Method:** The study evaluates 13 widely used text embedding models using synthetic data in a financial context to assess their performance in capturing numerical details.

**Key Contributions:**

	1. Evaluation of 13 text embedding models in terms of their numerical encoding capabilities.
	2. Insights into the limitations of current text embedding models regarding numerical details.
	3. Recommendations for future research to enhance numerical understanding in embedding models.

**Result:** The analysis reveals that the majority of text embedding models struggle to accurately capture nuanced numerical content, indicating a significant limitation in their applicability for domains with critical numerical information.

**Limitations:** The study is based on synthetic data, and real-world performance may vary.

**Conclusion:** The findings highlight the need for improved methodologies in training embedding models to enhance their ability to handle numerical information effectively, which could benefit various NLP applications.

**Abstract:** Text embedding models are widely used in natural language processing applications. However, their capability is often benchmarked on tasks that do not require understanding nuanced numerical information in text. As a result, it remains unclear whether current embedding models can precisely encode numerical content, such as numbers, into embeddings. This question is critical because embedding models are increasingly applied in domains where numbers matter, such as finance and healthcare. For example, Company X's market share grew by 2\% should be interpreted very differently from Company X's market share grew by 20\%, even though both indicate growth in market share. This study aims to examine whether text embedding models can capture such nuances. Using synthetic data in a financial context, we evaluate 13 widely used text embedding models and find that they generally struggle to capture numerical details accurately. Our further analyses provide deeper insights into embedding numeracy, informing future research to strengthen embedding model-based NLP systems with improved capacity for handling numerical content.

</details>


### [42] [A Survey of the State-of-the-Art in Conversational Question Answering Systems](https://arxiv.org/abs/2509.05716)

*Manoj Madushanka Perera, Adnan Mahmood, Kasun Eranda Wijethilake, Fahmida Islam, Maryam Tahermazandarani, Quan Z. Sheng*

**Main category:** cs.CL

**Keywords:** Conversational Question Answering, Natural Language Processing, Large Language Models

**Relevance Score:** 9

**TL;DR:** This survey examines Conversational Question Answering (ConvQA) systems, analyzing their components, methods, and the impact of large language models.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive overview of state-of-the-art ConvQA systems and their applications across various domains, particularly in maintaining coherence and relevance in conversations.

**Method:** The paper reviews the core components of ConvQA systems, discusses advanced machine learning techniques, and explores the role of large language models. It also analyzes key datasets related to ConvQA.

**Key Contributions:**

	1. Comprehensive analysis of ConvQA components and techniques
	2. Examination of the role of large language models in ConvQA
	3. Identification of key datasets for ConvQA research

**Result:** An overview of how ConvQA systems utilize various machine learning methods to enhance conversation quality, along with a detailed analysis of ConvQA datasets.

**Limitations:** 

**Conclusion:** The survey outlines significant advancements in ConvQA and suggests open research directions for future improvement in the field.

**Abstract:** Conversational Question Answering (ConvQA) systems have emerged as a pivotal area within Natural Language Processing (NLP) by driving advancements that enable machines to engage in dynamic and context-aware conversations. These capabilities are increasingly being applied across various domains, i.e., customer support, education, legal, and healthcare where maintaining a coherent and relevant conversation is essential. Building on recent advancements, this survey provides a comprehensive analysis of the state-of-the-art in ConvQA. This survey begins by examining the core components of ConvQA systems, i.e., history selection, question understanding, and answer prediction, highlighting their interplay in ensuring coherence and relevance in multi-turn conversations. It further investigates the use of advanced machine learning techniques, including but not limited to, reinforcement learning, contrastive learning, and transfer learning to improve ConvQA accuracy and efficiency. The pivotal role of large language models, i.e., RoBERTa, GPT-4, Gemini 2.0 Flash, Mistral 7B, and LLaMA 3, is also explored, thereby showcasing their impact through data scalability and architectural advancements. Additionally, this survey presents a comprehensive analysis of key ConvQA datasets and concludes by outlining open research directions. Overall, this work offers a comprehensive overview of the ConvQA landscape and provides valuable insights to guide future advancements in the field.

</details>


### [43] [Exploring Subjective Tasks in Farsi: A Survey Analysis and Evaluation of Language Models](https://arxiv.org/abs/2509.05719)

*Donya Rooein, Flor Miriam Plaza-del-Arco, Debora Nozza, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** Farsi, NLP, Sentiment Analysis, Emotion Analysis, Toxicity Detection

**Relevance Score:** 4

**TL;DR:** This paper examines the state of subjective NLP tasks in Farsi, highlighting challenges in data availability and quality despite increasing overall digital content.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the challenges and limitations in NLP for the Farsi language, particularly in subjective tasks.

**Method:** Review of 110 publications on sentiment analysis, emotion analysis, and toxicity detection in Farsi, focusing on data availability and model performance.

**Key Contributions:**

	1. Identified key challenges in subjective task NLP for Farsi.
	2. Reviewed extensive literature on the topic and highlighted the lack of datasets.
	3. Showed instability in model performance with existing Farsi datasets.

**Result:** Significant challenges in data quality and availability were found, with existing datasets lacking essential demographic factors, leading to unstable model predictions.

**Limitations:** The existing datasets are limited in both quantity and quality, often omitting crucial demographic information.

**Conclusion:** The volume of available data is insufficient to enhance the performance of Farsi in NLP tasks significantly.

**Abstract:** Given Farsi's speaker base of over 127 million people and the growing availability of digital text, including more than 1.3 million articles on Wikipedia, it is considered a middle-resource language. However, this label quickly crumbles when the situation is examined more closely. We focus on three subjective tasks (Sentiment Analysis, Emotion Analysis, and Toxicity Detection) and find significant challenges in data availability and quality, despite the overall increase in data availability. We review 110 publications on subjective tasks in Farsi and observe a lack of publicly available datasets. Furthermore, existing datasets often lack essential demographic factors, such as age and gender, that are crucial for accurately modeling subjectivity in language. When evaluating prediction models using the few available datasets, the results are highly unstable across both datasets and models. Our findings indicate that the volume of data is insufficient to significantly improve a language's prospects in NLP.

</details>


### [44] [QCSE: A Pretrained Quantum Context-Sensitive Word Embedding for Natural Language Processing](https://arxiv.org/abs/2509.05729)

*Charles M. Varmantchaonala, Niclas GÖtting, Nils-Erik SchÜtte, Jean Louis E. K. Fendji, Christopher Gies*

**Main category:** cs.CL

**Keywords:** Quantum Natural Language Processing, context-sensitive embeddings, quantum computing

**Relevance Score:** 4

**TL;DR:** This paper introduces QCSE, a pretrained quantum context-sensitive embedding model that utilizes quantum computation to enhance natural language processing by capturing contextual relationships in languages.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how quantum computation can improve the encoding and understanding of natural languages, especially in low-resource languages.

**Method:** The paper presents five distinct methods for computing context matrices, utilizing quantum techniques like exponential decay, sinusoidal modulation, phase shifts, and hash-based transformations to create context-sensitive word embeddings.

**Key Contributions:**

	1. Introduction of QCSE, a quantum context-sensitive embedding model
	2. Development of innovative context matrix computation methods
	3. Demonstration of the efficacy of quantum embeddings in low-resource languages

**Result:** The QCSE model effectively captures context sensitivity and demonstrates the potential of quantum systems to enhance language representation, validated through evaluations on Fulani and English corpora.

**Limitations:** 

**Conclusion:** Quantum computation provides new techniques for context-aware representations in NLP, addressing challenges in language processing, particularly for underrepresented languages.

**Abstract:** Quantum Natural Language Processing (QNLP) offers a novel approach to encoding and understanding the complexity of natural languages through the power of quantum computation. This paper presents a pretrained quantum context-sensitive embedding model, called QCSE, that captures context-sensitive word embeddings, leveraging the unique properties of quantum systems to learn contextual relationships in languages. The model introduces quantum-native context learning, enabling the utilization of quantum computers for linguistic tasks. Central to the proposed approach are innovative context matrix computation methods, designed to create unique, representations of words based on their surrounding linguistic context. Five distinct methods are proposed and tested for computing the context matrices, incorporating techniques such as exponential decay, sinusoidal modulation, phase shifts, and hash-based transformations. These methods ensure that the quantum embeddings retain context sensitivity, thereby making them suitable for downstream language tasks where the expressibility and properties of quantum systems are valuable resources. To evaluate the effectiveness of the model and the associated context matrix methods, evaluations are conducted on both a Fulani corpus, a low-resource African language, dataset of small size and an English corpus of slightly larger size. The results demonstrate that QCSE not only captures context sensitivity but also leverages the expressibility of quantum systems for representing rich, context-aware language information. The use of Fulani further highlights the potential of QNLP to mitigate the problem of lack of data for this category of languages. This work underscores the power of quantum computation in natural language processing (NLP) and opens new avenues for applying QNLP to real-world linguistic challenges across various tasks and domains.

</details>


### [45] [Enhancing Factual Accuracy and Citation Generation in LLMs via Multi-Stage Self-Verification](https://arxiv.org/abs/2509.05741)

*Fernando Gabriela García, Qiyang Shi, Zilin Feng*

**Main category:** cs.CL

**Keywords:** VeriFact-CoT, Large Language Models, fact verification, citation integration, hallucination

**Relevance Score:** 8

**TL;DR:** Introduces VeriFact-CoT, a method to improve the accuracy and reliability of LLMs by integrating fact verification and citation processes.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle hallucination issues and lack of credible sources in LLMs for generating fact-sensitive content.

**Method:** A multi-stage mechanism involving 'fact verification-reflection-citation integration' enables self-examination and revision of LLM outputs.

**Key Contributions:**

	1. Develops a method for fact verification in LLMs
	2. Integrates citation processes to improve output reliability
	3. Enhances trustworthiness of LLMs for fact-sensitive applications

**Result:** The method improves the objective accuracy, trustworthiness, and traceability of LLM-generated content, making it reliable for critical applications.

**Limitations:** 

**Conclusion:** VeriFact-CoT significantly enhances the reliability of LLMs in high-fidelity applications.

**Abstract:** This research introduces VeriFact-CoT (Verified Factual Chain-of-Thought), a novel method designed to address the pervasive issues of hallucination and the absence of credible citation sources in Large Language Models (LLMs) when generating complex, fact-sensitive content. By incorporating a multi-stage mechanism of 'fact verification-reflection-citation integration,' VeriFact-CoT empowers LLMs to critically self-examine and revise their intermediate reasoning steps and final answers. This process significantly enhances the objective accuracy, trustworthiness, and traceability of the generated outputs, making LLMs more reliable for applications demanding high fidelity such as scientific research, news reporting, and legal consultation.

</details>


### [46] [LatinX: Aligning a Multilingual TTS Model with Direct Preference Optimization](https://arxiv.org/abs/2509.05863)

*Luis Felipe Chary, Miguel Arjona Ramirez*

**Main category:** cs.CL

**Keywords:** multilingual TTS, speech-to-speech translation, voice cloning

**Relevance Score:** 5

**TL;DR:** LatinX is a multilingual TTS model that translates speech while maintaining speaker identity across languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve speech-to-speech translation by maintaining the original speaker's identity and enhancing performance metrics like WER and speaker similarity.

**Method:** LatinX is trained in three stages: pre-training for text-to-audio mapping, supervised fine-tuning for zero-shot voice cloning, and alignment with Direct Preference Optimization using labeled pairs based on WER and similarity metrics.

**Key Contributions:**

	1. Introduction of LatinX for multilingual TTS that maintains speaker identity.
	2. Three-stage training process enhancing performance metrics.
	3. Cross-lingual analyses indicating stronger subjective speaker similarity.

**Result:** LatinX reduces WER and improves speaker similarity scores compared to the baseline, as verified by human evaluations.

**Limitations:** Gaps between objective measures (WER) and subjective perceived similarity metrics need further exploration.

**Conclusion:** LatinX demonstrates significant improvements in preserving speaker identity across languages and shows promise for future developments in TTS models.

**Abstract:** We present LatinX, a multilingual text-to-speech (TTS) model for cascaded speech-to-speech translation that preserves the source speaker's identity across languages. LatinX is a 12-layer decoder-only Transformer trained in three stages: (i) pre-training for text-to-audio mapping, (ii) supervised fine-tuning for zero-shot voice cloning, and (iii) alignment with Direct Preference Optimization (DPO) using automatically labeled pairs based on Word Error Rate (WER) and speaker-similarity metrics. Trained on English and Romance languages with emphasis on Portuguese, LatinX with DPO consistently reduces WER and improves objective similarity over the fine-tuned baseline. Human evaluations further indicate stronger perceived speaker similarity than a strong baseline (XTTSv2), revealing gaps between objective and subjective measures. We provide cross-lingual analyses and discuss balanced preference signals and lower-latency architectures as future work.

</details>


### [47] [ZhiFangDanTai: Fine-tuning Graph-based Retrieval-Augmented Generation Model for Traditional Chinese Medicine Formula](https://arxiv.org/abs/2509.05867)

*ZiXuan Zhang, Bowen Hao, Yingjie Li, Hongzhi Yin*

**Main category:** cs.CL

**Keywords:** Traditional Chinese Medicine, Graph-based Retrieval, Large Language Models, explainable AI, health informatics

**Relevance Score:** 4

**TL;DR:** ZhiFangDanTai is a framework utilizing GraphRAG and LLM fine-tuning to enhance explainable formula generation in Traditional Chinese Medicine, addressing limitations in existing datasets.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing TCM models lack comprehensive outputs including detailed formula compositions and explanations, and datasets with insufficient details hinder model effectiveness.

**Method:** The framework combines Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning to retrieve TCM knowledge and improve model outputs.

**Key Contributions:**

	1. Introduction of ZhiFangDanTai framework combining GraphRAG and LLM fine-tuning
	2. Theoretical proofs showing reduced generalization error and hallucination rates
	3. Open-source implementation available for community use.

**Result:** ZhiFangDanTai shows significant improvements in reducing generalization error and hallucination rates when tested on both collected and clinical datasets.

**Limitations:** Existing datasets lack complete details regarding TCM formulas that may impact the model's capability.

**Conclusion:** The proposed method provides a more effective approach to generate explainable TCM formulas, contributing to better applications in health informatics.

**Abstract:** Traditional Chinese Medicine (TCM) formulas play a significant role in treating epidemics and complex diseases. Existing models for TCM utilize traditional algorithms or deep learning techniques to analyze formula relationships, yet lack comprehensive results, such as complete formula compositions and detailed explanations. Although recent efforts have used TCM instruction datasets to fine-tune Large Language Models (LLMs) for explainable formula generation, existing datasets lack sufficient details, such as the roles of the formula's sovereign, minister, assistant, courier; efficacy; contraindications; tongue and pulse diagnosis-limiting the depth of model outputs. To address these challenges, we propose ZhiFangDanTai, a framework combining Graph-based Retrieval-Augmented Generation (GraphRAG) with LLM fine-tuning. ZhiFangDanTai uses GraphRAG to retrieve and synthesize structured TCM knowledge into concise summaries, while also constructing an enhanced instruction dataset to improve LLMs' ability to integrate retrieved information. Furthermore, we provide novel theoretical proofs demonstrating that integrating GraphRAG with fine-tuning techniques can reduce generalization error and hallucination rates in the TCM formula task. Experimental results on both collected and clinical datasets demonstrate that ZhiFangDanTai achieves significant improvements over state-of-the-art models. Our model is open-sourced at https://huggingface.co/tczzx6/ZhiFangDanTai1.0.

</details>


### [48] [MedFactEval and MedAgentBrief: A Framework and Workflow for Generating and Evaluating Factual Clinical Summaries](https://arxiv.org/abs/2509.05878)

*François Grolleau, Emily Alsentzer, Timothy Keyes, Philip Chung, Akshay Swaminathan, Asad Aali, Jason Hom, Tridu Huynh, Thomas Lew, April S. Liang, Weihan Chu, Natasha Z. Steele, Christina F. Lin, Jingkun Yang, Kameron C. Black, Stephen P. Ma, Fateme N. Haredasht, Nigam H. Shah, Kevin Schulman, Jonathan H. Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, Clinical Text Generation, Factual Accuracy Evaluation, Generative AI, Healthcare

**Relevance Score:** 9

**TL;DR:** A study introducing MedFactEval and MedAgentBrief for evaluating factual accuracy in LLM-generated clinical text, aiming to improve AI adoption in healthcare.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical issue of evaluating factual accuracy in LLM-generated clinical text, which is essential for quality assurance and scalable deployment in healthcare settings.

**Method:** The study introduces MedFactEval, a scalable evaluation framework using a multi-LLM majority vote, and MedAgentBrief, a model-agnostic workflow for generating factual discharge summaries.

**Key Contributions:**

	1. Introduction of MedFactEval for scalable factual accuracy evaluation in clinical texts
	2. Development of MedAgentBrief for generating high-quality discharge summaries
	3. Validation of the LLM Jury against a physician panel, showing high levels of agreement.

**Result:** The MedFactEval LLM Jury achieved almost perfect agreement with a panel of physicians, demonstrating its effectiveness with a Cohen's kappa of 81%, comparable to human experts.

**Limitations:** 

**Conclusion:** The paper presents a robust evaluation framework and workflow that enhances the responsible use of generative AI in clinical applications.

**Abstract:** Evaluating factual accuracy in Large Language Model (LLM)-generated clinical text is a critical barrier to adoption, as expert review is unscalable for the continuous quality assurance these systems require. We address this challenge with two complementary contributions. First, we introduce MedFactEval, a framework for scalable, fact-grounded evaluation where clinicians define high-salience key facts and an "LLM Jury"--a multi-LLM majority vote--assesses their inclusion in generated summaries. Second, we present MedAgentBrief, a model-agnostic, multi-step workflow designed to generate high-quality, factual discharge summaries. To validate our evaluation framework, we established a gold-standard reference using a seven-physician majority vote on clinician-defined key facts from inpatient cases. The MedFactEval LLM Jury achieved almost perfect agreement with this panel (Cohen's kappa=81%), a performance statistically non-inferior to that of a single human expert (kappa=67%, P < 0.001). Our work provides both a robust evaluation framework (MedFactEval) and a high-performing generation workflow (MedAgentBrief), offering a comprehensive approach to advance the responsible deployment of generative AI in clinical workflows.

</details>


### [49] [Let's Roleplay: Examining LLM Alignment in Collaborative Dialogues](https://arxiv.org/abs/2509.05882)

*Abhijnan Nath, Carine Graff, Nikhil Krishnaswamy*

**Main category:** cs.CL

**Keywords:** Large Language Models, friction agents, group collaboration, alignment methods, decision-making

**Relevance Score:** 9

**TL;DR:** This paper investigates how alignment methods for LLMs affect their effectiveness in multiturn, multiparty collaborations, emphasizing the role of friction agents to enhance group decision-making.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure LLMs function as reliable collaborators in long-horizon multiparty interactions, it is crucial to validate their behavior and alignment over multiple turns in conversations.

**Method:** The study employs a roleplay methodology to evaluate interventions from trained friction agents that prompt collaborative groups to reflect and deliberate more deeply during task conversations.

**Key Contributions:**

	1. Introduction of friction agents to LLM interactions
	2. Development of a counterfactual evaluation framework
	3. Demonstrated effectiveness of friction interventions over common alignment strategies

**Result:** The friction-aware approach outperformed common alignment baselines, facilitating better convergence to common ground and improving the correctness of task outcomes.

**Limitations:** 

**Conclusion:** The findings indicate that implementing friction-aware interventions in LLM interactions can significantly enhance collaborative decision-making processes.

**Abstract:** As Large Language Models (LLMs) integrate into diverse workflows, they are increasingly being considered "collaborators" with humans. If such AI collaborators are to be reliable, their behavior over multiturn interactions must be predictable, validated and verified before deployment. Common alignment techniques are typically developed under simplified single-user settings and do not account for the dynamics of long-horizon multiparty interactions. This paper examines how different alignment methods affect LLM agents' effectiveness as partners in multiturn, multiparty collaborations. We study this question through the lens of friction agents that intervene in group dialogues to encourage the collaborative group to slow down and reflect upon their reasoning for deliberative decision-making. Using a roleplay methodology, we evaluate interventions from differently-trained friction agents in collaborative task conversations. We propose a novel counterfactual evaluation framework that quantifies how friction interventions change the trajectory of group collaboration and belief alignment. Our results show that a friction-aware approach significantly outperforms common alignment baselines in helping both convergence to a common ground, or agreed-upon task-relevant propositions, and correctness of task outcomes.

</details>


### [50] [Enhancing the Robustness of Contextual ASR to Varying Biasing Information Volumes Through Purified Semantic Correlation Joint Modeling](https://arxiv.org/abs/2509.05908)

*Yue Gu, Zhihao Du, Ying Shi, Shiliang Zhang, Qian Chen, Jiqing Han*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Cross-attention, Biasing Information, Semantic Correlation, Machine Learning

**Relevance Score:** 4

**TL;DR:** The paper introduces PSC-Joint, an approach that improves automatic speech recognition (ASR) by integrating the most relevant biasing information while reducing computational costs through a purification mechanism.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of cross-attention-based contextual ASR models in recognizing personalized biasing phrases, especially as the volume of biasing information increases.

**Method:** The PSC-Joint approach identifies three levels of semantic correlation (list-level, phrase-level, and token-level) between ASR intermediate representations and biasing information, which are then jointly modeled.

**Key Contributions:**

	1. Introduction of the PSC-Joint approach for contextual ASR.
	2. Identification and modeling of multiple levels of semantic correlation.
	3. Development of a purification mechanism to enhance computational efficiency.

**Result:** PSC-Joint achieves average F1 score improvements of up to 21.34% on AISHELL-1 and 28.46% on KeSpeech with varying lengths of biasing lists.

**Limitations:** 

**Conclusion:** Integrating the most relevant biasing information reduces the effects of information volume variations in contextual ASR, leading to significant performance improvements.

**Abstract:** Recently, cross-attention-based contextual automatic speech recognition (ASR) models have made notable advancements in recognizing personalized biasing phrases. However, the effectiveness of cross-attention is affected by variations in biasing information volume, especially when the length of the biasing list increases significantly. We find that, regardless of the length of the biasing list, only a limited amount of biasing information is most relevant to a specific ASR intermediate representation. Therefore, by identifying and integrating the most relevant biasing information rather than the entire biasing list, we can alleviate the effects of variations in biasing information volume for contextual ASR. To this end, we propose a purified semantic correlation joint modeling (PSC-Joint) approach. In PSC-Joint, we define and calculate three semantic correlations between the ASR intermediate representations and biasing information from coarse to fine: list-level, phrase-level, and token-level. Then, the three correlations are jointly modeled to produce their intersection, so that the most relevant biasing information across various granularities is highlighted and integrated for contextual recognition. In addition, to reduce the computational cost introduced by the joint modeling of three semantic correlations, we also propose a purification mechanism based on a grouped-and-competitive strategy to filter out irrelevant biasing phrases. Compared with baselines, our PSC-Joint approach achieves average relative F1 score improvements of up to 21.34% on AISHELL-1 and 28.46% on KeSpeech, across biasing lists of varying lengths.

</details>


### [51] [Accelerating Large Language Model Inference via Early-Exiting Algorithms](https://arxiv.org/abs/2509.05915)

*Sangmin Bae*

**Main category:** cs.CL

**Keywords:** large language models, adaptive computation, early-exiting

**Relevance Score:** 6

**TL;DR:** This dissertation proposes a novel approach to enhance the efficiency of large language models by co-designing adaptive algorithms and model architectures to overcome computational bottlenecks in early-exiting mechanisms.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** The deployment of large language models is hindered by high computational costs which can be alleviated through adaptive computation methods, but these methods introduce system-level bottlenecks that affect overall efficiency.

**Method:** The dissertation develops an efficient parallel decoding mechanism and employs deep parameter sharing to create models that are both compact and efficient, addressing synchronization issues in dynamic inference.

**Key Contributions:**

	1. Efficient parallel decoding mechanism for early-exiting
	2. Architectural innovation through deep parameter sharing
	3. Unified framework for dynamic recursion depth assignment

**Result:** The work presents a unified framework utilizing lightweight routers to optimally assign recursion depth for each token, achieving a new balance between efficiency and performance.

**Limitations:** 

**Conclusion:** By optimizing both adaptive computation and parameter efficiency within a single model, this dissertation establishes a new Pareto frontier that improves the practical deployment of large language models.

**Abstract:** Large language models have achieved remarkable capabilities, but their practical deployment is hindered by significant computational costs. While adaptive computation methods like early-exiting promise to reduce these costs, they introduce a fundamental conflict: the per-token dynamism intended to save computation often creates system-level bottlenecks that can paradoxically reduce throughput in batched inference. This dissertation resolves this conflict by co-designing adaptive algorithms and model architectures to strike an optimal balance between dynamism and efficiency. To this end, our work first addresses critical sources of overhead in conventional early-exiting by proposing an efficient parallel decoding mechanism. We then show that deep parameter sharing provides an architectural foundation that not only yields compact, parameter-efficient models but also inherently mitigates the critical synchronization issues affecting dynamic inference. Finally, this work presents a unified framework where lightweight routers are pretrained to dynamically assign an optimal recursion depth for each token. This approach establishes a new Pareto frontier between efficiency and performance by effectively optimizing for both adaptive computation and parameter efficiency within a single model.

</details>


### [52] [KatotohananQA: Evaluating Truthfulness of Large Language Models in Filipino](https://arxiv.org/abs/2509.06065)

*Lorenzo Alfred Nery, Ronald Dawson Catignas, Thomas James Tiam-Lee*

**Main category:** cs.CL

**Keywords:** Large Language Models, Truthfulness Evaluation, Low-resource Languages, Multilingual NLP, Filipino Language

**Relevance Score:** 8

**TL;DR:** This paper introduces KatotohananQA, a Filipino adaptation of the TruthfulQA benchmark, highlighting performance disparities in LLM truthfulness between English and Filipino.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of multilingual benchmarks for evaluating truthfulness in large language models, particularly in low-resource languages like Filipino.

**Method:** Seven free-tier proprietary models were evaluated using a binary-choice framework to assess their performance on the KatotohananQA benchmark.

**Key Contributions:**

	1. Introduction of KatotohananQA benchmark for Filipino language
	2. Demonstration of performance gaps in LLMs between English and Filipino
	3. Identification of robustness disparities across various question characteristics

**Result:** Significant performance gaps exist between English and Filipino in truthfulness, with newer OpenAI models showing strong multilingual capabilities.

**Limitations:** The study is limited to seven proprietary models and focuses on binary-choice evaluations, which may not capture nuanced performance aspects.

**Conclusion:** The study emphasizes the need for broader multilingual evaluation frameworks to ensure fairness and reliability in the application of LLMs across different languages.

**Abstract:** Large Language Models (LLMs) achieve remarkable performance across various tasks, but their tendency to produce hallucinations limits reliable adoption. Benchmarks such as TruthfulQA have been developed to measure truthfulness, yet they are primarily available in English, leaving a gap in evaluating LLMs in low-resource languages. To address this, we present KatotohananQA, a Filipino translation of the TruthfulQA benchmark. Seven free-tier proprietary models were assessed using a binary-choice framework. Findings show a significant performance gap between English and Filipino truthfulness, with newer OpenAI models (GPT-5 and GPT-5 mini) demonstrating strong multilingual robustness. Results also reveal disparities across question characteristics, suggesting that some question types, categories, and topics are less robust to multilingual transfer which highlight the need for broader multilingual evaluation to ensure fairness and reliability in LLM usage.

</details>


### [53] [Multimodal Fine-grained Context Interaction Graph Modeling for Conversational Speech Synthesis](https://arxiv.org/abs/2509.06074)

*Zhenqi Jia, Rui Liu, Berrak Sisman, Haizhou Li*

**Main category:** cs.CL

**Keywords:** Conversational Speech Synthesis, Multimodal Dialogue, Prosody Modeling

**Relevance Score:** 5

**TL;DR:** MFCIG-CSS is a conversational speech synthesis system that uses multimodal fine-grained interaction graphs to improve natural prosody by modeling dialogue history.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To generate speech with natural prosody by effectively modeling multimodal dialogue history, addressing gaps in existing methods that ignore fine-grained interactions.

**Method:** MFCIG-CSS constructs two specialized graphs: a semantic interaction graph and a prosody interaction graph to encode word-level semantics and prosody interactions.

**Key Contributions:**

	1. Introduction of two specialized multimodal interaction graphs for CSS
	2. Demonstrated significant improvements over baseline models
	3. Advanced the understanding of multimodal dialogue history interactions

**Result:** MFCIG-CSS outperforms all baseline models on the DailyTalk dataset in terms of prosodic expressiveness.

**Limitations:** 

**Conclusion:** The proposed method enhances synthesized speech by utilizing fine-grained multimodal interactions, significantly improving naturalness and expressiveness of Conversational Speech Synthesis.

**Abstract:** Conversational Speech Synthesis (CSS) aims to generate speech with natural prosody by understanding the multimodal dialogue history (MDH). The latest work predicts the accurate prosody expression of the target utterance by modeling the utterance-level interaction characteristics of MDH and the target utterance. However, MDH contains fine-grained semantic and prosody knowledge at the word level. Existing methods overlook the fine-grained semantic and prosodic interaction modeling. To address this gap, we propose MFCIG-CSS, a novel Multimodal Fine-grained Context Interaction Graph-based CSS system. Our approach constructs two specialized multimodal fine-grained dialogue interaction graphs: a semantic interaction graph and a prosody interaction graph. These two interaction graphs effectively encode interactions between word-level semantics, prosody, and their influence on subsequent utterances in MDH. The encoded interaction features are then leveraged to enhance synthesized speech with natural conversational prosody. Experiments on the DailyTalk dataset demonstrate that MFCIG-CSS outperforms all baseline models in terms of prosodic expressiveness. Code and speech samples are available at https://github.com/AI-S2-Lab/MFCIG-CSS.

</details>


### [54] [Multimodal Reasoning for Science: Technical Report and 1st Place Solution to the ICML 2025 SeePhys Challenge](https://arxiv.org/abs/2509.06079)

*Hao Liang, Ruitao Wu, Bohan Zeng, Junbo Niu, Wentao Zhang, Bin Dong*

**Main category:** cs.CL

**Keywords:** multimodal reasoning, AI, caption-assisted reasoning, geometric reasoning, MathVerse

**Relevance Score:** 8

**TL;DR:** Introducing a caption-assisted reasoning framework that improves multimodal reasoning in AI by bridging visual and textual modalities with notable performance in competitions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Multimodal reasoning is a significant challenge in AI, particularly for text-based models like GPT-o3 that struggle with performance in multimodal contexts.

**Method:** We propose a caption-assisted reasoning framework that integrates visual and textual modalities to enhance reasoning capabilities.

**Key Contributions:**

	1. Development of a caption-assisted reasoning framework
	2. Demonstrated effectiveness in competition
	3. Validated generalization on MathVerse benchmark

**Result:** The framework achieved 1st place in the ICML 2025 AI for Math Workshop & Challenge 2: SeePhys, and demonstrated generalization on the MathVerse benchmark for geometric reasoning.

**Limitations:** 

**Conclusion:** Our method showcases effectiveness and robustness in multimodal reasoning tasks, with publicly available code for further research.

**Abstract:** Multimodal reasoning remains a fundamental challenge in artificial intelligence. Despite substantial advances in text-based reasoning, even state-of-the-art models such as GPT-o3 struggle to maintain strong performance in multimodal scenarios. To address this gap, we introduce a caption-assisted reasoning framework that effectively bridges visual and textual modalities. Our approach achieved 1st place in the ICML 2025 AI for Math Workshop \& Challenge 2: SeePhys, highlighting its effectiveness and robustness. Furthermore, we validate its generalization on the MathVerse benchmark for geometric reasoning, demonstrating the versatility of our method. Our code is publicly available at https://github.com/OpenDCAI/SciReasoner.

</details>


### [55] [Orthogonal Low-rank Adaptation in Lie Groups for Continual Learning of Large Language Models](https://arxiv.org/abs/2509.06100)

*Kefan Cao, Shuaicheng Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fine-tuning, Geometric Structure, Orthogonality, Multi-task Learning

**Relevance Score:** 8

**TL;DR:** Presentation of OLieRA, a method for fine-tuning large language models that incorporates Lie group theory to preserve parameter geometry and achieve better task performance.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To address catastrophic forgetting in language models during multi-task learning by preserving the intrinsic geometric structure of parameters while ensuring low-rank orthogonality.

**Method:** OLieRA utilizes Lie group theory and multiplicative updates to maintain the geometric structure of LLM parameters while enforcing orthogonality constraints in task subspaces.

**Key Contributions:**

	1. Introduction of Orthogonal Low-rank Adaptation in Lie Groups (OLieRA) for LLM fine-tuning
	2. Preservation of intrinsic geometric structure of parameters
	3. Empirical evidence of improved performance on benchmarks.

**Result:** OLieRA achieves state-of-the-art results on the Standard CL benchmark, demonstrating superior performance in multi-task settings compared to existing methods.

**Limitations:** 

**Conclusion:** The proposed method significantly improves the handling of catastrophic forgetting in LLMs, combining geometric parameter structure preservation with orthogonality in task subspaces.

**Abstract:** Large language models (LLMs) are prone to catastrophic forgetting in sequential multi-task settings. Parameter regularization methods such as O-LoRA and N-LoRA alleviate task interference by enforcing low-rank subspace orthogonality, but they overlook the fact that conventional additive fine-tuning disrupts the intrinsic geometric structure of LLM parameters, limiting performance. Our key insight is that the parameter space of LLMs possesses a geometric structure, which must be preserved in addition to enforcing orthogonality. Based on this, we propose Orthogonal Low-rank Adaptation in Lie Groups (OLieRA), which introduces Lie group theory into LLM fine-tuning: leveraging multiplicative updates to preserve parameter geometry while applying orthogonality constraints to task subspaces. Experiments demonstrate that OLieRA achieves state-of-the-art results on the Standard CL benchmark and remains among the top-performing methods in the Large Number of Tasks setting.

</details>


### [56] [Benchmarking Gender and Political Bias in Large Language Models](https://arxiv.org/abs/2509.06164)

*Jinrui Yang, Xudong Han, Timothy Baldwin*

**Main category:** cs.CL

**Keywords:** large language models, political bias, fairness in NLP, EuroParlVote, gender classification

**Relevance Score:** 9

**TL;DR:** EuroParlVote is a benchmark for evaluating LLMs in political contexts, linking speeches to vote outcomes and revealing biases against female MEPs and far-left/far-right groups.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address bias in LLMs when analyzing politically sensitive data, especially in the context of European Parliament debates.

**Method:** The EuroParlVote benchmark was created, which links European Parliament speeches to roll-call votes and includes demographic information about MEPs.

**Key Contributions:**

	1. Introduction of a novel benchmark (EuroParlVote) for evaluating LLMs in political contexts
	2. Demonstration of bias in LLMs regarding gender and political alignment
	3. Release of dataset and tools for further research on NLP fairness

**Result:** LLMs showed biases, misclassifying female MEPs and underperforming in predicting votes from far-left and far-right groups. Proprietary models outperformed open-weight models in robustness and fairness.

**Limitations:** The study focuses on European Parliament data, which may not generalize to other political contexts.

**Conclusion:** The released EuroParlVote dataset and tools will aid future research on fairness and accountability in NLP, highlighting the biases present in LLMs.

**Abstract:** We introduce EuroParlVote, a novel benchmark for evaluating large language models (LLMs) in politically sensitive contexts. It links European Parliament debate speeches to roll-call vote outcomes and includes rich demographic metadata for each Member of the European Parliament (MEP), such as gender, age, country, and political group. Using EuroParlVote, we evaluate state-of-the-art LLMs on two tasks -- gender classification and vote prediction -- revealing consistent patterns of bias. We find that LLMs frequently misclassify female MEPs as male and demonstrate reduced accuracy when simulating votes for female speakers. Politically, LLMs tend to favor centrist groups while underperforming on both far-left and far-right ones. Proprietary models like GPT-4o outperform open-weight alternatives in terms of both robustness and fairness. We release the EuroParlVote dataset, code, and demo to support future research on fairness and accountability in NLP within political contexts.

</details>


### [57] [Understanding the Influence of Synthetic Data for Text Embedders](https://arxiv.org/abs/2509.06184)

*Jacob Mitchell Springer, Vaibhav Adlakha, Siva Reddy, Aditi Raghunathan, Marius Mosbach*

**Main category:** cs.CL

**Keywords:** synthetic data, text embeddings, model generalization

**Relevance Score:** 7

**TL;DR:** The paper explores the impact of synthetic LLM-generated data on model generalization, releasing a high-quality dataset and revealing limited benefits and trade-offs in performance across tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the role of synthetic LLM-generated data in improving the generalization of text embedders and to provide a publicly available dataset for further analysis.

**Method:** The authors reproduce and release a synthetic dataset and analyze its effects on model performance across different datasets and tasks.

**Key Contributions:**

	1. Public release of a synthetic dataset for text embeddings.
	2. Analysis of the impact of synthetic data on model generalization.
	3. Identification of trade-offs in performance across different tasks.

**Result:** The synthetic data leads to consistent performance improvements but these benefits are sparse and localized, with observed trade-offs between tasks.

**Limitations:** Benefits of synthetic data are localized and may degrade performance on some tasks.

**Conclusion:** Current synthetic data approaches have limitations for building robust general-purpose embedders, challenging assumptions about their effectiveness across various tasks.

**Abstract:** Recent progress in developing general purpose text embedders has been driven by training on ever-growing corpora of synthetic LLM-generated data. Nonetheless, no publicly available synthetic dataset exists, posing a barrier to studying its role for generalization. To address this issue, we first reproduce and publicly release the synthetic data proposed by Wang et al. (Mistral-E5). Our synthetic data is high quality and leads to consistent improvements in performance. Next, we critically examine where exactly synthetic data improves model generalization. Our analysis reveals that benefits from synthetic data are sparse and highly localized to individual datasets. Moreover, we observe trade-offs between the performance on different categories and data that benefits one task, degrades performance on another. Our findings highlight the limitations of current synthetic data approaches for building general-purpose embedders and challenge the notion that training on synthetic data leads to more robust embedding models across tasks.

</details>


### [58] [Augmented Fine-Tuned LLMs for Enhanced Recruitment Automation](https://arxiv.org/abs/2509.06196)

*Mohamed T. Younes, Omar Walid, Khaled Shaban, Ali Hamdi, Mai Hassan*

**Main category:** cs.CL

**Keywords:** recruitment automation, large language models, synthetic dataset

**Relevance Score:** 8

**TL;DR:** A new approach to automate recruitment using fine-tuned Large Language Models (LLMs) for improved accuracy in candidate-job matching.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the recruitment process by addressing the limitations of generic LLMs and providing a structured methodology for effective candidate-job matching.

**Method:** The study creates a synthetic dataset using a standardized JSON format, parses resumes with a high-parameter LLM, and trains fine-tuned LLMs specifically for recruitment tasks.

**Key Contributions:**

	1. Introduction of a synthetic dataset for recruitment automation
	2. Development of a structured JSON format for data consistency
	3. Demonstration of enhanced performance metrics in recruitment tasks

**Result:** The fine-tuned Phi-4 model achieved an F1 score of 90.62%, demonstrating significant improvements in performance metrics over base models and other state-of-the-art LLMs.

**Limitations:** 

**Conclusion:** Fine-tuned LLMs have the potential to revolutionize recruitment workflows by ensuring better candidate-job matching and improving overall recruitment efficiency.

**Abstract:** This paper presents a novel approach to recruitment automation. Large Language Models (LLMs) were fine-tuned to improve accuracy and efficiency. Building upon our previous work on the Multilayer Large Language Model-Based Robotic Process Automation Applicant Tracking (MLAR) system . This work introduces a novel methodology. Training fine-tuned LLMs specifically tuned for recruitment tasks. The proposed framework addresses the limitations of generic LLMs by creating a synthetic dataset that uses a standardized JSON format. This helps ensure consistency and scalability. In addition to the synthetic data set, the resumes were parsed using DeepSeek, a high-parameter LLM. The resumes were parsed into the same structured JSON format and placed in the training set. This will help improve data diversity and realism. Through experimentation, we demonstrate significant improvements in performance metrics, such as exact match, F1 score, BLEU score, ROUGE score, and overall similarity compared to base models and other state-of-the-art LLMs. In particular, the fine-tuned Phi-4 model achieved the highest F1 score of 90.62%, indicating exceptional precision and recall in recruitment tasks. This study highlights the potential of fine-tuned LLMs. Furthermore, it will revolutionize recruitment workflows by providing more accurate candidate-job matching.

</details>


### [59] [MSLEF: Multi-Segment LLM Ensemble Finetuning in Recruitment](https://arxiv.org/abs/2509.06200)

*Omar Walid, Mohamed T. Younes, Khaled Shaban, Mai Hassan, Ali Hamdi*

**Main category:** cs.CL

**Keywords:** resume parsing, LLM fine-tuning, recruitment automation, ensemble learning, segment-aware architecture

**Relevance Score:** 9

**TL;DR:** This paper introduces MSLEF, a framework that enhances resume parsing in recruitment through multi-segment ensemble learning and LLM fine-tuning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the accuracy of resume parsing in recruitment automation by using a more adaptable and specialized approach.

**Method:** MSLEF employs a segment-aware architecture that integrates multiple fine-tuned LLMs, each focused on specific resume segments, utilizing weighted voting for enhanced accuracy.

**Key Contributions:**

	1. Introduction of multi-segment ensemble learning for resume parsing
	2. Development of segment-aware architecture for tailored resume analysis
	3. Improvement of recruitment metrics by utilizing specialized LLMs

**Result:** MSLEF demonstrates significant performance improvements over traditional single-model systems, achieving better metrics in Exact Match, F1 score, BLEU, ROUGE, and Recruitment Similarity, with a notable +7% enhancement in RS compared to the best single model.

**Limitations:** 

**Conclusion:** The framework's adaptability to varied resume formats and its enhanced generalization capabilities make it a promising solution for real-world hiring applications.

**Abstract:** This paper presents MSLEF, a multi-segment ensemble framework that employs LLM fine-tuning to enhance resume parsing in recruitment automation. It integrates fine-tuned Large Language Models (LLMs) using weighted voting, with each model specializing in a specific resume segment to boost accuracy. Building on MLAR , MSLEF introduces a segment-aware architecture that leverages field-specific weighting tailored to each resume part, effectively overcoming the limitations of single-model systems by adapting to diverse formats and structures. The framework incorporates Gemini-2.5-Flash LLM as a high-level aggregator for complex sections and utilizes Gemma 9B, LLaMA 3.1 8B, and Phi-4 14B. MSLEF achieves significant improvements in Exact Match (EM), F1 score, BLEU, ROUGE, and Recruitment Similarity (RS) metrics, outperforming the best single model by up to +7% in RS. Its segment-aware design enhances generalization across varied resume layouts, making it highly adaptable to real-world hiring scenarios while ensuring precise and reliable candidate representation.

</details>


### [60] [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751)

*Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** machine writing, large language models, iterative expansion, knowledge density, human-computer interaction

**Relevance Score:** 8

**TL;DR:** OmniThink is a machine writing framework that enhances the depth and novelty of content generated by large language models through a human-like process of iterative expansion.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current machine writing methods often lead to shallow and repetitive outputs due to limitations in predefined model scopes and the quality of retrieved information.

**Method:** OmniThink simulates the cognitive behavior of learners, allowing for slow and reflective writing, which deepens knowledge over time.

**Key Contributions:**

	1. Introduction of a slow-thinking framework for machine writing
	2. Enhancement of knowledge density in generated content
	3. Demonstration of human-like iterative expansion in content generation

**Result:** Experimental results show that OmniThink increases the knowledge density of generated articles while maintaining coherence and depth.

**Limitations:** 

**Conclusion:** OmniThink demonstrates significant potential in producing high-quality, long-form articles, as confirmed by human evaluations and expert feedback.

**Abstract:** Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.

</details>


### [61] [No Encore: Unlearning as Opt-Out in Music Generation](https://arxiv.org/abs/2509.06277)

*Jinju Kim, Taehan Kim, Abdul Waheed, Rita Singh*

**Main category:** cs.CL

**Keywords:** AI music generation, machine unlearning, Text-to-Music, creativity, copyright issues

**Relevance Score:** 4

**TL;DR:** This paper discusses the application of machine unlearning techniques to AI music generation to mitigate risks of copyright exploitation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address ethical and legal concerns related to copyright exploitation in AI-generated music.

**Method:** We explore existing machine unlearning methods applied to a pre-trained Text-to-Music (TTM) model, assessing their effectiveness in unlearning datasets while maintaining model performance.

**Key Contributions:**

	1. First application of machine unlearning in music generation
	2. Insights into challenges faced in applying unlearning techniques
	3. Foundational analysis for future works in music generative models

**Result:** Our experiments reveal the challenges of integrating unlearning in music generation while providing foundational insights for future research.

**Limitations:** Work in progress; results are preliminary and may evolve with further research.

**Conclusion:** The study presents preliminary results that lay the groundwork for further investigation into unlearning techniques for music generative models.

**Abstract:** AI music generation is rapidly emerging in the creative industries, enabling intuitive music generation from textual descriptions. However, these systems pose risks in exploitation of copyrighted creations, raising ethical and legal concerns. In this paper, we present preliminary results on the first application of machine unlearning techniques from an ongoing research to prevent inadvertent usage of creative content. Particularly, we explore existing methods in machine unlearning to a pre-trained Text-to-Music (TTM) baseline and analyze their efficacy in unlearning pre-trained datasets without harming model performance. Through our experiments, we provide insights into the challenges of applying unlearning in music generation, offering a foundational analysis for future works on the application of unlearning for music generative models.

</details>


### [62] [Mask-GCG: Are All Tokens in Adversarial Suffixes Necessary for Jailbreak Attacks?](https://arxiv.org/abs/2509.06350)

*Junjie Mu, Zonghao Ying, Zhekui Fan, Zonglei Jing, Yaoyuan Zhang, Zhengmin Yu, Wenxin Zhang, Quanchen Zou, Xiangzheng Zhang*

**Main category:** cs.CL

**Keywords:** jailbreak attacks, Large Language Models, token masking, efficiency, redundancy

**Relevance Score:** 8

**TL;DR:** Introducing Mask-GCG, a method that reduces token redundancy in jailbreak attacks on LLMs by employing learnable token masking.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency and effectiveness of jailbreak attacks on LLMs by reducing redundancy in suffixes used in prompts.

**Method:** Mask-GCG employs learnable token masking to identify and prune low-impact tokens in the suffix of prompts, enhancing computational efficiency and attack success.

**Key Contributions:**

	1. Introduction of Mask-GCG for token pruning in jailbreak attacks.
	2. Demonstration of token redundancy in existing methods.
	3. Enhanced efficiency and reduced computational overhead in executing attacks.

**Result:** Experimental evaluation shows that pruning low-impact tokens does not affect attack success rates, revealing significant redundancy in LLM prompts.

**Limitations:** The approach primarily focuses on suffix token manipulation and may not address all types of vulnerabilities in LLMs.

**Conclusion:** Mask-GCG effectively shortens the necessary time to conduct successful jailbreak attacks while offering insights into improving LLM design.

**Abstract:** Jailbreak attacks on Large Language Models (LLMs) have demonstrated various successful methods whereby attackers manipulate models into generating harmful responses that they are designed to avoid. Among these, Greedy Coordinate Gradient (GCG) has emerged as a general and effective approach that optimizes the tokens in a suffix to generate jailbreakable prompts. While several improved variants of GCG have been proposed, they all rely on fixed-length suffixes. However, the potential redundancy within these suffixes remains unexplored. In this work, we propose Mask-GCG, a plug-and-play method that employs learnable token masking to identify impactful tokens within the suffix. Our approach increases the update probability for tokens at high-impact positions while pruning those at low-impact positions. This pruning not only reduces redundancy but also decreases the size of the gradient space, thereby lowering computational overhead and shortening the time required to achieve successful attacks compared to GCG. We evaluate Mask-GCG by applying it to the original GCG and several improved variants. Experimental results show that most tokens in the suffix contribute significantly to attack success, and pruning a minority of low-impact tokens does not affect the loss values or compromise the attack success rate (ASR), thereby revealing token redundancy in LLM prompts. Our findings provide insights for developing efficient and interpretable LLMs from the perspective of jailbreak attacks.

</details>


### [63] [PL-CA: A Parametric Legal Case Augmentation Framework](https://arxiv.org/abs/2509.06356)

*Ao Chang, Yubo Chen, Jun Zhao*

**Main category:** cs.CL

**Keywords:** parametric RAG, multi-task legal dataset, LLM integration

**Relevance Score:** 8

**TL;DR:** This paper proposes PL-CA, a parametric RAG framework that addresses the limitations of conventional RAG in legal applications by encoding legal knowledge into parametric vectors and integrating them into LLMs, while also providing a multi-task legal dataset with expert annotations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to overcome the limitations of conventional RAG methods that lead to knowledge insufficiency and performance degradation in legal tasks due to context constraints and inadequate benchmarks.

**Method:** This study introduces a parametric RAG framework (P-RAG) that augments corpus knowledge into parametric vectors and integrates this knowledge into the LLM's feed-forward networks using LoRA, thus reducing context load and improving model performance on legal tasks.

**Key Contributions:**

	1. Introduction of a parametric RAG framework for data augmentation.
	2. Development of a multi-task legal dataset with expert annotations.
	3. Demonstrated ability to maintain model performance while reducing long context overhead.

**Result:** The proposed method demonstrates a significant reduction in overhead from long contexts while maintaining competitive performance compared to traditional RAG approaches, evidenced by experiments conducted on a newly constructed legal dataset.

**Limitations:** 

**Conclusion:** The findings indicate that PL-CA effectively addresses the challenges of knowledge integration and context length, making it a valuable advancement for legal task processing with LLMs.

**Abstract:** Conventional RAG is considered one of the most effective methods for addressing model knowledge insufficiency and hallucination, particularly in the judicial domain that requires high levels of knowledge rigor, logical consistency, and content integrity. However, the conventional RAG method only injects retrieved documents directly into the model's context, which severely constrains models due to their limited context windows and introduces additional computational overhead through excessively long contexts, thereby disrupting models' attention and degrading performance on downstream tasks. Moreover, many existing benchmarks lack expert annotation and focus solely on individual downstream tasks while real-world legal scenarios consist of multiple mixed legal tasks, indicating conventional benchmarks' inadequacy for reflecting models' true capabilities. To address these limitations, we propose PL-CA, which introduces a parametric RAG (P-RAG) framework to perform data augmentation on corpus knowledge and encode this legal knowledge into parametric vectors, and then integrates this parametric knowledge into the LLM's feed-forward networks (FFN) via LoRA, thereby alleviating models' context pressure. Additionally, we also construct a multi-task legal dataset comprising more than 2000 training and test instances, which are all expert-annotated and manually verified. We conduct our experiments on our dataset, and the experimental results demonstrate that our method reduces the overhead associated with excessively long contexts while maintaining competitive performance on downstream tasks compared to conventional RAG. Our code and dataset are provided in the appendix.

</details>


### [64] [Do LLMs exhibit the same commonsense capabilities across languages?](https://arxiv.org/abs/2509.06401)

*Ivan Martínez-Murillo, Elena Lloret, Paloma Moreda, Albert Gatt*

**Main category:** cs.CL

**Keywords:** multilingual commonsense generation, Large Language Models, MULTICOM benchmark, commonsense knowledge, underrepresented languages

**Relevance Score:** 8

**TL;DR:** This paper presents MULTICOM, a benchmark for evaluating multilingual commonsense generation in Large Language Models across four languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the multilingual commonsense generation capabilities of Large Language Models (LLMs) and to highlight their limitations in less-resourced languages.

**Method:** The study evaluates open-source LLMs such as LLaMA, Qwen, Gemma, EuroLLM, and Salamandra using the MULTICOM benchmark, which requires generating commonsensical sentences from triplets of words. Evaluation methods include automatic metrics, LLM-as-a-judge approaches, and human annotations.

**Key Contributions:**

	1. Introduction of the MULTICOM benchmark for multilingual commonsense generation.
	2. Evaluation of various open-source LLMs across multiple languages.
	3. Insights into the performance disparities among languages in commonsense generation tasks.

**Result:** Results indicate that LLMs perform better in English compared to less-resourced languages like Valencian and Dutch, revealing significant performance gaps that highlight the need for improvement in multilingual applications.

**Limitations:** The study primarily focuses on four languages and may not represent the capabilities or limitations of LLMs in other languages or dialects.

**Conclusion:** The findings point to the limitations of LLMs in generating commonsense knowledge in multilingual contexts, suggesting that while contextual support may help, it does not fully bridge the performance gap.

**Abstract:** This paper explores the multilingual commonsense generation abilities of Large Language Models (LLMs). To facilitate this investigation, we introduce MULTICOM, a novel benchmark that extends the COCOTEROS dataset to four languages: English, Spanish, Dutch, and Valencian. The task involves generating a commonsensical sentence that includes a given triplet of words. We evaluate a range of open-source LLMs, including LLaMA, Qwen, Gemma, EuroLLM, and Salamandra, on this benchmark. Our evaluation combines automatic metrics, LLM-as-a-judge approaches (using Prometheus and JudgeLM), and human annotations. Results consistently show superior performance in English, with significantly lower performance in less-resourced languages. While contextual support yields mixed results, it tends to benefit underrepresented languages. These findings underscore the current limitations of LLMs in multilingual commonsense generation. The dataset is publicly available at https://huggingface.co/datasets/gplsi/MULTICOM.

</details>


### [65] [WebExplorer: Explore and Evolve for Training Long-Horizon Web Agents](https://arxiv.org/abs/2509.06501)

*Junteng Liu, Yunji Li, Chi Zhang, Jingyang Li, Aili Chen, Ke Ji, Weiyu Cheng, Zijia Wu, Chengyu Du, Qidi Xu, Jiayuan Song, Zhengmao Zhu, Wenhu Chen, Pengyu Zhao, Junxian He*

**Main category:** cs.CL

**Keywords:** Large Language Models, web agents, data generation, information-seeking, reinforcement learning

**Relevance Score:** 9

**TL;DR:** WebExplorer introduces a systematic data generation approach for enhancing web agent capabilities in complex information-seeking tasks, achieving state-of-the-art performance with an 8B-sized model.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for improved web agents that can effectively perform complex information-seeking tasks and handle challenging data queries.

**Method:** The paper presents a model-based exploration and iterative query evolution approach for generating challenging query-answer pairs, resulting in the development of the WebExplorer-8B model through supervised fine-tuning and reinforcement learning.

**Key Contributions:**

	1. Introduction of the WebExplorer data generation method for complex queries
	2. Development of an 8B-sized model with superior performance
	3. Demonstration of effective long-horizon web agent capabilities

**Result:** WebExplorer-8B demonstrates state-of-the-art performance in various information-seeking benchmarks, outperforming even larger models in specific tasks.

**Limitations:** 

**Conclusion:** The study presents a viable solution for enhancing web agent performance in long-horizon problem solving and establishes a path for future research in this domain.

**Abstract:** The paradigm of Large Language Models (LLMs) has increasingly shifted toward agentic applications, where web browsing capabilities are fundamental for retrieving information from diverse online sources. However, existing open-source web agents either demonstrate limited information-seeking abilities on complex tasks or lack transparent implementations. In this work, we identify that the key challenge lies in the scarcity of challenging data for information seeking. To address this limitation, we introduce WebExplorer: a systematic data generation approach using model-based exploration and iterative, long-to-short query evolution. This method creates challenging query-answer pairs that require multi-step reasoning and complex web navigation. By leveraging our curated high-quality dataset, we successfully develop advanced web agent WebExplorer-8B through supervised fine-tuning followed by reinforcement learning. Our model supports 128K context length and up to 100 tool calling turns, enabling long-horizon problem solving. Across diverse information-seeking benchmarks, WebExplorer-8B achieves the state-of-the-art performance at its scale. Notably, as an 8B-sized model, WebExplorer-8B is able to effectively search over an average of 16 turns after RL training, achieving higher accuracy than WebSailor-72B on BrowseComp-en/zh and attaining the best performance among models up to 100B parameters on WebWalkerQA and FRAMES. Beyond these information-seeking tasks, our model also achieves strong generalization on the HLE benchmark even though it is only trained on knowledge-intensive QA data. These results highlight our approach as a practical path toward long-horizon web agents.

</details>


### [66] [Crown, Frame, Reverse: Layer-Wise Scaling Variants for LLM Pre-Training](https://arxiv.org/abs/2509.06518)

*Andrei Baroian, Kasper Notebomer*

**Main category:** cs.CL

**Keywords:** Layer-Wise Scaling, Transformer Models, Ablation Study, Pre-training

**Relevance Score:** 6

**TL;DR:** This paper introduces three new variants of Layer-Wise Scaling (LWS) for transformer-based models that optimize layer sizes and attention heads, demonstrating improved performance compared to isotropic baselines.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of uniform (isotropic) layer sizes in transformer-based language models, recognizing the varying functional roles and computational needs of different model depths.

**Method:** The authors developed three LWS variants—Framed, Reverse, and Crown—that utilize two or three-point linear interpolation to redistribute feedforward neural network (FFN) widths and attention heads during pre-training.

**Key Contributions:**

	1. Introduction of new LWS variants (Framed, Reverse, Crown) for transformer models
	2. Systematic ablation study of LWS indicating improved performance over isotropic models
	3. Optimization of transformer layer sizes based on functional roles rather than uniformity

**Result:** The proposed models achieved similar losses and better performance than an isotropic baseline while maintaining training throughput, within a fixed parameter budget of 180M on a dataset of 5B tokens.

**Limitations:** Results may be skewed due to a data type mismatch, where tokens were incorrectly interpreted, potentially affecting the reported outcomes.

**Conclusion:** This research marks a preliminary exploration into layer-wise architectures for model pre-training, suggesting that further scaling experiments with more data and parameters are necessary to evaluate the full potential of these architectures.

**Abstract:** Transformer-based language models traditionally use uniform (isotropic) layer sizes, yet they ignore the diverse functional roles that different depths can play and their computational capacity needs. Building on Layer-Wise Scaling (LWS) and pruning literature, we introduce three new LWS variants - Framed, Reverse, and Crown - that redistribute FFN widths and attention heads via two or three-point linear interpolation in the pre-training stage. We present the first systematic ablation of LWS and its variants, on a fixed budget of 180M parameters, trained on 5B tokens. All models converge to similar losses and achieve better performance compared to an equal-cost isotropic baseline, without a substantial decrease in training throughput. This work represents an initial step into the design space of layer-wise architectures for pre-training, but future work should scale experiments to orders of magnitude more tokens and parameters to fully assess their potential.

</details>


### [67] [LAMDAS: LLM as an Implicit Classifier for Domain-specific Data Selection](https://arxiv.org/abs/2509.06524)

*Jian Wu, Hang Yu, Bingchang Liu, Wenjie Yang, Peng Di, Jianguo Li, Yue Zhang*

**Main category:** cs.CL

**Keywords:** large language models, data selection, one-class classification

**Relevance Score:** 8

**TL;DR:** LAMDAS is a novel method for domain-specific data selection utilizing LLMs as implicit classifiers to improve data quality and training efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The scarcity of high-quality, human-curated data for fine-tuning large language models poses a critical bottleneck in adapting them to specific domains.

**Method:** LAMDAS utilizes the pre-trained large language model itself as an implicit classifier for identifying domain-specific data, reframing the data selection problem as a one-class classification task.

**Key Contributions:**

	1. Introduction of LAMDAS for implicit classification of data selection
	2. Significant performance gains using reduced data
	3. Effective balance of performance and computational efficiency

**Result:** Extensive experiments show that LAMDAS surpasses full-data training performance using significantly less data and outperforms nine state-of-the-art baselines across various scenarios.

**Limitations:** 

**Conclusion:** LAMDAS offers a compelling balance between performance improvements and computational efficiency in data selection for domain-specific applications.

**Abstract:** Adapting large language models (LLMs) to specific domains often faces a critical bottleneck: the scarcity of high-quality, human-curated data. While large volumes of unchecked data are readily available, indiscriminately using them for fine-tuning risks introducing noise and degrading performance. Strategic data selection is thus crucial, requiring a method that is both accurate and efficient. Existing approaches, categorized as similarity-based and direct optimization methods, struggle to simultaneously achieve these goals. In this paper, we introduce LAMDAS (LLM As an iMplicit classifier for domain-specific DAta Selection), a novel approach that leverages the pre-trained LLM itself as an implicit classifier, thereby bypassing explicit feature engineering and computationally intensive optimization process. LAMDAS reframes data selection as a one-class classification problem, identifying candidate data that "belongs" to the target domain defined by a small reference dataset. Extensive experimental results demonstrate that LAMDAS not only exceeds the performance of full-data training using a fraction of the data but also outperforms nine state-of-the-art (SOTA) baselines under various scenarios. Furthermore, LAMDAS achieves the most compelling balance between performance gains and computational efficiency compared to all evaluated baselines.

</details>


### [68] [SLiNT: Structure-aware Language Model with Injection and Contrastive Training for Knowledge Graph Completion](https://arxiv.org/abs/2509.06531)

*Mengxue Yang, Chun Yang, Jiaqi Zhu, Jiafan Li, Jingqi Zhang, Yuyang Li, Ying Li*

**Main category:** cs.CL

**Keywords:** Link Prediction, Knowledge Graphs, Large Language Models, Contrastive Learning, Structure-Aware Models

**Relevance Score:** 8

**TL;DR:** SLiNT is a modular framework that integrates structural information into large language models for improved link prediction in knowledge graphs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve link prediction by effectively utilizing structural information and reducing semantic ambiguity in knowledge graphs, especially under challenging conditions.

**Method:** SLiNT employs Structure-Guided Neighborhood Enhancement to enrich sparse entities, Dynamic Hard Contrastive Learning for fine-grained supervision, and Gradient-Decoupled Dual Injection for token-level interventions while maintaining the integrity of the LLM parameters.

**Key Contributions:**

	1. Introduction of SLiNT, a novel framework for link prediction in knowledge graphs
	2. Incorporation of Structure-Guided Neighborhood Enhancement for enriching sparse data
	3. Use of Dynamic Hard Contrastive Learning to fine-tune entity recognition accuracy

**Result:** SLiNT outperforms or is competitive with existing embedding-based and generation-based methods in link prediction tasks on WN18RR and FB15k-237 datasets.

**Limitations:** 

**Conclusion:** The proposed structure-aware representation learning approach significantly enhances knowledge graph completion capabilities.

**Abstract:** Link prediction in knowledge graphs requires integrating structural information and semantic context to infer missing entities. While large language models offer strong generative reasoning capabilities, their limited exploitation of structural signals often results in structural sparsity and semantic ambiguity, especially under incomplete or zero-shot settings. To address these challenges, we propose SLiNT (Structure-aware Language model with Injection and coNtrastive Training), a modular framework that injects knowledge-graph-derived structural context into a frozen LLM backbone with lightweight LoRA-based adaptation for robust link prediction. Specifically, Structure-Guided Neighborhood Enhancement (SGNE) retrieves pseudo-neighbors to enrich sparse entities and mitigate missing context; Dynamic Hard Contrastive Learning (DHCL) introduces fine-grained supervision by interpolating hard positives and negatives to resolve entity-level ambiguity; and Gradient-Decoupled Dual Injection (GDDI) performs token-level structure-aware intervention while preserving the core LLM parameters. Experiments on WN18RR and FB15k-237 show that SLiNT achieves superior or competitive performance compared with both embedding-based and generation-based baselines, demonstrating the effectiveness of structure-aware representation learning for scalable knowledge graph completion.

</details>


### [69] [HAVE: Head-Adaptive Gating and ValuE Calibration for Hallucination Mitigation in Large Language Models](https://arxiv.org/abs/2509.06596)

*Xin Tong, Zhi Lin, Jingya Wang, Bo Jin*

**Main category:** cs.CL

**Keywords:** Large Language Models, hallucinations, attention mechanisms, parameter-free, trustworthy generation

**Relevance Score:** 9

**TL;DR:** HAVE is a parameter-free framework that reduces hallucinations in Large Language Models (LLMs) by implementing head-adaptive gating and value calibration to improve attention mechanisms, yielding more reliable outputs without needing finetuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** This paper addresses two key challenges in LLMs: the inadequacy of head importance as input-agnostic and the poor representation of token contributions through raw attention weights, which lead to hallucinations in generated content.

**Method:** The HAVE framework introduces head-adaptive gating for instance-level soft reweighing of attention heads and value calibration that enhances attention with the contribution magnitude of value vectors, operating without finetuning in one forward pass.

**Key Contributions:**

	1. Development of the HAVE framework for LLMs
	2. Introduction of head-adaptive gating and value calibration
	3. Demonstrated improvements on multiple QA benchmarks

**Result:** Experiments show that HAVE reduces hallucinations and outperforms strong baselines like DAGCD across various QA benchmarks, demonstrating its effectiveness and efficiency.

**Limitations:** 

**Conclusion:** HAVE provides a transparent and reproducible approach for improving the reliability of LLM outputs in real-world applications, easily integrating with existing models.

**Abstract:** Large Language Models (LLMs) often produce hallucinations in retrieval-augmented or long-context generation, even when relevant evidence is present. This stems from two issues: head importance is treated as input-agnostic, and raw attention weights poorly reflect each token's true contribution. We present HAVE (Head-Adaptive Gating and ValuE Calibration), a parameter-free decoding framework that directly addresses both challenges. HAVE introduces head-adaptive gating, which performs instance-level soft reweighing of attention heads, and value calibration, which augments attention with the magnitude of value vectors to approximate write-back contribution. Together, these modules construct token-level evidence aligned with model updates and fuse it with the LM distribution through a lightweight uncertainty-scaled policy. HAVE requires no finetuning and operates in a single forward pass, making it efficient and broadly applicable. Experiments across multiple QA benchmarks and LLM families demonstrate that HAVE consistently reduces hallucinations and outperforms strong baselines, including DAGCD, with modest overhead. The framework is transparent, reproducible, and readily integrates with off-the-shelf LLMs, advancing trustworthy generation in real-world settings.

</details>


### [70] [Guided Decoding and Its Critical Role in Retrieval-Augmented Generation](https://arxiv.org/abs/2509.06631)

*Özgür Uğur, Musa Yılmaz, Esra Şavirdi, Özay Ezerceli, Mahmut El Huseyni, Selva Taş, Reyhan Bayraktar*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Large Language Models, guided decoding

**Relevance Score:** 9

**TL;DR:** This study explores guided decoding in Retrieval-Augmented Generation (RAG) systems, evaluating methods for structured output generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of structured and reliable responses in RAG systems, particularly with large language models, and to minimize hallucinations in outputs.

**Method:** The research compares three guided decoding methods—Outlines, XGrammar, and LM Format Enforcer—across various multi-turn prompting setups (0-turn, 1-turn, and 2-turn).

**Key Contributions:**

	1. Comparison of three guided decoding methods in RAG systems
	2. Insights on the impact of multi-turn interactions on output quality
	3. Theoretical and practical guidance for LLM deployment

**Result:** The study provides insights into success rates, hallucination rates, and output quality of the different methods, showing how multi-turn interactions affect guided decoding.

**Limitations:** 

**Conclusion:** The findings highlight unexpected performance variations in guided decoding methods, aiding in the selection for specific use cases and enhancing the understanding of structured output generation.

**Abstract:** The integration of Large Language Models (LLMs) into various applications has driven the need for structured and reliable responses. A key challenge in Retrieval-Augmented Generation (RAG) systems is ensuring that outputs align with expected formats while minimizing hallucinations. This study examines the role of guided decoding in RAG systems, comparing three methods, Outlines, XGrammar, and LM Format Enforcer, across different multi-turn prompting setups (0-turn, 1-turn, and 2-turn). By evaluating success rates, hallucination rates, and output quality, we provide insights into their performance and applicability. Our findings reveal how multi-turn interactions influence guided decoding, uncovering unexpected performance variations that can inform method selection for specific use cases. This work advances the understanding of structured output generation in RAG systems, offering both theoretical insights and practical guidance for LLM deployment.

</details>


### [71] [Modelling Intertextuality with N-gram Embeddings](https://arxiv.org/abs/2509.06637)

*Yi Xing*

**Main category:** cs.CL

**Keywords:** intertextuality, text analysis, network analysis, literary studies, n-gram embeddings

**Relevance Score:** 2

**TL;DR:** This paper introduces a quantitative model for analyzing intertextuality in literary texts through pairwise comparisons of n-gram embeddings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a scalable method for analyzing intertextuality and provide network-based insights into literary relationships.

**Method:** The proposed model performs pairwise comparisons of the embeddings of n-grams from two texts and averages the results to quantify intertextuality.

**Key Contributions:**

	1. Introduction of a quantitative model for intertextuality
	2. Demonstration of scalability with a large dataset
	3. Identification of centrality and community structures in intertextual networks

**Result:** Validation with known intertextuality levels on four texts and a scalability test on 267 texts showed the method's effectiveness and efficiency.

**Limitations:** 

**Conclusion:** The network analysis reveals centrality and community structures, successfully capturing intertextual relationships.

**Abstract:** Intertextuality is a central tenet in literary studies. It refers to the intricate links between literary texts that are created by various types of references. This paper proposes a new quantitative model of intertextuality to enable scalable analysis and network-based insights: perform pairwise comparisons of the embeddings of n-grams from two texts and average their results as the overall intertextuality. Validation on four texts with known degrees of intertextuality, alongside a scalability test on 267 diverse texts, demonstrates the method's effectiveness and efficiency. Network analysis further reveals centrality and community structures, affirming the approach's success in capturing and quantifying intertextual relationships.

</details>


### [72] [Domain-Aware RAG: MoL-Enhanced RL for Efficient Training and Scalable Retrieval](https://arxiv.org/abs/2509.06650)

*Hao Lin, Peitong Xie, Jingxue Chen, Jie Lin, Qingkun Tang, Qianchun Lu*

**Main category:** cs.CL

**Keywords:** Retrieval-Augmented Generation, Reinforcement Learning, Domain-specific Knowledge

**Relevance Score:** 8

**TL;DR:** MoLER is a domain-aware RAG method using Mixture of Losses and reinforcement learning for optimized retrieval performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve coarse-ranking in retrieval-augmented generation (RAG) systems which struggle with balancing domain-specific knowledge and query enhancement.

**Method:** MoLER employs a two-stage pipeline: continual pre-training with Mixture of Losses (MoL) followed by reinforcement learning using Group Relative Policy Optimization (GRPO). It incorporates a Multi-query Single-passage Late Fusion strategy to reduce computational costs during training.

**Key Contributions:**

	1. Introduction of MoLER for optimized retrieval in RAG systems
	2. Use of Mixture of Losses for balancing knowledge and language capabilities
	3. Multi-query Single-passage Late Fusion strategy to enhance efficiency

**Result:** Extensive experiments demonstrate that MoLER achieves state-of-the-art performance, outperforming baseline methods and improving retrieval in specialized domains.

**Limitations:** 

**Conclusion:** MoLER effectively bridges the knowledge gap in RAG systems, allowing for robust and scalable retrieval solutions.

**Abstract:** Retrieval-Augmented Generation (RAG) systems rely heavily on the retrieval stage, particularly the coarse-ranking process. Existing coarse-ranking optimization approaches often struggle to balance domain-specific knowledge learning with query enhencement, resulting in suboptimal retrieval performance. To address this challenge, we propose MoLER, a domain-aware RAG method that uses MoL-Enhanced Reinforcement Learning to optimize retrieval. MoLER has a two-stage pipeline: a continual pre-training (CPT) phase using a Mixture of Losses (MoL) to balance domain-specific knowledge with general language capabilities, and a reinforcement learning (RL) phase leveraging Group Relative Policy Optimization (GRPO) to optimize query and passage generation for maximizing document recall. A key innovation is our Multi-query Single-passage Late Fusion (MSLF) strategy, which reduces computational overhead during RL training while maintaining scalable inference via Multi-query Multi-passage Late Fusion (MMLF). Extensive experiments on benchmark datasets show that MoLER achieves state-of-the-art performance, significantly outperforming baseline methods. MoLER bridges the knowledge gap in RAG systems, enabling robust and scalable retrieval in specialized domains.

</details>


### [73] [IntrEx: A Dataset for Modeling Engagement in Educational Conversations](https://arxiv.org/abs/2509.06652)

*Xingwei Tan, Mahathi Parvatham, Chiara Gambi, Gabriele Pergola*

**Main category:** cs.CL

**Keywords:** second-language acquisition, interestingness, teacher-student interactions, large language models, engagement

**Relevance Score:** 7

**TL;DR:** The paper presents IntrEx, a dataset for analyzing interestingness in teacher-student conversations to enhance second-language acquisition.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in understanding the linguistic features that drive engagement in educational conversations.

**Method:** A large dataset annotated for interestingness was created using a rigorous comparison-based rating approach with over 100 second-language learners, allowing for analysis of interest evolution in dialogues.

**Key Contributions:**

	1. Introduction of the IntrEx dataset for interestingness in educational dialogues.
	2. Demonstrated effectiveness of fine-tuning LLMs on interestingness ratings.
	3. Analysis of linguistic and cognitive factors impacting engagement.

**Result:** LLMs fine-tuned on interestingness ratings perform better than larger models like GPT-4o, indicating the value of specialized datasets for modeling educational engagement.

**Limitations:** 

**Conclusion:** The findings suggest that specific linguistic and cognitive factors significantly influence engagement in educational dialogues, with implications for improving second-language acquisition strategies.

**Abstract:** Engagement and motivation are crucial for second-language acquisition, yet maintaining learner interest in educational conversations remains a challenge. While prior research has explored what makes educational texts interesting, still little is known about the linguistic features that drive engagement in conversations. To address this gap, we introduce IntrEx, the first large dataset annotated for interestingness and expected interestingness in teacher-student interactions. Built upon the Teacher-Student Chatroom Corpus (TSCC), IntrEx extends prior work by incorporating sequence-level annotations, allowing for the study of engagement beyond isolated turns to capture how interest evolves over extended dialogues. We employ a rigorous annotation process with over 100 second-language learners, using a comparison-based rating approach inspired by reinforcement learning from human feedback (RLHF) to improve agreement. We investigate whether large language models (LLMs) can predict human interestingness judgments. We find that LLMs (7B/8B parameters) fine-tuned on interestingness ratings outperform larger proprietary models like GPT-4o, demonstrating the potential for specialised datasets to model engagement in educational settings. Finally, we analyze how linguistic and cognitive factors, such as concreteness, comprehensibility (readability), and uptake, influence engagement in educational dialogues.

</details>


### [74] [ParCzech4Speech: A New Speech Corpus Derived from Czech Parliamentary Data](https://arxiv.org/abs/2509.06675)

*Vladislav Stankov, Matyáš Kopp, Ondřej Bojar*

**Main category:** cs.CL

**Keywords:** speech modeling, speech recognition, Czech language, audio-text alignment, speech corpus

**Relevance Score:** 4

**TL;DR:** ParCzech4Speech 1.0 is a processed speech corpus designed for better speech modeling with improved audio-text alignment.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to provide a more reliable resource for speech recognition and synthesis tasks by enhancing the dataset extracted from Czech parliamentary speeches.

**Method:** The corpus was processed using WhisperX and Wav2Vec 2.0 to align audio recordings with their corresponding transcripts.

**Key Contributions:**

	1. Introduction of three processing variants (segmented, unsegmented, raw-alignment) for flexible use cases.
	2. Improved audio-text alignment using advanced processing techniques.
	3. Public availability of the dataset enhances research opportunities in speech modeling.

**Result:** The new version includes 2,695 hours of speech data with better alignment reliability, offered in three variants for different tasks.

**Limitations:** 

**Conclusion:** The dataset aims to facilitate advancements in speech recognition technologies and is accessible under a CC-BY license for public use.

**Abstract:** We introduce ParCzech4Speech 1.0, a processed version of the ParCzech 4.0 corpus, targeted at speech modeling tasks with the largest variant containing 2,695 hours. We combined the sound recordings of the Czech parliamentary speeches with the official transcripts. The recordings were processed with WhisperX and Wav2Vec 2.0 to extract automated audio-text alignment. Our processing pipeline improves upon the ParCzech 3.0 speech recognition version by extracting more data with higher alignment reliability. The dataset is offered in three flexible variants: (1) sentence-segmented for automatic speech recognition and speech synthesis tasks with clean boundaries, (2) unsegmented preserving original utterance flow across sentences, and (3) a raw-alignment for further custom refinement for other possible tasks. All variants maintain the original metadata and are released under a permissive CC-BY license. The dataset is available in the LINDAT repository, with the sentence-segmented and unsegmented variants additionally available on Hugging Face.

</details>


### [75] [Will Annotators Disagree? Identifying Subjectivity in Value-Laden Arguments](https://arxiv.org/abs/2509.06704)

*Amir Homayounirad, Enrico Liscio, Tong Wang, Catholijn M. Jonker, Luciano C. Siebert*

**Main category:** cs.CL

**Keywords:** subjectivity, human values, annotation, model performance, argumentation

**Relevance Score:** 6

**TL;DR:** This paper investigates methods for recognizing subjectivity in human values that influence arguments, aiming to enhance model performance in identifying subjective arguments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding and identifying subjectivity in arguments is crucial, especially in tasks where human values lead to diverse interpretations and potential disagreements among annotators.

**Method:** The authors evaluate two primary approaches: inferring subjectivity through value prediction and directly identifying subjectivity, utilizing experiments to assess efficacy.

**Key Contributions:**

	1. Introduction of direct subjectivity identification methods
	2. Comparison between inferring subjectivity and direct identification
	3. Insights on loss function performance related to subjectivity

**Result:** Direct subjectivity identification is shown to significantly enhance the model's ability to flag subjective arguments, while combining contrastive loss with binary cross-entropy loss does not yield performance improvement but reduces reliance on per-label subjectivity.

**Limitations:** 

**Conclusion:** The proposed methods can enrich the annotation process by allowing for nuanced interpretations of subjective arguments, ultimately contributing to better understanding of annotator disagreement.

**Abstract:** Aggregating multiple annotations into a single ground truth label may hide valuable insights into annotator disagreement, particularly in tasks where subjectivity plays a crucial role. In this work, we explore methods for identifying subjectivity in recognizing the human values that motivate arguments. We evaluate two main approaches: inferring subjectivity through value prediction vs. directly identifying subjectivity. Our experiments show that direct subjectivity identification significantly improves the model performance of flagging subjective arguments. Furthermore, combining contrastive loss with binary cross-entropy loss does not improve performance but reduces the dependency on per-label subjectivity. Our proposed methods can help identify arguments that individuals may interpret differently, fostering a more nuanced annotation process.

</details>


### [76] [Anchoring Refusal Direction: Mitigating Safety Risks in Tuning via Projection Constraint](https://arxiv.org/abs/2509.06795)

*Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Qika Lin, Kai He, Ting Liu, Bing Qin, Mengling Feng*

**Main category:** cs.CL

**Keywords:** Instruction Fine-Tuning, Large Language Models, safety risks, ProCon, refusal direction

**Relevance Score:** 9

**TL;DR:** This paper introduces ProCon, a method to mitigate safety risks in LLMs during Instruction Fine-Tuning by addressing the drift of the refusal direction in hidden states.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the safety risks of LLMs during Instruction Fine-Tuning, particularly their compromised ability to refuse malicious instructions.

**Method:** The ProCon method employs a projection-constrained loss term to regularize hidden states in the refusal direction, with an emphasis on early training stages and broadening data distribution to enhance constraint signals.

**Key Contributions:**

	1. Introduction of ProCon method to stabilize refusal direction
	2. Demonstrated effectiveness in mitigating safety risks during IFT
	3. Proposed warm-up strategy for enhanced performance

**Result:** Experimental results show that ProCon effectively reduces safety risks associated with Instruction Fine-Tuning while preserving performance across various datasets and models.

**Limitations:** Overall performance barriers remain a limiting factor for the method.

**Conclusion:** ProCon stabilizes the refusal direction during training, providing insights for future safety research in LLMs.

**Abstract:** Instruction Fine-Tuning (IFT) has been widely adopted as an effective post-training strategy to enhance various abilities of Large Language Models (LLMs). However, prior studies have shown that IFT can significantly compromise LLMs' safety, particularly their ability to refuse malicious instructions, raising significant concerns. Recent research into the internal mechanisms of LLMs has identified the refusal direction (r-direction) in the hidden states, which plays a pivotal role in governing refusal behavior. Building on this insight, our study reveals that the r-direction tends to drift during training, which we identify as one of the causes of the associated safety risks. To mitigate such drift, our proposed ProCon method introduces a projection-constrained loss term that regularizes the projection magnitude of each training sample's hidden state onto the r-direction. Our initial analysis shows that applying an appropriate constraint can effectively mitigate the refusal direction drift and associated safety risks, but remains limited by overall performance barriers. To overcome this barrier, informed by our observation of early-stage sharp drift and a data-driven perspective, we introduce a warm-up strategy that emphasizes early-stage strong constraints and broaden the data distribution to strengthen constraint signals, leading to an enhanced ProCon method. Experimental results under various datasets, scenarios, and LLMs demonstrate that our method can significantly mitigate safety risks posed by IFT while preserving task performance gains. Even compared with strong baselines, our method consistently delivers superior overall performance. Crucially, our analysis indicates that ProCon can contribute to stabilizing the r-direction during training, while such an interpretability-driven exploration of LLMs' internal mechanisms lays a solid foundation for future safety research.

</details>


### [77] [MachineLearningLM: Continued Pretraining Language Models on Millions of Synthetic Tabular Prediction Tasks Scales In-Context ML](https://arxiv.org/abs/2509.06806)

*Haoyu Dong, Pengkun Zhang, Mingzhe Lu, Yanzhen Shen, Guolin Ke*

**Main category:** cs.CL

**Keywords:** Large Language Models, In-Context Learning, Machine Learning, Healthcare, Structural Causal Models

**Relevance Score:** 9

**TL;DR:** MachineLearningLM enhances LLMs' in-context learning capability without gradient descent, achieving superior performance in ML tasks across various domains, including healthcare.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the ability of LLMs to learn from many in-context examples for standard ML tasks, without relying on gradient descent.

**Method:** A continued-pretraining framework synthesizing ML tasks from millions of structural causal models (SCMs) and using a random-forest teacher for distillation into the LLM.

**Key Contributions:**

	1. Introduces MachineLearningLM for improved in-context learning with LLMs
	2. Achieves state-of-the-art accuracy in ML tasks across diverse domains
	3. Maintains general knowledge and reasoning abilities during pretraining

**Result:** MachineLearningLM outperforms strong LLM baselines by an average of about 15% on out-of-distribution tabular classification, showing significant improvements in accuracy as in-context demonstrations increase.

**Limitations:** 

**Conclusion:** The framework successfully maintains general chat capabilities while enhancing performance in various ML tasks, demonstrating a significant many-shot scaling law.

**Abstract:** Large language models (LLMs) possess broad world knowledge and strong general-purpose reasoning ability, yet they struggle to learn from many in-context examples on standard machine learning (ML) tasks, that is, to leverage many-shot demonstrations purely via in-context learning (ICL) without gradient descent. We introduce MachineLearningLM, a portable continued-pretraining framework that equips a general-purpose LLM with robust in-context ML capability while preserving its general knowledge and reasoning for broader chat workflows.   Our pretraining procedure synthesizes ML tasks from millions of structural causal models (SCMs), spanning shot counts up to 1,024. We begin with a random-forest teacher, distilling tree-based decision strategies into the LLM to strengthen robustness in numerical modeling. All tasks are serialized with a token-efficient prompt, enabling 3x to 6x more examples per context window and delivering up to 50x amortized throughput via batch inference.   Despite a modest setup (Qwen-2.5-7B-Instruct with LoRA rank 8), MachineLearningLM outperforms strong LLM baselines (e.g., GPT-5-mini) by an average of about 15% on out-of-distribution tabular classification across finance, physics, biology, and healthcare domains. It exhibits a striking many-shot scaling law: accuracy increases monotonically as in-context demonstrations grow from 8 to 1,024. Without any task-specific training, it attains random-forest-level accuracy across hundreds of shots. General chat capabilities, including knowledge and reasoning, are preserved: it achieves 75.4% on MMLU.

</details>


### [78] [MoGU V2: Toward a Higher Pareto Frontier Between Model Usability and Security](https://arxiv.org/abs/2509.06807)

*Yanrui Du, Fenglei Fan, Sendong Zhao, Jiawei Cao, Ting Liu, Bing Qin*

**Main category:** cs.CL

**Keywords:** Large Language Models, security, usability, MoGU framework, machine learning

**Relevance Score:** 8

**TL;DR:** The paper introduces the MoGU framework to balance security and usability in Large Language Models, improving upon it with MoGU_v2 for better adaptability and performance while addressing security risks without sacrificing usability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** With LLMs increasingly integrated into everyday life, ensuring they respond harmlessly to malicious prompts is crucial. Existing methods prioritize security, often at the expense of usability.

**Method:** The MoGU framework uses a dynamic intra-layer router to balance between security-optimized and usability-optimized model variants. An improved version, MoGU_v2, enhances this by tightening the connection between routers and hidden states.

**Key Contributions:**

	1. Introduction of the MoGU framework for balancing security and usability in LLMs.
	2. Development of MoGU_v2 with improved adaptability and tighter router integration.
	3. Robust performance of MoGU_v2 across various types of LLM applications.

**Result:** MoGU_v2 demonstrates strong adaptability and significant stability improvements across a range of LLMs, including mainstream and resource-constrained models, while effectively addressing security risks introduced by Instruction Fine-tuning.

**Limitations:** Parameter redundancy and performance bottlenecks in the initial MoGU framework.

**Conclusion:** MoGU_v2 represents a versatile solution to enhance LLM security in practical applications without compromising their usability.

**Abstract:** As Large Language Models (LLMs) increasingly permeate human life, their security has emerged as a critical concern, particularly their ability to maintain harmless responses to malicious instructions. Although extensive methods have improved LLMs' security, they often lead to conservative, rejection-oriented responses that compromise practical usability. This presents a key challenge: how to advance the Pareto frontier between LLMs' usability and security, rather than necessitate a trade-off between them. To address this, we propose the MoGU framework, in which the intra-layer router dynamically allocates weights by sensing hidden states, thereby balancing the contributions of security-optimized and usability-optimized variants. Despite its initial potential, the MoGU framework faces limitations such as parameter redundancy and performance bottlenecks. To overcome these, we further propose an improved MoGU_v2 framework that establishes a tighter coupling between the routers and hidden states. In MoGU_v2, routers are embedded only in layers encoding highly classifiable security features, and backbone modules are activated during router optimization to enable bidirectional adaptation. MoGU_V2 exhibits strong adaptability and stable improvements across various series of LLMs, including mainstream LLMs serving as brains in various applications, on-device LLMs optimized for resource-constrained scenarios, and reasoning LLMs tailored for user interpretability. Meanwhile, even facing risks introduced by Instruction Fine-tuning, MoGU_v2 can easily restore security without compromising the task performance gains via a simple data-mix strategy. These comprehensive improvements highlight MoGU_V2 as a robust and versatile solution for mitigating security risks in real-world applications.

</details>


### [79] [Saturation-Driven Dataset Generation for LLM Mathematical Reasoning in the TPTP Ecosystem](https://arxiv.org/abs/2509.06809)

*Valentin Quesnel, Damien Sileo*

**Main category:** cs.CL

**Keywords:** Large Language Models, automated theorem proving, symbolic training data

**Relevance Score:** 8

**TL;DR:** The paper addresses the issue of low-quality data in mathematical reasoning for LLMs by developing a scalable data engine derived from automated theorem proving, generating error-free symbolic data for training.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenge of scarce, high-quality data that limits the mathematical reasoning capabilities of Large Language Models.

**Method:** The framework uses E-prover's saturation capabilities on the TPTP axiom library to create a corpus of guaranteed-valid theorems, which are then filtered and transformed into tasks.

**Key Contributions:**

	1. A scalable data engine leveraging automated theorem proving for LLM training.
	2. Creation of a corpus of guaranteed-valid theorems without LLM intervention.
	3. Development of difficulty-controlled reasoning challenges.

**Result:** The framework eliminates factual errors by construction and generates three types of reasoning challenges. Zero-shot experiments show LLMs struggle with deep, structural reasoning.

**Limitations:** The experiments reveal a significant performance drop in LLMs on tasks that require deep reasoning.

**Conclusion:** The proposed framework serves as both a diagnostic tool for reasoning capabilities and a scalable source of symbolic training data, with code and data publicly available.

**Abstract:** The scarcity of high-quality, logically sound data is a critical bottleneck for advancing the mathematical reasoning of Large Language Models (LLMs). Our work confronts this challenge by turning decades of automated theorem proving research into a scalable data engine. Rather than relying on error-prone LLMs or complex proof-assistant syntax like Lean and Isabelle, our framework leverages E-prover's saturation capabilities on the vast TPTP axiom library to derive a massive, guaranteed-valid corpus of theorems. Our pipeline is principled and simple: saturate axioms, filter for "interesting" theorems, and generate tasks. With no LLMs in the loop, we eliminate factual errors by construction. This purely symbolic data is then transformed into three difficulty-controlled challenges: entailment verification, premise selection, and proof reconstruction. Our zero-shot experiments on frontier models reveal a clear weakness: performance collapses on tasks requiring deep, structural reasoning. Our framework provides both the diagnostic tool to measure this gap and a scalable source of symbolic training data to address it. We make the code and data publicly available.   https://github.com/sileod/reasoning_core https://hf.co/datasets/reasoning-core/rc1

</details>


### [80] [A Comparative Benchmark of Large Language Models for Labelling Wind Turbine Maintenance Logs](https://arxiv.org/abs/2509.06813)

*Max Malyi, Jonathan Shek, Alasdair McDonald, Andre Biscaya*

**Main category:** cs.CL

**Keywords:** Large Language Models, wind power, maintenance logs, human-in-the-loop, data labeling

**Relevance Score:** 7

**TL;DR:** This paper presents a framework for classifying wind turbine maintenance logs using LLMs, promotes its open-source availability, and evaluates the performance of various models in this context.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The significant barrier to automated analysis of unstructured maintenance logs impacts the effective operation and maintenance of wind power, influencing the Levelised Cost of Energy.

**Method:** A reproducible framework was developed to benchmark LLMs on the classification of wind turbine maintenance logs, analyzing both proprietary and open-source models.

**Key Contributions:**

	1. Development of a novel benchmarking framework for LLMs
	2. Open-source availability of the framework
	3. Identification of performance hierarchy among LLMs for maintenance log classification.

**Result:** The study identifies a performance hierarchy among LLMs, showing that classification accuracy varies with task semantic ambiguity, and emphasizes the effectiveness of using LLMs in conjunction with human experts for better data quality.

**Limitations:** No model achieves perfect accuracy; calibration significantly varies among models.

**Conclusion:** LLMs should be integrated into a Human-in-the-Loop system to assist experts in labeling data, enhancing the quality of operation and maintenance data and improving reliability analysis.

**Abstract:** Effective Operation and Maintenance (O&M) is critical to reducing the Levelised Cost of Energy (LCOE) from wind power, yet the unstructured, free-text nature of turbine maintenance logs presents a significant barrier to automated analysis. Our paper addresses this by presenting a novel and reproducible framework for benchmarking Large Language Models (LLMs) on the task of classifying these complex industrial records. To promote transparency and encourage further research, this framework has been made publicly available as an open-source tool. We systematically evaluate a diverse suite of state-of-the-art proprietary and open-source LLMs, providing a foundational assessment of their trade-offs in reliability, operational efficiency, and model calibration. Our results quantify a clear performance hierarchy, identifying top models that exhibit high alignment with a benchmark standard and trustworthy, well-calibrated confidence scores. We also demonstrate that classification performance is highly dependent on the task's semantic ambiguity, with all models showing higher consensus on objective component identification than on interpretive maintenance actions. Given that no model achieves perfect accuracy and that calibration varies dramatically, we conclude that the most effective and responsible near-term application is a Human-in-the-Loop system, where LLMs act as a powerful assistant to accelerate and standardise data labelling for human experts, thereby enhancing O&M data quality and downstream reliability analysis.

</details>


### [81] [COMPACT: Common-token Optimized Model Pruning Across Channels and Tokens](https://arxiv.org/abs/2509.06836)

*Eugene Kwek, Wenpeng Yin*

**Main category:** cs.CL

**Keywords:** LLMs, pruning, efficiency, memory, latency

**Relevance Score:** 9

**TL;DR:** This paper presents COMPACT, a pruning method for LLMs that improves efficiency in memory and latency while maintaining competitive performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the efficiency of LLMs in memory usage, latency, and serving costs, particularly for edge deployment and interactive applications.

**Method:** COMPACT jointly prunes rare vocabulary and removes intermediate channels in FFNs using common-token-weighted activations, aligning importance with the token distribution after pruning.

**Key Contributions:**

	1. Introduces a novel COMPACT pruning method for LLMs.
	2. Maintains a standard transformer architecture while enabling effective pruning.
	3. Demonstrates strong performance metrics across various LLM families.

**Result:** State-of-the-art performance observed in downstream tasks with reduced parameters and memory usage, while also lowering end-to-end latency.

**Limitations:** 

**Conclusion:** COMPACT combines the strengths of depth and width pruning without altering transformer architecture, achieving strong performance with significant memory and latency improvements.

**Abstract:** Making LLMs more efficient in memory, latency, and serving cost is crucial for edge deployment, interactive applications, and sustainable inference at scale. Pruning is a key technique toward this goal. However, prior pruning methods are limited: width pruning often breaks the standard transformer layout or requires custom inference code, while depth pruning removes entire layers and can cause abrupt accuracy drops. In this work, we propose COMPACT, which jointly (i) prunes rare vocabulary to shrink embedding/unembedding and (ii) prunes FFN intermediate channels using common-token-weighted activations, aligning importance with the post-pruning token distribution. COMPACT enjoys merits of both depth and width pruning, such as: deployment-friendliness (keeps a standard transformer architecture), scale-adaptivity (trade off vocab vs. FFN pruning), training-free operation with competitive pruning time, and strong memory savings alongside throughput gains. Experiments across Qwen, LLaMA, and Gemma families (0.5B-70B) show state-of-the-art downstream task performance at similar or higher pruning ratios, with substantial reductions in parameters, GPU memory, and end-to-end latency.

</details>


### [82] [EPT Benchmark: Evaluation of Persian Trustworthiness in Large Language Models](https://arxiv.org/abs/2509.06838)

*Mohammad Reza Mirbagheri, Mohammad Mahdi Mirkamali, Zahra Motoshaker Arani, Ali Javeri, Amir Mahdi Sadeghzadeh, Rasool Jalili*

**Main category:** cs.CL

**Keywords:** Large Language Models, trustworthiness, evaluation metric, cultural values, AI ethics

**Relevance Score:** 9

**TL;DR:** This study introduces the EPT metric for assessing the trustworthiness of Large Language Models (LLMs) across various ethical and cultural dimensions, revealing significant deficiencies particularly in safety.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical challenge of ensuring the trustworthiness of LLMs, focusing on their alignment with ethical, cultural, and social values.

**Method:** We developed the EPT metric and evaluated LLMs like ChatGPT and Claude using a labeled dataset and both automated and human assessments.

**Key Contributions:**

	1. Introduction of the EPT metric for assessing LLM trustworthiness
	2. Evaluation of multiple leading models against cultural and ethical benchmarks
	3. Public availability of the dataset for further research

**Result:** The evaluation showed significant deficiencies in the safety dimension among the tested models, indicating a need for improvement in this area.

**Limitations:** Focused primarily on Persian ethical-cultural values; applicability to other cultures may vary.

**Conclusion:** There are critical gaps in the trustworthiness of LLMs concerning Persian cultural values that require attention to create responsible AI systems.

**Abstract:** Large Language Models (LLMs), trained on extensive datasets using advanced deep learning architectures, have demonstrated remarkable performance across a wide range of language tasks, becoming a cornerstone of modern AI technologies. However, ensuring their trustworthiness remains a critical challenge, as reliability is essential not only for accurate performance but also for upholding ethical, cultural, and social values. Careful alignment of training data and culturally grounded evaluation criteria are vital for developing responsible AI systems. In this study, we introduce the EPT (Evaluation of Persian Trustworthiness) metric, a culturally informed benchmark specifically designed to assess the trustworthiness of LLMs across six key aspects: truthfulness, safety, fairness, robustness, privacy, and ethical alignment. We curated a labeled dataset and evaluated the performance of several leading models - including ChatGPT, Claude, DeepSeek, Gemini, Grok, LLaMA, Mistral, and Qwen - using both automated LLM-based and human assessments. Our results reveal significant deficiencies in the safety dimension, underscoring the urgent need for focused attention on this critical aspect of model behavior. Furthermore, our findings offer valuable insights into the alignment of these models with Persian ethical-cultural values and highlight critical gaps and opportunities for advancing trustworthy and culturally responsible AI. The dataset is publicly available at: https://github.com/Rezamirbagheri110/EPT-Benchmark.

</details>


### [83] [The Majority is not always right: RL training for solution aggregation](https://arxiv.org/abs/2509.06870)

*Wenting Zhao, Pranjal Aggarwal, Swarnadeep Saha, Asli Celikyilmaz, Jason Weston, Ilia Kulikov*

**Main category:** cs.CL

**Keywords:** aggregation, large language models, reinforcement learning, reasoning tasks, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces AggLM, a model that learns to aggregate solutions for large language models using reinforcement learning, improving performance on reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of aggregating multiple independent solutions from large language models, addressing limitations of traditional methods like majority voting.

**Method:** AggLM is trained to review and synthesize outputs from various solutions using reinforcement learning, balancing easy and hard examples during training.

**Key Contributions:**

	1. Introduction of AggLM for learning aggregation as a reasoning skill.
	2. Demonstration of improved performance over traditional aggregation methods like majority voting.
	3. Effective generalization to solutions from stronger models with fewer token requirements.

**Result:** AggLM surpasses both rule-based and reward-model baselines across multiple benchmarks and generalizes well to outputs from different, and often stronger, models using fewer tokens than traditional methods.

**Limitations:** 

**Conclusion:** The proposed approach shows significant improvements in aggregating reasoning results from LLMs, making it a valuable method for leveraging multiple outputs effectively.

**Abstract:** Scaling up test-time compute, by generating multiple independent solutions and selecting or aggregating among them, has become a central paradigm for improving large language models (LLMs) on challenging reasoning tasks. While most prior work relies on simple majority voting or reward model ranking to aggregate solutions, these approaches may only yield limited benefits. In this work, we propose to learn aggregation as an explicit reasoning skill: given a set of candidate solutions, we train an aggregator model to review, reconcile, and synthesize a final, correct answer using reinforcement learning from verifiable rewards. A key ingredient is careful balancing of easy and hard training examples, allowing the model to learn both to recover minority-but-correct answers as well as easy majority-correct answers. Empirically, we find our method, AggLM, outperforms both strong rule-based and reward-model baselines, across multiple benchmarks. Furthermore, it generalizes effectively to solutions from differing models, including stronger ones than contained in the training data, all while requiring substantially fewer tokens than majority voting with larger numbers of solutions.

</details>


### [84] [UNH at CheckThat! 2025: Fine-tuning Vs Prompting in Claim Extraction](https://arxiv.org/abs/2509.06883)

*Joe Wilder, Nikhil Kadapala, Benji Xu, Mohammed Alsaadi, Aiden Parsons, Mitchell Rogers, Palash Agarwal, Adam Hassick, Laura Dietz*

**Main category:** cs.CL

**Keywords:** prompting, in-context learning, claim extraction, LLM, social media

**Relevance Score:** 6

**TL;DR:** This paper explores methods for extracting check-worthy claims from social media using LLMs, achieving the best results with a fine-tuned FLAN-T5 model while noting alternative methods may yield higher quality claims despite lower METEOR scores.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and extract check-worthy claims from social media passages effectively using advanced prompting methods and in-context learning with LLMs.

**Method:** We evaluate different prompting techniques, including few-shot prompting and fine-tuning across various LLM families, focusing on their effectiveness in extracting claims.

**Key Contributions:**

	1. Experimentation with multiple prompting techniques for LLMs
	2. Identification of quality vs. score trade-offs in claim extraction
	3. Application of LLMs to specific task of check-worthy claim identification

**Result:** The highest METEOR score was attained with a fine-tuned FLAN-T5 model; however, other methods occasionally extracted higher-quality claims despite lower scores.

**Limitations:** No explicit limitations mentioned in the abstract.

**Conclusion:** While fine-tuning yields strong METEOR scores, alternative extraction methods can result in better quality claims, indicating a trade-off between score and claim quality.

**Abstract:** We participate in CheckThat! Task 2 English and explore various methods of prompting and in-context learning, including few-shot prompting and fine-tuning with different LLM families, with the goal of extracting check-worthy claims from social media passages. Our best METEOR score is achieved by fine-tuning a FLAN-T5 model. However, we observe that higher-quality claims can sometimes be extracted using other methods, even when their METEOR scores are lower.

</details>


### [85] [mmBERT: A Modern Multilingual Encoder with Annealed Language Learning](https://arxiv.org/abs/2509.06888)

*Marc Marone, Orion Weller, William Fleshman, Eugene Yang, Dawn Lawrie, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** multilingual models, encoder-only models, machine learning, classification, low-resource languages

**Relevance Score:** 7

**TL;DR:** mmBERT is a novel encoder-only language model pretrained on multilingual text, achieving high performance in classification and retrieval tasks across both high and low-resource languages.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There has been a lack of recent research on encoder models, especially multilingual models in the context of various machine learning tasks.

**Method:** mmBERT was built using an inverse mask ratio schedule and inverse temperature sampling ratio, integrating over 1700 low-resource languages only during the decay phase.

**Key Contributions:**

	1. Introduction of mmBERT as a pretrained multilingual encoder-only model
	2. Use of inverse mask ratio schedule and inverse temperature sampling ratio for improved training
	3. Significant performance improvements in classification tasks for high and low-resource languages.

**Result:** mmBERT demonstrates significant performance boosts, achieving classification results comparable to state-of-the-art models like OpenAI's o3 and Google's Gemini 2.5 Pro, even when low-resource languages are included only during a limited phase.

**Limitations:** 

**Conclusion:** The introduction of mmBERT highlights the potential of enhanced multilingual training strategies to improve model performance in NLP tasks.

**Abstract:** Encoder-only languages models are frequently used for a variety of standard machine learning tasks, including classification and retrieval. However, there has been a lack of recent research for encoder models, especially with respect to multilingual models. We introduce mmBERT, an encoder-only language model pretrained on 3T tokens of multilingual text in over 1800 languages. To build mmBERT we introduce several novel elements, including an inverse mask ratio schedule and an inverse temperature sampling ratio. We add over 1700 low-resource languages to the data mix only during the decay phase, showing that it boosts performance dramatically and maximizes the gains from the relatively small amount of training data. Despite only including these low-resource languages in the short decay phase we achieve similar classification performance to models like OpenAI's o3 and Google's Gemini 2.5 Pro. Overall, we show that mmBERT significantly outperforms the previous generation of models on classification and retrieval tasks -- on both high and low-resource languages.

</details>


### [86] [Proof-Carrying Numbers (PCN): A Protocol for Trustworthy Numeric Answers from LLMs via Claim Verification](https://arxiv.org/abs/2509.06902)

*Aivin V. Solatorio*

**Main category:** cs.CL

**Keywords:** Large Language Models, numeric hallucination, Proof-Carrying Numbers, mechanical verification, trust in AI

**Relevance Score:** 8

**TL;DR:** This paper proposes Proof-Carrying Numbers (PCN), a protocol to ensure numeric fidelity in Large Language Models (LLMs) by using mechanical verification and separating the verification process from the model.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of numeric hallucination in LLMs, where generated numbers may deviate from actual data, leading to misinformation and lack of trust in the outputs.

**Method:** PCN uses claim-bound tokens associated with structured claims that can be verified by a verifier according to specified policies, enforcing that only verified numbers are displayed as credible.

**Key Contributions:**

	1. Introduction of Proof-Carrying Numbers (PCN) for numeric fidelity in LLMs
	2. Verification layered in the renderer instead of the model to mitigate spoofing
	3. Lightweight and model-agnostic integration into existing applications

**Result:** PCN is proved to be sound, complete under honest tokens, ensures fail-closed behavior, and maintains monotonicity with policy refinement, making it efficient and model-agnostic for integration.

**Limitations:** 

**Conclusion:** The protocol establishes a trust model based on proof, where numeric claims must be verified before presentation, reducing the risk of misinformation through unverified data.

**Abstract:** Large Language Models (LLMs) as stochastic systems may generate numbers that deviate from available data, a failure known as \emph{numeric hallucination}. Existing safeguards -- retrieval-augmented generation, citations, and uncertainty estimation -- improve transparency but cannot guarantee fidelity: fabricated or misquoted values may still be displayed as if correct. We propose \textbf{Proof-Carrying Numbers (PCN)}, a presentation-layer protocol that enforces numeric fidelity through mechanical verification. Under PCN, numeric spans are emitted as \emph{claim-bound tokens} tied to structured claims, and a verifier checks each token under a declared policy (e.g., exact equality, rounding, aliases, or tolerance with qualifiers). Crucially, PCN places verification in the \emph{renderer}, not the model: only claim-checked numbers are marked as verified, and all others default to unverified. This separation prevents spoofing and guarantees fail-closed behavior. We formalize PCN and prove soundness, completeness under honest tokens, fail-closed behavior, and monotonicity under policy refinement. PCN is lightweight and model-agnostic, integrates seamlessly into existing applications, and can be extended with cryptographic commitments. By enforcing verification as a mandatory step before display, PCN establishes a simple contract for numerically sensitive settings: \emph{trust is earned only by proof}, while the absence of a mark communicates uncertainty.

</details>


### [87] [Beyond Two-Stage Training: Cooperative SFT and RL for LLM Reasoning](https://arxiv.org/abs/2509.06948)

*Liang Chen, Xueting Han, Li Shen, Jing Bai, Kam-Fai Wong*

**Main category:** cs.CL

**Keywords:** reinforcement learning, supervised fine-tuning, bilevel optimization, large language models, reasoning

**Relevance Score:** 9

**TL;DR:** This study presents a method that combines supervised fine-tuning and reinforcement learning in a bilevel optimization framework to improve reasoning abilities of language models more efficiently.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in reinforcement learning when applied to large language models by integrating it with supervised fine-tuning effectively.

**Method:** A bilevel optimization approach is introduced, conditioning the supervised fine-tuning objective on the optimal reinforcement learning policy, allowing SFT to guide RL's optimization process while training.

**Key Contributions:**

	1. Introduced a novel bilevel optimization framework for training language models.
	2. Demonstrated the effectiveness of joint training of SFT and RL on reasoning benchmarks.
	3. Achieved improvements in both efficiency and effectiveness compared to traditional decoupled approaches.

**Result:** Empirical evaluations on five reasoning benchmarks showed consistent improvements over baselines, achieving a superior balance between effectiveness and efficiency in training.

**Limitations:** No specific limitations mentioned.

**Conclusion:** The proposed method enhances the collaboration between supervised fine-tuning and reinforcement learning, leading to better performance in language models' reasoning capabilities.

**Abstract:** Reinforcement learning (RL) has proven effective in incentivizing the reasoning abilities of large language models (LLMs), but suffers from severe efficiency challenges due to its trial-and-error nature. While the common practice employs supervised fine-tuning (SFT) as a warm-up stage for RL, this decoupled two-stage approach limits interaction between SFT and RL, thereby constraining overall effectiveness. This study introduces a novel method for learning reasoning models that employs bilevel optimization to facilitate better cooperation between these training paradigms. By conditioning the SFT objective on the optimal RL policy, our approach enables SFT to meta-learn how to guide RL's optimization process. During training, the lower level performs RL updates while simultaneously receiving SFT supervision, and the upper level explicitly maximizes the cooperative gain-the performance advantage of joint SFT-RL training over RL alone. Empirical evaluations on five reasoning benchmarks demonstrate that our method consistently outperforms baselines and achieves a better balance between effectiveness and efficiency.

</details>


### [88] [Revolutionizing Reinforcement Learning Framework for Diffusion Large Language Models](https://arxiv.org/abs/2509.06949)

*Yinjie Wang, Ling Yang, Bowen Li, Ye Tian, Ke Shen, Mengdi Wang*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Diffusion Language Models, Mathematical Reasoning

**Relevance Score:** 8

**TL;DR:** TraceRL is a reinforcement learning framework for diffusion language models (DLMs) enhancing performance in math and coding tasks with new state-of-the-art models, TraDo.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved reasoning performance in complex tasks using diffusion language models, particularly for math and coding.

**Method:** Introduces TraceRL, a trajectory-aware reinforcement learning framework that utilizes a diffusion-based value model to enhance training stability and sampling flexibility.

**Key Contributions:**

	1. Proposes TraceRL for trajectory-aware reinforcement learning in DLMs.
	2. Demonstrates state-of-the-art performance with TraDo models on math and coding tasks.
	3. Provides an open-source framework for building and deploying diffusion LLMs.

**Result:** TraDo-4B-Instruct and TraDo-8B-Instruct models demonstrate significant accuracy improvements over baseline models on mathematical reasoning tasks, achieving 6.1% and 51.3% gains respectively.

**Limitations:** 

**Conclusion:** TraceRL enables the creation of advanced diffusion language models with superior performance on math reasoning and coding tasks, while providing a comprehensive framework for practical applications and reproducible research.

**Abstract:** We propose TraceRL, a trajectory-aware reinforcement learning framework for diffusion language models (DLMs) that incorporates preferred inference trajectory into post-training, and is applicable across different architectures. Equipped with a diffusion-based value model that enhances training stability, we demonstrate improved reasoning performance on complex math and coding tasks. Besides, it can also be applied to adapt block-specific models to larger blocks, which improves sampling flexibility. Employing TraceRL, we derive a series of state-of-the-art diffusion language models, namely TraDo. Although smaller than 7B-scale AR models, TraDo-4B-Instruct still consistently outperforms them across complex math reasoning tasks. TraDo-8B-Instruct achieves relative accuracy improvements of 6.1% over Qwen2.5-7B-Instruct and 51.3% over Llama3.1-8B-Instruct on mathematical reasoning benchmarks. Through curriculum learning, we also derive the first long-CoT DLM, outperforming Qwen2.5-7B-Instruct on MATH500 with an 18.1% relative accuracy gain. To facilitate reproducible research and practical applications, we release a comprehensive open-source framework for building, training, and deploying diffusion LLMs across diverse architectures. The framework integrates accelerated KV-cache techniques and inference engines for both inference and reinforcement learning, and includes implementations of various supervised fine-tuning and RL methods for mathematics, coding, and general tasks. Code and Models: https://github.com/Gen-Verse/dLLM-RL

</details>


### [89] [On the Same Wavelength? Evaluating Pragmatic Reasoning in Language Models across Broad Concepts](https://arxiv.org/abs/2509.06952)

*Linlu Qiu, Cedegao E. Zhang, Joshua B. Tenenbaum, Yoon Kim, Roger P. Levy*

**Main category:** cs.CL

**Keywords:** language models, pragmatics, Bayesian reasoning

**Relevance Score:** 8

**TL;DR:** This paper evaluates the pragmatic reasoning abilities of language models (LMs) using a framework inspired by a communication game, and explores improvements through Bayesian pragmatic reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of language models as conversational agents necessitates a better understanding of their pragmatic reasoning capabilities in context.

**Method:** An evaluation framework based on the Wavelength communication game is used to analyze LMs' performance in language comprehension and production, employing direct prompting and Chain-of-Thought (CoT) prompting, alongside a Rational Speech Act (RSA) approach.

**Key Contributions:**

	1. Evaluation framework derived from a communication game
	2. Findings show strong LM performance in language comprehension
	3. RSA significantly improves LM pragmatic reasoning abilities

**Result:** State-of-the-art LMs perform well in language comprehension, mirroring human accuracy. CoT prompting enhances language production outcomes, with RSA offering significant improvements over traditional methods.

**Limitations:** 

**Conclusion:** The study reveals strengths and limitations in LMs' pragmatic reasoning, highlighting the effectiveness of RSA in enhancing their capabilities and suggesting future research directions.

**Abstract:** Language use is shaped by pragmatics -- i.e., reasoning about communicative goals and norms in context. As language models (LMs) are increasingly used as conversational agents, it becomes ever more important to understand their pragmatic reasoning abilities. We propose an evaluation framework derived from Wavelength, a popular communication game where a speaker and a listener communicate about a broad range of concepts in a granular manner. We study a range of LMs on both language comprehension and language production using direct and Chain-of-Thought (CoT) prompting, and further explore a Rational Speech Act (RSA) approach to incorporating Bayesian pragmatic reasoning into LM inference. We find that state-of-the-art LMs, but not smaller ones, achieve strong performance on language comprehension, obtaining similar-to-human accuracy and exhibiting high correlations with human judgments even without CoT prompting or RSA. On language production, CoT can outperform direct prompting, and using RSA provides significant improvements over both approaches. Our study helps identify the strengths and limitations in LMs' pragmatic reasoning abilities and demonstrates the potential for improving them with RSA, opening up future avenues for understanding conceptual representation, language understanding, and social reasoning in LMs and humans.

</details>


### [90] [Multiple Noises in Diffusion Model for Semi-Supervised Multi-Domain Translation](https://arxiv.org/abs/2309.14394)

*Tsiry Mayet, Simon Bernard, Romain Herault, Clement Chatelain*

**Main category:** cs.CL

**Keywords:** multi-domain translation, semi-supervised learning, diffusion models

**Relevance Score:** 4

**TL;DR:** This paper presents a method called Multi-Domain Diffusion (MDD) for efficient and flexible multi-domain translation while enabling semi-supervised learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of multi-domain translation by creating a model that can learn mappings between various configurations of domains without requiring separate models for each case, enhancing efficiency and flexibility.

**Method:** The proposed MDD method uses diffusion models to manipulate noise levels for different domains, enabling reconstruction of missing views and supporting semi-supervised learning through a unified approach.

**Key Contributions:**

	1. Introduction of Multi-Domain Diffusion (MDD) for flexible domain translation.
	2. Unified handling of semi-supervised learning and domain translation.
	3. Performance validation on multiple challenging datasets.

**Result:** MDD demonstrated effectiveness in domain translation tasks across multiple datasets, showing improved performance compared to existing methods.

**Limitations:** 

**Conclusion:** The MDD approach simplifies the translation process across multiple domains while inherently incorporating semi-supervised learning capabilities without needing additional modifications.

**Abstract:** In this work, we address the challenge of multi-domain translation, where the objective is to learn mappings between arbitrary configurations of domains within a defined set (such as $(D_1, D_2)\rightarrow{}D_3$, $D_2\rightarrow{}(D_1, D_3)$, $D_3\rightarrow{}D_1$, etc. for three domains) without the need for separate models for each specific translation configuration, enabling more efficient and flexible domain translation. We introduce Multi-Domain Diffusion (MDD), a method with dual purposes: i) reconstructing any missing views for new data objects, and ii) enabling learning in semi-supervised contexts with arbitrary supervision configurations. MDD achieves these objectives by exploiting the noise formulation of diffusion models, specifically modeling one noise level per domain. Similar to existing domain translation approaches, MDD learns the translation between any combination of domains. However, unlike prior work, our formulation inherently handles semi-supervised learning without modification by representing missing views as noise in the diffusion process. We evaluate our approach through domain translation experiments on BL3NDT, a multi-domain synthetic dataset designed for challenging semantic domain inversion, the BraTS2020 dataset, and the CelebAMask-HQ dataset.

</details>


### [91] [Support or Refute: Analyzing the Stance of Evidence to Detect Out-of-Context Mis- and Disinformation](https://arxiv.org/abs/2311.01766)

*Xin Yuan, Jie Guo, Weidong Qiu, Zheng Huang, Shujun Li*

**Main category:** cs.CL

**Keywords:** misinformation, stance extraction, multi-modal evidence, machine learning, HCI

**Relevance Score:** 5

**TL;DR:** The paper introduces a stance extraction network (SEN) to address out-of-context mis- and disinformation by evaluating multi-modal evidence stances.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing prevalence of mis- and disinformation online necessitates new methods to identify and mitigate their impact, particularly targeting out-of-context information.

**Method:** The authors propose a stance extraction network (SEN) that extracts the stances of various pieces of multi-modal evidence, additionally integrating a support-refutation score based on named entity co-occurrence.

**Key Contributions:**

	1. Introduction of the stance extraction network (SEN) for multi-modal evidence evaluation.
	2. Integration of a support-refutation score based on entity co-occurrence.
	3. Demonstrated improved performance over state-of-the-art baselines.

**Result:** Extensive experiments on a large dataset show SEN outperformed existing models, achieving a 3.2% improvement in accuracy.

**Limitations:** The reliance on the quality of input data and the potential for varying performance across different contexts.

**Conclusion:** The proposed SEN provides a more nuanced approach to tackling out-of-context mis- and disinformation, suggesting that stance analysis can enhance detection performance.

**Abstract:** Mis- and disinformation online have become a major societal problem as major sources of online harms of different kinds. One common form of mis- and disinformation is out-of-context (OOC) information, where different pieces of information are falsely associated, e.g., a real image combined with a false textual caption or a misleading textual description. Although some past studies have attempted to defend against OOC mis- and disinformation through external evidence, they tend to disregard the role of different pieces of evidence with different stances. Motivated by the intuition that the stance of evidence represents a bias towards different detection results, we propose a stance extraction network (SEN) that can extract the stances of different pieces of multi-modal evidence in a unified framework. Moreover, we introduce a support-refutation score calculated based on the co-occurrence relations of named entities into the textual SEN. Extensive experiments on a public large-scale dataset demonstrated that our proposed method outperformed the state-of-the-art baselines, with the best model achieving a performance gain of 3.2% in accuracy. The source code and checkpoints are publicly available at https://github.com/yx3266/SEN.

</details>


### [92] [Grammaticality illusion or ambiguous interpretation? Event-related potentials reveal the nature of the missing-NP effect in Mandarin centre-embedded structures](https://arxiv.org/abs/2402.11282)

*Qihang Yang, Caimei Yang, Yu Liao, Ziman Zhuang*

**Main category:** cs.CL

**Keywords:** grammaticality illusion, Mandarin, EEG, ambiguity, double centre-embedded structures

**Relevance Score:** 2

**TL;DR:** The paper examines grammaticality illusions in Mandarin involving double centre-embedded structures, proposing that ambiguous verb interpretations rather than grammaticality account for these illusions, supported by EEG experiments.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To resolve the lack of consensus on the nature of grammaticality illusions in Mandarin double centre-embedded structures.

**Method:** Conducted two EEG experiments on quasi double centre-embedded structures while manipulating sentence complexity and semantic cues.

**Key Contributions:**

	1. Proposes a novel interpretation of grammaticality illusions in Mandarin
	2. Uses EEG experiments to provide empirical support
	3. Discusses implications for cross-linguistic understanding of sentence structure

**Result:** Experiment 1 indicated the presence of grammaticality illusions with the absence of P600 and presence of N400 effects; Experiment 2 showed the dispelling of these illusions when providing semantic cues, evidenced by a P600 effect.

**Limitations:** 

**Conclusion:** The study validates that ambiguous verb interpretations contribute to the illusion, with implications for garden-path theory, and highlights the influence of word-order differences in cross-linguistic variation.

**Abstract:** In several languages, omitting a verb phrase (VP) in double centre-embedded structures creates a grammaticality illusion. Similar illusion also exhibited in Mandarin missing-NP double centre-embedded structures. However, there is no consensus on its very nature. Instead of treating it as grammaticality illusion, we argue that ambiguous interpretations of verbs can best account for this phenomenon in Mandarin. To further support this hypothesis, we conducted two electroencephalography (EEG) experiments on quasi double centre-embedded structures whose complexity is reduced by placing the self-embedding relative clauses into the sentence's subject position. Experiment 1 showed that similar phenomenon even exhibited in this structure, evidenced by an absence of P600 effect and a presence of N400 effect. In Experiment 2, providing semantic cues to reduce ambiguity dispelled this illusion, as evidenced by a P600 effect. We interpret the results under garden-path theory and propose that word-order difference may account for this cross-linguistic variation.

</details>


### [93] [Repetition Improves Language Model Embeddings](https://arxiv.org/abs/2402.15449)

*Jacob Mitchell Springer, Suhas Kotha, Daniel Fried, Graham Neubig, Aditi Raghunathan*

**Main category:** cs.CL

**Keywords:** text embeddings, autoregressive models, NLP

**Relevance Score:** 7

**TL;DR:** This paper introduces echo embeddings, a method to adapt autoregressive language models into effective text embedding models without altering their architecture or requiring fine-tuning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To demonstrate that autoregressive language models can be effectively transformed into high-quality text embedding models without necessitating architecture modifications.

**Method:** The method involves repeating the input tokens and extracting embeddings from these repeated tokens, allowing the model to utilize information from all original tokens.

**Key Contributions:**

	1. Introduction of echo embeddings
	2. Improvement over classical LM embeddings
	3. Compatibility with supervised fine-tuning

**Result:** Echo embeddings improve over classical LM embeddings by over 5% in zero-shot settings and nearly match embeddings from bidirectionally-converted LMs trained with additional masked-language modeling.

**Limitations:** 

**Conclusion:** Repetition is a simple yet effective strategy to avoid the need for bidirectional attention in embedding models and suggests a unified architecture for various NLP tasks.

**Abstract:** Bidirectional models are considered essential for strong text embeddings. Recent approaches to adapt autoregressive language models (LMs) into strong text embedding models have largely had the requirement to modify the LM architecture to be bidirectional. We challenge this premise by introducing "echo embeddings" which converts autoregressive LMs into high quality text embedding models without changing the architecture or requiring fine-tuning. By repeating the input and extracting embeddings from the repeated tokens -- which have access to all original tokens -- echo embeddings improve over classical LM embeddings by over 5% in zero-shot settings. Our zero-shot embeddings nearly match those obtained by bidirectionally-converted LMs that undergo additional masked-language modeling training. Echo embeddings are also compatible with supervised fine-tuning, matching or outperforming bidirectionally-converted LMs in an apples-to-apples comparison, even with an identical compute budget during training and inference. Overall, repetition is a simple and effective strategy to circumvent the need for bidirectional attention in embedding models, paving the way towards a unified architecture for all NLP tasks.

</details>


### [94] [Linearly Controlled Language Generation with Performative Guarantees](https://arxiv.org/abs/2405.15454)

*Emily Cheng, Carmen Amo Alonso*

**Main category:** cs.CL

**Keywords:** Large Language Models, natural language generation, control theory, toxicity avoidance, sentiment control

**Relevance Score:** 9

**TL;DR:** This paper presents a control-theoretic approach for dynamically guiding language model activations during natural language generation to avoid undesired meanings and ensure quality while maintaining efficiency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing need for controlled language generation in critical applications of Large Language Models (LMs), ensuring efficiency and guaranteeing performance.

**Method:** The paper proposes a gradient-free intervention method that directly steers the activations of the token being generated in embedding space based on control theory, allowing for context-sensitive adjustments.

**Key Contributions:**

	1. Introduces a control-theoretic framework for language generation in latent space.
	2. Implements a lightweight intervention method for real-time control of token activations.
	3. Demonstrates effectiveness in toxicity avoidance and sentiment control without compromising text quality.

**Result:** The proposed method successfully demonstrates effectiveness in controlling attributes like toxicity and sentiment while maintaining text quality across various objectives.

**Limitations:** 

**Conclusion:** By applying optimal control techniques, the proposed lightweight intervention guarantees the generated text remains within pre-defined semantic boundaries without significantly impacting generation time.

**Abstract:** The increasing prevalence of Large Language Models (LMs) in critical applications highlights the need for controlled language generation strategies that are not only computationally efficient but that also enjoy performance guarantees. To achieve this, we use a common model of concept semantics as linearly represented in an LM's latent space. In particular, we take the view that natural language generation traces a trajectory in this continuous semantic space, realized by the language model's hidden activations. This view permits a control-theoretic treatment of text generation in latent space, in which we propose a lightweight, gradient-free intervention that dynamically steers trajectories away from regions corresponding to undesired meanings. In particular, we propose to directly intervene the activations of the token that is being generated in embedding space in an online fashion. Crucially, we do not simply steer activations towards a desirable region. Instead, our method relies on classical techniques from control theory to precisely control activations in a context-dependent way, and guarantees that they are brought into a specific pre-defined region of embedding space that corresponds to allowed semantics. Our intervention is computed in closed-form according to an optimal controller formulation, minimally impacting generation time. This control of the activations in embedding space allows for fine-grained steering of attributes of the generated sequence. We demonstrate the effectiveness of our approach on different objectives-- toxicity avoidance and sentiment control-- while maintaining text quality.

</details>


### [95] [Synth-SBDH: A Synthetic Dataset of Social and Behavioral Determinants of Health for Clinical Text](https://arxiv.org/abs/2406.06056)

*Avijit Mitra, Zhichao Yang, Emily Druhl, Raelene Goodwin, Hong Yu*

**Main category:** cs.CL

**Keywords:** SBDH, synthetic dataset, NLP, health informatics, clinical text

**Relevance Score:** 9

**TL;DR:** Synth-SBDH is a novel synthetic dataset designed to improve the extraction of social and behavioral determinants of health (SBDH) from clinical text, outperforming existing datasets and enhancing model performance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for high-quality datasets to extract social and behavioral determinants of health (SBDH) information from clinical text, as existing datasets are often limited in availability and coverage.

**Method:** The study introduces Synth-SBDH, a synthetic dataset with detailed SBDH annotations across 15 categories. It demonstrates its utility by training models on scenarios with real-world clinical datasets from two hospital settings and evaluating performance across three tasks.

**Key Contributions:**

	1. Introduction of the Synth-SBDH synthetic dataset with detailed annotations.
	2. Demonstrated improvements in extraction performance on real-world clinical datasets.
	3. Evaluation of dataset utility across multiple tasks with human-LLM alignment insights.

**Result:** Models trained on Synth-SBDH show up to 63.75% macro-F improvements compared to those without this training. The dataset is effective for rare SBDH categories and can be used under resource constraints, being cheaper than expert-annotated data.

**Limitations:** 

**Conclusion:** Synth-SBDH demonstrates significant improvements in SBDH extraction tasks and reveals areas for further refinements through human evaluation, achieving a 71.06% Human-LLM alignment.

**Abstract:** Social and behavioral determinants of health (SBDH) play a crucial role in health outcomes and are frequently documented in clinical text. Automatically extracting SBDH information from clinical text relies on publicly available good-quality datasets. However, existing SBDH datasets exhibit substantial limitations in their availability and coverage. In this study, we introduce Synth-SBDH, a novel synthetic dataset with detailed SBDH annotations, encompassing status, temporal information, and rationale across 15 SBDH categories. We showcase the utility of Synth-SBDH on three tasks using real-world clinical datasets from two distinct hospital settings, highlighting its versatility, generalizability, and distillation capabilities. Models trained on Synth-SBDH consistently outperform counterparts with no Synth-SBDH training, achieving up to 63.75% macro-F improvements. Additionally, Synth-SBDH proves effective for rare SBDH categories and under-resource constraints while being substantially cheaper than expert-annotated real-world data. Human evaluation reveals a 71.06% Human-LLM alignment and uncovers areas for future refinements.

</details>


### [96] [A Principled Framework for Evaluating on Typologically Diverse Languages](https://arxiv.org/abs/2407.05022)

*Esther Ploeger, Wessel Poelman, Andreas Holck Høeg-Petersen, Anders Schlichtkrull, Miryam de Lhoneux, Johannes Bjerva*

**Main category:** cs.CL

**Keywords:** multilingual NLP, language sampling, typological diversity

**Relevance Score:** 7

**TL;DR:** This paper presents a language sampling framework to select highly typologically diverse languages for multilingual NLP model evaluation, improving generalizability.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** As multilingual NLP research aims for models to perform well across languages, effective evaluation methods using representative language samples are crucial.

**Method:** The paper proposes a systematic language sampling framework informed by language typology, comparing it against existing sampling methods with various metrics.

**Key Contributions:**

	1. Introduction of a systematic language sampling framework for typologically diverse languages
	2. Comparison of sampling methods demonstrating superior diversity in selections
	3. Evidence linking language diversity in evaluation sets to improved model generalizability

**Result:** The proposed framework consistently retrieves more typologically diverse language selections compared to previous methods, positively impacting generalizability in multilingual model evaluation.

**Limitations:** 

**Conclusion:** Diverse language sampling is essential for accurately evaluating multilingual models and enhancing their generalizability across languages.

**Abstract:** Beyond individual languages, multilingual natural language processing (NLP) research increasingly aims to develop models that perform well across languages generally. However, evaluating these systems on all the world's languages is practically infeasible. To attain generalizability, representative language sampling is essential. Previous work argues that generalizable multilingual evaluation sets should contain languages with diverse typological properties. However, 'typologically diverse' language samples have been found to vary considerably in this regard, and popular sampling methods are flawed and inconsistent. We present a language sampling framework for selecting highly typologically diverse languages given a sampling frame, informed by language typology. We compare sampling methods with a range of metrics and find that our systematic methods consistently retrieve more typologically diverse language selections than previous methods in NLP. Moreover, we provide evidence that this affects generalizability in multilingual model evaluation, emphasizing the importance of diverse language sampling in NLP evaluation.

</details>


### [97] [Affective Computing in the Era of Large Language Models: A Survey from the NLP Perspective](https://arxiv.org/abs/2408.04638)

*Yiqun Zhang, Xiaocui Yang, Xingle Xu, Zeran Gao, Yijie Huang, Shiyi Mu, Shi Feng, Daling Wang, Yifei Zhang, Kaisong Song, Ge Yu*

**Main category:** cs.CL

**Keywords:** Affective Computing, Large Language Models, Reinforcement Learning, Natural Language Processing, Emotion recognition

**Relevance Score:** 8

**TL;DR:** This survey reviews Affective Computing (AC) in the context of Large Language Models (LLMs), focusing on Affective Understanding and Generation, adaptation techniques, reinforcement learning approaches, evaluation practices, and open challenges.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how recent advances in Large Language Models (LLMs) can enhance the performance and adaptability of Affective Computing (AC) across various domains.

**Method:** The paper consolidates existing AC tasks, reviews LLM-based studies, and discusses adaptation methods for improving Affective Understanding (AU) and Affective Generation (AG), including various techniques such as Instruction Tuning and Prompt Engineering. It also highlights reinforcement learning approaches tailored to improve emotional response quality.

**Key Contributions:**

	1. Overview of traditional AC tasks and LLM-based studies
	2. Review of adaptation techniques for AU/AG
	3. Compilation of benchmarks and evaluation practices for AC

**Result:** The survey identifies that while fine-tuned pre-trained language models perform well in Affective Understanding, they struggle with Affective Generation, particularly in producing diverse and contextually appropriate responses. The integration of LLMs shows promise in addressing these challenges.

**Limitations:** Challenges in ethics, data quality, safety, robust evaluation, and resource efficiency remain.

**Conclusion:** The paper outlines the current state of AC within the LLM landscape and provides guidance on practical implementations while highlighting numerous challenges such as ethics and evaluation that need addressing for effective deployment.

**Abstract:** Affective Computing (AC) integrates computer science, psychology, and cognitive science to enable machines to recognize, interpret, and simulate human emotions across domains such as social media, finance, healthcare, and education. AC commonly centers on two task families: Affective Understanding (AU) and Affective Generation (AG). While fine-tuned pre-trained language models (PLMs) have achieved solid AU performance, they often generalize poorly across tasks and remain limited for AG, especially in producing diverse, emotionally appropriate responses. The advent of Large Language Models (LLMs) (e.g., ChatGPT and LLaMA) has catalyzed a paradigm shift by offering in-context learning, broader world knowledge, and stronger sequence generation. This survey presents an NLP-oriented overview of AC in the LLM era. We (i) consolidate traditional AC tasks and preliminary LLM-based studies; (ii) review adaptation techniques that improve AU/AG, including Instruction Tuning (full and parameter-efficient methods such as LoRA, P-/Prompt-Tuning), Prompt Engineering (zero/few-shot, chain-of-thought, agent-based prompting), and Reinforcement Learning. For the latter, we summarize RL from human preferences (RLHF), verifiable/programmatic rewards (RLVR), and AI feedback (RLAIF), which provide preference- or rule-grounded optimization signals that can help steer AU/AG toward empathy, safety, and planning, achieving finer-grained or multi-objective control. To assess progress, we compile benchmarks and evaluation practices for both AU and AG. We also discuss open challenges-from ethics, data quality, and safety to robust evaluation and resource efficiency-and outline research directions. We hope this survey clarifies the landscape and offers practical guidance for building affect-aware, reliable, and responsible LLM systems.

</details>


### [98] [Self-Alignment: Improving Alignment of Cultural Values in LLMs via In-Context Learning](https://arxiv.org/abs/2408.16482)

*Rochelle Choenni, Ekaterina Shutova*

**Main category:** cs.CL

**Keywords:** large language models, cultural alignment, in-context learning, multilingual, human survey data

**Relevance Score:** 9

**TL;DR:** This paper presents a method to enhance the alignment of Large Language Models with cultural values using in-context learning and survey data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The alignment of Large Language Models with cultural values is crucial, given their widespread use and potential impact on different cultures.

**Method:** The authors propose a method that leverages in-context learning along with human survey data to adjust model responses to cultural values.

**Key Contributions:**

	1. Developed a simple method combining ICL and human survey data for values alignment.
	2. Demonstrated improvements in alignment across multiple LLMs.
	3. Showed applicability to culturally diverse countries beyond English-centric contexts.

**Result:** The method improves the alignment of five different models, including multilingual LLMs, with cultural probes, demonstrating effectiveness beyond English-centric applications.

**Limitations:** The method's performance in real-world applications and its generalizability across all cultural contexts require further investigation.

**Conclusion:** The proposed technique offers a promising approach to align language models more closely with diverse cultural values, showing potential utility in various languages.

**Abstract:** Improving the alignment of Large Language Models (LLMs) with respect to the cultural values that they encode has become an increasingly important topic. In this work, we study whether we can exploit existing knowledge about cultural values at inference time to adjust model responses to cultural value probes. We present a simple and inexpensive method that uses a combination of in-context learning (ICL) and human survey data, and show that we can improve the alignment to cultural values across 5 models that include both English-centric and multilingual LLMs. Importantly, we show that our method could prove useful in test languages other than English and can improve alignment to the cultural values that correspond to a range of culturally diverse countries.

</details>


### [99] [GASE: Generatively Augmented Sentence Encoding](https://arxiv.org/abs/2411.04914)

*Manuel Frank, Haithem Afli*

**Main category:** cs.CL

**Keywords:** sentence embeddings, data augmentation, generative models

**Relevance Score:** 7

**TL;DR:** A training-free method to enhance sentence embeddings using generative models for data augmentation at inference time.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To improve sentence embeddings without requiring model fine-tuning or extensive computational resources.

**Method:** The approach uses generative models to paraphrase, summarize, or extract keywords from input text during inference, pooling the original and synthetic embeddings.

**Key Contributions:**

	1. Training-free approach for sentence embedding enhancement
	2. Utilization of generative models for data augmentation at inference
	3. Demonstrated performance improvements on benchmark datasets

**Result:** Experiments show performance improvements on the Massive Text Embedding Benchmark for Semantic Textual Similarity, particularly for embedding models with lower baseline performance.

**Limitations:** 

**Conclusion:** Generative augmentation increases semantic diversity, improving the robustness and generalisability of sentence embeddings, with performance gains varying by model and dataset.

**Abstract:** We propose a training-free approach to improve sentence embeddings leveraging test-time compute by applying generative text models for data augmentation at inference time. Unlike conventional data augmentation that utilises synthetic training data, our approach does not require access to model parameters or the computational resources typically required for fine-tuning state-of-the-art models. Generatively Augmented Sentence Encoding variates the input text by paraphrasing, summarising, or extracting keywords, followed by pooling the original and synthetic embeddings. Experimental results on the Massive Text Embedding Benchmark for Semantic Textual Similarity (STS) demonstrate performance improvements across a range of embedding models using different generative models for augmentation. We find that generative augmentation leads to larger performance improvements for embedding models with lower baseline performance. These findings suggest that integrating generative augmentation at inference time adds semantic diversity and can enhance the robustness and generalisability of sentence embeddings for embedding models. Our results show that performance gains depend on the embedding model and the dataset.

</details>


### [100] [Exploring the Limits of Large Language Models: A Systematic Evaluation of Masked Text Processing Ability through MskQA and MskCal](https://arxiv.org/abs/2411.05665)

*Fuka Matsuzaki, Haru-Tada Sato*

**Main category:** cs.CL

**Keywords:** Large Language Models, masked text, reasoning, semantic cues, evaluation methods

**Relevance Score:** 9

**TL;DR:** This paper evaluates the limitations of LLMs in processing masked text, introducing two tasks: MskQA and MskCal, and revealing that performance depends on masking rates and semantic cues.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and performance variations of LLMs when processing masked text, given their reliance on semantic cues.

**Method:** The paper introduces two tasks (MskQA and MskCal) to evaluate reasoning abilities in LLMs, specifically testing GPT-4o and 4o-mini under different masking conditions.

**Key Contributions:**

	1. Introduction of MskQA and MskCal tasks for evaluating LLM reasoning on masked text.
	2. Identifies performance dependency on masking rates and semantic cues in LLMs.
	3. Performance analysis of GPT-4o versus 4o-mini in masked text contexts.

**Result:** LLMs show resilience to masked text but perform significantly worse under solid masking conditions compared to partial lifting. GPT-4o outperforms 4o-mini, especially in numerical reasoning tasks.

**Limitations:** Focused on specific models; results may vary with other LLM architectures or tasks.

**Conclusion:** The findings demonstrate the importance of semantic information in reasoning tasks for LLMs and highlight the need for improved evaluation methods to fully assess LLM comprehension abilities.

**Abstract:** This paper sheds light on the limitations of Large Language Models (LLMs) by rigorously evaluating their ability to process masked text. We introduce two novel tasks: MskQA, measuring reasoning on masked question-answering datasets like RealtimeQA, and MskCal, assessing numerical reasoning on masked arithmetic problems.Testing GPT-4o and 4o-mini reveals that while LLMs exhibit some resilience to masked text, their performance is highly contingent on masking rates and semantic cues. Specifically, "solid masking," where semantic clues are entirely absent, leads to a significant performance drop compared to "partial lifting," where some semantic information is retained, indicating LLMs' reliance on surface-level patterns. Interestingly, GPT-4o consistently outperforms 4o-mini, particularly in MskCal, demonstrating a greater ability to handle numerical reasoning with masked text. This underscores the crucial role of semantic cues in the reasoning process of LLMs. Our study illuminates the interplay between background knowledge and reasoning ability in masked text processing, paving the way for a deeper understanding of LLM capabilities and limitations, and highlighting the need for more robust evaluation methods to accurately assess their true comprehension abilities.

</details>


### [101] [HierTOD: A Task-Oriented Dialogue System Driven by Hierarchical Goals](https://arxiv.org/abs/2411.07152)

*Lingbo Mo, Shun Jiang, Akash Maharaj, Bernard Hishamunda, Yunyao Li*

**Main category:** cs.CL

**Keywords:** Task-Oriented Dialogue, Hierarchical goals, Enterprise systems

**Relevance Score:** 7

**TL;DR:** HierTOD is an enterprise Task-Oriented Dialogue system designed to handle complex workflows through hierarchical goals, providing proactive task assistance and improving dialogue management.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Task-Oriented Dialogue systems struggle in enterprise environments due to complexity and lack of standardized documentation, necessitating a more robust solution for task completion.

**Method:** HierTOD employs hierarchical goals to facilitate mixed-initiative dialogue, integrating components for natural language understanding, composite goal retrieval, dialogue management, and response generation.

**Key Contributions:**

	1. Introduction of hierarchical goals in enterprise TOD systems
	2. Unified approach to slot-filling and step-by-step guidance
	3. Demonstrated effectiveness through user studies

**Result:** HierTOD enhances task assistance efficiency, judged positively by human evaluators, and unifies slot-filling with step-by-step task execution guidance.

**Limitations:** 

**Conclusion:** User studies validate HierTOD's effectiveness in improving task completion across varied workflows.

**Abstract:** Task-Oriented Dialogue (TOD) systems assist users in completing tasks through natural language interactions, often relying on a single-layered workflow structure for slot-filling in public tasks, such as hotel bookings. However, in enterprise environments, which involve rich domain-specific knowledge, TOD systems face challenges due to task complexity and the lack of standardized documentation. In this work, we introduce HierTOD, an enterprise TOD system driven by hierarchical goals that can support composite workflows. By focusing on goal-driven interactions, our system serves a more proactive role, facilitating mixed-initiative dialogue and improving task completion. Equipped with components for natural language understanding, composite goal retriever, dialogue management, and response generation, backed by a well-organized data service with domain knowledge base and retrieval engine, HierTOD delivers efficient task assistance as judged by human evaluators. Furthermore, our system implementation unifies two TOD paradigms: slot-filling for information collection and step-by-step guidance for task execution. Our user study demonstrates the effectiveness and helpfulness of HierTOD in performing both paradigms.

</details>


### [102] [Lessons from Studying Two-Hop Latent Reasoning](https://arxiv.org/abs/2411.16353)

*Mikita Balesni, Tomek Korbak, Owain Evans*

**Main category:** cs.CL

**Keywords:** large language models, latent reasoning, two-hop question answering, synthetic facts, model performance

**Relevance Score:** 8

**TL;DR:** The paper investigates the latent reasoning capabilities of large language models (LLMs) through two-hop question answering, revealing nuanced insights on their reasoning abilities.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the limitations and capabilities of LLMs in two-hop question answering and the implications for agentic tasks.

**Method:** Controlled experiments fine-tuned LLMs on synthetic facts to evaluate their two-hop reasoning performance, avoiding the influences of memorization.

**Key Contributions:**

	1. Introduces a controlled setting for studying LLM reasoning.
	2. Demonstrates LLMs' capabilities in latent two-hop reasoning with nuanced results.
	3. Cautions researchers against misinterpreting LLM performance based on experimental design.

**Result:** Models can answer two-hop questions successfully when one fact is synthetic and the other is natural, indicating latent reasoning capabilities, but they struggle to composite two synthetic facts.

**Limitations:** Unclear scalability of reasoning abilities with model size; mixed results from prior studies on LLM reasoning.

**Conclusion:** LLMs are shown to possess latent two-hop reasoning abilities; however, clarity on how this ability scales with model size is still needed.

**Abstract:** Large language models can use chain-of-thought (CoT) to externalize reasoning, potentially enabling oversight of capable LLM agents. Prior work has shown that models struggle at two-hop question-answering without CoT. This capability is so basic that if it was a fundamental limitation, it would imply that many complex agentic tasks would similarly require CoT. We investigate LLM latent reasoning capabilities using two-hop question answering as a case study. Previous work on the gap between latent and externalized two-hop reasoning produced mixed evidence with inconclusive results. In this paper, we introduce a controlled setting for investigating two-hop reasoning in LLMs, where a positive result provides definitive evidence for latent reasoning. We fine-tune LLMs (including Llama 3 8B and GPT-4o) on synthetic facts and test two-hop reasoning over these facts. By using synthetic facts, we rule out memorization and reasoning shortcuts as explanations for two-hop performance. We observe a nuanced picture: Models fail to compose two synthetic facts, but can succeed when one fact is synthetic and the other is natural. These results demonstrate that LLMs are undeniably capable of latent two-hop reasoning, although it remains unclear how this ability scales with model size. Finally, we highlight a lesson for researchers studying LLM reasoning: when drawing conclusions about LLM latent reasoning, one must be careful to avoid both spurious successes (that stem from memorization and reasoning shortcuts) and spurious failures (that may stem from artificial experimental setups, divorced from training setups of frontier LLMs).

</details>


### [103] [Fine-Tuning Large Language Models for Scientific Text Classification: A Comparative Study](https://arxiv.org/abs/2412.00098)

*Zhyar Rzgar K Rostam, Gábor Kertész*

**Main category:** cs.CL

**Keywords:** Text Classification, Large Language Models, SciBERT, Domain Adaptation, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** This study explores the fine-tuning of LLMs for automated classification of scientific texts, revealing that domain-specific models like SciBERT outperform general-purpose models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rapid increase of online textual content, there is a need for improved methods of automated text classification, especially for domain-specific content.

**Method:** Four LLMs (BERT, SciBERT, BioBERT, and BlueBERT) were fine-tuned on three datasets from the WoS-46985 dataset and evaluated for scientific text classification.

**Key Contributions:**

	1. Fine-tuning of multiple LLMs for a specific domain
	2. Comparison of domain-specific and general-purpose models
	3. Demonstration of improved performance on scientific texts

**Result:** Domain-specific models, especially SciBERT, consistently outperformed general-purpose models in both abstract-based and keyword-based classification tasks.

**Limitations:** 

**Conclusion:** The study underscores the necessity of domain-specific adaptations for LLMs to enhance their performance in specialized text classification.

**Abstract:** The exponential growth of online textual content across diverse domains has necessitated advanced methods for automated text classification. Large Language Models (LLMs) based on transformer architectures have shown significant success in this area, particularly in natural language processing (NLP) tasks. However, general-purpose LLMs often struggle with domain-specific content, such as scientific texts, due to unique challenges like specialized vocabulary and imbalanced data. In this study, we fine-tune four state-of-the-art LLMs BERT, SciBERT, BioBERT, and BlueBERT on three datasets derived from the WoS-46985 dataset to evaluate their performance in scientific text classification. Our experiments reveal that domain-specific models, particularly SciBERT, consistently outperform general-purpose models in both abstract-based and keyword-based classification tasks. Additionally, we compare our achieved results with those reported in the literature for deep learning models, further highlighting the advantages of LLMs, especially when utilized in specific domains. The findings emphasize the importance of domain-specific adaptations for LLMs to enhance their effectiveness in specialized text classification tasks.

</details>


### [104] [Think-to-Talk or Talk-to-Think? When LLMs Come Up with an Answer in Multi-Hop Arithmetic Reasoning](https://arxiv.org/abs/2412.01113)

*Keito Kudo, Yoichi Aoki, Tatsuki Kuribayashi, Shusaku Sone, Masaya Taniguchi, Ana Brassard, Keisuke Sakaguchi, Kentaro Inui*

**Main category:** cs.CL

**Keywords:** language models, multi-hop reasoning, arithmetic reasoning, internal computation, incremental reasoning

**Relevance Score:** 7

**TL;DR:** This study reveals that language models (LMs) use an incremental reasoning strategy for arithmetic multi-hop problems, deriving sub-answers while generating reasoning chains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To mechanistically interpret the internal problem-solving process of language models, especially in the context of arithmetic multi-hop reasoning.

**Method:** Experiments were conducted to observe how language models resolve sub-problems and construct reasoning chains incrementally.

**Key Contributions:**

	1. Identifies an incremental reasoning strategy in language models
	2. Demonstrates the generation of sub-answers during reasoning chain construction
	3. Provides insights into the internal computation of LMs during problem-solving

**Result:** The study found that LMs generate sub-answers during the process of creating reasoning chains rather than obtaining answers immediately upon reading the problem statements.

**Limitations:** 

**Conclusion:** The reasoning chains produced by LMs provide a faithful reflection of their internal computation process.

**Abstract:** This study investigates the incremental, internal problem-solving process of language models (LMs) with arithmetic multi-hop reasoning as a case study. We specifically investigate when LMs internally resolve sub/whole problems through first reading the problem statements, generating reasoning chains, and achieving the final answer to mechanistically interpret LMs' multi-hop problem-solving process. Our experiments reveal a systematic incremental reasoning strategy underlying LMs. They have not derived an answer at the moment they first read the problem; instead, they obtain (sub)answers while generating the reasoning chain. Therefore, the generated reasoning chains can be regarded as faithful reflections of the model's internal computation.

</details>


### [105] [Concept Bottleneck Large Language Models](https://arxiv.org/abs/2412.07992)

*Chung-En Sun, Tuomas Oikarinen, Berk Ustun, Tsui-Wei Weng*

**Main category:** cs.CL

**Keywords:** interpretable models, large language models, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces Concept Bottleneck Large Language Models (CB-LLMs), which enhance interpretability in LLMs by integrating transparency directly into the modeling process for tasks like text classification and generation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the interpretability limitations of traditional black-box LLMs and improve user trust and safety in model outputs.

**Method:** Develop CB-LLMs for text classification and generation, allowing intrinsic interpretability and explicit reasoning during NLP tasks.

**Key Contributions:**

	1. Introduces a novel framework for interpretable LLMs (CB-LLMs).
	2. Demonstrates improved performance in text classification with intrinsic interpretability.
	3. Enhances text generation with control over model behavior and content safety.

**Result:** CB-LLMs perform competitively with traditional models in text classification and provide safer, controlled outputs in text generation while enabling precise concept detection.

**Limitations:** 

**Conclusion:** CB-LLMs enhance the safety, reliability, and trustworthiness of LLMs through built-in interpretability and control mechanisms.

**Abstract:** We introduce Concept Bottleneck Large Language Models (CB-LLMs), a novel framework for building inherently interpretable Large Language Models (LLMs). In contrast to traditional black-box LLMs that rely on limited post-hoc interpretations, CB-LLMs integrate intrinsic interpretability directly into the LLMs -- allowing accurate explanations with scalability and transparency. We build CB-LLMs for two essential NLP tasks: text classification and text generation. In text classification, CB-LLMs is competitive with, and at times outperforms, traditional black-box models while providing explicit and interpretable reasoning. For the more challenging task of text generation, interpretable neurons in CB-LLMs enable precise concept detection, controlled generation, and safer outputs. The embedded interpretability empowers users to transparently identify harmful content, steer model behavior, and unlearn undesired concepts -- significantly enhancing the safety, reliability, and trustworthiness of LLMs, which are critical capabilities notably absent in existing models. Our code is available at https://github.com/Trustworthy-ML-Lab/CB-LLMs.

</details>


### [106] [Process-Supervised Reward Models for Verifying Clinical Note Generation: A Scalable Approach Guided by Domain Expertise](https://arxiv.org/abs/2412.12583)

*Hanyin Wang, Chufan Gao, Qiping Xu, Bolun Liu, Guleid Hussein, Hariprasad Korsapati, Mohamad El Labban, Kingsley Iheasirim, Mohamed Hassan, Gokhan Anil, Brian Bartlett, Jimeng Sun*

**Main category:** cs.CL

**Keywords:** Process-supervised reward models, clinical note generation, large language models

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for training process-supervised reward models (PRMs) to evaluate LLM-generated clinical notes, achieving state-of-the-art performance in distinguishing high-quality outputs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of applying reward models in clinical note generation where ground-truth answers aren't available.

**Method:** The framework involves defining meaningful steps, introducing realistic errors, and leveraging LLMs to generate supervision data at scale, focusing on optimal loss functions and data selection strategies for training PRMs.

**Key Contributions:**

	1. Novel framework for training PRMs in clinical settings
	2. Improved accuracy in evaluating LLM-generated clinical notes
	3. Insights into training PRMs using optimal loss functions and data strategies

**Result:** The PRM built on LLaMA-3.1 8B outperforms existing models, achieving 98.8% accuracy in distinguishing gold-standard samples and 56.2% accuracy in selecting preferred clinical notes.

**Limitations:** 

**Conclusion:** The study demonstrates that PRMs can effectively serve various generative tasks by identifying key factors that influence performance, opening new paths for applications in health informatics.

**Abstract:** Process-supervised reward models (PRMs) excel at providing step-by-step verification for large language model (LLM) outputs in domains like mathematics and coding. However, their application to fields lacking ground-truth answers, such as clinical note generation, poses significant challenges. We introduce a novel framework for training PRMs to deliver step-level reward signals for LLM-generated clinical notes. By precisely defining meaningful "steps," injecting realistic "errors" informed by domain expertise, and leveraging LLMs to generate process supervision data at scale, we overcome previous limitations. Our PRM, built on LLaMA-3.1 8B, consistently outperforms proprietary reasoning and non-reasoning models, achieving state-of-the-art performance on two key evaluations: (1) distinguishing gold-standard from error-containing samples with 98.8% accuracy, and (2) selecting physician-preferred clinical notes with 56.2% accuracy. We investigate critical components for effective PRM training, including optimal loss functions and data selection strategies, and present a comprehensive physician reader study identifying predictors of downstream Best-of-N performance. Our study sheds light on unlocking the potential of PRMs for diverse generative tasks across domains.

</details>


### [107] [Revealing the impact of synthetic native samples and multi-tasking strategies in Hindi-English code-mixed humour and sarcasm detection](https://arxiv.org/abs/2412.12761)

*Debajyoti Mazumder, Aakash Kumar, Jasabanta Patro*

**Main category:** cs.CL

**Keywords:** code-mixed, humor detection, sarcasm detection, multi-task learning, multilingual models

**Relevance Score:** 7

**TL;DR:** The paper explores strategies for improving code-mixed humor and sarcasm detection through native sample mixing, multi-task learning, and prompting with VMLMs, revealing that native sample mixing and MTL significantly enhance detection performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need to enhance the detection of humor and sarcasm in code-mixed languages, which poses specific challenges in natural language processing.

**Method:** Utilized three approaches: native sample mixing by adding monolingual samples, multi-task learning (MTL) with related tasks, and evaluation of very large multilingual language models (VMLMs) through few-shot prompting and instruction finetuning.

**Key Contributions:**

	1. Introduction of native sample mixing for code-mixed tasks.
	2. Demonstration of multi-task learning impact on humor and sarcasm detection.
	3. Provision of code for reproducibility of study findings.

**Result:** Native sample mixing improved humor and sarcasm detection F1-scores by up to 6.76% and 8.64%, respectively. MTL further boosted performance, achieving up to 10.67% and 12.35% improvements. VMLM approaches did not outperform the other methods.

**Limitations:** The study highlights remaining areas for improvement in the model's performance.

**Conclusion:** Native sample mixing and MTL are effective strategies for enhancing humor and sarcasm detection, while VMLM methods require further development.

**Abstract:** In this paper, we reported our experiments with various strategies to improve code-mixed humour and sarcasm detection. Particularly, we tried three approaches: (i) native sample mixing, (ii) multi-task learning (MTL), and (iii) prompting and instruction finetuning very large multilingual language models (VMLMs). In native sample mixing, we added monolingual task samples to code-mixed training sets. In MTL learning, we relied on native and code-mixed samples of a semantically related task (hate detection in our case). Finally, in our third approach, we evaluated the efficacy of VMLMs via few-shot context prompting and instruction finetuning. Some interesting findings we got are (i) adding native samples improved humor (raising the F1-score up to 6.76%) and sarcasm (raising the F1-score up to 8.64%) detection, (ii) training MLMs in an MTL framework boosted performance for both humour (raising the F1-score up to 10.67%) and sarcasm (increment up to 12.35% in F1-score) detection, and (iii) prompting and instruction finetuning VMLMs couldn't outperform the other approaches. Finally, our ablation studies and error analysis discovered the cases where our model is yet to improve. We provided our code for reproducibility.

</details>


### [108] [Knowledge Editing through Chain-of-Thought](https://arxiv.org/abs/2412.17727)

*Changyue Wang, Weihang Su, Qingyao Ai, Yichen Tang, Yiqun Liu*

**Main category:** cs.CL

**Keywords:** knowledge editing, large language models, chain-of-thought, generalization, stability

**Relevance Score:** 9

**TL;DR:** EditCoT is a new framework for knowledge editing in LLMs that allows flexible updates without needing retraining, improving stability and generalization across various tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing in-context knowledge editing methods, which are often task-specific and unstable when generalizing across diverse tasks.

**Method:** EditCoT generates a chain-of-thought (CoT) for inputs and iteratively refines it using a CoT editor based on updated knowledge.

**Key Contributions:**

	1. Introduces EditCoT for flexible knowledge editing without retraining.
	2. Demonstrates state-of-the-art performance across multiple languages and tasks.
	3. Enhances stability and generalization compared to existing editing methods.

**Result:** EditCoT achieves state-of-the-art performance on diverse benchmarks, demonstrating superior generalization, effectiveness, and stability compared to current methods.

**Limitations:** 

**Conclusion:** The proposed framework marks a significant advancement in knowledge updating for LLMs, avoiding the high costs of complete retraining while maintaining performance.

**Abstract:** Knowledge Editing is a technique that updates large language models (LLMs) with new information to maintain their world knowledge. This approach avoids the need to rebuild the model from scratch, thereby addressing the high costs associated with frequent retraining. Among these, the in-context editing paradigm stands out for its effectiveness in integrating new knowledge while preserving the model's original capabilities. Despite its potential, existing in-context knowledge editing methods are often task-specific, focusing primarily on multi-hop QA tasks using structured knowledge triples. Moreover, their reliance on few-shot prompting for task decomposition makes them unstable and less effective in generalizing across diverse tasks. In response to these limitations, we propose EditCoT, a novel knowledge editing framework that flexibly and efficiently updates LLMs across various tasks without retraining. EditCoT works by generating a chain-of-thought (CoT) for a given input and then iteratively refining this CoT process using a CoT editor based on updated knowledge. We evaluate EditCoT across a diverse range of benchmarks, covering multiple languages and tasks. The results demonstrate that our approach achieves state-of-the-art performance while offering superior generalization, effectiveness, and stability compared to existing methods, marking a significant advancement in the field of knowledge updating. The code and data of EditCoT are available at: https://github.com/bebr2/EditCoT .

</details>


### [109] [OmniThink: Expanding Knowledge Boundaries in Machine Writing through Thinking](https://arxiv.org/abs/2501.09751)

*Zekun Xi, Wenbiao Yin, Jizhan Fang, Jialong Wu, Runnan Fang, Jiang Yong, Pengjun Xie, Fei Huang, Huajun Chen, Ningyu Zhang*

**Main category:** cs.CL

**Keywords:** Machine Writing, Large Language Models, Cognitive Behavior, Knowledge Density, Content Generation

**Relevance Score:** 9

**TL;DR:** OmniThink enhances machine writing by emulating human cognitive processes, improving content quality in long-form generation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the depth and novelty of content generated by large language models, which often suffer from redundancy and shallow outputs.

**Method:** OmniThink employs a slow-thinking framework that iteratively expands and reflects on topics, simulating human-like learning processes.

**Key Contributions:**

	1. Introduction of the OmniThink framework for iterative expansion and reflection.
	2. Demonstration of improved knowledge density in generated articles.
	3. Human evaluation results supporting the effectiveness of the framework.

**Result:** OmniThink significantly increases the knowledge density of generated articles while maintaining coherence and depth, as validated by human evaluations.

**Limitations:** The performance and effectiveness may vary depending on the specific topic or domain.

**Conclusion:** OmniThink has the potential to solve practical challenges in long-form content generation, providing richer and more original outputs.

**Abstract:** Machine writing with large language models often relies on retrieval-augmented generation. However, these approaches remain confined within the boundaries of the model's predefined scope, limiting the generation of content with rich information. Specifically, vanilla-retrieved information tends to lack depth, novelty, and suffers from redundancy, which negatively impacts the quality of generated articles, leading to shallow, unoriginal, and repetitive outputs. To address these issues, we propose OmniThink, a slow-thinking machine writing framework that emulates the human-like process of iterative expansion and reflection. The core idea behind OmniThink is to simulate the cognitive behavior of learners as they slowly deepen their knowledge of the topics. Experimental results demonstrate that OmniThink improves the knowledge density of generated articles without compromising metrics such as coherence and depth. Human evaluations and expert feedback further highlight the potential of OmniThink to address real-world challenges in the generation of long-form articles. Code is available at https://github.com/zjunlp/OmniThink.

</details>


### [110] [Error Classification of Large Language Models on Math Word Problems: A Dynamically Adaptive Framework](https://arxiv.org/abs/2501.15581)

*Yuhong Sun, Zhangyue Yin, Xuanjing Huang, Xipeng Qiu, Hui Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Math Word Problems, Error Classification, Dynamic Error Analysis, Error-Aware Prompting

**Relevance Score:** 9

**TL;DR:** This paper introduces MWPES-300K, a dataset of 304,865 error samples from 15 different LLMs focused on Math Word Problems (MWPs), and proposes a framework for automated error classification to enhance mathematical reasoning performance.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze and understand error patterns in LLMs' reasoning about Math Word Problems rather than solely focusing on accuracy improvements.

**Method:** The authors collect error samples from multiple LLMs across four MWP datasets and propose a novel framework for dynamic error classification to reduce human bias and enhance analysis.

**Key Contributions:**

	1. Introduction of MWPES-300K, a large-scale error sample dataset for MWPs.
	2. Development of a dynamic error classification framework for better analysis of LLMs' reasoning errors.
	3. Proposal of Error-Aware Prompting to improve mathematical reasoning performance.

**Result:** The experimental results indicate that error patterns vary significantly based on dataset characteristics and model capabilities, with a structured Error-Aware Prompting approach leading to improved performance in mathematical reasoning tasks.

**Limitations:** The reliance on specific MWPs may limit generalizability to all mathematical reasoning tasks.

**Conclusion:** By providing deeper insights into error patterns and utilizing error-aware prompting, this research proposes a pathway to enhance LLM performance in mathematical reasoning.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities across various domains. Math Word Problems (MWPs) serve as a crucial benchmark for evaluating LLMs' reasoning abilities. While most research primarily focuses on improving accuracy, it often neglects understanding and addressing the underlying patterns of errors. Current error classification methods rely on static and predefined categories, which limit their ability to capture the full spectrum of error patterns in mathematical reasoning. To enable systematic error analysis, we collect error samples from 15 different LLMs of varying sizes across four distinct MWP datasets using multiple sampling strategies. Based on this extensive collection, we introduce MWPES-300K, a comprehensive dataset containing 304,865 error samples that cover diverse error patterns and reasoning paths. To reduce human bias and enable fine-grained analysis of error patterns, we propose a novel framework for automated dynamic error classification in mathematical reasoning. Experimental results demonstrate that dataset characteristics significantly shape error patterns, which evolve from basic to complex manifestations as model capabilities increase. With deeper insights into error patterns, we propose Error-Aware Prompting (EAP) that incorporates common error patterns as explicit guidance, leading to significant improvements in mathematical reasoning performance.

</details>


### [111] [Position: LLMs Can be Good Tutors in English Education](https://arxiv.org/abs/2502.05467)

*Jingheng Ye, Shen Wang, Deqing Zou, Yibo Yan, Kun Wang, Hai-Tao Zheng, Ruitong Liu, Zenglin Xu, Irwin King, Philip S. Yu, Qingsong Wen*

**Main category:** cs.CL

**Keywords:** large language models, English education, personalized learning, task prediction, educational methodologies

**Relevance Score:** 9

**TL;DR:** This paper discusses the integration of large language models (LLMs) in English education, highlighting their roles as data enhancers, task predictors, and agents for personalized learning.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of adaptability in traditional approaches to language learning when utilizing LLMs in education.

**Method:** The paper argues for the potential of LLMs to serve as effective tutors by defining their roles in enhancing data, predicting tasks, and acting as agents for personalized education.

**Key Contributions:**

	1. Identification of three critical roles of LLMs in English education.
	2. Suggestion for interdisciplinary research to improve LLM-based education.
	3. Promotion of personalized and inclusive education through LLMs.

**Result:** LLMs can significantly improve the process of English education by customizing learning experiences and enhancing educational methodologies.

**Limitations:** 

**Conclusion:** The thoughtful integration of LLMs into educational practices can foster innovation and address challenges in English education.

**Abstract:** While recent efforts have begun integrating large language models (LLMs) into English education, they often rely on traditional approaches to learning tasks without fully embracing educational methodologies, thus lacking adaptability to language learning. To address this gap, we argue that LLMs have the potential to serve as effective tutors in English Education. Specifically, LLMs can play three critical roles: (1) as data enhancers, improving the creation of learning materials or serving as student simulations; (2) as task predictors, serving as learner assessment or optimizing learning pathway; and (3) as agents, enabling personalized and inclusive education. We encourage interdisciplinary research to explore these roles, fostering innovation while addressing challenges and risks, ultimately advancing English Education through the thoughtful integration of LLMs.

</details>


### [112] [Reinforced Lifelong Editing for Language Models](https://arxiv.org/abs/2502.05759)

*Zherui Li, Houcheng Jiang, Hao Chen, Baolong Bi, Zhenhong Zhou, Fei Sun, Junfeng Fang, Xiang Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Model Editing, Reinforcement Learning, Hypernetworks, Lifelong Learning

**Relevance Score:** 8

**TL;DR:** RLEdit is a novel reinforcement learning-based method for lifelong editing of large language models, outperforming existing methods in efficiency and effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Addressing the challenge of keeping large language models' knowledge accurate and up-to-date through efficient editing methods that do not require retraining.

**Method:** RLEdit treats editing losses as rewards and optimizes hypernetwork parameters across the entire knowledge sequence, aligning with reinforcement learning principles.

**Key Contributions:**

	1. Introduction of RLEdit for lifelong model editing using reinforcement learning principles.
	2. Demonstrated significant improvements over existing hypernetwork-based editing methods.
	3. Provided an extensive empirical evaluation showing effectiveness and efficiency gains.

**Result:** RLEdit outperforms existing editing methods, achieving a 59.24% improvement in effectiveness while only requiring 2.11% of the time compared to traditional approaches.

**Limitations:** 

**Conclusion:** The proposed RLEdit method offers a significantly more effective and efficient solution for lifelong editing of large language models.

**Abstract:** Large language models (LLMs) acquire information from pre-training corpora, but their stored knowledge can become inaccurate or outdated over time. Model editing addresses this challenge by modifying model parameters without retraining, and prevalent approaches leverage hypernetworks to generate these parameter updates. However, they face significant challenges in lifelong editing due to their incompatibility with LLM parameters that dynamically change during the editing process. To address this, we observed that hypernetwork-based lifelong editing aligns with reinforcement learning modeling and proposed RLEdit, an RL-based editing method. By treating editing losses as rewards and optimizing hypernetwork parameters at the full knowledge sequence level, we enable it to precisely capture LLM changes and generate appropriate parameter updates. Our extensive empirical evaluation across several LLMs demonstrates that RLEdit outperforms existing methods in lifelong editing with superior effectiveness and efficiency, achieving a 59.24% improvement while requiring only 2.11% of the time compared to most approaches. Our code is available at: https://github.com/zhrli324/RLEdit.

</details>


### [113] [Improve LLM-as-a-Judge Ability as a General Ability](https://arxiv.org/abs/2502.11689)

*Jiachen Yu, Shaoning Sun, Xiaohui Hu, Jiaxu Yan, Kaidong Yu, Xuelong Li*

**Main category:** cs.CL

**Keywords:** large language models, judgment capability, ethical AI, preference optimization, data efficiency

**Relevance Score:** 9

**TL;DR:** This paper presents LLM-as-a-Judge, a two-stage training approach to enhance the judgment capabilities of large language models (LLMs) while optimizing data usage and aligning outputs with human values.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the alignment of LLM responses with human values and societal norms, ensuring ethical and reliable AI outputs by enhancing the judgment capability of LLMs.

**Method:** A two-stage training approach consisting of supervised fine-tuning (SFT) warm-up followed by direct preference optimization (DPO) enhancement, along with an efficient data synthesis method for generating judgmental content.

**Key Contributions:**

	1. Introduction of a two-stage training approach for LLMs
	2. Development of an efficient data synthesis method
	3. Open-sourcing of model weights and training data to aid in future research

**Result:** Achieves state-of-the-art performance on RewardBench using only 2% to 40% of the data required by other methods, while also enhancing the general capabilities of the model.

**Limitations:** 

**Conclusion:** The proposed training method significantly improves judgment accuracy and has a positive impact on downstream optimization tasks; the model's weights and training data are open-sourced for further research.

**Abstract:** LLM-as-a-Judge leverages the generative and reasoning capabilities of large language models (LLMs) to evaluate LLM responses across diverse scenarios, providing accurate preference signals. This approach plays a vital role in aligning LLMs with human values, ensuring ethical and reliable AI outputs that align with societal norms. Recent studies have raised many methods to train LLM as generative judges, but most of them are data consuming or lack accuracy, and only focus on LLM's judge ability. In this work, we regard judge ability as a general ability of LLM and implement a two-stage training approach, comprising supervised fine-tuning (SFT) warm-up and direct preference optimization (DPO) enhancement, to achieve judge style adaptation and improve judgment accuracy. Additionally, we introduce an efficient data synthesis method to generate judgmental content. Experimental results demonstrate that our approach, utilizing only about 2% to 40% of the data required by other methods, achieves SOTA performance on RewardBench. Furthermore, our training method enhances the general capabilities of the model by constructing complicated judge task, and the judge signals provided by our model have significantly enhanced the downstream DPO training performance of our internal models in our test to optimize policy model with Judge Model. We also open-source our model weights and training data to facilitate further research.

</details>


### [114] [Soft Token Attacks Cannot Reliably Audit Unlearning in Large Language Models](https://arxiv.org/abs/2502.15836)

*Haokun Chen, Sebastian Szyller, Weilin Xu, Nageen Himayat*

**Main category:** cs.CL

**Keywords:** machine unlearning, soft token attacks, large language models, auditing, LLMs

**Relevance Score:** 8

**TL;DR:** This paper critiques the effectiveness of soft token attacks (STAs) for auditing unlearning in large language models (LLMs), showing they can retrieve information regardless of unlearning efforts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the reliability of soft token attacks (STAs) as tools for auditing the effectiveness of machine unlearning in large language models (LLMs).

**Method:** The authors employed common benchmarks such as Who Is Harry Potter? and TOFU to evaluate the performance of STAs in a strong auditor setting, examining how they interact with unlearning algorithms.

**Key Contributions:**

	1. Critique of soft token attacks for auditing unlearning in LLMs
	2. Evidence showing STAs can retrieve unwanted information regardless of unlearning
	3. Demonstration of STAs eliciting random strings that overshadow their intended function.

**Result:** The results demonstrate that STAs can extract arbitrary information from LLMs despite the implementation of unlearning techniques, raising concerns about their suitability as auditing tools.

**Limitations:** The study primarily explores STAs without extensive evaluation of alternative auditing methods for unlearning.

**Conclusion:** STAs must be applied with caution due to their capability of eliciting extended random strings, underlining potential inadequacies in ensuring effective unlearning within LLMs.

**Abstract:** Large language models (LLMs) are trained using massive datasets, which often contain undesirable content such as harmful texts, personal information, and copyrighted material. To address this, machine unlearning aims to remove information from trained models. Recent work has shown that soft token attacks (STA) can successfully extract unlearned information from LLMs, but in this work we show that STAs can be an inadequate tool for auditing unlearning. Using common benchmarks such as Who Is Harry Potter? and TOFU, we demonstrate that in a strong auditor setting such attacks can elicit any information from the LLM, regardless of the deployed unlearning algorithm or whether the queried content was originally present in the training corpus. We further show that STA with just a few soft tokens (1-10) can elicit random strings over 400 characters long, indicating that STAs must be used carefully to effectively audit unlearning. Example code can be found at: https://github.com/IntelLabs/LLMart/tree/main/examples/unlearning

</details>


### [115] [Evaluating the Robustness and Accuracy of Text Watermarking Under Real-World Cross-Lingual Manipulations](https://arxiv.org/abs/2502.16699)

*Mansour Al Ghanim, Jiaqi Xue, Rochana Prih Hastuti, Mengxin Zheng, Yan Solihin, Qian Lou*

**Main category:** cs.CL

**Keywords:** watermarking, cross-lingual, adversarial scenarios, EMNLP 2025, language evaluation

**Relevance Score:** 4

**TL;DR:** This paper benchmarks watermarking methods in cross-lingual contexts, addressing the lack of evaluation in this area.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to fill the gap in literature regarding the evaluation of watermarking methods in cross-lingual settings, where much of the existing research has focused on English. This research is crucial for understanding the effectiveness of watermarking techniques against cross-lingual adversaries.

**Method:** The authors evaluate four watermarking methods across four languages, examining the quality of the watermarked text and the detectability of the watermarks during practical translation attacks.

**Key Contributions:**

	1. Benchmarking watermarking methods in multiple languages
	2. Identifying challenges in watermark detectability in cross-lingual contexts
	3. Providing insights into practical adversary scenarios for watermarking

**Result:** The study reveals the effectiveness and limitations of current watermarking methods when facing adversaries who possess cross-lingual knowledge.

**Limitations:** The study primarily focuses on four languages, and further research may be needed to assess effectiveness in other languages or dialects.

**Conclusion:** The findings underscore the necessity of improving watermarking techniques to ensure their robustness in cross-lingual adversarial scenarios.

**Abstract:** We present a study to benchmark representative watermarking methods in cross-lingual settings. The current literature mainly focuses on the evaluation of watermarking methods for the English language. However, the literature for evaluating watermarking in cross-lingual settings is scarce. This results in overlooking important adversary scenarios in which a cross-lingual adversary could be in, leading to a gray area of practicality over cross-lingual watermarking. In this paper, we evaluate four watermarking methods in four different and vocabulary rich languages. Our experiments investigate the quality of text under different watermarking procedure and the detectability of watermarks with practical translation attack scenarios. Specifically, we investigate practical scenarios that an adversary with cross-lingual knowledge could take, and evaluate whether current watermarking methods are suitable for such scenarios. Finally, from our findings, we draw key insights about watermarking in cross-lingual settings.

</details>


### [116] [PlainQAFact: Retrieval-augmented Factual Consistency Evaluation Metric for Biomedical Plain Language Summarization](https://arxiv.org/abs/2503.08890)

*Zhiwen You, Yue Guo*

**Main category:** cs.CL

**Keywords:** factual consistency, large language models, plain language summarization

**Relevance Score:** 9

**TL;DR:** Introducing PlainQAFact, a new metric for evaluating factual consistency in plain language summaries, specifically in the medical domain.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the risks posed by hallucinated outputs from large language models in medical communications and improve comprehension for lay audiences.

**Method:** PlainQAFact classifies sentence types and utilizes a retrieval-augmented QA scoring method to assess factual consistency.

**Key Contributions:**

	1. Introduction of PlainQAFact as a novel evaluation metric.
	2. Development of a fine-grained, human-annotated dataset (PlainFact).
	3. Demonstration of superior performance in factual consistency evaluation compared to existing metrics.

**Result:** PlainQAFact consistently outperforms existing metrics in evaluating factual consistency in plain language summaries across various settings.

**Limitations:** 

**Conclusion:** PlainQAFact serves as the first evaluation metric designed for plain language summarization, providing a benchmark and practical tool for reliable medical communication.

**Abstract:** Hallucinated outputs from large language models (LLMs) pose risks in the medical domain, especially for lay audiences making health-related decisions. Existing automatic factual consistency evaluation methods, such as entailment- and question-answering (QA) -based, struggle with plain language summarization (PLS) due to elaborative explanation phenomenon, which introduces external content (e.g., definitions, background, examples) absent from the scientific abstract to enhance comprehension. To address this, we introduce PlainQAFact, an automatic factual consistency evaluation metric trained on a fine-grained, human-annotated dataset PlainFact, for evaluating factual consistency of both source-simplified and elaborately explained sentences. PlainQAFact first classifies sentence type, then applies a retrieval-augmented QA scoring method. Empirical results show that existing evaluation metrics fail to evaluate the factual consistency in PLS, especially for elaborative explanations, whereas PlainQAFact consistently outperforms them across all evaluation settings. We further analyze PlainQAFact's effectiveness across external knowledge sources, answer extraction strategies, answer overlap measures, and document granularity levels, refining its overall factual consistency assessment. Taken together, our work presents the first evaluation metric designed for PLS factual consistency evaluation, providing the community with both a robust benchmark and a practical tool to advance reliable and safe plain language communication in the medical domain. PlainQAFact and PlainFact are available at: https://github.com/zhiwenyou103/PlainQAFact

</details>


### [117] [Learning to Reason for Long-Form Story Generation](https://arxiv.org/abs/2503.22828)

*Alexander Gurung, Mirella Lapata*

**Main category:** cs.CL

**Keywords:** story generation, reinforcement learning, natural language processing, large language models, narrative coherence

**Relevance Score:** 8

**TL;DR:** This paper proposes a method for long-form story generation using RL with Verified Rewards, leveraging an unlabeled book dataset to enhance reasoning for next-chapter predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of generating high-quality long-form stories by moving beyond manual prompting techniques, particularly for story generation tasks.

**Method:** The authors introduce a method called Next-Chapter Prediction and a reward formulation based on Verified Rewards via Completion Likelihood Improvement, utilizing an unlabeled book dataset to learn reasoning for story coherence.

**Key Contributions:**

	1. Introduction of Next-Chapter Prediction for story generation
	2. Utilization of Verified Rewards for training on unlabeled datasets
	3. Demonstration of improved narrative quality through pairwise human judgments

**Result:** Their approach yields chapters that are preferred by human judges over those generated using traditional supervised fine-tuning methods, showing especially strong performance in Scifi and Fantasy genres.

**Limitations:** 

**Conclusion:** The findings suggest that RL-based methods can effectively improve the quality of generated narratives, particularly for long-form storytelling.

**Abstract:** Generating high-quality stories spanning thousands of tokens requires competency across a variety of skills, from tracking plot and character arcs to keeping a consistent and engaging style. Due to the difficulty of sourcing labeled datasets and precise quality measurements, most work using large language models (LLMs) for long-form story generation uses combinations of hand-designed prompting techniques to elicit author-like behavior. This is a manual process that is highly dependent on the specific story-generation task. Motivated by the recent success of applying RL with Verifiable Rewards to domains like math and coding, we propose a general story-generation task (Next-Chapter Prediction) and a reward formulation (Verified Rewards via Completion Likelihood Improvement) that allows us to use an unlabeled book dataset as a learning signal for reasoning. We learn to reason over a story's condensed information and generate a detailed plan for the next chapter. Our reasoning is evaluated via the chapters it helps a story-generator create, and compared against non-trained and supervised finetuning (SFT) baselines. Pairwise human judgments reveal the chapters our learned reasoning produces are preferred across almost all metrics, and the effect is more pronounced in Scifi and Fantasy genres.

</details>


### [118] [Nemotron-H: A Family of Accurate and Efficient Hybrid Mamba-Transformer Models](https://arxiv.org/abs/2504.03624)

*NVIDIA, :, Aaron Blakeman, Aarti Basant, Abhinav Khattar, Adithya Renduchintala, Akhiad Bercovich, Aleksander Ficek, Alexis Bjorlin, Ali Taghibakhshi, Amala Sanjay Deshmukh, Ameya Sunil Mahabaleshwarkar, Andrew Tao, Anna Shors, Ashwath Aithal, Ashwin Poojary, Ayush Dattagupta, Balaram Buddharaju, Bobby Chen, Boris Ginsburg, Boxin Wang, Brandon Norick, Brian Butterfield, Bryan Catanzaro, Carlo del Mundo, Chengyu Dong, Christine Harvey, Christopher Parisien, Dan Su, Daniel Korzekwa, Danny Yin, Daria Gitman, David Mosallanezhad, Deepak Narayanan, Denys Fridman, Dima Rekesh, Ding Ma, Dmytro Pykhtar, Dong Ahn, Duncan Riach, Dusan Stosic, Eileen Long, Elad Segal, Ellie Evans, Eric Chung, Erick Galinkin, Evelina Bakhturina, Ewa Dobrowolska, Fei Jia, Fuxiao Liu, Gargi Prasad, Gerald Shen, Guilin Liu, Guo Chen, Haifeng Qian, Helen Ngo, Hongbin Liu, Hui Li, Igor Gitman, Ilia Karmanov, Ivan Moshkov, Izik Golan, Jan Kautz, Jane Polak Scowcroft, Jared Casper, Jarno Seppanen, Jason Lu, Jason Sewall, Jiaqi Zeng, Jiaxuan You, Jimmy Zhang, Jing Zhang, Jining Huang, Jinze Xue, Jocelyn Huang, Joey Conway, John Kamalu, Jon Barker, Jonathan Cohen, Joseph Jennings, Jupinder Parmar, Karan Sapra, Kari Briski, Kateryna Chumachenko, Katherine Luna, Keshav Santhanam, Kezhi Kong, Kirthi Sivamani, Krzysztof Pawelec, Kumar Anik, Kunlun Li, Lawrence McAfee, Leon Derczynski, Lindsey Pavao, Luis Vega, Lukas Voegtle, Maciej Bala, Maer Rodrigues de Melo, Makesh Narsimhan Sreedhar, Marcin Chochowski, Markus Kliegl, Marta Stepniewska-Dziubinska, Matthieu Le, Matvei Novikov, Mehrzad Samadi, Michael Andersch, Michael Evans, Miguel Martinez, Mike Chrzanowski, Mike Ranzinger, Mikolaj Blaz, Misha Smelyanskiy, Mohamed Fawzy, Mohammad Shoeybi, Mostofa Patwary, Nayeon Lee, Nima Tajbakhsh, Ning Xu, Oleg Rybakov, Oleksii Kuchaiev, Olivier Delalleau, Osvald Nitski, Parth Chadha, Pasha Shamis, Paulius Micikevicius, Pavlo Molchanov, Peter Dykas, Philipp Fischer, Pierre-Yves Aquilanti, Piotr Bialecki, Prasoon Varshney, Pritam Gundecha, Przemek Tredak, Rabeeh Karimi, Rahul Kandu, Ran El-Yaniv, Raviraj Joshi, Roger Waleffe, Ruoxi Zhang, Sabrina Kavanaugh, Sahil Jain, Samuel Kriman, Sangkug Lym, Sanjeev Satheesh, Saurav Muralidharan, Sean Narenthiran, Selvaraj Anandaraj, Seonmyeong Bak, Sergey Kashirsky, Seungju Han, Shantanu Acharya, Shaona Ghosh, Sharath Turuvekere Sreenivas, Sharon Clay, Shelby Thomas, Shrimai Prabhumoye, Shubham Pachori, Shubham Toshniwal, Shyamala Prayaga, Siddhartha Jain, Sirshak Das, Slawek Kierat, Somshubra Majumdar, Song Han, Soumye Singhal, Sriharsha Niverty, Stefania Alborghetti, Suseella Panguluri, Swetha Bhendigeri, Syeda Nahida Akter, Szymon Migacz, Tal Shiri, Terry Kong, Timo Roman, Tomer Ronen, Trisha Saar, Tugrul Konuk, Tuomas Rintamaki, Tyler Poon, Ushnish De, Vahid Noroozi, Varun Singh, Vijay Korthikanti, Vitaly Kurin, Wasi Uddin Ahmad, Wei Du, Wei Ping, Wenliang Dai, Wonmin Byeon, Xiaowei Ren, Yao Xu, Yejin Choi, Yian Zhang, Ying Lin, Yoshi Suhara, Zhiding Yu, Zhiqi Li, Zhiyu Li, Zhongbo Zhu, Zhuolin Yang, Zijia Chen*

**Main category:** cs.CL

**Keywords:** Transformer, inference-time scaling, Mamba layers, Model compression, FP8 training

**Relevance Score:** 6

**TL;DR:** Introduction of Nemotron-H, a family of hybrid Mamba-Transformer models optimized for efficient inference with comparable accuracy to existing models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning capabilities while reducing inference costs for Transformer models.

**Method:** Implementation of Mamba layers replacing self-attention layers in the Transformer architecture and development of a new compression technique called MiniPuzzle.

**Key Contributions:**

	1. Introduction of hybrid Mamba-Transformer architecture
	2. Development of MiniPuzzle compression technique for models
	3. Release of model checkpoints with integration to popular frameworks.

**Result:** Nemotron-H models demonstrate either improved or on-par accuracy compared to current state-of-the-art models while achieving up to 3× faster inference speeds.

**Limitations:** 

**Conclusion:** Nemotron-H-47B-Base, a variant of the 56B model, is developed for efficiency, achieving similar accuracy with 20% faster inference; new FP8 training recipe yields comparable results to BF16.

**Abstract:** As inference-time scaling becomes critical for enhanced reasoning capabilities, it is increasingly becoming important to build models that are efficient to infer. We introduce Nemotron-H, a family of 8B and 56B/47B hybrid Mamba-Transformer models designed to reduce inference cost for a given accuracy level. To achieve this goal, we replace the majority of self-attention layers in the common Transformer model architecture with Mamba layers that perform constant computation and require constant memory per generated token. We show that Nemotron-H models offer either better or on-par accuracy compared to other similarly-sized state-of-the-art open-sourced Transformer models (e.g., Qwen-2.5-7B/72B and Llama-3.1-8B/70B), while being up to 3$\times$ faster at inference. To further increase inference speed and reduce the memory required at inference time, we created Nemotron-H-47B-Base from the 56B model using a new compression via pruning and distillation technique called MiniPuzzle. Nemotron-H-47B-Base achieves similar accuracy to the 56B model, but is 20% faster to infer. In addition, we introduce an FP8-based training recipe and show that it can achieve on par results with BF16-based training. This recipe is used to train the 56B model. We are releasing Nemotron-H base model checkpoints with support in Hugging Face and NeMo.

</details>


### [119] [Self-Critique and Refinement for Faithful Natural Language Explanations](https://arxiv.org/abs/2505.22823)

*Yingming Wang, Pepa Atanasova*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Explanations, Self-critique, Refinement, Feature Attribution

**Relevance Score:** 9

**TL;DR:** The paper introduces SR-NLE, a framework for LLMs to improve the faithfulness of their natural language explanations through an iterative self-critique and refinement process, reducing unfaithfulness rates significantly without external supervision.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the faithfulness of natural language explanations generated by large language models, which often do not accurately represent the model's reasoning processes.

**Method:** The SR-NLE framework utilizes iterative critique and refinement mechanisms, including natural language self-feedback and feature attribution feedback, to enhance explanation quality.

**Key Contributions:**

	1. Introduction of the SR-NLE framework for self-critique and refinement of explanations.
	2. Demonstration of significant reduction in unfaithfulness rates in LLM explanations.
	3. Utilization of novel feedback mechanisms, including feature attribution to improve explanation quality.

**Result:** SR-NLE significantly reduces unfaithfulness rates across three datasets and four state-of-the-art LLMs, achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline models.

**Limitations:** 

**Conclusion:** The framework shows that LLMs can refine their explanations to better match their reasoning processes with appropriate guidance, without needing additional training or fine-tuning.

**Abstract:** With the rapid development of Large Language Models (LLMs), Natural Language Explanations (NLEs) have become increasingly important for understanding model predictions. However, these explanations often fail to faithfully represent the model's actual reasoning process. While existing work has demonstrated that LLMs can self-critique and refine their initial outputs for various tasks, this capability remains unexplored for improving explanation faithfulness. To address this gap, we introduce Self-critique and Refinement for Natural Language Explanations (SR-NLE), a framework that enables models to improve the faithfulness of their own explanations -- specifically, post-hoc NLEs -- through an iterative critique and refinement process without external supervision. Our framework leverages different feedback mechanisms to guide the refinement process, including natural language self-feedback and, notably, a novel feedback approach based on feature attribution that highlights important input words. Our experiments across three datasets and four state-of-the-art LLMs demonstrate that SR-NLE significantly reduces unfaithfulness rates, with our best method achieving an average unfaithfulness rate of 36.02%, compared to 54.81% for baseline -- an absolute reduction of 18.79%. These findings reveal that the investigated LLMs can indeed refine their explanations to better reflect their actual reasoning process, requiring only appropriate guidance through feedback without additional training or fine-tuning.

</details>


### [120] [TreeReview: A Dynamic Tree of Questions Framework for Deep and Efficient LLM-based Scientific Peer Review](https://arxiv.org/abs/2506.07642)

*Yuan Chang, Ziyue Li, Hengyuan Zhang, Yuanbo Kong, Yanru Wu, Hayden Kwok-Hay So, Zhijiang Guo, Liya Zhu, Ngai Wong*

**Main category:** cs.CL

**Keywords:** Large Language Models, peer review, hierarchical question-answering

**Relevance Score:** 9

**TL;DR:** TreeReview is a framework that enhances peer review by modeling it as a hierarchical question-answering process, overcoming limitations in LLMs for generating detailed reviews.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM methods for peer review lack thoroughness and efficiency, making it challenging to produce insightful evaluations.

**Method:** TreeReview constructs a tree of review questions and resolves them iteratively, incorporating dynamic question expansion for deeper probing.

**Key Contributions:**

	1. Introduction of a hierarchical question-answering model for peer review
	2. Dynamic question expansion for deeper probing
	3. Significant reduction in LLM token usage during review generation

**Result:** TreeReview shows significant improvement in generating comprehensive review feedback while reducing LLM token usage by up to 80% compared to traditional methods.

**Limitations:** 

**Conclusion:** The method outperforms strong baselines, demonstrating its effectiveness in providing expert-aligned review feedback in a more efficient manner.

**Abstract:** While Large Language Models (LLMs) have shown significant potential in assisting peer review, current methods often struggle to generate thorough and insightful reviews while maintaining efficiency. In this paper, we propose TreeReview, a novel framework that models paper review as a hierarchical and bidirectional question-answering process. TreeReview first constructs a tree of review questions by recursively decomposing high-level questions into fine-grained sub-questions and then resolves the question tree by iteratively aggregating answers from leaf to root to get the final review. Crucially, we incorporate a dynamic question expansion mechanism to enable deeper probing by generating follow-up questions when needed. We construct a benchmark derived from ICLR and NeurIPS venues to evaluate our method on full review generation and actionable feedback comments generation tasks. Experimental results of both LLM-based and human evaluation show that TreeReview outperforms strong baselines in providing comprehensive, in-depth, and expert-aligned review feedback, while reducing LLM token usage by up to 80% compared to computationally intensive approaches. Our code and benchmark dataset are available at https://github.com/YuanChang98/tree-review.

</details>
