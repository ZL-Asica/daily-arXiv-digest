# 2025-05-01

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 13]

- [cs.CL](#cs.CL) [Total: 62]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Countering underproduction of peer produced goods](https://arxiv.org/abs/2504.21240)

*Kaylea Champion, Benjamin Mako Hill*

**Main category:** cs.HC

**Keywords:** peer production, underproduction, Wikipedia, contributor behavior, experienced contributors

**Relevance Score:** 4

**TL;DR:** This paper examines how experienced contributors in peer production, particularly in Wikipedia, tend to contribute to underproduced goods and highlights the importance of retaining these contributors to counteract underproduction.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the phenomenon of underproduction in peer-produced goods and explore ways to mitigate it.

**Method:** Utilized a longitudinal dataset from English Wikipedia to analyze contributor behavior over time, particularly focusing on experienced contributors and their contributions to underproduced goods.

**Key Contributions:** Identifies the tendency of experienced contributors to engage with underproduced goods., Highlights the evolution of contributors' efforts towards underproduction over time., Emphasizes the significance of contributor retention in mitigating underproduction.

**Result:** Findings indicate that more experienced contributors are inclined to focus their efforts on underproduced goods, demonstrating a shift in their contributions over time.

**Limitations:** The study focuses specifically on Wikipedia, which may limit the generalizability of the findings to other peer production platforms.

**Future Work:** Investigating strategies for improving the quality of contributions to underproduced goods and understanding the engagement of new contributors.

**Conclusion:** Retaining experienced contributors, including those without accounts, is essential for addressing underproduction in peer production environments.

**Abstract:** Peer produced goods such as online knowledge bases and free/libre open source software rely on contributors who often choose their tasks regardless of consumer needs. These goods are susceptible to underproduction: when popular goods are relatively low quality. Although underproduction is a common feature of peer production, very little is known about how to counteract it. We use a detailed longitudinal dataset from English Wikipedia to show that more experienced contributors -- including those who contribute without an account -- tend to contribute to underproduced goods. A within-person analysis shows that contributors' efforts shift toward underproduced goods over time. These findings illustrate the value of retaining contributors in peer production, including those contributing without accounts, as a means to counter underproduction.

</details>


### [2] [Passive Measurement of Autonomic Arousal in Real-World Settings](https://arxiv.org/abs/2504.21242)

*Samy Abdel-Ghaffar, Isaac Galatzer-Levy, Conor Heneghan, Xin Liu, Sarah Kernasovskiy, Brennan Garrett, Andrew Barakat, Daniel McDuff*

**Main category:** cs.HC

**Keywords:** autonomic nervous system, stress measurement, Fitbit Body Response Algorithm, real-world validation, health monitoring

**Relevance Score:** 7

**TL;DR:** The paper presents the Fitbit Body Response Algorithm for continuous remote measurement of autonomic nervous system (ANS) activation using wrist-based sensors, validated through stress tests and assessments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of validated methods for measuring ANS activity in real-world contexts, which is important for understanding stress effects on health.

**Method:** The Fitbit Body Response Algorithm was validated through two experiments: a Trier Social Stress Test and ecological momentary assessments, analyzing data from 132 participants.

**Key Contributions:** Introduction of the Fitbit Body Response Algorithm for measuring ANS activation remotely., Validation via real-world experiments demonstrating high model accuracy., Discussion on challenges faced when sensing in non-laboratory settings.

**Result:** The model achieved an accuracy of 0.85 in predicting perceived stress when using all available sensor modalities, outperforming models with limited signal access.

**Limitations:** The study may be limited by the specific populations tested and the contexts in which stress was measured.

**Future Work:** Future research could explore additional sensor modalities and apply the algorithm across diverse populations and stress-inducing scenarios.

**Conclusion:** The study demonstrates the feasibility of using wrist-based sensors for effective monitoring of ANS activity in everyday environments, while discussing real-world sensing challenges.

**Abstract:** The autonomic nervous system (ANS) is activated during stress, which can have negative effects on cardiovascular health, sleep, the immune system, and mental health. While there are ways to quantify ANS activity in laboratories, there is a paucity of methods that have been validated in real-world contexts. We present the Fitbit Body Response Algorithm, an approach to continuous remote measurement of ANS activation through widely available remote wrist-based sensors. The design was validated via two experiments, a Trier Social Stress Test (n = 45) and ecological momentary assessments (EMA) of perceived stress (n=87), providing both controlled and ecologically valid test data. Model performance predicting perceived stress when using all available sensor modalities was consistent with expectations (accuracy=0.85) and outperformed models with access to only a subset of the signals. We discuss and address challenges to sensing that arise in real world settings that do not present in conventional lab environments.

</details>


### [3] [MagicCraft: Natural Language-Driven Generation of Dynamic and Interactive 3D Objects for Commercial Metaverse Platforms](https://arxiv.org/abs/2504.21332)

*Ryutaro Kurai, Takefumi Hiraki, Yuichi Hiroi, Yutaro Hirao, Monica Perusquía-Hernández, Hideaki Uchiyama, Kiyoshi Kiyokawa*

**Main category:** cs.HC

**Keywords:** 3D modeling, metaverse, generative AI, user interface, content creation

**Relevance Score:** 6

**TL;DR:** MagicCraft enables users to generate 3D objects from natural language prompts, simplifying 3D content creation for metaverse platforms.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of creating dynamic and interactive 3D objects without requiring advanced modeling and programming skills.

**Method:** MagicCraft uses generative AI models to convert text descriptions into images, which are then transformed into 3D models, while also predicting object behavior and assigning attributes and scripts. It includes an interface for users to refine the generated objects.

**Key Contributions:** Development of MagicCraft system for generating 3D objects from natural language prompts., Significant reduction in the skill threshold for 3D content creation., Positive feedback from both expert designers and novice users on the system's usability.

**Result:** Users with no prior experience successfully created complex, interactive objects, significantly reducing the time and skill required for 3D model creation. Evaluations by experts highlighted improvements in content creation workflows.

**Limitations:** 

**Future Work:** 

**Conclusion:** MagicCraft enhances accessibility in 3D content creation on metaverse platforms, allowing broader participation in the design process.

**Abstract:** Metaverse platforms are rapidly evolving to provide immersive spaces for user interaction and content creation. However, the generation of dynamic and interactive 3D objects remains challenging due to the need for advanced 3D modeling and programming skills. To address this challenge, we present MagicCraft, a system that generates functional 3D objects from natural language prompts for metaverse platforms. MagicCraft uses generative AI models to manage the entire content creation pipeline: converting user text descriptions into images, transforming images into 3D models, predicting object behavior, and assigning necessary attributes and scripts. It also provides an interactive interface for users to refine generated objects by adjusting features such as orientation, scale, seating positions, and grip points.   Implemented on Cluster, a commercial metaverse platform, MagicCraft was evaluated by 7 expert CG designers and 51 general users. Results show that MagicCraft significantly reduces the time and skill required to create 3D objects. Users with no prior experience in 3D modeling or programming successfully created complex, interactive objects and deployed them in the metaverse. Expert feedback highlighted the system's potential to improve content creation workflows and support rapid prototyping. By integrating AI-generated content into metaverse platforms, MagicCraft makes 3D content creation more accessible.

</details>


### [4] [Cross-Reality Lifestyle: Integrating Physical and Virtual Lives through Multi-Platform Metaverse](https://arxiv.org/abs/2504.21337)

*Yuichi Hiroi, Yuji Hatada, Takefumi Hiraki*

**Main category:** cs.HC

**Keywords:** cross-reality lifestyles, metaverse, virtual reality, physical activities, user experience

**Relevance Score:** 8

**TL;DR:** The paper explores the integration of physical and virtual activities in cross-reality environments, proposing a framework for developing applications that enhance user experiences using metaverse technologies.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates how technology is reshaping the interaction between physical and virtual spaces, particularly in the context of modern multi-platform metaverse environments.

**Method:** The paper introduces the concept of 'cross-reality lifestyles' and identifies three patterns of integration: amplification, complementary, and emergence. A technical framework for content design, platform infrastructure, and device interfaces is proposed based on analysis of commercial platforms.

**Key Contributions:** Introduction of the concept of 'cross-reality lifestyles'., Identification of three patterns of integration for physical and virtual activities., Development of a technical framework for creating cross-reality applications.

**Result:** The analysis demonstrates how users combine physical and virtual activities, providing insights into user engagement in cross-reality environments.

**Limitations:** 

**Future Work:** Further exploration of user engagement metrics and long-term impacts of cross-reality lifestyles on society is needed.

**Conclusion:** The proposed framework can guide the development of applications that leverage cross-reality experiences, highlighting the importance of integrated design in metaverse technologies.

**Abstract:** Technological advances are redefining the relationship between physical and virtual space. Traditionally, when users engage in virtual reality (VR), they are completely cut off from the physical space; similarly, they are unable to access virtual experiences while engaged in physical activities. However, modern multi-platform metaverse environments allow simultaneous participation through mobile devices, creating new opportunities for integrated experiences. This study introduces the concept of "cross-reality lifestyles" to examine how users actively combine their physical and virtual activities. We identify three patterns of integration: 1) amplification: one space enhances experiences in the other; 2) complementary: spaces offer different but equally valuable alternatives; and 3) emergence: simultaneous engagement creates entirely new experiences. By analyzing commercial platforms, we create a technical framework that addresses content design, platform infrastructure, and device interfaces. This framework guides the development of cross-reality applications while demonstrating how metaverse technologies blur the traditional boundaries between physical and virtual experiences.

</details>


### [5] [ImaginateAR: AI-Assisted In-Situ Authoring in Augmented Reality](https://arxiv.org/abs/2504.21360)

*Jaewook Lee, Filippo Aleotti, Diego Mazala, Guillermo Garcia-Hernando, Sara Vicente, Oliver James Johnston, Isabel Kraus-Liang, Jakub Powierza, Donghoon Shin, Jon E. Froehlich, Gabriel Brostow, Jessica Van Brummelen*

**Main category:** cs.HC

**Keywords:** augmented reality, AI-assisted authoring, 3D asset generation, human-computer interaction, user study

**Relevance Score:** 8

**TL;DR:** ImaginateAR is an AI-assisted AR authoring system that allows users to create personalized AR content simply through voice commands, enhancing creative flexibility.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the difficulty non-experts face in authoring personalized AR content, which traditionally requires specialized knowledge and tools.

**Method:** ImaginateAR employs custom pipelines for offline scene understanding and fast 3D asset generation, combined with LLM-driven speech interaction, enabling users to create scenes by describing them verbally.

**Key Contributions:** Introduction of an AI-driven AR authoring tool that emphasizes voice interaction and user creativity., Technical improvements over prior methods in scene understanding and asset generation., User study reveals insights on user preferences and design implications for future AR tools.

**Result:** ImaginateAR produces more accurate outdoor scene graphs and generates 3D meshes faster than previous methods, demonstrating its enhanced capabilities for AR content creation.

**Limitations:** The study was limited to a small sample size (N=20) and focused on outdoor scenes, which may not generalize to all AR environments.

**Future Work:** Exploration of how AI can support more diverse AR authoring tasks and investigation of user interactions in various contexts.

**Conclusion:** The findings suggest preferred roles for AI in creativity and provide insights for future AR authoring tools based on user interactions and preferences.

**Abstract:** While augmented reality (AR) enables new ways to play, tell stories, and explore ideas rooted in the physical world, authoring personalized AR content remains difficult for non-experts, often requiring professional tools and time. Prior systems have explored AI-driven XR design but typically rely on manually-defined environments and fixed asset libraries, limiting creative flexibility and real-world relevance. We introduce ImaginateAR, a mobile AI-assisted AR authoring system that aims to let anyone build anything, anywhere -- simply by speaking their imagination. ImaginateAR is powered by custom pipelines for offline scene understanding, fast 3D asset generation, and LLM-driven speech interaction. Users might say "a dragon enjoying a campfire" (P7) and iteratively refine the scene using both AI and manual tools. Our technical evaluation shows that ImaginateAR produces more accurate outdoor scene graphs and generates 3D meshes faster than prior methods. A three-part user study (N=20) revealed preferred roles for AI in authoring, what and how users create in free-form use, and design implications for future AR authoring tools.

</details>


### [6] [Coping with Uncertainty in UX Design Practice: Practitioner Strategies and Judgment](https://arxiv.org/abs/2504.21397)

*Prakash Shukla, Phuong Bui, Paul Parsons*

**Main category:** cs.HC

**Keywords:** UX design, uncertainty, design judgment, adaptive strategies, stakeholder dynamics

**Relevance Score:** 6

**TL;DR:** This study investigates how UX practitioners navigate uncertainty in design through a diary study and interviews, identifying strategies such as adaptive framing and negotiation.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the specific character of uncertainty in UX design practice, which has been underexplored compared to related fields.

**Method:** Multi-week diary study and follow-up interviews with ten UX designers.

**Key Contributions:** Identifies specific strategies employed by UX practitioners to deal with uncertainty, Introduces concepts of temporal and sacrificial judgment in design, Sheds light on the situated nature of uncertainty in UX design practice

**Result:** The study identifies various strategies used by designers, including adaptive framing and negotiation, to handle uncertainty in their projects.

**Limitations:** 

**Future Work:** Further research could explore the implications of these findings for design education and practice in different contexts.

**Conclusion:** UX practitioners engage with uncertainty as a persistent and situated feature of their practice, highlighting the importance of design judgment.

**Abstract:** The complexity of UX design practice extends beyond ill-structured design problems to include uncertainties shaped by shifting stakeholder priorities, team dynamics, limited resources, and implementation constraints. While prior research in related fields has addressed uncertainty in design more broadly, the specific character of uncertainty in UX practice remains underexplored. This study examines how UX practitioners experience and respond to uncertainty in real-world projects, drawing on a multi-week diary study and follow-up interviews with ten designers. We identify a range of practitioner strategies-including adaptive framing, negotiation, and judgment-that allow designers to move forward amid ambiguity. Our findings highlight the central role of design judgment in navigating uncertainty, including emergent forms such as temporal and sacrificial judgment, and extend prior understandings by showing how UX practitioners engage uncertainty as a persistent, situated feature of practice.

</details>


### [7] [A Comprehensive Survey of Electrical Stimulation Haptic Feedback in Human-Computer Interaction](https://arxiv.org/abs/2504.21477)

*Simin Yang, Xian Wang, Yang Li, Lik-Hang Lee, Tristan Camille, Pan Hui*

**Main category:** cs.HC

**Keywords:** Haptic Feedback, Human-Computer Interaction, Electrical Haptics, Systematic Review, Feedback Modalities

**Relevance Score:** 8

**TL;DR:** This paper reviews advancements in electrical haptic feedback within HCI, analyzing 110 research papers to explore trends, challenges, and applications.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive understanding of the current state of research in electrical haptic interaction and its applications in HCI.

**Method:** A systematic literature review of 110 research papers focusing on haptic devices, perception mechanisms, and feedback modalities.

**Key Contributions:** Systematic analysis of 110 research papers, Insights into current trends and challenges in electrical haptic feedback, Identification of integration opportunities with other feedback modalities

**Result:** The study highlights significant trends and challenges in electrical haptic feedback and identifies the integration of haptic modalities as an emerging area of exploration.

**Limitations:** 

**Future Work:** Future directions include exploring the integration of electrical haptic feedback with other sensory modalities and addressing existing challenges in the field.

**Conclusion:** The findings emphasize the critical role of electrical haptic feedback in enhancing interactive experiences and suggest directions for future research.

**Abstract:** Haptic perception and feedback play a pivotal role in interactive experiences, forming an essential component of human-computer interaction (HCI). In recent years, the field of haptic interaction has witnessed significant advancements, particularly in the area of electrical haptic feedback, driving innovation across various domains. To gain a comprehensive understanding of the current state of research and the latest developments in electrical haptic interaction, this study systematically reviews the literature in this area. Our investigation covers key aspects including haptic devices, haptic perception mechanisms, the comparison and integration of electrical haptic feedback with other feedback modalities, and their diverse applications. Specifically, we conduct a systematic analysis of 110 research papers to explore the forefront of electrical haptic feedback, providing insights into its latest trends, challenges, and future directions.

</details>


### [8] [A User-Centered Teleoperation GUI for Automated Vehicles: Identifying and Evaluating Information Requirements for Remote Driving and Assistance](https://arxiv.org/abs/2504.21563)

*Maria-Magdalena Wolf, Henrik Schmidt, Michael Christl, Jana Fank, Frank Diermeyer*

**Main category:** cs.HC

**Keywords:** Teleoperation, Graphical User Interface, Situational Awareness, User Experience, Usability

**Relevance Score:** 6

**TL;DR:** This paper discusses the development and evaluation of a GUI for teleoperation, emphasizing situational awareness through visual perception and informational elements. A dynamic GUI prototype outperforms a static version in usability and task completion time.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Teleoperation serves as a fallback for automated vehicles but faces challenges related to situational awareness, necessitating effective GUI design to enhance operator perception.

**Method:** The study involved expert interviews (N = 9) to identify key informational elements for teleoperation, followed by the development of a static and dynamic GUI prototype. A click-dummy study was conducted with participants (N = 36) to evaluate both GUI versions.

**Key Contributions:** Identification of key informational elements for teleoperation GUIs through expert interviews., Development of both static and dynamic GUI prototypes for teleoperation., Evaluation showing superior performance of dynamic GUIs in usability tests.

**Result:** The dynamic GUI significantly outperformed the static version in terms of usability and task completion time, achieving good System Usability Scale ratings. However, the User Experience Questionnaire score indicated room for improvement.

**Limitations:** The study does not evaluate the GUI in a real vehicle context, limiting the understanding of its practical applicability.

**Future Work:** Future research should focus on evaluating the GUI in conjunction with actual vehicle teleoperation.

**Conclusion:** The study highlights the importance of GUI design in teleoperation and suggests future evaluation of the GUI with real vehicle interaction to enhance user experience further.

**Abstract:** Teleoperation emerged as a promising fallback for situations beyond the capabilities of automated vehicles. Nevertheless, teleoperation still faces challenges, such as reduced situational awareness. Since situational awareness is primarily built through the remote operator's visual perception, the Graphical User Interface (GUI) design is critical. In addition to video feeds, supplemental informational elements are crucial - not only for the predominantly studied Remote Driving but also for the arising desk-based Remote Assistance concepts. This work develops a GUI for different teleoperation concepts by identifying key informational elements during the teleoperation process through expert interviews (N = 9). Following this, a static and dynamic GUI prototype is developed and evaluated in a click-dummy study (N = 36). Thereby, the dynamic GUI adapts the number of displayed elements according to the teleoperation phase. Results show that both GUIs achieve good System Usability Scale (SUS) ratings, with the dynamic GUI significantly outperforming the static version in both usability and task completion time. The User Experience Questionnaire (UEQ) score shows potential for improvement. To enhance the user experience, the GUI should be evaluated in a follow-up study that includes interaction with a real vehicle.

</details>


### [9] [A Conversational Approach to Well-being Awareness Creation and Behavioural Intention](https://arxiv.org/abs/2504.21702)

*Antonia Azzini, Ilaria Baroni, Irene Celino*

**Main category:** cs.HC

**Keywords:** digital tools, conversational agents, well-being, behavioral change, intrinsic motivation

**Relevance Score:** 8

**TL;DR:** This study examines the efficacy of a digital conversational tool, Allegra, in promoting healthy lifestyles through effective communication and user interaction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how digital tools can influence awareness and behavioral change towards a healthier lifestyle.

**Method:** Participants interacted with a conversational application named Allegra, designed to simulate a well-being counselor and structured interactions based on three different conversational styles. Afterwards, they completed a questionnaire regarding their motivation and behavioral change.

**Key Contributions:** Development of a digital conversational tool for well-being promotion, Identification of intrinsic motivation as a critical factor in behavioral change, Evaluation of conversational styles and their impact on user engagement

**Result:** The study found that intrinsic motivation positively influences awareness creation and behavioral intention for a healthy lifestyle, but different communication styles did not have a significant effect.

**Limitations:** The study did not find statistically significant results for different conversational styles, suggesting complexity in user interactions that may require further investigation.

**Future Work:** Future research should explore other elements of conversational design that could enhance user engagement and effectiveness of digital well-being tools.

**Conclusion:** Overall, intrinsic motivation is a key factor in promoting well-being through digital tools, while the choice of conversational style has limited impact on user outcomes.

**Abstract:** The promotion of a healthy lifestyle is one of the main drivers of an individual's overall physical and psycho-emotional well-being. Digital technologies are more and more adopted as ''facilitators'' for this goal, to raise awareness and solicit healthy lifestyle habits.   This study aims to experiment the effects of the adoption of a digital conversational tool to influence awareness creation and behavioural change in the context of a well-being lifestyle. Our aim is to collect evidence of the aspects that must be taken into account when designing and implementing such tools in well-being promotion campaigns.   To this end, we created a conversational application for promoting well-being and healthy lifestyles, which presents relevant information and asks specific questions to its intended users within an interaction happening through a chat interface; the conversational tool presents itself as a well-being counsellor named Allegra and follows a coaching approach to structure the interaction with the user. In our user study, participants were asked to first interact with Allegra in one of three experimental conditions, corresponding to different conversational styles; then, they answered a questionnaire about their experience. The questionnaire items were related to intrinsic motivation factors as well as awareness creation and behavioural change. The collected data allowed us to assess the hypotheses of our model that put in connection those variables.   Our results confirm the positive effect of intrinsic motivation factors on both awareness creation and behavioural intention in the context of well-being and healthy lifestyle; on the other hand, we did not record any statistically significant effect of different language and communication styles on the outcomes.

</details>


### [10] [Adaptive 3D UI Placement in Mixed Reality Using Deep Reinforcement Learning](https://arxiv.org/abs/2504.21731)

*Feiyu Lu, Mengyu Chen, Hsiang Hsu, Pranav Deshpande, Cheng Yao Wang, Blair MacIntyre*

**Main category:** cs.HC

**Keywords:** mixed reality, reinforcement learning, user experience, 3D content placement, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper explores the use of reinforcement learning for optimizing 3D content placement in Mixed Reality (MR) environments to enhance user experience.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of effective 3D content placement in dynamic Mixed Reality experiences that optimally supports users' tasks.

**Method:** The paper investigates the use of reinforcement learning to continuously place virtual content based on users' poses and surrounding environments.

**Key Contributions:** Exploration of reinforcement learning for continuous 3D content placement in MR., Preliminary evaluation demonstrating RL's effectiveness in optimizing user rewards., Identification of future research directions for personalized MR experiences.

**Result:** Preliminary results show that reinforcement learning can effectively optimize content placement to maximize user rewards while on the move.

**Limitations:** The study is preliminary and requires further validation with more extensive user studies.

**Future Work:** Future research directions include harnessing RL for improved personalized and optimized UI and content placement in MR.

**Conclusion:** Reinforcement learning offers promising avenues for personalizing and optimizing user interfaces and content placement in MR applications.

**Abstract:** Mixed Reality (MR) could assist users' tasks by continuously integrating virtual content with their view of the physical environment. However, where and how to place these content to best support the users has been a challenging problem due to the dynamic nature of MR experiences. In contrast to prior work that investigates optimization-based methods, we are exploring how reinforcement learning (RL) could assist with continuous 3D content placement that is aware of users' poses and their surrounding environments. Through an initial exploration and preliminary evaluation, our results demonstrate the potential of RL to position content that maximizes the reward for users on the go. We further identify future directions for research that could harness the power of RL for personalized and optimized UI and content placement in MR.

</details>


### [11] ['SSL?! What on earth is that?': Towards Designing Age-Inclusive Secure Smartphone Browsing](https://arxiv.org/abs/2403.02145)

*Pavithren V. S. Pakianathan, L. Siddharth, Sujithra Raviselvam, Kristin L. Wood, Hyowon Lee, Pin Sym Foong, Jianying Zhou, Simon Tangi Perrault*

**Main category:** cs.HC

**Keywords:** Phishing, Usability, Trust mechanisms, Human-computer interaction, AI support

**Relevance Score:** 7

**TL;DR:** This study evaluates the effectiveness of current trust mechanisms against phishing websites and proposes a new mechanism incorporating social, community, and AI support.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the rising rates of phishing attacks facilitated by certified phishing websites, the study investigates the usability of existing trust mechanisms designed to protect users.

**Method:** The study involved 30 participants (18 adults and 12 older adults) who were asked to identify a phishing website among genuine sites while browsing with Chrome on Android. Additionally, participants provided feedback on current mechanisms and expressed preferences for support modalities in a conceptualized trust mechanism.

**Key Contributions:** Firsthand evaluation of current online trust mechanisms' usability against phishing sites, Assessment of user preferences for support modalities in a new conceptualized mechanism, Insights on age-specific differences in support preferences for phishing detection.

**Result:** None of the participants successfully identified the phishing website using current trust mechanisms, which they rated poorly in usability. Older adults showed a preference for social support in trust-related decisions.

**Limitations:** Study limited to a small sample size; findings may not generalize across larger populations or different demographics.

**Future Work:** Further research is needed to develop and test improved trust mechanisms that incorporate user preferences and address usability across diverse user groups.

**Conclusion:** Future trust mechanisms should consider age-specific needs and significantly improve usability to effectively support users in recognizing phishing threats.

**Abstract:** Owing to the increase in 'certified' phishing websites, there is a steady increase in the number of phishing cases and general susceptibility to phishing. Trust mechanisms (e.g., HTTPS Lock Indicators, SSL Certificates) that help differentiate genuine and phishing websites should therefore be evaluated for their effectiveness in preventing vulnerable users from accessing phishing websites. In this article, we present a study involving 18 adults (male-6; female-12) and 12 older adults (male-4; female-8) to understand the usability of current trust mechanisms and preferred modalities in a conceptualized mechanism. In the first part of the study, using Chrome browser on Android, we asked the participants to browse a banking website and a government website for digital particulars. We asked them to identify which one of the two was a phishing website, rate the usability of both websites and provide qualitative feedback on the trust mechanisms. In the second part, we conceptualized an alternative trust mechanism, which allows seeking social, community and AI-based support to make website trust-related decisions. Herein, we asked the participants as to which modality (social, community or AI) they prefer to seek support from and why it is preferred. Using the current trust mechanisms, none of the participants were able to identify the phishing website. As the participants rated the current mechanisms poorly in terms of usability, they expressed various difficulties that largely did not differ between adults and older adults. In the conceptualized mechanism, we observed a notable difference in the preferred modalities, in that, older adults primarily preferred social support. In addition to these overall findings, specific observations suggest that future trust mechanisms should not only consider age-specific needs but also incorporate substantial improvement in terms of usability.

</details>


### [12] [Emotive Speech-to-Text Interfaces in XR: A Narrative Review of Psychophysiological and Accessibility Advances](https://arxiv.org/abs/2405.13924)

*Sunday David Ubur, Denis Gracanin*

**Main category:** cs.HC

**Keywords:** Speech-to-Text, Emotional Expression, Extended Reality, Emotion Recognition, Human-Computer Interaction

**Relevance Score:** 8

**TL;DR:** This narrative review discusses advancements and challenges in incorporating emotional expression into Speech-to-Text (STT) systems within Extended Reality (XR), particularly for individuals with hearing impairments.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To identify advancements, limitations, and research gaps in integrating emotional expression into STT technology.

**Method:** A narrative review analyzing relevant articles from 2020 to 2024, categorized into themes related to emotional expression and communication technologies.

**Key Contributions:** Identification of research gaps in emotional expression in STT systems, Discussion on the integration of emotional nuances through animated text and emojilization, Insights into empathic applications in healthcare and education

**Result:** The study identifies innovations in live transcription, closed captioning, AR, VR, and emotion recognition technologies, while highlighting the absence of emotional nuance in current STT outputs as a significant challenge.

**Limitations:** The existing STT technologies still struggle to incorporate emotional nuances, impacting the effectiveness of communication for users.

**Future Work:** Future research should focus on developing STT technologies that can adequately capture emotional nuances and enhance user interaction in XR environments.

**Conclusion:** Innovations are urgently needed to capture emotional expressions in STT technology, particularly for enhancing communication in immersive environments like AR and VR.

**Abstract:** This narrative review on emotional expression in Speech-to-Text (STT) interfaces with Extended Reality (XR) aims to identify advancements, limitations, and research gaps in incorporating emotional expression into transcribed text generated by STT systems. Using a rigorous search strategy, relevant articles published between 2020 and 2024 are extracted and categorized into themes such as communication enhancement technologies, innovations in captioning, visual and affective augmentation, emotion recognition in AR and VR, and empathic machines. The findings reveal the evolution of tools and techniques to meet the needs of individuals with hearing impairments, showcasing innovations in live transcription, closed captioning, AR, VR, and emotion recognition technologies. Despite improvements in accessibility, the absence of emotional nuance in transcribed text remains a significant communication challenge. The study underscores the urgency for innovations in STT technology to capture emotional expressions. The research discusses integrating emotional expression into text through strategies like animated text captions, emojilization tools, and models associating emotions with animation properties. Extending these efforts into AR and VR environments opens new possibilities for immersive and emotionally resonant experiences, especially in educational contexts. The study also explores empathic applications in healthcare, education, and human-robot interactions, highlighting the potential for personalized and effective interactions. The multidisciplinary nature of the literature underscores the potential for collaborative and interdisciplinary research.

</details>


### [13] [ObjectFinder: An Open-Vocabulary Assistive System for Interactive Object Search by Blind People](https://arxiv.org/abs/2412.03118)

*Ruiping Liu, Jiaming Zhang, Angela Schön, Karin Müller, Junwei Zheng, Kailun Yang, Anhong Guo, Kathrin Gerling, Rainer Stiefelhagen*

**Main category:** cs.HC

**Keywords:** assistive technology, object search, blind users, open-vocabulary models, human-computer interaction

**Relevance Score:** 8

**TL;DR:** ObjectFinder is an open-vocabulary wearable system designed to aid blind individuals in searching for objects interactively, providing real-time localization and intent-based information gathering.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Blind individuals face significant challenges in searching for objects in unfamiliar environments, necessitating advanced assistive technologies.

**Method:** ObjectFinder combines open-vocabulary object detection with a multimodal large language model to facilitate flexible user queries and real-time localization of target objects.

**Key Contributions:** Development of an open-vocabulary assistive system for object search, Real-time egocentric localization for enhanced user navigation, User-centered design approach involving blind co-designers

**Result:** In user studies, ObjectFinder notably increased independence and preference among blind participants compared to existing assistive applications, enhancing their ability to gather context and navigate.

**Limitations:** The user study involved only eight participants, which may limit the generalizability of the findings.

**Future Work:** Further research should explore the scalability of ObjectFinder and its application in various environments or scenarios.

**Conclusion:** The study highlights the potential for improving assistive technologies through user-centered design and open-vocabulary systems to support interactive object search.

**Abstract:** Searching for objects in unfamiliar scenarios is a challenging task for blind people. It involves specifying the target object, detecting it, and then gathering detailed information according to the user's intent. However, existing description- and detection-based assistive technologies do not sufficiently support the multifaceted nature of interactive object search tasks. We present ObjectFinder, an open-vocabulary wearable assistive system for interactive object search by blind people. ObjectFinder allows users to query target objects using flexible wording. Once the target object is detected, it provides egocentric localization information in real-time, including distance and direction. Users can then initiate different branches to gather detailed information based on their intent towards the target object, such as navigating to it or perceiving its surroundings. ObjectFinder is powered by a seamless combination of open-vocabulary models, namely an open-vocabulary object detector and a multimodal large language model. The ObjectFinder design concept and its development were carried out in collaboration with a blind co-designer. To evaluate ObjectFinder, we conducted an exploratory user study with eight blind participants. We compared ObjectFinder to BeMyAI and Google Lookout, popular description- and detection-based assistive applications. Our findings indicate that most participants felt more independent with ObjectFinder and preferred it for object search, as it enhanced scene context gathering and navigation, and allowed for active target identification. Finally, we discuss the implications for future assistive systems to support interactive object search.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [14] [Waking Up an AI: A Quantitative Framework for Prompt-Induced Phase Transition in Large Language Models](https://arxiv.org/abs/2504.21012)

*Makoto Sato*

**Main category:** cs.CL

**Keywords:** cognitive dynamics, human intuition, large language models, conceptual fusion, AI responsiveness

**Relevance Score:** 7

**TL;DR:** This paper proposes a method to quantitatively analyze cognitive dynamics of humans and LLMs by comparing their responses to prompts that fuse or separate semantically distant concepts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the underlying cognitive dynamics of intuitive human thinking compared to large language models (LLMs).

**Method:** A two-part framework consisting of Transition-Inducing Prompts (TIP) to trigger changes in LLM responsiveness, and Transition Quantifying Prompts (TQP) to evaluate these changes using a separate LLM in controlled experiments.

**Key Contributions:** Introduction of TIP and TQP for measuring LLM responsiveness, Findings suggest LLMs lack the ability to integrate concepts as humans do, Provides insights into cognitive differences between human and AI thinking

**Result:** Current LLMs showed no significant difference in responsiveness to semantically fused versus non-fused prompts, indicating a gap in replicating human conceptual integration processes.

**Limitations:** The study may not capture the full range of human cognitive processes due to the controlled experimental design.

**Future Work:** Further exploration of LLMs' capabilities in conceptual integration and human-like intuition.

**Conclusion:** The method allows for reproducible measurement of cognitive responsiveness, highlighting differences in intuition and conceptual leaps between humans and LLMs.

**Abstract:** What underlies intuitive human thinking? One approach to this question is to compare the cognitive dynamics of humans and large language models (LLMs). However, such a comparison requires a method to quantitatively analyze AI cognitive behavior under controlled conditions. While anecdotal observations suggest that certain prompts can dramatically change LLM behavior, these observations have remained largely qualitative. Here, we propose a two-part framework to investigate this phenomenon: a Transition-Inducing Prompt (TIP) that triggers a rapid shift in LLM responsiveness, and a Transition Quantifying Prompt (TQP) that evaluates this change using a separate LLM. Through controlled experiments, we examined how LLMs react to prompts embedding two semantically distant concepts (e.g., mathematical aperiodicity and traditional crafts)--either fused together or presented separately--by changing their linguistic quality and affective tone. Whereas humans tend to experience heightened engagement when such concepts are meaningfully blended producing a novel concept--a form of conceptual fusion--current LLMs showed no significant difference in responsiveness between semantically fused and non-fused prompts. This suggests that LLMs may not yet replicate the conceptual integration processes seen in human intuition. Our method enables fine-grained, reproducible measurement of cognitive responsiveness, and may help illuminate key differences in how intuition and conceptual leaps emerge in artificial versus human minds.

</details>


### [15] [Analyzing Feedback Mechanisms in AI-Generated MCQs: Insights into Readability, Lexical Properties, and Levels of Challenge](https://arxiv.org/abs/2504.21013)

*Antoun Yaacoub, Zainab Assaghir, Lionel Prevost, Jérôme Da-Rugna*

**Main category:** cs.CL

**Keywords:** AI-generated feedback, linguistic characteristics, educational technology

**Relevance Score:** 7

**TL;DR:** Study analyzes the linguistic characteristics of AI-generated feedback for computer science MCQs using Google's Gemini model, revealing its adaptability and potential for personalized learning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the linguistic characteristics of AI-generated feedback in educational settings and its impact on learning outcomes.

**Method:** Analyzed a dataset of over 1,200 computer science multiple-choice questions (MCQs) across three difficulty levels and tones, employing a fine-tuned RoBERTa-based model to predict linguistic properties.

**Key Contributions:** Comprehensive analysis of linguistic characteristics of AI-generated feedback., Development of a RoBERTa-based model for predicting feedback attributes., Insights into the interaction between feedback tone and question difficulty.

**Result:** Achieved a Mean Absolute Error of 2.0 for readability and 0.03 for vocabulary richness, with significant interaction effects between feedback tone and question difficulty.

**Limitations:** Focus on a specific AI model and educational context may limit generalizability.

**Future Work:** Further exploration of AI feedback mechanisms across different subjects and educational levels.

**Conclusion:** AI-generated feedback can dynamically adapt to various educational contexts, improving learning outcomes while necessitating ethical considerations in design and deployment.

**Abstract:** Artificial Intelligence (AI)-generated feedback in educational settings has garnered considerable attention due to its potential to enhance learning outcomes. However, a comprehensive understanding of the linguistic characteristics of AI-generated feedback, including readability, lexical richness, and adaptability across varying challenge levels, remains limited. This study delves into the linguistic and structural attributes of feedback generated by Google's Gemini 1.5-flash text model for computer science multiple-choice questions (MCQs). A dataset of over 1,200 MCQs was analyzed, considering three difficulty levels (easy, medium, hard) and three feedback tones (supportive, neutral, challenging). Key linguistic metrics, such as length, readability scores (Flesch-Kincaid Grade Level), vocabulary richness, and lexical density, were computed and examined. A fine-tuned RoBERTa-based multi-task learning (MTL) model was trained to predict these linguistic properties, achieving a Mean Absolute Error (MAE) of 2.0 for readability and 0.03 for vocabulary richness. The findings reveal significant interaction effects between feedback tone and question difficulty, demonstrating the dynamic adaptation of AI-generated feedback within diverse educational contexts. These insights contribute to the development of more personalized and effective AI-driven feedback mechanisms, highlighting the potential for improved learning outcomes while underscoring the importance of ethical considerations in their design and deployment.

</details>


### [16] [Nested Named-Entity Recognition on Vietnamese COVID-19: Dataset and Experiments](https://arxiv.org/abs/2504.21016)

*Ngoc C. Lê, Hai-Chung Nguyen-Phung, Thu-Huong Pham Thi, Hue Vu, Phuong-Thao Nguyen Thi, Thu-Thuy Tran, Hong-Nhung Le Thi, Thuy-Duong Nguyen-Thi, Thanh-Huy Nguyen*

**Main category:** cs.CL

**Keywords:** COVID-19, Vietnam, named-entity recognition, public health, AI for social good

**Relevance Score:** 5

**TL;DR:** The paper discusses a named-entity recognition (NER) study aimed at improving COVID-19 prevention efforts in Vietnam by automating the process of tracing and localizing individuals in contact with confirmed cases.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the effectiveness of COVID-19 prevention efforts in Vietnam through automated traceability and localization of individuals in contact with infected patients, which are currently done manually.

**Method:** The study involves the creation of a manually annotated COVID-19 dataset for Vietnamese, which includes a nested named entity recognition task to define new entity types for the system.

**Key Contributions:** Development of a new dataset for NER specific to COVID-19 in Vietnamese language., Introduction of new entity types for improved task definition in NER applications., Demonstration of NER's potential in enhancing disease prevention strategies.

**Result:** The study provides insights into challenges faced during the manual tracing process and demonstrates the applicability of NER in automating tasks to aid disease prevention.

**Limitations:** The study is focused on a specific geographical area (Vietnam) and may need adaptation for other regions; challenges in dataset annotation and system training may impact generalizability.

**Future Work:** Future research could explore the scalability of the NER system to other languages and regions, as well as the integration of additional AI techniques to enhance prediction accuracy.

**Conclusion:** Automating the traceability and localization process can significantly reduce the workload and improve efficiency in managing COVID-19 prevention efforts in Vietnam.

**Abstract:** The COVID-19 pandemic caused great losses worldwide, efforts are taken place to prevent but many countries have failed. In Vietnam, the traceability, localization, and quarantine of people who contact with patients contribute to effective disease prevention. However, this is done by hand, and take a lot of work. In this research, we describe a named-entity recognition (NER) study that assists in the prevention of COVID-19 pandemic in Vietnam. We also present our manually annotated COVID-19 dataset with nested named entity recognition task for Vietnamese which be defined new entity types using for our system.

</details>


### [17] [ViQA-COVID: COVID-19 Machine Reading Comprehension Dataset for Vietnamese](https://arxiv.org/abs/2504.21017)

*Hai-Chung Nguyen-Phung, Ngoc C. Lê, Van-Chien Nguyen, Hang Thi Nguyen, Thuy Phuong Thi Nguyen*

**Main category:** cs.CL

**Keywords:** COVID-19, machine reading comprehension, Vietnamese, artificial intelligence, dataset

**Relevance Score:** 8

**TL;DR:** The paper introduces ViQA-COVID, the first machine reading comprehension (MRC) dataset focused on COVID-19 for the Vietnamese language, aiming to aid disease prevention through AI.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The COVID-19 pandemic has caused widespread disruption and increased the need for AI applications to support health measures.

**Method:** The authors developed the ViQA-COVID dataset tailored for Vietnamese to facilitate machine reading comprehension in the context of COVID-19.

**Key Contributions:** Introduction of the ViQA-COVID dataset for Vietnamese, First multi-span extraction MRC dataset for Vietnamese, Contributes to better AI applications in health informatics during pandemics.

**Result:** ViQA-COVID provides vital resources for building models and systems that can enhance understanding and response to COVID-19 related queries in Vietnamese.

**Limitations:** 

**Future Work:** Encouraging further research on MRC with Vietnamese and expanding the dataset for broader applications.

**Conclusion:** The dataset is expected to contribute significantly to machine reading comprehension research in Vietnamese, promoting multilingual AI applications.

**Abstract:** After two years of appearance, COVID-19 has negatively affected people and normal life around the world. As in May 2022, there are more than 522 million cases and six million deaths worldwide (including nearly ten million cases and over forty-three thousand deaths in Vietnam). Economy and society are both severely affected. The variant of COVID-19, Omicron, has broken disease prevention measures of countries and rapidly increased number of infections. Resources overloading in treatment and epidemics prevention is happening all over the world. It can be seen that, application of artificial intelligence (AI) to support people at this time is extremely necessary. There have been many studies applying AI to prevent COVID-19 which are extremely useful, and studies on machine reading comprehension (MRC) are also in it. Realizing that, we created the first MRC dataset about COVID-19 for Vietnamese: ViQA-COVID and can be used to build models and systems, contributing to disease prevention. Besides, ViQA-COVID is also the first multi-span extraction MRC dataset for Vietnamese, we hope that it can contribute to promoting MRC studies in Vietnamese and multilingual.

</details>


### [18] [HYPEROFA: Expanding LLM Vocabulary to New Languages via Hypernetwork-Based Embedding Initialization](https://arxiv.org/abs/2504.21018)

*Enes Özeren, Yihong Liu, Hinrich Schütze*

**Main category:** cs.CL

**Keywords:** pre-trained language models, hypernetwork, natural language processing, multi-language, continual pre-training

**Relevance Score:** 8

**TL;DR:** HYPEROFA enhances token embedding initialization for mid- and low-resource languages by utilizing a hypernetwork, improving performance over traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Address suboptimal performance of pre-trained language models on mid- and low-resource languages due to limited exposure during pre-training.

**Method:** HYPEROFA uses a hypernetwork to map from a multilingual word vector space to the token embedding space, allowing for flexible token embeddings for target languages.

**Key Contributions:** Introduction of HYPEROFA for adaptive token embedding initialization., Comparison with OFA showing superior performance in specific contexts., Public availability of code for broader use in the community.

**Result:** HYPEROFA outperforms random initialization and matches or exceeds the performance of the state-of-the-art approach, OFA, in both continual pre-training and downstream tasks.

**Limitations:** The method's performance may still vary based on the specific characteristics of the languages involved.

**Future Work:** Exploring further enhancements and applications of hypernetwork-based approaches in diverse linguistic contexts.

**Conclusion:** The hypernetwork-based initialization provides a more expressive and adaptive solution for embedding target-language tokens, enhancing performance.

**Abstract:** Many pre-trained language models (PLMs) exhibit suboptimal performance on mid- and low-resource languages, largely due to limited exposure to these languages during pre-training. A common strategy to address this is to introduce new tokens specific to the target languages, initialize their embeddings, and apply continual pre-training on target-language data. Among such methods, OFA (Liu et al., 2024a) proposes a similarity-based subword embedding initialization heuristic that is both effective and efficient. However, OFA restricts target-language token embeddings to be convex combinations of a fixed number of source-language embeddings, which may limit expressiveness. To overcome this limitation, we propose HYPEROFA, a hypernetwork-based approach for more adaptive token embedding initialization. The hypernetwork is trained to map from an external multilingual word vector space to the PLMs token embedding space using source-language tokens. Once trained, it can generate flexible embeddings for target-language tokens, serving as a good starting point for continual pretraining. Experiments demonstrate that HYPEROFA consistently outperforms random initialization baseline and matches or exceeds the performance of OFA in both continual pre-training convergence and downstream task performance. We make the code publicly available.

</details>


### [19] [Kill two birds with one stone: generalized and robust AI-generated text detection via dynamic perturbations](https://arxiv.org/abs/2504.21019)

*Yinghan Zhou, Juan Wen, Wanli Peng, Yiming Xue, Ziwei Zhang, Zhengxian Wu*

**Main category:** cs.CL

**Keywords:** AI-generated text, detection, robustness, generalization, reinforcement learning

**Relevance Score:** 8

**TL;DR:** This paper presents DP-Net, a novel method for detecting AI-generated text, emphasizing its generalization and robustness against adversarial attacks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved detection methods for AI-generated text (AIGT) due to concerns about misuse and the limitations of current detection techniques.

**Method:** The proposed method, DP-Net, utilizes dynamic perturbations facilitated by reinforcement learning to enhance both generalization and robustness in AIGT detection.

**Key Contributions:** Introduction of a unified mechanism for AIGT detection addressing both generalization and robustness., Development of DP-Net, which utilizes reinforcement learning for dynamic perturbations., Empirical validation showing DP-Net's superior performance against state-of-the-art methods.

**Result:** DP-Net demonstrated superior performance in generalization across three cross-domain scenarios and exhibited enhanced robustness against two types of text adversarial attacks compared to existing methods.

**Limitations:** The paper may not address the implications of using DP-Net in real-world applications extensively.

**Future Work:** Future research could explore additional scenarios and further enhancements to the robustness of the method.

**Conclusion:** The findings support that DP-Net is an effective solution for addressing the dual challenges of generalization and robustness in AIGT detection tasks, thereby contributing valuable techniques in the field.

**Abstract:** The growing popularity of large language models has raised concerns regarding the potential to misuse AI-generated text (AIGT). It becomes increasingly critical to establish an excellent AIGT detection method with high generalization and robustness. However, existing methods either focus on model generalization or concentrate on robustness. The unified mechanism, to simultaneously address the challenges of generalization and robustness, is less explored. In this paper, we argue that robustness can be view as a specific form of domain shift, and empirically reveal an intrinsic mechanism for model generalization of AIGT detection task. Then, we proposed a novel AIGT detection method (DP-Net) via dynamic perturbations introduced by a reinforcement learning with elaborated reward and action. Experimentally, extensive results show that the proposed DP-Net significantly outperforms some state-of-the-art AIGT detection methods for generalization capacity in three cross-domain scenarios. Meanwhile, the DP-Net achieves best robustness under two text adversarial attacks. The code is publicly available at https://github.com/CAU-ISS-Lab/AIGT-Detection-Evade-Detection/tree/main/DP-Net.

</details>


### [20] [Context-Enhanced Contrastive Search for Improved LLM Text Generation](https://arxiv.org/abs/2504.21020)

*Jaydip Sen, Rohit Pandey, Hetvi Waghela*

**Main category:** cs.CL

**Keywords:** Large Language Models, Natural Language Processing, Contrastive Search

**Relevance Score:** 9

**TL;DR:** The paper introduces Context-Enhanced Contrastive Search (CECS), an enhancement of the Contrastive Search algorithm that improves text generation quality by balancing coherence, diversity, and relevance.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in generating high-quality long-form text using traditional decoding methods that often produce incoherent or repetitive outputs.

**Method:** The proposed CECS introduces dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control to optimize text generation.

**Key Contributions:** Introduction of dynamic contextual importance weighting, Development of multi-level Contrastive Search, Implementation of adaptive temperature control

**Result:** CECS outperformed existing Contrastive Search techniques, showing significant improvements in coherence and relevance as measured by BLEU, ROUGE, and semantic similarity.

**Limitations:** 

**Future Work:** Further exploration of CECS applications in various domains and enhancement of the algorithm for broader use cases.

**Conclusion:** CECS demonstrates potential for practical applications in areas like legal document drafting and customer service chatbots due to its enhanced text generation capabilities.

**Abstract:** Recently, Large Language Models (LLMs) have demonstrated remarkable advancements in Natural Language Processing (NLP). However, generating high-quality text that balances coherence, diversity, and relevance remains challenging. Traditional decoding methods, such as bean search and top-k sampling, often struggle with either repetitive or incoherent outputs, particularly in tasks that require long-form text generation. To address these limitations, the paper proposes a novel enhancement of the well-known Contrastive Search algorithm, Context-Enhanced Contrastive Search (CECS) with contextual calibration. The proposed scheme introduces several novelties including dynamic contextual importance weighting, multi-level Contrastive Search, and adaptive temperature control, to optimize the balance between fluency, creativity, and precision. The performance of CECS is evaluated using several standard metrics such as BLEU, ROUGE, and semantic similarity. Experimental results demonstrate significant improvements in both coherence and relevance of the generated texts by CECS outperforming the existing Contrastive Search techniques. The proposed algorithm has several potential applications in the real world including legal document drafting, customer service chatbots, and content marketing.

</details>


### [21] [ConformalNL2LTL: Translating Natural Language Instructions into Temporal Logic Formulas with Conformal Correctness Guarantees](https://arxiv.org/abs/2504.21022)

*Jun Wang, David Smith Sundarsingh, Jyotirmoy V. Deshmukh, Yiannis Kantaros*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Linear Temporal Logic, Robotics, Conformal Prediction, LLM

**Relevance Score:** 8

**TL;DR:** Introduction of ConformalNL2LTL, a method for translating Natural Language to Linear Temporal Logic that incorporates uncertainty-aware translation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To reduce the effort in defining LTL-encoded tasks for robotic applications by translating NL instructions accurately and reliably.

**Method:** The new method constructs LTL formulas iteratively through a sequence of open-vocabulary Question-Answering problems using LLMs, incorporating conformal prediction for uncertainty quantification.

**Key Contributions:** Development of ConformalNL2LTL for NL-to-LTL translation., Utilization of conformal prediction for uncertainty assessment in LLM outputs., Demonstration of the method's effectiveness in achieving user-defined accuracy goals.

**Result:** ConformalNL2LTL achieves specified user-defined translation accuracy while minimizing the need for external help during the translation process.

**Limitations:** The method's performance may vary based on the complexity of NL commands and the underlying LLM model's capabilities.

**Future Work:** Exploration of further enhancements in translation accuracy and expanded vocabulary handling in future applications.

**Conclusion:** The proposed method offers a systematic approach to mitigate the challenges of manual LTL task definition, ensuring better accuracy and confidence in translation.

**Abstract:** Linear Temporal Logic (LTL) has become a prevalent specification language for robotic tasks. To mitigate the significant manual effort and expertise required to define LTL-encoded tasks, several methods have been proposed for translating Natural Language (NL) instructions into LTL formulas, which, however, lack correctness guarantees. To address this, we introduce a new NL-to-LTL translation method, called ConformalNL2LTL, that can achieve user-defined translation success rates over unseen NL commands. Our method constructs LTL formulas iteratively by addressing a sequence of open-vocabulary Question-Answering (QA) problems with LLMs. To enable uncertainty-aware translation, we leverage conformal prediction (CP), a distribution-free uncertainty quantification tool for black-box models. CP enables our method to assess the uncertainty in LLM-generated answers, allowing it to proceed with translation when sufficiently confident and request help otherwise. We provide both theoretical and empirical results demonstrating that ConformalNL2LTL achieves user-specified translation accuracy while minimizing help rates.

</details>


### [22] [Param$Δ$ for Direct Weight Mixing: Post-Train Large Language Model at Zero Cost](https://arxiv.org/abs/2504.21023)

*Sheng Cao, Mingrui Wu, Karthik Prasad, Yuandong Tian, Zechun Liu*

**Main category:** cs.CL

**Keywords:** large language models, post-training, model development, machine learning, knowledge transfer

**Relevance Score:** 8

**TL;DR:** Introduces $Param\Delta$, a method that streamlines post-training of language models by transferring knowledge from existing models without additional training, achieving high performance with reduced costs.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance post-training phase efficiency for large language models while tackling issues like overfitting and high computational costs.

**Method:** Computes the difference between weights of a post-trained model and a base model, adding this difference to the updated base model weights to create a new model without additional training.

**Key Contributions:** Introduces a novel framework for post-training in language models without additional training., Achieves high performance comparable to traditional post-training methods., Offers a cost-effective approach to facilitate the iterative cycle of model development.

**Result:** $Param\Delta$ Model demonstrates performance comparable to direct post-training methods, achieving approximately 95% performance of a traditional post-trained model using various models including Llama3 and Llama3.1.

**Limitations:** 

**Future Work:** Further exploration of $Param\Delta$ across diverse model architectures and applications in various domains.

**Conclusion:** $Param\Delta$ provides a cost-efficient framework that leverages model updates to accelerate iterative development in the open-weight community.

**Abstract:** The post-training phase of large language models is essential for enhancing capabilities such as instruction-following, reasoning, and alignment with human preferences. However, it demands extensive high-quality data and poses risks like overfitting, alongside significant computational costs due to repeated post-training and evaluation after each base model update. This paper introduces $Param\Delta$, a novel method that streamlines post-training by transferring knowledge from an existing post-trained model to a newly updated base model with ZERO additional training. By computing the difference between post-trained model weights ($\Theta_\text{post}$) and base model weights ($\Theta_\text{base}$), and adding this to the updated base model ($\Theta'_\text{base}$), we define $Param\Delta$ Model as: $\Theta_{\text{Param}\Delta} = \Theta_\text{post} - \Theta_\text{base} + \Theta'_\text{base}$. This approach surprisingly equips the new base model with post-trained capabilities, achieving performance comparable to direct post-training. We did analysis on LLama3, Llama3.1, Qwen, and DeepSeek-distilled models. Results indicate $Param\Delta$ Model effectively replicates traditional post-training. For example, the $Param\Delta$ Model obtained from 70B Llama3-inst, Llama3-base, Llama3.1-base models attains approximately 95\% of Llama3.1-inst model's performance on average. $Param\Delta$ brings a new perspective on how to fully leverage models in the open-weight community, where checkpoints for base and instruct models are readily available and frequently updated, by providing a cost-free framework to accelerate the iterative cycle of model development.

</details>


### [23] [WebEvolver: Enhancing Web Agent Self-Improvement with Coevolving World Model](https://arxiv.org/abs/2504.21024)

*Tianqing Fang, Hongming Zhang, Zhisong Zhang, Kaixin Ma, Wenhao Yu, Haitao Mi, Dong Yu*

**Main category:** cs.CL

**Keywords:** autonomous agents, world model, large language model, self-improvement, performance enhancement

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel framework for enhancing agent self-improvement through the use of a co-evolving World Model LLM, addressing performance stagnation in autonomous learning environments.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the stagnation in performance that occurs during autonomous learning cycles in web environments, stemming from limited exploration and exploitation of pre-trained web knowledge.

**Method:** The proposed framework introduces a co-evolving World Model LLM that predicts observations based on current actions and states. It serves as both a virtual web server for generating training data and as a simulation engine for action selection during inference.

**Key Contributions:** Introduction of a co-evolving World Model LLM for agent self-improvement., Demonstration of improved adaptability and performance in web environments., Validation of the framework through experiments yielding significant performance gains.

**Result:** Experiments showed a 10% performance gain in agent learning compared to existing self-evolving agents across real-world environments like Mind2Web-Live, WebVoyager, and GAIA-web.

**Limitations:** The framework's effectiveness may vary with different environments and types of tasks beyond the tested web scenarios.

**Future Work:** Further exploration into enhancing the adaptability of world models and expanding their application in diverse autonomous learning settings.

**Conclusion:** The integration of world models into autonomous agent frameworks is essential for sustained adaptability and performance enhancement in complex environments.

**Abstract:** Agent self-improvement, where the backbone Large Language Model (LLM) of the agent are trained on trajectories sampled autonomously based on their own policies, has emerged as a promising approach for enhancing performance. Recent advancements, particularly in web environments, face a critical limitation: their performance will reach a stagnation point during autonomous learning cycles, hindering further improvement. We argue that this stems from limited exploration of the web environment and insufficient exploitation of pre-trained web knowledge in LLMs. To improve the performance of self-improvement, we propose a novel framework that introduces a co-evolving World Model LLM. This world model predicts the next observation based on the current observation and action within the web environment. Leveraging LLMs' pretrained knowledge of abundant web content, the World Model serves dual roles: (1) as a virtual web server generating self-instructed training data to continuously refine the agent's policy, and (2) as an imagination engine during inference, enabling look-ahead simulation to guide action selection for the agent LLM. Experiments in real-world web environments (Mind2Web-Live, WebVoyager, and GAIA-web) show a 10% performance gain over existing self-evolving agents, demonstrating the efficacy and generalizability of our approach, without using any distillation from more powerful close-sourced models. Our work establishes the necessity of integrating world models into autonomous agent frameworks to unlock sustained adaptability.

</details>


### [24] [Durghotona GPT: A Web Scraping and Large Language Model Based Framework to Generate Road Accident Dataset Automatically in Bangladesh](https://arxiv.org/abs/2504.21025)

*MD Thamed Bin Zaman Chowdhury, Moazzem Hossain, Md. Ridwanul Islam*

**Main category:** cs.CL

**Keywords:** accident data, Large Language Models, web scraping, Durghotona GPT, traffic safety

**Relevance Score:** 8

**TL;DR:** A framework named 'Durghotona GPT' automates the generation of accident datasets from Bangladeshi newspapers using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the global concerns of road accidents by improving the accuracy and availability of accident data.

**Method:** Web scraping and processing accident reports from three major newspapers in Bangladesh using LLMs (GPT-4, GPT-3.5, and Llama-3).

**Key Contributions:** Introduction of 'Durghotona GPT' for automated accident data collection, Evaluation of Llama-3 as a cost-effective alternative to GPT-4, Development of a user interface for the framework

**Result:** Llama-3 achieved 89% accuracy, demonstrating comparability to GPT-4 and providing a cost-effective alternative for generating accident datasets.

**Limitations:** 

**Future Work:** Expand data collection methods and refine LLMs to improve dataset accuracy and applicability.

**Conclusion:** The framework enhances the quality of accident data, supporting applications in traffic safety, urban planning, and public health.

**Abstract:** Road accidents pose significant concerns globally. They lead to large financial losses, injuries, disabilities, and societal challenges. Accurate and timely accident data is essential for predicting and mitigating these events. This paper presents a novel framework named 'Durghotona GPT' that integrates web scraping and Large Language Models (LLMs) to automate the generation of comprehensive accident datasets from prominent national dailies in Bangladesh. The authors collected accident reports from three major newspapers: Prothom Alo, Dhaka Tribune, and The Daily Star. The collected news was then processed using the newest available LLMs: GPT-4, GPT-3.5, and Llama-3. The framework efficiently extracts relevant information, categorizes reports, and compiles detailed datasets. Thus, this framework overcomes limitations of manual data collection methods such as delays, errors, and communication gaps. The authors' evaluation demonstrates that Llama-3, an open-source model, performs comparably to GPT-4. It achieved 89% accuracy in the authors' evaluation. Therefore, it can be considered a cost-effective alternative for similar tasks. The results suggest that the framework developed by the authors can drastically enhance the quality and availability of accident data. As a result, it can support critical applications in traffic safety analysis, urban planning, and public health. The authors also developed an interface for 'Durghotona GPT' for ease of use as part of this paper. Future work will focus on expanding data collection methods and refining LLMs to further increase dataset accuracy and applicability.

</details>


### [25] [Creating and Evaluating Code-Mixed Nepali-English and Telugu-English Datasets for Abusive Language Detection Using Traditional and Deep Learning Models](https://arxiv.org/abs/2504.21026)

*Manish Pandey, Nageshwar Prasad Yadav, Mokshada Adduru, Sawan Rai*

**Main category:** cs.CL

**Keywords:** abusive language detection, code-mixed text, low-resource languages, NLP, social media

**Relevance Score:** 8

**TL;DR:** The paper presents a dataset and analysis focused on detecting abusive language in code-mixed Telugu-English and Nepali-English texts, addressing the challenges in moderation on social media.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of detecting abusive language in code-mixed texts, especially for underrepresented languages like Telugu and Nepali.

**Method:** A novel manually annotated dataset of 2,000 Telugu-English and 5,000 Nepali-English comments was created and tested using various ML and DL models, including hyperparameter optimization and 10-fold cross-validation.

**Key Contributions:** Introduction of a dataset for Telugu-English and Nepali-English abusive language detection, Comparative analysis of various ML and DL models, Insights into the challenges of code-mixed language processing

**Result:** The study found that traditional abuse detection models face difficulties in parsing code-mixed texts and provided a comparative analysis among multiple detection models.

**Limitations:** The dataset is limited to only two low-resource languages and may not capture the full range of abusive language usages across all contexts.

**Future Work:** Future research could expand the dataset to include more languages and explore additional context and nuances in abusive language detection.

**Conclusion:** The findings underline the need for tailored moderation strategies in multilingual social media and provide benchmarks for further research in NLP for low-resource languages.

**Abstract:** With the growing presence of multilingual users on social media, detecting abusive language in code-mixed text has become increasingly challenging. Code-mixed communication, where users seamlessly switch between English and their native languages, poses difficulties for traditional abuse detection models, as offensive content may be context-dependent or obscured by linguistic blending. While abusive language detection has been extensively explored for high-resource languages like English and Hindi, low-resource languages such as Telugu and Nepali remain underrepresented, leaving gaps in effective moderation. In this study, we introduce a novel, manually annotated dataset of 2 thousand Telugu-English and 5 Nepali-English code-mixed comments, categorized as abusive and non-abusive, collected from various social media platforms. The dataset undergoes rigorous preprocessing before being evaluated across multiple Machine Learning (ML), Deep Learning (DL), and Large Language Models (LLMs). We experimented with models including Logistic Regression, Random Forest, Support Vector Machines (SVM), Neural Networks (NN), LSTM, CNN, and LLMs, optimizing their performance through hyperparameter tuning, and evaluate it using 10-fold cross-validation and statistical significance testing (t-test). Our findings provide key insights into the challenges of detecting abusive language in code-mixed settings and offer a comparative analysis of computational approaches. This study contributes to advancing NLP for low-resource languages by establishing benchmarks for abusive language detection in Telugu-English and Nepali-English code-mixed text. The dataset and insights can aid in the development of more robust moderation strategies for multilingual social media environments.

</details>


### [26] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)

*Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill*

**Main category:** cs.CL

**Keywords:** synthetic data, healthcare, Post-Traumatic Stress Disorder, therapeutic conversations, evaluation metrics

**Relevance Score:** 8

**TL;DR:** This paper examines the effectiveness of synthetic data, specifically Prolonged Exposure (PE) therapeutic conversations for PTSD, as a tool for training and evaluating clinical models, highlighting both its potential benefits and limitations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address privacy concerns, limited access to real-world data, and high costs of annotation in healthcare, this research explores synthetic data as a solution for developing clinical models.

**Method:** The study systematically compares real and synthetic dialogues using various metrics, including linguistic and structural analyses, focusing on aspects such as turn-taking patterns and treatment fidelity.

**Key Contributions:** Introduction of PE-specific metrics derived from linguistic analysis and semantic modeling., Demonstration of the potential for synthetic data to address data scarcity and privacy concerns in clinical settings., Identification of key fidelity markers that synthetic conversations fail to capture, emphasizing the need for improved evaluation frameworks.

**Result:** While synthetic dialogues exhibit similar structural features to real dialogues, they fail to capture important fidelity markers, indicating significant limitations in reflecting the nuances of therapeutic interactions.

**Limitations:** Synthetic dialogues do not adequately reflect subtle dynamics of therapeutic interactions, particularly in distress monitoring and other fidelity markers.

**Future Work:** Advocacy for fidelity-aware metrics that extend beyond surface fluency to better assess clinical significance and treatment fidelity in synthetic data.

**Conclusion:** Synthetic data can complement real-world datasets in healthcare applications, but critical gaps exist in capturing the dynamics of therapeutic interactions, necessitating the development of fidelity-aware evaluation metrics.

**Abstract:** The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.

</details>


### [27] [UrbanPlanBench: A Comprehensive Urban Planning Benchmark for Evaluating Large Language Models](https://arxiv.org/abs/2504.21027)

*Yu Zheng, Longyi Liu, Yuming Lin, Jie Feng, Guozhen Zhang, Depeng Jin, Yong Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, urban planning, benchmarks, fine-tuning, dataset

**Relevance Score:** 8

**TL;DR:** The paper introduces UrbanPlanBench, a benchmark to assess LLMs in urban planning, revealing shortcomings in their performance, and presents the UrbanPlanText dataset for fine-tuning models to improve their understanding of planning knowledge.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how LLMs can assist urban planners and evaluate their effectiveness in this domain.

**Method:** The authors developed UrbanPlanBench to benchmark LLMs' performance in urban planning and created the UrbanPlanText dataset for supervised fine-tuning with instruction pairs from exams and textbooks.

**Key Contributions:** Introduction of UrbanPlanBench benchmark for LLM evaluation in urban planning, Creation of UrbanPlanText dataset with over 30,000 instruction pairs, Findings on LLM performance gaps, particularly in planning regulations

**Result:** Extensive evaluation shows a significant imbalance in LLMs' knowledge, with 70% performing poorly on planning regulations; however, fine-tuned models show improved performance in memorization and comprehension tasks.

**Limitations:** LLMs still struggle with domain-specific terminology and reasoning despite improvements from fine-tuning.

**Future Work:** Encouragement for further research to enhance LLM capabilities in urban planning and their practical application.

**Conclusion:** The study highlights LLMs' limitations in urban planning while providing resources for improving their integration into the field.

**Abstract:** The advent of Large Language Models (LLMs) holds promise for revolutionizing various fields traditionally dominated by human expertise. Urban planning, a professional discipline that fundamentally shapes our daily surroundings, is one such field heavily relying on multifaceted domain knowledge and experience of human experts. The extent to which LLMs can assist human practitioners in urban planning remains largely unexplored. In this paper, we introduce a comprehensive benchmark, UrbanPlanBench, tailored to evaluate the efficacy of LLMs in urban planning, which encompasses fundamental principles, professional knowledge, and management and regulations, aligning closely with the qualifications expected of human planners. Through extensive evaluation, we reveal a significant imbalance in the acquisition of planning knowledge among LLMs, with even the most proficient models falling short of meeting professional standards. For instance, we observe that 70% of LLMs achieve subpar performance in understanding planning regulations compared to other aspects. Besides the benchmark, we present the largest-ever supervised fine-tuning (SFT) dataset, UrbanPlanText, comprising over 30,000 instruction pairs sourced from urban planning exams and textbooks. Our findings demonstrate that fine-tuned models exhibit enhanced performance in memorization tests and comprehension of urban planning knowledge, while there exists significant room for improvement, particularly in tasks requiring domain-specific terminology and reasoning. By making our benchmark, dataset, and associated evaluation and fine-tuning toolsets publicly available at https://github.com/tsinghua-fib-lab/PlanBench, we aim to catalyze the integration of LLMs into practical urban planning, fostering a symbiotic collaboration between human expertise and machine intelligence.

</details>


### [28] [Beyond One-Size-Fits-All: Inversion Learning for Highly Effective NLG Evaluation Prompts](https://arxiv.org/abs/2504.21117)

*Hanhua Hong, Chenghao Xiao, Yang Wang, Yiqi Liu, Wenge Rong, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** natural language generation, evaluation methodology, inversion learning, LLM-based evaluation, prompt engineering

**Relevance Score:** 8

**TL;DR:** This paper presents a method for improving evaluation of natural language generation systems by automatically generating model-specific evaluation prompts using inversion learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating natural language generation systems is difficult due to diverse valid outputs and limitations in human evaluation, prompting the need for a scalable solution.

**Method:** The authors propose an inversion learning method that creates effective reverse mappings from model outputs to input instructions, allowing for automated generation of evaluation prompts from just one sample.

**Key Contributions:** Introduction of inversion learning for prompt generation, Elimination of manual prompt engineering, Improved robustness in model evaluation

**Result:** The method improves efficiency by eliminating manual prompt engineering, leading to more robust evaluations of NLG systems.

**Limitations:** The method's effectiveness may vary with different types of models and outputs.

**Future Work:** Further exploration of inversion learning in other AI evaluation contexts and potential improvements in evaluation prompt design.

**Conclusion:** The proposed method enhances both the efficiency and reliability of LLM-based evaluations, opening new directions for this field.

**Abstract:** Evaluating natural language generation (NLG) systems is challenging due to the diversity of valid outputs. While human evaluation is the gold standard, it suffers from inconsistencies, lack of standardisation, and demographic biases, limiting reproducibility. LLM-based evaluation offers a scalable alternative but is highly sensitive to prompt design, where small variations can lead to significant discrepancies. In this work, we propose an inversion learning method that learns effective reverse mappings from model outputs back to their input instructions, enabling the automatic generation of highly effective, model-specific evaluation prompts. Our method requires only a single evaluation sample and eliminates the need for time-consuming manual prompt engineering, thereby improving both efficiency and robustness. Our work contributes toward a new direction for more robust and efficient LLM-based evaluation.

</details>


### [29] [LLM Enhancer: Merged Approach using Vector Embedding for Reducing Large Language Model Hallucinations with External Knowledge](https://arxiv.org/abs/2504.21132)

*Naheed Rayhan, Md. Ashrafuzzaman*

**Main category:** cs.CL

**Keywords:** Large Language Models, Information accuracy, User interaction, Data acquisition, Natural language processing

**Relevance Score:** 9

**TL;DR:** The LLM ENHANCER system improves the accuracy of responses from LLMs by integrating multiple online information sources.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs like ChatGPT often produce inaccurate information, limiting their application in critical scenarios. The need for a system that enhances their reliability and accuracy is paramount.

**Method:** The LLM ENHANCER integrates information from sources like Google, Wikipedia, and DuckDuckGo using parallel data acquisition and custom agent tools. It employs vector embeddings to determine relevant information for LLM user interaction.

**Key Contributions:** Integration of multiple online knowledge sources to enhance LLM responses., Reduction of hallucinations while ensuring natural dialogue., Use of vector embeddings for relevant information retrieval.

**Result:** The system effectively reduces hallucinations in chat-based LLMs while maintaining naturalness and accuracy in responses.

**Limitations:** The system's performance may vary based on the quality and currency of the integrated online sources.

**Future Work:** Future research could explore additional sources of information and enhance the system's adaptability to various domains.

**Conclusion:** The LLM ENHANCER demonstrates a viable approach to improving the response quality of LLMs in real-world applications by sourcing accurate information.

**Abstract:** Large Language Models (LLMs), such as ChatGPT, have demonstrated the capability to generate human like, natural responses across a range of tasks, including task oriented dialogue and question answering. However, their application in real world, critical scenarios is often hindered by a tendency to produce inaccurate information and a limited ability to leverage external knowledge sources. This paper introduces the LLM ENHANCER system, designed to integrate multiple online sources such as Google, Wikipedia, and DuckDuckGo to enhance data accuracy. The LLMs employed within this system are open source. The data acquisition process for the LLM ENHANCER system operates in parallel, utilizing custom agent tools to manage the flow of information. Vector embeddings are used to identify the most pertinent information, which is subsequently supplied to the LLM for user interaction. The LLM ENHANCER system mitigates hallucinations in chat based LLMs while preserving response naturalness and accuracy.

</details>


### [30] [Detecting Manipulated Contents Using Knowledge-Grounded Inference](https://arxiv.org/abs/2504.21165)

*Mark Huasong Meng, Ruizhe Wang, Meng Xu, Chuan Yan, Guangdong Bai*

**Main category:** cs.CL

**Keywords:** fake news detection, zero-day manipulation, retrieval-augmented generation, large language models, fact-checking

**Relevance Score:** 9

**TL;DR:** Manicod is a tool designed for detecting zero-day manipulated content using real-time contextual information sourced from search engines and processed by LLMs through RAG, achieving a high F1 score.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The need to effectively detect zero-day manipulated content in fake news due to limitations of existing solutions relying on historical data or curated context.

**Method:** Manicod sources contextual information about claims from mainstream search engines and vectorizes it for LLM inference via retrieval-augmented generation.

**Key Contributions:** Introduction of Manicod for real-time fake news detection, Development of a dataset with 4270 manipulated fake news pieces, Outperforming existing methods in fact-checking metrics.

**Result:** Manicod achieved an F1 score of 0.856 on its dataset comprising 4270 manipulated fake news pieces, outperforming existing methods by up to 1.9x in F1 score.

**Limitations:** 

**Future Work:** Exploration of further enhancements in real-time detection methodologies and application to various news domains.

**Conclusion:** The effectiveness of Manicod demonstrates the importance of real-time contextual information for accurately detecting manipulated content in news.

**Abstract:** The detection of manipulated content, a prevalent form of fake news, has been widely studied in recent years. While existing solutions have been proven effective in fact-checking and analyzing fake news based on historical events, the reliance on either intrinsic knowledge obtained during training or manually curated context hinders them from tackling zero-day manipulated content, which can only be recognized with real-time contextual information. In this work, we propose Manicod, a tool designed for detecting zero-day manipulated content. Manicod first sources contextual information about the input claim from mainstream search engines, and subsequently vectorizes the context for the large language model (LLM) through retrieval-augmented generation (RAG). The LLM-based inference can produce a "truthful" or "manipulated" decision and offer a textual explanation for the decision. To validate the effectiveness of Manicod, we also propose a dataset comprising 4270 pieces of manipulated fake news derived from 2500 recent real-world news headlines. Manicod achieves an overall F1 score of 0.856 on this dataset and outperforms existing methods by up to 1.9x in F1 score on their benchmarks on fact-checking and claim verification.

</details>


### [31] [Small or Large? Zero-Shot or Finetuned? Guiding Language Model Choice for Specialized Applications in Healthcare](https://arxiv.org/abs/2504.21191)

*Lovedeep Gondara, Jonathan Simkin, Graham Sayle, Shebnum Devji, Gregory Arbour, Raymond Ng*

**Main category:** cs.CL

**Keywords:** Language Models, Finetuning, Domain-Specific Pretraining, Small Language Models, Large Language Models

**Relevance Score:** 9

**TL;DR:** The study evaluates the performance of Small Language Models (SLMs) and Large Language Models (LLMs) in the context of electronic pathology report classification, focusing on the impact of finetuning and domain-specific pretraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To guide language model selection for specific tasks in medical domain applications, particularly regarding finetuning versus zero-shot usage and the impact of model size and domain specificity.

**Method:** Evaluated various SLMs and one LLM using electronic pathology reports from the British Columbia Cancer Registry (BCCR) under differing conditions of finetuning and zero-shot performance across three classification scenarios.

**Key Contributions:** Detailed evaluation of SLM vs LLM performance in medical classification tasks., Insights on the benefits of finetuning and domain-specific pretraining., Demonstration of the ongoing relevance of SLMs in the age of LLMs.

**Result:** Finetuning improved SLM performance across all scenarios; zero-shot LLM outperformed zero-shot SLMs but was outperformed by finetuned SLMs, with domain-adjacent SLMs performing better than generic ones.

**Limitations:** Focus on a specific domain (pathology reports) may limit generalizability; evaluation based on a single dataset.

**Future Work:** Further exploration of SLM performance across diverse healthcare applications and additional datasets.

**Conclusion:** SLMs are shown to be more effective when finetuned for specialized tasks, significantly outperforming zero-shot LLMs, indicating their continued relevance and efficiency in specific applications.

**Abstract:** This study aims to guide language model selection by investigating: 1) the necessity of finetuning versus zero-shot usage, 2) the benefits of domain-adjacent versus generic pretrained models, 3) the value of further domain-specific pretraining, and 4) the continued relevance of Small Language Models (SLMs) compared to Large Language Models (LLMs) for specific tasks. Using electronic pathology reports from the British Columbia Cancer Registry (BCCR), three classification scenarios with varying difficulty and data size are evaluated. Models include various SLMs and an LLM. SLMs are evaluated both zero-shot and finetuned; the LLM is evaluated zero-shot only. Finetuning significantly improved SLM performance across all scenarios compared to their zero-shot results. The zero-shot LLM outperformed zero-shot SLMs but was consistently outperformed by finetuned SLMs. Domain-adjacent SLMs generally performed better than the generic SLM after finetuning, especially on harder tasks. Further domain-specific pretraining yielded modest gains on easier tasks but significant improvements on the complex, data-scarce task. The results highlight the critical role of finetuning for SLMs in specialized domains, enabling them to surpass zero-shot LLM performance on targeted classification tasks. Pretraining on domain-adjacent or domain-specific data provides further advantages, particularly for complex problems or limited finetuning data. While LLMs offer strong zero-shot capabilities, their performance on these specific tasks did not match that of appropriately finetuned SLMs. In the era of LLMs, SLMs remain relevant and effective, offering a potentially superior performance-resource trade-off compared to LLMs.

</details>


### [32] [Automatic Legal Writing Evaluation of LLMs](https://arxiv.org/abs/2504.21202)

*Ramon Pires, Roseval Malaquias Junior, Rodrigo Nogueira*

**Main category:** cs.CL

**Keywords:** Large Language Models, Legal writing, Benchmark, Automated evaluation, Brazilian Bar Examination

**Relevance Score:** 6

**TL;DR:** Introducing oab-bench, a benchmark for evaluating legal writing with LLMs, comprising 105 questions from the Brazilian Bar Examination, showing potential for automated assessment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the scarcity of benchmarks for evaluating legal writing and to enable better assessment of LLMs in domain-specific tasks.

**Method:** Creation of the oab-bench with 105 questions and comprehensive evaluation guidelines; evaluation of four LLMs on this benchmark.

**Key Contributions:** Development of oab-bench, a comprehensive benchmark for legal writing assessment., Demonstration of LLMs' potential as automated evaluators in legal contexts., Provision of publicly available resources including questions and evaluations.

**Result:** Claude-3.5 Sonnet achieved the best average score of 7.93 out of 10, correlating well with human evaluations on legal writing.

**Limitations:** The complexity and subjectivity inherent in legal writing assessment may limit the applicability of the evaluations.

**Future Work:** Further exploration of LLMs' capabilities in other legal contexts and continuous updates to the benchmark dataset.

**Conclusion:** LLMs, particularly advanced models, show promise as reliable automated judges in the evaluation of legal writing despite its subjective nature.

**Abstract:** Despite the recent advances in Large Language Models, benchmarks for evaluating legal writing remain scarce due to the inherent complexity of assessing open-ended responses in this domain. One of the key challenges in evaluating language models on domain-specific tasks is finding test datasets that are public, frequently updated, and contain comprehensive evaluation guidelines. The Brazilian Bar Examination meets these requirements. We introduce oab-bench, a benchmark comprising 105 questions across seven areas of law from recent editions of the exam. The benchmark includes comprehensive evaluation guidelines and reference materials used by human examiners to ensure consistent grading. We evaluate the performance of four LLMs on oab-bench, finding that Claude-3.5 Sonnet achieves the best results with an average score of 7.93 out of 10, passing all 21 exams. We also investigated whether LLMs can serve as reliable automated judges for evaluating legal writing. Our experiments show that frontier models like OpenAI's o1 achieve a strong correlation with human scores when evaluating approved exams, suggesting their potential as reliable automated evaluators despite the inherently subjective nature of legal writing assessment. The source code and the benchmark -- containing questions, evaluation guidelines, model-generated responses, and their respective automated evaluations -- are publicly available.

</details>


### [33] [Pretraining Large Brain Language Model for Active BCI: Silent Speech](https://arxiv.org/abs/2504.21214)

*Jinzhao Zhou, Zehong Cao, Yiqun Duan, Connor Barkley, Daniel Leong, Xiaowei Jiang, Quoc-Toan Nguyen, Ziyi Zhao, Thomas Do, Yu-Cheng Chang, Sheng-Fu Liang, Chin-teng Lin*

**Main category:** cs.CL

**Keywords:** silent speech decoding, brain-computer interface, large language model

**Relevance Score:** 8

**TL;DR:** Explores silent speech decoding in active brain-computer interfaces (BCI) using a new dataset and a Large Brain Language Model (LBLM) trained with a novel pretraining method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance communication through more natural BCI systems compared to traditional methods by improving silent speech decoding capabilities.

**Method:** A Large Brain Language Model (LBLM) is pretrained using a Future Spectro-Temporal Prediction (FSTP) paradigm on a new dataset of EEG recordings, capturing both temporal and spectral dependencies.

**Key Contributions:** Introduction of the Large Brain Language Model (LBLM) for silent speech decoding., Development of a new silent speech dataset from EEG recordings offering over 120 hours of data., Proposing the Future Spectro-Temporal Prediction (FSTP) pretraining paradigm for effective representation learning.

**Result:** LBLM outperforms fully-supervised and pre-trained baseline models, achieving 47.0% accuracy in semantic-level classification and 39.6% in word-level classification, with significant performance gains.

**Limitations:** 

**Future Work:** Further exploration of BCI systems using the pretrained LBLM and improvements in silent speech decoding techniques.

**Conclusion:** The proposed LBLM and the new dataset advance the field of silent speech decoding and provide foundational research resources for future studies.

**Abstract:** This paper explores silent speech decoding in active brain-computer interface (BCI) systems, which offer more natural and flexible communication than traditional BCI applications. We collected a new silent speech dataset of over 120 hours of electroencephalogram (EEG) recordings from 12 subjects, capturing 24 commonly used English words for language model pretraining and decoding. Following the recent success of pretraining large models with self-supervised paradigms to enhance EEG classification performance, we propose Large Brain Language Model (LBLM) pretrained to decode silent speech for active BCI. To pretrain LBLM, we propose Future Spectro-Temporal Prediction (FSTP) pretraining paradigm to learn effective representations from unlabeled EEG data. Unlike existing EEG pretraining methods that mainly follow a masked-reconstruction paradigm, our proposed FSTP method employs autoregressive modeling in temporal and frequency domains to capture both temporal and spectral dependencies from EEG signals. After pretraining, we finetune our LBLM on downstream tasks, including word-level and semantic-level classification. Extensive experiments demonstrate significant performance gains of the LBLM over fully-supervised and pretrained baseline models. For instance, in the difficult cross-session setting, our model achieves 47.0\% accuracy on semantic-level classification and 39.6\% in word-level classification, outperforming baseline methods by 5.4\% and 7.3\%, respectively. Our research advances silent speech decoding in active BCI systems, offering an innovative solution for EEG language model pretraining and a new dataset for fundamental research.

</details>


### [34] [Phi-4-Mini-Reasoning: Exploring the Limits of Small Reasoning Language Models in Math](https://arxiv.org/abs/2504.21233)

*Haoran Xu, Baolin Peng, Hany Awadalla, Dongdong Chen, Yen-Chun Chen, Mei Gao, Young Jin Kim, Yunsheng Li, Liliang Ren, Yelong Shen, Shuohang Wang, Weijian Xu, Jianfeng Gao, Weizhu Chen*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Small Language Models, reasoning, distillation, reinforcement learning

**Relevance Score:** 7

**TL;DR:** A systematic training recipe for Small Language Models (SLMs) is proposed to enhance reasoning capabilities using Chain-of-Thought (CoT) data, leading to superior performance on math tasks compared to larger models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve reasoning in Small Language Models (SLMs) which face challenges due to limited model capacity, while leveraging the benefits seen in Large Language Models (LLMs) through Chain-of-Thought (CoT) techniques.

**Method:** The methodology consists of four steps: 1. Mid-training on distilled long-CoT data, 2. Supervised fine-tuning on high-quality long-CoT data, 3. Rollout DPO with a curated preference dataset, 4. Reinforcement Learning with Verifiable Reward.

**Key Contributions:** Systematic training recipe for SLMs to enhance reasoning capabilities., Demonstrated performance of Phi-4-Mini-Reasoning model surpassing larger models on math tasks., Introduction of Rollout DPO and Reinforcement Learning approaches in the training process.

**Result:** The Phi-4-Mini-Reasoning model outperforms larger models on math reasoning tasks, exceeding DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500.

**Limitations:** The detailed modeling recipe of previous works was not fully disclosed, which may limit reproducibility.

**Future Work:** Further exploration of the effectiveness of the training recipe on other reasoning tasks and model types.

**Conclusion:** A well-designed training recipe that utilizes high-quality CoT data can unlock strong reasoning capabilities in small models, demonstrating effective distillation from LLMs.

**Abstract:** Chain-of-Thought (CoT) significantly enhances formal reasoning capabilities in Large Language Models (LLMs) by training them to explicitly generate intermediate reasoning steps. While LLMs readily benefit from such techniques, improving reasoning in Small Language Models (SLMs) remains challenging due to their limited model capacity. Recent work by Deepseek-R1 demonstrates that distillation from LLM-generated synthetic data can substantially improve the reasoning ability of SLM. However, the detailed modeling recipe is not disclosed. In this work, we present a systematic training recipe for SLMs that consists of four steps: (1) large-scale mid-training on diverse distilled long-CoT data, (2) supervised fine-tuning on high-quality long-CoT data, (3) Rollout DPO leveraging a carefully curated preference dataset, and (4) Reinforcement Learning (RL) with Verifiable Reward. We apply our method on Phi-4-Mini, a compact 3.8B-parameter model. The resulting Phi-4-Mini-Reasoning model exceeds, on math reasoning tasks, much larger reasoning models, e.g., outperforming DeepSeek-R1-Distill-Qwen-7B by 3.2 points and DeepSeek-R1-Distill-Llama-8B by 7.7 points on Math-500. Our results validate that a carefully designed training recipe, with large-scale high-quality CoT data, is effective to unlock strong reasoning capabilities even in resource-constrained small models.

</details>


### [35] [Memorization and Knowledge Injection in Gated LLMs](https://arxiv.org/abs/2504.21239)

*Xu Pan, Ely Hahami, Zechen Zhang, Haim Sompolinsky*

**Main category:** cs.CL

**Keywords:** Large Language Models, Continual Learning, Memory Injection, Gated Mechanism

**Relevance Score:** 9

**TL;DR:** Introducing MEGa, a continual learning framework for LLMs that integrates memories into model weights to enhance knowledge retention and recall.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLMs face challenges in sequentially adding memories and effectively integrating new knowledge, in contrast to human learning capabilities.

**Method:** The study presents MEGa, a framework that embeds event memories into LLM weights, using a gating mechanism to activate relevant weights during inference based on query embeddings.

**Key Contributions:** Introduction of a new framework for continual learning in LLMs., Demonstrated effectiveness of the MEGa model in retaining memories over traditional memory injection methods., Innovative use of gated low-rank weights for memory storage and retrieval.

**Result:** MEGa outperforms existing methods in two datasets, demonstrating improved performance in avoiding catastrophic forgetting while allowing recall of entire memories and answering related queries.

**Limitations:** Further testing in more diverse everyday scenarios and real-world applications is needed to validate the framework's effectiveness.

**Future Work:** Exploration of additional memory types and broader datasets for improved generalization and applicability.

**Conclusion:** The findings suggest that embedding memories in gated weights can enhance LLMs' ability to learn continuously and respond accurately to new information.

**Abstract:** Large Language Models (LLMs) currently struggle to sequentially add new memories and integrate new knowledge. These limitations contrast with the human ability to continuously learn from new experiences and acquire knowledge throughout life. Most existing approaches add memories either through large context windows or external memory buffers (e.g., Retrieval-Augmented Generation), and studies on knowledge injection rarely test scenarios resembling everyday life events. In this work, we introduce a continual learning framework, Memory Embedded in Gated LLMs (MEGa), which injects event memories directly into the weights of LLMs. Each memory is stored in a dedicated set of gated low-rank weights. During inference, a gating mechanism activates relevant memory weights by matching query embeddings to stored memory embeddings. This enables the model to both recall entire memories and answer related questions. On two datasets - fictional characters and Wikipedia events - MEGa outperforms baseline approaches in mitigating catastrophic forgetting. Our model draws inspiration from the complementary memory system of the human brain.

</details>


### [36] [Talk Before You Retrieve: Agent-Led Discussions for Better RAG in Medical QA](https://arxiv.org/abs/2504.21252)

*Xuanzhao Dong, Wenhui Zhu, Hao Wang, Xiwen Chen, Peijie Qiu, Rui Yin, Yi Su, Yalin Wang*

**Main category:** cs.CL

**Keywords:** medical question answering, retrieval-augmented generation, human-like reasoning, AI in healthcare, multi-turn brainstorming

**Relevance Score:** 9

**TL;DR:** Discuss-RAG is a proposed module to enhance medical question answering (QA) systems by modeling human-like reasoning and improving content relevance through collaborative agent-based reasoning.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Medical question answering remains challenging for large language models, primarily due to hallucinations and outdated knowledge, necessitating improvements in retrieval processes.

**Method:** Discuss-RAG introduces a summarizer agent and a decision-making agent to enhance the traditional RAG system by emulating collaborative brainstorming and evaluating retrieved snippets.

**Key Contributions:** Introduction of a summarizer agent to enhance relevance in medical QA., Development of a decision-making agent for evaluating retrieved data snippets., Demonstrated significant accuracy improvements on benchmark medical QA datasets.

**Result:** Discuss-RAG outperforms existing systems like MedRAG, with significant accuracy improvements of up to 16.67% on BioASQ and 12.20% on PubMedQA across various benchmark datasets.

**Limitations:** The study focuses on medical QA and may not directly generalize to other domains or applications of RAG systems.

**Future Work:** Further exploration of agent-based reasoning in other medical applications and enhancing the information retrieval process.

**Conclusion:** The experimental results demonstrate that Discuss-RAG effectively enhances the performance of medical QA systems, addressing key limitations of existing methods.

**Abstract:** Medical question answering (QA) is a reasoning-intensive task that remains challenging for large language models (LLMs) due to hallucinations and outdated domain knowledge. Retrieval-Augmented Generation (RAG) provides a promising post-training solution by leveraging external knowledge. However, existing medical RAG systems suffer from two key limitations: (1) a lack of modeling for human-like reasoning behaviors during information retrieval, and (2) reliance on suboptimal medical corpora, which often results in the retrieval of irrelevant or noisy snippets. To overcome these challenges, we propose Discuss-RAG, a plug-and-play module designed to enhance the medical QA RAG system through collaborative agent-based reasoning. Our method introduces a summarizer agent that orchestrates a team of medical experts to emulate multi-turn brainstorming, thereby improving the relevance of retrieved content. Additionally, a decision-making agent evaluates the retrieved snippets before their final integration. Experimental results on four benchmark medical QA datasets show that Discuss-RAG consistently outperforms MedRAG, especially significantly improving answer accuracy by up to 16.67% on BioASQ and 12.20% on PubMedQA. The code is available at: https://github.com/LLM-VLM-GSL/Discuss-RAG.

</details>


### [37] [BiasGuard: A Reasoning-enhanced Bias Detection Tool For Large Language Models](https://arxiv.org/abs/2504.21299)

*Zhiting Fan, Ruizhe Chen, Zuozhu Liu*

**Main category:** cs.CL

**Keywords:** bias detection, LLM fairness, reinforcement learning, decision-making, BiasGuard

**Relevance Score:** 9

**TL;DR:** BiasGuard is a new tool for detecting bias in LLM-generated content that uses a two-stage approach to provide accurate fairness judgments.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To ensure fairness in LLMs by addressing limitations of existing bias detection methods that struggle with understanding intentions and fairness criteria.

**Method:** BiasGuard utilizes a two-stage approach: the first stage initializes the model to reason based on fairness specifications, and the second stage employs reinforcement learning to enhance reasoning and judgment capabilities.

**Key Contributions:** Introduction of BiasGuard for bias detection in LLMs., Two-stage reasoning approach to enhance judgment accuracy., Demonstration of improved performance over existing bias detection tools.

**Result:** Experiments across five datasets show BiasGuard outperforms existing tools in accuracy and reduces over-fairness misjudgments.

**Limitations:** 

**Future Work:** Further refinement of BiasGuard and exploration of additional datasets for validation.

**Conclusion:** BiasGuard demonstrates the importance of reasoning-enhanced decision-making in bias detection and shows the effectiveness of its two-stage optimization process.

**Abstract:** Identifying bias in LLM-generated content is a crucial prerequisite for ensuring fairness in LLMs. Existing methods, such as fairness classifiers and LLM-based judges, face limitations related to difficulties in understanding underlying intentions and the lack of criteria for fairness judgment. In this paper, we introduce BiasGuard, a novel bias detection tool that explicitly analyzes inputs and reasons through fairness specifications to provide accurate judgments. BiasGuard is implemented through a two-stage approach: the first stage initializes the model to explicitly reason based on fairness specifications, while the second stage leverages reinforcement learning to enhance its reasoning and judgment capabilities. Our experiments, conducted across five datasets, demonstrate that BiasGuard outperforms existing tools, improving accuracy and reducing over-fairness misjudgments. We also highlight the importance of reasoning-enhanced decision-making and provide evidence for the effectiveness of our two-stage optimization pipeline.

</details>


### [38] [Confidence in Large Language Model Evaluation: A Bayesian Approach to Limited-Sample Challenges](https://arxiv.org/abs/2504.21303)

*Xiao Xiao, Yu Su, Sijing Zhang, Zhang Chen, Yadong Chen, Tian Liu*

**Main category:** cs.CL

**Keywords:** Bayesian inference, large language models, evaluation frameworks, probabilistic output, capability assessment

**Relevance Score:** 9

**TL;DR:** This study proposes a Bayesian approach for evaluating large language models (LLMs) that incorporates prior knowledge and improves assessment under limited sample conditions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional evaluation frameworks for LLMs rely on deterministic metrics, which do not adequately capture the probabilistic nature of their outputs.

**Method:** A Bayesian hypothesis testing framework is proposed, treating model capabilities as latent variables and utilizing a curated query set for discriminative response evaluation.

**Key Contributions:** Introduced a Bayesian approach for LLM evaluation, Formalized model ranking as a hypothesis testing problem, Demonstrated improved discrimination with fewer samples

**Result:** The proposed Bayesian method demonstrates superior discrimination compared to traditional methods, maintaining statistical robustness even with small sample sizes while providing probabilistic insights about model performance.

**Limitations:** The study may require further validation across different LLM architectures beyond GPT-series models.

**Future Work:** Future research could explore the application of this Bayesian framework on more diverse model architectures and real-world tasks.

**Conclusion:** This work enhances LLM evaluation methodologies by integrating Bayesian inference, facilitating better assessments in real-world applications.

**Abstract:** Large language models (LLMs) exhibit probabilistic output characteristics, yet conventional evaluation frameworks rely on deterministic scalar metrics. This study introduces a Bayesian approach for LLM capability assessment that integrates prior knowledge through probabilistic inference, addressing limitations under limited-sample regimes. By treating model capabilities as latent variables and leveraging a curated query set to induce discriminative responses, we formalize model ranking as a Bayesian hypothesis testing problem over mutually exclusive capability intervals. Experimental evaluations with GPT-series models demonstrate that the proposed method achieves superior discrimination compared to conventional evaluation methods. Results indicate that even with reduced sample sizes, the approach maintains statistical robustness while providing actionable insights, such as probabilistic statements about a model's likelihood of surpassing specific baselines. This work advances LLM evaluation methodologies by bridging Bayesian inference with practical constraints in real-world deployment scenarios.

</details>


### [39] [Does the Prompt-based Large Language Model Recognize Students' Demographics and Introduce Bias in Essay Scoring?](https://arxiv.org/abs/2504.21330)

*Kaixun Yang, Mladen Raković, Dragan Gašević, Guanliang Chen*

**Main category:** cs.CL

**Keywords:** Automated Essay Scoring, Large Language Models, Bias in Education, Demographic Inference, Prompt-Based Tools

**Relevance Score:** 9

**TL;DR:** This study investigates bias in Automated Essay Scoring (AES) using prompt-based tools like ChatGPT, examining how demographic predictions may influence scoring fairness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the presence of bias in Automated Essay Scoring systems using large language models and the implications of these biases in educational contexts.

**Method:** The study utilized a dataset of over 25,000 students' essays, designed prompts to elicit demographic inferences, and performed multivariate regression analysis to explore the relationship between demographic predictions and scoring biases.

**Key Contributions:** Exploration of biases in AES using prompt-based paradigms, Analysis of demographic inference from student essays, Identification of unfair scoring patterns for non-native speakers

**Result:** The findings indicate that prompt-based LLMs can infer demographics from essays, with scoring biases heightened for essays from non-native English speakers when their language background is correctly identified.

**Limitations:** 

**Future Work:** Further research is needed to explore mitigation strategies for bias in AES systems and the impact on diverse student populations.

**Conclusion:** The research highlights significant biases in AES for non-native speakers and underscores the need for addressing these biases in prompt-based LLM applications in education.

**Abstract:** Large Language Models (LLMs) are widely used in Automated Essay Scoring (AES) due to their ability to capture semantic meaning. Traditional fine-tuning approaches required technical expertise, limiting accessibility for educators with limited technical backgrounds. However, prompt-based tools like ChatGPT have made AES more accessible, enabling educators to obtain machine-generated scores using natural-language prompts (i.e., the prompt-based paradigm). Despite advancements, prior studies have shown bias in fine-tuned LLMs, particularly against disadvantaged groups. It remains unclear whether such biases persist or are amplified in the prompt-based paradigm with cutting-edge tools. Since such biases are believed to stem from the demographic information embedded in pre-trained models (i.e., the ability of LLMs' text embeddings to predict demographic attributes), this study explores the relationship between the model's predictive power of students' demographic attributes based on their written works and its predictive bias in the scoring task in the prompt-based paradigm. Using a publicly available dataset of over 25,000 students' argumentative essays, we designed prompts to elicit demographic inferences (i.e., gender, first-language background) from GPT-4o and assessed fairness in automated scoring. Then we conducted multivariate regression analysis to explore the impact of the model's ability to predict demographics on its scoring outcomes. Our findings revealed that (i) prompt-based LLMs can somewhat infer students' demographics, particularly their first-language backgrounds, from their essays; (ii) scoring biases are more pronounced when the LLM correctly predicts students' first-language background than when it does not; and (iii) scoring error for non-native English speakers increases when the LLM correctly identifies them as non-native.

</details>


### [40] [Retrieval-Enhanced Few-Shot Prompting for Speech Event Extraction](https://arxiv.org/abs/2504.21372)

*Máté Gedeon*

**Main category:** cs.CL

**Keywords:** Speech Event Extraction, Automatic Speech Recognition, Natural Language Processing, Large Language Models, Semantic Search

**Relevance Score:** 8

**TL;DR:** The paper presents a modular pipeline for Speech Event Extraction that integrates Automatic Speech Recognition and Natural Language Processing using Large Language Models, showing significant performance improvements over traditional benchmarks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective extraction of structured event information from spoken language in a manner that combines ASR and NLP techniques.

**Method:** A hybrid filtering mechanism is employed for classifying speech segments, utilizing rule-based, BERT-based, and LLM-based models, followed by few-shot prompting of LLMs enriched via semantic retrieval for event trigger and argument extraction.

**Key Contributions:** Proposed a modular, pipeline-based SpeechEE framework integrating ASR and NLP with LLMs., Demonstrated significant performance improvements using the o1-mini model in event classification tasks., Provided insights into hybrid models combining textual and acoustic features.

**Result:** The system achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming previous benchmarks.

**Limitations:** 

**Future Work:** Exploration of further hybrid models that integrate both textual and acoustic features for enhancing event extraction tasks.

**Conclusion:** Pipeline approaches using retrieval-augmented LLMs can surpass end-to-end systems, offering better interpretability and modularity for event extraction tasks.

**Abstract:** Speech Event Extraction (SpeechEE) is a challenging task that lies at the intersection of Automatic Speech Recognition (ASR) and Natural Language Processing (NLP), requiring the identification of structured event information from spoken language. In this work, we present a modular, pipeline-based SpeechEE framework that integrates high-performance ASR with semantic search-enhanced prompting of Large Language Models (LLMs). Our system first classifies speech segments likely to contain events using a hybrid filtering mechanism including rule-based, BERT-based, and LLM-based models. It then employs few-shot LLM prompting, dynamically enriched via semantic similarity retrieval, to identify event triggers and extract corresponding arguments. We evaluate the pipeline using multiple LLMs (Llama3-8B, GPT-4o-mini, and o1-mini) highlighting significant performance gains with o1-mini, which achieves 63.3% F1 on trigger classification and 27.8% F1 on argument classification, outperforming prior benchmarks. Our results demonstrate that pipeline approaches, when empowered by retrieval-augmented LLMs, can rival or exceed end-to-end systems while maintaining interpretability and modularity. This work provides practical insights into LLM-driven event extraction and opens pathways for future hybrid models combining textual and acoustic features.

</details>


### [41] [The Distribution of Dependency Distance and Hierarchical Distance in Contemporary Written Japanese and Its Influencing Factors](https://arxiv.org/abs/2504.21421)

*Linxuan Wang, Shuiyuan Yu*

**Main category:** cs.CL

**Keywords:** dependency distance, hierarchical distance, predicate valency, Japanese linguistics, cognitive processing

**Relevance Score:** 3

**TL;DR:** This paper examines the interaction between dependency distance and hierarchical distance in Japanese sentences, highlighting the role of predicate valency in regulating complexity and distribution patterns.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding how dependency and hierarchical distances are regulated in the Japanese language can shed light on cognitive processing mechanisms in linguistics.

**Method:** The study compares probability distributions of dependency distance (DD) and hierarchical distance (HD) while controlling for sentence length, analyzing mean dependency distance (MDD) and mean hierarchical distance (MHD) using the Balanced Corpus of Contemporary Written Japanese.

**Key Contributions:** Identified the trade-off relationship between MDD and MHD in Japanese based on predicate valency., Quantified the impact of sentence length on dependency and hierarchical distances., Provided evidence that valency affects the probability distributions of DD and HD differently.

**Result:** The research found that the valency of predicates significantly influences the relationship between MDD and MHD, with higher valency thresholds correlating to different complexity levels in sentence structure.

**Limitations:** The study is limited to the Japanese language and may not generalize to other languages or linguistic structures.

**Future Work:** Future research could explore similar relationships in other languages or delve deeper into cognitive processing associated with different types of predicates.

**Conclusion:** Cognitive load and predicate valency play crucial roles in shaping the dependency and hierarchical distances in Japanese, with implications for understanding linguistic complexity.

**Abstract:** To explore the relationship between dependency distance (DD) and hierarchical distance (HD) in Japanese, we compared the probability distributions of DD and HD with and without sentence length fixed, and analyzed the changes in mean dependency distance (MDD) and mean hierarchical distance (MHD) as sentence length increases, along with their correlation coefficient based on the Balanced Corpus of Contemporary Written Japanese. It was found that the valency of the predicates is the underlying factor behind the trade-off relation between MDD and MHD in Japanese. Native speakers of Japanese regulate the linear complexity and hierarchical complexity through the valency of the predicates, and the relative sizes of MDD and MHD depend on whether the threshold of valency has been reached. Apart from the cognitive load, the valency of the predicates also affects the probability distributions of DD and HD. The effect of the valency of the predicates on the distribution of HD is greater than on that of DD, which leads to differences in their probability distributions and causes the mean of MDD to be lower than that of MHD.

</details>


### [42] [RWKV-X: A Linear Complexity Hybrid Language Model](https://arxiv.org/abs/2504.21463)

*Haowen Hou, Zhiyi Huang, Kaifeng Tan, Rongchang Lu, Fei Richard Yu*

**Main category:** cs.CL

**Keywords:** RWKV-X, sparse attention, language modeling, efficiency, long-context

**Relevance Score:** 6

**TL;DR:** RWKV-X is a new hybrid architecture that efficiently combines short-range modeling with sparse attention for long-range context, achieving linear-time complexity for training and constant-time for inference, and outperforms earlier models in language tasks.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a scalable and efficient architecture for language modeling that can handle both short and long context without the complexity issues of full attention mechanisms.

**Method:** RWKV-X integrates the RWKV architecture with a sparse attention mechanism, allowing it to handle long-range dependencies while maintaining linear complexity in training and constant complexity in decoding.

**Key Contributions:** Introduces a hybrid architecture combining RWKV with sparse attention for better handling of long-range context., Achieves linear-time complexity for training and constant-time complexity for inference, unlike prior full attention models., Provides open access to code and checkpoints for further research.

**Result:** RWKV-X demonstrates near-perfect accuracy on a 64K passkey retrieval benchmark and outperforms previous RWKV-7 models in long-context benchmarks, while still excelling in short-context tasks.

**Limitations:** 

**Future Work:** Further exploration of scalability and performance improvements, as well as applications in diverse language modeling tasks.

**Conclusion:** RWKV-X is a promising architecture for general-purpose language modeling, capable of efficiently handling sequences up to 1 million tokens, with consistent speed and memory usage.

**Abstract:** In this paper, we introduce \textbf{RWKV-X}, a novel hybrid architecture that combines the efficiency of RWKV for short-range modeling with a sparse attention mechanism designed to capture long-range context. Unlike previous hybrid approaches that rely on full attention layers and retain quadratic complexity, RWKV-X achieves linear-time complexity in training and constant-time complexity in inference decoding. We demonstrate that RWKV-X, when continually pretrained on 64K-token sequences, achieves near-perfect accuracy on the 64K passkey retrieval benchmark. It consistently outperforms prior RWKV-7 models on long-context benchmarks, while maintaining strong performance on short-context tasks. These results highlight RWKV-X as a scalable and efficient backbone for general-purpose language modeling, capable of decoding sequences up to 1 million tokens with stable speed and memory usage. To facilitate further research and analysis, we have made the checkpoints and the associated code publicly accessible at: https://github.com/howard-hou/RWKV-X.

</details>


### [43] [Homa at SemEval-2025 Task 5: Aligning Librarian Records with OntoAligner for Subject Tagging](https://arxiv.org/abs/2504.21474)

*Hadi Bayrami Asl Tekanlou, Jafar Razmara, Mahsa Sanaei, Mostafa Rahgouy, Hamed Babaei Giglou*

**Main category:** cs.CL

**Keywords:** subject tagging, ontology alignment, retrieval-augmented generation, multilingual records, digital libraries

**Relevance Score:** 5

**TL;DR:** Homa is a system for subject tagging that uses ontology alignment and retrieval-augmented generation techniques to match technical records with the GND taxonomy for improved subject indexing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for effective subject tagging of technical records to enhance information retrieval in digital libraries.

**Method:** The authors employ OntoAligner for ontology alignment, framing the subject tagging as a semantic similarity alignment task between records and GND categories.

**Key Contributions:** Introduction of Homa system for subject tagging, Application of retrieval-augmented generation techniques, Evaluation of OntoAligner's effectiveness in multilingual contexts.

**Result:** Experimental results show the method's effectiveness in subject indexing and its adaptability with multilingual records, reflecting both strengths and limitations.

**Limitations:** The paper discusses limitations in the system's adaptability and effectiveness with diverse record types.

**Future Work:** Future research may focus on improving alignment techniques and expanding the applicability of the system to additional contexts.

**Conclusion:** Alignment techniques hold potential for enhancing subject tagging systems in digital libraries, leveraging ontology and semantic similarity.

**Abstract:** This paper presents our system, Homa, for SemEval-2025 Task 5: Subject Tagging, which focuses on automatically assigning subject labels to technical records from TIBKAT using the Gemeinsame Normdatei (GND) taxonomy. We leverage OntoAligner, a modular ontology alignment toolkit, to address this task by integrating retrieval-augmented generation (RAG) techniques. Our approach formulates the subject tagging problem as an alignment task, where records are matched to GND categories based on semantic similarity. We evaluate OntoAligner's adaptability for subject indexing and analyze its effectiveness in handling multilingual records. Experimental results demonstrate the strengths and limitations of this method, highlighting the potential of alignment techniques for improving subject tagging in digital libraries.

</details>


### [44] [Advancing Arabic Reverse Dictionary Systems: A Transformer-Based Approach with Dataset Construction Guidelines](https://arxiv.org/abs/2504.21475)

*Serry Sibaee, Samar Ahmed, Abdullah Al Harbi, Omer Nacar, Adel Ammar, Yasser Habashi, Wadii Boulila*

**Main category:** cs.CL

**Keywords:** Arabic natural language processing, reverse dictionary, transformer model

**Relevance Score:** 5

**TL;DR:** This study presents an Arabic Reverse Dictionary system using a novel transformer-based approach to improve Arabic natural language processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the critical gap in Arabic natural language processing and facilitate word retrieval based on meanings or descriptions.

**Method:** A novel transformer-based approach with a semi-encoder neural network architecture and a comprehensive dataset construction process for Arabic RD tasks.

**Key Contributions:** Development of an Arabic Reverse Dictionary system using a transformer-based model., Establishment of formal quality standards for Arabic lexicographic definitions., Creation of a modular Python library (RDTL) for configurable training pipelines.

**Result:** Experiments show that Arabic-specific models outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score.

**Limitations:** 

**Future Work:** Future research might explore enhancements in dataset quality and more applications of the reverse dictionary approach in other languages.

**Conclusion:** The work contributes significantly to Arabic computational linguistics, providing valuable tools for language learning and professional communication.

**Abstract:** This study addresses the critical gap in Arabic natural language processing by developing an effective Arabic Reverse Dictionary (RD) system that enables users to find words based on their descriptions or meanings. We present a novel transformer-based approach with a semi-encoder neural network architecture featuring geometrically decreasing layers that achieves state-of-the-art results for Arabic RD tasks. Our methodology incorporates a comprehensive dataset construction process and establishes formal quality standards for Arabic lexicographic definitions. Experiments with various pre-trained models demonstrate that Arabic-specific models significantly outperform general multilingual embeddings, with ARBERTv2 achieving the best ranking score (0.0644). Additionally, we provide a formal abstraction of the reverse dictionary task that enhances theoretical understanding and develop a modular, extensible Python library (RDTL) with configurable training pipelines. Our analysis of dataset quality reveals important insights for improving Arabic definition construction, leading to eight specific standards for building high-quality reverse dictionary resources. This work contributes significantly to Arabic computational linguistics and provides valuable tools for language learning, academic writing, and professional communication in Arabic.

</details>


### [45] [Improving Informally Romanized Language Identification](https://arxiv.org/abs/2504.21540)

*Adrian Benton, Alexander Gutkin, Christo Kirov, Brian Roark*

**Main category:** cs.CL

**Keywords:** Language Identification, Romanization, Indic Languages, Natural Language Processing, Machine Learning

**Relevance Score:** 6

**TL;DR:** This paper discusses improving language identification accuracy for romanized text of Indic languages by synthesizing training sets that incorporate natural spelling variations.

**Read time:** 16 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenges of language identification for languages that are informally written in Latin script, highlighting the high variability in spelling that confuses language detection systems.

**Method:** The authors synthesize training sets that include natural spelling variation and compare the performance of language identification systems trained on these synthetic samples against those trained on naturally occurring examples and higher capacity models.

**Key Contributions:** Introduction of synthetic training sets that consider natural spelling variation for LID., Demonstration of state-of-the-art performance for LID in romanized Indic languages., Potential framework for improving LID in multi-script languages.

**Result:** The study achieves state-of-the-art language identification performance for 20 Indic languages, improving the F1 score from 74.7% to 85.4% using a linear classifier on synthetic data, and up to 88.2% when including harvested text.

**Limitations:** 

**Future Work:** Future research could explore further integrations of different types of data for LID and test the methods on other language pairs beyond Indic languages.

**Conclusion:** The findings suggest that incorporating synthetic samples with natural spelling variations significantly enhances language identification accuracy for romanized versions of Indic languages.

**Abstract:** The Latin script is often used to informally write languages with non-Latin native scripts. In many cases (e.g., most languages in India), there is no conventional spelling of words in the Latin script, hence there will be high spelling variability in written text. Such romanization renders languages that are normally easily distinguished based on script highly confusable, such as Hindi and Urdu. In this work, we increase language identification (LID) accuracy for romanized text by improving the methods used to synthesize training sets. We find that training on synthetic samples which incorporate natural spelling variation yields higher LID system accuracy than including available naturally occurring examples in the training set, or even training higher capacity models. We demonstrate new state-of-the-art LID performance on romanized text from 20 Indic languages in the Bhasha-Abhijnaanam evaluation set (Madhani et al., 2023a), improving test F1 from the reported 74.7% (using a pretrained neural model) to 85.4% using a linear classifier trained solely on synthetic data and 88.2% when also training on available harvested text.

</details>


### [46] [TartuNLP at SemEval-2025 Task 5: Subject Tagging as Two-Stage Information Retrieval](https://arxiv.org/abs/2504.21547)

*Aleksei Dorkin, Kairit Sirts*

**Main category:** cs.CL

**Keywords:** information retrieval, subject tagging, bi-encoder, cross-encoder, librarians

**Relevance Score:** 4

**TL;DR:** A two-stage information retrieval system is proposed for assigning subject tags to library records, utilizing bi-encoders for candidate extraction and cross-encoders for re-ranking.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The project aims to enhance the tagging process for librarians by efficiently generating relevant subject tags for documents using an information retrieval framework.

**Method:** The system employs a two-stage approach: firstly, a bi-encoder model retrieves a broad set of candidate tags, and secondly, a cross-encoder refines the selection through fine-grained re-ranking.

**Key Contributions:** Introduction of a two-stage retrieval system for tagging library records, Demonstration of improved recall and qualitative results compared to existing methods, Application of bi-encoder and cross-encoder models in library tag assignment

**Result:** The two-stage method leads to significant improvements in recall over traditional single-stage methods, and it yields competitive results in qualitative assessments.

**Limitations:** 

**Future Work:** Exploration of further improvements in tag relevance and retrieval efficiency, including potential integration with existing library systems.

**Conclusion:** The proposed information retrieval system effectively aids librarians in tagging documents, suggesting an advancement in library science practices.

**Abstract:** We present our submission to the Task 5 of SemEval-2025 that aims to aid librarians in assigning subject tags to the library records by producing a list of likely relevant tags for a given document. We frame the task as an information retrieval problem, where the document content is used to retrieve subject tags from a large subject taxonomy. We leverage two types of encoder models to build a two-stage information retrieval system -- a bi-encoder for coarse-grained candidate extraction at the first stage, and a cross-encoder for fine-grained re-ranking at the second stage. This approach proved effective, demonstrating significant improvements in recall compared to single-stage methods and showing competitive results according to qualitative evaluation.

</details>


### [47] [Precision Where It Matters: A Novel Spike Aware Mixed-Precision Quantization Strategy for LLaMA-based Language Models](https://arxiv.org/abs/2504.21553)

*Lucas Maisonnave, Cyril Moineau, Olivier Bichler, Fabrice Rastello*

**Main category:** cs.CL

**Keywords:** Large Language Models, Quantization, LLaMA architecture, Mixed-precision, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents a novel mixed-precision quantization approach for LLaMA-like LLMs, improving efficiency and performance in resource-constrained environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The size of large language models (LLMs) poses significant challenges for their deployment and inference.

**Method:** The authors propose a mixed-precision quantization approach that applies higher precision to specific projection layers while quantizing the rest of the model to lower bit-widths.

**Key Contributions:** Novel mixed-precision quantization approach tailored for LLaMA architectures, Identification of activation spikes concentrated in specific projection layers, Demonstrated improvements in performance metrics for LLaMA-like models

**Result:** Experimental results show significant improvements in perplexity and zero-shot accuracy for LLaMA2, LLaMA3, and Mistral models using 8-bit per-tensor quantization compared to existing methods.

**Limitations:** 

**Future Work:** Further exploration of quantization techniques tailored to other model architectures and their deployment under various constraints.

**Conclusion:** Architecture-specific quantization strategies can enhance the efficiency and performance of LLMs, making them better suited for deployment in resource-constrained environments.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable capabilities in various natural language processing tasks. However, their size presents significant challenges for deployment and inference. This paper investigates the quantization of LLMs, focusing on the LLaMA architecture and its derivatives. We challenge existing assumptions about activation outliers in LLMs and propose a novel mixed-precision quantization approach tailored for LLaMA-like models. Our method leverages the observation that activation spikes in LLaMA architectures are predominantly concentrated in specific projection layers. By applying higher precision (FP16 or FP8) to these layers while quantizing the rest of the model to lower bit-widths, we achieve superior performance compared to existing quantization techniques. Experimental results on LLaMA2, LLaMA3, and Mistral models demonstrate significant improvements in perplexity and zero-shot accuracy, particularly for 8-bit per-tensor quantization. Our approach outperforms general-purpose methods designed to handle outliers across all architecture types, highlighting the benefits of architecture-specific quantization strategies. This research contributes to the ongoing efforts to make LLMs more efficient and deployable, potentially enabling their use in resource-constrained environments. Our findings emphasize the importance of considering model-specific characteristics in developing effective quantization pipelines for state-of-the-art language models by identifying and targeting a small number of projections that concentrate activation spikes.

</details>


### [48] [DNB-AI-Project at SemEval-2025 Task 5: An LLM-Ensemble Approach for Automated Subject Indexing](https://arxiv.org/abs/2504.21589)

*Lisa Kluge, Maximilian Kähler*

**Main category:** cs.CL

**Keywords:** LLMs, subject tagging, keyword generation, open-access catalog, SemEval-2025

**Relevance Score:** 9

**TL;DR:** Development of a system for automated subject tagging using LLMs for a technical library's catalog.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance keyword generation for open-access catalogs using LLMs, improving subject tagging accuracy.

**Method:** Utilizing few-shot prompting with LLMs and implementing post-processing steps for keyword mapping, aggregation, and ranking.

**Key Contributions:** Implementation of few-shot prompting with LLMs for keyword generation., Post-processing techniques for keyword mapping and aggregation., Quantitative and qualitative evaluation results highlighting system performance.

**Result:** Achieved fourth place in quantitative evaluation and best results in qualitative evaluation by subject indexing experts.

**Limitations:** Limited testing on diverse datasets beyond the technical library context.

**Future Work:** Expanding the system's application to more varied datasets and improving LLM training techniques.

**Conclusion:** The system demonstrates the effectiveness of LLMs for keyword suggestion in library contexts.

**Abstract:** This paper presents our system developed for the SemEval-2025 Task 5: LLMs4Subjects: LLM-based Automated Subject Tagging for a National Technical Library's Open-Access Catalog. Our system relies on prompting a selection of LLMs with varying examples of intellectually annotated records and asking the LLMs to similarly suggest keywords for new records. This few-shot prompting technique is combined with a series of post-processing steps that map the generated keywords to the target vocabulary, aggregate the resulting subject terms to an ensemble vote and, finally, rank them as to their relevance to the record. Our system is fourth in the quantitative ranking in the all-subjects track, but achieves the best result in the qualitative ranking conducted by subject indexing experts.

</details>


### [49] [Robust Misinformation Detection by Visiting Potential Commonsense Conflict](https://arxiv.org/abs/2504.21604)

*Bing Wang, Ximing Li, Changchun Li, Bingrui Zhao, Bo Fu, Renchu Guan, Shengsheng Wang*

**Main category:** cs.CL

**Keywords:** Misinformation Detection, Commonsense Reasoning, Machine Learning, Natural Language Processing, Dataset CoMis

**Relevance Score:** 7

**TL;DR:** Proposes a novel method for misinformation detection by leveraging commonsense conflicts in texts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing challenge of online misinformation which has significant negative impacts.

**Method:** Introduces the MD-PCC method that constructs commonsense expressions for articles to detect potential conflicts; develops a commonsense-oriented dataset named CoMis.

**Key Contributions:** Introduction of a novel augmentation method for misinformation detection (MD-PCC), Creation of a new commonsense-oriented dataset (CoMis) focusing on commonsense conflicts, Integration and performance testing of MD-PCC with existing misinformation detection frameworks

**Result:** Empirical results show that MD-PCC consistently outperforms existing misinformation detection baselines across multiple benchmark datasets.

**Limitations:** The applicability may depend on the quality of commonsense knowledge available and the diversity of the dataset.

**Future Work:** Exploration of further commonsense reasoning techniques to improve detection capabilities and expansion of the dataset for broader application.

**Conclusion:** MD-PCC provides a promising approach to enhance misinformation detection using commonsense reasoning.

**Abstract:** The development of Internet technology has led to an increased prevalence of misinformation, causing severe negative effects across diverse domains. To mitigate this challenge, Misinformation Detection (MD), aiming to detect online misinformation automatically, emerges as a rapidly growing research topic in the community. In this paper, we propose a novel plug-and-play augmentation method for the MD task, namely Misinformation Detection with Potential Commonsense Conflict (MD-PCC). We take inspiration from the prior studies indicating that fake articles are more likely to involve commonsense conflict. Accordingly, we construct commonsense expressions for articles, serving to express potential commonsense conflicts inferred by the difference between extracted commonsense triplet and golden ones inferred by the well-established commonsense reasoning tool COMET. These expressions are then specified for each article as augmentation. Any specific MD methods can be then trained on those commonsense-augmented articles. Besides, we also collect a novel commonsense-oriented dataset named CoMis, whose all fake articles are caused by commonsense conflict. We integrate MD-PCC with various existing MD backbones and compare them across both 4 public benchmark datasets and CoMis. Empirical results demonstrate that MD-PCC can consistently outperform the existing MD baselines.

</details>


### [50] [RDF-Based Structured Quality Assessment Representation of Multilingual LLM Evaluations](https://arxiv.org/abs/2504.21605)

*Jonas Gwozdz, Andreas Both*

**Main category:** cs.CL

**Keywords:** Large Language Models, Assessment Framework, Multilingual LLM Quality

**Relevance Score:** 8

**TL;DR:** This paper presents an RDF-based framework for assessing the reliability of multilingual LLMs, particularly in handling conflicting information.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for systematic assessment of LLM reliability when they face conflicting information, as current methods are inadequate.

**Method:** The framework analyzes model responses in four context scenarios: complete, incomplete, conflicting, and no-context, specifically for German and English.

**Key Contributions:** Introduction of an RDF-based framework for assessing multilingual LLM reliability, Identification of context prioritization patterns in LLM responses, Demonstration of language-specific performance variations in LLMs

**Result:** The experiment in the fire safety domain revealed significant insights into how context prioritization varies and the performance disparities between languages.

**Limitations:** The study is limited to the fire safety domain and may not generalize to other contexts without further validation.

**Future Work:** Future research should extend the framework to diverse domains and further explore language-specific performance and context handling.

**Conclusion:** The proposed framework successfully identifies knowledge conflicts and encourages future assessments in various domains using LLMs.

**Abstract:** Large Language Models (LLMs) increasingly serve as knowledge interfaces, yet systematically assessing their reliability with conflicting information remains difficult. We propose an RDF-based framework to assess multilingual LLM quality, focusing on knowledge conflicts. Our approach captures model responses across four distinct context conditions (complete, incomplete, conflicting, and no-context information) in German and English. This structured representation enables the comprehensive analysis of knowledge leakage-where models favor training data over provided context-error detection, and multilingual consistency. We demonstrate the framework through a fire safety domain experiment, revealing critical patterns in context prioritization and language-specific performance, and demonstrating that our vocabulary was sufficient to express every assessment facet encountered in the 28-question study.

</details>


### [51] [Meeseeks: An Iterative Benchmark Evaluating LLMs Multi-Turn Instruction-Following Ability](https://arxiv.org/abs/2504.21625)

*Jiaming Wang*

**Main category:** cs.CL

**Keywords:** Large Language Models, instruction-following, benchmark

**Relevance Score:** 9

**TL;DR:** Meeseeks is a benchmark for evaluating LLMs' instruction-following abilities through iterative feedback and self-correction, reflecting practical user interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The reliance on accurate instruction-following in real-world applications of LLMs demands a benchmark that closely simulates human-LLM interactions.

**Method:** Meeseeks features an iterative feedback process that allows LLMs to self-correct based on specific failures, with a structured evaluation of 38 capability tags across three dimensions.

**Key Contributions:** Introduces an iterative feedback mechanism for LLM evaluation., Organizes capabilities into three evaluative dimensions., Provides a comprehensive set of 38 capability tags for instruction-following assessment.

**Result:** Evaluation across LLMs using Meeseeks highlights significant insights into their instruction-following capabilities and discrepancies in performance based on user-like interactions.

**Limitations:** 

**Future Work:** Exploration of further capabilities and potential improvements in LLM self-correction during instruction-following tasks.

**Conclusion:** Meeseeks offers a new approach for benchmarking LLMs, demonstrating that iterative feedback mechanisms can enhance the reliability of instruction-following in practical applications.

**Abstract:** The ability to follow instructions accurately is fundamental for Large Language Models (LLMs) to serve as reliable agents in real-world applications. While existing instruction-following benchmarks are either single-turn or introduce new requirements in each turn without allowing self-correction, Meeseeks simulates realistic human-LLM interactions through an iterative feedback process. This design enables models to self-correct based on specific requirement failures, better reflecting real-world user-end usage patterns. The benchmark implements a comprehensive evaluation system with 38 capability tags organized across three dimensions: Intent Recognition, Granular Content Validation, and Output Structure Validation. Through rigorous evaluation across LLMs, Meeseeks provides valuable insights into LLMs' instruction-following capabilities in practical applications.

</details>


### [52] [Sadeed: Advancing Arabic Diacritization Through Small Language Model](https://arxiv.org/abs/2504.21635)

*Zeina Aldallal, Sara Chrouf, Khalil Hennara, Mohamed Motaism Hamed, Muhammad Hreden, Safwan AlModhayan*

**Main category:** cs.CL

**Keywords:** Arabic NLP, diacritization, language model, benchmark, machine translation

**Relevance Score:** 6

**TL;DR:** Sadeed is a fine-tuned language model for Arabic text diacritization that achieves competitive results with modest resources and introduces a new benchmark for evaluation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of Arabic text diacritization, which is complicated by the language's morphological richness.

**Method:** Sadeed is based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B, trained on high-quality diacritized datasets through a rigorous data-cleaning pipeline.

**Key Contributions:** Introduction of Sadeed, a novel model for Arabic text diacritization., Development of SadeedDiac-25 benchmark for better evaluation metrics., Rigorous data-cleaning approach for dataset curation.

**Result:** Sadeed achieves results competitive with proprietary models and outperforms traditional methods; introduces SadeedDiac-25 as a new benchmark for fair evaluation.

**Limitations:** Highlights existing limitations in current benchmarking practices for Arabic diacritization.

**Future Work:** Improving evaluation metrics and exploring further applications of Sadeed in various Arabic NLP tasks.

**Conclusion:** Sadeed and SadeedDiac-25 collectively advance Arabic NLP applications, aiding fields like machine translation and language learning.

**Abstract:** Arabic text diacritization remains a persistent challenge in natural language processing due to the language's morphological richness. In this paper, we introduce Sadeed, a novel approach based on a fine-tuned decoder-only language model adapted from Kuwain 1.5B Hennara et al. [2025], a compact model originally trained on diverse Arabic corpora. Sadeed is fine-tuned on carefully curated, high-quality diacritized datasets, constructed through a rigorous data-cleaning and normalization pipeline. Despite utilizing modest computational resources, Sadeed achieves competitive results compared to proprietary large language models and outperforms traditional models trained on similar domains. Additionally, we highlight key limitations in current benchmarking practices for Arabic diacritization. To address these issues, we introduce SadeedDiac-25, a new benchmark designed to enable fairer and more comprehensive evaluation across diverse text genres and complexity levels. Together, Sadeed and SadeedDiac-25 provide a robust foundation for advancing Arabic NLP applications, including machine translation, text-to-speech, and language learning tools.

</details>


### [53] [20min-XD: A Comparable Corpus of Swiss News Articles](https://arxiv.org/abs/2504.21677)

*Michelle Wastl, Jannis Vamvas, Selena Calleri, Rico Sennrich*

**Main category:** cs.CL

**Keywords:** cross-lingual, NLP, comparable corpus, semantic similarity, news articles

**Relevance Score:** 4

**TL;DR:** 20min-XD is a French-German, document-level comparable corpus of news articles comprising around 15,000 article pairs. It aims to support various NLP applications.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a resource for NLP applications and linguistically motivated studies through a cross-lingual dataset.

**Method:** Articles were sourced from the Swiss online news outlet 20 Minuten/20 minutes and automatically aligned based on semantic similarity.

**Key Contributions:** Creation of a large French-German document-level comparable corpus, ., Release of both document- and sentence-aligned versions with accompanying code.

**Result:** The dataset shows a wide range of cross-lingual similarities, making it suitable for various applications in NLP.

**Limitations:** 

**Future Work:** Further exploration of the dataset's applications in different NLP tasks and studies.

**Conclusion:** The released dataset will support research in cross-lingual NLP and similar studies.

**Abstract:** We present 20min-XD (20 Minuten cross-lingual document-level), a French-German, document-level comparable corpus of news articles, sourced from the Swiss online news outlet 20 Minuten/20 minutes. Our dataset comprises around 15,000 article pairs spanning 2015 to 2024, automatically aligned based on semantic similarity. We detail the data collection process and alignment methodology. Furthermore, we provide a qualitative and quantitative analysis of the corpus. The resulting dataset exhibits a broad spectrum of cross-lingual similarity, ranging from near-translations to loosely related articles, making it valuable for various NLP applications and broad linguistically motivated studies. We publicly release the dataset in document- and sentence-aligned versions and code for the described experiments.

</details>


### [54] [Investigating the Effect of Parallel Data in the Cross-Lingual Transfer for Vision-Language Encoders](https://arxiv.org/abs/2504.21681)

*Andrei-Alexandru Manea, Jindřich Libovický*

**Main category:** cs.CL

**Keywords:** Vision-Language models, multilingual tasks, parallel data

**Relevance Score:** 6

**TL;DR:** The paper investigates the effectiveness of transferring a pre-trained Vision-Language encoder using parallel data across multiple languages, emphasizing the impact of the size and domain of the parallel data used.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limited availability of pre-trained Vision-Language models and datasets in languages other than English, and to explore alternative approaches to multilingual tasks.

**Method:** The study evaluates the performance of multilingual Vision-Language models by transferring a trained encoder using various types of parallel data, including machine-translated and authentic caption-like data.

**Key Contributions:** Introduces an alternative approach for multilingual VL tasks by transferring an already trained encoder., Analyzes the effect of parallel data's domain and the number of languages on model performance., Demonstrates the effectiveness of authentic parallel data over machine-translated data in specific contexts.

**Result:** The results indicate that while machine-translated task data perform well on average, authentic parallel data tends to outperform it in certain languages, showing that most languages indeed benefit from multilingual training.

**Limitations:** The study may be limited by the specific languages and domains analyzed, which may not generalize to all scenarios.

**Future Work:** Future research could investigate additional language pairs and larger datasets to further validate the findings and optimize parallel data selection.

**Conclusion:** The findings suggest that the choice of parallel data is crucial for optimizing multilingual Vision-Language tasks, and that various languages can achieve improvements through dedicated multilingual training efforts.

**Abstract:** Most pre-trained Vision-Language (VL) models and training data for the downstream tasks are only available in English. Therefore, multilingual VL tasks are solved using cross-lingual transfer: fine-tune a multilingual pre-trained model or transfer the text encoder using parallel data. We study the alternative approach: transferring an already trained encoder using parallel data. We investigate the effect of parallel data: domain and the number of languages, which were out of focus in previous work. Our results show that even machine-translated task data are the best on average, caption-like authentic parallel data outperformed it in some languages. Further, we show that most languages benefit from multilingual training.

</details>


### [55] [Enhancing Health Mention Classification Performance: A Study on Advancements in Parameter Efficient Tuning](https://arxiv.org/abs/2504.21685)

*Reem Abdel-Salam, Mary Adewunmi*

**Main category:** cs.CL

**Keywords:** Health Mention Classification, natural language processing, public health monitoring

**Relevance Score:** 9

**TL;DR:** This paper discusses the challenges of Health Mention Classification in social media and proposes a methodology that improves classification accuracy using POS tagger information and PEFT techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage social media posts for real-time public health monitoring by improving Health Mention Classification (HMC).

**Method:** The study employs conventional fine-tuning with enhanced parameters of biomedical natural language methods, utilizing part-of-speech tagger information and improving PEFT techniques.

**Key Contributions:** Improved HMC through the use of POS tagger information., Enhanced performance with PEFT techniques., Successful experimentation on multiple datasets.

**Result:** Using the proposed method, significant improvements in F1-score were observed across three datasets (RHDM, PHM, and Illness), outperforming state-of-the-art methods.

**Limitations:** 

**Future Work:** 

**Conclusion:** The proposed methodology effectively classifies health mentions in social media with optimized model size and training efficiency.

**Abstract:** Health Mention Classification (HMC) plays a critical role in leveraging social media posts for real-time tracking and public health monitoring. Nevertheless, the process of HMC presents significant challenges due to its intricate nature, primarily stemming from the contextual aspects of health mentions, such as figurative language and descriptive terminology, rather than explicitly reflecting a personal ailment. To address this problem, we argue that clearer mentions can be achieved through conventional fine-tuning with enhanced parameters of biomedical natural language methods (NLP). In this study, we explore different techniques such as the utilisation of part-of-speech (POS) tagger information, improving on PEFT techniques, and different combinations thereof. Extensive experiments are conducted on three widely used datasets: RHDM, PHM, and Illness. The results incorporated POS tagger information, and leveraging PEFT techniques significantly improves performance in terms of F1-score compared to state-of-the-art methods across all three datasets by utilising smaller models and efficient training. Furthermore, the findings highlight the effectiveness of incorporating POS tagger information and leveraging PEFT techniques for HMC. In conclusion, the proposed methodology presents a potentially effective approach to accurately classifying health mentions in social media posts while optimising the model size and training efficiency.

</details>


### [56] [Investigating Literary Motifs in Ancient and Medieval Novels with Large Language Models](https://arxiv.org/abs/2504.21742)

*Emelie Hallenberg*

**Main category:** cs.CL

**Keywords:** literary motifs, Greek love novels, large language models

**Relevance Score:** 2

**TL;DR:** This study investigates the common and differing literary motifs in Greek love novels from the first century CE to the 15th century using fine-tuned large language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the similarities and differences in literary motifs in Greek romantic narratives across centuries.

**Method:** The study employs fine-tuned large language models to analyze a corpus of Greek fictional love novels for motif identification.

**Key Contributions:** Application of large language models in literary analysis, Identification of common and differing motifs across time periods, Quantitative data supporting qualitative insights in literary studies.

**Result:** The analysis reveals persistent and fluctuating motifs, indicating trends and external influences over time.

**Limitations:** 

**Future Work:** Further exploration of external influences on motif trends and expansion of the analysis to other literary genres or periods.

**Conclusion:** The method effectively extracts a defined set of literary motifs, facilitating both quantitative and qualitative analysis.

**Abstract:** The Greek fictional narratives often termed love novels or romances, ranging from the first century CE to the middle of the 15th century, have long been considered as similar in many ways, not least in the use of particular literary motifs. By applying the use of fine-tuned large language models, this study aims to investigate which motifs exactly that the texts in this corpus have in common, and in which ways they differ from each other. The results show that while some motifs persist throughout the corpus, others fluctuate in frequency, indicating certain trends or external influences. Conclusively, the method proves to adequately extract literary motifs according to a set definition, providing data for both quantitative and qualitative analyses.

</details>


### [57] [Improving Retrieval-Augmented Neural Machine Translation with Monolingual Data](https://arxiv.org/abs/2504.21747)

*Maxime Bouthors, Josep Crego, François Yvon*

**Main category:** cs.CL

**Keywords:** neural machine translation, retrieval-augmented, cross-lingual, monolingual corpora, translation performance

**Relevance Score:** 6

**TL;DR:** This paper improves retrieval-augmented neural machine translation (RANMT) by utilizing in-domain monolingual target-side corpora for translation, outperforming traditional translation memories.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage the abundance of in-domain monolingual target-side corpora for improving translation quality in RANMT systems, as conventional methods rely heavily on bilingual corpora.

**Method:** The authors designed advanced cross-lingual retrieval systems with both sentence-level and word-level matching objectives to retrieve relevant segments in the target language based on source-side queries.

**Key Contributions:** Introduction of cross-lingual retrieval systems for RANMT, Implementation of sentence-level and word-level matching objectives, Demonstrated performance improvements using monolingual target-side corpora

**Result:** Experiments demonstrated that the proposed cross-lingual retrieval objectives significantly enhance translation performance over traditional translation memory-based models, particularly in scenarios where monolingual resources are abundant.

**Limitations:** The paper does not address the potential drawbacks of using monolingual resources, such as the quality and relevance of the retrieved segments in practical applications.

**Future Work:** Future research could explore further optimization of retrieval systems and their integration into various translation tasks.

**Conclusion:** The new techniques developed allow for better utilization of target-side monolingual corpora, leading to performance improvements in real-world settings compared to baseline and general-purpose cross-lingual retrievers.

**Abstract:** Conventional retrieval-augmented neural machine translation (RANMT) systems leverage bilingual corpora, e.g., translation memories (TMs). Yet, in many settings, in-domain monolingual target-side corpora are often available. This work explores ways to take advantage of such resources by retrieving relevant segments directly in the target language, based on a source-side query. For this, we design improved cross-lingual retrieval systems, trained with both sentence level and word-level matching objectives. In our experiments with two RANMT architectures, we first demonstrate the benefits of such cross-lingual objectives in a controlled setting, obtaining translation performances that surpass standard TM-based models. We then showcase our method on a real-world set-up, where the target monolingual resources far exceed the amount of parallel data and observe large improvements of our new techniques, which outperform both the baseline setting, and general-purpose cross-lingual retrievers.

</details>


### [58] [MAC-Tuning: LLM Multi-Compositional Problem Reasoning with Enhanced Knowledge Boundary Awareness](https://arxiv.org/abs/2504.21773)

*Junsheng Huang, Zhitao He, Sandeep Polisetty, Qingyun Wang, May Fung*

**Main category:** cs.CL

**Keywords:** large language models, confidence estimation, multi-problem setting, answer prediction, hallucination

**Relevance Score:** 9

**TL;DR:** The paper introduces MAC-Tuning, a novel approach for improving confidence estimation in LLMs during multi-problem settings, addressing hallucination issues.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of LLMs, hallucinations (generating false information) have become a significant concern that this research aims to mitigate, particularly in multi-problem environments.

**Method:** The proposed method, MAC-Tuning, involves separating the processes of answer prediction and confidence estimation during the fine-tuning phase on instruction data.

**Key Contributions:** Introduction of MAC-Tuning method for simultaneous multi-problem answer prediction and confidence estimation., Improvement of average precision in LLM outputs by up to 25%., Focus on addressing the challenge of hallucination in generated responses.

**Result:** Experimental results reveal that MAC-Tuning improves average precision by up to 25% compared to existing methods.

**Limitations:** 

**Future Work:** Future research could explore further refinements of MAC-Tuning and its application to different LLM architectures.

**Conclusion:** The findings suggest that the separation of learning processes enhances the predictive capabilities and reliability of LLMs.

**Abstract:** With the widespread application of large language models (LLMs), the issue of generating non-existing facts, known as hallucination, has garnered increasing attention. Previous research in enhancing LLM confidence estimation mainly focuses on the single problem setting. However, LLM awareness of its internal parameterized knowledge boundary under the more challenging multi-problem setting, which requires answering multiple problems accurately simultaneously, remains underexplored. To bridge this gap, we introduce a novel method, Multiple Answers and Confidence Stepwise Tuning (MAC-Tuning), that separates the learning of answer prediction and confidence estimation during fine-tuning on instruction data. Extensive experiments demonstrate that our method outperforms baselines by up to 25% in average precision.

</details>


### [59] [WebThinker: Empowering Large Reasoning Models with Deep Research Capability](https://arxiv.org/abs/2504.21776)

*Xiaoxi Li, Jiajie Jin, Guanting Dong, Hongjin Qian, Yutao Zhu, Yongkang Wu, Ji-Rong Wen, Zhicheng Dou*

**Main category:** cs.CL

**Keywords:** Large reasoning models, Deep research agent, Web navigation

**Relevance Score:** 8

**TL;DR:** WebThinker is a deep research agent that enhances large reasoning models by enabling them to autonomously search the web and draft research reports, overcoming limitations of static internal knowledge.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large reasoning models on complex, knowledge-intensive tasks that require dynamic information synthesis from the web.

**Method:** WebThinker integrates a Deep Web Explorer for real-time web searching and navigating, combined with an Autonomous Think-Search-and-Draft strategy and an RL-based training strategy via iterative online Direct Preference Optimization.

**Key Contributions:** Development of a deep research agent that autonomously navigates web resources., Integration of real-time reasoning, information gathering, and report drafting., Introduction of a reinforcement learning-based training strategy for improved performance.

**Result:** WebThinker demonstrates significant performance improvements on various reasoning benchmarks and scientific report generation tasks, outperforming existing methods and proprietary systems.

**Limitations:** 

**Future Work:** Further enhancement of LRM capabilities and exploration of additional applications in research and beyond.

**Conclusion:** The WebThinker framework presents a significant advancement in the reliability and applicability of large reasoning models for complex scenarios, leading to enhanced capabilities in deep research systems.

**Abstract:** Large reasoning models (LRMs), such as OpenAI-o1 and DeepSeek-R1, demonstrate impressive long-horizon reasoning capabilities. However, their reliance on static internal knowledge limits their performance on complex, knowledge-intensive tasks and hinders their ability to produce comprehensive research reports requiring synthesis of diverse web information. To address this, we propose \textbf{WebThinker}, a deep research agent that empowers LRMs to autonomously search the web, navigate web pages, and draft research reports during the reasoning process. WebThinker integrates a \textbf{Deep Web Explorer} module, enabling LRMs to dynamically search, navigate, and extract information from the web when encountering knowledge gaps. It also employs an \textbf{Autonomous Think-Search-and-Draft strategy}, allowing the model to seamlessly interleave reasoning, information gathering, and report writing in real time. To further enhance research tool utilization, we introduce an \textbf{RL-based training strategy} via iterative online Direct Preference Optimization (DPO). Extensive experiments on complex reasoning benchmarks (GPQA, GAIA, WebWalkerQA, HLE) and scientific report generation tasks (Glaive) demonstrate that WebThinker significantly outperforms existing methods and strong proprietary systems. Our approach enhances LRM reliability and applicability in complex scenarios, paving the way for more capable and versatile deep research systems. The code is available at https://github.com/RUC-NLPIR/WebThinker.

</details>


### [60] [How Real Are Synthetic Therapy Conversations? Evaluating Fidelity in Prolonged Exposure Dialogues](https://arxiv.org/abs/2504.21800)

*Suhas BN, Dominik Mattioli, Saeed Abdullah, Rosa I. Arriaga, Chris W. Wiese, Andrew M. Sherrill*

**Main category:** cs.CL

**Keywords:** Synthetic data, Post-Traumatic Stress Disorder, Therapeutic conversations, Fidelity metrics, Healthcare AI

**Relevance Score:** 8

**TL;DR:** This paper explores the use of synthetic data for training models in PTSD therapy by comparing real and synthetic therapeutic conversations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The adoption of synthetic data in healthcare is necessary due to privacy concerns, limited access to real data, and high annotation costs.

**Method:** The authors systematically compare real and synthetic dialogues using various metrics including linguistic, structural, and protocol-specific ones such as turn-taking patterns and treatment fidelity. They introduce PE-specific metrics derived from linguistic analysis and semantic modeling.

**Key Contributions:** Introduction of a novel framework for assessing clinical fidelity in synthetic dialogues., Systematic comparison of real vs synthetic therapeutic conversations using comprehensive metrics., Highlighting the limitations of synthetic data in capturing subtle therapeutic dynamics.

**Result:** Findings indicate that while synthetic dialogues match certain structural features of real dialogues, they fail to capture critical fidelity markers like distress monitoring, highlighting gaps in existing evaluation frameworks.

**Limitations:** Synthetic dialogues do not adequately reflect key fidelity markers, limiting their effectiveness as a stand-alone data source.

**Future Work:** Further development of fidelity-aware metrics and investigation into mitigating the identified limitations of synthetic data in therapeutic contexts.

**Conclusion:** Synthetic data shows potential in mitigating data scarcity but struggles with capturing the dynamics of therapeutic interactions; the authors propose fidelity-aware metrics for better assessment.

**Abstract:** The growing adoption of synthetic data in healthcare is driven by privacy concerns, limited access to real-world data, and the high cost of annotation. This work explores the use of synthetic Prolonged Exposure (PE) therapeutic conversations for Post-Traumatic Stress Disorder (PTSD) as a scalable alternative for training and evaluating clinical models. We systematically compare real and synthetic dialogues using linguistic, structural, and protocol-specific metrics, including turn-taking patterns and treatment fidelity. We also introduce and evaluate PE-specific metrics derived from linguistic analysis and semantic modeling, offering a novel framework for assessing clinical fidelity beyond surface fluency. Our findings show that although synthetic data holds promise for mitigating data scarcity and protecting patient privacy, it can struggle to capture the subtle dynamics of therapeutic interactions. In our dataset, synthetic dialogues match structural features of real-world dialogues (e.g., speaker switch ratio: 0.98 vs. 0.99), however, synthetic interactions do not adequately reflect key fidelity markers (e.g., distress monitoring). We highlight gaps in existing evaluation frameworks and advocate for fidelity-aware metrics that go beyond surface fluency to uncover clinically significant failures. Our findings clarify where synthetic data can effectively complement real-world datasets -- and where critical limitations remain.

</details>


### [61] [DeepSeek-Prover-V2: Advancing Formal Mathematical Reasoning via Reinforcement Learning for Subgoal Decomposition](https://arxiv.org/abs/2504.21801)

*Z. Z. Ren, Zhihong Shao, Junxiao Song, Huajian Xin, Haocheng Wang, Wanjia Zhao, Liyue Zhang, Zhe Fu, Qihao Zhu, Dejian Yang, Z. F. Wu, Zhibin Gou, Shirong Ma, Hongxuan Tang, Yuxuan Liu, Wenjun Gao, Daya Guo, Chong Ruan*

**Main category:** cs.CL

**Keywords:** large language model, theorem proving, formal reasoning, machine learning, AI

**Relevance Score:** 6

**TL;DR:** DeepSeek-Prover-V2 is an advanced LLM for formal theorem proving, achieving notable performance on various benchmarks by integrating informal and formal reasoning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the capabilities of large language models in formal theorem proving by seamlessly integrating informal and formal mathematical reasoning.

**Method:** The model uses a cold-start training procedure where DeepSeek-V3 decomposes complex problems into subgoals, synthesizing proofs through a step-by-step reasoning process, followed by reinforcement learning.

**Key Contributions:** Introduction of DeepSeek-Prover-V2 for formal theorem proving in Lean 4., Creation of the ProverBench evaluation benchmark with 325 formalized problems., Demonstrated a significant improvement in solving capabilities over existing models.

**Result:** Achieved a state-of-the-art performance with an 88.9% pass ratio on the MiniF2F-test and successfully tackled problems from PutnamBench and AIME competitions, including a newly introduced benchmark called ProverBench.

**Limitations:** 

**Future Work:** Further exploration of the capabilities of LLMs in formal reasoning and potential extensions of the model for enhanced problem-solving.

**Conclusion:** DeepSeek-Prover-V2 significantly narrows the gap between informal and formal mathematical reasoning in LLMs, showcasing its utility in theorem proving.

**Abstract:** We introduce DeepSeek-Prover-V2, an open-source large language model designed for formal theorem proving in Lean 4, with initialization data collected through a recursive theorem proving pipeline powered by DeepSeek-V3. The cold-start training procedure begins by prompting DeepSeek-V3 to decompose complex problems into a series of subgoals. The proofs of resolved subgoals are synthesized into a chain-of-thought process, combined with DeepSeek-V3's step-by-step reasoning, to create an initial cold start for reinforcement learning. This process enables us to integrate both informal and formal mathematical reasoning into a unified model. The resulting model, DeepSeek-Prover-V2-671B, achieves state-of-the-art performance in neural theorem proving, reaching 88.9% pass ratio on the MiniF2F-test and solving 49 out of 658 problems from PutnamBench. In addition to standard benchmarks, we introduce ProverBench, a collection of 325 formalized problems, to enrich our evaluation, including 15 selected problems from the recent AIME competitions (years 24-25). Further evaluation on these 15 AIME problems shows that the model successfully solves 6 of them. In comparison, DeepSeek-V3 solves 8 of these problems using majority voting, highlighting that the gap between formal and informal mathematical reasoning in large language models is substantially narrowing.

</details>


### [62] [TRUST: An LLM-Based Dialogue System for Trauma Understanding and Structured Assessments](https://arxiv.org/abs/2504.21851)

*Sichang Tu, Abigail Powers, Stephen Doogan, Jinho D. Choi*

**Main category:** cs.CL

**Keywords:** Large Language Models, dialogue systems, mental healthcare, PTSD, clinical interviews

**Relevance Score:** 9

**TL;DR:** This paper presents TRUST, an LLM-powered dialogue system designed for conducting formal diagnostic interviews for PTSD, aiming to enhance mental healthcare accessibility.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the lack of existing dialogue systems for standard diagnostic interviews in mental healthcare, which affects accessibility.

**Method:** The authors develop the TRUST framework, which consists of cooperative LLM modules and a Dialogue Acts schema tailored for clinical interviews, using a patient simulation approach based on real-life transcripts.

**Key Contributions:** Introduction of the TRUST framework for mental health diagnostic interviews., Development of a Dialogue Acts schema for guiding clinical responses., Implementation of a patient simulation approach to streamline testing.

**Result:** Evaluation metrics indicate that TRUST performs comparably to real-life clinical interviews, as confirmed by expert evaluations.

**Limitations:** The system is currently at the level of average clinicians and may require further improvements in response appropriateness and communication styles.

**Future Work:** Future research may focus on enhancing communication styles and exploring other diagnostic use cases.

**Conclusion:** TRUST demonstrates the potential to improve mental healthcare accessibility, although there is room for enhancements in communication styles.

**Abstract:** Objectives: While Large Language Models (LLMs) have been widely used to assist clinicians and support patients, no existing work has explored dialogue systems for standard diagnostic interviews and assessments. This study aims to bridge the gap in mental healthcare accessibility by developing an LLM-powered dialogue system that replicates clinician behavior. Materials and Methods: We introduce TRUST, a framework of cooperative LLM modules capable of conducting formal diagnostic interviews and assessments for Post-Traumatic Stress Disorder (PTSD). To guide the generation of appropriate clinical responses, we propose a Dialogue Acts schema specifically designed for clinical interviews. Additionally, we develop a patient simulation approach based on real-life interview transcripts to replace time-consuming and costly manual testing by clinicians. Results: A comprehensive set of evaluation metrics is designed to assess the dialogue system from both the agent and patient simulation perspectives. Expert evaluations by conversation and clinical specialists show that TRUST performs comparably to real-life clinical interviews. Discussion: Our system performs at the level of average clinicians, with room for future enhancements in communication styles and response appropriateness. Conclusions: Our TRUST framework shows its potential to facilitate mental healthcare availability.

</details>


### [63] [LLMs and Finetuning: Benchmarking cross-domain performance for hate speech detection](https://arxiv.org/abs/2310.18964)

*Ahmad Nasir, Aadish Sharma, Kokil Jaidka, Saifuddin Ahmed*

**Main category:** cs.CL

**Keywords:** Hate Speech Detection, Large Language Models, Machine Learning

**Relevance Score:** 7

**TL;DR:** This study explores how pre-trained and fine-tuned LLMs can effectively detect hate speech across diverse digital platforms, revealing the impact of training parameters and dataset features on model performance.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of hate speech detection in online communication and assess the effectiveness of LLMs in this context.

**Method:** The study employs ordinary least squares analyses to evaluate the performance of pre-trained and fine-tuned LLMs in hate speech detection and examines their adaptability across different domains.

**Key Contributions:** Demonstrates the effectiveness of LLMs for hate speech detection., Identifies the diminished returns of fine-grained labels with larger dataset sizes., Provides recommendations for benchmarking experiments in hate speech detection.

**Result:** The findings indicate that LLMs outperform state-of-the-art methods, even without pretraining. However, the advantage of using fine-grained hate speech labels diminishes as dataset sizes increase.

**Limitations:** Concerns regarding the validity and reproducibility of experimental results remain.

**Future Work:** Future research should focus on overcoming the identified challenges and developing more robust benchmarking methodologies.

**Conclusion:** While LLMs show significant potential for hate speech detection, limitations in the validity and reproducibility of results persist; the paper discusses these challenges and proposes best practices for future research.

**Abstract:** In the evolving landscape of online communication, hate speech detection remains a formidable challenge, further compounded by the diversity of digital platforms. This study investigates the effectiveness and adaptability of pre-trained and fine-tuned Large Language Models (LLMs) in identifying hate speech, to address two central questions: (1) To what extent does the model performance depend on the fine-tuning and training parameters?, (2) To what extent do models generalize to cross-domain hate speech detection? and (3) What are the specific features of the datasets or models that influence the generalization potential? The experiment shows that LLMs offer a huge advantage over the state-of-the-art even without pretraining. Ordinary least squares analyses suggest that the advantage of training with fine-grained hate speech labels is washed away with the increase in dataset size. While our research demonstrates the potential of large language models (LLMs) for hate speech detection, several limitations remain, particularly regarding the validity and the reproducibility of the results. We conclude with an exhaustive discussion of the challenges we faced in our experimentation and offer recommended best practices for future scholars designing benchmarking experiments of this kind.

</details>


### [64] [Round Trip Translation Defence against Large Language Model Jailbreaking Attacks](https://arxiv.org/abs/2402.13517)

*Canaan Yung, Hadi Mohaghegh Dolatabadi, Sarah Erfani, Christopher Leckie*

**Main category:** cs.CL

**Keywords:** Large Language Models, Social-engineered attacks, Round Trip Translation, Defense mechanisms, Machine Learning

**Relevance Score:** 9

**TL;DR:** Proposes Round Trip Translation (RTT) to defend LLMs against social-engineered attacks, achieving over 70% mitigation of specific attack types.

**Read time:** 6 min

<details>
  <summary>Details</summary>

**Motivation:** Existing defensive measures for LLMs against social-engineered attacks are inadequate, necessitating a robust solution.

**Method:** The RTT algorithm paraphrases adversarial prompts to help LLMs better identify harmful behaviors.

**Key Contributions:** First algorithm specifically designed to defend against social-engineered attacks on LLMs., Mitigated over 70% of PAIR attacks., Reduced MathsAttack success rate by almost 40%.

**Result:** RTT mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks and reduced MathsAttack success rates by nearly 40%.

**Limitations:** 

**Future Work:** Exploration of additional attack types and enhancement of defense methods across various LLM architectures.

**Conclusion:** RTT is a versatile and effective method for defending LLMs against tailored social-engineered threats, with public code available for further development.

**Abstract:** Large language models (LLMs) are susceptible to social-engineered attacks that are human-interpretable but require a high level of comprehension for LLMs to counteract. Existing defensive measures can only mitigate less than half of these attacks at most. To address this issue, we propose the Round Trip Translation (RTT) method, the first algorithm specifically designed to defend against social-engineered attacks on LLMs. RTT paraphrases the adversarial prompt and generalizes the idea conveyed, making it easier for LLMs to detect induced harmful behavior. This method is versatile, lightweight, and transferrable to different LLMs. Our defense successfully mitigated over 70% of Prompt Automatic Iterative Refinement (PAIR) attacks, which is currently the most effective defense to the best of our knowledge. We are also the first to attempt mitigating the MathsAttack and reduced its attack success rate by almost 40%. Our code is publicly available at https://github.com/Cancanxxx/Round_Trip_Translation_Defence   This version of the article has been accepted for publication, after peer review (when applicable) but is not the Version of Record and does not reflect post-acceptance improvements, or any corrections. The Version of Record is available online at: https://doi.org/10.48550/arXiv.2402.13517 Use of this Accepted Version is subject to the publisher's Accepted Manuscript terms of use https://www.springernature.com/gp/open-research/policies/accepted-manuscript-terms

</details>


### [65] [Does Generative AI speak Nigerian-Pidgin?: Issues about Representativeness and Bias for Multilingualism in LLMs](https://arxiv.org/abs/2404.19442)

*David Ifeoluwa Adelani, A. Seza Doğruöz, Iyanuoluwa Shode, Anuoluwapo Aremu*

**Main category:** cs.CL

**Keywords:** Naija, West African Pidgin English, Generative AI, Machine Translation, linguistic diversity

**Relevance Score:** 6

**TL;DR:** This paper analyzes the linguistic characteristics of Naija and West African Pidgin English (WAPE) and highlights the underrepresentation of Naija in Generative AI applications, revealing significant linguistic differences between the two pidgins.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the linguistic differences between Naija and WAPE, and to investigate the implications of these differences for Generative AI applications and Machine Translation.

**Method:** Statistical analyses and Machine Translation experiments are conducted to compare Naija and WAPE, alongside interviews with volunteer Wikipedia contributors in Naija.

**Key Contributions:** Identifies linguistic differences between Naija and WAPE., Demonstrates the underrepresentation of Naija in Generative AI applications., Provides historical context and insights from interviews with contributors.

**Result:** The analysis reveals that Naija and WAPE do not represent each other linguistically, with distinct vocabulary and word order, and that Generative AI primarily operates with WAPE, neglecting Naija.

**Limitations:** The study may not cover all linguistic aspects of Naija and its representation in AI due to the complexity of pidgin languages.

**Future Work:** Further research is needed to enhance the representation of Naija in AI systems and to better integrate multilingual aspects into Generative AI.

**Conclusion:** Generative AI's dependence on WAPE and the linguistic uniqueness of Naija suggest challenges in teaching LLMs due to its underrepresentation in training data.

**Abstract:** Nigeria is a multilingual country with 500+ languages. Naija is a Nigerian Pidgin spoken by approximately 120M speakers and it is a mixed language (e.g., English, Portuguese, Yoruba, Hausa and Igbo). Although it has mainly been a spoken language until recently, there are some online platforms (e.g., Wikipedia), publishing in written Naija as well. West African Pidgin English (WAPE) is also spoken in Nigeria and it is used by BBC to broadcast news on the internet to a wider audience not only in Nigeria but also in other West African countries (e.g., Cameroon and Ghana). Through statistical analyses and Machine Translation experiments, our paper shows that these two pidgin varieties do not represent each other (i.e., there are linguistic differences in word order and vocabulary) and Generative AI operates only based on WAPE. In other words, Naija is underrepresented in Generative AI, and it is hard to teach LLMs with few examples. In addition to the statistical analyses, we also provide historical information on both pidgins as well as insights from the interviews conducted with volunteer Wikipedia contributors in Naija.

</details>


### [66] [Emergence of a High-Dimensional Abstraction Phase in Language Transformers](https://arxiv.org/abs/2405.15471)

*Emily Cheng, Diego Doimo, Corentin Kervadec, Iuri Macocco, Jade Yu, Alessandro Laio, Marco Baroni*

**Main category:** cs.CL

**Keywords:** language models, geometric properties, high-dimensionality, linguistic processing, transformer-based LMs

**Relevance Score:** 9

**TL;DR:** The study analyzes the geometric properties of language models (LMs) and identifies a high-dimensionality phase that corresponds to core linguistic processing.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the geometric mapping of linguistic contexts in language models and its implications for their function.

**Method:** Analyzed five pre-trained transformer-based LMs across three input datasets to investigate their geometric characteristics.

**Key Contributions:** Identification of a high-dimensionality phase in LMs, Relationship between high-dimensionality and language modeling performance, Insights into how representations transfer to downstream tasks

**Result:** Discovered a distinct high-dimensionality phase related to linguistic abstraction and downstream task prediction, which also correlates with language modeling performance.

**Limitations:** The analysis is limited to a specific set of LMs and datasets, and further exploration is needed across more architectures and contexts.

**Future Work:** To investigate the high-dimensionality phase in other language models and its implications for broader linguistic processing.

**Conclusion:** A central high-dimensionality phase is critical for linguistic processing in various LM architectures, indicating implications for model design.

**Abstract:** A language model (LM) is a mapping from a linguistic context to an output token. However, much remains to be known about this mapping, including how its geometric properties relate to its function. We take a high-level geometric approach to its analysis, observing, across five pre-trained transformer-based LMs and three input datasets, a distinct phase characterized by high intrinsic dimensionality. During this phase, representations (1) correspond to the first full linguistic abstraction of the input; (2) are the first to viably transfer to downstream tasks; (3) predict each other across different LMs. Moreover, we find that an earlier onset of the phase strongly predicts better language modelling performance. In short, our results suggest that a central high-dimensionality phase underlies core linguistic processing in many common LM architectures.

</details>


### [67] [Extracting and Transferring Abilities For Building Multi-lingual Ability-enhanced Large Language Models](https://arxiv.org/abs/2410.07825)

*Zhipeng Chen, Kun Zhou, Liang Song, Wayne Xin Zhao, Bingning Wang, Weipeng Chen, Ji-Rong Wen*

**Main category:** cs.CL

**Keywords:** Multi-lingual Ability Transfer, Large Language Models, Language-Agnostic Weights, Neural Network, Low-Resource Languages

**Relevance Score:** 8

**TL;DR:** The paper presents a method for transferring multi-lingual abilities from large language models (LLMs) without needing additional training on low-resource languages.

**Read time:** 17 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing need for multilingual capability in LLMs, especially for low-resource languages, where training data may be scarce.

**Method:** The Multi-lingual Ability Extraction and Transfer (MAET) approach extracts language-agnostic ability-related weights from LLMs through two stages: extraction and transfer, utilizing neuron selection and weight manipulation.

**Key Contributions:** Introduction of the MAET framework for multilingual ability transfer, Effective extraction of ability-related weights from LLMs, Demonstrated performance improvements in mathematical and scientific tasks over baseline methods.

**Result:** Extensive experiments demonstrate that MAET effectively extracts and transfers language abilities, outperforming traditional training-based methods.

**Limitations:** The method may require further validation across a broader range of languages and tasks to establish generalizability.

**Future Work:** Exploration of additional task-specific enhancements and broader language applicability.

**Conclusion:** MAET offers a novel approach for enhancing multilingual capabilities in LLMs with improved efficiency and effectiveness, particularly for low-resource languages.

**Abstract:** Multi-lingual ability transfer has become increasingly important for the broad application of large language models (LLMs). Existing work highly relies on training with the multi-lingual ability-related data, which may be not available for low-resource languages. To solve it, we propose a Multi-lingual Ability Extraction and Transfer approach, named as MAET. Our key idea is to decompose and extract language-agnostic ability-related weights from LLMs, and transfer them across different languages by simple addition and subtraction operations without training. Specially, our MAET consists of the extraction and transfer stages. In the extraction stage, we firstly locate key neurons that are highly related to specific abilities, and then employ them to extract the transferable ability-specific weights. In the transfer stage, we further select the ability-related parameter tensors, and design the merging strategy based on the linguistic and ability specific weights, to build the multi-lingual ability-enhanced LLM. To demonstrate the effectiveness of our proposed approach, we conduct extensive experiments on mathematical and scientific tasks in both high-resource lingual and low-resource lingual scenarios. Experiment results have shown that MAET can effectively and efficiently extract and transfer the advanced abilities, and outperform training-based baseline methods. Our code and data are available at https://github.com/RUCAIBox/MAET.

</details>


### [68] [Adsorb-Agent: Autonomous Identification of Stable Adsorption Configurations via Large Language Model Agent](https://arxiv.org/abs/2410.16658)

*Janghoon Ock, Tirtha Vinchurkar, Yayati Jadhav, Amir Barati Farimani*

**Main category:** cs.CL

**Keywords:** Adsorption energy, Catalysis, Large Language Model, Machine Learning, Computational efficiency

**Relevance Score:** 5

**TL;DR:** Adsorb-Agent, an LLM agent, enhances the identification of stable adsorption configurations for optimal catalyst discovery by reducing computational costs and improving adsorption energy predictions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Determining adsorption energy is crucial in catalysis for screening optimal catalysts, but current methods are computationally intensive and may not identify global minima.

**Method:** The paper introduces Adsorb-Agent, an LLM that strategically explores configurations to find stable adsorption states while requiring fewer initial configurations than traditional methods.

**Key Contributions:** Introduction of Adsorb-Agent as an LLM for adsorption energy prediction, Demonstrated efficiency in identifying stable configurations, Improved accuracy in comparing adsorption energies across complex systems

**Result:** Adsorb-Agent identifies comparable adsorption energies for 83.7% of tested systems and achieves lower energies closer to the global minimum in 35% of cases, particularly excelling in complex systems.

**Limitations:** 

**Future Work:** Further improvements on Adsorb-Agent's exploration strategies and applications to more complex catalytic systems.

**Conclusion:** Adsorb-Agent can significantly accelerate catalyst discovery by improving the accuracy of adsorption energy predictions while decreasing computational demands.

**Abstract:** Adsorption energy is a key reactivity descriptor in catalysis, enabling efficient screening for optimal catalysts. However, determining adsorption energy typically requires evaluating numerous adsorbate-catalyst configurations. Current algorithmic approaches rely on exhaustive enumeration of adsorption sites and configurations, which makes the process computationally intensive and does not inherently guarantee the identification of the global minimum energy. In this work, we introduce Adsorb-Agent, a Large Language Model (LLM) agent designed to efficiently identify system-specific stable adsorption configurations corresponding to the global minimum adsorption energy. Adsorb-Agent leverages its built-in knowledge and emergent reasoning capabilities to strategically explore adsorption configurations likely to hold adsorption energy. By reducing the reliance on exhaustive sampling, it significantly decreases the number of initial configurations required while improving the accuracy of adsorption energy predictions. We evaluate Adsorb-Agent's performance across twenty representative systems encompassing a range of complexities. The Adsorb-Agent successfully identifies comparable adsorption energies for 83.7% of the systems and achieves lower energies, closer to the actual global minimum, for 35% of the systems, while requiring significantly fewer initial configurations than conventional methods. Its capability is particularly evident in complex systems, where it identifies lower adsorption energies for 46.7% of systems involving intermetallic surfaces and 66.7% of systems with large adsorbate molecules. These results demonstrate the potential of Adsorb-Agent to accelerate catalyst discovery by reducing computational costs and improving the reliability of adsorption energy predictions.

</details>


### [69] [KnowRA: Knowledge Retrieval Augmented Method for Document-level Relation Extraction with Comprehensive Reasoning Abilities](https://arxiv.org/abs/2501.00571)

*Chengcheng Mai, Yuxiang Wang, Ziyu Gong, Hanxiang Wang, Yihua Huang*

**Main category:** cs.CL

**Keywords:** relation extraction, document-level, knowledge retrieval, axis attention, machine learning

**Relevance Score:** 6

**TL;DR:** This paper introduces KnowRA, a method for document-level relation extraction (Doc-RE) that enhances reasoning capabilities by incorporating external knowledge and a novel axis attention mechanism.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Doc-RE methods focus on single reasoning abilities and neglect the utilization of external knowledge for complex reasoning in long documents.

**Method:** KnowRA uses a document graph for semantic encoding, integrates a co-reference resolution model, expands the graph into a document knowledge graph with external knowledge retrieval, and employs an axis attention mechanism for cross-sentence reasoning.

**Key Contributions:** Introduction of the KnowRA method for Doc-RE, Development of a document knowledge graph incorporating external knowledge, Implementation of an axis attention mechanism for reasoning across sentences.

**Result:** Extensive experiments on two datasets show that KnowRA outperforms state-of-the-art baselines in document-level relation extraction tasks.

**Limitations:** The effectiveness of the method may vary depending on the quality and relevance of the external knowledge retrieved.

**Future Work:** Further exploration of optimizing knowledge retrieval and reasoning paradigms for even more complex relation extraction tasks.

**Conclusion:** KnowRA successfully enhances document-level relation extraction capabilities by effectively integrating external knowledge and improving reasoning mechanisms.

**Abstract:** Document-level relation extraction (Doc-RE) aims to extract relations between entities across multiple sentences. Therefore, Doc-RE requires more comprehensive reasoning abilities like humans, involving complex cross-sentence interactions between entities, contexts, and external general knowledge, compared to the sentence-level RE. However, most existing Doc-RE methods focus on optimizing single reasoning ability, but lack the ability to utilize external knowledge for comprehensive reasoning on long documents. To solve these problems, a knowledge retrieval augmented method, named KnowRA, was proposed with comprehensive reasoning to autonomously determine whether to accept external knowledge to assist DocRE. Firstly, we constructed a document graph for semantic encoding and integrated the co-reference resolution model to augment the co-reference reasoning ability. Then, we expanded the document graph into a document knowledge graph by retrieving the external knowledge base for common-sense reasoning and a novel knowledge filtration method was presented to filter out irrelevant knowledge. Finally, we proposed the axis attention mechanism to build direct and indirect associations with intermediary entities for achieving cross-sentence logical reasoning. Extensive experiments conducted on two datasets verified the effectiveness of our method compared to the state-of-the-art baselines. Our code is available at https://anonymous.4open.science/r/KnowRA.

</details>


### [70] [Leveraging Conditional Mutual Information to Improve Large Language Model Fine-Tuning For Classification](https://arxiv.org/abs/2502.11258)

*Thanushon Sivakaran, En-Hui Yang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Conditional Mutual Information, Fine-tuning, Knowledge Distillation, GLUE Benchmark

**Relevance Score:** 9

**TL;DR:** This paper explores using Conditional Mutual Information (CMI) to enhance fine-tuning of large language models (LLMs) for classification tasks, demonstrating improved performance through both minimizing and maximizing CMI.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the underexplored potential of information theory, specifically Conditional Mutual Information (CMI), in enhancing the performance of large language models (LLMs).

**Method:** The paper adapts a CMI-constrained deep learning framework for applying CMI in LLM fine-tuning, focusing on minimizing CMI for model performance and maximizing CMI for knowledge distillation.

**Key Contributions:** Introduction of Conditional Mutual Information for LLM fine-tuning, Adaptation of a CMI-constrained framework for LLM tasks, Demonstration of superior GLUE task performance using CMI methods

**Result:** Minimizing CMI during LLM fine-tuning leads to superior performance on 6 out of 8 GLUE classification tasks compared to BERT, while maximizing CMI during knowledge distillation significantly improves performance on 6 out of 8 GLUE tasks compared to DistilBERT.

**Limitations:** 

**Future Work:** Exploring further applications of information theory principles in LLM development and investigating additional metrics for model performance optimization.

**Conclusion:** The findings highlight the adaptability of CMI for optimizing both standalone LLMs and student models, positioning it as a valuable framework for advancing LLM fine-tuning.

**Abstract:** Although large language models (LLMs) have demonstrated remarkable capabilities in recent years, the potential of information theory (IT) to enhance LLM development remains underexplored. This paper introduces the information theoretic principle of Conditional Mutual Information (CMI) to LLM fine-tuning for classification tasks, exploring its promise in two main ways: minimizing CMI to improve a model's standalone performance and maximizing CMI to enhance knowledge distillation (KD) for more capable student models. To apply CMI in LLM fine-tuning, we adapt the recently proposed CMI-constrained deep learning framework, which was initially developed for image classification, with some modification. By minimizing CMI during LLM fine-tuning, we achieve superior performance gains on 6 of 8 GLUE classification tasks compared to BERT. Additionally, maximizing CMI during the KD process results in significant performance improvements in 6 of 8 GLUE classification tasks compared to DistilBERT. These findings demonstrate CMI's adaptability for optimizing both standalone LLMs and student models, showcasing its potential as a robust framework for advancing LLM fine-tuning. Our work bridges the gap between information theory and LLM development, offering new insights for building high-performing language models.

</details>


### [71] [Mapping Trustworthiness in Large Language Models: A Bibliometric Analysis Bridging Theory to Practice](https://arxiv.org/abs/2503.04785)

*José Siqueira de Cerqueira, Kai-Kristian Kemell, Muhammad Waseem, Rebekah Rousi, Nannan Xi, Juho Hamari, Pekka Abrahamsson*

**Main category:** cs.CL

**Keywords:** Large Language Models, Trustworthiness, Bibliometric Analysis, Trust-enhancing Techniques, Ethical AI

**Relevance Score:** 9

**TL;DR:** This study analyzes the trustworthiness of Large Language Models (LLMs) through a bibliometric mapping of 2,006 publications, revealing key trends and proposing a structured mapping of trust-enhancing techniques for responsible LLM deployment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding the trustworthiness of LLMs and the lack of consensus on how to implement it in practice.

**Method:** Conducting a bibliometric mapping analysis of publications, including co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, alongside a systematic review of 68 core papers.

**Key Contributions:** Bibliometric mapping analysis of LLM trustworthiness literature, Identification of key trends and influential authors, Proposal of 20 trust-enhancing techniques for practical implementation

**Result:** Identified key research trends, influential authors, and existing organizational trust frameworks related to LLMs, proposing 20 trust-enhancing techniques throughout the LLM lifecycle.

**Limitations:** The study reveals a gap in translating theoretical trust principles into actionable strategies for LLM development.

**Future Work:** Further research is needed to operationalize the proposed trust-enhancing techniques effectively in various LLM applications.

**Conclusion:** The study highlights the importance of translating trust principles into development strategies to improve LLM transparency and accountability in real-world applications.

**Abstract:** The rapid proliferation of Large Language Models (LLMs) has raised pressing concerns regarding their trustworthiness, spanning issues of reliability, transparency, fairness, and ethical alignment. Despite the increasing adoption of LLMs across various domains, there remains a lack of consensus on how to operationalize trustworthiness in practice. This study bridges the gap between theoretical discussions and implementation by conducting a bibliometric mapping analysis of 2,006 publications from 2019 to 2025. Through co-authorship networks, keyword co-occurrence analysis, and thematic evolution tracking, we identify key research trends, influential authors, and prevailing definitions of LLM trustworthiness. Additionally, a systematic review of 68 core papers is conducted to examine conceptualizations of trust and their practical implications. Our findings reveal that trustworthiness in LLMs is often framed through existing organizational trust frameworks, emphasizing dimensions such as ability, benevolence, and integrity. However, a significant gap exists in translating these principles into concrete development strategies. To address this, we propose a structured mapping of 20 trust-enhancing techniques across the LLM lifecycle, including retrieval-augmented generation (RAG), explainability techniques, and post-training audits. By synthesizing bibliometric insights with practical strategies, this study contributes towards fostering more transparent, accountable, and ethically aligned LLMs, ensuring their responsible deployment in real-world applications.

</details>


### [72] [JuDGE: Benchmarking Judgment Document Generation for Chinese Legal System](https://arxiv.org/abs/2503.14258)

*Weihang Su, Baoqing Yue, Qingyao Ai, Yiran Hu, Jiaqi Li, Changyue Wang, Kaiyuan Zhang, Yueyue Wu, Yiqun Liu*

**Main category:** cs.CL

**Keywords:** Judgment Document Generation, Legal NLP, Evaluation Benchmark

**Relevance Score:** 4

**TL;DR:** JuDGE is a benchmark for evaluating judgment document generation in the Chinese legal system.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate the performance of judgment document generation from factual case descriptions in the legal domain.

**Method:** A comprehensive dataset of factual descriptions and corresponding judgment documents was created, alongside an automated evaluation framework. Various baseline approaches, including in-context learning and RAG, were evaluated using general and legal-domain LLMs.

**Key Contributions:** Creation of the JuDGE benchmark dataset for legal judgment generation, Development of an automated evaluation framework, Assessment of baseline approaches in generating legal documents

**Result:** RAG approaches showed improved performance in generating legal judgment documents, but significant room for improvement remains.

**Limitations:** The study is limited to the Chinese legal system and may not generalize to other jurisdictions.

**Future Work:** Further improvements in model accuracy and generalization to other legal systems are needed.

**Conclusion:** While the benchmark and evaluation methods are established, further research is needed to enhance the quality of generated documents.

**Abstract:** This paper introduces JuDGE (Judgment Document Generation Evaluation), a novel benchmark for evaluating the performance of judgment document generation in the Chinese legal system. We define the task as generating a complete legal judgment document from the given factual description of the case. To facilitate this benchmark, we construct a comprehensive dataset consisting of factual descriptions from real legal cases, paired with their corresponding full judgment documents, which serve as the ground truth for evaluating the quality of generated documents. This dataset is further augmented by two external legal corpora that provide additional legal knowledge for the task: one comprising statutes and regulations, and the other consisting of a large collection of past judgment documents. In collaboration with legal professionals, we establish a comprehensive automated evaluation framework to assess the quality of generated judgment documents across various dimensions. We evaluate various baseline approaches, including few-shot in-context learning, fine-tuning, and a multi-source retrieval-augmented generation (RAG) approach, using both general and legal-domain LLMs. The experimental results demonstrate that, while RAG approaches can effectively improve performance in this task, there is still substantial room for further improvement. All the codes and datasets are available at: https://github.com/oneal2000/JuDGE.

</details>


### [73] [Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)

*Ivo Petrov, Jasper Dekoninck, Lyuben Baltadzhiev, Maria Drencheva, Kristian Minchev, Mislav Balunović, Nikola Jovanović, Martin Vechev*

**Main category:** cs.CL

**Keywords:** Large Language Models, Mathematical Reasoning, USAMO, Failure Modes, Proof Generation

**Relevance Score:** 6

**TL;DR:** Evaluation of LLMs on rigorous mathematical reasoning reveals significant inadequacies in current models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To comprehensively evaluate full-solution reasoning capabilities of LLMs on challenging mathematical problems beyond just final numerical answers.

**Method:** Expert human annotators evaluated several state-of-the-art reasoning models on six problems from the 2025 USAMO shortly after their release.

**Key Contributions:** First comprehensive evaluation of LLMs on full-solution mathematical reasoning, Identification of common failure modes in LLM performance, Discussion of optimization artifacts in model training

**Result:** Only Gemini-2.5-Pro achieved a non-trivial score of 25%, while all other models scored less than 5%.

**Limitations:** Study limited to six specific USAMO problems and may not generalize to all mathematical reasoning tasks.

**Future Work:** Future research should focus on enhancing reasoning and proof generation capabilities of LLMs for better performance in mathematical tasks.

**Conclusion:** Current LLMs are inadequate for rigorous mathematical reasoning tasks, indicating the need for improvements in reasoning and proof generation.

**Abstract:** Recent math benchmarks for large language models (LLMs) such as MathArena indicate that state-of-the-art reasoning models achieve impressive performance on mathematical competitions like AIME, with the leading model, Gemini-2.5-Pro, achieving scores comparable to top human competitors. However, these benchmarks evaluate models solely based on final numerical answers, neglecting rigorous reasoning and proof generation which are essential for real-world mathematical tasks. To address this, we introduce the first comprehensive evaluation of full-solution reasoning for challenging mathematical problems. Using expert human annotators, we evaluated several state-of-the-art reasoning models on the six problems from the 2025 USAMO within hours of their release. Our results reveal that all tested models struggled significantly: only Gemini-2.5-Pro achieves a non-trivial score of 25%, while all other models achieve less than 5%. Through detailed analysis of reasoning traces, we identify the most common failure modes and find several unwanted artifacts arising from the optimization strategies employed during model training. Overall, our results suggest that current LLMs are inadequate for rigorous mathematical reasoning tasks, highlighting the need for substantial improvements in reasoning and proof generation capabilities.

</details>


### [74] [VisualPuzzles: Decoupling Multimodal Reasoning Evaluation from Domain Knowledge](https://arxiv.org/abs/2504.10342)

*Yueqi Song, Tianyue Ou, Yibo Kong, Zecheng Li, Graham Neubig, Xiang Yue*

**Main category:** cs.CL

**Keywords:** Visual reasoning, Benchmarking, Machine learning, Multimodal, Reasoning capabilities

**Relevance Score:** 7

**TL;DR:** Introducing VisualPuzzles, a benchmark for evaluating visual reasoning that minimizes reliance on domain-specific knowledge.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks mix reasoning with specialized knowledge, obscuring genuine reasoning evaluation in non-expert contexts.

**Method:** VisualPuzzles includes diverse questions across categories like algorithmic and analogical reasoning, largely sourced from logical reasoning questions of the Chinese Civil Service Examination.

**Key Contributions:** Introduction of VisualPuzzles for visual reasoning evaluation, Demonstration of SOTA models lagging behind human performance, Insights on model reasoning patterns relative to knowledge-intensive benchmarks

**Result:** Experiments reveal that VisualPuzzles demands less domain knowledge and fosters complex reasoning, showing SOTA models underperform compared to humans.

**Limitations:** The dependency on specific question sources and potential biases in question design.

**Future Work:** Exploring further enhancements in reasoning capabilities and cross-benchmark comparisons.

**Conclusion:** VisualPuzzles is an effective tool to evaluate reasoning beyond mere factual recall, highlighting distinctive reasoning patterns of models compared to knowledge-heavy benchmarks.

**Abstract:** Current multimodal benchmarks often conflate reasoning with domain-specific knowledge, making it difficult to isolate and evaluate general reasoning abilities in non-expert settings. To address this, we introduce VisualPuzzles, a benchmark that targets visual reasoning while deliberately minimizing reliance on specialized knowledge. VisualPuzzles consists of diverse questions spanning five categories: algorithmic, analogical, deductive, inductive, and spatial reasoning. One major source of our questions is manually translated logical reasoning questions from the Chinese Civil Service Examination. Experiments show that VisualPuzzles requires significantly less intensive domain-specific knowledge and more complex reasoning compared to benchmarks like MMMU, enabling us to better evaluate genuine multimodal reasoning. Evaluations show that state-of-the-art multimodal large language models consistently lag behind human performance on VisualPuzzles, and that strong performance on knowledge-intensive benchmarks does not necessarily translate to success on reasoning-focused, knowledge-light tasks. Additionally, reasoning enhancements such as scaling up inference compute (with "thinking" modes) yield inconsistent gains across models and task types, and we observe no clear correlation between model size and performance. We also found that models exhibit different reasoning and answering patterns on VisualPuzzles compared to benchmarks with heavier emphasis on knowledge. VisualPuzzles offers a clearer lens through which to evaluate reasoning capabilities beyond factual recall and domain knowledge.

</details>


### [75] [Learning Optimal Prompt Ensemble for Multi-source Visual Prompt Transfer](https://arxiv.org/abs/2504.12311)

*Enming Zhang, Liwen Cao, Yanru Wu, Zijie Zhao, Guan Wang, Yang Li*

**Main category:** cs.CL

**Keywords:** prompt tuning, multi-source prompt transfer, information-theoretic metric, gradient alignment, foundation models

**Relevance Score:** 8

**TL;DR:** HGPrompt is a framework for multi-source prompt transfer that optimizes transferability and stability to enhance generalization while mitigating representation collapse.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to improve the adaptation of foundation models to downstream tasks by combining multiple source prompts without losing performance due to mutual interference.

**Method:** The authors propose HGPrompt, which uses an information-theoretic metric for evaluating transferability and a novel Gradient Alignment Regularization to balance the gradient conflicts among prompts during knowledge transfer.

**Key Contributions:** Introduction of HGPrompt framework for multi-source prompt transfer., Development of an information-theoretic metric for assessing prompt transferability., Implementation of Gradient Alignment Regularization to reduce interference among prompts.

**Result:** HGPrompt achieves state-of-the-art performance on the VTAB benchmark, successfully demonstrating its effectiveness in multi-source prompt transfer.

**Limitations:** 

**Future Work:** Future research could explore further optimizations in prompt tuning and extending the framework to additional downstream tasks.

**Conclusion:** The framework effectively enhances the generalization capabilities of foundation models by optimizing prompt aggregation while minimizing representation collapse.

**Abstract:** Prompt tuning has emerged as a lightweight adaptation strategy for adapting foundation models to downstream tasks, particularly in resource-constrained systems. As pre-trained prompts have become valuable intellectual assets, combining multiple source prompts offers a promising approach to enhance generalization to new tasks by leveraging complementary knowledge from diverse sources. However, naive aggregation of these prompts often leads to representation collapse due to mutual interference, undermining their collective potential. To address these challenges, we propose HGPrompt, an adaptive framework for multi-source prompt transfer that learns optimal ensemble weights by jointly optimizing dual objectives: transferability and stability. Specifically, we first introduce an information-theoretic metric to evaluate the transferability of prompt-induced features on the target task, capturing the intrinsic alignment between the feature representations. Additionally, we propose a novel Gradient Alignment Regularization to mitigate gradient conflicts among prompts, enabling stable and coherent knowledge transfer from multiple sources while suppressing interference. Extensive experiments on the large-scale VTAB benchmark demonstrate that HGPrompt achieves state-of-the-art performance, validating its effectiveness in multi-source prompt transfer.

</details>
