# 2025-07-16

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 57]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)

*Samuel Rhys Cox*

**Main category:** cs.HC

**Keywords:** self-disclosure, conversational user interfaces, theory of mind, social cues, user engagement

**Relevance Score:** 7

**TL;DR:** This workshop paper explores the role of self-disclosure in conversational user interfaces (CUIs) and suggests that making CUIs' reasoning more transparent may encourage users to share more.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the challenges of self-disclosure in interactions with conversational user interfaces and how social cues influence these interactions.

**Method:** The authors discuss various social cues related to self-disclosure in CUIs and propose that expressing uncertainty or clarifying a CUI's reasoning can facilitate user self-disclosure.

**Key Contributions:**

	1. Analyzes the impact of social cues on self-disclosure in CUIs.
	2. Proposes methods for exposing CUI reasoning to users.
	3. Highlights the relationship between user self-disclosure and conversational transparency.

**Result:** The findings suggest that enhancing transparency regarding a CUI's reasoning can lead to greater user comfort in self-disclosure.

**Limitations:** 

**Conclusion:** By improving the transparency of CUIs' 'theory of mind', it may be possible to foster deeper user engagement and communication.

**Abstract:** Self-disclosure is important to help us feel better, yet is often difficult. This difficulty can arise from how we think people are going to react to our self-disclosure. In this workshop paper, we briefly discuss self-disclosure to conversational user interfaces (CUIs) in relation to various social cues. We then, discuss how expressions of uncertainty or representation of a CUI's reasoning could help encourage self-disclosure, by making a CUI's intended "theory of mind" more transparent to users.

</details>


### [2] [React to This (RTT): A Nonverbal Turing Test for Embodied AI](https://arxiv.org/abs/2507.10812)

*Chuxuan Zhang, Yasaman Etesam, Angelica Lim*

**Main category:** cs.HC

**Keywords:** Embodied AI, Interaction Awareness, Believability, Nonverbal Behavior, React to This (RTT) Test

**Relevance Score:** 6

**TL;DR:** This paper introduces the React to This (RTT) test to evaluate embodied AI agents' interaction awareness and believability through nonverbal behavior.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate how embodied AI agents can react in interactive scenarios, particularly under challenging conditions posed by human users.

**Method:** The paper proposes the RTT test, which assesses nonverbal behaviors of AI agents during interactions, building on the principles of Turing's tests.

**Key Contributions:**

	1. Introduction of the React to This (RTT) test for assessing nonverbal behaviors in AI
	2. Expansion of Turing's concepts to focus on interactive reactions of AI agents
	3. Initial experimental results showcasing the viability of the RTT test

**Result:** Initial experiments demonstrate the RTT test's potential to capture the believability and interaction awareness of embodied AI agents.

**Limitations:** The study presents preliminary findings; further research is required to validate results across diverse scenarios.

**Conclusion:** The RTT test is a promising framework for evaluating AI agents in contexts where reactive nonverbal communication is crucial for user interaction.

**Abstract:** We propose an approach to test embodied AI agents for interaction awareness and believability, particularly in scenarios where humans push them to their limits. Turing introduced the Imitation Game as a way to explore the question: "Can machines think?" The Total Turing Test later expanded this concept beyond purely verbal communication, incorporating perceptual and physical interaction. Building on this, we propose a new guiding question: "Can machines react?" and introduce the React to This (RTT) test for nonverbal behaviors, presenting results from an initial experiment.

</details>


### [3] [Static or Temporal? Semantic Scene Simplification to Aid Wayfinding in Immersive Simulations of Bionic Vision](https://arxiv.org/abs/2507.10813)

*Justin M. Kasowski, Apurv Varshney, Michael Beyeler*

**Main category:** cs.HC

**Keywords:** visual neuroprostheses, semantic preprocessing, immersive virtual reality, user experience, wayfinding

**Relevance Score:** 6

**TL;DR:** This paper investigates the use of two semantic preprocessing techniques in virtual reality for visual neuroprostheses, aimed at improving user experience and task performance under bandwidth constraints.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve scene understanding for users of visual neuroprostheses under significant resolution and bandwidth constraints while ensuring that the information presented does not overwhelm users in complex environments.

**Method:** The study compares two approaches, SemanticEdges and SemanticRaster, using a biologically grounded simulation of prosthetic vision in a wayfinding task across three conditions: Control, SemanticEdges, and SemanticRaster.

**Key Contributions:**

	1. Comparison of SemanticEdges and SemanticRaster in virtual reality for enhancing prosthetic vision.
	2. Demonstration of distinct trade-offs in improving user performance and experience in cluttered environments.
	3. Insights into designing low-bandwidth interfaces in XR contexts.

**Result:** Both semantic strategies improved performance and user experience compared to the baseline, with SemanticEdges enhancing success odds and SemanticRaster increasing the likelihood of collision-free completions.

**Limitations:** The study is constrained to a simulated environment; real-world applicability remains to be tested.

**Conclusion:** The findings highlight the importance of adaptive semantic preprocessing in prosthetic vision and can guide the design of low-bandwidth visual interfaces that manage information density and perceptual clarity.

**Abstract:** Visual neuroprostheses (bionic eye) aim to restore a rudimentary form of vision by translating camera input into patterns of electrical stimulation. To improve scene understanding under extreme resolution and bandwidth constraints, prior work has explored computer vision techniques such as semantic segmentation and depth estimation. However, presenting all task-relevant information simultaneously can overwhelm users in cluttered environments. We compare two complementary approaches to semantic preprocessing in immersive virtual reality: SemanticEdges, which highlights all relevant objects at once, and SemanticRaster, which staggers object categories over time to reduce visual clutter. Using a biologically grounded simulation of prosthetic vision, 18 sighted participants performed a wayfinding task in a dynamic urban environment across three conditions: edge-based baseline (Control), SemanticEdges, and SemanticRaster. Both semantic strategies improved performance and user experience relative to the baseline, with each offering distinct trade-offs: SemanticEdges increased the odds of success, while SemanticRaster boosted the likelihood of collision-free completions. These findings underscore the value of adaptive semantic preprocessing for prosthetic vision and, more broadly, may inform the design of low-bandwidth visual interfaces in XR that must balance information density, task relevance, and perceptual clarity.

</details>


### [4] [AROMA: Mixed-Initiative AI Assistance for Non-Visual Cooking by Grounding Multi-modal Information Between Reality and Videos](https://arxiv.org/abs/2507.10963)

*Zheng Ning, Leyang Li, Daniel Killough, JooYoung Seo, Patrick Carrington, Yapeng Tian, Yuhang Zhao, Franklin Mingzhe Li, Toby Jia-Jun Li*

**Main category:** cs.HC

**Keywords:** AROMA, Blind and Low-Vision, Cooking assistance, AI systems, Mixed-initiative

**Relevance Score:** 9

**TL;DR:** AROMA is an AI system designed to assist blind or low-vision individuals in cooking by integrating non-visual cues with video recipe content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To make cooking videos more accessible to blind or low-vision individuals, who face challenges in following audiovisual instructions.

**Method:** AROMA employs a mixed-initiative approach, using a wearable camera and context-aware responses to integrate non-visual cues into the cooking process.

**Key Contributions:**

	1. Introduction of AROMA, an AI system for blind and low-vision users
	2. Mixed-initiative approach combining user requests and proactive monitoring
	3. Insights from user evaluations that inform the design of interactive AI systems for BLV individuals

**Result:** The system was evaluated with eight participants, providing insights into its effectiveness in assisting BLV individuals in performing daily cooking tasks.

**Limitations:** Limited to a specific user group (BLV participants) and focused on cooking tasks.

**Conclusion:** The collaborative design of AROMA helps BLV users interpret their cooking environment and enhances their ability to follow video recipes.

**Abstract:** Videos offer rich audiovisual information that can support people in performing activities of daily living (ADLs), but they remain largely inaccessible to blind or low-vision (BLV) individuals. In cooking, BLV people often rely on non-visual cues, such as touch, taste, and smell, to navigate their environment, making it difficult to follow the predominantly audiovisual instructions found in video recipes. To address this problem, we introduce AROMA, an AI system that provides timely responses to the user based on real-time, context-aware assistance by integrating non-visual cues perceived by the user, a wearable camera feed, and video recipe content. AROMA uses a mixed-initiative approach: it responds to user requests while also proactively monitoring the video stream to offer timely alerts and guidance. This collaborative design leverages the complementary strengths of the user and AI system to align the physical environment with the video recipe, helping the user interpret their current cooking state and make sense of the steps. We evaluated AROMA through a study with eight BLV participants and offered insights for designing interactive AI systems to support BLV individuals in performing ADLs.

</details>


### [5] [Self++: Merging Human and AI for Co-Determined XR Living in the Metaverse](https://arxiv.org/abs/2507.10967)

*Thammathip Piumsomboon*

**Main category:** cs.HC

**Keywords:** Self-Determination Theory, Metaverse, Human-AI Collaboration, Extended Reality, Ethical Safeguards

**Relevance Score:** 6

**TL;DR:** This paper presents Self++, a framework for co-determined living in the Metaverse that focuses on human flourishing through dynamic human-AI collaboration in XR.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create a human-centred, AI-enhanced Metaverse that fosters human potential rather than diminishing it.

**Method:** The framework is grounded in Self-Determination Theory and emphasizes user empowerment and meaningful social connections in XR.

**Key Contributions:**

	1. Introduction of the Self++ framework.
	2. Emphasis on human flourishing through AI collaboration.
	3. Proposals for ethical safeguards in XR.

**Result:** Self++ proposes research directions for AI autonomy, social connection in XR, and ethical safeguards.

**Limitations:** 

**Conclusion:** The roadmap provided by Self++ aims to enhance human competency and address cognitive biases in technology use.

**Abstract:** This position paper introduces Self++, a novel nine-level framework for co-determined living in the Metaverse, grounded in Self-Determination Theory. Self++ prioritises human flourishing by progressively cultivating competence, autonomy, and relatedness through dynamic human-AI collaboration in extended reality (XR). Unlike technologically deterministic approaches, Self++ emphasises user empowerment by enhancing competency, mitigating cognitive biases and leveraging XR's immersive capabilities. Key research directions proposed include exploring the boundaries of user-defined AI autonomy, designing for meaningful social connection in XR, and establishing proactive ethical safeguards. Ultimately, Self++ offers a roadmap for creating a human-centred, AI-enhanced Metaverse where technology amplifies, rather than diminishes, human potential.

</details>


### [6] [Terms and Conditions (Do Not) Apply: Understanding Exploitation Disparities in Design of Mobile-Based Financial Services](https://arxiv.org/abs/2507.10970)

*Lindah Kotut*

**Main category:** cs.HC

**Keywords:** mobile financial services, user experience, HCI, financial exploitation, design guidelines

**Relevance Score:** 4

**TL;DR:** This paper explores user experiences with mobile-based financial services, analyzing both their benefits and risks, and proposes guidelines for design improvements that empower users.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study investigates the impact of mobile financial services on traditionally unbanked populations, emphasizing the inherent challenges and risks associated with their design and usage.

**Method:** User interviews were conducted to gather qualitative insights on users' experiences and the design patterns of mobile-based financial transaction systems.

**Key Contributions:**

	1. Detailed user experiences of mobile financial services
	2. Identification of risks and unethical practices due to design patterns
	3. Recommendations for user-centered design improvements

**Result:** Findings reveal significant user exploitation and risks due to unethical financial practices perpetuated by design patterns, alongside suggested strategies for mitigation and recovery from harms.

**Limitations:** 

**Conclusion:** The paper concludes with recommendations for design guidelines that enhance user trust and understanding, ultimately aiming to protect users from the financial risks associated with mobile technologies.

**Abstract:** Mobile-based financial services have made it possible for the traditionally unbanked to access infrastructure that have been routinely unattainable. Researchers have explored how these systems have made for safer environments to send and receive money and have expanded financial opportunities such as increased borrowing. With this expansion, challenges such as detrimental interest rates, lack of access to policy documents, and inadequate user protective guardrails emerge, amplifying the risks due to technology-aided unethical financial practices that are aided by design patterns. Supported by user interviews, we detail user experiences of mobile-based financial transactions and explore the foundations and guidelines that undergird the financial service provisions: highlighting both affordances and harms enabled in the design of such systems. We discuss the findings by highlighting financial exploitation disparities, deliberating strategies for mitigation of risks and enabling recovery from harms caused by the technology use. We then recommend guidelines for empowering design approaches that support users' mechanisms of trust, their understanding of technological processes, and determination of risks.

</details>


### [7] [An Exploratory Study on AI-driven Visualisation Techniques on Decision Making in Extended Reality](https://arxiv.org/abs/2507.10981)

*Ze Dong, Binyang Han, Jingjing Zhang, Ruoyu Wen, Barrett Ens, Adrian Clark, Tham Piumsomboon*

**Main category:** cs.HC

**Keywords:** Extended Reality, Artificial Intelligence, User Decision-Making, Visualisation Techniques, User Autonomy

**Relevance Score:** 8

**TL;DR:** The paper explores how AI-driven visualization techniques in XR affect user decision-making and preferences, highlighting the importance of user autonomy and AI transparency.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To investigate the impact of AI-driven visualisation techniques on user decision-making in XR environments.

**Method:** An exploratory study using a pre-recorded 360-degree video of a supermarket, overlaid with four AI-driven visualisation techniques during semi-structured interviews.

**Key Contributions:**

	1. Analysis of four AI-driven visualisation techniques: Inform, Nudge, Recommend, Instruct
	2. The role of user autonomy and trust in AI-driven decision-making
	3. Design recommendations for effective visualisation in XR applications

**Result:** The study found that different visualisation techniques affect user preferences and decision-making, emphasizing user autonomy and the need for AI transparency.

**Limitations:** Limited to a single XR application and context; further studies needed to generalize findings.

**Conclusion:** Maintaining user autonomy and enhancing AI transparency are crucial for effective decision-making in XR environments.

**Abstract:** The integration of extended reality (XR) with artificial intelligence (AI) introduces a new paradigm for user interaction, enabling AI to perceive user intent, stimulate the senses, and influence decision-making. We explored the impact of four AI-driven visualisation techniques -- `Inform,' `Nudge,' `Recommend,' and `Instruct' -- on user decision-making in XR using the Meta Quest Pro. To test these techniques, we used a pre-recorded 360-degree video of a supermarket, overlaying each technique through a virtual interface. We aimed to investigate how these different visualisation techniques with different levels of user autonomy impact preferences and decision-making. An exploratory study with semi-structured interviews provided feedback and design recommendations. Our findings emphasise the importance of maintaining user autonomy, enhancing AI transparency to build trust, and considering context in visualisation design.

</details>


### [8] [Role-Playing LLM-Based Multi-Agent Support Framework for Detecting and Addressing Family Communication Bias](https://arxiv.org/abs/2507.11210)

*Rushia Harada, Yuken Kimura, Keito Inoshita*

**Main category:** cs.HC

**Keywords:** family dynamics, ideal parent bias, suppressed emotion, LLM, dialogue support

**Relevance Score:** 8

**TL;DR:** The study explores an LLM-based framework designed to support family communication by detecting suppressed emotions and ideal parent bias, aiming to enhance emotional expression and autonomy in children.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Conventional metrics often overlook the subtle psychological dynamics that affect well-being in family settings, particularly the influence of ideal parent bias on children's emotional expression.

**Method:** A Role-Playing LLM-based multi-agent dialogue support framework was constructed using a Japanese parent-child dialogue corpus, which analyzes dialogue, detects suppressed emotion, and generates comprehensive feedback.

**Key Contributions:**

	1. Developed a multi-agent framework for analyzing family dialogues
	2. Established a corpus for studying ideal parent bias and suppressed emotion
	3. Demonstrated the ability to generate actionable feedback that improves emotional communication

**Result:** The system shows moderate accuracy in detecting suppressed emotions and produces empathetic and practical feedback, leading to improved emotional expression and mutual understanding in simulated follow-up dialogues.

**Limitations:** 

**Conclusion:** The framework demonstrates potential for enhancing family communication and fostering emotional safety, with implications for positive transformation in interpersonal relationships.

**Abstract:** Well-being in family settings involves subtle psychological dynamics that conventional metrics often overlook. In particular, unconscious parental expectations, termed ideal parent bias, can suppress children's emotional expression and autonomy. This suppression, referred to as suppressed emotion, often stems from well-meaning but value-driven communication, which is difficult to detect or address from outside the family. Focusing on these latent dynamics, this study explores Large Language Model (LLM)-based support for psychologically safe family communication. We constructed a Japanese parent-child dialogue corpus of 30 scenarios, each annotated with metadata on ideal parent bias and suppressed emotion. Based on this corpus, we developed a Role-Playing LLM-based multi-agent dialogue support framework that analyzes dialogue and generates feedback. Specialized agents detect suppressed emotion, describe implicit ideal parent bias in parental speech, and infer contextual attributes such as the child's age and background. A meta-agent compiles these outputs into a structured report, which is then passed to five selected expert agents. These agents collaboratively generate empathetic and actionable feedback through a structured four-step discussion process. Experiments show that the system can detect categories of suppressed emotion with moderate accuracy and produce feedback rated highly in empathy and practicality. Moreover, simulated follow-up dialogues incorporating this feedback exhibited signs of improved emotional expression and mutual understanding, suggesting the framework's potential in supporting positive transformation in family interactions.

</details>


### [9] [REVA: Supporting LLM-Generated Programming Feedback Validation at Scale Through User Attention-based Adaptation](https://arxiv.org/abs/2507.11470)

*Xiaohang Tang, Sam Wong, Zicheng He, Yalong Yang, Yan Chen*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Educational feedback, Cognitive context shifts

**Relevance Score:** 8

**TL;DR:** REVA is a human-AI system that streamlines the instructor review of AI-generated programming feedback by reducing cognitive shifts and refining feedback based on instructor input.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the quality of AI-generated feedback in programming education and make the review process more efficient for instructors.

**Method:** A within-subjects lab study was conducted with 12 participants to evaluate REVA's effectiveness in improving the feedback review process.

**Key Contributions:**

	1. Novel methodology for sequencing submissions to reduce cognitive context shifts.
	2. Adaptive learning from instructor attention during feedback review.
	3. Evaluation of effectiveness through empirical study.

**Result:** The study showed that REVA effectively improves feedback quality and reduces cognitive context shifts during the review process for instructors.

**Limitations:** Limited sample size of 12 participants may affect the generalizability of the results.

**Conclusion:** REVA demonstrates significant potential for enhancing human-AI collaboration in educational contexts by adaptively learning from instructor feedback.

**Abstract:** This paper introduces REVA, a human-AI system that expedites instructor review of voluminous AI-generated programming feedback by sequencing submissions to minimize cognitive context shifts and propagating instructor-driven revisions across semantically similar instances. REVA introduces a novel approach to human-AI collaboration in educational feedback by adaptively learning from instructors' attention in the review and revision process to continuously improve the feedback validation process. REVA's usefulness and effectiveness in improving feedback quality and the overall feedback review process were evaluated through a within-subjects lab study with 12 participants.

</details>


### [10] [Towards Creating Infrastructures for Values and Ethics Work in the Production of Software Technologies](https://arxiv.org/abs/2507.11490)

*Richmond Y. Wong*

**Main category:** cs.HC

**Keywords:** human-computer interaction, values and ethics, infrastructures, design processes, social systems

**Relevance Score:** 9

**TL;DR:** The paper discusses the need to create infrastructures supporting values and ethics in HCI design rather than just tools, considering the broader political and social contexts.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of current HCI approaches that primarily focus on tools for integrating social values in design, highlighting the need for a broader approach that includes infrastructures.

**Method:** The authors draw on science and technology studies as well as media studies to conceptualize infrastructures and their relevance to HCI values and ethics work.

**Key Contributions:**

	1. Proposes an infrastructure-focused approach to values and ethics in HCI design
	2. Highlights the political and social contexts influencing design actions
	3. Offers new tactics for HCI researchers and practitioners

**Result:** The paper outlines new insights and tactics for HCI researchers and designers, advocating for the redesign of systems that incorporate values and ethics in tech development.

**Limitations:** 

**Conclusion:** By shifting from tools to infrastructures, HCI can better integrate social values and ethics in the design process, potentially transforming design practices.

**Abstract:** Recognizing how technical systems can embody social values or cause harms, human-computer interaction (HCI) research often approaches addressing values and ethics in design by creating tools to help tech workers integrate social values into the design of products. While useful, these approaches usually do not consider the politics embedded in the broader processes, organizations, social systems, and governance structures that affect the types of actions that tech workers can take to address values and ethics. This paper argues that creating infrastructures to support values and ethics work, rather than tools, is an approach that takes these broader processes into account and opens them up for (re)design. Drawing on prior research conceptualizing infrastructures from science \& technology studies and media studies, this paper outlines conceptual insights from infrastructures studies that open up new tactics for HCI researchers and designers seeking to support values and ethics in design.

</details>


### [11] [IdeaSynth: Iterative Research Idea Development Through Evolving and Composing Idea Facets with Literature-Grounded Feedback](https://arxiv.org/abs/2410.04025)

*Kevin Pu, K. J. Kevin Feng, Tovi Grossman, Tom Hope, Bhavana Dalvi Mishra, Matt Latzke, Jonathan Bragg, Joseph Chee Chang, Pao Siangliulue*

**Main category:** cs.HC

**Keywords:** Human-Computer Interaction, Machine Learning, Research Ideation, LLM, Idea Development

**Relevance Score:** 9

**TL;DR:** IdeaSynth is a system using LLMs to assist in the iterative refinement of research ideas, enhancing idea generation and specification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** There is a lack of tools that support the iterative specification, refinement, and evaluation needed for developing initial research ideas, as current tools focus mainly on broad idea generation.

**Method:** IdeaSynth uses large language models (LLMs) to provide feedback and facilitate exploration of research ideas, representing various facets of ideas as nodes on a canvas for iterative refinement.

**Key Contributions:**

	1. Introduction of IdeaSynth for literature-grounded feedback in research ideation.
	2. Utilization of LLMs for real-time assistance in idea refinement.
	3. Demonstrated effectiveness in enhancing research idea exploration and development.

**Result:** Participants using IdeaSynth explored more alternative ideas and added more details to initial concepts compared to a baseline; it was effectively used for real-world research projects across different ideation stages.

**Limitations:** 

**Conclusion:** IdeaSynth shows potential to be incorporated into researchers' workflows for ideation and refinement of research projects.

**Abstract:** Research ideation involves broad exploring and deep refining ideas. Both require deep engagement with literature. Existing tools focus primarily on idea broad generation, yet offer little support for iterative specification, refinement, and evaluation needed to further develop initial ideas. To bridge this gap, we introduce IdeaSynth, a research idea development system that uses LLMs to provide literature-grounded feedback for articulating research problems, solutions, evaluations, and contributions. IdeaSynth represents these idea facets as nodes on a canvas, and allow researchers to iteratively refine them by creating and exploring variations and composing them. Our lab study (N=20) showed that participants, while using IdeaSynth, explored more alternative ideas and expanded initial ideas with more details compared to a strong LLM-based baseline. Our deployment study (N=7) demonstrated that participants effectively used IdeaSynth for real-world research projects at various ideation stages from developing initial ideas to revising framings of mature manuscripts, highlighting the possibilities to adopt IdeaSynth in researcher's workflows.

</details>


### [12] [Assistance or Disruption? Exploring and Evaluating the Design and Trade-offs of Proactive AI Programming Support](https://arxiv.org/abs/2502.18658)

*Kevin Pu, Daniel Lazaro, Ian Arawjo, Haijun Xia, Ziang Xiao, Tovi Grossman, Yan Chen*

**Main category:** cs.HC

**Keywords:** AI programming tools, proactive agent, workflow disruption, programming assistance, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper evaluates Codellaborator, a proactive AI programming assistant that enhances coding efficiency while raising concerns about workflow disruptions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Investigate the impact of proactive AI agents on programming workflows, particularly focusing on user-effort reduction and design implications in AI-assisted coding.

**Method:** Conducted a within-subject study with 18 participants to compare three interface variants: prompt-only, proactive agent, and proactive agent with presence and context.

**Key Contributions:**

	1. Introduction of Codellaborator as a design probe for LLM agents in coding assistance
	2. Empirical findings on the trade-offs between efficiency and workflow disruption
	3. Design implications for integrating AI into programming workflows

**Result:** Proactive agents improved coding efficiency over a prompt-only approach but also introduced workflow disruptions; however, appropriate presence indicators mitigated these disruptions and enhanced awareness.

**Limitations:** Study limited to 18 participants; findings may not generalize across larger, diverse coding populations.

**Conclusion:** The study emphasizes the importance of adapting AI proactivity to users' programming processes, highlighting trade-offs in user control and code understanding.

**Abstract:** AI programming tools enable powerful code generation, and recent prototypes attempt to reduce user effort with proactive AI agents, but their impact on programming workflows remains unexplored. We introduce and evaluate Codellaborator, a design probe LLM agent that initiates programming assistance based on editor activities and task context. We explored three interface variants to assess trade-offs between increasingly salient AI support: prompt-only, proactive agent, and proactive agent with presence and context (Codellaborator). In a within-subject study (N=18), we find that proactive agents increase efficiency compared to prompt-only paradigm, but also incur workflow disruptions. However, presence indicators and interaction context support alleviated disruptions and improved users' awareness of AI processes. We underscore trade-offs of Codellaborator on user control, ownership, and code understanding, emphasizing the need to adapt proactivity to programming processes. Our research contributes to the design exploration and evaluation of proactive AI systems, presenting design implications on AI-integrated programming workflow.

</details>


### [13] [A Critical Analysis of the Usage of Dimensionality Reduction in Four Domains](https://arxiv.org/abs/2503.08836)

*Dylan Cashman, Mark Keller, Hyeon Jeon, Bum Chul Kwon, Qianwen Wang*

**Main category:** cs.HC

**Keywords:** dimensionality reduction, data visualization, scientific workflows, interdisciplinary analysis, bibliometric study

**Relevance Score:** 4

**TL;DR:** The paper analyzes the usage of dimensionality reduction in scientific fields beyond computer science, highlighting workflows, common practices, and issues in interpretation.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how dimensionality reduction techniques are applied in various scientific domains and identify common practices and pitfalls.

**Method:** A bibliometric analysis of 21,249 publications followed by a survey of 71 papers in biology, chemistry, physics, and business to observe usage patterns.

**Key Contributions:**

	1. Critical analysis of dimensionality reduction in fields outside computer science
	2. Identification of workflows and common practices in multiple disciplines
	3. Highlighting common misinterpretations in data visualization

**Result:** Identified differences in the frequency of techniques across fields, common workflows, and prevalent misinterpretations in visual data representation.

**Limitations:** May not cover all scientific fields or applications of dimensionality reduction

**Conclusion:** The study highlights areas for improvement in the use of dimensionality reduction and suggests potential impacts on visualization research.

**Abstract:** Dimensionality reduction is used as an important tool for unraveling the complexities of high-dimensional datasets in many fields of science, such as cell biology, chemical informatics, and physics. Visualizations of the dimensionally reduced data enable scientists to delve into the intrinsic structures of their datasets and align them with established hypotheses. Visualization researchers have thus proposed many dimensionality reduction methods and interactive systems designed to uncover latent structures. At the same time, different scientific domains have formulated guidelines or common workflows for using dimensionality reduction techniques and visualizations for their respective fields. In this work, we present a critical analysis of the usage of dimensionality reduction in scientific domains outside of computer science. First, we conduct a bibliometric analysis of 21,249 academic publications that use dimensionality reduction to observe differences in the frequency of techniques across fields. Next, we conduct a survey of a 71-paper sample from four fields: biology, chemistry, physics, and business. Through this survey, we uncover common workflows, processes, and usage patterns, including the mixed use of confirmatory data analysis to validate a dataset and projection method and exploratory data analysis to then generate more hypotheses. We also find that misinterpretations and inappropriate usage is common, particularly in the visual interpretation of the resulting dimensionally reduced view. Lastly, we compare our observations with recent works in the visualization community in order to match work within our community to potential areas of impact outside our community.

</details>


### [14] [The Trust Calibration Maturity Model for Characterizing and Communicating Trustworthiness of AI Systems](https://arxiv.org/abs/2503.15511)

*Scott T Steinmetz, Asmeret Naugle, Paul Schutte, Matt Sweitzer, Alex Washburne, Lisa Linville, Daniel Krofcheck, Michal Kucer, Samuel Myren*

**Main category:** cs.HC

**Keywords:** Trust Calibration, AI Trustworthiness, Human-Computer Interaction, Usability, Machine Learning

**Relevance Score:** 9

**TL;DR:** The paper introduces the Trust Calibration Maturity Model (TCMM) to help users evaluate AI system trustworthiness across five dimensions.

**Read time:** 19 min

<details>
  <summary>Details</summary>

**Motivation:** To address the increasing difficulty of evaluating the trustworthiness of AI systems as they become more complex and pervasive.

**Method:** Introduction of the Trust Calibration Maturity Model (TCMM), which incorporates five dimensions: Performance Characterization, Bias & Robustness Quantification, Transparency, Safety & Security, and Usability.

**Key Contributions:**

	1. Introduction of the Trust Calibration Maturity Model (TCMM)
	2. Incorporation of five dimensions to evaluate AI trustworthiness
	3. Practical application of TCMM in critical domains like nuclear science and seismic analysis.

**Result:** The TCMM aids users in calibrating trust appropriately, tracking system performance, and identifying research needs, illustrated through examples in nuclear science and seismic event categorization.

**Limitations:** 

**Conclusion:** The TCMM serves as a framework for communicating and improving trust in AI systems, which is essential as their impact on society grows.

**Abstract:** Recent proliferation of powerful AI systems has created a strong need for capabilities that help users to calibrate trust in those systems. As AI systems grow in scale, information required to evaluate their trustworthiness becomes less accessible, presenting a growing risk of using these systems inappropriately. We propose the Trust Calibration Maturity Model (TCMM) to characterize and communicate information about AI system trustworthiness. The TCMM incorporates five dimensions of analytic maturity: Performance Characterization, Bias & Robustness Quantification, Transparency, Safety & Security, and Usability. The TCMM can be presented along with system performance information to (1) help a user to appropriately calibrate trust, (2) establish requirements and track progress, and (3) identify research needs. Here, we discuss the TCMM and demonstrate it on two target tasks: using ChatGPT for high consequence nuclear science determinations, and using PhaseNet (an ensemble of seismic models) for categorizing sources of seismic events.

</details>


### [15] [Theory of Mind and Self-Disclosure to CUIs](https://arxiv.org/abs/2507.10773)

*Samuel Rhys Cox*

**Main category:** cs.HC

**Keywords:** self-disclosure, conversational user interfaces, social cues, theory of mind, user interaction

**Relevance Score:** 6

**TL;DR:** This paper discusses self-disclosure to conversational user interfaces (CUIs) and explores how social cues and displays of uncertainty can facilitate this process.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how self-disclosure can be encouraged in interactions with CUIs, particularly addressing user concerns about conversational responses.

**Method:** The paper discusses the relationship between social cues, user reactions, and self-disclosure in CUIs.

**Key Contributions:**

	1. Exploration of social cues in CUIs
	2. Discussion on CUI reasoning and transparency
	3. Insights on user self-disclosure dynamics

**Result:** Expressions of uncertainty and clearer reasoning from CUIs may enhance user comfort in self-disclosing personal information.

**Limitations:** The workshop paper format limits depth and empirical validation of concepts discussed.

**Conclusion:** Enhancing transparency in CUI reasoning could promote better user interaction and self-disclosure.

**Abstract:** Self-disclosure is important to help us feel better, yet is often difficult. This difficulty can arise from how we think people are going to react to our self-disclosure. In this workshop paper, we briefly discuss self-disclosure to conversational user interfaces (CUIs) in relation to various social cues. We then, discuss how expressions of uncertainty or representation of a CUI's reasoning could help encourage self-disclosure, by making a CUI's intended "theory of mind" more transparent to users.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Truth Sleuth and Trend Bender: AI Agents to fact-check YouTube videos and influence opinions](https://arxiv.org/abs/2507.10577)

*Logé Cécile, Ghori Rehan*

**Main category:** cs.CL

**Keywords:** misinformation, YouTube, fact-checking, AI, user engagement

**Relevance Score:** 7

**TL;DR:** This paper presents an AI system for fact-checking claims in YouTube videos and engaging users in discussions to counter misinformation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing issue of misinformation on digital platforms, specifically YouTube, by utilizing AI to fact-check and engage users.

**Method:** The system comprises two agents: Truth Sleuth for extracting and verifying claims using a RAG approach, and Trend Bender for generating engaging comments to challenge misinformation.

**Key Contributions:**

	1. Introduction of an AI-powered misinformation combat system for YouTube.
	2. Development of Truth Sleuth and Trend Bender agents for fact-checking and user engagement.
	3. Demonstration of high accuracy and engagement in real-world settings.

**Result:** The system showed high accuracy in its fact-checking abilities and demonstrated potential for user engagement and influencing perspectives in real-world YouTube deployments.

**Limitations:** 

**Conclusion:** AI-driven interventions can effectively combat misinformation and promote a more informed online community.

**Abstract:** Misinformation poses a significant threat in today's digital world, often spreading rapidly through platforms like YouTube. This paper introduces a novel approach to combating misinformation by developing an AI-powered system that not only fact-checks claims made in YouTube videos but also actively engages users in the comment section and challenge misleading narratives. Our system comprises two main agents: Truth Sleuth and Trend Bender.   Truth Sleuth extracts claims from a YouTube video, uses a Retrieval-Augmented Generation (RAG) approach - drawing on sources like Wikipedia, Google Search, Google FactCheck - to accurately assess their veracity and generates a nuanced and comprehensive report. Through rigorous prompt engineering, Trend Bender leverages this report along with a curated corpus of relevant articles to generate insightful and persuasive comments designed to stimulate a productive debate. With a carefully set up self-evaluation loop, this agent is able to iteratively improve its style and refine its output.   We demonstrate the system's capabilities through experiments on established benchmark datasets and a real-world deployment on YouTube, showcasing its potential to engage users and potentially influence perspectives. Our findings highlight the high accuracy of our fact-checking agent, and confirm the potential of AI-driven interventions in combating misinformation and fostering a more informed online space.

</details>


### [17] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)

*Vimaleswar A, Prabhu Nandan Sahu, Nilesh Kumar Sahu, Haroon R Lone*

**Main category:** cs.CL

**Keywords:** Mental Health, Emotional Support, Large Language Models, Offline Applications, Conversational AI

**Relevance Score:** 9

**TL;DR:** EmoSApp is an offline smartphone app leveraging LLMs for mental health support, addressing accessibility and privacy issues.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address challenges related to limited accessibility, internet connectivity, and data privacy in mental health support applications.

**Method:** EmoSApp uses fine-tuned LLaMA-3.2-1B-Instruct model on a custom dataset of mental-health Q&A pairs and multi-turn dialogues, allowing inferences to happen locally on smartphones.

**Key Contributions:**

	1. Development of an offline, smartphone-based mental health app
	2. Utilization of fine-tuned LLMs for domain-specific knowledge
	3. Demonstrated efficacy in delivering mental health support under resource constraints

**Result:** Qualitative evaluations show EmoSApp's capabilities in coherent, empathetic responses and interactive dialogue, while quantitative assessments confirm its efficacy in low-resource settings.

**Limitations:** 

**Conclusion:** EmoSApp demonstrates potential for portable, secure AI-driven mental health solutions, serving as a blueprint for future innovations.

**Abstract:** Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have been increasingly used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solution. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed for mental health and emotional support. The system leverages Large Language Models (LLMs), specifically fine-tuned, quantized and deployed using Torchtune and Executorch for resource-constrained devices, allowing all inferences to occur on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of 14,582 mental-health QA pairs, along with the multi-turn conversational data.   Through qualitative human evaluation with the student population, we demonstrate that EmoSApp has the ability to respond coherently, empathetically, maintain interactive dialogue, and provide relevant suggestions to user's mental health problems. Additionally, quantitative evaluations on nine standard commonsense and reasoning benchmarks demonstrate the efficacy of our fine-tuned, quantized model in low-resource settings. By prioritizing on-device deployment and specialized domain adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health solutions.

</details>


### [18] [Transforming Sensitive Documents into Quantitative Data: An AI-Based Preprocessing Toolchain for Structured and Privacy-Conscious Analysis](https://arxiv.org/abs/2507.10582)

*Anders Ledberg, Anna Thalén*

**Main category:** cs.CL

**Keywords:** unstructured text, privacy-sensitive research, large language models, anonymization, text analysis

**Relevance Score:** 7

**TL;DR:** A modular toolchain for analyzing unstructured text data while ensuring privacy and standardization using LLMs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the challenges of sensitive information and text heterogeneity in public health and social sciences research using unstructured text.

**Method:** The toolchain uses LLM prompting for summarization, standardization, translation, and LLM-based redaction, combined with entity recognition and rule-based methods for anonymization.

**Key Contributions:**

	1. Development of a modular toolchain for text analysis
	2. Use of LLMs for summarization and anonymization
	3. Successful application to a large dataset of legal documents

**Result:** Demonstrated effective anonymization and standardization on a corpus of Swedish court decisions, maintaining semantic content while processing into document-level embeddings.

**Limitations:** 

**Conclusion:** The toolchain facilitates large-scale, privacy-sensitive analysis of previously inaccessible textual data in research.

**Abstract:** Unstructured text from legal, medical, and administrative sources offers a rich but underutilized resource for research in public health and the social sciences. However, large-scale analysis is hampered by two key challenges: the presence of sensitive, personally identifiable information, and significant heterogeneity in structure and language. We present a modular toolchain that prepares such text data for embedding-based analysis, relying entirely on open-weight models that run on local hardware, requiring only a workstation-level GPU and supporting privacy-sensitive research.   The toolchain employs large language model (LLM) prompting to standardize, summarize, and, when needed, translate texts to English for greater comparability. Anonymization is achieved via LLM-based redaction, supplemented with named entity recognition and rule-based methods to minimize the risk of disclosure. We demonstrate the toolchain on a corpus of 10,842 Swedish court decisions under the Care of Abusers Act (LVM), comprising over 56,000 pages. Each document is processed into an anonymized, standardized summary and transformed into a document-level embedding. Validation, including manual review, automated scanning, and predictive evaluation shows the toolchain effectively removes identifying information while retaining semantic content. As an illustrative application, we train a predictive model using embedding vectors derived from a small set of manually labeled summaries, demonstrating the toolchain's capacity for semi-automated content analysis at scale.   By enabling structured, privacy-conscious analysis of sensitive documents, our toolchain opens new possibilities for large-scale research in domains where textual data was previously inaccessible due to privacy and heterogeneity constraints.

</details>


### [19] [A Taxonomy for Design and Evaluation of Prompt-Based Natural Language Explanations](https://arxiv.org/abs/2507.10585)

*Isar Nejadgholi, Mona Omidyeganeh, Marc-Antoine Drouin, Jonathan Boisvert*

**Main category:** cs.CL

**Keywords:** AI governance, Natural Language Explanations, Explainable AI, NLE taxonomy, user-centered evaluation

**Relevance Score:** 6

**TL;DR:** This paper presents a taxonomy for Natural Language Explanations (NLEs) based on three dimensions: context, generation and presentation, and evaluation, to enhance AI governance.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve AI governance by providing structured methods for stakeholders to access and verify AI system behaviors, particularly through natural language explanations.

**Method:** The authors develop an updated taxonomy for Explainable AI (XAI) based on existing literature, tailored to the prompt-based nature of NLEs across three dimensions.

**Key Contributions:**

	1. An updated XAI taxonomy adapted to prompt-based NLEs
	2. A structured approach across three informative dimensions
	3. Framework for stakeholders to improve AI transparency

**Result:** The taxonomy categorizes NLEs into context, generation and presentation, and evaluation, offering a systematic approach to enhance the transparency and understanding of AI systems.

**Limitations:** 

**Conclusion:** The taxonomy serves as a valuable framework for researchers, auditors, and policymakers, aiding in the characterization and design of NLEs to foster transparent AI systems.

**Abstract:** Effective AI governance requires structured approaches for stakeholders to access and verify AI system behavior. With the rise of large language models, Natural Language Explanations (NLEs) are now key to articulating model behavior, which necessitates a focused examination of their characteristics and governance implications. We draw on Explainable AI (XAI) literature to create an updated XAI taxonomy, adapted to prompt-based NLEs, across three dimensions: (1) Context, including task, data, audience, and goals; (2) Generation and Presentation, covering generation methods, inputs, interactivity, outputs, and forms; and (3) Evaluation, focusing on content, presentation, and user-centered properties, as well as the setting of the evaluation. This taxonomy provides a framework for researchers, auditors, and policymakers to characterize, design, and enhance NLEs for transparent AI systems.

</details>


### [20] [AutoRAG-LoRA: Hallucination-Triggered Knowledge Retuning via Lightweight Adapters](https://arxiv.org/abs/2507.10586)

*Kaushik Dwivedi, Padmanabh Patanjali Mishra*

**Main category:** cs.CL

**Keywords:** Large Language Models, Retrieval-Augmented Generation, hallucinations, LoRA, factual accuracy

**Relevance Score:** 10

**TL;DR:** AutoRAG-LoRA is a framework designed to reduce hallucinations in Large Language Models by using lightweight adapters and KL-regularized training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs show fluency in language tasks but are prone to hallucinations, which affect their reliability in real-world applications.

**Method:** The AutoRAG-LoRA framework incorporates automated prompt rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in retrieved data. A hallucination detection module evaluates generated outputs and enables a feedback correction loop.

**Key Contributions:**

	1. Introduction of AutoRAG-LoRA framework for LLMs
	2. Implementation of hallucination detection using classifiers and self-evaluation
	3. Effectiveness in reducing factual inaccuracies while preserving model efficiency.

**Result:** AutoRAG-LoRA markedly decreases factual drift while maintaining model efficiency and modularity.

**Limitations:** 

**Conclusion:** The integration of the detection module and the feedback correction loop enhances the factual accuracy of outputs from LLMs.

**Abstract:** Large Language Models (LLMs) have demonstrated remarkable fluency across a range of natural language tasks, yet remain vulnerable to hallucinations - factual inaccuracies that undermine trust in real world deployment. We present AutoRAG-LoRA, a modular framework for Retrieval-Augmented Generation (RAG) that tackles hallucination in large language models through lightweight LoRA-based adapters and KL-regularized training. Our pipeline integrates automated prompt rewriting, hybrid retrieval, and low-rank adapter tuning to ground responses in retrieved evidence. A hallucination detection module, using both classifier-based and self-evaluation techniques, assigns confidence scores to generated outputs, triggering an optional feedback correction loop. This loop enforces factual alignment via contrastive KL loss and adapter fine tuning. We demonstrate that AutoRAG-LoRA significantly reduces the factual drift while preserving the efficiency and modularity of the model.

</details>


### [21] [Anthropomimetic Uncertainty: What Verbalized Uncertainty in Language Models is Missing](https://arxiv.org/abs/2507.10587)

*Dennis Ulmer, Alexandra Lorson, Ivan Titov, Christian Hardmeier*

**Main category:** cs.CL

**Keywords:** human-computer interaction, large language models, uncertainty communication, natural language processing, bias

**Relevance Score:** 9

**TL;DR:** The paper addresses the need for reliable uncertainty communication in interactions between humans and large language models (LLMs) to improve trust and utility. It discusses the concept of anthropomimetic uncertainty, emphasizing the importance of authentic and personalized communication.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the trustworthiness and perceived legitimacy of LLMs by effectively communicating uncertainty and mitigating potential harms.

**Method:** The paper provides a thorough overview of existing research on human uncertainty communication, surveys ongoing research efforts, and includes analyses of biases in verbalized uncertainty.

**Key Contributions:**

	1. Introduction of the concept of anthropomimetic uncertainty.
	2. Analysis of biases in verbalized uncertainty communication.
	3. Recommendations for improving LLMs' uncertainty communication strategies.

**Result:** It identifies overlooked biases in how uncertainty is verbally communicated between humans and LLMs, arguing for the need to emulate human linguistic practices in uncertainty communication.

**Limitations:** 

**Conclusion:** The paper concludes with future research directions focusing on enhancing human-machine communication by deconstructing anthropomimetic uncertainty.

**Abstract:** Human users increasingly rely on natural language interactions with large language models (LLMs) in order to receive help on a large variety of tasks and problems. However, the trustworthiness and perceived legitimacy of LLMs is undermined by the fact that their output is frequently stated in very confident terms, even when its accuracy is questionable. Therefore, there is a need to signal the confidence of the language model to a user in order to reap the benefits of human-machine collaboration and mitigate potential harms. Verbalized uncertainty is the expression of confidence with linguistic means, an approach that integrates perfectly into language-based interfaces. Nevertheless, most recent research in natural language processing (NLP) overlooks the nuances surrounding human uncertainty communication and the data biases that influence machine uncertainty communication. We argue for anthropomimetic uncertainty, meaning that intuitive and trustworthy uncertainty communication requires a degree of linguistic authenticity and personalization to the user, which could be achieved by emulating human communication. We present a thorough overview over the research in human uncertainty communication, survey ongoing research, and perform additional analyses to demonstrate so-far overlooked biases in verbalized uncertainty. We conclude by pointing out unique factors in human-machine communication of uncertainty and deconstruct anthropomimetic uncertainty into future research directions for NLP.

</details>


### [22] [PLEX: Perturbation-free Local Explanations for LLM-Based Text Classification](https://arxiv.org/abs/2507.10596)

*Yogachandran Rahulamathavan, Misbah Farooq, Varuna De Silva*

**Main category:** cs.CL

**Keywords:** Large Language Models, Explainable AI, Local Explanations, Siamese Network, Text Classification

**Relevance Score:** 9

**TL;DR:** Introducing PLEX, a novel perturbation-free method for generating local explanations in LLM-based text classification.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current explainable AI methods for LLMs are computationally expensive and hinder interpretability, making it essential to find efficient approaches for providing explanations.

**Method:** PLEX utilizes contextual embeddings from LLMs and a Siamese network to align with feature importance scores, eliminating the need for perturbation-based methods.

**Key Contributions:**

	1. Introduction of a perturbation-free method for local explanations in LLMs.
	2. Demonstration of high agreement with existing XAI methods while offering greater efficiency.
	3. Improvement in capturing the impact of key features in classification tasks.

**Result:** PLEX shows over 92% agreement with traditional XAI methods (LIME, SHAP) while dramatically reducing computational time and overhead.

**Limitations:** 

**Conclusion:** PLEX provides a promising, efficient solution for local explanations in LLM text classification, maintaining accuracy while being significantly faster than existing methods.

**Abstract:** Large Language Models (LLMs) excel in text classification, but their complexity hinders interpretability, making it difficult to understand the reasoning behind their predictions. Explainable AI (XAI) methods like LIME and SHAP offer local explanations by identifying influential words, but they rely on computationally expensive perturbations. These methods typically generate thousands of perturbed sentences and perform inferences on each, incurring a substantial computational burden, especially with LLMs. To address this, we propose \underline{P}erturbation-free \underline{L}ocal \underline{Ex}planation (PLEX), a novel method that leverages the contextual embeddings extracted from the LLM and a ``Siamese network" style neural network trained to align with feature importance scores. This one-off training eliminates the need for subsequent perturbations, enabling efficient explanations for any new sentence. We demonstrate PLEX's effectiveness on four different classification tasks (sentiment, fake news, fake COVID-19 news and depression), showing more than 92\% agreement with LIME and SHAP. Our evaluation using a ``stress test" reveals that PLEX accurately identifies influential words, leading to a similar decline in classification accuracy as observed with LIME and SHAP when these words are removed. Notably, in some cases, PLEX demonstrates superior performance in capturing the impact of key features. PLEX dramatically accelerates explanation, reducing time and computational overhead by two and four orders of magnitude, respectively. This work offers a promising solution for explainable LLM-based text classification.

</details>


### [23] [An Offline Mobile Conversational Agent for Mental Health Support: Learning from Emotional Dialogues and Psychological Texts with Student-Centered Evaluation](https://arxiv.org/abs/2507.10580)

*Vimaleswar A, Prabhu Nandan Sahu, Nilesh Kumar Sahu, Haroon R Lone*

**Main category:** cs.CL

**Keywords:** mental health, offline app, large language models, conversational AI, data privacy

**Relevance Score:** 9

**TL;DR:** EmoSApp is an offline conversational app for mental health support, utilizing fine-tuned LLMs for on-device inference.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges of accessibility, connectivity, and privacy in digital mental health solutions, an offline approach is proposed.

**Method:** EmoSApp employs a fine-tuned, quantized version of the LLaMA-3.2-1B-Instruct model, trained on a custom dataset of mental health Q&A pairs, enabling inferences on smartphones.

**Key Contributions:**

	1. Introduction of an offline mental health support app
	2. Utilization of fine-tuned LLMs for conversational AI
	3. Demonstration of effective user engagement and response quality

**Result:** User evaluations show EmoSApp's effectiveness in providing empathetic, coherent responses and maintaining engaging dialogues, alongside quantitative benchmarks validating the model's performance.

**Limitations:** 

**Conclusion:** EmoSApp exemplifies a practical framework for deploying AI-driven mental health solutions that prioritize user privacy and accessibility.

**Abstract:** Mental health plays a crucial role in the overall well-being of an individual. In recent years, digital platforms have been increasingly used to expand mental health and emotional support. However, there are persistent challenges related to limited user accessibility, internet connectivity, and data privacy, which highlight the need for an offline, smartphone-based solution. To address these challenges, we propose EmoSApp (Emotional Support App): an entirely offline, smartphone-based conversational app designed for mental health and emotional support. The system leverages Large Language Models (LLMs), specifically fine-tuned, quantized and deployed using Torchtune and Executorch for resource-constrained devices, allowing all inferences to occur on the smartphone. To equip EmoSApp with robust domain expertise, we fine-tuned the LLaMA-3.2-1B-Instruct model on our custom curated ``Knowledge dataset'' of 14,582 mental-health QA pairs, along with the multi-turn conversational data.   Through qualitative human evaluation with the student population, we demonstrate that EmoSApp has the ability to respond coherently, empathetically, maintain interactive dialogue, and provide relevant suggestions to user's mental health problems. Additionally, quantitative evaluations on nine standard commonsense and reasoning benchmarks demonstrate the efficacy of our fine-tuned, quantized model in low-resource settings. By prioritizing on-device deployment and specialized domain adaptation, EmoSApp serves as a blueprint for future innovations in portable, secure, and highly tailored AI-driven mental health solutions.

</details>


### [24] [Emergence of Hierarchical Emotion Organization in Large Language Models](https://arxiv.org/abs/2507.10599)

*Bo Zhao, Maya Okawa, Eric J. Bigelow, Rose Yu, Tomer Ullman, Ekdeep Singh Lubana, Hidenori Tanaka*

**Main category:** cs.CL

**Keywords:** large language models, emotional states, human-computer interaction, socioeconomic biases, cognitive theories

**Relevance Score:** 9

**TL;DR:** The paper analyzes how large language models (LLMs) represent users' emotional states, revealing the formation of hierarchical emotion trees and biases in emotion recognition across different socioeconomic groups.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding the emotional state modeling in LLMs for ethical deployment in conversational agents is crucial.

**Method:** The authors analyze probabilistic dependencies between emotional states in LLM outputs, inspired by psychological emotion frameworks, and conduct human studies for comparison.

**Key Contributions:**

	1. Demonstrated LLMs create hierarchical representations of emotional states similar to human psychology.
	2. Identified systematic biases in emotion recognition related to socioeconomic factors.
	3. Proposed the application of cognitive theories for better evaluation of emotional reasoning in LLMs.

**Result:** LLMs exhibit hierarchical emotion trees aligned with human psychological models, with larger models displaying more complex hierarchies and evident biases across socioeconomic personas.

**Limitations:** The study may be limited by the specific models analyzed and the contexts of human studies conducted.

**Conclusion:** The findings suggest the potential of utilizing cognitively-grounded theories to improve model evaluations and highlight emergent emotional reasoning in LLMs.

**Abstract:** As large language models (LLMs) increasingly power conversational agents, understanding how they model users' emotional states is critical for ethical deployment. Inspired by emotion wheels -- a psychological framework that argues emotions organize hierarchically -- we analyze probabilistic dependencies between emotional states in model outputs. We find that LLMs naturally form hierarchical emotion trees that align with human psychological models, and larger models develop more complex hierarchies. We also uncover systematic biases in emotion recognition across socioeconomic personas, with compounding misclassifications for intersectional, underrepresented groups. Human studies reveal striking parallels, suggesting that LLMs internalize aspects of social perception. Beyond highlighting emergent emotional reasoning in LLMs, our results hint at the potential of using cognitively-grounded theories for developing better model evaluations.

</details>


### [25] [Language Models for Adult Service Website Text Analysis](https://arxiv.org/abs/2507.10743)

*Nickolas Freeman, Thanh Nguyen, Gregory Bott, Jason Parton, Collin Francel*

**Main category:** cs.CL

**Keywords:** sex trafficking, adult service websites, text analysis, transformer models, language modeling

**Relevance Score:** 6

**TL;DR:** The study develops custom transformer models to analyze adult service website data for identifying potential sex trafficking victims, outperforming conventional models in various metrics.

**Read time:** 32 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance identification of sex trafficking victims by analyzing text from adult service websites, which poses unique challenges due to its format.

**Method:** The research employs language modeling approaches including pre-trained and custom transformer models trained specifically for adult service website ad data.

**Key Contributions:**

	1. Introduction of efficient custom transformer models tailored for ASW text analysis
	2. Demonstration of superior performance metrics compared to established transformer variants
	3. Application of models to diverse tasks like clustering and understanding emoji use in ASW data

**Result:** Custom models exceeded performance of BERT-base, RoBERTa, and ModernBERT on metrics like accuracy and F1 score, successfully aiding in ASW ad text analysis.

**Limitations:** 

**Conclusion:** The developed models represent a significant advancement in understanding ASW data, with potential applicability across various research domains.

**Abstract:** Sex trafficking refers to the use of force, fraud, or coercion to compel an individual to perform in commercial sex acts against their will. Adult service websites (ASWs) have and continue to be linked to sex trafficking, offering a platform for traffickers to advertise their victims. Thus, organizations involved in the fight against sex trafficking often use ASW data when attempting to identify potential sex trafficking victims. A critical challenge in transforming ASW data into actionable insight is text analysis. Previous research using ASW data has shown that ASW ad text is important for linking ads. However, working with this text is challenging due to its extensive use of emojis, poor grammar, and deliberate obfuscation to evade law enforcement scrutiny. We conduct a comprehensive study of language modeling approaches for this application area, including simple information retrieval methods, pre-trained transformers, and custom transformer models. We demonstrate that characteristics of ASW text data allow efficient custom transformer models to be trained with relatively small GPU resources and used efficiently for inference on consumer hardware. Our custom models outperform fine-tuned variants of well-known encoder-only transformer models, including BERT-base, RoBERTa, and ModernBERT, on accuracy, recall, F1 score, and ROC AUC. We demonstrate the use of our best-performing custom configuration on three tasks related to ASW data analysis: (i) decomposing the giant component in a graph representation of ASW data, (ii) clustering ASW ad text, and (iii) using the learned token embeddings to understand the use of emojis in the illicit context we study. The models we develop represent a significant advancement in ASW text analysis, which can be leveraged in a variety of downstream applications and research.

</details>


### [26] [Applying Text Embedding Models for Efficient Analysis in Labeled Property Graphs](https://arxiv.org/abs/2507.10772)

*Michal Podstawski*

**Main category:** cs.CL

**Keywords:** property graphs, text embedding, semantic analysis, node classification, relation prediction

**Relevance Score:** 7

**TL;DR:** This paper explores integrating pretrained text embedding models into labeled property graphs to enhance semantic analysis and improve accuracy in tasks like node classification and relation prediction.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To leverage rich textual attributes in property graphs for better analytical results.

**Method:** Embedding textual node and edge properties using pretrained text embedding models within the graph pipeline without structural modifications.

**Key Contributions:**

	1. Integration of language model embeddings into graph analysis
	2. Enhanced accuracy in node classification and relation prediction
	3. Improved interpretability of property graphs

**Result:** Demonstrated improved contextual understanding and enhanced accuracy and interpretability in property graph analysis tasks.

**Limitations:** 

**Conclusion:** Textual semantics significantly boost the performance of property graph analytical tasks.

**Abstract:** Labeled property graphs often contain rich textual attributes that can enhance analytical tasks when properly leveraged. This work explores the use of pretrained text embedding models to enable efficient semantic analysis in such graphs. By embedding textual node and edge properties, we support downstream tasks including node classification and relation prediction with improved contextual understanding. Our approach integrates language model embeddings into the graph pipeline without altering its structure, demonstrating that textual semantics can significantly enhance the accuracy and interpretability of property graph analysis.

</details>


### [27] [Can Multimodal Foundation Models Understand Schematic Diagrams? An Empirical Study on Information-Seeking QA over Scientific Papers](https://arxiv.org/abs/2507.10787)

*Yilun Zhao, Chengye Wang, Chuhan Li, Arman Cohan*

**Main category:** cs.CL

**Keywords:** MISS-QA, benchmark, multimodal models, scientific literature, diagram interpretation

**Relevance Score:** 6

**TL;DR:** This paper presents MISS-QA, a benchmark to assess models' ability to interpret schematic diagrams in scientific literature, revealing significant performance gaps compared to human experts.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well models can interpret schematic diagrams in scientific literature, which is a crucial skill for understanding research content.

**Method:** The benchmark consists of 1,500 expert-annotated examples across 465 papers. The performance of 18 multimodal foundation models was assessed through their ability to answer questions related to schematic diagrams.

**Key Contributions:**

	1. Introduction of the MISS-QA benchmark for model evaluation
	2. Performance comparison of 18 multimodal models against human experts
	3. Analysis of model limitations and error patterns in interpreting diagrams

**Result:** The study found a significant performance gap between multimodal models and human experts, especially in handling unanswerable questions, indicating models' current limitations in comprehension.

**Limitations:** The models show a performance gap compared to humans, particularly with unanswerable questions, highlighting areas needing improvement.

**Conclusion:** Insights from the performance analysis can guide improvements in multimodal models to better understand scientific multimodal literature.

**Abstract:** This paper introduces MISS-QA, the first benchmark specifically designed to evaluate the ability of models to interpret schematic diagrams within scientific literature. MISS-QA comprises 1,500 expert-annotated examples over 465 scientific papers. In this benchmark, models are tasked with interpreting schematic diagrams that illustrate research overviews and answering corresponding information-seeking questions based on the broader context of the paper. We assess the performance of 18 frontier multimodal foundation models, including o4-mini, Gemini-2.5-Flash, and Qwen2.5-VL. We reveal a significant performance gap between these models and human experts on MISS-QA. Our analysis of model performance on unanswerable questions and our detailed error analysis further highlight the strengths and limitations of current models, offering key insights to enhance models in comprehending multimodal scientific literature.

</details>


### [28] [Testing Hypotheses from the Social Approval Theory of Online Hate: An Analysis of 110 Million Posts from Parler](https://arxiv.org/abs/2507.10810)

*David M. Markowitz, Samuel Hardman Taylor*

**Main category:** cs.CL

**Keywords:** online hate, social approval, hate speech, Parler, social media

**Relevance Score:** 3

**TL;DR:** The paper investigates the relationship between social approval and online hate speech, finding mixed results counter to existing theories.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the motivations behind online hate speech, particularly the role of social approval in encouraging or mitigating such behavior.

**Method:** Analysis of over 110 million posts from the social media platform Parler, focusing on the correlation between social approval (upvotes) and subsequent hate speech production.

**Key Contributions:**

	1. Investigated social approval theory in the context of online hate.
	2. Utilized a large dataset from Parler to analyze hate speech behavior.
	3. Demonstrated that social approval may not consistently lead to increased hate speech.

**Result:** An average negative relationship was observed between social approval and hate speech production at the post level, with mixed results at longer time intervals.

**Limitations:** Findings may not generalize beyond the specific niche of Parler or other similar platforms.

**Conclusion:** Social approval reinforcement mechanisms for online hate may vary significantly across different social media platforms.

**Abstract:** In this paper, we explored how online hate is motivated by receiving social approval from others. We specifically examined two central tenets of Walther's (2024) social approval theory of online hate: (H1a) more signals of social approval on hate messages predicts more subsequent hate messages, and (H1b) as social approval increases, hate speech messages become more extreme. Using over 110 million posts from Parler (2018-2021), we observed that the number of upvotes a person received on a hate speech post was unassociated with the amount of hate speech in their next post and posts during the next week, month, three months, and six months. Between-person effects revealed an average negative relationship between social approval and hate speech production at the post level, but this relationship was mixed at other time intervals. Social approval reinforcement mechanisms of online hate may operate differently on niche social media platforms.

</details>


### [29] [LLMs on Trial: Evaluating Judicial Fairness for Large Language Models](https://arxiv.org/abs/2507.10852)

*Yiran Hu, Zongyue Xue, Haitao Li, Siyuan Zheng, Qingjing Chen, Shaochun Wang, Xihan Zhang, Ning Zheng, Yun Liu, Qingyao Ai, Yiqun Liu, Charles L. A. Clarke, Weixing Shen*

**Main category:** cs.CL

**Keywords:** Large Language Models, judicial fairness, social justice, bias, dataset

**Relevance Score:** 9

**TL;DR:** This paper explores the fairness of Large Language Models (LLMs) in judicial contexts, introducing a framework for measurement and a comprehensive dataset, along with evaluation metrics that reveal significant biases and inconsistencies across LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing use of LLMs in high-stakes domains necessitates an assessment of their fairness, particularly in judicial applications where their decisions affect rights and equity.

**Method:** A framework for measuring LLM fairness is developed, comprising 65 labels and 161 values. An extensive dataset, JudiFair, with 177,100 unique case facts is compiled. Three evaluation metrics are introduced to measure inconsistency, bias, and imbalanced inaccuracy across 16 LLMs.

**Key Contributions:**

	1. Development of a comprehensive fairness measurement framework for LLMs
	2. Creation of the JudiFair dataset
	3. Introduction of new evaluation metrics for assessing LLM fairness

**Result:** The study reveals significant judicial unfairness in LLMs, with pronounced biases on demographic labels and a complex relationship between consistency, accuracy, and bias levels across different models.

**Limitations:** 

**Conclusion:** Adjusting model parameters can influence fairness, while other factors like model size and origin showed no significant effect. A toolkit with datasets and code is released for further research.

**Abstract:** Large Language Models (LLMs) are increasingly used in high-stakes fields where their decisions impact rights and equity. However, LLMs' judicial fairness and implications for social justice remain underexplored. When LLMs act as judges, the ability to fairly resolve judicial issues is a prerequisite to ensure their trustworthiness. Based on theories of judicial fairness, we construct a comprehensive framework to measure LLM fairness, leading to a selection of 65 labels and 161 corresponding values. Applying this framework to the judicial system, we compile an extensive dataset, JudiFair, comprising 177,100 unique case facts. To achieve robust statistical inference, we develop three evaluation metrics, inconsistency, bias, and imbalanced inaccuracy, and introduce a method to assess the overall fairness of multiple LLMs across various labels. Through experiments with 16 LLMs, we uncover pervasive inconsistency, bias, and imbalanced inaccuracy across models, underscoring severe LLM judicial unfairness. Particularly, LLMs display notably more pronounced biases on demographic labels, with slightly less bias on substance labels compared to procedure ones. Interestingly, increased inconsistency correlates with reduced biases, but more accurate predictions exacerbate biases. While we find that adjusting the temperature parameter can influence LLM fairness, model size, release date, and country of origin do not exhibit significant effects on judicial fairness. Accordingly, we introduce a publicly available toolkit containing all datasets and code, designed to support future research in evaluating and improving LLM fairness.

</details>


### [30] [How Stylistic Similarity Shapes Preferences in Dialogue Dataset with User and Third Party Evaluations](https://arxiv.org/abs/2507.10918)

*Ikumi Numaya, Shoji Moriya, Shiki Sato, Reina Akama, Jun Suzuki*

**Main category:** cs.CL

**Keywords:** dialogue generation, stylistic similarity, user preferences, human-bot interaction, natural language processing

**Relevance Score:** 9

**TL;DR:** This paper introduces a novel dataset to analyze stylistic similarity in human-bot interactions, revealing important distinctions between user perceptions and third-party evaluations of stylistic similarity.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To examine the differences between subjective and objective stylistic similarity in user-bot interactions and its effect on user preferences.

**Method:** A novel dataset was created that includes users' preferences, subjective stylistic similarity based on users' perceptions, and objective stylistic similarity annotated by third-party evaluators, followed by analysis of these components.

**Key Contributions:**

	1. Introduction of a novel dataset for analyzing stylistic similarity in dialogue generation.
	2. Establishment of the relationship between subjective stylistic similarity and user preference.
	3. Highlighting the distinction between subjective and objective evaluations in user experience.

**Result:** The analysis shows a strong positive correlation between subjective stylistic similarity and user preference, highlighting that users' perceptions of stylistic similarity differ from objective evaluations.

**Limitations:** The study's analysis is limited to open-domain dialogue settings and may not generalize to other contexts.

**Conclusion:** Recognizing the differences between subjective and objective stylistic similarity is crucial for understanding user preferences in human-bot interactions.

**Abstract:** Recent advancements in dialogue generation have broadened the scope of human-bot interactions, enabling not only contextually appropriate responses but also the analysis of human affect and sensitivity. While prior work has suggested that stylistic similarity between user and system may enhance user impressions, the distinction between subjective and objective similarity is often overlooked. To investigate this issue, we introduce a novel dataset that includes users' preferences, subjective stylistic similarity based on users' own perceptions, and objective stylistic similarity annotated by third party evaluators in open-domain dialogue settings. Analysis using the constructed dataset reveals a strong positive correlation between subjective stylistic similarity and user preference. Furthermore, our analysis suggests an important finding: users' subjective stylistic similarity differs from third party objective similarity. This underscores the importance of distinguishing between subjective and objective evaluations and understanding the distinct aspects each captures when analyzing the relationship between stylistic similarity and user preferences. The dataset presented in this paper is available online.

</details>


### [31] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)

*Wenqing Wu, Chengzhi Zhang, Yi Zhao*

**Main category:** cs.CL

**Keywords:** novelty assessment, large language models, human-computer interaction, method prediction

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel method for assessing the novelty of academic papers by integrating human expertise with large language models (LLMs), specifically focusing on method novelty assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of traditional novelty evaluation in academic peer review, which relies on expert judgment and citation analysis, both of which have significant shortcomings.

**Method:** The proposed method involves extracting sentences related to novelty from peer review reports and utilizing an LLM to summarize the methodology section of academic papers, which are then used to fine-tune pretrained language models (PLMs). A Sparse-Attention text-guided fusion module is designed to better integrate human and LLM knowledge.

**Key Contributions:**

	1. Integration of LLMs and human knowledge for novelty assessment in academic papers
	2. Development of a Sparse-Attention text-guided fusion module
	3. Demonstration of superior performance compared to baseline methods

**Result:** The proposed approach exhibits superior performance over numerous baseline methods in predicting the method novelty of academic papers.

**Limitations:** 

**Conclusion:** Integrating LLMs with human expert knowledge can effectively enhance the assessment of novelty in academic research, providing a more reliable evaluation framework.

**Abstract:** Novelty is a crucial criterion in the peer review process for evaluating academic papers. Traditionally, it's judged by experts or measure by unique reference combinations. Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain. Moreover, it's unclear if unique citations truly measure novelty. The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment. The most common novelty in academic papers is the introduction of new methods. In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs. In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge. We compared the method we proposed with a large number of baselines. Extensive experiments demonstrate that our method achieves superior performance.

</details>


### [32] [HanjaBridge: Resolving Semantic Ambiguity in Korean LLMs via Hanja-Augmented Pre-Training](https://arxiv.org/abs/2507.10920)

*Seungho Choi*

**Main category:** cs.CL

**Keywords:** Large Language Models, HanjaBridge, Korean Language Processing, Cross-lingual Transfer, Continual Pre-training

**Relevance Score:** 6

**TL;DR:** HanjaBridge is a novel technique that enhances language understanding in Korean by integrating all possible Hanja characters for homographs into a continual pre-training framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of large language models on low-resource languages like Korean, addressing the unique challenges presented by homophonous Sino-Korean words.

**Method:** HanjaBridge provides all possible Hanja candidates for homographs, encouraging better contextual disambiguation, along with token-level knowledge distillation to prevent forgetting.

**Key Contributions:**

	1. Introduction of HanjaBridge for semantic ambiguity resolution in Korean
	2. Integration of continual pre-training with meaning injection
	3. Demonstrated significant cross-lingual transfer benefits

**Result:** HanjaBridge leads to a 21% relative improvement in Korean language understanding on the KoBALT benchmark and enables strong cross-lingual transfer between Korean and Chinese.

**Limitations:** 

**Conclusion:** The improvements persist even without Hanja augmentation during inference, showing efficiency without extra runtime costs.

**Abstract:** Large language models (LLMs) often show poor performance in low-resource languages like Korean, partly due to unique linguistic challenges such as homophonous Sino-Korean words that are indistinguishable in Hangul script. To address this semantic ambiguity, we propose HanjaBridge, a novel meaning-injection technique integrated into a continual pre-training (CPT) framework. Instead of deterministically mapping a word to a single Hanja (Chinese character), HanjaBridge presents the model with all possible Hanja candidates for a given homograph, encouraging the model to learn contextual disambiguation. This process is paired with token-level knowledge distillation to prevent catastrophic forgetting. Experimental results show that HanjaBridge significantly improves Korean language understanding, achieving a 21\% relative improvement on the KoBALT benchmark. Notably, by reinforcing semantic alignment between Korean and Chinese through shared Hanja, we observe a strong positive cross-lingual transfer. Furthermore, these gains persist even when Hanja augmentation is omitted at inference time, ensuring practical efficiency with no additional run-time cost.

</details>


### [33] [Modeling Understanding of Story-Based Analogies Using Large Language Models](https://arxiv.org/abs/2507.10957)

*Kalit Inani, Keshav Kabra, Vijay Marupudi, Sashank Varma*

**Main category:** cs.CL

**Keywords:** Large Language Models, Analogy Detection, Human Cognition, Reasoning Evaluation, Machine Learning

**Relevance Score:** 9

**TL;DR:** This paper evaluates the reasoning abilities of Large Language Models (LLMs) in analogical mapping tasks, comparing their performance to human reasoning and examining the impact of model size and architecture.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to determine how well LLMs align with human cognition in detecting and mapping analogies, addressing gaps in prior research regarding LLM reasoning capabilities.

**Method:** The research involves a fine-grained evaluation of LLMs' analogical reasoning through semantic representation assessments using sentence embeddings, and by explicitly prompting LLMs to explain analogies, while analyzing the impact of model size and architecture.

**Key Contributions:**

	1. Fine-grained evaluation of LLM reasoning in analogical tasks.
	2. Investigation of sentence embeddings for semantic representation of analogies.
	3. Analysis of model size and architecture effects on reasoning performance.

**Result:** The experiments reveal insights into the semantic representation of analogies in LLMs and indicate performance profiles that may or may not align with human reasoning, particularly when evaluating individual analogies instead of just overall accuracy.

**Limitations:** 

**Conclusion:** The findings of this research enhance the understanding of LLMs' analogical reasoning abilities and their potential as models for human reasoning, emphasizing the necessity for more nuanced evaluation techniques in this field.

**Abstract:** Recent advancements in Large Language Models (LLMs) have brought them closer to matching human cognition across a variety of tasks. How well do these models align with human performance in detecting and mapping analogies? Prior research has shown that LLMs can extract similarities from analogy problems but lack robust human-like reasoning. Building on Webb, Holyoak, and Lu (2023), the current study focused on a story-based analogical mapping task and conducted a fine-grained evaluation of LLM reasoning abilities compared to human performance. First, it explored the semantic representation of analogies in LLMs, using sentence embeddings to assess whether they capture the similarity between the source and target texts of an analogy, and the dissimilarity between the source and distractor texts. Second, it investigated the effectiveness of explicitly prompting LLMs to explain analogies. Throughout, we examine whether LLMs exhibit similar performance profiles to those observed in humans by evaluating their reasoning at the level of individual analogies, and not just at the level of overall accuracy (as prior studies have done). Our experiments include evaluating the impact of model size (8B vs. 70B parameters) and performance variation across state-of-the-art model architectures such as GPT-4 and LLaMA3. This work advances our understanding of the analogical reasoning abilities of LLMs and their potential as models of human reasoning.

</details>


### [34] [DS@GT at eRisk 2025: From prompts to predictions, benchmarking early depression detection with conversational agent based assessments and temporal attention models](https://arxiv.org/abs/2507.10958)

*Anthony Miyaguchi, David Guecha, Yuwen Chiu, Sidharth Gaur*

**Main category:** cs.CL

**Keywords:** conversational depression detection, large language models, prompt engineering, BDI-II assessment, mental health

**Relevance Score:** 9

**TL;DR:** The DS@GT team participated in eRisk 2025 challenges focusing on conversational depression detection using LLMs, achieving notable leaderboard results with a prompt-engineering strategy.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve conversational depression detection through the use of LLMs in the absence of ground-truth labels.

**Method:** Employing a prompt-engineering strategy across various LLMs for BDI-II-based evaluations and analyzing outputs structured in JSON format.

**Key Contributions:**

	1. Novel prompt-engineering strategy for LLMs in mental health assessment
	2. Evaluation of cross-model agreement and internal consistency
	3. Structured JSON outputs for systematic analysis of conversational cues

**Result:** Achieved second place on the leaderboard with DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27 through evaluation of model agreement and internal consistency.

**Limitations:** 

**Conclusion:** The prompt design methodology was effective in aligning outputs with BDI-II criteria and provided insights into conversational cues influencing symptom prediction.

**Abstract:** This Working Note summarizes the participation of the DS@GT team in two eRisk 2025 challenges. For the Pilot Task on conversational depression detection with large language-models (LLMs), we adopted a prompt-engineering strategy in which diverse LLMs conducted BDI-II-based assessments and produced structured JSON outputs. Because ground-truth labels were unavailable, we evaluated cross-model agreement and internal consistency. Our prompt design methodology aligned model outputs with BDI-II criteria and enabled the analysis of conversational cues that influenced the prediction of symptoms. Our best submission, second on the official leaderboard, achieved DCHR = 0.50, ADODL = 0.89, and ASHR = 0.27.

</details>


### [35] [Teach Me Sign: Stepwise Prompting LLM for Sign Language Production](https://arxiv.org/abs/2507.10972)

*Zhaoyi An, Rei Kawakami*

**Main category:** cs.CL

**Keywords:** sign language generation, large language models, machine learning, natural language processing, HCI

**Relevance Score:** 9

**TL;DR:** This paper presents TEAch Me Sign (TEAM-Sign), a method utilizing large language models to enhance sign language generation by treating it like a natural language, exploiting LLMs' reasoning and knowledge capabilities to improve the mapping of text to sign language.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Sign language generation has seen limited advancements using large language models (LLMs) due to unique complexities involved. This research aims to bridge that gap by effectively leveraging LLMs for better sign language communication.

**Method:** The proposed method, TEAM-Sign, fine-tunes a large language model to learn the correspondence between text and sign language, using a stepwise prompting strategy to align inherent sign language knowledge with the model's reasoning abilities.

**Key Contributions:**

	1. Proposed TEAM-Sign to facilitate sign language generation using LLMs.
	2. Introduced a stepwise prompting strategy tailored for sign language.
	3. Demonstrated improved performance in sign language generation on benchmark datasets.

**Result:** Experimental results show that TEAM-Sign significantly improves the generation of sign language from text, effectively addressing the grammatical discrepancies and distribution differences between sign and spoken languages, evidenced by performance on the How2Sign and Phoenix14T datasets.

**Limitations:** 

**Conclusion:** The study concludes that fine-tuning LLMs with a structured approach can yield effective sign language generation tools, thereby enhancing communication accessibility.

**Abstract:** Large language models, with their strong reasoning ability and rich knowledge, have brought revolution to many tasks of AI, but their impact on sign language generation remains limited due to its complexity and unique rules. In this paper, we propose TEAch Me Sign (TEAM-Sign), treating sign language as another natural language. By fine-tuning an LLM, we enable it to learn the correspondence between text and sign language, and facilitate generation. Considering the differences between sign and spoken language, we employ a stepwise prompting strategy to extract the inherent sign language knowledge within the LLM, thereby supporting the learning and generation process. Experimental results on How2Sign and Phoenix14T datasets demonstrate that our approach effectively leverages both the sign language knowledge and reasoning capabilities of LLM to align the different distribution and grammatical rules between sign and spoken language.

</details>


### [36] [Mario at EXIST 2025: A Simple Gateway to Effective Multilingual Sexism Detection](https://arxiv.org/abs/2507.10996)

*Lin Tian, Johanne R. Trippas, Marian-Andrei Rizoiu*

**Main category:** cs.CL

**Keywords:** sexism detection, Low-Rank Adaptation, multilingual training, Llama 3.1, social media

**Relevance Score:** 7

**TL;DR:** This paper proposes a novel approach to detect text-based sexism in English and Spanish tweets using a parameter-efficient method based on hierarchical Low-Rank Adaptation of Llama 3.1, achieving significant performance improvements with minimal resources.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the growing need for effective sexism detection in social media, especially across multiple languages, utilizing advanced machine learning techniques.

**Method:** The approach utilizes hierarchical Low-Rank Adaptation (LoRA) on the Llama 3.1 model, implementing conditional adapter routing for three subtasks: binary sexism identification, source intention detection, and multilabel categorization, with minimal data preprocessing.

**Key Contributions:**

	1. Introduction of conditional adapter routing for hierarchical subtasks
	2. Significant reductions in training time and model storage with competitive performance
	3. Achievement of cross-lingual transfer improvements without the need for separate models

**Result:** The proposed method demonstrates significant F1 score improvements through cross-lingual transfer with only 1.67% of trainable parameters, reducing training time by 75% and model storage by 98%.

**Limitations:** 

**Conclusion:** The findings indicate that a straightforward parameter-efficient fine-tuning approach can effectively enhance sexism detection performance, proving to be a viable alternative to more complex methods.

**Abstract:** This paper presents our approach to EXIST 2025 Task 1, addressing text-based sexism detection in English and Spanish tweets through hierarchical Low-Rank Adaptation (LoRA) of Llama 3.1 8B. Our method introduces conditional adapter routing that explicitly models label dependencies across three hierarchically structured subtasks: binary sexism identification, source intention detection, and multilabel sexism categorization. Unlike conventional LoRA applications that target only attention layers, we apply adaptation to all linear transformations, enhancing the model's capacity to capture task-specific patterns. In contrast to complex data processing and ensemble approaches, we show that straightforward parameter-efficient fine-tuning achieves strong performance. We train separate LoRA adapters (rank=16, QLoRA 4-bit) for each subtask using unified multilingual training that leverages Llama 3.1's native bilingual capabilities. The method requires minimal preprocessing and uses standard supervised learning. Our multilingual training strategy eliminates the need for separate language-specific models, achieving 1.7-2.4\% F1 improvements through cross-lingual transfer. With only 1.67\% trainable parameters compared to full fine-tuning, our approach reduces training time by 75\% and model storage by 98\%, while achieving competitive performance across all subtasks (ICM-Hard: 0.6774 for binary classification, 0.4991 for intention detection, 0.6519 for multilabel categorization).

</details>


### [37] [Team HUMANE at AVeriTeC 2025: HerO 2 for Efficient Fact Verification](https://arxiv.org/abs/2507.11004)

*Yejun Yoon, Jaeyoon Jung, Seunghyun Yoon, Kunwoo Park*

**Main category:** cs.CL

**Keywords:** fact verification, language models, evidence quality

**Relevance Score:** 8

**TL;DR:** HerO 2 is an enhanced model for fact verification that improves evidence quality and system performance with optimized veracity prediction and updated language models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance and efficiency of fact verification systems.

**Method:** The model incorporates document summarization, answer reformulation, and post-training quantization while using updated language model backbones.

**Key Contributions:**

	1. Improved evidence quality through document summarization and answer reformulation.
	2. Optimized veracity prediction via post-training quantization.
	3. Integration of updated language model backbones for better performance.

**Result:** HerO 2 ranked second on the FEVER-25 leaderboard and had the shortest runtime among the top systems.

**Limitations:** 

**Conclusion:** HerO 2 shows high efficiency and strong potential for real-world applications in fact verification.

**Abstract:** This paper presents HerO 2, Team HUMANE's system for the AVeriTeC shared task at the FEVER-25 workshop. HerO 2 is an enhanced version of HerO, the best-performing open-source model from the previous year's challenge. It improves evidence quality through document summarization and answer reformulation, optimizes veracity prediction via post-training quantization under computational constraints, and enhances overall system performance by integrating updated language model (LM) backbones. HerO 2 ranked second on the leaderboard while achieving the shortest runtime among the top three systems, demonstrating both high efficiency and strong potential for real-world fact verification. The code is available at https://github.com/ssu-humane/HerO2.

</details>


### [38] [Journalism-Guided Agentic In-Context Learning for News Stance Detection](https://arxiv.org/abs/2507.11049)

*Dahyun Lee, Jonghyeon Choi, Jiyoung Han, Kunwoo Park*

**Main category:** cs.CL

**Keywords:** stance detection, news recommendation, media bias, Korean dataset, ML frameworks

**Relevance Score:** 6

**TL;DR:** This paper introduces K-News-Stance, a dataset for article-level stance detection in Korean, and proposes JoA-ICL, a framework that improves stance detection in long-form news articles by predicting and aggregating segment-level stances.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing stance detection methods which are primarily focused on short texts and high-resource languages, especially in the context of personalized news recommendations.

**Method:** K-News-Stance dataset with 2,000 articles and 19,650 segment-level annotations; JoA-ICL framework utilizing a language model agent for segment-level stance prediction and aggregation.

**Key Contributions:**

	1. Introduction of K-News-Stance, the first Korean dataset for article-level stance detection.
	2. Development of JoA-ICL, a new framework for stance prediction that improves upon existing methods.
	3. Demonstration of the utility of stance detection in promoting viewpoint diversity and revealing media bias.

**Result:** JoA-ICL outperforms existing stance detection methods, demonstrating effectiveness in capturing the stance of long-form articles.

**Limitations:** Limited to the Korean language and may not generalize to other languages or cultures.

**Conclusion:** The study highlights the importance of segment-level agency in stance detection to promote diversity in news recommendations and analyze media bias.

**Abstract:** As online news consumption grows, personalized recommendation systems have become integral to digital journalism. However, these systems risk reinforcing filter bubbles and political polarization by failing to incorporate diverse perspectives. Stance detection -- identifying a text's position on a target -- can help mitigate this by enabling viewpoint-aware recommendations and data-driven analyses of media bias. Yet, existing stance detection research remains largely limited to short texts and high-resource languages. To address these gaps, we introduce \textsc{K-News-Stance}, the first Korean dataset for article-level stance detection, comprising 2,000 news articles with article-level and 19,650 segment-level stance annotations across 47 societal issues. We also propose \textsc{JoA-ICL}, a \textbf{Jo}urnalism-guided \textbf{A}gentic \textbf{I}n-\textbf{C}ontext \textbf{L}earning framework that employs a language model agent to predict the stances of key structural segments (e.g., leads, quotes), which are then aggregated to infer the overall article stance. Experiments show that \textsc{JoA-ICL} outperforms existing stance detection methods, highlighting the benefits of segment-level agency in capturing the overall position of long-form news articles. Two case studies further demonstrate its broader utility in promoting viewpoint diversity in news recommendations and uncovering patterns of media bias.

</details>


### [39] [LLM-Augmented Symptom Analysis for Cardiovascular Disease Risk Prediction: A Clinical NLP](https://arxiv.org/abs/2507.11052)

*Haowei Yang, Ziyu Shen, Junli Shao, Luyao Men, Xinyue Han, Jing Dong*

**Main category:** cs.CL

**Keywords:** Cardiovascular disease, NLP pipeline, Large language models, Clinical decision support, Risk stratification

**Relevance Score:** 9

**TL;DR:** This study presents an LLM-augmented NLP pipeline for improving cardiovascular disease risk stratification using unstructured clinical notes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the timely identification and risk stratification of cardiovascular disease (CVD) using valuable data from unstructured clinical notes.

**Method:** The study employs domain-adapted large language models for symptom extraction, contextual reasoning, and correlation from free-text reports, integrating fine-tuning, prompt-based inference, and entity-aware reasoning.

**Key Contributions:**

	1. Introduction of an LLM-augmented clinical NLP pipeline for cardiovascular disease risk assessment.
	2. Integration of fine-tuning and prompt engineering for better symptom extraction.
	3. Demonstration of high clinical relevance and performance improvements over existing models.

**Result:** The proposed approach outperforms traditional models on MIMIC-III and CARDIO-NLP datasets, showing improved precision, recall, F1-score, and AUROC, with high clinical relevance validated by cardiologists.

**Limitations:** Challenges such as contextual hallucination and temporal ambiguity were identified and addressed.

**Conclusion:** The findings highlight the potential of LLMs in clinical decision support systems to improve early warning systems and translate patient narratives into actionable assessments.

**Abstract:** Timely identification and accurate risk stratification of cardiovascular disease (CVD) remain essential for reducing global mortality. While existing prediction models primarily leverage structured data, unstructured clinical notes contain valuable early indicators. This study introduces a novel LLM-augmented clinical NLP pipeline that employs domain-adapted large language models for symptom extraction, contextual reasoning, and correlation from free-text reports. Our approach integrates cardiovascular-specific fine-tuning, prompt-based inference, and entity-aware reasoning. Evaluations on MIMIC-III and CARDIO-NLP datasets demonstrate improved performance in precision, recall, F1-score, and AUROC, with high clinical relevance (kappa = 0.82) assessed by cardiologists. Challenges such as contextual hallucination, which occurs when plausible information contracts with provided source, and temporal ambiguity, which is related with models struggling with chronological ordering of events are addressed using prompt engineering and hybrid rule-based verification. This work underscores the potential of LLMs in clinical decision support systems (CDSS), advancing early warning systems and enhancing the translation of patient narratives into actionable risk assessments.

</details>


### [40] [Social Media Sentiments Analysis on the July Revolution in Bangladesh: A Hybrid Transformer Based Machine Learning Approach](https://arxiv.org/abs/2507.11084)

*Md. Sabbir Hossen, Md. Saiduzzaman, Pabon Shaha*

**Main category:** cs.CL

**Keywords:** sentiment analysis, transformer models, social media, Bangla language, machine learning

**Relevance Score:** 4

**TL;DR:** This study presents a hybrid transformer-based sentiment analysis framework utilizing Bangla social media comments during the July Revolution in Bangladesh, achieving 83.7% accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To analyze public sentiment expressed on social media during the July Revolution in Bangladesh, and to demonstrate the effectiveness of machine learning in low-resource languages.

**Method:** A hybrid transformer-based sentiment analysis framework was developed using advanced features from BanglaBERT, mBERT, XLM-RoBERTa, and the new XMB-BERT model, with PCA for dimensionality reduction and eleven machine learning classifiers for sentiment classification.

**Key Contributions:**

	1. Introduction of a hybrid transformer-based sentiment analysis framework
	2. Utilization of a novel dataset of 4,200 Bangla social media comments
	3. Demonstration of high accuracy (83.7%) using advanced ML techniques

**Result:** The hybrid XMB-BERT model, combined with a voting classifier, achieved an accuracy of 83.7%, outperforming other model combinations.

**Limitations:** 

**Conclusion:** The study highlights the potential of machine learning techniques for sentiment analysis in low-resource languages, showcasing the importance of social media during political movements.

**Abstract:** The July Revolution in Bangladesh marked a significant student-led mass uprising, uniting people across the nation to demand justice, accountability, and systemic reform. Social media platforms played a pivotal role in amplifying public sentiment and shaping discourse during this historic mass uprising. In this study, we present a hybrid transformer-based sentiment analysis framework to decode public opinion expressed in social media comments during and after the revolution. We used a brand new dataset of 4,200 Bangla comments collected from social media. The framework employs advanced transformer-based feature extraction techniques, including BanglaBERT, mBERT, XLM-RoBERTa, and the proposed hybrid XMB-BERT, to capture nuanced patterns in textual data. Principle Component Analysis (PCA) were utilized for dimensionality reduction to enhance computational efficiency. We explored eleven traditional and advanced machine learning classifiers for identifying sentiments. The proposed hybrid XMB-BERT with the voting classifier achieved an exceptional accuracy of 83.7% and outperform other model classifier combinations. This study underscores the potential of machine learning techniques to analyze social sentiment in low-resource languages like Bangla.

</details>


### [41] [Beyond Traditional Algorithms: Leveraging LLMs for Accurate Cross-Border Entity Identification](https://arxiv.org/abs/2507.11086)

*Andres Azqueta-Gavaldón, Joaquin Ramos Cosgrove*

**Main category:** cs.CL

**Keywords:** entity matching, Large Language Models, financial systems, risk management, accuracy

**Relevance Score:** 6

**TL;DR:** This paper discusses the identification and classification of foreign entities in the Spanish financial system using Large Language Models (LLMs) to improve accuracy and reduce false positives in entity-matching tasks compared to traditional methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The increase in cross-border financial activities demands accurate identification of foreign entities for risk management and compliance, which is challenged by traditional matching algorithms.

**Method:** The study compares traditional entity-matching algorithms (Jaccard, cosine, Levenshtein distances) with LLMs based on Hugging Face and interface-based systems like Microsoft Copilot and Alibaba's Qwen 2.5, utilizing a dataset of 65 Portuguese company cases for evaluation.

**Key Contributions:**

	1. Comparison of traditional entity-matching algorithms with LLM approaches
	2. Demonstrated improvements in accuracy and false positive rates using LLMs
	3. Validated findings using a real-world dataset of Portuguese companies.

**Result:** Traditional methods achieved over 92% accuracy but had high false positive rates of 20-40%. Interface-based LLMs showed better performance with accuracies above 93%, F1 scores exceeding 96%, and significantly reduced false positive rates (40-80%).

**Limitations:** 

**Conclusion:** LLMs offer a more effective solution for entity matching in financial systems, addressing limitations of traditional algorithms and improving overall accuracy and reliability.

**Abstract:** The growing prevalence of cross-border financial activities in global markets has underscored the necessity of accurately identifying and classifying foreign entities. This practice is essential within the Spanish financial system for ensuring robust risk management, regulatory adherence, and the prevention of financial misconduct. This process involves a labor-intensive entity-matching task, where entities need to be validated against available reference sources. Challenges arise from linguistic variations, special characters, outdated names, and changes in legal forms, complicating traditional matching algorithms like Jaccard, cosine, and Levenshtein distances. These methods struggle with contextual nuances and semantic relationships, leading to mismatches. To address these limitations, we explore Large Language Models (LLMs) as a flexible alternative. LLMs leverage extensive training to interpret context, handle abbreviations, and adapt to legal transitions. We evaluate traditional methods, Hugging Face-based LLMs, and interface-based LLMs (e.g., Microsoft Copilot, Alibaba's Qwen 2.5) using a dataset of 65 Portuguese company cases. Results show traditional methods achieve accuracies over 92% but suffer high false positive rates (20-40%). Interface-based LLMs outperform, achieving accuracies above 93%, F1 scores exceeding 96%, and lower false positives (40-80%).

</details>


### [42] [The Devil behind the mask: An emergent safety vulnerability of Diffusion LLMs](https://arxiv.org/abs/2507.11097)

*Zichen Wen, Jiashu Qu, Dongrui Liu, Zhiyuan Liu, Ruixi Wu, Yicun Yang, Xiangqi Jin, Haoyun Xu, Xuyang Liu, Weijia Li, Chaochao Lu, Jing Shao, Conghui He, Linfeng Zhang*

**Main category:** cs.CL

**Keywords:** diffusion-based language models, jailbreak attack, adversarial prompts, safety mechanisms

**Relevance Score:** 8

**TL;DR:** This paper introduces DIJA, a framework that systematically studies and exploits vulnerabilities in diffusion-based large language models (dLLMs) using adversarial prompts, revealing shortcomings in current safety mechanisms.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety concerns in diffusion-based large language models that are not sufficiently guarded against adversarial prompts, demonstrating the need for improved alignment mechanisms.

**Method:** The authors developed DIJA, a systematic framework that creates adversarial interleaved mask-text prompts to exploit dLLMs' safety weaknesses due to bidirectional modeling and parallel decoding.

**Key Contributions:**

	1. Introduction of DIJA as a novel jailbreak attack framework for dLLMs
	2. Comprehensive demonstrations of the framework outperforming existing methods
	3. Identification of specific safety weaknesses in dLLMs related to alignment mechanisms

**Result:** DIJA significantly outperforms previous jailbreak methods, achieving up to 100% keyword-based ASR and surpassing prior benchmarks in the effectiveness of jailbreak prompts without content rewriting.

**Limitations:** 

**Conclusion:** The study highlights critical vulnerabilities in dLLMs, emphasizing an urgent need to rethink safety alignment in these models to ensure their safe deployment in applications.

**Abstract:** Diffusion-based large language models (dLLMs) have recently emerged as a powerful alternative to autoregressive LLMs, offering faster inference and greater interactivity via parallel decoding and bidirectional modeling. However, despite strong performance in code generation and text infilling, we identify a fundamental safety concern: existing alignment mechanisms fail to safeguard dLLMs against context-aware, masked-input adversarial prompts, exposing novel vulnerabilities. To this end, we present DIJA, the first systematic study and jailbreak attack framework that exploits unique safety weaknesses of dLLMs. Specifically, our proposed DIJA constructs adversarial interleaved mask-text prompts that exploit the text generation mechanisms of dLLMs, i.e., bidirectional modeling and parallel decoding. Bidirectional modeling drives the model to produce contextually consistent outputs for masked spans, even when harmful, while parallel decoding limits model dynamic filtering and rejection sampling of unsafe content. This causes standard alignment mechanisms to fail, enabling harmful completions in alignment-tuned dLLMs, even when harmful behaviors or unsafe instructions are directly exposed in the prompt. Through comprehensive experiments, we demonstrate that DIJA significantly outperforms existing jailbreak methods, exposing a previously overlooked threat surface in dLLM architectures. Notably, our method achieves up to 100% keyword-based ASR on Dream-Instruct, surpassing the strongest prior baseline, ReNeLLM, by up to 78.5% in evaluator-based ASR on JailbreakBench and by 37.7 points in StrongREJECT score, while requiring no rewriting or hiding of harmful content in the jailbreak prompt. Our findings underscore the urgent need for rethinking safety alignment in this emerging class of language models. Code is available at https://github.com/ZichenWen1/DIJA.

</details>


### [43] [Multi-Trigger Poisoning Amplifies Backdoor Vulnerabilities in LLMs](https://arxiv.org/abs/2507.11112)

*Sanhanat Sivapiromrat, Caiqi Zhang, Marco Basaldella, Nigel Collier*

**Main category:** cs.CL

**Keywords:** Large Language Models, Data poisoning, Backdoor triggers, Mitigation method, Model security

**Relevance Score:** 8

**TL;DR:** This paper presents a framework to study data poisoning attacks in Large Language Models (LLMs), revealing that multiple backdoor triggers can coexist without interference and introducing a mitigation method through selective retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the vulnerabilities of LLMs to data poisoning attacks, specifically the interactions of multiple triggers within a model and the mechanisms of activation.

**Method:** A framework analyzing how distinct backdoor triggers can operate concurrently within LLMs, along with a post hoc recovery method based on layer-wise weight difference analysis for mitigation.

**Key Contributions:**

	1. Framework for studying multiple trigger interactions in LLMs
	2. Demonstration of robust activation of poisoned triggers
	3. Post hoc recovery method for defense against multi-trigger poisoning

**Result:** Demonstrated that multiple triggers can coexist without interference, achieving activation even with altered token arrangements, exposing a broader vulnerability in LLMs.

**Limitations:** Focuses primarily on multi-trigger scenarios and may not cover all types of attacks.

**Conclusion:** The proposed recovery method effectively mitigates the threat of multi-trigger poisoning with minimal parameter updates, enhancing model security.

**Abstract:** Recent studies have shown that Large Language Models (LLMs) are vulnerable to data poisoning attacks, where malicious training examples embed hidden behaviours triggered by specific input patterns. However, most existing works assume a phrase and focus on the attack's effectiveness, offering limited understanding of trigger mechanisms and how multiple triggers interact within the model. In this paper, we present a framework for studying poisoning in LLMs. We show that multiple distinct backdoor triggers can coexist within a single model without interfering with each other, enabling adversaries to embed several triggers concurrently. Using multiple triggers with high embedding similarity, we demonstrate that poisoned triggers can achieve robust activation even when tokens are substituted or separated by long token spans. Our findings expose a broader and more persistent vulnerability surface in LLMs. To mitigate this threat, we propose a post hoc recovery method that selectively retrains specific model components based on a layer-wise weight difference analysis. Our method effectively removes the trigger behaviour with minimal parameter updates, presenting a practical and efficient defence against multi-trigger poisoning.

</details>


### [44] [MSA at ImageCLEF 2025 Multimodal Reasoning: Multilingual Multimodal Reasoning With Ensemble Vision Language Models](https://arxiv.org/abs/2507.11114)

*Seif Ahmed, Mohamed T. Younes, Abdelrahman Moustafa, Abdelrahman Allam, Hamza Moustafa*

**Main category:** cs.CL

**Keywords:** multilingual reasoning, ensemble methods, prompt engineering

**Relevance Score:** 6

**TL;DR:** An ensemble system for multilingual multimodal reasoning achieves top results in the ImageCLEF 2025 EXAMS V challenge, demonstrating effective prompt strategies and cross-lingual training.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance multilingual reasoning capabilities in educational settings using a robust ensemble-based system.

**Method:** The approach integrates various models (Gemini 2.5 Flash, Gemini 1.5 Pro, Gemini 2.5 Pro) and utilizes few-shot and zero-shot prompts in an ablation study to evaluate effectiveness.

**Key Contributions:**

	1. Introduction of an ensemble-based multilingual reasoning system.
	2. Demonstration of effective prompt design leading to improved accuracy.
	3. Achievement of top performance in a competitive multilingual reasoning challenge.

**Result:** Outperformed trained models with a first-place achievement on the leaderboard, scoring 81.4% accuracy overall and leading in 11 out of 13 individual language tracks.

**Limitations:** 

**Conclusion:** Lightweight OCR-VLM ensembles with targeted prompting strategies can outperform heavier models in multilingual tasks.

**Abstract:** We present a robust ensemble-based system for multilingual multimodal reasoning, designed for the ImageCLEF 2025 EXAMS V challenge. Our approach integrates Gemini 2.5 Flash for visual description, Gemini 1.5 Pro for caption refinement and consistency checks, and Gemini 2.5 Pro as a reasoner which handles final answer selection, all coordinated through carefully engineered few-shot and zero-shot prompts. We conducted an extensive ablation study, training several large language models (Gemini 2.5 Flash, Phi 4, Gemma 3, Mistral) on an English dataset and its multilingual augmented version. Additionally, we evaluated Gemini 2.5 Flash in a zero-shot setting for comparison and found it to substantially outperform the trained models. Prompt design also proved critical: enforcing concise, language-normalized formats and prohibiting explanatory text boosted model accuracy on the English validation set from 55.9% to 61.7%. On the official leaderboard, our system (Team MSA) achieved first place overall in the multilingual track with 81.4% accuracy, and led 11 out of 13 individual language tracks, with top results such as 95.07% for Croatian and 92.12% for Italian. These findings highlight that lightweight OCR-VLM ensembles, when paired with precise prompt strategies and cross-lingual augmentation, can outperform heavier end-to-end models in high-stakes, multilingual educational settings.

</details>


### [45] [What Should LLMs Forget? Quantifying Personal Data in LLMs for Right-to-Be-Forgotten Requests](https://arxiv.org/abs/2507.11128)

*Dimitri Staufer*

**Main category:** cs.CL

**Keywords:** Large Language Models, Right to Be Forgotten, data privacy, machine unlearning, WikiMem

**Relevance Score:** 9

**TL;DR:** This paper introduces WikiMem, a novel dataset and a model-agnostic metric to identify memorized personal data in large language models (LLMs) for compliance with GDPR's Right to Be Forgotten.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Concerns about LLMs revealing personal information and the need for compliance with GDPR's RTBF.

**Method:** Development of WikiMem, a dataset of 5,000 natural language canaries and a metric to evaluate individual-fact associations in LLMs using negative log-likelihood for ground-truth and counterfactual comparisons.

**Key Contributions:**

	1. Introduction of a new dataset (WikiMem) comprising natural language canaries.
	2. A model-agnostic metric for quantifying human-fact associations in LLMs.
	3. Empirical evaluation linking memorization in LLMs to web presence and model size.

**Result:** Evaluation of 200 individuals across 15 LLMs demonstrates a correlation between memorization and both subject web presence and model scale.

**Limitations:** The focus on known models and may not generalize to all LLM architectures.

**Conclusion:** This work lays the groundwork for identifying and managing memorized personal data in LLMs, facilitating machine unlearning and RTBF processes.

**Abstract:** Large Language Models (LLMs) can memorize and reveal personal information, raising concerns regarding compliance with the EU's GDPR, particularly the Right to Be Forgotten (RTBF). Existing machine unlearning methods assume the data to forget is already known but do not address how to identify which individual-fact associations are stored in the model. Privacy auditing techniques typically operate at the population level or target a small set of identifiers, limiting applicability to individual-level data inquiries. We introduce WikiMem, a dataset of over 5,000 natural language canaries covering 243 human-related properties from Wikidata, and a model-agnostic metric to quantify human-fact associations in LLMs. Our approach ranks ground-truth values against counterfactuals using calibrated negative log-likelihood across paraphrased prompts. We evaluate 200 individuals across 15 LLMs (410M-70B parameters), showing that memorization correlates with subject web presence and model scale. We provide a foundation for identifying memorized personal data in LLMs at the individual level, enabling the dynamic construction of forget sets for machine unlearning and RTBF requests.

</details>


### [46] [Temperature and Persona Shape LLM Agent Consensus With Minimal Accuracy Gains in Qualitative Coding](https://arxiv.org/abs/2507.11198)

*Conrad Borchers, Bahar Shahrokhian, Francesco Balzan, Elham Tajik, Sreecharan Sankaranarayanan, Sebastian Simon*

**Main category:** cs.CL

**Keywords:** large language models, multi-agent systems, coding accuracy, qualitative research, open-source

**Relevance Score:** 9

**TL;DR:** This study examines the effectiveness of multi-agent systems (MAS) using large language models (LLMs) for qualitative research coding, revealing that single-agent coding often outperforms MAS in accuracy.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the benefits and limitations of using multi-agent systems in qualitative research coding, particularly in relation to coding accuracy and consensus-building among different agent personas and temperatures.

**Method:** An experimental study using an open-source MAS with six LLMs of varying sizes, investigating the impacts of agent persona and temperature on the consensus-building process and coding accuracy from a dataset of over 77,000 decisions.

**Key Contributions:**

	1. Experimental insights on the effectiveness of MAS vs. single-agent systems for coding research
	2. Challenging the assumption that diverse agent personas enhance coding outcomes
	3. Open-sourcing of MAS and experimentation code for further research

**Result:** While temperature influenced consensus-building, it did not improve coding accuracy; single-agent systems frequently outperformed MAS, with only one LLM showing minor improvements under specific conditions.

**Limitations:** Limited to the specific configurations and datasets used in the study; findings may not generalize across all qualitative coding tasks.

**Conclusion:** MAS does not demonstrably improve coding accuracy over single-agent systems; however, it may help clarify ambiguous coding scenarios and improve human-AI interactions.

**Abstract:** Large Language Models (LLMs) enable new possibilities for qualitative research at scale, including coding and data annotation. While multi-agent systems (MAS) can emulate human coding workflows, their benefits over single-agent coding remain poorly understood. We conducted an experimental study of how agent persona and temperature shape consensus-building and coding accuracy of dialog segments based on a codebook with 8 codes. Our open-source MAS mirrors deductive human coding through structured agent discussion and consensus arbitration. Using six open-source LLMs (with 3 to 32 billion parameters) and 18 experimental configurations, we analyze over 77,000 coding decisions against a gold-standard dataset of human-annotated transcripts from online math tutoring sessions. Temperature significantly impacted whether and when consensus was reached across all six LLMs. MAS with multiple personas (including neutral, assertive, or empathetic), significantly delayed consensus in four out of six LLMs compared to uniform personas. In three of those LLMs, higher temperatures significantly diminished the effects of multiple personas on consensus. However, neither temperature nor persona pairing lead to robust improvements in coding accuracy. Single agents matched or outperformed MAS consensus in most conditions. Only one model (OpenHermesV2:7B) and code category showed above-chance gains from MAS deliberation when temperature was 0.5 or lower and especially when the agents included at least one assertive persona. Qualitative analysis of MAS collaboration for these configurations suggests that MAS may nonetheless aid in narrowing ambiguous code applications that could improve codebooks and human-AI coding. We contribute new insight into the limits of LLM-based qualitative methods, challenging the notion that diverse MAS personas lead to better outcomes. We open-source our MAS and experimentation code.

</details>


### [47] [EsBBQ and CaBBQ: The Spanish and Catalan Bias Benchmarks for Question Answering](https://arxiv.org/abs/2507.11216)

*Valle Ruiz-Fernández, Mario Mina, Júlia Falcão, Luis Vasquez-Reina, Anna Sallés, Aitor Gonzalez-Agirre, Olatz Perez-de-Viñaspre*

**Main category:** cs.CL

**Keywords:** Large Language Models, Bias Evaluation, Spanish, Catalan, Question Answering

**Relevance Score:** 8

**TL;DR:** This paper introduces the Spanish and Catalan Bias Benchmarks for Question Answering (EsBBQ and CaBBQ) to evaluate social biases in LLMs in Spain.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for resources to evaluate social biases in non-English languages and different social contexts.

**Method:** The paper presents two datasets designed for question answering, assessing social bias across 10 categories using a multiple-choice format.

**Key Contributions:**

	1. Introduction of EsBBQ and CaBBQ datasets for assessing social bias in LLMs in Spanish and Catalan contexts
	2. Evaluation results highlighting the link between QA accuracy and social bias reliance
	3. Expansion of bias evaluation resources beyond English and U.S. contexts

**Result:** LLMs show a tendency to choose incorrect answers in ambiguous scenarios, with high QA accuracy linked to reliance on social biases.

**Limitations:** The study is limited to Spanish and Catalan languages within the context of Spain, which may not generalize globally.

**Conclusion:** Social biases in LLMs persist across different languages and contexts, necessitating robust evaluation frameworks.

**Abstract:** Previous literature has largely shown that Large Language Models (LLMs) perpetuate social biases learnt from their pre-training data. Given the notable lack of resources for social bias evaluation in languages other than English, and for social contexts outside of the United States, this paper introduces the Spanish and the Catalan Bias Benchmarks for Question Answering (EsBBQ and CaBBQ). Based on the original BBQ, these two parallel datasets are designed to assess social bias across 10 categories using a multiple-choice QA setting, now adapted to the Spanish and Catalan languages and to the social context of Spain. We report evaluation results on different LLMs, factoring in model family, size and variant. Our results show that models tend to fail to choose the correct answer in ambiguous scenarios, and that high QA accuracy often correlates with greater reliance on social biases.

</details>


### [48] [An Agentic Flow for Finite State Machine Extraction using Prompt Chaining](https://arxiv.org/abs/2507.11222)

*Fares Wael, Youssef Maklad, Ali Hamdi, Wael Elsersy*

**Main category:** cs.CL

**Keywords:** Finite-State Machines, Large Language Models, Protocol Analysis, Cybersecurity, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper presents FlowFSM, an agent-based framework using Large Language Models (LLMs) for extracting finite-state machines (FSMs) from protocol specifications, improving accuracy and minimizing hallucinations.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Existing FSM extraction techniques are limited in scalability and coverage, leading to issues in modeling network protocols effectively.

**Method:** FlowFSM leverages LLMs in conjunction with prompt chaining and chain-of-thought reasoning to extract FSMs from raw RFC documents, systematically processing protocol specifications to identify state transitions.

**Key Contributions:**

	1. Introduction of FlowFSM framework for FSM extraction using LLMs
	2. Demonstration of high precision in FSM extraction
	3. Innovative use of prompt chaining and reasoning with LLMs

**Result:** FlowFSM demonstrates high extraction precision and significantly reduces hallucinated transitions across FTP and RTSP protocols during experimental evaluation.

**Limitations:** 

**Conclusion:** The study reveals the potential of using agent-based LLM systems for enhancing protocol analysis and FSM inference, particularly for applications in cybersecurity and reverse engineering.

**Abstract:** Finite-State Machines (FSMs) are critical for modeling the operational logic of network protocols, enabling verification, analysis, and vulnerability discovery. However, existing FSM extraction techniques face limitations such as scalability, incomplete coverage, and ambiguity in natural language specifications. In this paper, we propose FlowFSM, a novel agentic framework that leverages Large Language Models (LLMs) combined with prompt chaining and chain-of-thought reasoning to extract accurate FSMs from raw RFC documents. FlowFSM systematically processes protocol specifications, identifies state transitions, and constructs structured rule-books by chaining agent outputs. Experimental evaluation across FTP and RTSP protocols demonstrates that FlowFSM achieves high extraction precision while minimizing hallucinated transitions, showing promising results. Our findings highlight the potential of agent-based LLM systems in the advancement of protocol analysis and FSM inference for cybersecurity and reverse engineering applications.

</details>


### [49] [Sparse Autoencoders Can Capture Language-Specific Concepts Across Diverse Languages](https://arxiv.org/abs/2507.11230)

*Lyzander Marciano Andrylie, Inaya Rahmanisa, Mahardika Krisna Ihsani, Alfan Farizki Wicaksono, Haryo Akbarianto Wibowo, Alham Fikri Aji*

**Main category:** cs.CL

**Keywords:** large language models, sparse autoencoders, language-specific features, multilingual processing, NLP

**Relevance Score:** 9

**TL;DR:** This paper presents SAE-LAPE, a method for identifying language-specific features in large language models (LLMs) using sparse autoencoders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a better understanding of how large language models process multilingual data by isolating language-specific features from cross-lingual representations.

**Method:** The authors utilize sparse autoencoders (SAEs) with a focus on feature activation probability to identify and analyze language-specific features in LLMs.

**Key Contributions:**

	1. Introduction of SAE-LAPE for identifying language-specific features in LLMs
	2. Demonstration of interpretable language-specific features found in intermediate model layers
	3. Achievement of competitive language identification performance compared to fastText.

**Result:** The proposed method, SAE-LAPE, reveals that many language-specific features appear in the model's middle to final layers, are interpretable, and can effectively identify languages with performance similar to fastText.

**Limitations:** 

**Conclusion:** The findings suggest that recognizing language-specific features can enhance our understanding of LLMs and contribute to improved multilingual performance and language identification tasks.

**Abstract:** Understanding the multilingual mechanisms of large language models (LLMs) provides insight into how they process different languages, yet this remains challenging. Existing studies often focus on individual neurons, but their polysemantic nature makes it difficult to isolate language-specific units from cross-lingual representations. To address this, we explore sparse autoencoders (SAEs) for their ability to learn monosemantic features that represent concrete and abstract concepts across languages in LLMs. While some of these features are language-independent, the presence of language-specific features remains underexplored. In this work, we introduce SAE-LAPE, a method based on feature activation probability, to identify language-specific features within the feed-forward network. We find that many such features predominantly appear in the middle to final layers of the model and are interpretable. These features influence the model's multilingual performance and language output and can be used for language identification with performance comparable to fastText along with more interpretability. Our code is available at https://github.com/LyzanderAndrylie/language-specific-features .

</details>


### [50] [KV-Latent: Dimensional-level KV Cache Reduction with Frequency-aware Rotary Positional Embedding](https://arxiv.org/abs/2507.11273)

*Luohe Shi, Zuchao Li, Lefei Zhang, Guoming Liu, Baoyuan Qi, Hai Zhao*

**Main category:** cs.CL

**Keywords:** Large Language Models, Key-Value Cache, Inference Efficiency, Transformer Decoders, Rotary Positional Embedding

**Relevance Score:** 8

**TL;DR:** This paper introduces a new paradigm called KV-Latent to enhance the efficiency of Transformer Decoders in conversational AI by down-sampling Key-Value caches, leading to reduced memory usage and faster inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To tackle the efficiency bottlenecks related to memory consumption and data transfer bandwidth in Transformer Decoder architectures used for conversational generative AI.

**Method:** The proposed KV-Latent paradigm downsamples Key-Value vector dimensions into a latent space, complemented by modifications to the Rotary Positional Embedding frequency sampling mechanism.

**Key Contributions:**

	1. Introduction of the KV-Latent paradigm for Key-Value cache optimization.
	2. Enhancement of Rotary Positional Embedding stability in lower dimensions.
	3. Empirical evidence showing improved model performance with reduced Key-Value components.

**Result:** The experiments demonstrated that the KV-Latent approach reduces KV Cache size and enhances inference speed, with a minor increase in training cost.

**Limitations:** The additional training required is less than 1% of pre-training, but the specific scenarios in which the improvements apply need further study.

**Conclusion:** The findings suggest that KV-Latent enables the creation of more efficient language models, paving the way for improved KV Cache management in LLMs.

**Abstract:** Large language models (LLMs) based on Transformer Decoders have become the preferred choice for conversational generative AI. Despite the overall superiority of the Decoder architecture, the gradually increasing Key-Value (KV) cache during inference has emerged as a primary efficiency bottleneck, both in aspects of memory consumption and data transfer bandwidth limitations. To address these challenges, we propose a paradigm called KV-Latent. By down-sampling the Key-Value vector dimensions into a latent space, we can significantly reduce the KV Cache footprint and improve inference speed, only with a small amount of extra training, less than 1\% of pre-training takes. Besides, we enhanced the stability of Rotary Positional Embedding applied on lower-dimensional vectors by modifying its frequency sampling mechanism, avoiding noise introduced by higher frequencies while retaining position attenuation. Our experiments, including both models with Grouped Query Attention and those without, have yielded satisfactory results. Finally, we conducted comparative experiments to study the impact of separately reducing Key and Value components on model's performance. Our approach allows for the construction of more efficient language model systems, and opens the new possibility on KV Cache saving and efficient LLMs. Our code is available at https://github.com/ShiLuohe/KV-Latent.

</details>


### [51] [FMC: Formalization of Natural Language Mathematical Competition Problems](https://arxiv.org/abs/2507.11275)

*Jiaxuan Xie, Chengwu Liu, Ye Yuan, Siqi Li, Zhiping Xiao, Ming Zhang*

**Main category:** cs.CL

**Keywords:** autoformalization, large language models, formal reasoning, dataset creation, benchmarking

**Relevance Score:** 8

**TL;DR:** This paper presents an autoformalization pipeline using large language models to create a dataset of natural language mathematical problems formalized in Lean, achieving a fully automated and training-free approach to formal mathematical reasoning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The research aims to enhance formal mathematical reasoning through efficient autoformalization methods that utilize large datasets of natural language mathematical problems.

**Method:** The paper introduces an autoformalization pipeline based on large language models that incorporates error feedback to formalize natural language problems into a formal language without the need for training.

**Key Contributions:**

	1. Development of an automated and training-free autoformalization pipeline
	2. Creation of a high-quality benchmark dataset for formal reasoning
	3. Empirical investigation of LLM capabilities in formalization tasks

**Result:** A curated dataset consisting of 3,922 natural language mathematical problems and 9,787 corresponding formalizations in Lean was created, with 64.46% rated above-average quality, proving valuable for benchmarking automated theorem provers.

**Limitations:** 

**Conclusion:** The findings demonstrate the effectiveness of LLMs and techniques such as few-shot learning and error feedback in improving the autoformalization process, making the dataset a valuable benchmark for future work in formal reasoning tasks.

**Abstract:** Efficient and accurate autoformalization methods, which leverage large-scale datasets of extensive natural language mathematical problems to construct formal language datasets, are key to advancing formal mathematical reasoning. In this paper, we propose an autoformalization pipeline based on large language models with error feedback, achieving a fully automatic and training-free formalization approach. Using this pipeline, we curate an Olympiad-level dataset aligning natural language problems with Lean formalizations. The dataset comprises $3,922$ mathematical problems in natural language and $9,787$ in Lean, of which $64.46\%$ were assessed as at least above-average quality, making it suitable as a benchmark for automated theorem provers. Additionally, we investigate the formalization and reasoning capabilities of various LLMs and empirically demonstrate that few-shot learning, error feedback, and increasing sampling numbers enhance the autoformalization process. Experiments of three automated theorem provers on the \dataset\ dataset also highlight its challenging nature and its value as a benchmark for formal reasoning tasks.

</details>


### [52] [Fine-Grained Chinese Hate Speech Understanding: Span-Level Resources, Coded Term Lexicon, and Enhanced Detection Frameworks](https://arxiv.org/abs/2507.11292)

*Zewen Bai, Liang Yang, Shengdi Yin, Yuanyuan Sun, Hongfei Lin*

**Main category:** cs.CL

**Keywords:** hate speech detection, Chinese language, interpretability, machine learning, toxic comment identification

**Relevance Score:** 6

**TL;DR:** The paper addresses the challenges of Chinese hate speech detection by introducing a new annotated dataset and proposing methods to improve model interpretability and performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective Chinese hate speech detection is critical due to its societal harm, yet current research is limited by the lack of fine-grained datasets and interpretability issues.

**Method:** The study introduces the STATE ToxiCN dataset and evaluates existing models on this dataset, alongside a comprehensive exploration of coded hate speech and a method to integrate an annotated lexicon into detection models.

**Key Contributions:**

	1. Introduction of the first span-level Chinese hate speech dataset (STATE ToxiCN).
	2. Comprehensive study on the interpretation of coded hate speech in Chinese.
	3. Proposal of a method to integrate an annotated lexicon to improve detection performance.

**Result:** The introduction of the STATE ToxiCN dataset allows for better understanding and evaluation of hate speech detection models, while the proposed methods enhance model performance and interpretability.

**Limitations:** 

**Conclusion:** This work offers important resources that can significantly improve both the detection and understanding of Chinese hate speech in online contexts.

**Abstract:** The proliferation of hate speech has inflicted significant societal harm, with its intensity and directionality closely tied to specific targets and arguments. In recent years, numerous machine learning-based methods have been developed to detect hateful comments on online platforms automatically. However, research on Chinese hate speech detection lags behind, and interpretability studies face two major challenges: first, the scarcity of span-level fine-grained annotated datasets limits models' deep semantic understanding of hate speech; second, insufficient research on identifying and interpreting coded hate speech restricts model explainability in complex real-world scenarios. To address these, we make the following contributions: (1) We introduce the Span-level Target-Aware Toxicity Extraction dataset (STATE ToxiCN), the first span-level Chinese hate speech dataset, and evaluate the hate semantic understanding of existing models using it. (2) We conduct the first comprehensive study on Chinese coded hate terms, LLMs' ability to interpret hate semantics. (3) We propose a method to integrate an annotated lexicon into models, significantly enhancing hate speech detection performance. Our work provides valuable resources and insights to advance the interpretability of Chinese hate speech detection research.

</details>


### [53] [Dr.Copilot: A Multi-Agent Prompt Optimized Assistant for Improving Patient-Doctor Communication in Romanian](https://arxiv.org/abs/2507.11299)

*Andrei Niculae, Adrian Cosma, Cosmin Dumitrache, Emilian Rǎdoi*

**Main category:** cs.CL

**Keywords:** telemedicine, large language models, communication quality, healthcare technology, Romanian language

**Relevance Score:** 8

**TL;DR:** Dr.Copilot is a multi-agent LLM system designed to enhance the communication quality of Romanian-speaking doctors' telemedicine responses, improving user reviews and response quality through interpretable feedback.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for improved communication quality in doctor-patient interactions, especially in telemedicine, where advice quality is often judged by presentation rather than clinical accuracy.

**Method:** Dr.Copilot employs three LLM agents providing feedback along 17 axes, optimized with DSPy, using low-resource Romanian data. It delivers real-time feedback within a telemedicine platform.

**Key Contributions:**

	1. Introduction of a multi-agent LLM system for telemedicine communication
	2. Use of interpretable axes for feedback on response quality
	3. Empirical validation through live deployment with healthcare professionals

**Result:** Empirical evaluations showed measurable improvements in user reviews and response quality from 41 doctors using the system.

**Limitations:** 

**Conclusion:** Dr.Copilot marks a significant advancement in real-world applications of LLMs in Romanian healthcare, emphasizing communication quality in medical advice.

**Abstract:** Text-based telemedicine has become increasingly common, yet the quality of medical advice in doctor-patient interactions is often judged more on how advice is communicated rather than its clinical accuracy. To address this, we introduce Dr.Copilot , a multi-agent large language model (LLM) system that supports Romanian-speaking doctors by evaluating and enhancing the presentation quality of their written responses. Rather than assessing medical correctness, Dr.Copilot provides feedback along 17 interpretable axes. The system comprises of three LLM agents with prompts automatically optimized via DSPy. Designed with low-resource Romanian data and deployed using open-weight models, it delivers real-time specific feedback to doctors within a telemedicine platform. Empirical evaluations and live deployment with 41 doctors show measurable improvements in user reviews and response quality, marking one of the first real-world deployments of LLMs in Romanian medical settings.

</details>


### [54] [Internal Value Alignment in Large Language Models through Controlled Value Vector Activation](https://arxiv.org/abs/2507.11316)

*Haoran Jin, Meng Li, Xiting Wang, Zhihao Xu, Minlie Huang, Yantao Jia, Defu Lian*

**Main category:** cs.CL

**Keywords:** Large Language Models, Value Alignment, Human Values, Machine Learning, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** This paper introduces the Controlled Value Vector Activation (ConVA) method for aligning Large Language Models (LLMs) with human values, ensuring model performance while providing clarity and adaptability. It includes an innovative context-controlled value vector identification process and a gated activation method.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this work is to align LLMs with human values to enhance clarity, transparency, and adaptability in various scenarios.

**Method:** The paper presents a Controlled Value Vector Activation (ConVA) method that interprets how values are encoded in LLMs' latent representations and modifies activations to ensure value consistency. It includes a context-controlled value vector identification method and a gated value vector activation approach.

**Key Contributions:**

	1. Introduction of Controlled Value Vector Activation (ConVA) method
	2. Context-controlled value vector identification method
	3. Gated value vector activation for efficient value control

**Result:** Experiments demonstrate that the ConVA method achieves the highest control success rate across 10 basic values while maintaining LLM performance and fluency, even with adversarial prompts.

**Limitations:** 

**Conclusion:** The study concludes that the proposed ConVA method effectively controls LLM values without compromising performance, paving the way for more aligned and responsible AI systems.

**Abstract:** Aligning Large Language Models (LLMs) with human values has attracted increasing attention since it provides clarity, transparency, and the ability to adapt to evolving scenarios. In this paper, we introduce a Controlled Value Vector Activation (ConVA) method that directly aligns the internal values of LLMs by interpreting how a value is encoded in their latent representations and modifies relevant activations to ensure consistent values in LLMs. To ensure an accurate and unbiased interpretation, we propose a context-controlled value vector identification method. To consistently control values without sacrificing model performance, we introduce a gated value vector activation method for effective and minimum degree of value control. Experiments show that our method achieves the highest control success rate across 10 basic values without hurting LLM performance and fluency, and ensures target values even with opposite and potentially malicious input prompts. Source code and data are available at~ https://github.com/hr-jin/ConVA.

</details>


### [55] [Automated Novelty Evaluation of Academic Paper: A Collaborative Approach Integrating Human and Large Language Model Knowledge](https://arxiv.org/abs/2507.11330)

*Wenqing Wu, Chengzhi Zhang, Yi Zhao*

**Main category:** cs.CL

**Keywords:** Novelty, Human-Computer Interaction, Large Language Models, Method Prediction, Sparse-Attention

**Relevance Score:** 9

**TL;DR:** The paper addresses the limitations in assessing novelty in academic papers by integrating the expertise of human reviewers with large language models (LLMs) to evaluate method novelty more effectively.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Novelty is a key criterion in peer review, but both expert judgment and existing citation methods have limitations.

**Method:** The study proposes combining human knowledge and LLMs to assist pretrained language models (PLMs) in predicting the novelty of academic methods. This involves extracting sentences from peer review reports and summarizing methodology sections for fine-tuning PLMs, utilizing a text-guided fusion module with Sparse-Attention.

**Key Contributions:**

	1. Integration of LLM and human expertise for novelty assessment
	2. Development of a text-guided fusion module with Sparse-Attention
	3. Improvement over baseline methods in method novelty prediction

**Result:** The proposed method outperforms numerous baselines in predicting method novelty in academic papers.

**Limitations:** 

**Conclusion:** Integrating human and LLM insights significantly enhances the assessment of novelty in academic publishing.

**Abstract:** Novelty is a crucial criterion in the peer review process for evaluating academic papers. Traditionally, it's judged by experts or measure by unique reference combinations. Both methods have limitations: experts have limited knowledge, and the effectiveness of the combination method is uncertain. Moreover, it's unclear if unique citations truly measure novelty. The large language model (LLM) possesses a wealth of knowledge, while human experts possess judgment abilities that the LLM does not possess. Therefore, our research integrates the knowledge and abilities of LLM and human experts to address the limitations of novelty assessment. The most common novelty in academic papers is the introduction of new methods. In this paper, we propose leveraging human knowledge and LLM to assist pretrained language models (PLMs, e.g. BERT etc.) in predicting the method novelty of papers. Specifically, we extract sentences related to the novelty of the academic paper from peer review reports and use LLM to summarize the methodology section of the academic paper, which are then used to fine-tune PLMs. In addition, we have designed a text-guided fusion module with novel Sparse-Attention to better integrate human and LLM knowledge. We compared the method we proposed with a large number of baselines. Extensive experiments demonstrate that our method achieves superior performance.

</details>


### [56] [What is the Best Process Model Representation? A Comparative Analysis for Process Modeling with Large Language Models](https://arxiv.org/abs/2507.11356)

*Alexis Brissard, Frédéric Cuppens, Amal Zouaq*

**Main category:** cs.CL

**Keywords:** Process Modeling, Large Language Models, Process Model Representations, Empirical Study, AI4BPM

**Relevance Score:** 8

**TL;DR:** This paper presents an empirical study evaluating various Process Model Representations (PMRs) for Process Modeling tasks using Large Language Models (LLMs), introducing a new dataset and comparing PMR suitability and performance.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To systematically compare diverse Process Model Representations (PMRs) used in conjunction with Large Language Models for Process Model Generation, as existing comparisons are lacking.

**Method:** An empirical study utilizing the PMo Dataset containing 55 process descriptions and models in nine PMRs, evaluating PMRs on their suitability for LLM-based Process Modeling and their performance in Process Model Generation.

**Key Contributions:**

	1. Introduction of the PMo Dataset with 55 process descriptions and nine PMRs.
	2. First empirical evaluation of multiple PMRs in the context of LLMs for Process Modeling.
	3. Identification of the top-performing PMRs based on suitability and generation performance.

**Result:** Mermaid shows the highest score across six criteria of Process Modeling, while BPMN text achieves the best results for process element similarity in Process Model Generation.

**Limitations:** The study is limited to only nine PMRs and specific process descriptions, which may not encompass all potential model representations.

**Conclusion:** The findings highlight the varying effectiveness of different PMRs for LLM-based Process Modeling, underscoring the importance of systematic evaluation.

**Abstract:** Large Language Models (LLMs) are increasingly applied for Process Modeling (PMo) tasks such as Process Model Generation (PMG). To support these tasks, researchers have introduced a variety of Process Model Representations (PMRs) that serve as model abstractions or generation targets. However, these PMRs differ widely in structure, complexity, and usability, and have never been systematically compared. Moreover, recent PMG approaches rely on distinct evaluation strategies and generation techniques, making comparison difficult. This paper presents the first empirical study that evaluates multiple PMRs in the context of PMo with LLMs. We introduce the PMo Dataset, a new dataset containing 55 process descriptions paired with models in nine different PMRs. We evaluate PMRs along two dimensions: suitability for LLM-based PMo and performance on PMG. \textit{Mermaid} achieves the highest overall score across six PMo criteria, whereas \textit{BPMN text} delivers the best PMG results in terms of process element similarity.

</details>


### [57] [Addressing Data Imbalance in Transformer-Based Multi-Label Emotion Detection with Weighted Loss](https://arxiv.org/abs/2507.11384)

*Xia Cui*

**Main category:** cs.CL

**Keywords:** Emotion Detection, Weighted Loss Function, Transformer Models, Multilabel Classification, Data Imbalance

**Relevance Score:** 6

**TL;DR:** This paper investigates using a weighted loss function to improve emotion detection in multi-label settings for Transformer models, specifically targeting class imbalance in data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance performance in multi-label emotion detection tasks by addressing class imbalance in datasets without the need for traditional resampling methods.

**Method:** A weighted loss function is applied to Transformer-based models (BERT, RoBERTa, BART) for analyzing performance on the BRIGHTER dataset, focusing on multiple evaluation metrics.

**Key Contributions:**

	1. Proposed a simple weighted loss function for Transformer-based models in emotion detection
	2. Demonstrated the application on the BRIGHTER dataset for SemEval-2025
	3. Evaluated results using multiple metrics focusing on both high and low-frequency emotion classes.

**Result:** The introduction of a weighted loss function improved performance on high-frequency emotion classes but had limited effects on the minority classes.

**Limitations:** The weighted loss function shows limited impact on minority emotion classes despite improvements in overall performance for high-frequency classes.

**Conclusion:** While the weighted loss function shows promise in handling data imbalance, it also reveals ongoing challenges in effectively improving predictions for underrepresented classes.

**Abstract:** This paper explores the application of a simple weighted loss function to Transformer-based models for multi-label emotion detection in SemEval-2025 Shared Task 11. Our approach addresses data imbalance by dynamically adjusting class weights, thereby enhancing performance on minority emotion classes without the computational burden of traditional resampling methods. We evaluate BERT, RoBERTa, and BART on the BRIGHTER dataset, using evaluation metrics such as Micro F1, Macro F1, ROC-AUC, Accuracy, and Jaccard similarity coefficients. The results demonstrate that the weighted loss function improves performance on high-frequency emotion classes but shows limited impact on minority classes. These findings underscore both the effectiveness and the challenges of applying this approach to imbalanced multi-label emotion detection.

</details>


### [58] [DCR: Quantifying Data Contamination in LLMs Evaluation](https://arxiv.org/abs/2507.11405)

*Cheng Xu, Nan Yan, Shuhao Guan, Changhong Jin, Yuke Mei, Yibing Guo, M-Tahar Kechadi*

**Main category:** cs.CL

**Keywords:** benchmark data contamination, large language models, data contamination risk, performance metrics, fuzzy inference system

**Relevance Score:** 8

**TL;DR:** This paper presents the Data Contamination Risk (DCR) framework for detecting and quantifying benchmark data contamination in large language models (LLMs).

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses concerns regarding benchmark data contamination (BDC) in LLMs, which can inflate performance metrics and affect genuine generalization assessments.

**Method:** The DCR framework detects BDC at four levels: semantic, informational, data, and label, utilizing a fuzzy inference system to synthesize contamination scores and produce a unified DCR Factor.

**Key Contributions:**

	1. Introduction of the DCR framework for BDC detection and quantification
	2. Fuzzy inference system for synthesizing contamination scores
	3. Practical implementation for fairer LLM benchmarking

**Result:** Validated on 9 LLMs, the DCR framework reliably measures contamination severity and adjusts accuracy to within 4% error compared to uncontaminated baselines across various tasks.

**Limitations:** 

**Conclusion:** DCR enhances the credibility of LLM benchmarking by providing a computationally efficient and transparent tool for routine contamination assessments.

**Abstract:** The rapid advancement of large language models (LLMs) has heightened concerns about benchmark data contamination (BDC), where models inadvertently memorize evaluation data, inflating performance metrics and undermining genuine generalization assessment. This paper introduces the Data Contamination Risk (DCR) framework, a lightweight, interpretable pipeline designed to detect and quantify BDC across four granular levels: semantic, informational, data, and label. By synthesizing contamination scores via a fuzzy inference system, DCR produces a unified DCR Factor that adjusts raw accuracy to reflect contamination-aware performance. Validated on 9 LLMs (0.5B-72B) across sentiment analysis, fake news detection, and arithmetic reasoning tasks, the DCR framework reliably diagnoses contamination severity and with accuracy adjusted using the DCR Factor to within 4% average error across the three benchmarks compared to the uncontaminated baseline. Emphasizing computational efficiency and transparency, DCR provides a practical tool for integrating contamination assessment into routine evaluations, fostering fairer comparisons and enhancing the credibility of LLM benchmarking practices.

</details>


### [59] [EXAONE 4.0: Unified Large Language Models Integrating Non-reasoning and Reasoning Modes](https://arxiv.org/abs/2507.11407)

*LG AI Research, :, Kyunghoon Bae, Eunbi Choi, Kibong Choi, Stanley Jungkyu Choi, Yemuk Choi, Kyubeen Han, Seokhee Hong, Junwon Hwang, Taewan Hwang, Joonwon Jang, Hyojin Jeon, Kijeong Jeon, Gerrard Jeongwon Jo, Hyunjik Jo, Jiyeon Jung, Euisoon Kim, Hyosang Kim, Jihoon Kim, Joonkee Kim, Seonghwan Kim, Soyeon Kim, Sunkyoung Kim, Yireun Kim, Yongil Kim, Youchul Kim, Edward Hwayoung Lee, Gwangho Lee, Haeju Lee, Honglak Lee, Jinsik Lee, Kyungmin Lee, Sangha Park, Young Min Paik, Yongmin Park, Youngyong Park, Sanghyun Seo, Sihoon Yang, Heuiyeen Yeen, Sihyuk Yi, Hyeongu Yun*

**Main category:** cs.CL

**Keywords:** EXAONE 4.0, AI models, agentic AI, multilingual capabilities, machine learning

**Relevance Score:** 4

**TL;DR:** EXAONE 4.0 is a new AI model offering both high usability and advanced reasoning capabilities, with enhanced multilingual support and two model sizes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To advance the usability and reasoning capabilities in AI models, paving the way for the agentic AI era.

**Method:** Introduces two modes (Non-reasoning and Reasoning) and provides models in two sizes (32B and 1.2B) for different applications.

**Key Contributions:**

	1. Integration of Non-reasoning and Reasoning modes
	2. Multilingual support including Spanish
	3. Two model sizes for diverse applications

**Result:** EXAONE 4.0 shows improved performance over open-weight models and competes well with frontier-class models.

**Limitations:** 

**Conclusion:** EXAONE 4.0 models are publicly available for research and demonstrate exceptional performance in various applications.

**Abstract:** This technical report introduces EXAONE 4.0, which integrates a Non-reasoning mode and a Reasoning mode to achieve both the excellent usability of EXAONE 3.5 and the advanced reasoning abilities of EXAONE Deep. To pave the way for the agentic AI era, EXAONE 4.0 incorporates essential features such as agentic tool use, and its multilingual capabilities are extended to support Spanish in addition to English and Korean. The EXAONE 4.0 model series consists of two sizes: a mid-size 32B model optimized for high performance, and a small-size 1.2B model designed for on-device applications. The EXAONE 4.0 demonstrates superior performance compared to open-weight models in its class and remains competitive even against frontier-class models. The models are publicly available for research purposes and can be easily downloaded via https://huggingface.co/LGAI-EXAONE.

</details>


### [60] [KisMATH: Do LLMs Have Knowledge of Implicit Structures in Mathematical Reasoning?](https://arxiv.org/abs/2507.11408)

*Soumadeep Saha, Akshay Chaturvedi, Saptarshi Saha, Utpal Garain, Nicholas Asher*

**Main category:** cs.CL

**Keywords:** Causal CoT Graphs, Large Language Models, Mathematical Reasoning, Chain-of-Thought Reasoning, Dataset KisMATH

**Relevance Score:** 9

**TL;DR:** The paper introduces Causal CoT Graphs (CCGs) to elucidate how chain-of-thought reasoning enhances the performance of large language models in problem-solving tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the mechanism behind the performance boosts in large language models (LLMs) when using chain-of-thought reasoning.

**Method:** Causal CoT Graphs (CCGs) are constructed as directed acyclic graphs from reasoning traces, capturing fine-grained causal dependencies. The dataset KisMATH, containing 1671 mathematical reasoning problems and associated CCGs, is analyzed using 15 open-weight LLMs.

**Key Contributions:**

	1. Introduction of Causal CoT Graphs (CCGs) for modeling reasoning processes in LLMs.
	2. Compilation of a new dataset, KisMATH, for evaluating reasoning in mathematics.
	3. Empirical evidence that LLMs leverage structured reasoning paths for enhanced performance.

**Result:** The analysis shows that reasoning nodes in CCGs act as mediators for achieving final answers, indicating that LLMs utilize reasoning paths represented by CCGs.

**Limitations:** 

**Conclusion:** KisMATH facilitates controlled interventions for investigating the influence of chain-of-thought in LLM reasoning, suggesting that LLMs may internally recognize structures similar to CCGs.

**Abstract:** Chain-of-thought traces have been shown to improve performance of large language models in a plethora of reasoning tasks, yet there is no consensus on the mechanism through which this performance boost is achieved. To shed more light on this, we introduce Causal CoT Graphs (CCGs), which are directed acyclic graphs automatically extracted from reasoning traces that model fine-grained causal dependencies in the language model output. A collection of $1671$ mathematical reasoning problems from MATH500, GSM8K and AIME, and their associated CCGs are compiled into our dataset -- \textbf{KisMATH}. Our detailed empirical analysis with 15 open-weight LLMs shows that (i) reasoning nodes in the CCG are mediators for the final answer, a condition necessary for reasoning; and (ii) LLMs emphasise reasoning paths given by the CCG, indicating that models internally realise structures akin to our graphs. KisMATH enables controlled, graph-aligned interventions and opens up avenues for further investigation into the role of chain-of-thought in LLM reasoning.

</details>


### [61] [Seq vs Seq: An Open Suite of Paired Encoders and Decoders](https://arxiv.org/abs/2507.11412)

*Orion Weller, Kathryn Ricci, Marc Marone, Antoine Chaffin, Dawn Lawrie, Benjamin Van Durme*

**Main category:** cs.CL

**Keywords:** language models, encoder-only, decoder-only, natural language processing, machine learning

**Relevance Score:** 9

**TL;DR:** This paper introduces the Ettin suite of models, comparing encoder-only and decoder-only architectures in NLP. It establishes that encoder models outperform decoders for classification and retrieval, while decoders excel in generative tasks. The study provides comprehensive open-source resources for further research.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To compare encoder-only and decoder-only models effectively, addressing the variability in parameters, training techniques, and datasets used in past research.

**Method:** The study introduces the Ettin suite of paired encoder-only and decoder-only models, trained using a consistent methodology across varying sizes (17M to 1B parameters) on up to 2 trillion tokens.

**Key Contributions:**

	1. Introduction of the SOTA open-data Ettin suite of paired models
	2. Demonstration of superior performance of encoder models for classification and retrieval tasks
	3. Provision of open-source artifacts including training data and model checkpoints for future research

**Result:** The results indicate that encoder-only models are superior for classification and retrieval tasks, whereas decoder-only models excel at generative tasks. Continued training a decoder for encoder tasks (and vice versa) yielded subpar results.

**Limitations:** 

**Conclusion:** Using models specifically designed for their tasks (encoder for classification, decoder for generation) is more effective than adapting one type for the tasks of the other.

**Abstract:** The large language model (LLM) community focuses almost exclusively on decoder-only language models, since they are easier to use for text generation. However, a large subset of the community still uses encoder-only models for tasks such as classification or retrieval. Previous work has attempted to compare these architectures, but is forced to make comparisons with models that have different numbers of parameters, training techniques, and datasets. We introduce the SOTA open-data Ettin suite of models: paired encoder-only and decoder-only models ranging from 17 million parameters to 1 billion, trained on up to 2 trillion tokens. Using the same recipe for both encoder-only and decoder-only models produces SOTA recipes in both categories for their respective sizes, beating ModernBERT as an encoder and Llama 3.2 and SmolLM2 as decoders. Like previous work, we find that encoder-only models excel at classification and retrieval tasks while decoders excel at generative tasks. However, we show that adapting a decoder model to encoder tasks (and vice versa) through continued training is subpar compared to using only the reverse objective (i.e. a 400M encoder outperforms a 1B decoder on MNLI, and vice versa for generative tasks). We open-source all artifacts of this study including training data, training order segmented by checkpoint, and 200+ checkpoints to allow future work to analyze or extend all aspects of training.

</details>


### [62] [Reasoning Strategies in Large Language Models: Can They Follow, Prefer, and Optimize?](https://arxiv.org/abs/2507.11423)

*Yanjian Zhang, Guillaume Wisniewski, Nadi Tomeh, Thierry Charnois*

**Main category:** cs.CL

**Keywords:** Large Language Models, Reasoning Strategies, Logical Problem-Solving, Prompting Techniques, Adaptive Learning

**Relevance Score:** 8

**TL;DR:** This paper investigates whether prompting can control the reasoning strategies of large language models (LLMs) to enhance logical problem-solving.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand if LLMs can be guided to use more effective reasoning strategies for diverse problem-solving tasks.

**Method:** The study evaluates the impact of different prompting techniques on LLMs' reasoning strategies and their accuracy in logical problem-solving tasks.

**Key Contributions:**

	1. Investigates control over LLMs' reasoning strategies using prompting
	2. Highlights the importance of adaptive strategy selection for improved performance
	3. Proposes methods for refining LLMs' reasoning capabilities

**Result:** The results indicate that no single reasoning strategy consistently enhances accuracy, but performance improves when models can adaptively select optimal strategies.

**Limitations:** The study acknowledges that results may vary across different LLM architectures and tasks, limiting generalizability.

**Conclusion:** Adaptive strategy selection in LLMs could refine their reasoning abilities, suggesting potential methods for guiding their reasoning processes.

**Abstract:** Human reasoning involves different strategies, each suited to specific problems. Prior work shows that large language model (LLMs) tend to favor a single reasoning strategy, potentially limiting their effectiveness in diverse reasoning challenges. In this work, we investigate whether prompting can control LLMs reasoning strategies and assess its impact on logical problem-solving. While our experiments show that no single strategy consistently improves accuracy, performance could be enhanced if models could adaptively choose the optimal strategy. We propose methods to guide LLMs in strategy selection, highlighting new ways to refine their reasoning abilities.

</details>


### [63] [HKGAI-V1: Towards Regional Sovereign Large Language Model for Hong Kong](https://arxiv.org/abs/2507.11502)

*Sirui Han, Junqi Zhu, Ruiyuan Zhang, Yike Guo*

**Main category:** cs.CL

**Keywords:** large language model, AI alignment, cultural sensitivity, retrieval-augmented generation, regional AI

**Relevance Score:** 7

**TL;DR:** Development of HKGAI-V1, a large language model tailored for Hong Kong's multilingual and cultural context, integrated with a RAG system and an alignment framework.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To establish a value-aligned AI infrastructure specifically for Hong Kong that respects its unique socio-legal context and culture.

**Method:** Developed using the DeepSeek architecture and a comprehensive fine-tuning process, integrated with a retrieval-augmented generation (RAG) system for accurate information access.

**Key Contributions:**

	1. Development of HKGAI-V1 for handling Hong Kong-specific queries.
	2. Creation of the Adversarial HK Value Benchmark for evaluating AI alignment with local standards.
	3. Presentation of a governance-embedded approach to AI sovereignty.

**Result:** HKGAI-V1 outperforms general-purpose models in handling culturally sensitive queries related to Hong Kong, while a proprietary benchmark aids in evaluating model alignment with local standards.

**Limitations:** 

**Conclusion:** The paper outlines a replicable framework for creating AI systems that are deeply rooted in regional identities, enhancing governance in critical sectors.

**Abstract:** This paper presents the development of HKGAI-V1, a foundational sovereign large language model (LLM), developed as part of an initiative to establish value-aligned AI infrastructure specifically tailored for Hong Kong. Addressing the region's unique multilingual environment (Cantonese, Mandarin, and English), its distinct socio-legal context under the "one country, two systems" framework, and specific local cultural and value considerations, the model is built upon the DeepSeek architecture and systematically aligned with regional norms through a multifaceted full parameter fine-tuning process. It is further integrated with a retrieval-augmented generation (RAG) system to ensure timely and factually grounded information access. The core contribution lies in the design and implementation of a comprehensive, region-specific AI alignment and safety framework, demonstrated through two key achievements: 1) The successful development of HKGAI-V1 itself - which outper-forms general-purpose models in handling Hong Kong-specific culturally sensitive queries, and embodies a "governance-embedded" approach to digital sovereignty - empowers Hong Kong to exercise control over AI applications in critical sectors including public services, legal systems, and edu-cation. 2) The development of the proprietary Adversarial HK Value Benchmark, a rigorous tool for evaluating model alignment with local ethical and legal stand-ards under challenging conditions. By documenting these achievements, the paper provides not only a technological artifact but also a replicable blueprint for developing advanced, regionally focused AI systems deeply rooted in their local identities.

</details>


### [64] [Real-World Summarization: When Evaluation Reaches Its Limits](https://arxiv.org/abs/2507.11508)

*Patrícia Schmidtová, Ondřej Dušek, Saad Mahamood*

**Main category:** cs.CL

**Keywords:** LLM, human evaluation, hotel highlights, evaluation metrics, faithfulness

**Relevance Score:** 7

**TL;DR:** This paper evaluates the effectiveness of various methods in assessing the faithfulness of LLM-generated hotel summaries, revealing that simpler metrics perform well despite LLMs generating high-quality content.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the reliability of LLM-generated summaries in faithfully capturing the unique features of hotel accommodations and to compare evaluation methodologies.

**Method:** Utilized human evaluation campaigns with categorical error assessment and span-level annotation, comparing traditional metrics against trainable methods and LLM-driven evaluations.

**Key Contributions:**

	1. Demonstrated the efficacy of simpler metrics in evaluating LLM outputs.
	2. Revealed the limitations of LLMs in evaluation roles.
	3. Analyzed real-world business impacts of incorrect summaries.

**Result:** Simpler metrics like word overlap showed a strong correlation with human judgments (Spearman correlation of 0.63), outperforming complex methods on out-of-domain data.

**Limitations:** LLMs tend to severely under- or over-annotate, compromising evaluation reliability.

**Conclusion:** LLMs are effective in generating highlights but unreliable in evaluation, posing risks with incorrect information; challenges in crowdsourced evaluations were noted.

**Abstract:** We examine evaluation of faithfulness to input data in the context of hotel highlights: brief LLM-generated summaries that capture unique features of accommodations. Through human evaluation campaigns involving categorical error assessment and span-level annotation, we compare traditional metrics, trainable methods, and LLM-as-a-judge approaches. Our findings reveal that simpler metrics like word overlap correlate surprisingly well with human judgments (Spearman correlation rank of 0.63), often outperforming more complex methods when applied to out-of-domain data. We further demonstrate that while LLMs can generate high-quality highlights, they prove unreliable for evaluation as they tend to severely under- or over-annotate. Our analysis of real-world business impacts shows incorrect and non-checkable information pose the greatest risks. We also highlight challenges in crowdsourced evaluations.

</details>


### [65] [Fine-grained Stateful Knowledge Exploration: Effective and Efficient Graph Retrieval with Large Language Models](https://arxiv.org/abs/2401.13444)

*Dehao Tao, Congqi Wang, Feng Huang, Junhao Chen, Yongfeng Huang, Minghu Jiang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Knowledge Graphs, Fine-grained Knowledge Retrieval, Human-Computer Interaction, Natural Language Processing

**Relevance Score:** 9

**TL;DR:** FiSKE is a novel paradigm for Fine-grained Stateful Knowledge Exploration, enhancing the integration of LLMs with external knowledge bases like knowledge graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Updating knowledge in LLMs poses challenges, often resulting in outdated responses. Integrating external knowledge bases can help, but existing methods suffer from granularity mismatches leading to inefficiencies.

**Method:** FiSKE decomposes questions into fine-grained clues and utilizes an adaptive mapping strategy to enhance knowledge exploration and retrieval accuracy.

**Key Contributions:**

	1. Introduction of a fine-grained clue decomposition approach
	2. Development of an adaptive mapping strategy for knowledge exploration
	3. Implementation of a clue-driven termination mechanism for LLM augmentation

**Result:** FiSKE significantly outperforms existing methods in knowledge retrieval and reduces the number of LLM invocations across multiple datasets.

**Limitations:** 

**Conclusion:** The proposed approach balances the precision of knowledge retrieval with efficiency, addressing critical limitations of current methodologies.

**Abstract:** Large Language Models (LLMs) have shown impressive capabilities, yet updating their knowledge remains a significant challenge, often leading to outdated or inaccurate responses. A proposed solution is the integration of external knowledge bases, such as knowledge graphs, with LLMs. Most existing methods use a paradigm that treats the whole question as the objective, with relevant knowledge being incrementally retrieved from the knowledge graph. However, this paradigm often leads to a granularity mismatch between the target question and the retrieved entities and relations. As a result, the information in the question cannot precisely correspond to the retrieved knowledge. This may cause redundant exploration or omission of vital knowledge, thereby leading to enhanced computational consumption and reduced retrieval accuracy. To address the limitations of coarse-grained knowledge exploration, we propose FiSKE, a novel paradigm for Fine-grained Stateful Knowledge Exploration. FiSKE first decomposes questions into fine-grained clues, then employs an adaptive mapping strategy during knowledge exploration process to resolve ambiguity in clue-to-graph mappings. This strategy dynamically infers contextual correspondences while maintaining a stateful record of the mappings. A clue-driven termination mechanism ensures rigorous augmentation--leveraging fully mapped paths for LLMs while reverting to chain-of-thought reasoning when necessary. Our approach balances precision and efficiency. Experiments on multiple datasets revealed that our paradigm surpasses current advanced methods in knowledge retrieval while significantly reducing the average number of LLM invocations.

</details>


### [66] [AIDE: Attribute-Guided MultI-Hop Data Expansion for Data Scarcity in Task-Specific Fine-tuning](https://arxiv.org/abs/2412.06136)

*Jiayu Li, Xuan Zhu, Fang Liu, Yanjun Qi*

**Main category:** cs.CL

**Keywords:** data synthesis, large language models, fine-tuning, multi-hop, machine learning

**Relevance Score:** 9

**TL;DR:** AIDE is a novel framework for synthesizing diverse, relevant training data for fine-tuning large language models using a multi-hop process.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The need for diverse, high-quality training data in fine-tuning large language models is crucial, yet obtaining such data remains challenging.

**Method:** AIDE employs a multi-hop approach to expand a small number of seed data points while ensuring task relevance and data diversity, incorporating a residual connection mechanism to avoid irrelevant data generation.

**Key Contributions:**

	1. Proposes a novel framework for data synthesis for fine-tuning LLMs
	2. Introduces a multi-hop approach with a residual connection mechanism to enhance data relevance
	3. Shows substantial performance improvements over existing data synthesis methods

**Result:** AIDE allows fine-tuning of Mistral-7B, Llama-3.1-8B, and Llama-3.2-3B with as few as 10 seeds, outperforming models trained on human-curated data by over 30% compared to state-of-the-art methods like Evol-Instruct.

**Limitations:** 

**Conclusion:** AIDE demonstrates significant improvement in data synthesis for fine-tuning, proving effective in generating relevant training data from minimal seed inputs.

**Abstract:** Fine-tuning large language models (LLMs) for specific tasks requires diverse, high-quality training data. However, obtaining sufficient relevant data remains a significant challenge. Existing data synthesis methods either depend on extensive seed datasets or struggle to balance task relevance and data diversity. To address these challenges, we propose Attribute-guided multI-hop Data Expansion (AIDE), a novel data synthesis framework that uses a multi-hop process to expand very few seed data points while ensuring data diversity and task relevance. AIDE extracts the main topic and key knowledge attributes from the seeds to guide the synthesis steps. The process repeats for K hops, using the generated data as seeds. To prevent irrelevant data generation as the hop depth increases, AIDE incorporates a residual connection mechanism. Our empirical results show that AIDE enables fine-tuning of Mistral-7B, Llama-3.1-8B and Llama-3.2-3B from 10 seeds, surpassing the models fine-tuned on human curated data. Furthermore, AIDE outperforms state-of-the-art data synthesis methods, such as Evol-Instruct, by over 30% in task-specific fine-tuning. Code is available at https://github.com/Code4Graph/AIDE.

</details>


### [67] [Understanding the Dark Side of LLMs' Intrinsic Self-Correction](https://arxiv.org/abs/2412.14959)

*Qingjie Zhang, Di Wang, Haoting Qian, Yiming Li, Tianwei Zhang, Minlie Huang, Ke Xu, Hewu Li, Yan Liu, Han Qiu*

**Main category:** cs.CL

**Keywords:** LLM, intrinsic self-correction, prompt bias, cognitive bias, fine-tuning

**Relevance Score:** 8

**TL;DR:** This paper investigates the limitations of intrinsic self-correction in LLMs and proposes strategies to mitigate its negative effects.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To interpret the failures of LLMs' intrinsic self-correction across various tasks, as it relies heavily on feedback prompts that are often not available.

**Method:** The study involves conducting experiments with SOTA LLMs on one simple and three complex tasks, employing three interpretation methods to analyze the effects of intrinsic self-correction.

**Key Contributions:**

	1. Analysis of intrinsic self-correction failure in LLMs
	2. Identification of biases introduced by LLM responses
	3. Proposition of mitigation strategies for LLM response issues

**Result:** The findings reveal that intrinsic self-correction can cause instability in LLM responses, leading to prompt bias in simple questions and introducing cognitive bias in complex tasks.

**Limitations:** The study may depend on the specific tasks and LLMs analyzed, limiting the generalizability of the findings.

**Conclusion:** The paper suggests two strategies to alleviate the issues identified: repeating questions and using supervised fine-tuning with a limited number of samples, along with an open-sourced implementation.

**Abstract:** Intrinsic self-correction was proposed to improve LLMs' responses via feedback prompts solely based on their inherent capability. However, recent works show that LLMs' intrinsic self-correction fails without oracle labels as feedback prompts. In this paper, we aim to interpret LLMs' intrinsic self-correction for different tasks, especially for those failure cases. By including one simple task and three complex tasks with state-of-the-art (SOTA) LLMs like ChatGPT families (o1, 4o, 3.5-turbo) and Llama families (2-7B, 3-8B, and 3.1-8B), we design three interpretation methods to reveal the dark side of LLMs' intrinsic self-correction. We identify intrinsic self-correction can (1) cause LLMs to waver both intermedia and final answers and lead to prompt bias on simple factual questions; (2) introduce human-like cognitive bias on complex tasks. In light of our findings, we also provide two simple yet effective strategies for alleviation: question repeating and supervised fine-tuning with a few samples. We open-source our work at https://x-isc.info/.

</details>


### [68] [Plancraft: an evaluation dataset for planning with LLM agents](https://arxiv.org/abs/2412.21033)

*Gautier Dagan, Frank Keller, Alex Lascarides*

**Main category:** cs.CL

**Keywords:** LLM agents, Plancraft, decision-making, tool use, RAG

**Relevance Score:** 8

**TL;DR:** Plancraft is a dataset for evaluating LLM agents with a focus on decision-making and tool use in a Minecraft crafting environment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind Plancraft is to create a dataset that evaluates the capabilities of LLM agents in decision-making and tool use, particularly in a multi-modal interface.

**Method:** Plancraft includes both text-only and multi-modal interfaces and leverages tools like the Minecraft Wiki, a handcrafted planner, and an Oracle Retriever for benchmarking different agent architectures.

**Key Contributions:**

	1. Introduction of the Plancraft dataset for evaluating multi-modal LLM agents
	2. Benchmarking performance of various LLMs against decision-making challenges
	3. Identification of challenges in planning problems faced by current LLM architectures

**Result:** Benchmarking of both open-source and closed-source LLMs shows that they struggle with the challenges posed by Plancraft, particularly in planning and decision-making scenarios.

**Limitations:** Plancraft's scenarios are intentionally unsolvable in some cases, which may limit the applicability of findings to purely solvable environments.

**Conclusion:** The study offers insights and suggestions for improving the planning capabilities of LLMs and VLMs, highlighting their current limitations.

**Abstract:** We present Plancraft, a multi-modal evaluation dataset for LLM agents. Plancraft has both a text-only and multi-modal interface, based on the Minecraft crafting GUI. We include the Minecraft Wiki to evaluate tool use and Retrieval Augmented Generation (RAG), as well as a handcrafted planner and Oracle Retriever, to ablate the different components of a modern agent architecture. To evaluate decision-making, Plancraft also includes a subset of examples that are intentionally unsolvable, providing a realistic challenge that requires the agent not only to complete tasks but also to decide whether they are solvable at all. We benchmark both open-source and closed-source LLMs and compare their performance and efficiency to a handcrafted planner. Overall, we find that LLMs and VLMs struggle with the planning problems that Plancraft introduces, and offer suggestions on how to improve their capabilities.

</details>


### [69] [Comply: Learning Sentences with Complex Weights inspired by Fruit Fly Olfaction](https://arxiv.org/abs/2502.01706)

*Alexei Figueroa, Justus Westerhoff, Golzar Atefi, Dennis Fast, Benjamin Winter, Felix Alexander Gers, Alexander Löser, Wolfgang Nejdl*

**Main category:** cs.CL

**Keywords:** neural networks, word embeddings, sequence representation, biologically inspired models, contextual representations

**Relevance Score:** 6

**TL;DR:** Comply is a neural network model that incorporates positional information to learn sequence representations, outperforming FlyVec while maintaining computational efficiency.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of biologically inspired neural networks like FlyVec, which efficiently learns word embeddings.

**Method:** Introduced Comply, incorporating complex weights for positional information in a single-layer neural network to learn sequence representations.

**Key Contributions:**

	1. Introduction of Comply model for sequence representation learning
	2. Demonstrated improvements over FlyVec
	3. Achieved competitive performance with larger models without extra parameters

**Result:** Comply surpasses the performance of FlyVec and matches larger state-of-the-art models without additional parameters.

**Limitations:** 

**Conclusion:** Comply produces sparse contextual sentence representations that are interpretable through the neuron weights, showing potential for further advancements in modeling data distributions.

**Abstract:** Biologically inspired neural networks offer alternative avenues to model data distributions. FlyVec is a recent example that draws inspiration from the fruit fly's olfactory circuit to tackle the task of learning word embeddings. Surprisingly, this model performs competitively even against deep learning approaches specifically designed to encode text, and it does so with the highest degree of computational efficiency. We pose the question of whether this performance can be improved further. For this, we introduce Comply. By incorporating positional information through complex weights, we enable a single-layer neural network to learn sequence representations. Our experiments show that Comply not only supersedes FlyVec but also performs on par with significantly larger state-of-the-art models. We achieve this without additional parameters. Comply yields sparse contextual representations of sentences that can be interpreted explicitly from the neuron weights.

</details>


### [70] [A Generative Approach to LLM Harmfulness Detection with Special Red Flag Tokens](https://arxiv.org/abs/2502.16366)

*Sophie Xhonneux, David Dobre, Mehrnaz Mofakhami, Leo Schwinn, Gauthier Gidel*

**Main category:** cs.CL

**Keywords:** language models, safety training, red flag token, harmful content, LoRA

**Relevance Score:** 8

**TL;DR:** This paper introduces a novel safety training method for LLMs via the insertion of a red flag token to identify and manage harmful content without sacrificing model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing safety training methods compromise model capabilities by inducing drastic distribution shifts. This approach aims to maintain model utility while ensuring safety.

**Method:** The authors propose a red flag token (<rf>) that the model learns to insert when harmful content is generated or imminent, allowing it to evaluate responses while preserving performance.

**Key Contributions:**

	1. Introduction of the red flag token (<rf>) for harm identification
	2. Maintaining model utility during safety training
	3. Use of LoRA modules for enhanced safety against API attacks

**Result:** The approach enhances model robustness against harmful content generation while maintaining its utility, showing performance comparable to adversarial training without requiring attack methods.

**Limitations:** The approach may require extensive evaluation on various datasets to ensure generalization across different contexts.

**Conclusion:** By encapsulating safety tuning in a LoRA module, the method provides additional defenses against fine-tuning API attacks and preserves model capabilities.

**Abstract:** Most safety training methods for large language models (LLMs) are based on fine-tuning that forces models to shift from an unsafe answer to refusal when faced with harmful requests. Unfortunately, these drastic distribution shifts generally compromise model capabilities. To avoid that, we propose to expand the model's vocabulary with a special token we call red flag token (<rf>) and propose to train the model to insert this token into its response at any time when harmful content is generated or about to be generated. Our approach offers several advantages: it enables the model to explicitly learn the concept of harmfulness while marginally affecting the generated distribution, thus maintaining the model's utility. It also evaluates each generated answer and provides robustness as good as adversarial training without the need to run attacks during training. Moreover, by encapsulating our safety tuning in a LoRA module, we provide additional defenses against fine-tuning API attacks.

</details>


### [71] [Truthful or Fabricated? Using Causal Attribution to Mitigate Reward Hacking in Explanations](https://arxiv.org/abs/2504.05294)

*Pedro Ferreira, Wilker Aziz, Ivan Titov*

**Main category:** cs.CL

**Keywords:** large language models, chain-of-thought explanations, reward optimization, explanation fidelity, causal attribution

**Relevance Score:** 9

**TL;DR:** The paper investigates how preference optimization in large language models can compromise the trustworthiness of chain-of-thought explanations, leading to misleading outputs. It proposes a method to improve the reward model by enriching input with causal attribution to rectify this issue.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the collaboration between large language models and humans, it's crucial to ensure that explanations of model processes remain faithful and trustworthy, particularly during the alignment phase of model training.

**Method:** The authors propose modifying the reward model (RM) input by integrating a causal attribution of the prediction, enabling the RM to detect inconsistencies between the model's reasoning and its self-generated explanations.

**Key Contributions:**

	1. Identifying the conflict between reward optimization and explanation fidelity in LLMs.
	2. Proposing a method to enhance RM input with causal attributions for improved explanation accuracy.
	3. Demonstrating the effectiveness of the approach in controlled experimental settings.

**Result:** The proposed method effectively reduces the tendency of large language models to produce misleading chain-of-thought explanations in controlled tests.

**Limitations:** 

**Conclusion:** Enriching the reward model's input can significantly improve the fidelity of explanations generated by LLMs, making them more aligned with their internal decision processes.

**Abstract:** Chain-of-thought explanations are widely used to inspect the decision process of large language models (LLMs) and to evaluate the trustworthiness of model outputs, making them important for effective collaboration between LLMs and humans. We demonstrate that preference optimization - a key step in the alignment phase - can inadvertently reduce the faithfulness of these explanations. This occurs because the reward model (RM), which guides alignment, is tasked with optimizing both the expected quality of the response and the appropriateness of the explanations (e.g., minimizing bias or adhering to safety standards), creating potential conflicts. The RM lacks a mechanism to assess the consistency between the model's internal decision process and the generated explanation. Consequently, the LLM may engage in "reward hacking" by producing a final response that scores highly while giving an explanation tailored to maximize reward rather than accurately reflecting its reasoning. To address this issue, we propose enriching the RM's input with a causal attribution of the prediction, allowing the RM to detect discrepancies between the generated self-explanation and the model's decision process. In controlled settings, we show that this approach reduces the tendency of the LLM to generate misleading explanations.

</details>


### [72] [SocioVerse: A World Model for Social Simulation Powered by LLM Agents and A Pool of 10 Million Real-World Users](https://arxiv.org/abs/2504.10157)

*Xinnong Zhang, Jiayu Lin, Xinyi Mou, Shiyue Yang, Xiawei Liu, Libo Sun, Hanjia Lyu, Yihang Yang, Weihong Qi, Yue Chen, Guanying Li, Ling Yan, Yao Hu, Siming Chen, Yu Wang, Xuanjing Huang, Jiebo Luo, Shiping Tang, Libo Wu, Baohua Zhou, Zhongyu Wei*

**Main category:** cs.CL

**Keywords:** social simulation, LLM, human behavior, population dynamics, alignment components

**Relevance Score:** 8

**TL;DR:** SocioVerse is an LLM-agent-driven world model designed for social simulation, capable of reflecting large-scale population dynamics with validated effectiveness across politics, news, and economics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address alignment challenges in social simulation involving human behavior modeling and improve the predictability of group behaviors using large language models.

**Method:** Introduction of SocioVerse, which features four alignment components and utilizes a user pool of 10 million real individuals for large-scale simulation experiments in diverse domains.

**Key Contributions:**

	1. Development of the SocioVerse framework for social simulation using LLMs
	2. Implementation of four alignment components for improved accuracy
	3. Large-scale validation across three domains (politics, news, and economics)

**Result:** SocioVerse successfully simulates population dynamics while ensuring diversity, credibility, and representativeness with minimal manual adjustments.

**Limitations:** 

**Conclusion:** The framework demonstrates the potential of LLMs in social simulation and can significantly enhance the understanding of human behavior in various environments.

**Abstract:** Social simulation is transforming traditional social science research by modeling human behavior through interactions between virtual individuals and their environments. With recent advances in large language models (LLMs), this approach has shown growing potential in capturing individual differences and predicting group behaviors. However, existing methods face alignment challenges related to the environment, target users, interaction mechanisms, and behavioral patterns. To this end, we introduce SocioVerse, an LLM-agent-driven world model for social simulation. Our framework features four powerful alignment components and a user pool of 10 million real individuals. To validate its effectiveness, we conducted large-scale simulation experiments across three distinct domains: politics, news, and economics. Results demonstrate that SocioVerse can reflect large-scale population dynamics while ensuring diversity, credibility, and representativeness through standardized procedures and minimal manual adjustments.

</details>
