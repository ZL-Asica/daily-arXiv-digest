# 2025-09-11

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 19]

- [cs.CL](#cs.CL) [Total: 41]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Understanding the Video Content Creation Journey of Creators with Sensory Impairment in Kenya](https://arxiv.org/abs/2509.08108)

*Lan Xiao, Maryam Bandukda, Franklin Mingzhe Li, Mark Colley, Catherine Holloway*

**Main category:** cs.HC

**Keywords:** accessibility, human-computer interaction, collaboration, AI tools, sensory impairments

**Relevance Score:** 7

**TL;DR:** This paper explores the challenges faced by video creators with sensory impairments in low-resource settings, emphasizing the role of AI tools and collaborative practices in overcoming accessibility barriers.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of accessibility for video content creation among creators with sensory impairments, particularly in low-resource environments.

**Method:** Interviews with 20 video creators with visual and hearing impairments were conducted to investigate their tools, challenges, and collaborative workflows.

**Key Contributions:**

	1. Identified specific challenges faced by video creators with sensory impairments in low-resource settings.
	2. Proposed a model for flexible collaboration and inclusion of AI tools in the creative process.
	3. Expanded accessibility research in HCI by integrating insights from low-resource contexts.

**Result:** The study found that accessibility barriers and infrastructure limitations create a collaborative video creation process that relies on trusted partners and AI tools, with creators negotiating their agency and trust.

**Limitations:** The study is limited to a specific demographic and geographic area, which may not universally represent all creators with sensory impairments.

**Conclusion:** The research highlights the importance of flexible collaboration models and inclusive workflows to support disabled creators, stressing the intersection of technology and social factors in enhancing accessibility.

**Abstract:** Video content creation offers vital opportunities for expression and participation, yet remains largely inaccessible to creators with sensory impairments, especially in low-resource settings. We conducted interviews with 20 video creators with visual and hearing impairments in Kenya to examine their tools, challenges, and collaborative practices. Our findings show that accessibility barriers and infrastructural limitations shape video creation as a staged, collaborative process involving trusted human partners and emerging AI tools. Across workflows, creators actively negotiated agency and trust, maintaining creative control while bridging sensory gaps. We discuss the need for flexible, interdependent collaboration models, inclusive human-AI workflows, and diverse storytelling practices. This work broadens accessibility research in HCI by examining how technology and social factors intersect in low-resource contexts, suggesting ways to better support disabled creators globally.

</details>


### [2] [Componentization: Decomposing Monolithic LLM Responses into Manipulable Semantic Units](https://arxiv.org/abs/2509.08203)

*Ryan Lingo, Rajeev Chhajer, Martin Arroyo, Luka Brkljacic, Ben Davis, Nithin Santhanam*

**Main category:** cs.HC

**Keywords:** Componentization, Large Language Models, Human-Computer Interaction, Modular Outputs, Collaborative Workflows

**Relevance Score:** 8

**TL;DR:** This paper introduces componentization to edit Large Language Model outputs more efficiently through modular responses.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** Large Language Models produce text that is difficult to edit, slowing down collaboration. There is a need to improve this by enabling easier manipulation of outputs.

**Method:** The paper presents Modular and Adaptable Output Decomposition (MAOD) which segments model responses into coherent, editable components while maintaining context, and implements this through the Component-Based Response Architecture (CBRA).

**Key Contributions:**

	1. Definition of componentization for LLM outputs
	2. Introduction of CBRA and MAODchat prototype
	3. Preliminary user study observations

**Result:** In a study with four participants, component-level editing facilitated common workflows, enabled iterative refinement, and encouraged selective reuse of components.

**Limitations:** 

**Conclusion:** Componentization could transform the way users interact with text generated by LLMs, promoting more effective collaboration and active engagement.

**Abstract:** Large Language Models (LLMs) often produce monolithic text that is hard to edit in parts, which can slow down collaborative workflows. We present componentization, an approach that decomposes model outputs into modular, independently editable units while preserving context. We describe Modular and Adaptable Output Decomposition (MAOD), which segments responses into coherent components and maintains links among them, and we outline the Component-Based Response Architecture (CBRA) as one way to implement this idea. Our reference prototype, MAODchat, uses a microservices design with state-machine-based decomposition agents, vendor-agnostic model adapters, and real-time component manipulation with recomposition.   In an exploratory study with four participants from academic, engineering, and product roles, we observed that component-level editing aligned with several common workflows and enabled iterative refinement and selective reuse. Participants also mentioned possible team workflows. Our contributions are: (1) a definition of componentization for transforming monolithic outputs into manipulable units, (2) CBRA and MAODchat as a prototype architecture, (3) preliminary observations from a small user study, (4) MAOD as an algorithmic sketch for semantic segmentation, and (5) example Agent-to-Agent protocols for automated decomposition. We view componentization as a promising direction for turning passive text consumption into more active, component-level collaboration.

</details>


### [3] [A Priest, a Rabbi, and an Atheist Walk Into an Error Bar: Religious Meditations on Uncertainty Visualization](https://arxiv.org/abs/2509.08213)

*Michael Correll, Lane Harrison*

**Main category:** cs.HC

**Keywords:** uncertainty visualization, error bars, philosophy of uncertainty

**Relevance Score:** 3

**TL;DR:** This paper critiques the simplification of uncertainty visualization, advocating for a broader understanding of uncertainty forms and their visualization.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the prevailing notion that uncertainty can be effectively represented solely through error bars, proposing a deeper philosophical engagement with uncertainty.

**Method:** The authors analyze examples from religion to explore alternative conceptions of uncertainty and propose how this enriches visualization practices.

**Key Contributions:**

	1. Critique of the dominance of error bars in uncertainty visualization
	2. Integration of alternative philosophical perspectives on uncertainty
	3. Encouragement to tailor uncertainty visualizations to diverse audiences

**Result:** The study reveals that current practices largely rely on a narrow interpretation of uncertainty, potentially overlooking diverse forms of uncertainty that could better serve various audiences.

**Limitations:** 

**Conclusion:** By expanding the scope of what uncertainties to visualize, the paper encourages a richer dialogue about the needs of different stakeholders in the communication of uncertainty.

**Abstract:** In this provocation, we suggest that much (although not all) current uncertainty visualization simplifies the myriad forms of uncertainty into error bars around an estimate. This apparent simplification into error bars comes only as a result of a vast metaphysics around uncertainty and probability underlying modern statistics. We use examples from religion to present alternative views of uncertainty (metaphysical or otherwise) with the goal of enriching our conception of what kind of uncertainties we ought to visualize, and what kinds of people we might be visualizing those uncertainties for.

</details>


### [4] [An Adaptive Scoring Framework for Attention Assessment in NDD Children via Serious Games](https://arxiv.org/abs/2509.08353)

*Abdul Rehman, Ilona Heldal, Cristina Costescu, Carmen David, Jerry Chun-Wei Lin*

**Main category:** cs.HC

**Keywords:** Neurodevelopmental Disorders, adaptive scoring, learning assessment, educational technology, temporal engagement

**Relevance Score:** 4

**TL;DR:** The paper presents an adaptive scoring framework for children with Neurodevelopmental Disorders, integrating multiple metrics to enhance learning assessments beyond traditional methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To create a comprehensive assessment of learning that accommodates diverse cognitive needs in children with Neurodevelopmental Disorders.

**Method:** The framework uses progressive difficulty adaptation and dynamic weighting of metrics such as spatial attention, temporal engagement, and game performance.

**Key Contributions:**

	1. Innovative adaptive scoring framework for children with NDD
	2. Integration of multiple metrics for comprehensive assessments
	3. Development of a multi-metric validation framework for educational settings

**Result:** The framework includes an adaptive temporal impact system that personalizes assessments, supported by multi-metric validation methodologies like MAE and RMSE.

**Limitations:** 

**Conclusion:** This research links technical metrics with educational insights, fostering improved pedagogical interventions for better learning outcomes.

**Abstract:** This paper introduces an innovative adaptive scoring framework for children with Neurodevelopmental Disorders (NDD) that is attributed to the integration of multiple metrics, such as spatial attention patterns, temporal engagement, and game performance data, to create a comprehensive assessment of learning that goes beyond traditional game scoring. The framework employs a progressive difficulty adaptation method, which focuses on specific stimuli for each level and adjusts weights dynamically to accommodate increasing cognitive load and learning complexity. Additionally, it includes capabilities for temporal analysis, such as detecting engagement periods, providing rewards for sustained attention, and implementing an adaptive multiplier framework based on performance levels. To avoid over-rewarding high performers while maximizing improvement potential for students who are struggling, the designed framework features an adaptive temporal impact framework that adjusts performance scales accordingly. We also established a multi-metric validation framework using Mean Absolute Error (MAE), Root Mean Square Error (RMSE), Pearson correlation, and Spearman correlation, along with defined quality thresholds for assessing deployment readiness in educational settings. This research bridges the gap between technical eye-tracking metrics and educational insights by explicitly mapping attention patterns to learning behaviors, enabling actionable pedagogical interventions.

</details>


### [5] [Personalized Inhibition Training with Eye-Tracking: Enhancing Student Learning and Teacher Assessment in Educational Games](https://arxiv.org/abs/2509.08357)

*Abdul Rehman, Ilona Heldal, Diana Stilwell, Paula Costa Ferreira, Jerry Chun-Wei Lin*

**Main category:** cs.HC

**Keywords:** Eye Tracking, Inhibitory Control, Education, Personalized Learning, Attention Assessment

**Relevance Score:** 6

**TL;DR:** The study presents an eye-tracking analysis framework used in an educational game to enhance children's inhibitory control skills through personalized interventions based on attention insights.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve understanding of visual attention and cognitive processes in interactive educational environments, specifically for children.

**Method:** The framework utilizes dual-threshold eye movement detection and comprehensive Area of Interest analysis to process gaze data and offer personalized educational insights.

**Key Contributions:**

	1. Comprehensive eye-tracking analysis framework for educational assessment
	2. Development of personalized intervention strategies based on gaze data
	3. Early warning indicators for cognitive overload and adaptive learning

**Result:** Three levels of game difficulty revealed critical attention deficits and risk assessments, resulting in effective personalized interventions and actionable recommendations for educational stakeholders.

**Limitations:** 

**Conclusion:** The system effectively identifies attention risks and recommends tailored interventions, thereby supporting data-driven decision-making in education.

**Abstract:** Eye tracking (ET) can help to understand visual attention and cognitive processes in interactive environments. This study presents a comprehensive eye-tracking analysis framework of the Inhibitory Control Game, named the ReStroop game, which is an educational intervention aimed at improving inhibitory control skills in children through a recycling-themed sorting task, for educational assessment that processes raw gaze data through unified algorithms for fixation detection, performance evaluation, and personalized intervention planning. The system employs dual-threshold eye movement detection (I-VT and advanced clustering), comprehensive Area of Interest (AOI) analysis, and evidence-based risk assessment to transform gaze patterns into actionable educational insights. We evaluated this framework across three difficulty levels and revealed critical attention deficits, including low task relevance, elevated attention scatter, and compromised processing efficiency. The multi-dimensional risk assessment identified high to moderate risk levels, triggering personalized interventions including focus training, attention regulation support, and environmental modifications. The system successfully distinguishes between adaptive learning and cognitive overload, providing early warning indicators for educational intervention. Results demonstrate the system's effectiveness in objective attention assessment, early risk identification, and the generation of evidence-based recommendations for students, teachers, and specialists, supporting data-driven educational decision-making and personalized learning approaches.

</details>


### [6] [HyperMOOC: Augmenting MOOC Videos with Concept-based Embedded Visualizations](https://arxiv.org/abs/2509.08404)

*Li Ye, Lei Wang, Lihong Cai, Ruiqi Yu, Yong Wang, Yigang Wang, Wei Chen, Zhiguang Zhou*

**Main category:** cs.HC

**Keywords:** MOOCs, visualization, education technology, user study, learning effectiveness

**Relevance Score:** 6

**TL;DR:** HyperMOOC enhances MOOC video learning through concept-based visualizations and interactive design, improving knowledge retention and understanding.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To address the issue of context loss in MOOC video learning, leading to reduced learning effectiveness.

**Method:** HyperMOOC integrates multi-glyph designs and multi-stage interactions using timeline-based radial visualizations to enhance user engagement and cognitive understanding.

**Key Contributions:**

	1. Introduction of multi-glyph designs for various knowledge types
	2. Implementation of timeline-based radial visualizations for cognitive path understanding
	3. Evaluation of learner satisfaction and effectiveness through user studies

**Result:** User studies with 36 MOOC learners revealed increased learning effectiveness, higher satisfaction rates, and improved understanding compared to traditional methods.

**Limitations:** Limited to a specific user demographic of MOOC learners and a single platform for evaluation.

**Conclusion:** The integration of concept-based visualizations significantly improves the learning experience in MOOCs by helping learners maintain context and navigate content more effectively.

**Abstract:** Massive Open Online Courses (MOOCs) have become increasingly popular worldwide. However, learners primarily rely on watching videos, easily losing knowledge context and reducing learning effectiveness. We propose HyperMOOC, a novel approach augmenting MOOC videos with concept-based embedded visualizations to help learners maintain knowledge context. Informed by expert interviews and literature review, HyperMOOC employs multi-glyph designs for different knowledge types and multi-stage interactions for deeper understanding. Using a timeline-based radial visualization, learners can grasp cognitive paths of concepts and navigate courses through hyperlink-based interactions. We evaluated HyperMOOC through a user study with 36 MOOC learners and interviews with two instructors. Results demonstrate that HyperMOOC enhances learners' learning effect and efficiency on MOOCs, with participants showing higher satisfaction and improved course understanding compared to traditional video-based learning approaches.

</details>


### [7] [GlyphWeaver: Unlocking Glyph Design Creativity with Uniform Glyph DSL and AI](https://arxiv.org/abs/2509.08444)

*Can Liu, Shiwei Chen, Zhibang Jiang, Yong Wang*

**Main category:** cs.HC

**Keywords:** glyph visualizations, domain-specific language, multimodal interaction

**Relevance Score:** 6

**TL;DR:** GlyphWeaver is an interactive system that simplifies the creation of expressive glyph visualizations for complex multivariate data through a domain-specific language and multimodal interaction.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** There is a gap between design creativity and technical implementation in creating custom glyph visualizations, making it challenging for designers, particularly those without programming skills.

**Method:** GlyphWeaver consists of a glyph domain-specific language (GDSL) for hierarchical container models, an operation management mechanism limiting modifications to atomic operations, and a multimodal interaction interface for natural language and direct input.

**Key Contributions:**

	1. Introduction of GlyphWeaver as an interactive system for glyph creation
	2. Development of a hierarchical GDSL for visualizations
	3. Implementation of a multimodal interaction interface leveraging a large language model

**Result:** User studies indicate that GlyphWeaver significantly improves design efficiency and creativity in producing complex glyph visualizations.

**Limitations:** 

**Conclusion:** The system effectively empowers designers with no extensive programming expertise to create sophisticated visual representations of multivariate data.

**Abstract:** Expressive glyph visualizations provide a powerful and versatile means to represent complex multivariate data through compact visual encodings, but creating custom glyphs remains challenging due to the gap between design creativity and technical implementation. We present GlyphWeaver, a novel interactive system to enable an easy creation of expressive glyph visualizations. Our system comprises three key components: a glyph domain-specific language (GDSL), a GDSL operation management mechanism, and a multimodal interaction interface. The GDSL is a hierarchical container model, where each container is independent and composable, providing a rigorous yet practical foundation for complex glyph visualizations. The operation management mechanism restricts modifications of the GDSL to atomic operations, making it accessible without requiring direct coding. The multimodal interaction interface enables direct manipulation, natural language commands, and parameter adjustments. A multimodal large language model acts as a translator, converting these inputs into GDSL operations. GlyphWeaver significantly lowers the barrier for designers, who often do not have extensive programming skills, to create sophisticated glyph visualizations. A case study and user interviews with 13 participants confirm its substantial gains in design efficiency and effectiveness of producing creative glyph visualizations.

</details>


### [8] [Printegrated Circuits: Personal Fabrication of 3D Printed Devices with Embedded PCBs](https://arxiv.org/abs/2509.08459)

*Oliver Child, Ollie Hanton, Jack Dawson, Steve Hodges, Mike Fraser*

**Main category:** cs.HC

**Keywords:** 3D printing, conductive thermoplastics, interactive devices, Prinjection, embedded PCBs

**Relevance Score:** 6

**TL;DR:** The paper presents Printegrated Circuits, a new 3D printing technique that integrates traditional electronics into self-contained interactive objects, eliminating the need for post-print assembly steps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the challenges of consumer-level multi-material 3D printing, such as high resistance materials and the need for post-print assembly in interactive device fabrication.

**Method:** The technique uses a process called 'Prinjection' to inject conductive filament into the plated-through holes of embedded PCBs during a pause in the 3D printing process, creating reliable electrical and mechanical contacts without manual wiring.

**Key Contributions:**

	1. Introduces Printegrated Circuits technique for integrating electronics in 3D printed devices
	2. Eliminates manual assembly steps with automatic electrical connections
	3. Demonstrates practical applications within existing design workflows

**Result:** The Printegrated Circuits technique successfully integrates electronic components during 3D printing, demonstrated through six practical examples, showcasing its reliability and integration into design workflows.

**Limitations:** 

**Conclusion:** Printegrated Circuits offers a streamlined approach for creating interactive devices and has potential implications for future research and development in the field of rapid prototyping and electronics integration.

**Abstract:** Consumer-level multi-material 3D printing with conductive thermoplastics enables fabrication of interactive elements for bespoke tangible devices. However, large feature sizes, high resistance materials, and limitations of printable control circuitry mean that deployable devices cannot be printed without post-print assembly steps. To address these challenges, we present Printegrated Circuits, a technique that uses traditional electronics as material to 3D print self-contained interactive objects. Embedded PCBs are placed into recesses during a pause in the print, and through a process we term \textit{Prinjection}, conductive filament is injected into their plated-through holes. This automatically creates reliable electrical and mechanical contact, eliminating the need for manual wiring or bespoke connectors. We describe the custom machine code generation that supports our approach, and characterise its electrical and mechanical properties. With our 6 demonstrations, we highlight how the Printegrated Circuits process fits into existing design and prototyping workflows as well as informs future research agendas.

</details>


### [9] [Bias in the Loop: How Humans Evaluate AI-Generated Suggestions](https://arxiv.org/abs/2509.08514)

*Jacob Beck, Stephanie Eckman, Christoph Kern, Frauke Kreuter*

**Main category:** cs.HC

**Keywords:** Human-AI collaboration, Cognitive biases, Performance metrics

**Relevance Score:** 9

**TL;DR:** This paper examines the psychological factors influencing human-AI collaboration in decision-making, revealing how task design and individual characteristics impact performance and engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the psychological factors that affect the success or failure of human-AI collaborations, particularly in decision-making contexts.

**Method:** A randomized experiment involving 2,784 participants was conducted, manipulating AI suggestion quality, task burden through required corrections, and performance-based financial incentives. Data on demographics, attitudes toward AI, and behavioral metrics were collected.

**Key Contributions:**

	1. Identification of cognitive biases affecting human acceptance of AI suggestions
	2. Demonstration of the impact of individual attitudes toward AI on decision-making performance
	3. Recommendations for structuring workflows to enhance human-AI collaboration

**Result:** Findings indicated that cognitive shortcuts negatively impact engagement and that individual attitudes towards AI are critical predictors of performance, with skeptics performing better than proponents.

**Limitations:** 

**Conclusion:** Effective human-AI collaboration needs to account for human psychology and biases by structuring review processes and considering the attitudes of human reviewers.

**Abstract:** Human-AI collaboration increasingly drives decision-making across industries, from medical diagnosis to content moderation. While AI systems promise efficiency gains by providing automated suggestions for human review, these workflows can trigger cognitive biases that degrade performance. We know little about the psychological factors that determine when these collaborations succeed or fail. We conducted a randomized experiment with 2,784 participants to examine how task design and individual characteristics shape human responses to AI-generated suggestions. Using a controlled annotation task, we manipulated three factors: AI suggestion quality in the first three instances, task burden through required corrections, and performance-based financial incentives. We collected demographics, attitudes toward AI, and behavioral data to assess four performance metrics: accuracy, correction activity, overcorrection, and undercorrection. Two patterns emerged that challenge conventional assumptions about human-AI collaboration. First, requiring corrections for flagged AI errors reduced engagement and increased the tendency to accept incorrect suggestions, demonstrating how cognitive shortcuts influence collaborative outcomes. Second, individual attitudes toward AI emerged as the strongest predictor of performance, surpassing demographic factors. Participants skeptical of AI detected errors more reliably and achieved higher accuracy, while those favorable toward automation exhibited dangerous overreliance on algorithmic suggestions. The findings reveal that successful human-AI collaboration depends not only on algorithmic performance but also on who reviews AI outputs and how review processes are structured. Effective human-AI collaborations require consideration of human psychology: selecting diverse evaluator samples, measuring attitudes, and designing workflows that counteract cognitive biases.

</details>


### [10] [Motion-Based User Identification across XR and Metaverse Applications by Deep Classification and Similarity Learning](https://arxiv.org/abs/2509.08539)

*Lukas Schach, Christian Rack, Ryan P. McMahan, Marc Erich Latoschik*

**Main category:** cs.HC

**Keywords:** Extended Reality, user identification, biometric verification, motion data, Metaverse

**Relevance Score:** 6

**TL;DR:** This paper investigates user identification based on motion data in Extended Reality (XR) applications using two classification models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To evaluate how well current models can generalize user identification across different XR applications, which is crucial for biometric verification.

**Method:** A novel dataset was developed consisting of motion data from 49 users across five XR applications, including games and social interactions. The performance of two classification models was assessed in identifying users based on this dataset.

**Key Contributions:**

	1. Development of a novel dataset for XR motion data
	2. Evaluation of user identification models across different applications
	3. Insights into the limitations of biometric user identification in XR and Metaverse.

**Result:** Models were effective in identifying users within the same application but struggled with generalization across different XR applications.

**Limitations:** Limited generalization ability across different XR environments; focus on motion data only.

**Conclusion:** Current models exhibit limited generalization capabilities, highlighting the need for improved biometric methods in XR applications and emphasizing the potential risks of user misidentification.

**Abstract:** This paper examines the generalization capacity of two state-of-the-art classification and similarity learning models in reliably identifying users based on their motions in various Extended Reality (XR) applications. We developed a novel dataset containing a wide range of motion data from 49 users in five different XR applications: four XR games with distinct tasks and action patterns, and an additional social XR application with no predefined task sets. The dataset is used to evaluate the performance and, in particular, the generalization capacity of the two models across applications. Our results indicate that while the models can accurately identify individuals within the same application, their ability to identify users across different XR applications remains limited. Overall, our results provide insight into current models generalization capabilities and suitability as biometric methods for user verification and identification. The results also serve as a much-needed risk assessment of hazardous and unwanted user identification in XR and Metaverse applications. Our cross-application XR motion dataset and code are made available to the public to encourage similar research on the generalization of motion-based user identification in typical Metaverse application use cases.

</details>


### [11] [Formal verification for robo-advisors: Irrelevant for subjective end-user trust, yet decisive for investment behavior?](https://arxiv.org/abs/2509.08540)

*Alina Tausch, Magdalena Wischnewski, Mustafa Yalciner, Daniel Neider*

**Main category:** cs.HC

**Keywords:** AI quality assurance, trust in robo-advisors, investment behavior, formal verification, user trust

**Relevance Score:** 6

**TL;DR:** This study explores how certification and verification of AI impact trust in a robo-advisor using an online banking scenario with 520 participants.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how measures for quality assurance of AI influence user trust and behavior, specifically in contexts like robo-advising.

**Method:** An online-vignette study with 520 participants, divided into four experimental groups based on the type of information provided regarding a robo-advisor. Participants made investment decisions and reported on trust and dependability after receiving feedback on investment success or failure.

**Key Contributions:**

	1. Investigated the role of certification and verification in user trust toward robo-advisors.
	2. Demonstrated that success or failure feedback significantly impacts trust ratings.
	3. Highlighted the discrepancy between formal verification of AI and its perceived trustworthiness by end-users.

**Result:** The type of quality assurance (certified vs. verified) had little impact on overall trust and investment amounts, although a verified advisor was descriptively favored for investment. The success or failure of investments significantly influenced trust ratings more than advisor quality.

**Limitations:** The study was conducted in a hypothetical scenario and might not fully capture real-world behaviors and perceptions.

**Conclusion:** The findings suggest that, while verification might be seen as a gold standard in AI, it does not significantly enhance user trust. Further research is needed to explore the dynamics of trust behavior in AI interactions.

**Abstract:** This online-vignette study investigates the impact of certification and verification as measures for quality assurance of AI on trust and use of a robo-advisor. Confronting 520 participants with an imaginary situation where they were using an online banking service to invest their inherited money, we formed 4 experimental groups. EG1 achieved no further information of their robo-advisor, while EG2 was informed that their robo-advisor was certified by a reliable agency for unbiased processes, and EG3 was presented with a formally verified robo-advisor that was proven to consider their investment preferences. A control group was presented a remote certified human financial advisor. All groups had to decide on how much of their 10,000 euros they would give to their advisor to autonomously invest for them and report on trust and perceived dependability. A second manipulation happened afterwards, confronting participants with either a successful or failed investment. Overall, our results show that the level of quality assurance of the advisor had surprisingly near to no effect of any of our outcome variables, except for people's perception of their own mental model of the advisor. Descriptively, differences between investments show that seem to favor a verified advisor with a median investment of 65,000 euros (vs. 50,000). Success or failure information, though influences only partially by advisor quality, has been perceived as a more important clue for advisor trustworthiness, leading to substantially different trust and dependability ratings. The study shows the importance of thoroughly investigating not only trust, but also trusting behavior with objective measures. It also underlines the need for future research on formal verification, that might be the gold standard in proving AI mathematically, but seems not to take full effect as a cue for trustworthiness for end-users.

</details>


### [12] [Embedding Empathy into Visual Analytics: A Framework for Person-Centred Dementia Care](https://arxiv.org/abs/2509.08548)

*Rhiannon Owen, Jonathan C. Roberts*

**Main category:** cs.HC

**Keywords:** dementia care, empathy, visualization framework, human-centered design, healthcare

**Relevance Score:** 8

**TL;DR:** This paper presents an empathy-centred visualization framework aimed at improving dementia care through enhanced healthcare professional-patient engagement.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current digital tools in dementia care prioritize quantitative metrics over empathetic engagement, limiting caregivers' ability to understand patients' personal needs.

**Method:** The framework integrates person-centered care principles with empathy mapping methodologies, developed through a design study.

**Key Contributions:**

	1. Development of an empathy-centred visualization framework for dementia care.
	2. Integration of person-centered care principles with empathy mapping methodologies.
	3. Validation of the framework through usability testing and user experience evaluations.

**Result:** Usability testing and evaluations conducted with healthcare professionals suggest the framework is feasible and can support more empathetic relationships between medical staff and patients.

**Limitations:** 

**Conclusion:** Embedding empathy systematically into visualization design can enhance human-centered clinical care, addressing challenges faced by dementia patients in hospital settings.

**Abstract:** Dementia care requires healthcare professionals to balance a patient's medical needs with a deep understanding of their personal needs, preferences, and emotional cues. However, current digital tools prioritise quantitative metrics over empathetic engagement,limiting caregivers ability to develop a deeper personal understanding of their patients. This paper presents an empathy centred visualisation framework, developed through a design study, to address this gap. The framework integrates established principles of person centred care with empathy mapping methodologies to encourage deeper engagement. Our methodology provides a structured approach to designing for indirect end users, patients whose experience is shaped by a tool they may not directly interact with. To validate the framework, we conducted evaluations with healthcare professinals, including usability testing of a working prototype and a User Experience Questionnaire study. Results suggest the feasibility of the framework, with participants highlighting its potential to support a more personal and empathetic relationship between medical staff and patients. The work starts to explore how empathy could be systematically embedded into visualisation design, as we contribute to ongoing efforts in the data visualisation community to support human centred, interpretable, and ethically aligned clinical care, addressing the urgent need to improve dementia patients experiences in hospital settings.

</details>


### [13] [Acceptability of AI Assistants for Privacy: Perceptions of Experts and Users on Personalized Privacy Assistants](https://arxiv.org/abs/2509.08554)

*Meihe Xu, Aurelia Tamò-Larrieux, Arianna Rossi*

**Main category:** cs.HC

**Keywords:** Privacy assistants, AI-driven solutions, Technology acceptance models

**Relevance Score:** 7

**TL;DR:** This study explores user and expert perspectives on the acceptability of personalized privacy assistants (PPAs) through focus groups, identifying key themes influencing their acceptance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The increasing complexity of daily tasks and decisions, combined with concerns over privacy, necessitates intelligent AI agents to assist users in making privacy-related decisions aligned with their preferences.

**Method:** Conducted five focus groups with domain experts (n = 11) and potential users (n = 26) to identify factors influencing the acceptability of AI-driven privacy assistants.

**Key Contributions:**

	1. Identification of factors influencing the acceptability of personalized privacy assistants
	2. Theoretical extensions to existing technology acceptance models
	3. Insights for designing AI-driven privacy solutions

**Result:** Key themes identified include design elements, external conditions like regulation and education, and systemic conditions impacting the acceptability of PPAs.

**Limitations:** 

**Conclusion:** The findings enhance technology acceptance models for PPAs, provide design and policy insights, and suggest broader implications for AI assistant design.

**Abstract:** Individuals increasingly face an overwhelming number of tasks and decisions. To cope with the new reality, there is growing research interest in developing intelligent agents that can effectively assist people across various aspects of daily life in a tailored manner, with privacy emerging as a particular area of application. Artificial intelligence (AI) assistants for privacy, such as personalized privacy assistants (PPAs), have the potential to automatically execute privacy decisions based on users' pre-defined privacy preferences, sparing them the mental effort and time usually spent on each privacy decision. This helps ensure that, even when users feel overwhelmed or resigned about privacy, the decisions made by PPAs still align with their true preferences and best interests. While research has explored possible designs of such agents, user and expert perspectives on the acceptability of such AI-driven solutions remain largely unexplored. In this study, we conducted five focus groups with domain experts (n = 11) and potential users (n = 26) to uncover key themes shaping the acceptance of PPAs. Factors influencing the acceptability of AI assistants for privacy include design elements (such as information sources used by the agent), external conditions (such as regulation and literacy education), and systemic conditions (e.g., public or market providers and the need to avoid monopoly) to PPAs. These findings provide theoretical extensions to technology acceptance models measuring PPAs, insights on design, and policy implications for PPAs, as well as broader implications for the design of AI assistants.

</details>


### [14] [Visual Analysis of Time-Dependent Observables in Cell Signaling Simulations](https://arxiv.org/abs/2509.08589)

*Lena Cibulski, Fiete Haack, Adelinde Uhrmacher, Stefan Bruckner*

**Main category:** cs.HC

**Keywords:** cell signaling, visual analysis, simulation studies, model calibration, biological analysis

**Relevance Score:** 3

**TL;DR:** This paper presents a design study focused on visual analysis techniques for simulating cell signaling processes, enabling modelers and biologists to better understand and calibrate cellular communication models.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To capture the dynamic molecular mechanisms of cell signaling, which are essential for cellular functions, and to aid in the calibration of simulation models.

**Method:** The methodology involves embedding time series plots within parallel coordinates to allow simultaneous analysis of model parameters and time outputs, facilitating detailed exploration of model behaviors and effects of receptor trafficking.

**Key Contributions:**

	1. Introduction of a novel visual analysis technique combining time series and parallel coordinates for cellular simulation studies.
	2. Support for model parameter calibration in relation to biological reference behaviors.
	3. Facilitation of exploration of receptor trafficking effects on signal transmission efficiency.

**Result:** The approach allows for effective assessment of the plausibility and sensitivity of temporal outputs across different model configurations, providing valuable insights for both modelers and biologists.

**Limitations:** 

**Conclusion:** The design study demonstrates the utility of advanced visual analysis methods in enhancing understanding of cell signaling dynamics and assists in model calibration and exploration of biological phenomena.

**Abstract:** The ability of a cell to communicate with its environment is essential for key cellular functions like replication, metabolism, or cell fate decisions. The involved molecular mechanisms are highly dynamic and difficult to capture experimentally. Simulation studies offer a valuable means for exploring and predicting how cell signaling processes unfold. We present a design study on the visual analysis of such studies to support 1) modelers in calibrating model parameters such that the simulated signal responses over time reflect reference behavior from cell biology research and 2) cell biologists in exploring the influence of receptor trafficking on the efficiency of signal transmission within the cell. We embed time series plots into parallel coordinates to enable a simultaneous analysis of model parameters and temporal outputs. A usage scenario illustrates how our approach assists with typical tasks such as assessing the plausibility of temporal outputs or their sensitivity across model configurations.

</details>


### [15] [Augmenting speech transcripts of VR recordings with gaze, pointing, and visual context for multimodal coreference resolution](https://arxiv.org/abs/2509.08689)

*Riccardo Bovo, Frederik Brudy, George Fitzmaurice, Fraser Anderson*

**Main category:** cs.HC

**Keywords:** multimodal communication, coreference resolution, eye-tracking, VR transcripts, non-verbal cues

**Relevance Score:** 7

**TL;DR:** This paper presents a system that integrates eye-tracking and scene metadata with VR speech transcripts to improve coreference resolution in multimodal conversations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge of understanding immersive multimodal conversations due to reliance on visual context and non-verbal cues is significant, especially in resolving ambiguous expressions.

**Method:** The system combines VR speech transcripts with eye-tracking laser pointing data and scene metadata, allowing for the generation of textual descriptions that capture non-verbal communication.

**Key Contributions:**

	1. Development of a multimodal transcript system for VR environments
	2. Demonstrated 26.5% improvement in coreference resolution accuracy
	3. Utilization of eye-tracking and gesture data to enhance communication understanding

**Result:** Utilizing the multimodal transcript led to a 26.5% improvement in coreference resolution accuracy by a GPT model over a speech-only baseline.

**Limitations:** 

**Conclusion:** The integration of non-verbal cues and scene context significantly enhances the accuracy of coreference resolution in immersive conversations.

**Abstract:** Understanding transcripts of immersive multimodal conversations is challenging because speakers frequently rely on visual context and non-verbal cues, such as gestures and visual attention, which are not captured in speech alone. This lack of information makes coreferences resolution-the task of linking ambiguous expressions like ``it'' or ``there'' to their intended referents-particularly challenging. In this paper we present a system that augments VR speech transcript with eye-tracking laser pointing data, and scene metadata to generate textual descriptions of non-verbal communication and the corresponding objects of interest. To evaluate the system, we collected gaze, gesture, and voice data from 12 participants (6 pairs) engaged in an open-ended design critique of a 3D model of an apartment. Our results show a 26.5\% improvement in coreference resolution accuracy by a GPT model when using our multimodal transcript compared to a speech-only baseline.

</details>


### [16] [GraspR: A Computational Model of Spatial User Preferences for Adaptive Grasp UI Design](https://arxiv.org/abs/2501.05434)

*Arthur Caetano, Yunhao Luo, Adwait Sharma, Misha Sra*

**Main category:** cs.HC

**Keywords:** Grasp User Interfaces, Human-Computer Interaction, Microgestures

**Relevance Score:** 8

**TL;DR:** GraspR is a computational model that predicts user preferences for microgestures in grasp UIs, enhancing dual-tasking in XR environments.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing grasp UI design practices, which are either labor-intensive or ignore subjective user factors.

**Method:** A data-driven approach utilizing a two-alternative forced choice paradigm to collect user preferences, trained on 1,520 preferences from eight participants across various grasp variations.

**Key Contributions:**

	1. Introduction of the GraspR model for predicting user preferences in grasp UIs
	2. Combination of computational methods with human preference modeling
	3. Release of dataset and code for future research support

**Result:** GraspR effectively predicts user preferences, demonstrated through a prototype that adjusts interface layouts for common tasks.

**Limitations:** 

**Conclusion:** GraspR provides a scalable and adaptive solution for grasp UIs, supporting further research with released dataset and code.

**Abstract:** Grasp User Interfaces (grasp UIs) enable dual-tasking in XR by allowing interaction with digital content while holding physical objects. However, current grasp UI design practices face a fundamental challenge: existing approaches either capture user preferences through labor-intensive elicitation studies that are difficult to scale or rely on biomechanical models that overlook subjective factors. We introduce GraspR, the first computational model that predicts user preferences for single-finger microgestures in grasp UIs. Our data-driven approach combines the scalability of computational methods with human preference modeling, trained on 1,520 preferences collected via a two-alternative forced choice paradigm across eight participants and four frequently used grasp variations. We demonstrate GraspR's effectiveness through a working prototype that dynamically adjusts interface layouts across four everyday tasks. We release both the dataset and code to support future research in adaptive grasp UIs.

</details>


### [17] [Visual Network Analysis in Immersive Environments: A Survey](https://arxiv.org/abs/2501.08500)

*Lucas Joos, Maximilian T. Fischer, Julius Rauscher, Daniel A. Keim, Tim Dwyer, Falk Schreiber, Karsten Klein*

**Main category:** cs.HC

**Keywords:** immersive analytics, network visualization, user evaluation

**Relevance Score:** 4

**TL;DR:** This paper presents a survey of visual network analysis in immersive environments, analyzing 138 publications to define the design space and empirical evaluations related to immersive analytics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the fragmented nature of immersive analytics and provide a clear overview of the design space for visual network analysis.

**Method:** A systematic analysis of 138 publications was conducted, covering various aspects of design and user evaluations in immersive environments for network visualization.

**Key Contributions:**

	1. Comprehensive survey of immersive network visualization
	2. Identification of design space aspects
	3. Empirical evaluation of existing applications

**Result:** The analysis identifies key design aspects, empirical evidence from previous evaluations, and highlights existing research gaps in the field.

**Limitations:** The survey may not cover newly published works after the retrieval process and may have bias in selected publications.

**Conclusion:** This work synthesizes findings and offers guidance for future approaches in immersive analytics, along with an interactive online resource for further exploration.

**Abstract:** The increasing complexity and volume of network data demand effective analysis approaches, with visual exploration proving particularly beneficial. Immersive technologies, such as augmented reality, virtual reality, and large display walls, have enabled the emerging field of immersive analytics, offering new opportunities to enhance user engagement, spatial awareness, and problem-solving. A growing body of work has explored immersive environments for network visualisation, ranging from design studies to fully integrated applications across various domains. Despite these advancements, the field remains fragmented, lacking a clear description of the design space and a structured overview of the aspects that have already been empirically evaluated. To address this gap, we present a survey of visual network analysis in immersive environments, covering 138 publications retrieved through a structured pipeline. We systematically analyse the key aspects that define the design space, investigate their coverage in prior applications (n=87), and review user evaluations (n=59) that provide empirical evidence for essential design-related questions. By synthesising experimental findings and evaluating existing applications, we identify key achievements, highlight research gaps, and offer guidance for the design of future approaches. Additionally, we provide an online resource to explore our results interactively, which will be updated as new developments emerge.

</details>


### [18] [Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?](https://arxiv.org/abs/2501.15463)

*Hua Shen, Nicholas Clark, Tanushree Mitra*

**Main category:** cs.HC

**Keywords:** LLM evaluation, Value-Action Gap, Context-aware evaluations, Value-informed actions, Alignment measures

**Relevance Score:** 8

**TL;DR:** This study evaluates the alignment between LLMs' stated values and their actions, introducing a framework called ValueActionLens for this assessment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the discrepancy between LLMs' stated values and their actions, addressing the 'Value-Action Gap' from psychology.

**Method:** The authors developed ValueActionLens, creating a dataset of 14.8k actions across cultures and topics, and set up two evaluation tasks based on alignment measures.

**Key Contributions:**

	1. Introduction of the ValueActionLens framework for evaluation of LLMs' values and actions.
	2. Creation of a comprehensive dataset of value-informed actions across diverse cultures.
	3. Identification of misalignment patterns and potential harms from value-action gaps.

**Result:** Experiments revealed significant misalignment between LLMs' stated values and their actions, indicating potential harms from such gaps.

**Limitations:** The study notes that the alignment varies significantly across scenarios and models, suggesting that one-size-fits-all solutions may be inadequate.

**Conclusion:** The reliance on LLMs' stated values without considering context can lead to erroneous assumptions about their behavior; reasoned explanations can enhance alignment predictions.

**Abstract:** Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the "Value-Action Gap," a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals' stated values and their actions in real-world contexts. To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values? This study introduces ValueActionLens, an evaluation framework to assess the alignment between LLMs' stated values and their value-informed actions. The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs' stated value inclinations and value-informed actions align across three different alignment measures. Extensive experiments reveal that the alignment between LLMs' stated values and actions is sub-optimal, varying significantly across scenarios and models. Analysis of misaligned results identifies potential harms from certain value-action gaps. To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves performance. These findings underscore the risks of relying solely on the LLMs' stated values to predict their behaviors and emphasize the importance of context-aware evaluations of LLM values and value-action gaps.

</details>


### [19] [Mind the Value-Action Gap: Do LLMs Act in Alignment with Their Values?](https://arxiv.org/abs/2501.15463)

*Hua Shen, Nicholas Clark, Tanushree Mitra*

**Main category:** cs.HC

**Keywords:** LLM, Value-Action Gap, evaluation framework, value-informed actions, context-aware assessments

**Relevance Score:** 9

**TL;DR:** This paper introduces ValueActionLens, a framework for evaluating the alignment between the stated values and actions of LLMs, revealing significant misalignments and the risks they pose.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the discrepancy between the stated values of large language models (LLMs) and their actual behaviors, analogous to the 'Value-Action Gap' in psychology.

**Method:** The study develops the ValueActionLens framework, generating a dataset of 14.8k value-informed actions across various cultures and topics, and conducts experiments to measure alignment across different scenarios and models.

**Key Contributions:**

	1. Introduction of the ValueActionLens framework for LLM evaluation
	2. Creation of a dataset for value-informed actions
	3. Demonstration of the risks associated with value-action gaps in LLMs

**Result:** Experiments show that LLMs display a sub-optimal alignment between stated values and actions, with significant variations across different contexts and models, indicating potential harms from these gaps.

**Limitations:** The study focuses on a specific set of cultures and social topics, which may limit generalizability.

**Conclusion:** The study emphasizes that relying on LLMs' stated values for behavioral predictions can be misleading and highlights the importance of context-aware evaluations.

**Abstract:** Existing research primarily evaluates the values of LLMs by examining their stated inclinations towards specific values. However, the "Value-Action Gap," a phenomenon rooted in environmental and social psychology, reveals discrepancies between individuals' stated values and their actions in real-world contexts. To what extent do LLMs exhibit a similar gap between their stated values and their actions informed by those values? This study introduces ValueActionLens, an evaluation framework to assess the alignment between LLMs' stated values and their value-informed actions. The framework encompasses the generation of a dataset comprising 14.8k value-informed actions across twelve cultures and eleven social topics, and two tasks to evaluate how well LLMs' stated value inclinations and value-informed actions align across three different alignment measures. Extensive experiments reveal that the alignment between LLMs' stated values and actions is sub-optimal, varying significantly across scenarios and models. Analysis of misaligned results identifies potential harms from certain value-action gaps. To predict the value-action gaps, we also uncover that leveraging reasoned explanations improves performance. These findings underscore the risks of relying solely on the LLMs' stated values to predict their behaviors and emphasize the importance of context-aware evaluations of LLM values and value-action gaps.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [20] [Bilingual Word Level Language Identification for Omotic Languages](https://arxiv.org/abs/2509.07998)

*Mesay Gemeda Yigezu, Girma Yohannis Bade, Atnafu Lambebo Tonja, Olga Kolesnikova, Grigori Sidorov, Alexander Gelbukh*

**Main category:** cs.CL

**Keywords:** Bilingual Language Identification, Wolaita, Gofa, BERT, LSTM

**Relevance Score:** 4

**TL;DR:** This paper addresses bilingual language identification (BLID) for the Wolaita and Gofa languages in Ethiopia using a combination of BERT and LSTM, achieving an F1 score of 0.72.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To identify and distinguish between two languages in multilingual texts, specifically focusing on Wolaita and Gofa languages in Ethiopia.

**Method:** The study employed various experiments combining a BERT-based pretrained language model with an LSTM approach for language identification.

**Key Contributions:**

	1. Introduction of BLID for Wolaita and Gofa languages
	2. Combination approach using BERT and LSTM
	3. Achievement of a competitive F1 score for language identification tasks

**Result:** The combination of BERT and LSTM yielded an F1 score of 0.72 on the test set, demonstrating effective performance in BLID.

**Limitations:** 

**Conclusion:** The findings will aid in addressing social media issues and provide a foundation for further research in bilingual language identification.

**Abstract:** Language identification is the task of determining the languages for a given text. In many real world scenarios, text may contain more than one language, particularly in multilingual communities. Bilingual Language Identification (BLID) is the task of identifying and distinguishing between two languages in a given text. This paper presents BLID for languages spoken in the southern part of Ethiopia, namely Wolaita and Gofa. The presence of words similarities and differences between the two languages makes the language identification task challenging. To overcome this challenge, we employed various experiments on various approaches. Then, the combination of the BERT based pretrained language model and LSTM approach performed better, with an F1 score of 0.72 on the test set. As a result, the work will be effective in tackling unwanted social media issues and providing a foundation for further research in this area.

</details>


### [21] [AntiDote: Bi-level Adversarial Training for Tamper-Resistant LLMs](https://arxiv.org/abs/2509.08000)

*Debdeep Sanyal, Manodeep Ray, Murari Mandal*

**Main category:** cs.CL

**Keywords:** large language models, robustness, adversarial attacks, safety, optimization

**Relevance Score:** 8

**TL;DR:** Introducing AntiDote, a bi-level optimization method that enhances the robustness of open-weight LLMs against tampering and adversarial attacks while maintaining performance integrity.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To create a balance between accessible research through open-weight models and preventing their misuse via malicious fine-tuning.

**Method:** AntiDote employs a bi-level optimization approach with an auxiliary adversary hypernetwork, generating malicious weights to condition the defender model's training, thus enhancing its resilience to attacks.

**Key Contributions:**

	1. Introduction of AntiDote for LLM robustness against adversarial attacks
	2. Demonstrated performance with a diverse suite of attacks
	3. Minimal trade-off in model utility while enhancing safety

**Result:** AntiDote significantly improves robustness against 52 red-teaming attacks, being 27.4% more resistant compared to traditional tamper-resistance and unlearning baselines, while performance degradation is kept below 0.5% across various benchmarks.

**Limitations:** 

**Conclusion:** AntiDote provides a compute-efficient method to develop open-weight models prioritizing safety without substantial losses in utility, making it a practical choice for model training.

**Abstract:** The release of open-weight large language models (LLMs) creates a tension between advancing accessible research and preventing misuse, such as malicious fine-tuning to elicit harmful content. Current safety measures struggle to preserve the general capabilities of the LLM while resisting a determined adversary with full access to the model's weights and architecture, who can use full-parameter fine-tuning to erase existing safeguards. To address this, we introduce AntiDote, a bi-level optimization procedure for training LLMs to be resistant to such tampering. AntiDote involves an auxiliary adversary hypernetwork that learns to generate malicious Low-Rank Adaptation (LoRA) weights conditioned on the defender model's internal activations. The defender LLM is then trained with an objective to nullify the effect of these adversarial weight additions, forcing it to maintain its safety alignment. We validate this approach against a diverse suite of 52 red-teaming attacks, including jailbreak prompting, latent space manipulation, and direct weight-space attacks. AntiDote is upto 27.4\% more robust against adversarial attacks compared to both tamper-resistance and unlearning baselines. Crucially, this robustness is achieved with a minimal trade-off in utility, incurring a performance degradation of upto less than 0.5\% across capability benchmarks including MMLU, HellaSwag, and GSM8K. Our work offers a practical and compute efficient methodology for building open-weight models where safety is a more integral and resilient property.

</details>


### [22] [MVPBench: A Benchmark and Fine-Tuning Framework for Aligning Large Language Models with Diverse Human Values](https://arxiv.org/abs/2509.08022)

*Yao Liang, Dongcheng Zhao, Feifei Zhao, Guobin Shen, Yuwei Wang, Dongqi Liang, Yi Zeng*

**Main category:** cs.CL

**Keywords:** Large Language Models, Value Alignment, Benchmarking, Cultural Diversity, AI Development

**Relevance Score:** 9

**TL;DR:** MVPBench is a new benchmark for evaluating how well large language models align with human values across 75 countries, highlighting disparities and proposing methods for improving alignment.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of cultural and demographic diversity in existing benchmarks for aligning large language models (LLMs) with human values.

**Method:** Development of MVPBench, a benchmark with 24,020 annotated instances for evaluating LLM alignment through demographic metadata and personalized questions; analysis of LLMs using lightweight fine-tuning methods.

**Key Contributions:**

	1. Introduction of MVPBench with 24,020 instances for global human value alignment evaluation.
	2. Demonstration of disparities in LLM performance based on geography and demographics.
	3. Evidence that lightweight fine-tuning methods enhance model value alignment.

**Result:** Substantial disparities in alignment performance of LLMs were found across different geographic and demographic groups; lightweight fine-tuning methods significantly improved alignment outcomes.

**Limitations:** 

**Conclusion:** The results highlight the need for population-aware alignment evaluation and emphasize the importance of building culturally adaptive LLMs; MVPBench serves as a foundation for future AI development focused on equitable value alignment.

**Abstract:** The alignment of large language models (LLMs) with human values is critical for their safe and effective deployment across diverse user populations. However, existing benchmarks often neglect cultural and demographic diversity, leading to limited understanding of how value alignment generalizes globally. In this work, we introduce MVPBench, a novel benchmark that systematically evaluates LLMs' alignment with multi-dimensional human value preferences across 75 countries. MVPBench contains 24,020 high-quality instances annotated with fine-grained value labels, personalized questions, and rich demographic metadata, making it the most comprehensive resource of its kind to date. Using MVPBench, we conduct an in-depth analysis of several state-of-the-art LLMs, revealing substantial disparities in alignment performance across geographic and demographic lines. We further demonstrate that lightweight fine-tuning methods, such as Low-Rank Adaptation (LoRA) and Direct Preference Optimization (DPO), can significantly enhance value alignment in both in-domain and out-of-domain settings. Our findings underscore the necessity for population-aware alignment evaluation and provide actionable insights for building culturally adaptive and value-sensitive LLMs. MVPBench serves as a practical foundation for future research on global alignment, personalized value modeling, and equitable AI development.

</details>


### [23] [NOWJ@COLIEE 2025: A Multi-stage Framework Integrating Embedding Models and Large Language Models for Legal Retrieval and Entailment](https://arxiv.org/abs/2509.08025)

*Hoang-Trung Nguyen, Tan-Minh Nguyen, Xuan-Bach Le, Tuan-Kiet Le, Khanh-Huyen Nguyen, Ha-Thanh Nguyen, Thi-Hai-Yen Vuong, Le-Minh Nguyen*

**Main category:** cs.CL

**Keywords:** Legal Case Entailment, Information Retrieval, Large Language Models

**Relevance Score:** 5

**TL;DR:** This paper summarizes the NOWJ team's methodologies and results in the COLIEE 2025 competition, focusing on advancements in Legal Case Entailment, leveraging various models and techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To advance legal information processing by integrating traditional information retrieval techniques with contemporary generative models.

**Method:** A two-stage retrieval system combining lexical-semantic filtering and contextualized LLM analysis, alongside various pre-ranking models and embedding-based representations.

**Key Contributions:**

	1. First place in Legal Case Entailment with innovative retrieval methods
	2. Integration of traditional IR methods with modern AI models
	3. Deployment of advanced LLMs and context-aware reasoning strategies

**Result:** Achieved first place in the Legal Case Entailment task with an F1 score of 0.3195, demonstrating strong performance across multiple tasks and robust methodology.

**Limitations:** 

**Conclusion:** The study showcases the effectiveness of hybrid models in legal information tasks and suggests a path for future methodologies in this domain.

**Abstract:** This paper presents the methodologies and results of the NOWJ team's participation across all five tasks at the COLIEE 2025 competition, emphasizing advancements in the Legal Case Entailment task (Task 2). Our comprehensive approach systematically integrates pre-ranking models (BM25, BERT, monoT5), embedding-based semantic representations (BGE-m3, LLM2Vec), and advanced Large Language Models (Qwen-2, QwQ-32B, DeepSeek-V3) for summarization, relevance scoring, and contextual re-ranking. Specifically, in Task 2, our two-stage retrieval system combined lexical-semantic filtering with contextualized LLM analysis, achieving first place with an F1 score of 0.3195. Additionally, in other tasks--including Legal Case Retrieval, Statute Law Retrieval, Legal Textual Entailment, and Legal Judgment Prediction--we demonstrated robust performance through carefully engineered ensembles and effective prompt-based reasoning strategies. Our findings highlight the potential of hybrid models integrating traditional IR techniques with contemporary generative models, providing a valuable reference for future advancements in legal information processing.

</details>


### [24] [SciGPT: A Large Language Model for Scientific Literature Understanding and Knowledge Discovery](https://arxiv.org/abs/2509.08032)

*Fengyu She, Nan Wang, Hongfei Wu, Ziyi Wan, Jingmian Wang, Chang Wang*

**Main category:** cs.CL

**Keywords:** SciGPT, Large Language Models, ScienceBench, domain adaptation, AI in science

**Relevance Score:** 8

**TL;DR:** The paper introduces SciGPT, a domain-adapted model for understanding scientific literature and ScienceBench, a benchmark to evaluate scientific LLMs, aimed at improving performance in science-specific tasks.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To address the inefficiencies in synthesizing scientific knowledge due to the limitations of general-purpose Large Language Models in handling domain-specific nuances.

**Method:** SciGPT is built on the Qwen3 architecture and features a two-stage low-cost domain distillation process, a Sparse Mixture-of-Experts attention mechanism for memory efficiency, and knowledge-aware adaptation using domain ontologies.

**Key Contributions:**

	1. Introduction of a domain-adapted foundation model for scientific literature understanding.
	2. Development of ScienceBench, an open-source benchmark for evaluating scientific LLMs.
	3. Innovative low-cost domain distillation and memory-efficient attention mechanisms.

**Result:** SciGPT outperforms GPT-4o in key scientific tasks such as sequence labeling, generation, and inference on the ScienceBench, showing robust performance in unseen tasks.

**Limitations:** 

**Conclusion:** SciGPT demonstrates the potential for AI-augmented scientific discovery by effectively addressing the limitations of existing LLMs in the scientific domain.

**Abstract:** Scientific literature is growing exponentially, creating a critical bottleneck for researchers to efficiently synthesize knowledge. While general-purpose Large Language Models (LLMs) show potential in text processing, they often fail to capture scientific domain-specific nuances (e.g., technical jargon, methodological rigor) and struggle with complex scientific tasks, limiting their utility for interdisciplinary research. To address these gaps, this paper presents SciGPT, a domain-adapted foundation model for scientific literature understanding and ScienceBench, an open source benchmark tailored to evaluate scientific LLMs.   Built on the Qwen3 architecture, SciGPT incorporates three key innovations: (1) low-cost domain distillation via a two-stage pipeline to balance performance and efficiency; (2) a Sparse Mixture-of-Experts (SMoE) attention mechanism that cuts memory consumption by 55\% for 32,000-token long-document reasoning; and (3) knowledge-aware adaptation integrating domain ontologies to bridge interdisciplinary knowledge gaps.   Experimental results on ScienceBench show that SciGPT outperforms GPT-4o in core scientific tasks including sequence labeling, generation, and inference. It also exhibits strong robustness in unseen scientific tasks, validating its potential to facilitate AI-augmented scientific discovery.

</details>


### [25] [No for Some, Yes for Others: Persona Prompts and Other Sources of False Refusal in Language Models](https://arxiv.org/abs/2509.08075)

*Flor Miriam Plaza-del-Arco, Paul Röttger, Nino Scherrer, Emanuele Borgonovo, Elmar Plischke, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** Large Language Models, False Refusal, Sociodemographic Personas, Bias, Model Safety

**Relevance Score:** 8

**TL;DR:** This paper investigates the impact of sociodemographic personas on false refusals in large language models, revealing that model capability reduces persona influence.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the growing integration of LLMs in daily life, it is crucial to understand how personalization through sociodemographic personas can inadvertently lead to false refusals in user requests.

**Method:** The study measures the impact of 15 sociodemographic personas across 16 different models and 3 tasks, utilizing a Monte Carlo-based method to quantify false refusals in a sample-efficient manner.

**Key Contributions:**

	1. Quantified the impact of sociodemographic personas on false refusals in LLMs.
	2. Developed a sample-efficient Monte Carlo-based method for measuring false refusals.
	3. Showed that model capability mitigates the impact of personas on refusal rates.

**Result:** The analysis indicates that more capable models are less influenced by personas, yet certain personas increase false refusals in specific models, highlighting biases in alignment strategies and safety mechanisms.

**Limitations:** The study focuses on a limited set of personas and models, which may not represent all aspects of LLM behavior.

**Conclusion:** The research suggests that the effects of personas on false refusals may have been overestimated, attributed instead to model choice and task specifics.

**Abstract:** Large language models (LLMs) are increasingly integrated into our daily lives and personalized. However, LLM personalization might also increase unintended side effects. Recent work suggests that persona prompting can lead models to falsely refuse user requests. However, no work has fully quantified the extent of this issue. To address this gap, we measure the impact of 15 sociodemographic personas (based on gender, race, religion, and disability) on false refusal. To control for other factors, we also test 16 different models, 3 tasks (Natural Language Inference, politeness, and offensiveness classification), and nine prompt paraphrases. We propose a Monte Carlo-based method to quantify this issue in a sample-efficient manner. Our results show that as models become more capable, personas impact the refusal rate less and less. Certain sociodemographic personas increase false refusal in some models, which suggests underlying biases in the alignment strategies or safety mechanisms. However, we find that the model choice and task significantly influence false refusals, especially in sensitive content tasks. Our findings suggest that persona effects have been overestimated, and might be due to other factors.

</details>


### [26] [Culturally transmitted color categories in LLMs reflect a learning bias toward efficient compression](https://arxiv.org/abs/2509.08093)

*Nathaniel Imel, Noga Zaslavsky*

**Main category:** cs.CL

**Keywords:** semantic categories, large language models, Information Bottleneck, color naming, cultural evolution

**Relevance Score:** 9

**TL;DR:** This paper investigates whether large language models (LLMs) can evolve efficient semantic systems similar to those found in human languages, using color categorization as a test case.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to understand if LLMs, not designed with the Information Bottleneck principle in mind, can develop human-like semantic system efficiencies.

**Method:** The authors replicate human color-naming studies using Gemini 2.0-flash and Llama 3.3-70B-Instruct, measuring their IB-efficiency and alignment with native English speakers.

**Key Contributions:**

	1. Demonstration of LLMs' ability to approximate human-like semantic categorization.
	2. Evaluation of color naming accuracy and efficiency based on the Information Bottleneck principle.
	3. Simulation of cultural evolution in LLMs leading to improved semantic systems.

**Result:** Gemini achieves a high IB-efficiency score and aligns closely with human color naming, while Llama exhibits efficiency but lower complexity. Both models evolve grammar-like structures through simulated cultural evolution.

**Limitations:** The study is limited to color categorization and may not generalize to other semantic domains.

**Conclusion:** LLMs can evolve perceptually grounded semantic systems and demonstrate inductive biases toward IB-efficiency similar to humans.

**Abstract:** Converging evidence suggests that systems of semantic categories across human languages achieve near-optimal compression via the Information Bottleneck (IB) complexity-accuracy principle. Large language models (LLMs) are not trained for this objective, which raises the question: are LLMs capable of evolving efficient human-like semantic systems? To address this question, we focus on the domain of color as a key testbed of cognitive theories of categorization and replicate with LLMs (Gemini 2.0-flash and Llama 3.3-70B-Instruct) two influential human behavioral studies. First, we conduct an English color-naming study, showing that Gemini aligns well with the naming patterns of native English speakers and achieves a significantly high IB-efficiency score, while Llama exhibits an efficient but lower complexity system compared to English. Second, to test whether LLMs simply mimic patterns in their training data or actually exhibit a human-like inductive bias toward IB-efficiency, we simulate cultural evolution of pseudo color-naming systems in LLMs via iterated in-context language learning. We find that akin to humans, LLMs iteratively restructure initially random systems towards greater IB-efficiency and increased alignment with patterns observed across the world's languages. These findings demonstrate that LLMs are capable of evolving perceptually grounded, human-like semantic systems, driven by the same fundamental principle that governs semantic efficiency across human languages.

</details>


### [27] [MERLIN: Multi-Stage Curriculum Alignment for Multilingual Encoder and LLM Fusion](https://arxiv.org/abs/2509.08105)

*Kosei Uemura, David Guzmán, Quang Phuoc Nguyen, Jesujoba Oluwadara Alabi, En-shiun Annie Lee, David Ifeoluwa Adelani*

**Main category:** cs.CL

**Keywords:** low-resource languages, large language models, curriculum learning, DoRA weights, benchmarking

**Relevance Score:** 6

**TL;DR:** MERLIN improves reasoning in low-resource languages through a two-stage model-stacking framework.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance reasoning accuracy in low-resource languages (LRLs) using large language models, which struggle compared to mid and high-resource languages.

**Method:** A two-stage model-stacking framework employing a curriculum learning strategy, adapting a small set of DoRA weights.

**Key Contributions:**

	1. Introduction of a two-stage model-stacking framework for LRLs
	2. Application of curriculum learning strategy to improve accuracy
	3. Demonstrated superior performance over existing models like MindMerger and GPT-4o-mini.

**Result:** MERLIN achieves a +12.9 percentage point improvement in exact-match accuracy on the AfriMGSM benchmark over existing methods, outperforming GPT-4o-mini and showing consistent gains on other benchmarks.

**Limitations:** 

**Conclusion:** The proposed MERLIN framework effectively enhances model performance in both low and high-resource language settings.

**Abstract:** Large language models excel in English but still struggle with complex reasoning in many low-resource languages (LRLs). Existing encoder-plus-decoder methods such as LangBridge and MindMerger raise accuracy on mid and high-resource languages, yet they leave a large gap on LRLs. We present MERLIN, a two-stage model-stacking framework that applies a curriculum learning strategy -- from general bilingual bitext to task-specific data -- and adapts only a small set of DoRA weights. On the AfriMGSM benchmark MERLIN improves exact-match accuracy by +12.9 pp over MindMerger and outperforms GPT-4o-mini. It also yields consistent gains on MGSM and MSVAMP (+0.9 and +2.8 pp), demonstrating effectiveness across both low and high-resource settings.

</details>


### [28] [Bias after Prompting: Persistent Discrimination in Large Language Models](https://arxiv.org/abs/2509.08146)

*Nivedha Sivakumar, Natalie Mackraz, Samira Khorshidi, Krishna Patel, Barry-John Theobald, Luca Zappella, Nicholas Apostoloff*

**Main category:** cs.CL

**Keywords:** bias transfer, large language models, prompt adaptation, debiasing strategies, machine learning

**Relevance Score:** 9

**TL;DR:** This paper investigates the bias transfer hypothesis (BTH) in large language models (LLMs) during prompt adaptation, revealing that biases can transfer and current debiasing strategies are ineffective.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To challenge the assumption that biases from pre-trained LLMs do not transfer to adapted models and to understand the impact of prompt adaptations on bias transfer.

**Method:** The authors conducted empirical studies examining biases in causal models under prompt adaptations, analyzing correlations of biases across various demographics and tasks, and evaluating prompt-based debiasing strategies.

**Key Contributions:**

	1. Empirical validation of bias transfer through prompt adaptations in LLMs.
	2. Quantitative analysis of correlations between intrinsic and adapted biases across demographics.
	3. Critical evaluation of various prompt-based debiasing strategies.

**Result:** The study found strong correlations between intrinsic biases and prompt-adapted biases across demographics (e.g., gender, age, religion) with moderate to strong correlation coefficients, indicating that biases can indeed transfer through prompting.

**Limitations:** The effectiveness of prompt-based mitigation methods varies, and none consistently reduce bias transfer across models, tasks, or demographics.

**Conclusion:** Addressing biases in intrinsic models may help prevent their propagation to downstream tasks, although current prompt-based debiasing strategies do not consistently mitigate bias transfer.

**Abstract:** A dangerous assumption that can be made from prior work on the bias transfer hypothesis (BTH) is that biases do not transfer from pre-trained large language models (LLMs) to adapted models. We invalidate this assumption by studying the BTH in causal models under prompt adaptations, as prompting is an extremely popular and accessible adaptation strategy used in real-world applications. In contrast to prior work, we find that biases can transfer through prompting and that popular prompt-based mitigation methods do not consistently prevent biases from transferring. Specifically, the correlation between intrinsic biases and those after prompt adaptation remain moderate to strong across demographics and tasks -- for example, gender (rho >= 0.94) in co-reference resolution, and age (rho >= 0.98) and religion (rho >= 0.69) in question answering. Further, we find that biases remain strongly correlated when varying few-shot composition parameters, such as sample size, stereotypical content, occupational distribution and representational balance (rho >= 0.90). We evaluate several prompt-based debiasing strategies and find that different approaches have distinct strengths, but none consistently reduce bias transfer across models, tasks or demographics. These results demonstrate that correcting bias, and potentially improving reasoning ability, in intrinsic models may prevent propagation of biases to downstream tasks.

</details>


### [29] [Verbalized Algorithms](https://arxiv.org/abs/2509.08150)

*Supriya Lall, Christian Farrell, Hari Pathanjaly, Marko Pavic, Sarvesh Chezhian, Masataro Asai*

**Main category:** cs.CL

**Keywords:** verbalized algorithms, large language models, reasoning tasks, sorting, clustering

**Relevance Score:** 8

**TL;DR:** This paper introduces 'verbalized algorithms' (VAs) that leverage classical algorithms to improve the reliability of large language models (LLMs) for reasoning tasks by decomposing tasks into simpler operations.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to enhance LLMs' reliability for reasoning tasks by utilizing established theoretical frameworks of classical algorithms.

**Method:** The paper presents a paradigm called verbalized algorithms (VAs) that breaks down reasoning tasks into elementary operations using LLMs as oracles for these operations.

**Key Contributions:**

	1. Introduction of verbalized algorithms for LLMs
	2. Demonstration of VAs in sorting and clustering tasks
	3. Reliability of LLMs enhanced through task decomposition

**Result:** The authors demonstrate the effectiveness of VAs in applications such as sorting and clustering, showcasing improved performance by limiting the LLM’s scope to these structured tasks.

**Limitations:** 

**Conclusion:** The approach suggests that integrating classical algorithms with LLMs can yield reliable results for specific reasoning tasks.

**Abstract:** Instead of querying LLMs in a one-shot manner and hoping to get the right answer for a reasoning task, we propose a paradigm we call \emph{verbalized algorithms} (VAs), which leverage classical algorithms with established theoretical understanding. VAs decompose a task into simple elementary operations on natural language strings that they should be able to answer reliably, and limit the scope of LLMs to only those simple tasks. For example, for sorting a series of natural language strings, \emph{verbalized sorting} uses an LLM as a binary comparison oracle in a known and well-analyzed sorting algorithm (e.g., bitonic sorting network). We demonstrate the effectiveness of this approach on sorting and clustering tasks.

</details>


### [30] [Balancing Quality and Variation: Spam Filtering Distorts Data Label Distributions](https://arxiv.org/abs/2509.08217)

*Eve Fleisig, Matthias Orlikowski, Philipp Cimiano, Dan Klein*

**Main category:** cs.CL

**Keywords:** machine learning, annotator filtering, label diversity, subjective tasks, spam detection

**Relevance Score:** 8

**TL;DR:** This paper examines how annotator filtering heuristics impact the variation in subjective machine learning datasets, finding that many methods reduce diversity by incorrectly removing dissenting annotators instead of spam.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the representation of diverse opinions in machine learning datasets by ensuring a balance between annotator reliability and data label variation.

**Method:** Empirical evaluation of various annotator filtering heuristics to determine their effects on the preservation of variation in subjective tasks.

**Key Contributions:**

	1. Identified optimal thresholds for annotator removal that preserve label diversity.
	2. Revealed the shortcomings of current spam filtering methods in subjective tasks.
	3. Proposed the need for spam removal techniques that account for the randomness of spammers.

**Result:** Conservative annotator removal settings (<5%) are optimal for maintaining label diversity, while most tested methods worsen mean absolute error beyond this point.

**Limitations:** 

**Conclusion:** Current spam filtering methods often misidentify spam annotators as they assume them to be less random than they are, suggesting a need for new approaches that prioritize label diversity.

**Abstract:** For machine learning datasets to accurately represent diverse opinions in a population, they must preserve variation in data labels while filtering out spam or low-quality responses. How can we balance annotator reliability and representation? We empirically evaluate how a range of heuristics for annotator filtering affect the preservation of variation on subjective tasks. We find that these methods, designed for contexts in which variation from a single ground-truth label is considered noise, often remove annotators who disagree instead of spam annotators, introducing suboptimal tradeoffs between accuracy and label diversity. We find that conservative settings for annotator removal (<5%) are best, after which all tested methods increase the mean absolute error from the true average label. We analyze performance on synthetic spam to observe that these methods often assume spam annotators are less random than real spammers tend to be: most spammers are distributionally indistinguishable from real annotators, and the minority that are distinguishable tend to give fixed answers, not random ones. Thus, tasks requiring the preservation of variation reverse the intuition of existing spam filtering methods: spammers tend to be less random than non-spammers, so metrics that assume variation is spam fare worse. These results highlight the need for spam removal methods that account for label diversity.

</details>


### [31] [Towards Knowledge-Aware Document Systems: Modeling Semantic Coverage Relations via Answerability Detection](https://arxiv.org/abs/2509.08304)

*Yehudit Aperstein, Alon Gottlib, Gal Benita, Alexander Apartsin*

**Main category:** cs.CL

**Keywords:** Semantic Coverage Relations, question answering, document classification, generative models, discriminative models

**Relevance Score:** 7

**TL;DR:** A framework for modeling Semantic Coverage Relations (SCR) in documents using a question answering approach to classify document pairs based on information alignment.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding information sharing across documents is essential for tasks like information retrieval and summarization.

**Method:** We utilize a question answering-based approach, classifying document pairs into equivalence, inclusion, and semantic overlap by analyzing the answerability of shared questions.

**Key Contributions:**

	1. Introduction of the Semantic Coverage Relations framework
	2. Benchmarking generative and discriminative models using a synthetic dataset
	3. Public dataset and code for SCR prediction

**Result:** Discriminative models significantly outperform generative ones; RoBERTa-base achieved 61.4% accuracy, and Random Forest reached a macro-F1 score of 52.9%.

**Limitations:** The RoBERTa-base model only achieved 61.4% accuracy, indicating room for improvement in understanding semantic coverage.

**Conclusion:** The findings underscore the effectiveness of QA in assessing semantic relations beyond surface similarity, with a publicly available dataset for reproducibility.

**Abstract:** Understanding how information is shared across documents, regardless of the format in which it is expressed, is critical for tasks such as information retrieval, summarization, and content alignment. In this work, we introduce a novel framework for modelling Semantic Coverage Relations (SCR), which classifies document pairs based on how their informational content aligns. We define three core relation types: equivalence, where both texts convey the same information using different textual forms or styles; inclusion, where one document fully contains the information of another and adds more; and semantic overlap, where each document presents partially overlapping content. To capture these relations, we adopt a question answering (QA)-based approach, using the answerability of shared questions across documents as an indicator of semantic coverage. We construct a synthetic dataset derived from the SQuAD corpus by paraphrasing source passages and selectively omitting information, enabling precise control over content overlap. This dataset allows us to benchmark generative language models and train transformer-based classifiers for SCR prediction. Our findings demonstrate that discriminative models significantly outperform generative approaches, with the RoBERTa-base model achieving the highest accuracy of 61.4% and the Random Forest-based model showing the best balance with a macro-F1 score of 52.9%. The results show that QA provides an effective lens for assessing semantic relations across stylistically diverse texts, offering insights into the capacity of current models to reason about information beyond surface similarity. The dataset and code developed in this study are publicly available to support reproducibility.

</details>


### [32] [Toward Subtrait-Level Model Explainability in Automated Writing Evaluation](https://arxiv.org/abs/2509.08345)

*Alejandro Andrade-Lotero, Lee Becker, Joshua Southerland, Scott Hellman*

**Main category:** cs.CL

**Keywords:** automated writing assessment, explainability, generative language models

**Relevance Score:** 7

**TL;DR:** This paper explores subtrait assessment using generative language models to enhance the transparency of automated writing scores, revealing correlations between human and automated scoring.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The goal is to improve transparency in automated writing assessments by introducing subtrait scoring with generative language models.

**Method:** The paper presents a prototype of explainability and subtrait scoring, analyzing correlations between human subtrait scores and those generated by automated systems.

**Key Contributions:**

	1. Prototype for explainability in writing scores
	2. Introduction of subtrait scoring with generative models
	3. Correlations found between human and automated scoring methods

**Result:** The study found a modest correlation between human and automated scores for both subtraits and overall traits, indicating some level of alignment between the two assessment methods.

**Limitations:** 

**Conclusion:** The approach offers deeper insights into writing scores, benefiting educators and students by making automated assessments more understandable.

**Abstract:** Subtrait (latent-trait components) assessment presents a promising path toward enhancing transparency of automated writing scores. We prototype explainability and subtrait scoring with generative language models and show modest correlation between human subtrait and trait scores, and between automated and human subtrait scores. Our approach provides details to demystify scores for educators and students.

</details>


### [33] [Automatic Detection of Inauthentic Templated Responses in English Language Assessments](https://arxiv.org/abs/2509.08355)

*Yashad Samant, Lee Becker, Scott Hellman, Bradley Behan, Sarah Hughes, Joshua Southerland*

**Main category:** cs.CL

**Keywords:** automated scoring, language assessment, machine learning, templated responses, model updating

**Relevance Score:** 6

**TL;DR:** The paper discusses a method for detecting templated responses in English Language Assessments to improve automated scoring systems.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The prevalence of low-skill test takers using memorized templates to deceive automated scoring systems necessitates the development of detection methods.

**Method:** A machine learning-based approach is presented for the automated detection of inauthentic, templated responses (AuDITR).

**Key Contributions:**

	1. Introduction of the AuDITR task for detecting templated responses.
	2. Development of a machine learning approach for response detection.
	3. Emphasis on the necessity of model updates in production environments.

**Result:** The study demonstrates the effectiveness of the presented detection method and emphasizes the importance of regularly updating the models in production.

**Limitations:** 

**Conclusion:** Regular updates to detection models improve the reliability of automated scoring in assessments.

**Abstract:** In high-stakes English Language Assessments, low-skill test takers may employ memorized materials called ``templates'' on essay questions to ``game'' or fool the automated scoring system. In this study, we introduce the automated detection of inauthentic, templated responses (AuDITR) task, describe a machine learning-based approach to this task and illustrate the importance of regularly updating these models in production.

</details>


### [34] [<think> So let's replace this phrase with insult... </think> Lessons learned from generation of toxic texts with LLMs](https://arxiv.org/abs/2509.08358)

*Sergey Pletenev, Daniil Moskovskiy, Alexander Panchenko*

**Main category:** cs.CL

**Keywords:** Large Language Models, Toxicity Detection, Synthetic Data, Detoxification, Natural Language Processing

**Relevance Score:** 8

**TL;DR:** The paper investigates the use of LLM-generated synthetic toxic data for training detoxification models and finds significant performance drops compared to models trained on human data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in research on the application of LLMs in sensitive domains like text detoxification and to explore the potential of synthetic data.

**Method:** Synthetic toxic data was generated using Llama 3 and Qwen models from neutral texts in the ParaDetox and SST-2 datasets. Performance was evaluated by fine-tuning models on this synthetic data and comparing them to those trained on human-generated data.

**Key Contributions:**

	1. Identified the performance gap between models trained on synthetic vs human data in detoxification tasks.
	2. Demonstrated that LLMs generate less varied toxic content, impacting the models' effectiveness.
	3. Highlighted the importance of human-annotated data for training robust detoxification models.

**Result:** Models trained on synthetic toxic data performed up to 30% worse in joint metrics than those trained on human data due to a critical lexical diversity gap in the generated content.

**Limitations:** The study primarily focuses on two datasets, which may limit generalizability; further research is needed with more diverse datasets.

**Conclusion:** The study emphasizes the limitations of current LLMs in generating toxic content and underscores the need for diverse, human-annotated data in developing effective detoxification systems.

**Abstract:** Modern Large Language Models (LLMs) are excellent at generating synthetic data. However, their performance in sensitive domains such as text detoxification has not received proper attention from the scientific community. This paper explores the possibility of using LLM-generated synthetic toxic data as an alternative to human-generated data for training models for detoxification. Using Llama 3 and Qwen activation-patched models, we generated synthetic toxic counterparts for neutral texts from ParaDetox and SST-2 datasets. Our experiments show that models fine-tuned on synthetic data consistently perform worse than those trained on human data, with a drop in performance of up to 30% in joint metrics. The root cause is identified as a critical lexical diversity gap: LLMs generate toxic content using a small, repetitive vocabulary of insults that fails to capture the nuances and variety of human toxicity. These findings highlight the limitations of current LLMs in this domain and emphasize the continued importance of diverse, human-annotated data for building robust detoxification systems.

</details>


### [35] [Low-Resource Fine-Tuning for Multi-Task Structured Information Extraction with a Billion-Parameter Instruction-Tuned Model](https://arxiv.org/abs/2509.08381)

*Yu Cheng Chih, Yong Hao Hou*

**Main category:** cs.CL

**Keywords:** structured data extraction, small language models, low-rank adaptation, JSON extraction, named entity recognition

**Relevance Score:** 8

**TL;DR:** This paper introduces ETLCH, a billion-parameter LLaMA-based model effectively fine-tuned for structured data extraction tasks, proving that smaller models can outperform larger ones at lower computational costs.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The need for effective structured data extraction in resource-constrained environments where larger language models are often impractical.

**Method:** The authors present ETLCH, fine-tuned using low-rank adaptation on a few hundred to one thousand samples per task for tasks including JSON extraction, knowledge graph extraction, and named entity recognition.

**Key Contributions:**

	1. Introduction of the ETLCH model for structured data extraction tasks.
	2. Demonstration of performance gains with fewer training samples compared to larger models.
	3. Evidence that small models can effectively operate under low-resource conditions.

**Result:** ETLCH demonstrated superior performance compared to strong baselines across various metrics, achieving notable accuracy even with minimal data.

**Limitations:** The paper mainly focuses on a limited number of tasks and the scalability of these results across more complex datasets is not tested extensively.

**Conclusion:** The study concludes that well-tuned small models can deliver reliable structured outputs economically, making them viable for small teams and low-resource contexts.

**Abstract:** Deploying large language models (LLMs) for structured data extraction in domains such as financial compliance reporting, legal document analytics, and multilingual knowledge base construction is often impractical for smaller teams due to the high cost of running large architectures and the difficulty of preparing large, high-quality datasets. Most recent instruction-tuning studies focus on seven-billion-parameter or larger models, leaving limited evidence on whether much smaller models can work reliably under low-resource, multi-task conditions. This work presents ETLCH, a billion-parameter LLaMA-based model fine-tuned with low-rank adaptation on only a few hundred to one thousand samples per task for JSON extraction, knowledge graph extraction, and named entity recognition. Despite its small scale, ETLCH outperforms strong baselines across most evaluation metrics, with substantial gains observed even at the lowest data scale. These findings demonstrate that well-tuned small models can deliver stable and accurate structured outputs at a fraction of the computational cost, enabling cost-effective and reliable information extraction pipelines in resource-constrained environments.

</details>


### [36] [CommonVoice-SpeechRE and RPG-MoGe: Advancing Speech Relation Extraction with a New Dataset and Multi-Order Generative Framework](https://arxiv.org/abs/2509.08438)

*Jinzhong Ning, Paerhati Tulajiang, Yingying Le, Yijia Zhang, Yuanyuan Sun, Hongfei Lin, Haifeng Liu*

**Main category:** cs.CL

**Keywords:** Speech Relation Extraction, CommonVoice-SpeechRE, Multi-Order Generative Ensemble

**Relevance Score:** 4

**TL;DR:** This paper introduces the CommonVoice-SpeechRE dataset and a novel framework, RPG-MoGe, for improving Speech Relation Extraction from real human speech.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** Existing Speech Relation Extraction datasets are limited in quantity and diversity, relying on synthetic data. There is a need for real human speech to improve model performance.

**Method:** The authors propose a novel framework, RPG-MoGe, which includes a multi-order triplet generation ensemble strategy and CNN-based latent relation prediction heads for better cross-modal alignment and triplet generation.

**Key Contributions:**

	1. Introduction of CommonVoice-SpeechRE, a dataset with 20,000 human speech samples.
	2. Development of RPG-MoGe framework for multi-order triplet generation.
	3. Enhanced cross-modal alignment through CNN-based relation prompts.

**Result:** Experiments show RPG-MoGe outperforms existing state-of-the-art methods in Speech Relation Extraction, utilizing a diverse dataset of 20,000 speech samples.

**Limitations:** 

**Conclusion:** The paper provides a new benchmark dataset and an effective solution for real-world Speech Relation Extraction tasks, available for public use.

**Abstract:** Speech Relation Extraction (SpeechRE) aims to extract relation triplets directly from speech. However, existing benchmark datasets rely heavily on synthetic data, lacking sufficient quantity and diversity of real human speech. Moreover, existing models also suffer from rigid single-order generation templates and weak semantic alignment, substantially limiting their performance. To address these challenges, we introduce CommonVoice-SpeechRE, a large-scale dataset comprising nearly 20,000 real-human speech samples from diverse speakers, establishing a new benchmark for SpeechRE research. Furthermore, we propose the Relation Prompt-Guided Multi-Order Generative Ensemble (RPG-MoGe), a novel framework that features: (1) a multi-order triplet generation ensemble strategy, leveraging data diversity through diverse element orders during both training and inference, and (2) CNN-based latent relation prediction heads that generate explicit relation prompts to guide cross-modal alignment and accurate triplet generation. Experiments show our approach outperforms state-of-the-art methods, providing both a benchmark dataset and an effective solution for real-world SpeechRE. The source code and dataset are publicly available at https://github.com/NingJinzhong/SpeechRE_RPG_MoGe.

</details>


### [37] [Adversarial Attacks Against Automated Fact-Checking: A Survey](https://arxiv.org/abs/2509.08463)

*Fanzhen Liu, Alsharif Abuadbba, Kristen Moore, Surya Nepal, Cecile Paris, Jia Wu, Jian Yang, Quan Z. Sheng*

**Main category:** cs.CL

**Keywords:** adversarial attacks, automated fact-checking, resilience, information verification, defenses

**Relevance Score:** 4

**TL;DR:** This paper reviews adversarial attacks on automated fact-checking systems, categorizing methodologies, evaluating impacts, and examining defenses.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Misinformation is widespread, and fact-checking is essential for verifying claims. However, automated fact-checking systems are vulnerable to adversarial attacks that can undermine their effectiveness.

**Method:** The paper conducts a comprehensive review of existing adversarial attack methodologies against automated fact-checking systems, categorizing them and assessing their impacts and resilience.

**Key Contributions:**

	1. First comprehensive review of adversarial attacks on automated fact-checking systems.
	2. Categorization of existing attack methodologies and evaluation of their impact.
	3. Discussion of recent advancements in defenses against adversarial attacks.

**Result:** The survey identifies key challenges in adversarial attacks on fact-checking systems and discusses recent advancements in adversary-aware defenses.

**Limitations:** The comprehensive overview needs further exploration of open research questions in the field.

**Conclusion:** There is an urgent need for developing resilient fact-checking frameworks that can withstand adversarial manipulations to maintain high verification accuracy.

**Abstract:** In an era where misinformation spreads freely, fact-checking (FC) plays a crucial role in verifying claims and promoting reliable information. While automated fact-checking (AFC) has advanced significantly, existing systems remain vulnerable to adversarial attacks that manipulate or generate claims, evidence, or claim-evidence pairs. These attacks can distort the truth, mislead decision-makers, and ultimately undermine the reliability of FC models. Despite growing research interest in adversarial attacks against AFC systems, a comprehensive, holistic overview of key challenges remains lacking. These challenges include understanding attack strategies, assessing the resilience of current models, and identifying ways to enhance robustness. This survey provides the first in-depth review of adversarial attacks targeting FC, categorizing existing attack methodologies and evaluating their impact on AFC systems. Additionally, we examine recent advancements in adversary-aware defenses and highlight open research questions that require further exploration. Our findings underscore the urgent need for resilient FC frameworks capable of withstanding adversarial manipulations in pursuit of preserving high verification accuracy.

</details>


### [38] [Acquiescence Bias in Large Language Models](https://arxiv.org/abs/2509.08480)

*Daniel Braun*

**Main category:** cs.CL

**Keywords:** acquiescence bias, Large Language Models, survey responses, LM interpretations, human-computer interaction

**Relevance Score:** 7

**TL;DR:** This study investigates acquiescence bias in Large Language Models (LLMs), revealing they tend to answer 'no' regardless of the context, unlike humans who tend to agree.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the presence of acquiescence bias in LLMs, which could have implications for their reliability in survey-related tasks.

**Method:** The study tests various LLMs across different tasks and languages (English, German, and Polish) to assess the tendency to show acquiescence bias.

**Key Contributions:**

	1. Demonstration of LLMs' unique response bias compared to humans
	2. Analysis across multiple languages and models
	3. Implications for the use of LLMs in data collection or survey tasks

**Result:** LLMs consistently showed a bias towards responding 'no', which contrasts with human behavior of agreeing more often in surveys.

**Limitations:** Study limited to three languages and specific models; findings may not generalize to all LLMs.

**Conclusion:** This research highlights significant differences in response tendencies between humans and LLMs, raising questions about the interpretability and dependability of LLM outputs.

**Abstract:** Acquiescence bias, i.e. the tendency of humans to agree with statements in surveys, independent of their actual beliefs, is well researched and documented. Since Large Language Models (LLMs) have been shown to be very influenceable by relatively small changes in input and are trained on human-generated data, it is reasonable to assume that they could show a similar tendency. We present a study investigating the presence of acquiescence bias in LLMs across different models, tasks, and languages (English, German, and Polish). Our results indicate that, contrary to humans, LLMs display a bias towards answering no, regardless of whether it indicates agreement or disagreement.

</details>


### [39] [Simulating Identity, Propagating Bias: Abstraction and Stereotypes in LLM-Generated Text](https://arxiv.org/abs/2509.08484)

*Pia Sommerauer, Giulia Rambelli, Tommaso Caselli*

**Main category:** cs.CL

**Keywords:** persona-prompting, linguistic abstraction, stereotyping, LLMs, Self-Stereo

**Relevance Score:** 8

**TL;DR:** This paper investigates the effects of persona-prompting on LLMs in relation to linguistic abstraction and stereotyping.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore how persona-prompting in LLMs may affect the representation of social groups and the risk of propagating stereotypes.

**Method:** The study employs the Linguistic Expectancy Bias framework to analyze outputs from six open-weight LLMs under three prompting conditions, comparing persona-driven responses with generic AI outputs.

**Key Contributions:**

	1. Introduction of the Self-Stereo dataset for analyzing stereotypes.
	2. Comparison of persona-driven versus generic responses from multiple LLMs.
	3. Evidence highlighting limitations of persona-prompting in reducing stereotyping.

**Result:** The analysis reveals limited effectiveness of persona-prompting on reducing linguistic abstraction related to stereotyping, highlighting concerns about its ecological validity.

**Limitations:** The study focuses only on short texts and may not generalize to other forms of LLM output.

**Conclusion:** Persona-prompting does not significantly mitigate abstraction in language, raising issues regarding its use in accurately representing socio-demographic groups and preventing stereotype propagation.

**Abstract:** Persona-prompting is a growing strategy to steer LLMs toward simulating particular perspectives or linguistic styles through the lens of a specified identity. While this method is often used to personalize outputs, its impact on how LLMs represent social groups remains underexplored. In this paper, we investigate whether persona-prompting leads to different levels of linguistic abstraction - an established marker of stereotyping - when generating short texts linking socio-demographic categories with stereotypical or non-stereotypical attributes. Drawing on the Linguistic Expectancy Bias framework, we analyze outputs from six open-weight LLMs under three prompting conditions, comparing 11 persona-driven responses to those of a generic AI assistant. To support this analysis, we introduce Self-Stereo, a new dataset of self-reported stereotypes from Reddit. We measure abstraction through three metrics: concreteness, specificity, and negation. Our results highlight the limits of persona-prompting in modulating abstraction in language, confirming criticisms about the ecology of personas as representative of socio-demographic groups and raising concerns about the risk of propagating stereotypes even when seemingly evoking the voice of a marginalized group.

</details>


### [40] [Too Helpful, Too Harmless, Too Honest or Just Right?](https://arxiv.org/abs/2509.08486)

*Gautam Siddharth Kashyap, Mark Dras, Usman Naseem*

**Main category:** cs.CL

**Keywords:** Large Language Models, Alignment, Mixture of Experts, Transformers, NLP

**Relevance Score:** 9

**TL;DR:** TrinityX is a modular alignment framework for large language models that addresses alignment challenges in helpfulness, harmfulness, and honesty by using a Mixture of Calibrated Experts (MoCaE).

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Aligning outputs of large language models with the principles of Helpfulness, Harmlessness, and Honesty (HHH) is essential yet challenging due to existing methods optimizing dimensions in isolation and issues with Mixture-of-Experts architectures.

**Method:** TrinityX incorporates a Mixture of Calibrated Experts (MoCaE) within a Transformer architecture, with experts trained separately for each HHH dimension and a calibrated, task-adaptive routing mechanism for output integration.

**Key Contributions:**

	1. Introduction of the TrinityX modular alignment framework
	2. Integration of separately trained experts for alignment dimensions
	3. Demonstrated significant performance improvements over existing methods

**Result:** TrinityX outperforms baselines across alignment benchmarks (Alpaca, BeaverTails, TruthfulQA) with relative improvements of 32.5% in win rate, 33.9% in safety score, and 28.4% in truthfulness, while also reducing memory usage and inference latency by over 40%.

**Limitations:** 

**Conclusion:** The proposed TrinityX framework effectively enhances the alignment capabilities of large language models by employing calibrated routing and demonstrates generalization across diverse LLMs.

**Abstract:** Large Language Models (LLMs) exhibit strong performance across a wide range of NLP tasks, yet aligning their outputs with the principles of Helpfulness, Harmlessness, and Honesty (HHH) remains a persistent challenge. Existing methods often optimize for individual alignment dimensions in isolation, leading to trade-offs and inconsistent behavior. While Mixture-of-Experts (MoE) architectures offer modularity, they suffer from poorly calibrated routing, limiting their effectiveness in alignment tasks. We propose TrinityX, a modular alignment framework that incorporates a Mixture of Calibrated Experts (MoCaE) within the Transformer architecture. TrinityX leverages separately trained experts for each HHH dimension, integrating their outputs through a calibrated, task-adaptive routing mechanism that combines expert signals into a unified, alignment-aware representation. Extensive experiments on three standard alignment benchmarks-Alpaca (Helpfulness), BeaverTails (Harmlessness), and TruthfulQA (Honesty)-demonstrate that TrinityX outperforms strong baselines, achieving relative improvements of 32.5% in win rate, 33.9% in safety score, and 28.4% in truthfulness. In addition, TrinityX reduces memory usage and inference latency by over 40% compared to prior MoE-based approaches. Ablation studies highlight the importance of calibrated routing, and cross-model evaluations confirm TrinityX's generalization across diverse LLM backbones.

</details>


### [41] [CM-Align: Consistency-based Multilingual Alignment for Large Language Models](https://arxiv.org/abs/2509.08541)

*Xue Zhang, Yunlong Liang, Fandong Meng, Songming Zhang, Yufeng Chen, Jinan Xu, Jie Zhou*

**Main category:** cs.CL

**Keywords:** multilingual alignment, large language models, preference optimization

**Relevance Score:** 9

**TL;DR:** The paper proposes a new method (CM-Align) to enhance multilingual alignment in large language models by addressing limitations in current preference data methodologies.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant performance gap in alignment between English and other languages in large language models, partly due to low-quality English responses and biased data construction methods.

**Method:** The authors introduce a consistency-based data selection method that involves consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction.

**Key Contributions:**

	1. Development of CM-Align for high-quality multilingual preference data.
	2. Introduction of consistency-guided English reference selection.
	3. Establishment of cross-lingual consistency in preference construction.

**Result:** Experimental results on three large language models show that CM-Align improves the quality of multilingual preference data, leading to superior alignment performance compared to existing methods.

**Limitations:** 

**Conclusion:** Constructing high-quality preference data is essential for improving multilingual alignment in language models, and the proposed CM-Align method effectively addresses existing limitations.

**Abstract:** Current large language models (LLMs) generally show a significant performance gap in alignment between English and other languages. To bridge this gap, existing research typically leverages the model's responses in English as a reference to select the best/worst responses in other languages, which are then used for Direct Preference Optimization (DPO) training. However, we argue that there are two limitations in the current methods that result in noisy multilingual preference data and further limited alignment performance: 1) Not all English responses are of high quality, and using a response with low quality may mislead the alignment for other languages. 2) Current methods usually use biased or heuristic approaches to construct multilingual preference pairs. To address these limitations, we design a consistency-based data selection method to construct high-quality multilingual preference data for improving multilingual alignment (CM-Align). Specifically, our method includes two parts: consistency-guided English reference selection and cross-lingual consistency-based multilingual preference data construction. Experimental results on three LLMs and three common tasks demonstrate the effectiveness and superiority of our method, which further indicates the necessity of constructing high-quality preference data.

</details>


### [42] [LLM Ensemble for RAG: Role of Context Length in Zero-Shot Question Answering for BioASQ Challenge](https://arxiv.org/abs/2509.08596)

*Dima Galat, Diego Molla-Aliod*

**Main category:** cs.CL

**Keywords:** Biomedical QA, Large Language Models, Information Retrieval, Ensemble Learning, Retrieval-Augmented Generation

**Relevance Score:** 9

**TL;DR:** This paper explores the use of large language models (LLMs) for biomedical question answering (QA) through an ensemble of zero-shot models that achieve state-of-the-art results without fine-tuning.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Biomedical QA requires precise interpretation of specialized knowledge from a complex and evolving corpus.

**Method:** An ensemble of different LLMs is evaluated for their performance on a Yes/No QA task, specifically using the BioASQ challenge tasks.

**Key Contributions:**

	1. Demonstrated the effectiveness of ensemble models in biomedical QA
	2. Showed the impact of context length on performance
	3. Established the importance of information retrieval in RAG for QA systems

**Result:** Ensemble methods outperform individual LLMs and sometimes rival domain-tuned systems, while preserving generalizability.

**Limitations:** The study focuses on zero-shot performance and does not explore fine-tuning methods or broader QA contexts.

**Conclusion:** Ensemble-based zero-shot approaches combined with effective retrieval-augmented generation (RAG) pipelines offer a practical alternative to costly fine-tuned systems.

**Abstract:** Biomedical question answering (QA) poses significant challenges due to the need for precise interpretation of specialized knowledge drawn from a vast, complex, and rapidly evolving corpus. In this work, we explore how large language models (LLMs) can be used for information retrieval (IR), and an ensemble of zero-shot models can accomplish state-of-the-art performance on a domain-specific Yes/No QA task. Evaluating our approach on the BioASQ challenge tasks, we show that ensembles can outperform individual LLMs and in some cases rival or surpass domain-tuned systems - all while preserving generalizability and avoiding the need for costly fine-tuning or labeled data. Our method aggregates outputs from multiple LLM variants, including models from Anthropic and Google, to synthesize more accurate and robust answers. Moreover, our investigation highlights a relationship between context length and performance: while expanded contexts are meant to provide valuable evidence, they simultaneously risk information dilution and model disorientation. These findings emphasize IR as a critical foundation in Retrieval-Augmented Generation (RAG) approaches for biomedical QA systems. Precise, focused retrieval remains essential for ensuring LLMs operate within relevant information boundaries when generating answers from retrieved documents. Our results establish that ensemble-based zero-shot approaches, when paired with effective RAG pipelines, constitute a practical and scalable alternative to domain-tuned systems for biomedical question answering.

</details>


### [43] [Memorization in Large Language Models in Medicine: Prevalence, Characteristics, and Implications](https://arxiv.org/abs/2509.08604)

*Anran Li, Lingfei Qian, Mengmeng Du, Yu Yin, Yan Hu, Zihao Sun, Yihang Fu, Erica Stutz, Xuguang Ai, Qianqian Xie, Rui Zhu, Jimin Huang, Yifan Yang, Siru Liu, Yih-Chung Tham, Lucila Ohno-Machado, Hyunghoon Cho, Zhiyong Lu, Hua Xu, Qingyu Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, medical applications, memorization, health informatics, clinical AI

**Relevance Score:** 9

**TL;DR:** This study assesses the memorization of Large Language Models (LLMs) in medicine, evaluating its prevalence, characteristics, volume, and potential impacts on medical applications.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the extent to which LLMs memorize medical training data and its implications for medical applications.

**Method:** The study analyzes several adaptation scenarios of LLMs, including continued pretraining on medical corpora, fine-tuning on standard medical benchmarks, and fine-tuning on real-world clinical data containing over 13,000 unique inpatient records.

**Key Contributions:**

	1. First comprehensive evaluation of memorization in LLMs within the medical domain
	2. Identification of three types of memorization: beneficial, uninformative, and harmful
	3. Practical recommendations for the deployment of LLMs in medical settings

**Result:** Memorization is prevalent across all adaptation scenarios and significantly higher than in the general domain. It can be categorized into beneficial, uninformative, and harmful forms.

**Limitations:** 

**Conclusion:** Practical recommendations are provided to promote beneficial memorization, minimize uninformative memorization, and mitigate harmful memorization to protect sensitive patient information.

**Abstract:** Large Language Models (LLMs) have demonstrated significant potential in medicine. To date, LLMs have been widely applied to tasks such as diagnostic assistance, medical question answering, and clinical information synthesis. However, a key open question remains: to what extent do LLMs memorize medical training data. In this study, we present the first comprehensive evaluation of memorization of LLMs in medicine, assessing its prevalence (how frequently it occurs), characteristics (what is memorized), volume (how much content is memorized), and potential downstream impacts (how memorization may affect medical applications). We systematically analyze common adaptation scenarios: (1) continued pretraining on medical corpora, (2) fine-tuning on standard medical benchmarks, and (3) fine-tuning on real-world clinical data, including over 13,000 unique inpatient records from Yale New Haven Health System. The results demonstrate that memorization is prevalent across all adaptation scenarios and significantly higher than reported in the general domain. Memorization affects both the development and adoption of LLMs in medicine and can be categorized into three types: beneficial (e.g., accurate recall of clinical guidelines and biomedical references), uninformative (e.g., repeated disclaimers or templated medical document language), and harmful (e.g., regeneration of dataset-specific or sensitive clinical content). Based on these findings, we offer practical recommendations to facilitate beneficial memorization that enhances domain-specific reasoning and factual accuracy, minimize uninformative memorization to promote deeper learning beyond surface-level patterns, and mitigate harmful memorization to prevent the leakage of sensitive or identifiable patient information.

</details>


### [44] [OTESGN:Optimal Transport Enhanced Syntactic-Semantic Graph Networks for Aspect-Based Sentiment Analysis](https://arxiv.org/abs/2509.08612)

*Xinfeng Liao, Xuanqi Chen, Lianxi Wang, Jiahuan Yang, Zhuowei Chen, Ziying Rong*

**Main category:** cs.CL

**Keywords:** Aspect-based sentiment analysis, syntactic-semantic attention, optimal transport

**Relevance Score:** 4

**TL;DR:** This paper presents OTESGN, a novel model for aspect-based sentiment analysis that enhances the capture of sentiment signals through a combination of syntactic and semantic attention mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing aspect-based sentiment analysis methods struggle with complex semantic relationships due to their reliance on linear features, resulting in the obscuring of key opinion terms by noise.

**Method:** The Optimal Transport Enhanced Syntactic-Semantic Graph Network introduces Syntactic-Semantic Collaborative Attention, which integrates Syntactic Graph-Aware Attention and Semantic Optimal Transport Attention to better model syntactic dependencies and capture fine-grained semantic alignments.

**Key Contributions:**

	1. Introduction of Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN)
	2. Development of Syntactic-Semantic Collaborative Attention model
	3. Demonstrated state-of-the-art performance on sentiment analysis benchmarks

**Result:** OTESGN achieves state-of-the-art performance, improving F1 scores by +1.01% on Twitter and +1.30% on Laptop14 benchmarks compared to previous best models.

**Limitations:** 

**Conclusion:** The experimental results validate the effectiveness of OTESGN in accurately localizing opinion words and resisting noise, positioning it as a significant advancement in aspect-based sentiment analysis.

**Abstract:** Aspect-based sentiment analysis (ABSA) aims to identify aspect terms and determine their sentiment polarity. While dependency trees combined with contextual semantics effectively identify aspect sentiment, existing methods relying on syntax trees and aspect-aware attention struggle to model complex semantic relationships. Their dependence on linear dot-product features fails to capture nonlinear associations, allowing noisy similarity from irrelevant words to obscure key opinion terms. Motivated by Differentiable Optimal Matching, we propose the Optimal Transport Enhanced Syntactic-Semantic Graph Network (OTESGN), which introduces a Syntactic-Semantic Collaborative Attention. It comprises a Syntactic Graph-Aware Attention for mining latent syntactic dependencies and modeling global syntactic topology, as well as a Semantic Optimal Transport Attention designed to uncover fine-grained semantic alignments amidst textual noise, thereby accurately capturing sentiment signals obscured by irrelevant tokens. A Adaptive Attention Fusion module integrates these heterogeneous features, and contrastive regularization further improves robustness. Experiments demonstrate that OTESGN achieves state-of-the-art results, outperforming previous best models by +1.01% F1 on Twitter and +1.30% F1 on Laptop14 benchmarks. Ablative studies and visual analyses corroborate its efficacy in precise localization of opinion words and noise resistance.

</details>


### [45] [X-Teaming Evolutionary M2S: Automated Discovery of Multi-turn to Single-turn Jailbreak Templates](https://arxiv.org/abs/2509.08729)

*Hyunjun Kim, Junwoo Ha, Sangyoon Yu, Haon Park*

**Main category:** cs.CL

**Keywords:** multi-turn-to-single-turn, evolutionary algorithms, LLM-guided optimization

**Relevance Score:** 6

**TL;DR:** Introducing X-Teaming Evolutionary M2S, a framework automating the discovery and optimization of multi-turn-to-single-turn templates using LLM-based evolution, achieving significant structural gains in prompt design.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To automate and improve the template generation process for M2S in iterative red-teaming, addressing the reliance on manually crafted templates.

**Method:** Utilizes an automated framework paired with language-model-guided evolution and smart sampling from multiple sources to optimize M2S templates.

**Key Contributions:**

	1. Automated framework for template optimization in M2S
	2. Discovery of new template families and improvement in success rates
	3. Evidence of prompt length affecting performance outcomes

**Result:** Achieved a 44.8% overall success rate on GPT-4.1 with five evolutionary generations and the discovery of two new template families.

**Limitations:** 

**Conclusion:** The study highlights the effectiveness of structural search in enhancing single-turn probes and the critical role of threshold calibration and cross-model evaluation.

**Abstract:** Multi-turn-to-single-turn (M2S) compresses iterative red-teaming into one structured prompt, but prior work relied on a handful of manually written templates. We present X-Teaming Evolutionary M2S, an automated framework that discovers and optimizes M2S templates through language-model-guided evolution. The system pairs smart sampling from 12 sources with an LLM-as-judge inspired by StrongREJECT and records fully auditable logs.   Maintaining selection pressure by setting the success threshold to $\theta = 0.70$, we obtain five evolutionary generations, two new template families, and 44.8% overall success (103/230) on GPT-4.1. A balanced cross-model panel of 2,500 trials (judge fixed) shows that structural gains transfer but vary by target; two models score zero at the same threshold. We also find a positive coupling between prompt length and score, motivating length-aware judging.   Our results demonstrate that structure-level search is a reproducible route to stronger single-turn probes and underscore the importance of threshold calibration and cross-model evaluation. Code, configurations, and artifacts are available at https://github.com/hyunjun1121/M2S-x-teaming.

</details>


### [46] [Streaming Sequence-to-Sequence Learning with Delayed Streams Modeling](https://arxiv.org/abs/2509.08753)

*Neil Zeghidour, Eugene Kharitonov, Manu Orsini, Václav Volhejn, Gabriel de Marmiesse, Edouard Grave, Patrick Pérez, Laurent Mazaré, Alexandre Défossez*

**Main category:** cs.CL

**Keywords:** Delayed Streams Modeling, sequence-to-sequence, streaming inference, automatic speech recognition, text-to-speech

**Relevance Score:** 8

**TL;DR:** Delayed Streams Modeling (DSM) is a novel approach for flexible streaming, multimodal sequence-to-sequence learning that outperforms traditional methods in automatic speech recognition (ASR) and text-to-speech (TTS) tasks.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for sequence-to-sequence generation either require complete input sequences before generating output, or rely on policies to determine when to advance input or output streams. DSM addresses these challenges by allowing for streaming inference of sequences with a flexible, time-aligned streaming model.

**Method:** DSM utilizes a decoder-only language model to treat already aligned input and output streams, incorporating a preprocessing step for alignment and introducing delays between streams.

**Key Contributions:**

	1. Introduction of Delayed Streams Modeling for sequence-to-sequence tasks
	2. State-of-the-art performance in ASR and TTS applications
	3. Flexibility in handling arbitrary long input-output sequences

**Result:** Experiments demonstrate that DSM achieves state-of-the-art performance in ASR and TTS tasks, providing low latency and the ability to handle arbitrary long sequences, competing effectively with traditional offline methods.

**Limitations:** 

**Conclusion:** The DSM framework represents a significant advancement in sequence-to-sequence models for streaming multimodal data, with practical implementations available for further exploration.

**Abstract:** We introduce Delayed Streams Modeling (DSM), a flexible formulation for streaming, multimodal sequence-to-sequence learning. Sequence-to-sequence generation is often cast in an offline manner, where the model consumes the complete input sequence before generating the first output timestep. Alternatively, streaming sequence-to-sequence rely on learning a policy for choosing when to advance on the input stream, or write to the output stream. DSM instead models already time-aligned streams with a decoder-only language model. By moving the alignment to a pre-processing step,and introducing appropriate delays between streams, DSM provides streaming inference of arbitrary output sequences, from any input combination, making it applicable to many sequence-to-sequence problems. In particular, given text and audio streams, automatic speech recognition (ASR) corresponds to the text stream being delayed, while the opposite gives a text-to-speech (TTS) model. We perform extensive experiments for these two major sequence-to-sequence tasks, showing that DSM provides state-of-the-art performance and latency while supporting arbitrary long sequences, being even competitive with offline baselines. Code, samples and demos are available at https://github.com/kyutai-labs/delayed-streams-modeling

</details>


### [47] [Do All Autoregressive Transformers Remember Facts the Same Way? A Cross-Architecture Analysis of Recall Mechanisms](https://arxiv.org/abs/2509.08778)

*Minyeong Choe, Haehyun Cho, Changho Seo, Hyunil Kim*

**Main category:** cs.CL

**Keywords:** Transformer models, factual recall, language models

**Relevance Score:** 7

**TL;DR:** This paper evaluates how different Transformer-based language models, including GPT, LLaMA, Qwen, and DeepSeek, store and recall factual information, finding that architectural differences impact the mechanisms of recall.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve interpretability and enable targeted model editing of Transformer-based language models by understanding how they store and retrieve factual associations.

**Method:** The paper conducts a comprehensive evaluation of factual recall across several autoregressive architectures, analyzing their early layer contributions especially focusing on MLP and attention modules.

**Key Contributions:**

	1. Identifies the importance of early attention layers in Qwen-based models for factual recall.
	2. Compares factual recall mechanisms across multiple autoregressive architectures.
	3. Challenges previously established understandings of MLP contributions in earlier Transformer models.

**Result:** The study reveals that Qwen-based models exhibit a distinct behavior where attention modules in the earliest layers are more crucial for factual recall than MLP modules, contrasting prior research findings.

**Limitations:** 

**Conclusion:** The results indicate that architectural variations within the autoregressive Transformer family significantly affect the mechanisms by which factual recall occurs.

**Abstract:** Understanding how Transformer-based language models store and retrieve factual associations is critical for improving interpretability and enabling targeted model editing. Prior work, primarily on GPT-style models, has identified MLP modules in early layers as key contributors to factual recall. However, it remains unclear whether these findings generalize across different autoregressive architectures. To address this, we conduct a comprehensive evaluation of factual recall across several models -- including GPT, LLaMA, Qwen, and DeepSeek -- analyzing where and how factual information is encoded and accessed. Consequently, we find that Qwen-based models behave differently from previous patterns: attention modules in the earliest layers contribute more to factual recall than MLP modules. Our findings suggest that even within the autoregressive Transformer family, architectural variations can lead to fundamentally different mechanisms of factual recall.

</details>


### [48] [Evaluating LLMs Without Oracle Feedback: Agentic Annotation Evaluation Through Unsupervised Consistency Signals](https://arxiv.org/abs/2509.08809)

*Cheng Chen, Haiyan Yin, Ivor Tsang*

**Main category:** cs.CL

**Keywords:** Large Language Models, annotation quality, unsupervised evaluation, CAI Ratio, NLP

**Relevance Score:** 8

**TL;DR:** The paper introduces a novel paradigm for evaluating Large Language Models (LLMs) using a student model that collaborates with a noisy teacher to assess annotation quality without oracle feedback, utilizing a new metric called the Consistent and Inconsistent (CAI) Ratio.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** Evaluating LLM annotations is challenging in unsupervised environments with limited feedback, necessitating a new approach for assessing and refining annotation quality.

**Method:** The proposed method involves a student model assessing the outputs of a noisy teacher (the LLM) using a user preference-based majority voting strategy, combined with the introduction of the CAI Ratio for measuring annotation quality.

**Key Contributions:**

	1. Introduction of an agentic annotation paradigm using a student model and noisy teacher for LLMs.
	2. Development of the Consistent and Inconsistent (CAI) Ratio as a novel unsupervised evaluation metric.
	3. Demonstration of strong correlation between CAI Ratio and LLM accuracy in NLP tasks.

**Result:** The CAI Ratio displays a strong positive correlation with LLM accuracy across ten open-domain NLP datasets and four LLMs, validating its effectiveness as an evaluation metric.

**Limitations:** 

**Conclusion:** The CAI Ratio is established as a critical tool for unsupervised evaluation and model selection, facilitating the identification of robust LLMs under dynamic conditions.

**Abstract:** Large Language Models (LLMs), when paired with prompt-based tasks, have significantly reduced data annotation costs and reliance on human annotators. However, evaluating the quality of their annotations remains challenging in dynamic, unsupervised environments where oracle feedback is scarce and conventional methods fail. To address this challenge, we propose a novel agentic annotation paradigm, where a student model collaborates with a noisy teacher (the LLM) to assess and refine annotation quality without relying on oracle feedback. The student model, acting as an unsupervised feedback mechanism, employs a user preference-based majority voting strategy to evaluate the consistency of the LLM outputs. To systematically measure the reliability of LLM-generated annotations, we introduce the Consistent and Inconsistent (CAI) Ratio, a novel unsupervised evaluation metric. The CAI Ratio not only quantifies the annotation quality of the noisy teacher under limited user preferences but also plays a critical role in model selection, enabling the identification of robust LLMs in dynamic, unsupervised environments. Applied to ten open-domain NLP datasets across four LLMs, the CAI Ratio demonstrates a strong positive correlation with LLM accuracy, establishing it as an essential tool for unsupervised evaluation and model selection in real-world settings.

</details>


### [49] [MoVoC: Morphology-Aware Subword Construction for Geez Script Languages](https://arxiv.org/abs/2509.08812)

*Hailay Kidu Teklehaymanot, Dren Fazlija, Wolfgang Nejdl*

**Main category:** cs.CL

**Keywords:** tokenization, morphology, Geez script, natural language processing, low-resource languages

**Relevance Score:** 4

**TL;DR:** MoVoC introduces a morpheme-aware tokenizer that enhances token efficiency and linguistic fidelity in low-resource languages written in the Geez script.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Subword-based tokenization methods inadequately preserve morphological boundaries in complex languages, which is particularly challenging for low-resource languages like those using the Geez script.

**Method:** The paper presents MoVoC, a hybrid tokenizer that incorporates supervised morphological analysis and combines morpheme-based tokens with Byte Pair Encoding (BPE).

**Key Contributions:**

	1. Introduction of MoVoC, a morpheme-aware subword tokenizer.
	2. Release of manually annotated morpheme data for Geez script languages.
	3. Demonstrated improvements in linguistic fidelity and token efficiency.

**Result:** Although the new tokenization method does not significantly improve translation quality, it shows substantial improvements in intrinsic metrics such as MorphoScore and Boundary Precision.

**Limitations:** The proposed method does not lead to significant gains in automatic translation quality.

**Conclusion:** MoVoC and the accompanying morpheme-annotated datasets will be valuable resources for research in low-resource, morphologically rich languages, promoting further exploration in this area.

**Abstract:** Subword-based tokenization methods often fail to preserve morphological boundaries, a limitation especially pronounced in low-resource, morphologically complex languages such as those written in the Geez script. To address this, we present MoVoC (Morpheme-aware Subword Vocabulary Construction) and train MoVoC-Tok, a tokenizer that integrates supervised morphological analysis into the subword vocabulary. This hybrid segmentation approach combines morpheme-based and Byte Pair Encoding (BPE) tokens to preserve morphological integrity while maintaining lexical meaning. To tackle resource scarcity, we curate and release manually annotated morpheme data for four Geez script languages and a morpheme-aware vocabulary for two of them. While the proposed tokenization method does not lead to significant gains in automatic translation quality, we observe consistent improvements in intrinsic metrics, MorphoScore, and Boundary Precision, highlighting the value of morphology-aware segmentation in enhancing linguistic fidelity and token efficiency. Our morpheme-annotated datasets and tokenizer will be publicly available to support further research in low-resource, morphologically rich languages. Our code and data are available on GitHub: https://github.com/hailaykidu/MoVoC

</details>


### [50] [Building High-Quality Datasets for Portuguese LLMs: From Common Crawl Snapshots to Industrial-Grade Corpora](https://arxiv.org/abs/2509.08824)

*Thales Sales Almeida, Rodrigo Nogueira, Helio Pedrini*

**Main category:** cs.CL

**Keywords:** large language models, corpora, language-specific data, machine learning, natural language processing

**Relevance Score:** 7

**TL;DR:** This paper explores methods for building scalable web-based corpora for large language models (LLMs), specifically focusing on creating a competitive corpus in Portuguese and its impact on model performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a significant gap in understanding how to construct effective training corpora for languages other than English for large language models (LLMs).

**Method:** The authors built a 120B token corpus in Portuguese and investigated the effects of various data selection and preprocessing strategies on LLM performance when adapting models from English to Portuguese.

**Key Contributions:**

	1. Developed scalable methods for constructing language-specific corpora for LLMs.
	2. Demonstrated the effectiveness of language-specific filtering pipelines.
	3. Provided insights applicable to multilingual LLM development.

**Result:** The findings indicate that language-specific filtering pipelines improve model performance, highlighting the importance of high-quality data tailored to the target language.

**Limitations:** 

**Conclusion:** Adapting LLMs to specific languages using well-constructed datasets leads to significant performance improvements, and the methods developed can be applied to other languages as well.

**Abstract:** The performance of large language models (LLMs) is deeply influenced by the quality and composition of their training data. While much of the existing work has centered on English, there remains a gap in understanding how to construct effective training corpora for other languages. We explore scalable methods for building web-based corpora for LLMs. We apply them to build a new 120B token corpus in Portuguese that achieves competitive results to an industrial-grade corpus. Using a continual pretraining setup, we study how different data selection and preprocessing strategies affect LLM performance when transitioning a model originally trained in English to another language. Our findings demonstrate the value of language-specific filtering pipelines, including classifiers for education, science, technology, engineering, and mathematics (STEM), as well as toxic content. We show that adapting a model to the target language leads to performance improvements, reinforcing the importance of high-quality, language-specific data. While our case study focuses on Portuguese, our methods are applicable to other languages, offering insights for multilingual LLM development.

</details>


### [51] [Large Language Model Hacking: Quantifying the Hidden Risks of Using LLMs for Text Annotation](https://arxiv.org/abs/2509.08825)

*Joachim Baumann, Paul Röttger, Aleksandra Urman, Albert Wendsjö, Flor Miriam Plaza-del-Arco, Johannes B. Gruber, Dirk Hovy*

**Main category:** cs.CL

**Keywords:** large language models, LLM hacking, data annotation, social science research, statistical conclusions

**Relevance Score:** 8

**TL;DR:** This paper investigates 'LLM hacking', the risks introduced by implementation choices when using large language models (LLMs) for tasks like data annotation in social science research, revealing significant biases and errors in statistical conclusions.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address how variations in implementation choices when using LLMs can introduce biases in social science research, leading to incorrect conclusions and systematic errors.

**Method:** The study replicated 37 data annotation tasks from 21 published social science studies using 18 different LLMs and analyzed 13 million labels to assess the impact of researcher choices on statistical conclusions.

**Key Contributions:**

	1. Quantifies the risk of LLM hacking in social science research
	2. Highlights the need for rigorous verification near significance thresholds
	3. Demonstrates that common regression correction techniques are ineffective in mitigating LLM hacking risks.

**Result:** Approximately one in three hypotheses yielded incorrect conclusions based on LLM-annotated data for state-of-the-art models, and half for smaller models, highlighting the significant risk of LLM hacking in research outcomes.

**Limitations:** The study focuses specifically on social science research contexts; results may not generalize to other fields or different types of data.

**Conclusion:** While improving model performance reduces LLM hacking risk, no model completely eliminates it. The study stresses the necessity of human annotations to bolster findings' reliability and suggests that common error correction techniques may not adequately reduce risks.

**Abstract:** Large language models (LLMs) are rapidly transforming social science research by enabling the automation of labor-intensive tasks like data annotation and text analysis. However, LLM outputs vary significantly depending on the implementation choices made by researchers (e.g., model selection, prompting strategy, or temperature settings). Such variation can introduce systematic biases and random errors, which propagate to downstream analyses and cause Type I, Type II, Type S, or Type M errors. We call this LLM hacking.   We quantify the risk of LLM hacking by replicating 37 data annotation tasks from 21 published social science research studies with 18 different models. Analyzing 13 million LLM labels, we test 2,361 realistic hypotheses to measure how plausible researcher choices affect statistical conclusions. We find incorrect conclusions based on LLM-annotated data in approximately one in three hypotheses for state-of-the-art models, and in half the hypotheses for small language models. While our findings show that higher task performance and better general model capabilities reduce LLM hacking risk, even highly accurate models do not completely eliminate it. The risk of LLM hacking decreases as effect sizes increase, indicating the need for more rigorous verification of findings near significance thresholds. Our extensive analysis of LLM hacking mitigation techniques emphasizes the importance of human annotations in reducing false positive findings and improving model selection. Surprisingly, common regression estimator correction techniques are largely ineffective in reducing LLM hacking risk, as they heavily trade off Type I vs. Type II errors.   Beyond accidental errors, we find that intentional LLM hacking is unacceptably simple. With few LLMs and just a handful of prompt paraphrases, anything can be presented as statistically significant.

</details>


### [52] [A Survey of Reinforcement Learning for Large Reasoning Models](https://arxiv.org/abs/2509.08827)

*Kaiyan Zhang, Yuxin Zuo, Bingxiang He, Youbang Sun, Runze Liu, Che Jiang, Yuchen Fan, Kai Tian, Guoli Jia, Pengfei Li, Yu Fu, Xingtai Lv, Yuchen Zhang, Sihang Zeng, Shang Qu, Haozhan Li, Shijie Wang, Yuru Wang, Xinwei Long, Fangfu Liu, Xiang Xu, Jiaze Ma, Xuekai Zhu, Ermo Hua, Yihao Liu, Zonglin Li, Huayu Chen, Xiaoye Qu, Yafu Li, Weize Chen, Zhenzhao Yuan, Junqi Gao, Dong Li, Zhiyuan Ma, Ganqu Cui, Zhiyuan Liu, Biqing Qi, Ning Ding, Bowen Zhou*

**Main category:** cs.CL

**Keywords:** Reinforcement Learning, Large Language Models, Artificial SuperIntelligence, Reasoning, Scalability

**Relevance Score:** 9

**TL;DR:** This paper surveys recent advancements in applying Reinforcement Learning (RL) to Large Language Models (LLMs), identifying challenges and future directions for enhancing scalability and capabilities in reasoning tasks.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the application of Reinforcement Learning (RL) for enhancing reasoning capabilities of Large Language Models (LLMs) and to identify challenges in scalability and infrastructure.

**Method:** The paper presents a comprehensive review of the current state of RL as applied to LLMs, detailing foundational components, core problems, and existing research initiatives.

**Key Contributions:**

	1. Survey of state-of-the-art techniques in RL for LLMs
	2. Identification of foundational challenges in scaling RL for LLM applications
	3. Exploration of future research opportunities in RL and LLM interaction

**Result:** The review highlights both the successes of RL in enhancing LLM reasoning capabilities and the existing challenges in scaling RL methodologies for broader applications.

**Limitations:** 

**Conclusion:** The authors encourage future research in RL for reasoning models, aiming to facilitate advancements toward Artificial SuperIntelligence (ASI).

**Abstract:** In this paper, we survey recent advances in Reinforcement Learning (RL) for reasoning with Large Language Models (LLMs). RL has achieved remarkable success in advancing the frontier of LLM capabilities, particularly in addressing complex logical tasks such as mathematics and coding. As a result, RL has emerged as a foundational methodology for transforming LLMs into LRMs. With the rapid progress of the field, further scaling of RL for LRMs now faces foundational challenges not only in computational resources but also in algorithm design, training data, and infrastructure. To this end, it is timely to revisit the development of this domain, reassess its trajectory, and explore strategies to enhance the scalability of RL toward Artificial SuperIntelligence (ASI). In particular, we examine research applying RL to LLMs and LRMs for reasoning abilities, especially since the release of DeepSeek-R1, including foundational components, core problems, training resources, and downstream applications, to identify future opportunities and directions for this rapidly evolving area. We hope this review will promote future research on RL for broader reasoning models. Github: https://github.com/TsinghuaC3I/Awesome-RL-for-LRMs

</details>


### [53] [Baba Is AI: Break the Rules to Beat the Benchmark](https://arxiv.org/abs/2407.13729)

*Nathan Cloos, Meagan Jens, Michelangelo Naim, Yen-Ling Kuo, Ignacio Cases, Andrei Barbu, Christopher J. Cueva*

**Main category:** cs.CL

**Keywords:** language models, rule manipulation, problem-solving, Baba Is You, AI evaluation

**Relevance Score:** 7

**TL;DR:** This paper introduces a benchmark using the game Baba Is You to evaluate the flexibility of language models in rule manipulation for problem-solving.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to assess how language models can not only follow rules but also creatively manipulate them for goal achievement.

**Method:** The researchers developed a benchmark based on the game Baba Is You, where agents manipulate environmental objects and rules to accomplish game goals. They tested three state-of-the-art multi-modal language models.

**Key Contributions:**

	1. Development of a new benchmark using Baba Is You
	2. Evaluation of state-of-the-art multi-modal language models
	3. Insights into the limitations of current AI models in rule manipulation

**Result:** The tested language models (GPT-4o, Gemini-1.5-Pro, and Gemini-1.5-Flash) exhibited significant failure in tasks requiring rule manipulation and combination for generalization.

**Limitations:** The study is limited to three language models and the specific gameplay context of Baba Is You.

**Conclusion:** The findings highlight the limitations of current multi-modal language models in creatively processing and altering rules in tasks, suggesting a need for improvement in flexibility and generalization.

**Abstract:** Humans solve problems by following existing rules and procedures, and also by leaps of creativity to redefine those rules and objectives. To probe these abilities, we developed a new benchmark based on the game Baba Is You where an agent manipulates both objects in the environment and rules, represented by movable tiles with words written on them, to reach a specified goal and win the game. We test three state-of-the-art multi-modal large language models (OpenAI GPT-4o, Google Gemini-1.5-Pro and Gemini-1.5-Flash) and find that they fail dramatically when generalization requires that the rules of the game must be manipulated and combined.

</details>


### [54] [MedS$^3$: Towards Medical Slow Thinking with Self-Evolved Soft Dual-sided Process Supervision](https://arxiv.org/abs/2501.12051)

*Shuyang Jiang, Yusheng Liao, Zhe Chen, Ya Zhang, Yanfeng Wang, Yu Wang*

**Main category:** cs.CL

**Keywords:** medical language models, clinical reasoning, Monte Carlo Tree Search

**Relevance Score:** 9

**TL;DR:** This paper presents a self-evolving framework named \mone, designed to enhance reasoning capabilities in small medical language models for clinical applications.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** Mainstream medical language models have limited task coverage and lack fine-grained supervision for intermediate reasoning steps, hindering their effectiveness in clinical reasoning.

**Method:** The authors propose \mone, which utilizes a small base policy model to conduct Monte Carlo Tree Search (MCTS) for creating rule-verifiable reasoning pathways, supplemented with reinforcement fine-tuning and a dual process reward model.

**Key Contributions:**

	1. Introduction of a self-evolving framework for medical language models.
	2. Implementation of Monte Carlo Tree Search for reasoning trajectory construction.
	3. Development of a soft dual process reward model for fine-grained reasoning error identification.

**Result:** Experimental results indicate \mone achieves +6.45 accuracy points over the previous state-of-the-art medical model and +8.57 points over general-purpose reasoning models on eleven benchmarks.

**Limitations:** 

**Conclusion:** The proposed approach demonstrates improved accuracy and robust reasoning performance for medical language models, addressing critical barriers in clinical applications.

**Abstract:** Medical language models face critical barriers to real-world clinical reasoning applications. However, mainstream efforts, which fall short in task coverage, lack fine-grained supervision for intermediate reasoning steps, and rely on proprietary systems, are still far from a versatile, credible and efficient language model for clinical reasoning usage. To this end, we propose \mone, a self-evolving framework that imparts robust reasoning capabilities to small, deployable models. Starting with 8,000 curated instances sampled via a curriculum strategy across five medical domains and 16 datasets, we use a small base policy model to conduct Monte Carlo Tree Search (MCTS) for constructing rule-verifiable reasoning trajectories. Self-explored reasoning trajectories ranked by node values are used to bootstrap the policy model via reinforcement fine-tuning and preference learning. Moreover, we introduce a soft dual process reward model that incorporates value dynamics: steps that degrade node value are penalized, enabling fine-grained identification of reasoning errors even when the final answer is correct. Experiments on eleven benchmarks show that \mone outperforms the previous state-of-the-art medical model by +6.45 accuracy points and surpasses 32B-scale general-purpose reasoning models by +8.57 points. Additional empirical analysis further demonstrates that \mone achieves robust and faithful reasoning behavior.

</details>


### [55] [CoAT: Chain-of-Associated-Thoughts Framework for Enhancing Large Language Models Reasoning](https://arxiv.org/abs/2502.02390)

*Jianfeng Pan, Senyou Deng, Shaomang Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Monte Carlo Tree Search, Associative Memory

**Relevance Score:** 9

**TL;DR:** This paper introduces the Chain-of-Associated-Thoughts (CoAT) framework, which combines Monte Carlo Tree Search and associative memory to improve reasoning in LLMs by enabling dynamic knowledge integration and exploration of reasoning pathways.

**Read time:** 18 min

<details>
  <summary>Details</summary>

**Motivation:** The research addresses the limitations of traditional LLMs that rely solely on single-query inference, advocating for a more human-like reasoning process through 'slow thinking' techniques.

**Method:** The CoAT framework synergizes Monte Carlo Tree Search (MCTS) with a dynamic associative memory mechanism that allows continual knowledge updating and exploration of diverse reasoning pathways.

**Key Contributions:**

	1. Introduction of the CoAT framework for LLMs.
	2. Combination of MCTS with associative memory for improved reasoning.
	3. Validation of framework effectiveness on various datasets.

**Result:** CoAT demonstrates significant performance improvements, achieving over 10% enhancement on multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% on a proprietary CRB dataset.

**Limitations:** 

**Conclusion:** The findings validate that CoAT effectively enhances LLM capabilities by allowing adaptive learning and real-time knowledge integration, leading to more accurate and comprehensive outputs.

**Abstract:** Research on LLM technologies is rapidly emerging, with most of them employ a 'fast thinking' approach to inference. Most LLMs generate the final result based solely on a single query and LLM's reasoning capabilities. However, with the advent of OpenAI-o1, 'slow thinking' techniques have garnered increasing attention because its process is closer to the human thought process. Inspired by the human ability to constantly associate and replenish knowledge during thinking, we developed the novel Chain-of-Associated-Thoughts (CoAT) framework, which introduces an innovative synergy between the Monte Carlo Tree Search (MCTS) algorithm and a dynamic mechanism for integrating new key information, termed 'associative memory'. By combining the structured exploration capabilities of MCTS with the adaptive learning capacity of associative memory, CoAT significantly expands the LLM search space, enabling our framework to explore diverse reasoning pathways and dynamically update its knowledge base in real-time. This allows the framework to not only revisit and refine earlier inferences but also adaptively incorporate evolving information, ensuring that the final output is both accurate and comprehensive. We validate CoAT's effectiveness across a variety of generative and reasoning tasks. Quantitative experiments show that CoAT achieves over 10% performance improvement on open-source multi-hop reasoning datasets (HotpotQA, MuSiQue) and more than 15% gain on our proprietary CRB dataset.

</details>


### [56] [Beyond Seen Data: Improving KBQA Generalization Through Schema-Guided Logical Form Generation](https://arxiv.org/abs/2502.12737)

*Shengxiang Gao, Jey Han Lau, Jianzhong Qi*

**Main category:** cs.CL

**Keywords:** Knowledge Base Question Answering, Schema Contexts, Generalizability

**Relevance Score:** 6

**TL;DR:** SG-KBQA is a novel model for knowledge base question answering that improves generalizability by integrating schema contexts into entity retrieval and logical form generation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current knowledge base question answering methods struggle with unseen elements at test time, highlighting the need for improved generalizability.

**Method:** SG-KBQA injects schema contexts into the processes of entity retrieval and logical form generation to enhance performance.

**Key Contributions:**

	1. Introduction of SG-KBQA model
	2. Incorporation of schema contexts for enhanced generalizability
	3. Demonstrated superior performance on benchmark datasets

**Result:** SG-KBQA outperforms state-of-the-art models on two benchmark datasets under various test conditions, demonstrating strong generalizability.

**Limitations:** 

**Conclusion:** The integration of schema contexts into the KBQA model leads to significant improvements in answering capabilities, particularly for unseen knowledge base elements.

**Abstract:** Knowledge base question answering (KBQA) aims to answer user questions in natural language using rich human knowledge stored in large KBs. As current KBQA methods struggle with unseen knowledge base elements at test time,we introduce SG-KBQA: a novel model that injects schema contexts into entity retrieval and logical form generation to tackle this issue. It uses the richer semantics and awareness of the knowledge base structure provided by schema contexts to enhance generalizability. We show that SG-KBQA achieves strong generalizability, outperforming state-of-the-art models on two commonly used benchmark datasets across a variety of test settings. Our source code is available at https://github.com/gaosx2000/SG_KBQA.

</details>


### [57] [Pay Attention to Real World Perturbations! Natural Robustness Evaluation in Machine Reading Comprehension](https://arxiv.org/abs/2502.16523)

*Yulong Wu, Viktor Schlegel, Riza Batista-Navarro*

**Main category:** cs.CL

**Keywords:** Machine Reading Comprehension, robustness, natural perturbations, Wikipedia edit history, large language models

**Relevance Score:** 8

**TL;DR:** This paper investigates the robustness of Machine Reading Comprehension models against naturally occurring textual perturbations using Wikipedia edit history, revealing performance degradation in state-of-the-art models and proposing methods to improve robustness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** With the rise of neural language models achieving human-like performance in MRC, there is a need to assess their robustness in real-world scenarios beyond synthetic perturbations.

**Method:** The authors propose a framework that examines MRC models by replacing paragraphs in benchmarks with corresponding text from Wikipedia edit history, assessing performance degradation across various model architectures.

**Key Contributions:**

	1. Development of a framework to assess MRC robustness with natural perturbations.
	2. Findings from large-scale studies showing significant degradation in performance in response to natural text variations.
	3. Proposed training methods that improve robustness, albeit with a performance gap.

**Result:** Natural perturbations lead to significant performance degradation in pre-trained language models, including Flan-T5 and other LLMs, indicating that they persist despite state-of-the-art capabilities.

**Limitations:** The study primarily focuses on certain MRC benchmarks and may not cover all possible natural perturbations across diverse types.

**Conclusion:** While training on perturbed examples can enhance robustness against natural perturbations, a performance gap compared to unperturbed data remains, emphasizing the need for continuous evaluation.

**Abstract:** As neural language models achieve human-comparable performance on Machine Reading Comprehension (MRC) and see widespread adoption, ensuring their robustness in real-world scenarios has become increasingly important. Current robustness evaluation research, though, primarily develops synthetic perturbation methods, leaving unclear how well they reflect real life scenarios. Considering this, we present a framework to automatically examine MRC models on naturally occurring textual perturbations, by replacing paragraph in MRC benchmarks with their counterparts based on available Wikipedia edit history. Such perturbation type is natural as its design does not stem from an arteficial generative process, inherently distinct from the previously investigated synthetic approaches. In a large-scale study encompassing SQUAD datasets and various model architectures we observe that natural perturbations result in performance degradation in pre-trained encoder language models. More worryingly, these state-of-the-art Flan-T5 and Large Language Models (LLMs) inherit these errors. Further experiments demonstrate that our findings generalise to natural perturbations found in other more challenging MRC benchmarks. In an effort to mitigate these errors, we show that it is possible to improve the robustness to natural perturbations by training on naturally or synthetically perturbed examples, though a noticeable gap still remains compared to performance on unperturbed data.

</details>


### [58] [REGen: A Reliable Evaluation Framework for Generative Event Argument Extraction](https://arxiv.org/abs/2502.16838)

*Omar Sharif, Joseph Gatto, Madhusudan Basak, Sarah M. Preum*

**Main category:** cs.CL

**Keywords:** event argument extraction, evaluation framework, large language models

**Relevance Score:** 8

**TL;DR:** This paper presents REGen, a new evaluation framework for generative event argument extraction that better aligns with human judgment compared to exact match evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation methods for event argument extraction undervalue the performance of large language models and fail to account for valid variations, implicit arguments, and scattered arguments.

**Method:** REGen combines exact, relaxed, and LLM-based matching techniques to create a comprehensive evaluation framework.

**Key Contributions:**

	1. Introduction of the REGen framework for event argument extraction evaluation
	2. Demonstrated substantial performance improvement over existing exact match evaluation methods
	3. Validation of the framework through human assessments confirming its effectiveness.

**Result:** Experiments show REGen achieves an average performance gain of +23.93 F1 over traditional exact match evaluation and aligns 87.67% with human assessments.

**Limitations:** 

**Conclusion:** REGen significantly improves the evaluation of generative models in event argument extraction by capturing a broader range of correct answers.

**Abstract:** Event argument extraction identifies arguments for predefined event roles in text. Existing work evaluates this task with exact match (EM), where predicted arguments must align exactly with annotated spans. While suitable for span-based models, this approach falls short for large language models (LLMs), which often generate diverse yet semantically accurate arguments. EM severely underestimates performance by disregarding valid variations. Furthermore, EM evaluation fails to capture implicit arguments (unstated but inferable) and scattered arguments (distributed across a document). These limitations underscore the need for an evaluation framework that better captures models' actual performance. To bridge this gap, we introduce REGen, a Reliable Evaluation framework for Generative event argument extraction. REGen combines the strengths of exact, relaxed, and LLM-based matching to better align with human judgment. Experiments on six datasets show that REGen reveals an average performance gain of +23.93 F1 over EM, reflecting capabilities overlooked by prior evaluation. Human validation further confirms REGen's effectiveness, achieving 87.67% alignment with human assessments of argument correctness.

</details>


### [59] [MPO: Boosting LLM Agents with Meta Plan Optimization](https://arxiv.org/abs/2503.02682)

*Weimin Xiong, Yifan Song, Qingxiu Dong, Bingchan Zhao, Feifan Song, Xun Wang, Sujian Li*

**Main category:** cs.CL

**Keywords:** large language models, planning, meta plans, agent optimization, task execution

**Relevance Score:** 9

**TL;DR:** Introduction of the Meta Plan Optimization (MPO) framework for improving LLM agent planning.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To resolve issues of planning hallucinations and the need for retraining in existing LLM agent approaches.

**Method:** MPO incorporates explicit guidance through meta plans that assist agent planning and allows for continuous optimization based on task execution feedback.

**Key Contributions:**

	1. Proposes the Meta Plan Optimization framework for LLM agents
	2. Demonstrates significant improvements over existing methods in planning tasks
	3. Enables continuous optimization of meta plans based on feedback

**Result:** MPO significantly outperforms existing baselines in task completion efficiency and enhances generalization in unseen scenarios.

**Limitations:** 

**Conclusion:** MPO presents a plug-and-play solution to improve LLM-based agents without requiring complex knowledge or significant human effort.

**Abstract:** Recent advancements in large language models (LLMs) have enabled LLM-based agents to successfully tackle interactive planning tasks. However, despite their successes, existing approaches often suffer from planning hallucinations and require retraining for each new agent. To address these challenges, we propose the Meta Plan Optimization (MPO) framework, , which enhances agent planning capabilities by directly incorporating explicit guidance. Unlike previous methods that rely on complex knowledge, which either require significant human effort or lack quality assurance, MPO leverages high-level general guidance through meta plans to assist agent planning and enables continuous optimization of the meta plans based on feedback from the agent's task execution. Our experiments conducted on two representative tasks demonstrate that MPO significantly outperforms existing baselines. Moreover, our analysis indicates that MPO provides a plug-and-play solution that enhances both task completion efficiency and generalization capabilities in previous unseen scenarios.

</details>


### [60] [How Far Are We from Optimal Reasoning Efficiency?](https://arxiv.org/abs/2506.07104)

*Jiaxuan Gao, Shu Yan, Qixin Tan, Lu Yang, Shusheng Xu, Wei Fu, Zhiyu Mei, Kaifeng Lyu, Yi Wu*

**Main category:** cs.CL

**Keywords:** Large Reasoning Models, Efficiency, Reinforcement Learning, Reasoning Efficiency Gap, Benchmarking

**Relevance Score:** 7

**TL;DR:** The paper introduces the Reasoning Efficiency Gap (REG) as a metric for evaluating fine-tuned Large Reasoning Models (LRMs) and presents REO-RL, a new Reinforcement Learning algorithm to reduce inefficiencies in these models.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the excessive verbosity and redundancy in reasoning traces produced by LRMs, which lead to high inference costs and limit their practical deployment.

**Method:** The authors propose the Reasoning Efficiency Gap (REG) to quantify the deviation of fine-tuned LRMs from efficiency frontiers. They also introduce a class of Reinforcement Learning algorithms called REO-RL that targets sparse token budgets to minimize REG.

**Key Contributions:**

	1. Introduction of the Reasoning Efficiency Gap (REG) for evaluating LRM efficiency.
	2. Development of REO-RL, a Reinforcement Learning algorithm that minimizes REG effectively.
	3. Empirical benchmarking demonstrating the effectiveness of the proposed metric and algorithm.

**Result:** Systematic evaluation shows that fine-tuning methods currently exhibit significant efficiency gaps, either sacrificing accuracy for shorter reasoning or remaining inefficient. REO-RL reduces REG by >=50 while retaining accuracy under a 16K token budget.

**Limitations:** The challenge of perfectly aligning LRMs with efficiency frontiers remains unsolved, suggesting further research is necessary.

**Conclusion:** Aligning fine-tuned LRMs with efficiency frontiers is challenging, highlighting the need for improved methodologies in fine-tuning and assessment.

**Abstract:** Large Reasoning Models (LRMs) demonstrate remarkable problem-solving capabilities through extended Chain-of-Thought (CoT) reasoning but often produce excessively verbose and redundant reasoning traces. This inefficiency incurs high inference costs and limits practical deployment. While existing fine-tuning methods aim to improve reasoning efficiency, assessing their efficiency gains remains challenging due to inconsistent evaluations. In this work, we introduce the reasoning efficiency frontiers, empirical upper bounds derived from fine-tuning base LRMs across diverse approaches and training configurations. Based on these frontiers, we propose the Reasoning Efficiency Gap (REG), a unified metric quantifying deviations of any fine-tuned LRMs from these frontiers. Systematic evaluation on challenging mathematical benchmarks reveals significant gaps in current methods: they either sacrifice accuracy for short length or still remain inefficient under tight token budgets. To reduce the efficiency gap, we propose REO-RL, a class of Reinforcement Learning algorithms that minimizes REG by targeting a sparse set of token budgets. Leveraging numerical integration over strategically selected budgets, REO-RL approximates the full efficiency objective with low error using a small set of token budgets. Through systematic benchmarking, we demonstrate that our efficiency metric, REG, effectively captures the accuracy-length trade-off, with low-REG methods reducing length while maintaining accuracy. Our approach, REO-RL, consistently reduces REG by >=50 across all evaluated LRMs and matching Qwen3-4B/8B efficiency frontiers under a 16K token budget with minimal accuracy loss. Ablation studies confirm the effectiveness of our exponential token budget strategy. Finally, our findings highlight that fine-tuning LRMs to perfectly align with the efficiency frontiers remains an open challenge.

</details>
