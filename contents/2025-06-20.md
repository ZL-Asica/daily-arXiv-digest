# 2025-06-20

<div id=toc></div>

## Table of Contents

- [cs.CL](#cs.CL) [Total: 19]

<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [1] [HiURE: Hierarchical Exemplar Contrastive Learning for Unsupervised Relation Extraction](https://arxiv.org/abs/2205.02225)

*Shuliang Liu, Xuming Hu, Chenwei Zhang, Shu`ang Li, Lijie Wen, Philip S. Yu*

**Main category:** cs.CL

**Keywords:** unsupervised relation extraction, contrastive learning, hierarchical signals

**Relevance Score:** 7

**TL;DR:** This paper introduces HiURE, a novel contrastive learning framework aimed at improving unsupervised relation extraction by leveraging hierarchical signals and exemplar-wise contrastive learning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of existing unsupervised relation extraction methods that suffer from gradual drift problems and unreasonable distance constraints between similar entities.

**Method:** HiURE employs a contrastive learning approach with cross hierarchy attention to derive hierarchical signals from relational feature space for better optimization of relation representations in sentences.

**Key Contributions:**

	1. Introduction of a novel hierarchical approach for relation extraction
	2. Implementation of exemplar-wise contrastive learning
	3. Demonstration of superior performance on benchmark datasets

**Result:** HiURE demonstrates superior effectiveness and robustness on two public datasets compared to state-of-the-art models in the field of unsupervised relation extraction.

**Limitations:** 

**Conclusion:** The proposed HiURE framework enhances unsupervised relation extraction by addressing known deficiencies in previous methodologies and yielding improved performance metrics.

**Abstract:** Unsupervised relation extraction aims to extract the relationship between entities from natural language sentences without prior information on relational scope or distribution. Existing works either utilize self-supervised schemes to refine relational feature signals by iteratively leveraging adaptive clustering and classification that provoke gradual drift problems, or adopt instance-wise contrastive learning which unreasonably pushes apart those sentence pairs that are semantically similar. To overcome these defects, we propose a novel contrastive learning framework named HiURE, which has the capability to derive hierarchical signals from relational feature space using cross hierarchy attention and effectively optimize relation representation of sentences under exemplar-wise contrastive learning. Experimental results on two public datasets demonstrate the advanced effectiveness and robustness of HiURE on unsupervised relation extraction when compared with state-of-the-art models.

</details>


### [2] [An Effective Incorporating Heterogeneous Knowledge Curriculum Learning for Sequence Labeling](https://arxiv.org/abs/2402.13534)

*Xuemei Tang, Jun Wang, Qi Su, Chu-ren Huang, Jinghang Gu*

**Main category:** cs.CL

**Keywords:** sequence labeling, curriculum learning, machine learning, natural language processing, training speed

**Relevance Score:** 6

**TL;DR:** This paper presents a two-stage curriculum learning framework for sequence labeling tasks that improves performance and speeds up training by gradually introducing data from easy to hard.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance sequence labeling performance while managing data heterogeneity and training costs associated with incorporating external knowledge.

**Method:** A two-stage curriculum learning (TCL) framework is proposed for sequence labeling tasks, progressively introducing data instances based on difficulty levels.

**Key Contributions:**

	1. Development of a two-stage curriculum learning framework for sequence labeling tasks
	2. Demonstration of improved performance and faster training on multiple datasets
	3. Introduction of metrics for evaluating task difficulty levels.

**Result:** Experiments show that the TCL framework improves performance on six Chinese word segmentation and part-of-speech tagging datasets, while also speeding up the training process.

**Limitations:** 

**Conclusion:** The TCL framework effectively enhances the performance of sequence labeling models and accelerates training, addressing challenges posed by complex model architectures.

**Abstract:** Sequence labeling models often benefit from incorporating external knowledge. However, this practice introduces data heterogeneity and complicates the model with additional modules, leading to increased expenses for training a high-performing model. To address this challenge, we propose a two-stage curriculum learning (TCL) framework specifically designed for sequence labeling tasks. The TCL framework enhances training by gradually introducing data instances from easy to hard, aiming to improve both performance and training speed. Furthermore, we explore different metrics for assessing the difficulty levels of sequence labeling tasks. Through extensive experimentation on six Chinese word segmentation (CWS) and Part-of-speech tagging (POS) datasets, we demonstrate the effectiveness of our model in enhancing the performance of sequence labeling models. Additionally, our analysis indicates that TCL accelerates training and alleviates the slow training problem associated with complex models.

</details>


### [3] [Lean Workbook: A large-scale Lean problem set formalized from natural language math problems](https://arxiv.org/abs/2406.03847)

*Huaiyuan Ying, Zijian Wu, Yihan Geng, Zheng Yuan, Dahua Lin, Kai Chen*

**Main category:** cs.CL

**Keywords:** large language models, synthetic data, mathematics, Lean 4, theorem proving

**Relevance Score:** 8

**TL;DR:** The paper presents a synthetic data generation pipeline to enhance LLMs' capabilities in translating mathematical problems into formal languages, addressing the shortage of training data in math theorem proving.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation is to improve the performance of large language models (LLMs) in solving mathematical problems using formal languages like Lean, where existing training data is limited.

**Method:** The authors propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements and vice versa.

**Key Contributions:**

	1. Introduction of a synthetic data generation pipeline for LLMs in mathematical contexts
	2. Creation of a substantial dataset with 57K formal-informal pairs
	3. Open-sourcing of code and dataset for community use

**Result:** The pipeline generates a dataset of approximately 57,000 formal-informal question pairs, enhancing LLM performance in translation and understanding of complex mathematical problems and proofs.

**Limitations:** 

**Conclusion:** The synthetic data approach effectively provides valuable training data that helps LLMs perform better in mathematical problem solving and theorem proving.

**Abstract:** Large language models have demonstrated impressive capabilities across various natural language processing tasks, especially in solving mathematical problems. However, large language models are not good at math theorem proving using formal languages like Lean. A significant challenge in this area is the scarcity of training data available in these formal languages. To address this issue, we propose a novel pipeline that iteratively generates and filters synthetic data to translate natural language mathematical problems into Lean 4 statements, and vice versa. Our results indicate that the synthetic data pipeline can provide useful training data and improve the performance of LLMs in translating and understanding complex mathematical problems and proofs. Our final dataset contains about 57K formal-informal question pairs along with searched proof from the math contest forum and 21 new IMO questions. We open-source our code at https://github.com/InternLM/InternLM-Math and our data at https://huggingface.co/datasets/InternLM/Lean-Workbook.

</details>


### [4] [A Systematic Survey of Natural Language Processing for the Greek Language](https://arxiv.org/abs/2407.09861)

*Juli Bakagianni, Kanella Pouli, Maria Gavriilidou, John Pavlopoulos*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, monolingual surveys, Greek NLP

**Relevance Score:** 4

**TL;DR:** The paper introduces a framework for conducting systematic monolingual NLP surveys, applied to Greek NLP, addressing selection bias and resource gaps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a standardized methodology for monolingual NLP surveys that assesses language-specific challenges and identifies resource availability.

**Method:** Development of a structured search protocol and taxonomies for NLP tasks and resources to minimize bias and improve coverage.

**Key Contributions:**

	1. Generalizable framework for monolingual NLP surveys
	2. Structured search protocol to minimize bias
	3. Creation of NLP task taxonomy and resource identification methodologies

**Result:** The framework is successfully applied to Greek NLP, revealing task-specific progress and resource gaps, with results publicly available and regularly updated.

**Limitations:** 

**Conclusion:** The study demonstrates the framework's effectiveness for Greek NLP and suggests its applicability to other lesser-resourced languages.

**Abstract:** Comprehensive monolingual Natural Language Processing (NLP) surveys are essential for assessing language-specific challenges, resource availability, and research gaps. However, existing surveys often lack standardized methodologies, leading to selection bias and fragmented coverage of NLP tasks and resources. This study introduces a generalizable framework for systematic monolingual NLP surveys. Our approach integrates a structured search protocol to minimize bias, an NLP task taxonomy for classification, and language resource taxonomies to identify potential benchmarks and highlight opportunities for improving resource availability. We apply this framework to Greek NLP (2012-2023), providing an in-depth analysis of its current state, task-specific progress, and resource gaps. The survey results are publicly available (https://doi.org/10.5281/zenodo.15314882) and are regularly updated to provide an evergreen resource. This systematic survey of Greek NLP serves as a case study, demonstrating the effectiveness of our framework and its potential for broader application to other not so well-resourced languages as regards NLP.

</details>


### [5] [Robust Utility-Preserving Text Anonymization Based on Large Language Models](https://arxiv.org/abs/2407.11770)

*Tianyu Yang, Xiaodan Zhu, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** anonymization, large language models, data utility, privacy evaluation, real-time applications

**Relevance Score:** 9

**TL;DR:** This paper presents a framework for anonymizing text that balances privacy and data utility in the context of LLMs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing anonymization techniques struggle against LLMs' re-identification capability while aiming to maintain data utility for downstream tasks.

**Method:** The framework consists of three components: a privacy evaluator, a utility evaluator, and an optimization component, working together to enhance anonymization techniques.

**Key Contributions:**

	1. Proposed a novel framework for anonymization using LLMs
	2. Demonstrated improved performance over existing anonymization methods
	3. Investigated lightweight models for real-time anonymization applications.

**Result:** Experimental results show that the proposed framework outperforms existing methods in reducing re-identification risks and improving data utility.

**Limitations:** 

**Conclusion:** The study provides insights into the integration of anonymization processes with LLMs, emphasizing the dual focus on privacy and downstream task efficacy.

**Abstract:** Anonymizing text that contains sensitive information is crucial for a wide range of applications. Existing techniques face the emerging challenges of the re-identification ability of large language models (LLMs), which have shown advanced capability in memorizing detailed information and reasoning over dispersed pieces of patterns to draw conclusions. When defending against LLM-based re-identification, anonymization could jeopardize the utility of the resulting anonymized data in downstream tasks. In general, the interaction between anonymization and data utility requires a deeper understanding within the context of LLMs. In this paper, we propose a framework composed of three key LLM-based components: a privacy evaluator, a utility evaluator, and an optimization component, which work collaboratively to perform anonymization. Extensive experiments demonstrate that the proposed model outperforms existing baselines, showing robustness in reducing the risk of re-identification while preserving greater data utility in downstream tasks. We provide detailed studies on these core modules. To consider large-scale and real-time applications, we investigate the distillation of the anonymization capabilities into lightweight models. All of our code and datasets will be made publicly available at https://github.com/UKPLab/acl2025-rupta.

</details>


### [6] [RadioRAG: Online Retrieval-augmented Generation for Radiology Question Answering](https://arxiv.org/abs/2407.15621)

*Soroosh Tayebi Arasteh, Mahshad Lotfinia, Keno Bressem, Robert Siepmann, Lisa Adams, Dyke Ferber, Christiane Kuhl, Jakob Nikolas Kather, Sven Nebelung, Daniel Truhn*

**Main category:** cs.CL

**Keywords:** retrieval-augmented generation, large language models, radiology, diagnostic accuracy, real-time data retrieval

**Relevance Score:** 9

**TL;DR:** This paper presents RadioRAG, an end-to-end framework that enhances the diagnostic accuracy of large language models (LLMs) in radiology by using real-time data retrieval from authoritative online sources.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To overcome the limitation of LLMs generating outdated or inaccurate information, this research aims to harness real-time data through a retrieval-augmented generation (RAG) framework specifically designed for radiology.

**Method:** The study involved prompting various LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3) with and without the RadioRAG framework using 80 radiology-specific questions to investigate accuracy improvements.

**Key Contributions:**

	1. Development of the RadioRAG framework for real-time data retrieval in radiology
	2. Improved diagnostic accuracy of LLMs in radiology-specific queries
	3. Demonstration of variability in LLM performance based on data augmentation

**Result:** RadioRAG improved the diagnostic accuracy of most tested LLMs, with relative accuracy increases up to 54%, and matched or exceeded the performance of human radiologists in certain radiologic subspecialties, notably in breast imaging and emergency radiology.

**Limitations:** Variance in RadioRAG's effectiveness across different LLM models.

**Conclusion:** Integrating real-time domain-specific data through RadioRAG significantly enhances the factual accuracy of LLMs in radiology, demonstrating the effectiveness of RAG in medical applications.

**Abstract:** Large language models (LLMs) often generate outdated or inaccurate information based on static training datasets. Retrieval-augmented generation (RAG) mitigates this by integrating outside data sources. While previous RAG systems used pre-assembled, fixed databases with limited flexibility, we have developed Radiology RAG (RadioRAG), an end-to-end framework that retrieves data from authoritative radiologic online sources in real-time. We evaluate the diagnostic accuracy of various LLMs when answering radiology-specific questions with and without access to additional online information via RAG. Using 80 questions from the RSNA Case Collection across radiologic subspecialties and 24 additional expert-curated questions with reference standard answers, LLMs (GPT-3.5-turbo, GPT-4, Mistral-7B, Mixtral-8x7B, and Llama3 [8B and 70B]) were prompted with and without RadioRAG in a zero-shot inference scenario RadioRAG retrieved context-specific information from Radiopaedia in real-time. Accuracy was investigated. Statistical analyses were performed using bootstrapping. The results were further compared with human performance. RadioRAG improved diagnostic accuracy across most LLMs, with relative accuracy increases ranging up to 54% for different LLMs. It matched or exceeded non-RAG models and the human radiologist in question answering across radiologic subspecialties, particularly in breast imaging and emergency radiology. However, the degree of improvement varied among models; GPT-3.5-turbo and Mixtral-8x7B-instruct-v0.1 saw notable gains, while Mistral-7B-instruct-v0.2 showed no improvement, highlighting variability in RadioRAG's effectiveness. LLMs benefit when provided access to domain-specific data beyond their training data. RadioRAG shows potential to improve LLM accuracy and factuality in radiology question answering by integrating real-time domain-specific data.

</details>


### [7] [Root Defence Strategies: Ensuring Safety of LLM at the Decoding Level](https://arxiv.org/abs/2410.06809)

*Xinyi Zeng, Yuying Shang, Jiawei Chen, Jingyuan Zhang, Yu Tian*

**Main category:** cs.CL

**Keywords:** large language models, harmful outputs, defense mechanism, speculative decoding, usability

**Relevance Score:** 8

**TL;DR:** This paper proposes a novel decoder-oriented defense mechanism for large language models (LLMs) that improves their ability to handle harmful outputs while maintaining usability and response speed.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** As large language models advance, they face increased risks of producing harmful outputs from prompts. Current solutions have limitations that affect effectiveness and usability.

**Method:** The proposed method involves a robust decoder-oriented defense architecture that corrects harmful queries instead of rejecting them, along with the introduction of speculative decoding to improve performance.

**Key Contributions:**

	1. Novel decoder-oriented defense mechanism
	2. Speculative decoding for enhanced usability
	3. Improved model security without sacrificing reasoning speed

**Result:** Experiments show that the new approach enhances model security and usability, improving the capacity to recognize harmful outputs without compromising reasoning speed.

**Limitations:** 

**Conclusion:** The architecture provides a more effective and user-friendly way for LLMs to handle potentially harmful instructions, thereby maintaining their helpfulness.

**Abstract:** Large language models (LLMs) have demonstrated immense utility across various industries. However, as LLMs advance, the risk of harmful outputs increases due to incorrect or malicious instruction prompts. While current methods effectively address jailbreak risks, they share common limitations: 1) Judging harmful responses from the prefill-level lacks utilization of the model's decoding outputs, leading to relatively lower effectiveness and robustness. 2) Rejecting potentially harmful responses based on a single evaluation can significantly impair the model's helpfulness.This paper examines the LLMs' capability to recognize harmful outputs, revealing and quantifying their proficiency in assessing the danger of previous tokens. Motivated by pilot experiment results, we design a robust defense mechanism at the decoding level. Our novel decoder-oriented, step-by-step defense architecture corrects harmful queries directly rather than rejecting them outright. We introduce speculative decoding to enhance usability and facilitate deployment to boost secure decoding speed. Extensive experiments demonstrate that our approach improves model security without compromising reasoning speed. Notably, our method leverages the model's ability to discern hazardous information, maintaining its helpfulness compared to existing methods.

</details>


### [8] [Pap2Pat: Benchmarking Outline-Guided Long-Text Patent Generation with Patent-Paper Pairs](https://arxiv.org/abs/2410.07009)

*Valentin Knappich, Simon Razniewski, Anna Hätty, Annemarie Friedrich*

**Main category:** cs.CL

**Keywords:** Large Language Models, patent drafting, PAP2PAT, NLP, machine learning

**Relevance Score:** 7

**TL;DR:** This paper presents PAP2PAT, a benchmark for patent drafting using LLMs, emphasizing the challenges of long technical documents and the effectiveness of outline-guided generation from prepublication research papers.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for efficient patent drafting and the underutilization of LLMs in generating complex technical texts, particularly for patents which consist largely of descriptions.

**Method:** Development of PAP2PAT, a benchmark with 1.8k patent-paper pairs; implementation of chunk-based outline-guided generation by leveraging the research paper to generate patent drafts.

**Key Contributions:**

	1. Introduction of PAP2PAT benchmark for patent drafting
	2. Proposed chunk-based outline-guided generation approach
	3. Findings on LLM performance in patent-style language generation

**Result:** LLMs can utilize information from research papers for patent drafting but struggle with detail; fine-tuning improves language style but increases hallucination errors.

**Limitations:** The study is limited by the availability of suitable prepublication research papers to serve as invention reports and the inherent challenges of patent drafting complexity.

**Conclusion:** While LLMs show promise in supporting patent drafting, challenges remain in detail and accuracy, necessitating further research and improvement.

**Abstract:** Dealing with long and highly complex technical text is a challenge for Large Language Models (LLMs), which still have to unfold their potential in supporting expensive and timeintensive processes like patent drafting. Within patents, the description constitutes more than 90% of the document on average. Yet, its automatic generation remains understudied. When drafting patent applications, patent attorneys typically receive invention reports (IRs), which are usually confidential, hindering research on LLM-supported patent drafting. Often, prepublication research papers serve as IRs. We leverage this duality to build PAP2PAT, an open and realistic benchmark for patent drafting consisting of 1.8k patent-paper pairs describing the same inventions. To address the complex longdocument patent generation task, we propose chunk-based outline-guided generation using the research paper as invention specification. Our extensive evaluation using PAP2PAT and a human case study show that LLMs can effectively leverage information from the paper, but still struggle to provide the necessary level of detail. Fine-tuning leads to more patent-style language, but also to more hallucination. We release our data and code https://github.com/boschresearch/Pap2Pat.

</details>


### [9] [Interchangeable Token Embeddings for Extendable Vocabulary and Alpha-Equivalence](https://arxiv.org/abs/2410.17161)

*İlker Işık, Ramazan Gokberk Cinbis, Ebru Aydin Gol*

**Main category:** cs.CL

**Keywords:** language models, interchangeable tokens, alpha-equivalence, token embeddings, formal logic

**Relevance Score:** 6

**TL;DR:** The paper addresses the limitation of language models in handling interchangeable tokens, proposing a dual-part token embedding strategy to enhance generalization and alpha-equivalence recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** This study explores the incapacity of language models to recognize interchangeable tokens, which hampers their performance in formal logic tasks and generalization to larger vocabularies.

**Method:** The authors introduce alpha-covariance as a metric for robustness to transformations and propose a dual-part token embedding strategy that combines semantic consistency with token distinguishability.

**Key Contributions:**

	1. Introduction of alpha-covariance metric for evaluating robustness in token transformations
	2. Development of a dual-part token embedding strategy for semantic consistency and distinguishability
	3. Demonstrated improved generalization capabilities in several formal logic tasks

**Result:** The proposed method outperforms a baseline relying on alpha-renaming, showing improved generalization in tasks like linear temporal logic solving and propositional logic assignment prediction.

**Limitations:** 

**Conclusion:** The findings lay groundwork for developing language models capable of effective interchangeable token representation, enhancing reasoning in formal domains.

**Abstract:** Language models lack the notion of interchangeable tokens: symbols that are semantically equivalent yet distinct, such as bound variables in formal logic. This limitation prevents generalization to larger vocabularies and hinders the model's ability to recognize alpha-equivalence, where renaming bound variables preserves meaning. We formalize this machine learning problem and introduce alpha-covariance, a metric for evaluating robustness to such transformations. To tackle this task, we propose a dual-part token embedding strategy: a shared component ensures semantic consistency, while a randomized component maintains token distinguishability. Compared to a baseline that relies on alpha-renaming for data augmentation, our approach demonstrates improved generalization to unseen tokens in linear temporal logic solving, propositional logic assignment prediction, and copying with an extendable vocabulary, while introducing a favorable inductive bias for alpha-equivalence. Our findings establish a foundation for designing language models that can learn interchangeable token representations, a crucial step toward more flexible and systematic reasoning in formal domains. Our code and project page are available at https://necrashter.github.io/interchangeable-token-embeddings

</details>


### [10] [REVOLVE: Optimizing AI Systems by Tracking Response Evolution in Textual Optimization](https://arxiv.org/abs/2412.03092)

*Peiyan Zhang, Haibo Jin, Leyang Hu, Xinnuo Li, Liying Kang, Man Luo, Yangqiu Song, Haohan Wang*

**Main category:** cs.CL

**Keywords:** large language models, optimization, human-computer interaction, machine learning

**Relevance Score:** 8

**TL;DR:** REVOLVE is an optimization method for LLM systems focusing on the evolution of responses over time, leading to more effective and stable results in task-specific optimizations.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges in optimizing LLM-based systems for specific tasks without relying on manual interventions, due to limitations of existing methods focused on immediate feedback.

**Method:** REVOLVE tracks the evolution of responses across iterations in LLM systems to enable thoughtful and progressive adjustments for optimization.

**Key Contributions:**

	1. Introduces the REVOLVE optimization approach
	2. Achieves significant performance improvements over existing methods
	3. Demonstrates computational savings with fewer iterations needed for convergence.

**Result:** REVOLVE outperforms competitive baselines with a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization, converging in fewer iterations.

**Limitations:** 

**Conclusion:** REVOLVE suggests leveraging established optimization principles in LLM systems, paving the way for further advancements in this hybrid domain.

**Abstract:** Recent advancements in large language models (LLMs) have significantly enhanced the ability of LLM-based systems to perform complex tasks through natural language processing and tool interaction. However, optimizing these LLM-based systems for specific tasks remains challenging, often requiring manual interventions like prompt engineering and hyperparameter tuning. Existing automatic optimization methods, such as textual feedback-based techniques (e.g., TextGrad), tend to focus on immediate feedback, analogous to using immediate derivatives in traditional numerical gradient descent. However, relying solely on such feedback can be limited when the adjustments made in response to this feedback are either too small or fluctuate irregularly, potentially slowing down or even stalling the optimization process. To overcome these challenges, more adaptive methods are needed, especially in situations where the system's response is evolving slowly or unpredictably. In this paper, we introduce REVOLVE, an optimization method that tracks how "R"esponses "EVOLVE" across iterations in LLM systems. By focusing on the evolution of responses over time, REVOLVE enables more stable and effective optimization by making thoughtful, progressive adjustments at each step. Experimental results demonstrate that REVOLVE outperforms competitive baselines, achieving a 7.8% improvement in prompt optimization, a 20.72% gain in solution refinement, and a 29.17% increase in code optimization. Additionally, REVOLVE converges in fewer iterations, resulting in significant computational savings. Beyond its practical contributions, REVOLVE highlights a promising direction, where the rich knowledge from established optimization principles can be leveraged to enhance LLM systems, which paves the way for further advancements in this hybrid domain.

</details>


### [11] [Large Language Models for Automated Literature Review: An Evaluation of Reference Generation, Abstract Writing, and Review Composition](https://arxiv.org/abs/2412.13612)

*Xuemei Tang, Xufeng Duan, Zhenguang G. Cai*

**Main category:** cs.CL

**Keywords:** large language models, literature review, evaluation metrics, automation, hallucination rates

**Relevance Score:** 9

**TL;DR:** This paper evaluates LLMs' capacity to automate literature reviews by introducing a framework and metrics to assess their performance in reference generation, summaries, and review composition.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To assess how effectively LLMs can automate literature reviews, which are traditionally complex and resource-intensive tasks.

**Method:** A framework is introduced for automatic evaluation of LLMs focusing on reference generation, literature summaries, and review composition, using multidimensional metrics to measure performance.

**Key Contributions:**

	1. Introduction of a systematic framework for evaluating LLM performance in literature reviews
	2. Development of multidimensional metrics for assessing hallucination rates and factual consistency
	3. Insights into the varying effectiveness of LLMs across different academic disciplines.

**Result:** The study reveals that advanced LLMs still generate inaccurate references and their performance varies by discipline, indicating significant room for improvement.

**Limitations:** Results depend on the specific models tested; the framework may require further refinement for broader applicability.

**Conclusion:** LLMs need further research and refinement to enhance their reliability in automating literature reviews.

**Abstract:** Large language models (LLMs) have emerged as a potential solution to automate the complex processes involved in writing literature reviews, such as literature collection, organization, and summarization. However, it is yet unclear how good LLMs are at automating comprehensive and reliable literature reviews. This study introduces a framework to automatically evaluate the performance of LLMs in three key tasks of literature writing: reference generation, literature summary, and literature review composition. We introduce multidimensional evaluation metrics that assess the hallucination rates in generated references and measure the semantic coverage and factual consistency of the literature summaries and compositions against human-written counterparts. The experimental results reveal that even the most advanced models still generate hallucinated references, despite recent progress. Moreover, we observe that the performance of different models varies across disciplines when it comes to writing literature reviews. These findings highlight the need for further research and development to improve the reliability of LLMs in automating academic literature reviews.

</details>


### [12] [Aligning AI Research with the Needs of Clinical Coding Workflows: Eight Recommendations Based on US Data Analysis and Critical Review](https://arxiv.org/abs/2412.18043)

*Yidong Gan, Maciej Rybinski, Ben Hachey, Jonathan K. Kummerfeld*

**Main category:** cs.CL

**Keywords:** clinical coding, AI, health informatics, evaluation methods, automation

**Relevance Score:** 7

**TL;DR:** This position paper addresses the misalignment between AI clinical coding research and real-world clinical practice, proposing eight recommendations for better evaluation methods and alternative AI-based approaches to assist clinical coders.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the practicality of AI coding research by addressing the shortcomings of current evaluation methods and aligning them with real clinical contexts.

**Method:** The paper analyzes existing evaluation methods in automated clinical coding and offers recommendations based on challenges observed in the use of electronic health records.

**Key Contributions:**

	1. Eight specific recommendations to improve current evaluation methods in clinical coding.
	2. Proposed new AI-based methods to assist clinical coders.
	3. Analysis of the misalignment between existing evaluation methods and real clinical practice.

**Result:** The authors identify that many evaluations oversimplify the task by focusing on a limited number of coding results, which does not reflect real-world coding complexity.

**Limitations:** The scope of the recommendations is based on the evaluation of US English electronic health records, and may not be generalizable to other contexts or languages.

**Conclusion:** The paper concludes with eight recommendations for improving evaluation methods and suggests alternative AI methods to support clinical coders more effectively.

**Abstract:** Clinical coding is crucial for healthcare billing and data analysis. Manual clinical coding is labour-intensive and error-prone, which has motivated research towards full automation of the process. However, our analysis, based on US English electronic health records and automated coding research using these records, shows that widely used evaluation methods are not aligned with real clinical contexts. For example, evaluations that focus on the top 50 most common codes are an oversimplification, as there are thousands of codes used in practice. This position paper aims to align AI coding research more closely with practical challenges of clinical coding. Based on our analysis, we offer eight specific recommendations, suggesting ways to improve current evaluation methods. Additionally, we propose new AI-based methods beyond automated coding, suggesting alternative approaches to assist clinical coders in their workflows.

</details>


### [13] [Can LLMs Ask Good Questions?](https://arxiv.org/abs/2501.03491)

*Yueheng Zhang, Xiaoyuan Liu, Yiyou Sun, Atheer Alharbi, Hend Alzahrani, Tianneng Shi, Basel Alomair, Dawn Song*

**Main category:** cs.CL

**Keywords:** large language models, question generation, human-computer interaction, evaluate questions, machine learning

**Relevance Score:** 8

**TL;DR:** This study evaluates the quality of questions generated by large language models (LLMs) compared to human-authored questions across several dimensions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the distinctive characteristics of questions generated by LLMs and inform future work on question quality.

**Method:** The study compares questions generated by two open-source and two proprietary LLMs across six dimensions: type, length, context coverage, answerability, uncommonness, and required answer length.

**Key Contributions:**

	1. Comparison of LLM-generated vs human-authored questions across six dimensions
	2. Insights into the characteristics of LLM-generated questions
	3. Implications for future work on question quality and applications

**Result:** LLM-generated questions tend to require longer descriptive answers and have a more evenly distributed context focus, differing from the positional bias found in traditional QA tasks.

**Limitations:** 

**Conclusion:** The findings highlight key differences in question generation between LLMs and humans, which can guide future improvements in question quality.

**Abstract:** We evaluate questions generated by large language models (LLMs) from context, comparing them to human-authored questions across six dimensions: question type, question length, context coverage, answerability, uncommonness, and required answer length. Our study spans two open-source and two proprietary state-of-the-art models. Results reveal that LLM-generated questions tend to demand longer descriptive answers and exhibit more evenly distributed context focus, in contrast to the positional bias often seen in QA tasks. These findings provide insights into the distinctive characteristics of LLM-generated questions and inform future work on question quality and downstream applications.

</details>


### [14] [Perspective Transition of Large Language Models for Solving Subjective Tasks](https://arxiv.org/abs/2501.09265)

*Xiaolong Wang, Yuanchi Zhang, Ziyue Wang, Yuzhuang Xu, Fuwen Luo, Yile Wang, Peng Li, Yang Liu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Perspective Transition, Natural Language Processing, Subjective Tasks, In-Context Learning

**Relevance Score:** 9

**TL;DR:** This paper introduces Reasoning through Perspective Transition (RPT), a novel method that enhances large language models' performance on subjective tasks by enabling them to dynamically select the best perspective for problem-solving.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of large language models on subjective tasks and proposes a method that allows them to leverage different perspectives for improved contextual understanding and response accuracy.

**Method:** The authors propose RPT, which uses in-context learning to enable LLMs to switch between direct, role, and third-person perspectives depending on the subjective problem at hand.

**Key Contributions:**

	1. Introduction of the RPT method for dynamic perspective selection
	2. Demonstration of improved performance on subjective tasks over fixed perspective methods
	3. Extensive experiments with multiple LLMs showing the applicability of the method.

**Result:** RPT significantly outperforms traditional fixed perspective methods in twelve subjective tasks using various LLMs, demonstrating its effectiveness in enhancing response quality.

**Limitations:** 

**Conclusion:** The study concludes that perspective adaptation can lead to more nuanced and effective responses from LLMs on subjective tasks, highlighting the potential for better interpretative performance in varying contexts.

**Abstract:** Large language models (LLMs) have revolutionized the field of natural language processing, enabling remarkable progress in various tasks. Different from objective tasks such as commonsense reasoning and arithmetic question-answering, the performance of LLMs on subjective tasks is still limited, where the perspective on the specific problem plays crucial roles for better interpreting the context and giving proper response. For example, in certain scenarios, LLMs may perform better when answering from an expert role perspective, potentially eliciting their relevant domain knowledge. In contrast, in some scenarios, LLMs may provide more accurate responses when answering from a third-person standpoint, enabling a more comprehensive understanding of the problem and potentially mitigating inherent biases. In this paper, we propose Reasoning through Perspective Transition (RPT), a method based on in-context learning that enables LLMs to dynamically select among direct, role, and third-person perspectives for the best way to solve corresponding subjective problem. Through extensive experiments on totally 12 subjective tasks by using both closed-source and open-source LLMs including GPT-4, GPT-3.5, Llama-3, and Qwen-2, our method outperforms widely used single fixed perspective based methods such as chain-of-thought prompting and expert prompting, highlights the intricate ways that LLMs can adapt their perspectives to provide nuanced and contextually appropriate responses for different problems.

</details>


### [15] [I-MCTS: Enhancing Agentic AutoML via Introspective Monte Carlo Tree Search](https://arxiv.org/abs/2502.14693)

*Zujie Liang, Feng Wei, Wujiang Xu, Lin Chen, Yuxi Qian, Xinhui Wu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Monte Carlo Tree Search, AutoML

**Relevance Score:** 8

**TL;DR:** This paper presents Introspective Monte Carlo Tree Search (I-MCTS), an enhanced method for automating machine learning tasks using large language models, addressing issues in code generation diversity and quality.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the effectiveness of LLM-based agents in automating machine learning tasks, particularly addressing their limitations in diversity and suboptimal code generation.

**Method:** The authors introduce I-MCTS, which refines tree nodes through introspection and analysis of solutions from parent and sibling nodes, alongside integrating an LLM-based value model for better node evaluation.

**Key Contributions:**

	1. Introduction of Introspective Monte Carlo Tree Search (I-MCTS) for better decision-making in ML tasks
	2. Integration of an LLM-based value model for direct node evaluation
	3. Implementation of a hybrid rewarding mechanism for optimizing node traversal

**Result:** The proposed approach results in a 6% absolute performance improvement over existing AutoML agents, indicating its effectiveness in enhancing AutoML systems.

**Limitations:** 

**Conclusion:** I-MCTS significantly refines the agentic decision-making process in AutoML by enhancing node selection and evaluation methods, while improving overall performance.

**Abstract:** Recent advancements in large language models (LLMs) have shown remarkable potential in automating machine learning tasks. However, existing LLM-based agents often struggle with low-diversity and suboptimal code generation. While recent work has introduced Monte Carlo Tree Search (MCTS) to address these issues, limitations persist in the quality and diversity of thoughts generated, as well as in the scalar value feedback mechanisms used for node selection. In this study, we introduce Introspective Monte Carlo Tree Search (I-MCTS), a novel approach that iteratively expands tree nodes through an introspective process that meticulously analyzes solutions and results from parent and sibling nodes. This facilitates a continuous refinement of the node in the search tree, thereby enhancing the overall decision-making process. Furthermore, we integrate a Large Language Model (LLM)-based value model to facilitate direct evaluation of each node's solution prior to conducting comprehensive computational rollouts. A hybrid rewarding mechanism is implemented to seamlessly transition the Q-value from LLM-estimated scores to actual performance scores. This allows higher-quality nodes to be traversed earlier. Applied to the various ML tasks, our approach demonstrates a 6% absolute improvement in performance compared to the strong open-source AutoML agents, showcasing its effectiveness in enhancing agentic AutoML systems. Resource available at https://github.com/jokieleung/I-MCTS

</details>


### [16] [CODESYNC: Synchronizing Large Language Models with Dynamic Code Evolution at Scale](https://arxiv.org/abs/2502.16645)

*Chenlong Wang, Zhaoyang Chu, Zhengxiang Cheng, Xuyi Yang, Kaiyue Qiu, Yao Wan, Zhou Zhao, Xuanhua Shi, Dongping Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, real-time code updates, benchmarking LLMs, Python libraries, code evolution

**Relevance Score:** 8

**TL;DR:** The paper introduces CODESYNC, a data engine for real-time code knowledge updates in Python libraries, and CODESYNCBENCH, a benchmark to evaluate LLMs' adaptability to dynamic code evolution, revealing current LLM limitations in handling code updates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges faced by Large Language Models (LLMs) in adapting to evolving third-party library APIs, which often results in non-executable or suboptimal code.

**Method:** The paper presents CODESYNC for identifying outdated code patterns and performing real-time updates from third-party Python libraries, along with CODESYNCBENCH to benchmark LLMs on their ability to handle dynamic code evolution.

**Key Contributions:**

	1. Introduction of CODESYNC for real-time code updates from Python libraries
	2. Development of CODESYNCBENCH, a benchmark for evaluating LLMs against evolving code patterns
	3. Public availability of experimental code and dataset for further research

**Result:** Experiments on 14 state-of-the-art LLMs demonstrate that they struggle to keep up with dynamic code evolution, even when employing advanced knowledge updating techniques.

**Limitations:** Focus is primarily on Python libraries, which might limit applicability to other programming languages or ecosystems.

**Conclusion:** CODESYNC and CODESYNCBENCH provide a framework and benchmark for future research on improving LLMs' capabilities in real-time code knowledge updating.

**Abstract:** Large Language Models (LLMs) have exhibited exceptional performance in software engineering yet face challenges in adapting to continually evolving code knowledge, particularly regarding the frequent updates of third-party library APIs. This limitation, stemming from static pre-training datasets, often results in non-executable code or implementations with suboptimal safety and efficiency. To this end, this paper introduces CODESYNC, a data engine for identifying outdated code patterns and collecting real-time code knowledge updates from Python third-party libraries. Building upon CODESYNC, we develop CODESYNCBENCH, a comprehensive benchmark for assessing LLMs' ability to stay synchronized with code evolution, which covers real-world updates for 220 APIs from six Python libraries. Our benchmark offers 3,300 test cases across three evaluation tasks and an update-aware instruction tuning dataset consisting of 2,200 training samples. Extensive experiments on 14 state-of-the-art LLMs reveal that they struggle with dynamic code evolution, even with the support of advanced knowledge updating methods (e.g., DPO, ORPO, and SimPO). We believe that our benchmark can offer a strong foundation for the development of more effective methods for real-time code knowledge updating in the future. The experimental code and dataset are publicly available at: https://github.com/Lucky-voyage/Code-Sync.

</details>


### [17] [Alleviating Distribution Shift in Synthetic Data for Machine Translation Quality Estimation](https://arxiv.org/abs/2502.19941)

*Xiang Geng, Zhejian Lai, Jiajun Chen, Hao Yang, Shujian Huang*

**Main category:** cs.CL

**Keywords:** Quality Estimation, Machine Translation, Synthetic Data Generation

**Relevance Score:** 6

**TL;DR:** DCSQE is a novel framework designed to address distribution shift in synthetic Quality Estimation (QE) data for machine translation, enhancing model performance via constrained beam search and diverse generation models.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the quality of Quality Estimation models by addressing the distribution shift in synthetic data generated for machine translations without reference translations.

**Method:** The DCSQE framework employs constrained beam search and distinct generation models to enhance translation diversity, alongside a method for annotating translations with improved accuracy mimicking human behavior.

**Key Contributions:**

	1. Introduction of DCSQE framework for synthetic QE data
	2. Use of constrained beam search for improved translation quality
	3. Identification of phrase-level errors to mimic human annotation

**Result:** DCSQE demonstrated superior performance compared to state-of-the-art baselines such as CometKiwi in both supervised and unsupervised settings.

**Limitations:** 

**Conclusion:** The proposed DCSQE framework not only mitigates the effects of distribution shift but also provides insights beneficial for synthetic data generation in reward models across various tasks.

**Abstract:** Quality Estimation (QE) models evaluate the quality of machine translations without reference translations, serving as the reward models for the translation task. Due to the data scarcity, synthetic data generation has emerged as a promising solution. However, synthetic QE data often suffers from distribution shift, which can manifest as discrepancies between pseudo and real translations, or in pseudo labels that do not align with human preferences. To tackle this issue, we introduce DCSQE, a novel framework for alleviating distribution shift in synthetic QE data. To reduce the difference between pseudo and real translations, we employ the constrained beam search algorithm and enhance translation diversity through the use of distinct generation models. DCSQE uses references, i.e., translation supervision signals, to guide both the generation and annotation processes, enhancing the quality of token-level labels. DCSQE further identifies the shortest phrase covering consecutive error tokens, mimicking human annotation behavior, to assign the final phrase-level labels. Specially, we underscore that the translation model can not annotate translations of itself accurately. Extensive experiments demonstrate that DCSQE outperforms SOTA baselines like CometKiwi in both supervised and unsupervised settings. Further analysis offers insights into synthetic data generation that could benefit reward models for other tasks. The code is available at https://github.com/NJUNLP/njuqe.

</details>


### [18] [Adding Chocolate to Mint: Mitigating Metric Interference in Machine Translation](https://arxiv.org/abs/2503.08327)

*José Pombal, Nuno M. Guerreiro, Ricardo Rei, André F. T. Martins*

**Main category:** cs.CL

**Keywords:** metric interference, MINTADJUST, machine translation, evaluation metrics, language pairs

**Relevance Score:** 6

**TL;DR:** This paper addresses the problem of metric interference in machine translation, proposing MINTADJUST for more reliable evaluation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The rise of strong automatic metrics has increased the risk of gaming these metrics during model development, which can lead to misleading performance evaluations due to metric interference.

**Method:** The paper analyzes cases of metric interference (MINT) in machine translation, particularly in training data filtering and decoding with quality signals, and proposes MINTADJUST as a solution for reliable evaluation.

**Key Contributions:**

	1. Introduction of the term metric interference (MINT) in the context of machine translation.
	2. Proposition of MINTADJUST for effective evaluation despite MINT.
	3. Demonstrated improvement over existing evaluation metrics in multiple language pairs.

**Result:** MINTADJUST is shown to rank translations and systems more accurately than state-of-the-art metrics on the WMT24 MT shared task test set, particularly benefiting high-quality systems.

**Limitations:** 

**Conclusion:** MINTADJUST offers a solution to the problem of metric interference, enabling more trustworthy performance evaluations in machine translation.

**Abstract:** As automatic metrics become increasingly stronger and widely adopted, the risk of unintentionally "gaming the metric" during model development rises. This issue is caused by metric interference (MINT), i.e., the use of the same or related metrics for both model tuning and evaluation. MINT can misguide practitioners into being overoptimistic about the performance of their systems: as system outputs become a function of the interfering metric, their estimated quality loses correlation with human judgments. In this work, we analyze two common cases of MINT in machine translation-related tasks: filtering of training data, and decoding with quality signals. Importantly, we find that MINT strongly distorts instance-level metric scores, even when metrics are not directly optimized for-questioning the common strategy of leveraging a different, yet related metric for evaluation that is not used for tuning. To address this problem, we propose MINTADJUST, a method for more reliable evaluation under MINT. On the WMT24 MT shared task test set, MINTADJUST ranks translations and systems more accurately than state-of-the-art metrics across a majority of language pairs, especially for high-quality systems. Furthermore, MINTADJUST outperforms AUTORANK, the ensembling method used by the organizers.

</details>


### [19] [Resolving UnderEdit & OverEdit with Iterative & Neighbor-Assisted Model Editing](https://arxiv.org/abs/2503.11895)

*Bhiman Kumar Baghel, Scott M. Jordan, Zheyuan Ryan Shi, Xiang Lorraine Li*

**Main category:** cs.CL

**Keywords:** Large Language Models, model editing, UnderEdit, OverEdit, machine learning

**Relevance Score:** 8

**TL;DR:** This paper introduces two methods for more efficient model editing of Large Language Models (LLMs) that aim to reduce the issues of UnderEdit and OverEdit during parameter updates.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the efficiency of updating LLMs without the high computational costs associated with retraining or fine-tuning, addressing the limitations of existing editing methods.

**Method:** The paper proposes two methods: iterative model editing, which uses successive edits to combat UnderEdit, and neighbor-assisted model editing, which utilizes neighboring knowledge to mitigate OverEdit.

**Key Contributions:**

	1. Iterative model editing to improve UnderEdit.
	2. Neighbor-assisted model editing to minimize OverEdit.
	3. Demonstrated performance improvements across multiple LLMs, algorithms, and benchmarks.

**Result:** The proposed methods significantly enhance editing performance, reducing UnderEdit by up to 38 percentage points and OverEdit by up to 6 across various LLMs and benchmarks.

**Limitations:** 

**Conclusion:** These techniques not only provide a more effective approach to model editing but are also broadly applicable to any locate-and-edit strategy.

**Abstract:** Large Language Models (LLMs) are widely deployed in downstream tasks, but keeping their knowledge up-to-date via retraining or fine-tuning is often computationally expensive. Model editing provides a more efficient alternative by updating a targeted subset of parameters, which often follows the locate-and-edit paradigm. Despite this efficiency, existing methods are limited: edits may fail to inject knowledge (UnderEdit) or unintentionally disrupt unrelated neighboring knowledge (OverEdit). To address these challenges, we propose two complementary methods: iterative model editing, which applies successive edits to mitigate UnderEdit, and neighbor-assisted model editing, which incorporates neighboring knowledge during editing to reduce OverEdit. Our extensive experiments show that these techniques improve editing performance across multiple LLMs, algorithms, and benchmarks, reducing UnderEdit by up to 38 percentage points and OverEdit by up to 6, while remaining broadly applicable to any locate-and-edit method.

</details>
