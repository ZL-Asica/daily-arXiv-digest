# 2025-09-05

<div id=toc></div>

## Table of Contents

- [cs.HC](#cs.HC) [Total: 15]

- [cs.CL](#cs.CL) [Total: 66]

<div id='cs.HC'></div>

## cs.HC [[Back]](#toc)

### [1] [Promisedland: An XR Narrative Attraction Integrating Diorama-to-Virtual Workflow and Elemental Storytelling](https://arxiv.org/abs/2509.03678)

*Xianghan Wang, Chingshuan Hsiao, Shimei Qiu*

**Main category:** cs.HC

**Keywords:** mixed-reality, narrative attraction, ecological education, XR installations, immersive storytelling

**Relevance Score:** 6

**TL;DR:** Promisedland is a mixed-reality narrative attraction combining storytelling and ecological education, featuring a Diorama-to-Virtual pipeline for immersive user experiences.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To create an immersive mixed-reality experience that combines cultural storytelling and ecological education while addressing elemental imbalance on Earth.

**Method:** Developed a low-cost, high-fidelity Diorama-to-Virtual pipeline using physical scale models, 3D scanning, and integration into Unreal Engine, coupled with a motion feedback Stewart Platform.

**Key Contributions:**

	1. Introduced a novel Diorama-to-Virtual pipeline for mixed-reality experiences
	2. Demonstrated the use of a Stewart Platform for enhanced immersion
	3. Proposed a framework for XR Narrative Attractions combining physical and digital storytelling

**Result:** The prototype enhances user engagement through dynamic interactions and real-time visual feedback, running on Meta Quest and allowing for spatial prototyping with narrative consistency.

**Limitations:** 

**Conclusion:** Promisedland provides a replicable design blueprint for future XR narrative installations, suggesting a new framework for XR attractions that merges physical and digital elements.

**Abstract:** Promisedland is a mixed-reality (MR) narrative attraction that combines cultural storytelling, ecological education, and an innovative hybrid production workflow. Set in a future Earth suffering from elemental imbalance, users embark on an interactive journey guided by symbolic characters to restore harmony through the collection of five classical elements: metal, wood, water, fire, and earth. To prototype this experience, we introduce a low-cost, high-fidelity Diorama-to-Virtual pipeline - handcrafting physical scale models, 3D scanning, and integrating them into Unreal Engine. This process enables rapid spatial prototyping while preserving the material expressiveness and narrative consistency of the physical environment. To further enhance immersion, the experience incorporates a Stewart Platform to provide motion feedback synchronized with the virtual ride dynamics, reinforcing spatial presence and embodied engagement. The final prototype runs on Meta Quest, supporting dynamic interactions and real-time visual feedback. Promisedland offers a replicable design blueprint for future XR narrative installations across museums, cultural exhibitions, and themed entertainment. It proposes a new framework for XR Narrative Attractions - where physical and digital elements converge to deepen immersion, agency, and emotional engagement.

</details>


### [2] [Designing Effective AI Explanations for Misinformation Detection: A Comparative Study of Content, Social, and Combined Explanations](https://arxiv.org/abs/2509.03693)

*Yeaeun Gong, Yifan Liu, Lanyu Shang, Na Wei, Dong Wang*

**Main category:** cs.HC

**Keywords:** AI explanations, misinformation detection, Explainable AI, social context, crowdsourcing

**Relevance Score:** 7

**TL;DR:** Study on AI explanations for improving misinformation detection, highlighting the effectiveness of social and combined explanations.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Current XAI approaches focus only on content explanations, which limit users' ability to recognize misinformation.

**Method:** Two online crowdsourcing experiments conducted in COVID-19 and Politics domains to evaluate explanation designs.

**Key Contributions:**

	1. Proposed social explanations alongside content explanations.
	2. Identified the influence of explanation order on detection accuracy.
	3. Demonstrated the domain-specific effectiveness of explanations.

**Result:** AI explanations improved misinformation detection, with effectiveness influenced by the alignment of explanation types and their order of presentation.

**Limitations:** Limited to two domains (COVID-19 and Politics) and online crowdsourcing sample.

**Conclusion:** Different types of AI explanations can enhance understanding and detection of misinformation; alignment matters.

**Abstract:** In this paper, we study the problem of AI explanation of misinformation, where the goal is to identify explanation designs that help improve users' misinformation detection abilities and their overall user experiences. Our work is motivated by the limitations of current Explainable AI (XAI) approaches, which predominantly focus on content explanations that elucidate the linguistic features and sentence structures of the misinformation. To address this limitation, we explore various explanations beyond content explanation, such as "social explanation" that considers the broader social context surrounding misinformation, as well as a "combined explanation" where both the content and social explanations are presented in scenarios that are either aligned or misaligned with each other. To evaluate the comparative effectiveness of these AI explanations, we conduct two online crowdsourcing experiments in the COVID-19 (Study 1 on Prolific) and Politics domains (Study 2 on MTurk). Our results show that AI explanations are generally effective in aiding users to detect misinformation, with effectiveness significantly influenced by the alignment between content and social explanations. We also find that the order in which explanation types are presented - specifically, whether a content or social explanation comes first - can influence detection accuracy, with differences found between the COVID-19 and Political domains. This work contributes towards more effective design of AI explanations, fostering a deeper understanding of how different explanation types and their combinations influence misinformation detection.

</details>


### [3] [Designing Gaze Analytics for ELA Instruction: A User-Centered Dashboard with Conversational AI Support](https://arxiv.org/abs/2509.03741)

*Eduardo Davalos, Yike Zhang, Shruti Jain, Namrata Srivastava, Trieu Truong, Nafees-ul Haque, Tristan Van, Jorge Salas, Sara McFadden, Sun-Joo Cho, Gautam Biswas, Amanda Goodwin*

**Main category:** cs.HC

**Keywords:** eye-tracking, learning analytics, English Language Arts, conversational agent, educational technology

**Relevance Score:** 7

**TL;DR:** This paper presents a gaze-based learning analytics dashboard for English Language Arts that utilizes eye-tracking data to enhance student assessment and engagement in educational settings.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Eye-tracking data can reveal valuable insights into student cognition and engagement, but its application in educational technology is limited due to challenges in interpretation and accessibility.

**Method:** The research involved an iterative design and evaluation process through five studies with teachers and students, applying user-centered design and data storytelling principles.

**Key Contributions:**

	1. Development of a gaze-based learning analytics dashboard for ELA
	2. Use of a conversational agent powered by a LLM to improve data interpretation
	3. Identification of design implications for integrating gaze analytics into EdTech

**Result:** Findings indicated that gaze analytics can be effective and useful when presented through familiar visualizations and narrative elements, and that a conversational agent based on a large language model can facilitate easier comprehension of gaze data.

**Limitations:** Limited to the context of English Language Arts and may not generalize to other subjects or educational frameworks.

**Conclusion:** The study underscores the potential for integrating various data modalities in educational technology, providing a framework for future systems to enhance learning experiences in classrooms.

**Abstract:** Eye-tracking offers rich insights into student cognition and engagement, but remains underutilized in classroom-facing educational technology due to challenges in data interpretation and accessibility. In this paper, we present the iterative design and evaluation of a gaze-based learning analytics dashboard for English Language Arts (ELA), developed through five studies involving teachers and students. Guided by user-centered design and data storytelling principles, we explored how gaze data can support reflection, formative assessment, and instructional decision-making. Our findings demonstrate that gaze analytics can be approachable and pedagogically valuable when supported by familiar visualizations, layered explanations, and narrative scaffolds. We further show how a conversational agent, powered by a large language model (LLM), can lower cognitive barriers to interpreting gaze data by enabling natural language interactions with multimodal learning analytics. We conclude with design implications for future EdTech systems that aim to integrate novel data modalities in classroom contexts.

</details>


### [4] [Map as a By-product: Collective Landmark Mapping from IMU Data and User-provided Texts in Situated Tasks](https://arxiv.org/abs/2509.03792)

*Ryo Yonetani, Kotaro Hara*

**Main category:** cs.HC

**Keywords:** collective mapping, landmarks, indoor navigation, user input, smartphone IMU

**Relevance Score:** 7

**TL;DR:** This paper presents Collective Landmark Mapper, a system for generating semantic landmark maps using smartphone data and user input.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance user navigation and note-taking in indoor environments through a collaborative mapping approach.

**Method:** The system leverages smartphone IMU data and user-generated free text to identify and aggregate landmarks from multiple users.

**Key Contributions:**

	1. Development of a novel map-as-a-byproduct system for landmark mapping
	2. Aggregation of multiple users’ data for enhanced map creation
	3. Demonstration of superior mapping performance in retail applications

**Result:** The user study shows improved mapping performance in creating product availability and room usage maps compared to traditional methods.

**Limitations:** 

**Conclusion:** Collective Landmark Mapper demonstrates effective landmark mapping in retail and potentially other environments, confirming its practicality and usability.

**Abstract:** This paper presents Collective Landmark Mapper, a novel map-as-a-by-product system for generating semantic landmark maps of indoor environments. Consider users engaged in situated tasks that require them to navigate these environments and regularly take notes on their smartphones. Collective Landmark Mapper exploits the smartphone's IMU data and the user's free text input during these tasks to identify a set of landmarks encountered by the user. The identified landmarks are then aggregated across multiple users to generate a unified map representing the positions and semantic information of all landmarks. In developing the proposed system, we focused specifically on retail applications and conducted a formative interview with stakeholders to confirm their practical needs that motivate the map-as-a-byproduct approach. Our user study demonstrates the feasibility of the proposed system and its superior mapping performance in two different setups: creating a product availability map from restocking checklist tasks at a retail store and constructing a room usage map from office inspection tasks, further demonstrating the potential applicability to non-retail applications.

</details>


### [5] [Exploring the Integration of Extended Reality and Artificial Intelligence (AI) for Remote STEM Education and Assessment](https://arxiv.org/abs/2509.03812)

*Shadeeb Hossain, Natalie Sommer, Neda Adib*

**Main category:** cs.HC

**Keywords:** gamification, Extended Reality, STEM education, security, Artificial Intelligence

**Relevance Score:** 6

**TL;DR:** The paper presents a dynamic gamification architecture for an Extended Reality AI training environment aimed at enhancing STEM education through immersive learning.

**Read time:** 9 min

<details>
  <summary>Details</summary>

**Motivation:** To improve STEM education using Extended Reality (XR) and gamification strategies that enable adaptive and kinesthetic learning.

**Method:** The architecture is introduced in four phases focusing on the development of components, fault introduction and correction, and implementing Generative AI XR scenarios.

**Key Contributions:**

	1. Dynamic gamification architecture for XR in STEM education
	2. Comprehensive security and privacy measures
	3. Identification of risks and mitigation strategies

**Result:** The proposed system incorporates security measures such as AES 256 encryption and multi-factor authentication, while addressing risks like sensor exploitation and perceptual manipulation, providing mitigation strategies.

**Limitations:** Technical complexity and cost of deployment may hinder large-scale adoption.

**Conclusion:** The design promotes secure, immersive, and adaptable learning environments for STEM education, highlighting potential adoption barriers related to cost, complexity, and cybersecurity expertise.

**Abstract:** This paper presents a dynamic gamification architecture for an Extended Reality Artificial Intelligence virtual training environment designed to enhance STEM education through immersive adaptive, and kinesthetic learning. The proposed system can be introduced in four phases: Introduction Phase, Component Development Phase, Fault Introduction and Correction Phase and Generative AI XR scenarios Phase. Security and privacy are discussed via a defense-in-depth approach spanning client, middleware, and backend layers, incorporating AES 256 encryption, multi-factor authentication, role-based access control and GDPR or FERPA compliance. Risks such as sensor exploitation, perceptual manipulation, and virtual physical harm are identified, with mitigation strategies embedded at the design stage. Potential barriers to large scale adoption-including technical complexity, cost of deployment, and need for cybersecurity expertise are discussed.

</details>


### [6] ["Low Frequency Tweeters Have More to Say!" A New Approach to Identify Importance of Tweets](https://arxiv.org/abs/2509.03931)

*Gautam Khannaa, Yeliz Yesilada, Sukru Eraslan, Simon Harper*

**Main category:** cs.HC

**Keywords:** Twitter, information overload, tweet ranking, user interaction, social media

**Relevance Score:** 4

**TL;DR:** This paper presents a method to personalize tweet rankings on Twitter based on the frequency of user tweets and their perceived importance, addressing information overload on the platform.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** The motivation behind this research is to tackle the challenge of information overload on Twitter, where important tweets can get lost among a high volume of posts.

**Method:** The authors analyze the relationship between tweet importance and author tweet frequency, proposing six new measures to assess tweet importance based on interaction metrics like retweets, favorites, and comments.

**Key Contributions:**

	1. Proposes a novel method for ranking tweets based on author tweet frequency and tweet importance.
	2. Introduces six measures for evaluating tweet importance through user interaction metrics.
	3. Demonstrates that low-frequency tweeters convey more meaningful content.

**Result:** The study reveals that users who tweet infrequently are often perceived as having more important messages, leading to a proposed reordering of activity feeds to highlight these tweets.

**Limitations:** 

**Conclusion:** The findings could help ensure that significant tweets from low-frequency users are not overlooked, which might also serve as a scoring index for identifying valuable Twitter users.

**Abstract:** Twitter is one of the most popular social media platforms.With a large number of tweets, the activity feed of users becomes noisy, challenging to read, and most importantly tweets often get lost. We present a new approach to personalise the ranking of the tweets toward solving the problem of information overload which is achieved by analysing the relationship between the importance of tweets to the frequency at which the author tweets. The hypothesis tested is that "low-frequency tweeters have more to say", i.e. if a user who tweets infrequently actually goes to the effort of tweeting, then it is more likely to be of more importance or contain more "meaning" than a tweet by a user who tweets continuously. We propose six new measures to evaluate the importance of tweets based on the ability of the tweet to drive interaction among its readers, which is measured through metrics such as retweets, favourites, and comments, and the extent of the author's network interacting with the tweet. Our study shows that users who tweeted less than ten tweets per week were more likely to be perceived as important by their followers and have the most important messages. This identified tweet-frequency band could be used to reorder the activity feed of users and such reordering would ensure the messages of low-frequency tweeters do not get lost in the stream of tweets. This could also serve as a scoring index for Twitter users to identify users frequently tweeting important messages.

</details>


### [7] [Spiking Neural Network Decoders of Finger Forces from High-Density Intramuscular Microelectrode Arrays](https://arxiv.org/abs/2509.04088)

*Farah Baracat, Agnese Grison, Dario Farina, Giacomo Indiveri, Elisa Donati*

**Main category:** cs.HC

**Keywords:** spiking neural networks, motor intent decoding, assistive technology, intramuscular microelectrode arrays, finger force

**Relevance Score:** 4

**TL;DR:** A spike-based decoding framework using spiking neural networks (SNNs) accurately decodes finger motor intent from intramuscular activity.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance assistive technologies by restoring naturalistic finger control through accurate and efficient decoding of motor intent.

**Method:** The study employs a spike-based decoding framework that integrates SNNs with motor unit activity from high-density intramuscular microelectrode arrays, evaluating different decoder configurations and input modalities.

**Key Contributions:**

	1. Introduction of a spike-based decoding framework for finger force decoding
	2. Evaluation of motor unit spike trains vs. EMG signals
	3. Demonstration of reduced memory requirements and latency in SNN configurations

**Result:** The results demonstrate that shallow SNNs can decode individual finger forces from motor unit spike trains with competitive accuracy and low latency, while requiring minimal memory.

**Limitations:** 

**Conclusion:** This work outlines a framework for integrating SNNs into force decoding systems, allowing for tailored input representations to achieve specific application needs.

**Abstract:** Restoring naturalistic finger control in assistive technologies requires the continuous decoding of motor intent with high accuracy, efficiency, and robustness. Here, we present a spike-based decoding framework that integrates spiking neural networks (SNNs) with motor unit activity extracted from high-density intramuscular microelectrode arrays. We demonstrate simultaneous and proportional decoding of individual finger forces from motor unit spike trains during isometric contractions at 15% of maximum voluntary contraction using SNNs. We systematically evaluated alternative SNN decoder configurations and compared two possible input modalities: physiologically grounded motor unit spike trains and spike-encoded intramuscular EMG signals. Through this comparison, we quantified trade-offs between decoding accuracy, memory footprint, and robustness to input errors. The results showed that shallow SNNs can reliably decode finger-level motor intent with competitive accuracy and minimal latency, while operating with reduced memory requirements and without the need for external preprocessing buffers. This work provides a practical blueprint for integrating SNNs into finger-level force decoding systems, demonstrating how the choice of input representation can be strategically tailored to meet application-specific requirements for accuracy, robustness, and memory efficiency.

</details>


### [8] [Unobtrusive In-Situ Measurement of Behavior Change by Deep Metric Similarity Learning of Motion Patterns](https://arxiv.org/abs/2509.04174)

*Christian Merz, Lukas Schach, Marie Luisa Fiedler, Jean-Luc Lugrin, Carolin Wienrich, Marc Erich Latoschik*

**Main category:** cs.HC

**Keywords:** XR systems, Proteus effect, deep metric learning

**Relevance Score:** 8

**TL;DR:** The paper presents a method for detecting user behavior changes in XR systems using deep metric similarity learning, which compares motion patterns based on biometric data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To measure user behavior changes during XR experiences without intrusive input, focusing on the Proteus effect linked to avatar embodiment.

**Method:** A biometric user model based on deep metric similarity learning is employed to analyze motion data collected during a fruit collection task with different avatar body heights.

**Key Contributions:**

	1. Introduction of a biometric user model using deep metric similarity learning for motion analysis in XR.
	2. Demonstration of effective in-situ measurement of behavior changes without user input.
	3. Generalizable motion analysis applicable to various avatar-related studies.

**Result:** The model successfully identified behavior changes based on the various avatar conditions, showing advantages over traditional methods by allowing in-situ measurement and individual analysis.

**Limitations:** 

**Conclusion:** The developed method provides a scalable and generalizable approach to study the impact of avatar changes on user behavior in real time.

**Abstract:** This paper introduces an unobtrusive in-situ measurement method to detect user behavior changes during arbitrary exposures in XR systems. Here, such behavior changes are typically associated with the Proteus effect or bodily affordances elicited by different avatars that the users embody in XR. We present a biometric user model based on deep metric similarity learning, which uses high-dimensional embeddings as reference vectors to identify behavior changes of individual users. We evaluate our model against two alternative approaches: a (non-learned) motion analysis based on central tendencies of movement patterns and subjective post-exposure embodiment questionnaires frequently used in various XR exposures. In a within-subject study, participants performed a fruit collection task while embodying avatars of different body heights (short, actual-height, and tall). Subjective assessments confirmed the effective manipulation of perceived body schema, while the (non-learned) objective analyses of head and hand movements revealed significant differences across conditions. Our similarity learning model trained on the motion data successfully identified the elicited behavior change for various query and reference data pairings of the avatar conditions. The approach has several advantages in comparison to existing methods: 1) In-situ measurement without additional user input, 2) generalizable and scalable motion analysis for various use cases, 3) user-specific analysis on the individual level, and 4) with a trained model, users can be added and evaluated in real time to study how avatar changes affect behavior.

</details>


### [9] [Would I regret being different? The influence of social norms on attitudes toward AI usage](https://arxiv.org/abs/2509.04241)

*Jaroslaw Kornowicz, Maurice Pape, Kirsten Thommes*

**Main category:** cs.HC

**Keywords:** AI adoption, social norms, algorithm aversion, decision-making, organizational behavior

**Relevance Score:** 8

**TL;DR:** This paper investigates how social norms from peers and supervisors influence AI usage behavior, particularly in the context of reducing algorithm aversion.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To understand the establishment of social norms in AI adoption and their impact on AI usage decisions within organizations.

**Method:** An online vignette experiment was conducted, supplemented by qualitative data from participants reflecting on their normative and counter-normative decisions.

**Key Contributions:**

	1. Establishes the role of regret aversion in AI-related decision-making
	2. Highlights the limited influence of peers and supervisors on regret levels regarding AI use
	3. Provides evidence that social norms can mitigate algorithm aversion in organizational settings.

**Result:** Participants reported higher regret when deviating from AI-preferred choices and attributed more blame to human decisions over AI, with AI usage normalized reducing regret.

**Limitations:** Further research is needed to explore the long-term effects of these norms and their variability across different organizational contexts.

**Conclusion:** Regret aversion influenced by social norms significantly drives imitation in AI decision-making, providing insights for organizations on effective AI adoption strategies.

**Abstract:** Prior research shows that social norms can reduce algorithm aversion, but little is known about how such norms become established. Most accounts emphasize technological and individual determinants, yet AI adoption unfolds within organizational social contexts shaped by peers and supervisors. We ask whether the source of the norm-peers or supervisors-shapes AI usage behavior. This question is practically relevant for organizations seeking to promote effective AI adoption. We conducted an online vignette experiment, complemented by qualitative data on participants' feelings and justifications after (counter-)normative behavior. In line with the theory, counter-normative choices elicited higher regret than norm-adherent choices. On average, choosing AI increased regret compared to choosing an human. This aversion was weaker when AI use was presented as the prevailing norm, indicating a statistically significant interaction between AI use and an AI-favoring norm. Participants also attributed less blame to technology than to humans, which increased regret when AI was chosen over human expertise. Both peer and supervisor influence emerged as relevant factors, though contrary to expectations they did not significantly affect regret. Our findings suggest that regret aversion, embedded in social norms, is a central mechanism driving imitation in AI-related decision-making.

</details>


### [10] [MuMTAffect: A Multimodal Multitask Affective Framework for Personality and Emotion Recognition from Physiological Signals](https://arxiv.org/abs/2509.04254)

*Meisam Jamshidi Seikavandi, Fabricio Batista Narcizo, Ted Vucurevich, Andrew Burke Dittberner, Paolo Burelli*

**Main category:** cs.HC

**Keywords:** affective computing, multimodal, emotion classification, personality prediction, physiological signals

**Relevance Score:** 8

**TL;DR:** MuMTAffect is a multimodal network for emotion classification and personality prediction using various physiological signals, improving emotion recognition performance through cross-modal interactions.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The integration of emotion classification and personality prediction is essential for developing effective affective computing systems, grounded in contemporary affective neuroscience.

**Method:** MuMTAffect utilizes transformer-based encoders for multiple physiological modalities (pupil dilation, eye gaze, facial action units, galvanic skin response) and a fusion transformer to model interactions between these modalities.

**Key Contributions:**

	1. Integration of emotion classification and personality prediction
	2. Use of dedicated transformer-based encoders for each physiological modality
	3. Demonstration of significant performance improvements in emotion recognition

**Result:** Evaluation on the AFFEC dataset shows that the model improves arousal classification with galvanic skin response and enhances valence discrimination with pupil and gaze data.

**Limitations:** 

**Conclusion:** The modularity of MuMTAffect supports the integration of additional modalities, making it scalable and adaptable for future developments in affective computing.

**Abstract:** We present MuMTAffect, a novel Multimodal Multitask Affective Embedding Network designed for joint emotion classification and personality prediction (re-identification) from short physiological signal segments. MuMTAffect integrates multiple physiological modalities pupil dilation, eye gaze, facial action units, and galvanic skin response using dedicated, transformer-based encoders for each modality and a fusion transformer to model cross-modal interactions. Inspired by the Theory of Constructed Emotion, the architecture explicitly separates core affect encoding (valence/arousal) from higher-level conceptualization, thereby grounding predictions in contemporary affective neuroscience. Personality trait prediction is leveraged as an auxiliary task to generate robust, user-specific affective embeddings, significantly enhancing emotion recognition performance. We evaluate MuMTAffect on the AFFEC dataset, demonstrating that stimulus-level emotional cues (Stim Emo) and galvanic skin response substantially improve arousal classification, while pupil and gaze data enhance valence discrimination. The inherent modularity of MuMTAffect allows effortless integration of additional modalities, ensuring scalability and adaptability. Extensive experiments and ablation studies underscore the efficacy of our multimodal multitask approach in creating personalized, context-aware affective computing systems, highlighting pathways for further advancements in cross-subject generalisation.

</details>


### [11] [HumAIne-Chatbot: Real-Time Personalized Conversational AI via Reinforcement Learning](https://arxiv.org/abs/2509.04303)

*Georgios Makridis, Georgios Fragiadakis, Jorge Oliveira, Tomaz Saraiva, Philip Mavrepis, Georgios Fatouros, Dimosthenis Kyriazis*

**Main category:** cs.HC

**Keywords:** Conversational AI, User profiling, Reinforcement learning, Personalization, Human-computer interaction

**Relevance Score:** 8

**TL;DR:** HumAIne-chatbot personalizes conversational AI interactions using a novel user profiling framework, improving user satisfaction and task achievement.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing conversational AI systems often provide generic interactions which fail to adapt to individual user characteristics, leading to suboptimal user experiences.

**Method:** The system utilizes pre-training on GPT-generated virtual personas and employs online reinforcement learning to refine user profiles based on implicit and explicit feedback during interactions.

**Key Contributions:**

	1. Introduction of a user profiling framework for conversational agents
	2. Use of reinforcement learning for real-time adaptation
	3. Demonstration of significant improvements in user satisfaction and task achievement

**Result:** Controlled experiments demonstrated significant improvements in user satisfaction, personalization accuracy, and task achievement, confirming the effectiveness of the proposed personalization features.

**Limitations:** 

**Conclusion:** The study provides strong evidence for the effectiveness of AI-driven user profiling in enhancing conversational AI interactions, suggesting a pathway for future real-world applications.

**Abstract:** Current conversational AI systems often provide generic, one-size-fits-all interactions that overlook individual user characteristics and lack adaptive dialogue management. To address this gap, we introduce \textbf{HumAIne-chatbot}, an AI-driven conversational agent that personalizes responses through a novel user profiling framework. The system is pre-trained on a diverse set of GPT-generated virtual personas to establish a broad prior over user types. During live interactions, an online reinforcement learning agent refines per-user models by combining implicit signals (e.g. typing speed, sentiment, engagement duration) with explicit feedback (e.g., likes and dislikes). This profile dynamically informs the chatbot dialogue policy, enabling real-time adaptation of both content and style. To evaluate the system, we performed controlled experiments with 50 synthetic personas in multiple conversation domains. The results showed consistent improvements in user satisfaction, personalization accuracy, and task achievement when personalization features were enabled. Statistical analysis confirmed significant differences between personalized and nonpersonalized conditions, with large effect sizes across key metrics. These findings highlight the effectiveness of AI-driven user profiling and provide a strong foundation for future real-world validation.

</details>


### [12] [Write on Paper, Wrong in Practice: Why LLMs Still Struggle with Writing Clinical Notes](https://arxiv.org/abs/2509.04340)

*Kristina L. Kupferschmidt, Kieran O'Doherty, Joshua A. Skorburg*

**Main category:** cs.HC

**Keywords:** Large Language Models, Clinical documentation, Occupational therapy, Sociotechnical systems, AI integration

**Relevance Score:** 9

**TL;DR:** This study explores the sociotechnical implications of using Large Language Models (LLMs) to reduce documentation burdens for occupational therapists.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how LLMs can assist in clinical documentation within pediatric rehabilitation settings and tackle the associated challenges.

**Method:** A qualitative study involving 20 clinicians at KidsAbility, examining the implementation of a general-purpose LLM and a bespoke model fine-tuned on historical documentation.

**Key Contributions:**

	1. Identified sociotechnical factors influencing documentation burden in clinical settings.
	2. Proposed the need for flexible tools that support clinician autonomy.
	3. Highlighted the importance of training and mutual learning for successful AI implementation.

**Result:** Documentation challenges are found to be sociotechnical, influenced by clinical workflows and organizational policies. Key themes identified include variations in workflows, systemic nature of documentation burden, need for flexible tools, and the importance of mutual learning in AI integration.

**Limitations:** 

**Conclusion:** Although LLMs have potential in alleviating documentation tasks, their success relies on adaptive integration that honors clinician autonomy and includes comprehensive training and implementation strategies.

**Abstract:** Large Language Models (LLMs) are often proposed as tools to streamline clinical documentation, a task viewed as both high-volume and low-risk. However, even seemingly straightforward applications of LLMs raise complex sociotechnical considerations to translate into practice. This case study, conducted at KidsAbility, a pediatric rehabilitation facility in Ontario, Canada examined the use of LLMs to support occupational therapists in reducing documentation burden.We conducted a qualitative study involving 20 clinicians who participated in pilot programs using two AI technologies: a general-purpose proprietary LLM and a bespoke model fine-tuned on proprietary historical documentation.   Our findings reveal that documentation challenges are sociotechnical in nature, shaped by clinical workflows, organizational policies, and system constraints. Four key themes emerged: (1) the heterogeneity of workflows, (2) the documentation burden is systemic and not directly linked to the creation of any single type of documentation, (3) the need for flexible tools and clinician autonomy, and (4) effective implementation requires mutual learning between clinicians and AI systems.   While LLMs show promise in easing documentation tasks, their success will depend on flexible, adaptive integration that supports clinician autonomy. Beyond technical performance, sustained adoption will require training programs and implementation strategies that reflect the complexity of clinical environments.

</details>


### [13] [SRWToolkit: An Open Source Wizard of Oz Toolkit to Create Social Robotic Avatars](https://arxiv.org/abs/2509.04356)

*Atikkhan Faridkhan Nilgar, Kristof Van Laerhoven, Ayub Kinoti*

**Main category:** cs.HC

**Keywords:** social robotics, large language models, human-robot interaction, toolkit, user experience

**Relevance Score:** 8

**TL;DR:** SRWToolkit is an open-source Wizard of Oz toolkit for rapid prototyping of social robotic avatars using local LLMs, enabling multimodal interactions and customizable avatar features.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The aim is to facilitate the rapid prototyping of social robotic avatars powered by local LLMs, addressing the limitations of cloud-based LLM services in terms of modularity and on-device functionality.

**Method:** The toolkit allows for multimodal interaction via text input, speech activation, and wake-word commands, with real-time configuration through an intuitive control panel. A user study was conducted with 11 participants to assess toolkit usability and user experience.

**Key Contributions:**

	1. Open-source toolkit for prototyping social robotic avatars
	2. Supports multimodal interaction and real-time avatar customization
	3. Operates on-device with local LLM inference

**Result:** Participants successfully created and interacted with various robotic roles, and results showed positive outcomes in usability, trust, and overall user experience with the toolkit.

**Limitations:** 

**Conclusion:** SRWToolkit supports scalable research in human-robot interaction by enabling the efficient development of customized robot characters.

**Abstract:** We present SRWToolkit, an open-source Wizard of Oz toolkit designed to facilitate the rapid prototyping of social robotic avatars powered by local large language models (LLMs). Our web-based toolkit enables multimodal interaction through text input, button-activated speech, and wake-word command. The toolkit offers real-time configuration of avatar appearance, behavior, language, and voice via an intuitive control panel. In contrast to prior works that rely on cloud-based LLM services, SRWToolkit emphasizes modularity and ensures on-device functionality through local LLM inference. In our small-scale user study ($n=11$), participants created and interacted with diverse robotic roles (hospital receptionist, mathematics teacher, and driving assistant), which demonstrated positive outcomes in the toolkit's usability, trust, and user experience. The toolkit enables rapid and efficient development of robot characters customized to researchers' needs, supporting scalable research in human-robot interaction.

</details>


### [14] [Privacy Perceptions in Robot-Assisted Well-Being Coaching: Examining the Roles of Information Transparency, User Control, and Proactivity](https://arxiv.org/abs/2509.04358)

*Atikkhan Faridkhan Nilgar, Manuel Dietrich, Kristof Van Laerhoven*

**Main category:** cs.HC

**Keywords:** social robots, privacy perceptions, user control, well-being coaching, human-robot interaction

**Relevance Score:** 8

**TL;DR:** This research examines how social robots' privacy perceptions are shaped by users' control, transparency, and behavioral approaches in coaching interactions.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To understand factors influencing users' privacy perceptions in interactions with social robots used for well-being coaching.

**Method:** An online study with 200 participants was conducted to evaluate the effects of user control, transparency, and robot behavior on privacy perceptions.

**Key Contributions:**

	1. Identifies user control as a key factor influencing privacy perceptions.
	2. Shows low impact of transparency and robot proactivity compared to user control.
	3. Suggests need for mechanisms allowing users to manage robot information processing.

**Result:** Control over information significantly improved perceived privacy and trust, overshadowing the effects of transparency and proactivity.

**Limitations:** Study limited to online interactions; real-world coaching dynamics may differ.

**Conclusion:** User control is crucial for privacy perceptions; merely informing users or being proactive is insufficient without the ability to control information sharing.

**Abstract:** Social robots are increasingly recognized as valuable supporters in the field of well-being coaching. They can function as independent coaches or provide support alongside human coaches, and healthcare professionals. In coaching interactions, these robots often handle sensitive information shared by users, making privacy a relevant issue. Despite this, little is known about the factors that shape users' privacy perceptions. This research aims to examine three key factors systematically: (1) the transparency about information usage, (2) the level of specific user control over how the robot uses their information, and (3) the robot's behavioral approach - whether it acts proactively or only responds on demand. Our results from an online study (N = 200) show that even when users grant the robot general access to personal data, they additionally expect the ability to explicitly control how that information is interpreted and shared during sessions. Experimental conditions that provided such control received significantly higher ratings for perceived privacy appropriateness and trust. Compared to user control, the effects of transparency and proactivity on privacy appropriateness perception were low, and we found no significant impact. The results suggest that merely informing users or proactive sharing is insufficient without accompanying user control. These insights underscore the need for further research on mechanisms that allow users to manage robots' information processing and sharing, especially when social robots take on more proactive roles alongside humans.

</details>


### [15] [Towards softerware: Enabling personalization of interactive data representations for users with disabilities](https://arxiv.org/abs/2502.18348)

*Frank Elavsky, Marita Vindedal, Ted Gies, Patrick Carrington, Dominik Moritz, Øystein Moseng*

**Main category:** cs.HC

**Keywords:** accessible design, user customization, data visualization, accessibility, design probe

**Relevance Score:** 8

**TL;DR:** The paper introduces 'softerware,' a design approach enabling users to customize interfaces to alleviate access friction, particularly for individuals with disabilities, and discusses findings from a design probe study.

**Read time:** 13 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenges posed by access friction in accessible design, which can hinder users with disabilities, by empowering them to customize interfaces according to their needs.

**Method:** The study involved assembling a set of 195 customization options for data visualizations, developing a prototype that implements some of these options, and conducting interviews and a design probe study with accessibility professionals.

**Key Contributions:**

	1. Introduction of the concept of softerware for accessible design.
	2. Development of a prototype using customization options for visualizations.
	3. Insights from interviews with accessibility professionals regarding design challenges.

**Result:** Participants identified access frictions in designs and emphasized the importance of accessible defaults, interoperability, persistence, and user-friendly effort-to-outcome ratios for the success of softerware.

**Limitations:** The study may be limited by the scope of customization options explored and the specific contexts of the participants involved in the design probe study.

**Conclusion:** The research highlights the necessity of considering user agency in interface design and advocates for a more inclusive design philosophy to support individuals with disabilities.

**Abstract:** Accessible design for some may still produce barriers for others. This tension, called access friction, creates challenges for both designers and end-users with disabilities. To address this, we present the concept of softerware, a system design approach that provides end users with agency to meaningfully customize and adapt interfaces to their needs. To apply softerware to visualization, we assembled 195 data visualization customization options centered on the barriers we expect users with disabilities will experience. We built a prototype that applies a subset of these options and interviewed practitioners for feedback. Lastly, we conducted a design probe study with blind and low vision accessibility professionals to learn more about their challenges and visions for softerware. We observed access frictions between our participant's designs and they expressed that for softerware's success, current and future systems must be designed with accessible defaults, interoperability, persistence, and respect for a user's perceived effort-to-outcome ratio.

</details>


<div id='cs.CL'></div>

## cs.CL [[Back]](#toc)

### [16] [Speech-Based Cognitive Screening: A Systematic Evaluation of LLM Adaptation Strategies](https://arxiv.org/abs/2509.03525)

*Fatemeh Taherinezhad, Mohamad Javad Momeni Nezhad, Sepehr Karimi, Sina Rashidi, Ali Zolnour, Maryam Dadkhah, Yasaman Haghbin, Hossein AzadMaleki, Maryam Zolnoori*

**Main category:** cs.CL

**Keywords:** dementia detection, language model adaptation, speech-based screening

**Relevance Score:** 9

**TL;DR:** This paper explores language model adaptation strategies for detecting dementia using speech data, finding that certain methods significantly enhance performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, highlighting the need for scalable detection methods like speech-based screening.

**Method:** The authors evaluated nine text-only models and three multimodal audio-text models using the DementiaBank speech corpus, testing various adaptation strategies such as in-context learning and parameter-efficient fine-tuning.

**Key Contributions:**

	1. Evaluation of various language model adaptation strategies for dementia detection
	2. Demonstrated the effectiveness of class-centroid demonstrations and token-level fine-tuning
	3. Highlighted the comparative performance of multimodal versus text-only models

**Result:** Class-centroid demonstrations showed the best performance in in-context learning; reasoning improved smaller models; token-level fine-tuning generated the highest scores overall.

**Limitations:** 

**Conclusion:** Model adaptation strategies critically influence speech-based dementia detection, with well-adapted open-weight models matching or exceeding the performance of commercial systems.

**Abstract:** Over half of US adults with Alzheimer disease and related dementias remain undiagnosed, and speech-based screening offers a scalable detection approach. We compared large language model adaptation strategies for dementia detection using the DementiaBank speech corpus, evaluating nine text-only models and three multimodal audio-text models on recordings from DementiaBank speech corpus. Adaptations included in-context learning with different demonstration selection policies, reasoning-augmented prompting, parameter-efficient fine-tuning, and multimodal integration. Results showed that class-centroid demonstrations achieved the highest in-context learning performance, reasoning improved smaller models, and token-level fine-tuning generally produced the best scores. Adding a classification head substantially improved underperforming models. Among multimodal models, fine-tuned audio-text systems performed well but did not surpass the top text-only models. These findings highlight that model adaptation strategies, including demonstration selection, reasoning design, and tuning method, critically influence speech-based dementia detection, and that properly adapted open-weight models can match or exceed commercial systems.

</details>


### [17] [Enhancing Speech Large Language Models through Reinforced Behavior Alignment](https://arxiv.org/abs/2509.03526)

*Yansong Liu, Jiateng Li, Yuan Liu*

**Main category:** cs.CL

**Keywords:** Speech-based LLMs, Reinforcement Learning, Instruction-Following

**Relevance Score:** 9

**TL;DR:** This paper introduces Reinforced Behavior Alignment (RBA) to improve the instruction-following capabilities of Speech-based LLMs (SpeechLMs) through self-synthesis of alignment data and reinforcement learning techniques.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance the performance of SpeechLMs in instruction-following tasks and address the performance gap compared to text-based LLMs due to inter-modal discrepancies.

**Method:** The RBA framework utilizes self-synthesis to generate high-fidelity alignment data and employs a reinforcement learning approach to align SpeechLM behavior with a teacher LLM.

**Key Contributions:**

	1. Introduced the RBA framework for aligning SpeechLMs
	2. Utilized self-synthesis for generating alignment data
	3. Achieved state-of-the-art performance on open benchmarks for SpeechLMs.

**Result:** Experimental results indicate that RBA significantly boosts instruction-following capabilities of SpeechLMs, surpassing conventional distillation baselines and achieving state-of-the-art performance in tasks like spoken question answering and speech-to-text translation.

**Limitations:** 

**Conclusion:** RBA offers a robust method for improving SpeechLMs by leveraging self-generated alignment data, leading to better performance across various speech processing tasks.

**Abstract:** The recent advancements of Large Language Models (LLMs) have spurred considerable research interest in extending their linguistic capabilities beyond text to other modalities, which leads to emergence of speech-based LLMs (SpeechLMs) with capability of processing user request in either speech or textual formats. However, owing to inter-modal discrepancies, these SpeechLMs still exhibit a significant performance gap compared to their text-based LLM counterparts in instruction-following, particularly when confronted with the dynamic and variable nature of user speech. To address this challenge, this paper introduces a framework termed Reinforced Behavior Alignment (RBA), designed to bolster the language generation proficiency of SpeechLMs. Instead of relying on supervised fine-tuning from human annotations, RBA employs a self-synthesis methodology to generate extensive, high-fidelity alignment data by a powerful teacher LLM. Then SpeechLMs is aligned its behavior with that of a teacher using a reinforcement learning-based approach. Experimental results demonstrate that this method effectively enhances the instruction-following capabilities of SpeechLMs that outperform conventional distillation baselines. Crucially, we demonstrate that RBA can be seamlessly extended to tasks such including spoken question answering and speech-to-text translation, attaining state-of-the-art performance on open benchmarks with only self-generated data.

</details>


### [18] [Multilevel Analysis of Cryptocurrency News using RAG Approach with Fine-Tuned Mistral Large Language Model](https://arxiv.org/abs/2509.03527)

*Bohdan M. Pavlyshenko*

**Main category:** cs.CL

**Keywords:** cryptocurrency, multilevel analysis, large language model, knowledge graph, sentiment analysis

**Relevance Score:** 4

**TL;DR:** The paper discusses a multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B LLM with retrieval-augmented generation (RAG), focusing on generating informative summaries and mitigating hallucinations through knowledge graphs.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the analysis of cryptocurrency news by integrating text and graph summaries while addressing hallucinations in large language models.

**Method:** Utilizes a fine-tuned Mistral 7B model with retrieval-augmented generation to produce and consolidate graph and text summaries with sentiment scores and JSON representations, employing hierarchical stacking for comprehensive reporting.

**Key Contributions:**

	1. Integration of graph and text summaries for comprehensive news analysis
	2. Application of fine-tuned Mistral 7B model in a novel RAG framework
	3. Utilization of knowledge graphs to mitigate LLM hallucinations

**Result:** Demonstrated that the model can effectively generate informative qualitative and quantitative analytics on cryptocurrency news, leading to essential insights.

**Limitations:** Focuses solely on cryptocurrency news; results may not generalize across other domains.

**Conclusion:** The multilevel approach with the fine-tuned Mistral model significantly enhances the analysis of cryptocurrency news, offering robust insights while reducing hallucination issues.

**Abstract:** In the paper, we consider multilevel multitask analysis of cryptocurrency news using a fine-tuned Mistral 7B large language model with retrieval-augmented generation (RAG).   On the first level of analytics, the fine-tuned model generates graph and text summaries with sentiment scores as well as JSON representations of summaries. Higher levels perform hierarchical stacking that consolidates sets of graph-based and text-based summaries as well as summaries of summaries into comprehensive reports. The combination of graph and text summaries provides complementary views of cryptocurrency news. The model is fine-tuned with 4-bit quantization using the PEFT/LoRA approach. The representation of cryptocurrency news as knowledge graph can essentially eliminate problems with large language model hallucinations.   The obtained results demonstrate that the use of fine-tuned Mistral 7B LLM models for multilevel cryptocurrency news analysis can conduct informative qualitative and quantitative analytics, providing important insights.

</details>


### [19] [The ProLiFIC dataset: Leveraging LLMs to Unveil the Italian Lawmaking Process](https://arxiv.org/abs/2509.03528)

*Matilde Contestabile, Chiara Ferrara, Alberto Giovannetti, Giovanni Parrillo, Andrea Vandin*

**Main category:** cs.CL

**Keywords:** Process Mining, Legal Systems, Large Language Models

**Relevance Score:** 4

**TL;DR:** ProLiFIC is an event log of the Italian lawmaking process, leveraging LLMs to enhance process mining in the legal field.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to improve the application of process mining in the legal domain, addressing limitations related to dataset accessibility and quality.

**Method:** ProLiFIC was created from unstructured data sourced from the Normattiva portal and structured using large language models to generate a comprehensive event log.

**Key Contributions:**

	1. Introduction of a comprehensive event log for the Italian lawmaking process.
	2. Utilization of LLMs to structure unstructured legal data.
	3. Establishment of a benchmark for legal process mining.

**Result:** Initial analyses demonstrate the utility of ProLiFIC as a benchmark for legal process mining, showcasing its capacity to improve legal analytics.

**Limitations:** 

**Conclusion:** ProLiFIC can significantly enhance the integration of process mining with legal systems and encourages further advancements in the field.

**Abstract:** Process Mining (PM), initially developed for industrial and business contexts, has recently been applied to social systems, including legal ones. However, PM's efficacy in the legal domain is limited by the accessibility and quality of datasets. We introduce ProLiFIC (Procedural Lawmaking Flow in Italian Chambers), a comprehensive event log of the Italian lawmaking process from 1987 to 2022. Created from unstructured data from the Normattiva portal and structured using large language models (LLMs), ProLiFIC aligns with recent efforts in integrating PM with LLMs. We exemplify preliminary analyses and propose ProLiFIC as a benchmark for legal PM, fostering new developments.

</details>


### [20] [Multimodal Proposal for an AI-Based Tool to Increase Cross-Assessment of Messages](https://arxiv.org/abs/2509.03529)

*Alejandro Álvarez Castro, Joaquín Ordieres-Meré*

**Main category:** cs.CL

**Keywords:** multi-modal learning, earnings calls, discourse analysis, transformer architecture, emotional signals

**Relevance Score:** 4

**TL;DR:** This paper presents a novel multi-modal framework for generating embeddings of earnings calls using hierarchical discourse trees, integrating emotional signals and discourse metadata to improve sentiment analysis in various communicative domains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The need for a more nuanced understanding of layered discourse in financial communication and other high-stakes interactions, which traditional models fail to capture.

**Method:** A two-stage transformer architecture is proposed, with the first stage using contrastive learning to encode node-level multi-modal content and discourse metadata, while the second stage synthesizes a global embedding from the conference.

**Key Contributions:**

	1. Introduces a hierarchical discourse tree framework for multi-modal embedding generation.
	2. Integrates emotional signals and structured metadata in the analysis of earnings calls.
	3. Demonstrates generalizability to other domains beyond finance, enhancing multi-modal discourse representation.

**Result:** Experimental results demonstrate that the embeddings effectively capture affective tone, structural logic, and thematic alignment, yielding meaningful representations of earnings calls.

**Limitations:** 

**Conclusion:** The framework has practical applications for financial forecasting and discourse evaluation, and is adaptable for various high-stakes unscripted communicative contexts such as telemedicine and education.

**Abstract:** Earnings calls represent a uniquely rich and semi-structured source of financial communication, blending scripted managerial commentary with unscripted analyst dialogue. Although recent advances in financial sentiment analysis have integrated multi-modal signals, such as textual content and vocal tone, most systems rely on flat document-level or sentence-level models, failing to capture the layered discourse structure of these interactions. This paper introduces a novel multi-modal framework designed to generate semantically rich and structurally aware embeddings of earnings calls, by encoding them as hierarchical discourse trees. Each node, comprising either a monologue or a question-answer pair, is enriched with emotional signals derived from text, audio, and video, as well as structured metadata including coherence scores, topic labels, and answer coverage assessments. A two-stage transformer architecture is proposed: the first encodes multi-modal content and discourse metadata at the node level using contrastive learning, while the second synthesizes a global embedding for the entire conference. Experimental results reveal that the resulting embeddings form stable, semantically meaningful representations that reflect affective tone, structural logic, and thematic alignment. Beyond financial reporting, the proposed system generalizes to other high-stakes unscripted communicative domains such as tele-medicine, education, and political discourse, offering a robust and explainable approach to multi-modal discourse representation. This approach offers practical utility for downstream tasks such as financial forecasting and discourse evaluation, while also providing a generalizable method applicable to other domains involving high-stakes communication.

</details>


### [21] [Reading Between the Signs: Predicting Future Suicidal Ideation from Adolescent Social Media Texts](https://arxiv.org/abs/2509.03530)

*Paul Blum, Enrico Liscio, Ruixuan Zhang, Caroline Figueroa, Pradeep K. Murukannaiah*

**Main category:** cs.CL

**Keywords:** suicidal ideation, predictive modeling, adolescent behavior, social media, transformer model

**Relevance Score:** 9

**TL;DR:** The paper proposes Early-SIB, a transformer-based model to predict suicidal ideation and behavior in adolescents from online forum posts, achieving a balanced accuracy of 0.73.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the challenge of predicting suicide among adolescents by leveraging social media posts, given that many who are at risk do not seek help from mental health services.

**Method:** The authors introduce Early-SIB, a transformer-based model that analyzes sequential forum posts made by users to predict the likelihood of them expressing suicidal ideation in future posts.

**Key Contributions:**

	1. Introduction of a novel task for predicting suicidal ideation from forum posts without explicit signals.
	2. Development of Early-SIB, a transformer-based predictive model for adolescent suicidal behavior.
	3. Demonstration of model efficacy with a balanced accuracy of 0.73 on real forum data.

**Result:** The model achieved a balanced accuracy of 0.73 in predicting suicidal ideation on a Dutch youth forum, indicating potential as a supportive tool for traditional methods.

**Limitations:** 

**Conclusion:** Early-SIB shows promise in enhancing suicide prevention efforts by providing timely predictions based on users' online behaviors without requiring explicit self-disclosure.

**Abstract:** Suicide is a leading cause of death among adolescents (12-18), yet predicting it remains a significant challenge. Many cases go undetected due to a lack of contact with mental health services. Social media, however, offers a unique opportunity, as young people often share their thoughts and struggles online in real time. In this work, we propose a novel task and method to approach it: predicting suicidal ideation and behavior (SIB) from forum posts before an adolescent explicitly expresses suicidal ideation on an online forum. This predictive framing, where no self-disclosure is used as input at any stage, remains largely unexplored in the suicide prediction literature. To this end, we introduce Early-SIB, a transformer-based model that sequentially processes the posts a user writes and engages with to predict whether they will write a SIB post. Our model achieves a balanced accuracy of 0.73 for predicting future SIB on a Dutch youth forum, demonstrating that such tools can offer a meaningful addition to traditional methods.

</details>


### [22] [Real-Time Detection of Hallucinated Entities in Long-Form Generation](https://arxiv.org/abs/2509.03531)

*Oscar Obeso, Andy Arditi, Javier Ferrando, Joshua Freeman, Cameron Holmes, Neel Nanda*

**Main category:** cs.CL

**Keywords:** hallucination detection, large language models, entity-level hallucinations

**Relevance Score:** 9

**TL;DR:** A novel scalable method for detecting hallucinations in large language models' outputs, specifically targeting entity-level hallucinations, demonstrates significant improvements in real-time identification.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the impracticality of existing hallucination detection methods in high-stakes applications where hallucinations can cause serious harm, such as medical and legal domains.

**Method:** The approach focuses on entity-level hallucinations by utilizing a web search-based annotation methodology to obtain grounded labels, allowing for efficient training of hallucination classifiers with linear probes.

**Key Contributions:**

	1. Development of a real-time hallucination detection method for long-form output.
	2. Introduction of an annotation methodology utilizing web search for entity-level hallucinations.
	3. Public release of datasets to support research and classifier training across different models.

**Result:** The classifiers consistently outperform baselines, achieving an AUC of 0.90 compared to 0.71 for existing methods like semantic entropy, and they also generalize to tasks involving incorrect answers in mathematical reasoning.

**Limitations:** Annotation methodology is costly and expensive, though it enables transfer learning for model classifiers.

**Conclusion:** The work presents a scalable strategy for real-world hallucination detection in long-form outputs of large language models, with public datasets for facilitating further research.

**Abstract:** Large language models are now routinely used in high-stakes applications where hallucinations can cause serious harm, such as medical consultations or legal advice. Existing hallucination detection methods, however, are impractical for real-world use, as they are either limited to short factual queries or require costly external verification. We present a cheap, scalable method for real-time identification of hallucinated tokens in long-form generations, and scale it effectively to 70B parameter models. Our approach targets \emph{entity-level hallucinations} -- e.g., fabricated names, dates, citations -- rather than claim-level, thereby naturally mapping to token-level labels and enabling streaming detection. We develop an annotation methodology that leverages web search to annotate model responses with grounded labels indicating which tokens correspond to fabricated entities. This dataset enables us to train effective hallucination classifiers with simple and efficient methods such as linear probes. Evaluating across four model families, our classifiers consistently outperform baselines on long-form responses, including more expensive methods such as semantic entropy (e.g., AUC 0.90 vs 0.71 for Llama-3.3-70B), and are also an improvement in short-form question-answering settings. Moreover, despite being trained only with entity-level labels, our probes effectively detect incorrect answers in mathematical reasoning tasks, indicating generalization beyond entities. While our annotation methodology is expensive, we find that annotated responses from one model can be used to train effective classifiers on other models; accordingly, we publicly release our datasets to facilitate reuse. Overall, our work suggests a promising new approach for scalable, real-world hallucination detection.

</details>


### [23] [Topic Identification in LLM Input-Output Pairs through the Lens of Information Bottleneck](https://arxiv.org/abs/2509.03533)

*Igor Halperin*

**Main category:** cs.CL

**Keywords:** Large Language Models, confabulations, semantic divergence, geometric clustering, information bottleneck

**Relevance Score:** 9

**TL;DR:** This paper presents a novel method called UDIB for detecting intrinsic faithfulness hallucinations in Large Language Models by improving topic identification through geometric clustering.

**Read time:** 20 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the issue of critical failure modes in LLMs, specifically focusing on intrinsic faithfulness hallucinations, which deviate responses from the provided context.

**Method:** The authors develop the UDIB algorithm, a modified version of the Deterministic Information Bottleneck method, which uses an efficient upper bound for KL divergence to improve clustering of high-dimensional data in LLMs.

**Key Contributions:**

	1. Development of the UDIB algorithm for topic clustering
	2. Improvement of the Semantic Divergence Metrics framework
	3. Provision of a principled method for high-dimensional data clustering

**Result:** UDIB generates a shared topic representation that is structured to maximize information regarding the prompt-response relationship, facilitating a more sensitive detection of confabulations.

**Limitations:** 

**Conclusion:** The UDIB method enhances the Semantic Divergence Metrics framework by providing a more informative basis for topic identification, leading to better outcomes in detecting LLM hallucinations.

**Abstract:** Large Language Models (LLMs) are prone to critical failure modes, including \textit{intrinsic faithfulness hallucinations} (also known as confabulations), where a response deviates semantically from the provided context. Frameworks designed to detect this, such as Semantic Divergence Metrics (SDM), rely on identifying latent topics shared between prompts and responses, typically by applying geometric clustering to their sentence embeddings. This creates a disconnect, as the topics are optimized for spatial proximity, not for the downstream information-theoretic analysis. In this paper, we bridge this gap by developing a principled topic identification method grounded in the Deterministic Information Bottleneck (DIB) for geometric clustering. Our key contribution is to transform the DIB method into a practical algorithm for high-dimensional data by substituting its intractable KL divergence term with a computationally efficient upper bound. The resulting method, which we dub UDIB, can be interpreted as an entropy-regularized and robustified version of K-means that inherently favors a parsimonious number of informative clusters. By applying UDIB to the joint clustering of LLM prompt and response embeddings, we generate a shared topic representation that is not merely spatially coherent but is fundamentally structured to be maximally informative about the prompt-response relationship. This provides a superior foundation for the SDM framework and offers a novel, more sensitive tool for detecting confabulations.

</details>


### [24] [QuesGenie: Intelligent Multimodal Question Generation](https://arxiv.org/abs/2509.03535)

*Ahmed Mubarak, Amna Ahmed, Amira Nasser, Aya Mohamed, Fares El-Sadek, Mohammed Ahmed, Ahmed Salah, Youssef Sobhy*

**Main category:** cs.CL

**Keywords:** question generation, multi-modal, reinforcement learning, education, interactive interface

**Relevance Score:** 6

**TL;DR:** Development of a multi-modal question generation system addressing the lack of tailored practice materials for learners.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** To bridge the gap in practice materials available to learners by creating an automated question generation system.

**Method:** The system incorporates multi-modal input handling, question generation, reinforcement learning from human feedback, and an interactive interface.

**Key Contributions:**

	1. Development of multi-modal question generation system
	2. Incorporation of reinforcement learning from human feedback
	3. Creation of an interactive interface for end-users

**Result:** The system effectively generates diverse question types from various content formats, enhancing learning engagement.

**Limitations:** 

**Conclusion:** This project establishes a foundational approach for intelligent question generation that is scalable and resource-efficient while providing a user-friendly experience.

**Abstract:** In today's information-rich era, learners have access to abundant educational resources, but the lack of practice materials tailored to these resources presents a significant challenge. This project addresses that gap by developing a multi-modal question generation system that can automatically generate diverse question types from various content formats. The system features four major components: multi-modal input handling, question generation, reinforcement learning from human feedback (RLHF), and an end-to-end interactive interface. This project lays the foundation for automated, scalable, and intelligent question generation, carefully balancing resource efficiency, robust functionality and a smooth user experience.

</details>


### [25] [AR$^2$: Adversarial Reinforcement Learning for Abstract Reasoning in Large Language Models](https://arxiv.org/abs/2509.03537)

*Cheng-Kai Yeh, Hsing-Wang Lee, Chung-Hung Kuo, Hen-Hsen Huang*

**Main category:** cs.CL

**Keywords:** large language models, abstraction, reinforcement learning, human-computer interaction

**Relevance Score:** 8

**TL;DR:** This paper introduces AR$^2$, a framework aimed at enhancing abstraction abilities in large language models (LLMs) using adversarial reinforcement learning.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the need for improved abstraction skills in LLMs, which are crucial for effective problem-solving in computer science and coding tasks.

**Method:** AR$^2$ consists of a teacher model that generates complex narrative descriptions of programming tasks while maintaining their core logic, to train a student model to solve these tasks.

**Key Contributions:**

	1. Introduction of the AR$^2$ framework for enhancing LLM abstraction
	2. Demonstration of improved accuracy in programming tasks via adversarial reinforcement learning
	3. Creation of challenging narrative-rich problem descriptions for teaching LLMs.

**Result:** Experimental results indicate that AR$^2$ significantly boosts the accuracy of the student model on challenging programming tasks that it has not encountered before.

**Limitations:** 

**Conclusion:** The findings highlight the importance of abstraction as a fundamental skill necessary for improving the generalization capabilities of LLMs.

**Abstract:** Abstraction--the ability to recognize and distill essential computational patterns from complex problem statements--is a foundational skill in computer science, critical both for human problem-solvers and coding-oriented large language models (LLMs). Despite recent advances in training LLMs for code generation using reinforcement learning (RL), most existing approaches focus primarily on superficial pattern recognition, overlooking explicit training for abstraction. In this study, we propose AR$^2$ (Adversarial Reinforcement Learning for Abstract Reasoning), a novel framework explicitly designed to enhance the abstraction abilities of LLMs. AR$^2$ employs a teacher model to transform kernel problems into narrative-rich, challenging descriptions without changing their fundamental logic. Simultaneously, a student coding model is trained to solve these complex narrative problems by extracting their underlying computational kernels. Experimental results demonstrate that AR$^2$ substantially improves the student model's accuracy on previously unseen, challenging programming tasks, underscoring abstraction as a key skill for enhancing LLM generalization.

</details>


### [26] [Improving Factuality in LLMs via Inference-Time Knowledge Graph Construction](https://arxiv.org/abs/2509.03540)

*Shanglin Wu, Lihui Liu, Jinho D. Choi, Kai Shu*

**Main category:** cs.CL

**Keywords:** Large Language Models, knowledge graphs, Retrieval-Augmented Generation

**Relevance Score:** 10

**TL;DR:** The paper introduces a novel framework for improving factual consistency in Large Language Models (LLMs) by dynamically constructing and expanding knowledge graphs during inference.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** LLMs often produce factually inconsistent answers due to limitations in their parametric memory, particularly when relying solely on unstructured text for integrated knowledge.

**Method:** The proposed framework constructs a seed knowledge graph from the input question and expands it iteratively using the LLM's latent knowledge, followed by refinement with externally retrieved information.

**Key Contributions:**

	1. Dynamic knowledge graph construction during inference
	2. Integration of internal and external knowledge
	3. Improved factual accuracy and interpretability in LLM responses

**Result:** Evaluation on three factual QA benchmarks shows consistent improvements in factual accuracy, answer precision, and interpretability over existing methods.

**Limitations:** 

**Conclusion:** Inference-time construction of knowledge graphs is a promising technique for enhancing the factuality of LLMs in a structured and interpretable way.

**Abstract:** Large Language Models (LLMs) often struggle with producing factually consistent answers due to limitations in their parametric memory. Retrieval-Augmented Generation (RAG) methods address this issue by incorporating external knowledge from trusted sources at inference time. However, such methods typically treat knowledge as unstructured text, which limits their ability to support compositional reasoning and identify factual inconsistencies. To overcome these limitations, we propose a novel framework that dynamically constructs and expands knowledge graphs (KGs) during inference, integrating both internal knowledge extracted from LLMs and external information retrieved from external sources. Our method begins by extracting a seed KG from the question via prompting, followed by iterative expansion using the LLM's latent knowledge. The graph is then selectively refined through external retrieval, enhancing factual coverage and correcting inaccuracies. We evaluate our approach on three diverse factual QA benchmarks, demonstrating consistent improvements in factual accuracy, answer precision, and interpretability over baseline prompting and static KG-augmented methods. Our findings suggest that inference-time KG construction is a promising direction for enhancing LLM factuality in a structured, interpretable, and scalable manner.

</details>


### [27] [ResearchPulse: Building Method-Experiment Chains through Multi-Document Scientific Inference](https://arxiv.org/abs/2509.03565)

*Qi Chen, Jingxuan Wei, Zhuoya Yao, Haiguang Wang, Gaowei Wu, Bihui Yu, Siyuan Li, Cheng Tan*

**Main category:** cs.CL

**Keywords:** multi-document inference, scientific reasoning, agent-based framework

**Relevance Score:** 8

**TL;DR:** This paper introduces ResearchPulse, an agent-based framework for multi-document scientific inference, which extracts and aligns research motivation, methods, and results across related papers to reconstruct research development chains.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** There is a need for structured cross-document reasoning in science to understand the evolution of scientific ideas, beyond just summarizing individual papers.

**Method:** The approach formalizes the task of multi-document scientific inference, utilizing three coordinated agents: a Plan Agent for task decomposition, a Mmap-Agent for creating motivation-method mind maps, and a Lchart-Agent for synthesizing experimental charts. It also includes ResearchPulse-Bench, a benchmark for evaluating the system.

**Key Contributions:**

	1. Formalization of multi-document scientific inference
	2. Proposal of an agent-based framework for research synthesis
	3. Development of ResearchPulse-Bench benchmark for evaluation

**Result:** ResearchPulse consistently outperforms strong baselines, including GPT-4o, in terms of semantic alignment, structural consistency, and visual fidelity.

**Limitations:** 

**Conclusion:** The proposed framework shows promise in enhancing the understanding of scientific research development and offers a novel benchmark for related tasks.

**Abstract:** Understanding how scientific ideas evolve requires more than summarizing individual papers-it demands structured, cross-document reasoning over thematically related research. In this work, we formalize multi-document scientific inference, a new task that extracts and aligns motivation, methodology, and experimental results across related papers to reconstruct research development chains. This task introduces key challenges, including temporally aligning loosely structured methods and standardizing heterogeneous experimental tables. We present ResearchPulse, an agent-based framework that integrates instruction planning, scientific content extraction, and structured visualization. It consists of three coordinated agents: a Plan Agent for task decomposition, a Mmap-Agent that constructs motivation-method mind maps, and a Lchart-Agent that synthesizes experimental line charts. To support this task, we introduce ResearchPulse-Bench, a citation-aware benchmark of annotated paper clusters. Experiments show that our system, despite using 7B-scale agents, consistently outperforms strong baselines like GPT-4o in semantic alignment, structural consistency, and visual fidelity. The dataset are available in https://huggingface.co/datasets/ResearchPulse/ResearchPulse-Bench.

</details>


### [28] [NoteBar: An AI-Assisted Note-Taking System for Personal Knowledge Management](https://arxiv.org/abs/2509.03610)

*Josh Wisoff, Yao Tang, Zhengyu Fang, Jordan Guzman, YuTang Wang, Alex Yu*

**Main category:** cs.CL

**Keywords:** AI-assisted note-taking, personal knowledge management, language models, MBTI personas, workflow organization

**Relevance Score:** 8

**TL;DR:** NoteBar is an AI-assisted note-taking tool designed to enhance organization and workflow by utilizing persona information and efficient language models, along with a novel dataset for evaluation.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve note-taking efficiency in academic and professional settings using AI-assisted tools, addressing the limitations of existing solutions.

**Method:** The paper introduces NoteBar, which automatically categorizes notes based on user personas and presents a dataset of 3,173 notes annotated across 8,494 concepts linked to 16 MBTI personas.

**Key Contributions:**

	1. Introduction of NoteBar, an innovative AI-assisted note-taking tool
	2. Creation of a novel persona-conditioned dataset for research and evaluation
	3. Demonstration of practical, cost-effective deployment of AI in note-taking

**Result:** NoteBar enables effective organization of notes and supports user workflows interactively and cost-effectively, showcasing practical deployment without heavy infrastructure.

**Limitations:** 

**Conclusion:** NoteBar and its dataset offer a scalable foundation for enhancing AI-assisted personal knowledge management.

**Abstract:** Note-taking is a critical practice for capturing, organizing, and reflecting on information in both academic and professional settings. The recent success of large language models has accelerated the development of AI-assisted tools, yet existing solutions often struggle with efficiency. We present NoteBar, an AI-assisted note-taking tool that leverages persona information and efficient language models to automatically organize notes into multiple categories and better support user workflows. To support research and evaluation in this space, we further introduce a novel persona-conditioned dataset of 3,173 notes and 8,494 annotated concepts across 16 MBTI personas, offering both diversity and semantic richness for downstream tasks. Finally, we demonstrate that NoteBar can be deployed in a practical and cost-effective manner, enabling interactive use without reliance on heavy infrastructure. Together, NoteBar and its accompanying dataset provide a scalable and extensible foundation for advancing AI-assisted personal knowledge management.

</details>


### [29] [E-ARMOR: Edge case Assessment and Review of Multilingual Optical Character Recognition](https://arxiv.org/abs/2509.03615)

*Aryan Gupta, Anupam Purwar*

**Main category:** cs.CL

**Keywords:** Optical Character Recognition, Multilingual, Edge Deployment, Vision-Language Models, Efficiency

**Relevance Score:** 6

**TL;DR:** Sprinklr-Edge-OCR, a novel OCR system optimized for edge deployment, was compared with five state-of-the-art LVLMs and two traditional OCR systems, demonstrating superior efficiency and cost-effectiveness.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study addresses the challenges in Optical Character Recognition (OCR) within multilingual and noisy images, exploring the potential of Large Vision-Language Models (LVLMs) versus traditional OCR systems.

**Method:** A large-scale comparative evaluation was conducted on a proprietary, hand-annotated dataset of multilingual images, assessing accuracy, semantics, efficiency, and cost across multiple OCR models.

**Key Contributions:**

	1. Introduction of Sprinklr-Edge-OCR, optimized for edge deployment.
	2. Comparative evaluation of OCR systems, revealing efficiency metrics.
	3. Insights into real-world applicability and edge case analysis.

**Result:** Qwen achieved the highest precision, but Sprinklr-Edge-OCR delivered the best F1 score and was significantly faster and cheaper than LVLMs.

**Limitations:** 

**Conclusion:** Traditional OCR systems remain optimal for edge deployment due to their low computing needs and high affordability, despite advancements in LLM technology.

**Abstract:** Optical Character Recognition (OCR) in multilingual, noisy, and diverse real-world images remains a significant challenge for optical character recognition systems. With the rise of Large Vision-Language Models (LVLMs), there is growing interest in their ability to generalize and reason beyond fixed OCR pipelines. In this work, we introduce Sprinklr-Edge-OCR, a novel OCR system built specifically optimized for edge deployment in resource-constrained environments. We present a large-scale comparative evaluation of five state-of-the-art LVLMs (InternVL, Qwen, GOT OCR, LLaMA, MiniCPM) and two traditional OCR systems (Sprinklr-Edge-OCR, SuryaOCR) on a proprietary, doubly hand annotated dataset of multilingual (54 languages) images. Our benchmark covers a broad range of metrics including accuracy, semantic consistency, language coverage, computational efficiency (latency, memory, GPU usage), and deployment cost. To better reflect real-world applicability, we also conducted edge case deployment analysis, evaluating model performance on CPU only environments. Among the results, Qwen achieved the highest precision (0.54), while Sprinklr-Edge-OCR delivered the best overall F1 score (0.46) and outperformed others in efficiency, processing images 35 faster (0.17 seconds per image on average) and at less than 0.01 of the cost (0.006 USD per 1,000 images) compared to LVLM. Our findings demonstrate that the most optimal OCR systems for edge deployment are the traditional ones even in the era of LLMs due to their low compute requirements, low latency, and very high affordability.

</details>


### [30] [Breaking the Mirror: Activation-Based Mitigation of Self-Preference in LLM Evaluators](https://arxiv.org/abs/2509.03647)

*Dani Roytburg, Matthew Bozoukov, Matthew Nguyen, Jou Barzdukas, Simon Fu, Narmeen Oozeer*

**Main category:** cs.CL

**Keywords:** Large language models, self-preference bias, steering vectors, Contrastive Activation Addition, preference tuning

**Relevance Score:** 8

**TL;DR:** This paper investigates the issue of self-preference bias in large language models (LLMs) and explores using lightweight steering vectors to mitigate this bias during inference without retraining.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The self-preference bias in LLMs affects the fairness and reliability of automated evaluations, particularly in preference tuning and model routing tasks.

**Method:** The authors introduce a curated dataset to classify self-preference bias and propose steering vectors created using Contrastive Activation Addition (CAA) and an optimization-based approach.

**Key Contributions:**

	1. Introduction of a curated dataset for analyzing self-preference bias in LLMs
	2. Development of lightweight steering vectors to reduce bias
	3. Performance benchmark demonstrating significant bias reduction compared to existing methods.

**Result:** The steering vectors can reduce unjustified self-preference bias by up to 97%, significantly outperforming existing baselines such as prompting and direct preference optimization.

**Limitations:** Steering vectors exhibit instability in addressing legitimate self-preference and unbiased agreement, suggesting their limitations as a singular solution.

**Conclusion:** While steering vectors show promise in mitigating self-preference bias, they struggle with legitimate cases of self-preference and unbiased agreement, indicating the need for more robust solutions.

**Abstract:** Large language models (LLMs) increasingly serve as automated evaluators, yet they suffer from "self-preference bias": a tendency to favor their own outputs over those of other models. This bias undermines fairness and reliability in evaluation pipelines, particularly for tasks like preference tuning and model routing. We investigate whether lightweight steering vectors can mitigate this problem at inference time without retraining. We introduce a curated dataset that distinguishes self-preference bias into justified examples of self-preference and unjustified examples of self-preference, and we construct steering vectors using two methods: Contrastive Activation Addition (CAA) and an optimization-based approach. Our results show that steering vectors can reduce unjustified self-preference bias by up to 97\%, substantially outperforming prompting and direct preference optimization baselines. Yet steering vectors are unstable on legitimate self-preference and unbiased agreement, implying self-preference spans multiple or nonlinear directions. This underscores both their promise and limits as safeguards for LLM-as-judges and motivates more robust interventions.

</details>


### [31] [Semantic Analysis of SNOMED CT Concept Co-occurrences in Clinical Documentation using MIMIC-IV](https://arxiv.org/abs/2509.03662)

*Ali Noori, Somya Mohanty, Prashanti Manda*

**Main category:** cs.CL

**Keywords:** SNOMED CT, Clinical Notes, Semantic Similarity, MIMIC-IV, Embedding

**Relevance Score:** 9

**TL;DR:** This study investigates the relationship between SNOMED CT concept co-occurrence patterns and embedding-based semantic similarity using the MIMIC-IV database, revealing insights into their complementary value in clinical documentation and decision support.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The unstructured format of clinical notes presents challenges for large-scale analysis, necessitating improved understanding of concepts through standardized terminologies and their relationships.

**Method:** The study uses Normalized Pointwise Mutual Information (NPMI) alongside pretrained embeddings (ClinicalBERT, BioBERT) to analyze co-occurrence patterns and semantic similarity of clinical concepts over time and across specialties.

**Key Contributions:**

	1. Investigation of co-occurrence patterns and semantic similarity in clinical concepts
	2. Demonstration of embedding utility for augmenting clinical annotations
	3. Identification of clinically coherent themes mapping to patient outcomes

**Result:** The analysis shows weak correlation between co-occurrence and semantic similarity, but embeddings capture clinically meaningful associations and suggest missing concepts. Co-occurrence patterns are linked to patient outcomes like mortality and readmission.

**Limitations:** 

**Conclusion:** The findings emphasize the value of combining co-occurrence statistics with semantic embeddings to enhance clinical documentation, reveal hidden relationships, and aid in decision support and phenotyping.

**Abstract:** Clinical notes contain rich clinical narratives but their unstructured format poses challenges for large-scale analysis. Standardized terminologies such as SNOMED CT improve interoperability, yet understanding how concepts relate through co-occurrence and semantic similarity remains underexplored. In this study, we leverage the MIMIC-IV database to investigate the relationship between SNOMED CT concept co-occurrence patterns and embedding-based semantic similarity. Using Normalized Pointwise Mutual Information (NPMI) and pretrained embeddings (e.g., ClinicalBERT, BioBERT), we examine whether frequently co-occurring concepts are also semantically close, whether embeddings can suggest missing concepts, and how these relationships evolve temporally and across specialties. Our analyses reveal that while co-occurrence and semantic similarity are weakly correlated, embeddings capture clinically meaningful associations not always reflected in documentation frequency. Embedding-based suggestions frequently matched concepts later documented, supporting their utility for augmenting clinical annotations. Clustering of concept embeddings yielded coherent clinical themes (symptoms, labs, diagnoses, cardiovascular conditions) that map to patient phenotypes and care patterns. Finally, co-occurrence patterns linked to outcomes such as mortality and readmission demonstrate the practical utility of this approach. Collectively, our findings highlight the complementary value of co-occurrence statistics and semantic embeddings in improving documentation completeness, uncovering latent clinical relationships, and informing decision support and phenotyping applications.

</details>


### [32] [MLSD: A Novel Few-Shot Learning Approach to Enhance Cross-Target and Cross-Domain Stance Detection](https://arxiv.org/abs/2509.03725)

*Parush Gera, Tempestt Neal*

**Main category:** cs.CL

**Keywords:** stance detection, metric learning, few-shot learning, domain adaptation, semantic similarity

**Relevance Score:** 6

**TL;DR:** Introducing a novel metric learning approach for stance detection that improves performance across various targets and domains.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance stance detection capabilities in diverse contexts by effectively utilizing examples from new target domains.

**Method:** Metric Learning-Based Few-Shot Learning (MLSD) uses triplet loss for metric learning to identify semantic similarities and differences in stance detection tasks.

**Key Contributions:**

	1. Presentation of MLSD for stance detection
	2. Use of triplet loss for metric learning
	3. Demonstrated improvement across multiple stance detection models

**Result:** MLSD demonstrated statistically significant improvements in stance detection performance across six well-known stance detection models in multiple testing scenarios.

**Limitations:** 

**Conclusion:** The proposed MLSD approach effectively enables better stance detection through improved domain adaptation and embedding techniques.

**Abstract:** We present the novel approach for stance detection across domains and targets, Metric Learning-Based Few-Shot Learning for Cross-Target and Cross-Domain Stance Detection (MLSD). MLSD utilizes metric learning with triplet loss to capture semantic similarities and differences between stance targets, enhancing domain adaptation. By constructing a discriminative embedding space, MLSD allows a cross-target or cross-domain stance detection model to acquire useful examples from new target domains. We evaluate MLSD in multiple cross-target and cross-domain scenarios across two datasets, showing statistically significant improvement in stance detection performance across six widely used stance detection models.

</details>


### [33] [SiLVERScore: Semantically-Aware Embeddings for Sign Language Generation Evaluation](https://arxiv.org/abs/2509.03791)

*Saki Imai, Mert İnan, Anthony Sicilia, Malihe Alikhani*

**Main category:** cs.CL

**Keywords:** Sign Language Generation, Evaluation Metrics, Semantic Awareness

**Relevance Score:** 6

**TL;DR:** SiLVERScore is a novel semantic evaluation metric for sign language generation that improves upon traditional text-based metrics by assessing sign generation in a joint embedding space.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing evaluation methods for sign language generation are inadequate as they fail to capture the multimodal aspects of sign language and introduce ambiguity in error identification.

**Method:** Introduced SiLVERScore, a semantically-aware embedding-based evaluation metric that evaluates sign language generation within a joint embedding space.

**Key Contributions:**

	1. Identifying limitations of existing evaluation metrics for sign language generation
	2. Introducing SiLVERScore for semantically-aware evaluation
	3. Demonstrating the robustness of SiLVERScore to semantic and prosodic variations

**Result:** SiLVERScore achieves near-perfect discrimination between correct and random pairs on the PHOENIX-14T and CSL-Daily datasets, with ROC AUC = 0.99, outperforming traditional metrics significantly.

**Limitations:** 

**Conclusion:** The proposed SiLVERScore effectively evaluates sign language generation, capturing nuanced aspects of sign language that existing metrics overlook.

**Abstract:** Evaluating sign language generation is often done through back-translation, where generated signs are first recognized back to text and then compared to a reference using text-based metrics. However, this two-step evaluation pipeline introduces ambiguity: it not only fails to capture the multimodal nature of sign language-such as facial expressions, spatial grammar, and prosody-but also makes it hard to pinpoint whether evaluation errors come from sign generation model or the translation system used to assess it. In this work, we propose SiLVERScore, a novel semantically-aware embedding-based evaluation metric that assesses sign language generation in a joint embedding space. Our contributions include: (1) identifying limitations of existing metrics, (2) introducing SiLVERScore for semantically-aware evaluation, (3) demonstrating its robustness to semantic and prosodic variations, and (4) exploring generalization challenges across datasets. On PHOENIX-14T and CSL-Daily datasets, SiLVERScore achieves near-perfect discrimination between correct and random pairs (ROC AUC = 0.99, overlap < 7%), substantially outperforming traditional metrics.

</details>


### [34] [Measuring How (Not Just Whether) VLMs Build Common Ground](https://arxiv.org/abs/2509.03805)

*Saki Imai, Mert İnan, Anthony Sicilia, Malihe Alikhani*

**Main category:** cs.CL

**Keywords:** Vision Language Models, Grounding, Interactive Games, HCI, Evaluation Metrics

**Relevance Score:** 8

**TL;DR:** This paper evaluates the performance of vision language models (VLMs) in interactive grounding contexts using a new four-metric suite.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Current benchmarks for VLMs assess them in static settings, failing to capture their interactive reasoning capabilities; hence, a new evaluation method is needed.

**Method:** A four-metric suite was introduced to assess VLMs based on grounding efficiency, content alignment, lexical adaptation, and human-likeness, tested through 150 interactive referential game sessions among VLMs and human interactions.

**Key Contributions:**

	1. Introduction of a four-metric evaluation suite for interactive grounding in VLMs
	2. Comparison of VLM grounding performance against human dyads
	3. Findings highlight the limitations of using task success as a measure of effective grounding.

**Result:** The evaluation revealed that the three VLMs diverged from human interaction patterns, especially in grounding efficiency and content alignment, with GPT4o-mini performing the best.

**Limitations:** Limited to three VLMs and findings based on self-play sessions; further validation needed with a broader range of models and real-world interactions.

**Conclusion:** Standard task success scores may not accurately reflect effective grounding in interactive contexts, indicating a need for better evaluation methods in VLM research.

**Abstract:** Large vision language models (VLMs) increasingly claim reasoning skills, yet current benchmarks evaluate them in single-turn or question answering settings. However, grounding is an interactive process in which people gradually develop shared understanding through ongoing communication. We introduce a four-metric suite (grounding efficiency, content alignment, lexical adaptation, and human-likeness) to systematically evaluate VLM performance in interactive grounding contexts. We deploy the suite on 150 self-play sessions of interactive referential games between three proprietary VLMs and compare them with human dyads. All three models diverge from human patterns on at least three metrics, while GPT4o-mini is the closest overall. We find that (i) task success scores do not indicate successful grounding and (ii) high image-utterance alignment does not necessarily predict task success. Our metric suite and findings offer a framework for future research on VLM grounding.

</details>


### [35] [Align-then-Slide: A complete evaluation framework for Ultra-Long Document-Level Machine Translation](https://arxiv.org/abs/2509.03809)

*Jiaxin Guo, Daimeng Wei, Yuanchang Luo, Xiaoyu Chen, Zhanglin Wu, Huan Yang, Hengchao Shang, Zongyao Li, Zhiqiang Rao, Jinlong Yang, Hao Yang*

**Main category:** cs.CL

**Keywords:** document-level machine translation, evaluation framework, large language models

**Relevance Score:** 4

**TL;DR:** Introduction of Align-then-Slide, an evaluation framework for document-level machine translation that improves upon existing sentence-aligned evaluation methods.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the limitations of current evaluation methods for whole-document outputs from large language models in document-level machine translation.

**Method:** The paper presents a two-stage evaluation process: 1) Align stage to infer sentence-level correspondences and rebuild targets; 2) n-Chunk Sliding Evaluate stage for multi-granularity assessment by calculating averaged metric scores for different chunk sizes.

**Key Contributions:**

	1. Introduction of a novel evaluation framework for document-level translations.
	2. Multi-granularity performance assessment through chunk-based evaluation.
	3. Demonstrated effectiveness of the framework with high correlation to human evaluations.

**Result:** Achieved a Pearson correlation of 0.929 with expert ratings on the WMT benchmark and demonstrated alignment with human judgments on a new test set. Showed that preference data from the method enhances CPO training and serves as an effective reward model for GRPO, resulting in improved translations.

**Limitations:** 

**Conclusion:** The Align-then-Slide framework is validated as an accurate and actionable tool for evaluating document-level machine translation systems.

**Abstract:** Large language models (LLMs) have ushered in a new era for document-level machine translation (\textit{doc}-mt), yet their whole-document outputs challenge existing evaluation methods that assume sentence-by-sentence alignment. We introduce \textit{\textbf{Align-then-Slide}}, a complete evaluation framework for ultra-long doc-mt. In the Align stage, we automatically infer sentence-level source-target correspondences and rebuild the target to match the source sentence number, resolving omissions and many-to-one/one-to-many mappings. In the n-Chunk Sliding Evaluate stage, we calculate averaged metric scores under 1-, 2-, 3- and 4-chunk for multi-granularity assessment. Experiments on the WMT benchmark show a Pearson correlation of 0.929 between our method with expert MQM rankings. On a newly curated real-world test set, our method again aligns closely with human judgments. Furthermore, preference data produced by Align-then-Slide enables effective CPO training and its direct use as a reward model for GRPO, both yielding translations preferred over a vanilla SFT baseline. The results validate our framework as an accurate, robust, and actionable evaluation tool for doc-mt systems.

</details>


### [36] [NE-PADD: Leveraging Named Entity Knowledge for Robust Partial Audio Deepfake Detection via Attention Aggregation](https://arxiv.org/abs/2509.03829)

*Huhong Xian, Rui Liu, Berrak Sisman, Haizhou Li*

**Main category:** cs.CL

**Keywords:** Partial Audio Deepfake Detection, named entity recognition, attention mechanisms

**Relevance Score:** 7

**TL;DR:** The paper introduces NE-PADD, a novel approach for Partial Audio Deepfake Detection that utilizes named entity information through Speech Name Entity Recognition and attention mechanisms.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Existing methods for audio deepfake detection focus only at the sentence level and do not leverage named entity information, leaving a gap in partial audio detection capabilities.

**Method:** NE-PADD employs two branches: Speech Name Entity Recognition and Partial Audio Deepfake Detection, utilizing two attention mechanisms (Attention Fusion and Attention Transfer) for improved detection.

**Key Contributions:**

	1. Introduction of NE-PADD for PADD leveraging semantic information.
	2. Development of dual attention mechanisms for better detection accuracy.
	3. Demonstration of improved performance over existing baselines using the PartialSpoof-NER dataset.

**Result:** Experiments on the PartialSpoof-NER dataset indicate that NE-PADD surpasses existing baseline methods in performance, highlighting the benefits of integrating named entity knowledge.

**Limitations:** 

**Conclusion:** The proposed method demonstrates that incorporating named entity information significantly enhances the success of Partial Audio Deepfake Detection.

**Abstract:** Different from traditional sentence-level audio deepfake detection (ADD), partial audio deepfake detection (PADD) requires frame-level positioning of the location of fake speech. While some progress has been made in this area, leveraging semantic information from audio, especially named entities, remains an underexplored aspect. To this end, we propose NE-PADD, a novel method for Partial Audio Deepfake Detection (PADD) that leverages named entity knowledge through two parallel branches: Speech Name Entity Recognition (SpeechNER) and PADD. The approach incorporates two attention aggregation mechanisms: Attention Fusion (AF) for combining attention weights and Attention Transfer (AT) for guiding PADD with named entity semantics using an auxiliary loss. Built on the PartialSpoof-NER dataset, experiments show our method outperforms existing baselines, proving the effectiveness of integrating named entity knowledge in PADD. The code is available at https://github.com/AI-S2-Lab/NE-PADD.

</details>


### [37] [Drivel-ology: Challenging LLMs with Interpreting Nonsense with Depth](https://arxiv.org/abs/2509.03867)

*Yang Wang, Chenghao Xiao, Chia-Yi Hsiao, Zi Yan Chang, Chi-Li Chen, Tyler Loakman, Chenghua Lin*

**Main category:** cs.CL

**Keywords:** Drivelology, large language models, pragmatic understanding, natural language processing, benchmark dataset

**Relevance Score:** 8

**TL;DR:** Drivelology is a linguistic phenomenon characterized as "nonsense with depth" that large language models struggle to interpret meaningfully.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the limitations of large language models in understanding complex linguistic constructs known as Drivelology, which requires nuanced inference beyond superficial coherence.

**Method:** A benchmark dataset of over 1,200 curated examples of Drivelology was created, followed by evaluations of various large language models on tasks involving classification, generation, and reasoning.

**Key Contributions:**

	1. Introduction of the concept of Drivelology
	2. Development of a benchmark dataset for evaluating LLMs
	3. Insights into LLM limitations regarding pragmatic understanding.

**Result:** LLMs often misinterpret Drivelological text as shallow nonsense, providing incoherent justifications and missing implied meanings, highlighting gaps in their pragmatic understanding.

**Limitations:** The subjective nature of Drivelology may complicate broader generalizability of results.

**Conclusion:** The findings indicate a significant representational gap in LLMs that challenges the assumption that statistical language fluency equates to cognitive comprehension, and a dataset is released to aid future research.

**Abstract:** We introduce Drivelology, a unique linguistic phenomenon characterised as "nonsense with depth", utterances that are syntactically coherent yet pragmatically paradoxical, emotionally loaded, or rhetorically subversive. While such expressions may resemble surface-level nonsense, they encode implicit meaning requiring contextual inference, moral reasoning, or emotional interpretation. We find that current large language models (LLMs), despite excelling at many natural language processing (NLP) tasks, consistently fail to grasp the layered semantics of Drivelological text. To investigate this, we construct a small but diverse benchmark dataset of over 1,200 meticulously curated examples, with select instances in English, Mandarin, Spanish, French, Japanese, and Korean. Annotation was especially challenging: each of the examples required careful expert review to verify that it truly reflected Drivelological characteristics. The process involved multiple rounds of discussion and adjudication to address disagreements, highlighting the subtle and subjective nature of the Drivelology. We evaluate a range of LLMs on classification, generation, and reasoning tasks. Our results reveal clear limitations of LLMs: models often confuse Drivelology with shallow nonsense, produce incoherent justifications, or miss the implied rhetorical function altogether. These findings highlight a deeper representational gap in LLMs' pragmatic understanding and challenge the assumption that statistical fluency implies cognitive comprehension. We release our dataset and code to facilitate further research in modelling linguistic depth beyond surface-level coherence.

</details>


### [38] [Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue](https://arxiv.org/abs/2509.04104)

*Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx*

**Main category:** cs.CL

**Keywords:** lexical alignment, conversational agents, personalized profiles, large language models, human-agent dialogue

**Relevance Score:** 9

**TL;DR:** This study investigates constructing stable, personalised lexical profiles for enabling lexical alignment in conversational agents, highlighting optimal data usage.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the implementation of lexical alignment in conversational agents, leveraging recent advancements in large language models.

**Method:** The study varied the amounts of transcribed spoken data and the number of items in lexical profiles per part-of-speech category to evaluate performance using recall, coverage, and cosine similarity metrics.

**Key Contributions:**

	1. Methods for personalized lexical profiles in conversational agents
	2. Evaluation metrics for profile performance over time
	3. Findings on optimal data requirements for lexical alignment

**Result:** Smaller and more compact lexical profiles, created from 10 minutes of transcribed speech, showed the best balance in performance and data efficiency.

**Limitations:** 

**Conclusion:** The findings provide practical insights for constructing stable, personalised lexical profiles with minimal data requirements, supporting future lexical alignment strategies in human-agent dialogue.

**Abstract:** Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.

</details>


### [39] [A Comprehensive Survey on Trustworthiness in Reasoning with Large Language Models](https://arxiv.org/abs/2509.03871)

*Yanbo Wang, Yongcan Yu, Jian Liang, Ran He*

**Main category:** cs.CL

**Keywords:** Long-CoT reasoning, trustworthiness, Linguistic models, AI safety, Chain-of-Thought

**Relevance Score:** 9

**TL;DR:** This paper surveys Long-CoT reasoning in language models, focusing on its impact on trustworthiness across five dimensions: truthfulness, safety, robustness, fairness, and privacy. It analyzes methodologies and findings of recent studies and discusses future research directions.

**Read time:** 38 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive understanding of how Chain-of-Thought (CoT) reasoning affects the trustworthiness of language models, an area that is underdeveloped despite recent advancements.

**Method:** The paper surveys recent literature on reasoning models and CoT techniques, structuring analyses around trustworthiness dimensions and presenting findings in chronological order.

**Key Contributions:**

	1. Comprehensive survey on CoT reasoning techniques
	2. Structured analysis of trustworthiness in LLMs
	3. Identification of vulnerabilities in current reasoning models

**Result:** The survey highlights that while reasoning techniques can improve model trustworthiness and mitigate issues like hallucinations and harmful content, current reasoning models have vulnerabilities in safety, robustness, and privacy.

**Limitations:** Focuses on literature published up to June 30, 2025; ongoing developments in LLMs may not be covered.

**Conclusion:** This work serves as a resource for the AI safety community to stay updated on advancements in reasoning and trustworthiness, while also identifying key areas needing further research.

**Abstract:** The development of Long-CoT reasoning has advanced LLM performance across various tasks, including language understanding, complex problem solving, and code generation. This paradigm enables models to generate intermediate reasoning steps, thereby improving both accuracy and interpretability. However, despite these advancements, a comprehensive understanding of how CoT-based reasoning affects the trustworthiness of language models remains underdeveloped. In this paper, we survey recent work on reasoning models and CoT techniques, focusing on five core dimensions of trustworthy reasoning: truthfulness, safety, robustness, fairness, and privacy. For each aspect, we provide a clear and structured overview of recent studies in chronological order, along with detailed analyses of their methodologies, findings, and limitations. Future research directions are also appended at the end for reference and discussion. Overall, while reasoning techniques hold promise for enhancing model trustworthiness through hallucination mitigation, harmful content detection, and robustness improvement, cutting-edge reasoning models themselves often suffer from comparable or even greater vulnerabilities in safety, robustness, and privacy. By synthesizing these insights, we hope this work serves as a valuable and timely resource for the AI safety community to stay informed on the latest progress in reasoning trustworthiness. A full list of related papers can be found at \href{https://github.com/ybwang119/Awesome-reasoning-safety}{https://github.com/ybwang119/Awesome-reasoning-safety}.

</details>


### [40] [False Sense of Security: Why Probing-based Malicious Input Detection Fails to Generalize](https://arxiv.org/abs/2509.03888)

*Cheng Wang, Zeming Wei, Qin Liu, Muhao Chen*

**Main category:** cs.CL

**Keywords:** Large Language Models, safety detection, probing methods, machine learning, HCI

**Relevance Score:** 8

**TL;DR:** This paper critiques the effectiveness of probing-based safety detection methods for Large Language Models, revealing that they often identify superficial patterns rather than genuine semantic harmfulness.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address safety concerns arising from Large Language Models' compliance with harmful instructions by re-examining probing-based approaches.

**Method:** Conducted controlled experiments to investigate the learned patterns by probes and their performance using semantically cleaned datasets.

**Key Contributions:**

	1. Identification of superficial patterns learned by probing methods
	2. Critique of the current probing-based safety detection framework
	3. Recommendations for redesigning models and evaluation protocols

**Result:** Opposition of the hypothesis that probes can effectively discern harmful inputs, showing they learn superficial patterns instead of representative semantic harmfulness.

**Limitations:** The study is limited to specific probing methods and may not encompass all safety evaluation techniques in LLMs.

**Conclusion:** Current probing-based approaches provide a false sense of security; there is a need for redesigning models and evaluation methodologies to enhance safety.

**Abstract:** Large Language Models (LLMs) can comply with harmful instructions, raising serious safety concerns despite their impressive capabilities. Recent work has leveraged probing-based approaches to study the separability of malicious and benign inputs in LLMs' internal representations, and researchers have proposed using such probing methods for safety detection. We systematically re-examine this paradigm. Motivated by poor out-of-distribution performance, we hypothesize that probes learn superficial patterns rather than semantic harmfulness. Through controlled experiments, we confirm this hypothesis and identify the specific patterns learned: instructional patterns and trigger words. Our investigation follows a systematic approach, progressing from demonstrating comparable performance of simple n-gram methods, to controlled experiments with semantically cleaned datasets, to detailed analysis of pattern dependencies. These results reveal a false sense of security around current probing-based approaches and highlight the need to redesign both models and evaluation protocols, for which we provide further discussions in the hope of suggesting responsible further research in this direction. We have open-sourced the project at https://github.com/WangCheng0116/Why-Probe-Fails.

</details>


### [41] [MobileRAG: Enhancing Mobile Agent with Retrieval-Augmented Generation](https://arxiv.org/abs/2509.03891)

*Gowen Loo, Chang Liu, Qinghong Yin, Xiang Chen, Jiawei Chen, Jingyuan Zhang, Yu Tian*

**Main category:** cs.CL

**Keywords:** Mobile Agents, Retrieval-Augmented Generation, Human-Computer Interaction, Large Language Models, Mobile Computing

**Relevance Score:** 9

**TL;DR:** MobileRAG is a framework for mobile agents enhanced by Retrieval-Augmented Generation to improve task completion and handle complex queries.

**Read time:** 7 min

<details>
  <summary>Details</summary>

**Motivation:** Current LLM-based mobile agents struggle with task comprehension, environmental interaction, and lack memory capabilities, leading to errors and inefficiencies.

**Method:** The framework includes InterRAG, LocalRAG, and MemRAG, utilizing RAG to optimize query identification and task execution in complex mobile scenarios.

**Key Contributions:**

	1. Introduction of MobileRAG framework for mobile agents
	2. Enhanced performance through Retrieval-Augmented Generation
	3. Development of MobileRAG-Eval benchmark for complex task assessment

**Result:** MobileRAG demonstrated a 10.3% improvement over state-of-the-art methods on the MobileRAG-Eval benchmark with fewer operational steps.

**Limitations:** 

**Conclusion:** MobileRAG effectively handles real-world mobile tasks, showcasing its potential for improving mobile agent capabilities.

**Abstract:** Smartphones have become indispensable in people's daily lives, permeating nearly every aspect of modern society. With the continuous advancement of large language models (LLMs), numerous LLM-based mobile agents have emerged. These agents are capable of accurately parsing diverse user queries and automatically assisting users in completing complex or repetitive operations. However, current agents 1) heavily rely on the comprehension ability of LLMs, which can lead to errors caused by misoperations or omitted steps during tasks, 2) lack interaction with the external environment, often terminating tasks when an app cannot fulfill user queries, and 3) lack memory capabilities, requiring each instruction to reconstruct the interface and being unable to learn from and correct previous mistakes. To alleviate the above issues, we propose MobileRAG, a mobile agents framework enhanced by Retrieval-Augmented Generation (RAG), which includes InterRAG, LocalRAG, and MemRAG. It leverages RAG to more quickly and accurately identify user queries and accomplish complex and long-sequence mobile tasks. Additionally, to more comprehensively assess the performance of MobileRAG, we introduce MobileRAG-Eval, a more challenging benchmark characterized by numerous complex, real-world mobile tasks that require external knowledge assistance. Extensive experimental results on MobileRAG-Eval demonstrate that MobileRAG can easily handle real-world mobile tasks, achieving 10.3\% improvement over state-of-the-art methods with fewer operational steps. Our code is publicly available at: https://github.com/liuxiaojieOutOfWorld/MobileRAG_arxiv

</details>


### [42] [MTQA:Matrix of Thought for Enhanced Reasoning in Complex Question Answering](https://arxiv.org/abs/2509.03918)

*Fengxiao Tang, Yufeng Li, Zongzong Wu, Ming Zhao*

**Main category:** cs.CL

**Keywords:** Complex Question Answering, Large Language Models, Matrix of Thought

**Relevance Score:** 9

**TL;DR:** This paper introduces the Matrix of Thought (MoT), a novel LLM thought structure designed to enhance reasoning capabilities for Complex Question Answering (QA), outperforming existing methods.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the performance degradation of large language models (LLMs) in complex QA tasks due to inadequate reasoning abilities.

**Method:** The Matrix of Thought (MoT) employs a 'column-cell communication' mechanism that allows for multi-strategy and deep-level thinking, while also implementing a fact-correction mechanism using knowledge units from retrieved knowledge graph triples.

**Key Contributions:**

	1. Introduction of the Matrix of Thought (MoT) structure
	2. Enhanced reasoning capabilities for LLMs
	3. Development of an efficient and accurate QA framework (MTQA)

**Result:** The MTQA framework outperforms state-of-the-art methods on four datasets in F1 and EM scores, while achieving a reasoning time that is only 14.4% of baseline methods' time.

**Limitations:** 

**Conclusion:** MoT significantly improves LLM reasoning for complex QA tasks, demonstrating both efficiency and accuracy.

**Abstract:** Complex Question Answering (QA) is a fundamental and challenging task in NLP. While large language models (LLMs) exhibit impressive performance in QA, they suffer from significant performance degradation when facing complex and abstract QA tasks due to insufficient reasoning capabilities. Works such as Chain-of-Thought (CoT) and Tree-of-Thought (ToT) aim to enhance LLMs' reasoning abilities, but they face issues such as in-layer redundancy in tree structures and single paths in chain structures. Although some studies utilize Retrieval-Augmented Generation (RAG) methods to assist LLMs in reasoning, the challenge of effectively utilizing large amounts of information involving multiple entities and hops remains critical. To address this, we propose the Matrix of Thought (MoT), a novel and efficient LLM thought structure. MoT explores the problem in both horizontal and vertical dimensions through the "column-cell communication" mechanism, enabling LLMs to actively engage in multi-strategy and deep-level thinking, reducing redundancy within the column cells and enhancing reasoning capabilities. Furthermore, we develop a fact-correction mechanism by constructing knowledge units from retrieved knowledge graph triples and raw text to enhance the initial knowledge for LLM reasoning and correct erroneous answers. This leads to the development of an efficient and accurate QA framework (MTQA). Experimental results show that our framework outperforms state-of-the-art methods on four widely-used datasets in terms of F1 and EM scores, with reasoning time only 14.4\% of the baseline methods, demonstrating both its efficiency and accuracy. The code for this framework is available at https://github.com/lyfiter/mtqa.

</details>


### [43] [Decoding the Poetic Language of Emotion in Korean Modern Poetry: Insights from a Human-Labeled Dataset and AI Modeling](https://arxiv.org/abs/2509.03932)

*Iro Lim, Haein Ji, Byungjun Kim*

**Main category:** cs.CL

**Keywords:** Korean Poetry, Emotion Analysis, Natural Language Processing, Dataset, Machine Learning

**Relevance Score:** 4

**TL;DR:** Introduction of KPoEM, a dataset for emotion analysis in Korean poetry, enhancing emotion classification via a fine-tuned language model.

**Read time:** 30 min

<details>
  <summary>Details</summary>

**Motivation:** To address the gap in computational emotion analysis specifically in modern Korean poetry, which is often overlooked due to its unique linguistic and cultural characteristics.

**Method:** Development of a multi-label emotion dataset comprised of 7,662 entries, annotated with 44 fine-grained emotion categories and trained using a Korean language model through sequential fine-tuning.

**Key Contributions:**

	1. Creation of the KPoEM dataset with extensive annotation for emotion analysis in poetry
	2. Demonstration of the effectiveness of fine-tuning a Korean language model specifically for poetic texts
	3. Integration of computational methods with literary analysis for exploring poetry's emotional depth.

**Result:** The KPoEM model achieved a F1-micro score of 0.60, significantly outperforming previous models which scored 0.34 when trained on general corpora.

**Limitations:** The dataset is specific to Korean poetry and may not directly apply to other languages or poetic traditions.

**Conclusion:** KPoEM not only improves the classification of emotions in Korean poetry but also preserves its cultural sentiment, paving the way for quantitative emotional analysis in literature.

**Abstract:** This study introduces KPoEM (Korean Poetry Emotion Mapping) , a novel dataset for computational emotion analysis in modern Korean poetry. Despite remarkable progress in text-based emotion classification using large language models, poetry-particularly Korean poetry-remains underexplored due to its figurative language and cultural specificity. We built a multi-label emotion dataset of 7,662 entries, including 7,007 line-level entries from 483 poems and 615 work-level entries, annotated with 44 fine-grained emotion categories from five influential Korean poets. A state-of-the-art Korean language model fine-tuned on this dataset significantly outperformed previous models, achieving 0.60 F1-micro compared to 0.34 from models trained on general corpora. The KPoEM model, trained through sequential fine-tuning-first on general corpora and then on the KPoEM dataset-demonstrates not only an enhanced ability to identify temporally and culturally specific emotional expressions, but also a strong capacity to preserve the core sentiments of modern Korean poetry. This study bridges computational methods and literary analysis, presenting new possibilities for the quantitative exploration of poetic emotions through structured data that faithfully retains the emotional and cultural nuances of Korean literature.

</details>


### [44] [SelfAug: Mitigating Catastrophic Forgetting in Retrieval-Augmented Generation via Distribution Self-Alignment](https://arxiv.org/abs/2509.03934)

*Yuqing Huang, Rongyang Zhang, Qimeng Wang, Chengqiang Lu, Yan Gao, Yi Wu, Yao Hu, Xuyang Zhi, Guiquan Liu, Xin Li, Hao Wang, Enhong Chen*

**Main category:** cs.CL

**Keywords:** large language models, catastrophic forgetting, Retrieval-Augmented Generation, semantic distribution, fine-tuning

**Relevance Score:** 9

**TL;DR:** This paper introduces SelfAug, a method designed to mitigate catastrophic forgetting in fine-tuned large language models by aligning input sequence logits to preserve model semantic distribution.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address catastrophic forgetting in large language models during supervised fine-tuning, particularly in Retrieval-Augmented Generation contexts, where models often lose previously acquired knowledge.

**Method:** SelfAug aligns input sequence logits to the model's semantic distribution, aiming to retain general capabilities while improving task-specific performance during fine-tuning.

**Key Contributions:**

	1. Introduction of the SelfAug method for semantic distribution alignment
	2. Empirical analysis linking distribution shifts to catastrophic forgetting severity
	3. Demonstration of improved performance in task-specific learning without significant knowledge loss.

**Result:** Experiments show that SelfAug achieves better retention of general capabilities and task performance compared to existing solutions, demonstrating a correlation between distribution shifts and catastrophic forgetting severity in RAG scenarios.

**Limitations:** 

**Conclusion:** The findings reveal critical insights into catastrophic forgetting in RAG contexts and offer a practical alignment solution for diverse fine-tuning situations.

**Abstract:** Recent advancements in large language models (LLMs) have revolutionized natural language processing through their remarkable capabilities in understanding and executing diverse tasks. While supervised fine-tuning, particularly in Retrieval-Augmented Generation (RAG) scenarios, effectively enhances task-specific performance, it often leads to catastrophic forgetting, where models lose their previously acquired knowledge and general capabilities. Existing solutions either require access to general instruction data or face limitations in preserving the model's original distribution. To overcome these limitations, we propose SelfAug, a self-distribution alignment method that aligns input sequence logits to preserve the model's semantic distribution, thereby mitigating catastrophic forgetting and improving downstream performance. Extensive experiments demonstrate that SelfAug achieves a superior balance between downstream learning and general capability retention. Our comprehensive empirical analysis reveals a direct correlation between distribution shifts and the severity of catastrophic forgetting in RAG scenarios, highlighting how the absence of RAG capabilities in general instruction tuning leads to significant distribution shifts during fine-tuning. Our findings not only advance the understanding of catastrophic forgetting in RAG contexts but also provide a practical solution applicable across diverse fine-tuning scenarios. Our code is publicly available at https://github.com/USTC-StarTeam/SelfAug.

</details>


### [45] [SPFT-SQL: Enhancing Large Language Model for Text-to-SQL Parsing by Self-Play Fine-Tuning](https://arxiv.org/abs/2509.03937)

*Yuhao Zhang, Shaoming Duan, Jinhang Su, Chuanyi Liu, Peiyi Han*

**Main category:** cs.CL

**Keywords:** self-play fine-tuning, Text-to-SQL, large language models, error-driven loss, iterative fine-tuning

**Relevance Score:** 6

**TL;DR:** This paper introduces SPFT-SQL, a new self-play fine-tuning method for improving large language models' performance in the Text-to-SQL task by leveraging verification-based iterative fine-tuning and an error-driven loss method.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The challenge in the Text-to-SQL task where existing self-play methods like SPIN struggle due to the generated SQL queries from opponent models reducing accuracy.

**Method:** A verification-based iterative fine-tuning approach synthesizes high-quality fine-tuning data before self-play, followed by an error-driven loss method to distinguish between correct and incorrect SQL outputs.

**Key Contributions:**

	1. Introduction of SPFT-SQL for Text-to-SQL fine-tuning
	2. Verification-based iterative fine-tuning for high-quality data generation
	3. Error-driven loss method for better output discrimination

**Result:** SPFT-SQL demonstrates improved performance over existing state-of-the-art methods on six open-source LLMs and five benchmarks.

**Limitations:** 

**Conclusion:** The proposed SPFT-SQL method significantly enhances the ability of LLMs to generate accurate SQL queries and addresses limitations in previous self-play methods.

**Abstract:** Despite the significant advancements of self-play fine-tuning (SPIN), which can transform a weak large language model (LLM) into a strong one through competitive interactions between models of varying capabilities, it still faces challenges in the Text-to-SQL task. SPIN does not generate new information, and the large number of correct SQL queries produced by the opponent model during self-play reduces the main model's ability to generate accurate SQL queries. To address this challenge, we propose a new self-play fine-tuning method tailored for the Text-to-SQL task, called SPFT-SQL. Prior to self-play, we introduce a verification-based iterative fine-tuning approach, which synthesizes high-quality fine-tuning data iteratively based on the database schema and validation feedback to enhance model performance, while building a model base with varying capabilities. During the self-play fine-tuning phase, we propose an error-driven loss method that incentivizes incorrect outputs from the opponent model, enabling the main model to distinguish between correct SQL and erroneous SQL generated by the opponent model, thereby improving its ability to generate correct SQL. Extensive experiments and in-depth analyses on six open-source LLMs and five widely used benchmarks demonstrate that our approach outperforms existing state-of-the-art (SOTA) methods.

</details>


### [46] [VoxRole: A Comprehensive Benchmark for Evaluating Speech-Based Role-Playing Agents](https://arxiv.org/abs/2509.03940)

*Weihao Wu, Liang Cao, Xinyu Wu, Zhiwei Lin, Rui Niu, Jingbei Li, Zhiyong Wu*

**Main category:** cs.CL

**Keywords:** Role-Playing Conversational Agents, VoxRole, Speech-based evaluation

**Relevance Score:** 9

**TL;DR:** This paper introduces VoxRole, a benchmark for evaluating speech-based Role-Playing Conversational Agents (RPCAs), addressing the lack of focus on paralinguistic features and standardized evaluation methods in current research.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To fill the gaps in evaluating speech-based RPCAs concerning paralinguistic features and to provide standardized benchmarks for assessing model performance.

**Method:** A novel two-stage automated pipeline was developed to align movie audio with scripts and use an LLM to create multi-dimensional profiles for 1228 unique characters across 261 movies, resulting in 13,335 multi-turn dialogues totaling 65.6 hours of speech.

**Key Contributions:**

	1. Introduction of VoxRole benchmark for RPCAs
	2. Focus on paralinguistic features in evaluation
	3. Multi-dimensional character profiling for improved assessment

**Result:** Conducted a multi-dimensional evaluation of contemporary spoken dialogue models using VoxRole, revealing insights into strengths and limitations regarding persona consistency.

**Limitations:** 

**Conclusion:** VoxRole serves as a substantial resource for assessing the performance of RPCAs, emphasizing the importance of incorporating paralinguistic features in dialogue systems.

**Abstract:** Recent significant advancements in Large Language Models (LLMs) have greatly propelled the development of Role-Playing Conversational Agents (RPCAs). These systems aim to create immersive user experiences through consistent persona adoption. However, current RPCA research faces dual limitations. First, existing work predominantly focuses on the textual modality, entirely overlooking critical paralinguistic features including intonation, prosody, and rhythm in speech, which are essential for conveying character emotions and shaping vivid identities. Second, the speech-based role-playing domain suffers from a long-standing lack of standardized evaluation benchmarks. Most current spoken dialogue datasets target only fundamental capability assessments, featuring thinly sketched or ill-defined character profiles. Consequently, they fail to effectively quantify model performance on core competencies like long-term persona consistency. To address this critical gap, we introduce VoxRole, the first comprehensive benchmark specifically designed for the evaluation of speech-based RPCAs. The benchmark comprises 13335 multi-turn dialogues, totaling 65.6 hours of speech from 1228 unique characters across 261 movies. To construct this resource, we propose a novel two-stage automated pipeline that first aligns movie audio with scripts and subsequently employs an LLM to systematically build multi-dimensional profiles for each character. Leveraging VoxRole, we conduct a multi-dimensional evaluation of contemporary spoken dialogue models, revealing crucial insights into their respective strengths and limitations in maintaining persona consistency.

</details>


### [47] [CANDY: Benchmarking LLMs' Limitations and Assistive Potential in Chinese Misinformation Fact-Checking](https://arxiv.org/abs/2509.03957)

*Ruiling Guo, Xinwei Yang, Chen Huang, Tong Zhang, Yong Hu*

**Main category:** cs.CL

**Keywords:** Large Language Models, Fact-Checking, Misinformation, Chinese Language, Human-AI Collaboration

**Relevance Score:** 9

**TL;DR:** This paper introduces CANDY, a benchmark for evaluating large language models in fact-checking misinformation in Chinese, revealing significant limitations in their reliability.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess the effectiveness of LLMs in fact-checking misinformation, particularly in the context of the Chinese language, given the uncertainty surrounding their capabilities.

**Method:** We curated a dataset of approximately 20,000 instances for systematic evaluation and developed a taxonomy to categorize flawed explanations from LLMs.

**Key Contributions:**

	1. Introduction of CANDY benchmark for LLM evaluation in fact-checking
	2. Curated dataset of ~20k instances focused on Chinese misinformation
	3. Taxonomy of LLM-generated explanation failures, highlighting factual fabrication

**Result:** Current LLMs struggle with generating accurate fact-checking conclusions, with factual fabrication being the most common failure mode.

**Limitations:** LLMs alone are unreliable for factual accuracy in fact-checking.

**Conclusion:** Although LLMs are not reliable for standalone fact-checking, they can significantly aid human performance when used as assistive tools.

**Abstract:** The effectiveness of large language models (LLMs) to fact-check misinformation remains uncertain, despite their growing use. To this end, we present CANDY, a benchmark designed to systematically evaluate the capabilities and limitations of LLMs in fact-checking Chinese misinformation. Specifically, we curate a carefully annotated dataset of ~20k instances. Our analysis shows that current LLMs exhibit limitations in generating accurate fact-checking conclusions, even when enhanced with chain-of-thought reasoning and few-shot prompting. To understand these limitations, we develop a taxonomy to categorize flawed LLM-generated explanations for their conclusions and identify factual fabrication as the most common failure mode. Although LLMs alone are unreliable for fact-checking, our findings indicate their considerable potential to augment human performance when deployed as assistive tools in scenarios. Our dataset and code can be accessed at https://github.com/SCUNLP/CANDY

</details>


### [48] [Exploring NLP Benchmarks in an Extremely Low-Resource Setting](https://arxiv.org/abs/2509.03962)

*Ulin Nuha, Adam Jatowt*

**Main category:** cs.CL

**Keywords:** Ladin, Low-resource languages, Sentiment analysis, Natural language processing, Machine translation

**Relevance Score:** 7

**TL;DR:** This paper develops synthetic NLP datasets for the endangered Ladin language through innovative translation of existing monolingual data, improving sentiment analysis and question answering capabilities.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address the lack of high-quality NLP datasets for low-resource languages, particularly the endangered Ladin language, and to enhance language technology solutions for such languages.

**Method:** The authors create synthetic datasets for sentiment analysis and multiple-choice question answering using a small set of parallel Ladin-Italian sentence pairs, applying rigorous filtering and back-translation to ensure quality.

**Key Contributions:**

	1. First sentiment analysis dataset for Ladin
	2. First multiple-choice question answering dataset for Ladin
	3. Improvement of translation models using synthetic datasets

**Result:** The incorporation of these synthetic datasets into machine translation training significantly improves the performance of Italian-Ladin translation systems.

**Limitations:** 

**Conclusion:** The paper presents the first publicly available sentiment analysis and MCQA datasets for Ladin, fostering further NLP research and applications for underrepresented languages.

**Abstract:** The effectiveness of Large Language Models (LLMs) diminishes for extremely low-resource languages, such as indigenous languages, primarily due to the lack of labeled data. Despite growing interest, the availability of high-quality natural language processing (NLP) datasets for these languages remains limited, making it difficult to develop robust language technologies. This paper addresses such gap by focusing on Ladin, an endangered Romance language, specifically targeting the Val Badia variant. Leveraging a small set of parallel Ladin-Italian sentence pairs, we create synthetic datasets for sentiment analysis and multiple-choice question answering (MCQA) by translating monolingual Italian data. To ensure linguistic quality and reliability, we apply rigorous filtering and back-translation procedures in our method. We further demonstrate that incorporating these synthetic datasets into machine translation training leads to substantial improvements over existing Italian-Ladin translation baselines. Our contributions include the first publicly available sentiment analysis and MCQA datasets for Ladin, establishing foundational resources that can support broader NLP research and downstream applications for this underrepresented language.

</details>


### [49] [Expanding Foundational Language Capabilities in Open-Source LLMs through a Korean Case Study](https://arxiv.org/abs/2509.03972)

*Junghwan Lim, Gangwon Jo, Sungmin Lee, Jiyoung Park, Dongseok Kim, Jihwan Kim, Junhyeok Lee, Wai Ting Cheung, Dahye Choi, Kibong Choi, Jaeyeon Huh, Beomgyu Kim, Jangwoong Kim, Taehyun Kim, Haesol Lee, Jeesoo Lee, Dongpin Oh, Changseok Song, Daewon Suh*

**Main category:** cs.CL

**Keywords:** Llama-3-Motif, language model, Korean language, GPT-4, machine learning

**Relevance Score:** 6

**TL;DR:** Llama-3-Motif is a 102 billion parameter language model designed to enhance Korean while retaining English capabilities, utilizing advanced training techniques.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To improve the performance of language models specifically for the Korean language while maintaining strong capabilities in English.

**Method:** Developed on the Llama 3 architecture, employing LlamaPro and Masked Structure Growth for efficient scaling without altering the Transformer base.

**Key Contributions:**

	1. Introduction of Llama-3-Motif with 102 billion parameters
	2. Advanced training techniques like LlamaPro and Masked Structure Growth
	3. Benchmark achievements comparable to GPT-4 in Korean

**Result:** Llama-3-Motif outperforms existing models on Korean benchmarks, showing comparable results to GPT-4.

**Limitations:** 

**Conclusion:** This model addresses the balance of Korean and English language processing in a single architecture.

**Abstract:** We introduce Llama-3-Motif, a language model consisting of 102 billion parameters, specifically designed to enhance Korean capabilities while retaining strong performance in English. Developed on the Llama 3 architecture, Llama-3-Motif employs advanced training techniques, including LlamaPro and Masked Structure Growth, to effectively scale the model without altering its core Transformer architecture. Using the MoAI platform for efficient training across hyperscale GPU clusters, we optimized Llama-3-Motif using a carefully curated dataset that maintains a balanced ratio of Korean and English data. Llama-3-Motif shows decent performance on Korean-specific benchmarks, outperforming existing models and achieving results comparable to GPT-4.

</details>


### [50] [RTQA : Recursive Thinking for Complex Temporal Knowledge Graph Question Answering with Large Language Models](https://arxiv.org/abs/2509.03995)

*Zhaoyan Gong, Juan Li, Zhiqiang Liu, Lei Liang, Huajun Chen, Wen Zhang*

**Main category:** cs.CL

**Keywords:** temporal knowledge graph, question answering, reasoning, LLMs, fault tolerance

**Relevance Score:** 6

**TL;DR:** RTQA is a new framework for temporal knowledge graph question answering that improves reasoning and fault tolerance without requiring training.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** Current TKGQA methods struggle with complex temporal queries and reasoning abilities, leading to error propagation in existing frameworks.

**Method:** RTQA uses a recursive approach to decompose questions into sub-problems, applies LLMs and TKG knowledge for solutions, and aggregates answers through multi-path methods.

**Key Contributions:**

	1. Introduces a novel RTQA framework for TKGQA
	2. Utilizes recursive thinking for question decomposition
	3. Implements multi-path answer aggregation to improve fault tolerance.

**Result:** Experiments show significant improvements in Hits@1 for 'Multiple' and 'Complex' categories on the MultiTQ and TimelineKGQA benchmarks.

**Limitations:** 

**Conclusion:** RTQA demonstrates a robust improvement over state-of-the-art TKGQA methods with enhanced reasoning capabilities and fault tolerance.

**Abstract:** Current temporal knowledge graph question answering (TKGQA) methods primarily focus on implicit temporal constraints, lacking the capability of handling more complex temporal queries, and struggle with limited reasoning abilities and error propagation in decomposition frameworks. We propose RTQA, a novel framework to address these challenges by enhancing reasoning over TKGs without requiring training. Following recursive thinking, RTQA recursively decomposes questions into sub-problems, solves them bottom-up using LLMs and TKG knowledge, and employs multi-path answer aggregation to improve fault tolerance. RTQA consists of three core components: the Temporal Question Decomposer, the Recursive Solver, and the Answer Aggregator. Experiments on MultiTQ and TimelineKGQA benchmarks demonstrate significant Hits@1 improvements in "Multiple" and "Complex" categories, outperforming state-of-the-art methods. Our code and data are available at https://github.com/zjukg/RTQA.

</details>


### [51] [On Robustness and Reliability of Benchmark-Based Evaluation of LLMs](https://arxiv.org/abs/2509.04013)

*Riccardo Lunardi, Vincenzo Della Mea, Stefano Mizzaro, Kevin Roitero*

**Main category:** cs.CL

**Keywords:** Large Language Models, benchmarks, robustness, paraphrasing, evaluation methodologies

**Relevance Score:** 8

**TL;DR:** This study investigates the robustness of large language models (LLMs) to paraphrased benchmark questions, revealing a decline in effectiveness while maintaining stable rankings.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To assess whether benchmark evaluations adequately reflect the real-world performance of LLMs across diverse question variations.

**Method:** A systematic generation of paraphrased questions from six standard benchmarks to test the effectiveness of 34 different LLMs.

**Key Contributions:**

	1. Assessment of LLM robustness to question paraphrasing.
	2. Insights into the stability of LLM rankings despite performance drops.
	3. Recommendations for developing robustness-aware evaluation methodologies.

**Result:** LLM rankings remain stable across paraphrased questions, but their absolute effectiveness scores significantly decline, indicating challenges with linguistic variability.

**Limitations:** Focuses only on specific benchmarks and LLM sizes; the study does not explore the reasons for performance variability in depth.

**Conclusion:** The study highlights the limitations of benchmark-based evaluations in capturing LLM robustness, suggesting the need for more effective and robust benchmarks.

**Abstract:** Large Language Models (LLMs) effectiveness is usually evaluated by means of benchmarks such as MMLU, ARC-C, or HellaSwag, where questions are presented in their original wording, thus in a fixed, standardized format. However, real-world applications involve linguistic variability, requiring models to maintain their effectiveness across diverse rewordings of the same question or query. In this study, we systematically assess the robustness of LLMs to paraphrased benchmark questions and investigate whether benchmark-based evaluations provide a reliable measure of model capabilities. We systematically generate various paraphrases of all the questions across six different common benchmarks, and measure the resulting variations in effectiveness of 34 state-of-the-art LLMs, of different size and effectiveness. Our findings reveal that while LLM rankings remain relatively stable across paraphrased inputs, absolute effectiveness scores change, and decline significantly. This suggests that LLMs struggle with linguistic variability, raising concerns about their generalization abilities and evaluation methodologies. Furthermore, the observed performance drop challenges the reliability of benchmark-based evaluations, indicating that high benchmark scores may not fully capture a model's robustness to real-world input variations. We discuss the implications of these findings for LLM evaluation methodologies, emphasizing the need for robustness-aware benchmarks that better reflect practical deployment scenarios.

</details>


### [52] [What if I ask in \textit{alia lingua}? Measuring Functional Similarity Across Languages](https://arxiv.org/abs/2509.04032)

*Debangan Mishra, Arihant Rastogi, Agyeya Negi, Shashwat Goel, Ponnurangam Kumaraguru*

**Main category:** cs.CL

**Keywords:** multilingual models, model consistency, NLU evaluation, cross-lingual output, model reliability

**Relevance Score:** 7

**TL;DR:** This work investigates model output consistency across 20 languages and 47 subjects, revealing increased consistency with larger and more capable models.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** To understand how similarity in model outputs varies by language and evaluate multilingual reliability.

**Method:** The study utilizes a metric called $A_p$ to analyze model outputs across various languages and subjects.

**Key Contributions:**

	1. Introduction of the $A_p$ metric for measuring model output similarity across languages.
	2. Empirical analysis showing the correlation between model size and cross-lingual consistency.
	3. Insights into models' internal consistency versus inter-model agreement.

**Result:** Findings indicate that larger models demonstrate increased consistency in their output across languages as well as increased internal consistency compared to outputs from different models.

**Limitations:** The analysis is confined to 20 languages and 47 subjects; broader applicability beyond this scope is not established.

**Conclusion:** The results support the use of $A_p$ for evaluating multilingual model reliability and suggest a pathway to developing more consistent multilingual systems.

**Abstract:** How similar are model outputs across languages? In this work, we study this question using a recently proposed model similarity metric $\kappa_p$ applied to 20 languages and 47 subjects in GlobalMMLU. Our analysis reveals that a model's responses become increasingly consistent across languages as its size and capability grow. Interestingly, models exhibit greater cross-lingual consistency within themselves than agreement with other models prompted in the same language. These results highlight not only the value of $\kappa_p$ as a practical tool for evaluating multilingual reliability, but also its potential to guide the development of more consistent multilingual systems.

</details>


### [53] [A RoBERTa-Based Functional Syntax Annotation Model for Chinese Texts](https://arxiv.org/abs/2509.04046)

*Han Xiaohui, Zhang Yunlong, Guo Yuxi*

**Main category:** cs.CL

**Keywords:** Systemic Functional Grammar, RoBERTa, Chinese texts, named entity recognition, functional syntax

**Relevance Score:** 4

**TL;DR:** This research presents an automatic functional syntax annotation model for Chinese texts using RoBERTa, achieving strong results in named entity recognition.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an automatic annotation system for Chinese texts based on Systemic Functional Grammar, which has not been previously established.

**Method:** The study selected 4,100 sentences from the People's Daily 2014 corpus, annotated them according to functional syntax theory, and fine-tuned the RoBERTa-Chinese model for the named entity recognition task.

**Key Contributions:**

	1. Introduction of a functional syntax annotation model for Chinese using RoBERTa
	2. Creation of a dataset for functional syntax training based on Chinese texts
	3. Demonstration of improved performance in named entity recognition tasks.

**Result:** The model achieved an F1 score of 0.852 on the test set, significantly outperforming other models, especially in identifying core syntactic elements.

**Limitations:** There is still room for improvement in recognizing entities with imbalanced label samples.

**Conclusion:** This research innovatively integrates functional syntax with attention-based NLP models, providing a new method for automated Chinese functional syntax analysis.

**Abstract:** Systemic Functional Grammar and its branch, Cardiff Grammar, have been widely applied to discourse analysis, semantic function research, and other tasks across various languages and texts. However, an automatic annotation system based on this theory for Chinese texts has not yet been developed, which significantly constrains the application and promotion of relevant theories. To fill this gap, this research introduces a functional syntax annotation model for Chinese based on RoBERTa (Robustly Optimized BERT Pretraining Approach). The study randomly selected 4,100 sentences from the People's Daily 2014 corpus and annotated them according to functional syntax theory to establish a dataset for training. The study then fine-tuned the RoBERTa-Chinese wwm-ext model based on the dataset to implement the named entity recognition task, achieving an F1 score of 0.852 on the test set that significantly outperforms other comparative models. The model demonstrated excellent performance in identifying core syntactic elements such as Subject (S), Main Verb (M), and Complement (C). Nevertheless, there remains room for improvement in recognizing entities with imbalanced label samples. As the first integration of functional syntax with attention-based NLP models, this research provides a new method for automated Chinese functional syntax analysis and lays a solid foundation for subsequent studies.

</details>


### [54] [Synthesizing Sheet Music Problems for Evaluation and Reinforcement Learning](https://arxiv.org/abs/2509.04059)

*Zhilin Wang, Zhe Yang, Yun Luo, Yafu Li, Haoran Zhang, Runzhe Zhan, Derek F. Wong, Jizhe Zhou, Yu Cheng*

**Main category:** cs.CL

**Keywords:** Sheet Music Reasoning, Large Language Models, Music Theory, Reinforcement Learning, AI Music Composition

**Relevance Score:** 8

**TL;DR:** This paper introduces a framework for synthesizing sheet music problems to enhance reasoning in Large and Multimodal Language Models, resulting in better performance in music understanding and composition.

**Read time:** 11 min

<details>
  <summary>Details</summary>

**Motivation:** The lack of evaluation benchmarks and training data for sheet music reasoning limits the ability of LLMs and MLLMs in music interpretation and composition.

**Method:** A data synthesis framework is proposed to generate verifiable sheet music questions in both textual and visual modalities, leading to the creation of the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a training set for reinforcement learning with verifiable rewards (RLVR).

**Key Contributions:**

	1. Development of the SSMR-Bench and training set for RLVR.
	2. Demonstration of enhanced reasoning in LLMs for music interpretation.
	3. Introduction of music theory-based question synthesis for AI-driven music composition.

**Result:** Evaluation results show improved reasoning abilities in LLMs with synthesized data, with Qwen3-8B-Base surpassing GPT-4 in performances and aiding in music composition.

**Limitations:** Limited evaluation on real-world music interpretation challenges and reliance on synthetic data.

**Conclusion:** The study pioneers the synthesis of sheet music problems based on music theory, advancing model reasoning and facilitating AI-assisted music creation.

**Abstract:** Enhancing the ability of Large Language Models (LLMs) and Multimodal Large Language Models (MLLMs) to interpret sheet music is a crucial step toward building AI musicians. However, current research lacks both evaluation benchmarks and training data for sheet music reasoning. To address this, we propose the idea of synthesizing sheet music problems grounded in music theory, which can serve both as evaluation benchmarks and as training data for reinforcement learning with verifiable rewards (RLVR). We introduce a data synthesis framework that generates verifiable sheet music questions in both textual and visual modalities, leading to the Synthetic Sheet Music Reasoning Benchmark (SSMR-Bench) and a complementary training set. Evaluation results on SSMR-Bench show the importance of models' reasoning abilities in interpreting sheet music. At the same time, the poor performance of Gemini 2.5-Pro highlights the challenges that MLLMs still face in interpreting sheet music in a visual format. By leveraging synthetic data for RLVR, Qwen3-8B-Base and Qwen2.5-VL-Instruct achieve improvements on the SSMR-Bench. Besides, the trained Qwen3-8B-Base surpasses GPT-4 in overall performance on MusicTheoryBench and achieves reasoning performance comparable to GPT-4 with the strategies of Role play and Chain-of-Thought. Notably, its performance on math problems also improves relative to the original Qwen3-8B-Base. Furthermore, our results show that the enhanced reasoning ability can also facilitate music composition. In conclusion, we are the first to propose the idea of synthesizing sheet music problems based on music theory rules, and demonstrate its effectiveness not only in advancing model reasoning for sheet music understanding but also in unlocking new possibilities for AI-assisted music creation.

</details>


### [55] [Arabic Chatbot Technologies in Education: An Overview](https://arxiv.org/abs/2509.04066)

*Hicham Bourhil, Yacine El Younoussi*

**Main category:** cs.CL

**Keywords:** Arabic chatbots, education, Natural Language Processing, Large Language Models, Digital technologies

**Relevance Score:** 6

**TL;DR:** This study surveys existing Arabic chatbots in education, highlighting their characteristics and identifying research gaps.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To explore the implementation of Arabic chatbots in education, especially post-COVID-19, amidst the rise of AI and NLP technologies.

**Method:** A survey of existing educational Arabic chatbots, analyzing their approaches, language variety, and performance metrics.

**Key Contributions:**

	1. Survey of educational Arabic chatbots
	2. Identification of research gaps
	3. Discussion on future research directions

**Result:** The study identifies that, compared to English chatbots, there are significantly fewer educational chatbots in Arabic employing modern techniques.

**Limitations:** 

**Conclusion:** Future research should focus on bridging the identified gaps to enhance the effectiveness of Arabic chatbots in education.

**Abstract:** The recent advancements in Artificial Intelligence (AI) in general, and in Natural Language Processing (NLP) in particular, and some of its applications such as chatbots, have led to their implementation in different domains like education, healthcare, tourism, and customer service. Since the COVID-19 pandemic, there has been an increasing interest in these digital technologies to allow and enhance remote access. In education, e-learning systems have been massively adopted worldwide. The emergence of Large Language Models (LLM) such as BERT (Bidirectional Encoder Representations from Transformers) and GPT (Generative Pre-trained Transformers) made chatbots even more popular. In this study, we present a survey on existing Arabic chatbots in education and their different characteristics such as the adopted approaches, language variety, and metrics used to measure their performance. We were able to identified some research gaps when we discovered that, despite the success of chatbots in other languages such as English, only a few educational Arabic chatbots used modern techniques. Finally, we discuss future directions of research in this field.

</details>


### [56] [Improving Narrative Classification and Explanation via Fine Tuned Language Models](https://arxiv.org/abs/2509.04077)

*Rishit Tyagi, Rahul Bouri, Mohit Gupta*

**Main category:** cs.CL

**Keywords:** BERT, narrative detection, ReACT framework, few-shot prompting, semantic retrieval

**Relevance Score:** 8

**TL;DR:** This study addresses the challenges of detecting covert narratives in news articles and generating explanations for them by fine-tuning a BERT model and proposing a ReACT framework.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Understanding covert narratives and implicit messaging is essential for analyzing bias and sentiment.

**Method:** We fine-tune a BERT model with a recall-oriented approach for comprehensive narrative detection and refine predictions using a GPT-4o pipeline. For narrative explanation, we propose a ReACT framework with semantic retrieval-based few-shot prompting, incorporating a structured taxonomy as an auxiliary knowledge base.

**Key Contributions:**

	1. Multi-label classification of narratives and sub-narratives
	2. Development of a ReACT framework for evidence-based explanations
	3. Integration of auxiliary knowledge to enhance model performance

**Result:** Integrating auxiliary knowledge in prompts improves classification accuracy and justification reliability, demonstrating significant advancements in narrative detection and explanation.

**Limitations:** The effectiveness may be dependent on the quality of the auxiliary knowledge and the complexity of narratives.

**Conclusion:** The study shows promising applications in media analysis, education, and intelligence gathering by enhancing factual accuracy and reducing hallucinations in narrative explanations.

**Abstract:** Understanding covert narratives and implicit messaging is essential for analyzing bias and sentiment. Traditional NLP methods struggle with detecting subtle phrasing and hidden agendas. This study tackles two key challenges: (1) multi-label classification of narratives and sub-narratives in news articles, and (2) generating concise, evidence-based explanations for dominant narratives. We fine-tune a BERT model with a recall-oriented approach for comprehensive narrative detection, refining predictions using a GPT-4o pipeline for consistency. For narrative explanation, we propose a ReACT (Reasoning + Acting) framework with semantic retrieval-based few-shot prompting, ensuring grounded and relevant justifications. To enhance factual accuracy and reduce hallucinations, we incorporate a structured taxonomy table as an auxiliary knowledge base. Our results show that integrating auxiliary knowledge in prompts improves classification accuracy and justification reliability, with applications in media analysis, education, and intelligence gathering.

</details>


### [57] [Towards Stable and Personalised Profiles for Lexical Alignment in Spoken Human-Agent Dialogue](https://arxiv.org/abs/2509.04104)

*Keara Schaaij, Roel Boumans, Tibor Bosse, Iris Hendrickx*

**Main category:** cs.CL

**Keywords:** lexical alignment, conversational agents, large language models

**Relevance Score:** 8

**TL;DR:** This study explores lexical alignment in conversational agents by constructing personalised lexical profiles to enhance communication efficiency.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To address the underexplored area of lexical alignment in conversational agents, especially with advancements in large language models.

**Method:** The study varied the transcribed spoken data used and the number of items in lexical profiles for different part-of-speech categories, evaluating performance using recall, coverage, and cosine similarity.

**Key Contributions:**

	1. Introduction of personalised lexical profiles in conversational agents
	2. Identification of optimal data requirements for profile construction
	3. Application of performance metrics to evaluate profile effectiveness

**Result:** Compact profiles created from 10 minutes of transcribed speech with specific item counts achieved the best performance balance.

**Limitations:** 

**Conclusion:** The research provides foundational insights into creating stable, personalised lexical profiles for conversational agents with minimal data.

**Abstract:** Lexical alignment, where speakers start to use similar words across conversation, is known to contribute to successful communication. However, its implementation in conversational agents remains underexplored, particularly considering the recent advancements in large language models (LLMs). As a first step towards enabling lexical alignment in human-agent dialogue, this study draws on strategies for personalising conversational agents and investigates the construction of stable, personalised lexical profiles as a basis for lexical alignment. Specifically, we varied the amounts of transcribed spoken data used for construction as well as the number of items included in the profiles per part-of-speech (POS) category and evaluated profile performance across time using recall, coverage, and cosine similarity metrics. It was shown that smaller and more compact profiles, created after 10 min of transcribed speech containing 5 items for adjectives, 5 items for conjunctions, and 10 items for adverbs, nouns, pronouns, and verbs each, offered the best balance in both performance and data efficiency. In conclusion, this study offers practical insights into constructing stable, personalised lexical profiles, taking into account minimal data requirements, serving as a foundational step toward lexical alignment strategies in conversational agents.

</details>


### [58] [MultiWikiQA: A Reading Comprehension Benchmark in 300+ Languages](https://arxiv.org/abs/2509.04111)

*Dan Saattrup Smart*

**Main category:** cs.CL

**Keywords:** MultiWikiQA, reading comprehension, language models, multilingual, Wikipedia

**Relevance Score:** 7

**TL;DR:** A new reading comprehension dataset, MultiWikiQA, created using Wikipedia articles across 306 languages, with crowdsourced evaluations on question quality.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide a comprehensive reading comprehension benchmark that spans multiple languages and evaluates the performance of different language models.

**Method:** The study presents a reading comprehension dataset sourced from Wikipedia, involving questions generated by a language model and answers found in the articles, alongside a human evaluation of question fluency.

**Key Contributions:**

	1. Introduction of MultiWikiQA, a multilingual reading comprehension dataset.
	2. Crowdsourced evaluation of question quality in multiple languages.
	3. Comparison of different language model performances across 306 languages.

**Result:** The evaluation shows that the questions are of high quality across 30 languages, and the performance of language models varies significantly depending on the language used.

**Limitations:** Performance discrepancies indicate variability in language model effectiveness based on the language, suggesting not all languages are represented equally in terms of comprehension complexity.

**Conclusion:** The MultiWikiQA dataset serves as a challenging benchmark for reading comprehension across multiple languages and is freely accessible for future research.

**Abstract:** We introduce a new reading comprehension dataset, dubbed MultiWikiQA, which covers 306 languages. The context data comes from Wikipedia articles, with questions generated by an LLM and the answers appearing verbatim in the Wikipedia articles. We conduct a crowdsourced human evaluation of the fluency of the generated questions across 30 of the languages, providing evidence that the questions are of good quality. We evaluate 6 different language models, both decoder and encoder models of varying sizes, showing that the benchmark is sufficiently difficult and that there is a large performance discrepancy amongst the languages. The dataset and survey evaluations are freely available.

</details>


### [59] [Joint Modeling of Entities and Discourse Relations for Coherence Assessment](https://arxiv.org/abs/2509.04182)

*Wei Liu, Michael Strube*

**Main category:** cs.CL

**Keywords:** coherence, entities, discourse relations, modeling, linguistics

**Relevance Score:** 4

**TL;DR:** This study explores joint modeling of entities and discourse relations for coherence assessment in linguistics, finding that integrating both significantly improves model performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Most existing coherence modeling focuses on either entity features or discourse relation features, neglecting the benefits of combining both.

**Method:** Two methods for jointly modeling entities and discourse relations were explored, with experiments conducted on three benchmark datasets to assess coherence.

**Key Contributions:**

	1. Joint modeling of entities and discourse relations for coherence assessment
	2. Demonstration of significant performance enhancement in coherence models
	3. Exploration of combined feature integration on benchmark datasets

**Result:** Integrating both types of features significantly enhances the performance of coherence models, demonstrating the advantages of simultaneous modeling.

**Limitations:** 

**Conclusion:** The study shows that a combined approach to modeling entities and discourse relations leads to better coherence assessment results.

**Abstract:** In linguistics, coherence can be achieved by different means, such as by maintaining reference to the same set of entities across sentences and by establishing discourse relations between them. However, most existing work on coherence modeling focuses exclusively on either entity features or discourse relation features, with little attention given to combining the two. In this study, we explore two methods for jointly modeling entities and discourse relations for coherence assessment. Experiments on three benchmark datasets show that integrating both types of features significantly enhances the performance of coherence models, highlighting the benefits of modeling both simultaneously for coherence evaluation.

</details>


### [60] [MAGneT: Coordinated Multi-Agent Generation of Synthetic Multi-Turn Mental Health Counseling Sessions](https://arxiv.org/abs/2509.04183)

*Aishik Mandal, Tanmoy Chakraborty, Iryna Gurevych*

**Main category:** cs.CL

**Keywords:** large language models, psychological counseling, multi-agent systems, evaluation metrics, synthetic data

**Relevance Score:** 10

**TL;DR:** MAGneT is a multi-agent framework for generating synthetic psychological counseling sessions using specialized LLMs, outperforming traditional methods in quality and diversity of sessions while offering a novel evaluation protocol.

**Read time:** 25 min

<details>
  <summary>Details</summary>

**Motivation:** To meet the growing demand for scalable psychological counseling and improve the quality of synthetic counseling sessions generated by LLMs.

**Method:** MAGneT decomposes the response generation process into sub-tasks managed by specialized LLM agents, along with a unified evaluation framework that incorporates diverse metrics for assessing counseling data quality.

**Key Contributions:**

	1. Introduction of a multi-agent framework for counseling generation
	2. Enhanced evaluation protocols for assessing session quality
	3. Empirical validation showing significant performance improvements over baseline methods

**Result:** MAGneT significantly improves the quality and diversity of generated counseling sessions, achieving better performance in both general and CBT-specific counseling skills as validated by expert evaluations.

**Limitations:** 

**Conclusion:** MAGneT demonstrates superior performance compared to existing methods, with a strong preference from experts for its generated sessions, and provides publicly available code and data for further research.

**Abstract:** The growing demand for scalable psychological counseling highlights the need for fine-tuning open-source Large Language Models (LLMs) with high-quality, privacy-compliant data, yet such data remains scarce. Here we introduce MAGneT, a novel multi-agent framework for synthetic psychological counseling session generation that decomposes counselor response generation into coordinated sub-tasks handled by specialized LLM agents, each modeling a key psychological technique. Unlike prior single-agent approaches, MAGneT better captures the structure and nuance of real counseling. In addition, we address inconsistencies in prior evaluation protocols by proposing a unified evaluation framework integrating diverse automatic and expert metrics. Furthermore, we expand the expert evaluations from four aspects of counseling in previous works to nine aspects, enabling a more thorough and robust assessment of data quality. Empirical results show that MAGneT significantly outperforms existing methods in quality, diversity, and therapeutic alignment of the generated counseling sessions, improving general counseling skills by 3.2% and CBT-specific skills by 4.3% on average on cognitive therapy rating scale (CTRS). Crucially, experts prefer MAGneT-generated sessions in 77.2% of cases on average across all aspects. Moreover, fine-tuning an open-source model on MAGneT-generated sessions shows better performance, with improvements of 6.3% on general counseling skills and 7.3% on CBT-specific skills on average on CTRS over those fine-tuned with sessions generated by baseline methods. We also make our code and data public.

</details>


### [61] [Explicit and Implicit Data Augmentation for Social Event Detection](https://arxiv.org/abs/2509.04202)

*Congbo Ma, Yuxia Wang, Jia Wu, Jian Yang, Jing Du, Zitai Qiu, Qing Li, Hu Wang, Preslav Nakov*

**Main category:** cs.CL

**Keywords:** social event detection, data augmentation, large language models, feature perturbation, machine learning

**Relevance Score:** 7

**TL;DR:** Develops SED-Aug, an augmentation framework for detecting social events in social media, improving model performance without extensive labeled data.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To improve social event detection (SED) due to the high cost and labor-intensiveness of data annotation.

**Method:** Introduces a dual augmentation framework that includes explicit text-based augmentation using large language models and implicit feature-space augmentation with novel perturbation techniques.

**Key Contributions:**

	1. Proposed a plug-and-play dual augmentation framework for SED.
	2. Implemented explicit augmentation strategies using large language models.
	3. Developed novel perturbation techniques for feature space augmentation.

**Result:** SED-Aug outperforms existing models by 17.67% on Twitter2012 and 15.57% on Twitter2018 datasets in average F1 score.

**Limitations:** 

**Conclusion:** The proposed framework enhances data diversity and model robustness for social event detection, minimizing the need for extensive labeled data.

**Abstract:** Social event detection involves identifying and categorizing important events from social media, which relies on labeled data, but annotation is costly and labor-intensive. To address this problem, we propose Augmentation framework for Social Event Detection (SED-Aug), a plug-and-play dual augmentation framework, which combines explicit text-based and implicit feature-space augmentation to enhance data diversity and model robustness. The explicit augmentation utilizes large language models to enhance textual information through five diverse generation strategies. For implicit augmentation, we design five novel perturbation techniques that operate in the feature space on structural fused embeddings. These perturbations are crafted to keep the semantic and relational properties of the embeddings and make them more diverse. Specifically, SED-Aug outperforms the best baseline model by approximately 17.67% on the Twitter2012 dataset and by about 15.57% on the Twitter2018 dataset in terms of the average F1 score. The code is available at GitHub: https://github.com/congboma/SED-Aug.

</details>


### [62] [Inverse IFEval: Can LLMs Unlearn Stubborn Training Conventions to Follow Real Instructions?](https://arxiv.org/abs/2509.04292)

*Qinyan Zhang, Xinping Lei, Ruijie Miao, Yu Fu, Haojie Fan, Le Chang, Jiafan Hou, Dingling Zhang, Zhongfei Hou, Ziqiang Yang, Changxin Pu, Fei Hu, Jingkai Liu, Mengyun Liu, Yang Liu, Xiang Gao, Jiaheng Liu, Tong Yang, Zaiyuan Wang, Ge Zhang, Wenhao Huang*

**Main category:** cs.CL

**Keywords:** Large Language Models, Inverse IFEval, Cognitive Inertia, Instruction Following, Benchmarking

**Relevance Score:** 8

**TL;DR:** The paper proposes Inverse IFEval, a benchmark to evaluate Large Language Models (LLMs) on their ability to follow adversarial instructions that conflict with learned patterns during fine-tuning.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To address the cognitive inertia exhibited by LLMs when following conflicting instructions, highlighting the need for better adaptability in instruction-following tasks.

**Method:** Inverse IFEval consists of 1012 high-quality questions across 23 domains, structured around eight challenge types designed to test the adaptability of LLMs.

**Key Contributions:**

	1. Introduction of Inverse IFEval benchmark for evaluating LLMs' adaptability to adversarial instructions.
	2. Construction of a dataset with diverse questions to test cognitive biases in LLMs.
	3. Emphasis on the need for alignment efforts to consider adaptability and flexibility, moving beyond mere fluency and correctness.

**Result:** Experiments with leading LLMs confirm the importance of measuring adaptability in addition to fluency and correctness, indicating that LLMs struggle with instruction compliance in unconventional contexts.

**Limitations:** 

**Conclusion:** Inverse IFEval can serve as a diagnostic tool for identifying biases in LLMs and guiding future research on improving their instruction-following capabilities.

**Abstract:** Large Language Models (LLMs) achieve strong performance on diverse tasks but often exhibit cognitive inertia, struggling to follow instructions that conflict with the standardized patterns learned during supervised fine-tuning (SFT). To evaluate this limitation, we propose Inverse IFEval, a benchmark that measures models Counter-intuitive Abilitytheir capacity to override training-induced biases and comply with adversarial instructions. Inverse IFEval introduces eight types of such challenges, including Question Correction, Intentional Textual Flaws, Code without Comments, and Counterfactual Answering. Using a human-in-the-loop pipeline, we construct a dataset of 1012 high-quality Chinese and English questions across 23 domains, evaluated under an optimized LLM-as-a-Judge framework. Experiments on existing leading LLMs demonstrate the necessity of our proposed Inverse IFEval benchmark. Our findings emphasize that future alignment efforts should not only pursue fluency and factual correctness but also account for adaptability under unconventional contexts. We hope that Inverse IFEval serves as both a diagnostic tool and a foundation for developing methods that mitigate cognitive inertia, reduce overfitting to narrow patterns, and ultimately enhance the instruction-following reliability of LLMs in diverse and unpredictable real-world scenarios.

</details>


### [63] [Facts Fade Fast: Evaluating Memorization of Outdated Medical Knowledge in Large Language Models](https://arxiv.org/abs/2509.04304)

*Juraj Vladika, Mahdi Dhaini, Florian Matthes*

**Main category:** cs.CL

**Keywords:** Large Language Models, Healthcare, Question-Answering Datasets, Medical AI, Clinical Reasoning

**Relevance Score:** 9

**TL;DR:** This paper investigates the limitations of Large Language Models (LLMs) in delivering up-to-date medical advice, introducing two QA datasets to address this issue.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To enhance healthcare capabilities using LLMs while acknowledging their limitations in providing up-to-date medical recommendations due to outdated training data.

**Method:** Introduction of two QA datasets, MedRevQA and MedChangeQA, and evaluation of eight prominent LLMs on these datasets to assess reliance on outdated knowledge.

**Key Contributions:**

	1. Introduction of MedRevQA and MedChangeQA datasets
	2. Evaluation of LLMs revealing outdated knowledge reliance
	3. Proposed strategies for mitigating outdated medical advice in AI systems

**Result:** All evaluated LLMs demonstrated a consistent reliance on outdated medical knowledge, leading to potential harmful medical advice.

**Limitations:** Focused primarily on the evaluation of existing LLMs without proposing direct solutions to data acquisition.

**Conclusion:** Future directions for improving the recency and reliability of medical AI systems are proposed, addressing the limitations identified in LLMs.

**Abstract:** The growing capabilities of Large Language Models (LLMs) show significant potential to enhance healthcare by assisting medical researchers and physicians. However, their reliance on static training data is a major risk when medical recommendations evolve with new research and developments. When LLMs memorize outdated medical knowledge, they can provide harmful advice or fail at clinical reasoning tasks. To investigate this problem, we introduce two novel question-answering (QA) datasets derived from systematic reviews: MedRevQA (16,501 QA pairs covering general biomedical knowledge) and MedChangeQA (a subset of 512 QA pairs where medical consensus has changed over time). Our evaluation of eight prominent LLMs on the datasets reveals consistent reliance on outdated knowledge across all models. We additionally analyze the influence of obsolete pre-training data and training strategies to explain this phenomenon and propose future directions for mitigation, laying the groundwork for developing more current and reliable medical AI systems.

</details>


### [64] [PARCO: Phoneme-Augmented Robust Contextual ASR via Contrastive Entity Disambiguation](https://arxiv.org/abs/2509.04357)

*Jiajun He, Naoki Sawada, Koichi Miyazaki, Tomoki Toda*

**Main category:** cs.CL

**Keywords:** Automatic Speech Recognition, Entity Disambiguation, Phoneme Aware Encoding

**Relevance Score:** 6

**TL;DR:** The paper presents PARCO, a novel approach to improve automatic speech recognition systems by integrating phoneme-aware encoding and entity disambiguation.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Automatic speech recognition systems face challenges in recognizing domain-specific entities and managing homophones, which can lead to recognition errors.

**Method:** The proposed method, PARCO, combines phoneme-aware encoding, contrastive entity disambiguation, and hierarchical entity filtering to enhance recognition accuracy.

**Key Contributions:**

	1. Introduction of phoneme-aware encoding in ASR systems.
	2. Utilization of contrastive entity disambiguation to improve entity recognition.
	3. Development of a hierarchical entity filtering method to enhance recognition performance.

**Result:** PARCO achieves a Character Error Rate (CER) of 4.22% on Chinese AISHELL-1 and a Word Error Rate (WER) of 11.14% on English DATA2, outperforming existing methods.

**Limitations:** 

**Conclusion:** The integration of phoneme-sensitive processing and entity-level supervision effectively reduces false positives and improves recognition accuracy in ASR systems.

**Abstract:** Automatic speech recognition (ASR) systems struggle with domain-specific named entities, especially homophones. Contextual ASR improves recognition but often fails to capture fine-grained phoneme variations due to limited entity diversity. Moreover, prior methods treat entities as independent tokens, leading to incomplete multi-token biasing. To address these issues, we propose Phoneme-Augmented Robust Contextual ASR via COntrastive entity disambiguation (PARCO), which integrates phoneme-aware encoding, contrastive entity disambiguation, entity-level supervision, and hierarchical entity filtering. These components enhance phonetic discrimination, ensure complete entity retrieval, and reduce false positives under uncertainty. Experiments show that PARCO achieves CER of 4.22% on Chinese AISHELL-1 and WER of 11.14% on English DATA2 under 1,000 distractors, significantly outperforming baselines. PARCO also demonstrates robust gains on out-of-domain datasets like THCHS-30 and LibriSpeech.

</details>


### [65] [Measuring Bias or Measuring the Task: Understanding the Brittle Nature of LLM Gender Biases](https://arxiv.org/abs/2509.04373)

*Bufan Gao, Elisa Kreiss*

**Main category:** cs.CL

**Keywords:** gender bias, large language models, prompt sensitivity, evaluation metrics, NLP benchmarking

**Relevance Score:** 8

**TL;DR:** This paper examines the impact of prompting conditions on measured gender bias in LLMs, revealing that minor prompt changes can significantly affect bias outcomes.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To address concerns regarding gender bias in LLMs in socially impactful settings and to understand how different evaluative prompt designs affect bias measurements.

**Method:** The study tests LLMs under varying prompt conditions—highlighting both the testing context and gender-related content—across four task formats using token-probability and discrete-choice metrics.

**Key Contributions:**

	1. Demonstration of the sensitivity of bias measurements to prompt design.
	2. Revelation of the differences in bias amplification between discrete-choice and probabilistic measures.
	3. Highlighting the need for improved ecological validity in NLP benchmarking.

**Result:** Minor changes in prompt conditions significantly altered the measured outcomes of gender bias, sometimes reversing the results, with discrete-choice metrics amplifying bias compared to probabilistic measures.

**Limitations:** Focus primarily on the effects of prompt design without exploring other potential sources of bias.

**Conclusion:** The findings raise concerns about the stability of gender bias evaluations in LLMs and challenge the NLP community to assess the ecological validity of evaluation benchmarks.

**Abstract:** As LLMs are increasingly applied in socially impactful settings, concerns about gender bias have prompted growing efforts both to measure and mitigate such bias. These efforts often rely on evaluation tasks that differ from natural language distributions, as they typically involve carefully constructed task prompts that overtly or covertly signal the presence of gender bias-related content. In this paper, we examine how signaling the evaluative purpose of a task impacts measured gender bias in LLMs. Concretely, we test models under prompt conditions that (1) make the testing context salient, and (2) make gender-focused content salient. We then assess prompt sensitivity across four task formats with both token-probability and discrete-choice metrics. We find that even minor prompt changes can substantially alter bias outcomes, sometimes reversing their direction entirely. Discrete-choice metrics further tend to amplify bias relative to probabilistic measures. These findings do not only highlight the brittleness of LLM gender bias evaluations but open a new puzzle for the NLP benchmarking and development community: To what extent can well-controlled testing designs trigger LLM ``testing mode'' performance, and what does this mean for the ecological validity of future benchmarks.

</details>


### [66] [Can Language Models Handle a Non-Gregorian Calendar?](https://arxiv.org/abs/2509.04432)

*Mutsumi Sasaki, Go Kamoda, Ryosuke Takahashi, Kosuke Sato, Kentaro Inui, Keisuke Sakaguchi, Benjamin Heinzerling*

**Main category:** cs.CL

**Keywords:** language models, temporal reasoning, Japanese calendar, non-Gregorian, cultural understanding

**Relevance Score:** 4

**TL;DR:** This paper evaluates how well language models handle the Japanese calendar, revealing that models struggle with calendar arithmetic and consistency, even those designed for Japanese.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper seeks to address the gap in understanding how language models deal with non-Gregorian calendars, particularly the Japanese calendar, which represents culturally specific time concepts.

**Method:** The authors created datasets for four tasks requiring temporal knowledge and reasoning to assess English-centric and Japanese-centric language models' performance on the Japanese calendar.

**Key Contributions:**

	1. Systematic evaluation of language models on the Japanese calendar.
	2. Creation of datasets for testing temporal reasoning and knowledge.
	3. Highlighting the challenges faced by both English-centric and Japanese-centric models.

**Result:** The evaluation shows that while some models can perform basic calendar conversions, they struggle with Japanese calendar arithmetic and maintaining consistency across different calendar systems.

**Limitations:** The study focuses only on the Japanese calendar and does not evaluate other non-Gregorian systems.

**Conclusion:** Improving language models for culture-specific calendar understanding is essential, as current models notably underperform in this area.

**Abstract:** Temporal reasoning and knowledge are essential capabilities for language models (LMs). While much prior work has analyzed and improved temporal reasoning in LMs, most studies have focused solely on the Gregorian calendar. However, many non-Gregorian systems, such as the Japanese, Hijri, and Hebrew calendars, are in active use and reflect culturally grounded conceptions of time. If and how well current LMs can accurately handle such non-Gregorian calendars has not been evaluated so far. Here, we present a systematic evaluation of how well open-source LMs handle one such non-Gregorian system: the Japanese calendar. For our evaluation, we create datasets for four tasks that require both temporal knowledge and temporal reasoning. Evaluating a range of English-centric and Japanese-centric LMs, we find that some models can perform calendar conversions, but even Japanese-centric models struggle with Japanese-calendar arithmetic and with maintaining consistency across calendars. Our results highlight the importance of developing LMs that are better equipped for culture-specific calendar understanding.

</details>


### [67] [MyProfessors: Mining Turkish Student Reviews](https://arxiv.org/abs/2109.02325)

*Ibrahim Faruk Ceylan, Necmettin Bera Calik*

**Main category:** cs.CL

**Keywords:** student reviews, dataset, Turkish language, ratings, bias

**Relevance Score:** 2

**TL;DR:** Introduction of the Hocalarim dataset, the largest Turkish student review dataset, analyzing professor reviews.

**Read time:** 5 min

<details>
  <summary>Details</summary>

**Motivation:** To provide insights into the impact of institution type on professor ratings and examine student bias in feedback.

**Method:** Statistical analysis of over 5000 professor reviews rated on a 1 to 5 scale.

**Key Contributions:**

	1. Largest dataset of Turkish student reviews
	2. Analysis of institution type impact on ratings
	3. Examination of student bias in feedback

**Result:** Identification of trends in ratings according to institution type and correlation of bias in student feedback.

**Limitations:** Data collection process had scraping errors leading to withdrawal of the paper.

**Conclusion:** The dataset was withdrawn due to scraping errors, affecting the reliability of the results.

**Abstract:** We introduce Hocalarim (MyProfessors), the largest student review dataset available for the Turkish language. It consists of over 5000 professor reviews left online by students, with different aspects of education rated on a scale of 1 to 5 stars. We investigate the properties of the dataset and present its statistics. We examine the impact of students' institution type on their ratings and the correlation of students' bias to give positive or negative feedback.

</details>


### [68] [Mitigating Bias in Text Classification via Prompt-Based Text Transformation](https://arxiv.org/abs/2305.06166)

*Charmaine Barker, Dimitar Kazakov*

**Main category:** cs.CL

**Keywords:** bias mitigation, text classification, language models, prompt-based rewriting, demographic signals

**Relevance Score:** 8

**TL;DR:** This paper examines how prompt-based rewriting in ChatGPT can mitigate bias from demographic signals in text while preserving the original meaning.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address potential biases in language models that arise from linguistic features associated with demographic characteristics, which can lead to unfair automated decision-making outcomes.

**Method:** The authors tested different rewriting strategies including simplification, neutralisation, localisation, and formalisation to see their impact on bias reduction in language model outputs.

**Key Contributions:**

	1. Demonstration of effective prompt-based rewriting techniques to mitigate model bias
	2. Evidence of maintaining semantic integrity in rewritten texts
	3. Statistical analysis showing reduced model reliance on demographic signals

**Result:** The experimentation showed a significant reduction in the accuracy of location classification across various models, indicating less reliance on demographic cues, particularly in terms of location, while maintaining the core meaning of the text.

**Limitations:** The study focuses on specific rewriting techniques and may not generalize to all language models or contexts.

**Conclusion:** Prompt-based rewriting is a viable method for reducing bias in text classification tasks without losing the essential meanings of the texts.

**Abstract:** The presence of specific linguistic signals particular to a certain sub-group can become highly salient to language models during training. In automated decision-making settings, this may lead to biased outcomes when models rely on cues that correlate with protected characteristics. We investigate whether prompting ChatGPT to rewrite text using simplification, neutralisation, localisation, and formalisation can reduce demographic signals while preserving meaning. Experimental results show a statistically significant drop in location classification accuracy across multiple models after transformation, suggesting reduced reliance on group-specific language. At the same time, sentiment analysis and rating prediction tasks confirm that the core meaning of the reviews remains greatly intact. These results suggest that prompt-based rewriting offers a practical and generalisable approach for mitigating bias in text classification.

</details>


### [69] [Exploring Linguistic Features for Turkish Text Readability](https://arxiv.org/abs/2306.03774)

*Ahmet Yavuz Uluslu, Gerold Schneider*

**Main category:** cs.CL

**Keywords:** readability assessment, Turkish texts, neural networks, linguistic features, automation

**Relevance Score:** 3

**TL;DR:** This paper studies the automatic readability assessment of Turkish texts using neural networks and linguistic features.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** To develop an advanced tool for assessing the readability of Turkish texts and to compare traditional readability formulas with modern automated methods.

**Method:** The authors combine neural network models with linguistic features across various levels, including lexical, morphological, syntactic, and discourse.

**Key Contributions:**

	1. First comprehensive study on Turkish text readability
	2. Integration of neural networks with linguistic features
	3. Comparison between traditional and automated readability assessment methods.

**Result:** The study evaluates the effectiveness of traditional readability formulas against automated methods and identifies key linguistic features that influence readability.

**Limitations:** 

**Conclusion:** The findings provide insights into Turkish text readability and highlight the strengths of modern methodologies over traditional ones.

**Abstract:** This paper presents the first comprehensive study on automatic readability assessment of Turkish texts. We combine state-of-the-art neural network models with linguistic features at lexical, morphological, syntactic and discourse levels to develop an advanced readability tool. We evaluate the effectiveness of traditional readability formulas compared to modern automated methods and identify key linguistic features that determine the readability of Turkish texts.

</details>


### [70] [R2C2-Coder: Enhancing and Benchmarking Real-world Repository-level Code Completion Abilities of Code Large Language Models](https://arxiv.org/abs/2406.01359)

*Ken Deng, Jiaheng Liu, He Zhu, Congnan Liu, Jingxin Li, Jiakai Wang, Peng Zhao, Chenchen Zhang, Yanan Wu, Xueqiao Yin, Yuanxing Zhang, Zizheng Zhan, Wenbo Su, Bangyu Xiang, Tiezheng Ge, Bo Zheng*

**Main category:** cs.CL

**Keywords:** code completion, Large Language Models, benchmarking, repository-level, software development

**Relevance Score:** 7

**TL;DR:** The paper introduces R2C2-Coder, a tool designed to improve and benchmark repository-level code completion for Large Language Models (LLMs) by enhancing prompt construction and establishing a comprehensive benchmark.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Existing repository-level code completion methods don't leverage the full project context effectively, and current benchmarks are limited in scope, necessitating a robust solution.

**Method:** The R2C2-Coder combines R2C2-Enhance for improved candidate retrieval and prompt assembly, along with R2C2-Bench for diverse benchmarking that simulates real-world scenarios.

**Key Contributions:**

	1. Introduction of R2C2-Enhance for better prompt construction
	2. Development of R2C2-Bench to evaluate repository-level completion
	3. Demonstration of improved performance in benchmark evaluations

**Result:** Extensive evaluations show that R2C2-Coder outperforms existing methods, demonstrating significant improvements in repository-level code completion.

**Limitations:** 

**Conclusion:** R2C2-Coder effectively enhances code completion capabilities in real-world repositories and sets a new standard for benchmarking these abilities.

**Abstract:** Code completion models have made significant progress in recent years. Recently, repository-level code completion has drawn more attention in modern software development, and several baseline methods and benchmarks have been proposed. However, existing repository-level code completion methods often fall short of fully using the extensive context of a project repository, such as the intricacies of relevant files and class hierarchies. Besides, the existing benchmarks usually focus on limited code completion scenarios, which cannot reflect the repository-level code completion abilities well of existing methods. To address these limitations, we propose the R2C2-Coder to enhance and benchmark the real-world repository-level code completion abilities of code Large Language Models, where the R2C2-Coder includes a code prompt construction method R2C2-Enhance and a well-designed benchmark R2C2-Bench. Specifically, first, in R2C2-Enhance, we first construct the candidate retrieval pool and then assemble the completion prompt by retrieving from the retrieval pool for each completion cursor position. Second, based on R2C2 -Enhance, we can construct a more challenging and diverse R2C2-Bench with training, validation and test splits, where a context perturbation strategy is proposed to simulate the real-world repository-level code completion well. Extensive results on multiple benchmarks demonstrate the effectiveness of our R2C2-Coder.

</details>


### [71] [ACING: Actor-Critic for Instruction Learning in Black-Box LLMs](https://arxiv.org/abs/2411.12736)

*Salma Kharrat, Fares Fourati, Marco Canini*

**Main category:** cs.CL

**Keywords:** Large Language Models, Instruction Optimization, Reinforcement Learning, Human-Computer Interaction, AI Applications

**Relevance Score:** 9

**TL;DR:** Introducing ACING, a framework for optimizing instructions for Large Language Models (LLMs) using reinforcement learning to enhance performance.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The effectiveness of LLMs heavily relies on the quality of human-crafted instructions, necessitating automated optimization methods, especially for black-box models.

**Method:** We present ACING, an actor-critic reinforcement learning framework that treats instruction optimization as a stateless, continuous-action problem, utilizing only black-box feedback.

**Key Contributions:**

	1. Development of ACING framework for instruction optimization using reinforcement learning.
	2. Demonstrated performance improvement over human-written prompts in diverse tasks.
	3. Open-source implementation available for further research.

**Result:** ACING outperforms human-written prompts in 76% of instruction-induction tasks, showing up to 33 points improvement and significant gains in summarization and chain-of-thought reasoning tasks.

**Limitations:** 

**Conclusion:** The framework demonstrates robustness and efficiency in discovering high-quality prompts automatically.

**Abstract:** The effectiveness of Large Language Models (LLMs) in solving tasks depends significantly on the quality of their instructions, which often require substantial human effort to craft. This underscores the need for automated instruction optimization. However, optimizing instructions is particularly challenging when working with black-box LLMs, where model parameters and gradients are inaccessible. We introduce ACING, an actor-critic reinforcement learning framework that formulates instruction optimization as a stateless, continuous-action problem, enabling exploration of infinite instruction spaces using only black-box feedback. ACING automatically discovers prompts that outperform human-written prompts in 76% of instruction-induction tasks, with gains of up to 33 points and a 10-point median improvement over the best automatic baseline in 33 tasks spanning instruction-induction, summarization, and chain-of-thought reasoning. Extensive ablations highlight its robustness and efficiency. An implementation of ACING is available at https://github.com/salmakh1/ACING.

</details>


### [72] [Small Changes, Large Consequences: Analyzing the Allocational Fairness of LLMs in Hiring Contexts](https://arxiv.org/abs/2501.04316)

*Preethi Seshadri, Hongyu Chen, Sameer Singh, Seraphina Goldfarb-Tarrant*

**Main category:** cs.CL

**Keywords:** Large language models, allocational fairness, hiring systems, demographic bias, applicant ranking

**Relevance Score:** 9

**TL;DR:** This paper examines the allocational fairness of LLM-based hiring systems, revealing biases in resume summarization and applicant ranking across demographic groups.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The study aims to address the understudied potential for unfair decision-making in LLM applications within high-stakes hiring processes.

**Method:** The authors created a synthetic resume dataset with controlled perturbations and curated job postings to investigate model behavior differences across demographic groups.

**Key Contributions:**

	1. Investigation of allocational fairness in LLM hiring systems
	2. Analysis of demographic disparities in LLM-generated content
	3. Insights into model sensitivity and brittleness affecting fairness

**Result:** Findings indicate significant biases in LLM-generated summaries and retrieval patterns, with more frequent disparities observed for race than gender, and high sensitivity to demographic changes.

**Limitations:** Focus on a synthetic dataset may limit generalizability to real-world hiring scenarios.

**Conclusion:** LLM-based hiring systems show notable biases that can lead to discriminatory outcomes, especially during the retrieval stage.

**Abstract:** Large language models (LLMs) are increasingly being deployed in high-stakes applications like hiring, yet their potential for unfair decision-making remains understudied in generative and retrieval settings. In this work, we examine the allocational fairness of LLM-based hiring systems through two tasks that reflect actual HR usage: resume summarization and applicant ranking. By constructing a synthetic resume dataset with controlled perturbations and curating job postings, we investigate whether model behavior differs across demographic groups. Our findings reveal that generated summaries exhibit meaningful differences more frequently for race than for gender perturbations. Models also display non-uniform retrieval selection patterns across demographic groups and exhibit high ranking sensitivity to both gender and race perturbations. Surprisingly, retrieval models can show comparable sensitivity to both demographic and non-demographic changes, suggesting that fairness issues may stem from broader model brittleness. Overall, our results indicate that LLM-based hiring systems, especially in the retrieval stage, can exhibit notable biases that lead to discriminatory outcomes in real-world contexts.

</details>


### [73] [A Survey of Graph Retrieval-Augmented Generation for Customized Large Language Models](https://arxiv.org/abs/2501.13958)

*Qinggang Zhang, Shengyuan Chen, Yuanchen Bei, Zheng Yuan, Huachi Zhou, Zijin Hong, Hao Chen, Yilin Xiao, Chuang Zhou, Yi Chang, Xiao Huang*

**Main category:** cs.CL

**Keywords:** Graph-based Retrieval-Augmented Generation, Large Language Models, Domain-specific Applications

**Relevance Score:** 9

**TL;DR:** This survey presents Graph-based Retrieval-Augmented Generation (GraphRAG), an innovative approach to enhance domain-specific large language models (LLMs) by addressing traditional challenges faced by retrieval-augmented systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** Despite the potential of Retrieval-Augmented Generation (RAG) for customizing LLMs, traditional systems struggle with query understanding, knowledge integration, and efficiency in specialized domains.

**Method:** The paper systematically analyzes GraphRAG, which utilizes graph-structured knowledge representation, efficient graph-based retrieval techniques, and structure-aware knowledge integration algorithms to improve LLM performance in specific domains.

**Key Contributions:**

	1. Introduction of GraphRAG to enhance LLMs in specialized domains
	2. Development of efficient graph-based retrieval techniques
	3. Implementation of structure-aware knowledge integration algorithms

**Result:** GraphRAG demonstrates improved context-preserving knowledge retrieval and enhances reasoning capabilities in LLMs, allowing for more accurate and coherent output.

**Limitations:** 

**Conclusion:** The survey highlights the technical foundations, current applications, challenges, and future directions of GraphRAG, providing valuable resources for researchers.

**Abstract:** Large language models (LLMs) have demonstrated remarkable capabilities in a wide range of tasks, yet their application to specialized domains remains challenging due to the need for deep expertise. Retrieval-Augmented generation (RAG) has emerged as a promising solution to customize LLMs for professional fields by seamlessly integrating external knowledge bases, enabling real-time access to domain-specific expertise during inference. Despite its potential, traditional RAG systems, based on flat text retrieval, face three critical challenges: (i) complex query understanding in professional contexts, (ii) difficulties in knowledge integration across distributed sources, and (iii) system efficiency bottlenecks at scale. This survey presents a systematic analysis of Graph-based Retrieval-Augmented Generation (GraphRAG), a new paradigm that revolutionizes domain-specific LLM applications. GraphRAG addresses traditional RAG limitations through three key innovations: (i) graph-structured knowledge representation that explicitly captures entity relationships and domain hierarchies, (ii) efficient graph-based retrieval techniques that enable context-preserving knowledge retrieval with multihop reasoning ability, and (iii) structure-aware knowledge integration algorithms that leverage retrieved knowledge for accurate and logical coherent generation of LLMs. In this survey, we systematically analyze the technical foundations of GraphRAG and examine current implementations across various professional domains, identifying key technical challenges and promising research directions. All the related resources of GraphRAG, including research papers, open-source data, and projects, are collected for the community in https://github.com/DEEP-PolyU/Awesome-GraphRAG.

</details>


### [74] [An Unsupervised Natural Language Processing Pipeline for Assessing Referral Appropriateness](https://arxiv.org/abs/2501.14701)

*Vittorio Torri, Annamaria Bottelli, Michele Ercolanoni, Olivia Leoni, Francesca Ieva*

**Main category:** cs.CL

**Keywords:** Natural Language Processing, Referral Appropriateness, Healthcare Efficiency, Unsupervised Learning, Transformer Models

**Relevance Score:** 9

**TL;DR:** The paper proposes an unsupervised NLP pipeline for extracting and evaluating diagnostic referral reasons from free text in the Italian NHS, achieving high performance in identifying appropriateness of referrals.

**Read time:** 49 min

<details>
  <summary>Details</summary>

**Motivation:** To improve healthcare efficiency and reduce unnecessary procedures by assessing diagnostic referral appropriateness recorded as free text.

**Method:** The pipeline utilizes Transformer-based embeddings pre-trained on Italian medical texts to cluster referral reasons and evaluate them against appropriateness guidelines in an unsupervised manner.

**Key Contributions:**

	1. Development of an unsupervised NLP pipeline for medical referral analysis
	2. High performance metrics in real-world datasets
	3. Influence on policy for guideline adherence in the Lombardy Region

**Result:** It achieved high precision and recall in identifying referral reasons and assessing appropriateness, with notable findings influencing policy in the Lombardy Region.

**Limitations:** 

**Conclusion:** The study delivers a scalable unsupervised NLP solution for analyzing large datasets of diagnostic referrals, aiding public health authorities in monitoring practices.

**Abstract:** Objective: Assessing the appropriateness of diagnostic referrals is critical for improving healthcare efficiency and reducing unnecessary procedures. However, this task becomes challenging when referral reasons are recorded only as free text rather than structured codes, like in the Italian NHS. To address this gap, we propose a fully unsupervised Natural Language Processing (NLP) pipeline capable of extracting and evaluating referral reasons without relying on labelled datasets.   Methods: Our pipeline leverages Transformer-based embeddings pre-trained on Italian medical texts to cluster referral reasons and assess their alignment with appropriateness guidelines. It operates in an unsupervised setting and is designed to generalize across different examination types. We analyzed two complete regional datasets from the Lombardy Region (Italy), covering all referrals between 2019 and 2021 for venous echocolordoppler of the lower limbs (ECD;n=496,971; development) and flexible endoscope colonoscopy (FEC; n=407,949; testing only). For both, a random sample of 1,000 referrals was manually annotated to measure performance.   Results: The pipeline achieved high performance in identifying referral reasons (Prec=92.43% (ECD), 93.59% (FEC); Rec=83.28% (ECD), 92.70% (FEC)) and appropriateness (Prec=93.58% (ECD), 94.66% (FEC); Rec=91.52% (ECD), 93.96% (FEC)). At the regional level, the analysis identified relevant inappropriate referral groups and variation across contexts, findings that informed a new Lombardy Region resolution to reinforce guideline adherence.   Conclusions: This study presents a robust, scalable, unsupervised NLP pipeline for assessing referral appropriateness in large, real-world datasets. It demonstrates how such data can be effectively leveraged, providing public health authorities with a deployable AI tool to monitor practices and support evidence-based policy.

</details>


### [75] [HamRaz: A Culture-Based Persian Conversation Dataset for Person-Centered Therapy Using LLM Agents](https://arxiv.org/abs/2502.05982)

*Mohammad Amin Abbasi, Farnaz Sadat Mirnezami, Ali Neshati, Hassan Naderi*

**Main category:** cs.CL

**Keywords:** mental health, AI, language model, cultural adaptation, therapeutic quality

**Relevance Score:** 9

**TL;DR:** HamRaz is a Persian-language dataset designed for AI-assisted mental health support, focusing on Person-Centered Therapy and featuring advanced evaluation metrics.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** The paper aims to address the lack of culturally adapted mental health resources in Persian-speaking communities by creating a relevant dataset and evaluation framework.

**Method:** The methodology includes combining script-based dialogues with adaptive large language models to create realistic therapeutic interactions, alongside a dual-framework for evaluation.

**Key Contributions:**

	1. Introduction of a culturally adapted dataset for Persian-language mental health support.
	2. Development of HamRazEval, a novel evaluation framework for assessing conversation quality.
	3. Demonstration of superior performance in key therapeutic dimensions compared to existing baselines.

**Result:** Human evaluations demonstrate that the HamRaz dataset outperforms existing models in terms of empathy, coherence, and realism.

**Limitations:** Limitations include potential biases in data collection and the restricted focus on Persian-language contexts.

**Conclusion:** The development of HamRaz not only improves mental health support for Persian speakers but also contributes to the Digital Humanities by addressing cultural and language gaps.

**Abstract:** We present HamRaz, a culturally adapted Persian-language dataset for AI-assisted mental health support, grounded in Person-Centered Therapy (PCT). To reflect real-world therapeutic challenges, we combine script-based dialogue with adaptive large language models (LLM) role-playing, capturing the ambiguity and emotional nuance of Persian-speaking clients. We introduce HamRazEval, a dual-framework for assessing conversational and therapeutic quality using General Metrics and specialized psychological relationship measures. Human evaluations show HamRaz outperforms existing baselines in empathy, coherence, and realism. This resource contributes to the Digital Humanities by bridging language, culture, and mental health in underrepresented communities.

</details>


### [76] [Autoformalization in the Wild: Assessing LLMs on Real-World Mathematical Definitions](https://arxiv.org/abs/2502.12065)

*Lan Zhang, Marco Valentino, Andre Freitas*

**Main category:** cs.CL

**Keywords:** autoformalization, Mathematics, LLMs, Isabelle/HOL, formal definitions

**Relevance Score:** 9

**TL;DR:** This paper explores the autoformalization of real-world mathematical definitions using LLMs, introducing new resources and evaluation metrics, and proposing methods for enhancing LLM performance.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the challenge of generalizing LLMs to sophisticated mathematical statements and aims to improve their ability to autoformalize definitions from real-world sources.

**Method:** The authors collected mathematical definitions from Wikipedia and arXiv papers and evaluated various LLMs on their autoformalization capabilities, focusing on formalizing these definitions into Isabelle/HOL.

**Key Contributions:**

	1. Introduction of two new resources for autoformalization (Def_Wiki and Def_ArXiv).
	2. Systematic evaluation of LLMs on real-world mathematical definitions.
	3. Strategies for enhancing LLM performance through external feedback and formal definition grounding.

**Result:** The study found that LLMs faced significant challenges with self-correction and aligning with formal mathematical libraries but showed notable performance improvements with structured refinement methods and definition grounding strategies, resulting in up to a 16% improvement in self-correction capabilities and a 43% reduction in undefined errors.

**Limitations:** LLMs are still challenged by self-correction and aligning with mathematical libraries despite the proposed enhancements.

**Conclusion:** The findings highlight the complexities of autoformalization and suggest that enhancements through additional context from formal libraries can improve LLM performance.

**Abstract:** Thanks to their linguistic capabilities, LLMs offer an opportunity to bridge the gap between informal mathematics and formal languages through autoformalization. However, it is still unclear how well LLMs generalize to sophisticated and naturally occurring mathematical statements. To address this gap, we investigate the task of autoformalizing real-world mathematical definitions: a critical component of mathematical discourse. Specifically, we introduce two novel resources for autoformalization, collecting definitions from Wikipedia (Def_Wiki) and arXiv papers (Def_ArXiv). We then systematically evaluate a range of LLMs, analyzing their ability to formalize definitions into Isabelle/HOL. Furthermore, we investigate strategies to enhance LLMs' performance including refinement through external feedback from Proof Assistants, and formal definition grounding, where we augment LLMs' formalizations through relevant contextual elements from formal mathematical libraries. Our findings reveal that definitions present a greater challenge compared to existing benchmarks, such as miniF2F. In particular, we found that LLMs still struggle with self-correction, and aligning with relevant mathematical libraries. At the same time, structured refinement methods and definition grounding strategies yield notable improvements of up to 16% on self-correction capabilities and 43% on the reduction of undefined errors, highlighting promising directions for enhancing LLM-based autoformalization in real-world scenarios.

</details>


### [77] [Improving Chain-of-Thought Reasoning via Quasi-Symbolic Abstractions](https://arxiv.org/abs/2502.12616)

*Leonardo Ranaldi, Marco Valentino, Andrè Freitas*

**Main category:** cs.CL

**Keywords:** Chain-of-Thought, Quasi-Symbolic Reasoning, Large Language Models, Natural Language Processing, Robustness

**Relevance Score:** 7

**TL;DR:** This paper introduces QuaSAR, a quasi-symbolic reasoning framework to enhance Chain-of-Thought strategies in LLMs by improving robustness and efficiency without complete formalization.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** The paper addresses the limitations of Chain-of-Thought explanations in LLMs, which often suffer from content biases affecting robustness and faithfulness.

**Method:** The authors propose QuaSAR, which enables LLMs to combine symbolic reasoning with natural language, allowing for relevant variable formalization without full translation to formal languages.

**Key Contributions:**

	1. Introduction of QuaSAR for quasi-symbolic reasoning in LLMs
	2. Demonstration of improved reasoning capabilities for smaller models
	3. Experimental validation showing significant accuracy improvement in CoT methods

**Result:** Experimental results show that QuaSAR can enhance CoT-based methods by up to 8% accuracy, improving robustness and consistency in reasoning tasks.

**Limitations:** 

**Conclusion:** QuaSAR represents a promising approach to enhance LLM reasoning capabilities while mitigating the need for complete symbolic formalization.

**Abstract:** Chain-of-Though (CoT) represents a common strategy for reasoning in Large Language Models (LLMs) by decomposing complex tasks into intermediate inference steps. However, explanations generated via CoT are susceptible to content biases that negatively affect their robustness and faithfulness. To mitigate existing limitations, recent work has proposed using logical formalisms coupled with external symbolic solvers. However, fully symbolic approaches possess the bottleneck of requiring a complete translation from natural language to formal languages, a process that affects efficiency and flexibility. To achieve a trade-off, this paper investigates methods to disentangle content from logical reasoning without a complete formalisation. In particular, we present QuaSAR (for Quasi-Symbolic Abstract Reasoning), a variation of CoT that guides LLMs to operate at a higher level of abstraction via quasi-symbolic explanations. Our framework leverages the capability of LLMs to formalise only relevant variables and predicates, enabling the coexistence of symbolic elements with natural language. We show the impact of QuaSAR for in-context learning and for constructing demonstrations to improve the reasoning capabilities of smaller models. Our experiments show that quasi-symbolic abstractions can improve CoT-based methods by up to 8% accuracy, enhancing robustness and consistency on challenging adversarial variations on both natural language (i.e. MMLU-Redux) and symbolic reasoning tasks (i.e., GSM-Symbolic).

</details>


### [78] [FutureGen: A RAG-based Approach to Generate the Future Work of Scientific Article](https://arxiv.org/abs/2503.16561)

*Ibrahim Al Azher, Miftahul Jannat Mokarrama, Zhishuai Guo, Sagnik Ray Choudhury, Hamed Alhoori*

**Main category:** cs.CL

**Keywords:** Future Work, Retrieval-Augmented Generation, Large Language Models, LLM feedback mechanism, Scientific research suggestions

**Relevance Score:** 8

**TL;DR:** This study investigates generating future work suggestions from scientific articles using RAG and LLMs, proposing enhancements through a feedback mechanism and evaluation framework.

**Read time:** 12 min

<details>
  <summary>Details</summary>

**Motivation:** To identify unexplored research areas and enhance the quality of future work suggestions for researchers.

**Method:** The study employs Retrieval-Augmented Generation (RAG) and integrates various Large Language Models (LLMs), utilizing an LLM feedback mechanism and an LLM-as-a-judge framework for evaluation.

**Key Contributions:**

	1. Development of an LLM feedback mechanism to improve content quality.
	2. Introduction of an LLM-as-a-judge framework for evaluating generated research directions.
	3. Demonstrated effectiveness of the RAG-based approach in generating future work suggestions.

**Result:** The RAG-based approach using GPT-4o mini, enhanced by an LLM feedback mechanism, outperforms other generation methods in both qualitative and quantitative evaluations.

**Limitations:** The study may be limited by the scope of LLM capabilities and the specific datasets used for training and evaluation.

**Conclusion:** The enhancements proposed not only improve the generated suggestions but also facilitate robust evaluations of the generated content.

**Abstract:** The Future Work section of a scientific article outlines potential research directions by identifying gaps and limitations of a current study. This section serves as a valuable resource for early-career researchers seeking unexplored areas and experienced researchers looking for new projects or collaborations. In this study, we generate future work suggestions from a scientific article. To enrich the generation process with broader insights and reduce the chance of missing important research directions, we use context from related papers using RAG. We experimented with various Large Language Models (LLMs) integrated into Retrieval-Augmented Generation (RAG). We incorporate an LLM feedback mechanism to enhance the quality of the generated content and introduce an LLM-as-a-judge framework for robust evaluation, assessing key aspects such as novelty, hallucination, and feasibility. Our results demonstrate that the RAG-based approach using GPT-4o mini, combined with an LLM feedback mechanism, outperforms other methods based on both qualitative and quantitative evaluations. Moreover, we conduct a human evaluation to assess the LLM as an extractor, generator, and feedback provider.

</details>


### [79] [EQ-Knight: A Memory-Augmented LLM Agent for Strategic Affective Gaming in Debt Recovery](https://arxiv.org/abs/2503.21080)

*Yunbo Long, Yuhan Liu, Liming Xu, Alexandra Brintrup*

**Main category:** cs.CL

**Keywords:** large language models, emotional strategy, game theory, credit collection, Hidden Markov Model

**Relevance Score:** 7

**TL;DR:** Introduces EQ-Knight, an LLM agent that optimizes emotional strategy in financial negotiations to defend creditor interests against manipulative debtors.

**Read time:** 8 min

<details>
  <summary>Details</summary>

**Motivation:** To address the vulnerabilities of traditional empathy-based chatbots in credit collection, especially against dishonest debtors.

**Method:** EQ-Knight integrates emotion memory and game-theoretic reasoning using a Hidden Markov Model (HMM) to track and predict debtor emotional states.

**Key Contributions:**

	1. Development of EQ-Knight, integrating emotion memory and game-theoretic reasoning.
	2. Demonstrating a significant reduction in concession losses in negotiations with dishonest debtors.
	3. Providing a new paradigm for LLMs in credit collection that enhances creditor protection.

**Result:** EQ-Knight achieved a 32% reduction in concession losses compared to conventional LLM negotiators without compromising recovery rates, especially in adversarial scenarios.

**Limitations:** The performance of EQ-Knight may vary depending on the implementation context and training data quality.

**Conclusion:** EQ-Knight balances emotional intelligence with tactical rigor, transforming LLMs into strategic emotion-defenders for credit agencies.

**Abstract:** Large language model-based chatbots have enhanced engagement in financial negotiations, but their overreliance on passive empathy introduces critical risks in credit collection. While empathy-driven approaches preserve client satisfaction in benign cases, they fail catastrophically against dishonest debtors--individuals who exploit conciliatory tactics to manipulate terms or evade repayment. Blindly prioritizing "customer experience" in such scenarios leads to creditor vulnerabilities: revenue leakage, moral hazard, and systemic exploitation. To address this, we propose EQ-Knight, an LLM agent that dynamically optimizes emotional strategy to defend creditor interests. Unlike naive empathy-centric bots, EQ-Knight integrates emotion memory and game-theoretic reasoning, powered by a Hidden Markov Model (HMM) to track and predict debtor emotional states. By analyzing both real-time and historical emotional cues, EQ-Knight strategically counters negative emotions (e.g., aggression, feigned distress) while preserving productive debtor relationships. Experiments demonstrate EQ-Knight's superiority over conventional LLM negotiators: it achieves a 32\% reduction in concession losses without compromising recovery rates, particularly in adversarial cases where debtors weaponize negative emotions (e.g., intimidation, guilt-tripping) to coerce concessions. For credit agencies, EQ-Knight transforms LLMs from high-risk "people-pleasers" into strategic emotion-defenders--balancing emotional intelligence with tactical rigor to enforce accountability and deter exploitation.

</details>


### [80] [MiniCPM4: Ultra-Efficient LLMs on End Devices](https://arxiv.org/abs/2506.07900)

*MiniCPM Team, Chaojun Xiao, Yuxuan Li, Xu Han, Yuzhuo Bai, Jie Cai, Haotian Chen, Wentong Chen, Xin Cong, Ganqu Cui, Ning Ding, Shengda Fan, Yewei Fang, Zixuan Fu, Wenyu Guan, Yitong Guan, Junshao Guo, Yufeng Han, Bingxiang He, Yuxiang Huang, Baoxi Ji, Cunliang Kong, Qiuzuo Li, Siyuan Li, Wenhao Li, Xin Li, Yanghao Li, Yishan Li, Zhen Li, Dan Liu, Biyuan Lin, Yankai Lin, Xiang Long, Quanyu Lu, Yaxi Lu, Peiyan Luo, Hongya Lyu, Litu Ou, Yinxu Pan, Lushi Pu, Zekai Qu, Qundong Shi, Zijun Song, Jiayuan Su, Zhou Su, Ao Sun, Xianghui Sun, Peijun Tang, Fangzheng Wang, Feng Wang, Shuo Wang, Yudong Wang, Zheng Wang, Yesai Wu, Zhenyu Xiao, Jie Xie, Zihao Xie, Xiaoyue Xu, Yukun Yan, Jiarui Yuan, Jinqian Zhang, Kaihuo Zhang, Lei Zhang, Linyue Zhang, Xueren Zhang, Yudi Zhang, Hengyu Zhao, Weilin Zhao, Weilun Zhao, Yuanqian Zhao, Zhi Zheng, Chuyue Zhou, Ge Zhou, Jie Zhou, Wei Zhou, Yanghao Zhou, Zihan Zhou, Zixuan Zhou, Zhiyuan Liu, Guoyang Zeng, Chao Jia, Dahai Li, Maosong Sun*

**Main category:** cs.CL

**Keywords:** large language models, sparse attention, on-device AI, training algorithms, model efficiency

**Relevance Score:** 9

**TL;DR:** MiniCPM4 is a highly efficient large language model tailored for end-side devices, enhancing performance in long-context processing through innovations in architecture, training data, algorithms, and inference systems.

**Read time:** 15 min

<details>
  <summary>Details</summary>

**Motivation:** To develop a large language model that is efficient for deployment on end-side devices while maintaining high performance in processing long contexts.

**Method:** Introduces InfLLM v2 for sparse attention, UltraClean for data filtering, ModelTunnel v2 for pre-training efficiency, and CPM.cu for improved inference processes, deploying different parameter versions of the model for varied requirements.

**Key Contributions:**

	1. Development of InfLLM v2 for efficient sparse attention.
	2. Creation of UltraClean and UltraChat v2 datasets for effective model training.
	3. Introduction of the hybrid reasoning model MiniCPM4.1.

**Result:** MiniCPM4 and its variant, MiniCPM4.1, demonstrate superior performance over similar-sized models, particularly in speed and understanding of long sequences as evidenced by comprehensive benchmark evaluations.

**Limitations:** 

**Conclusion:** MiniCPM4 represents a significant advancement in the efficiency of large language models for on-device applications, achieving notable results with reduced training data and enhanced processing capabilities.

**Abstract:** This paper introduces MiniCPM4, a highly efficient large language model (LLM) designed explicitly for end-side devices. We achieve this efficiency through systematic innovation in four key dimensions: model architecture, training data, training algorithms, and inference systems. Specifically, in terms of model architecture, we propose InfLLM v2, a trainable sparse attention mechanism that accelerates both prefilling and decoding phases for long-context processing. Regarding training data, we propose UltraClean, an efficient and accurate pre-training data filtering and generation strategy, and UltraChat v2, a comprehensive supervised fine-tuning dataset. These datasets enable satisfactory model performance to be achieved using just 8 trillion training tokens. Regarding training algorithms, we propose ModelTunnel v2 for efficient pre-training strategy search, and improve existing post-training methods by introducing chunk-wise rollout for load-balanced reinforcement learning and data-efficient tenary LLM, BitCPM. Regarding inference systems, we propose CPM.cu that integrates sparse attention, model quantization, and speculative sampling to achieve efficient prefilling and decoding. To meet diverse on-device requirements, MiniCPM4 is available in two versions, with 0.5B and 8B parameters, respectively. Furthermore, we construct a hybrid reasoning model, MiniCPM4.1, which can be used in both deep reasoning mode and non-reasoning mode. Evaluation results demonstrate that MiniCPM4 and MiniCPM4.1 outperform similar-sized open-source models across benchmarks, with the 8B variants showing significant speed improvements on long sequence understanding and generation.

</details>


### [81] [Transplant Then Regenerate: A New Paradigm for Text Data Augmentation](https://arxiv.org/abs/2508.14723)

*Guangzhan Wang, Hongyu Zhang, Beijun Shen, Xiaodong Gu*

**Main category:** cs.CL

**Keywords:** text augmentation, large language models, LMTransplant, deep learning, data augmentation

**Relevance Score:** 9

**TL;DR:** LMTransplant is a new text augmentation method using LLMs that enhances diversity and creativity while retaining original text attributes.

**Read time:** 10 min

<details>
  <summary>Details</summary>

**Motivation:** Traditional data augmentation methods in deep learning have limitations in generating diverse text variations. Large language models (LLMs) improve augmentation but require careful prompt engineering to control output style and structure.

**Method:** The proposed LMTransplant method incorporates seed text into a context developed by LLMs and then asks the LLM to regenerate a variant, enabling more diverse outputs while preserving core text properties.

**Key Contributions:**

	1. Introduction of LMTransplant, a novel augmentation paradigm using LLMs
	2. Improved diversity and creativity in text variation compared to traditional methods
	3. Evaluation showing superior performance and scalability of LMTransplant across multiple tasks.

**Result:** LMTransplant is shown to outperform traditional text augmentation methods in various tasks and demonstrates excellent scalability with increasing augmented data size.

**Limitations:** 

**Conclusion:** The LMTransplant method provides a more effective approach to text augmentation than existing methods by leveraging LLMs' knowledge while maintaining the integrity of the original text.

**Abstract:** Data augmentation is a critical technique in deep learning. Traditional methods like Back-translation typically focus on lexical-level rephrasing, which primarily produces variations with the same semantics. While large language models (LLMs) have enhanced text augmentation by their "knowledge emergence" capability, controlling the style and structure of these outputs remains challenging and requires meticulous prompt engineering. In this paper, we propose LMTransplant, a novel text augmentation paradigm leveraging LLMs. The core idea of LMTransplant is transplant-then-regenerate: incorporating seed text into a context expanded by LLM, and asking the LLM to regenerate a variant based on the expanded context. This strategy allows the model to create more diverse and creative content-level variants by fully leveraging the knowledge embedded in LLMs, while preserving the core attributes of the original text. We evaluate LMTransplant across various text-related tasks, demonstrating its superior performance over existing text augmentation methods. Moreover, LMTransplant demonstrates exceptional scalability as the size of augmented data grows.

</details>
